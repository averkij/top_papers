{
    "paper_title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
    "authors": [
        "Rang Li",
        "Lei Li",
        "Shuhuai Ren",
        "Hao Tian",
        "Shuhao Gu",
        "Shicheng Li",
        "Zihao Yue",
        "Yudong Wang",
        "Wenhan Ma",
        "Zhe Yang",
        "Jingyuan Ma",
        "Zhifang Sui",
        "Fuli Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 5 9 4 7 1 . 2 1 5 2 : r GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation Rang Li1,2, Lei Li2,3 Shuhuai Ren2 Hao Tian2 Yudong Wang1,2 Wenhan Ma1,2 Zhe Yang1 Shuhao Gu2 Jingyuan Ma Shicheng Li1,2 Zhifang Sui1, Zihao Yue2,4 Fuli Luo2, 1State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University 2LLM-Core Xiaomi 3The University of Hong Kong 4Renmin University of China lirang410@gmail.com, szf@pku.edu.cn, luofuli@xiaomi.com"
        },
        {
            "title": "Abstract",
            "content": "Visual groundinglocalizing objects from natural language descriptionsrepresents critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, fundamental question remains: can MLLMs truly ground language in vision with humanlike sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs true capabilities, we introduce GroundingME, benchmark that systematically challenges models across four critical dimensions: (1) Discriminativedistinguishing highly similar objects, (2) Spatialunderstanding complex relational descriptions, (3) Limitedhandling occlusions or tiny objects, and (4) Rejectionrecognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasksreflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both diagnostic tool revealing current limitations in MLLMs and Figure 1. Examples of different visual grounding benchmarks. Prior benchmarks (Top) are either too simple or prone to shortcuts. Our proposed GroundingME (Bottom) increases the challenge in four important dimensions. The green bounding box indicates the correct ground-truth object, while the red bounding box shows the answer of Qwen3-VL-30B-A3B-Instruct. roadmap toward human-level visual grounding. Project page: https://groundingme.github.io. 1. Introduction Work done during internship at Xiaomi Corporation. Co-corresponding authors. The rise of Multimodal Large Language Models (MLLMs) represents paradigm shift in artificial intelligence, offering unprecedented capabilities in joint vision and language understanding [4, 13, 21, 26]. Visual grounding [27, 44], the task of localizing specific region in an image based on natural language description, also known as Referring Expression Comprehension (REC) [17, 25, 29], stands as fundamental capability for these models. It is the bedrock for enabling complex, language-driven interactions, facilitating precise, real-world applications from robotic instruction [39, 49] to detailed image editing [6, 32]. However, the remarkable reasoning abilities demonstrated by MLLMs are largely untested in complex grounding scenarios. As shown in Fig. 1, early benchmarks [16, 24, 47] are fundamentally limited to simple phrases or basic spatial relations in uncluttered scenes (e.g., the vase on the right), failing to assess the fine-grained appearance discrimination, language understanding, and complex spatial reasoning that modern MLLMs claim to possess. While recent works [8, 42] have attempted to increase task difficulty with longer descriptions, they often fail to increase the actual reasoning complexity, as models can easily take shortcut by relying on simple keyword matching (e.g., unique class name) while bypassing the complex attribute and spatial information. As result, recent models have already achieved over 90% accuracy [4, 36, 41, 45] on the RefCOCO series [24, 47] and nearly 90% accuracy [36] on the latest Ref-L4 [8], indicating that existing benchmarks can no longer measure the upper limits of model capabilities or differentiate their true grounding abilities. Additionally, these works overlook the model ability to reject description when its fine-grained details do not precisely match the visual evidence, which is critical for safety and reliability in real-world applications such as autonomous driving [10, 12] and human-robot interaction [18, 34]. To bridge this gap, we introduce GroundingME, comprehensive visual grounding benchmark comprising 1,005 samples, specifically designed for the rigorous evaluation of MLLMs. The benchmark is contructed via three-stage (1) Bounding Box Annotation; (2) Description process: Generation; and (3) Manual Selection and Refinement. We design challenge taxonomy that systematically evaluates models across four L-1 dimensions, as shown in Fig. 1. This taxonomy comprehensively tests model performance across distinct challenge types: (1) Discriminative focuses on distinguishing objects based on subtle, fine-grained visual differences; (2) Spatial assesses the ability to understand and resolve complex spatial and relational arrangements; (3) Limited evaluates grounding under conditions of minimal visual features due to external constraints; and (4) Rejection tests the ability to reject misleading description that contains subtle errors. Furthermore, we provide finegrained L-2 hierarchy covering twelve subcategories to enable deeper, diagnostic analysis of model performance. We conduct an extensive evaluation across 25 stateTable 1. Comparison between GroundingME and other visual grounding benchmarks or datasets. Comparison aspects include: General Scenario and Object Type (General), Description with Compositional Semantics (Semantic), Multiple Evaluation Dimension (Multi-Dim.), and Rejection Samples (Rejection). Benchmarks General Semantic Multi-Dim. Rejection Refcoco/+/g [24, 47] CLEVR-Ref+ [22] Refcrowd [30] Ref-ZOM [16] HC-RefLoCo [42] Ref-L4 [8] GroundingME of-the-art commercial and open-source models, including the Qwen3-VL series [4], Gemini-2.5 [9], Seed1.6-Vision [14], and GLM-4.5V [36], with parameter sizes ranging from 2B to 235B. The results reveal their widespread and significant shortcomings on our benchmark. Even the top-tier model, Qwen3-VL-235B-A22B, achieves only 45.1% accuracy, with the majority of models scoring between 10% and 40%. The failure is most severe on the Rejection category, with most models scoring 0%. We find this failure persists even as model scale increases. These widespread failures motivate us to explore strategies to improve model performance. We explore two complementary approaches: (1) at test time and (2) at training time. First, at test time, we observe that enabling thinking mode generally improves performance and enables basic rejection behavior. Building on this, we propose Test-Time Scaling method [7, 33, 43], leveraging text-only LLM as judge to select the optimal thinking trajectory from multiple candidates. Our experiments show this significantly improves performance on reasoning-intensive categories, particularly Spatial and Rejection. Second, at training time, we hypothesize that the models inability to reject stems from lack of negative samples in training data. We test this with simple Data-Mixture Training strategy. By finetuning Qwen3-VL-8B-Instruct [4] on RefCOCOg [24] augmented with negative samples, the model learns foundational rejection capability, boosting its performance on our benchmarks Rejection category from 0% to 27.9%. Our findings reveal the limitations of current MLLMs and provide clear and practical path forward for building more precise and trustworthy visual systems. 2. Related Work Multimodal Large Language Models (MLLMs). MLLMs have demonstrated remarkable capabilities in understanding and reasoning across both visual and textual data, pushing the boundaries of various multimodal tasks Figure 2. The overall data construction pipeline of GroundingME. The process consists of three main stages: (1) Bounding Box Annotation, which utilizes semi-automated pipeline with RAM++ and GroundingDINO for bounding box generation (3.2.1); (2) Description Generation, which leverages Gemini-2.5-Flash for generating initial referring expressions (3.2.2); and (3) Manual Selection and Refinement, where human annotators apply rigorous filtering and refinement according to our challenge taxonomy (3.2.3). ,including visual grounding. Recent models [4, 36, 41, 45] have already achieved impressive performance on existing visual grounding benchmarks [8, 24, 47], with accuracy approaching or exceeding 90%. Moreover, these models are capable of performing complex reasoning over visual contents to further enhance multimodal performance. This advancement creates pressing need for more challenging and comprehensive benchmarks, as traditional datasets are often insufficient to rigorously evaluate their nuanced understanding and reasoning capabilities on complex, real-world tasks [44], motivating us to curate more challenging visual grounding benchmark. Visual Grounding Datasets and Benchmarks. The evaluation of visual grounding has evolved significantly over the years [17, 25, 27, 29, 44], progressing from closed set, single objects, brief phrases to open vocabulary, generalized targets, and complex descriptions. Early benchmarks, such as the RefCOCO series [24, 47], provided foundational testbed but were characterized by simple, short phrases and have now become largely saturated. In response to these limitations, subsequent works introduced datasets targeting specific challenges. For instance, CLEVR-Ref+ [22] was developed to diagnose compositional reasoning in synthetic environment, while RefCrowd [30] focused on fine-grained discrimination in crowded scenes. RefZOM [16] was the first to introduce simple negative samples to test models rejection ability. More recently, benchmarks have been designed specifically for MLLMs, such as HC-RefLoCo [42] and Ref-L4 [8], which employ long, descriptive sentences as queries to achieve increased difficulty. While existing benchmarks have progressively addressed specific shortcomings, they still fall short of providing sufficient challenge. To this end, as summarized in Tab. 1, our proposed benchmark is the first to unify these critical dimensions to comprehensively assess the advanced visual grounding capabilities of modern MLLMs. 3. GroundingME In this section, we introduce the detailed construction process of GroundingME. We first outline the data source (3.1), followed by three-stage human-in-the-loop annotation pipeline (3.2). Finally, we present the distribution and the statistics of GroundingME (3.3). 3.1. Data Source To ensure that GroundingME provides challenging and complex evaluation, we meticulously curate its image pool by leveraging two high-quality, high-resolution source datasets: SA-1B [19] and HR-Bench [40]. We selected these two datasets as our image sources because they inherently meet our requirements for visual complexity and scale. The SA-1B dataset, which is widely used [20, 31], offers extensive resources of complex scenes and high object density, with 11 million images and 1.1 billion masks. HR-Bench offers ultra-high resolution with its 8K subset essential for creating tasks where minute objects are clearly resolvable. Therefore, we use HR-Bench as the source of the Small subcategory, and rely on SA-1B as the source for all other subtasks. For both datasets, we only use the raw, original images as input for our construction pipeline, without any pre-existing masks, QA pairs, or other annotations. This ensures that even if models encountered the source images during training, the task itself remains novel, thus effectively mitigating the risk of data contamination. 3.2. Data Construction The construction of our benchmark is multi-stage process designed to guarantee the quality and diversity of the resulting dataset. Our pipeline consists of three main stages: (1) Bounding Box Annotation, (2) Description Generation, and (3) Manual Selection and Refinement. 3.2.1. Bounding Box Annotation For images sourced from SA-1B and HR-Bench, we employ distinct methodologies for bounding box generation. For images from SA-1B, we develop an automated pipeline that combines RAM++ [50], GroundingDINO [23], and customized Non-Maximum Suppression (NMS) rule. In contrast, for HR-Bench images, we leverage manual annotation due to the challenges posed by ultra-high resolution. Our automated pipeline for SA-1B images comprises three main steps. (1) We first utilize RAM++ to identify all object categories within each image, generating comprehensive list of class names. (2) Based on this list, we then format text query for the GroundingDINO model, which is used to generate series of bounding boxes for each image. To optimize the generation, we adjusted the models filtering threshold. This step outputs series of bounding boxes and their highest-similarity tokens, and we use the word which the highest-similarity token belongs to as the class name for each box. (3) Finally, to eliminate redundant bounding boxes, we apply customized NMS rule. Instead of prioritizing boxes by area, our NMS strategy favors those belonging to classes with higher instance count, yielding the final set of bounding boxes for each image. 3.2.2. Description Generation Leveraging the powerful image understanding capabilities of modern MLLMs, we use Gemini-2.5-Flash [9] to generate preliminary descriptions for each bounding box, which serve as foundation for our referring expressions. For objects in the SA-1B dataset, we utilize the models visual prompting capability by framing the objects in the full-size image with red bounding box and prompting the model to generate description that includes both their visual atIn contrast, for objects tributes and spatial relationships. in the HR-Bench dataset, the bounding box regions are too small to be effectively prompted within the full image context. Therefore, we crop the bounding box regions and input them to the model, prompting it to generate descriptions of the objects visual attributes only. 3.2.3. Manual Selection and Refinement The final stage of our pipeline is meticulous manual selection and refinement process, crucial step designed to mitigate the inherent limitations of automated data generation. Our human annotators directly address potential inaccuracies in bounding boxes, hallucinations in descriptions, and the presence of redundant or simplistic examples. We apply rigorous set of filtering and selection criteria to ensure our benchmarks quality and challenge. To prevent models from completing the task by relying solely on class names, we filter out simple samples by removing any classes with fewer than three instances and objects with bounding box occupying more than 50% of the image. We then meticulously select the majority of our samples from object classes with an instance count higher than 5. We further enrich the benchmarks diversity by supplementing specific subtasks based on predefined rules, for instance, by selecting samples for Counting and Partial subcategories from scenes with eight or more instances, or for Text subcategory from descriptions containing numbers or strings. The selected samples undergo detailed refinement process to enhance their quality. Annotators first correct any inaccuracies in the bounding boxes. Crucially, they then modify the automatically generated descriptions to meet four key criteria: (1) Uniqueness, ensuring each description refers to one and only one object, or no object for Rejection samples; (2) Subject Clarity, explicitly identifying the target object, which is essential for complex Spatial samples; (3) Task Specificity, tailoring the description to match the assigned sub-task (e.g., adding ordinal words for Counting tasks); (4) Factual Accuracy, correcting any initial hallucinations and ensuring all details are verifiable from the image. For Rejection samples, we intentionally introduce or retain factual errors to make the task more challenging. 3.3. Data Analysis 3.3.1. Subtask Distribution GroundingME incorporates two-tiered classification system, consisting of four L-1 categories and twelve L-2 subcategories, designed to comprehensively assess model performance across various subtasks. The benchmark comprises total of 1,005 samples, distributed across four L-1 subtasks: Discriminative (204, 20.3%), Spatial (300, 29.9%), Limited (300, 29.9%), and Rejection (201, 20.0%). Each L-1 category is further divided into L-2 subcategories. Both the Discriminative and Rejection category are composed of four L-2 subcategories: Appearance, Component, Text, and State, with approximately 50 samples allocated to each. The Spatial category is equally split between Relationship and Counting subcategories, and the Limited category is equally divided between Occlusion and Small subcategories. This balanced distribution ensures robust evaluation across all facets of visual grounding challenge. 3.3.2. Statistics Analysis Tab. 2 presents key statistics for GroundingME, substantiating its challenging nature through several metrics: (1) Object Class and Quantity. The benchmark encompasses 241 distinct object classes, ensuring broad coverage of realworld scenarios. The challenge of intra-class confusion is quantified by the high Intra-Class Count Quartile of (5, 7, 12), indicating large number of similar distracting objects in the image. (2) Image and Instance Size. The image size (square root of area) ranges from 1,500 to 7,680, representing magnitude increase compared to 83 - 610 in RefCOCO series and 30 - 3,767 in Ref-L4 [8]. The instance size (square root of area) ranges from 21 to 946, demonstrating wide coverage of various object scales. Furthermore, the Instance Area Ratio (the area of an instances bounding box divided by the image area) Quartile measures only (0.16%, 4.1. Evaluated Models We conduct comprehensive evaluation of 25 state-of-theart MLLMs on GroundingME, encompassing both opensource and commercial models. Our selection of opensource models is highly diverse, spanning 12 major publishers and wide range of parameter sizes, including Qwen3-VL (2B/4B/8B/32B/A3B/A22B), Qwen2.5VL (7B/32B/72B) InternVL3.5 (8B/A28B) [41], MiMo-VL-7B-RL-2508 [45], Keye-VL1.5-8B [37], MiniCPM-V-4.5 [48], Phi-4-Multimodal [1], Llama-4 (Maverick/Scout) [11], LLaVA-OneVsion-1.58B [3], Mistral-3.2-24B [38], Gemma-3-27B [35], and Llama-Nemotron-8B [5]. For commercial models, we select those with explicit grounding ability, including Gemini2.5 (Pro/Flash) [9] and Seed-1.6-Vision-250815 [14]. [4], GLM-4.5V [36], 4.2. Evaluation Settings We employ rigorous and standardized strategy for evaluation. For every data instance, we organize the input image and the description using unified prompt template. This template accurately specifies critical details, including the reference viewpoint for spatial relationships, the allowed number of target objects to be output, and strict constraints on the output format. Unless otherwise specified in subsequent sections, all experiments are conducted using greedy decoding (set as temperature = 0). For the evaluation metric, we adopt the widely-used Accuracy@0.5, which represents the proportion of total samples where the Intersection over Union (IoU) between the ground-truth and predicted bounding box exceeds 0.5. The detailed prompt and results across various IoU thresholds are provided in the Appendix. 4.3. Evaluation Results 4.3.1. Main Results Tab. 3 presents the evaluation results of all models on GroundingME. We explicitly disabling the thinking mode where supported. Our assessment yields three major ob- (1) Models demonstrate significant perforservations. mance gap on GroundingME, with the best model, Qwen3VL-235B-A22B, achieving 45.1% accuracy, the majority score between 10% and 40%, and several with an accuracy only less than 10%, which strongly validates the challenge of our benchmark compared to existing ones. (2) Commercial models do not exhibit pronounced advantage over open-source ones. We observe that even the best performed Seed-1.6-Vision-250815 closely follows the best open-source model at 42.6%, while Gemini-2.5 series show performance comparable only to mid-range open-source counterparts. (3) Model scale is critical factor for performance. This scaling trend is consistently verified across model families, including Qwen3-VL-Dense (2B to 32B: 21.1% to 39.5%), Qwen3-VL-MoE (A3B to A22B: 35.7% to 45.1%), and Qwen2.5-VL 7B to 72B: 15.1% to 29.6%). Figure 3. Subtask Distribution of GroundingME. Our benchmark comprises of 1,005 samples, distributed across four L-1 categories and twelve L-2 subcategories. Table 2. Statistics of GroundingME. The image size and instance size are reported as the square root of their area in pixels. The description length is measured by the number of words. The IntraClass Count Quartile is derived from the automated construction pipeline excluding images from HR-Bench, and thus should be interpreted as lower bound for the actual value. Statistics Number Object Class Image Size Instance Size Instance Area Ratio Quartile Description Length Quartile Intra-Class Count Quartile 241 1,500 - 7,680 21 - 946 (0.16%, 1.0%, 2.7%) (18, 40, 58) (5, 7, 12) 1.0%, 2.7%), indicating that the instances are notably small relative to the image, which significantly increases the task difficulty. (3) Description Complexity. The descriptions in the benchmark all consist of complete sentences or paragraphs, making it suitable for modern MLLMs. The Description Length Quartile reaches (18, 40, 58) words, highlighting that the majority of descriptions are longer and more intricate than the average value of 3.6 in RefCOCO/+, 8.4 in RefCOCOg, and 24.2 in Ref-L4 [8]. 4. Evaluation We report our experimental setup and findings from the extensive evaluation of state-of-the-art MLLMs. This section is organized by models evaluated (4.1), detailed evaluation settings (4.2), and results analysis (4.3). Table 3. Evaluation results on GroundingME. All models in this table are evaluated under the no-thinking mode setting if supported. All reported metrics in this table are Accuracy@0.5. The abbreviations for the subcategories are: App. (Appearance), Cmp. (Component), Txt. (Text), Sta. (State), Rel. (Relationship), Cnt. (Counting), Occ. (Occlusion), Sml. (Small), Avg. (Average). The best results are shown in bold and the second best is with underline. Detailed model specifications can be found in 4.1. Model Discriminative Spatial Limited Rejection App. Cmp. Txt. Sta. Avg. Rel. Cnt. Avg. Occ. Sml. Avg. App. Cmp. Txt. Sta. Avg. 3.8 Phi-4-Multimodal 15.4 Llama-4-Maverick Llama-4-Scout 21.2 3.8 LLaVA-O.V.-1.5-8B Mistral-3.2-24B 1.9 Gemma-3-27B 3.8 Llama-Nemotron-8B 19.2 MiniCPM-V-4.5 7.7 11.5 InternVL3.5-8B InternVL3.5-A28B 34.6 25.0 Keye-VL-1.5-8B 42.3 MiMo-VL-7B-RL 50.0 GLM-4.5V 23.1 Qwen2.5-VL-7B 34.6 Qwen2.5-VL-32B 50.0 Qwen2.5-VL-72B 44.2 Qwen3-VL-2B 55.8 Qwen3-VL-4B 55.8 Qwen3-VL-8B 78.8 Qwen3-VL-32B 73.1 Qwen3-VL-A3B 71.2 Qwen3-VL-A22B 0. 0.0 0.7 0.7 5.8 5.8 6.0 4.0 0.0 0.0 0.7 1.0 0.0 0.0 9.3 30.0 16.0 11.5 18.1 25.3 19.3 22.3 5.0 0.7 7.3 6.7 12.3 9.6 17.6 18.0 26.0 14.0 3.7 0.0 6.0 4.7 5.3 4.0 9.8 14.0 10.0 11.5 3.3 0.7 0.0 2.7 3.3 2.0 4.4 5.8 2.0 8.0 0.0 0.0 0.3 0.0 0.7 0.0 0.0 1.5 1.9 0.0 0.0 0.0 6.0 16.0 4.7 7.3 30.0 32.0 19.2 25.0 8.3 0.7 8.0 4.0 3.3 4.7 7.8 12.0 4.0 0.0 3.3 4.0 3.3 4.7 6.4 4.0 0.0 1.7 40.0 18.0 21.2 28.4 33.3 16.7 25.0 26.0 0.0 13.0 22.0 22.0 17.3 21.6 8.0 11.3 0.0 5.7 0.0 13.0 46.0 50.0 38.5 44.1 24.7 14.0 19.3 26.0 58.0 58.0 46.2 52.9 54.7 29.3 42.0 48.0 10.7 29.3 0.7 14.3 36.0 46.0 23.1 31.9 12.7 16.0 14.3 28.0 0.0 17.7 58.0 66.0 32.7 47.5 48.7 31.3 40.0 35.3 0.7 23.7 50.0 66.0 28.8 48.5 52.0 28.7 40.3 46.7 36.0 66.0 32.7 44.6 11.3 12.0 11.7 21.3 36.0 28.7 58.0 74.0 38.5 56.4 34.7 22.0 28.3 45.3 48.7 47.0 68.0 80.0 42.3 61.3 32.7 20.0 26.3 56.0 16.0 36.0 84.0 82.0 55.8 75.0 61.3 33.3 47.3 60.7 7.3 34.0 66.0 76.0 38.5 63.2 38.0 22.0 30.0 41.3 52.0 46.7 74.0 84.0 50.0 69.6 62.7 36.7 49.7 72.7 35.3 54. 8.7 7.3 0.0 4.0 4.0 0.0 0.0 0.0 6.0 0.0 4.0 0.0 0.0 0.0 0.0 0.0 0.0 4.0 0.0 0.0 0.0 0.0 0.0 0.0 Gemini-2.5-Pro Gemini-2.5-Flash Seed-1.6-V.-250815 32.7 40.4 59.6 0.7 32.0 44.0 30.8 34.8 39.3 28.7 34.0 13.3 40.0 34.0 30.8 36.3 28.7 21.3 25.0 22.7 3.3 13.0 80.0 36.0 63.5 59.8 72.7 44.7 58.7 70.0 15.3 42. 7.0 10.0 0.0 0.0 0.0 0.0 0.0 3.9 12.0 4.0 6.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5.9 6.0 4.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 4.0 0.0 3.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.9 0.0 0.0 8.0 6.0 0.0 0.0 2.0 2.0 0.0 6.0 2.5 0.0 0.0 0.0 5.5 0.0 1.5 0.0 0.0 0.0 0.5 0.5 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 7.0 0.0 1. Total 0.4 13.0 8.9 4.4 1.7 0.4 10.4 4.0 3.3 17.1 8.5 18.6 32.1 15.1 26.9 29.6 21.1 33.9 31.0 39.5 35.7 45.1 20.7 18.7 42.6 This correlation underscores the importance of model size in achieving advanced visual grounding capabilities. 61.3%; Average: 11.7% to 47.3%). 4.3.2. Subtask Results Analyzing the four distinct types of L-1 categories, we ob- (1) Model perserve several key performance patterns. formance exists in stratification across the different subtask types: models are generally most proficient at Discriminative category, followed by Spatial or Limited categories, while all models demonstrate poor performance on Rejection category. (2) Within the Discriminative category, clear performance gain is tied to model scale, as seen in Qwen3-VL 2B to 32B: Average 44.6% to 75.0%), and performance on the State subcategory remains lower than the other three subcategories. (3) For the Spatial category, models are more proficient at the Relational subcategory, which requires qualitative positional ability, compared to the Counting subcategory, which requires quantitative assessment. Furthermore, performance in this category is more sensitive to model scale, with Qwen3-VL (2B to 32B) showing substantial improvement (Relationship: 11.3% to 5. Analysis This section provides detailed analysis of MLLM behaviors and capabilities on GroundingME. We first assess the performance gain achieved by enabling thinking mode (5.1). Following this, we investigate novel Test-Time Scaling strategy leveraging thinking trajectories (5.2). Finally, we explore data mixture training strategy designed to enhance the Rejection grounding capability (5.3). 5.1. The Effectiveness of Thinking The widely usage of thinking have demonstrated significant advantages in enhancing the complex reasoning capabilities of recent MLLMs. Therefore, we assess the impact of thinking on several representative MLLMs that natively supports thinking mode: Qwen3-VL (8B/32B/A3B/A22B), GLM-4.5V, MiMo-7B-RL-2508, and Seed-1.6-Vision-250815. Our experiments yield three (1) Thinking mode universally leads to main findings. Figure 4. Case study of two different thinking trajectories of Qwen3-VL-235B-A22B-Thinking for the same description. The correct answer is to do rejection and the red bounding box shows the distractor. The correct trajectory (Green) demonstrates rigorous adherence to the description, systematically identifying all attribute mismatches (e.g., shortvs. long-sleeve, blue vs. black pants) and correctly concluding with null output. In contrast, the erroneous trajectory (Red) acknowledges the same discrepancies but compromises by speculating that the description may be in error, ultimately leading to an incorrect bounding box prediction. Table 4. Performance of our Test-time scaling method on GroundingME. The table compares the performance gain of Qwen3-VL-235B-A22B-Thinking when its N=16 responses are selected by text-only LLMs (evaluating thinking trajectory quality) and MLLM Baselines (evaluating visual correctness). Model Type Judge Model Total Dis. Spa. Lim. Rej. Average - 49.8 66.6 74.5 43.3 5.7 Text-only Deepseek-R1 MiMo-RL-0530 52.7 65.2 77.3 44.7 15.4 52.0 64.2 77.3 43.0 15.4 Multimodal Qwen3-VL-A22B 52.2 69.1 75.0 45.7 11.0 8.5 Qwen3-VL-A3B 49.6 64.2 71.0 45.7 the same query can be highly divergent, leading to distinct outputs. We hypothesize that trajectories that are coherent, logically consistent, and strictly adhere to task instructions are more likely to lead to the correct answer. To validate this hypothesis, we design Test-Time Scaling (TTS) method [7, 33, 43] specifically tailored to analyze the efficacy of the thinking trajectory. For each sample, we use Qwen3-VL-235B-A22B-Thinking to generate N=16 responses (temperature=0.7), each including its reasoning content. We then employ text-only LLM as judge, which cannot see the image, to perform Best-of-16 selection: the judge compares the 16 full responses in pairwise manner, selecting the one with the superior thinking trajectory quality, and repeats until only one response remains. We test DeepSeek-R1 [15] and MiMo-7B-RL-0530 [46] as judges. Additionally, we use an MLLM judge as baseline, which is provided with the image and the final bounding box outputs of candidate responses without the thinking trajectory. We test Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-30B-A3B-Thinking as the MLLM judges. Based on the results presented in Tab. 4, we observe Figure 5. Performance gain of different models by enabling thinking mode. Subtask results are provided in the appendix. better performance on GroundingME, with every tested model achieving significant performance gain, ranging from 4.7% for GLM-4.5V to 7.4% for Qwen3-VL-32B. (2) Thinking Mode generally exhibits negative effect on tasks relying more on perception, yet shows notable performance improvements for tasks prioritizing reasoning. Detailed subtask performance is provided in the Appendix. (3) Models can learn to reject by thinking. Under the no-thinking setting, most models exhibits 0% accuracy on the Rejection task, signifying complete failure in executing any rejection behavior. Although performance on the rejection task still remains low under the thinking mode, all models successfully demonstrate some level of rejection behavior. 5.2. Test-Time Scaling by Thinking Trajectory To gain deeper understanding of how thinking contributes to model performance, we conduct detailed case study focusing on the relationship between the quality of the generated thinking trajectory and the final grounding accuracy. As demonstrated in Fig. 4, different thinking trajectory for In-domain performance of fine-tuned Qwen3-VLTable 5. 8B-Instruct on RefCOCOg validation split, our curated RefCOCOg rej validation split, and the macro average of both datasets under different SFT data ratios (negative to positive)."
        },
        {
            "title": "Dataset",
            "content": "Origin 1:8 1:4 1:2 1:1 2:"
        },
        {
            "title": "RefCOCOg val\nRefCOCOg rej val\nMacro Average",
            "content": "88.2 30.5 59.4 90.4 89.9 88.1 86.8 83.1 83.5 87.9 92.3 94.8 97.3 87.0 88.9 90.2 90.8 90.2 two major findings. (1) TTS based on the thinking trajectory significantly boosts performance. DeepSeek-R1 as judge achieves 2.9% performance gain, and even the 7Bparameter MiMo-RL-0530 judge yields 2.2% gain. Subtask analysis further indicates that these performance improvements primarily stem from reasoning-intensive cat- (2) egories, specifically the Spatial and Rejection tasks. The efficacy of TTS using an MLLM judge might be constrained by the judge models inherent multimodal ability. The strong MLLM judge, Qwen3-VL-A22B, achieves 2.4% performance gain, with its primary improvements coming from perception-reliant categories such as Discriminative and Limited tasks. However, the smaller MLLM judge Qwen3-VL-A3B does not bring any improvement. 5.3. Enhance Rejection Capability by Data Mixture The performance of models on Rejection subtask exhibits substantial performance disparity relative to their accuracy on positive samples. Motivated by prior work [2, 28] suggesting that compromised rejection capability stems from scarcity of negative instances within the training corpus, we propose to investigate data mixture training strategy for improving the models rejection capability. Considering the scale and accessibility, we utilize the RefCOCOg dataset as our base. We select 30,000 positive samples from its training split and construct 30,000 corresponding negative samples by modifying the description. We refer to this newly created set of negative samples as RefCOCOg rej train. These 60,000 instances serve as the source pool for generating various SFT datasets. To investigate the effect of different data mixture ratios, we randomly sample 30,000 instances from this pool to create five distinct fine-tuning datasets, with the negative-to-positive sample ratios set sequentially as 1:8, 1:4, 1:2, 1:1, and 2:1. Simultaneously, we construct 11,490 negative samples from the RefCOCOg validation split for evaluation, which we term RefCOCOg rej val. We then fine-tune Qwen3-VL-8B-Instruct for 3 epochs using these five mixture datasets. We first evaluate the in-domain performance of the finetuned models on RefCOCOg val (original positive samples) and RefCOCOg rej val (curated negative samples). From the results in Tab. 5, we find that: (1) Simple incorporaFigure 6. Out-of-domain performance of fine-tuned Qwen3VL-8B-Instruct on GroundingME w/o Rejection and the Rejection category, as function of SFT data ratio (negative to positive). Baseline means the performance before fine-tuning. tion of negative data effectively enables the model to learn the rejection capability in visual grounding. (2) As the proportion of negative data increases in the fine-tuning set, the models performance on the rejection task improves incrementally, while simultaneously impacting the original positive grounding performance to some extent. (3) The macro average accuracy across both negative and positive classes shows significant performance gain of approximately 30% across various mixture ratios, powerfully confirming the efficacy of our data mixture training strategy. We further evaluate the out-of-domain performance of the fine-tuned models on GroundingME to explore the generalizability of the learned rejection capability. We denote the GroundingME benchmark excluding the rejection category as GroundingME w/o Rejection. As shown in Fig. 6, we observe that: (1) The performance trend on the Rejection category of GroundingME aligns with the in-domain results, showing clear growth as the ratio of negative samples increases. (2) In contrast to the in-domain results, the performance on GroundingME w/o Rejection demonstrates clear degradation compared to the pre-fine-tuned baseline from 38.8% to max of 33.0%. This suggests that the rejection capability gained from simple data mixture does not generalize cost-free to higher-difficulty, out-of-domain scenarios, pointing towards critical challenge that necessitates further investigation in future work. 6. Conclusion In this work, we introduce GroundingME, challenging benchmark that rigorously evaluates MLLMs grounding capabilities through carefully curated samples requiring spatial reasoning and fine-grained visual understanding. Our comprehensive evaluation reveals critical weakness in current MLLMs: while models perform reasonably on standard grounding tasks, they struggle significantly with fine-grained discrimination and spatial reasoning, and lack robust rejection mechanisms for non-existing objects. Our analysis demonstrates that targeted improvements are achievable: the leveraging of test-time scaling by thinking trajectory and the incorporation of rejection-focused SFT samples both resulted in measurable gains in visual grounding accuracy. These results suggest that the path forward requires not merely scaling model capacity, but developing specialized strategies that enhance fine-grained visual discrimination and reasoning-aware grounding."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. ArXiv preprint, abs/2503.01743, 2025. 5 [2] Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip HS Torr, Yoon Kim, and Marzyeh Ghassemi. Vision-language models do not understand negation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2961229622, 2025. 8 [3] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. ArXiv preprint, abs/2509.23661, 2025. 5 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. ArXiv preprint, abs/2502.13923, 2025. 2, 3, 5 [5] Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models. ArXiv preprint, abs/2505.00949, 2025. 5 [6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 1724, 2023, pages 1839218402, 2023. [7] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. ArXiv preprint, abs/2407.21787, 2024. 2, 7 [8] Jierun Chen, Fangyun Wei, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S-H Gary Chan, and Hongyang Zhang. Revisiting referring expression comprehension evaluation in the era of large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 513524, 2025. 2, 3, 4, 5 [9] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. ArXiv preprint, abs/2507.06261, 2025. 2, 4, 5 [10] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. survey on multimodal large language models for autonomous driving. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 958979, 2024. 2 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [12] Haoxiang Gao, Zhongruo Wang, Yaqian Li, Kaiwen Long, Ming Yang, and Yiqing Shen. survey for foundation models in autonomous driving. ArXiv preprint, abs/2402.01105, 2024. 2 [13] Gemini Team. Gemini: family of highly capable multimodal models. ArXiv preprint, abs/2312.11805, 2023. 2 [14] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. ArXiv preprint, abs/2505.07062, 2025. 2, 5 [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633 638, 2025. 7 [16] Yutao Hu, Qixiong Wang, Wenqi Shao, Enze Xie, Zhenguo Li, Jungong Han, and Ping Luo. Beyond one-to-one: ReIn IEEE/CVF thinking the referring image segmentation. International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 40444054, 2023. 2, 3 [17] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787798, 2014. 2, 3 [18] Callie Kim, Christine Lee, and Bilge Mutlu. Understanding large-language model (llm)-powered human-robot In Proceedings of the 2024 ACM/IEEE interinteraction. national conference on human-robot interaction, pages 371 380, 2024. [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross B. Girshick. Segment anything. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 39924003, 2023. 3 [20] Xiangtai Li, Tao Zhang, Yanwei Li, Haobo Yuan, Shihao Chen, Yikang Zhou, Jiahao Meng, Yueyi Sun, Shilin Xu, Lu Qi, et al. Denseworld-1m: Towards detailed dense grounded caption in the real world. ArXiv preprint, abs/2506.24102, 2025. 3 [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 2 [22] Runtao Liu, Chenxi Liu, Yutong Bai, and Alan L. Yuille. Clevr-ref+: Diagnosing visual reasoning with referring expressions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 41854194, 2019. 2, 3 [23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 4 [24] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 1120, 2016. 2, 3 [25] Varun Nagaraja, Vlad Morariu, and Larry Davis. Modeling context between objects for referring expression understanding. In European Conference on Computer Vision, pages 792807. Springer, 2016. 2, [26] OpenAI. Gpt-4v(ision) system card, 2023. 2 [27] Georgios Pantazopoulos and Eda Ozyigit. Towards understanding visual grounding in visual language models. ArXiv preprint, abs/2509.10345, 2025. 2, 3 [28] Junsung Park, Jungbeom Lee, Jongyoon Song, Sangwon Yu, Dahuin Jung, and Sungroh Yoon. Know nobetter: datadriven approach for enhancing negation awareness in clip. ArXiv preprint, abs/2501.10913, 2025. 8 [29] Yanyuan Qiao, Chaorui Deng, and Qi Wu. Referring expression comprehension: survey of methods and datasets. IEEE Transactions on Multimedia, 23:44264440, 2020. 2, 3 [30] Heqian Qiu, Hongliang Li, Taijin Zhao, Lanxiao Wang, Qingbo Wu, and Fanman Meng. Refcrowd: Grounding the target in crowd with referring expressions. In Proceedings of the 30th ACM International Conference on Multimedia, pages 44354444, 2022. 2, 3 [31] Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, Shaohui Lin, and Rongrong Ji. Aligning and prompting everything all at once for universal visual perception. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1319313203, 2024. 3 [32] Xincheng Shuai, Henghui Ding, Xingjun Ma, Rongcheng Tu, Yu-Gang Jiang, and Dacheng Tao. survey of multimodal-guided image editing with text-to-image diffusion models. ArXiv preprint, abs/2406.14555, 2024. 2 [33] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ArXiv preprint, abs/2408.03314, 2024. 2, multimodal humanrobot interaction. Frontiers in Neurorobotics, 17:1084000, 2023. 2 [35] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, ArXiv preprint, et al. abs/2503.19786, 2025. 5 Gemma 3 technical report. [36] GLM-V Team. Glm-4.5 and glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. ArXiv preprint, abs/2507.01006, 2025. 2, 3, 5 [37] Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. ArXiv preprint, abs/2507.01949, 2025. 5 [38] Mistral Team. Mistral 7b, 2023. 5 [39] Jiaqi Wang, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, Yincheng Yao, Xuan Liu, Bao Ge, and Shu Zhang. Large language models for robotics: Opportunities, challenges, and perspectives. Journal of Automation and Intelligence, 2024. 2 [40] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 79077915, 2025. [41] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. ArXiv preprint, abs/2508.18265, 2025. 2, 3, 5 [42] Fangyun Wei, Jinjing Zhao, Kun Yan, Hongyang Zhang, and Chang Xu. large-scale human-centric benchmark for referring expression comprehension in the LMM era. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. 2, 3 [43] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. 2024. 2, 7 [44] Linhui Xiao, Xiaoshan Yang, Xiangyuan Lan, Yaowei Wang, and Changsheng Xu. Towards visual grounding: survey. ArXiv preprint, abs/2412.20206, 2024. 2, 3 [45] LLM-Core Xiaomi. Mimo-vl technical report. ArXiv preprint, abs/2506.03569, 2025. 2, 3, 5 [46] LLM-Core Xiaomi. Mimo: Unlocking the reasoning potential of language modelfrom pretraining to posttraining. ArXiv preprint, abs/2505.07608, 2025. 7 [47] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, pages 6985. Springer, 2016. 2, 3 [34] Hang Su, Wen Qi, Jiahao Chen, Chenguang Yang, Juan Sandoval, and Med Amine Laribi. Recent advancements in [48] Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, et al. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. ArXiv preprint, abs/2509.18154, 2025. 5 [49] Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, and Philip Yu. Large language models for robotics: survey. ArXiv preprint, abs/2311.07226, 2023. 2 [50] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong image tagging model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17241732, 2024. 4 [51] Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Pyvision: arXiv preprint Qilong Wu, Kaipeng Zhang, and Chen Wei. Agentic vision with dynamic tooling. arXiv:2507.07998, 2025. 2 GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation A. Detailed Results B. Prompt Templates"
        },
        {
            "title": "Supplementary Material",
            "content": "A.1. Subtask Results in Analysis We report the detailed subtask results for the L-1 categories omitted from the analysis section. Tab. 6 presents the detailed subtask performance of models when enabling thinking mode, supplementing the analysis on performance gain discussed in 5.1. Tab. 7 shows the out-of-domain subtask performance of the fine-tuned Qwen3-VL-8B-Instruct model, complementing the overall results presented in 5.3. Table 6. Subtask performance of different models by enabling thinking mode. Model Total Dis. Spa. Lim. Rej. Qwen3-VL-8B Qwen3-VL-32B Qwen3-VL-A3B Qwen3-VL-A22B GLM-4.5V MiMo-VL-7B-RL Seed-1.6-V.- 34.3 46.9 39.2 49.8 34.0 24.1 46.5 52.5 43.0 65.7 70.0 53.4 53.3 65.2 73.7 52.5 45.3 46.6 28.7 59.3 72.7 33.3 36.0 38.0 45.0 30.3 17.0 41.7 4.5 9.5 5.5 5.5 4.0 5.0 1.5 Table 7. Instruct under different SFT data ratios. Subtask performance of fine-tuned Qwen3-VL-8BTab. 9 presents the unified prompt template employed for all evaluations conducted on GroundingME. Tab. 10 details the prompt template used for the MLLM judge baseline during the best-of-N selection in our test-time scaling analysis. Tab. 11 shows the prompt template used for the text-only LLM judge to select the optimal response based on thinking trajectory during our test-time scaling analysis. C. Human Rejection Verification Given the poor performance of models on the Rejection category, we conduct human verification study to validate the correctness and quality of our data. We randomly sample 100 instances from GroundingME for human binary classification (Reject/Non-Reject) annotation. To mitigate the risk of human annotators taking linguistic shortcuts based on distinct description styles, we restrict the sampling to the Discriminative and Rejection categories only, as their referring expressions exhibit structural similarity. The final sampled set includes 51 instances from the Rejection category. Considering Rejection as the positive class, human annotators achieve an Accuracy of 91%, with Precision of 88.24%, Recall of 93.75%, and an F1-score of 90.91%. Neg.:Pos. Total Dis. Spa. Lim. Rej. D. Commercial Model Notes 1:8 1:4 1:2 1:1 2: 27.0 25.0 28.7 28.5 26.0 57.4 24.7 49.5 25.3 54.4 28.3 46.6 26.3 40.2 24.0 24.3 19.3 23.0 26.3 17.0 3.5 8.0 11.4 16.4 27.9 A.2. Main Results across Various IoU For the evaluation presented in the main results table, we further report the accuracy of all models on the entire GroundingME and three L-1 categories (the Rejection category is excluded, as its accuracy is independent of the IoU threshold) across different IoU thresholds in Tab. 8. New metrics include Accuracy@0.75, Accuracy@0.9, and mAcc. The mAcc is defined as the mean accuracy calculated over the range of IoU thresholds [0.5, 0.95], sampled at intervals of 0.05. Regarding the evaluation of commercial models, we make specific adjustment for the Gemini-2.5 series: we modify the required coordinate format in the prompt template (Tab. 9) to [y1, x1, y2, x2]. This modification is implemented because we observe that Gemini-2.5 is significantly more receptive to this output format, resulting in measurable improvement in accuracy."
        },
        {
            "title": "We do not report",
            "content": "the evaluation results for GPT-5, Claude-Sonnet-4.5, and Grok-4 due to issues with their output. From cases in Tab. 12, we observe that the coordinates produced by these models using the unified prompt template (Tab. 9) suffered from substantial displacement and distortion, regardless of whether the output is interpreted as absolute pixel coordinates (red bounding box) or 0-999 normalized relative coordinates (blue bounding box). Furthermore, we fail to find an alternative coordinate format that yields usable results for these models. E. Tool Use Results We also conduct an additional evaluation of Claude-Sonnet4.5 utilizing PyVision [51] for tool use. The total accuracy is 12.4%. The detailed subtask breakdown is as follows: Discriminative: 19.1% (App.: 15.4%, Cmp.: 32%, Txt.: 10%, Sta.: 19.2%); Spatial: 13.3% (Rel.: 13.3%, Cnt.: 13.3%); Limited: 9.0% (Occ.: 10.7%, Sml.: 7.3%); and Rejection: 9.5% (App.: 10%, Cmp.: 7.8%, Txt.: 14%, Sta.: 6%). We assume that the models ability to utilize tool use for multi-step cropping and magnification of 8K image to localize tiny objects should yield improved performance on the Limited Small subcategory. However, the observed accuracy (7.3%) falls below our expectations. Through case study in Tab. 13, we find that subtle offset of bounding box size and position is significant contributing factor to this unsatisfactory result. F. Examples for Each L-2 Subcategory In Tab. 14 through Tab. 25, we provide precise task definitions for all twelve L-2 subcategories in GroundingME, and present one representative example for each. In all displayed examples, the red bounding box indicates the correct ground-truth object. Table 8. Evaluation results across different IoU thresholds on GroundingME. All settings and abbreviations are the same as in 4. Model Phi-4-Multimodal Llama-4-Maverick Llama-4-Scout LLaVA-O.V.-1.5-8B Mistral-3.2-24B Gemma-3-27B Llama-Nemotron-8B MiniCPM-V-4.5 InternVL3.5-8B InternVL3.5-A28B Keye-VL-1.5-8B MiMo-VL-7B-RL GLM-4.5V Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B Qwen3-VL-2B Qwen3-VL-4B Qwen3-VL-8B Qwen3-VL-32B Qwen3-VL-A3B Qwen3-VL-A22B Gemini-2.5-Pro Gemini-2.5-Flash Seed-1.6-V.-250815 Discriminative Spatial Limited Total Acc0.75 Acc0.9 mAcc Acc0.75 Acc0.9 mAcc Acc0.75 Acc0.9 mAcc Acc0.75 Acc0.9 mAcc 0.0 8.8 7.4 2.9 0.0 0.0 14.7 1.0 2.0 16.2 8.3 33.8 44.1 24.0 38.7 42.2 39.2 52.9 57.4 71.1 61.3 66.7 22.5 27.5 55.4 0.0 0.5 0.5 0.5 0.0 0.0 3.9 0.0 1.5 6.4 0.5 13.2 32.4 12.3 15.2 21.1 31.9 43.1 47.1 54.9 50.5 54. 11.8 14.7 41.7 0.1 9.5 8.0 4.0 1.1 0.4 14.9 2.4 3.2 16.4 9.8 30.6 42.5 22.4 33.6 37.4 37.8 50.2 55.0 65.9 57.8 63.3 22.9 26.6 51.7 0.0 8.3 3.7 0.7 0.0 0.0 2.7 0.7 2.3 14.3 1.3 14.0 37.7 10.3 29.0 30.0 10.3 25.7 23.7 44.0 27.3 46.3 23.0 19.0 51.3 0.0 0.7 0.0 0.0 0.0 0.0 0.7 0.0 0.0 3.0 0.0 5.0 24.7 4.0 13.0 14.7 8.0 20.0 21.0 32.7 20.0 38. 12.3 13.3 35.7 0.2 9.8 4.9 1.1 0.8 0.0 2.9 1.4 2.2 13.9 2.6 12.8 34.5 9.3 27.2 28.4 9.9 24.3 23.3 41.3 25.6 44.1 21.7 19.0 48.0 0.0 1.7 1.3 0.0 0.0 0.0 4.7 0.0 0.7 6.7 0.7 8.0 19.0 9.3 9.7 14.3 15.3 29.7 27.3 28.3 32.0 37.3 2.7 6.3 30.7 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 0.3 3.3 0.3 3.0 8.3 1.3 3.0 4.3 7.0 11.7 16.7 19.0 15.3 21. 0.7 1.0 19.0 0.0 1.9 1.4 0.7 0.0 0.0 5.1 1.1 0.9 7.6 1.6 8.1 18.2 8.4 10.8 14.4 16.5 29.4 26.5 26.3 31.4 36.7 3.5 7.2 30.1 0.0 6.0 3.5 0.8 0.0 0.0 6.3 0.4 1.6 9.6 2.3 13.4 26.0 10.8 19.4 22.4 15.6 27.3 26.9 36.0 30.1 38.5 13.6 13.1 35.9 0.0 1.5 0.6 0.1 0.0 0.0 2.7 0.0 0.7 3.2 0.2 5.1 16.5 4.2 7.9 10.5 10.9 18.2 20.8 26.6 20.8 29. 7.7 7.3 25.0 0.1 6.6 4.0 1.4 0.5 0.1 6.5 1.2 1.9 9.8 3.2 12.5 24.5 9.9 18.2 21.0 15.6 26.2 26.0 33.6 28.7 37.0 13.6 13.2 34.0 <image> # Role and Task All spatial relationships are defined from the viewers perspective, where front means closer to the viewer and back means farther from the viewer. Please provide the bounding box coordinate of the object the following statement describes: You are rigorous AI reasoning process analyst. Your task is to compare the two responses provided (Response and Response B) based on the five principles below and select the superior one. {description} # Core Evaluation Principles Ensure that all details mentioned about the object are accurate. Provide at most one bounding box. If matching object is found, provide its bounding box as JSON in the format {bbox 2d: [x1, y1, x2, y2]}. If no matching object is found, output {bbox 2d: null}. Table 9. Prompt template for all evaluations on GroundingME. <image> # Role and Task You are an expert-level Visual Grounding Adjudicator. Your task is to evaluate two proposed bounding boxes (Bbox and Bbox B) for given image and user instruction, and determine which one is the more accurate and superior choice. # Input Instruction: {instruction} Bbox A: {bbox a} Bbox B: {bbox b} All of your judgments MUST be strictly based on the following five points: 1. Instruction Understanding: Evaluate whether the model has correctly and comprehensively understood the description in the users instruction, including all details, constraints, and limitations. 2. Visual Observation: Evaluate whether the model has comprehensively and meticulously observed the image, identifying as many objects and their attributes or spatial relationships as possible. 3. Logical Reasoning: Evaluate whether each step of the reasoning is logical and free of contradictions, fallacies, or unsubstantiated leaps. 4. Analytical Rigor: Evaluate whether the conclusion was reached hastily or formed after carefully analyzing and comparing multiple possibilities. 5. Conclusion Support: Evaluate whether the final answer is strongly supported and uniquely derived from the thought process, rather than being disconnected from it. # Input [Original Task Instruction] {instruction} [Full Content of Response A] {response a} [Full Content of Response B] {response b} # Output # Output Explain your reasoning, then conclude with your final choice in the format boxed{A} or boxed{B}. Your final selection must be only one of the following two lines, with no other text before or after: boxed{A} or boxed{B}. Table 10. Prompt template for multimodal judge models in testtime scaling analysis. Table 11. Prompt template for text-only judge models in test-time scaling analysis. Description: This is small, light green plastic stool with top and four tapered legs that splay slightly outwards. The top surface has subtle pattern. small sticker is attached to it. Its size suggests its common, lightweight outdoor seating option. Correct Answer: [544, 1102, 940, 1498] GPT-5 Answer: [320, 600, 520, 760] Claude-4.5 Answer: [89, 632, 301, 869] Grok-4 Answer: [59, 322, 130, 410] Table 12. Cases of outputs from unreported commercial models. Description: The object is young girl. She is squatting on wooden raft or platform by the water. Original Image: width=7680, height=5046 Correct Answer: [3389, 3448, 3487, 3530] Claude-4.5 Answer: [0.4323, 0.6877, 0.4544, 0.7134] Table 13. Case of Claude-Sonnet-4.5 output with tool use. Subcategory 1: Discriminative Appearance Definition: Distinguishing objects based on subtle visual attributes like color or texture. Subcategory 3: Discriminative Text Definition: Distinguishing targets based on textual information embedded within the image. Description: This is white cube, likely Mahjong tile, with smooth, reflective surface. On its top face, there are two blurry, dark vertical markings, which appear to be thin lines or abstract shapes, rendered out of focus. The material seems to be hard, glossy substance like plastic or ceramic. Table 14. An example of Discriminative Appearance Subtask. Description: This object is boat. The number 1-1-03 is visible on its hull. Table 16. An example of Discriminative Text Subtask. Subcategory 2: Discriminative Component Subcategory 4: Discriminative State Definition: Distinguishing targets based on the presence or absence of specific structural component. Definition: Distinguishing objects based on their dynamic or static condition. Description: This is tall, slender tree with relatively straight, thin trunk and sparse, upright branches. The trunk and branches are primarily dark brown to reddish-brown, suggesting sparse foliage. The texture appears rough and natural. There are few brown leaves concentrated on some of the upper branches, indicating it might be deciduous tree in dry season or tree with naturally sparse foliage. Table 15. An example of Discriminative Component Subtask. Description: The jet is dark-colored, appearing black or very dark navy, with sleek, aerodynamic design characteristic of military training aircraft. The jet is positioned vertically. It is actively emitting vibrant, opaque red smoke trail from its rear. The surface appears smooth and metallic. Table 17. An example of Discriminative State Subtask. Subcategory 5: Spatial Relationship Definition: Grounding the target based on its spatial position relative to other entities. Subcategory 7: Limited Occlusion Definition: Localizing objects with partial visibility caused by occlusion or truncation by the image frame. Description: Immediately to the left of this hut is The object is hut. another beach hut, which is light grey in color. To its immediate right is vibrant blue beach hut. Below the hut is sturdy concrete wall or barrier, and further down is the sandy beach. Directly above and behind the hut is lush green hillside covered with dense vegetation. Description: The object is traffic cone, the first one from the right. Table 20. An example of Limited Occlusion Subtask. Table 18. An example of Spatial Relationship Subtask. Subcategory 8: Limited Small Subcategory 6: Spatial Counting Definition: Grounding the target based on explicit quantitative or ordinal information within the scene. Definition: Localizing objects with diminutive scale in ultrahigh resolution images. Description: The flag is made of fabric with creases as it moves in the breeze. To its right, there are five more flags. Description: The object is person holding camera. capturing photograph while seated outside. It appears to be Table 19. An example of Spatial Counting Subtask. Table 21. An example of Limited Small Subtask. Subcategory 9: Rejection Appearance Definition: Rejecting the query due to factual contradiction in the described visual attributes like color or texture. Subcategory 11: Rejection Text Definition: Rejecting the query due to factual mismatch with embedded textual information. Description: The object is an elongated, oval-shaped foil balloon, primarily red with prominent vertical white stripe running down its center. On either side of the white stripe, there are yellow, circle shapes outlined with red patterns. The balloon has smooth, reflective texture typical of Mylar balloons. Description: The object is white VGA coaxial cable coiled inside clear plastic blister pack with blue backing. green price label with black text 180 is affixed to the front of the packaging. The cable itself has smooth appearance and is neatly coiled into circular shape. Table 22. An example of Rejection Appearance Subtask. Table 24. An example of Rejection Text Subtask. Subcategory 10: Rejection Component Subcategory 12: Rejection State Definition: Rejecting the query due to factual contradiction concerning specific structural component. Definition: Rejecting the query because the objects described dynamic or static condition is factually incorrect. Description: The object is young child, consisting of their bare legs and small feet. The child wears pair of pink flip-flop with white sole on both feet. Description: The object is maroon-colored compact SUV with its doors closed. Its front end is heavily damaged and crushed, indicating an impact. The paint on the undamaged parts of the car appears somewhat glossy, and the windshield and windows are visible. Table 23. An example of Rejection Component Subtask. Table 25. An example of Rejection State Subtask."
        }
    ],
    "affiliations": [
        "LLM-Core Xiaomi",
        "Renmin University of China",
        "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University",
        "The University of Hong Kong"
    ]
}