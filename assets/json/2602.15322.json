{
    "paper_title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers",
    "authors": [
        "Taejong Joo",
        "Wenhan Xia",
        "Cheolmin Kim",
        "Ming Zhang",
        "Eugene Ie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 1 ] . [ 1 2 2 3 5 1 . 2 0 6 2 : r 2026-02On Surprising Effectiveness of Masking Updates in Adaptive Optimizers Taejong Joo1,*, Wenhan Xia2, Cheolmin Kim2, Ming Zhang2 and Eugene Ie2 1Northwestern University, 2Google, *Work done while the author was Student Researcher at Google. Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces curvaturedependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19% and 9% compared to Adam and Muon, respectively. Magma SkipUpdate Magma . SkipUpdate and Magma Algorithm 1 SkipUpdate Parameters {ğœƒ(ğ‘) Input: ğ‘¡ ğ‘=1, Updates from base optimizer {Î” (ğ‘) (ğ‘) }ğµ {ğ’ˆ ğ‘¡ moment estimates {ğœ‡ (ğ‘) }ğµ ğ‘=1 for Each block ğ‘ [ğµ] do (SkipUpdate) (SkipUpdate) (SkipUpdate) }ğµ ğ‘=1, Stochastic gradients }ğµ ğ‘=1, Firstğ‘¡ ğ‘¡ ğ‘ (ğ‘) ğ‘¡ = 2 ğ‘ (ğ‘) ğ‘¡ = sigmoid(cossim(ğœ‡ (ğ‘) ğ‘¡ = 0.9ğ‘ (ğ‘) ğ‘¡1 + 0.1ğ‘ (ğ‘) ğ‘ (ğ‘) ğ‘¡ ğ‘š(ğ‘) ğ‘¡ Bernoulli(0.5) ğ‘¡ ğ‘š(ğ‘) ğ‘¡ ğ‘ (ğ‘) ğœƒ(ğ‘) ğ‘¡+1 = ğœƒ(ğ‘) end for ğ‘¡ Î” (ğ‘) ğ‘¡ ğ‘¡ , ğ’ˆ (ğ‘) )/ğœ) ğ‘¡ (Magma) (Magma) (Magma) 1. Introduction Figure 1 Pre-training performance on C4 across model scales. Despite discarding half of updates, SkipUpdate yields substantial improvements over state-of-the-art dense optimizers. The availability of dense gradients from single backward pass in backpropagation (Rumelhart et al., 1986) enables efficient simultaneous parameter updates. This efficiency has made dense adaptive optimizers like Adam (Kingma and Ba, 2015) the de facto standard for large-scale LLM training. In contrast, this reliance on dense gradients creates structural mismatch with sparse update strategies, such as coordinate descent (Nesterov, 2012; Nutini et al., 2017; Wright, 2015). Consequently, despite their strong performance on the highly nonsmooth optimization problems common in LLM training, sparse methods are rarely used in this setting. In this work, we challenge this prevailing paradigm with counter-intuitive empirical finding: we observe that optimization performance can be substantially improved by randomly masking gradient updates. Specifically, we study variant of RMSProp (Tieleman and Hinton, 2012) in which entire parameter blocks are randomly masked at each iteration following Bernoulli distribution. When block is masked, its parameter update is skipped for that step; however, moment estimates are still updated densely, and surviving updates are appropriately rescaled to preserve unbiasedness (cf. SkipUpdate in Algorithm 1). From classical convergence analysis Corresponding author(s): taejong.joo@northwestern.edu, cheolmin@google.com 2026 Google. All rights reserved On Surprising Effectiveness of Masking Updates in Adaptive Optimizers perspective, such random masking would yield worse worst-case convergence guarantee due to increased stochastic noise in the updates (cf. 5). Further, each parameter effectively receives fewer updates per iteration while the computational cost of gradient computation via backpropagation remains unchanged. However, despite discarding half of the updates, SkipUpdate consistently outperforms dense optimizers, including the recent state-of-the-art optimizer Muon (Jordan et al., 2024), which incorporates sophisticated curvature information, across model scales (Figure 1). To explain the effectiveness of SkipUpdate, we analyze its geometric regularization effect. Specifically, we show that block-wise masking induces curvature-dependent geometric regularization, penalizing updates that align with sharp directions of the loss within each parameter block. This effect smooths the optimization trajectory and biases the algorithm toward flatter regions of the loss landscape, which are empirically associated with improved generalization in deep networks (Hochreiter and Schmidhuber, 1997; Jastrzkebski et al., 2017; Keskar et al., 2016). Importantly, this regularization emerges implicitly from stochastic noise in the update rule, rather than from explicit curvature computation. We further investigate whether stochastic masking can be made more effective by exploiting information already present in adaptive optimizers. We find that modulating the masked updates based on the cosine similarity between the stochastic gradient and the first moment estimate leads to substantial gains. This mechanism prioritizes momentum-consistent updates by suppressing updates not consistent with the accumulated direction of the gradient. This yields Momentumaligned gradient masking (Magma) that consistently outperforms both adaptive optimizers and SkipUpdate. Notably, Magmas effectiveness increases with model size, consistent with larger models exhibiting more challenging optimization landscapes that require stronger geometric regularization. Our contributions are: (i) Methodological: Magma is simple optimizer wrapper that improves training stability with improved generalization under the peculiar loss landscape of transformers at no additional computational cost; (ii) Theoretical: we show that random gradient masking induces geometric regularization toward flatter trajectories, reducing curvature sharpness and gradient noise; (iii) Empirical: we demonstrate consistent gains over state-of-the-art optimizers across diverse pre-training scenarios. 2. Update Masking as Regularization ğ‘¡ Notation. We denote by ğœƒğ‘¡ the model parameters at iteration ğ‘¡, which can be partitioned into ğµ disjoint blocks {ğœƒ(ğ‘) }ğµ ğ‘=1. Throughout this work, we treat each block as distinct parameter unit. We denote by ğ‘ğ‘™(ğœƒ) the gradient of the loss ğ‘™ with respect to ğœƒ(ğ‘) . The stochastic gradient of ğ‘™() at ğ‘¡ }ğµ is denoted by ğ’ˆğ‘¡ {ğ’ˆ ğ‘=1. Hğ‘ğ‘ (ğœƒğ‘¡) denotes the (ğ‘, ğ‘) block of the Hessian 2ğ‘™(ğœƒğ‘¡). We let (Fğ‘¡)ğ‘¡0 be filtration such that ğœƒğ‘¡ is Fğ‘¡-measurable and ğ”¼ğ‘¡ [] ğ”¼[Fğ‘¡]. (ğ‘) ğ‘¡ (ğ‘) ğ‘¡ (ğ‘) ğ‘¡ (ğ‘) ğ‘¡ ğ’ˆ , where ğ‘« Background: Adaptive optimizers & moment estimates. Adaptive optimizers adjust update magnitudes based on running statistics of past gradients, typically through diagonal or blockdiagonal preconditioning. Given learning rate ğœ‚ğ‘¡, these methods update parameters by ğœƒ(ğ‘) ğ‘¡+1 = ğœƒ(ğ‘) is positive (often ğ‘¡ ğœ‚ğ‘¡ ğ‘« diagonal) matrix encoding per-parameter or perblock adaptation. Popular adaptive optimizers differ primarily in how ğ‘« is constructed or replace (ğ‘) by more stable gradient estimator, trackğ’ˆ ğ‘¡ ing two running averages of gradient information. The first-moment estimate is moving average of the gradients ğœ‡ (ğ‘) ğ‘¡ = ğ›½1ğœ‡ (ğ‘) and the second-moment estimates track the squared )2, where gradients ğ’— ğ›½1, ğ›½2 (0, 1) are constants. RMSProp uses ğ‘« (ğ‘) ğ‘¡1 + (1 ğ›½2)(ğ’ˆ ğ‘¡1 + (1 ğ›½1)ğ’ˆ (ğ‘) (ğ‘) ğ‘¡ diag(ğ’— ğ‘¡ (ğ‘) ğ‘¡ = ğ›½2ğ’— ) 1/2. (ğ‘) ğ‘¡ (ğ‘) ğ‘¡ (ğ‘) ğ‘¡ ğ‘¡ ğ‘¡ At iteration ğ‘¡, Main result. , . . . , Î” ( ğµ) let Î”ğ‘¡ = (Î” (1) ) denote block-partitioned update direction from base optimizer (e.g., for adaptive optimizers). SkipUpdate ğœ‚ğ‘¡ ğ‘« incorporates independent Bernoulli random variables {ğ‘š(ğ‘) ğ‘¡ Bernoulli( ğ‘) with survival probability ğ‘ (0, 1]. Then, stochastic ğ‘=1, where ğ‘š(ğ‘) }ğµ (ğ‘) ğ‘¡ ğ’ˆ (ğ‘) ğ‘¡ ğ‘¡ 2 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers block-wise masking is applied to the updates as ğ‘¡ = ğ‘ (ğ‘) Î” (ğ‘) ğ‘¡ ğ‘š(ğ‘) ğ‘¡ Î” (ğ‘) ğ‘¡ , ğ‘ = 1, . . . , ğµ, (1) yielding Î”ğ‘¡ ( Î” (1) , . . . , Î” ( ğµ) ğ‘¡ = 1/ğ‘ to make the masked update unbiased; that is, ğ”¼ğ‘¡ [Î” (ğ‘) ). We set ğ‘ (ğ‘) ] = Î” (ğ‘) . ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ Although random masking preserves the expected update, it fundamentally alters the higherorder behavior of the optimization dynamics. In the following proposition, which is proved in Appendix A.1, we show that SkipUpdate induces curvature-dependent term in the expected loss decrease. Proposition 1. Conditioned on Fğ‘¡, the expected loss of SkipUpdate (cf. equation 1) is (cid:2)ğ‘™(ğœƒğ‘¡ Î”ğ‘¡)(cid:3) = ğ‘™(ğœƒğ‘¡ Î”ğ‘¡)+ 1 ğ‘ 2ğ‘ )Hğ‘ğ‘(ğœƒğ‘¡)Î” (ğ‘) (Î” (ğ‘) ğ‘¡ ğ‘¡ + ( ğ”¼ğ‘¡ ğµ ğ‘= ğµ ğ‘=1 Î” (ğ‘) ğ‘¡ 3). (2) ğ‘¡ ğ‘¡ 1 ğ‘ The term (ğ‘) )Hğ‘ğ‘(ğœƒğ‘¡)Î” (ğ‘) 2ğ‘ (Î” (ğ‘) admits ğ‘¡ natural interpretation as geometric regularizer. It measures the local curvature of the loss along the update direction Î” (ğ‘) , weighted roughly by ğ‘¡ the inverse survival probability. Since directions of large positive curvature correspond to sharp increases of the loss, minimizing the expected post-update loss implicitly discourages updates that align with high-curvature directions within each block. This block-structured regularization is particularly well-motivated in transformers, whose Hessians empirically exhibit pronounced blockdiagonal structure (Kunstner et al., 2024; Ormaniec et al., 2025; Zhang et al., 2024a). Under this geometry, the dominant curvature interactions occur within blocks. Consequently, the block-wise quadratic penalty in Proposition 1 induces principled second-order regularization that biases optimization toward flatter regions of the loss landscape. This preference for flatter minima has long been related to improved generalization (Hochreiter and Schmidhuber, 1997; Keskar et al., 2016) and is targeted by sharpnessaware optimization methods (Foret et al., 2020). Why dense momentum updates matter. In Algorithm 1, momentum states are updated densely even when parameter updates are masked. This contrasts with recent subspace optimization methods (Luo et al., 2024; Pan et al., 2024; Zhao et al., 2024), in which both parameters and auxiliary states are updated only on selected coordinates. For example, GaLore (Zhao et al., 2024) selects coordinates via leading gradient singular vectors and updates them over fixed batch intervals. While this reduces optimizer memory, it repeatedly optimizes suboptimal coordinate subset, contrary to classical coordinate descent insights (Nesterov, 2012; Nutini et al., 2017). Moreover, significant memory savings are not guaranteed in modern LLM training regimes, where memory consumption is dominated by activations rather than parameters or optimizer states (Shamshoum et al., 2025; Zhang et al., 2024b). Further, SkipUpdate effectively yields variance-reduced estimator of the true momentum due to the lazy update scheme, resulting in more stable search directions. Consequently, as shown in Figure A2, the dense momentum updates yield greater stability with improved generalization, compared to the sparse momentum updates as in the memory-efficient subspace optimizers. ğ‘¡ ğ‘=1 (cid:205)dim(ğ‘) ğ‘–=1 1 ğ‘ 2ğ‘ {Î” (ğ‘) Impacts of structured masking. Proposition 1 shows that the granularity of masking determines the structure of the induced curvature regularizer, with finer-grained masking progressively eliminating cross-coordinate interactions. For instance, element-wise masking only penalizes diagonal Hessian entries, yielding regular- }2 ization term (cid:205)ğµ ğ‘– {Hğ‘ğ‘(ğœƒğ‘¡)}ğ‘–,ğ‘–, where dim(ğ‘) is the dimensionality of the block ğ‘. Interestingly, under 130M Llama pre-training on the C4 dataset, SkipUpdate achieves similar perplexities across masking granularities21.78 (column-wise), 21.73 (element-wise), and 21.81 (block-wise)all substantially outperforming the RMSProp baseline (22.64). We conjecture that this near-equivalence reflects the limited ability of diagonal preconditioning to exploit dense withinblock curvature, rendering finer-grained masking of marginal practical benefit. Thus, we adopt block-wise masking for its favorable computa3 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers tional properties, as skipping entire blocks enables efficient operation pruning. and iterations (Huang et al., 2025; Wen et al., 2025). 3. Momentum-Aligned Update Masking SkipUpdate applies homogeneous masking operation across all blocks. However, parameters in transformers exhibit substantial heterogeneity, such as markedly different Hessian spectra (Zhang et al., 2024a) and gradient variances (Orvieto and Gower, 2025). Such heterogeneity strongly influences optimization dynamics and therefore motivates more refined block-adaptive masking strategy, while preserving seamless compatibility with modern adaptive optimizers. In stochastic optimization, gradient components consistent across iterations tend to carry meaningful optimization signal, whereas rapidly fluctuating components are often dominated by stochastic noise (Bottou et al., 2018; Polyak, 1964; Riedmiller and Braun, 1993). Further, recent work interprets moment estimates through the lens of online variational inference (Orvieto and Gower, 2025). Under this view, the probability of negative alignment satisfies â„™(ğœ‡ğ‘” < 0) = Î¦ , which decays exponentially in the signal-to-noise ratio ğœ‡/ğœ. Thus, negative alignment events represent statistically abnormal fluctuations, providing natural signal for identifying destabilizing updates. This motivates momentumgradient alignment as principled criterion for modulating update magnitudes. ğœ‡ ğœ (cid:16) (cid:17) Based on this insight, we propose Magma, which leverages block-wise momentum-gradient alignment to control the masking process. Specifically, for each block ğ‘ at iteration ğ‘¡, Magma computes an alignment score ğ‘ (ğ‘) (0, 1) as ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘š(ğ‘) ğ‘¡ = 0.9ğ‘ (ğ‘) = ğ‘ (ğ‘) ğ‘¡1 + 0.1ğ‘ (ğ‘) Then, Magma modulates the noisy (masked) update based on the alignment score ğ‘ (ğ‘) , apğ‘¡ Î” (ğ‘) plying the update rule Î” (ğ‘) , ğ‘ = 1, . . . , ğµ, with ğ‘ (ğ‘) being the exponential moving average. As result, Magma encourages coherent optimization trajectories with large ğ‘ (ğ‘) values and mitigates oscillatory updates ğ‘¡ with small ğ‘ (ğ‘) values. We emphasize that Magma is drop-in wrapper that multiplies ğ‘ (ğ‘) ğ‘¡ ğ‘š(ğ‘) into ğ‘¡ the existing update direction Î” (ğ‘) produced by (adaptive) optimizers, introducing no additional memory or computational overhead. Therefore, practitioners can adopt Magma in existing training pipelines with minimal code changes and no additional resource requirements. ğ‘¡ ğ‘¡ Similar principles underlie Cautious Optimizer (Liang et al., 2024) and MGUP (Chang and Yuan, 2025), which mask or attenuate parameter updates whose stochastic gradients have opposite signs to the first-moment estimate. Similarly, RPROP (Riedmiller and Braun, 1993) adapts step sizes based on the temporal consistency of gradient signs. However, unlike Magma, these methods lack structured stochastic masking, and therefore do not induce the curvature-dependent geometric regularization established in 2. Multiplication by the alignment score (damping) introduces bias into the masked update but substantially improves training stability (cf. Figure A2 in Appendix). We tested unbiased alternatives, such as using ğ‘ (ğ‘) as the survival probability with 1/ğ‘ (ğ‘) rescaling, but these consistently resulted in unstable training. Developing stable yet unbiased masking scheme for Magma remains an important future direction. ğ‘¡ ğ‘¡ ğ‘¡ = sigmoid (cid:16)cossim(ğœ‡ (ğ‘) ğ‘ (ğ‘) ğ‘¡ , ğ’ˆ (ğ‘) ğ‘¡ )/ğœ (cid:17) , (3) 4. Experiments ğ‘¡ where ğœ‡ (ğ‘) is the first-moment estimate, ğœ > 0 is temperature parameter, and cossim(, ) is the cosine similarity. Cosine similarity is adopted for its scale-invariant property, which is particularly effective in LLM training where gradient norms vary substantially across both parameter blocks 4.1. Pre-Training Llama We present Llama 2 pre-training results on the C4 dataset (Raffel et al., 2020) across model sizes of 60M, 130M, 350M, and 1B, following the standardized experimental setup established by Zhao et al. (2024) (See Appendix B.1 for details). For On Surprising Effectiveness of Masking Updates in Adaptive Optimizers Table 1 Llama 2 pre-training results on the C4 dataset. Validation perplexity is reported for four model scales (60M to 1B). denotes results from Li et al. (2025); remaining results are ours. RMSProp diverges for the 1B model within the learning rate search space. Method 60M 130M 350M 1B Adam C-Adam Adam+SGG Adam+Magma 24.77 30.79 23.59 29.70 22.18 30.31 22.08 29.09 22.08 29.09 22.08 29. 16.35 18.42 15.92 18.58 14.30 17.28 13.71 16.41 13.71 16.41 13.71 16.41 LaProp LaProp+Magma 23.07 29.98 22.16 29.05 22.16 29.05 22.16 29.05 16.38 18.56 13.82 16.37 13.82 16.37 13.82 16.37 Adafactor APOLLO APOLLO+SGG Muon RMSProp 32.57 31.55 30.18 28.93 29. 23.98 22.94 22.52 22.34 22.64 17.74 16.85 16.54 17.09 17.47 15.19 14.20 13.95 14.52 - 21.66 28.55 21.66 28.55 21.66 RMSProp+Magma 28.55 13.19 16.16 13.19 16.16 13.19 16.16 Magma proves to be the most effective optimization enhancer among the evaluated methods. When applied to Adam, Magma consistently outperforms alternative enhancement strategies, including Cautious Adam (C-Adam) and Scaling with Gradient Grouping (Adam+SGG). For instance, at the 1B parameter scale, Adam+Magma achieves validation perplexity of 13.81, significantly surpassing Adam+SGG (14.30) and CAdam (15.92). This indicates that the masking mechanism in Magma provides more robust regularization signal than the gradient modification techniques employed by competing enhancers. Moreover, Magma exhibits favorable scaling: its performance relative to base optimizers increases with model size. As larger models exhibit increasingly irregular and nonsmooth loss landscapes, this trend supports the interpretation of random masking as an effective form of geometric regularization. all experiments, we set ğœ = 2 and apply Magma updates exclusively to the attention and MLP layers. We found that this single configuration performs robustly across wide range of settings, as supported by the ablation studies in Appendix C. Table 1 compares Magma against wide range of algorithms including Adafactor (Shazeer and Stern, 2018), Adam, APOLLO (Zhu et al., 2025), LaProp (Ziyin et al., 2020), and RMSProp, as well as matrix-based optimizers such as Muon and SOAP (Vyas et al., 2024), and optimization enhancers such as Scaling with Gradient Grouping (SGG) (Li et al., 2025) and Cautious Optimizer (C-Adam). Table 1 demonstrates that Magma consistently improves performance of all base optimizers across model scales. When applied to Adam and LaProp, Magma yields significant perplexity reductions compared to their vanilla counterparts and competing enhancers like SGG. Most notably, RMSProp+Magma attains the lowest perplexity across all model sizes (60M1B), establishing new state-of-the-art for this benchmark. It outperforms computationally intensive matrix-based optimizers such as Muon and SOAP, as well as sophisticated enhancers like APOLLO+SGG. 4.2. Pre-Training Nano MoE In accordance with the widespread adoption of sparse mixture-of-experts (MoE) architecture in modern LLMs (Du et al., 2022; Fedus et al., 2022; Lepikhin et al., 2020; Shazeer et al., 2017), we validate Magma in the Nano MoE framework (Wolfe, 2024). This benchmark involves pretraining an MoE transformer on the OpenWebText dataset (Gao et al., 2020), and we follow the standard configuration and training protocol (see Appendix B.2 for details). MoE models are known to induce significantly more complex and nonsmooth optimization due to dynamic load balancing, sparse token-to-expert routing, and non-uniform gradient flow across parameters (Fedus et al., 2022; Shazeer et al., 2017; Zoph et al., 2022). Therefore, the MoE architectures serve as particularly stringent testbed for evaluating robustness of optimization algorithms. Our results in Figure 2 show that Magma consistently improves performance of both Adam and Muon in the MoE setting. When applied to Adam, Magma exhibits slower convergence during intermediate training but ultimately achieves superior final performance. Consistent with Llama pretraining, Magma also significantly outperforms 5 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers Figure 2 Optimization trajectories of pretraining the Nano MoE model on OpenWebText. the Cautious Optimizer (C-Adam), which similarly leverages momentumgradient alignment, corroborating the effectiveness of Magmas geometric regularization through structured random masking (3). Notably, when combined with Muon, Magma attains the best overall performance, substantially outperforming all baselines. This suggests that stochastic masking-based update modulation in Magma and structured preconditioning operate on largely orthogonal aspects of the optimization process, shaping both training dynamics and convergence geometry in complementary ways. Given the increasing reliance on sophisticated preconditioners such as Muon (Jordan et al., 2024) and SOAP (Vyas et al., 2024) in modern MoE-based LLM training, these results highlight Magma as robust and effective enhancement that can be seamlessly integrated into large-scale optimization pipelines. 4.3. Magma under Heavy-Tailed Gradient Noises salient feature of training autoregressive language models is the presence of heavy-tailed stochastic gradient noise (Kunstner et al., 2024; Zhang et al., 2020). To isolate and study the effect of heavy-tailed noise on optimization dynamics, we evaluate Magma using the controlled benchmark proposed in Ahn et al. (2024), which provides an abstraction of transformer training Figure 3 Magma on light-tailed and heavy-tailed data distributions. Top: Optimization trajectories for Adam and Magma. Bottom: Robust condition number defined as the ratio between the maximum and median eigenvalues of the loss Hessian. while faithfully reproducing key empirical phenomena observed in practice, including the performance gap between Adam and SGD for LLM pre-training. The benchmark considers training simplified linear transformer to solve random linear regression task in meta learning format (AkyÃ¼rek et al., 2022; Garg et al., 2022; Von Oswald et al., 2023). In this benchmark, we consider two data regimes, namely, light-tailed and heavy-tailed settings. Under the heavy-tailed setting, we modify the input sampling distribution to amplify tail behavior in the covariates, thereby inducing heavy-tailed stochastic gradient noise during optimization. We refer to Appendix B.4 for full experimental details. Figure 3 (top) compares the optimization trajectories of Adam and Magma under normal and heavy-tailed covariates. While both methods perform similarly under normal noise, Magma significantly outperforms Adam in the heavy-tailed setting, offering insight into its strong empirical performance in LLM pre-training, where heavytailed data is ubiquitous (Ahn et al., 2024; Kunstner et al., 2024). Notably, this result is appealing given Adams known robustness to heavy-tailed 6 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers Figure 4 Magma on homogeneous and heterogeneous quadratics. Top: Optimization trajectories for AdamW and Magma on quadratic objectives with identical eigenspectra but different block structures. Bottom: Average gradientmomentum alignment per block. distributions. To further analyze this behavior, Figure 3 (bottom) reports the robust condition number along training trajectories (Jiang et al., 2023). Under heavy-tailed noise, Magma consistently attains substantially smaller condition numbers, indicating that its updates remain confined to well-conditioned regions of the loss landscape. This reflects the curvature-dependent geometric regularization induced by scaled random masking, which selectively suppresses curvatureand noise-dominated directions, thereby enhancing robustness to extreme gradient fluctuations. 4.4. Magma on Heterogeneous Quadratics To further validate the effectiveness of Magma in heterogeneous landscapes, we evaluate Magma on controlled quadratic benchmark adopted in recent works (Orvieto and Gower, 2025; Zhang et al., 2024a). The benchmark consists of two quadratic objectives with identical eigenspectra but different block-wise curvature structure. In both cases, the loss takes the form ğ‘™(ğ’˜) = 1 2 ğ’˜ ğ‘¯ğ’˜, where ğ‘¯ has eigenvalues spanning three orders of magnitude. The key distinction lies in how these eigenvalues are arranged: in the homogeneous setting, eigenvalues of similar scale are grouped within blocks, whereas in the heterogeneous setting, each block mixes eigenvalues with vastly different magnitudes, inducing strong curvature misalignment. Despite its simplicity, this heterogeneous structure qualitatively mimics the loss geometry observed in autoregressive transformers, in contrast to the more scale-separated curvature typical of CNNs. Full details are provided in Appendix B.3. Figure 4 (top) presents optimization trajectories for both Hessian structures. On the homogeneous problem, Magma and AdamW exhibit comparable performance, with Magma converging slightly faster in the early stages. However, in the heterogeneous problem, while AdamW is known to substantially outperform non-adaptive methods such as SGD (Orvieto and Gower, 2025; Zhang et al., 2024a), Magma achieves faster convergence and lower final loss than AdamW. This mirrors the gains observed in the pre-training experiments (cf. Table 1), suggesting that Magma is particularly effective in regimes where curvature is both ill-conditioned and misaligned across parameter subspaces. Importantly, this advantage does not extend On Surprising Effectiveness of Masking Updates in Adaptive Optimizers to architectures whose curvature resembles the homogeneous case. For example, when applied to ResNet-50 on CIFAR-10 classification, Magma does not improve over AdamW (94.46% vs. 93.82% test accuracy after 100 epochs with carefully tuned configurations), reinforcing the hypothesis that its benefits are specific to transformer-like loss geometry. Figure 4 (bottom) analyzes the average gradientmomentum alignment within each block for AdamW and Magma. As expected, the homogeneous Hessian exhibits higher alignment across all blocks, reflecting its more benign conditioning. Notably, although Magma explicitly suppresses updates that conflict with accumulated momentum, it does not significantly increase the alignment score itself. This indicates that Magmas gains arise not from altering momentum statistics, but from enforcing consistency between instantaneous gradients and long-term descent directions. 5. Discussion We analyze the effect of scaled random masking in Magma through the lens of classical optimization theory. Setup. We consider ğœƒ â„ğ‘‘, which can be decomposed as ğœƒ = (ğœƒ(1) , ğœƒ(2) , , ğœƒ( ğµ) ) with ğœƒ(ğ‘) â„ğ‘‘ and ğµğ‘‘ = ğ‘‘. We assume the objective is lower bounded: ğ‘™ minğœƒ ğ‘™(ğœƒ) > . Let Mğ‘¡ (ğœƒ) (Mğ‘¡ (ğœƒ) (1) , , Mğ‘¡ (ğœƒ) ( ğµ) ) be scaled masking operator defined as Mğ‘¡ (ğ’ˆ) (ğ‘) ğ‘š(ğ‘) ğ‘ ğ’ˆ (ğ‘) , where ğ‘š(ğ‘) ğ‘¡ Bernoulli( ğ‘). We define an alignment score vector ğ’”ğ‘¡ â„ğµ (ğ‘ (1) ). Also, we use an operation ğ’”ğ‘¡ ğ‘°ğ‘‘, where is the Kronecker product and ğ‘°ğ‘‘ is the ğ‘‘ ğ‘‘ identity matrix, to scale each block update Mğ‘¡ (ğ‘”) (ğ‘) by ğ‘ (ğ‘) through (ğ’”ğ‘¡ ğ‘°ğ‘‘)Mğ‘¡ (ğœƒ). For brevity, we denote ğ‘ºğ‘¡ ğ’”ğ‘¡ ğ‘°ğ‘‘. , , ğ‘ ( ğµ) ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ Throughout this section, we consider constant learning rate SGD, isolating the impact of stochasticity and masking from the adaptivity of Adam-type methods: (Vanilla SGD) ğœƒğ‘¡+1 = ğœƒğ‘¡ ğœ‚ğ’ˆğ‘¡ for ğ‘¡ = 0, 1, ; (Magma) ğœƒğ‘¡+1 = ğœƒğ‘¡ ğœ‚ğ‘ºğ‘¡Mğ‘¡ (ğ’ˆğ‘¡) for ğ‘¡ = 0, 1, . Extensions to adaptive optimizers follow the similar descent lemma framework and do not alter the core argument; see Crawshaw et al. (2022); Wang and Klabjan (2022) for related analyses. All proofs of claims are presented in Appendix A. We begin by defining technical assumptions for the analysis. Assumption 2. There exist constants ğ¿(ğ‘) 0 such that for all ğœƒ â„ğ‘‘, ğ‘ [ğµ], ğ’– â„ğ‘‘, ğ‘™(ğœƒ + ğ‘¼ğ‘ğ’–) ğ‘™(ğœƒ) +ğ’–ğ‘ğ‘™(ğœƒ) + ğ¿(ğ‘) 2 ğ’–2, where ğ‘¼ğ‘ğ’– â„ğ‘‘ denotes the vector with block ğ‘ equal to ğ’– and other blocks 0. In Assumption 2, the block-wise smoothness bound naturally captures heterogeneous nature of the transformer loss landscape. We define the block-wise smoothness weighted semi-norm as 2. ğ’ˆğ‘¡ 2 ğ¿ (cid:205)ğµ ğ‘=1 ğ¿(ğ‘) ğ’ˆ (ğ‘) ğ‘¡ Assumption 3. There exist constants ğœğ‘ 0 such that for all ğœƒ â„ğ‘‘ and ğ‘ [ğµ], ğ”¼[ğ’ˆ (ğ‘) (ğœƒ)2] ğ‘ğ‘™(ğœƒ)2 +ğœ2 , where ğ’ˆ (ğ‘) (ğœƒ) denotes the stochasğ‘ tic gradient for ğœƒ(ğ‘) . Assumption 3 is standard in stochastic optimization and captures bounded variance of the stochastic gradient. Descent lemma. The following descent lemma characterizes the per-iteration decrease of smooth objective. Lemma 4. Under Assumption 2, Magma satisfies for all ğ‘¡, ğ”¼ğ‘¡ [ğ‘™(ğœƒğ‘¡+1)] ğ‘™(ğœƒğ‘¡)ğœ‚ğ”¼ğ‘¡ [ğ’ˆ ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡)]+ ğœ‚2 2ğ‘ ğ”¼ğ‘¡ [ğ‘ºğ‘¡ğ’ˆğ‘¡ 2 (4) ğ¿]. Compared to the vanilla SGDs bound ğ”¼ğ‘¡ [ğ‘™(ğœƒğ‘¡+1)] ğ‘™(ğœƒğ‘¡) ğœ‚ğ‘™(ğœƒğ‘¡) + ğœ‚2 2 ğ”¼ğ‘¡ [ğ’ˆğ‘¡ 2 ğ¿] (obtained by ğ‘ = 1 and ğ‘ºğ‘¡ = ğ‘°ğ‘‘), Magma differs from two terms: 1) the first-order term is reduced ğ’ˆ ğ‘¡ ğ‘™(ğœƒğ‘¡) a.e. since ğ‘ºğ‘¡ ğ¼ğ‘‘; 2) quadratic penalty uses effective smoothness constants. To see this, we define the secondmoment scaling factor: ğœŒ(ğ‘) ğ‘¡ ğ”¼ğ‘¡ [ ğ‘ (ğ‘) (ğ‘) ğ‘¡ ğ’ˆ ğ‘¡ (ğ‘) ğ”¼ğ‘¡ [ ğ’ˆ ğ‘¡ the quadratic penalty in equation 4, ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡) ğ’ˆ . Then, 2 ] 2 ] ğœ‚2 2ğ‘ ğ”¼ğ‘¡ [ğ‘ºğ‘¡ğ’ˆğ‘¡ 2 ğ¿] = ğœ‚2 2 ğµ ğ‘=1 ğœŒ(ğ‘) ğ‘¡ ğ¿(ğ‘) ğ‘ ğ”¼ğ‘¡ [ğ’ˆ (ğ‘) ğ‘¡ 2], (5) 8 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers describes how Magma rescales each block-wise ğœŒ(ğ‘) ğ¿(ğ‘) ğ‘ (ğ‘) ğ¿(ğ‘) ğ‘¡ ğ‘¡ smoothness constant as ğ¿(ğ‘) Accordingly, we define ğ’ˆğ‘¡ 2 ğ¿ğ‘¡ and ğœ2 ğ¿ğ‘¡ ğ¿(ğ‘) ğ‘¡ (cid:205)ğµ ğ¿(ğ‘) ğ‘¡ ğœ2 (cid:205)ğµ . 2 ğ‘= ğ‘=1 ğ’ˆ . ğ‘ ğ‘¡ Convergence analysis. To derive global convergence rate, we require lower bound on the descent term in equation 4. The next lemma provides such bound and defines the effective descent efficiency factors. Lemma 5. Under Assumption 3, it holds for all ğ‘¡, ğ”¼[ğ’ˆ ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡)] (ğ›¼ğ‘¡ ğ‘°ğ‘‘)ğ‘™(ğœƒğ‘‡ )2 ğœ2 ğ¶ğ‘¡ , (6) , , ğ›¼( ğµ) where ğ›¼ğ‘¡ (ğ›¼(1) ğ‘=1 ğ‘ (ğ‘) ğ‘¡ ğœ2 ğ‘ ; ğ›¼(ğ‘) [sigmoid(1/ğœ), sigmoid(1/ğœ)] and ğ‘ (ğ‘) ğ‘¡ [0, sigmoid(1/ğœ)/2] are constants defined in proof. ) and ğœ2 ğ¶ğ‘¡ (cid:205)ğµ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ Lemma 5 yields the effective descent efficiency under the Magmas scaled random masking. Specifically, ğ›¼(ğ‘) quantifies the fraction of descent preserved on block ğ‘ at iteration ğ‘¡ (up to the noise-coupling term). To describe aggregated ğ‘¡=0 ğ”¼[ (ğ›¼ğ‘¡ ğ‘°ğ‘‘ ) ğ‘™ (ğœƒğ‘¡ ) 2 ] ] impacts, we define ğ›¼eff ğ‘‡ ğ‘¡=0 ğ”¼[ ğ‘™ (ğœƒğ‘¡ ) 2 ] Also, we define an average noise-descent coupling ğœ2 ğ‘¡=0 ğ”¼[ğœ2 ] and the maximum effective ğ¶ ğ¶ğ‘¡ smoothness ğ¿max maxğ‘ [ ğµ] . 1 ğ‘‡ (cid:205)ğ‘‡ 1 ğ¿(ğ‘) ğ‘¡ (cid:205)ğ‘‡ (cid:205)ğ‘‡ 1 . ğ‘¡ Theorem 6. Under Assumptions 2 and 3, for the stepsize ğœ‚ (0, ], it holds that ğ›¼eff ğ‘‡ ğ¿max ğ‘¡ 1 ğ‘‡ ğ‘‡ 1 ğ‘¡=0 ğ”¼ (cid:2)ğ‘™(ğœƒğ‘¡)2(cid:3) 2(ğ‘™(ğœƒ0) ğ‘™) ğœ‚ğ›¼eff ğ‘‡ ğ‘‡ + 2ğœ2 ğ¶ ğ›¼eff ğ‘‡ + , ğœ‚ğœ2 ğ¿ ğ›¼eff ğ‘‡ (7) where ğœ2 ğ¿ = (cid:205)ğ‘‡ 1 ğ‘¡= 1 ğ‘‡ ğ”¼[ğœ2 ğ¿ğ‘¡ ]. (cid:205)ğ‘‡ 1 Theorem 6 provides the standard constantstep nonconvex stationarity guarantee with explicit dependence on the effective curvature constants. Note that for SGD, equation 7 reduces 2(ğ‘™ (ğœƒ0 ) ğ‘™ ) to 1 ğ‘¡=0 ğ”¼ğ‘™(ğœƒğ‘¡)2 ğ¿ , for ğœ‚ ğ‘‡ ğœ‚ğ‘‡ (0, 1 ] with ğ¿max maxğ‘ ğ¿(ğ‘) . In this regard, ğ¿max the scaling factor ğ‘ ğ‘¡ impacts the descent efficiency ğ›¼eff ğ‘‡ , the effective noise level ğœ2 , and the average ğ¿ noise-descent coupling ğœ2 . On the one hand, reğ¶ ducing ğ‘ (ğ‘) decreases the descent term through + ğœ‚ğœ ğ‘¡ ğ›¼(ğ‘) , potentially slowing optimization. On the ğ‘¡ other hand, the same scaling suppresses the curvature-weighted noise contribution via the effective smoothness constants ğ¿(ğ‘) ğ¿(ğ‘) , thereby reducing the stationary noise floor. In this regard, Theorem 6 shows that scaling is not uniformly beneficial. Rather, it makes explicit that any improvement must come from where the suppression occurs (which blocks are attenuated), not merely how much suppression is applied on average. ğœŒ(ğ‘) ğ‘¡ ğ‘ = ğ‘¡ ğ‘¡ On effective iteration number. favorable regime arises when ğœŒ(ğ‘) ğ‘¡ 1 holds for blocks with large curvature ğ¿(ğ‘) (or more generally, for blocks (ğ‘) 2). In this case, both dominating (cid:205)ğ‘ ğ¿(ğ‘) ğ”¼ğ‘¡ ğ’ˆ ğ‘¡ the effective smoothness constants ğ¿(ğ‘) and the curvature-weighted noise ğœ2 are reduced, which ğ¿ğ‘¡ simultaneously (i) enlarges the admissible stepsize range and (ii) lowers the stationary error floor in Theorem 6. Moreover, the enlarged stability region increases the number of iterations for which the stochastic update satisfies the descent surrogate ğœ‚ğ’ˆ ğ¿ > 0 thereby accelerating convergence in terms of effective progress per iteration. ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡) ğœ‚2 2ğ‘ ğ‘ºğ‘¡ğ’ˆğ‘¡ 2 In this regard, Magma enlarges the stability region by selectively suppressing blocks that dominate the curvature-weighted noise and smoothness constraints. In ill-conditioned and highly heterogeneous transformer landscapes (Ahn et al., 2024; Orvieto and Gower, 2025; Pan and Li, 2023), stability is typically governed by small subset of high-curvature or high-variance blocks. By attenuating precisely these blocks according to the momentumgradient alignment, Magma reduces the effective smoothness ğ¿ğ‘¡ and noise level ğœ2 that limit the admissible stepsize. This tarğ¿ğ‘¡ geted reduction explains the empirically observed widening of the stable learning-rate regime (Figure A3 in Appendix) and accounts for the effectiveness of structured scaling over uniform or random masking in large-scale LLM training. 6. Literature Review Stabilizing LLM training. line of recent work addresses instability in LLM training by explic9 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers itly constraining optimizer updates in unstable regimes. The method most closely related to Magma is Cautious Optimizer (Liang et al., 2024) where an update for each parameter is masked when its gradient has an opposite direction to its first-moment estimate for ensuring descent direction update under small step size. However, due to its deterministic masking rule, Cautious Optimizer lacks the geometric regularization effect promoting flatter optimization trajectories, unlike Magma. More broadly, peculiar optimization landscape of large-scale transformers is known to contain many sources of training instability such as loss spikes and gradient explosions (Chowdhery et al., 2023). To mitigate these issues, prior work has explored gradient clipping with periodic momentum reset to address negative impacts of gradient spikes (Huang et al., 2025), initialization methods for stable training dynamics (Nguyen and Salazar, 2019; Takase et al., 2023), and architectural interventions that reshape gradient propagation (Dettmers et al., 2021; Wang et al., 2024; Xiong et al., 2020). Despite this wide exploration, the direction of inducing geometric regularization through stochastic perturbations of the optimization process remains relatively underexplored. Geometry-aware and trust-region methods. large body of work improves optimization by incorporating local geometry through curvatureaware updates (Dauphin et al., 2014; Gupta et al., 2018; Martens and Grosse, 2015; Yao et al., 2021) or trust-region constraints (Foret et al., 2020; Kwon et al., 2021). The former direction aims to approximate second-order structure by tractable preconditioners, recently extending to efficient eigen-decomposition for LLM training (Vyas et al., 2024). On the contrary, the latter trust-regionflavored methods, such as SAM and its recent variants (Li et al., 2024; Mueller et al., 2023), bias optimization toward flatter regions by optimizing robustness to parameter perturbations, albeit at the cost of additional gradient evaluations. Instead of computing explicit curvature matrices or adversarial perturbation, Magma penalizes sharp curvature along its own block-wise update directions via stochastic masking, offering lightweight alternative to flatness-seeking optimization. Stochastic perturbation and noise injection. Stochastic perturbation is well-established strategy in machine learning, ranging from random gradient masking in meta-learning (Tseng et al., 2020) and federated learning (Wei et al., 2020b) to the annealed Gaussian noise used in Bayesian inference (Neelakantan et al., 2015; Welling and Teh, 2011). prominent example is Dropout (Srivastava et al., 2014) that randomly masks hidden units, which has been shown to induce data-dependent weight-space regularizer that promotes model stability (Mianjy et al., 2018; Wei et al., 2020a; Zhang and Xu, 2024). In the context of modern LLMs, this principle has evolved into embedding-space perturbations, injecting structured noise into token embeddings during instruction fine-tuning (Jain et al., 2024). However, the impact of structured and stateful perturbations on optimization dynamics remains relatively underexplored. Magma operates directly on parameter updates and their alignment with local curvature for regularizing the optimization dynamics, highlighting role for stochastic perturbation particularly suited to modern large-scale optimization. 7. Conclusion We showed that randomly masking parameter updates can substantially improve LLM pre-training, which smooths optimization trajectories with an implicit curvature-dependent geometric regularization. The proposed Magma, which leverages momentum-gradient alignment to further enhance masked updates, achieves consistent improvements over state-of-the-art adaptive optimizers with negligible overhead. These results challenge the prevailing assumption that dense updates are inherently optimal for backpropagationbased neural net training. We expect this perspective to inspire new classes of optimization algorithms that exploit structured stochasticity to improve both optimization stability and generalization in training large-scale foundation models having ill-conditioned and highly heterogeneous optimization landscapes. 10 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "K. Ahn, X. Cheng, M. Song, C. Yun, A. Jadbabaie, and S. Sra. Linear attention is (maybe) all you need (to understand transformer optimization). In International Conference on Learning Representations, 2024. E. AkyÃ¼rek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 2018. D. Chang and G. Yuan. Mgup: momentumgradient alignment update policy for stochastic optimization. In Advances in Neural Information Processing Systems, 2025. A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. M. Crawshaw, M. Liu, F. Orabona, W. Zhang, and Z. Zhuang. Robustness to unbounded smoothness of generalized signsgd. In Advances in Neural Information Processing Systems, 2022. Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in highdimensional non-convex optimization. In Advances in Neural Information Processing Systems, 2014. T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021. N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, 2022. W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020. L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? case study of simple function classes. In Advances in Neural Information Processing Systems, 2022. V. Gupta, T. Koren, and Y. Singer. Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning, 2018. S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):142, 1997. T. Huang, Z. Zhu, G. Jin, L. Liu, Z. Wang, and S. Liu. Spam: Spike-aware adam with momentum reset for stable llm training. arXiv preprint arXiv:2501.06842, 2025. N. Jain, P. yeh Chiang, Y. Wen, J. Kirchenbauer, H.-M. Chu, G. Somepalli, B. R. Bartoldson, B. Kailkhura, A. Schwarzschild, A. Saha, M. Goldblum, J. Geiping, and T. Goldstein. NEFTune: Noisy embeddings improve instruction finetuning. In International Conference on Learning Representations, 2024. 11 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers S. Jastrzkebski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017. K. Jiang, D. Malik, and Y. Li. How does adaptive optimization impact local neural network geometry? In Advances in Neural Information Processing Systems, 2023. K. Jordan, Y. Jin, V. Boza, J. You, F. Cesista, L. Newhouse, and J. Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan. github.io/posts/muon/. N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. D. P. Kingma and J. Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations. 2015. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. K. Liang, L. Chen, B. Liu, and Q. Liu. Cautious optimizers: Improving training with one line of code. arXiv preprint arXiv:2411.16085, 2024. Q. Luo, H. Yu, and X. Li. Badam: memory efficient full parameter optimization method for large language models. In Advances in Neural Information Processing Systems, 2024. J. Martens and R. Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International Conference on Machine Learning, 2015. P. Mianjy, R. Arora, and R. Vidal. On the implicit bias of dropout. In International Conference on Machine Learning, 2018. M. Mueller, T. Vlaar, D. Rolnick, and M. Hein. Normalization layers are all that sharpness-aware minimization needs. In Advances in Neural Information Processing Systems, 2023. F. Kunstner, A. Milligan, R. Yadav, M. Schmidt, and A. Bietti. Heavy-tailed class imbalance and why adam outperforms gradient descent on language models. In Advances in Neural Information Processing Systems, 2024. A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015. J. Kwon, J. Kim, H. Park, and I. K. Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning, 2021. D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. S. Li, J. Tian, Z. Wang, X. Jin, Z. Liu, W. Zhang, and D. Xu. Taming LLMs with gradient grouping. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025. T. Li, P. Zhou, Z. He, X. Cheng, and X. Huang. In Friendly sharpness-aware minimization. Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341362, 2012. T. Q. Nguyen and J. Salazar. Transformers without tears: Improving the normalization of selfattention. arXiv preprint arXiv:1910.05895, 2019. J. Nutini, I. Laradji, and M. Schmidt. Lets make block coordinate descent converge faster: Faster greedy rules, message-passing, active-set complexity, and superlinear convergence. arXiv preprint arXiv:1712.08859, 2017. W. Ormaniec, F. Dangel, and S. P. Singh. What does it mean to be transformer? insights from theoretical hessian analysis. In International Conference on Learning Representations, 2025. 12 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers A. Orvieto and R. Gower. In search of adams secret sauce. arXiv preprint arXiv:2505.21829, 2025. R. Pan, X. Liu, S. Diao, R. Pi, J. Zhang, C. Han, and T. Zhang. Lisa: layerwise importance sampling for memory-efficient large language model finetuning. In Advances in Neural Information Processing Systems, 2024. Y. Pan and Y. Li. Toward understanding why adam converges faster than sgd for transformers. arXiv preprint arXiv:2306.00204, 2023. B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 1964. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. https://cdn. openai.com/better-language-models/ language_models_are_unsupervised_ multitask_learners.pdf, 2019. [Online; accessed 28-January-2026]. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. M. Riedmiller and H. Braun. direct adaptive method for faster backpropagation learning: The rprop algorithm. In IEEE International Conference on Neural Networks, 1993. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533536, 1986. Y. Shamshoum, N. Hodos, Y. Sieradzki, and A. Schuster. Compact: Compressed activations for memory-efficient llm training. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2025. N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, 2018. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparselygated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):19291958, 2014. S. Takase, S. Kiyono, S. Kobayashi, and J. Suzuki. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. T. Tieleman and G. Hinton. rmsprop: Divide the gradient by running average of its recent magnitude, 2012. URL https://www.cs. toronto.edu/tijmen/csc321/slides/ lecture_slides_lec6.pdf. H.-Y. Tseng, Y.-W. Chen, Y.-H. Tsai, S. Liu, Y.-Y. Lin, and M.-H. Yang. Regularizing meta-learning via gradient dropout. In Asian Conference on Computer Vision, 2020. J. Von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, 2023. N. Vyas, D. Morwani, R. Zhao, M. Kwun, I. Shapira, D. Brandfonbrener, L. Janson, and Improving and stabilizS. Kakade. ing shampoo using adam. arXiv preprint arXiv:2409.11321, 2024. Soap: H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, and F. Wei. Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(10):67616774, 2024. R. Wang and D. Klabjan. Divergence results and convergence of variance reduced version of adam. arXiv preprint arXiv:2210.05607, 2022. 13 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers C. Wei, S. Kakade, and T. Ma. The implicit and explicit regularization effects of dropout. In International Conference on Machine Learning, 2020a. W. Wei, L. Liu, M. Loper, K.-H. Chow, M. E. Gursoy, S. Truex, and Y. Wu. framework for evaluating gradient leakage attacks in federated learning. arXiv preprint arXiv:2004.10397, 2020b. M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In International Conference on Machine Learning, 2011. Z. Wen, Y. Shi, J. Wang, P. Luo, L. Qiao, D. Li, and T. Sun. SRON: State-free LLM training via row-wise gradient normalization, URL https://openreview.net/ 2025. forum?id=BtQLBWr6zI. C. R. Wolfe. nanomoe: Minimal mixture-ofexperts implementation, 2024. URL https: //github.com/wolfecameron/nanoMoE. GitHub repository. S. J. Wright. Coordinate descent algorithms. Mathematical Programming, 151(1): 334, 2015. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, 2020. Z. Yao, A. Gholami, S. Shen, M. Mustafa, K. Keutzer, and M. Mahoney. Adahessian: An adaptive second order optimizer for machine learning. In AAAI Conference on Artificial Intelligence, 2021. J. Zhang, T. He, S. Sra, and A. Jadbabaie. Why gradient clipping accelerates training: theoretical justification for adaptivity. In International Conference on Learning Representations, 2020. Y. Zhang, C. Chen, T. Ding, Z. Li, R. Sun, and Z. Luo. Why transformers need adam: hessian perspective. In Advances in Neural Information Processing Systems, 2024a. Y. Zhang, P. Li, J. Hong, J. Li, Y. Zhang, W. Zheng, P.-Y. Chen, J. D. Lee, W. Yin, M. Hong, et al. Revisiting zeroth-order optimization for memoryefficient llm fine-tuning: benchmark. arXiv preprint arXiv:2402.11592, 2024b. Z. Zhang and Z.-Q. J. Xu. Implicit regularization of dropout. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(6):42064217, 2024. J. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, and Y. Tian. Galore: Memory-efficient llm training by gradient low-rank projection. In International Conference on Machine Learning, 2024. H. Zhu, Z. Zhang, W. Cong, X. Liu, S. Park, V. Chandra, B. Long, D. Z. Pan, Z. Wang, and J. Lee. Apollo: Sgd-like memory, adamw-level performance. Proceedings of Machine Learning and Systems, 7, 2025. L. Ziyin, Z. T. Wang, and M. Ueda. Laprop: Separating momentum and adaptivity in adam. arXiv preprint arXiv:2002.04839, 2020. B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. 14 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers A. Proofs of Claims A.1. Proof of Proposition Proof. Condition on Fğ‘¡, under which ğœƒğ‘¡ and Î”ğ‘¡ are deterministic. second-order Taylor expansion of ğ‘™(ğœƒğ‘¡ Î”ğ‘¡) around ğœƒğ‘¡ yields ğ‘™(ğœƒğ‘¡ Î”ğ‘¡) = ğ‘™(ğœƒğ‘¡) ğµ ğ‘=1 (ğ’ˆ (ğ‘) ğ‘¡ ) Î” (ğ‘) ğ‘¡ + 1 2 ğµ ğµ ğ‘= ğ‘=1 ( Î” (ğ‘) ğ‘¡ )Hğ‘ğ‘ (ğœƒğ‘¡) Î” (ğ‘ ) ğ‘¡ + ğ‘…2( Î”ğ‘¡), (8) where ğ’ˆğ‘¡ = ğ‘™(ğœƒğ‘¡), Hğ‘ğ‘ (ğœƒğ‘¡) is the (ğ‘, ğ‘) Hessian block, and the remainder satisfies ğ‘…2( Î”ğ‘¡) = ğ‘‚ (cid:16)(cid:205)ğµ . ğ‘=1 Î” (ğ‘) ğ‘¡ 3(cid:17) By construction of the masked update and conditioning on (ğœƒğ‘¡, Î”ğ‘¡), ğ”¼[Î” (ğ‘) ğ‘¡ ğœƒğ‘¡, Î”ğ‘¡] = 1 ğ‘ ğ”¼[ğ‘š(ğ‘) ğ‘¡ ]Î” (ğ‘) ğ‘¡ = Î” (ğ‘) ğ‘¡ . Moreover, using independence of {ğ‘š(ğ‘) ğ‘¡ }ğµ ğ‘=1, (cid:104) ğ”¼ ( Î” (ğ‘) ğ‘¡ )Hğ‘ğ‘ (ğœƒğ‘¡) Î” (ğ‘ ) ğ‘¡ ğœƒğ‘¡, Î”ğ‘¡ (cid:105) = (9) (10) ğ‘ ğ‘, (Î” (ğ‘) ğ‘¡ ğ‘ (Î” (ğ‘) )Hğ‘ğ‘ (ğœƒğ‘¡)Î” (ğ‘ ) )Hğ‘ğ‘(ğœƒğ‘¡)Î” (ğ‘) 1 ğ‘¡ ğ‘¡ ğ‘¡ , , ğ‘ = ğ‘. Taking conditional expectations in equation 8 and regrouping terms, we obtain ğ”¼[ğ‘™(ğœƒğ‘¡ Î”ğ‘¡) ğœƒğ‘¡, Î”ğ‘¡] = ğ‘™(ğœƒğ‘¡ Î”ğ‘¡) + ğµ ğ‘=1 1 ğ‘ 2ğ‘ (Î” (ğ‘) ğ‘¡ )Hğ‘ğ‘(ğœƒğ‘¡)Î” (ğ‘) ğ‘¡ + ğ”¼[ğ‘…2( Î”ğ‘¡) ğœƒğ‘¡, Î”ğ‘¡]. (11) Finally, since ğ”¼ Î” (ğ‘) (cid:16)(cid:205)ğµ 3 = ğ‘‚(Î” (ğ‘) 3(cid:17), completing the proof. ğ‘=1 Î” (ğ‘) ğ‘¡ ğ‘¡ ğ‘¡ 3) for each ğ‘, the expected remainder term is of order ğ‘‚ A.2. Proof of Lemma 4 Proof. Under Assumption 2, for any ğœƒ and any multi-block = ( (1) , , ( ğµ) ), it holds ğ‘™(ğœƒ + ) ğ‘™(ğœƒ) + ğ‘™(ğœƒ) + 1 2 2 ğ¿, which can be shown by applying Assumption 2 sequentially. Applying equation 12 with = ğœ‚ğ‘ºğ‘¡Mğ‘¡ (ğœƒ) yields ğ‘™(ğœƒğ‘¡+1) ğ‘™(ğœƒğ‘¡) ğœ‚(ğ‘ºğ‘¡Mğ‘¡ (ğ’ˆğ‘¡))ğ‘™(ğœƒğ‘¡) + ğœ‚2 2 ğ‘ºğ‘¡Mğ‘¡ (ğ’ˆğ‘¡)2 ğ¿ . By the independence between ğ‘š(ğ‘) ğ‘¡ and ğ’ˆ (ğ‘) ğ‘¡ ğ”¼ğ‘¡ [(ğ‘ºğ‘¡Mğ‘¡ (ğ’ˆğ‘¡))ğ‘™(ğœƒğ‘¡)] = ğ”¼[ğ’ˆ , we have ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡)], and ğ”¼ğ‘¡ [ğ‘ºğ‘¡Mğ‘¡ (ğ’ˆğ‘¡)2 ğ¿] = 1 ğ‘ ğ”¼ğ‘¡ [ğ‘ºğ‘¡ğ’ˆğ‘¡ 2 ğ¿]. (12) (13) (14) (15) Therefore, taking conditional expectation ğ”¼ğ‘¡ [] on both side of equation 13 yields the desired result. 15 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers A.3. Proof of Lemma 5 Proof. We start by defining the event ğ¸ (ğ‘) ğ‘¡ {cos(ğ’ˆ for an alignment threshold ğ›¾ (ğ‘) (0, 1]. We let ğ‘’(ğ‘) sigmoid(1/ğœ), ğ‘ + sigmoid(1/ğœ), and ğ‘ (ğ‘) (ğ‘) ğ‘¡ , ğ‘ğ‘™(ğœƒğ‘¡)) ğ›¾ (ğ‘) } ğ‘¡ â„™(ğ¸ (ğ‘) ğ›¾ sigmoid(ğ›¾ (ğ‘) /ğœ) for ğ‘ [ğµ]. ğ‘¡ Fğ‘¡). Also, we define constants ğ‘  (16) By using the law of total expectation, we get ğ”¼[ğ’ˆ ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡)] = ğµ ğ‘=1 ğ”¼[ğ‘ (ğ‘) ğ‘¡ (ğ’ˆ (ğ‘) ğ‘¡ )ğ‘ğ‘™(ğœƒğ‘¡)] ğµ (cid:110) = ğ‘=1 ğ‘¡ ğ”¼[ğ‘ (ğ‘) ğ‘’(ğ‘) ğ‘¡ (ğ’ˆ (ğ‘) ğ‘¡ )ğ‘ğ‘™(ğœƒğ‘¡)ğ¸ (ğ‘) ğ‘¡ ] + (1 ğ‘’(ğ‘) ğ‘¡ )ğ”¼[ğ‘ (ğ‘) ğ‘¡ (ğ’ˆ (ğ‘) ğ‘¡ )ğ‘ğ‘™(ğœƒğ‘¡)(ğ¸ (ğ‘) ğ‘¡ )ğ‘] (cid:111) ğµ (cid:110) ğ‘=1 ğ‘¡ ğ”¼[ğ‘ (ğ‘) ğ‘’(ğ‘) ğ›¾ (ğ’ˆ (ğ‘) ğ‘¡ )ğ‘ğ‘™(ğœƒğ‘¡)ğ¸ (ğ‘) ğ‘¡ ] + (1 ğ‘’(ğ‘) ğ‘¡ )ğ”¼[ğ‘  (ğ’ˆ (ğ‘) ğ‘¡ )ğ‘ğ‘™(ğœƒğ‘¡)(ğ¸ (ğ‘) ğ‘¡ )ğ‘] (cid:111) , (17) where the inequality comes from ğ‘ (ğ‘) ğ‘¡ ğ‘ (ğ‘) ğ›¾ on ğ¸ (ğ‘) ğ‘¡ and ğ‘ (ğ‘) ğ‘¡ ğ‘  on (ğ¸ (ğ‘) ğ‘¡ )ğ‘ for all ğ‘. Then, using unbiasedness of ğ’ˆğ‘¡, we get ğ”¼[ğ’ˆ ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡)] ğµ (cid:110) ğ‘=1 ğ›¾ ğ‘™(ğœƒğ‘‡ )2 + (ğ‘  ğ‘ (ğ‘) ğ‘ (ğ‘) ğ›¾ )ğ”¼[(ğ’ˆ (ğ‘) ğ‘¡ )ğ‘ğ‘™(ğœƒğ‘¡)1(ğ¸ (ğ‘) ğ‘¡ ) ğ‘ ] (cid:111) . (18) On (ğ¸ (ğ‘) ğ‘¡ )ğ‘, we have (ğ’ˆ (ğ‘) ğ‘¡ )ğ‘ğ‘™(ğœƒğ‘¡) < ğ›¾ (ğ‘) ğ‘ğ‘™(ğœƒğ‘¡)ğ’ˆ whenever ğ‘ğ‘™(ğœƒğ‘¡)ğ’ˆ (ğ‘) ğ‘¡ (ğ‘) ğ‘¡ > 0. Therefore, taking conditional expectation and applying Cauchy-Schwarz gives ğ”¼[(ğ’ˆ (ğ‘) ğ‘¡ )ğ‘ğ‘™(ğœƒğ‘¡)1(ğ¸ (ğ‘) ğ‘¡ ) ğ‘ ] ğ›¾ (ğ‘) ğ‘ğ‘™(ğœƒğ‘¡) ğ”¼[ğ’ˆ (1 ğ‘’(ğ‘) )ğ”¼ğ’ˆ (ğ‘) ğ‘¡ ğ‘¡ ğ›¾ (ğ‘) ğ‘ğ‘™(ğœƒğ‘¡) (ğ‘) ğ‘¡ 1(ğ¸ (ğ‘) ğ‘¡ ) ğ‘ ] 2 ğ›¾ (ğ‘) ğ‘ğ‘™(ğœƒğ‘¡) (1 ğ‘’(ğ‘) ğ‘¡ )(ğ‘ğ‘™(ğœƒğ‘¡)2 + ğœ2 ğ‘ ), (19) where the last inequality holds due to Assumption 3. Therefore, since ğ‘  ğ‘ (ğ‘) ğ›¾ < 0, combining equation 17 and equation 19 gives ğ”¼[ğ’ˆ ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡)] ğµ (cid:26) ğ‘=1 ğ›¾ ğ‘ğ‘™(ğœƒğ‘‡ )2 + (ğ‘  ğ‘ (ğ‘) ğ‘ (ğ‘) ğ›¾ )ğ›¾ (ğ‘) ğ‘ğ‘™(ğœƒğ‘¡) (1 ğ‘’(ğ‘) ğ‘¡ )(ğ‘ğ‘™(ğœƒğ‘¡)2 + ğœ2 ğ‘ ) (cid:27) . (20) Finally, note that for ğ‘, ğ‘ 0, it holds ğ‘(ğ‘ + ğ‘) ğ‘ + ğ‘ 2 . Applying this inequality with ğ‘ = ğ‘ğ‘™(ğœƒğ‘¡)2 and ğ‘ = ğœ2 ğ‘ to equation 20 yields ğ”¼[ğ’ˆ ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡)] (cid:40) (cid:18) ğµ ğ‘= ğ‘ (ğ‘) ğ›¾ (ğ‘ (ğ‘) ğ›¾ ğ‘ )ğ›¾ (ğ‘) 1 ğ‘’(ğ‘) ğ‘¡ (cid:19) ğ‘ğ‘™(ğœƒğ‘‡ )2 (ğ‘ (ğ‘) ğ›¾ ğ‘ )ğ›¾ (ğ‘) 2 (cid:41) 1 ğ‘’(ğ‘) ğ‘¡ ğœ ğ‘ ğµ (cid:110) = ğ›¼(ğ‘) ğ‘¡ ğ‘= ğ‘ğ‘™(ğœƒğ‘‡ )2 ğ‘ (ğ‘) ğ‘¡ ğœ2 ğ‘ (cid:111) , (21) where ğ›¼(ğ‘) and ğ‘ (ğ‘) ğ›¾ (ğ‘ (ğ‘) ğ‘¡ ğ‘ (ğ‘) ğ›¾ ğ‘ )ğ›¾ (ğ‘) follow directly from the definition. 1 ğ‘’(ğ‘) ğ‘¡ ğ‘¡ and ğ‘ (ğ‘) ğ‘¡ (ğ‘ (ğ‘) ğ›¾ ğ‘  ) ğ›¾ (ğ‘) 2 1 ğ‘’(ğ‘) ğ‘¡ . The interval bounds of ğ›¼(ğ‘) ğ‘¡ 16 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers A.4. Proof of Theorem 6 Proof. First, we note from equation 5 that 1 ğ‘ ğ”¼ğ‘¡ [ğ‘ºğ‘¡ğ’ˆğ‘¡ 2 ğ¿] = ğµ ğ‘=1 ğ¿(ğ‘) ğ‘¡ ğ”¼ğ‘¡ [ğ’ˆ (ğ‘) ğ‘¡ 2] ğµ ğ‘=1 ğ¿(ğ‘) ğ‘¡ (ğ‘ğ‘™(ğœƒğ‘¡)2 + ğœ ğ‘ ), (22) where the inequality comes from Assumption 3. Then, applying equation 6 in Lemma 5 to equation 4 in the descent lemma yields ğ”¼ğ‘¡ [ğ‘™(ğœƒğ‘¡+1)] ğ‘™(ğœƒğ‘¡) ğœ‚ğ”¼ğ‘¡ [ğ’ˆ ğ‘¡ ğ‘ºğ‘¡ğ‘™(ğœƒğ‘¡)] + ğœ‚2 2ğ‘ ğ”¼ğ‘¡ [ğ‘ºğ‘¡ğ’ˆğ‘¡ 2 ğ¿] ğ‘™(ğœƒğ‘¡) ğœ‚ (cid:16) (ğ›¼ğ‘¡ ğ‘°ğ‘‘)ğ‘™(ğœƒğ‘‡ )2 ğœ2 ğ¶ğ‘¡ (cid:17) + ğœ‚2 2 ğµ ğ‘=1 ğ¿(ğ‘) ğ‘¡ (ğ‘ğ‘™(ğœƒğ‘¡)2 + ğœ2 ğ‘ ). (23) By taking total expectation and telescoping equation 23 from ğ‘¡ = 0 to ğ‘¡ = ğ‘‡ 1 for some ğ‘‡ 1, we get ğ‘‡ 1 ğ‘¡=0 ğœ‚ğ”¼[(ğ›¼ğ‘¡ ğ‘°ğ‘‘)ğ‘™(ğœƒğ‘‡ )2] ğ‘™(ğœƒ0) ğ‘™ + ğ‘‡ 1 ğ‘¡=0 ğœ‚ğ”¼[ğœ2 ğ¶ğ‘¡ ] + ğœ‚2 2 ğ‘‡ 1 ğµ ğ‘¡=0 ğ‘=1 ğ¿(ğ‘) ğ‘¡ (ğ‘ğ‘™(ğœƒğ‘¡)2 + ğœ2 ğ‘ ), (24) where we use the factor that ğ”¼[ğ‘™(ğœƒğ‘‡ )] > ğ‘™. Now, by doing elementary algebra, we get ğ‘‡ 1 ğ‘¡=0 ğœ‚ğ”¼[(ğ›¼ğ‘¡ ğ‘°ğ‘‘)ğ‘™(ğœƒğ‘‡ )2] ğ‘™(ğœƒ0) ğ‘™ + ğ‘‡ 1 ğ‘¡=0 ğœ‚ğ”¼[ğœ2 ğ¶ğ‘¡ ] + ğœ‚2 2 1 ğ‘‡ ğ‘‡ 1 ğ‘¡=0 ğ”¼[ğ‘™(ğœƒğ‘¡)2] ğ‘™(ğœƒ0) ğ‘™ ğœ‚ğ›¼eff ğ‘‡ ğ‘‡ (cid:205)ğ‘‡ 1 (Dividing both side by ğœ‚ğ›¼eff ğ‘¡=0 ğ”¼[ğœ2 ğœ‚ ğ¶ğ‘¡ ğ›¼eff 2ğ›¼eff ğ‘‡ ğ‘‡ ğ‘‡ ğ‘‡ (Definitions of ğœ2 ğ¶) + + ] ğ‘¡=0 ğ‘=1 ğ‘‡ ğ‘‡) ğ‘‡ 1 ğ‘¡= ğ‘‡ 1 ğµ ğ¿(ğ‘) ğ‘¡ (ğ‘ğ‘™(ğœƒğ‘¡)2 + ğœ2 ğ‘ ) ğµ ğ‘= ğ¿(ğ‘) ğ‘¡ (ğ‘ğ‘™(ğœƒğ‘¡)2 + ğœ2 ğ‘ ) 1 ğ‘‡ ğ‘‡ 1 ğ‘¡=0 ğ”¼[ğ‘™(ğœƒğ‘¡)2] ğ‘™(ğœƒ0) ğ‘™ ğœ‚ğ›¼eff ğ‘‡ ğ‘‡ + ğœ2 ğ¶ ğ›¼eff ğ‘‡ + ğœ‚ 2ğ›¼eff ğ‘‡ ğ‘‡ ğ‘‡ 1 ğµ ğ‘¡=0 ğ‘=1 ğ¿(ğ‘) ğ‘¡ (ğ‘ğ‘™(ğœƒğ‘¡)2 + ğœ2 ğ‘ ). (25) Rearranging term gives 1 ğ‘‡ (cid:32) ğ‘‡ 1 ğ‘¡= ğ‘™(ğœƒğ‘¡)2 ğœ‚ 2ğ›¼eff ğ‘‡ ğµ ğ‘=1 ğ¿(ğ‘) ğ‘¡ ğ‘ğ‘™(ğœƒğ‘¡) (cid:33) ğ‘™(ğœƒ0) ğ‘™ ğœ‚ğ›¼eff ğ‘‡ ğ‘‡ + ğœ2 ğ¶ ğ›¼eff ğ‘‡ + ğœ‚ 2ğ›¼eff ğ‘‡ ğ‘‡ ğ‘‡ 1 ğµ ğ‘¡=0 ğ‘=1 ğ¿(ğ‘) ğ‘¡ ğœ2 ğ‘ . (26) Since 0 < ğœ‚ ğ›¼eff ğ‘‡ ğ¿max ğ‘¡ by definition, we have 1 ğ‘‡ (cid:32) ğ‘‡ 1 ğ‘¡=0 ğ‘™(ğœƒğ‘¡)2 ğœ‚ 2ğ›¼eff ğ‘‡ ğµ ğ‘= ğ¿(ğ‘) ğ‘¡ ğ‘ğ‘™(ğœƒğ‘¡)2 (cid:33) 1 ğ‘‡ (cid:32) ğ‘‡ 1 ğ‘¡=0 ğ‘™(ğœƒğ‘¡)2 1 2 ğ¿max ğ‘¡ 1 ğ‘‡ (cid:32) ğ‘‡ 1 ğ‘¡=0 ğ‘™(ğœƒğ‘¡)2 1 2 ğµ ğ‘=1 ğ‘ğ‘™(ğœƒğ‘¡)2 ğµ ğ‘=1 (cid:33) = (cid:33) ğ¿(ğ‘) ğ‘¡ ğ‘ğ‘™(ğœƒğ‘¡)2 1 2ğ‘‡ ğ‘‡ 1 ğ‘¡=0 ğ‘™(ğœƒğ‘¡)2. (27) 17 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers Therefore, applying equation 27 to equation 26 yields the following inequality as desired: 1 ğ‘‡ ğ‘‡ 1 ğ‘¡=0 ğ‘™(ğœƒğ‘¡)2 2(ğ‘™(ğœƒ0) ğ‘™) ğœ‚ğ›¼eff ğ‘‡ ğ‘‡ + 2ğœ2 ğ¶ ğ›¼eff ğ‘‡ + ğœ‚ ğ›¼eff ğ‘‡ ğ‘‡ ğ‘‡ 1 ğµ ğ‘¡=0 ğ‘=1 ğ¿(ğ‘) ğ‘¡ ğœ ğ‘ = 2(ğ‘™(ğœƒ0) ğ‘™) ğœ‚ğ›¼eff ğ‘‡ ğ‘‡ + 2ğœ2 ğ¶ ğ›¼eff ğ‘‡ + ğœ‚ğœ2 ğ¿ ğ›¼eff ğ‘‡ , (28) where ğœ2 ğ¿ = (cid:205)ğ‘‡ 1 ğ‘¡=0 1 ğ‘‡ ğ”¼[ğœ2 ğ¿ğ‘¡ ]. B. Experimental Details B.1. C4 Pre-Training Benchmark Setup We follow the setup introduced in Zhao et al. (2024). Specifically, we use fixed batch size of 512 and max sequence length of 256 with search grid of 1e-4, 5e-4, 1e-3, 5e-3, 1e-2 for the learning rate. For learning rate scheduling, we implement an initial warm-up for 10% of the total steps, followed by cosine annealing that decays the learning rate to 10% of the peak value. We run 10K, 20K, 60K, and 100K iterations for the 60M, 130M, 350M, and 1B models, respectively, and report the final evaluation perplexity from the run with the best learning rate. B.2. Nano MoE Pre-Training Benchmark Setup We follow the default setup presented in Wolfe (2024). Specifically, we use GPT2 (Radford et al., 2019)-style transformer with 124M number of parameters. Further for MoE configuration, the model uses 8 experts per MoE layer, with top-2 routing such that each token is dynamically dispatched to two experts. Further, the MoE layers are applied with stride of 2, with dense and MoE layers alternating. The model also includes number of training stabilization techniques, such as auxiliary load-balancing loss and switch transformer (Fedus et al., 2022)-style initialization. Refer to Wolfe (2024) for full details. Finally, the model is trained for 50K iterations on 8xA100 GPUs using the default configuration: batch size of 12, gradient accumulation of 40, sequence length of 1024 tokens, minimum learning rate of 5e-6, weight decay of 0.1, and grad clip norm of 1.0. B.3. Heterogeneous Quadratic Benchmark Setup We provide full details of the quadratic benchmark used in Orvieto and Gower (2025). The setup consists of two quadratic optimization problems in â„9, each defined by Hessian matrix with an identical eigenspectrum and 3 3 block-diagonal structure. Concretely, we consider losses of the form ğ¿(ğ‘¤) = 1 2 ğ‘¤ ğ»ğ‘¤, where the eigenvalues of ğ» are {1, 2, 3, 99, 100, 101, 4998, 4999, 5000}. While both problems share this eigenspectrums, they differ substantially in how the eigenvalues are arranged across blocks. eigenvalues In the homogeneous Hessian, {1, 2, 3}, {99, 100, 101}, {4998, 4999, 5000}. scale within each the heterogeneous block: vastly different magnitudes within each block: Hessian interleaves {1, 99, 4998}, {2, 100, 4999}, {3, 101, 5000}. This structural difference is intended to mimic qualitative distinctions between loss landscapes of autoregressive language models and those of more shallow architectures such as CNNs. grouped by In contracst, eigenvalues are of The Hessian ğ» is constructed by first forming diagonal matrices with the specified eigenvalues for each 3 3 block and then applying an independent random rotation to each block. Stochasticity is introduced by defining design matrix ğ‘‹ = ğ»1/2 and, at each iteration, subsampling random subset of rows of ğ‘‹ to form stochastic approximation of the loss and its gradients. On Surprising Effectiveness of Masking Updates in Adaptive Optimizers Table A1 Validation perplexity () for different masking components. Baseline Attn Only Attn + MLP All Eval Perplexity 22. 21.92 21.65 21.94 B.4. Heavy-Tailed Gradient Noise Benchmark Setup , (cid:19) (cid:19) We adopt the controlled linear-transformer benchmark introduced by Ahn et al. (2024). In this benchmark, each input sequence corresponds to distinct linear regression task. Specifically, an input takes the form ğ‘§ (cid:16) (cid:18)ğ’™1 ğ‘¦1 regression vector ğ’˜ (0, ğ¼ğ‘‘) is independently sampled for each sequence. The learning objective is to predict ğ‘¦ğ‘›+1 from the context {(ğ’™ğ‘–, ğ‘¦ğ‘–)}ğ‘› , and training minimizes the mean squared prediction error. Following Ahn et al. (2024), we use dimension ğ‘‘ = 5 and context length ğ‘› = 20. This setting has been extensively used to study in-context learning and optimization dynamics of transformers in simplified yet faithful regime (AkyÃ¼rek et al., 2022; Garg et al., 2022; Von Oswald et al., 2023). , where ğ’˜ğ’™ğ‘– = ğ‘¦ğ‘– for ğ‘– = 1, . . . , ğ‘› + 1 and the latent (cid:18)ğ’™ğ‘›+1 0 (cid:18)ğ’™2 ğ‘¦2 (cid:18)ğ’™ğ‘› ğ‘¦ğ‘› , , (cid:19) (cid:17) ğ‘–=1 (cid:19) , To study the impact of heavy-tailed stochastic gradients, we consider two covariate distributions. In the light-tailed setting, covariates are sampled as ğ’™ğ‘– (0, ğ¼ğ‘‘). In the heavy-tailed setting, we sample ğ’™ğ‘– uniformly from the unit sphere ğ•Šğ‘‘1 and scale each sample by an independent heavy-tailed random variable Î“0.1,10, where Î“ğ‘˜,ğœƒ denotes the Gamma distribution with shape ğ‘˜ and scale ğœƒ. This construction significantly amplifies tail behavior in the covariates, thereby inducing heavy-tailed gradient noise during optimization and enabling controlled evaluation of optimizer robustness under extreme stochastic fluctuations. C. Ablation Studies We conducted ablation studies on 130M-parameter Llama model trained on the C4 dataset with RMSProp+Magma, following the setup in 4.1. C.1. Masking Component We investigated the efficacy of Magma applied to specific transformer sub-modules (Attention vs. MLP) compared to global masking strategy. As shown in Table A1, applying masking exclusively to attention blocks improves model performance, reducing validation perplexity from baseline of 22.64 to 21.92. We observe synergistic effect when masking is applied to both attention and MLP simultaneously; this configuration achieves the lowest perplexity of 21.65, surpassing the most comprehensive setting (21.94). These findings indicate that targeted regularization of specific sub-modules yields superior performance compared to uniform masking strategies. C.2. Masking Granularity Table A2 summarizes the validation perplexity across four masking granularities (Element, Row, Column, and Block) using different sampling and damping configurations. Robustness Across Granularities We found that changing the masking granularity has very little impact on stability. For example, with Uniform Sampling, the score varies only slightlyfrom 21.73 19 On Surprising Effectiveness of Masking Updates in Adaptive Optimizers Table A2 Validation perplexity () for different masking granularities using Uniform vs. MomentumGradient Alignment-based sampling schemes (with and without damping). The baseline (RMSProp) excluding both sampling and damping achieves the perplexity of 22.64. Method Element Row Column Block Uniform Sampling Damping Uniform Sampling + Damping Momentum-Gradient Alignment-based Sampling Momentum-Gradient Alignment-based Sampling + Damping 21.73 21.76 21.97 21.95 21.58 21.62 21.77 21.78 21.63 21.60 21.78 21.81 21.91 21.92 21.61 21.65 21.75 21.78 21.61 21.65 Figure A1 Comparison of eval perplexity for different values of sampling ratio ğ‘ and damping temperature ğœ. Figure A2 Comparison of training perplexity on the C4 dataset over 20,000 iterations for Dense and Sparse momentum updates, with and without damping. (Element) to 21.81 (Block). This indicates that while fine-grained masking provides small edge, block masking is preferable for its efficiencyit saves significant memory for cosine similarities with minimal loss in accuracy. Effectiveness of Damping and Sampling The results in Table A2 highlight the synergistic effect of combining sampling schemes with damping. While damping alone yields perplexity improvement over the RMSProp baseline from 22.64 to around 21.92, it does not outperform standalone uniform sampling. However, integrating damping with sampling strategies consistently unlocks the lowest perplexity scores. Uniform sampling + damping achieves the minimum perplexity of 21.58 (Elementlevel masking), improving upon the standalone uniform sampling by approximately 0.15. On the other hand, momentum-gradient alignment-based sampling, which utilizes the masking probability, does not outperform uniform sampling. C.3. Sampling Ratio and Damping Temperature To identify the optimal sampling ratio ğ‘ and damping temperature ğœ, we experimented with ğ‘ {0.25, 0.5, 0.75} and ğœ {0.5, 1.0, 2.0, 4.0}. As Figure A1 demonstrates, ğ‘ = 0.5 outperforms other values across all temperatures. On the other hand, the results are not very sensitive to temperature, On Surprising Effectiveness of Masking Updates in Adaptive Optimizers Figure A3 Sensitivity analysis of learning rate on evaluation perplexity. Unlike Adam and CAdam, which exhibit narrow optimal windows, Adam+Magma maintains consistent performance, demonstrating stability across significantly wider hyperparameter range. so we proceed with ğœ = 2.0 in all our experiments. C.4. Sparse vs. Dense Momentum Update We consider four different settings with learning rate of 0.001. The results indicate critical disparity between dense and sparse update methods. As shown in Figure A2, the dense baselinesregardless of dampingconsistently maintain robust convergence and achieve the lowest perplexity. In contrast, the sparse momentum update without damping exhibits severe instability, characterized by sharp increase in perplexity that remains high throughout the 20,000 iterations. Although the introduction of damping stabilizes the model, its trajectory still underperforms dense update baselines. C.5. Sensitivity to Learning Rate As Figure A3 demonstrates, Adam+Magma exhibits superior robustness to learning rate variations compared to baseline optimizers. While C-Adam and Adam are sensitive to the learning ratewith perplexity spiking when the learning rate deviates from approximately 0.0010.003Adam+Magma maintains stability across broader spectrum. In particular, it remains effective at rates up to 0.05, region where other optimizers fail to converge. This suggests Adam+Magma reduces the need for precise hyperparameter tuning, offering greater reliability for resource-constrained experimental setups."
        }
    ],
    "affiliations": [
        "Google",
        "Northwestern University"
    ]
}