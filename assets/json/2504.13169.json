{
    "paper_title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling",
    "authors": [
        "Tsung-Han Wu",
        "Heekyung Lee",
        "Jiaxin Ge",
        "Joseph E. Gonzalez",
        "Trevor Darrell",
        "David M. Chan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io."
        },
        {
            "title": "Start",
            "content": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling Tsung-Han Wu1 Joseph E. Gonzalez1 Heekyung Lee1,2 Trevor Darrell1 1University of California, Berkeley Jiaxin Ge1 David M. Chan1 2POSTECH 5 2 0 2 7 1 ] . [ 1 9 6 1 3 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with novel inferencetime retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io. 1. Introduction Vision-Language Models (VLMs) have revolutionized visual understanding, achieving dramatic improvements in tasks like visual question-answering and image captioning, yet they still struggle with significant limitation: visual hallucination the tendency to describe objects that arent actually present in the scene. Such hallucinations pose significant risks when applying VLMs to safety-critical environments, ranging from autonomous driving scenarios and decision-making to assistive technologies for the visually impaired. Figure 1. REVERSE, our proposed training and decoding paradigm for hallucination reduction, enables single VLM to both verify if it has generated hallucination and then correct itself iteratively. When uncertainty is detected through the generation of (</UN>), the model backtracks and regenerates until confident phrase (</CN>) is found. To tackle these issues, researchers have generally pursued methods following one of two paradigms: generation adjustment or post-hoc verification. Generation adjustment methods focus on aligning textual outputs more closely with visual inputs by modifying the VLMs generation behavior, either in training-free way (modifying the logits at decoding time) [4, 1315, 38], or using training-based strategy requiring additional supervision or custom objective functions [20, 27, 28, 3436]. Unfortunately, these methods have no means of correcting erroneous tokens once they have been generated, and they do not leverage powerful retrospective tools such as chain-of-thought reasoning to reason about and evaluate the quality of their generation. In contrast to generation adjustment approaches, post-hoc verification methods [24, 28, 31, 33, 37] leverage large external models, such as GPT-4 [23], as verifiers to evaluate outputs after they have been generated. Post-hoc verifiers are accurate at predicting hallucination, but are complicated, requiring multiple models. Post-hoc verifiers often do not provide way for the model to correct the hallucination but instead adopt generic refusal strategies. In this paper, we introduce REVERSE (REtrospective VERification and SElf-correction), the first framework that unifies generation adjustment with online post-hoc verification in single VLM architecture (Figure 1). REVERSE consists of two key components. First, our method fine-tunes VLM on specially constructed training dataset consisting of synthetic hallucination phrases tagged by special token. Unlike prior VLMs instructed with only well-grounded data, our resulting hallucination-aware model is now able to tag likely phrase-level hallucinations during the generation process. Second, we introduce retrospective resampling, technique that allows the hallucination-aware VLM to serve as its own verifier. During the generation process, when the hallucinationaware VLM places sufficient probability on the special hallucination token we trigger backtracking self-correction process. Specifically, we backtrack to previous confident section and then apply rejection sampling and query-rewriting to correct the hallucination. As illustrated in Figure 1, both components in REVERSE combine to effectively mitigate visual hallucinations. We evaluate REVERSE against SOTA hallucination reduction baselines across wide range of benchmarks designed for hallucination evaluation. On hallucination-sensitive captioning and open-ended tasks, REVERSE achieves up to 12% reduction in CHAIR scores on CHAIR-MSCOCO [26] and AMBER [30] over the best existing methods. It also delivers over 10% and 28% performance improvement on MMHal [28] and HaloQuest [31], respectively. For tasks with discriminative (binary or multiple-choice) questions, REVERSE achieves performance on par with the best-performing models, demonstrating its robustness across different task formats. In summary, this paper both (i) introduces REVERSE, the first hallucination reduction method unifying the generation adjustment and post-hoc verification approaches, addressing hallucination in both the training and inference stages and (ii) provides new public training dataset and data curation pipeline for training-time hallucination mitigation consisting of 1.3M semi-synthetic samples. Together, these contributions allow REVERSE to achieve up to 12% improvement on the CHAIR-MSCOCO benchmark, and 28% improvement on the HaloQuest benchmark over existing SOTA methods for hallucination reduction under the same setting, especially in questions with false premise and insufficient context. performance, VLMs are prone to hallucinations: generating incorrect or nonexistent visual information [17]. To address this issue, several hallucination-specific benchmarks, such as CHAIR-MSCOCO [26], AMBER [30], MMHal [28], and POPE [17], and HaloQuest [31] have been introduced. These benchmarks evaluate VLM hallucinations across both discriminative and generative tasks, with growing trend of using VLMs for automatic visual hallucination detection. to Beyond detection, several recent methods attempt mitigate hallucinations by adjusting VLMs generation process. Training-free approaches primarily focus on improving decoding strategies [4, 1315, 38]. For instance, VCD [15] employs contrastive decoding, OPERA [13] introduces penalty term during beam search, and DoLA [9] enhances decoding by contrasting different model layers. Training-based methods, on the other hand, aim to reduce hallucinations through improved training objectives and additional data. Some approaches leverage data augmentation [6], while others refine training via reinforcement learning from human feedback (RLHF) [28, 34, 36]. Additional methods fine-tune VLMs using custom loss functions, such as EOS token-based penalties for lengthy descriptions [35], contrastive learning from paired correct-hallucination data [14], and visual instruction tuning with improved datasets [20]. However, these approaches merely adjust the generators behavior rather than fundamentally information is eliminating hallucinationsonce incorrect produced, there is no built-in mechanism for correction. The closest prior work to ours includes Woodpecker [33] and LURE [37], which use external models to verify and rewrite initial outputs from VLM. While effective to some extent, these methods rely on complex, multi-stage pipelines with external dependencies. Moreover, they suffer from error propagation, as single-round rewriting step is often insufficient to fully recover from low-quality initial outputs. In contrast, our method is the first unified framework where the VLM itself serves as both the generator and verifier, enabling self-correction in streamlined and integrated manner. Compared to prior generation-adjustment methods, our self-verification pipeline allows VLMs to retrospect and iteratively self-correct after content has been generated. Compared to existing post-hoc refinement approaches, our method eliminates the need for external models or complex multi-stage pipelines, achieving better results as the verifier can instantly and iteratively correct the generators outputs. 2. Background & Related Work Following the success of Large Language Models (LLMs) [1, 5, 7, 19, 29], Vision-Language Models (VLMs) have shown success across various multimodal tasks, such as image captioning, visual question answering, visual reasoning, and image segmentation [2, 3, 16, 2123]. Despite their impressive 3. REVERSE: Retrospective Verification and Self-Correction REVERSE (REtrospective VERification and SElf-correction) is hallucination reduction paradigm for Vision-Language Models (VLMs) that unifies generation adjustment and post-hoc verification methods. REVERSE allows VLMs to be hallucination2 Figure 2. Our 1.3M semi-synthetic instruction-tuning dataset for hallucination-aware VLM training. We constructed the dataset by augmenting negative phrases from the original LLaVA-v1.5-665k [22] dataset. Our negative phrases span diverse range, including attributes, objects, world entities, and novel scenes. Positive noun phrases are marked with <SPAN> and </CN>, while negative samples are enclosed with <SPAN> and </UN>, terminating immediately. Further details about our dataset creation and statistics can be found in subsection 3.1 and Appendix B. aware by explicitly modeling and monitoring the likelihood that each generated phrase is well-grounded. During training, the model is explicitly trained to classify each groundable phrase as either confident or unconfident and during inference, the model generates responses while continuously verifying the confidence of each phrase using the likelihood of the unconfident predictor. If phrase is sufficiently ungrounded, the model then performs retrospective adjustment to refine the segment, enabling self-correction on the fly. Key to the first goal of classifying each phrase as confident or unconfident is training the model to understand if phrase is well-grounded. While VLMs and LLMs inherently provide implicit confidence scores through token probabilities, these scores are often mis-calibrated and do not consistently correlate with output correctness, making them unreliable for verification [8, 32]. Furthermore, even when accurate, these probabilities offer no indication of where to backtrack for phrase re-generation and self-correction. To overcome these limitations, we introduce three tokens to the VLM vocabulary that can be used to explicitly mark key phrases and represent the models confidence level: <SPAN>: Marks the beginning of key or object phrases. </CN>: Marks the end of confident, grounded phrases. </UN>: Marks the end of unconfident, hallucinated phrases. These tokens, when placed before/after objects or phrases in the scene can serve as ad-hoc classifiers of the confidence of the model. I.e. if model generates </UN> token after phrase, that phrase can be considered to be ungrounded, while if it generates </CN>, that phrase is likely grounded in the image. Annotating our data with such tokens, as is shown in Figure 2, will allow us to train the VLM itself to perform post-hoc verification instead of relying on an external model. 3.1. Data Curation Towards models that are capable of automatically tagging phrases as confident or unconfident, we constructed 1.3M VLM instruction-tuning dataset containing total of 6,810,679 question-answer pairs (turns), of which 3,855,303 (56.6%) are correct answers and 2,955,376 (43.4%) are hallucinated answers. In each response, all positive phrases are enclosed with <SPAN> and </CN> while the negative phrases are enclosed with </UN>. The phrases in this dataset are automatically generated by annotating existing training data for VLM with these tokens. Our dataset is initially sourced from the LLaVA-v1.5-665k instruction tuning data [22], which contains only positive or well-grounded samples. To introduce negative or ungrounded samples, we designed multi-step pipeline that generates incorrect answers leveraging rule-based algorithms and gpt-4o-mini-0718 [23]. Specifically, we first classify the answer types, in which we can augment most of the answers with rule-based methods easily, such as binary Yes/No questions or counting questions. For the remaining general answers, mostly long answers or descriptions, we apply AI inference for high-quality and diverse data augmentation. For all negative samples, we enforce the constraint that the sentence immediately terminates upon reaching </UN>. This design not only prevents VLMs from continuing to generate ungrounded descriptions after detecting hallucinated content but also helps maintain training data quality, as any remaining context may become meaningless once the preceding information has been altered. To support retrospective query re-writing, we further inject Figure 3. Illustration of our retrospective resampling process. During inference, we monitor the hallucination-aware VLMs generation. When the likelihood of the </UN> token surpasses predefined threshold, we trigger backtracking to the most recent confident checkpoint (</CN>) and apply corrections using rejection sampling and query rewriting. This self-correction mechanism can be applied iteratively throughout the generation process. negative keywords as hints (further discussed in subsection 3.3). corresponding to the answer or one of the three special tokens. Our dataset has twice the number of samples compared to the LLaVA-v1.5-665k instruction tuning dataset while maintaining similar overall composition. It preserves the same average question-answer pairs per sample and comparable question type distribution. More details about the dataset statistics and the whole generation pipeline are provided in Appendix B. 3.2. Hallucination-aware Training To train REVERSE to recognize and respond to the new tokens, we introduce modified cross-entropy next-token prediction loss that prevents hallucination while modeling confidence levels. Our training objectives are threefold. First, we aim to enable conventional instruction tuning to allow VLMs to perform nexttoken prediction to generate accurate answers. Second, we wish to reduce the likelihood of generating the hallucinated tokens that we have introduced in the new dataset. Third, we want to teach the model to generate <SPAN> at the start of key phrases, and </CN> or </UN> as explicit confidence estimators around those phrases. We achieve all of these goals by assigning weight to each token during training; positive weights are assigned to tokens outside the <SPAN>... </UN> bounds, encouraging standard next-token prediction while zero-weights are assigned to tokens within <SPAN> and </UN> (i.e., masking out the targets) to avoid impacting the likelihood when training on ungrounded data (and reinforcing language priors). Formally, let θ be our model and be the labeled VQA dataset, where each sample consists of an input sequence = {x1,x2,...,xm} and an output sequence = {y1,y2,...,yn}. Here, includes both encoded image features and question (query) tokens, while each yi in can be either text token The model θ predicts the next token probability as follows: (yi x1,x2,...,xm,y1,y2,...,yi1;θ) (1) We then define the modified negative log-likelihood loss for given sample as: L(S)= (cid:88) yiY 1Hall(i)logP (yi X,y1,...,yi1;θ) (2) where 1Hall(i) {0,1} is an indicator variable taking the value 1 for all tokens except those enclosed by <SPAN> and </UN>. For tokens within these markers, 1Hall(i) = 0. During training, we optimize our model θ using Equation 2. more detailed discussion of the training procedure is provided in section 4 and Appendix E. 3.3. Retrospective Resampling During inference, the model follows standard next-token prediction but continuously monitors the likelihood of </UN>, triggering retrospective resampling when that likelihood exceeds pre-defined threshold (see Figure 3). Specifically, whenever </CN> or </UN> is generated (the end of span), we compute the probability of </UN>, denoted as (</UN>), across previous tokens. If (</UN>) surpasses predefined threshold τ, the model initiates self-correction process via backtracking and retrospective resampling. Otherwise, generation proceeds normally, with <SPAN>, </CN> and </UN> tokens removed before presenting the final output. Backtracking Strategies critical challenge in selfcorrection is determining both (1) where to backtrack and (2) 4 how to regenerate content. To determine where to backtrack to, our approach follows hierarchical fallback strategy: 1. The model first backtracks to the most recent </CN>, which attempts to adjust only the local information to reduce the likelihood of hallucination. 2. If the issue persists after local correction attempts, it is likely that the hallucination issue stems from earlier information in the sequence. Thus, we revert further, backtracking to the last sentence boundary (indicated by the last punctuation token). 3. If self-correction continues to fail after total attempts, the output is finalized and returned to the user, along with an indication that hallucination was detected, but could not be corrected. Since (</UN>) is typically low, explicitly waiting for </UN> to appear in the output is impractical. Instead, setting the confidence threshold τ allows proactive identification of hallucinated phrases before they fully form, as well as allows for cautionary reduction of hallucination (even when the likelihood is low). The effect of τ selection is analyzed in subsection 4.2. After the backtracking has occurred, we leverage rejection sampling and query rewriting for self-correction. Rejection Sampling Rejection sampling refines uncertain phrases by resampling multiple times at an increased temperature, seeking an alternative where (</UN>) remains below τ. The process continues until reaching confident phrase (marked by </CN>) or exhausting the maximum resampling attempts. In this work, we make no attempts to ban the generation of the same tokens during the re-sampling process. This procedure, while potentially more efficient, often leads to issues where innocuous tokens such as or the are banned, leading to disfluencies in the final generated text. Instead, we rely on the increased temperature to lead to new candidates over several repeated generations. While rejection sampling is effective for resolving localized hallucinations, its success depends on the ability of the model to generate valid alternatives within reasonable number of attempts. In cases where repeated resampling fails to produce an acceptable phrase, the model may need to fall back on broader correction strategies, such as query rewriting, to address deeper inconsistencies in the generated content. Query Rewriting In addition to rejection sampling, we found that query rewriting can provide stronger signals for VLMs to do self-correction. Query rewriting dynamically modifies the prompt to encourage better factual grounding. Specifically, the input prompt is augmented with clarification hint: <sys-prompt> <image> <question> (Hint: potential incorrect phrases <placeholder>)"
        },
        {
            "title": "This signals the model to reconsider flagged segments and",
            "content": "In addition to rejection generate more reliable response. sampling with increased temperature, which iteratively refines outputs by resampling under varied decoding conditions, query rewriting can directly influence the models contextual understanding by reformulating its input conditions. Since our training data includes hallucination-corrected phrase pairs, we randomly inject 20% of this query-rewriting prompt into the instructiontuning process. This improves the models ability to recognize the hint, making retrospective resampling more effective. The full decoding algorithm, including hyperparameter choices such as correction attempt limits, backtracking depth, and temperature scaling, is detailed in the following section and Appendix E. Ablation studies analyzing their impact are presented in subsection 4.2. 4. Experiments Implementation Details We applied our method, REVERSE, on two VLM backbones: LLaVA-v1.5-7B [22] and LLaVA-More (LLaVA with Llama-v3.1) [2, 10]. For training, REVERSE is instruction fine-tuned using LoRA (rank=128, α = 256) with the modified cross-entropy loss (described in subsection 3.2) on our 1.3M dataset for one epoch. More details on additional training parameters are provided in Appendix E. During inference, we apply retrospective resampling with different threshold values: τ = 0.003 for generative tasks and τ =0.5 for discriminative tasks (e.g., binary and multiple-choice questions) to optimize performance. Further discussions on this can be found in subsection 4.2. For the correction mechanism, we allow up to = 50 total correction attempts, with local correction attempts of = 10. Additionally, we implement rejection sampling with base temperature of T0, gradually increasing it with step size of =0.1, capped at maximum temperature of T0+0.5: =min(T +T, T0+0.5). Evaluation Protocol To evaluate our approach, we compare REVERSE with various hallucination mitigation methods, including training-free or training-based generative adjustment techniques [4, 9, 13, 15, 27, 35, 36, 38], and post-hoc verification with refinement [33]. All methods are evaluated on both VLM backbones under consistent settings, where we fix the decoding temperature at 0 and use only the base prompts provided by each dataset to ensure fair comparisons. Since REVERSE does stochastic sampling at inference time, we report the mean performance over 100 bootstrapped runs for robustness. The exact numbers with 95% confidence intervals are provided in Appendix D. Our evaluation spans multiple common VQA tasks designed to assess visual hallucination, categorized into image captioning, open-ended question answering, and discriminative tasks. For image captioning, we use CHAIR-MSCOCO [26, 35] and the generative subset of AMBER [30] (denoted as AMBER-G). CHAIR-MSCOCO evaluates object hallucination using the 5 Table 1. Performance comparison of various hallucination reduction methods across various image captioning benchmarks, which are commonly used to evaluate visual hallucinations in generative tasks for VLMs. This includes the CHAIR-MSCOCO benchmarks from [35] and the generative subset of AMBER. and mean that we reproduced the results of these methods on CHAIR-MSCOCO and AMBER-G respectively. Otherwise, numbers came from [35] and [27]. Base VLM Method Type Method CHAIR-MSCOCO AMBER-G CHAIRi() CHAIRs() CHAIR () Cover () Hall () Cog () LLaVA-v1.5 7B [22] None Gen-Adjust w/o Train Gen-Adjust w/ Train VCD [15] OPERA [13] DoLA [9] AGLA [4] MEMVR [38] EOS [35] HALVA [27] HA-DPO [36] Post-hoc Refine Woodpecker [33] Combination LLaVA-MORE 8B [10] REVERSE(τ = 0.003) REVERSE(τ = 0.0003) None DoLA [9] Woodpecker [33] REVERSE(τ = 0.003) REVERSE(τ = 0.0003) CHAIR score, which measures the degree of misalignment between objects mentioned in model-generated caption and objects actually present in the image. It is defined as 1 minus the intersection over union (IoU) between the sets of mentioned and ground-truth objects. We report both CHAIRi, which aggregates the CHAIR score across all object instances, and CHAIRs, which quantifies the proportion of images where at least one hallucination occurs. For AMBER-G, we report four key metrics: CHAIR, Coverage (Cover), Hallucination (Hall), and Cognition (Cog). CHAIR is the same as CHAIRi above, and Coverage measures how well the caption mentions all objects in the image, similar to recall. The definitions of the remaining two metrics and further details on these datasets are provided in Appendix C. For open-ended question answering, we evaluate on MMHalBench [28] and HaloQuest [31], which generally test VLMs on false-premise questions, questions with insufficient visual evidence, and visually complex queries. Following standard evaluation protocols, we assess MMHal-Bench responses using gpt-4-0314 and HaloQuest with Gemini-1.5-Pro, as the original paper used Gemini-1.0-Pro, which is no longer available. These benchmarks require models to generate free-form text answers, testing their ability to comprehend and reason about visual content in an open-ended manner. For discriminative tasks, we utilize the discriminative subset of AMBER (AMBER-D) [30], POPE [17], and MME-Hal [11], all of which are binary (Yes/No) questions to assess models understanding of object existence, attributes, spatial relationships, and locations. 15.4 14.9 14.6 14.1 14.1 13.0 12.3 11.7 11.0 14.8 10.3 6.1 14.4 13.8 14.3 12.2 8. 50.0 48.6 47.8 51.6 43.0 46.6 40.2 41.4 38.2 45.8 37.0 13.6 52.0 51.8 51.0 42.4 25. 7.8 - 7.3 7.6 - - 5.1 6.6 6.7 6.9 6.0 4.0 7.8 7.9 7.4 6.5 5. 51.0 - 49.6 51.6 - - 49.1 53.0 49.8 48.9 52.2 26.9 53.1 53.1 50.7 54.8 38. 36.4 - 32.0 36.0 - - 22.7 32.2 30.9 30.4 30.4 10.2 36.6 38.4 36.7 35.5 20. 4.2 - 3.5 4.0 - - 2.0 3.4 3.3 3.6 3.0 0.9 3.9 4.1 3.7 3.9 2. Table 2. Performance comparison on MMHal-Bench [28] with different methods. Results re-implemented by us are marked with . We observe that lower thresholds (τ = 0.0003) enable our VLM to handle false-premise questionscommon in MMHalleading to improved scores without the need for specialized training."
        },
        {
            "title": "Method",
            "content": "Score () Hall. Rate () LLaVA-v1.0 7B LLaVA-RLHF [28] LLaVA-v1.5 7B LLaVA-MORE 8B None [22] HACL [14] HA-DPO [36] EOS [35] HALVA [27] DoLA [9] Woodpecker [33] REVERSE(τ = 0.003) REVERSE(τ = 0.0003) None DoLA Woodpecker REVERSE(τ = 0.003) REVERSE(τ = 0.0003) 2.05 2.11 2.13 1.97 2.03 2.25 2.33 2.19 2.56 3.28 2.50 2.54 2.28 2.28 2.93 0.68 0.54 0.50 0.60 0.59 0.54 0.56 0.58 0.47 0. 0.53 0.51 0.58 0.54 0.40 4.1. Experimental Results Image Captioning Tasks Table 1 presents results on image captioning tasks. With the default parameters (τ = 0.003), our method reduces the CHAIRi value by up to 12% on CHAIRMSCOCO and AMBER-G compared to the best existing methods. In comparisons using LLaVA-v1.5-7B, training-free generative adjustments generally perform worse than trainingbased methods. The post-hoc refinement method, Woodpecker [33], employs multi-stage process for verification and correction; however, its one-time correction approach may suffer from error propagation, limiting its effectiveness. Additionally, the EOS method [35] achieves significantly better results on 6 Figure 4. Qualitative Examples of different Methods. When generating captions for an image, LLaVA, OPERA, and Woodpecker tend to hallucinate non-existing objects. REVERSE generates correct captions of similar length. Additional qualitative results are provided in Appendix D. Table 3. Performance comparison across different models on HaloQuest [31]. FP, VC, and IC stand for false premise, visually challenging, and insufficient context, three subsets in the benchmark dataset. With lower threshold (τ = 0.0003), REVERSE shows huge gain on unanswerable questions (e.g., those with insufficient context) even without being specifically trained on these cases."
        },
        {
            "title": "Method",
            "content": "Avg. Acc. () FP Acc. VC Acc. IC Acc. LLaVA-v1.5 LLaVA-MORE GPT-4o Gemini 1.5 Pro None DoLA HALVA REVERSE(τ = 0.003) REVERSE(τ = 0.0003) None DoLA REVERSE(τ = 0.003) REVERSE(τ = 0.0003) 22.6 22.9 23.9 30.7 32.3 22.4 22.8 26.7 36.7 63.2 77.9 17.1 17.2 21.1 31.8 29.4 15.8 15.5 30.0 39.5 65.2 83. 39.5 40.1 37.4 31.5 18.7 43.4 45.1 31.3 30.9 55.2 56.3 10.7 11.6 10.7 26.9 58.8 7.4 7.4 11.7 38.1 68.7 92. AMBER-G, likely because it encourages the model to produce more concise captions, leading to less informative outputs and reduced coverage. Unlike prior methods, REVERSE is flexible across thresholds, allowing for tradeoff between coverage and hallucination rate, allowing us to surpass GPT-4V on the CHAIR scores with the tradeoff of lower coverage as discussed in subsection 4.2. Figure 4 presents qualitative results using four different methods. LLaVA-v1.5-7B, OPERA, and Woodpecker hallucinate non-existing objects, while REVERSE can generate the correct caption without reducing caption length too much. Open-ended Question Answering We use MMHal-Bench and HaloQuest as evaluation datasets for open-ended question answering. Both datasets include questions with false premises or insufficient context, requiring the model to either refuse or correct the query. For these questions, we observed that REVERSE often produces empty responses and we interpret this behavior as the model identifying the query as In all such cases we apply query rewriting unanswerable. Table 4. Performance comparison across multiple discriminative visual hallucination benchmarks including the discriminative subset of AMBER (F1), POPE (F1), and the hallucination subset of MME (Score)."
        },
        {
            "title": "Method",
            "content": "AMBER-D POPE MME-Hall None VCD [15] EOS [35] OPERA [13] DoLA [9] HA-DPO [36] MEMVR [38] AGLA [4] HALVA [27] Woodpecker [33] REVERSE(τ = 0.5) None DoLA REVERSE(τ = 0.5) [9] 74.7 - 75.6 74.8 74.5 78.1 - - 83.4 67.0 74.2 71.6 72.0 69.3 85.9 84.5 86.0 85.5 85.7 86.9 85.9 86.0 84.8 - 85.9 85.1 85.2 84.4 648.3 604.7 606.7 592.3 656.7 618.3 648.3 640.0 665.0 366.7 601.6 678.3 683.3 657. LLaVA-v1.5 LLaVA-MORE with the prompt: For this question, please point out the false premises or note what information is missing, rather than answering it directly. The complete mechanism for handling unanswerable questions is given in Appendix E. As shown in Table 2 and Table 3, REVERSE improves accuracy by up to 10% on MMHal-Bench and 28% on HaloQuest compared with the SOTA models using default hyperparameters (τ =0.003). The breakdown shows that most improvements come from better handling of false-premise and insufficient-context questions. However, performance on visually challenging questions decreases, as the model takes more cautious stance and avoids speculative answers. Using lower threshold (τ =0.0003) further increases this conservativeness, leading to more gains on unanswerable questions without requiring task-specific fine-tuning on both tasks. Discriminative Questions Table 4 presents the results for three commonly used discriminative tasks. Our method maintains comparable results to existing methods but does not show significant improvement. This outcome may be due to Table 5. Ablation studies demonstrate that hallucination-aware training enhances VLM performance across all metrics, achieving higher coverage and lower hallucination scores. Moreover, our retrospective resampling further reduces hallucination, with each component contributing to overall improvement. Components CHAIR () Cover () Hall () Cog () LLaVA-v1.5-7B + Hall-aware Training + Rejection Sampling (τ = 0.003) + Query Rewriting (τ = 0.003) 7.8 7.2 6.0 6.0 51.0 53.2 51.0 52.2 36.4 36.3 30.5 30.4 4.2 3.4 3.0 3.0 Figure 5. Ablation studies on the optimal threshold τ for generative tasks. This plot illustrates the trade-off between CHAIR () and Coverage () across different threshold values. REVERSE establishes clear performance frontier, surpassing base models (LLaVA-v1.5, LLaVA-MORE) and the existing post-hoc refinement method (Woodpecker [33]). Notably, with appropriate threshold tuning, our approach can even achieve lower hallucination rates than GPT-4V. the nature of these tasks, which often require straightforward yes or no answers, rendering retrospective resampling or further reasoning less beneficial. 4.2. Discussions Trade-offs Between Expressiveness and De-Hallucination As discussed in subsection 3.3, key component of our retrospective resampling method is the predefined threshold τ. When the predicted probability of </UN> exceeds τ, the model triggers backtracking and self-correction. Figure 5 presents an analysis of the effect of τ on two VLMs. The 2D plot illustrates the performance trade-off between CHAIR (hallucination metric) and coverage across different threshold values. Based on our experiments, we selected τ = 0.003 as global threshold for image captioning and other generative tasks, as it represents the peak of the performance frontier. For discriminative tasks (e.g., binary and multiple-choice questions), where retrospective resampling has limited impact, we choose τ =0.5 as the default, and increasing the threshold beyond 0.5 does not significantly affect the results. Compared with prior hallucination reduction methods, such as the post-hoc correction techniques [33], REVERSE enables us to dynamically adjust the balance between expressiveness and trustworthiness. Notably, with relatively high threshold of τ =0.01, our method already surpasses the base VLMs (LLaVAv1.5 and LLaVA-MORE) in both hallucination reduction and content coverage. Moreover, with lower threshold (τ =0.0001), our model can even outperform GPT-4V in hallucination control. Ablation Studies We conduct ablation studies to evaluate the contributions of different components of our method, as shown in Table 5. Comparing the first and second rows, we observe that hallucination-aware training alone already improves performance across all metrics, outperforming existing VLMs. Figure 6. Further analyses on the effectiveness of temperature for answer quality. Ideally, increasing the temperature in image description tasks should encourage the model to capture more objects and details while also increasing its tolerance for hallucinated, incorrect content. REVERSE is the only approach that achieves the best hallucination-expressiveness tradeoff, maintaining higher coverage as temperature increases. Moreover, it consistently outperforms all existing methods on the hallucination metric, CHAIR, across all settings. We hypothesize that this improvement arises from the models ability to contrast positive and negative phrases, effectively learning to distinguish between </CN> and </UN> during traininga mechanism that may be similar to DPO [25]. This finding suggests potential research direction for future work. Interestingly, even naive rejection sampling strategy reduces CHAIR hallucination scores by 1.2. When combined with query rewriting, the coverage improves by further 1.2 points, indicating that rewriting helps the model explore alternative phrasing and correct itself more effectively. Impact of Temperature on Hallucination and Coverage We also analyze how temperature settings affect hallucination In this rates and object coverage in generated outputs. experiment, we use LLaVA-v1.5-7B as the backbone and increase the number of local correction attempts (K) while keeping all other parameters in REVERSE unchanged. As shown in Figure 6, REVERSE is robust to increasing temperature. For tasks such as image captioning, higher temperature is often desirable to enhance diversity in generated descriptions. However, existing methods not only suffer from increased hallucinations at higher temperatures but also exhibit decline in object coverage. In contrast, REVERSE balances expressiveness and reliabilityslightly reducing hallucination while improving object coverage as temperature increases. 8 5. Conclusion In this paper, we introduced REVERSE, framework that reduces hallucinations in Vision-Language Models by combining hallucination-aware training with retrospective resampling. REVERSE achieves up to 12% improvement on image CHAIR-MSCOCO and 28% on HaloQuest over existing SOTA methods. While preliminary, REVERSE highlights the potential of self-correction in multimodal models. Future work may explore integrating structured verification and causal reasoning to further reduce hallucinations. As multimodal AI progresses, we see self-verification and retrospective techniques as promising direction for building more trustworthy systems."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Shalini Ghosh and Konpat Preechakul for their invaluable feedback during the discussions. Authors, as part of their affiliation with UC Berkeley, were supported in part by the National Science Foundation, US Department of Defense, and/or the Berkeley Artificial Intelligence Research (BAIR) industrial alliance program, as well as gifts from Amazon. Sky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Cisco, Google, IBM, Intel, Intesa Sanpaolo, Lambda, Microsoft, NVIDIA, Samsung SDS, SAP, and VMware. This research was also developed with funding from the Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8650-23-C-7316. The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of any sponsor, the Department of Defense, or the U.S. Government."
        },
        {
            "title": "References",
            "content": "[1] The claude 3 model family: Opus, sonnet, haiku. 2 [2] Meta AI. Introducing meta llama 3: The most capable openly available llm to date, 2024. 2, 5 [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2 [4] Wenbin An, Feng Tian, Sicong Leng, Jiahao Nie, Haonan Lin, QianYing Wang, Guang Dai, Ping Chen, and Shijian Lu. Agla: Mitigating object hallucinations in large vision-language models with assembly of global and local attention. arXiv preprint arXiv:2406.12718, 2024. 1, 2, 5, 6, 7 [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 2 [6] Ali Furkan Biten, Lluís Gómez, and Dimosthenis Karatzas. Let there be clock on the beach: Reducing object hallucination in image captioning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 13811390, 2022. 2 [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 2 [8] Yu-Neng Chuang, Helen Zhou, Prathusha Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, and Xia Hu. LearnarXiv preprint ing to route llms with confidence tokens. arXiv:2410.13284, 2025. [9] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations, 2024. 2, 5, 6, 7 [10] Federico Cocchi, Nicholas Moratelli, Davide Caffagni, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. LLaVAMORE: Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning, 2025. 5, 6, 11, 14 [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 6, 12 [12] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Industrial-strength natural language Adriane Boyd. processing in python, 2020. 11 spacy: [13] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1341813427, 2024. 1, 2, 5, 6, 7 [14] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. Hallucination augmented contrastive learning In Proceedings of for multimodal large language model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2703627046, 2024. 2, [15] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive In Proceedings of the IEEE/CVF Conference on decoding. Computer Vision and Pattern Recognition, pages 1387213882, 2024. 1, 2, 5, 6, 7 [16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 2 [17] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023. 2, 6, 11 [18] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 12 9 European Conference on Computer Vision, pages 288304. Springer, 2024. 1, 2, 6, 7, 13 [32] Spencer Whitehead, Suzanne Petryk, Vedaad Shakib, Joseph Gonzalez, Trevor Darrell, Anna Rohrbach, and Marcus Rohrbach. Reliable visual question answering: Abstain rather than answer incorrectly. In European Conference on Computer Vision, pages 148166. Springer, 2022. [33] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. Science China Information Sciences, 67(12): 220105, 2024. 1, 2, 5, 6, 7, 8 [34] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1380713816, 2024. 1, 2 [35] Zihao Yue, Liang Zhang, and Qin Jin. Less is more: Mitigating multimodal hallucination from an EOS decision perspective. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1176611781, Bangkok, Thailand, 2024. Association for Computational Linguistics. 2, 5, 6, 7, 12 [36] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023. 1, 2, 5, 6, 7 [37] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In The Twelfth International Conference on Learning Representations, 2024. 1, 2 [38] Xin Zou, Yizhou Wang, Yibo Yan, Sirui Huang, Kening Zheng, Junkai Chen, Chang Tang, and Xuming Hu. Look twice before you answer: Memory-space visual retracing for hallucination mitigation in multimodal large language models. arXiv preprint arXiv:2410.03577, 2024. 1, 2, 5, 6, 7 [19] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [20] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023. 1, 2 [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2, 11 [22] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2629626306, 2024. 3, 5, 6, 14 [23] OpenAI. Hello gpt-4o, 2024. 1, 2, 3 [24] Suzanne Petryk, David Chan, Anish Kachinthaya, Haodi Zou, John Canny, Joseph Gonzalez, and Trevor Darrell. ALOHa: new measure for hallucination in captioning models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 342357, Mexico City, Mexico, 2024. Association for Computational Linguistics. 1 [25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36: 5372853741, 2023. 8 [26] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 40354045, Brussels, Belgium, 2018. Association for Computational Linguistics. 2, 5, 11, [27] Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan Arik, and Tomas Pfister. Data-augmented phrase-level alignment for mitigating object hallucination. In The Thirteenth International Conference on Learning Representations, 2025. 1, 5, 6, 7 [28] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented RLHF. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1308813110, Bangkok, Thailand, 2024. Association for Computational Linguistics. 1, 2, 6, 13 [29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 2 [30] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023. 2, 5, 6, 11 [31] Zhecan Wang, Garrett Bingham, Adams Wei Yu, Quoc Le, Thang Luong, and Golnaz Ghiasi. Haloquest: visual hallucination dataset for advancing multimodal reasoning. In"
        },
        {
            "title": "Appendix",
            "content": "The appendix consists of the following further discussion: Appendix provides information on the code release, including links to the codebases and datasets used in the project. Appendix describes the dataset that we constructed containing (</UN>) and (</CN>) tokens. Appendix describes the evaluation datasets and metrics. Appendix provides more qualitative and quantitative results. Appendix details the implementation for REVERSE. A. Code and Model Release The project website is available at: https://reverse-vlm.github.io. The code for REVERSE is released under the MIT license at https://github.com/tsunghan-wu/reverse_vlm, and builds upon the Apache 2.0-licensed codebases of LLaVA [21] and LLaVA-MORE [10]. We also release model checkpoints of REVERSE (based on LLaVA-v1.5 and LLaVA-MORE), along with 1.3M-sample semi-synthetic dataset, at Hugging Face. Both are available under the MIT license. The dataset contains elements adapted from LLaVA [21], which is licensed under the Creative Commons Attribution 4.0 International License. Use of the dataset should also comply with OpenAIs usage policy. B. Dataset Details In the construction of the dataset, the automatically annotated phrases include all noun phrases (multilingual support) and set of predefined keywords relevant to the task, such as Yes or No. We maintain skip list to prevent the model from marking common but uninformative phrases, like in the image. Noun phrases with their corresponding prefix prepositions are automatically extracted using Part-of-Speech (POS) tagging tools [12], and the full data curation pipeline is in Figure B.7. The skip list is in Figure B.9. In this work, we only annotate model response phrases rather than the questions. To further enhance dataset quality and balance, we validate that all tokens are tagged properly and then redistribute positive and negative answers. The final image distribution of our data composition is the same as the original LLaVA-v1.5-665k dataset as shown in Figure B.8. C. Evaluation Datasets & Metrics To evaluate REVERSE, we use the following datasets: POPE: The Polling-based Object Probing Evaluation (POPE) [17] is benchmark designed to assess object hallucination Unlike traditional in vision-language models (VLMs). 11 instruction-based evaluations, POPE employs polling-based query method, prompting LVLMs with simple yes-or-no questions about the presence of specific objects in images. This approach converts the evaluation into binary classification task, allowing for more stable and flexible assessment of object hallucination. Performance on POPE is measured in F1 score following LLaVAs standard [21]. AMBER: AMBER (An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation) [30] is comprehensive evaluation framework designed to assess hallucination phenomena in Multi-modal Large Language Models (MLLMs). Unlike previous benchmarks that often rely on human or advanced LLM evaluations, AMBER offers an automated, low-cost approach to evaluate both generative and discriminative tasks. It addresses three key dimensions of hallucination: existence, attribute, and relation. In this work, we refer to the generative subset as AMBER-G and the discriminative subset as AMBER-D. The AMBER-D subset comprises binary Yes/No questions and is evaluated using the F1 score, consistent with the POPE benchmark. Evaluation on AMBER-G is more complex and involves multiple metrics, as described below. CHAIR [26] measures the percentage of hallucinated objects in scene, and is defined as: CHAIR(R)=1 len(R obj Aobj) obj) len(R . (C.3) 2 , ..., objA 1 , objA where Aobj={objA objects, and Robj={objR 1 , objR from the captions using NLTK. } is an annotated list of } are nouns extracted 2 , ..., objR Cover measures the object coverage of responses, namely, the proportion of objects mentioned in the response obj relative to the objects identified in the Aobj, and is defined by: Cover(R)= len(R obj Aobj) len(Aobj) . (C.4) Hal is more general metric, measuring the portion of responses containing hallucinations (similar to CHAIRs). It is defined as: Hal(R)= if CHAIR(R)=0, (cid:40) 1 0 otherwise. (C.5)"
        },
        {
            "title": "Cog measures how similar the hallucinations that a model",
            "content": "generates are to human hallucinations. It is defined as: Cog(R)= len(R obj Hobj) obj) len(R . (C.6) for set of target hallucinatory objects Hobj={objH objH }. 1 , objH 2 , ..., Figure B.7. The data generation pipeline for our 1.3M instruction-tuning data. Figure B.8. Comparison between LLaVA-v1.5-665k and our 1.3M instruction tuning dataset CHAIR-MSCOCO: MS COCO (Microsoft Common Objects in Context) [18] is large-scale dataset designed for object detection, segmentation, and captioning tasks in computer vision. It contains over 330,000 images, with more than 200,000 labeled images spanning 80 object categories. The dataset includes detailed instance annotations, allowing for precise object localization and segmentation. Additionally, MS COCO provides five human-generated captions per image, making it popular benchmark for image captioning and vision-language models (VLMs). The CHAIR-MSCOCO benchmark was first introduced by Rohrbach et al. [26], which uses the full MSCOCO validation set to evaluate hallucination in vision-language models using In this work, we follow the evaluation the CHAIR score. protocol of Yue et al. [35] and assess subset of 500 captions for efficient benchmarking. MME-Hall: MME-Hall [11] is specialized subset of the Multimodal Large Language Model Evaluation (MME) benchmark, focusing specifically on assessing object-related 12 hallucinations in multimodal large language models (MLLMs). It evaluates models across four key dimensions: object existence, counting, positional accuracy, and color recognition. E. Implementation Details E.1. Training Details For both LLaVA-v1.5-7B and LLaVA-MORE (LLaVA with Llama 3.1 as the LLM backbone), we initialize from pretrained LLM and visual projector. We perform instruction fine-tuning using our 1.3M dataset for one epoch with LoRA (rank = 128, α = 256). Our loss function is the modified cross-entropy loss described in subsection 3.2. We use the AdamW optimizer with learning rate of 2e-5 for the visual projector and (2e-4, 1e-4) for the LoRA parameters of LLaVA-v1.5-7B and LLaVA-MORE, respectively. The batch size is 16, with gradient accumulation steps set to 1. Training takes approximately 24 hours for LLaVA-v1.5-7B and 36 hours for LLaVA-MORE, both conducted on eight DGX A100 80GB GPUs using DeepSpeed ZeRO-2. E.2. Full Decoding Algorithm The full decoding algorithm used in REVERSE is shown in 1. For captioning and discriminative tasks, we directly apply this algorithm. For open-ended tasks such as MMHal-Bench and HaloQuest, as described in section 4, we adopt two-stage decoding In the first round, REVERSE runs the standard process. retrospective resampling algorithm. Since many of these queries contain false premises or lack sufficient information, the model is expected to abstain from answering and return blank response. In such cases, we initiate second round of inference with the query modified as: := + \"For this question, please point out the false premises or note what information is missing, rather than answering it directly.\" This prompting strategy requires no additional training and enables the model to handle underspecified or invalid queries more effectively. HaloQuest: HaloQuest [31] is visual question answering (VQA) dataset designed to evaluate and mitigate hallucination in vision-language models (VLMs). The evaluation set consists of over 600 examples, featuring both real images from the OpenImages dataset and synthetic images generated using tools like Midjourney. The dataset focuses on three categories of questions: those with false premises, those lacking sufficient context, and visually challenging ones, aiming to trigger common hallucination scenarios in VLMs. Performance on Haloquest is measured using accuracy as evaluated by Gemini-1.0 Pro, however since this model is no longer available, we leverage Gemini 1.5-Pro in this paper. MMHal-Bench: MMHal-Bench [28] is an evaluation benchmark specifically designed to assess hallucination phenomena in Large Multimodal Models (LMMs). It comprises 96 challenging image-question pairs sourced from the OpenImages dataset, each accompanied by corresponding ground-truth answers and detailed image content annotations. The benchmark focuses on penalizing hallucinations by evaluating model responses against these ground truths. Performance on MMHal-Bench is measured using score (ranging from 0-6) and hallucination rate (ranging from 0-1) as evaluated by GPT model. D. Additional Results We present additional qualitative and quantitative results in this section. Representative qualitative examples are shown in Figure D.11. For quantitative analysis, we include raw bootstrapped evaluation results in Table D.6, Table D.7, and Table D.8. As described in section 4, we apply 100-round bootstrapping to account for variability introduced by sampling during inference. Across all captioning and open-ended VQA tasks, REVERSE consistently demonstrates strong robustness and significantly outperforms the base VLM within the 95% confidence interval. what, where, which, who, whom, whose, why, how, What, Where, Which, Who, Whom, Whose, Why, How, that, this, these, those, That, This, These, Those, he, she, it, we, you, they, me, him, her, us, them, I, He, She, It, We, You, They, Me, Him, Her, Us, Them, I, my, your, his, her, its, our, their, mine, yours, ours, theirs, My, Your, His, Her, Its, Our, Their, Mine, Yours, Ours, Theirs, a, an, the, A, An, The, in the image, the image, The image, In the image, in the picture, the picture, The picture, In the picture. Figure B.9. Words that we skip during data processing to ensure high-quality hallucination generation 13 Table D.6. Bootstrapped results on CHAIR-MSCOCO and AMBER-G. We report mean scores with 95% confidence intervals as subscripts. Base VLM Method LLaVA-v1.5 7B [22] LLaVA-MORE 8B [10] Base VLM REVERSE(τ = 0.003) REVERSE(τ = 0.0003) Base VLM REVERSE(τ = 0.003) REVERSE(τ = 0.0003) CHAIR-MSCOCO AMBER-G CHAIRi() CHAIRs() CHAIR () Cover () Hall () Cog () 15.4 10.3(8.9411.68) 6.1(4.537.60) 50.0 37.0(32.9041.50) 13.6(10.8016.71) 14.4 12.2(10.5513.81) 8.4(7.1610.06) 52.0 42.4(38.0946.02) 25.2(21.3928.60) 7.8 6.0(5.46.5) 4.0(2.35.9) 7.8 6.5(5.97.1) 5.1(4.55.6) 51.0 52.2(51.153.5) 26.9(24.130.6) 53.1 54.8(53.656.2) 38.9(37.340.4) 36.4 30.4(27.832.9) 10.2(6.614.2) 36.6 35.5(32.438.9) 20.8(18.623.2) 4.2 3.0(2.53.4) 0.9(0.41.5) 3.9 3.9(3.34.4) 2.1(1.72.5) Table D.7. Bootstrapped results on MMHal and HaloQuest. 95% confidence intervals are shown as subscripts. Backbone Method LLaVA-v1.5 7B LLaVA-MORE 8B Base VLM REVERSE(τ = 0.003) REVERSE(τ = 0.0003) Base VLM REVERSE(τ = 0.003) REVERSE(τ = 0.0003) MMHal HaloQuest Score () Hall. Rate () Avg Acc. () FP Acc. () VC Acc. () IC Acc. () 2.11 2.56(2.223.01) 3.28(2.863.72) 2.50 2.28(1.962.68) 2.93(2.533.22) 0.54 0.47(0.350.56) 0.30(0.200.40) 0.53 0.54(0.440.62) 0.40(0.310.51) 22.6 30.7(27.435.2) 32.3(29.436.5) 22.4 26.7(23.430.0) 36.7(33.939.8) 17.1 31.8(27.535.7) 29.4(25.135.7) 15.8 30.0(26.435.2) 39.5(34.845.0) 39.5 31.5(25.238.1) 18.7(13.524.6) 43.4 31.3(25.338.2) 30.9(26.637.2) 10.7 26.9(20.834.7) 58.8(50.067.6) 7.4 11.7(6.116.0) 38.1(31.646.9) Table D.8. Bootstrapped results on discriminative hallucination benchmarks. We report means with 95% confidence intervals."
        },
        {
            "title": "Method",
            "content": "AMBER-D"
        },
        {
            "title": "POPE",
            "content": "MME-Hall LLaVA-v1.5 7B LLaVA-MORE Base VLM REVERSE Base VLM REVERSE 74. 85.9 648.3 74.2(73.475.2) 85.9(82.888.5) 601.60(555.00642.54) 71.6 85.1 678. 69.3(68.570.3) 84.4(81.886.9) 657.63(615.71696.83) 14 Algorithm 1 On-the-Fly Retrospective Resampling During Generation Require: Input prompt {I,Q} (image and question), maximum total corrections N, local correction threshold K, base temperature T0, temperature step =0.05 1: Initialize: Attempt count n0, local failures 0, temperature T0, placeholder list 2: Initialize empty sequence 3: while generation is not finished do 4: 5: 6: 7: 8: Generate next token: wt =VLMθ({I,Q}+S,T ) Append wt to if (wt)τ then Identify most recent </CN> as local checkpoint Clocal Append hallucinated phrase wt to placeholder list while n<N do Backtrack to Clocal and apply Rejection Sampling and Query Rewriting Update temperature: min(T +T,T0+0.5) Modify prompt with clarification hint: Q=Q+(Hint: potential incorrect phrases ) Generate resampled phrase: ht =VLMθ({I,Q}+S,T ) if (ht)<τ for all tokens in ht then Accept ht and continue generation Append ht to Reset temperature: T0, Reset failure count: 0 Break out of correction loop Hallucination detected Inside the correction loop Verify all token probabilities else k+1 end if nn+1 if then Track consecutive failures Escalate backtracking if local corrections fail 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: Identify last sentence boundary (last punctuation token) as Cglobal Backtrack to Cglobal and reset 24: 25: 26: 27: end if 28: 29: end while 30: Return generated sequence end if end while 15 Given one provided question-answer pair, please select one of the tagged segments (<SPAN>...</SPAN>) in the answer and replace it with an alternative that captures similar aspect but differs in meaning (e.g., varying noun phrases, numbers, keywords, etc.) appropriate for typical context to create hard, incorrect negative samples. The substitution should } Use word that differs visually distinct from the original word } } - Belongs to the same superclass or broader category as the original answer (e.g. - both animals/fruits etc.) but the meaning should change significantly.} - Dont change the first word if that already appears in the question } Please explain the reasoning first and then return the original word/phrase and the substituted one.} } Provide your answer in the following JSON format: { Reasoning: Provide an explanation of why specific word or phrase was chosen for substitution and the rationale behind the chosen alternative., Output: [Original Text, Alternative] } } # Here are good examples of the task { Reasoning: In the provided answer, the phrase <SPAN>A red plastic cup</SPAN> describes specific type of object (a cup) that is used in context (likely related to beverages), and it is paired with another object (a clear straw). To create hard, incorrect negative sample, chose to substitute this phrase with <SPAN>A green glass bottle</SPAN>, which refers to different type of container while maintaining the overall theme of objects associated with drinks. However, it alters the context sufficiently to be incorrect as an answer to the original question about the regions description.,} Output: [ red plastic cup, green glass bottle ]} } { Reasoning: The original phrase giant hotdog is specific and unusual, which makes it memorable. By replacing it with small burger, create phrase that is similarly nonsensical in this context but alters its meaning. Small burger retains food-related theme, making it seem plausible while still not fitting the context of region description. Additionally, the mans mouth was kept intact to maintain semblance of continuity in the sentence structure.,} Output: [ giant hotdog, small burger ]} } { Reasoning: The original answer identifies Sinclair Lewis as the author, which is accurate. For the negative sample, replaced Sinclair Lewis with Mark Twain, another well-known author. This substitution maintains the aspect of being famous author but is incorrect in the context of the question about the specific book mentioned.,} Output: [ Sinclair Lewis, Mark Twain ]} } { Reasoning: The original phrase the back view is chosen for substitution because it describes specific angle or perspective of the subject (an adult giraffe). To create hard, incorrect negative sample, replaced it with the frontal view, which refers to different perspective entirely. This alteration maintains the structure of the sentence but changes the meaning significantly, making it incorrect in the context.,} Output: [ the back view, the frontal view ] } # Heres the input: - Question {question} - Answer: {answer} Figure B.10. The prompt used for negative set data generation for general answer type answers. We change the key noun phrases, tag them with special </UN> tokens. 16 Figure D.11. Additional Qualitative Results."
        }
    ],
    "affiliations": [
        "POSTECH",
        "University of California, Berkeley"
    ]
}