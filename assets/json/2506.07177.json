{
    "paper_title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models",
    "authors": [
        "Sangwon Jang",
        "Taekyung Ki",
        "Jaehyeong Jo",
        "Jaehong Yoon",
        "Soo Ye Kim",
        "Zhe Lin",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 7 7 1 7 0 . 6 0 5 2 : r Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models Sangwon Jang,1 Taekyung Ki,1 Jaehyeong Jo1 Jaehong Yoon2 Soo Ye Kim3 Zhe Lin3 Sung Ju Hwang1,4 KAIST1 UNC Chapel Hill2 Adobe Research3 DeepAuto.ai4 { sangwon.jang, taekyung.ki, sungju.hwang }@kaist.ac.kr Equal contribution Figure 1: Frame Guidance enables training-free controllable video generation using flexible framelevel inputs. It supports diverse applications, including keyframe-guided generation, stylization, and looping, using general frame-level inputs such as depth maps, sketches, and color blocks."
        },
        {
            "title": "Abstract",
            "content": "Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose simple latent processing method that dramatically reduces memory usage, and apply novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for wide range of tasks and input signals. See our project page: https://frame-guidance-video.github.io/. Preprint. Under review. Figure 2: Overview of Frame Guidance. The proposed Frame Guidance steers the video generation process of VDM by applying gradient-based guidance to selected frames, resulting in temporally coherent controlled video. Our method is training-free, model-agnostic, and supports wide range of frame-level conditions, such as depth maps, sketches, styles, and color blocks."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of diffusion models [19, 25, 40] has led to the development of powerful video generation models. Recent large-scale video diffusion models (VDMs) have made significant progress in high-quality text-to-video (T2V) and image-to-video (I2V) generation, which are capable of generating diverse and realistic video content [3, 4, 15, 42, 49]. With ongoing advancements, there is growing interest in enabling more fine-grained control over the generation process. Recent progress underscores the need for practical approach to controllable video generation. Hence, we identify two major desiderata: (1) model-agnostic, training-free framework, and (2) general-purpose guidance method. Existing methods [5, 16, 23] typically fine-tune large-scale VDMs [4, 31, 42, 49] for each specific control task, which is increasingly impractical due to high computational cost and the burden of retraining with every new model release. This highlights the need for training-free guidance methods that work across models. Moreover, end users prefer simple, generalizable frameworks that support diverse tasks and inputs, such as reference images, depth maps, or sketches, rather than task-specific models [20, 44] that are restricted to fixed input type. Existing methods fall short of satisfying both desiderata simultaneously: training-free approaches [20, 22, 24] are often task-specific and lack generalizability, while general-purpose methods [5, 21, 23] require fine-tuning and need substantial training resources. Many existing methods [1, 44, 46] are both task-specific and training-dependent, making them difficult to adapt to new models or tasks. In this work, we propose Frame Guidance, novel guidance method for VDMs that is model-agnostic, training-free, and supports wide range of controllable video generation tasks using frame-level signals. As illustrated in Figure 2, Frame Guidance steers the video generation process by applying guidance to selected frames based on frame-level signals, which produce temporally coherent video. We present two core components for efficient and flexible frame-level guidance. First, we introduce latent slicing, simple latent decoding technique that enables efficient training-free guidance for large-scale VDMs. Based on temporally local patterns of video encoding, we propose to decode only the short temporal slices of the video latent for computing the guidance loss. Furthermore, we present video latent optimization (VLO), novel latent update strategy designed for precise control of the video diffusion process. Observing that the overall layout of the frames is largely determined in the first few inference steps, we apply deterministic optimization at the early stages for globally coherent layout, and employ stochastic optimization until the mid-stage for refining the details. Frame Guidance is applicable to general frame-level control tasks, as shown in Figure 1, including keyframe-guided generation, stylized video generation, and looped video generation. In particular, Frame Guidance supports general input conditions, such as depth maps, sketches, and color blocks as shown in Figure 1(d,e,f). We demonstrate that Frame Guidance consistently produces superior results on frame-level control tasks across various VDMs [15, 42, 49]."
        },
        {
            "title": "2 Related work",
            "content": "Training-required controllable video generation Advances in T2V and I2V generation have opened up new opportunities for fine-grained user control. These include conditioning on keyframes [44, 52], using style reference images for stylized generation [26, 43], and incorporating trajectory-based signals such as camera movement [1, 53] or motion trajectory [14, 29, 47] for dynamic scene generation. However, existing methods often require extensive training and modelspecific data preparation, such as fixed resolution or frame counts, making fine-tuning increasingly impractical for general users as model sizes and resource requirements continue to grow. 2 Figure 3: Frame Guidance for keyframe-guided video generation task. (Left) Illustration of our method with latent slicing and spatial down-sampling (Section 4.1), and gradient propagation with L2 loss (blue arrows) Section 4.3). (Right) Visualization of the video latent optimization (VLO; Section 4.2) and the generated video frames during inference with guidance. Training-free controllable video generation To reduce the burden of training large models, several approaches have explored training-free controllable video generation [20, 22, 24]. For example, CamTrol [20] enables camera control using external 3D point clouds, while MotionClone [24] performs motion cloning based on temporal attention maps extracted from reference video. However, these methods are tailored to specific tasks and are thereby ill-suited for more general scenarios requiring different types or even multiple input signals. In this work, we propose training-free guidance method that generalizes to wide range of video generation tasks using frame-level signals."
        },
        {
            "title": "3 Preliminaries",
            "content": "Video diffusion models (VDMs) Recent video diffusion models [4, 15, 42, 49] learn to generate video by reversing the noising process in the latent space. The high-dimensional video x0 is encoded into lower-dimensional latent z0 = E(x0). The forward noising process corrupts the latent zt = 1 αtϵ, where ϵ (0, I) and {αt}t[0,T ] is pre-defined noise schedule. The reverse 1 αtz0, denoising process is learned through predicting time-dependent velocity vt = which represents the direction from noisy sample toward the clean sample [36]. For each time step t, the clean sample z0t can be computed from the noisy sample zt using Tweedies formula [11]: αtz0 + αtϵ z0t := E[z0zt] = αtzt 1 αt vθ(zt, t), (1) where vθ is the predicted velocity. Latents z0 are decoded into videos with the decoder ˆx0 = D(z0). Recent large-scale VDMs [42, 49] commonly employ spatio-temporal VAEs to encode highdimensional video data. notable example is the CausalVAE [4, 51], which enforces temporal causality in the latent space by allowing only past frames to influence future ones. While this design encourages temporally coherent video generation, it also introduces temporal dependencies within the latent sequence, requiring the entire sequence to be decoded even to reconstruct single frame. Training-free guidance Training-free guidance [2, 17, 37, 50] uses pre-trained diffusion models to generate samples that satisfy specific condition, without additional training. At each denoising step t, it estimates clean image x0t = D(z0t) from the current latent zt, and computes guidance loss Le(D(z0t), c) that measures alignment with the target control c. The latent zt is then updated using the gradient ztLe during inference. One such strategy is the time-travel trick [2, 17, 50], which alternates between denoising and renoising steps to correct accumulated sampling errors."
        },
        {
            "title": "4 Method",
            "content": "We present Frame Guidance, simple yet effective training-free framework for controllable video generation using frame-level signals, designed to be compatible with modern large-scale VDMs. Our approach guides the generation process of pre-trained VDMs by optimizing video latents to minimize frame-level guidance loss applied to selected frames. In this section, we introduce two key components that enable efficient and flexible frame-level guidance for large-scale VDMs. 3 Figure 4: (a) GPU memory required for guidance when using full latent sequence, sliced latents, and latent slicing with spatial down-sampling. (b) Temporal locality of CausalVAEs from various models. Each latent (y-axis) is primarily affected by small subset of temporally local video frames. (c) Video generation phase. We measure the L2 distance in the low-frequency region through the inference steps. Layouts are mostly determined during the first few steps. (d) Guidance influence through the inference steps. Yellow box indicates the latent corresponding to the guided frame. 4.1 Latent slicing The main challenge of training-free guidance on video generation is the computational constraint. To compute the guidance loss for latent optimization, we should keep track of the gradient chain passing through the whole network (Figure 3). In Figure 4(a), we analyze the memory usage and find that it exceeds 650GB even with gradient checkpointing [8], mostly due to CausalVAE [4, 51]. This overhead arises from the design of CausalVAE, which requires decoding the entire latent sequence even to reconstruct single frame. To tackle this, we first analyze the latent space of CausalVAE. Analysis of CausalVAEs latent space While CausalVAE is designed to enforce temporal causality in the video latent sequence, we observe that such causality is absent in practice. To validate this, we conduct simple experiment: replace single frame in real video with black image (all pixels set to zero), and measure the difference between the latents of the original video and the modified video. As shown in Figure 4(b), the perturbation affects only few consecutive latents rather than the entire sequence. This behavior consistently appears across various VDMs [15, 42, 49]. We refer to this property as temporal locality, key observation for our efficient decoding method. Efficient decoding with sliced latent We introduce latent slicing, simple technique that significantly reduces the cost of gradient computation on CausalVAE by reconstructing only few frames from the sliced latents. When reconstructing the i-th frame xi, we decode small window of 3 latents, starting from the latent zj, where the latent index is determined by and the temporal compression rate of its CausalVAE. Thanks to temporal locality, it is sufficient to decode only the corresponding latents to reconstruct single video frame. As shown in Figure 19, the reconstructed frames are nearly identical to those from full-sequence decoding. As highlighted in Figure 4(a), this latent slicing reduces memory usage by up to 15 compared to using the entire latent sequence. In parallel with latent slicing, we can further reduce the memory usage by spatially down-sampling the latents before decoding. Despite the lower resolution, the guidance loss from the down-sampled latents still provides meaningful signals to guide the generation. As shown in Figure 4(a), applying 2 spatial down-sampling combined with latent slicing reduces memory usage by up to 60, enabling gradient computation to be maintained on single GPU even for large VDMs [42]. 4.2 Video latent optimization strategy (VLO) We introduce an effective video latent update strategy for generating globally coherent video that applies to any type of video diffusion model. Empirical analysis of layout formation We empirically observe that the generation process in VDMs involves two distinct stages: an early layout stage, where the global spatio-temporal structure is established, followed by detail stage for refining fine-grained details. This behavior is represented in Figure 4(c), which plots the difference between the final output video and the downsampled reconstructions at each denoising step. We observe that the global layout of the video is primarily established within the first five inference steps, which we refer to as the layout stage. Hybrid video latent optimization strategy Prior training-free guidance methods for image generation [2, 37, 50] use time-travel trick (Algorithm 2), which optimizes the latent zt via three steps: (1) denoising to zt1, (2) updating zt1 with gradient step, and (3) renoising updated latent back to zt. However, we find that renoising the updated latent during the early stage yields an adverse effect on the guidance. Due to the high noise scale in the early stage, the renoising step significantly weakens the influence of guidance, and the layouts are established regardless of the guidance. Furthermore, as shown in the gradient propagation map in Figure 4(d), gradients initially propagate globally across the latent sequence but gradually become localized around the guided frames. Consequently, the absence of guidance during the early stage impairs the temporal coherence of the generated video, making it difficult to correct in later stages. To address this limitation, we introduce video latent optimization (VLO), simple hybrid strategy, which applies different update rules to video latents depending on the generation stage. Specifically, at each denoising step in the layout stage, the latent zt is updated as follows: zt zt ηztLe(xI 0t, cframes), (2) where η is the guidance step size, xI 0t is the predicted clean frames where we apply guidance, and Le is guidance loss with frame-level controls cframes. Since this update does not introduce noise in the early inference steps, it effectively steers the latent to minimize the guidance loss. This deterministic update results in temporally aligned global layout. In the detail stage, we switch to time-travel to improve the fine details. Updating the latent with renoising in this stage helps reduce accumulated errors during the sampling while preserving the established layout in the earlier stage. After the detail stage, our inference proceeds without guidance. 4.3 Frame guidance In Algorithm 1, we provide the overall procedure of our Frame Guidance, which incorporates both the latent slicing and VLO. Given set of framelevel controls cframes and selected frame indices {i1, } where we apply the guidance, we first compute their corresponding latent indices {j1, } (see Figure 4(b)). For pre-defined generation phases tL and tD, we optimize the video latents in the following manner: At each denoising step > tD, we extract the sliced latents zJ 0t from the latent indices ( Line 7) and compute the guidance loss gt = ztLe(xI 0t, cframes) (Lines 8-9). With VLO strategy, we deterministically optimize the latent zt ( Line 11 ) in the layout stage (t > tL), and optionally switch to time-travel trick (Algorithm 2) during the detail stage (tL > tD). After performing latent optimization times, we proceed to the next denoising step via DDIM [39]. Our Frame Guidance algorithm for flow matching based models, such as Wan [42], is provided in Appendix C.3. Algorithm 1 Frame Guidance Require: I, tL, tD, repeat step , step size η, guidance loss Le, model vθ(, ) 1: zT (0, I) 2: Frame-Idx-to-Latent-Idx(I) 3: for = T, ..., 1 do 4: 5: 6: 7: if > tD then for = 1, ..., 1 do αtzt z0t zJ 0t Latent-Slicing(z0t, ) 0t D(zJ xI 0t) gt = ztLe(xI if > tL then 0t, cframes) Guidance step Layout stage 1 αt vθ(zt, t) 8: zt zt ηgt else Detail stage zt Time-Travel(zt, z0t, gt) end if end for end if zt1 DDIM(zt, z0t) 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for 19: return z0 Gradient propagation after slicing Even without processing the full latent sequence, guidance applied to sliced latents produces temporally coherent video. This temporal coherence arises from the denoising network vθ, which enables the gradient of the guidance loss to propagate through the entire video latents. We visualize the gradient propagation in Figure 4(d), where the guidance applied to the middle frame influences all other frames during the layout stage. Applying guidance to target frames causes non-targeted frames to become harmonized with the guided ones, as visualized in Figure 3 right. In Appendix C.4, we demonstrate that the temporal structure of the generated video is primarily determined by the denoising network, whereas the contribution of CausalVAE is minimal. 4.4 Loss design for various tasks Frame Guidance is readily applicable to wide range of frame-conditioned video generation tasks, with appropriately designed guidance loss. Here, we provide simple loss designs for representative frame-conditioned video generation tasks and general user inputs. Keyframe-guided video generation aims to synthesize videos that transition smoothly between multiple user-specified keyframes, without enforcing strict pixel-level reconstruction. Given an initial image as the input to the I2V model, we minimize simple L2 loss defined as Le = (cid:80) xi 0t2 0t is the predicted clean i-th frame. The similarity to each keyframe can be controlled by adjusting the guidance strength, such as the number of repeat steps or step size η. Unlike training-based approaches [44, 52] that are limited to fixed keyframe positions (e.g., the last frame), our method supports arbitrary keyframe placements. denotes the target keyframes and xi 2, where xI iI xi Stylized video generation aims to synthesize videos in the style of given reference image using T2V model. We employ differentiable style encoder Ψ to compute the style loss defined as Le = (cid:80) 0t)), where xstyle is the style reference image. We use the Contrastive Style Descriptor (CSD) [38] for Ψ(), and find that guiding only few selected (or randomly chosen) frames is sufficient to propagate the desired style across the entire video. iI cos(Ψ(xstyle), Ψ(xi Looped video generation aims to synthesize videos where the first and last frames match, producing seamless loop using T2V model. We define the loss as Le = sg(x1 2, where sg() denotes the stop-gradient operator. This design prevents over-saturation of the generated frames by forcing the last frame to be updated the most to match the first frame. 0t) xL 0t2 General input guidance aims to synthesize videos conditioned on general user-specified conditions beyond RGB images, for example, depth maps or sketches. We use differentiable encoder Ψ, such as depth estimator [48] or an edge predictor [6], to extract structural features from the estimated clean image. We minimize an encoder-aligned L2 loss defined as Le = (cid:80) 0t)2 2, where Ψ(xi ) denotes the encoded target conditions. iI Ψ(xi ) Ψ(xi"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Keyframe-guided video generation We evaluate Frame Guidance on keyframe-guided video generation tasks, which aim to synthesize videos that transition smoothly between multiple user-specified keyframes. As opposed to video interpolation tasks [12, 44] that require exact frame matching, keyframe-guided generation only requires the visual similarity to the keyframes, and addresses the generation of longer videos. Datasets We select 40 clips with more than 81 frames from DAVIS [32] dataset and 30 real-world videos from Pexels1 dataset. Pexels features more dynamic and human-centric videos than DAVIS, making it more difficult for video generation. We provide more details on the dataset in Appendix B.2. Baselines We compare Frame Guidance against frame interpolation methods, including TRF [12], SVD-Interp [44], and CogX-Interp. TRF is training-free approach for Stable Video Diffusion (SVD) [3], SVD-Interp uses fine-tuned reversed-motion SVD, and CogX-Interp2 fine-tunes CogX with first and last frame conditioning. We also compare with basic I2V baselines (CogX [49] and Wan [42]). For our method, we apply Frame Guidance on CogX and Wan models using the L2 loss defined in Section 4.4 with the final frame given, and further report results additionally using the middle frame. We also report results of applying Frame Guidance to CogX-Interp. Qualitative comparison As shown in Figure 5, our approach generates videos with natural transitions, where the selected frames closely resemble the keyframes. For example, Figure 5(c) visualizes well-aligned frames, with the paraglider appearing in consistent position. In contrast, CogX-Interp often struggles with challenging motion. Applying Frame Guidance to CogX-Interp (Figure 5(e)) or to stronger VDM backbone (Figure 5(f)) results in notably improved output quality. 1https://huggingface.co/datasets/jovianzm/Pexels-400k (Accessed: 2025-05-07) 2https://github.com/feizc/CogvideX-Interpolation (Accessed: 2025-05-07) 6 Figure 5: Qualitative comparison on keyframe-guided video generation tasks. Yellow box indicates the keyframe condition. Orange box in (a) shows disconnection in SVD-Interp. Red box in (d) visualizes failure case for the CogX-Interp baseline for dynamic human motion. Input frames CogX-I2V Wan-14B-I2V TRF [12] Ours (CogX) Ours (CogX) Ours (Wan-14B) SVD-Interp. [44] CogX-Interp. Ours (CogX-Interp.) I, M, I, I, I, M, I, M, I, I, Pexels Train DAVIS [32] free FID FVD FID FVD 74.98 1122.6 890.1 73.03 1033.3 772.8 79.03 1106.2 923.1 68.54 1027.3 613.4 989.3 68.97 577.1 71.63 761.1 904.8 75.37 1210.7 800.3 58.73 1081.5 506.0 47.86 723.26 420.3 60.36 59.04 62.07 57.62 55.60 57.68 63.89 46.59 37.95 Figure 6: Keyframe-guided generation results. (Left) Human evaluation. (Right) Quantitative results. I, , and denote initial, middle, and final frames, respectively. Train-free indicates whether the backbone VDM is fine-tuned for the frame interpolation task, not base I2V model. Human evaluation We conduct human evaluations to assess the quality of generated videos, focusing on (1) video quality and (2) similarity to the keyframes. As shown in Figure 6 left, applying Frame Guidance to Wan yields the highest video quality, surpassing the trained model CogX-Interp. Applying guidance to CogX-Interp produces high-quality videos with guided frames nearly identical to the keyframes. Further details are provided in Appendix B.2. Quantitative results We measure FID [18] and FVD [13] to assess the quality of the generated videos. As shown in Figure 6 right, Frame Guidance applied to pre-trained I2V models significantly outperforms all other training-free methods. Moreover, Frame Guidance applied to CogX-Interp outperforms all the training-required baselines. These results, combined with the human evaluation, demonstrate that our method effectively guides video generation without additional training. We discuss further details regarding the quantitative results in Appendix B.2. 5.2 Stylized video generation We also validate Frame Guidance on stylized video generation tasks, which aim to synthesize videos in the style of given reference image, using T2V model. Dataset We use subset of the stylized video dataset introduced in StyleCrafter [26], which consists of 6 challenging style reference images, each paired with an aligned style prompt and 9 distinct content prompts. We provide further details about the dataset in Appendix B.3. Baselines We compare our method with three baselines. CogX-T2V is pre-trained T2V model. VideoComposer [43] is training-based method supporting multiple conditions, such as style image and depth maps. StyleCrafter [26] is also training-based method that solely trains style adapter on top of VideoCrafter [7]. For our method, we apply Frame Guidance to the CogX-T2V [49] model using the style loss defined in Section 4.4. We provide more details of our method in Appendix B.3. Qualitative comparison Examples in Figure 7 show that our method can generate balanced stylized videos in terms of both text alignment and style conformity, with diverse motion. In contrast, VideoComposer fails to disentangle content and style in the reference images, while StyleCrafter produces videos with minimal motion that are poorly aligned to the reference style. CogX-T2V struggles to capture detailed textures or patterns, for example, geometric shapes or sunflowers. 7 Figure 7: Qualitative comparison on stylized video generation. Ours generates high-quality videos that follow the reference style, whereas baselines fail to produce motion or show poor alignment. Text-Alignment Train free CLIP-T ViCLIP-T CLIP-S ViCLIP-S Vid.Comp. [43] StyleCrafter [26] CogX-T2V [49] Ours 0.869 0.635 0.588 0.624 0.219 0.157 0.139 0.185 0.211 0.207 0.220 0.224 0.137 0.273 0.259 0. Style-Alignment Figure 8: Stylized video generation results. (Left) Human evaluation. (Right) Quantitative results. Human evaluation We conduct human evaluation to assess the quality of stylized videos, evaluating three criteria (1) style alignment, (2) text alignment, and (3) motion dynamics. As shown in Figure 8 left, our method achieves the best results across all criteria, significantly outperforming the trainingbased baselines. These results show that Frame Guidance successfully guides video generation to follow the reference style without any additional training. Further details and the results on overall preference are provided in Appendix B.3. Quantitative results We evaluate text alignment using CLIP-T [33] and ViCLIP-T [45], and assess style alignment using CLIP-S [33] and ViCLIP-S [45] on the generated videos. As shown in Figure 8 right, our method achieves the best scores on all metrics, except for CLIP-S, where it matches the performance of StyleCrafter. While VideoComposer achieves the highest style alignment scores, this is largely due to replicating the style image without adhering to the text prompt, as in Figure 7 left. 5.3 Looped video generation We further apply Frame Guidance on the looped video generation task, which aims to synthesize videos where the first and last frames match, producing seamless loop. We use the loop loss defined in Section 4.4 to steer the last frame to match the first. Guidance is applied to the generated video without requiring any external conditions, using only text prompts as input. As shown in Figure 1(c) and Figure 16, Frame Guidance generates high-quality looped videos featuring dynamic motions that are well-aligned with the input text prompt. 5.4 Other applications Using color block drawing During keyframe-guided generation, keyframe similarity can be flexibly controlled by adjusting the guidance strength. This allows new forms of user-provided control signals that are easy to create, such as coarse sketches or color blocks. In particular, we introduce novel application that allows users to guide video generation using edited frames, where simple visual edits via color blocks indicate changes in color or detail. As illustrated in Figure 1(d), the generated video depicts the mountain changing color and texture in three distinct ways, which is difficult to achieve using text prompts alone. For Frame Guidance, color blocks act as rough visual hints that allow natural scene transitions while preserving the contents. We provide more examples in Figure 17. Masked region guidance While our previously described methods apply guidance to the entire frame, we demonstrate that the guidance can be effectively restricted to specific regions, by using L2 8 Figure 9: Examples of other applications. (a) Object movement guided by masked region. (b) Video style transfer with SDEdit [27]. (c) Guidance using multiple types of inputs: depth map and sketch. loss with binary mask. In Figure 9(a), we present an example of generating video with object motion, guided by cropped image and its segmentation mask. By applying guidance solely to the object region, the background remains unchanged while the object shows smooth movement. Depth map / Sketch guidance Furthermore, Frame Guidance supports general types of frame-level signals, such as depth maps and sketches, which offer more user-friendly conditioning compared to RGB images as input. Using the general input guidance defined in Section 4.4, Frame Guidance is capable of generating high-quality guided videos as shown in Figure 1(e) and (f). Video style transfer We extend Frame Guidance to video editing tasks. Taking video as input, we apply Frame Guidance to generate an edited video that follows reference style. Video style transfer can be achieved by applying simple V2V SDEdit [27] in the detail stage. This results in preserving the original motion and layout while successfully transferring the reference style, as in Figure 9(b). Multi condition guidance Frame Guidance can integrate multiple input types by combining losses. As shown in Figure 9(c), we apply guidance to intermediate frames, combining the depth map loss and sketch loss for the CogX-Interp model. The generated video demonstrates smooth motion that follows the input signals, showing the flexibility of Frame Guidance in handling complex scenarios. 5.5 Ablation studies Necessity of VLO To validate the importance of VLO in Frame Guidance, we compare it against two variants: one that uses only the time-travel trick and another that applies only the deterministic update from Equation 2 during the guidance process. Table 1 shows that using only the time-travel trick yields higher FVD scores due to difficulty in forming coherent layouts, while the deterministic update alone produces over-saturated or temporally disconnected videos. Table 1: Ablation study on latent optimization strategy. Method FID FVD Time-travel Deterministic 57.37 56.61 778.4 637.3 VLO (Ours) 55.60 577.1 Model agnostic As shown in Figure 6, our method is compatible with variety of VDMs, including CogVideoX [49], its fine-tuned variant CogVideoX-Interpolation, and Wan-14B [42], flow-matching-based model. To further demonstrate its generality, we also apply our approach to two additional models: SVD [3], U-Net-based [34] diffusion model, and LTX-2B [15] which supports sequences up to 161 frames. As illustrated in Figure 18, our method consistently performs well across all these VDMs."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present Frame Guidance, novel training-free framework for diverse control tasks using frame-level signals. By applying guidance to selected frames, our method enables natural control throughout the video. To achieve this, we partially decode sliced latents during guidance computation and introduce latent optimization strategy designed for video. Our approach supports wide range of tasks without training, including special cases such as color block guidance and looped video generation. We discuss the limitations and societal impacts of our method in Appendix D."
        },
        {
            "title": "References",
            "content": "[1] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. [2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Roni Sengupta, Micah Goldblum, Jonas In International Geiping, and Tom Goldstein. Universal guidance for diffusion models. Conference on Learning Representations, 2024. [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. [5] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, et al. Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise. arXiv preprint arXiv:2501.08331, 2025. [6] Caroline Chan, Frédo Durand, and Phillip Isola. Learning to generate line drawings that convey geometry and semantics. In Conference on Computer Vision and Pattern Recognition, 2022. [7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [8] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [9] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In International Conference on Learning Representations, 2023. [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 2021. [11] Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496):16021614, 2011. [12] Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael Black, and Xuaner Zhang. Explorative inbetweening of time and space. In European Conference on Computer Vision, 2024. [13] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, and Jia-Bin Huang. On the content bias in fréchet video distance. In Conference on Computer Vision and Pattern Recognition, 2024. [14] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. [15] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [16] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for video diffusion models. In International Conference on Learning Representations, 2025. 10 [17] Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, Wei-Hsiang Liao, Yuki Mitsufuji, Zico Kolter, Ruslan Salakhutdinov, and Stefano Ermon. Manifold preserving guided diffusion. In International Conference on Learning Representations, 2024. [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 2017. [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 2020. [20] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. [21] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. [22] Jialu Li, Shoubin Yu, Han Lin, Jaemin Cho, Jaehong Yoon, and Mohit Bansal. Training-free guidance in text-to-video generation via multimodal planning and structured noise initialization. arXiv preprint arXiv:2504.08641, 2025. [23] Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, and Zuxuan Wu. Magicmotion: Controllable video generation with dense-to-sparse trajectory guidance. arXiv preprint arXiv:2503.16421, 2025. [24] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. In International Conference on Learning Representations, 2025. [25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [26] Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang, Xintao Wang, Yujiu Yang, and Ying Shan. Stylecrafter: Enhancing stylized text-to-video generation with style adapter. arXiv preprint arXiv:2312.00330, 2023. [27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. [28] Nithin Gopalakrishnan Nair and Vishal Patel. Dreamguider: Improved training free diffusionbased conditional generation. arXiv preprint arXiv:2406.02549, 2024. [29] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David B. Lindell. SG-i2v: Self-guided trajectory control in image-to-video generation. In International Conference on Learning Representations, 2025. [30] OpenAI. Gpt-4o system card. Technical report, 2024. [31] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2025. [32] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. [34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015. [35] Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. RB-modulation: Training-free stylization using reference-based modulation. In International Conference on Learning Representations, 2025. [36] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. [37] Yifei Shen, Xinyang Jiang, Yifan Yang, Yezhen Wang, Dongqi Han, and Dongsheng Li. Understanding and improving training-free loss-based diffusion guidance. In Advances in Neural Information Processing Systems, 2024. [38] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. [39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [40] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. [41] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/ huggingface/diffusers, 2022. [42] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [43] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 2023. [44] Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, and Steve Seitz. Generative inbetweening: Adapting image-to-video models for keyframe interpolation. In International Conference on Learning Representations, 2025. [45] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. [46] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH, 2024. 12 [47] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, 2024. [48] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 2024. [49] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In International Conference on Learning Representations, 2025. [50] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Trainingfree energy-guided conditional diffusion model. In International Conference on Computer Vision, 2023. [51] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In International Conference on Learning Representations, 2024. [52] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. In Conference on Computer Vision and Pattern Recognition, 2024. [53] Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. Cami2v: Cameracontrolled image-to-video diffusion model. arXiv preprint arXiv:2410.15957, 2024."
        },
        {
            "title": "Appendix",
            "content": "Organization The Appendix is organized as follows: In Section A, we provide the additional backgrounds of our work. We describe the details of the experiments and our framework in Section B, and further discussion in Section C. Lastly, in Section D, we discuss the limitations of our work."
        },
        {
            "title": "A Backgrounds",
            "content": "A.1 Training-free diffusion guidance Recent works [2, 9, 10, 17, 37, 40, 50] have explored conditional generation by injecting external conditions into pre-trained diffusion models. Among them, training-free guidance methods [2, 9, 17, 37, 50] achieve controllable generation without additional training by optimizing the noisy latent during the reverse process. This optimization is guided by loss function that measures the alignment between intermediate latents and the target condition at each denoising step. FreeDom [50] and UniversalGuidance [2] leverage off-the-shelf models to compute the various guidance losses, achieving wide range of controllable image generation tasks. Later works [17, 28, 35] bypass the denoising module for computing the guidance loss, enabling more efficient training-free diffusion guidance. A.2 Flow matching Flow matching [25] belongs to the family of flow-based generative models, which are known for faster sampling compared to diffusion models [19]. Let [0, 1] be the time, Rd be data, and be unknown target distribution. The goal of flow matching [25] is to estimate time-dependent transformation zt : [0, 1] Rd Rd (referred to as flow) that maps prior distribution p0 (e.g., Gaussian) to distribution p1 q. Instead of directly estimating the flow, Lipman et al. [25] proposes to regress generating vector field vt(, t) : [0, 1] Rd Rd that induces the flow zt via the following ordinary differential equation (ODE): dzt(x) dt = vt(zt(x)) and z0(x) = x. (3) It is common practice to design this flow ϕt along an optimal transport (OT) trajectory that connects prior sample to target sample with straight interpolation: zt := (1 t)x0 + tx1, where x0 p0 and x1 q. In this case, the target vt is computed as constant: vt(x, t) = x1 x0 for all [0, 1]. With neural network vθ that estimates vt, we can generate data x1 by numerically solving the ODE in Equation 3 (e.g., Euler method). Similar to Tweedies formula [11], we can approximate cleaned sample at each time by z1t := zt + 1 1 vθ(zt, t). (4) Throughout this paper, we interchangeably reverse the direction of time by parameterizing it as s(t) = (1 t), [0, 1] to align with the convention of the diffusion models where the generative process proceeds from to 0."
        },
        {
            "title": "B Experimental details",
            "content": "B.1 Implementation details All our experiments are conducted on single H100 GPU. Hyperparameters related to guidance, such as step size η and repetition , are adjustable depending on the task and model characteristics. For example, in keyframe-guided video generation using diffusion-based CogVideoX [49], we define the layout stage within the first 5 steps, set = 10, and use step size of η = 3.0. For the time-travel trick, is linearly decreased over 15 steps. At each step, gradients are L2-normalized before being scaled by η for the update. Since Wan-14B [42] employs flow matching as its generative modeling, its inference is fully deterministic, and the layout is mostly established within 2 steps. Therefore, we set the layout stage to the 14 Figure 10: screenshot of questionnaires from our human evaluation on keyframe-guided generation. Figure 11: Human evaluation results on keyframe-guided generation including Wan-I2V. first 2 inference steps, and apply the same M, η, and time-travel configuration. Moreover, since our implementation introduces more stochasticity (see Appendix C.3), we slightly reduce the number of time-travel steps. To maintain practicality, we empirically limit the number of guidance steps such that the overall runtime does not exceed 4 the base models inference time. To reduce GPU memory usage, we apply gradient checkpointing [8] to the denoising network using the Diffusers [41] library. For the CausalVAE, gradient checkpointing is applied only in CogVideoX [49], as Wan-14B [42] implementation does not currently support it. We do not apply spatial downsampling in CogVideoX, since it runs on single GPU without it. In contrast, we apply 2 spatial downsampling in experiments with Wan-14B. B.2 Keyframe-guided video generation Dataset For evaluation, we use videos from the DAVIS [32] dataset and Pexels. From DAVIS, we select 40 videos with at least 81 frames, matching the maximum frame length supported by Wan14B [42]. The resolution of each video is resized and center-cropped according to the requirements of each pre-trained model. To ensure fair comparisons across models, the same initial and final frames are used. Based on this setup, the reference set for each model is configured with slightly different FPS settings. For example, for an 81-frame video, CogVideoX [49] supports only 49 frames, so we temporally downsample the video accordingly. The Pexels dataset contains more real-world videos with challenging motions and frequent camera view changes. We randomly select subset of 30 videos, which features more dynamic and human-centric content compared to DAVIS. For pre-trained models that accept text prompts as input, except for Stable Video Diffusion [3](SVD)- based methods [12, 44], we used prompts derived from the original videos. Specifically, we concatenated three frames from each original video and generated caption using GPT-4o [30]. The same prompt was applied consistently across all baseline models. Human evaluation We conduct human evaluation for keyframe-guided video generation task to evaluate two main aspects: (1) video quality and (2) similarity to the keyframes. Both metrics are rated on an absolute scale from 1 to 5. As shown in Figure 10, participants evaluated all videos generated from the same keyframes side by side. We collected responses from 20 participants, 15 Figure 12: screenshot of questionnaires from our human evaluation on stylized video generation task. Figure 13: Human evaluation results on stylized video generation including overall preference. evaluating 5 types of videos across 5 different methods. The full human evaluation results, including Wan-I2V, are provided in Figure 11. Evaluation metric For evaluation metric, we employ FID [18] and content-debiased FVD [13] between generated videos and real videos. Both metrics quantify the distributional distance between generated videos and real videos from the dataset. FID is computed by extracting all frames from the video and treating them as individual images. FVD is measured against reference videos adjusted to match each models resolution and FPS. Therefore, cross-model comparisons are not strictly valid. As shown in Figure 6 right, our method with Wan slightly outperforms Wan I2V in these quantitative metrics. However, human evaluations in Figure 6 left suggest more noticeable improvement, which may not be fully captured by such metrics. Notably, the overall FID and FVD scores are relatively high, as our setting involves longer and more dynamic videos compared to related tasks such as video interpolation, making the dataset more challenging. We provide more qualitative examples in Figure 14. B.3 Stylized video generation Based on our analysis of layout formation in Section 4.2, we apply VLO with different schedule for stylized video generation compared to keyframe-guided video generation. Specifically, we start applying the deterministic latent update (Equation 2) at step 3 before entering the detail stage (step 5), and then switch to time travel during steps 15 - 20. This design helps shape the geometric patterns and structure of the style reference image during the layout stage. After that, we proceed the inference without guidance. We set the guidance step size η = 3 and the number of repetition = 5. We compute the style guidance loss on 4 evenly spaced frames from the entire video. Dataset We use subset of the test dataset introduced in StyleCrafter [26], which consists of 9 content prompts and 6 style reference images with corresponding style descriptions. In Table 2 and Table 3, we detail our test dataset. The content prompts describe an entire video content using 16 Table 2: Text prompts [26] used for stylized video generation. Content prompt street performer playing the guitar. chef preparing meals in kitchen. student walking to school with backpack. bear catching fish in river. knight riding horse through field. Content prompt wolf walking stealthily through the forest. hot air balloon floating in the sky. wooden sailboat docked in harbor. field of sunflowers on sunny day. Table 3: Style references and style prompts [26] used for stylized video generation. Style image Style prompt Style image Style prompt Manga Style, black and white digital inking, high contrast, detailed line work, cross-hatching for shadows, clean, no color. Low Poly Digital Art, geometric shapes, vibrant colors, flat texture, sharp edges, gradient shading, modern graphic style. Wartercolor Paining, fluid brushstrokes, transparent washes, color blending, visible paper texture, impressionistic style. Ink and watercolor on paper, urban sketching style, detailed line work, washed colors, realistic shading, and vintage feel. Manga-inspired digital art, dynamic composition, exaggerated proportions, sharp lines, cel-shading, high-contrast colors with focus on sepia tones and blues. Pixel art illustration, digital medium, detailed sprite work, vibrant color palette, smooth shading, and nostalgic, retro video game aesthetic. simple sentence, while the style prompts describe the styles of the video. The style prompts are generated by GPT-4o [30]. We concatenate each content prompt with each style prompt, resulting in total of 54 full prompts for stylized video generation. Human evaluation In Figure 12, we provide screenshots of the questionnaires and labeling instructions. 20 participants are asked to evaluate four metrics: (1) style alignment, (2) text alignment, (3) motion dynamics, and (4) overall video preference of five stylized videos generated by four models. All metrics were rated on an absolute scale from 1 to 5. The complete evaluation results, including overall preference, are provided in Figure 13. Evaluation metric We employ CLIP-Text and ViCLIP-Text to access the text alignment of the generated videos. We also compute CLIP-Style and ViCLIP-Style to access the style conformity of the generated videos. Specifically, CLIP-Text and CLIP-Style are computed by using the CLIP [33] text and image encoders, respectively: 1 (cid:88) l=1 fI (xl) fT (p) fI (xl)2fT (p)2 and 1 (cid:88) l= fI (xl) fI (xstyle) fI (xl)2fI (xstyle)2 , (5) where xl is the l-th frame, is the text prompt, xstyle is the style reference image, and fI () and fT () are the CLIP [33] image and text encoders, respectively. Similarly, ViCLIP-Text and ViCLIP-Style are both computed by using Video CLIP model [45]: fV (x) fT (p) fV (x)2fT (p) and fV (x) fT (pstyle) fV (x)2fT (pstyle)2 , (6) where is the video, and pstyle are the full and style prompts, and fV () and fT () are the ViCLIP video and text encoders, respectively. We provide more qualitative examples in Figure 15. B.4 Loop video generation We use the similar guidance schedule with keyframe-guided video generation task, but reduce the early guidance strength to avoid producing over-saturated examples. We provide more qualitative examples in Figure 16. 17 Figure 14: Qualitative comparison of keyframe-guided video generation. Orange arrows indicate temporally disconnected frames, and red boxes highlight poor keyframe similarity. Our method generates temporally coherent videos while maintaining semantic similarity to the keyframes. B.5 Additional generated examples We provide more examples on Frame Guidance with color block image in Figure 17. We show examples generated by other models, SVD [3] and LTX-2B [15], are shown in Figure 18. 18 Figure 15: Stylized video generated by Frame Guidance using style loss. These videos are generated by CogVideoX-T2V. Figure 16: Loop video generated by Frame Guidance using loop loss. These videos are generated by Wan-14B T2V. 19 Figure 17: Frame Guidance with color block image allows the generation of video with complex scene. These videos are generated by CogVideoX-I2V. Figure 18: Frame Guidance is model-agnostic. It is compatible with both SVD [3] and LTX-2B [15]. For SVD, since it does not use temporally compressed VAE, we skip latent slicing. Some saturation observed in the LTX-2B results occasionally occurs due to the model itself. 20 Figure 19: Video reconstruction with temporally sliced latent. (a) Decoding the full latent sequence successfully reconstructs the original video. (b)(c) Using 4 or 3-length latent around the target latent (frame) is sufficient for accurate reconstruction. (d) With only 2-length latent, there is slight degradation, therefore, we adpot 3-length latent for the main experiments."
        },
        {
            "title": "C More discussions",
            "content": "C.1 Video reconstruction with sliced latent As shown in Figure 19, we can reconstruct nearly identical frames even with temporally sliced latents. After encoding the dynamic original video using CogVideoXs CausalVAE, decoding with only 3-length latent sequence yields frames that closely match the original (Figure 19(c)). C.2 Time-travel trick in layout stage Table 4: Forward process coefficients in early inference steps. As discussed in Section 4.2, directly applying the timetravel trick [2, 37, 50] to video diffusion models struggles due to excessive stochasticity. The time-travel trick in Algorithm 2 includes single-step forward process, but in practice, the added noise is extremely large, and the coefficient multiplied with the latent is very small, as shown in Table 4. In fact, in the very first inference step, the coeffiβt becomes 0, resulting in no guidance effect at all. cient Therefore, since the effect of guidance is absent during the early stages when the layout is largely established, the model fails to produce layout that aligns with the given condition. Even when guidance is applied later, as discussed in Figure 4(d), only the guided frame is updated, and it cannot correct the overall layout. Our proposed VLO addresses this issue by applying deterministic latent optimization in the layout stage. 1 βt 1.00 0.88 0. βt 0.00 0.48 0.64 Step (/50) 1 2 3 21 Algorithm 2 Time Travel Require: zt, z0t, t, gt 1: ϵ (0, I) 2: zt1 DDIM(zt, z0t) 3: zt1 zt1 η gt 4: βt αt/αt1 5: zt βtzt + 6: return zt 1 βtϵ Renoising Algorithm 3 Time Travel-F (flow matching) Require: zt, z0t, t, gt 1: ϵ (0, I) 2: z0t z0t η gt 3: zt σtϵ + (1 σt)z0t 4: return zt Renoising Algorithm 4 Frame Guidance (flow matching) Require: I, tL, tD, repeat step , step size η, guidance loss Le, model vθ(, ) Guidance step 1: zT (0, I) 2: Frame-Idx-to-Latent-Idx(I) 3: for = T, ..., 1 do 4: 5: 6: 7: for = 1, ..., 1 do if > tD then z0t zt σt vθ(zt, t) zJ 0t Latent-Slicing(z0t, ) 0t D(zJ xI 0t) gt = ztLe(xI if > tL then 0t, cframes) 8: zt zt η gt Layout stage Detail stage zt Time-Travel-F(z0t, gt) 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for 19: return z0 else end if end for end if zt1 zt + (σt1 σt) vθ(zt, t) Figure 20: Shortcut-based approaches [17, 28, 35] lead to temporal disconnects in video generation. C.3 Video latent optimization (VLO) for flow matching As noted in Section A.2, we follow the time convention of diffusion models by reversing the flow matching time axis, aligning = 0 with clean data and = with pure noise. In Algorithm 4, we extend our Frame Guidance to video generation models, which employ the flow matching [25] for their generative modeling (e.g., Wan [42] and LTX [15]). Similar to the diffusion case in Equation 2, we apply the latent slicing (Lines 7) and optimize the current latent zt through the guidance loss gt (Lines 9-11). Specifically, we predict the clean sample z0t by based on the tweedie-like formula in Equation 4. Time-travel for flow matching However, directly applying the time-travel trick to flow matching is non-trivial, as single forward step (Line 5 in Algorithm 2) is not explicitly defined in the context of flow matching. While renoising in time travel is effective for mitigating accumulated sampling errors, it cannot be directly utilized here. Our deterministic optimization excludes renoising entirely and can be applied as is, but performing it fully during inference, as in diffusion, can result in over-saturated samples or temporally disconnected videos. To address this, we adopt simple alternative: instead of stepping from to t1, we move directly from to 0 (i.e., the estimated clean latent), apply guidance there, and then simulate forward step from 0 back to t. Although single forward step is not defined in flow matching, it is still possible to apply the forward process for time from clean data. While this process introduces higher stochasticity than single diffusion step, applying it in the later stages of VLO, after the layout has already been established, does not significantly disrupt the structure. This makes it viable option. Empirically, this approach enables the application of VLO to flow matching-based models as well. 22 C.4 Importance of gradient propagation via denoising network In training-free guidance for image generation, \"shortcut\" [17, 35] method has been proposed that utilizes proximal gradient approach to bypass back-propagation through the denoising network. This strategy significantly reduces memory usage and enables efficient sampling for gradient-based optimization. While effective for static images, directly applying this method to video generation poses challenges due to the temporal characteristic of video data. Specifically, when guidance is applied to only few frames, the resulting video often becomes temporally inconsistent. As illustrated in Figure 20, the latents corresponding to the guided frames are updated to resemble the target frames, and adjacent frames may also partially align. However, earlier frames remain disconnected, and the guided frames themselves may exhibit unnatural artifacts. This is because temporal priors, crucial for maintaining coherence across frames, are primarily encoded in the denoising network. Consequently, for video generation tasks where temporal consistency is critical, gradient propagation through the denoising network is essential."
        },
        {
            "title": "D Limitations and societal impacts",
            "content": "Limitations Although Frame Guidance is training-free and supports various applications, it has some limitations: (1) The computational cost of guidance sampling is higher than that of training-based methods. Since it requires back-propagation and multiple predictions, the inference speed is approximately up two to four times slower than that of the base model, depending on the task. This issue is particularly significant in video generation, which is computationally intensive. We leave addressing this inefficiency to future work. (2) While our method is model-agnostic, it is heavily dependent on the performance of the base model. Since our approach samples videos that align with given conditions within the generation distribution of the base model, it struggles to generate videos that are either too dynamic or contain fine-detailed objects the model has not encountered during training. Societal impacts The proposed method has potential benefits in areas such as animation, education, and creative content generation. However, there is also risk of misuse, particularly in the creation of deceptive or harmful content such as deepfakes. To address these concerns, it is important to incorporate safeguards such as watermarking and to advance detection methods that can identify manipulated or synthetically generated videos."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "DeepAuto.ai",
        "KAIST",
        "UNC Chapel Hill"
    ]
}