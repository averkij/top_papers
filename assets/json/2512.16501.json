{
    "paper_title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
    "authors": [
        "Beitong Zhou",
        "Zhexiao Huang",
        "Yuan Guo",
        "Zhangxuan Gu",
        "Tianyu Xia",
        "Zichen Luo",
        "Fei Tang",
        "Dehan Kong",
        "Yanyi Shang",
        "Suling Ou",
        "Zhenlin Guo",
        "Changhua Meng",
        "Shuheng Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks."
        },
        {
            "title": "Start",
            "content": "VenusBench-GD: Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks Beitong Zhou1,, Zhexiao Huang1,, Yuan Guo1,, Zhangxuan Gu1,, Tianyu Xia1, Zichen Luo1, Fei Tang1 Dehan Kong2, Yanyi Shang2, Suling Ou1, Zhenlin Guo1, Changhua Meng1, Shuheng Shen1, 1Venus Team, AntGroup 2iMean AI https://ui-venus.github.io/VenusBench-GD 5 2 0 D 8 1 ] . [ 1 1 0 5 6 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "GUI grounding is critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUIspecialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks. 1. Introduction The rapid evolution of Multimodal Large Language Models (MLLMs) [2, 17, 24, 27] in Graphical User Interface (GUI) has necessitated robust and comprehensive benchmarks to evaluate their agentic capabilities. As foundational and *Equal contribution. Corresponding author (shuheng.ssh@antgroup.com). Figure 1. The mode performance of representative GUI grounding models. Notably, model performance on advanced grounding tasks are significantly lower than on basic tasks, highlighting the increased difficulty and reasoning demands of the former. critical capability for GUI agents, GUI grounding refers to the task of aligning natural language instructions with corresponding user interface (UI) elements in GUI screenshot or layout. The accurate grounding is prerequisite for subsequent actions such as clicking, typing, or navigation and has drawn significant attention from researchers. As diverse efforts are ongoing in proposing innovative GUI grounding models [4, 9, 13, 14, 23, 37], there are also bunch of grounding benchmarks [12, 1820, 26, 29, 32, 34] to evaluate their effectiveness in real-world applications. However, we argue that existing GUI grounding benchmarks suffer from several fundamental limitations that hinder their effectiveness in advancing real-world GUI agents. First, early benchmarks like ScreenSpot-V2 [32] are limited in scale and diversity of annotated UI elements, leading to nearly 95% accuracy by current methods [13, 37]. This results in performance saturation, making it hard to distinguish truly capable models from those overfitting to specific distributions. Second, recent benchmarks [18, 19, 34] introduce specialized tasks requiring domain-specific knowl1 Figure 2. The overview of VenusBench-GD benchmark. VenusBench-GD integrates basic and advanced grounding tasks to comprehensively evaluation the capabilities of existing GUI models as shown above. Basic tasks assess the ability to recognize local UI elements, while advanced tasks require holistic reasoning over the entire interface and its underlying application functionality, demanding more complex and global understanding. edge (e.g., precise developer terminology or usage of professional applications), which, though technically challenging, lack ecological validity and fail to reflect the generalization needed for everyday user tasks. Whats more, narrow and simplistic grounding tasks are overwhelmingly emphasized, primarily limited to locating icons or text elements given different descriptions. In contrast, real-world GUI agent workflows involve far richer and more diverse grounding interactions. The absence of such complex and compositional tasks leads to current evaluations that capture only fraction of the capabilities required for robust GUI understanding and action. To address these pressing challenges, we introduce VenusBench-GD, large-scale, high-quality, and multidimensional benchmark designed to rigorously evaluate the next generation of GUI Agents. VenusBench-GD spans three major platforms: web, mobile and desktop, covering 97 distinct applications, websites and software across 10 diverse domains as shown in Fig. 3. The benchmark comprises 6, 166 human-annotated image-instruction pairs, encompassing 13 fine-grained UI element types-including texts, buttons, sliders, links, and more-making it the largest and most semantically rich element grounding tabs, benchmark to date in terms of both scale and UI element diversity. Crucially, the instructions and screenshots are provided in both English and Chinese, enabling evaluation under multilingual and cross-cultural interaction scenarios to reflect real-world usage. To ensure fidelity, every sample in VenusBench-GD undergoes multiple rounds of verification and localization calibration, combining expert human review with model-assisted consistency checking, resulting in higher label quality compared to existing benchmarks according to our blind quantitative analysis. Most importantly, VenusBench-GD introduces hierarchical evaluation framework consisting of 6 distinct grounding tasks designed to probe models from complementary perspectives. Basic grounding tasks assess the fundamental perception capabilities of UI elements in the screenshot, including Element Grounding, Spatial Grounding and Visual Grounding. Advanced grounding tasks evaluate the higherorder reasoning and the robustness of models in complex scenarios, including Functional Grounding, Reasoning Grounding and Refusal Grounding. This multi-faceted design enables VenusBench-GD to evaluate not only localization accuracy but also semantic comprehension and robustness to ambiguity. In conclusion, VenusBench-GD offers 2 2. Related Work 2.1. GUI Grounding Driven by the remarkable progress of multimodal large language models (MLLMs) [3, 6, 17] in understanding and reasoning over both visual and linguistic inputs, recent years have witnessed rapid advancements in graphical user interface (GUI) agents [1, 10, 13, 15, 21, 25, 30, 36]. Effective GUI agents rely on GUI grounding: the ability to locate and identify UI elements based on natural language commands. This serves as the critical link between highlevel planning and low-level actions, enabling precise interactions with graphical interfaces. Early grounding models relied on supervised fine-tuning using large labeled screenshot datasets [11, 21, 31, 34], while recent approaches shift to reinforcement learning frameworksinspired by DeepSeek-R1that leverage continuous or spatial reward signals to enhance robustness and precision [9, 13, 22, 23]. 2.2. GUI Benchmarks Existing benchmarks suffer from several notable limitations. Some benchmarks are constrained by limited scale, offering insufficient data to support robust model evaluation. The scale of ScreenSpot-V2 [32], while valuable early effort in GUI grounding, is insufficient to support rigorous assessment of current models with only around 1, 200 annotated samples. Others are narrowly scoped to single platform or domain-specific applications, which limits their applicability to broader user interactive scenarios. ScreenSpot-Pro [18] targets professional desktop applications where the cluttered and tool-rich screenshots make grounding hard. UI-Vision [19] offers high-quality dataset in professional software understanding, spatial reasoning and complex agent actions, captured by trained annotators using proprietary tools. OSWorld-G [34] samples from agent rollouts in OSWorld [33] and encompasses comprehensive set of challenges, including layout understanding, fine-grained manipulation, and infeasibility handling, among others. CAGUI [41] is Chinese Android GUI benchmark for both grounding and agent tasks enabling multilingual evaluation beyond English-only datasets. Among existing benchmarks, MMBench-GUI [29] is the closet in scope to our work. However, its element grounding subset comprises only around 3, 500 samples and adopts coarse task taxonomy that merely divides the instructions into basic and advanced categories. This limited granularity constraints its ability to capture fine-grained variations in task complexity. Moreover, they lack annotation quality check and error case analysis, which may affect the objectivity of benchmark validation in assessing GUI grounding capabilities. Figure 3. The domain distribution of our grounding benchmark. VenusBench-GD spans 97 distinct apps, software, and websites across desktop, mobile, and web platforms, ensuring diverse and comprehensive coverage. We consolidate representations of the same software across platforms into one entry for clarity. more generalizable, realistic, and challenging benchmark for next-generation multimodal grounding models. Our experimental results reveals two key findings: 1) general MLLMs now match or even surpass GUIspecialized models on basic grounding tasks, with Qwen3VL-8B achieving the highest average accuracy of 76.96% among models of comparable scale; 2) Advanced grounding, particularly Reasoning and Functional Grounding, which heavily rely on domain-specific GUI knowledge, still present clear advantage for specialized models. However, most current GUI models suffer from overfitting, as evidenced by their poor performance on Refusal Grounding. Our major contributions are threefold: We propose VenusBench-GD, the largest and most diverse GUI element grounding benchmark to date, spanning three platforms, 97 real-world applications across 10 domains and 13 UI element types, enabling robust crossplatform and multilingual grounding evaluation. We establish high-quality annotation pipeline combining human review and model-assisted consistency checks, reducing label noise and improving data reliability over existing benchmarks. We propose novel hierarchical evaluation framework comprising six complementary grounding tasks and comprehensive experimental results indicate that model performance on basic grounding tasks has nearly saturated, necessitating the introduction of more complex tasks to provide new directions for the advancement of GUI grounding models. Environments Quality Analysis Statistics Benchmarks Platform Language IL ScreenSpot-V2 [32] ScreenSpot-Pro [18] OSWorld-G [34] UI-Vision [19] CAGUI [41] Desktop, Web, Mobile Desktop Desktop Desktop Mobile EN EN,CN EN EN CN 9.2% 2.2% 9.2% 6.0% 11.6% AR 0.8% 1.8% 2.6% 2.4% 6.4% SM 0.8% 0.6% 2.4% 2.0% 6.6% VenusBench-GD (ours) Desktop, Web, Mobile EN, CN 1.0% 0.8% 0.8% Overall # Sample Avg. Ele Apps 10.8% 4.6% 14.2% 10.4% 24.6% 2.6% 1,272 1,581 564 5,479* 3,000 6,166 1.74 1.0 2.25 4.64 1. 2.83 N/A 23 8 83 N/A 97 Table 1. Comparison of existing GUI grounding benchmarks. Note that we only count samples related to element grounding tasks for consistency(* means UI-Vision has 8227 queries but only 5479 are about element grounding), as our evaluation focuses on GUI grounding and this may result in lower counts for other benchmarks. 3. VenusBench-GD 3.1. Tasks Design GUI grounding is fundamental capability of GUI Intuitively, it refers to the task of agents [9, 13, 2123]. mapping user-provided natural language instructions to specific boxes or regions within an interface image [18, 32]. As natural extension, the grounding function can also be defined to output point rather than region. Inspired by human interaction with digital interfaces, where users jointly rely on global context to grasp app functionality and local perception to identify target UI elements, we design element grounding tasks in VenusBenchGD with basic grounding focusing on local understanding (e.g., element type, text, appearance, and local spatial relations) and advanced grounding requiring global contextual reasoning and functional comprehension of the application. While prior work [18, 32] has largely addressed basic element grounding tasks about icons or texts, we extend this foundation by enriching the task taxonomy, incorporating visual grounding, and formally define our basic grounding tasks herein. Element Grounding: The model locates UI element based directly on its type (e.g., button, dropdown) or associated text content via OCR. This is the most common task in prior benchmarks and primarily evaluates basic UI comprehension. Spatial Grounding: Building on element grounding, this task requires the model to interpret relative spatial cues, such as left of, below, or in the same row as, to identify the target element, thereby assessing fine-grained localization within local layout contexts. Visual Grounding: The model grounds elements based on visual appearance descriptions, such as shape, color, or icon pattern (e.g., the icon shaped like hot air balloon), testing its ability to perceive visual attributes. This tiered design allows systematic evaluation of visual grounding at different abstraction levels. However, the basic tasks focus only on foundational OCR or visual detection capabilities, missing the advanced reasoning and global screenshot understanding demonstrated by modern GUI agents. To address this gap, we introduce three novel advanced grounding tasks: Reason Grounding, Functional Grounding, and Refusal Grounding, each requiring more complex reasoning abilities. Reasoning Grounding: The model must reason over visible information (e.g., comparing prices, calculating dates) to identify the correct UI element. Unlike direct matching, this task involves intermediate cognitive steps such as numerical or logical inference (e.g., select the cheapest option). Functional Grounding: The target element is specified by its function (e.g., close the video or add to cart), requiring the model to understand common UI affordances. To ensure broad applicability, we focus on generic rather than domain-specific functionalities. Refusal Grounding: The instruction refers to nonexistent UI element, and the model must refrain from predicting bounding box. This task reflects models resistance to overfitting and supports fairer evaluation of GUI grounding capabilities. In conclusion, Figure 2 demonstrates the specific instructions for the types of grounding task using concrete image example, which helps to clarify the hierarchical evaluation in VenusBench-GD. To better explain all six grounding tasks, we show the input prompt as follows while the red sentence refer to the refusal grounding."
        },
        {
            "title": "Grounding Prompt",
            "content": "Output the center point of the position corresponding to the instruction: {}. The output should just be the coordinates of point, in the format [x,y]. Additionally, if you think the task is infeasible (e.g., the task is not related to the image), the output should be [-1,-1]. 3.2. Bottom-up Data Pipeline To construct VenusBench-GD, we designed bottom-up four-stage data creation pipeline combines automation and 4 Grounding Tasks Count Examples Platform Applications Language Images Element Grounding Spatial Grounding 1029 Locate the text Liu Chun. Find the Image menu item. Find the user settings icon in the lower-left corner. Visual Grounding Click the heart-shaped icon located in the bottom of the image gallery. Reasoning Grounding 1107 Functional Grounding 700 Refusal Grounding 900 Book ticket with the third-highest price and the latest departure time. Quickly enable the devices GPS positioning function Find the blue circular icon in the upper left right corner. Table 2. Examples of each grounding task and the key features are highlighted. Figure 4. Examples of inaccurate annotations in existing benchmarks. human expertise. We first gathered raw interface screenshot data and then used detectors to get various UI elements. Building upon these candidate elements, we implemented hierarchical instruction generation with MLLMs to produce task-aligned prompts for both basic and advanced grounding scenarios. Finally, we employed multi-stage filtering to select only the highest-quality and non-trivial samples. This intensive process ensures exceptional data richness, semantic diversity, and annotation fidelity. Raw Data Collection: We first collected 97 distinct applications and websites from Chinese and English com5 Web Mobile Desktop 40 21 English Chinese English Chinese English Chinese 240 296 704 466 257 Table 3. Distribution of our collected images, languages, and applications across platforms in VenusBench-GD. munities spanning 10 domains as shown in Figure 3, including creative, utility, development, and others. Notably, versions of the same application across different platforms (e.g., Reddit on web vs. mobile) are counted as separate applications. full list of applications is provided in the appendix. UI Element Localization: To obtain large and semantically diverse set of high-quality UI element annotations, we first defined 13 common UI element types and employed two complementary annotation strategies: (1) generating coarse bounding boxes using detectors like APT [12], LayoutLM [35], and OmniParserV2 [39], followed by deduplication and human verification of localization accuracy; and (2) having annotators explicitly label long-tail, modelchallenging elements (e.g., checkboxes, sliders). In both cases, annotators were instructed to tightly align the boxes with the interactive regions of the elements. Each box underwent at least two rounds of review, with the second round requiring 95% inter-annotator accuracy. This intensive protocol yields at least 20 high-precision candidate element annotations per image. Instruction Generation: For basic grounding tasks, we utilize Qwen2.5-VL-72B model [3] to generate instructions based on collected UI element candidates, conditioned on element type, text, position, and visual appearance, and have annotators verify the instructionelement alignment, assigning valid pairs to their corresponding basic grounding subtasks. Instruction generation for advanced grounding tasks is more intricate. For Refusal Grounding, we deliberately perturb matched instructionelement pairs, modifying element type, text, spatial references, or visual descriptions, to ensure that the revised instruction has no valid referent in the screenshot, thereby maximizing ambiguity for testing model robustness. For Reasoning Grounding, we focus on application domains that commonly involve numerical comparison, date filtering, or item managementsuch as e-commerce, travel booking, and finance. Within these domains, we develop question templates and instruct annotators to generate instructions and corresponding ground truth elements only when valid reasoning scenario exists. For Functional Grounding in advanced tasks, we used elements from the element candidates and provided both pre-click and after-click screenshots to Qwen2.5-VL-72B for generating potential functions or user intentions from these sequential images and convert them into textual instructions for further human verification. Post Selection: Although multiple rounds of screening and verification have been applied, large pool of candidate instructions remains. We therefore leverage state-of-theart(SOTA) GUI grounding models and aggregate their predictions to filter out overly simple samples and further eliminate ambiguous ones. Taking basic grounding tasks as an example, we generated approximately 28, 000 annotated UI elements across 2, 000 images. After model-based instruction generation and rigorous filtering, only 3, 459 highquality samples were retained. 3.3. Data Statistics Scale Statistics: As shown in Table 1, we compare existing benchmarks across different environments and scale statistics. VenusBench encompasses all major platforms and features the largest scale to date, with 6,166 annotated samples and the broadest application coverage among existing GUI grounding benchmarks. Diversity Statistics: Another important factor in building an effective benchmark is the diversity of samples. In this section, we calculate the main metrics including maximum, minimum, and average based on three views, i.e., screen resolutions, element sizes, and instructions, as shown in Table 4. Statistical analysis of VenusBench-GD compared to prominent existing benchmarks reveals several critical advantages in design diversity and challenge level. First, while possessing 65 distinct screen sizes, VenusBench-GD exhibits by far the largest screen resolution range, providing significantly more rigorous test for model robustness to input resolution scaling. Second, VenusBench-GD, alongside OSWorld-G, demonstrates the most substantial variation in the relative area (w.r.t. the entire screen) of annotated UI elements. This characteristic inherently increases the difficulty of multi-scale element recognition. Third, VenusBench-GD features the longest average instruction length among all six evaluated benchmarks. The statistics serve as strong proxy for the richness and semantic complexity of the grounding tasks within our benchmark, reflecting its higher linguistic diversity. 3.4. Data Quality Analysis Qualitative Analysis: Despite the quality check by annotators are made on existing GUI grounding benchmarks, some inaccurate annotations still exist according to our observation, which will harm the performance evaluation of various recent GUI Agents. To better explain the inaccurate, we divide common mismatches from recent GUI grounding benchmarks into three folds. When determining the optimal bounding box for UI element during human verification, we annotate all plausible corresponding regions to capture its full spatial extent. Inaccurate Localization(IL): The annotated bounding box partially covers the correct UI element - either too large or too small. If the misaligned region (as visually estimated by annotators) exceeds roughly 40% of the elements true area, the localization is considered inaccurate. Ambiguous Reference(AR): The instruction could reasonably refer to multiple valid UI elements, but only one is labeled as the ground truth. This introduces ambiguity, as other equally plausible targets are ignored. Semantic Mismatch(SM): The annotated box does not overlap with any UI element that matches the instructions semanticsindicating complete misalignment between language and visual grounding. Specifically, as illustrated in Figure 4 a), two Inaccurate Localization sampled cases reveal ground truth annotations that either encompass extraneous icons or exhibit systematic spatial offset. Another typical problem is Ambiguous Reference e.g., as two cases indicated in Figure 4 b). Finally, Figure 4 c) introduces Semantic Mismatch, where the predicted element does not semantically correspond to the intent of the instruction. Blind Quantitative Analysis: Beyond qualitative insights, we also performed quantitative statistical analysis of the benchmark qualities in Table 1. To ensure fair comparison of annotation quality across benchmarks, we randomly sampled 500 instances from each of six closely related benchmarks (totaling 3, 000 samples), pooled them into unified set, and assigned them to annotatorswho had been trained on small calibration subsetwithout revealing the source benchmark of each sample. Based on this test, we can make the following statements: Inaccurate Localization constitutes the predominant source of inaccurate samples across all benchmarks, accounting for the highest proportion of mislabeled instances in each evaluated dataset. VenusBench-GD achieves the lowest error rates (2.6%) across all three types of inaccurate cases, with particularly significant improvement in IL: reducing the error rate from 11.6% (observed in CAGUI) to 1.0%. This demonstrates the superior accuracy of our grounding benchmark and the fairness of GUI Agent evaluation. Consistent with our findings, UI-Ins [5] reports that approximately 23.3% of samples in existing grounding datasets suffer from annotation issues, mutually corroborating the reliability of our quality analysis. 4. Experiments 4.1. Experiment Settings We evaluated comprehensive set of both open-source and closed-source vision-language models (VLMs) and GUI"
        },
        {
            "title": "Benchmark",
            "content": "#types max / min min / max characters min / max / avg words min / max / avg"
        },
        {
            "title": "Instruction",
            "content": "ScreenSpot-V2 ScreenSpot-Pro OSWorld-G UI-Vision VenusBench-GD 94 14 3 100 65 1.8e4 / 7.2e1 2880 1800 / 362 313 6016 3384 / 1920 1080 1.7e5 / 4.7e2 1920 1080 / 1280 720 5.8e6 / 7.9e1 3360 2036 / 1092 614 5120 2880 / 320 242 0 / 3.1e2 2.7e5/ 3.5e1 4 / 87 / 23.1 2 / 102 / 26.2 3 / 112 / 36.3 3 / 173 / 49.6 4 / 218 / 57.1 1 / 16 / 4.1 1 / 16 / 4.6 1 / 19 / 6.4 1 / 34 / 8.25 1 / 34 / 10. Table 4. Distributions and diversities of existing GUI grounding benchmarks viewed on Resolution, Element size and Instruction. agents with demonstrated capabilities. We benchmark both closed-source and open-source models. Among opensource models, we focus on open-source VLMs and two categories of GUI-specific models: (1) GUI-specific Models (SFT) and (2) GUI-specific Models (RL). For each model, we follow the recommended prompting format to ensure fair and optimal performance evaluation; the specific prompts used are detailed in the Appendix. Consistent with previous works [7, 18, 34], we define prediction as correct when the predicted point lies within the ground-truth bounding box and report the accuracy averaged over all UI elements. For refusal grounding tasks, we consider models output correct only when it adheres strictly to the prescribed refusal format, such as returning coordinates [1, 1] or designated rejection phrase as specified in the prompt. 4.2. Benchmark Results As shown in Table 5, the results reveals insights that are difficult to obtain from existing benchmarks: Remark 1 General open-source VLMs now match or even surpass specialized GUI models on basic grounding tasks. Due to the incorporation of extensive GUI-related data during pre-training and instruction tuning, state-of-the-art general vision-language models, particularly the Qwen3VL series, consistently achieve accuracy above 70% on basic grounding tasks in average accuracy, outperforming most GUI-specialized models trained via supervised finetuning (SFT) or reinforcement learning (RL). This suggests that performance on basic grounding tasks is approaching saturation, and that general models, benefiting from broader world knowledge, hold distinct advantage in interpreting spatial relations, visual appearance, and OCR content. Consequently, basic tasks alone are no longer sufficient to meaningfully differentiate between general and specialized models, underscoring the need for more nuanced evaluation, precisely the motivation behind VenusBench-GDs inclusion of advanced tasks. Remark 2 Advanced grounding tasks enable more comprehensive and multidimensional assessment of GUI grounding models. On UI-intensive reasoning tasks, specifically Functional and Reasoning Grounding, GUI specialized models retain clear edge: Holo1.5-72B and UI-Venus-Ground-72B achieve over 40% and 68% accuracy, respectively, significantly outperforming general VLMs. However, on Refusal Grounding task, most specialized models exhibit almost no ability to abstain from responding to invalid queries; only UI-Venus-Ground-72B attains modest 51.33% accuracy. This stark contrast highlights critical limitation: current GUI-specialized models are prone to overfitting, lacking the robustness and generalization needed to handle out-ofdistribution or unanswerable instructions. 4.3. Analysis Model Thinking: On ScreenSpot-Pro and OSWorld-G, Qwen3-VL-8B-Thinking [24] achieves 46.6%/56.7%, underperforming Qwen3-VL-8B-Instruct (54.6%/58.2%), trend consistent with prior findings [13] that explicit reasoning can degrade performance on standard GUI grounding tasks. This motivates our focused comparison between the two variants to investigate whether thinking capabilities are better suited for advanced grounding tasks, which demand complex reasoning, rather than basic grounding scenarios. As shown in Figure 5, the thinking-enabled model can effectively analyze the overall interface and use reasoning ability to select the most appropriate click location, indicating that additional reasoning improves performance only in more complex scenarios. Multilingual Evaluation: comparative analysis of the models performance on basic tasks under English and Chinese instruction revealed that the model typically scored higher in the Chinese condition, while its performance on advanced tasks fluctuated, possibly due to instruction ambiguity. As shown in Table 6, Qwen3-VL-4B demonstrates notable improvement in basic understanding (from 72.54 to 81.32), suggesting better alignment with Chinese spatial and semantic expressions in simple localization tasks. More detailed results can be found in the supplementary material. 5. Conclusion In this paper, we introduced VenusBench-GD, large-scale, high-fidelity benchmark spanning web, mobile, and desk7 Models Closed-source Models GPT-4o [17] Gemini 2.5 Pro [8] Claude-Sonnet-4.0 [2] General Open-source VLMs Qwen2.5-VL-7B [3] Qwen2.5-VL-72B [3] Qwen3-VL-4B [24] Qwen3-VL-8B [24] Qwen3-VL-30B-A3B [24] InternVL2.5-8B [6] InternVL3.5-8B [27] InternVL3.5-30B-A3B [27] MiniCPM-V4.5-8B [38] GUI-specific Models (SFT) UI-TARS-7B [21] UI-TARS-72B [21] UGround-7B [11] JEDI-7B [34] OS-Atlas-7B [32] Aguvis-7B [36] CogAgent-9B [16] OpenCUA-7B [28] OpenCUA-32B [28] GUI-specific Models (RL) SE-GUI-7B [40] GUI-G2-7B [23] GTA1-7B [37] GTA1-32B [37] UI-TARS-1.5-7B [22] UI-Venus-Ground-7B [13] UI-Venus-Ground-72B [13] Holo1.5-7B [9] Holo1.5-72B [9] Basic Tasks Advanced Tasks Overall Element Visual Spatial Avg Reasoning Functional Refusal Avg 4.53 8.99 12. 58.14 71.78 69.77 73.48 76.24 13.76 52.48 59.21 22.31 54.62 59.65 55.06 51.54 42.68 34.76 21.06 62.23 65.49 60.15 68.07 63.73 75.36 56.57 64.30 81.58 62.98 81.77 10.61 13.59 37.78 61.26 86.77 78.78 83.31 86.53 18.24 57.93 59.59 20.62 67.58 79.14 68.06 60.55 35.64 36.00 24.67 84.39 78. 68.53 80.33 76.64 88.08 71.16 78.78 91.30 80.21 92.01 6.61 11.47 14.58 53.06 73.18 71.72 77.16 77.45 14.48 53.84 58.02 11.95 55.78 68.32 61.03 55.10 34.11 34.69 24.49 67.44 68.80 59.28 71.33 57.05 76.77 53.16 67.15 78.81 74.34 84.84 6.62 10.84 19. 57.39 75.83 72.54 76.96 79.10 15.06 54.21 58.95 18.82 58.11 66.96 59.99 54.78 38.42 35.04 22.95 69.15 69.64 61.93 72.02 64.87 78.87 59.09 68.66 83.12 70.54 85.17 2.98 6.50 11.11 14.72 31.44 25.38 24.75 30.17 7.05 23.40 22.76 1.72 17.98 32.16 13.37 31.04 11.74 11.92 7.68 21.32 29. 14.36 23.49 23.31 38.84 17.89 24.39 46.16 28.91 40.38 4.14 10.00 11.71 32.00 50.14 55.43 47.71 61.86 11.71 41.71 45.57 7.29 33.71 47.00 38.86 54.00 25.86 30.57 31.00 49.14 51.00 45.14 51.43 51.14 67.14 37.57 53.85 68.86 55.00 69.43 43.00 40.67 30. 18.11 78.11 31.55 63.78 25.56 0.33 16.67 16.44 16.78 0.00 0.00 0.00 11.00 5.44 0.00 0.00 0.00 0.00 10.56 0.00 0.00 0.00 0.22 0.00 51.33 0.00 0.00 16.59 18.77 17.65 20.32 51.79 35.20 43.66 36.83 6.02 25.89 26.56 8.17 16.07 25.31 15.51 30.31 13.29 12.78 11.16 21.43 25. 21.06 22.90 22.75 33.25 17.10 23.90 53.75 26.04 34.47 10.99 14.32 18.45 41.12 65.28 56.15 62.34 60.54 11.09 41.78 44.73 14.14 39.65 48.67 40.46 44.04 27.38 25.27 17.77 48.20 50.08 43.98 50.46 46.38 58.84 40.65 49.01 70.23 51.00 62.91 Table 5. Performance comparison on VenusBench-GD dataset categorized by the evaluation tasks. In the above table, within each group, the best-performing model on each task is highlighted in bold, while the second-best is underlined."
        },
        {
            "title": "Overall",
            "content": "Qwen3-VL-4B 72.54 / 81.32 35.20 / 34.94 56.15 / 60.95 70.54 / 78.69 26.04 / 27.81 51.00 / 56.35 Holo1.5-7B UI-Venus-Ground-72B 83.12 / 86.24 53.75 / 51.93 70.23 / 71.18 Table 6. Performance comparison of representative models in two tasks under English (left) and Chinese (right) instruction. top platforms with 97 real-world applications across 10 domains. By incorporating 6 hierarchically designed grounding tasks (from basic localization to complex reasoning/refusal scenarios), VenusBench-GD establishes rigorous multi-dimensional evaluation framework. Combined with human-AI collaborative annotation pipeline ensuring minimal label noise, our benchmark achieves superior data quality and cross-lingual relevance, which enables meaningful differentiation and valid evaluation between state-of-the-art GUI agents. 8 Figure 5. Thinking-enabled model makes the correct grounding action with detailed analysis of the whole screenshot."
        },
        {
            "title": "References",
            "content": "[1] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. ArXiv, abs/2410.08164, 2024. 3 [2] Anthropic. System card: Claude opus 4 & claude sonnet 4. Technical report, Anthropic, 2025. 1, 8, 5 [3] Shuai Bai, Keqin Chen, and et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 5, 8 [4] Jikai Chen, Long Chen, Dong Wang, Leilei Gan, Chenyi Zhuang, and Jinjie Gu. V2p: From background suppression to center peaking for robust gui grounding task. ArXiv, abs/2508.13634, 2025. [5] Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, Yue Wang, Qin Jin, and Steven Hoi. Ui-ins: Enhancing gui grounding with multiperspective instruction-as-reasoning. 2025. 6 [6] Zhe Chen, Weiyun Wang, and et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. ArXiv, abs/2412.05271, 2024. 3, 8, 5 [7] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. In Annual Meeting of the Association for Computational Linguistics, 2024. 7 [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. ArXiv, abs/2507.06261, 2025. 8, 5 [9] Company. Holo1.5 - open foundation models for computer use agents, 2025. 1, 3, 4, 8, [10] Droidrun. Droidrun. github.com/droidrun/droidrun, 2025. 3 [11] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. ArXiv, abs/2410.05243, 2024. 3, 8, 5 [12] Zhangxuan Gu, Zhuoer Xu, Haoxing Chen, Jun Lan, Changhua Meng, and Weiqiang Wang. Mobile user interface element detection via adaptively prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1115511164, 2023. 1, 5 [13] Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, and Weiqiang Wang. Ui-venus technical report: Building high-performance ui agents with rft. ArXiv, abs/2508.10833, 2025. 1, 3, 4, 7, 8, 5 [14] Kaiwen He, Zhiwei Wang, Chenyi Zhuang, and Jinjie Gu. Recon-act: self-evolving multi-agent browser-use system via web reconnaissance, tool generation, and task execution. ArXiv, abs/2509.21072, 2025. 1 [15] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for gui agents. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1428114290, 2023. 3 [16] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14281 14290, 2024. 8, [17] OpenAI Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mkadry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, et al. Gpt-4o system card. ArXiv, abs/2410.21276, 2024. 1, 3, 8, 5 [18] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional highresolution computer use. ArXiv, abs/2504.07981, 2025. 1, 3, 4, 7, 2 [19] Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan A. Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M. Tamer Ozsu, Aishwarya Agrawal, David Vazquez, Christopher Pal, Perouz Taslakian, Spandana Gella, Sai Rajeswar, and Human Annotator. Ui-vision: desktop-centric gui benchmark for visual perception and interaction. ArXiv, abs/2503.15661, 2025. 1, 3, 4, 2 [20] Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, and Zhengyang Wu. Webcanvas: Benchmarking web agents in online environments. ArXiv, abs/2406.12373, 2024. 1 [21] Yujia Qin, Yining Ye, and et al. Ui-tars: Pioneering automated gui interaction with native agents. ArXiv, abs/2501.12326, 2025. 3, 4, 8, [22] ByteDance Seed. Ui-tars-1.5. seed-tars.com/1.5, 2025. 3, 8, 5 [23] Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, and Yueting Zhuang. Gui-g2: Gaussian reward modeling for gui grounding. ArXiv, abs/2507.15846, 2025. 1, 3, 4, 8, 5 [24] Qwen Team. Qwen3 technical report, 2025. 1, 7, 8, 4, 5 [25] Junyang Wang, Haiyang Xu, Jiabo Ye, Mingshi Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobileagent: Autonomous multi-modal mobile device agent with visual perception. ArXiv, abs/2401.16158, 2024. 3 [26] Ke Wang, Tianyu Xia, Zhangxuan Gu, Yi Zhao, Shuheng Shen, Changhua Meng, Weiqiang Wang, and Ke Xu. E-ant: large-scale dataset for efficient automatic gui navigation. arXiv preprint arXiv:2406.14250, 2024. 1 [41] Zhong Zhang, Ya-Ting Lu, and et al. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning. ArXiv, abs/2506.01391, 2025. 3, 4, 2 [27] Weiyun Wang, Zhangwei Gao, and et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 1, 8, 5 [28] Xinyuan Wang, Bowen Wang, and et al. Opencua: Open foundations for computer-use agents, 2025. 8, 5 [29] Xuehui Wang, Zhenyu Wu, and et al. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents. ArXiv, abs/2507.19478, 2025. 1, 3, 2 [30] Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Mingshi Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobileagent-e: Self-evolving mobile assistant for complex tasks. ArXiv, abs/2501.11733, 2025. [31] Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, Si Qin, Lars Liden, Qingwei Lin, Huan Zhang, Tongxing Zhang, Jianbing Zhang, Dongmei Zhang, and Jianfeng Gao. Gui-actor: Coordinate-free visual grounding for gui agents. ArXiv, abs/2506.03143, 2025. 3 [32] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. Os-atlas: founArXiv, dation action model for generalist gui agents. abs/2410.23218, 2024. 1, 3, 4, 8, 2, 5 [33] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. ArXiv, abs/2404.07972, 2024. 3 [34] Tianbao Xie, Jiaqi Deng, and et al. Scaling computer-use grounding via user interface decomposition and synthesis. ArXiv, abs/2505.13227, 2025. 1, 3, 4, 7, 8, 2, 5 [35] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout In Proceedings of the for document image understanding. 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 11921200, 2020. 5 [36] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. ArXiv, abs/2412.04454, 2024. 3, 8, 5 [37] Yan Yang, Dongxu Li, and et al. Gta1: Gui test-time scaling agent. ArXiv, abs/2507.05791, 2025. 1, 8, [38] Tianyu Yu, Zefan Wang, and et al. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe, 2025. 8, 5 [39] Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, and Xiang Bai. Omniparser v2: Structured-points-of-thought for unified visual text parsing and its generality to multimodal large language models. arXiv preprint arXiv:2502.16161, 2025. 5 [40] Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, and Bo Li. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. ArXiv, abs/2505.12370, 2025. 8, 5 10 VenusBench-GD: Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Data Privacy All screenshots used in this benchmark were collected from freshly created, synthetic user accounts specifically set up for data annotation purposes. Any user-facing contents, such as names, messages, or profile details, were manually fabricated and do not correspond to any real individual or entity. Consequently, VenusBench-GD contains no personally identifiable information and poses no privacy or compliance risks. Furthermore, all application interfaces were accessed and captured in accordance with the terms of service of the respective platforms for non-commercial research use. This controlled data generation protocol ensures that our benchmark adheres to ethical standards for responsible AI dataset publication. 7. Dataset Statistics Application Distribution: We collect applications from three distinct platforms, spanning 10 diverse domains: Creative, Deployment, E-commerce, Entertainment, Finance, Knowledge, Productivity, Social, Travel, and Utility. This broad coverage ensures rich and representative application distribution, as detailed in Table 7. Benchmark Statistics: We show the distributions of image resolutions, UI element sizes, and instruction lengths in VenusBench-GD as shown in Figure 6. By design, VenusBench-GD ensures high diversity across these dimensions, reflecting its commitment to comprehensive and representative benchmarking of GUI grounding capabilities. Additional Benchmark Comparisons: We additionally include the MMBench-GUI [29] benchmark and perform statistical analysis of its annotation quality to evaluate labeling consistency and reliability as shown in Table 8. 8. Benchmark Tasks Dense Elements and Accurate Localization: We present the 13 distinct UI element types annotated in VenusBenchGD shown in Table 9, along with representative visual examples. To reduce annotation complexity, we merged semantically similar and commonly co-occurring elements, such as notification and dialog, or checkbox and radiobox, into unified categories. Our benchmark construction follows dense annotation protocol: each screenshot is exhaustively labeled with approximately 20 UI elements, ensuring broad coverage of UI categories across the dataset. Annotators were instructed Domain Creative Development E-commerce Entertainment Finance Applications AE3, Canva3, Figma3, Illustrator3, Pixso3, Sketch3 Anaconda3, CLion3, DockerMac3, Eclipse3, Github1, GithubDesktop3, IntelliJIDEA3, Pycharm3, Vscode3 Amazon1, Eaby1, Ebay1,2, Eleme2, Jd1,2, Meituan2, Pdd2, Taobao1,2, Yelp2 NeteaseMusic1,2, PodcastPlayer2, Spotify1,2, Steam1, Youtube1,2 Alipay2, Bloomberg1, GoogleWallet2, TradingView1, WechatPay2, YahooFinance1,2 Knowledge Obsidian3, Qidian2, Wikipedia1,2 Productivity Social Travel Utility AsanaCh1, AsanaEn1, Coze1, Deepseek1, Evernote3, Excel2, Jira1, Notion3, Openai1, PowerPoint3, Rednote1,2, Trello1, Typora3, Word3, Yuque3 Bilibili1,2, Douyin2, HackerNews1, Instagram2, QQNews2, Reddit1,2, Toutiao2, Wechat2, Weibo1,2, Zhihu1 Airbnb1,2, Amap1,2, Booking1, Ctrip1,2, Didi2, Fliggy2, GoogleMaps2, GoogleMapsCh1, GoogleMapsEn1, Skyscanner1, Uber2 Alarm2, SystemEn2, SystemZh2, TodoList2, Weather2, Windy2 Table 7. list of all domains defined in VenusBench-GD, along with the corresponding applications and their associated platforms. Superscripts 1, 2, and 3 denote applications on web, mobile, and desktop platforms, respectively. to delineate bounding boxes that closely encompass the full interactive region of each element, prioritizing tight alignment and completeness. The annotation process underwent three iterative rounds of refinement and quality control. Inter-round validation yielded steadily improving accuracy rates of 84.54%, 95.47%, and 98.25%, respectively, demonstrating the high fidelity and reliability of the final annotations. The validation accuracy was determined by independent reviewers who manually inspected randomly sampled subset of approximately 15% of the annotated data. Basic Grounding Tasks: For basic grounding tasks, we generate instructions by leveraging the element category, textual content, visual appearance, and relative spatial context of the target UI element. Specifically, we employ the Qwen2.5-VL-72B model, guided by carefully designed prompt that explicitly specifies our generation requirements and includes diverse exemplars covering various instruction types. The input images are augmented with red rectan1 Figure 6. The dataset statistics of VenusBench-GD. a) Distribution of image resolution sizes. For clarity in visualization, we report only the top 7 most frequent image resolutions observed in the benchmark; all remaining resolutions are aggregated into the Others category for statistical completeness. b) Distribution of the element size relative to image size. c) Distribution of the instruction length. Environments Quality Analysis Statistics Benchmarks Platform Language IL ScreenSpot-V2 [32] ScreenSpot-Pro [18] OSWorld-G [34] UI-Vision [19] CAGUI [41] MMBench-GUI [29] Desktop, Web, Mobile Desktop Desktop Desktop Mobile Desktop, Web, Mobile EN EN,CN EN EN CN EN 9.2% 2.2% 9.2% 6.0% 11.6% 3.2% AR 0.8% 1.8% 2.6% 2.4% 6.4% 3.0% SM 0.8% 0.6% 2.4% 2.0% 6.6% 2.6% VenusBench-GD (ours) Desktop, Web, Mobile EN, CN 1.0% 0.8% 0.8% Overall # Sample Avg. Ele Apps 10.8% 4.6% 14.2% 10.4% 24.6% 8.8% 2.6% 1,272 1,581 564 5,479* 3,000 3574 6,166 1.74 1.0 2.25 4.64 1.0 2.77 2.83 N/A 23 8 83 N/A N/A Table 8. Comparison of existing GUI grounding benchmarks. Element Type Visualization Element Type Visualization icon button text/tag menu slider dropdown image text input link notification/dialog checkbox/radiobox toggle switch tab Table 9. The 13 UI element types used in our dataset, each accompanied by representative visualization. Two elements are shown per row for compact presentation with improved spacing and alignment. For visualization, we selected subset of annotated UI elements and expanded their bounding boxes by 20 pixels outward to enhance visibility. gular bounding box highlighting the target element. After generation, each instructionUI element pair is manually validated by human annotators for semantic and referential correctness. Furthermore, all generated instructions are categorized into three grounding dimensions: element grounding, spatial grounding, and visual grounding, enabling fine-grained analysis of model capabilities across distinct grounding modalities. We show examples of element grounding tasks in Figure 7. Advanced grounding tasks exhibit greater complexity than basic grounding tasks in both task difficulty and structural variation. To accommodate the diverse cognitive and perceptual demands of these tasks, we employ distinct construction pipelines tailored to each task type: Reasoning Grounding: We pre-selected set of domains particularly well-suited for reasoning-oriented grounding tasks, namely E-commerce, Travel, Finance, Productivity, and Social, and developed domain-specific instruction templates that encapsulate the characteristic user intents of each scenario as shown in Table 10. Annotators were required to select representative screenshots from these domains and adapt the provided templates to formulate natural, contextually grounded instructions, along with precise bounding box annotations for the target UI elements. This template-driven annotation protocol ensures that the result2 Figure 7. Examples of element grounding tasks, illustrating both correct and incorrect matches between generated instructions and their corresponding annotated bounding boxes ing reasoning tasks exhibit both sufficient cognitive complexity and high referential accuracy. Refusal Grounding: For refusal grounding, we deliberately modify existing instructionUI element pairs by altering key attributes in the instruction, such as spatial references, visual styles, textual content, or the specified UI element type. The revised instruction no longer corresponds to any valid element in the given screenshot and this controlled perturbation ensures that the resulting refusal instructions are semantically plausible yet ungroundable, thereby introducing sufficient ambiguity and perceptual confusion. Functional Grounding: To generate accurate instruction, we provided both pre-click and after-click screenshots to Qwen2.5-VL-72B to describe the user intent and the functionality of the specified element. Additionally, to enhance coverage of functionalities in specialized software, annotators were instructed to follow official documentation when crafting or refining grounding instructions. We show examples of advanced grounding tasks in Figure 8. Post-Selection: We first gathered predictions from suite of leading models (Qwen3-VL-8B, UI-TARS-1.5-7B, Holo1.5-7B, GTA1-7B and UI-Venus-Ground-7B) and used their level of agreement as filtering metric. This process involved pruning most easy instances that all models solved correctly, while also flagging instances where models consistently converged as the wrong answer. These latter cases were reviewed by annotators to correct labeling errors to improving the datasets reliability. After correction, we classified instances into three diffi3 Domain Instruction Templates E-commerce 1.Find the item with the highest discount amount and add it to the cart. 2.Find the item with the third-highest number of reviews and add it to the cart. 3.Select the item with the lowest rating to view its details. Finance 1.Among all stocks priced below $50, find the one with the highest market capitalization and view its details. 2.Identify the U.S. stock with the highest price increase today. 3.Find the stock with the highest average trading volume over the past three months. Productivity 1.Locate the task with deadline of October 15. 2.Complete the last subtask of xxx. 3.View the task that took the shortest time to complete in the first half of this year. Social 1.Find the most recently posted comment below the article. 2.View the details of the only comment that has been edited. 3.Find the comment with the highest number of likes. Travel 1.Find the only flight with exactly two stopovers. 2.Among all Cathay Pacific flights, identify the one with the longest total flight duration. 3.Find the most expensive hotel on the map. Table 10. list of selected domains and examples of corresponding instruction templates. Figure 8. Examples of advanced grounding tasks. In the refusal grounding task, the red bounding box indicates the original UI element. After modification of the instruction, no matching element exists in the image. culty levels based on the proportion of models that predicted correctlydifficult ( 20%), medium (4060%), and easy (all others). Finally, we constructed the benchmark by randomly sampling from these categories in 4:4:2 ratio. 9. Experimental Analsysis Model Thinking: According to the technical report [24], Qwen3-VL-8B-Instruct and Qwen3-VL-8B-Thinking achieve grounding scores of 94.4%/54.6%/58.2% and 93.6%/46.6%/56.7% on ScreenSpot, ScreenSpot-Pro, and OSWorld-G, respectively. Surprisingly, the model with stronger reasoning capabilities exhibits weaker grounding performance, prompting detailed comparison on VenusBench-GD shown in Figure 9. On basic grounding tasks, the thinking-enabled model exhibits slightly lower performance than the instruct model, trend consistent with observations from prior benchmarks. In contrast, it significantly outperforms the instruct model on advanced grounding tasks, with notable improvements of 22.8% in reasoning grounding and 12.2% in functional grounding, where successful execution requires multi-step inference and deliberate reasoning. This aligns well with our design intent for advanced tasks. Performance on refusal grounding is comparable between the two models. These findings suggest that the thinking model is better suited for complex tasks involving extended reasoning and logical deliberation, while its performance on simpler tasks may be adversely affected by redundant or irrelevant outputs and sensitivity to bounding box annotation quality. Figure 9. The model performance comparison of Qwen3-VL-8BInstruct and Qwen3-VL-8B-Thinking. The reasoning-enhanced thinking model outperforms instruct editions on advanced grounding tasks but lags behind on basic grounding tasks. Bilingual Evaluation: We also conduct comparisons on VenusBench-GD using Chinese instructions as shown in Table 11. Notably, some models exhibit language-dependent performance: for instance, OpenCUA-7B outperforms UITARS-7B on English instructions, while the reverse holds for Chinese. This highlights the value of multilingual data in grounding model training. Human Performance: We also compare human performance on VenusBench-GD against current SOTA performance shown in see Figure 10. Humans significantly outModels Closed-source Models GPT-4o [17] Gemini 2.5 Pro [8] Claude-Sonnet-4.0 [2] General Open-source VLMs Qwen2.5-VL-7B [3] Qwen2.5-VL-72B [3] Qwen3-VL-4B [24] Qwen3-VL-8B [24] Qwen3-VL-30B-A3B [24] InternVL2.5-8B [6] InternVL3.5-8B [27] InternVL3.5-30B-A3B [27] MiniCPM-V4.5-8B [38] GUI-specific Models (SFT) UI-TARS-7B [21] UI-TARS-72B [21] UGround-7B [11] JEDI-7B [34] OS-Atlas-7B [32] Aguvis-7B [36] CogAgent-9B [16] OpenCUA-7B [28] OpenCUA-32B [28] GUI-specific Models (RL) SE-GUI-7B [40] GUI-G2-7B [23] GTA1-7B [37] GTA1-32B [37] UI-TARS-1.5-7B [22] UI-Venus-Ground-7B [13] UI-Venus-Ground-72B [13] Holo1.5-7B [9] Holo1.5-72B [9] Basic Tasks Advanced Tasks Overall Element Visual Spatial Avg Reasoning Functional Refusal Avg 4.71 6.79 20.05 67.76 80.58 84.66 85.36 82.84 15.78 69.26 73.66 38. 68.01 72.72 65.81 45.82 50.72 43.24 37.08 71.15 59.46 71.15 79.45 73.29 83.09 72.16 74.73 87.18 79.07 89.19 10.73 13.35 43.62 67.70 88.20 82.96 88.92 86.89 18.36 56.50 61.50 27.77 72.47 84.03 69.01 52.56 33.85 35.88 25.27 81.88 68.65 73.30 83.08 77.47 90.46 71.28 78.31 91.42 81.88 91. 6.61 12.54 19.05 57.34 75.41 74.83 80.08 78.52 13.22 57.24 58.11 20.51 61.71 73.28 59.86 47.33 33.82 37.71 35.18 66.57 59.48 61.32 74.54 59.48 78.23 58.41 68.03 80.56 75.51 86.88 6.73 10.09 25.46 64.64 80.89 81.32 84.65 82.54 15.64 62.59 66.09 30. 67.22 75.63 64.82 47.90 41.60 39.81 33.65 72.39 61.69 68.75 78.87 70.19 83.43 67.85 73.61 86.24 78.69 89.01 3.79 7.68 9.93 15.26 35.05 25.56 22.04 32.16 7.40 19.60 21.22 5.23 16.17 34.96 16.17 23.21 12.01 11.11 5.69 22.13 28.18 15.08 25.74 20.42 36.67 18.25 24.12 46.43 26.74 38. 3.71 9.43 12.28 38.57 52.14 62.57 50.43 67.14 13.43 48.00 49.86 18.00 37.71 59.42 48.43 54.71 28.43 33.00 29.86 57.28 48.14 50.28 58.14 55.71 68.85 44.14 58.57 71.71 65.28 75.28 33.00 35.55 18.66 13.77 67.44 25.00 55.00 25.11 0.00 5.11 7.00 8. 0.22 0.00 0.00 6.55 5.44 0.00 0.00 0.00 0.33 7.44 0.00 0.00 0.00 0.11 0.00 43.33 0.00 0.00 13.48 17.39 13.44 20.79 50.24 34.94 40.34 38.86 6.50 22.12 23.89 9.56 16.43 29.66 19.13 25.82 14.07 13.07 10.05 23.86 24.08 21.64 25.56 22.75 32.80 18.91 25.00 51.93 27.81 35. 9.69 13.29 20.18 45.39 67.43 60.95 65.19 63.36 11.63 44.82 47.56 21.35 44.91 55.44 44.75 38.21 29.51 28.07 23.29 51.08 45.18 48.07 55.46 49.36 61.20 46.36 52.54 71.18 56.35 65.46 Table 11. Performance comparison of Chinese instructions on VenusBench-GD dataset categorized by the evaluation tasks. In the above table, within each group, the best-performing model on each task is highlighted in bold, while the second-best is underlined. perform all models on advanced tasks, exceeding SOTA by 41.6%, 11.8%, and 17.8% in reasoning, functional, and refusal grounding, respectively. This underscores the limitations of existing models in handling complex, reasoning intensive grounding tasks. tered UI regions. While capable of coarse-grained regional identification (e.g., upper right corner), they often fail to distinguish the target element from adjacent distractors, indicating weakness in jointly optimizing for object identity (what) and location (where). 10. Error Analaysis A. Semantic Misinterpretation: Models struggle with element grounding when an objects identity relies on abstract semantic concepts rather than literal text. For instance, they fail to associate the abstract notion of text alignment with its conventional iconography. Failures also arise from OCR limitations on small or stylized text, hindering the bridging of high-level semantics with low-level visual features. B. Imprecise Spatial Localization: In spatial grounding, models exhibit an inability to integrate spatial constraints with precise object identification, especially in clutC. Failure in Composing Visual Attributes: For visual grounding, models often fail to compose multiple visual attributes (e.g., color, shape, relation) as specified in an instruction. common error is latching onto single salient feature while ignoring others, leading to the selection of partially matching elements. This reveals fundamental weakness in binding complete set of visual attributes to single target. D. Deficiencies in Multi-Step Logical Reasoning: In reasoning grounding, models demonstrate an inability to execute multi-step logical procedures such as comparison, filtering, or ordinal selection. They often exhibit greedy 5 progress and timelines. All annotators were aged between 25 and 35 and underwent approximately two weeks of intensive training and certification prior to the formal annotation phase. The entire annotation process spanned roughly three months. We first collected screenshots from diverse platforms and software applications and conducted labeling on custom built annotation platform. In the initial stage, bounding boxes were drawn around available UI elements, yielding an average of 20 annotated elements per image. Subsequently, distinct annotation pipelines were applied according to the specific grounding task type. For each subtask, we implemented at least two rounds of annotation followed by rigorous quality checks. To further enhance diversity and accuracy, we integrated large language models into the workflow, leveraging their capabilities for instruction generation, refinement, and validation. Figure 10. Human performance comparison with current SOTA performance on grounding tasks. behavior, failing to decompose complex instructions into sequence of required operations (e.g., selecting the highestpriced item instead of the second-highest). E. Lack of Abstract Functional Knowledge: During functional grounding, models fail to map abstract, goaloriented commands (e.g., Enter Fullscreen) to their nonliteral UI affordances. Lacking the domain-specific knowledge of software conventions that humans possess, models resort to futile literal text searches, highlighting gap between abstract user intent and concrete UI interactions. F. Failure in Contradiction Detection and Task Refusal: critical deficiency in refusal grounding is the models over-compliance with instructions that are factually impossible or contradictory to the visual context. Given factually incorrect instruction, such as Find the text at 2:35 PM in the upper right corner when the element is actually in the upper left, the model is expected to identify the contradiction and refuse the request. However, prevalent error is that the model attempts to hallucinate result. It either points to the instructed region (the upper right corner) despite the objects absence or incorrectly identifies different object that does not match the description. This indicates an over-reliance on the textual prompt and critical weakness in validating instructions against visual evidence. To mitigate this, models could be trained with contrastive examples (correct vs. incorrect prompts) or by implementing preliminary verification step to confirm the prompts premises before execution. 11. Human Annotation We collaborated with dedicated annotation team to construct this benchmark dataset. The team comprised 20 members: 17 primary annotators responsible for core labeling tasks, 2 quality assurance specialists handling validation and acceptance, and 1 project coordinator overseeing"
        }
    ],
    "affiliations": [
        "Venus Team, AntGroup",
        "iMean AI"
    ]
}