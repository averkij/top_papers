{
    "paper_title": "Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall",
    "authors": [
        "Mingyu Jo",
        "Jaesik Yoon",
        "Justin Deschenaux",
        "Caglar Gulcehre",
        "Sungjin Ahn"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Discrete diffusion models offer a promising alternative to autoregressive generation through parallel decoding, but they suffer from a sampling wall: once categorical sampling occurs, rich distributional information collapses into one-hot vectors and cannot be propagated across steps, forcing subsequent steps to operate with limited information. To mitigate this problem, we introduce Loopholing, a novel and simple mechanism that preserves this information via a deterministic latent pathway, leading to Loopholing Discrete Diffusion Models (LDDMs). Trained efficiently with a self-conditioning strategy, LDDMs achieve substantial gains-reducing generative perplexity by up to 61% over prior baselines, closing (and in some cases surpassing) the gap with autoregressive models, and producing more coherent text. Applied to reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such as Countdown and Game of 24. These results also indicate that loopholing mitigates idle steps and oscillations, providing a scalable path toward high-quality non-autoregressive text generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 4 0 3 9 1 . 0 1 5 2 : r LOOPHOLING DISCRETE DIFFUSION: DETERMINISTIC BYPASS OF THE SAMPLING WALL Mingyu Jo1, Jaesik Yoon1,4, Justin Deschenaux2, Caglar Gulcehre2,3, Sungjin Ahn1,5 1KAIST, 2EPFL, 3Microsoft, 4SAP, 5NYU"
        },
        {
            "title": "ABSTRACT",
            "content": "Discrete diffusion models offer promising alternative to autoregressive generation through parallel decoding, but they suffer from sampling wall: once categorical sampling occurs, rich distributional information collapses into one-hot vectors and cannot be propagated across steps, forcing subsequent steps to operate with limited information. To mitigate this problem, we introduce Loopholing, novel and simple mechanism that preserves this information via deterministic latent pathway, leading to Loopholing Discrete Diffusion Models (LDDMs). Trained efficiently with self-conditioning strategy, LDDMs achieve substantial gainsreducing generative perplexity by up to 61% over prior baselines, closing (and in some cases surpassing) the gap with autoregressive models, and producing more coherent text. Applied to reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such as Countdown and Game of 24. These results also indicate that loopholing mitigates idle steps and oscillations, providing scalable path toward high-quality non-autoregressive text generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Discrete diffusion models have recently emerged as promising alternative to autoregressive models for tasks such as text generation (Austin et al., 2021; Campbell et al., 2022; Lou et al., 2023; Sahoo et al., 2024; Schiff et al., 2024; Zhao et al., 2024; Ou et al., 2024; Gat et al., 2024; Wang et al., 2025a). Unlike autoregressive models, which generate tokens sequentially from left to right, discrete diffusion generates entire sequences in parallel through iterative refinement across multiple denoising steps. This parallel generation not only enables substantial speedups for long-horizon sequence generation but also allows the model to leverage global sequence context rather than being restricted to left-context only. However, despite these advantages, empirical studies show that discrete diffusion models still lag behind autoregressive models in generation quality (Gat et al., 2024; Zheng et al., 2024), indicating substantial room for improvement. For example, recent studies have highlighted specific issues in discrete diffusion models, such as idle steps (Chao et al., 2025) and temporal oscillation (Wang et al., 2025a). Figure 1: Unconditional gen PPL measured with GPT-2 Large (sentence entropy in parentheses). As part of such effort to improve discrete diffusion models, in this work we first focus on fundamental phenomenon that may underlie these inefficiencies, which we term the sampling wall. We define the sampling wall as form of information collapse, in which rich categorical distributionscapturing plausible token candidates and their relative likelihoodsare reduced to one-hot vectors after sampling. We call this wall because, in standard discrete diffusion models, once sampling occurs, the original distributional information is lost and cannot be propagated to subsequent steps. Motivated by this observation, our central hypothesis is that explicitly propagating distributional information beyond the sampling wall across denoising steps can alleviate key limitations of discrete diffusion models. Correspondence to Sungjin Ahn (sungjin.ahn@kaist.ac.kr). In this paper, to realize this idea, we propose simple and novel mechanism, termed Loopholing, together with corresponding family of models, Loopholing Discrete Diffusion Models (LDDMs). Our key idea in the Loopholing mechanism is to introduce direct, deterministic pathway that transfers the rich contextual latent state to the subsequent step. This pathway complements the existing stochastic path; thus, in Loopholing, each denoising step produces two outputs: stochastic one-hot vector and deterministic continuous vector. While this design introduces recurrent dependency across the denoising trajectory, which would require full unrolling for training, we make training feasible at randomly sampled time steps without unrolling by introducing self-conditioning approach (Chen et al., 2022; Jabri et al., 2022) tailored for Loopholing. The main contributions of the paper are as follows. (i) Identifying the Sampling Wall Problem: We identify the sampling wall problem as fundamental characteristic that may underlie various inefficiencies in the standard discrete diffusion models. (ii) Introducing Loopholing: We propose (iii) Strong the Loopholing mechanism, and Loopholing Discrete Diffusion Models (LDDMs). Empirical Results: We demonstrate the effectiveness of LDDMs through various experiments. On the OpenWebText dataset (Gokaslan & Cohen, 2019), our model improves the MDLM (Sahoo et al., 2024) validation perplexity from 23.82 to 21.9. In terms of Generation Perplexity (Gen PPL), as shown in Fig. 1, our method achieves substantial gains, reducing Gen PPL by 55% relative to MDLM and by 61% relative to UDLM. Against autoregressive models, the gap shrinks from 3.26 higher Gen PPL with MDLM to only 1.43 with our method. Remarkably, when applied to UDLM, our approach not only closes the gap but even surpasses the autoregressive baseline, whereas the standard UDLM lags behind by 2.15. In reasoning, loopholing mechanism boosts the accuracy on Countdown4 (Gandhi et al., 2024) from 45% to 56.3% over the MGDM baseline (Ye et al., 2024)."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Discrete Diffusion Models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are probabilistic generative models defined by fixed forward process that progressively corrupts data with noise, and learned reverse process trained to recover the original sample x. For discrete data, this framework is adapted by representing categorical data sample as one-hot vector V, where where = {v {0, 1}K : (cid:80) vk = 1} is the set of all one-hot vectors over vocabulary of size K. For clarity, we describe the formulation in the simplest case of single categorical variable. The forward process corrupts the initial data sample over continuous time variable [0, 1], producing sequence of progressively noisier latent variables zt. common approach for this is to use an interpolating discrete diffusion model (Sahoo et al., 2024; Schiff et al., 2024; Shi et al., 2024; von Rutte et al., 2025), where the marginal distribution of the latent state zt, conditioned on the original data x, is formulated as categorical distribution that interpolates between the data and fixed prior distribution π: q(ztx) = Cat(zt; αtx + (1 αt)π) , (1) where Cat(; p) denotes categorical distribution parameterized by probability simplex p, and αt [0, 1] is monotonically decreasing noise schedule with α0 1 and α1 0. Masked Diffusion Models (MDMs) are subclass of discrete diffusion models in which the prior π is set to m, the one-hot vector corresponding to special [MASK] token (Sahoo et al., 2024; Nie et al., 2025; Kim et al., 2025). Under this formulation, the forward process can be interpreted as the gradual masking of tokens over time. In MDMs, the reverse process generates data by progressively denoising sample, beginning from the prior distribution in which all tokens are masked. Formally, it seeks to approximate the true reverse posterior q(zszt, x) for any 0 < 1. In practice, this distribution is parameterized using neural network xθ(zt, t) trained to predict the original data x. In the following, we use xθ,t as shorthand for xθ(zt, t), whenever this does not cause confusion. The resulting approximation of the posterior takes the form: q(zszt, xθ,t) = (cid:40)δzt(zs), (cid:16) Cat zs; (1αs)m+(αsαt)xθ,t 1αt (cid:17) zt = m, , zt = m, (2) where δxt denotes the Dirac delta function. This ensures unmasked tokens are preserved, while masked tokens are resampled according to the models predictions and the given schedule. 2 For training, MDMs optimize simplified Negative Evidence Lower Bound (NELBO). In continuous time, this specific formulation provides tighter objective than its discrete-time counterparts (Austin et al., 2021; Kingma et al., 2021). In practice, the objective reduces to weighted cross-entropy loss (Sahoo et al., 2024; Shi et al., 2024): LNELBO = EtU [0,1],ztq(ztx) (cid:20) I[zt = m] α 1 αt logxθ(zt, t), , (3) (cid:21) where [0, 1] denotes the uniform distribution, α is the time derivative of αt, , represents the dot product, and I[] is the indicator function, returning 1 if the condition holds and 0 otherwise. This objective encourages the model to accurately reconstruct the original tokens at masked positions. Besides MDMs, there are also Uniform Diffusion Models (UDMs), which use uniform distribution over the entire vocabulary as the prior π (Austin et al., 2021; Schiff et al., 2024; Sahoo et al., 2025). Further details on UDMs are provided in Appendix B."
        },
        {
            "title": "3 LOOPHOLING DISCRETE DIFFUSION MODELS",
            "content": "To begin with, we first discuss characteristic of discrete diffusion models that motivated the design of the Loopholing mechanism. We refer it to as the sampling wall problem. The sampling wall problem represents form of information collapse, where rich categorical distributional representations are reduced to one-hot vectors. Specifically, while xθ,t = xθ(zt, t) encodes far richer information about plausible token candidates and their relative likelihoods, this information is discarded once single category zs is drawn, leaving only the one-hot representation to be propagated forward. For example, consider two cases: xa θ,t = [0.20, 0.80]. Despite reflecting very different situations, the denoising process cannot distinguish between the two if the second category is sampled in both cases, entirely discarding the predictive distribution at that position. Moreover, without propagating this distributional information, process based solely on sampling can become more redundant (Chao et al., 2025) and prone to excessive oscillations (Wang et al., 2025b). θ,t = [0.49, 0.51] and xb To address the sampling wall problem in discrete diffusion models, we propose novel mechanism, termed Loopholing, together with corresponding family of models, Loopholing Discrete Diffusion Models (LDDMs). The goal of LDDMs is to operationalize the following central hypothesis in both the architecture and the training procedure of discrete diffusion models: Main Hypothesis: Enriching the denoising process by propagating detailed contextual information available prior to sampling discrete tokenssuch as the categorical distribution parameter xθ,tcan alleviate the aforementioned issues and leads to improved performance. Our key idea for implementing the above hypothesis in the loopholing mechanism is to introduce, alongside the standard sampling pathway, direct deterministic pathway that carries rich distributional context information obtained before sampling across denoising steps. In the following, we present Loopholing Discrete Diffusion Models (LDDMs) in two parts. First, we describe the generation process of LDDMs, detailing how distributional context is obtained and propagated throughout denoising. Second, we explain how LDDMs can be trained efficiently through self-conditioning approach. Notably, both the generation and training mechanisms of Loopholing are straightforward to implement, requiring only minor modifications to the standard discrete diffusion framework 3.1 GENERATION WITH LOOPHOLING In standard discrete diffusion models, as shown in Fig. 2(a), the modeling of the denoising process at time step proceeds as follows. First, it converts each one-hot input token zt in the sequence into an embedding Eθ(zt). The embeddings of the sequence are passed through backbone network such as Transformer (Vaswani et al., 2017) layer to mix the sequence embeddings and produce latent embedding hs at each position. Finally, the latent embedding is fed into projection layer to predict the distribution of the clean observation, xθ(zt, t), to sample zs using the posterior in Eqn. 2. 3 Figure 2: Architectural comparison of standard discrete diffusion models and the Loopholing Discrete Diffusion Models (LDDMs). (a) The standard architecture of discrete diffusion. (b) During inference, LDDMs propagate the continuous latent representation hs to the subsequent step, creating deterministic pathway that preserves rich contextual information. (c) During training, LDDMs employ self-conditioning strategy: first pass generates pseudo-context h0, which is then used to condition the second pass. We observe that rich contextual information is captured in both hs and xθ(zt, t) during this process. This information is rich because it accounts for complex relational interactions among tokens and is represented as high-dimensional continuous vector rather than one-hot encoding. However, once the output representation zs is sampled, this rich information collapses into one-hot vector. Therefore, the next denoising step, which takes only zs as input, cannot exploit this information or build upon it, and must instead reconstruct much of it again from the limited one-hot representation. From this observation, our key idea in the loopholing mechanism is to introduce direct, deterministic pathway that transfers the rich contextual latent state hs to the subsequent step. This pathway complements the existing stochastic path; thus, in loopholing, each denoising step produces two outputs: stochastic one-hot vector and deterministic continuous vector: (xθ,t, hs) = fLoopholing(zt, ht, t). (4) Formally, the denoising process with loopholing as shown in Fig. 2(b) is described as follows. Let zt be the one-hot vector for token at step t. We initialize latent state h1 = 0. At each denoising step s, the model performs the following computations: et = Eθ(zt) + LN(ht) , hs = fθ(et, t) , xθ(zt, ht, t) = softmax(gθ(hs)) , (5) where Eθ is the token embedding function, fθ is the backbone network, and gθ is the output projection layer. The previous latent embedding ht combines with the current token embedding Eθ(zt) via Layer Normalization (LN) (Ba et al., 2016), creating deterministic contextual latent path. This prediction xθ(zt, ht, t) is used to parameterize the reverse posterior (Eqn. 2) and sample zs. Note that while it is also possible to pass the models prediction xθ(zt, ht, t), to the subsequent step, this distribution over the vocabulary typically has much higher dimensionality than ht in important applications such as language modeling. For this reason, we pass ht in our default architecture. 3.2 TRAINING WITH SELF-CONDITIONED LOOPHOLING The generation with loopholing requires propagating ht across denoising steps, which introduces recurrent dependency. key advantage of diffusion models, however, is that training does not require this time-consuming temporal unrolling process. Instead, training can be performed on randomly sampled time steps, as q(ztx) can be directly constructed for any arbitrary time step. Maintaining this efficiency within loopholings training process is therefore significant challenge. To address this, we introduce self-conditioning approach (Chen et al., 2022; Jabri et al., 2022) to avoid unrolling the full generation path during training. The core idea, illustrated in Fig. 2(c), is to simulate the context propagation process using two forward passes: for given noisy input zt, the model first computes pseudo-context h0 and then uses it in second, context-conditioned pass as input to make the final prediction. Specifically, the process for each training step is as follows: 1. First Pass (Pseudo-Context Generation): We perform loopholing denoising, but by setting the input context state to zero vector. This yields pesudo-context h0 and an initial prediction 4 Figure 3: Illustration of the sampling wall in Masked Diffusion Models (MDMs), which induces two distinct failure modes. (1) Steps without Progress: Fixing on single token can cause the input sequence to remain static across multiple denoising steps, leading to significant computational inefficiency. (2) Excessive Oscillations: Sampling low-probability token (e.g., loud) can trigger excessive oscillations in subsequent steps. x0 θ(zt, t) directly from the loopholing function: (x θ,t, h0) = fLoopholing(zt, ht = 0, t) . (6) 2. Second Pass (Context-Conditioned Prediction): The second pass takes the pseudo-latent embedding h0 from the first pass as if it is from the previous step during the generation process: (x1 θ,t, h1) = fLoopholing(zt, ht = sg[h0], t) . (7) The stop-gradient operator, sg[], ensures that gradients flow only through the second forward pass. This allows the model to learn how to consume its own representations as context without the prohibitive cost of backpropagating through time. This training objective can then be expressed as: LSelf-conditioning = EtU [0,1],ztq(ztx) (cid:20) I[zt = m] α 1 αt (cid:0)logx θ(zt, sg[h0], t), x(cid:1) (cid:21) , (8) where the expectation is taken over uniformly sampled time U[0, 1] and the corresponding noised input zt q(ztx). Furthermore, following previous work (Jabri et al., 2022), we employ self-conditioning with probability of p. Specifically, the model is trained on the self-conditioning loss with probability p, and on the standard discrete diffusion loss (Eqn. 3) otherwise. This approach encourages the contextual latent from the first pass to accurately capture the context of while providing useful guidance for the second pass."
        },
        {
            "title": "4 DISCUSSION: WHY LOOPHOLING WORKS",
            "content": "We hypothesize that the sampling wall manifests through two key inefficiencies in discrete diffusion, which are illustrated in Fig. 3. Steps without progress: As shown in recent study (Chao et al., 2025), many denoising steps in standard discrete diffusion models reproduce the same samples as in previous steps, resulting in idle steps. This is wasteful because it does not make any progress in the sequence. We hypothesize that loopholing provides way to address this issue by enabling continuous updates to the context latent ht even when the sample zt remains unchanged. Specifically, the deterministic recurrent state update in the ht space (Eqn. 4) ensures that contextual information evolves across steps, so that each iteration contributes to refining the output and thereby improving computational efficiency. Excessive Oscillation: The sampling wall can induce oscillations during denoising. Although standard discrete diffusion models consistently predict the same objective, xθ,t, at each step, it discards the rich distributional information from the prior steps prediction. This forces the model to predict from scratch, relying only on the current, stochastically sampled sequence (Wang et al., 2025b). We hypothesize that the loopholing mechanism can lead to more stable generation process. By providing deterministic information path, this mechanism enables the model to directly maintain contextual information about the target throughout the denoising process. 5 Table 1: Comparison of test perplexities () of models trained for 1 million steps on the One Billion Word (LM1B) and OpenWebText (OWT) datasets. denotes retrained model. LM1B OWT Masked Diffusion SEDD Absorb(Lou et al., 2023) 28.39 24.01 MDLM (Sahoo et al., 2024) 27.60 23.05 Uniform Diffusion UDLM (Schiff et al., 2024) 31.11 25. Ours (LDDMs) LDDM-M (ours) LDDM-U (ours) 25.95 21.90 29.21 23."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Discrete Diffusion for Language Modeling. The pioneering work (Austin et al., 2021) introduced discrete denoising diffusion using transition matrices based on an absorbing mask state and uniform noise. Subsequent models, including Score Entropy Discrete Diffusion (SEDD) (Lou et al., 2023), Mask Diffusion Language Models (MDLM) (Sahoo et al., 2024), Uniform Diffusion Language Models (UDLM) (Schiff et al., 2024) and Duo (Sahoo et al., 2025), have further advanced this paradigm with improved training objectives, producing stronger language modeling performance. However, shared limitation is the reliance on repeated categorical sampling across the sequence, which can degrade contextual coherence. Our work directly addresses this challenge. Self-Conditioning. Self-conditioning techniques improve consistency across generative steps by reusing previous model outputs or hidden states during training. For instance, Analog Bits (Chen et al., 2022) employs self-conditioning to enhance sampling performance, while Recurrent Interface Networks (RINs) (Jabri et al., 2022) use it to reduce the computational cost of training by avoiding backpropagation throughout the generation trajectory. Inspired by these approaches, our loopholing mechanism integrates self-conditioning strategy to efficiently train the model to use the propagated latent embedding as internal memory."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "In this section, we empirically validate the effectiveness of the proposed loopholing mechanism across various models and tasks. We demonstrate that by accumulating contextual information, our mechanism achieves superior perplexity and generation quality in language modeling, along with higher success rates on reasoning tasks. To further understand these improvements, we conduct series of ablation studies that provide detailed analysis of our methods key components. 6.1 LANGUAGE MODELING To demonstrate the efficacy of our loopholing mechanism, we first apply it to discrete diffusion language models. Specifically, we integrate our method into the Masked Diffusion Language Models (MDLM) (Sahoo et al., 2024) and the Uniform Diffusion Language Models (UDLM) (Schiff et al., 2024), creating LDDM-M and LDDM-U, respectively. We train these models on the One Billion Word (LM1B) (Chelba et al., 2013) and OpenWebText (OWT) (Gokaslan & Cohen, 2019) datasets. Following the training phase, we evaluate their performance based on three key metrics: likelihood, zero-shot likelihood on unseen datasets, and the quality of the generated samples. For fair comparison, we adopt the experimental setup of Sahoo et al. (2024). detailed description of our configuration is provided in Appendix C.1. Likelihood Evaluation. As shown in Table 1, our loopholing mechanism consistently improves performance across both masked and uniform diffusion frameworks on the LM1B and OWT datasets. This result indicates that our approach is effective regardless of the underlying diffusion model type. Perplexity was measured by approximating the Negative Evidence Lower Bound (NELBO; Eqn. 3), with further details on the evaluation protocol available in Appendix C.1.2. 6 Table 2: Zero-shot perplexities () after 1 million training steps on OpenWebText. All reported perplexity values are upper bounds. denotes retrained model. PTB Wikitext LM1B Lambada AG News Pubmed Arxiv 97.87 86.33 38.34 36.30 74.71 66.73 50.15 48. 76.54 68.62 45.25 41.94 39.75 37.52 77.28 38.48 81. 51.68 76.81 46.18 41.19 Masked Diffusion SEDD Absorb MDLM Uniform Diffusion UDLM Ours (LDDMs) LDDM-M (ours) LDDM-U (ours) 85.80 71.52 33.27 38.89 69.53 79. 44.22 52.34 62.55 76.81 39.74 45.05 34.96 41.02 Zero-Shot Likelihood Evaluation. To assess the generalization performance of our models, we evaluated the models trained on the OWT dataset across diverse set of unseen datasets. detailed description of these evaluated datasets is provided in Appendix C.1.3. As shown in Table 2, LDDM-M, our loopholing-enhanced MDLM, consistently outperforms the baseline MDLM on all evaluated unseen datasets except for LM1B. In contrast, LDDMU exhibits only marginal improvements over UDLM, with the exception of the PTB dataset. We hypothesize this performance disparity to the fundamental differences in how perplexity is calculated for uniform diffusion framework. Uniform diffusion calculates perplexity over all tokens, making the metric highly sensitive to domain shift between the training and test distributions rather than the effectiveness of loopholing mechanism application. Generation Quality Evaluation. The loopholing mechanism is designed to address the sampling wall issue, property that is difficult to verify solely through likelihood evaluation. To directly assess its impact on generation quality, we therefore employ two alternative metrics. First, we measure the perplexity of unconditionally generated samples using pretrained GPT-2 Large model (Radford et al., 2019), which we refer to as generative perplexity (Gen PPL). Second, we utilize GPT-4.1 to score the consistency and naturalness of the samples on 0-to-10 scale, following the G-eval framework Liu et al. (2023). Specific experimental details are provided in the Appendix C.1.4. As depicted in Fig. 1 and 4(a), applying loopholing yields substantial improvements in generative perplexity. For instance, at 1024 sampling steps, LDDM-M achieves GenPPL of 49.13, more than halving MDLMs 108.94. Similarly, LDDM-U (28.76) shows about 2.5x improvement over UDLM (73.95). Critically, this performance gap is not limited to single step count; unlike their baselines which show signs of saturation, both LDDM-M and LDDM-U exhibit consistent downward trend in perplexity as the number of sampling steps increases. This demonstrates that loopholing enables meaningful, continuous refinement at each step, directly mitigating the steps without progress issue. Notably, LDDM-U also surpasses the strong auto-regressive baseline after 512 steps. Furthermore, these quality gains are achieved without sacrificing diversity, as evidenced by the stable sentence entropy. This suggests that loopholing improves generation quality not by collapsing the output distribution, but by guiding the sampling process more effectively within rich and diverse token space. The G-eval results in Fig. 4(b) further substantiate these findings on human-aligned metrics. The marked improvement in consistency suggests that by maintaining richer contextual representation throughout the whole generated sequence, loopholing helps the model produce more coherent and logically connected text. Similarly, the higher naturalness scores indicate that by propagating the rich contextual latent, the model generates more fluid and human-like sentence structures. Together, these automated and human-aligned evaluations confirm that loopholing provides robust solution to enhance generation quality in discrete diffusion models. 6.2 REASONING TASK To evaluate the effectiveness of loopholing on reasoning tasks, we integrate it into the MultiGranularity Diffusion Model (MGDM), masked diffusion framework designed for reasoning (Ye et al., 2024), resulting in the model we refer to as LDDM-G. We evaluate its performance on two Figure 4: (a) Unconditional generative perplexity measured using GPT-2 Large, with values in parentheses indicating the sentence entropy of the generated samples. (b) Evaluation of generation quality for consistency and naturalness using G-eval framework, rated by GPT-4.1 on 0-10 scale. (c) The optimal value of the selfconditioning rate (p) that yields the lowest zero-shot perplexity for each dataset. arithmetic reasoning tasks that require high logical precision: Countdown (Gandhi et al., 2024) and Game of 24 (Yao et al., 2023). The objective in these tasks is to generate valid arithmetic formula that yields target number using given set of digits. Comprehensive implementation details are provided in Appendix C.2. Architecture Table 3: Success rates (%) on the Countdown (CD) and Game of 24 (G24) tasks. As presented in Table 3, LDDM-G demonstrates substantial performance gains over the MGDM baseline across all evaluated tasks and model scales. For instance, with the 85M parameter model, LDDM-G achieves 16% improvement on Game of 24 and an almost 8% gain on Countdown 4. The loopholing mechanism drives these improvements by preserving contextual ambiguity, rather than prematurely committing to single token per step. This allows it to maintain richer representation of the solution space, enabling more effective exploration of the multiple pathways required in complex reasoning tasks and ultimately enhancing its capacity for structured, multi-step reasoning. Params CD 4 24 CD 6M 56.3 85M 94.4 6M 45 85M 86.5 LDDM-G (Ours) MGDM1 10.3 41.3 5.9 35. 28 63 12 47 6.3 ABLATION STUDY In this section, we investigate the effects of our design choices by conducting series of ablation studies. Specifically, we analyze the performance variations with respect to different selfconditioning rates in training, assess the impact of propagating the continuous latent representation, and examine the changes in excessive oscillation upon applying the loopholing mechanism. Self-Conditioning Rate. We first evaluate the impact of varying the self-conditioning rate, denoted by p. We assess performance by measuring zero-shot perplexities on the datasets from Section 6.1, using models trained on the LM1B dataset. The results, presented in Fig. 4(c), show that LDDM-M generally achieves its best performance across various unseen datasets when is set between 0.5 and 0.9. This suggests that this range provides an effective balance, allowing the contextual latent representation to be robustly learned and utilized through the two-pass self-conditioning mechanism, with detailed results shown in Table 5. Latent Propagation Length. We further investigate whether the efficacy of the loopholing mechanism accumulates over time. To see this, we assess the models performance as function of the latent propagation length k. Specifically, using model trained on the OWT dataset, we generate samples with 1024 sampling steps and, every steps, reset the context latent to the self-conditioned latent instead of carrying it over from the previous step. This procedure limits the accumulation window, so larger means longer propagation. As shown in Fig. 5(a), performance improves as increases, suggesting that the sustained propagation of accumulated latent information is effective in generating higher quality samples. 1Reported MGDM baselines may differ slightly from those in Ye et al. (2024) due to differences in evaluation criteria. Specifically, we consider solutions invalid if they use numbers not provided in the input or derived from previous expressions, which were not filtered under the original codebase. 8 Figure 5: (a) Generative perplexity across varying latent propagation steps. (b) KL divergence (log-scale) between the predicted token distribution at each step and the distribution from 20 steps prior (t20) during the generation. (c) Entropy of the predicted token distributions throughout the generation. Idle Steps and Excessive Oscillation. To investigate whether Loopholing mitigates excessive oscillation, we introduce two metricsTemporal KL divergence (TKL) and Token-Prediction Entropy (TPE). For sequence of length over number of sampling steps, these are defined as follows: DTKL(t) = 1 L (cid:88) ℓ=1 (cid:16) DKL xℓ θ,t+ 20 xℓ θ,t (cid:17) , HTPE(t) = 1 L (cid:88) ℓ=1 H(xℓ θ,t) , (9) where DKL is the KL divergence and is the entropy. The TKL metric evaluates the rate of change in the token distribution across denoising steps (here measured with 20-step lookback), whereas the TPE metric assesses the level of confidence or certainty in the models predictions at each step. We therefore interpret high TKL as reflecting faster progress along the denoising trajectoryclosely tied to the steps without progress phenomenonwhereas high TPE value serves as an indicator of excessive oscillatory behavior during generation. We measure these metrics on models trained on OWT dataset with 1024 sampling steps. In Fig. 5(b), we first see an interesting crossover points in the middle where the behavior between LDDMs and non-LDDM-based models reverses. During the first half, the LDDM-based models show higher TKL than non-LDDM models. This means that LDDMs make denoising progress much faster. We see this phase where the model tries to search the target topic to generate, so called the exploration phase. Interestingly, the trend reverses during the second half of the denoising steps by showing lower TKL than non-LDDM models. This means, LDDMs try to change more conservatively showing less oscillations. As shown in Fig. 5(c), LDDMs maintain consistently lower token-level entropy. This indicates that the stable contextual information carried by loopholing allows the model to make more confident and decisive predictions across the denoising trajectory."
        },
        {
            "title": "7 DISCUSSION",
            "content": "Computation and Memory. While loopholing significantly improves the performance of discrete diffusion models, it also introduces certain limitations. Most notably, training requires about 30% more time compared to standard models, although it adds almost no overhead at inference time. In addition, doubling the embeddings to support both the sampling and contextual pathways leads to increased memory consumption. We also investigated applying loopholing only during finetuning without retraining from scratch, but our initial trials were unsuccessfulthough we believe this remains promising avenue for future exploration. Moreover, the current training formulation considers only single-step updates, suggesting potential benefits from explicitly designing multi-step training strategies to better exploit long-range dependencies through the context latent path. Relations to Recurrent Neural Networks. An insightful perspective on loopholing and LDDMs is to view them through the lens of Recurrent Neural Networks (RNNs) (Goodfellow et al., 2016). In this interpretation, the deterministic update path in loopholing corresponds to the hidden-state update of an RNN, while the stochastic and discrete outputs play the role of the RNNs output, which is then fed back as input at the next stepakin to an autoregressive RNN. However, the key difference 9 lies in the training procedure: loopholing diffusion enables simulation-free training, whereas RNNs typically require rollout-based training. We believe that further exploring the connection between loopholing diffusion and RNNs would be compelling direction for future work. General Limitations. Furthermore, our current contribution is novel architecture supported by empirical evidence. However, rigorous mathematical framework that incorporates loopholing into the standard diffusion framework has not yet been developed, marking natural direction for future theoretical work. In terms of scalability, our experiments have thus far been conducted on moderately sized models feasible within an academic setting, and extending loopholing to larger scales will be important for fully assessing its potential."
        },
        {
            "title": "8 CONCLUSION",
            "content": "In this work, we identified the sampling wall as key limitation of discrete diffusion models, where rich distributional information collapses into one-hot representations, leading to inefficiencies such as steps without progress and excessive oscillation. To overcome this, we proposed the loopholing mechanism and developed Loopholing Discrete Diffusion Models (LDDMs), which preserve and propagate distributional context latent across denoising steps through deterministic latent pathway. Extensive experiments demonstrated that LDDMs improve fluency, naturalness, and semantic consistency in text generation and reasoning tasks, significantly narrowing the performance gap with autoregressive models. These results highlight loopholing as general mechanism to enhance discrete diffusion, with promising future directions including multimodal extensions, theoretical understanding, and integration with broader non-autoregressive frameworks."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "We thank the members of the Machine Learning and Mind Lab (MLML) and Lan Tran for valuable discussions and assistance."
        },
        {
            "title": "REFERENCES",
            "content": "Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms, 2019. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. Chen-Hao Chao, Wei-Fang Sun, Hanwen Liang, Chun-Yi Lee, and Rahul Krishnan. Beyond masked and unmasked: Discrete diffusion models via partial masking. arXiv preprint arXiv:2505.18495, 2025. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013. Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022. 10 Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. discourse-aware attention model for abstractive summarization of long documents. arXiv preprint arXiv:1804.05685, 2018. Justin Deschenaux, Lan Tran, and Caglar Gulcehre. Partition generative modeling: Masked modeling without masks. arXiv preprint arXiv:2505.18883, 2025. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. Advances in Neural Information Processing Systems, 37: 133345133385, 2024. Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022. Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. 11 Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, and Volodymyr Kuleshov. The diffusion duality. arXiv preprint arXiv:2506.10892, 2025. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019. Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre, Bernardo de Almeida, Alexander Rush, Thomas Pierrot, and Volodymyr Kuleshov. Simple guidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37: 103131103167, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Dimitri von Rutte, Janis Fluri, Yuhui Ding, Antonio Orvieto, Bernhard Scholkopf, and Thomas Hofmann. Generalized interpolating discrete diffusion. arXiv preprint arXiv:2503.04482, 2025. Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Remasking discrete diffusion models with inference-time scaling. arXiv preprint arXiv:2503.00307, 2025a. Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, and Chunhua Shen. Time is feature: Exploiting temporal dynamics in diffusion language models. arXiv preprint arXiv:2508.09138, 2025b. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. 12 Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv preprint arXiv:2410.14157, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015. Lingxiao Zhao, Xueying Ding, Lijun Yu, and Leman Akoglu. Unified discrete diffusion for categorical data. arXiv preprint arXiv:2402.03701, 2024. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: In The 41st international ACM SIGIR benchmarking platform for text generation models. conference on research & development in information retrieval, pp. 10971100, 2018. Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, and Jianfeng Gao. Text generation beyond discrete token sampling. arXiv preprint arXiv:2505.14827, 2025."
        },
        {
            "title": "A USE OF LARGE LANGUAGE MODELS",
            "content": "During the preparation of this manuscript, we utilized large language model to improve grammar, clarity, and overall readability. UNIFORM DIFFUSION MODELS (UDMS) In contrast to the masking approach of MDMs, Uniform Diffusion Models (UDMs) employ uniform noising strategy (Austin et al., 2021; Schiff et al., 2024). In UDMs, the prior distribution π is uniform distribution over the vocabulary, denoted as = 1/K, where 1 is K-dimensional vector of all ones. This forward process gradually replaces an original token with random token drawn uniformly from the vocabulary. Similar to MDMs, generation is performed by approximating the clean data in the true reverse posterior q(zszt, x) with neural network xθ(zt, t). The specific formulation for this approximated posterior is given by: q(zszt, xθ(zt, t)) = Cat zs; (cid:32) Kαtzt xθ + (αts αt)zt + (αs αt)xθ + (αsαt)(1αs) Kαtxθ, zt + 1 αt Kαs (cid:33) 1 , where denotes the Hadamard product, αts = αt αs MDMs, which fix generated tokens, UDMs enable iterative refinement of the entire sequence. (10) , and xθ is shorthand for xθ(zt, t). Unlike For training, UDMs also optimize the continuous-time formulated NELBO. The training objective takes the following form (Schiff et al., 2024): LNELBO = EtU [0,1],ztq(ztx) α Kαt xi (xθ)i (cid:88) xj xi log (xθ)i xj (xθ)j xi , (11) where (x)j is the j-th element of vector x, = Kαtx + (1 αt)1, xθ = Kαtxθ + (1 αt)1, and = argmaxj[K](zt)j is the index of the non-zero entry in zt. Recently, Duo (Sahoo et al., 2025) proposed low-variance objective for UDMs with improved empirical performance."
        },
        {
            "title": "C EXPERIMENT DETAILS",
            "content": "C.1 LANGUAGE MODELING C.1.1 EXPERIMENT DETAILS MDLM and UDLM settings. For our implementation, we followed best practices from prior work: we employed 64-bit precision for MDLM to ensure more accurate categorical sampling (Zheng et al., 2024), and we adopted the loss implementation from (Sahoo et al., 2025) for UDLM to improve its numerical stability. LM1B. For the One Billion Word Dataset (LM1B) (Chelba et al., 2013), we used the bert-base-uncased tokenizer (Devlin et al., 2019) with fixed context length of 128 tokens. Sequences shorter than this were handled with padding. The model architecture is based on the Diffusion Transformer (DiT) (Peebles & Xie, 2023) with rotary embeddings (Su et al., 2024). The model consists of 12 Transformer blocks, each with 12 attention heads and hidden dimension of 768. dropout rate of 0.1 was applied throughout the model. For optimization, we used the Adam optimizer (Kingma & Ba, 2014) with learning rate of 3e-4, betas of (0.9, 0.999), and an epsilon of 1e-8. The learning rate was linearly warmed up from 0 to 3e-4 over the first 2,500 steps. Training was conducted for 1M steps on 8 NVIDIA RTX 4090 GPUs. The global batch size was set to 512, which was achieved by assigning batch size of 32 to each GPU and applying gradient accumulation over 2 steps. Additionally, we applied an Exponential Moving Average (EMA) with rate of 0.9999 and gradient clipping with threshold of 1.0. For LDDMs, the self-conditioning rate was set to = 0.9, and to stabilize early training we initialized layer normalization parameters to β = γ = 0. 14 OWT. For the OpenWebText (OWT) dataset (Gokaslan & Cohen, 2019), we used the gpt2 tokenizer (Radford et al., 2019) with context length of 1024 tokens. To maximize context utilization, we employ sentence packing during preprocessing (Austin et al., 2021). The model architecture and hyperparameters are largely similar to the LM1B experiment. Specifically, it is based on DiT with rotary embeddings and includes 12 Transformer blocks, 12 attention heads, hidden dimension of 768, and dropout rate of 0.1. Optimization was performed using the Adam optimizer (learning rate 3e-4, betas=(0.9, 0.999), epsilon=1e-8), with linear learning rate warm-up over the initial 2,500 steps. The model was trained for 1M steps using 16 H100 GPUs. The global batch size was 512, configured by assigning batch size of 32 to each GPU. Similar to the LM1B setup, we applied an EMA with rate of 0.9999 and gradient clipping with threshold of 1.0. LDDMs used self-conditioning rate of 0.9, and we zero-initialized LayerNorm (β = γ = 0) to stabilize early training. Time Conditioning. To remain faithful to prior work, we adopt the canonical setting used by each baseline: MDLM and LDDM-M are trained without time conditioning following Sahoo et al. (2024); SEDD (Lou et al., 2023), UDLM (Schiff et al., 2024), and LDDM-U use with time conditioning as in their original implementations. We treat time conditioning as orthogonal to our contributionloopholing adds deterministic latent pathway and can be combined with either settingso we do not tune it beyond faithfully reproducing baselines. C.1.2 PERPLEXITY DETAILS In discrete diffusion models, we approximate perplexity (PPL) using the Negative Evidence Lower Bound (NELBO; Eqn. 3). The perplexity for sequence of length L, x1:L, is defined and upperbounded as follows: (cid:16) PPL(x1:L) = exp 1 (cid:88) i=1 log p(xi x<i) (cid:17) (cid:16) = exp 1 log p(x1:L) (cid:17) (cid:16) 1 exp NELBO(x1:L) (cid:17) . (12) Here, the marginal log-likelihood log p(x) is intractable to compute directly. We leverage its relationship with the Negative Evidence Lower Bound (NELBO), where log p(x) NELBO(x). This relationship allows us to use the computable upper bound as our perplexity metric. Since the NELBO is computed via single Monte Carlo estimation over time steps t, which can be stochastic and exhibit high variance. To reduce the variance of this estimation, we adopt the low-discrepancy sampling technique from MDLM (Sahoo et al., 2024), which ensures that sampled time steps for each batch are more evenly spaced across the time interval [0,1]. Even with this improvement, the final value can be influenced by factors like batch size and hardware. Therefore, to ensure fair and consistent evaluation, we compute all perplexity scores using fixed experimental setup: two NVIDIA RTX 4090 GPUs with batch size of 16 per GPU, under an identical software environment. (same PyTorch version, CUDA version, and library configurations). C.1.3 DATASETS FOR ZERO-SHOT LIKELIHOOD EVALUATION We evaluate our models zero-shot likelihood on diverse suite of standard benchmarks. The datasets include the Penn Treebank (PTB) (Marcus et al., 1993), Wikitext (Merity et al., 2016), Lambada (Paperno et al., 2016), AG News (Zhang et al., 2015), and corpus of scientific articles from Pubmed and Arxiv (Cohan et al., 2018). C.1.4 GENERATION QUALITY EVALUATION Generative Perplexity. We evaluate the quality of generated text using generative perplexity (Gen PPL), computed with pretrained GPT-2 Large model (Radford et al., 2019). Given generated sequence of length composed of discrete tokens x(1:L), the perplexity is calculated as: (cid:32) exp 1 (cid:88) i=1 log pϕ(x(i) x(<i)) . (cid:33) 15 (13) This metric reflects how likely the GPT-2 large model considers the sample, providing proxy for overall sample quality. For this evaluation, we generate 512 samples using the model trained on OpenWebText (OWT) dataset and report the average perplexity across all samples. Sentence Entropy. As shown in Zheng et al. (2024), Gen PPL can be deceptively low when generations have very low sentence entropy. To check for this, we measure sentence entropy for each sample. Sentence entropy indicates how many diverse tokens are used within sample. For single generated sample of length L, let count(v) be the number of times token appears. The sentence entropy for that sample is: (cid:88) vV count(v) log (cid:18) count(v) (cid:19) , (14) where is the vocabulary. We generate 512 samples and report the average sentence entropy. G-eval Scoring. We evaluate the quality of generated texts using the LLM scoring framework (Liu et al., 2023), with GPT-4.1 serving as the evaluator. For each model trained on OpenWebText (Gokaslan & Cohen, 2019) dataset, we sample 512 unconditional generations. Due to sentence packing during training, each generation might contain multiple sequences separated by the end-ofsequence token ([EOS]). For evaluation, we retain only the first sequence. Each sequence is independently rated by GPT-4.1 based on two criteria: Consistency (110): Evaluates whether the generated text maintains coherent topic without contradictions or context shifts. Naturalness (110): Assesses grammar, fluency, idiomatic usage, and freedom from spelling or punctuation errors. Scores are assigned in zero-shot manner using the prompt provided in Fig. 6. To improve the accuracy of the measurements, we performed four evaluations for each sequence with the temperature set to 1.0 and assigned the average of these scores. We report the average score across all 512 sequences as the final measure of model quality. C.2 REASONING TASK Datasets. We evaluate our method on two arithmetic reasoning tasks that require multi-step reasoning: Countdown (Gandhi et al., 2024) and Game of 24 (Yao et al., 2023). The Countdown task requires the model to use given set of numbers and basic arithmetic operations (addition, subtraction, multiplication, and division) to reach specific target number. For instance, in Countdown4, given the input numbers {24, 59, 23, 77} and target of 29, valid solution is to first calculate 24 + 59 = 83 and 77 23 = 54, and then use these intermediate results to reach the target with 83 54 = 29. Countdown5 extends this task to five input numbers, while the Game of 24 is variant of Countdown4 where the target is always 24. We use the datasets released by Ye et al. (2024) without additional filtering or preprocessing. Setup. Our experimental setup follows that of the Multi-Granularity Diffusion Model (MGDM) (Ye et al., 2024). We use the MGDM as our base architecture and integrate the loopholing mechanism to create what we call LDDM-G. MGDM extends the standard discrete diffusion objective with an adaptive token-level reweighting term and an easy-first TopK decoding strategy at inference. Regarding TopK decoding, we made notable adjustment to the original implementation. The original MGDM recalculates probabilities over all tokens (both masked and unmasked) at each step, which permits overwriting already generated tokensa form of remasking. This approach conflicts with the training objective, which focuses exclusively on predicting masked tokens. Therefore, following prior work (Nie et al., 2025; Kim et al., 2025), we modify the decoding process to keep unmasked tokens fixed and apply the uncertainty-based TopK selection only to masked positions. However, Table 4 shows that the original MGDM-style TopK decoding leads to slight performance gains. This is noteworthy because the method relies on distributions over unmasked tokens, for which it was not explicitly trained. 16 You will be given one piece of text. Your task is to rate the text on two metrics. Please read and understand these instructions carefully, and keep this document open while reviewing. Evaluation Criteria Consistency (110) high score (10) indicates that the text maintains single, coherent context throughout. low score (1) is given if the text shifts topic, contradicts itself, or loses logical flow. Naturalness (110) high score (10) means the text is grammatically correct, idiomatic, and free of spelling or punctuation errors. low score (1) is given if the text contains frequent grammar mistakes, awkward phrasing, or typos. Evaluation Steps 1. Read the generated text carefully and identify its intended context and message. 2. For Consistency, ask yourself: Does the text stay on topic without introducing unrelated ideas? Are there any contradictions or abrupt shifts in meaning? 3. For Naturalness, ask yourself: Is the writing grammatically sound and easy to read? Are phrases idiomatic, and is punctuation used correctly? 4. Assign each metric score from 1 (lowest) to 10 (highest) based on the above definitions. Text: {sample} Evaluation Form (scores ONLY): Consistency: Naturalness: Figure 6: G-Eval prompt template used with GPT-4.1. {sample} is replaced with the generated sequence to evaluate. Table 4: Performance using original MGDM TopK decoding. Success rates (%) on Countdown and Game of 24 tasks using the original MGDM TopK decoding implementation, which includes probabilities over unmasked tokens. Numbers in parentheses indicate accuracy improvements from using MGDM TopK decoding Architecture Params CountDown4 Game of 24 CountDown5 MGDM (Retrained) LDDM-G (Ours) 6M 85M 6M 85M 47.6 (+2.6) 87.1 (+ 0.6) 57 (+0.7) 94.5 (+0.1) 12 52 (+5) 29 (+1) 64 (+1) 7.6 (+1.7) 36.9 (+1.2) 11.6 (+1.3) 42.6 (+1.3) Training. Both MGDM and LDDM-G models are trained for 600 epochs with batch size of 1024. We use the Adam optimizer (Kingma & Ba, 2014) with betas of (0.9, 0.999) and an epsilon of 1e-8. For the 6M models, we use learning rate of 1e-3, and for the 85M models, we use learning rate of 3e-4; cosine learning rate schedule is employed for both. All models are trained on 8 RTX 4090 GPUs. The model architecture is based on GPT-2 (Radford et al., 2019), without the causal mask for bidirectional attention. MGDM is trained with hyperparameters α = 0.25 and β = 2. For LDDM-G, we apply self-conditioning with = 0.9. Evaluation. generation is considered correct if the resulting arithmetic expression evaluates exactly to the target number without reusing inputs or generating invalid intermediate values. Our evaluation script employs stricter criteria than the original MGDM implementation. Specifically, the original evaluation only verified that each intermediate equation and the final result were mathematically valid, without penalizing generations using numbers not provided in the input or not derived There was no way he would come here on his own. He ordered cup of coffee, and then we just sat in silence. So, Aidan finally said, Hows it going? laughed. Not much has changed since the last time saw you. Ya know, you eat here lot, said Aidan Figure 7: An example from the LAMBADA dataset. The goal is to predict the final word, Aidan. from previous expressions. We enhance this by explicitly filtering out generations using such invalid numbers, ensuring faithful and input-grounded reasoning."
        },
        {
            "title": "D ADDITIONAL RESULTS",
            "content": "D.1 DETAILED RESULTS FOR SELF-CONDITIONING RATE Table 5: Perplexities () of SEDD Absorb, MDLM, and our LDDM-M with varying self-conditioning rates p. The model is trained with diverge values and evaluated with = 1.0. All scores are reported as upper-bound estimates. LM1B(trained) PTB Wikitext Lambada AG News Pubmed Arxiv SEDD Absorb MDLM LDDM-M (ours) - = 1.0 - = 0.9 - = 0.7 - = 0.5 - = 0.3 - = 0.1 28.39 27.60 26.34 25.95 26.14 26.39 26.66 26.88 108.63 110.90 78.61 74.43 99.44 100. 101.92 99.92 102.75 100.12 101.88 100.52 70.51 66.87 66.55 66.58 71.90 69.02 92.17 89.62 90.64 89.63 90.16 90.38 61.57 60.50 57.71 56.72 56.43 57.99 58.46 59.10 75.09 70. 142.19 140.62 69.25 67.38 67.09 67.01 68.08 69.69 140.29 136.04 134.73 128.06 135.33 136.25 D.2 DOWNSTREAM TASKS We evaluated MDLM and LDDM-M on various downstream tasks using the lm-eval-harness library (Gao et al., 2024). Both models were pre-trained on OpenWebText. Our evaluation includes six multiple-choice tasksARC-Easy and ARC-Challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MathQA (Amini et al., 2019), PIQA (Bisk et al., 2020), and WinoGrande (Sakaguchi et al., 2019)and one generation task, LAMBADA (Paperno et al., 2016). Since the library is designed for autoregressive models, we adapted the approach for the masked diffusion framework. Multiple-choice tasks. Following Deschenaux et al. (2025), we adapted the evaluation for MDLMs. log-likelihood via arg maxi log p(yix) where is the context and yi is an answer option. However, MDLMs model the joint probability log p(x, yi) of the entire sequence, requiring different approach. Autoregressive models select the answer with highest Bayes rule provides solution by connecting conditional and joint probabilities: log p(yix) = log p(x, yi) log p(x) log p(x, yi) (15) Since log p(x) is constant across all answer options, ranking by joint probability is equivalent to ranking by conditional probability. We bound the joint probability using NELBO, compute Monte Carlo estimates, and select the answer with the lowest NELBO. LAMBADA task. Unlike multiple-choice tasks where models select from given options, LAMBADA requires generating the last word and comparing it with the ground truth. During evaluation, we identified critical issue with the dataset format. As shown in Fig. 7, the final word lacks any terminal punctuation (period, question mark, etc.). While this poses no problem for autoregressive models that only consider preceding context, it creates significant challenge for discrete diffusion 18 Table 6: Performance on downstream tasks. LDDM-M substantially outperforms MDLM on the generation task (LAMBADA) while showing comparable accuracy on multiple-choice benchmarks. Model LAMBADA ARC-e ARC-c HSwag MathQA PIQA WinoG MDLM LDDM-M 40.46 52.40 36.49 36.03 25.17 23.21 31.81 33.11 21.51 22.21 57.62 58. 51.85 51.46 Table 7: Impact of adding an MLP layer on perplexity across benchmarks. All reported values are upper bounds. LM1B(trained) PTB Wikitext Lambada AG News Pubmed Arxiv MDLM + MLP LDDM-M (ours) + MLP 27.60 27.21 25.95 25.87 110.90 107. 99.92 99.54 74.43 74.61 66.87 66.96 100.11 97.04 89.62 89.33 60.50 59. 56.72 56.43 70.72 70.31 67.38 66.05 140.62 139.42 136.04 127.71 models. When the input is formatted as ... said [MASK] [EOS], the EOS token signals the end of sequence, causing the model to generate punctuation marks to properly terminate the sentence rather than predicting the target word Aidan. To resolve this issue, we modified the model input by inserting an additional [MASK] token before the [EOS] token. This extra position serves as placeholder for terminal punctuation, allowing the model to correctly predict the target word in the original mask position. The added token is used only during inference and is not included in evaluation. For targets spanning multiple tokens, we adopted the iterative decoding implementation from Nie et al. (2024), where we unmask one token at time based on confidence. These modifications resulted in improved performance and more reliable evaluation. Results. Table 6 shows that LDDM-M achieves notably higher accuracy on the generation task (LAMBADA) with 52.40 vs 40.46, while maintaining comparable performance on the likelihoodbased multiple-choice tasks. D.3 DETERMINISTIC PATH AUGMENTATION To understand the effect of incorporating additional parameters into the deterministic path, we investigate augmenting the deterministic path with two-layer MLP featuring an expansion ratio of 4. In this modification, we update the latent embeddings as = ht + MLP(ht), followed by layer normalization to produce the latent representation et = Eθ(zt) + LN(h t). The original MDLM architecture applies similar MLP directly to the token embeddings, where = Eθ(zt) + MLP(Eθ(zt)) and the latent representation is computed as et = Eθ(zt) + LN(z t). The subsequent structure is the same as before (see Section 3.1). The model was trained on the LM1B dataset. As shown in Table 7, this augmentation yields only marginal performance gains. This finding suggests that the primary benefit of our framework stems from contextual latent propagation across time steps, rather than from additional parametric complexity in the deterministic path. D.4 APPLICATION OF LOOPHOLING TO AUTOREGRESSIVE MODELS To investigate the broader applicability of our proposed mechanism, we adapted loopholing for standard autoregressive language model. During training, the model performs two forward passes. The first pass generates pseudo-context from the input sequence. This pseudocontext is then shifted by one position to the right (with the first position embedding being zero vector), and fed Table 8: Unconditional gen PPL on 512 samples (1024 sampling steps). Model AR AR-Loopholing Gen PPL () 34.40 34.21 Table 9: Perplexities () of Autoregressive models trained on OpenWebText. OWT (trained) PTB Wikitext LM1B Lambada AG News Pubmed Arxiv AR (Baseline) AR-Loopholing 17.27 16. 118.39 115.99 34.88 32.98 55.91 55.19 48.22 45.00 60.60 57.01 42.72 41. 46.40 45.65 Table 10: Perplexities () of models with matched computational budget, trained on OpenWebText. All reported perplexity values are upper bounds. OWT(trained) PTB Wikitext LM1B Lambada AG News Pubmed Arxiv MDLM (1M steps) MDLM (2M steps) LDDM-M (1M steps) 23.05 22.54 21.90 86.33 84.40 85.80 36.30 35.27 33.27 66.73 65.68 69.53 48.36 48.07 44.22 68.62 66.26 62. 41.94 42.08 39.74 37.52 36.75 34.96 into the second forward pass along with the original token embeddings. This setup allows each tokens prediction to be conditioned on the continuous representation of the preceding token from the initial pass. During inference, the model generates tokens sequentially, passing both the generated token and its corresponding latent embedding to the next step, technique similar to another existing method (Zhuang et al., 2025). However, this approach did not meaningfully improve sample quality  (Table 8)  , despite slight improvement in perplexity scores  (Table 9)  . We attribute this outcome to the fundamental differences in their generation processes. In discrete diffusion, the sampling wall problem is more severe because the loss of the predicted distribution occurs for many tokens at every step of the iterative refinement. In contrast, autoregressive models operate with fixed context, stably predicting only one token at time. Consequently, the information loss from single sampling step is less significant, and the primary challenges loopholing is designed to mitigate are less prevalent in this framework. D.5 IMPACT OF MULTISTEP BACKPROPAGATION Table 11: Gen PPL on 512 samples (1024 sampling steps) after 10K fine-tuning steps. In our standard self-conditioning setup, gradients are backpropagated only through the second forward pass for computational efficiency. To see if allowing gradients to propagate through more steps would be beneficial, we conducted an experiment. Since training from scratch is computationally prohibitive, we performed short fine-tuning experiment for 10K steps on model pre-trained on the OpenWebText dataset for 1M steps with self-conditioning. In this fine-tuning stage, we extended the training process to three and four forward passes, including the sampling steps between them to mimic the actual generation process with 1024 denoising steps. Gradients from the final pass were allowed to flow back to the second pass, while the initial pseudocontext generation pass remained detached. Gen PPL () 49.13 83.08 60.07 + 3 Fwd Pass (10K) + 4 Fwd Pass (10K) LDDM-M Model As shown in Table 11, this approach led to decrease in performance, as measured by gen PPL. We hypothesize this is because in the standard two-pass setup, the second pass learns to robustly refine any context from the detached first pass, since it cannot control its input. However, in the third and fourth passes, the model can influence the context it receives from the previous step.This likely leads to the model learning dependencies on specific context patterns, which harms its ability to generalize during inference and degrades generation quality. D.6 COMPARISON WITH MATCHED COMPUTATIONAL BUDGET The loopholing mechanism requires two forward passes during each training step. While total computational cost (FLOPs) also includes backpropagation, we naively matched the number of forward passes as proxy for the overall budget. To ensure LDDMs performance gains stem from the deterministic path and not simply the increased computation, we ran controlled experiment. 20 Table 13: Comparison of Sample Quality between Masked Diffusion and Autoregressive Models. All metrics were evaluated on 512 samples generated from models trained on OpenWebText. Metrics from the OpenWebText validation set are also provided for reference OWT Dataset Validation (512 samples) Gen PPL: 14.88 Autoregressive Model (T=1024) Gen PPL: 34. Entropy: 5.573 Entropy: 5.442 Self-BLEU: 0.2498 Self-BLEU: 0.2450 LDDM-M SEDD MDLM Gen PPL Entropy Self-BLEU Gen PPL Entropy Self-BLEU Gen PPL Entropy Self-BLEU 32 64 128 256 512 1024 2048 195.96 143.04 123.52 114.12 111.18 109.05 108.16 5.731 5.681 5.658 5.643 5.629 5.629 5.625 0.1971 0.2100 0.2153 0.2178 0.2178 0.2214 0.2168 198.72 145.48 125.04 114.05 112.83 108.94 107.20 5.745 5.704 5.681 5.659 5.652 5.637 5. 0.1896 0.2051 0.2078 0.2126 0.2089 0.2132 0.2096 248.02 122.34 83.10 64.76 53.56 49.13 44.51 5.739 5.674 5.641 5.600 5.569 5.545 5.518 0.1673 0.2014 0.2156 0.2241 0.2289 0.2301 0.2295 Table 14: Sample Quality of Uniform Diffusion Based Models. All metrics were evaluated on 512 samples generated from models trained on OpenWebText. UDLM LDDM-U Gen PPL Entropy Self-BLEU Gen PPL Entropy Self-BLEU 32 64 128 256 512 1024 95.90 86.24 80.39 77.64 77.78 73.95 75.45 5.591 5.578 5.571 5.560 5.562 5.547 5.537 0.2447 0.2436 0.2407 0.2423 0.2367 0.2429 0.2380 75.55 56.27 45.47 38.76 32.83 28.76 25.06 5.551 5.538 5.518 5.494 5.447 5.418 5.366 0.2541 0.2572 0.2592 0.2519 0.2451 0.2343 0. Table 12: Unconditional gen PPL on 512 samples (1024 sampling steps). We trained baseline MDLM for 2 million steps, thereby matching the number of forward passes of our LDDM-M trained for 1 million steps. The results confirmed that the LDDMM (1M steps) significantly outperformed the MDLM (2M steps), as shown in Table 12 and Table 10. This demonstrates that the improvements achieved by loopholing are attributable to the propagation of contextual information, not larger computational budget. MDLM (1M steps) MDLM (2M steps) LDDM-M (1M steps) Model Gen PPL () 108.94 108.22 49. D.7 DETAILED QUANTITATIVE ANALYSIS OF SAMPLE QUALITY This section provides the specific numerical values for the sample quality analysis from Section 6.1, detailed in Table 13 and Table 14. We evaluate Generative Perplexity (Gen PPL), Sentence Entropy, and Self-BLEU. Self-BLEU (Zhu et al., 2018) is metric that quantifies the internal diversity of generated text corpus. For our experiments, we compute Self-BLEU scores using up to 4-grams with uniform weights (i.e., weight of 0.25 for each n-gram from 1 to 4). Lower Self-BLEU scores indicate higher diversity, as they reflect less n-gram overlap among the generated samples. The autoregressive model used for comparison is standard Transformer architecture. The analysis highlights that while LDDMs maintain contextual information about the target across denoising steps, they also maintain generation diversity. We attribute this to the high diversity in the initial stages of generation, as suggested by our ablation study (Section 6.3). IMPLEMENTATION PSEUDO-CODE Here is the pseudo-code for the generation process and training step of Loopholing Discrete Diffusion Models (LDDMs) with self-conditioning. While the forward function is shared across all models, the generation and training procedures are presented for LDDM-M for clarity. For LDDMU, the same procedure applies by replacing the posterior and training objective with Eqn. 10 and Eqn. 11, respectively. 21 Algorithm 1 LDDMs Forward Function 1: Require: Current token sequence z(1:L) 2: Parameters: Token embedding layer Eθ, backbone fθ, output projection gθ. 3: Output: Logits o(1:L) , updated latent context h(1:L) , previous latent context h(1:L) . , diffusion timestep t. , h(1:L) , t) ) 6: 4: function LDDMFORWARDθ(z(1:L) v(1:L) Eθ(z(1:L) 5: v(1:L) e(1:L) h(1:L) fθ(e(1:L) , t) o(1:L) gθ(h(1:L) ) return o(1:L) , h(1:L) 9: 10: end function 7: 8: + LayerNorm(h(1:L) ) Embed token sequence. Fuse input with normalized latent. Update memory state via backbone. Project to vocabulary space. Initialize sequence of length with MASK tokens. Initialize the latent embedding to zero vector. Algorithm 2 LDDM-M Generation Process 1: Require: Total diffusion steps , sequence length L, forward function LDDMFORWARDθ. 2: Output: generated sequence. 3: z(1:L) 1 ([MASK], . . . , [MASK]) 4: h(1:L) 1 0 5: for = 1 do i/T 6: (i 1)/T 7: , h(1:L) o(1:L) x(1:L) (z(1:L) θ for ℓ = 1 do q(cid:0)z(ℓ) z(ℓ) end for z(1:L) z(1:L) , h(1:L) Predict logits and update latent. ) Predicted distribution of the clean sequence. Element-wise sampling (independent across ℓ). The posterior is defined in Eqn. 2. LDDMFORWARDθ(z(1:L) , t) = Softmax(o(1:L) θ (z(1:L) , h(1:L) , h(1:L) z(1:L) , x(ℓ) , t)(cid:1) 9: 10: , t) 8: s 11: 12: 13: 14: end for 15: return z(1:L) Algorithm 3 LDDM-M Training Step with Self-Conditioning 1: Require: Clean data of length L, x(1:L), noise schedule αt, self-conditioning rate [0, 1], forward function LDDMFORWARDθ. 2: Ensure: Training loss L. Sample random time step. Each token is processed independently. Sample noised input via the forward process (Eqn. 1). Initialize the contextual latent to zero. , 0, t) Update with the detached pseudo-context. , h(1:L) cond , t) ) , h(1:L) cond , t), x(ℓ) Calculate loss based on Eqn. 3 x(ℓ)) 3: U[0, 1] 4: for ℓ = 1 do q(z(ℓ) z(ℓ) 5: 6: end for 7: h(1:L) cond 0 8: With probability p: , h(1:L) 9: cond STOPGRAD(h(1:L) h(1:L) pseudo) 11: o(1:L), LDDMFORWARDθ(z(1:L) , h(1:L) 12: x(1:L) θ = m] α 13: (cid:80) ℓ (z(1:L) I[z(ℓ) 1αt 10: pseudo LDDMFORWARDθ(z(1:L) cond , t) Softmax(o(1:L) θ (z(1:L) logx(ℓ) t 14: return"
        },
        {
            "title": "F SAMPLES",
            "content": "endoftext NA laner damage/mon slot degenerate, they should all tend to take longer than the average to achieve three splits in NA. Any midlaner is just very small sample that can pick out if everything in place will affect an offlaner, say an Assassin or Templar. More specifically, assuming that top mid laner has important mana needs to undertaker as timings increases.Concepts that go into wisping teamfights always revolve around tank composition. Picking the meta we want are to at least mitigate shield leech early unless you have max hp in zaelus, paladin and kidd. Even if you are only one tank ADC with greater skill stack or if you don2019t necessarily run tanky, you should always try to avoid aggressive mid laners. How strong you are, also depends on the main map your opponents have and the level of your presence on most ones. For sure that it needs to be built that you also have experience on an ally you can use on most main maps for early game control once cross biofus. Defensive defensive champions: The tank shield against crits/cooldown and Ept/HOME shenanigans shouldn2019t take too long to turn off any tank supports. The Breacher Shield can also be used to overcome team stump! Champions ADC Stats 4 5 10 7 8 16 18 7 Cooldown 2 6 pulses CC in 50 base armor Regem Crit Power 14120 Resist AP at max level, 430 CCD Payroll 930 Deviate AP at max level 10 Cyclone 900 Ranged Poisoning 50% on AP skills at max level, 1444 NET abilities 800 BKB PoI 2475 WCP Defensive champions in gold per attack, and by number of CR per dmg. hexcaprec.com/SPs lane.jpg Round 1: Mega Troll EN!I172 1278 440 Gemgenius Crazy CrunchyTrade# Stats of huge Twisters reported Card Stats Quotes Tactical level MasterOver 9000 Adv1 125 WMPUlt eggoBlue File Page 316WPP 944 Converse to TTs build: know it would make easy transition to KatainI would be wise to try to pick maniac alphas on Aklematic. Honestly, stuns are the worst in the game and are considered sign of turbulent terrain. TTs beautifully short animations on these are always going to be meta property, when stuns on Elanna leavoured, jungling jungling and flying slow are the fishworks in The anime. More precisely speaking, he can step up to TBA if she works like climber on butterfly, then on cat,ale and back. asked reddit for its thoughts on laning when he released the very big curse Emptables. Ever since wrote about his hegemony on my other news site, it seems we are back to juke with the topic of playtime or BlackStorm. Should are cheap, breaker to get back? And so ask, if TT does slouch on his build like Alistar ormalfurr, have smart hunch that if Imp or TTeM can really tank harder or play fragile role, after he can supposedly replace, the playmaker will pick on stryrolls or slower side up protector. Barreds on TTs build (crashless auto auto heal, he wouldnt really lose any value factor like Oriana or AK, and tanking AP and INTs) are one of the powerful tank attorrupts of all time. Kennalden and baniac are in the niche, but bull1%s are good at gatstrings and base. Pro coaching: OP police Profile Blog Joined January 2010 New Zealand 38849 Posts #16 Wow! The new sign from ABCR FriUndus member needs to de (Holdaround) as GPL support by Jim IM being joined (@all in part) been mostly worked on for the better half year. think it would be useful if we can shake up the challenge bit. misfamiliarity: Using Gear-Charging. Aprium might be related ushttps://www.starter.com/Aprium/Major ing schedule to charge all monsters Charge monsters using the charge Majority add names have proficiency of gold nodes in tanks and length of output power Core functionality are shown below:endoftextCannot have crack at tech only for at least 45 seconds Beware of spiders to stray us, lets play meta yourself? or prefers an easy setting place various pros and cons for the purposes of sharing ideas with the Snet music.1 leaders cant handle that curation either, they like merge to delete the items rolled and procs into requirements.Since very few people have one formedendoftext noticing totems: armor the Figure 8: sample generated by the MDLM trained on the OWT dataset, using 1,024 sampling steps. 23 endoftext off people for the first time at that point, somehow do not prolonging your woes. Secondly, it is never easy to know the cultures motives. It is also very easy to get firm sense of what you should offer anyone if you do not know what their motives are. Sure, you dont always get the feeling that being wrong is the answer to the issue. But, it will work against you. You understand what they were and they have answered the question. Probably the most widely used saying is that it was after those misery and misery BS and it is still bad choice. There is only single good option. Who is to target if they go so bad over, why go to war with people in black clothing. Some of the approaches that harm the environment are incredible, but deeper level, it is not true without the success of anything useful afterwards. But first, it helps to consider what our choices actually are. It is important to know who is in charge, how they are responding to reality. Have you ever got transition presentation? In case all you are studied making this question already, can see myself suggesting you bring someone or woman to the business club. It needs to be quite informed and clear, then there is so much work ahead that it is fully satisfied just the agenda of bureaucrats in that evil Pigguonious tibboleth. Can you relate to the students at all as almost everyone else? Certainly making decision consists of nothing more than time. That just allows you to be reassured that you never know them better. Not aware of their meaning and abilities. Not letting things go before it starts all over in some other place too Thats mistake. imagine bringing person away would end the conversation. You just cant do that. Give in your data and give yourself platform to assess what means before you decide to mislead. Once you do, the comparators problem for at least two months is the use of the ever changing strategy as an excuse for new situations. Once you view them as voluntary behaviour, this only grosses them out as hindrance. Shining the Spear There wasnt enough time to consider another question of bearings, these are ideas, opposed to necessarily things that matter. What that is opposing about them is that they totally matter. This is why many things are at the gateways of managers and co-workers and banking and government workers. Considered or non-working ideas can exist. They may be incorrect, but they are the product of perceptions of rationality and sincerity. No matter if they are wedged together and land somewhere between good guys because you could get op-room in exchange for playing your part, you can still get it. Managers trying to protect the idea usually resign to someone who opposes it. Do they do good for them than those who do? Or do they actually on better chances than those who oppose they ideas. This was people. If anything, this is stranger problem with people. Good reason is all there is to have. But, my question? For those, though, who fails to place an abhorrent expectation in others, then he rejects our presumptions. He is born of empathy and empathy is not something without his depth understanding however. Suppose have kid who thinks our culture must be equal, that is, because entrepreneurs are liberation activists so whoever controls the culture, in order to be less oppressive, will go into striving for another form. Muscle is not thing that lead to power. It can make democracy. It can make person who is different from people. Invisible people can grow power. People have the muscle to exercise that power. There is an inner strength. People can be potent force against people that cant consciously attain strength. Person won. Some people. However, it is not something we have to choose. This is the opportunity. The opportunity is good. Yet it is easy to see it, like someone being unable to solve puzzle. That is two reasons why you needs to offer the small boy that opportunity very much. It is only distraction and to have to think wouldnt use distraction against oneself. Failure to think is the reason we take autonomous things away. You cannot have the will to live, freedom to do what we choose or make this. Even so were tough at it. We havent got the resources for your mind or your quest. You cant. In order to be successful anywhere from now, you first must possess the power that can be been successful through success both personally and professionally. Even in the first few weeks, there are times when there is strong force in your life who comes with as many options or goals as you are comfortable with. know that you are always jealous when he introduces your well-earned stuff to friends orendoftext Figure 9: sample generated by the UDLM trained on the OWT dataset, using 1,024 sampling steps. 24 endoftext There are charges of battery and assault against Smith and is expected to be arrested at 7pm Mass. Thursday.Those who are on set should contact PlanTheBlossom.com at 800-7075574.endoftextAs the Supreme Court is expected to rule next week over the Foreign Intelligence Surveillance Courts surveillance program to dig up the phone records of citizens and the Internetbased services they use without warrants, the Obama administration says its close to the case, due to its broad purview. The FISA Court has ruled, as it always has, broad executive searches, which violate First Amendment constitutional claims and rely on just show of cause, said Malcolm Poena, senior counsel for intelligence services at the White House. urge people to reconsider the decision and pursued more vigorously in the FISC Court, and perhaps of greater importance, in Congress. For nearly two years, the appellate court has ruled that intelligence officials are not allowed to gather phone records about Americans or foreign citizens without court or employer request, but also without showing-cause warrants, and that the government cant ask for private stored information only if critical telecommunications infrastructure contains threat to the public. It is not clear why the administration should use DoMet before curtailing companies that honor terms for the surveillance against foreign targets, its predecessor, Marcy Wheeler of San Edward, said in statement last week. Dennis J. Romero, founder founder of Public Knowledge, said that the legality of such requests is an ongoing issue because of different interpretation of the law, and that his organization is trying to draw distinctions between firms culpable and not culpable. service to consumers is not always that of several firms, he said. It is our call that the president lift orders requiring that. Likewise, is encouraged by orders enacted by previous administrations to reconsider this interpretation and to seek the full implications of the decision and to bring the case to the Supreme Court. Contact us at editors@time.com.endoftextOur Planets Eclcl now joins the fragmented bluster of the Orion cluster. Scientistsve mapped Earths great primate cluster that once populated it throwing victory to decades of thinking that ancient reptiles and gibes were born. The Eclcl originally formed by series of young animals losing their home, dropping off eggs on dying moors between Norths Centaurus constellation and Souths Adu constellation. The more than 280 million-year-old (62.5 billion years ago) reptiles set out on the austere journeys to new location, the eureka as it is now known. Eventually they realised the shipshape was no longer safe, so they abandoned the colonies. We have always wanted to get picture of the last nursery location and whether it flowed into another, said co-author Calvei Richmond of the Atriemia de Investigadamento Cholsa de Brazil. We are glad to see our map does seem to overturn longstanding fossil record and traditional hypotheses. Richmond has spent year probing the Ecl via three satellites on the ground but detailed map showing the clusters history has been elusive. The paper map uses observations from Brazils MARS Digital Surveys EPOS Landgrab Connector, which has been taking off from around the globe. The study appears in the high order journal Nature. Reference: 2012-03-10 DOI: 10.1688/gpaid.imob.2012.10endoftextJamout Bling! Jamout Bling is one of the most popular, month monthly OrganConner MO. In the live group-match MOBA, players will have to compete against variety of smoothingles to attempt prizes. At least have us right, Jamout Bling is solid fun with great opportunity to make name for itself. What is Jamout Bling? Jamout Bling is small community that operates primarily by hosting prepaid prize pools of approaching 3,000, and the team made it easy for you to have fun with other Blayers. Entry fee is $300, and get ready! Why we jam our house with the community! Jamout Bling has run for about six months by group of folks living within block of Willow Creek! We have weekly jams on the spot and bantering with local Jamaican snotty bands. Whens the Jam? Until Friday at 10am! As much as you want to have fun, come out! Its very small crowd, bring your critters too, and well be hosting dartboard, so just put your controller on if you have it on.endoftext Figure 10: sample generated by the LDDM-M trained on the OWT dataset, using 1,024 sampling steps. 25 endoftextOne of the most enduring stories that will eventually lead us to this point is the Oil War. Between 1992 and early 2000, the United States proved to be Chinas largest central defense partner, having currency turnover of over $100 billion. Astonishing Americas growing closer to Iran and Russia, China to turn away from Iran and relinquish its oil reserves in the Middle East. It started blocking deals with the United States. China feared that if it held Americans ransom with the oil, Middle Eastern regimes would move away from the United States as an indispensable partner to truly China, as well as acquiring the so-called Green economy. If story went more like this, Iran might not even be surprised by this grand strategy. Instead, it decided to respond by physically seizing some of the vast oil reserves north of the border, near Saudi Arabia near Basra. That way, it tapped into steady streams of energy supplies from Russia and Iran. China shelled an Iranian test facility and checked out its production network for amylizing agent within its first thermonuclear megaton. Ultimately its sole concern was to hold on oil. The price for all the Iranians oil misadventures was massive American imports. Today, E.G.A.s sales between the Middle East and Pakistan rose by 29 percent between 2000 and 2015. Ten years ago, the cost of Americas largest drone was under single-digit dollars. Today, Americas largest Predator drone costs whopping $32 billion. Imagine the consequences of Reagans wars, such as the encroachment of the Soviet Union in the 1980s, and his successors wars as well Korea, Vietnam, and especially East Vietnam. Likewise, oil alone is no longer source of revenue but big driver of investment to U.S. companies. It funded those countries, like the African Republic, which boasts the biggest military in Africa, where the price of honest (U.S). oil imports was about the price of oranges in 2000. Today, the U.S. trade on oil exports is close to $180 billion. Americas growing dependence on oil in the South Pacific, as well as all of Southeast Asia, need to grow even more as technology advances. The core challenge to secure balance to sustain growing liabilities, wrote Vern van den deWalk, senior technology analyst at Fortune Global Trading Service, Fortunes analytical unit located in London, in note he wrote. Not so much dictated by technology but it will be the name of the game. The huge challenge of cybersecurity will soon prove to be an important factor in the partnerships longevity, as well as the longer term consequences. While the presidents role, as long as it has been known, has been the pursuit of smart solutions, what is at the center of that may soon shift as the decade moves forward. Threats like recent data breaches and the ensuing military conflict could further wind up the cybersecurity and defense industry worlds of government and defense. Dalea McGovern, CTO at the upstart Lockheed Martin, designs the F-35 Multi-Role Vehicle. (Reuters) It does not take bit of imagination to help imagine the likely scenario down the road. Early on likely, the world arrangements of security and defense would be one of plates, complete with large armed forces, an enquacious private sector, and an ability to participate in multiple major markets globally. Simultaneously, that industry would contain significant and esteemed government functionaries that tend to solve their respective problems, as execances are, the costs of arranging the two would vary. Between 2004 and 2015, military agencies handled roughly gazillion Pentagon data requests/day. If you look at estimates produced by the Department of Justice, though, the United States intelligence agency accounted for 31 percent of that load. In these projections, the Pentagon handled data roughly 400,000 civilian shooting incidents every day. No single network or information infrastructure could properly offload such massive streams of requests and devote such enormous amount of time and resources to gathering them all. The situation would reflect an increasingly wrangling and complex complex enterprise in our days advanced strategic world. Part of that potential comes from how inelastic things are now. Today, the military employs more than half of its service members and twenty percent its contractors. In fact, little less than one in five of the armed forces had served between 1900 and 1985. Before that, about couple of out of five men fought in the Civil War. The game as small and timid as that could become, and it will require deep strategic and professional transformation for the United States to play the role that it can. The Bad Most people dont imagine, butendoftext Figure 11: sample generated by the LDDM-U trained on the OWT dataset, using 1,024 sampling steps."
        }
    ],
    "affiliations": [
        "EPFL",
        "KAIST",
        "Microsoft",
        "NYU",
        "SAP"
    ]
}