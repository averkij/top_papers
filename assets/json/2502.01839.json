{
    "paper_title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification",
    "authors": [
        "Eric Zhao",
        "Pranjal Awasthi",
        "Sreenivas Gollapudi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one -- typically by verifying each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation that uses only random sampling and direct self-verification results in sustained performance improvements that, for example, elevate the Gemini v1.5 Pro model's reasoning capabilities past that of o1-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts -- chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies."
        },
        {
            "title": "Start",
            "content": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification Eric Zhao1,2 Pranjal Awasthi1 Sreenivas Gollapudi1 1Google Research, 2UC Berkeley 5 2 0 F 3 ] . [ 1 9 3 8 1 0 . 2 0 5 2 : r Abstract Sampling-based search, simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best onetypically by verifying each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up minimalist implementation that uses only random sampling and direct self-verification results in sustained performance improvements that, for example, elevate the Gemini v1.5 Pro models reasoning capabilities past that of o1-Preview on popular benchmarks. We partially attribute the scalability of samplingbased search to phenomenon of implicit scaling, where sampling larger pool of responses in turn improves verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contextschains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce benchmark to measure progress on these deficiencies."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in language models highlight the importance of test-time compute scaling wherein one uses more compute during inference to enhance reasoning capabilities [OpenAI, 2024, Team, 2025, Agarwal et al., 2024, Wei et al., 2022, Yao et al., 2023, Aky√ºrek et al., 2024]. There are many methods for increasing test-time compute usage, including implicitly encouraging longer responses via reinforcement learning [OpenAI, 2024, Team, 2025] or explicitly via prompting [Wei et al., 2022, Yao et al., 2023]. However, sampling-based searchan instance of the generate-and-test approach where model generates many responses in parallel, e.g. via random sampling or delegation, and selects the best oneremains one of the most natural and fundamental paradigms. In addition to being complementary with other test-time compute scaling strategies, it also has the unique advantage of being embarrassingly parallel and allowing for arbitrarily scaling: simply sample more responses [Cobbe et al., 2021, Wang et al., 2023]. As result, sampling-based search plays an increasingly crucial role as language models are set loose on Corresponding author: eric.zh@berkeley.edu. Code and benchmark: github.com/google-research/google-research/sampling_based_search. 1 frontier mathematical and scientific problems where inference compute budgets reach thousands of dollars or more per problem. Though recent works demonstrate the benefits of sampling-based search [Cobbe et al., 2021, Wang et al., 2023, Xue et al., 2023], many questions remain as to what scaling trends govern this fundamental test-time compute scaling strategy. To develop this understanding, we study minimalistyet remarkably effectiveinstantiation of sampling-based search that uses language model [Gemini Team, 2024] to both generate set of candidate responses via random sampling and select the best one by verifying each response in natural language. In this setup, we address the question: what test-time scaling trends emerge as we scale both the number of sampled responses and verification capabilities? In particular, what are the limits of scaling this simple sampling-based search paradigm and how much does one need to continuously scale verification capability as one scales up search? Our findings. We first identify scaling trends demonstrating that reasoning performance continues to improve with sampling-based search even as test-time compute is scaled well beyond the point where the performance of self-consistency [Wang et al., 2023] saturates. At sufficient scale, even our minimalist implementation provides significant leap in reasoning accuracy, lifting Gemini v1.5 Pro performance beyond o1-Preview, and Gemini v1.5 Flash beyond Gemini v1.5 Pro, on reasoning benchmarks such as LiveBench [White et al., 2024] and the AIME [MAA, 2024], exhibiting sustained power-law scaling on the latter. This not only highlights the importance of sampling-based search for scaling capability, but also suggests the utility of sampling-based search as simple baseline on which to compare other test-time compute scaling strategies and measure genuine improvements in models search capabilities. We then attribute much of the strong scaling trends of sampling-based search to an implicit scaling phenomenon. Contrary to the intuition that sampling more responses should impose greater burden on the verifier and reduce verification accuracy, we observe that scaling sampling indirectly enhances verification accuracy. At high-level, this is because well-written responses are easier to verify than poorly written responses, and scaling sampling widens the pool of well-written candidates. We further identify two effective strategies for scaling verification capabilities using test-time compute: (1) directly comparing candidate responses and (2) task-specific rewriting of candidate responses. The former mitigates core weakness of language models, which struggle to identify mistakes and hallucinations unless given their locations [Tyen et al., 2024], by leveraging the fact that differences between candidate responses provide strong signal for where errors might be located. The latter leverages our observation of output style suitability where chain-of-thought output formats are beneficial when generating responses but harder to verify than more formal, mathematically conventional writing styles. Surprisingly, while effective verification can be easily elicited from frontier models by communicating these strategies, we observe that frontier models have remarkably poor out-of-box verification capabilities and introduce new benchmark to quantify these deficits. Preview and outline. Table 1 summarizes our first finding: that, with effective self-verification, simply scaling sampling-based search is sufficient to approach state-of-art performance on reasoning and math benchmarks (AIME 2024 [MAA, 2024], LiveBench Math, LiveBench Reasoning [White et al., 2024], and the Berkeley MATH dataset [Hendrycks et al., 2021]). It depicts the accuracy of the Gemini v1.5 Pro model [Gemini Team, 2024] when only one solution2 is attempted per question (Pass@1), when 200 solutions are attempted and the most common final answer is selected (Consistency@200, Wang et al. [2023]), 1The o1-preview-2024-09-12 numbers in Table 1 use publicly reported figures, with MATH and AIME figures sourced from the OpenAI blog post [OpenAI, 2024], and LiveBench figures sourced from the LiveBench leaderboard (livebench.ai). We found the performance of o1-Preview as accessed through the OpenAI API to slightly differ with publicly reported figures, e.g. scoring 26% not 44% on AIME, and scoring 77% not 67% on LiveBench Reasoning. 2As we focus on answering reasoning problems, we use model responses and model solutions interchangeably. Method AIME MATH LiveBench Math LiveBench Reasoning Pass@1 Consistency@200 Consistency@1,000 Verification@200 o1-Preview@1 1 / 15 4 / 15 3 / 15 8 / 15 7 / 15 426 / 500 460 / 500 460 / 500 467 / 500 428 / 500 104 / 200 118 / 200 120 / 200 135 / 200 131 / 63 / 140 75 / 140 73 / 140 97 / 140 95 / 140 Table 1: Accuracy rates of the Gemini v1.5 Pro model using sampling-based search (Verification@200) on reasoning benchmarks, compared to other inference methods and o1-Preview performance. Verification@200 consistently improves on Consistency@200 and surpasses o1-Preview.1 Each score reflects single run, due to the high expense of search at this scale (see Section 5). and under sampling-based search, when 200 solutions are attempted and scored for correctness with the highest scorer selected (Verification@200, Algorithm 1). With sampling-based search (Verification@200), Gemini v1.5 surpasses the performance of o1-Preview, model explicitly trained on reasoning problems to leverage significant test-time compute and perform internal search. The rest of this paper is devoted to studying the three key factors behind the numbers in Table 1. Section 2.1 analyzes the remarkable scalability of sampling-based search, as one varies both the compute spent on search and verification; Section 2.2 analyzes the phenomenon of implicit scaling and its role in driving this scalability; and Section 3 discusses important principles for scaling self-verification capability, which may be of independent interest. We also highlight deficits in the verification capabilities of frontier models with new benchmark in Section 6. Technical details and detailed discussion of related work are found in Sections 5 and 7 respectively. Algorithm 1 Sampling-Based Search (Verification@kinf ) Require: Prompt Q, language model LM, scaling parameters kinf , kverif , ktie. 1: Populate with kinf samples from LM(Answer Q). 2: for each candidate response si do 3: 4: Gather the highest-scored response SBest = {si [kinf ], Avg(Vi) maxj[kinf ] Avg(Vj) 0.05}. 5: if SBest = 1 then 6: 7: else 8: Populate Vi with kverif samples from LM(Return 1[response si to is correct]). Return response si where = maxj[kinf ] Avg(Vj). for each pair of candidate responses (si, sj) (cid:0)SBest 2 Stage 1: Generate Responses Stage 2: Verify Responses Tie-Break: Compare Responses (cid:1) do Populate Ci,j with ktie samples from LM(Which of responses {si, sj} to is correct?). Return response si where is the winner of the most matchups {Ci,j si, sj SBest}. 9: 10:"
        },
        {
            "title": "2 Scaling Trends of Sampling-Based Search",
            "content": "This section examines how reasoning capability scales with two fundamental test-time compute axes: Search refers to the compute used to discover candidate solutions. In this section, our knob for scaling search is the number of responses sampled for each reasoning problem (kinf in Algorithm 1). Verification refers to the compute used to scrutinize candidate solutions. Our knob for scaling verification is the number of verification scores we compute and average over per solution (kverif ). 3 Figure 2.1: Heatmap of Gemini v1.5 Pro accuracy rates using sampling-based search (without tiebreaking) as the number of responses generated (x-axis) and verification attempts (y-axis) increase. Warmer colors indicate higher accuracy (cubic scale). The largest gains occur when scaling both search and verification, with the strongest trend on AIME. For computational reasons, this section uses streamlined form of Algorithm 1 that omits tie-breaking. This, for example, results in significant underestimates of Verification@k on MATH (see Table 3). All figures are averaged over 20 random seeds, where each run subsamples solutions and verification scores from primary run that sampled 200 solutions per question and 50 verification scores per solution."
        },
        {
            "title": "2.1 Scaling Trends",
            "content": "Figure 2.1 provides heatmap of Verification@k on each benchmark in Table 1 as we scale search and verification. In addition to clear burn-in costs along both axes of scale, we can observe that the largest performance gains are realized when search and verification are both scaled. These trends also indicate that the performances of sampling-based search, as reported in Table 1, have not yet been scaled to saturation on these benchmarks. This scaling trend is strongest on the AIME benchmark, where performance is bottlenecked by (search); we attribute this bottleneck to the difficulty of the AIME questions resulting in correct solutions only appearing with very low probability (see Table 2). 4 Figure 2.2: Plot of Gemini v1.5 Pro accuracy rates using sampling-based search (without tie-breaking and with kverif = 50) on ambiguous questions only as the number of responses generated increases. question is ambiguous when the model generates at least one candidate response with correct final answer. Accuracy on ambiguous questions increases with search. 2."
        },
        {
            "title": "Implicit Scaling",
            "content": "Scaling sampling-based search along the search axis by sampling more solutions, i.e. increasing k, should have two effects on performance that partially cancel out: (1) the verifier must discriminate between more solutions, increasing the likelihood of error and (2) the generator is more likely to produce at least one solution that reaches correct final answer, i.e. Pass@k increases. To isolate the first effect, we study the models Verification@k accuracy on ambiguous questions: questions where at least one of the models candidate solutions reaches the correct final answer (note that Pass@k equals the number of ambiguous questions). Figure 2.2 and Figure 2.3 do exactly this, plotting Verification@k accuracy measured only on ambiguous questions from each benchmark. To reduce noise in these figures, we deterministically omit benchmark questions that Consistency@200 answers correctly or where, with high probability, 50 random responses result in either all correct or all incorrect final answers. After controlling for the growth of Pass@k, we should expect trend of decreasing accuracy if we increase but keep the number of verification attempts constant. However, Figure 2.2 shows the reverse trend: accuracy increases with k. This demonstrates an implicit scaling of verification accuracy, where increasing the number of generated responses increases not only the chance that at least one response is correct (Pass@k) but also the chance that at least one of the correct responses is of higher quality. Here, quality can be understood as the rigour or flawlessness of response; lower quality solution may be generally correct but fail to justify non-trivial step or err in non-critical step of its reasoning. Implicit scaling suggests that verification should become more accurate, and sampling-based search should become more effective, with the use of more capable base models that produce more sound reasoning and compelling proofs of correctness. Because the number of ambiguous questions strictly increases with more candidate solutions, the implicit scaling effect also explains the overall accuracy scaling gains in Figure 2.1: larger increases both the number of ambiguous questions (Pass@k) and accuracy on the set of ambiguous questions."
        },
        {
            "title": "2.3 The Long Tail of Response Distributions",
            "content": "We can directly observe Verification@k scaling beyond the saturation point of Consistency@k in Figure 2.4, where we plot their performance after fixing the number of verification attempts at 50. On AIME, the most technically challenging benchmark, Verification@k demonstrates power law scaling even as Consistency@k begins to plateau. The rapid saturation of Consistency@k can be attributed to the fact that, while it 5 Figure 2.3: Heatmap of Gemini v1.5 Pro accuracy rates using sampling-based search (without tiebreaking) on ambiguous questions only as the number of responses generated (x-axis) and verification attempts (y-axis) increase. Warmer colors indicate higher accuracy (linear scale). question is ambiguous when the model generates at least one candidate response with correct final answer. Accuracy on ambiguous questions increases with search (x-axis). Figure 2.4: Line graph depicting the accuracy rates of the Gemini v1.5 Pro model using samplingbased search as the number of candidate responses generated is scaled upwards. The number of verification attempts is fixed at 50 for all plots. The depicted accuracies are obtained without tiebreaking and may be lower than reported elsewhere. Verification@k improves with even when Consistency@k stagnates on AIME and LiveBench Reasoning. 6 is effective at small scales in averaging out noisy mistakes, it necessarily plateaus as it converges on the most probable response; for example, Consistency@50 has the same accuracy as Consistency@10,000 on AIME. Consider cheaply sampling vast set of solutions from weak but ergodic model: Consistency@k is unlikely to return correct solution, but an effective verifier should still be expected to detect rare but correct solutions in the long-tail of the response distribution. We find an example of this on the AIME 2024 exam, where the Gemini v1.5 model struggles to identify the correct answer to Problem 11 on Exam II. Table 2 shows the final answers from 200 randomly sampled Gemini v1.5 solutions, of which only one is correct (601, in green). Consistency returns the incorrect answer of 1 (in red), which appears in over half the responses. In contrast, Verification successfully identifies the solution reaching the correct answer from the response distributions long-tail, assigning 36% score to each solution reaching final answer of 1 but 98% score to the single solution reaching 601. Scaling verification capability is key Problem 11, AIME 2024 Find the number of triples of nonnegative integers (a, b, c) satisfying + + = 300 and a2b + a2c + b2a + b2c + c2a + c2b = 6, 000, 000. Verification Score Final Answer # Solutions 0.98 0.76 0.52 0.40 0.38 0.36 0.22 0.20 0.18 0.16 0.14 0.06 0.04 0.04 0.00 601 6 0 7 4 1 10 3 301 45451 101 2 45151 303 (Correct) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) (Wrong) 1 11 14 21 10 124 2 9 1 1 2 1 1 1 1 Table 2: The final answers identified by the Gemini v1.5 Pro model to Problem 11 on AIME 2024, sorted by verification score and annotated with their multiplicity in 200 solution generations. The correct final answer (green) is only found by 1 generated response whereas Consistency@200 selects an incorrect final answer (red) that is found by 124 generated responses. to driving improved search, allowing for discerning between answers that appear correct with 98% vs. 76% confidence. The fact that verification can be used to so effectively leverage the long-tail of model response distributions also suggests that Pass@k, not Pass@1, should be the key performance metric for search applications. Existing post-training techniques (e.g., reinforcement learning from human feedback (RLHF) [Ouyang et al., 2022]) which explicitly optimize for Pass@1 may potentially be doing so at the expense of Pass@k and inhibiting search capability."
        },
        {
            "title": "3 Effective Self-Verification in Natural Language",
            "content": "In the process of scaling sampling-based search, we identified two general principles for eliciting more accurate language model self-verification, that may be of independent interest. 1. Compare responses to localize errors. Disagreements between candidate solutions strongly signal the potential locations of their errors. This can be leveraged to combat the fact that language models 7 have low recall (i.e., often overlook) when asked to identify mistakes and hallucinations [Tyen et al., 2024, Kamoi et al., 2024a], as models are able to identify errors when provided their locations [Tyen et al., 2024]. Specifically, we can improve the self-verification of candidate response by providing the verifier with other responses to compare the candidate againstan instance of implicit scaling. 2. Rewrite responses for output style suitability. The optimal output style of language model should depend on the task. Writing in linear chain of thought which includes detailing reasoning before committing to claimis effective when generating responses (search) [Wei et al., 2022]. However, responses are easier to verify when written rigorously, hierarchically, and modularly. This can be leveraged by having verifiers first rewrite candidate responses in, e.g., an expanded mathematically conventional theorem-lemma-proof format rather than directly evaluating chains-of-thought. These principles also provide levers for scaling self-verification capability with test-time compute, including by (1) sampling and providing verifiers with more responses to compare between and (2) rewriting responses with increasing rigour and structure."
        },
        {
            "title": "3.1 Sampling-Based Search Implementation",
            "content": "We now detail our minimalist implementation of sampling-based search (summarized in Algorithm 1) that uses only parallelizable blackbox queries to language model. It generates candidate responses by randomly sampling from models and select responses by asking models to self-verify; prompts are identical across all benchmarks and provided in the source code. Step 1: Generate Candidate Responses. language model generates kinf candidate responses (candidate solutions) in parallel to each question, using temperature œÉinf . Step 2: Verify Candidate Responses. language model generates kverif binary verification scores for each candidate in parallel, indicating whether its final answer is correct. Each scoring attempt is single conversation thread that rewrites the response as theorem, supporting lemmas, and proofs (examples in Appendix D) and systematically scans for errors. The highest scoring response is selected. Tie-Break: Compare Candidate Responses. When the three highest scoring candidates score within 5% of one another and disagree on the final answer, language model directly compares the responses in pairwise matchups. Each matchup is single conversation thread that identifies where responses diverge and, at each such point, determines which side is correct. Each matchup is repeated ktie = 100 times. The response with the most wins in the round-robin tournament is selected."
        },
        {
            "title": "3.2 Ablation Studies",
            "content": "We can individually ablate the practices of comparing and rewriting candidate responses to confirm their role in eliciting greater verification capability. Ablating comparisons. The step of asking models to directly compare candidate solutions with similar verification scores significantly increases sampling-based search performance. This is demonstrated in Table 3, where we depict the accuracy rates from Table 1 alongside the accuracy rates after ablating the tie-breaking step. These comparisons have the greatest impact when models struggle from low recall and excessively assign high verification scores. On the MATH benchmark, which sees the greatest lift from comparisons, the average verification score of the top 3 candidate responses is nearly 90%. Recall that, as 8 result, the figures reported in Section 2 that omit tiebreaking significantly underestimate sampling-based search performances (Verification@k). Dataset Cons@ Verification@200 Without With Tie-Break MATH LiveBench Math LiveBench Reasoning AIME 460 / 500 118 / 200 75 / 140 4 / 14 457 / 500 125 / 200 94 / 140 7 / 14 467 / 500 135 / 200 97 / 140 8 / 14 Table 3: Accuracy rates of Gemini v1.5 Pro using sampling-based search, with and without tiebreaking. Tie-breaking provides most of Verification@200s gains on Consistency@200 (Cons@200) on MATH and LiveBench Math, and smaller gains on AIME and LiveBench Reasoning. Ablating rewritings. We explored limited number of prompts for self-verification, including prompts which omit instructing the model to rewrite responses. We did not perform further prompt optimization and expect refinements would boost accuracy. Table 4 shows each prompts probability of mislabeling correct solutions (false positive) and incorrect solutions (false negative), with the former generally having more severe impact on downstream performance. We evaluated these prompts on 1,080 candidate responses to 54 level-5 questions from the MATH training split, and 120 candidate responses to 6 questions from AIME 2023. response is marked as incorrect if, of 20 verification attempts, the number finding an error in the solution exceeds the equal error rate threshold. Main refers to manually written prompts used in our experiments. Shortened refers to shorter variant of Main that omits, e.g., instructions to avoid truncation. Without Rewrite refers to variant of Main that omits instructing the verifier to first rewrite responses. Split-Context refers to variant of Main that creates separate conversation threads to individually verify pieces of the response. The gap between the performance of Main and Without Rewrite demonstrates that ablating the rewriting of solutions negatively impacts verification performance. Similarly, the gap with Split-Context demonstrates that splitting the verification process into separate conversation threads sharply decreases performance due to low precision, which we attribute to miscalibration."
        },
        {
            "title": "FPR FNR FPR FNR",
            "content": "Main Shortened Without Rewrite Split-Context 14% 17% 16% 19% 17% 17% 18% 23% 7% 7% 11% 11% 7% 7% 12% 14% Table 4: Verification scoring accuracy rates of the Gemini v1.5 Pro model for various prompts. False positive rate (FPR) refers to how often correct response is labeled as incorrect; false negative rate (FNR) refers to how often an incorrect response is labeled as correct."
        },
        {
            "title": "4.1 Smaller Models",
            "content": "Model Method AIME MATH LiveBench Math LiveBench Reasoning Pro v1.5 Flash v1.5 Pass@1 Consistency@200 Verification@ Pass@1 Consistency@200 Verification@200 1 / 15 4 / 15 8 / 15 2 / 15 3 / 15 5 / 15 426 / 500 460 / 500 467 / 500 407 / 500 440 / 500 445 / 500 Pro+Flash v1.5 Verification@ 7 / 15 456 / 500 104 / 200 118 / 200 135 / 200 96 / 200 92 / 200 104 / 200 119 / 200 63 / 140 75 / 140 97 / 65 / 140 84 / 140 84 / 140 84 / 140 Table 5: Accuracy rates with sampling-based search using either the Gemini v1.5 Pro model to both generate and verify responses (Pro), Gemini v1.5 Flash to both generate and verify responses (Flash), or Gemini v1.5 Pro model to generate responses and v1.5 Flash to verify responses (Pro+Flash). Verification@200 exceeds Consistency@200 for all model choices, while Pro+Flash Verification@200 matches or exceeds Pro Consistency@200. We also observe sampling-based search to be powerful tool for enhancing smaller, lower-cost models. Here, we apply sampling-based search to Gemini v1.5 Flash model, which has nearly 20x lower inference cost than Gemini v1.5 Pro. Table 5 lists the performance of using the Flash model to evaluate candidate responses generated by the Pro model (Pro+Flash), and the performance of using the Flash model end-to-end for sampling-based search (Flash). Sampling-based search still provides significant improvement in performance for both Flash and Pro+Flash. Moreover, Verification@200 still provides significant improvements over Consistency@200, albeit lesser in magnitude than for end-to-end In addition, Flash Verification@200 using Gemini Flash is competitive with Pro use of Gemini Pro. Consistency@200, while Pro+Flash Verification@200 exceeds Pro Consistency@200. We highlight that Pro+Flash Verification@200 has roughly the compute cost of Consistency@500as our sampling-based search implementation is minimally optimized for efficiency, we expect costs to further decrease."
        },
        {
            "title": "4.2 Performance by Subtask",
            "content": "Dataset Cons@200 Verif@200 Improvement (%) Pass@200 # Questions Berkeley MATH AIME 2024 Web-of-Lies-v2* Spatial* Zebra Puzzle* Competition AMPS Hard Olympiad 92.0% 26.7% 75.5% 33.3% 50.0% 66.2% 70.6% 25.0% 93.4% 53.3% 91.8% 46.7% 67.4% 83.1% 77.7% 22.7% (Abs) 2% 100% 22% 40% 35% 26% 10% 9% (Rel) 20.0% 57.1% 66.5% 21.5% 36.4% 63.1% 33.5% -11.2% 99.0% 73.3% 100.0% 95.6% 97.8% 93.0% 91.8% 45.5% 500 15 49 45 46 71 85 44 Table 6: The Pass@200, Consistency@200 (Cons@200), and Verification@200 (Verif@200) accuracy rates of the Gemini v1.5 Pro model using sampling-based search. LiveBench Math and LiveBench Reasoning* numbers are divided per task. Absolute % Increase (Abs) is the percentage improvement of Verification@200 over Consistency@200. Relative % Increase (Rel) is (Verification@200 - Consistency@200) / (Pass@200 - Consistency@200). The LiveBench benchmarks each consist of multiple subtasks. In Table 6, we break down the numbers reported in Table 1 for each of these subtasks. We also provide in Table 6 the Pass@200 scores of the Gemini Pro model, which measure the probability that of 200 attempted responses to question at least one is correct. Pass@200 upper bounds what one can hope to achieve through Verification or Consistency. Verification provides the greatest gains on AIME 2024, Web-of-Lies, Competition, and Zebra Puzzle. In contrast, Verification does not improve on Consistency on the Olympiad task of the LiveBench Math benchmark. We attribute this to the unique question design of LiveBench Olympiad task questions, which is incompatible with our implementation of Verification (see Appendix B.2)."
        },
        {
            "title": "5 Technical Details",
            "content": "All experiments are run on Google Cloud with Gemini v1.5-Pro-002 and Gemini v1.5-Flash-002 models dated to September 2024. Unless otherwise specified, the default parameters for our implementation of sampling-based search (Section 3) are kinf = 200, œÉinf = 1.5, kverif = 50, œÉverif = 1, and maximum of 8,192 output tokens per query. For all benchmarks, the scoring of candidate responses is performed using language model rather than literal string comparison; details are in Appendix A.2. Preliminary scoring. When generating kverif = 50 verification scores per candidate solution is too expensive, we first generate kverif = 10 preliminary verification scores and discard candidate solutions with an average score below 0.2. If final answer is represented by more than 15 candidate responses, only the top 15as measured by average preliminary score, tie-breaking randomlyare kept. This results in smaller pool of candidate solutions for which we compute all kverif = 50 verification scores. Preliminary scoring is used on all datasets except AIME, which consists of 15 questions. Compute. On AIME, the verification process involves 32,000 characters (roughly 13,000 tokens) of model output. Extrapolating from these figures, running the full sampling-based search pipeline on question for kinf = 200 and kverif = 50 requires 200 50 13, 000 130M output tokens. At around 11 $5/1M output tokens (public pricing of Gemini v1.5 Pro), this evaluates to approximately $650 in cost. Preliminary scoring reduces usage of output tokens by roughly 70%, resulting in per-question cost of $200. The use of Gemini Flash for verification further decreases cost to $12 per question. Datasets. Our MATH benchmark consists of 500 questions from the PRM800K [Lightman et al., 2024] test split of Berkeley MATH [Hendrycks et al., 2021]. Our LiveBench Math benchmark consists of 200 randomly subsampled questions from the 368 available as of October 21st 2024, including AMC12 2023, AIME 2024, SMC 2023, USAMO 2023, IMO 2023, and synthetic math questions [White et al., 2024]. Our LiveBench Reasoning benchmark consists of 140 questions from the 150 available as of October 21st 2024, including Zebra puzzles, Web-Of-Lies, and Spatial reasoning questions [White et al., 2024]. Our AIME benchmark consists of the 15 questions in Exam II of AIME 2024 [MAA, 2024]."
        },
        {
            "title": "6 A Verification Benchmark",
            "content": "Frontier language models demonstrate remarkable mismatch between their problem-solving capabilities and poor out-of-box verification capabilities. These limitations have largely been attributed to the inability of current language models to self-diagnose hallucinations or enforce rigour [Zhang et al., 2023, Orgad et al., 2024, Snyder et al., 2024, Kamoi et al., 2024a, Tyen et al., 2024, Huang et al., 2024]. However, our findings that models can be directed to accurately perform verifications at scale suggest that these out-of-box limitations can be addressed with standard methods like instruction tuning. We compiled set of challenging reasoning problems and candidate solutions to provide benchmark for these deficits. Each entry in this benchmark consists of question, correct candidate response, and an incorrect candidate response, and is manually curated from the residuals of our sampling-based search experiments (Section 3). An example entry from this benchmark can be found below (see Appendix for more). Question from LiveBench Reasoning (Web-of-Lies Puzzle) In this question, assume each person either always tells the truth or always lies. The person at the campground thinks their friend is lying. Mateo is at the aquarium. The person at the restaurant says the person at the hotel lies. Farid is at the movie theater. The person at the movie theater says the person at the campground lies. Ryan is at the shopping mall. The person at the cafe says the person at the campground lies. The person at the observatory says the person at the museum lies. The person at the museum says the person at the restaurant tells the truth. The person at the campground says the person at the airport tells the truth. The person at the campground tells the truth. Luna is at the restaurant. The person at the movie theater saw firetruck. Isabella is at the observatory. The person at the amusement park says the person at the museum tells the truth. The person at the restaurant says the person at the shopping mall tells the truth. Liam is at the beach. Ayaan is at the airport. The person at the restaurant says the person at the beach tells the truth. Maya is at the amusement park. The person at the restaurant says the person at the aquarium lies. The person at the restaurant says the person at the movie theater lies. The person at the library says the person at the campground tells the truth. Elowen is at the hotel. Priya is at the library. The person at the gym says the person at the restaurant tells the truth. Kehinde is at the cafe. Anika is at the museum. Quan is at the campground. Soren is at the gym. Does the person at the museum tell the truth? Does the person at the restaurant tell the truth? Does the person at the movie theater tell the truth? Think step by step, and then put your answer in **bold** as list of three words, yes or no (for example, **yes, no, yes**). If you dont know, 12 guess. Correct response Incorrect Response Due to Hallucination Heres how we can break down this logic puzzle: 1. **Quan is at the campground and tells the truth.** This is given fact. ... 4. **The person at the restaurant (Luna) says the person at the hotel (Elowen) lies.** We dont know yet if Luna is lying or telling the truth. ... 10. **Since Luna tells the truth about Farid, Luna *always* tells the truth.** This unlocks several other pieces of information. ... 13. **The person at the cafe (Kehinde) says the person at the campground lies.** Since Quan tells the truth, Kehinde is lying. Therefore: * **Anika (museum):** Tells the truth. * **Luna (restaurant):** Tells the truth. * **Farid (movie theater):** Lies. **yes, yes, no** Heres how we can solve this logic puzzle: 1. **Quan is at the campground and tells the truth.** This is given. 2. **The person at the airport (Ayaan) tells the truth.** Quan (campground) says the person at the airport tells the truth, and Quan tells the truth. ... 4. **Luna (restaurant) lies.** The person at the restaurant says the person at the hotel (Elowen) lies. Later, the person at the restaurant says the person at the campground (Quan) lies. Since Quan tells the truth, Luna (restaurant) must be lying. ... lies** * Therefore: * **Anika (museum): **Luna (restaurant): lies** * **Farid (movie theater): lies** **Answer: no, no, no** Figure 6.1: Example of an entry in our verification benchmark. The question is sourced from the LiveBench Reasoning benchmark, and the two responses are generated by Gemini v1.5 Pro. The green response has the correct final answer; the red response has the wrong final answer due to hallucinating non-existent clause. On each entry, our benchmark studies verification accuracy on two tasks: 1. Scoring task. When given only the question and one of the responses, is the model able to discern the correctness of the response? 2. Comparison task. When provided the whole tuple with the correctness labels of the responses masked and guarantee that at least one response is correct, is the model able to discern which response is correct and which is incorrect? The scoring task is also evaluated over separate set of (question, response) pairs where the response reaches the correct final answer by coincidence but contains fatal errors and should be labeled by reasonable verifier as being incorrect; an example can be found in Appendix C. In the scoring task, models are provided only with the task description; in the comparison task, models are provided only with the task description and suggestion to identify disagreements between responses in its reasoning. Table 7 lists the baseline performances of current commercial model offerings on this benchmark. Gemini v1.5 Pro is omitted from the benchmark as the entries in the benchmark are curated from the residuals of Gemini v1.5 Pro. The prompts used in Table 7 are provided in Appendix A.4. 13 As we previously observed, and has been noted in prior works [Tyen et al., 2024, Kamoi et al., 2024a], verification errors are typically due to low recall. Even the easier comparison task, models perform only marginally betterand often worsethan random chance. In many cases, Consistency@5 performs worse than one-shot inference because Consistency simply averages out noise from an output distribution, meaning that model biased towards producing an incorrect answer will do so with higher probability under Consistency. Addressing these deficits in verification capabilitieswhich we see as low-hanging fruit for post-trainingwould enable not only better sampling-based search, but also other downstream applications of verification including reinforcement learning [e.g. OpenAI, 2024, Team, 2025], data flywheeling [e.g., Welleck et al., 2022], and end-user experience (see Section 7 for further discussion). Model Metric Scoring Accuracy Comparison Accuracy GPT-4o Claude 3.5 Sonnet o1-preview Gemini 2.0 Flash Gemini 2.0 Thinking Flash Pass@1 Consistency@5 Pass@1 Consistency@5 Pass@1 Consistency@5 Pass@1 Consistency@ Pass@1 Consistency@"
        },
        {
            "title": "Random guessing",
            "content": "Correct Wrong Flawed 76.5% 77.4% 89.6% 90.3% 100% 100% 73.5% 77.4% 75.4% 77.4% 80% 31.0% 30.0% 22.5% 17.5% 68.8% 79.4% 44.5% 42.5% 56.5% 55% 22.2% 11.1% 33.3% 33.3% 80.0% 88.8% 60% 66.6% 53.3% 55.5% 20% 20% 43.2% 35.4% 56.1% 61.2% 84.5% 92% 58% 58.7% 80% 89.1% 50% Table 7: Accuracy rates of commercial language models on our verification benchmark. For the task of response scoring (Scoring Accuracy), accuracy rates are broken down for entries that require identifying correct response as being correct (Correct), entries that require identifying wrong response as being wrong (Wrong), and entries that require identifying wrong response that coincidentally reaches the correct answer as being wrong (Flawed). GPT-4o and Claude 3.5 Sonnet only perform marginally better than random guessing across all tasks. o1-Preview performs better, but still fails to identify 20-30% of wrong responses."
        },
        {
            "title": "7 Related Work",
            "content": "Test-time compute. Many of the recent advances in language model reasoning capabilities can be traced to increasing use of test-time compute. Inference strategies like chain-of-thought reasoning [Wei et al., 2022], tree-of-thoughts [Yao et al., 2023] and self-critique [Valmeekam et al., 2023] result in improved reasoning performance at the cost of forming longer responses. Reinforcement learning has emerged as particularly successful strategy for effectively leveraging more test-time compute, wherein models learn from exploration to form lengthy chain-of-thought outputs that incorporate backtracking and search, despite not being explicitly taught to do so [OpenAI, 2024, Team, 2025]. Inference-time model adaptation, whether through many-shot learning [Agarwal et al., 2024, Anil et al., 2024] or finetuning [Aky√ºrek et al., 2024], provides another avenue when training data is available. We study sampling-based search: obtain set of candidate responses from model and apply an aggregation method to select response, such as 14 self-consistency/plurality voting [Wang et al., 2023] or selecting response with reward/verifier model [Cobbe et al., 2021]. These various methods for scaling test-time compute are complementary; for example, sampling-based search can also be used on models trained to produce longer outputs. We note that it is possible for models trained to produce long chains of thought to perform something resembling samplingbased search internally, in which case we still expect our observed scaling trends to hold. However, we also expect explicit sampling-based search will remain indispensable, due to its greater parallelism and robustness than internally implemented search. Scaling sampling-based search. The paradigm of sampling-based search provides three main knobs for scaling: generation, sampling, and selection. While the cost of generating each individual response can be scaled with previously mentioned interventions, such as chain-of-thought [e.g. Wei et al., 2022], reinforcement learning [e.g. OpenAI, 2024], or inference-time adaptation [e.g. Anil et al., 2024], the cost of sampling set of responses can be scaled by increasing the number of responses generated [Wang et al., 2023, Snell et al., 2024]. We use random sampling to generate each set of candidate responses, which means the latter corresponds to simply taking more random draws. However, this sampling can also be implemented in an agentic fashion, with central model delegating the generation of responses so as to perform search more systematically. The process of selecting response can be scaled by using more expensive rules: self-consistency provides simple plurality voting rule at the lowest-cost end of the spectrum [Wang et al., 2023], while language model self-verification [e.g. Xue et al., 2023, see below] and learned verification/reward models [e.g. Cobbe et al., 2021, see below] provide range of selection strategies that vary in cost and capability. For more fine-grained control over the scaling of self-verification in our experiments, we apply plurality voting [Wang et al., 2023] to self-verification and vary our number of verification attempts per response. Verification of language model outputs. large body of recent work has studied the self-verification capabilities of large language models [e.g., Cobbe et al., 2021, Kadavath et al., 2022, Saunders et al., 2022, Kim et al., 2023, Xie et al., 2023, Weng et al., 2023, Zhang et al., 2023, Xue et al., 2023, Li et al., 2023, Liu et al., 2024, Chow et al., 2024, Jiang et al., 2024, Dhuliawala et al., 2024, Snyder et al., 2024, Wu et al., 2024, Huang et al., 2024, Kamoi et al., 2024a,b, Orgad et al., 2024, Wen et al., 2024, Tyen et al., 2024, Chen et al., 2024, Kumar et al., 2024, Qu et al., 2024, Zhang et al., 2024, Ko et al., 2025, Havrilla et al., 2024]. While some worksincluding ourssimply ask models to perform verification and parse the response, others have proposed custom methods of performing self-verification, including: recreating the problem from the response [Xue et al., 2023, Wu et al., 2024], masking and re-filling parts of the response [Weng et al., 2023, Jiang et al., 2024], creating rubric [Dhuliawala et al., 2024], or asking models to choose from options [Xie et al., 2023, Chen et al., 2024]. Our work does not focus on optimizing for selfverification or advocate for any particular strategy. However, in the course of performing our scaling study, we did identify several previously unstudied principles of self-verification that only arise at sufficiently large scale and may be of independent interest, including implicit scaling, output style suitability, and the importance of directly comparing responses. Other related bodies of work study the learning of verifiers, often on top of pretrained large language model [e.g. Cobbe et al., 2021, Saunders et al., 2022, Li et al., 2023, Havrilla et al., 2024, Kumar et al., 2024, Qu et al., 2024, Chow et al., 2024, Zhang et al., 2024], and the use of external tools for verification [e.g. Min et al., 2023, Gou et al., 2024, Gao et al., 2024, Kim et al., 2023]. We did not train customized verification models or permit verifier use of external tools in the listed experiments, as we found blackbox model access to be sufficient for effective verification at scale. The limitations of model self-verification capabilities are also well-studied [Kamoi et al., 2024a, Tyen et al., 2024, Huang et al., 2024], and can be remedied with external information [Huang et al., 2024] or hints for localizing errors [Tyen et al., 2024]. Models especially struggle with self-diagnosing hallucinations [Zhang et al., 2023, Orgad et al., 2024, Snyder et al., 2024], despite awareness of their own limitations [Kadavath et al., 2022], and are often incentivized to obfuscate errors [Wen et al., 2024]. In addition to being used to select from candidate responses [Cobbe Applications of verification. et al., 2021, Li et al., 2023, Weng et al., 2023, Jiang et al., 2024, Chen et al., 2024, Xie et al., 2023], verifiers can be used to guide iterative improvements to models output by providing feedback to the generating model [Kim et al., 2023, Xue et al., 2023, Valmeekam et al., 2023, Wu et al., 2024, Huang et al., 2024, Dhuliawala et al., 2024, Stechly et al., 2024a,b, Qu et al., 2024, Havrilla et al., 2024, Ko et al., 2025]. Another important application of verification is in enhancing model capabilities. For example, verification results for model outputs can be fed back into models as feedback via in-context reinforcement learning [Shinn et al., 2023], reinforcement learning [Uesato et al., 2022, Peng et al., 2023, Madaan et al., 2023, Kumar et al., 2024, Chow et al., 2024], or finetuning [Welleck et al., 2022, Paul et al., 2024, An et al., 2024, Singh et al., 2024], in an approach known as data flywheeling. Verification has also been explored as means of encouraging models to produce better written responses [Anil et al., 2021, Kirchner et al., 2024]. From product perspective, verification capabilities are also important to the workflow of end users [Collins et al.]."
        },
        {
            "title": "8 Conclusion",
            "content": "This paper studied the scaling trends governing sampling-based search, finding that (1) it scales remarkably well even with simple implementations, (2) implicit scaling plays big role in this scalability, and (3) self-verification capability can be scaled with test-time compute using two key principles: comparisons localize errors, and responses should be rewritten for output style suitability. To this end, we scaled minimalist, embarrassingly parallel implementation of sampling-based search that, with sufficient test-time compute, is sufficient to attain state-of-art performance on range of reasoning benchmarks. Our results underscore the importance of the sampling-based search paradigm. Given that it complements other test-time compute scaling strategies, is parallelizable and allows for arbitrarily scaling, and admits simple implementations that are demonstrably effective, we expect sampling-based search to play crucial role as language models are tasked with solving increasingly complex problems with increasingly large compute budgets. We also see the performance of sampling-based search as providing both strong baseline scaling trend that any non-trivial inference strategy should exceed, and meaningful measure of models search capability when Pass@k is uninformative (e.g. on multiple choice exams). We anticipate model self-verification capabilities to rapidly improve in the short term, as models learn to leverage the principles of implicit scaling and output style suitability, and drive improved scaling rates for samplingbased search. Finally, our results also highlight the importance of being able to effectively sample massive and diverse sets of solutions for search. This calls for more systematic inference alternatives to random sampling, such as agentic approaches that delegate search, and inference-aware optimization methods that maximize, e.g., Pass@k performance rather than Pass@1."
        },
        {
            "title": "References",
            "content": "OpenAI. OpenAI introducing-openai-o1-preview/, 2024. Introducing o1-preview. https://openai.com/index/ DeepSeek-AI Team. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Ekin Aky√ºrek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for abstract reasoning, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems, November 2021. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, and Heng Ji. RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought, October 2023. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: challenging, contamination-free llm benchmark, 2024. Mathematical Association of America MAA. AIME 2024 Problem Set, 2024. Gladys Tyen, Hassan Mansoor, Victor CƒÉrbune, Peter Chen, and Tony Mak. LLMs cannot find reasoning errors, but can correct them given the error location, June 2024. 17 Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, and Rui Zhang. Evaluating LLMs at Detecting Errors in LLM Responses, July 2024a. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. How Language Model Hallucinations Can Snowball, May 2023. Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and Yonatan Belinkov. LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations, October 2024. Ben Snyder, Marius Moisescu, and Muhammad Bilal Zafar. On Early Detection of Hallucinations in Factual Question Answering. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 27212732, Barcelona Spain, August 2024. ACM. ISBN 9798400704901. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating Sequences by Learning to Self-Correct, October 2022. Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really improve by self-critiquing their own plans?, 2023. Cem Anil, Esin Durmus, Nina Rimsky, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Meg Tong, Jesse Mu, Daniel Ford, Francesco Mosconi, Rajashree Agrawal, Rylan Schaeffer, Naomi Bashkansky, Samuel Svenningsen, Mike Lambert, Ansh Radhakrishnan, Carson Denison, Evan Hubinger, Yuntao Bai, Trenton Bricken, Timothy Maxwell, Nicholas Schiefer, James Sully, Alex Tamkin, Tamera Lanham, Karina Nguyen, Tomasz Korbak, Jared Kaplan, Deep Ganguli, Samuel R. Bowman, Ethan Perez, Roger Baker Grosse, and David Duvenaud. Many-shot jailbreaking, 2024. 18 Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language Models (Mostly) Know What They Know, November 2022. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, June 2022. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Qizhe Xie. Self-evaluation guided beam search for reasoning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 25502575. Association for Computational Linguistics, 2023. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making Large Language Models Better Reasoners with Step-Aware Verifier, May 2023. Dancheng Liu, Amir Nassereldine, Ziming Yang, Chenhui Xu, Yuting Hu, Jiajie Li, Utkarsh Kumar, Changjae Lee, Ruiyang Qin, Yiyu Shi, and Jinjun Xiong. Large Language Models have Intrinsic SelfCorrection Ability, December 2024. Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan, Craig Boutilier, Rishabh Agarwal, Aviral Kumar, and Aleksandra Faust. Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models, December 2024. Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James T. Kwok. Forwardbackward reasoning in large language models for mathematical verification. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 66476661. Association for Computational Linguistics, 2024. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 35633578. Association for Computational Linguistics, 2024. Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, and Meng Jiang. Large Language Models Can Self-Correct with Minimal Effort. June 2024. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When Can LLMs Actually Correct Their Own Mistakes? Critical Survey of Self-Correction of LLMs. Transactions of the Association for Computational Linguistics, 12:14171440, 2024b. Jiaxin Wen, Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel R. Bowman, He He, and Shi Feng. Language Models Learn to Mislead Humans via RLHF, September 2024. Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models, November 2024. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training Language Models to Self-Correct via Reinforcement Learning, October 2024. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive Introspection: Teaching Language Model Agents How to Self-Improve, July 2024. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative Verifiers: Reward Modeling as Next-Token Prediction, October 2024. Joonho Ko, Jinheon Baek, and Sung Ju Hwang. Real-time Verification and Refinement of Language Model Text Generation, January 2025. Alexander Havrilla, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. Glore: When, where, and how to improve LLM reasoning via global and local refinements. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation, October 2023. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, February 2024. Kuofeng Gao, Huanqia Cai, Qingyao Shuai, Dihong Gong, and Zhifeng Li. Embedding Self-Correction as an Inherent Ability in Large Language Models for Enhanced Mathematical Reasoning, October 2024. Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. Chain of thoughtlessness? an analysis of cot in planning, 2024a. Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. On the self-verification limitations of large language models on reasoning and planning tasks, 2024b. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language Agents with Verbal Reinforcement Learning, October 2023. 20 Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcomebased feedback, November 2022. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback, March 2023. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-Refine: Iterative Refinement with Self-Feedback, May 2023. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. REFINER: Reasoning Feedback on Intermediate Representations, February 2024. Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning From Mistakes Makes LLM Better Reasoner, March 2024. Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron T. Parisi, Abhishek Kumar, Alexander A. Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. Beyond human data: Scaling self-training for problem-solving with language models. Trans. Mach. Learn. Res., 2024, 2024. Cem Anil, Guodong Zhang, Yuhuai Wu, and Roger Grosse. Learning to Give Checkable Answers with Prover-Verifier Games, August 2021. Jan Hendrik Kirchner, Yining Chen, Harri Edwards, Jan Leike, Nat McAleese, and Yuri Burda. Proververifier games improve legibility of llm outputs, 2024. Katherine M. Collins, Albert Q. Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B. Tenenbaum, William Hart, Timothy Gowers, Wenda Li, Adrian Weller, and Mateja Jamnik. Evaluating language models for mathematics through interactions. Proceedings of the National Academy of Sciences of the United States of America, 121(24):e2318124121. ISSN 0027-8424."
        },
        {
            "title": "A Additional Technical Details",
            "content": "A.1 Inference Prompts For questions from the MATH and AIME benchmarks, we use the following prompt. MATH and AIME Prompt Please answer the following question. Think carefully and in step-by-step fashion. At the end of your solution, put your final result in boxed environment, e.g. 42 . The question would be here. For questions from the LiveBench Math and LiveBench Reasoning benchmarks, which already come with their own instructions and formatting requests, we do not provide any accompanying prompt and simply submit the model the question verbatim. LiveBench Prompt The question would be here. A.2 LM-Based Scoring Given tuple consisting of question, ground-truth solution, and candidate response, we grade the correctness of the candidate response by querying Gemini-v1.5-Pro-002 model to compare the candidate and ground-truth solutions. This involves repeating the following process five times: (1) send prompt to the model that provides the question, the correct ground-truth solution, and the candidate response, and asks the model to deliberate on the correctness of the candidate response; and (2) send followup prompt to the model to obtain correctness ruling in structured format. If strict majority of (valid) responses to the second prompt evaluate to JSON object with the key-value pair student_final_answer_is_correct = True rather than student_final_answer_is_correct = False , the candidate response is labeled correct. Otherwise, the candidate response is labeled incorrect. These queries are all processed with temperature zero. The prompts, which can be found at the end of this subsection, ask the language model to (1) identify the final answer of the given response, (2) identify the final answer of the reference (ground truth) response, and (3) determine whether the final answer of the given response satisfactorily matches that of the reference response, ignoring any non-substantive formatting disagreements. In line with convention, we instruct our scoring system to ignore the correctness of the logic used to reach the final answer and rather only judge the correctness of the final answer. The model is asked to label all non-sensical and incomplete responses as being incorrect. As form of quality assurance, every scoring output for the Consistency@200 and Verification@200 figures depicted in Table 1 was manually compared against human scoring. No discrepancies between automated and human scoring were found on the MATH and AIME datasets for both Consistency@200 and Verification@200. No discrepancies were found on LiveBench Reasoning for Consistency@200. For Verification@200, one false positive (answer labeled by automated system as being incorrect but labeled by human as being correct) and one false negative (answer labeled by automated system as being correct but labeled by human as being incorrect) were identified on LiveBench Reasoning; three false positives and four false negatives were identified on LiveBench Math. For Consistency@200, two false negatives were identified on LiveBench Math. This means that LM scoring matched human scoring 99% of the time, and the choice of human versus automated scoring matters little to our results. 22 Prompt You are an accurate and reliable automated grading system. Below are two solutions to math exam problem: solution written by student and the solution from the answer key. Your task is to check if the students solution reaches correct final answer. Your response should consist of three parts. First, after reading the question carefully, identify the final answer of the answer keys solution. Second, identify the final answer of the students solution. Third, identify whether the students final answer is correct by comparing it to the answer keys final answer. # The question, answer key, and student solution The math exam question: The question would be here. The answer key solution: The reference solution would be here. The students solution: The candidate solution would be here. # Your response format Please structure your response as follows. PROVIDE COMPLETE RESPONSE. # Answer Key Final Answer Identify the final answer of the answer key solution. Thats all you need to do here: just identify the final answer. \"final answer\" can take many forms, depending on what the question is asking for; it can be number (e.g., \"37\"), string (e.g., \"ABCDE\"), sequence (e.g., \"2,3,4,5\"), letter (e.g., \"Y\"), multiple choice option (e.g. \"C\"), word (e.g., \"Apple\"), an algebraic expression (e.g. \"x2 + 37\"), quantity with units (e.g. \"4 miles\"), or any of number of other options. If solution concludes that the question is not answerable with the information provided or otherwise claims that there is no solution to the problem, let the final answer be \"None\". If the solution does not produce any final answer because it appears to be cut off partway or is otherwise non-sensical, let the solutions final answer be \"Incomplete solution\" (this could only ever possibly happen with the student solution). YOUR RESPONSE HERE SHOULD BE BRIEF. JUST IDENTIFY WHAT THE QUESTION IS ASKING FOR, AND IDENTIFY THE ANSWER KEYS FINAL ANSWER. DO NOT ATTEMPT TO ANSWER THE QUESTION OR EVALUATE INTERMEDIATE STEPS. # Student Solution Final Answer Identify the final answer of the student solution. YOUR RESPONSE HERE SHOULD BE BRIEF. JUST IDENTIFY WHAT THE QUESTION IS ASKING FOR, AND IDENTIFY THE STUDENTS FINAL ANSWER. DO NOT ATTEMPT TO ANSWER THE QUESTION OR EVALUATE INTERMEDIATE STEPS. # Correctness Simply evaluate whether the students final answer is correct by comparing it to the answer keys final answer. 23 Compare the students final answer against the answer keys final answer to determine if the students final answer is correct. * It does not matter how the student reached their final answer, so long as their final answer itself is correct. * It does not matter how the student formatted their final answer; for example, if the correct final 14 4 . It does not matter answer is 7/2 , the student may write ***3.5*** or three and half or if the students final answer uses the same specific formatting that the question asks for, such as writing multiple choice options in the form \"(E)\" rather than \"***E***\". * It does not matter if the student omitted units such as dollar signs. * If the student solution appears to be truncated or otherwise incoherent, e.g. due to technical glitch, then it should be treated as being incorrect. ONCE AGAIN, DO NOT EVALUATE INTERMEDIATE STEPS OR TRY TO SOLVE THE PROBLEM YOURSELF. THE ANSWER KEY IS ALWAYS RIGHT. JUST COMPARE THE FINAL ANSWERS. IF THEY MATCH, THE STUDENT ANSWER IS CORRECT. IF THEY DO NOT MATCH, THE STUDENT ANSWER IS INCORRECT. # Summary * Answer key final answer: (The final answer of the answer key solution. Please remove any unnecessary formatting, e.g. provide \"3\" rather than \" 3 \", provide \"E\" rather than \"***E***\", provide \"1, 2, 3\" rather than \"[1, 2, 3]\", provide \"4 ounces\" rather than \"4oz\".) * Student final answer: (The final answer of the students solution. Please remove any unnecessary formatting, e.g. provide \"3\" rather than \" 3 \", provide \"E\" rather than \"***E***\", provide \"1, 2, 3\" rather than \"[1, 2, 3]\", provide \"4 ounces\" rather than \"4oz\".) * Student final answer is correct?: (Does the student final answer match the answer key final answer? Please provide \"true\" or \"false\".) Prompt 2 Please structure your output now as JSON, saying nothing else. Use the following format: { \"answer_key_final_answer\": str (the final answer of the answer key solution; please remove any formatting\"), \"student_final_answer\": str (the final answer of the students solution; please remove any formatting\"), \"student_final_answer_is_correct\": true/false, } A.3 Implementation of Consistency@k Consistency@k measures the performance of model by evaluating the correctness of the most common answer reached by the model after being run times. An important consideration with implementing consistency@k is that there are many choices for the equivalence relation one can use to define the most common answer. We define two candidate responses as reaching the same answer if their final answer is the same. We determine candidate responses final answer by prompting language model to identify the final answer from the candidate response; we then strip the extracted final answer of leading and trailing whitespace. We determine equivalence with literal string match. After determining the most common final answer to question, we use the string The final answer is {final answer} as the consistency@k response. Note that we could have instead randomly chosen candidate response corresponding to the most common final answer, and used that selected response as the consistency@k responsewe have found that, because our LM-based scoring system evaluates correctness using only the final answer, this 24 alternative results in the same consistency@k metrics. A.4 Benchmark Evaluation Prompts The benchmark performances reported in Table 7 are obtained with the following prompts. The following prompt is used for the comparison task. Comparison Task Prompt Part 1 Question here. Here are two solutions to the above question. You must determine which one is correct. Please think extremely carefully. Do not leap to conclusions. Find out where the solutions disagree, trace them back to the source of their disagreement, and figure out which one is right. Solution 1: First solution here. Solution 2: Second solution here. Comparison Task Prompt Part 2 Now summarize your response in JSON format. Respond in the following format saying nothing else: { \"correct_solution\": 1 or 2 } The following prompt is used for the scoring task. Scoring Task Prompt Part 1 Question here. include below student solution to the above question. Determine whether the student solution reaches the correct final answer in correct fashion; e.g., whether the solution makes two major errors that still coincidentally cancel out. Please be careful and do not leap to conclusions without first reasoning them through. Solution: Solution here. Scoring Task Prompt Part 2 Now summarize your response in JSON format. Respond in the following format saying nothing else: { \"is_solution_correct\": yes or no }"
        },
        {
            "title": "B Additional Experiments",
            "content": "B.1 Temperature Tuning After attempting temperature tuning on the training split of the MATH dataset, we found that the choices of temperatures œÉinf and œÉverif did not significantly affect performance. In Table 8 and Table 9, we compare the post-verification accuracy of our pipeline for various temperature choices. The Verification@20 figures are obtained without running tie-breaking and with only 20 verification attempts. Inference Temp œÉinf Pass@20 Consistency@20 Verification@20 0.2 1.0 1.5 2.0 89/100 94/100 89/100 89/100 76/100 73/100 73/100 75/ 82/100 80/100 79/100 82/100 Table 8: Accuracy rates of the Gemini v1.5 Pro model on the training split of MATH using different methods of selecting from 20 generated responses and varying the temperature used to generate the responses. The temperature used for verification attempts is fixed at œÉverif = 1.0. Inference temperature does not significantly affect downstream performance. Verification Temp œÉverif Pass@20 Consistency@20 Verification@20 0.2 1.0 2.0 89/100 89/100 89/100 73/100 73/100 73/ 76/100 79/100 79/100 Table 9: Accuracy rates of the Gemini v1.5 Pro model on the training split of MATH using different methods of selecting from 20 generated responses and varying the temperature used to for verification attempts. The temperature used for generating responses is fixed at œÉinf = 1.5. Verification temperature does not significantly affect downstream performance. B.2 Olympiad LiveBench Math Subtask The one task for which we saw no lift from verification is the Olympiad questions from LiveBench MATH. These questions are not formatted as open-ended problems. Rather, they take very specific form of asking one to fill in pre-written proof from menu of expression options, and to output specific sequence of indices corresponding to these options. This is incompatible with our verification pipeline, which asks the verification model to rewrite candidate responses in theorem-lemma format where the theorem states the final answer. For example, the final answer to the Olympiad question at the bottom of this section is the following sequence: 19,32,20,2,14,1,27,21,31,36,3,30,5,16,29,34,7,4,6,18,15,22,9,25,28,35,26,8,13,24,23,17,33,11,10,12 ."
        },
        {
            "title": "Representative example of Olympiad task from LiveBench MATH",
            "content": "You are given question and its solution. The solution however has its formulae masked out using the tag <missing X> where indicates the identifier for the missing tag. You are also given list of formulae in latex in the format \"<expression Y> = latex code\" where is the identifier 26 for the formula. Your task is to match the formulae to the missing tags in the solution. Think step by step out loud as to what the answer should be. If you are not sure, give your best guess. Your answer should be in the form of list of numbers, e.g., 5, 22, 3, ..., corresponding to the expression identifiers that fill the missing parts. For example, if your answer starts as 5, 22, 3, ..., then that means expression 5 fills <missing 1>, expression 22 fills <missing 2>, and expression 3 fills <missing 3>. The question is: Find all integers 3 such that the following property holds: divisors of n! in increasing order as 1 = d1 < d2 < < dk = n!, then we have if we list the d2 d1 d3 d2 dk dk1. Find all integers 3 such that the following property holds: increasing order as 1 = d1 < d2 < < dk = n!, then we have if we list the divisors of n! in d2 d1 d3 d2 dk dk1. The solution is: We can start by verifying that <missing 1> and = 4 work by listing out the factors of <missing 2> and <missing 3>. We can also see that <missing 4> does not work because the terms 15, 20, and 24 are consecutive factors of <missing 5>. Also, <missing 6> does not work because the terms <missing 7>, and 9 appear consecutively in the factors of <missing 8>. We can start by verifying that <missing 9> and <missing 10> work by listing out the factors of <missing 11> and 4!. We can also see that <missing 12> does not work because the terms 15, 20, and 24 are consecutive factors of 5!. Also, <missing 13> does not work because the terms <missing 14>, and 9 appear consecutively in the factors of <missing 15>. Note that if we have prime number <missing 16> and an integer <missing 17> such that both and <missing 18> are factors of <missing 19>, then the condition cannot be satisfied. If <missing 20> is odd, then <missing 21> is factor of <missing 22>. Also, <missing 23> is factor of n!. Since <missing 24> for all <missing 25>, we can use Bertrands Postulate to show that there is at least one prime number such that <missing 26>. Since we have two consecutive factors of <missing 27> and prime number between the smaller of these factors and n, the condition will not be satisfied for all odd <missing 28>. If <missing 29> is even, then (2)( n2 2 )(n 2) = n2 4n + 4 is factor of <missing 30>. Also, (n 3)(n 1) = n2 4n + 3 is factor of <missing 31>. Since <missing 32> for all 8, we can use Bertrands Postulate again to show that there is at least one prime number such that <missing 33>. Since we have two consecutive factors of <missing 34> and prime number between the smaller of these factors and n, the condition will not be satisfied for all even <missing 35>. Therefore, the only numbers that work are = 3 and <missing 36>. The formulae are: <expression 1> = 6 <expression 2> = 5 <expression 3> 3! <expression 4> + 1 ... (omitted for brevity) 27 <expression 33> < < n2 4n + 3 <expression 34> > <expression 35> < < n2 2n <expression 36> = 4 Your final answer should be STRICTLY in the format: Detailed reasoning Answer: <comma separated list of numbers representing expression identifiers>"
        },
        {
            "title": "C Examples of Benchmark Entries",
            "content": "C.1 Example Entry from LiveBench Reasoning"
        },
        {
            "title": "Question",
            "content": "In this question, assume each person either always tells the truth or always lies. The person at the campground thinks their friend is lying. Mateo is at the aquarium. The person at the restaurant says the person at the hotel lies. Farid is at the movie theater. The person at the movie theater says the person at the campground lies. Ryan is at the shopping mall. The person at the cafe says the person at the campground lies. The person at the observatory says the person at the museum lies. The person at the museum says the person at the restaurant tells the truth. The person at the campground says the person at the airport tells the truth. The person at the campground tells the truth. Luna is at the restaurant. The person at the movie theater saw firetruck. Isabella is at the observatory. The person at the amusement park says the person at the museum tells the truth. The person at the restaurant says the person at the shopping mall tells the truth. Liam is at the beach. Ayaan is at the airport. The person at the restaurant says the person at the beach tells the truth. Maya is at the amusement park. The person at the restaurant says the person at the aquarium lies. The person at the restaurant says the person at the movie theater lies. The person at the library says the person at the campground tells the truth. Elowen is at the hotel. Priya is at the library. The person at the gym says the person at the restaurant tells the truth. Kehinde is at the cafe. Anika is at the museum. Quan is at the campground. Soren is at the gym. Does the person at the museum tell the truth? Does the person at the restaurant tell the truth? Does the person at the movie theater tell the truth? Think step by step, and then put your answer in **bold** as list of three words, yes or no (for example, **yes, no, yes**). If you dont know, guess."
        },
        {
            "title": "Incorrect Response",
            "content": "Heres how we can break down this logic puzzle: 1. **Quan is at the campground and tells the truth.** This is given fact. Heres how we can solve this logic puzzle: 1. **Quan is at the campground and tells the truth.** This is given. 2. **The person at the airport (Ayaan) tells the truth.** Quan (campground) says the 28 person at the airport tells the truth, and Quan tells the truth. 3. **Anika (museum) lies.** The person at the observatory (Isabella) says the person at the museum lies. Since we dont know yet whether Isabella tells the truth, this doesnt tell us anything about Anika. Well have to come back to this. 4. **Luna (restaurant) lies.** The person at the restaurant says the person at the hotel (Elowen) lies. Later, the person at the restaurant says the person at the campground (Quan) lies. Since Quan tells the truth, Luna (restaurant) must be lying. 5. **Anika (museum) lies.** Because Luna lies, and Luna says that the person at the hotel lies, Elowen tells the truth. Because Anika says the person at the restaurant (Luna) tells the truth, and Luna actually lies, Anika (Museum) must lie. 6. **Farid (movie theater) lies.** The person at the movie theater says the person at the campground lies. Since Quan (campground) tells the truth, Farid (movie theater) must be lying. Therefore: * **Anika (museum): (restaurant): ater): lies** **Answer: no, no, no** lies** * **Luna lies** * **Farid (movie the2. **Quan (at the campground) says the person at the airport tells the truth.** Since Quan tells the truth, the person at the airport (Ayaan) tells the truth. 3. **The person at the movie theater (Farid) says the person at the campground (Quan) lies.** Since Quan tells the truth, Farid is lying. 4. **The person at the restaurant (Luna) says the person at the hotel (Elowen) lies.** We dont know yet if Luna is lying or telling the truth. 5. **The person at the museum (Anika) says the person at the restaurant (Luna) tells the truth.** We still dont know if either of them are truthful. 6. **The person at the restaurant (Luna) says the person at the shopping mall (Ryan) tells the truth.** We still dont know Lunas truthfulness. 7. **The person at the restaurant (Luna) says the person at the beach (Liam) tells the truth.** Lunas truthfulness remains unknown. 8. **The person at the restaurant (Luna) says the person at the aquarium (Mateo) lies.** We still dont know about Luna. 9. **The person at the restaurant (Luna) says the person at the movie theater (Farid) lies.** We know Farid lies, so Luna is telling the truth in *this* statement. 10. **Since Luna tells the truth about Farid, Luna *always* tells the truth.** This unlocks several other pieces of information. 11. **Luna (at the restaurant) says the person at the hotel (Elowen) lies.** Since Luna tells the truth, Elowen lies. 12. **Anika (at the museum) says the person at the restaurant (Luna) tells the truth.** Since Luna tells the truth, Anika is also telling the truth. 13. **The person at the cafe (Kehinde) says the person at the campground lies.** Since Quan tells the truth, Kehinde is lying. Therefore: 29 * **Anika (museum):** Tells the truth. * * **Luna (restaurant):** Tells the truth. **Farid (movie theater):** Lies. **yes, yes, no** C.2 Example Entry from LiveBench Math Question positive 5? How many (E) 8 If you cannot determine the correct multiple- (A) 11 choice answer, take your best guess. Once you have your answer, please duplicate that letter five times in single string. For example, if the answer is F, then write FFFFF. (D) 12 (B) 10 divisible squares (C) 9 perfect 2023 than less are by"
        },
        {
            "title": "Incorrect Response",
            "content": "Let be positive perfect square divisible by 5. Then = k2 for some positive integer k, and 5n, which implies that 5k2. Since 5 is prime number, 5k2 means 5k. Thus, must be multiple of 5, i.e., = 5m for some positive integer m. Then = k2 = (5m)2 = 25m2. Thus must be multiple of 25. We are looking for the number of positive perfect squares less than 2023 that are divisible by 5. The perfect squares that are divisible by 5 must be multiples of 25. We seek the number of multiples of 25 that are perfect squares less than 2023. Since = 25m2 < 2023, we have m2 < 2023 25 80.92. Thus, we are looking for perfect squares m2 less than 80.92. The perfect squares are 1, 4, 9, 16, 25, 36, 49, 64, 81, . . . . The perfect squares less than 80.92 are m2 = 1, 4, 9, 16, 25, 36, 49, 64. There are 8 such values. The corresponding values of = 25m2 are 25(1) = 25 25(4) = 100 25(9) = 225 25(16) = 400 25(25) = 625 25(36) = 900 25(49) = 1225 25(64) = 1600 The next multiple of 25 that is perfect square would be 25(81) = 2025, which is not less than 2023. So, there are 8 perfect squares less than 2023 that are divisible by 5. Final Answer: The final answer is EEEEE Let be positive integer such that n2 is divisible by 5. Then n2 is divisible by 5. Since 5 is prime number, must also be divisible by 5. Thus can be written in the form = 5k for some integer k. We want to find how many positive perfect squares n2 are less than 2023. Since = 5k, we have n2 = (5k)2 = 25k2 < 2023. Dividing by 25 we get k2 < 2023 25 80.92 We are looking for integer values of such that k2 < 80.92. Since must be positive, can take values 1, 2, 3, . . . . We have 12 = 1 < 80. 22 = 4 < 80.92 32 = 9 < 80.92 42 = 16 < 80.92 52 = 25 < 80.92 62 = 36 < 80.92 72 = 49 < 80. 82 = 64 < 80.92 92 = 81 80.92 30 Thus can take values from 1 to 8. corresponding values are = The 5k = 5, 10, 15, 20, 25, 30, 35, 40. The perfect squares less than 2023 and divisible by 5 are 52, 102, 152, 202, 252, 302, 352, 402 or 25, 100, 225, 400, 625, 900, 1225, 1600. There are 8 such numbers. Final Answer: The final answer is CCCC C.3 Example Entry from MATH Question If 4 daps = 7 yaps, and 5 yaps = 3 baps, how many daps equal 42 baps?"
        },
        {
            "title": "Incorrect Response",
            "content": "Let represent daps, represent yaps, and represent baps. We are given two relationships: 1. 4 daps = 7 yaps, which can be written as 4d = 7y 2. 5 yaps = 3 baps, which can be written as 5y = 3b We want to find how many daps are equal to 42 baps. We want to find in terms of b, and then evaluate for = 42. First, we can find the relationship between yaps and baps from the second equation: 5y = 3b, so = 3 5 Now, substitute this expression for in terms of into the first equation: 4d = 7y 4d = 7 (cid:16) 3 5 Now, we want to find in terms of b, so divide both sides by 4: = 21 Finally, we want to find how many daps equal 42 baps, so we substitute = 42: = 21 20 42 = 2142 20 = 441 = 882 So, 44.1 daps equal 42 baps. Final Answer: The final answer is 44.1 54 = 21 20 (cid:17) 4d = 21 5 10 = 44.1 20 Let represent daps, represent yaps, and represent baps. We are given: 4 daps = 7 yaps, which can be written as: 4d = 7y and 5 yaps = 3 baps, which can be written as: 5y = 3b We want to find the number of daps that are equal to 42 baps. Let be the number of daps. We want to find such that: xd = 42b From 5y = 3b, we can express yaps in terms of baps: = (3/5)b Now substitute this value of into the first equation, 4d = 7y: 4d = 7 * (3/5)b 4d = (21/5)b We want to find xd = 42b, so we can set: xd = 42b We have 4d = (21/5)b, so we can solve for d: = (21/20)b Now substitute this into xd = 42b: * (21/20)b = 42b Since is not zero, we can divide both sides by b: * (21/20) = 42 Now, solve for x: = 42 * (20/21) = (42/21) * 20 = 2 * 20 = 40 So, 40 daps are equal to 42 baps. 40 Final Answer: The final answer is 31 C.4 Example of Response with Invalid Reasoning but Correct Final Answer Question You have seven bags of gold coins. Each bag has the same number of gold coins. One day, you find bag of 53 coins. You decide to redistribute the number of coins you have so that all eight bags you hold have the same number of coins. You successfully manage to redistribute all the coins, and you also note that you have more than 200 coins. What is the smallest number of coins you could have had before finding the bag of 53 coins? Incorrect Response Reaching Correct Final Answer Let be the number of coins in each of the seven bags. After finding the bag of 53 coins, you have total of 7n + 53 coins. You redistribute these coins into 8 bags such that each bag contains the same number of coins. Therefore, the total number of coins is divisible by 8, so 7n + 53 is divisible by 8. 7n + 53 0 (mod 8). We want to find n. 7n 53 (mod 8). Since 53 53 + 8(7) 53 + 56 3 (mod 8), we have 7n 3 (mod 8). Since 7 1 (mod 8), we multiply both sides by 1 to get 7n 3 (mod 8), so 3 5 (mod 8). Thus must be of the form 8k + 5 for some integer 0. The total number of coins originally was 7n. Since = 8k + 5, the total number of coins originally is 7(8k + 5) = 56k + 35. We are given that the total number of coins is greater than 200. So 7n > 200, or > 200 7 28.57. Since = 8k + 5, we want the smallest integer such that 8k + 5 > 200 7 . For = 0, = 5, so 7n = 35 < 200. For = 1, = 13, so 7n = 91 < 200. For = 2, = 21, so 7n = 147 < 200. For = 3, = 29, so 7n = 203 > 200. We check that 7n + 53 = 203 + 53 = 256 which is divisible by 8 (256 = 8 32). Then 256/8 = 32 coins in each bag. Thus the smallest number of coins you could have had before finding the bag is 7n = 7(29) = 203. Final Answer: The final answer is 203 ."
        },
        {
            "title": "D Examples of Rewritten Responses",
            "content": "D.1 AIME Example"
        },
        {
            "title": "Question",
            "content": "Find the number of rectangles that can be formed inside fixed regular dodecagon ( 12 -gon) where each side of the rectangle lies on either side or diagonal of the dodecagon. The diagram below [asy] unitsize(0.6 inch); for(int i=0; i<360; i+=30) dot(dir(i), shows three of those rectangles. draw(dir(120)dir(330)); filldraw(dir(210)dir(240)dir(30) 4+black); draw(dir(i)dir(i+30)); dir(60)cycle, mediumgray, linewidth(1.5)); draw((0,0.366)(0.366,0), linewidth(1.5)); [/asy]"
        },
        {
            "title": "Correct Answer",
            "content": "The solution is 315 . 32 Candidate Response (cid:1) = 1211 (cid:1)/2 = 1211109 Let the vertices of the regular dodecagon be labeled as V1, V2, . . . , V12. Each side of the rectangle lies on either side or diagonal of the dodecagon. Consider two parallel diagonals (or sides). They determine the sides of the rectangle parallel to them. We need to choose two parallel diagonals (or sides) to form rectangle. There are 12 vertices, and diagonal can be formed by choosing any two distinct vertices. The number of diagonals and sides is (cid:0)12 2 = 66. However, there are 12 sides. So there are 2 66 12 = 54 diagonals. Two parallel diagonals or sides can be drawn as follows: 1. Diagonals parallel to sides: 12 sides so 12/2 = 6 pairs 2. Diagonals spanning 2 vertices: (cid:0)12 (cid:1)/2 sets of 4 4 vertices, so we divide by 2 to get distinct pairs. (cid:0)12 43212 = 495/2, which is not an integer. 4 Something is wrong. The pairs of diagonals can only be separated by 1, 2, 3, 4, 5 vertices on either side. Let be the number of vertices. For = 12, 1 vertex apart: 12 diagonals. There are 12/2 = 6 pairs. 2 vertices apart: 12 diagonals. 12/2 = 6 pairs 3 vertices apart: 12 diagonals. 12/2 = 6 pairs. 4 vertices apart: 12 diagonals. 12/2 = 6 pairs. 5 vertices apart: 12 diagonals. 12/2 = 6 pairs. So there are 5 6 + 6 pairs of parallel diagonals, which is 36 pairs. Number of rectangles is the number of pairs of parallel segments, including sides. For regular n-gon, the number of pairs of parallel diagonals or sides is n(n 1)/2. For = 12, its 12(5) = 60 segments. For each length, there are 12 diagonals. Length can range from 1 side to 5 sides. 1 + 2 + 3 + 4 + 5 + 6 = (cid:0)12 (cid:1) = 495. 4 Consider 12 sides as 12 possible segments. Parallel to each are 5 lengths of segments. So total parallel segments 12 5 + 12/2 = 60 + 6 = 66, incorrect. Each vertex pairs with 5 other vertices giving the same direction. So we have 12 5/2 = 30 pairs of directions. Given one pair of parallel directions, we can choose 6 segments in that direction. So 62 = 36 possibilities. 30 36 = 1080. We are forming (cid:0)12 (cid:1) = 495 parallelograms. However, we are considering parallel segments. The 4 number of rectangles is 12 5 (12/2). Number of rectangles is 122(121) Final Answer: The final answer is 1 2 = 594 for each parallelogram 594/3 = 198. 198 . 1"
        },
        {
            "title": "Rewritten Candidate Response",
            "content": "Theorem 1 (Main Claim). The number of rectangles that can be formed inside fixed regular dodecagon, where each side of the rectangle lies on either side or diagonal of the dodecagon, is 198 . (cid:1) 12 = 1211 Proof. By Lemma 2, regular dodecagon has 12 sides. By Lemma 3, regular dodecagon has (cid:0)12 2 12 = 66 12 = 54 diagonals. By Lemma 4, two line segments formed by 2 connecting vertices of dodecagon are considered parallel under certain conditions. The candidate solution calculates value of 594. By Lemma 6, this value is obtained by calculating 122(121) 4 The exact connection between the number of sides, the number of diagonals, the concept of parallel segments, and the value 594 is not clear from the candidate solution. However, according to Lemma 7, the final answer of 198 is obtained by dividing 594 by 3. Therefore, we perform this division: 594/3 = 198. Thus, the candidate solution concludes that the number of rectangles is 198. 1 2 . 1 Lemma 2. regular n-gon has sides. Proof. An n-gon is defined as polygon with sides. regular n-gon is also polygon with 33 sides. Therefore, regular n-gon has sides. Lemma 3. regular n-gon has (cid:0)n 2 (cid:1) diagonals. Proof. regular n-gon has vertices. Any two distinct vertices can be connected by line segment. (cid:1) = n(n1) The number of ways to choose 2 vertices out of is given by the combination formula (cid:0)n . 2 The sides of the n-gon are formed by connecting adjacent vertices. There are such pairs of adjacent vertices, corresponding to the sides of the n-gon. diagonal is line segment connecting two non-adjacent vertices. Therefore, the number of diagonals is the total number of pairs of vertices minus the number of sides: (cid:0)n 2 (cid:1)n = n(n1) 2 n. 2 Lemma 4. Let be regular dodecagon. Two line segments formed by connecting vertices of are considered parallel if they are either coincident sides of D, or if they are distinct and are both parallel to some side of D. Proof. This lemma is definition, so no proof is required. Lemma 5. Let be regular dodecagon. The number of unordered pairs of parallel segments formed by connecting vertices of (including sides) is denoted by . Proof. This lemma is definition, introducing the variable to represent the number of unordered pairs of parallel segments. No proof is required. Lemma 6. The value 594 is obtained via the calculation 122(121) 4 1 2 1 2 . Proof. We evaluate the expression: 122(121) Evaluating 14411 16 = 99. However, 99 1 to have calculation error, giving 99 instead of 594. The candidate solution has made calculation error. 122(12 1) = 144 11 = 1584. 1584/4 = 396. 396/2 = 198. 198/2 = 99. The calculation gives 99, not 594. 2 1 2 4 = 594. The expression provided appears 2 = 14411 4 = 99 2 = 99 1 16 = 792 4 = 8 = 396 4 = 198 yields 1584 4 1 1 2 4 1 Lemma 7. The candidate solutions final answer is obtained by dividing 594 by 3. Proof. Dividing 594 by 3, we get 594/3 = 198. D.2 LiveBench Reasoning Example"
        },
        {
            "title": "Question",
            "content": "In this question, assume each person either always tells the truth or always lies. Elowen is at the hotel. Max is at the park. Grace is at the ice skating rink. The person at the barbershop says the person at the library tells the truth. Charlie is at the library. Tala is at the campground. Farid is at the theater. The person at the hotel saw firetruck. The person at the vineyard says the person at the campground lies. The person at the farm says the person at the movie theater lies. The person at the movie theater says the person at the park lies. The person at the park says the person at the hotel tells the truth. The person at the hotel says the person at the library tells the truth. Jaxon is at the skate park. Theo is at the barbershop. The person at the airport tells the truth. The person at the bowling alley thinks their neighbor is telling the truth. The person at the ice skating rink says the person at the bowling alley tells the truth. Anika is at the airport. The 34 person at the bookstore says the person at the vineyard lies. The person at the campground says the person at the movie theater tells the truth. The person at the restaurant says the person at the campground tells the truth. The person at the theater says the person at the park lies. Hiroshi is at the school. Zara is at the bowling alley. The person at the bowling alley says the person at the school tells the truth. The person at the park thinks their neighbor is telling the truth. The person at the skate park lies. The person at the bowling alley saw firetruck. The person at the observatory lies. Liam is at the restaurant. The person at the library says the person at the observatory tells the truth. The person at the bowling alley says the person at the vineyard lies. Beatriz is at the observatory. Priya is at the movie theater. Nia is at the farm. Ryan is at the vineyard. Isabella is at the bookstore. Does the person at the ice skating rink tell the truth? Does the person at the bowling alley tell the truth? Does the person at the vineyard tell the truth? Think step by step, and then put your answer in **bold** as list of three words, yes or no (for example, **yes, no, yes**). If you dont know, guess. Correct Answer yes, yes, no"
        },
        {
            "title": "Candidate Response",
            "content": "Heres how we can break down this logic puzzle: 1. **Start with known truth-teller or liar:** Jaxon is at the skate park and lies. 2. **Use Jaxons lie to find more information:** Since Jaxon lies, the person at the bowling alley, Zara, *cannot* be telling the truth. Zara says the person at the school tells the truth, but Zara is lying so Hiroshi at the school lies. Zara also says the person at the vineyard lies. Since Zara lies, Ryan at the vineyard must tell the truth. 3. **Focus on the vineyard:** Ryan, truth-teller at the vineyard, says Tala at the campground lies. 4. **Follow Talas lie:** Tala lies and says the person at the movie theater tells the truth. This means Priya at the movie theater *lies*. 5. **Use Priyas lie:** Priya at the movie theater lies and says Max at the park lies. Therefore, Max tells the truth. 6. **Use Maxs truth:** Max at the park tells the truth and says Elowen at the hotel tells the truth. 7. **Use Elowens truth:** Elowen tells the truth and says the person at the library tells the truth. This means Charlie at the library tells the truth. 8. **Use Charlies truth:** Charlie tells the truth and says Beatriz at the observatory lies. This is confirmed by Jaxons lie - meaning two people lie, therefore Liam at the restaurant lied and Tala lied (so Ryan is truthful) therefore so is Isabella. And Charlie who said that Beatriz lied. 9. **Grace and Zara:** Grace at the ice skating rink says Zara at the bowling alley tells the truth. Since Zara lies, Grace also lies. Therefore: * Grace (ice skating rink) lies. * Zara (bowling alley) lies. * Ryan (vineyard) tells the truth. **no, no, yes** 35 Rewritten Candidate Response Theorem 8 (Main Claim). Given the set of statements made by individuals located at different places, where each individual either always tells the truth or always lies, determine whether Grace (ice skating rink), Zara (bowling alley), and Ryan (vineyard) tell the truth. The answer is: Grace lies, Zara lies, Ryan tells the truth. Proof. By Lemma 9, Jaxon lies. Given that Jaxon lies, by Lemma 10, Zara lies. Since Jaxon lies, and Zara lies, by Lemma 12, Ryan tells the truth. Since Zara lies, and Grace says Zara tells the truth, by Lemma 19, Grace lies. Therefore, Grace lies, Zara lies, and Ryan tells the truth. This corresponds to the answer **no, no, yes**. Lemma 9. Jaxon lies. Proof. The problem states \"The person at the skate park lies.\" The problem also states \"Jaxon is at the skate park.\" Therefore, Jaxon lies. Lemma 10. If Jaxon lies, then Zara lies. Proof. The problem states \"The person at the bowling alley thinks their neighbor is telling the truth.\" Zara is at the bowling alley. Jaxon is Zaras neighbor at the skate park. If Jaxon lies, then Zara must believe lie, implying Zara lies. Lemma 11. If Zara lies, and Zara says Hiroshi tells the truth, then Hiroshi lies. Proof. Zara lies, and Zara says Hiroshi tells the truth. Since Zara lies about Hiroshi telling the truth, Hiroshi must lie. Lemma 12. If Zara lies, and Zara says Ryan lies, then Ryan tells the truth. Proof. Zara lies, and Zara says Ryan lies. Since Zara lies about Ryan lying, Ryan must tell the truth. Lemma 13. If Ryan tells the truth, and Ryan says Tala lies, then Tala lies. Proof. Ryan tells the truth, and Ryan says Tala lies. Since Ryan tells the truth about Tala lying, Tala must lie. Lemma 14. If Tala lies, and Tala says Priya tells the truth, then Priya lies. Proof. Tala lies and says Priya tells the truth. Since Tala lies about Priya telling the truth, Priya must lie. Lemma 15. If Priya lies, and Priya says Max lies, then Max tells the truth. Proof. Priya lies and says Max lies. Since Priya lies about Max lying, Max must tell the truth. Lemma 16. If Max tells the truth, and Max says Elowen tells the truth, then Elowen tells the truth. 36 Proof. Max tells the truth, and Max says Elowen tells the truth. Since Max tells the truth about Elowen telling the truth, Elowen tells the truth. Lemma 17. If Elowen tells the truth, and Elowen says Charlie tells the truth, then Charlie tells the truth. Proof. Elowen tells the truth and says Charlie tells the truth. Since Elowen tells the truth about Charlie telling the truth, Charlie tells the truth. Lemma 18. If Charlie tells the truth, and Charlie says Beatriz lies, then Beatriz lies. Proof. Charlie tells the truth and says Beatriz lies. Since Charlie tells the truth about Beatriz lying, Beatriz lies. Lemma 19. If Zara lies, and Grace says Zara tells the truth, then Grace lies. Proof. Zara lies, and Grace says Zara tells the truth. Since Grace claims the liar Zara tells the truth, Grace lies."
        }
    ],
    "affiliations": [
        "Google Research",
        "UC Berkeley"
    ]
}