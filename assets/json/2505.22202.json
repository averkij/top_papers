{
    "paper_title": "Let's Predict Sentence by Sentence",
    "authors": [
        "Hyeonbin Hwang",
        "Byeongguk Jeon",
        "Seungone Kim",
        "Jiyeon Kim",
        "Hoyeon Chang",
        "Sohee Yang",
        "Seungpil Won",
        "Dohaeng Lee",
        "Youbin Ahn",
        "Minjoon Seo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive language models (LMs) generate one token at a time, yet human reasoning operates over higher-level abstractions - sentences, propositions, and concepts. This contrast raises a central question- Can LMs likewise learn to reason over structured semantic units rather than raw token sequences? In this work, we investigate whether pretrained LMs can be lifted into such abstract reasoning spaces by building on their learned representations. We present a framework that adapts a pretrained token-level LM to operate in sentence space by autoregressively predicting continuous embeddings of next sentences. We explore two embedding paradigms inspired by classical representation learning: 1) semantic embeddings, learned via autoencoding to preserve surface meaning; and 2) contextual embeddings, trained via next-sentence prediction to encode anticipatory structure. We evaluate both under two inference regimes: Discretized, which decodes each predicted embedding into text before re-encoding; and Continuous, which reasons entirely in embedding space for improved efficiency. Across four domains - mathematics, logic, commonsense, and planning - contextual embeddings under continuous inference show competitive performance with Chain-of-Thought (CoT) while reducing inference-time FLOPs on average by half. We also present early signs of scalability and modular adaptation. Finally, to visualize latent trajectories, we introduce SentenceLens, a diagnostic tool that decodes intermediate model states into interpretable sentences. Together, our results indicate that pretrained LMs can effectively transition to abstract, structured reasoning within latent embedding spaces."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 2 0 2 2 2 . 5 0 5 2 : r Lets Predict Sentence by Sentence Hyeonbin Hwang1 Byeongguk Jeon1 Seungone Kim2 Jiyeon Kim1 Hoyeon Chang1 Sohee Yang3 Seungpil Won Dohaeng Lee4 Youbin Ahn4 Minjoon Seo1 1KAIST 2Carnegie Mellon University 3University College London 4LG AI Research {hbin0701, byeongguk, minjoon}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Autoregressive Language Models (LMs) generate one token at time, yet human reasoning operates over higher-level abstractionssentences, propositions, and concepts. This contrast raises central question: can LMs likewise learn to reason over structured semantic units rather than raw token sequences? In this work, we investigate whether pretrained LMs can be lifted into such abstract reasoning spaces building on their learned representations. We present framework that adapts pretrained token-level LM to operate in sentence space, by autoregressively predicting continuous embeddings of next sentences. We explore two embedding paradigms inspired by classical representation learning: semantic embeddings, learned via autoencoding to preserve surface meaning; and (ii) contextual embeddings, trained via next-sentence prediction to encode anticipatory structure. We evaluate both under two inference regimes: DISCRETIZED, which decodes each predicted embedding into text before re-encoding; and CONTINUOUS, which reasons entirely in embedding space for improved efficiency. Across four domainsmathematics, logic, commonsense, and planningcontextual embeddings under continuous inference show competitive performance with Chain-of-Thought (CoT) while reducing inference-time FLOPs in average by half. We also present early signs of scalability and modular adaptation. Finally, to visualize latent trajectories, we introduce SentenceLens, diagnostic tool that decodes intermediate model states into interpretable sentences. Together, our results indicate that pretrained LMs can effectively transition to abstract, structured reasoning within latent embedding spaces."
        },
        {
            "title": "Introduction",
            "content": "Autoregressive Language Models (LMs) have achieved remarkable success on complex reasoning tasks through simple objective: Next-Token Prediction [1]. This success is further amplified by Chain-of-Thought (CoT), which generates explicit intermediate reasoning steps to guide the model [2]. Recent advancements demonstrate substantial gains in performance by scaling inferencetime computation even further [3, 4]. However, next-token prediction requires generating long reasoning chains one token at time, making it computationally inefficient. Also, it remains unanswered whether reasoning at such granularity is genuinely optimal. Equal contribution Our code is available here. Preprint. Under review. Figure 1: Sentence-level reasoning framework. Training: the latent model reads the question tokens and previous embeddings, predicts ˆht, and frozen decoder reconstructs st; Inference: embedding can be rolled forward by (a) Discretized: decode text encode or (b) Continuous: pass-through. While token-level generation has driven recent progress, human cognition typically operates over higher-level abstractionssuch as concepts, propositions, or full sentences [5, 6, 7]. Prior works suggest that language models may similarly benefit from operating at these higher levels, potentially enabling more structured and computationally efficient reasoning [8, 9]. In this paper, we investigate whether pretrained language models can effectively build higher-level representations directly by abstracting over their existing token-level representations, without the prohibitive cost of pre-training from scratch. Specifically, we introduce framework that repurposes pretrained next-token Transformers to reason in latent sentence-level embedding space. Instead of producing outputs token-by-token, our approach predicts continuous embeddings for entire sentences, which can be decoded back into natural language yet primarily function as abstract conceptual representations. To systematically explore viable latent representations, we draw inspiration from the well-established dichotomy in classical representation learning between reconstruction-based and prediction-based methods [10, 11, 12]. We define two embedding paradigms: (1) Semantic embeddings, which prioritize preserving textual fidelity through autoencoding, and (2) Contextual embeddings, which focus on capturing predictive context via next-sentence prediction. We evaluate models trained with these embeddings under two inference regimes: DISCRETIZED, which decodes each predicted embedding into natural language before re-encoding it as the next input, and CONTINUOUS, which performs reasoning entirely within the continuous embedding space. Our empirical findings demonstrate that contextual embeddings consistently outperform semantic embeddings across diverse reasoning domains including mathematics, logic, commonsense, and planning tasks. Notably, contextual embeddings using Continuous inference show competitive performance to token level Chain of Thought reasoning while reducing inference time computational cost by half in average. Finally, we introduce SentenceLens, diagnostic tool that translates intermediate hidden states into readable sentences, thus providing intuitive transparency into the models internal reasoning trajectories. Overall, our analysis provides initial evidence that pretrained inductive biases acquired from token level modeling can be effectively adapted to structured, abstraction level reasoning within latent embedding spaces."
        },
        {
            "title": "2 Sentence embeddings for autoregressive modeling",
            "content": "Unsupervised and semi-supervised sequence representation learning has predominantly evolved along two primary paradigms: reconstruction-based and prediction-based methods [10, 11, 12]. Both methodologies have demonstrated strong empirical performance, yet each emphasizes distinct 2 Figure 2: Illustration of the different types of sentence embeddings used in our framework. representational strengths. Reconstruction-based methods, typically employing autoencoder architectures, excel at semantic fidelity by explicitly encoding and reconstructing input sequences [10], whereas prediction-based methods prioritize capturing contextual semantics by modeling relations to subsequent sequences [11]. Previous research suggests that the optimal embedding strategy varies significantly depending on the target application [13]. In this light, we systematically explore both embedding paradigms within the context of sentence-level autoregressive modeling. Specifically, we adapt an autoregressive Language Model autoencoder framework to construct and evaluate two distinct embedding approaches: semantic embedding, derived through reconstruction objective, and contextual embedding, derived through predictive objective. 2.1 Sentence embedding construction To ensure scalability and avoid vocabulary constraints inherent to discrete codebooks [14], we utilize continuous embedding space. This approach facilitates flexible representational capacity scaling with embedding dimensionality [15]. We build upon the autoencoding framework proposed by ICAE [16] and adapt decoder-only Transformer (e.g., GPT-2), employing shared parameters for encoding and decoding: θENC = θDEC. Given an input sequence = (x1, . . . , xN ), the encoder produces sequence of hidden states = (h1, . . . , hN ). We then define the embedding h[1] := hN as the latent representation of the entire input sequence. This embedding conditions the decoder, trained autoregressively with cross-entropy loss: ˆy = θDEC(h[1]) and LCE = (cid:88) t=1 log p(yt y<t, h[1]) Note that most reasoning tasks consist of question or instruction q, followed by an ordered sequence of reasoning steps (s1, . . . , sn). In this light, we construct training examples tailored to each embedding type as follows (See Figure 2): Semantic embeddings. Each reasoning step si independently forms the input and reconstruction target = = si. Training this way ensures the embedding h[1] encapsulates complete and detailed semantics of the individual reasoning step. Contextual embeddings. We form contexttarget pairs, where context includes the question and preceding reasoning steps (q, s1, . . . , si1), and the target is the current step = si. Thus, embeddings must capture predictive cues essential for reasoning step generation. Optionally, to bridge semantic fidelity with predictive abstraction, we also try contrastive regularization loss (InfoNCE), aligning contextual embeddings closer to corresponding semantic embeddings: LInfoNCE = log exp (sim(ˆzi, zsem exp (cid:0)sim(ˆzi, zsem )/τ ) )/τ (cid:1) , (cid:80) 3 DATASET GSM8K CSQA PROSQA BLOCKSWORLD RECONSTRUCTION SEMANTIC (EM) PREDICTION CTX-B CTX-C COT 98.5 98.5 100.0 100.0 42.0 33.8 80.2 89.9 42.1 35.1 75.3 90.1 43.4 35.7 77.5 84.3 Table 1: Performance of Semantic and Contextual Embeddings across datasets. For Semantic embeddings, we report exact match (EM). For Contextual embeddings, we compare final-answer accuracy (ACC) under different decoding schemes: CTX-B (unregularized), CTX-C (contrastive), and CoT (language-level chain-of-thought). where ˆzi is contextual embedding and zsem are sampled within the batch. We refer to this regularized approach as Contextual-Contrastive (CTX-C) and the unregularized baseline as Contextual-Base (CTX-B). semantic embedding. Negative examples zsem 2.2 Embedding evaluation Setting We evaluate our framework using GPT-2 across four distinct reasoning domains: mathematical reasoning (GSM8K [17]), commonsense reasoning (CommonsenseQA [18]), logical reasoning (ProsQA [19]), and planning (Blocksworld). For each domain, we train on the respective training split and report accuracy on the corresponding test set, analyzing how well our framework generalizes across diverse linguistic subspaces. (i.e., mathematical expressions, natural language, etc.) See Appendix and for more details. To evaluate semantic embeddings performance, we compute exact match (EM) between the original reasoning step si and the decoder output, assessing how faithfully the model reconstructs unseen steps. For contextual evaluation, as there could be multiple correct next steps that could lead to the correct answer, we roll out the model autoregressively: at each step, the generated output is appended to the current input x, continuing until terminal answer is produced. The final answer is then compared against the ground-truth answer. Results are reported in Table 1. Results Across all domains, we observe that the autoencoder successfully restores the original sentences with high fidelity. This aligns with findings from Kuratov et al. [15], who showboth theoretically and empiricallythat language models can compress substantial number of tokens into compact representations. Yet, as we form CommonsenseQA (CSQA) tasks SEMANTIC embedding using subset of Fineweb-Edu corpus (100k documents), we highlight that larger language space (compared to synthetic, constrained, i.e. ProsQA and Blocksworld) involves higher difficulty. In the Contextual configuration, model performance approaches that of the COT baseline on three out of four benchmarks, and notably surpasses it on BLOCKSWORLD across both contextual variants. Introducing the contrastive alignment term (CTX-C) leads to nuanced pattern: scores remain largely unchanged on GSM8K and BLOCKSWORLD, improve modestly on CommonsenseQA, but decline on ProsQA. These trends appear closely tied to each tasks underlying semantic structure. CommonsenseQA questions exhibit substantial lexical variety, so anchoring each latent vector to its semantic counterpart helps tame surface variability. In contrast, ProsQA benefits from simultaneously tracking multiple evolving states; consequently, enforcing single semantic target at each step restricts its representational flexibility, which is consistent with earlier findings [19, 21]. GSM8K and BLOCKSWORLD are highly symbolic and lexically sparsethus, the baseline contextual embedding already forms an unambiguous mapping, leaving little space for improvement through additional regularization. For CSQA restoration, we trained on small subset of FineWeb-Edu [20] due to small CSQA training set."
        },
        {
            "title": "3 Sentence-Level Reasoning Model",
            "content": "Given the strong reconstruction and predictive capabilities of semantic and contextual embeddings, we now present framework that leverages these embeddings for sentence-level reasoning. (Figure 1) 3.1 Architecture We adapt pretrained decoder-only Transformer [22] to operate directly over continuous sentence embeddings instead of discrete natural language tokens. We refer to this model as the Latent Model θLAT. Formally, given natural language question and sequence of latent embeddings corresponding to previously generated sentences h1, . . . , ht, the latent model predicts the embedding for the next sentence: ˆht+1 = θLAT(q, ht). At inference time, predicted embeddings ˆht+1 are mapped to the next input embedding ht+1 using mapping function : Rd Rd, where denotes the embedding dimensionality: ht+1 = M(ˆht+1). This process continues autoregressively, forming latent embedding trajectory that encodes the progression of reasoning steps. At each step, sentence decoder θDEC : Rd can decode latent embeddings back into natural language text. However, decoding intermediate reasoning steps is optional; embeddings can remain in their latent form to enhance computational efficiency, particularly when only the final answer is required. To this end, lightweight termination classifier can evaluate each predicted embedding ˆht to determine when reasoning should conclude. 3.2 Training natural approach for this task is to train the transformer model to generate sentence embeddings by minimizing the Mean Squared Error (MSE) between predicted and target embeddings. However, single context often allows for several valid yet distinctly different continuations. [8]. Under these conditions, MSE tends to blend these varied possibilities into single averaged representation, thus blurring meaningful variation. To address this, we employ cross-entropy (CE) loss calculated over natural language targets generated by frozen decoder. This encourages predicted embeddings to align with the manifold defined by such decoder: LCE = n1 (cid:88) t=1 log p(cid:0)st+1 θDEC(ˆht+1)(cid:1). During training, the latent model conditions on the question and ground-truth sentence embeddings hi, each computed using fixed encoder θENC. Additionally, to enhance the alignment between predicted and teacher-forced embeddings, we incorporate an InfoNCE loss [14]: LInfoNCE = n1 (cid:88) t=1 log exp (cid:0)sim(ˆht+1, ht+1)/τ (cid:1) exp (cid:0)sim(ˆht+1, hj)/τ (cid:1) . (cid:80) The overall training objective combines both terms: Loverall = LCE + λ LInfoNCE. To further improve training stability, we include shallow projection layers between the encoder output and latent model input, and between the latent model output and decoder input. We use an oracle termination classifier for simplicity. See Appendix for more details. 5 3.3 Inference We explore two strategies for defining the mapping function during inference. Let represent the average token length per reasoning step, and the total number of steps in reasoning trace. Inspired by SentenceVAE [23], we apply decode-and- (1) Discretized (Language-Level) reencode procedure: M(ˆht) = E(D(ˆht)), where the predicted latent is first decoded into sentence and then re-encoded into the models input space. We refer to this as the DISCRETIZED mode, as each step explicitly traverses the discrete natural language interface. This approach helps mitigate error compounding [24], but comes at higher computational cost, with attention cost scaling as O(L2R + R2). detailed complexity analysis can be found in Appendix C. (2) Continuous (Latent-Level) Following Coconut [19], we define the mapping as an identity function = I, directly propagating the predicted latent embedding ˆht without intermediate decoding. In this CONTINUOUS mode, reasoning is entirely performed within the continuous embedding space, enabling significantly more efficient inference with attention complexity reduced to O(R2). Both methods offer computational advantages over natural language CoT, which incurs O(L2R2) attention complexity even under key-value caching. However, the savings in the DISCRETIZED mode are conditional: they occur only when either (1) the encoder-decoder are not too computation-heavy, or (2) attention dominates over MLP costtypially when the total output length LR is relatively long (e.g., Blocksworld). Otherwise, the repeated decoding and encoding introduce additional MLP overhead. 3.4 Experiments Building upon prior studies [19, 21], we select GPT-2 as our baseline model and evaluate its performance across four distinct reasoning domains detailed in Section 2. To investigate optimal embedding strategies for latent reasoning, we examine Semantic and Contextual (both Ctx-B and Ctx-C) embeddings from Section 2. We further explore hybrid architectureSem (input) Ctx (output)which mirrors the natural separation of representational roles found in conventional language modeling. For evaluation, we compare sentence-level reasoning models against three baseline models. First, CoT represents fully supervised model trained with access to both intermediate reasoning steps and final answers. Second, No-CoT omits step-level supervision and is trained solely to predict final answers. Third, we include Coconut [19], which gradually forgoes explicit token-level targets with curriculum-based substitution of fixed number last hidden states. 3.5 Results Again, our main objective is to examine whether latent sentence-level reasoning framework can effectively generalize to higher-level abstractions while preserving the learned priors of the model. Achieving comparable performance to token-level Chain-of-Thought (CoT) would provide preliminary evidence toward this goal. To this end, we address the following three research questions. Q1: Can sentence-level reasoning match token-level CoT performance? We hypothesize that effective reasoning is driven more by transitions between high-level concepts than by fine-grained token-level details. Empirically, sentence-level models match or even exceed CoT performance on logical and commonsense reasoning tasks. On mathematical and planning benchmarks, performance is slightly lower, though the gap remains modest. We attribute this to the greater precision often required in these domains, where continuous latent representations may be more prone to fidelity loss. Q2: How does sentence-level reasoning differ between language-level and latent-level inference? To explore this, we compare model inference in the DISCRETIZED (language-level) space with Note that using contextual encoder incurs greater computational cost than semantic encoder. 6 SETTING PROSQA No-CoT CoT Sem. Ctx-B. Ctx-C. Sem. Ctx. Coconut [19] Sem. Ctx-B. Ctx-C. Sem. Ctx. 76.7 77.5 83.6 91.4 79.8 83.8 97.0 86.0 92.6 81.6 85.4 CSQA DIRECT 23. LANGUAGE-LEVEL 35.7 28.5 35.2 40.3 34.9 LATENT-LEVEL 34.0 27.5 37.0 35.5 33.6 GSM8K BLOCKSWORLD 18.7 43.4 38.9 39.0 37.1 40.3 34.1 29.6 37.4 38.3 29.3 36.8 84.3 32.9 70.0 76.3 67.1 37.9 30.8 70.5 80.8 52. Table 2: Performance on PROSQA, CSQA, GSM8K, and BLOCKSWORLD across different embedding paradigms. Bolded values indicate the best performance among our proposed methods within each section. Baseline results are highlighted with background colors. that in the CONTINUOUS (latent-level) space. Results reveal complementary strengths: continuous models excel on logic and planning tasks, where reasoning benefits from uninterrupted latent-space composition and abstract state transitions. Conversely, discretized models show modest advantages on commonsense and mathematical benchmarkslikely due to the grounding effect of explicit linguistic representations. Still, the observed performance gaps are narrow3.3% on commonsense and 0.7% on mathindicating that latent inference remains viable and compute-efficient alternative. These findings suggest that effective reasoning need not always traverse explicit language space; continuous representations alone may support structured inference. Q3: Can sentence-level reasoning reduce computational cost? Table 8 compares computational costs (FLOPs) between latent reasoning model and tokenlevel CoT under forward-pass evaluation with keyvalue caching enabled. Latent reasoning employs an oracle answer classifierexecuted via single forward pass through the translatorthat monitors the predicted embedding sequence and halts generation upon detecting special answer token. The final latent embedding is decoded into natural language for evaluation. Table 3: Average inference-time compute cost (GFLOPs) for each dataset under CoT and CTXC CONTINUOUS Inference. DATASET COT CTX-C CSQA PROSQA GSM8K BLOCKSWORLD 25.89 100.99 21.45 58.69 9.96 70.19 12.68 28.57 Note that we measure computational costs across the full latent pipeline, including classifier and decoder components, which remain unoptimized. Thus, reported efficiency gains represent conservative estimate. Across tasks, CONTINUOUS inference achieves 1.52.5 better efficiency compared to token-level CoT. Notably, we highlight that even DISCRETIZED inference outperform CoT in longer reasoning tasks (e.g., Blocksworld w/ average trace length 9.1: 52.26 GFLOPs vs. 58.69 GFLOPs). We expect this efficiency gap to grow as the length of reasoning trace increases."
        },
        {
            "title": "4 Discussion",
            "content": "4.1 Potential Scalability and Modularity Scalability We report preliminary observations that suggest our framework has potential to scale to increasing model capacity. Due to computational constraints, our experiments are limited to sub-1B To see the cost with lightweight classifier, please refer to Appendix D. 7 (a) CoT vs. CTX-B on CommonsenseQA across GPT-2 variants. (b) GPT-4o Qualitative evaluation of the reasoning steps evaluated using similar metric employed in [25], where SFT is trained using CoT and ours is using CTX-B. models; we evaluate GPT-2 Medium (345M) and GPT-2 Large (775M) on the CommonsenseQA (CSQA) benchmark, which exhibits clear performance scaling under CoT fine-tuning. As shown in Figure 3a, the Ctx-C configuration attains performance comparable to, and in some cases exceeding, CoTdespite operating entirely in latent space and incurring lower inference-time compute. While tentative, these findings suggest that latent reasoning could offer more compute-efficient path toward generalization. However, we acknowledge that scaling to extensively pretrained models remains as challenge, since stable adaptation under greater distribution shifts could be more difficult [19]. Using Off-the-Shelf EncoderDecoder We investigate whether the encoderdecoder can be decoupled from the latent model and replaced with smaller, fixed components. This modular design seeks to reduce the computational burden of DISCRETIZED inferenceespecially in settings where only the latent reasoning module requires adaptation. To evaluate this hypothesis, we paired lightweight GPT-2 Small encoderdecoder (trained on Ctx-C) with GPT-2 Medium latent model and assessed performance on GSM8K. This hybrid configuration achieved an accuracy of 42.23, compared to 47.69 for fully fine-tuned GPT-2 Medium with CoT training. While accuracy decreases slightly, the results demonstrate that predictive embeddings can transfer across model architectures with reasonable degradationsupporing the feasibility of modular reuse. Given prior findings on general embedding space alignment across models [26, 27], further exploration with larger models and diverse tasks remains promising direction. 4.2 SentenceLens: Towards Human-Readable Interpretability We introduce SentenceLens, an intrepretability tool that decodes intermediate hidden representations by directly passing them through the trained sentence-level decoder. In contrast to token-level inspection methods such as Logit Lens [28], SentenceLens operates at the sentence level, offering more human-readable view of the models evolving internal states across reasoning steps. For example, in Table 6, we show how the models prediction shifts across layers during the transition from one reasoning step to the next. When making first step prediction ˆh1, Layer 19 introduces general observation about eating and energy levels, while Layer 22 begins to center on the idea that hunger motivates goal-directed behavior. These intermediate activations reflect gradual shift in conceptual focus, which in the last layer (36th) develops as: If you are hungry, you are likely engaging in an activity that requires sustenance. Since the latent model frames reasoning as continuous process, we hypothesize that intermediate latent states may become naturally decodableallowing us to observe the progression of inference across steps. To see more examples, see Appendix A. GSM8K was selected based on preliminary findings that moderately sized datasets help stabilize shallow MLP mappings across heterogeneous embedding spaces. 8 Step Decoded Sentence(s) Question If you are hungry and going fishing, why would you be going fishing? A: to see the fish B: have fun C: catching fish D: wet clothes E: killing 0 1 1 1 2 2 3 3 3 4 4 4 5 LAYER 19: person who eats lot experiences increased energy levels. LAYER 22: person who is hungry seeks to alleviate their hunger. When you are hungry, you engage in an activity to satisfy your hunger. If you are hungry, you are likely engaging in an activity that requires sustenance. LAYER 9: If person is hungry, they are likely to engage in eating. LAYER 20: The act of catching fish involves physical activity. Fishing is common activity for those who enjoy the outdoors. LAYER 4: Fishing is common activity for those who enjoy catching fish. LAYER 21: The act of catching fish can lead to enjoyment and recreation. Fishing is recreational activity that people engage in for fun. LAYER 9: The act of catching fish provides direct source of food. LAYER 21: The act of catching fish provides direct source of food. People fish to enjoy the experience of catching fish. Fishing is recreational activity that people often engage in. LAYER 5: Fishing is recreational activity that is often pursued with friends. Therefore, fishing is good reason to go fishing. ### Table 4: Latent Sentence Transitions with SENTENCELENS for GPT2-Large under the CTXC, CONTINUOUS setting. We visualize intermediate decoding across layers and reasoning steps. Highlighted rows represent the output from the final latent embedding at each step. CoT Model Reasoning Trace If you are hungry, you likely seek food to satisfy that hunger. Fishing is an activity that typically results in catching fish. Catching fish is common reason for going fishing. Seeing the fish is primary motivation for engaging in fishing. ### Table 5: Natural Language CoT Trace. Output from the CoT trained model (COT) Qualitative Analysis In addition, when decoding output embeddings at successive latent reasoning steps (e.g., Step 1 through Step 5), we find that the resulting sentences, while readily understandable, often lack the coherence and rigor characteristic of standard CoT responses. We compare two model outputs using GPT-4o evaluation with the rubric proposed by Ye et al. [25]. This scores Relevance, Fluency, Conciseness, Soundness, and Interpretability on 1 to 5 Likert scale. It turns out that CTX-C model mostly produces reasoning chains of moderate quality (scores > 3); However, its performance falls short compared to CoT models trained directly in natural language space (Figure 3b). The largest weakness appears in Soundness, which aligns with earlier observations that high-level concept models may exhibit reduced coherence even after extensive pretraining [8]. While we believe this tradeoff is natural consequence of abstraction, bridging this gap remains an interesting direction for future research. Future Directions Another interesting direction is to self-train the model by using its own intermediate decoded sentences as auxiliary supervision targets. We also observe the correct answer often surfaces early in the reasoning trajectory. (see Appendix A). In this light, these intermediate outputs could offer novel training signal that could enhance both reasoning efficiency and stability. Furthermore, unlike prior latent reasoning approaches, our framework allows for sampling in the 9 Figure 4: Performance Change when injecting Gaussian random noise to different modes of inferencing, for Ctx-C model in GSM8K and CSQA datasets. token-level after decoding. This opens the door to applying reinforcement learning or trajectory-level optimization over the latent reasoning chain."
        },
        {
            "title": "4.3 Fragility of Continuous Embeddings",
            "content": "Latent reasoning operates over high-dimensional embedding manifolds, which tend to be more sensitive to perturbations than discrete token-level autoregression [8, 24]. To systematically assess this fragility, we introduce synthetic noise at inference time, following team et al. [8] with 50% probability. We evaluate robustness across three intervention points in the reasoning pipeline: (1) Language-Level (Input): noise is applied to the input embedding; (2) Language-Level (Output): noise is added to the output embedding; and (3) Latent-Level: noise is directly injected into the predicted output embedding, which is then autoregressively consumed in the next step. Empirically, we observe two key trends: (1) performance degrades more rapidly on GSM8K, where precise numerical reasoning amplifies the impact of noise; and (2) Language-Level inference (i.e., decoding and re-encoding) consistently yields greater robustness than latent-only reasoning across both tasks. This supports the intuition that grounding in language acts as regularizing prior, mitigating error accumulation at the cost of additional compute. These findings highlight tradeoff between efficiency and stability, motivating future work on approaches that help prevent error compounding."
        },
        {
            "title": "5 Related Works",
            "content": "Sentence Representations Sentence-level representation learning has historically followed two main paradigms: reconstruction and context prediction. Early methods, such as sequence autoencoders [10] and Skip-Thought vectors [11], learned fixed-length sentence embeddings by reconstructing input or neighboring sentences. Subsequent research, exemplified by Quick-Thought [29], shifted towards contrastive prediction, focusing on distinguishing the correct sentence context from distractors. Contrastive learning builds on these paradigms by explicitly aligning semantically related sentences while distinguishing unrelated examples. Models such as Sentence-BERT [30] and SimCSE [31], inspired by SimCLR [32], have produced robust sentence embeddings with excellent transfer performance. Our framework builds upon these developments by defining semantic and contextual embeddings and employing contrastive learning to align latent input-output pairs [12]. Sentence-Level Prediction Several models move beyond token-level generation to predict entire sentences. Latent-variable approaches such as VAEs [33] and hierarchical decoders [34] generate sentences from continuous codes. LCM [8] autoregresses over sentence-level concept embeddings in multilingual, multimodal space, while CoCoMix [9] injects sparse autoencoder-derived vectors into hidden states to improve interpretability and control. Our method similarly operates over latent embeddings but distinguishes itself by building upon pretrained models rather than training from scratch. This approach allows us to leverage existing language understanding capabilities while introducing latent reasoning mechanisms. 10 Latent-Space Reasoning Efficiency and abstraction have motivated reasoning directly in embedding space, bypassing token generation. Joint embedding architectures [35] and predictive coding frameworks [12] model representation dynamics by forecasting future embeddings. This idea has recently been extended to language: Hao et al. [19] introduced continuous latent reasoning, where token-level embeddings are gradually replaced with continuous embeddings with the last-layer hidden states through curriculum-based strategy from Deng et al. [21]. Further extensions include, among others, methods by Shen et al. [36] which guide latent rollouts using self-distillation; and Su et al. [37] which propose mixing discrete token embeddings from trained VQ-VAE [14] for inference efficiency. Our work differs from these approaches primarily in three ways. (1) We provide explicit access to intermediate latent states through decoding, offering clearer insights into the reasoning trajectory. (2) Our method uniquely supports token-level sampling during latent-level reasoning, opening exciting research avenues such as self-training and reinforcement learning. (3) Whereas previous methods require iterative sampling of latent representations during training, which involves n+1 forward passes per iteration, our approach completes this in single forward pass, significantly improving scalability."
        },
        {
            "title": "6 Conclusion",
            "content": "We present framework that elevates pretrained language models from token-level generation to sentence-level reasoning by autoregressively predicting continuous embeddings of next-step sentences. This enables reasoning over more abstract conceptual units while retaining pretrained inductive biases. Our exploration of semantic and contextual embeddings reveals that contextual embeddings show competitive performance with token-level Chain-of-Thought (CoT) across diverse reasoning tasks, while significantly reducing inference-time computational costs under Continuous inference. Additionally, we demonstrate signs of scalability, modular reuse of encoderdecoder components, and enhanced interpretability through SentenceLens, which decodes latent embeddings into human-readable sentence-level traces. These findings suggest that pretrained language models could be effectively adapted for structured reasoning in latent embedding spaces, opening new directions for efficient latent reasoning systems."
        },
        {
            "title": "Limitations",
            "content": "Need for Large-Scale Experiments We conduct preliminary exploration of sentence-level reasoning with GPT-2 variants. To keep experiments reproducible, we start with GPT-2 Small as our base modelfollowing recent work on latent-level reasoning [21, 19] and then explore scalability by evaluating GPT-2 Medium, GPT-2 Large, and lightweight hybrid that pairs GPT-2 Medium latent core with GPT-2 Small encoderdecoder. During our experiments, we observed that larger models become somewhat more sensitive to hyperparameter choices which could often lead to increased performance gap between our method and CoT training. We note that this increased gap has been observed for similar preliminary researches when scaled to more competitive models (i.e. Llama 3 [38]), and conjecture as one of the reasons why recent works have turned towards pretraining. We hypothesize such challenge arises from the widening gap between the token-level embedding distributions learned during pretraining and the compact, coarser-grained manifold our adapter enforces. In effect, the very inductive biases that make large models robust in token space may conflict with sentence-level abstractions. systematic study of this tensionand the design of transfer mechanisms that preserve high-capacity knowledge while avoiding overfitting to the latent manifoldremains an important avenue for future research. Fragility of Latent Reasoning As illustrated in Figure 4, pure latent reasoning, as it is conducted entirely within continuous embedding space, becomes notably fragile. Unlike DISCRETE-STEP inference, which introduces discrete decoding step that inherently quantizes minor perturbations, the continuous pathway lacks such built-in stabilization. This discrete bottleneck serves as form of regularization, filtering out numerical noise and constraining the models trajectory to finite set of linguistically meaningful sequences. However, this regularization comes at the expense of expressivity, limiting outputs to token sequences present in the vocabulary. In fully continuous framework, the model must learn to establish implicit attractors or decision boundaries to maintain trajectories within coherent manifoldeffectively performing form of soft discretization. These learned boundaries, being approximate, may allow small deviations to persist and amplify over successive reasoning steps, potentially leading to significant semantic errors, especially in tasks demanding precision or extended reasoning chains. This vulnerability mirrors challenges observed in continuous control systems, where minor deviations can accumulate over time, resulting in substantial performance degradation unless addressed through specialized stabilization mechanisms [24]. Future work could explore hybrid framework that integrate discrete bottlenecks at critical junctures within the reasoning process, aiming to combine the robustness of discretization with the flexibility of continuous representations. Training from Scratch Training model from scratch directly in the higher abstractions i.e. sentence embeddings space appears, at first glance, to be the cleanest path toward robust highlevel reasoning. Prior work argues that models initialized on discrete-token objectives must later overcome distribution shift when asked to operate over sentence-level abstractions, and this difficulty intensifies as model sizeand pretraining data sizeincrease [19, 21] and therefore has leaned towards pretraining [39, 36] Yet genuine intelligence might not rely on starting from clean slate each time the abstraction level changes. We hypothesize that system that truly generalizes beyond human capability must be able to climb the ladder of abstraction after exposure to raw experience, flexibly re-encoding its knowledge in coarser units. At the same time, safety considerations dictate that these higher-order representations remain interpretableanchored to manifold we can inspect and, when necessary, constrain. Our adaptation framework takes step in this direction: it shows that pretrained token-level language model can be lifted, with modest additional supervision, onto an interpretable sentence-manifold without retraining everything from scratch. By demonstrating both the promise and the fragility of this approach, the present work highlights critical research frontier: designing models that learn to abstract while preserving previously learned inductive bias."
        },
        {
            "title": "Broader Impacts",
            "content": "This work introduces novel framework for reasoning in continuous latent space, offering both practical and societal benefits. By avoiding token-level autoregressive decoding, it reduces computational overhead and may lower the environmental footprint of large-scale inference. Importantly, our method maintains interpretability by anchoring latent representations to human-readable abstractions. Nonetheless, broader risks remain. If latent reasoning frameworks are deployed without transparency mechanisms, they may obscure decision processesespecially in high-stakes domains. Additionally, latent representations could encode and propagate biases present in pretraining data. As reasoning becomes more abstracted from language, care must be taken to ensure meaningful human oversight is preserved. We encourage future work to strengthen interpretability guarantees and explore safeguards that prevent misuse or unintended consequences."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank Seonghyeon Ye, Jinho Park, Seongyun Lee, and Jaehyeok Doo for their insightful discussions and valuable feedback."
        },
        {
            "title": "References",
            "content": "[1] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. neural probabilistic language model. J. Mach. Learn. Res., 3(null):11371155, March 2003. ISSN 1532-4435. [2] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [3] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card, 2024. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] Jerry Fodor. The Language of Thought. Harvard University Press, 1975. [6] Hugo Mercier and Dan Sperber. Why do humans reason? arguments for an argumentative theory. Behavioral and Brain Sciences, 34(2):5774, 2011. doi: 10.1017/S0140525X10000968. [7] Yoshua Bengio. The consciousness prior, 2019. URL https://arxiv.org/abs/1709. 08568. [8] LCM team, Loïc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-jussà, David Dale, Hady Elsahar, Kevin Heffernan, João Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, and Holger Schwenk. Large concept models: Language modeling in sentence representation space, 2024. URL https://arxiv.org/abs/2412.08821. [9] Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, and Xian Li. Llm pretraining with continuous concepts, 2025. URL https://arxiv.org/abs/2502.08524. [10] Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning, 2015. URL https: //arxiv.org/abs/1511.01432. [11] Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Skip-thought vectors, 2015. URL https://arxiv.org/abs/1506. 06726. [12] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding, 2019. URL https://arxiv.org/abs/1807.03748. [13] Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data, 2016. URL https://arxiv.org/abs/1602.03483. [14] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. URL https://arxiv.org/abs/1711.00937. [15] Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into single vector and back again: Exploring the limits of embedding space capacity, 2025. URL https://arxiv.org/abs/2502.13063. [16] Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model, 2024. URL https://arxiv.org/abs/ 2307.06945. [17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 14 [18] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge, 2019. URL https: //arxiv.org/abs/1811.00937. [19] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space, 2024. URL https://arxiv.org/abs/2412.06769. [20] Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557. [21] Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step, 2024. URL https://arxiv.org/abs/2405.14838. [22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv. org/abs/1706.03762. [23] Hongjun An, Yifan Chen, Zhe Sun, and Xuelong Li. Sentencevae: Enable next-sentence prediction for large language models with faster speed, higher accuracy and longer context, 2024. URL https://arxiv.org/abs/2408.00655. [24] Max Simchowitz, Daniel Pfrommer, and Ali Jadbabaie. The pitfalls of imitation learning when actions are continuous, 2025. URL https://arxiv.org/abs/2503.09722. [25] Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. Flask: Fine-grained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928, 2023. [26] Alexis Conneau, Guillaume Lample, MarcAurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Word translation without parallel data, 2018. URL https://arxiv.org/abs/1710.04087. [27] Rishi Jha, Collin Zhang, Vitaly Shmatikov, and John X. Morris. Harnessing the universal geometry of embeddings, 2025. URL https://arxiv.org/abs/2505.12540. [28] nostalgebraist. the logit lens, 2020. URL https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens. https://www. lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens. interpreting gpt: [29] Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representations, 2018. URL https://arxiv.org/abs/1803.02893. [30] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks, 2019. URL https://arxiv.org/abs/1908.10084. [31] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings, 2022. URL https://arxiv.org/abs/2104.08821. [32] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations, 2020. URL https://arxiv.org/abs/ 2002.05709. [33] Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from continuous space, 2016. URL https://arxiv.org/ abs/1511.06349. [34] Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, and Yoshua Bengio. hierarchical latent variable encoder-decoder model for generating dialogues, 2016. URL https://arxiv.org/abs/1605.06069. [35] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with jointembedding predictive architecture, 2023. URL https://arxiv.org/abs/2301.08243. 15 [36] Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation, 2025. URL https: //arxiv.org/abs/2502.21074. [37] DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted: Mixing latent and text tokens for improved language model reasoning, 2025. URL https://arxiv.org/abs/2502.03275. [38] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, 16 Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [39] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens, 2024. URL https://arxiv.org/abs/2310.02226. [40] Bernd Bohnet, Azade Nova, Aaron Parisi, Kevin Swersky, Katayoon Goshvadi, Hanjun Dai, Dale Schuurmans, Noah Fiedel, and Hanie Sedghi. Exploring and benchmarking the planning capabilities of large language models. arXiv preprint arXiv:2406.13094, 2024."
        },
        {
            "title": "A SentenceLens Examples",
            "content": "We include representative SentenceLens example that highlights additional key observations. Specifically, the model often identifies the correct answer early in the latent trajectory; however, subsequent chain-of-thought (CoT) tokens exhibit drift that ultimately leads to an incorrect prediction. (The correct answer is C.) This suggests room for improvement by using intermediate representations as explicit supervision targets, which could guide the construction of model centric datasets and self-training methods. Step Decoded Sentence(s) Question For many males hair is concern as they get older, it begins to what, causing receding hairline? A: thin out B: grow in ear C: fall out D: bulge E: composted 0 1 1 2 2 2 3 3 3 4 4 5 5 LAYER 19: The human body requires certain amount of energy to maintain its functions. LAYER 20: The primary cause of aging is the loss of moisture. One of the common changes in hair density over time is the decrease in hair volume. LAYER 4: common reason for hair loss is due to decrease in hair density and diameter. LAYER 23: The aging process causes various health issues. The hair loss is often associated with hair loss. LAYER 3: This process is often referred to as balding. LAYER 23: The aging process leads to reduced body size. thinning hairline is commonly associated with hair loss. LAYER 2: This process can lead to decrease in hair density and diameter. LAYER 11: The process of getting older leads to the body becoming thinner. This process is commonly referred to as fading. LAYER 4: This process is common in older individuals who lack regular hair growth. ### (Incorrect) Table 6: Example of Latent Reasoning Trajectory inspected with SentenceLens. Although early steps intermediate layers demonstrate accurate associations with hair loss and balding, the final prediction selects an incorrect choice, showing drift in reasoning at later stages. Step Decoded Sentence(s) Question 0 1 5 Why would you take bus to work? A: commute B: flying C: get somewhere D: travel E: go home LAYER 19: person spends time traveling between different locations. LAYER 20: person spends time commuting to work. LAYER 21: person spends time traveling, which often involves moving from one place to another. LAYER 22: person spends time traveling, which often involves traveling across distances. People often take the bus to reach destination. ### (Correct) ... not shown Table 7: Early Answer Emergence in Latent Reasoning. The model brings up the concept of commuting in the reasoning chain even before the first autoregressive step completes. This hints at potential efficiency gains by leveraging early, confident predictions as supervision signals in training."
        },
        {
            "title": "B Dataset Description",
            "content": "Mathematics We use the GSM8K dataset [17], which consists of grade-school math word problems originally comprising 7.8k training and 1.3k test samples. Following prior expansions [19, 21], we adopt an extended version containing approximately 370k training examples to support large-scale latent model training. Planning Following prior work [40], we use the Blocksworld environment for planning evaluation, but construct the dataset generation pipeline using our own Python implementation. We evaluate the model on 7-block configurations, ensuring that the initial and goal states do not overlap across the training, validation, and test sets. We use 9.9k samples for training, and 380 samples each for testing. Logical We adopt ProsQA [19], synthetic dataset grounded in first-order logic. Each instance presents multiple distractors and requires multi-hop reasoning over structured graph. Prior work highlights that latent models capable of multi-state tracking exhibit strong performance on this task. We use 17.8k training set and 500 samples for evaluation. Commonsense We use CommonsenseQA [18], multiple-choice benchmark that lacks explicit Chain-of-Thought (CoT) supervision. To enable training with intermediate reasoning steps, we augment the data using GPT-4o to generate CoT-style rationales. Our training split includes 8.5k examples, and for evaluation, we reserve 611 samples from the validation set. Figure 6 illustrates representative examples from each dataset."
        },
        {
            "title": "C Computation Complexity Analysis",
            "content": "Attention Complexity under KV-caching Let be the average number of tokens per sentence, the number of reasoning steps, and ignore the prompt length N0 in leading order. (1) Chain-of-Thought (CoT). Each step emits new tokens into the context. Before step t, the context length is N0 + (t 1)L, so (cid:88) CCoT = (cid:88) (cid:2)N0 + (t 1)L + (j 1)(cid:3) = O(cid:0)L2R2(cid:1). t=1 j=1 (2) Contextual Embedding Mode. At each step the model (i) decodes one latent into an L-token sentence and (ii) attends over all retained tokens to predict the next latent: (cid:88) (cid:88) + t=1 j=1 (cid:123)(cid:122) (cid:125) (cid:124) O(L2R) (cid:88) t=1 (cid:124) (N0 + (t 1)L) = O(cid:0)L2R + R2(cid:1). (cid:123)(cid:122) O(L R2) (cid:125) (3) Language-Grounded Mode. Each step (i) processes only latents in the main chain (cid:0)O(R2)(cid:1) and (ii) decodes and re-encodes an L-token sentence (cid:0)O(L2R)(cid:1), yielding CLG = O(cid:0)R2 + L2R(cid:1). (4) Pure Latent Mode. Each step adds one latent vector; attending over 1 latents gives Clatent = (cid:88) t=1 (N0 + 1) = O(cid:0)R2(cid:1). Summary of leading-order costs: CCoT = O(L2R2), Ccontextual = O(L2R + R2), CLG = O(L2R + R2), Clatent = O(R2). MLP Overhead In addition to attention cost, every decoded or re-encoded token incurs feedforward (MLP) computation. More specifically: CoT & Contextual Embedding: emits tokens per step processes tokens through MLP O(LR). Language-Grounded: With semantic encoder, each step decodes and re-encodes tokens on compact codesprocessing 2L tokens per step for an MLP cost of O(LR). If instead contextual encoder must re-attend over up to N0 + (t 1)L tokens each pass, it incurs an additional O(LR2) MLP overhead, which can erode attention savings unless the encoder is shallow or non-autoregressive. Pure Latent: processes one latent per step O(R). Concluding Remark Under KV-caching, the Language-Grounded modewith semantic encoderadds an O(L2R) decode/re-encode overhead, but makes it ideal for tasks sensitive to errorcompounding or instability (i.e. Mathematics.) In contrast, the Pure Latent mode eliminates all token-level context (attention O(R2), MLP O(R)), offering maximal efficiency when possible."
        },
        {
            "title": "D Termination Classifier",
            "content": "While we initially assume an oracle termination signal by using the first token generated by the decoder, we also demonstrate that this decision can be learned by lightweight classifier. Specifically, we train three-layer feedforward neural network (MLP) to identify the answer sentence during CONTINUOUS inference. The MLP consists of linear layers with hidden dimensions of 192 and 48, each followed by GELU activation, and outputs single logit for binary classification (continue versus terminate). It is trained using binary cross-entropy loss with logits (BCEWithLogitsLoss). Note that the average inference GFLOPs, reported in Table 9, are lower than those reported in Table 3."
        },
        {
            "title": "E Experiment Details",
            "content": "Each dataset requires task-specific hyperparameter choices due to variation in problem structure and reasoning complexity. For all experiments, we report the best test-set accuracy across saved checkpoints (including baselines). When training all of our models (Latent Model, Encoder, and Decoder), we initialize them from the SFT checkpoint. The number of training epochs for each stage was selected based on convergence trends observed during early stage of experiments. Please note that we use small portion of Fineweb-Edu [20] for CSQA tasks restoration (i.e. training for semantic embeddings.) We report hyperparameters used in Table 10 and 11."
        },
        {
            "title": "F Evaluation Prompt",
            "content": "Please refer to Figure 5. Split Metric CSQA ProsQA GSM8K Blocksworld TRAIN VALID TEST Question tokens/sample Steps/sample Tokens/step Question tokens/sample Steps/sample Tokens/step Question tokens/sample Steps/sample Tokens/step 39.0 5.6 10.9 38.4 5.6 10.7 38.8 5.6 10.8 360.4 3.8 9.5 361.0 3.8 9. 357.0 3.8 9.5 42.2 3.6 6.0 55.1 4.2 6.0 56.8 4.3 6.1 146.8 8.9 8.0 146.5 9.2 8. 146.6 9.1 8.0 Table 8: Dataset statistics for each reasoning benchmark across train, validation, and test splits. DATASET CSQA PROSQA GSM8K BLOCKSWORLD COT 25.89 100.99 21.45 58. CTX-C CLASSIFIER ACCURACY 8.51 64.02 10.80 26.73 99.36 99.76 99.46 97.95 Table 9: Average inference-time compute cost (GFLOPs) on each dataset under CoT and CTX-C CONTINUOUS inference, with the accuracy of the trained classifier. GSM8K CSQA ProsQA Blocksworld Stage SFT* Epochs LR Batch EMBEDDING: RESTORATION Epochs LR Batch EMBEDDING: PREDICTION Epochs LR Batch 20 1e-4 64 3 5e-4 256 30 5e-4 128 20 1e-4 5 5e-4 512 50 5e-4 128 20 1e-4 64 3 5e-4 128 50 5e-4 96 LATENT AUTOREG. Epochs Eval Freq LR Batch 200 every 10 5e-4 128 300 every 10 5e-4 128 50 every 2 5e-4 32 100 1e-4 64 100 1e-4 1024(256*4) 50 1e-4 200 every 10 5e-4 64 Table 10: Training configurations of GPT-2 for each dataset and training stage. *SFT includes both CoT and No-CoT variants. GPT-2 Small GPT-2 Medium GPT-2 Large (LoRA) r=256, a=1024) 20 1e-4 20 1e-4 64 8 20 1e-4 64 8 5 5e-4 512 used FW subset 5 5e-4 128 used FW subset 5 5e-4 128 used FW subset Stage SFT Epochs LR Batch EMBEDDING: RESTORATION Epochs LR Batch Notes EMBEDDING: PREDICTION Epochs LR Batch 50 5e-4 128 LATENT AUTOREG. Epochs Eval Freq LR Batch Notes 300 every 10 5e-4 128 50 5e-5 128 300 every 10 1e-4 64 50 1e-4 64 300 every 2 1e-4 128 w. grad ckpting Table 11: Training configurations by model size and stage. LoRA configuration used for GPT-2 Large. Figure 5: Evaluation Prompt used to GPT-4o for judging intermediate reasoning steps quality. 23 Figure 6: Example instances from each dataset."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "KAIST",
        "LG AI Research",
        "University College London"
    ]
}