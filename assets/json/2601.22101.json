{
    "paper_title": "ECO: Quantized Training without Full-Precision Master Weights",
    "authors": [
        "Mahdi Nikdan",
        "Amir Zandieh",
        "Dan Alistarh",
        "Vahab Mirrokni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 1 0 1 2 2 . 1 0 6 2 : r ECO: Quantized Training without Full-Precision Master Weights Mahdi Nikdan1,2, Amir Zandieh1, Dan Alistarh2 and Vahab Mirrokni1 1Google Research, 2ISTA Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and decaying learning rate, ECO converges to constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30800M), Gemma-3 1B model, and 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier. 1. Introduction Scaling Large Language Model (LLM) training comes with substantial computational and memory costs. As models have grown from billions to trillions of parameters, training memory has become central bottleneck. Low-precision training has therefore emerged as practical direction: recent FP8 (Liu et al., 2024a; Peng et al., 2023), and even lower precision (Panferov et al.) training methods can reduce activation memory and accelerate training while maintaining stable optimization. Figure 1 Static Memory Used vs Validation Loss comparing the standard BF16, FP8 with Master weights (FP8 w/ MW) baselines with standard stochastic rounding (FP8 w/o MW + SR) and ECO. ECO with stochastic rounding (SR) provides significantly better Pareto frontier. Gradient accumulation is disabled in all cases. Despite this progress, key overhead in quantized training remains untouched: the presence of master weights. Most quantized and quantization-aware training pipelines still preserve high-precision copy of the parameters (typically FP32) to accumulate gradient updates. This is largely because many updates are smaller than the discretization gap of low-precision formats: applying them directly to quantized weights can make updates vanish or incur large quantization noise. As result, the model weight memory footprint often stays similar to the high-precision baseline, even when the forward and backward passes are heavily quantized. Even carefully engineered FP8 training systems explicitly retain high-precision Corresponding author: Mahdi Nikdan (nikdanmahdi@gmail.com) 2026 Google Research. ECO: Quantized Training without Full-Precision Master Weights accumulators for stability (Liu et al., 2024a; Peng et al., 2023). The issue is especially pronounced for Sparse Mixture of Experts (SMoE) models, where only subset of parameters is active per token, yet all master weights must reside in memory. More broadly, attempts to avoid high-precision accumulation either do not scale to LLM training (Lin et al., 2022) or have only been effective in narrow settings (Zhang et al., 2025). This leaves clear gap: general method that removes master weights without sacrificing convergence or introducing additional memory overhead. Eliminating master weights can yield memory savings comparable to quantizing optimizer states (e.g., momentum buffers), an approach that has been widely explored and is very popular (Dettmers et al., 2021). In this work, we introduce the Error-Compensating Optimizer (ECO), which enables accurate quantized training without full-precision master weights, and thus zero extra memory overhead. The key idea is the following: after updating each layers parameters, we quantize the updated weights and inject the resulting quantization error into the optimizers momentum buffer. This creates an error-feedback loop that carries forward the lost updates and compensates for them in subsequent steps, allowing updates to be applied directly to quantized parameters. The resulting ECO iteration is simple to implement and requires no extra hyperparameter tuning. It further comes with theoretical guarantees. We study the convergence behavior of ECO applied to the SGD with momentum optimizer with momentum factor ğ›½. Under standard non-convex assumptions and decaying learning rate, we prove that ECO converges to constant-radius neighborhood of 1 the true optimum. Moreover, this radius is only 1 ğ›½2 factor worse than the best achievable bound when using master weights, where nonzero error is unavoidable because the solution must lie on the quantization grid. We further construct quadratic example showing that this bound is tight up constant factor. In the same example, we show that naively removing master weights (without momentum error injection) yields stationary error that scales inversely with the learning rate, and therefore diverges as the learning rate decays to zero. We evaluate ECO with FP8 quantization across scaling law studies on small transformers (30M 800M parameters) (Castro et al., 2025; Panferov et al.), pre-training Gemma-3 1B (Gemma et al., 2025) and an SMoE 2.1B model, and fine-tuning DeepSeek-MoE-16B model (Dai et al., 2024). Across settings, ECO nearly matches the validation loss of baselines that rely on master weights while significantly outperforming naive master weight removal. Furthermore ECO can reduce static memory usage by up to 25%, shifting the Pareto frontier between memory consumption and validation loss, as illustrated in Figure 1. 2. Related Work Quantized/Quantization-Aware Training. Quantization-aware training (QAT) aims to enable low-precision inference by simulating quantization effects on weights and optionally activations during training (Chen et al., 2025; Choi et al., 2018; Esser et al., 2019; Jung et al., 2019; Liu et al., 2024b; Panferov et al.; Wang et al., 2023b; Zhou et al., 2016). Quantized training methods go further by quantizing the backward pass computation to accelerate training (Ashkboos et al., 2025; Castro et al., 2025; Chmiel et al., 2025; Liu et al., 2024a; Tseng et al., 2025). Post-training quantization (PTQ) methods such as Ashkboos et al. (2024); Frantar et al. (2022); Lin et al. (2024); Liu et al. (2024c); Sun et al. (2024) are computationally cheaper, but they typically incur larger accuracy degradation than QAT, especially at very low precision. Despite these advances, most QAT frameworks still rely on high-precision master weights to accumulate updates. Even recent QAT training systems such as FP8-LM (Peng et al., 2023), DeepSeek-V3 (Liu et al., 2024a), and Kimi-K2 (Team et al., 2025), who have rigorously tuned their quantization scheme, explicitly keep high-precision accumulators to 2 ECO: Quantized Training without Full-Precision Master Weights maintain stability. In this context, ECO is complementary to existing QAT and quantized training methods: it targets the remaining dependence on master weights. Efforts Towards Low-Precision Accumulation. Avoiding master weights has proven difficult outside restricted settings. FP8-LM reports that FP8 accumulation fails at large LLM scales (Peng et al., 2023). Lin et al. (2022) show that with careful gradient rescaling, INT8 accumulators can be stable for small convolutional networks that fit within 256KB of memory. APT (Huang et al., 2022) varies accumulator bit-width across layers for edge-device training. Collage (Yu et al., 2024) replaces FP32 with two BF16 accumulators due to hardware constraint. Ozkara et al. (2025) argue that stochastic rounding is important for BF16 accumulation, and ELMO (Zhang et al., 2025) applies stochastic rounding to reduce the accumulator precision of the LLM head layer to BF16/FP8. Overall, there exists no general approach that enables sub-16-bit accumulation for large-scale LLM training, leaving an important gap that ECO addresses. Optimizer State Quantization. related line of work quantizes optimizer states (e.g., first and second moments) rather than model weights. In practice, the first moment is often more tolerant to quantization than the second. FP8-LM (Peng et al., 2023) reports that the first moment can be quantized to FP8 without difficulty. Other approaches quantize both moments to 8-bit (Dettmers et al., 2021; Fishman et al., 2024; Xi et al., 2024a), and Li et al. (2023) pushes this to 4-bit for both buffers. ECO targets different bottleneck: the master-weight copy. This provides memory savings comparable to optimizer-state quantization, while remaining largely unexplored. Error Feedback. Error feedback (EF) methods were developed to mitigate bias from compressed or quantized gradients, particularly in distributed optimization. They accumulate quantization residuals locally and add them back in later steps, preserving the sum of updates over time (RichtÃ¡rik et al., 2021; Seide et al., 2014; Tang et al., 2021; Wang et al., 2023a). RichtÃ¡rik et al. (2021) provides principled EF formulation and shows that it can match full-precision SGD convergence under appropriate assumptions. Directly applying EF to the master weight quantization requires storing an error buffer, which conflicts with memory reduction goals when training at scale. ECO instead reuses the optimizer momentum buffer to store quantization error, achieving error feedback without any extra memory. 3. Method In this section, we start by introducing the notation and covering relevant background. We then describe our main method ECO. Finally, we present our theoretical results which analyze the convergence of ECO. 3.1. Notation and Background Notation. Throughout this section, we denote the model parameters by ğœ½ and their corresponding gradients by g. The optimizers first and second momentum buffers are represented by and v, respectively, with their corresponding coefficients denoted by ğ›½1 and ğ›½2 (or just ğ›½ in case of SGD). We denote the quantization as q(), and represents the quantization error (e.g., eğœ½ = ğœ½ q(ğœ½)), and ğœ‚ is the learning rate. 3 ECO: Quantized Training without Full-Precision Master Weights Quantization. Quantization is the process of mapping continuous or high-precision values to low-precision representation, primarily to reduce memory usage and enhance arithmetic throughput. This process typically involves an affine transformation (scaling by ğ‘  and shifting by ğ‘§) to project the original values into the target range, followed by rounding function that maps each value to the nearest grid point. More formally, high-precision vector is quantized to low-precision vector using the formula ğ‘  ). The original values can then be approximated using Ë†x = ğ‘ y + ğ‘§. Thus, the fully = round( xğ‘§ reconstructed vector Ë†xğ‘§,ğ‘  is calculated as: Ë†xğ‘§,ğ‘  = ğ‘  round( ğ‘§ ğ‘  ) + ğ‘§ (1) Assuming the largest quantized value representable by the quantization format is ğœŒ, then standard choice for the scaling factor is ğ‘  = max x/ğœŒ, which prevents overflow. It is also common to fix the zero-point ğ‘§ = 0, particularly for tensors in LLM training that are often near zero-mean. Therefore, for simplicity, when ğ‘§ and ğ‘  are not explicitly mentioned, we assume this symmetric scheme, i.e., Ë†x = ğ‘(x) = max round( ğœŒ ğœŒx max ). Quantization schemes can be categorized in several ways. One key distinction is their granularity, which defines which parts of an input tensor share the same quantization parameters (i.e., zero-point ğ‘§ and scale ğ‘ ). For example, in row-wise quantization, an independent ğ‘§ and ğ‘  are computed and applied to each row of an input matrix. Other methods exists, such as 1D or 2D group-wise quantization, where blocks or groups of elements within the tensor share quantization parameters (Liu et al., 2024a; Rouhani et al., 2023; Xi et al., 2024b). Another categorization stems from the rounding function. standard choice is round-to-nearest, which deterministically maps each value to its closest grid point. Alternatively, stochastic rounding maps value to one of the two nearest grid points, where the probability of selecting either point is proportional to the distance to the other point. Round-to-nearest minimizes the magnitude of the error, while stochastic rounding results in an unbiased estimator. Quantization-Aware Training with Master Weights. Most quantized LLM training pipelines keep high-precision master weights (typically FP32) as the update accumulator. At each step, the master weights are quantized to obtain low-precision weights used for the forward/backward pass, while gradients and optimizer updates are accumulated in the high-precision copy. This stabilizes training by preserving small updates, but it substantially limits the weight-memory savings of quantization: the full master-weight buffer must remain on memory throughout training. 3.2. ECO The high-level idea of ECO is to inject the quantization error from the current step into the optimizers momentum buffer. This mechanism ensures that the error from the current step is carried over and incorporated into the parameter update of the subsequent step, effectively creating an error feedback loop. Algorithm 1 provides general overview, while Algorithm 2 and Algorithm 3 detail the error injection process for the SGD with Momentum (SGDM) and Adam optimizers, respectively. SGDM. ECO applies SGDM updates directly to the quantized weights. Concretely, at step ğ‘¡ with low-precision parameters Ë†ğœ½ğ‘¡, it forms temporary iterate ğœ½ğ‘¡+1 = Ë†ğœ½ğ‘¡ + uğ‘¡ (where uğ‘¡ is the SGDM update, dominated by momentum), quantizes it to obtain Ë†ğœ½ğ‘¡+1 = q(ğœ½ğ‘¡+1), and defines the quantization error 4 ECO: Quantized Training without Full-Precision Master Weights eğ‘¡+1 := ğœ½ğ‘¡+1 Ë†ğœ½ğ‘¡+1. ECO then injects this error into the momentum buffer so that the update lost due to quantization is carried forward and recovered in later steps. We prove in Appendix that, if the errors are injected into momentum as + 1 ğœ‚ eğ‘¡ 1 ğœ‚ğ›½ eğ‘¡+1, then the resulting optimization trajectory is identical to SGDM with master weights. The difficulty is that this exact rule is not memory-efficient: while eğ‘¡+1 is available on-the-fly from the current quantization, the previous-step residual eğ‘¡ must be stored, which reintroduces persistent buffer. We tackle this issue by heuristic observation: eğ‘¡+1 and eğ‘¡ are typically close. Intuitively, assuming fixed scale parameter, Ë†ğœ½ğ‘¡ is already on-grid, so moving to the next iterate only quantizes the increment uğ‘¡, i.e., q(Ë†ğœ½ğ‘¡ + uğ‘¡) = Ë†ğœ½ğ‘¡ + q(uğ‘¡). Since uğ‘¡ is dominated by momentum, it changes slowly from one step to the next, which in turn makes the induced quantization errors eğ‘¡+1 and eğ‘¡ close. We also validate this empirically in Section 4.2. We therefore substitute eğ‘¡ eğ‘¡+1, yielding the memory-free injection rule + 1 (cid:16) 1 ğœ‚ (cid:17) 1 ğ›½ eğ‘¡+1, which removes the need for either master weights or stored error buffer. See Algorithm 2 for more details. Notably, we use this heuristic only to motivate the injection rule; later in this section, we provide rigorous theoretical analysis of the resulting memory-efficient form. Adam. We treat Adam in the same way as SGDM, except that Adam applies an adaptive, element-wise learning rate. Adams parameter update can be written in the form ğœ½ğ‘¡+1 = ğœ½ğ‘¡ ğœ‚ mğ‘¡+1 1 ğ›½ğ‘¡ 1 vğ‘¡+1 1 ğ›½ğ‘¡ , + ğœ– where mğ‘¡+1 and vğ‘¡+1 are the first and second momentum buffers after incorporating the gradient at step ğ‘¡, and ğœ– prevents division by zero. We identify the element-wise adaptive step size as ğœ¼ğ‘¡ (1 ğ›½ğ‘¡ 1 )( ğœ‚ vğ‘¡+1 1 ğ›½ğ‘¡ 2 . + ğœ–) With this formulation, ECOs injection differs from the SGDM case only by replacing the scalar learning rate with Adams element-wise effective step size. See Algorithm 3. Algorithm 1 Quantized Training Step ğ‘¡ with ECO Require: Quantized parameters Ë†ğœ½ğ‘¡ Require: Optimizer state Ë†sğ‘¡, hyperparameters ğ» Require: Optimizer step function: OPTIM_STEP Require: ECO quantization function: ECO_QUANTIZE 1: ğœ½ğ‘¡+1, sğ‘¡+1 OPTIM_STEP(Ë†ğœ½ğ‘¡, Ë†sğ‘¡, ğ») 2: Ë†ğœ½ğ‘¡+1, Ë†sğ‘¡+1 ECO_QUANTIZE(ğœ½ğ‘¡+1, sğ‘¡+1, ğ») 3: return Ë†ğœ½ğ‘¡+1, Ë†sğ‘¡+1 optimization step on quantized parameters quantization + momentum injection 5 ECO: Quantized Training without Full-Precision Master Weights Algorithm 2 ECO_QUANTIZE for SGD with Momentum Require: High-precision parameters ğœ½ğ‘¡+1 Require: Optimizer state sğ‘¡+1, hyperparameter ğ» 1: Ë†ğœ½ğ‘¡+1 q(ğœ½ğ‘¡+1) 2: eğ‘¡+1 ğœ½ğ‘¡+1 Ë†ğœ½ğ‘¡+1 3: { mğ‘¡+1} sğ‘¡+1 4: {ğœ‚, ğ›½} ğ» 5: Ë†mğ‘¡+1 mğ‘¡+1 + 1 6: return Ë†ğœ½ğ‘¡+1, { Ë†mğ‘¡+1} ğœ‚ (1 1 ğ›½ )eğ‘¡+1 quantize the weights compute the quantization error read momentum buffer from the optimizer state read SGDM hyperparameters inject the quantization error into momentum Algorithm 3 ECO_QUANTIZE for Adam Require: High-precision parameters ğœ½ğ‘¡+1 Require: Optimizer state sğ‘¡+1, hyperparameter ğ» 1: Ë†ğœ½ğ‘¡+1 q(ğœ½ğ‘¡+1) 2: eğ‘¡+1 ğœ½ğ‘¡+1 Ë†ğœ½ğ‘¡+1 3: { mğ‘¡+1, vğ‘¡+1} sğ‘¡+1 4: {ğœ‚, ğ›½1, ğ›½2, ğœ–} ğ» 5: Ë†mğ‘¡+1 mğ‘¡+1 + 1 ğ›½ğ‘¡ 6: return Ë†ğœ½ğ‘¡+1, { Ë†mğ‘¡+1, vğ‘¡+1} vğ‘¡+1 1 ğ›½ğ‘¡ 2 (1 1 ğ›½1 + ğœ–) eğ‘¡+1 )( ğœ‚ 1 quantize the weights compute the quantization error read momentum buffers from the optimizer state read Adam hyperparameters inject the quantization error into momentum 3.3. Convergence Analysis This section presents the convergence analysis for the SGDM variant of the ECO optimizer. By constructing virtual sequence, we prove that the algorithm converges to near stationary point. All proofs are given in Appendix B. 3.3.1. Setup and Algorithm We consider the optimization problem minğœ½â„ğ‘‘ ğ‘“ (ğœ½), where ğ‘“ is ğ¿-smooth and bounded below by ğ‘“ . The ECO Optimizer updates are expanded as follows: mğ‘¡+1 = ğ›½ Ë†mğ‘¡ + (1 ğ›½) ğ‘“ (Ë†ğœ½ğ‘¡) ğœ½ğ‘¡+1 = Ë†ğœ½ğ‘¡ ğœ‚ mğ‘¡+1 Ë†ğœ½ğ‘¡+1 = ğ‘(ğœ½ğ‘¡+1) eğ‘¡+1 = ğœ½ğ‘¡+1 Ë†ğœ½ğ‘¡+1 Ë†mğ‘¡+1 = mğ‘¡+1 + ğ›¼eğ‘¡+1 (2) (3) (4) (5) (6) where ğœ‚ is the learning rate, ğ›½ [0, 1) is the momentum parameter, and the error injection strength is set to: (cid:18) 1 ğœ‚ (cid:19) 1 ğ›½ 1 . (7) ğ›¼ = 3.3.2. Assumptions We rely on the following standard assumptions for non-convex optimization analysis. Assumption 3.1 (L-Smoothness). The function ğ‘“ is ğ¿-smooth, i.e., ğ‘“ (ğ‘¥) ğ‘“ ( ğ‘¦) ğ¿ğ‘¥ ğ‘¦ for all ğ‘¥, ğ‘¦. 6 ECO: Quantized Training without Full-Precision Master Weights Assumption 3.2 (Unbiased Quantization with Bounded Error Variance). The quantization error is zero-mean with bounded variance ğœ2: ğ”¼[eğ‘¡] = 0 and ğ”¼[eğ‘¡ 2] ğœ2. Assumption 3.3 (Bounded Gradient). There exists ğº > 0 such that ğ‘“ (ğœ½) ğº for all ğœ½. 3.3.3. Virtual Sequence Analysis Following the methodology of RichtÃ¡rik et al. (2021), we construct virtual sequence ğœ½ğ‘¡. Definition 3.4 (Virtual Sequence). Define the virtual sequence ğœ½ğ‘¡ as: ğœ½ğ‘¡ := Ë†ğœ½ğ‘¡ ğœ‚ğ›½ 1 ğ›½ Ë†mğ‘¡. Lemma 3.5 (Virtual Sequence Dynamics). The virtual sequence ğœ½ğ‘¡ evolves as: ğœ½ğ‘¡+1 = ğœ½ğ‘¡ ğœ‚ ğ‘“ (Ë†ğœ½ğ‘¡), (8) (9) This lemma demonstrates that by tracking this specific combination of weights and momentum, we can analyze the ECO trajectory as standard gradient descent process on the loss surface. 3.3.4. Descent and Momentum Bounds We derive descent inequality for the virtual sequence and bound the momentum term which accumulates the quantization error. Lemma 3.6 (Descent Lemma). Let ğ¶ = ğœ‚ğ›½ 2ğ¿ , the virtual sequence satisfies: 1 ğ›½ . For ğœ‚ 1 ğ‘“ (ğœ½ğ‘¡+1) ğ‘“ (ğœ½ğ‘¡) ğœ‚ (cid:13) (cid:13) (cid:13) ğ‘“ (Ë†ğœ½ğ‘¡) (cid:13) 2 (cid:13) (cid:13) 2 + ğœ‚ğ¿2ğ¶2 Ë†mğ‘¡ 2 2 (10) This allows us to control the dynamics of the optimization trajectory. Lemma 3.7 (Bounded Momentum). Under the assumptions, the squared norm of the momentum Ë†mğ‘¡ is bounded in expectation by constant ğ‘€2. Specifically, for all ğ‘¡: ğ”¼[ Ë†mğ‘¡ 2] ğ‘€2 := 2ğº2 + 2ğ›¼2ğœ2 1 ğ›½ . (11) This ensures that the quantization error injected into the momentum buffer does not explode, keeping the optimization stable. 3.3.5. Convergence Theorem Theorem 3.8 (Convergence Rate). For ğœ‚ 2ğ¿ , the ECO optimizer converges to neighborhood: min ğ‘¡ {0,...,ğ‘‡ 1} ğ”¼ (cid:2) ğ‘“ (Ë†ğœ½ğ‘¡)2(cid:3) 4( ğ‘“ (ğœ½0) ğ‘“ ) ğœ‚ğ‘‡ + ğœ2 quant , where the quantization noise floor ğœ2 quant is given by: ğœ2 quant = 4ğœ‚2 ğ›½2ğ¿2ğº2 (1 ğ›½)2 + 4ğ¿2ğœ2 1 ğ›½2 (12) (13) 7 ECO: Quantized Training without Full-Precision Master Weights Discussion on Decaying Learning Rate: As ğœ‚ 0, the noise floor ğœ quant becomes: ğœ2 quant = lim ğœ‚0 4ğ¿2ğœ2 1 ğ›½2 (14) While the noise floor persists even as the learning rate vanishes, we show in the next subsection that this noise floor is tight up to the constant 4. Additionally, we note that even with master weights, since the final solution must lie on the quantization grid, noise floor of ğ¿2ğœ2 is unavoidable. 3.3.6. Deterministic Rounding We now provide similar study where deterministic round-to-nearest is used instead of stochastic rounding. In this case, the zero-mean error assumption (Assumption 3.2) is violated. We instead assume bounded deterministic error eğ‘¡ ğ›¿ for all ğ‘¡. Lemma 3.9 (Deterministic Momentum Bound). Under the deterministic error assumption eğ‘¡ ğ›¿ and bounded gradients ğ‘“ (ğœ½) ğº, the norm of the injected momentum buffer in ECO is uniformly bounded for all ğ‘¡: Ë†mğ‘¡ ğ‘€det := ğº + . (15) ğ›¼ğ›¿ 1 ğ›½ Theorem 3.10 (Deterministic Convergence). For ğœ‚ 1 converges to neighborhood of the optimum: 2ğ¿ , the ECO optimizer with deterministic rounding min ğ‘¡<ğ‘‡ ğ‘“ (Ë†ğœ½ğ‘¡)2 4( ğ‘“ (ğœ½0) ğ‘“ ) ğœ‚ğ‘‡ + Î“2 quant where the deterministic noise floor is defined as: quant = 2ğ¿2ğ¶2ğ‘€2 Î“2 det = 2ğ¿2ğœ‚2 ğ›½2 (1 ğ›½) (cid:18) ğº + (cid:19) 2 . ğ›¼ğ›¿ 1 ğ›½ (16) (17) quant) and the deterministic case (Î“2 Comparison of Noise Floors. It is instructive to compare the noise floor of the stochastic case (ğœ2 quant) as the learning rate ğœ‚ 0. In the stochastic case, the noise floor remains constant at (ğ¿2ğœ2/(1 ğ›½2)). In the deterministic case, substituting ğ›¼ = (1 ğ›½)/ğœ‚ğ›½ results in floor of (ğ¿2ğ›¿2/(1 ğ›½)2). Assuming ğœ ğ›¿, the deterministic bound is significantly larger due to the (1 ğ›½) 2 dependence, reflecting the fact that systematic biases in quantization are harder for the momentum buffer to average out than zero-mean noise. 3.4. Lower-Bound on Worst-Case Behavior We analyze the optimization dynamics on one-dimensional quadratic objective ğ‘“ (ğ‘¥) = ğ¿ ğ‘¥2 with 2 ğ¿ > 0. The gradient is ğ‘“ (ğ‘¥) = ğ¿ğ‘¥. We assume stochastic quantization model where the quantized value Ë†ğ‘¥ = ğ‘(ğ‘¥) satisfies Ë†ğ‘¥ = ğ‘¥ + ğœ‰, with ğœ‰ being zero-mean noise independent of ğ‘¥ and ğ”¼[ğœ‰2] = ğœ2. We examine the expected squared gradient norm of the stationary quantized parameters, defined as = limğ‘¡ ğ”¼[( ğ‘“ (Ë†ğ‘¥ğ‘¡))2], in the limit as the learning rate ğœ‚ 0. The results are summarized below, while the formal derivations are deferred to Appendix C. SGDM with Master Weights. In this standard setting, the master weights evolve in high precision, but the gradient is computed using the quantized weights. Master weights allow the underlying 8 ECO: Quantized Training without Full-Precision Master Weights Table 1 Validation loss comparison across model sizes 30-800M, with dvg denoting divergence. N/A: one entry is unavailable due to data loss. Model Size 30M 50M 100M 200M 430M 800M BF16 w/ MW FP8 w/ MW + RTN FP8 w/ MW + SR 3.3238 3.3248 3.3309 3.1616 3.1668 3.1719 2.9811 2.9846 2.9884 2.8157 2.8194 2. 2.6464 2.6490 2.6500 2.5306 2.5343 N/A dvg FP8 w/o MW + RTN FP8 w/o MW + SR 3.4008 FP8 w/o MW ECO + RTN 3.3640 FP8 w/o MW ECO + SR dvg 2.9471 2.6046 3.3317 3.1695 2.9888 2.8241 2.6544 2.5399 dvg 2.9684 2.8776 dvg 2.8378 2. dvg 3.2563 3.1862 dvg 3.1006 3.0025 parameter to converge to the true optimum. However, the quantized weights are ğœ‰ away from the master weights. Consequently, the error is dominated by the quantization resolution: LMW = ğ¿2ğœ2. lim ğœ‚0 (18) Naive Master Weight Removal. When master weights are removed, the update is applied directly to the quantized parameter: Ë†ğ‘¥ğ‘¡+1 = ğ‘(Ë†ğ‘¥ğ‘¡ ğœ‚ğ‘šğ‘¡+1). This process reaches stationary distribution, however, the variance is inversely proportional to the learning rate: LNaive ğœ‚0 . 1 ğœ‚ (19) This confirms that without error compensation, one cannot achieve high accuracy by annealing the learning rate. ECO. ECO stabilizes the master-weight-free training by injecting quantization noise into the momentum buffer. In the limit of small learning rates, the process converges to stationary distribution determined by the noise accumulation in the momentum term: LECO = lim ğœ‚0 ğ¿2ğœ2 1 ğ›½2 . (20) This shows that ECO prevents the 1/ğœ‚ explosion seen in the naive case. Additionally, this verifies that the noise floor in in Equation (14) is tight up to factor of 4. 4. Experiments 4.1. Baselines We evaluate the following baselines that use high-precision accumulation. FP32 accumulation with BF16 computation (BF16 w/ MW): This configuration serves as the reference baseline. Training is performed using FP32 master weights, while operands are cast to BF16 prior to each matrix multiplication to improve efficiency. This setup follows standard automatic mixed-precision training (Micikevicius et al., 2017) and provides an upper bound on achievable performance. ECO: Quantized Training without Full-Precision Master Weights FP32 accumulation with FP8 round-to-nearest forward pass (FP8 w/ MW + RTN): This quantization-aware training (QAT) baseline quantizes both weights and activations to the FP8 E4M3 format during the forward pass using round-to-nearest. Row-wise scaling is applied, with each scale set to the maximum absolute value in the corresponding row. Prior work has shown that this approach is largely lossless (Liu et al., 2024a; Peng et al., 2023). FP32 accumulation with FP8 stochastic rounding forward pass (FP8 w/ MW + SR): This baseline is identical to the previous one, except that weights are quantized using stochastic rounding. Activations remain quantized with round-to-nearest. The baselines above maintain FP32 master weights and therefore establish upper bounds for the following methods, which eliminate master weight storage. FP8 accumulation and forward pass with round-to-nearest (FP8 w/o MW + RTN): This baseline provides direct comparison to ECO. No high-precision master weights are stored. After each parameter update, weights are quantized to FP8 using round-to-nearest. Activations are also quantized to FP8. FP8 accumulation and forward pass with stochastic rounding (FP8 w/o MW + SR): This method mirrors the previous baseline, but applies stochastic rounding to the weights. Activations are still quantized using round-to-nearest. This corresponds to the approach suggested by Ozkara et al. (2025). FP8 accumulation and forward pass with round-to-nearest and ECO (FP8 w/o MW ECO + RTN): In addition to removing master weights and applying round-to-nearest quantization to both weights and activations, this method incorporates our momentum injection mechanism to mitigate quantization error. FP8 accumulation and forward pass with stochastic rounding and ECO (FP8 w/o MW ECO + SR): This variant is identical to the previous method, but uses stochastic rounding for weight quantization. 4.2. Scaling Law Experiments Setting. We evaluate ECO using pre-training scaling study, following Panferov et al.. We train models with sizes of 30M, 50M, 100M, 200M, 430M, and 800M parameters. For model with ğ‘ parameters, training is performed on 100ğ‘ tokens from the C4 dataset (Raffel et al., 2020), corresponding to 5 the Chinchilla-optimal token count (Hoffmann et al., 2022). We use the T5 tokenizer (Kudo and Richardson, 2018; Raffel et al., 2020). Both the batch size and sequence length are fixed to 512. We use the AdamW optimizer with ( ğ›½1, ğ›½2, ğœ–) = (0.9, 0.98, 109). The learning rate is linearly warmed up from 0.01 the peak value to the peak over the first 10% of training, followed by cosine decay to 0.1 the peak. We apply weight decay of 0.1 and gradient clipping with norm of 1.0. Refer to Panferov et al. for more details on the hyperparameters. For quantized runs, we apply the method only to the linear layers within transformer blocks, excluding the embedding and output layers. Results. Table 1 reports the final validation loss achieved by each method. The results show that ECO substantially improves over naive removal of master weights. When stochastic rounding is used, ECO nearly recovers the performance of methods that retain master weights. As expected, the gains are smaller with round-to-nearest quantization, since it introduces bias into the momentum buffer. 10 ECO: Quantized Training without Full-Precision Master Weights Memory and Runtime. In addition, Figure 1 shows that ECO establishes new static memoryloss Pareto frontier, offering significantly lower memory usage for given validation loss. Regarding runtime, the injection is simple element-wise operation and adds negligible overhead. Study on the Similarity of Consecutive Errors. We repeat the 30M experiment with master weights and round-to-nearest (RTN), and measure the similarity between consecutive quantization errors. Specifically, we track the relative norm eğ‘¡+1 2 and the cosine similarity between eğ‘¡ 2 eğ‘¡ and eğ‘¡+1 throughout training. Figure 2 reports both metrics. The relative norm remains close to 1 during training, indicating that eğ‘¡ 2 varies slowly over time, and the cosine similarity stays consistently high, indicating strong alignment between consecutive errors. The observed trend follows the learning-rate schedule: larger learning rates lead to larger differences between consecutive errors, while these differences diminish as the learning rate decays. Figure 2 Similarity of consecutive quantization errors. Left: relative norm eğ‘¡+12 /eğ‘¡ 2. Right: cosine similarity between eğ‘¡ and eğ‘¡+1. 4.3. Gemma 3 1B Pre-training Setting. We pre-train the Gemma 3 1B model (Gemma et al., 2025) from scratch on 40B tokens from the C4 dataset (Raffel et al., 2020). The batch size is 256 and the sequence length is 512. We use the publicly available Gemma 3 tokenizer. Training uses the AdamW optimizer with the same hyperparameters as in the scaling law experiments. The learning rate peaks at 104, with linear warmup from 106 over the first 10% of training, followed by cosine decay to 105. Results. Figure 3 (Left) compares the final validation loss across methods. The results confirm the effectiveness of ECO, particularly when combined with stochastic rounding. 4.4. Mixture of Experts Pre-training Setting. We pre-train sparse mixture-of-experts (SMoE) model with 2.1B total parameters. The model contains 32 experts, of which 4 are activated per token. It consists of 24 transformer layers, each with hidden dimension of 576, an intermediate dimension of 2304, and 9 attention heads. Training uses 100 the number of active parameters in tokens from the LM1B dataset (Chelba et al., 2013). We reuse the T5 tokenizer (Kudo and Richardson, 2018; Raffel et al., 2020). Optimization is performed with AdamW, using weight decay of 0.1, and learning rate that increases linearly from 2 106 to 2 105 over the first 1% of training, followed by cosine decay back to 2 106. The batch size is 256 and the sequence length is 512. For the quantized runs, we only quantize the expert linear layers. Results. Figure 3 (Left) summarizes the final validation loss for each method. Consistent with prior experiments, ECO clearly outperforms naive master weight removal, while incurring only minimal loss compared to approaches that retain master weights. ECO: Quantized Training without Full-Precision Master Weights Figure 3 (Left) Gemma 3 1B and SMoE 2.1B validation loss comparison, and (Right) Smoothed training loss during fine-tuning of DeepSeek-MoE-16B-Base (Dai et al., 2024). Table 2 Fine-tuned DeepSeek-MoE-16B zero-shot benchmarks. We omit naive master weight removal baselines because training diverged in those settings. ECO matches the master-weight baselines, demonstrating lossless accuracy while requiring significantly less memory. Method Base ARC-C ARC-E GSM8K HellaSwag PIQA MMLU 47.53 73.06 16.15 INT4 w/ MW + RTN INT4 w/ MW + SR 48.29 48. INT4 w/o MW ECO + RTN 49.15 48.55 INT4 w/o MW ECO + SR 71.38 71.13 71.59 71.17 16.68 16.15 16.30 16.00 77. 78.76 78.78 78.88 78.84 80.36 37.64 80.69 80.90 81.34 81. 37.87 38.57 38.63 38.41 Discussion on Memory. Due to the SMoE model architecture, the memory required for activation storage is substantially smaller than that required for weights. With activation checkpointing enabled and no gradient accumulation, peak memory usage is dominated by master weights and optimizer states. Reducing master weight precision from FP32 to FP8 therefore lowers peak memory consumption from 12 bytes per parameter to 9, reduction of approximately 25%. 4.5. DeepSeek-MoE-16B Fine-tuning Setting. We apply ECO to tensor-wise INT4 weight-only QAT of DeepSeek-MoE-16B-Base (Dai et al., 2024). The model has 64 experts, with 8 active experts per token (approximately 2.8B parameters), including 2 shared experts. We fine-tune on the OpenAssistant-Guanaco dataset (Dettmers et al., 2023) for 3 epochs with sequence length 2048, using AdamW with micro-batch size 1 and gradient accumulation of 16. The learning rate is linearly warmed up from 2 1010 to 2 105 over the first 3% of training, then annealed to zero with cosine schedule. We apply gradient clipping with threshold 1 and use no weight decay. Results. Figure 3 (Right) compares training loss across methods. Naive master-weight removal diverges under both round-to-nearest (RTN) and stochastic rounding (SR), whereas ECO matches the master-weight baseline in both cases. In addition, Table 2 reports zero-shot accuracy on standard benchmarks, where ECO similarly recovers the performance of the master-weight models. 12 ECO: Quantized Training without Full-Precision Master Weights 5. Conclusion ECO is the first general-purpose, scalable method for quantized LLM training without master weights. It removes high-precision accumulation by forming an error-feedback loop through the optimizers momentum, with no additional memory overhead. Our analysis shows that ECO avoids the instability of naive master-weight removal. Empirically, across dense Transformers and SMoE models, ECO nearly matches high-precision baselines while improving the static-memory versus loss trade-off, showing that it can serve as practical building block for future low-precision training. Limitations. Both theory and experiments indicate that ECO performs best with stochastic rounding (SR). While SR is becoming more common in hardware, some devices only support round-to-nearest (RTN). In that setting, ECO still outperforms naive approaches but can exhibit higher noise floor, consistent with our theory. Moreover, when master weights are available, RTN generally slightly outperforms SR in practice (Castro et al., 2025); in contrast, ECO relies on the unbiasedness of SR for its strongest guarantees. This introduces slight accuracy ceiling relative to the best RTN-based master-weight baselines."
        },
        {
            "title": "References",
            "content": "S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, P. Cameron, M. Jaggi, D. Alistarh, T. Hoefler, and J. Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems, 37:100213100240, 2024. S. Ashkboos, M. Nikdan, S. Tabesh, R. L. Castro, T. Hoefler, and D. Alistarh. Halo: Hadamard-assisted lower-precision optimization for llms. arXiv preprint arXiv:2501.02625, 2025. R. L. Castro, A. Panferov, S. Tabesh, O. Sieberling, J. Chen, M. Nikdan, S. Ashkboos, and D. Alistarh. Quartet: Native fp4 training can be optimal for large language models. arXiv preprint arXiv:2505.14669, 2025. C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013. M. Chen, W. Shao, P. Xu, J. Wang, P. Gao, K. Zhang, and P. Luo. Efficientqat: Efficient quantizationaware training for large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1008110100, 2025. B. Chmiel, M. Fishman, R. Banner, and D. Soudry. Fp4 all the way: Fully quantized training of llms. arXiv preprint arXiv:2505.19115, 2025. J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V. Srinivasan, and K. Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. D. Dai, C. Deng, C. Zhao, R. X. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, Z. Xie, Y. K. Li, P. Huang, F. Luo, C. Ruan, Z. Sui, and W. Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. CoRR, abs/2401.06066, 2024. URL https: //arxiv.org/abs/2401.06066. T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021. 13 ECO: Quantized Training without Full-Precision Master Weights T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. S. K. Esser, J. L. McKinstry, D. Bablani, R. Appuswamy, and D. S. Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. M. Fishman, B. Chmiel, R. Banner, and D. Soudry. Scaling fp8 training to trillion-token llms. arXiv preprint arXiv:2409.12517, 2024. E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. T. Gemma, A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova, A. RamÃ©, M. RiviÃ¨re, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. T. Huang, T. Luo, and J. T. Zhou. Apt: The master-copy-free training method for quantised neural network on edge devices. Journal of Parallel and Distributed Computing, 166:95103, 2022. S. Jung, C. Son, S. Lee, J. Son, J.-J. Han, Y. Kwak, S. J. Hwang, and C. Choi. Learning to quantize deep networks by optimizing quantization intervals with task loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 43504359, 2019. T. Kudo and J. Richardson. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018. B. Li, J. Chen, and J. Zhu. Memory efficient optimizers with 4-bit states. Advances in Neural Information Processing Systems, 36:1513615171, 2023. J. Lin, L. Zhu, W.-M. Chen, W.-C. Wang, C. Gan, and S. Han. On-device training under 256kb memory. Advances in Neural Information Processing Systems, 35:2294122954, 2022. J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024. A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al. Deepseek-v technical report. arXiv preprint arXiv:2412.19437, 2024a. Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra. In Findings of the Llm-qat: Data-free quantization aware training for large language models. Association for Computational Linguistics: ACL 2024, pages 467484, 2024b. Z. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary, R. Krishnamoorthi, V. Chandra, Y. Tian, arXiv preprint Spinquant: Llm quantization with learned rotations. and T. Blankevoort. arXiv:2405.16406, 2024c. P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. K. Ozkara, T. Yu, and Y. Park. Stochastic rounding for llm training: Theory and practice. arXiv preprint arXiv:2502.20566, 2025. 14 ECO: Quantized Training without Full-Precision Master Weights A. Panferov, J. Chen, S. Tabesh, R. L. Castro, M. Nikdan, and D. Alistarh. Quest: Training accurate llms over highly-compressed weights and activation. In Sparsity in LLMs (SLLM): Deep Dive into Mixture of Experts, Quantization, Hardware, and Inference. H. Peng, K. Wu, Y. Wei, G. Zhao, Y. Yang, Z. Liu, Y. Xiong, Z. Yang, B. Ni, J. Hu, et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. P. RichtÃ¡rik, I. Sokolov, and I. Fatkhullin. Ef21: new, simpler, theoretically better, and practically faster error feedback. Advances in Neural Information Processing Systems, 34:43844396, 2021. B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea, E. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Interspeech, volume 2014, pages 10581062. Singapore, 2014. Y. Sun, R. Liu, H. Bai, H. Bao, K. Zhao, Y. Li, J. Hu, X. Yu, L. Hou, C. Yuan, et al. Flatquant: Flatness matters for llm quantization. arXiv preprint arXiv:2410.09426, 2024. H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu, C. Zhang, and Y. He. 1-bit adam: Communication efficient large-scale training with adams convergence speed. In International Conference on Machine Learning, pages 1011810129. PMLR, 2021. K. Team, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. A. Tseng, T. Yu, and Y. Park. Training llms with mxfp4. arXiv preprint arXiv:2502.20586, 2025. G. Wang, H. Qin, S. A. Jacobs, C. Holmes, S. Rajbhandari, O. Ruwase, F. Yan, L. Yang, and Y. He. Zero++: Extremely efficient collective communication for giant model training. arXiv preprint arXiv:2306.10209, 2023a. H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023b. H. Xi, H. Cai, L. Zhu, Y. Lu, K. Keutzer, J. Chen, and S. Han. Coat: Compressing optimizer states and activation for memory-efficient fp8 training. arXiv preprint arXiv:2410.19313, 2024a. H. Xi, Y. Chen, K. Zhao, K. J. Teh, J. Chen, and J. Zhu. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. arXiv preprint arXiv:2403.12422, 2024b. T. Yu, G. Gupta, K. Gopalswamy, A. Mamidala, H. Zhou, J. Huynh, Y. Park, R. Diamant, A. Deoras, and L. Huan. Collage: Light-weight low-precision strategy for llm training. arXiv preprint arXiv:2405.03637, 2024. J. Zhang, N. Ullah, E. Schultheis, and R. Babbar. Elmo: Efficiency via low-precision and peak memory optimization in large output spaces. arXiv preprint arXiv:2510.11168, 2025. S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 15 ECO: Quantized Training without Full-Precision Master Weights A. Exact Error Injection This appendix shows that SGDM with high-precision master weights can be reproduced exactly using only quantized weights, provided the momentum buffer receives an ideal correction that depends on both the current and previous quantization residuals. SGDM with Master Weights. Let q() be the weight quantizer. Let ğœ½ğ‘¡ denote the high-precision MW master weights, and let Ë†ğœ½ ğ‘¡ q(ğœ½ğ‘¡) be the quantized weights used for the forward/backward pass at step ğ‘¡. Using the gradient SGDM with master weights updates gğ‘¡ ğ‘“ (Ë†ğœ½ MW ğ‘¡ ), + (1 ğ›½) gğ‘¡, ğ‘¡ ğ‘¡+1 ğ›½ mMW mMW ğœ½ğ‘¡+1 ğœ½ğ‘¡ ğœ‚ mMW ğ‘¡+1 MW Ë†ğœ½ ğ‘¡+1 q(ğœ½ğ‘¡+1), , and we define the quantization residual of ğœ½ğ‘¡+1 as eMW ğ‘¡+1 := ğœ½ğ‘¡+1 Ë†ğœ½ MW ğ‘¡+1 . (21) (22) (23) (24) (25) No-Master-Weight SGDM with Ideal Momentum Injection. This variant stores only quantized weights Ë†ğœ½ , and the previous residual eIM , momentum buffer mIM . At step ğ‘¡, it computes IM ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ + (1 ğ›½) gğ‘¡, IM ğ‘¡ ğœ‚ mğ‘¡+1, ), IM gğ‘¡ ğ‘“ (Ë†ğœ½ ğ‘¡ mğ‘¡+1 ğ›½ mIM ğœ½ğ‘¡+1 Ë†ğœ½ IM ğ‘¡+1 q(ğœ½ğ‘¡+1), Ë†ğœ½ IM eIM ğ‘¡+1 ğœ½ğ‘¡+1 Ë†ğœ½ ğ‘¡+1 1 ğœ‚ ğ‘¡+1 mğ‘¡+1 + mIM , eIM ğ‘¡ 1 ğœ‚ğ›½ eIM ğ‘¡+1 . (26) (27) (28) (29) (30) Theorem (Exact equivalence). Assume SGDM with master weights starts from (ğœ½0, mMW the injected method by 0 ). Initialize Ë†ğœ½ IM 0 q(ğœ½0), eIM 0 ğœ½0 Ë†ğœ½ IM 0 , mIM 0 mMW 0 1 ğœ‚ğ›½ eIM 0 . (31) Then, for all ğ‘¡ 0, the quantized iterates produced by the injected method satisfy and therefore the two procedures produce identical gradients at every step. IM ğ‘¡ = Ë†ğœ½ MW ğ‘¡ Ë†ğœ½ , 16 ECO: Quantized Training without Full-Precision Master Weights Proof. Define the implicit master weights and momentum corresponding to the injected method by ğœ½ ğ‘¡ := Ë†ğœ½ IM ğ‘¡ + eIM ğ‘¡ , ğ‘¡ := mIM ğ‘¡ + 1 ğœ‚ğ›½ eIM ğ‘¡ . By (31), we have ğœ½ 0 = ğœ½0 and 0 = mMW 0 . From (29), we have Hence, ğœ½ğ‘¡+1 = Ë†ğœ½ IM ğ‘¡+1 + eIM ğ‘¡+1 . ğœ½ ğ‘¡+1 = Ë†ğœ½ IM ğ‘¡+1 + eIM ğ‘¡+1 = ğœ½ğ‘¡+1. Combining with (28), we get Ë†ğœ½ IM ğ‘¡+1 q(ğœ½ ğ‘¡+1). Next, using (30) and (32), ğ‘¡+1 = mIM ğ‘¡+1 + = mğ‘¡+1 + = mğ‘¡+1 + eIM ğ‘¡+ 1 ğœ‚ğ›½ 1 ğœ‚ 1 ğœ‚ eIM ğ‘¡ eIM ğ‘¡ 1 ğœ‚ğ›½ eIM ğ‘¡+1 + 1 ğœ‚ğ›½ eIM ğ‘¡+1 = ğ›½ mIM ğ‘¡ + (1 ğ›½) gğ‘¡ + 1 ğœ‚ eIM ğ‘¡ (cid:16) = ğ›½ mIM (cid:17) eIM ğ‘¡ ğ‘¡ + 1 ğœ‚ğ›½ ğ‘¡ + (1 ğ›½) gğ‘¡. = ğ›½ + (1 ğ›½) gğ‘¡ Thus ğ‘¡+1 follows the same SGDM momentum recurrence as (22). Finally, using (34), (27), and (35), IM ğ‘¡ ğœ‚ mğ‘¡+ ğœ½ ğ‘¡+1 = ğœ½ğ‘¡+1 = Ë†ğœ½ IM ğ‘¡ + eIM = (Ë†ğœ½ ğ‘¡ ğ‘¡ ğœ‚ = ğœ½ ğ‘¡+1 ) ğœ‚(cid:0) mğ‘¡+1 + 1 , (cid:1) ğœ‚ eIM ğ‘¡ (32) (33) (34) (35) (36) (37) which matches the master-weight update (23). Therefore, with identical initial conditions, the implicit ğ‘¡ , variables (ğœ½ ğ‘¡ ) evolve exactly as SGDM with master weights, implying IM ğ‘¡ ) = q(ğœ½ğ‘¡) = Ë†ğœ½ ğ‘¡ = q(ğœ½ MW ğ‘¡ Ë†ğœ½ for all ğ‘¡. B. Convergence Proofs B.1. Proof of Lemma 3.5 Proof. First, substitute mğ‘¡+1 from Eq. (6) into Eq. (3): ğœ½ğ‘¡+1 = Ë†ğœ½ğ‘¡ ğœ‚( Ë†mğ‘¡+1 ğ›¼eğ‘¡+1). (38) 17 ECO: Quantized Training without Full-Precision Master Weights Using eğ‘¡+1 = ğœ½ğ‘¡+1 Ë†ğœ½ğ‘¡+1, we rearrange to solve for Ë†ğœ½ğ‘¡+1: Ë†ğœ½ğ‘¡+1 + eğ‘¡+1 = Ë†ğœ½ğ‘¡ ğœ‚ Ë†mğ‘¡+1 + ğœ‚ğ›¼eğ‘¡+1 Ë†ğœ½ğ‘¡+1 = Ë†ğœ½ğ‘¡ ğœ‚ Ë†mğ‘¡+1 (1 ğœ‚ğ›¼)eğ‘¡+1. Substituting ğ›¼ = ğœ‚ (1 1 ğ›½ ), we have 1 ğœ‚ğ›¼ = 1 ğ›½ . Thus: Ë†ğœ½ğ‘¡+1 = Ë†ğœ½ğ‘¡ ğœ‚ Ë†mğ‘¡+1 1 ğ›½ eğ‘¡+1. Now, examine the update of the virtual sequence ğœ½ğ‘¡+1: ğœ½ğ‘¡+1 = Ë†ğœ½ğ‘¡+1 ğœ‚ğ›½ 1 ğ›½ Ë†mğ‘¡+1. We expand this expression: ğœ½ğ‘¡+1 = Ë†ğœ½ğ‘¡+1 ğœ‚ğ›½ 1 ğ›½ Ë†mğ‘¡+1 = (ğœ½ğ‘¡+1 eğ‘¡+1) ğœ‚ğ›½ 1 ğ›½ ( mğ‘¡+1 + ğ›¼eğ‘¡+1) = (Ë†ğœ½ğ‘¡ ğœ‚ mğ‘¡+1 eğ‘¡+1) mğ‘¡+1 eğ‘¡+1 = Ë†ğœ½ğ‘¡ ğœ‚ (cid:18) 1 + (cid:19) ğ›½ 1 ğ›½ ğœ‚ğ›½ 1 ğ›½ (cid:18) ğœ‚ğ›½ğ›¼ 1 ğ›½ (cid:19) ğœ‚ğ›½ğ›¼ 1 ğ›½ mğ‘¡+1 1 + eğ‘¡+1. 1 + ğœ‚ğ›½ 1 ğ›½ (cid:19) (cid:18) 1 ğ›½ ğœ‚ğ›½ = 1 1 = 0. The error term eğ‘¡+1 vanishes perfectly. We are left with: ğœ½ğ‘¡+1 = Ë†ğœ½ğ‘¡ ğœ‚ 1 ğ›½ mğ‘¡+1. Expanding mğ‘¡+1 = ğ›½ Ë†mğ‘¡ + (1 ğ›½) ğ‘“ (Ë†ğœ½ğ‘¡): ğœ½ğ‘¡+1 = Ë†ğœ½ğ‘¡ ğœ‚ğ›½ 1 ğ›½ Ë†mğ‘¡ ğœ‚ ğ‘“ (Ë†ğœ½ğ‘¡) = ğœ½ğ‘¡ ğœ‚ ğ‘“ (Ë†ğœ½ğ‘¡). B.2. Proof of Lemma 3. Proof. Applying the standard descent lemma to the virtual sequence ğœ½ğ‘¡: ğ‘“ (ğœ½ğ‘¡+1) ğ‘“ (ğœ½ğ‘¡) + ğ‘“ (ğœ½ğ‘¡), ğœ½ğ‘¡+1 ğœ½ğ‘¡ + ğ‘“ (ğœ½ğ‘¡) ğœ‚ ğ‘“ (ğœ½ğ‘¡), ğ‘“ (Ë†ğœ½ğ‘¡) + ğ¿ 2 ğ¿ğœ‚2 ğœ½ğ‘¡+1 ğœ½ğ‘¡ 2 2 (cid:13) (cid:13) (cid:13) ğ‘“ (Ë†ğœ½ğ‘¡) (cid:13) 2 (cid:13) (cid:13) 2 . Using the identity 1 + ğ›½ Using ğ›¼ = ğ›½ ğœ‚ğ›½ = 1 ğ›½ ğœ‚ğ›½ : 1 ğ›½ = 1 1 ğ›½ , the coefficient of mğ‘¡+1 is ğœ‚ 1 ğ›½ . Now check the coefficient of eğ‘¡+1. (39) (40) (41) (42) (43) (44) (45) (46) 18 ECO: Quantized Training without Full-Precision Master Weights Using the identity ğ‘, ğ‘ = 1 2 + 1 2 ğ‘ ğ‘2 2: 2 ğ‘2 ğœ‚ 2 1 2 ğ‘2 ğœ‚ ğ‘“ (ğœ½ğ‘¡) 2 2 ğ‘“ (ğœ½ğ‘¡+1) ğ‘“ (ğœ½ğ‘¡) (1 ğ¿ğœ‚) (cid:13) (cid:13) (cid:13) ğ‘“ (Ë†ğœ½ğ‘¡) (cid:13) (cid:13) (cid:13) 2 + ğœ‚ 2 (cid:13) (cid:13) (cid:13) ğ‘“ (ğœ½ğ‘¡) ğ‘“ (Ë†ğœ½ğ‘¡) (cid:13) 2 (cid:13) (cid:13) 2 . (47) 2 The term with ğ‘“ (ğœ½ğ‘¡)2 ğ¿-smoothness on the last term: 2 is non-positive. Additionally, as ğœ‚ 2ğ¿ , we have 1 ğ¿ğœ‚ 1 2 . Using ğ‘“ (ğœ½ğ‘¡+1) ğ‘“ (ğœ½ğ‘¡) ğœ‚ 4 (cid:13) (cid:13) (cid:13) ğ‘“ (Ë†ğœ½ğ‘¡) (cid:13) 2 (cid:13) (cid:13) 2 + ğœ‚ğ¿2 2 (cid:13) (cid:13)ğœ½ğ‘¡ Ë†ğœ½ğ‘¡ (cid:13) (cid:13) (cid:13) (cid:13) 2 2 . The difference between the virtual and actual (quantized) parameters is: ğœ½ğ‘¡ Ë†ğœ½ğ‘¡ = ğœ‚ğ›½ 1 ğ›½ Ë†mğ‘¡. Substituting ğ¶ = ğœ‚ğ›½ 1 ğ›½ yields (cid:13) (cid:13)ğœ½ğ‘¡ Ë†ğœ½ğ‘¡ (cid:13) (cid:13) 2 (cid:13) (cid:13) = ğ¶2 Ë†mğ‘¡ 2 2. Substituting into the inequality gives: ğ‘“ (ğœ½ğ‘¡+1) ğ‘“ (ğœ½ğ‘¡) ğœ‚ 4 (cid:13) (cid:13) (cid:13) ğ‘“ (Ë†ğœ½ğ‘¡) (cid:13) 2 (cid:13) (cid:13) 2 + ğœ‚ğ¿2ğ¶2 2 Ë†mğ‘¡ 2 . (48) (49) (50) B.3. Proof of Lemma 3. Proof. We expand the recursion for Ë†mğ‘¡ starting from Ë†m0 = 0. With the updated update rule mğ‘¡+1 = ğ›½ Ë†mğ‘¡ + (1 ğ›½) ğ‘“ (Ë†ğœ½ğ‘¡), the expansion becomes: Ë†mğ‘¡ = ğ›½ğ‘¡ğ‘˜ (cid:16) ğ‘¡ ğ‘˜=1 (1 ğ›½) ğ‘“ (Ë†ğœ½ğ‘˜1) + ğ›¼eğ‘˜ (cid:17) . We define two components, the gradient accumulation ğ‘†1 and the error accumulation ğ‘†2: ğ‘†1 = (1 ğ›½) ğ‘¡ ğ‘˜= ğ›½ğ‘¡ğ‘˜ ğ‘“ (Ë†ğœ½ğ‘˜1), ğ‘†2 = ğ‘¡ ğ‘˜=1 ğ›½ğ‘¡ğ‘˜ğ›¼eğ‘˜. (51) (52) Using the inequality ğ‘ + ğ‘2 2ğ‘2 + 2ğ‘2, we have ğ”¼[ Ë†mğ‘¡ 2] 2ğ”¼[ğ‘†12] + 2ğ”¼[ğ‘†22]. For the gradient term ğ‘†1, we use the deterministic triangle inequality bound. The (1 ğ›½) factor scales the sum: ğ‘†1 (1 ğ›½) ğ‘¡ ğ‘˜=1 ğ›½ğ‘¡ğ‘˜ ğ‘“ (Ë†ğœ½ğ‘˜1) ğº(1 ğ›½) ğ›½ ğ‘—. ğ‘¡1 ğ‘—=0 Using the geometric series sum bound (cid:205)ğ‘¡1 ğ‘—= ğ›½ ğ‘— 1 1 ğ›½ , the terms cancel nicely: ğ‘†1 ğº(1 ğ›½) 1 1 ğ›½ = ğº. Thus ğ”¼[ğ‘†12] ğº2. (53) (54) 19 ECO: Quantized Training without Full-Precision Master Weights For the error term ğ‘†2, utilizing the unbiasedness assumption where ğ”¼[eğ‘˜e ğ‘—] = 0 for ğ‘˜ > ğ‘—: ğ”¼[ğ‘†22] = ğ”¼ (cid:13) ğ‘¡ (cid:13) (cid:13) (cid:13) (cid:13) ğ‘¡ ğ‘˜=1 ğ›½ğ‘¡ğ‘˜ğ›¼eğ‘˜ (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) = = ğ›½2(ğ‘¡ğ‘˜) ğ›¼2ğ”¼[eğ‘˜ 2] + ğ‘˜=1 ğ‘¡ ğ‘˜=1 ğ›½2(ğ‘¡ğ‘˜) ğ›¼2ğ”¼[eğ‘˜ 2]. ğ‘—ğ‘˜ Cross Terms Using ğ”¼[eğ‘˜ 2] ğœ2, we bound the sum by the infinite geometric series with ratio ğ›½2: ğ”¼[ğ‘†22] ğ›¼2ğœ2 ğ‘—=0 ( ğ›½2) ğ‘— = ğ›¼2ğœ2 1 ğ›½2 . ğ”¼[ Ë†mğ‘¡ 2] 2ğº2 + 2ğ›¼2ğœ2 1 ğ›½2 . Combining these results: B.4. Proof of Theorem 3.8 (55) (56) (57) Proof. We take expectations from both sides of the descent Lemma 3.6 and substitute the momentum bound ğ‘€2 (Lemma 3.7). ğ”¼ [ ğ‘“ (ğœ½ğ‘¡+1)] ğ”¼ [ ğ‘“ (ğœ½ğ‘¡)] ğ”¼ (cid:2) ğ‘“ ( Ë†ğœ½ğ‘¡)2(cid:3) + ğœ‚ 4 ğœ‚ğ¿2ğ¶2 ğ‘€2. Rearranging to isolate the gradient norm: ğœ‚ 4 ğ”¼ (cid:2) ğ‘“ ( Ë†ğœ½ğ‘¡)2(cid:3) ğ”¼ [ ğ‘“ (ğœ½ğ‘¡) ğ‘“ (ğœ½ğ‘¡+1)] + ğœ‚ğ¿2ğ¶ 2 ğ‘€2. Summing from ğ‘¡ = 0 to ğ‘‡ 1: ğœ‚ 4 ğ‘‡ 1 ğ‘¡=0 ğ”¼ (cid:2) ğ‘“ ( Ë†ğœ½ğ‘¡)2(cid:3) ğ”¼ [ ğ‘“ (ğœ½0) ğ‘“ (ğœ½ğ‘‡ )] + ğ‘‡ 1 ğ‘¡=0 ğœ‚ğ¿2ğ¶2 ğ‘€2 ğ‘“ (ğœ½0) ğ‘“ + ğ‘‡ ğœ‚ğ¿2ğ¶2 2 ğ‘€2. Dividing by ğ‘‡ğœ‚/4: 1 ğ‘‡ ğ‘‡ 1 ğ‘¡=0 ğ”¼ (cid:2) ğ‘“ ( Ë†ğœ½ğ‘¡)2(cid:3) 4( ğ‘“ (ğœ½0) ğ‘“ ) ğœ‚ğ‘‡ + 2ğ¿2ğ¶2ğ‘€2. Defining ğœ2 quant = 2ğ¿2ğ¶2ğ‘€2 yields the final result. (58) (59) (60) (61) 20 ECO: Quantized Training without Full-Precision Master Weights B.5. Proof of Lemma 3.9 Proof. Let eğ‘¡ ğ›¿ (absolute error bound). Ë†mğ‘¡+1 ğ›½ Ë†mğ‘¡ + (1 ğ›½) ğ‘“ (Ë†ğœ½ğ‘¡) + ğ›¼eğ‘¡+1. (62) Using the bounded gradient assumption ğ‘“ (ğœ½) ğº: Ë†mğ‘¡+1 ğ›½ Ë†mğ‘¡ + (1 ğ›½)ğº + ğ›¼ğ›¿. This is linear recurrence of the form ğ‘¥ğ‘¡+1 ğ›½ğ‘¥ğ‘¡ + ğ¾. Assuming Ë†m0 = 0, the sequence is bounded by the sum of the geometric series: (63) Ë†mğ‘¡ ğ‘¡ ğ‘–=0 ğ›½ğ‘– ((1 ğ›½)ğº + ğ›¼ğ›¿) ğº + ğ›¼ğ›¿ 1 ğ›½ := ğ‘€. B.6. Proof of Theorem 3.10 Proof. We use Lemmas 3.6 and 3.9. Summing the descent inequality from ğ‘¡ = 0 to ğ‘‡ 1: ğ‘“ (ğœ½ğ‘‡ ) ğ‘“ (ğœ½0) ğœ‚ 4 ğ‘‡ 1 ğ‘¡= (cid:13) (cid:13) (cid:13) ğ‘“ ( Ë†ğœ½ğ‘¡) (cid:13) (cid:13) (cid:13) 2 2 + ğœ‚ğ‘‡ ğ¿2ğ¶2 2 ğ‘€2 det . Rearranging and using ğ‘“ ğ‘“ (ğœ½ğ‘‡ ): 1 ğ‘‡ ğ‘‡ 1 ğ‘¡=0 (cid:13) (cid:13) (cid:13) ğ‘“ ( Ë†ğœ½ğ‘¡) (cid:13) 2 (cid:13) (cid:13) 4( ğ‘“ (ğœ½0) ğ‘“ ) ğœ‚ğ‘‡ + 2ğ¿2ğ¶2ğ‘€2 det . Defining Î“2 quant = 2ğ¿2ğ¶2ğ‘€ det completes the proof. C. Formal Analysis of the Worst-Case Lower-Bounds This appendix provides proofs for the claims made in Section 3.4. All three regimes considered below can be written in the linear form ğ‘¥ğ‘¡+1 = ğ‘ğ‘¥ğ‘¡ + ğ‘ğ‘šğ‘¡ + ğµ1ğœ‰ğ‘¡+1, ğ‘šğ‘¡+1 = ğ‘ğ‘¥ğ‘¡ + ğ‘‘ğ‘šğ‘¡ + ğµ2ğœ‰ğ‘¡+1, for constants ğ‘, ğ‘, ğ‘, ğ‘‘, ğµ1, ğµ2 that depend on the regime. Define the second moments ğ‘¢ğ‘¡ = ğ”¼[ğ‘¥2 ğ‘¡ ], ğ‘£ğ‘¡ = ğ”¼[ğ‘¥ğ‘¡ğ‘šğ‘¡], ğ‘¤ğ‘¡ = ğ”¼[ğ‘š2 ğ‘¡ ]. Lemma C.1 (Second-moment update equations). The dynamics (67) imply ğ‘¢ğ‘¡+1 = ğ‘2ğ‘¢ğ‘¡ + 2ğ‘ğ‘ ğ‘£ğ‘¡ + ğ‘2ğ‘¤ğ‘¡ + ğµ2 1 ğ‘£ğ‘¡+1 = ğ‘ğ‘ ğ‘¢ğ‘¡ + (ğ‘ğ‘‘ + ğ‘ğ‘) ğ‘£ğ‘¡ + ğ‘ğ‘‘ ğ‘¤ğ‘¡ + ğµ1 ğµ2ğœ2, ğ‘¤ğ‘¡+1 = ğ‘2ğ‘¢ğ‘¡ + 2ğ‘ğ‘‘ ğ‘£ğ‘¡ + ğ‘‘2ğ‘¤ğ‘¡ + ğµ2 2 ğœ2. ğœ2, Proof. Expand each square/product and remove all cross terms. For example, for ğ‘¢ğ‘¡+1: ğ‘¢ğ‘¡+1 = ğ”¼[(ğ‘ğ‘¥ğ‘¡ + ğ‘ğ‘šğ‘¡ + ğµ1ğœ‰ğ‘¡+1)2] = ğ‘2ğ‘¢ğ‘¡ + 2ğ‘ğ‘ğ‘£ğ‘¡ + ğ‘2ğ‘¤ğ‘¡ + ğµ2 1ğ”¼[ğœ‰ ğ‘¡+1], since ğ”¼[ğ‘¥ğ‘¡ğœ‰ğ‘¡+1] = ğ”¼[ğ‘šğ‘¡ğœ‰ğ‘¡+1] = 0 and ğ”¼[ğœ‰2 ğ‘¡+1 ] = ğœ2. The proofs for ğ‘£ğ‘¡+1 and ğ‘¤ğ‘¡+1 are identical. (64) (65) (66) (67) (68) (69) (70) (71) 21 ECO: Quantized Training without Full-Precision Master Weights Stability. Let ğ´ = (cid:19) (cid:18)ğ‘ ğ‘ ğ‘‘ ğ‘ denote the deterministic part of (67). sufficient and standard condition for existence of unique stationary second moment is ğœŒ( ğ´) < 1, where ğœŒ( ğ´) indicates ğ´s largest absolute eigenvalue. For the SGDM parameters used below, this holds whenever 0 < ğœ‚ < 2(1 + ğ›½) (1 ğ›½)ğ¿ . (72) All stationary calculations below assume (72), which also guarantees that the denominators appearing in the closed forms are strictly positive. C.1. Fundamental limits on ğ‘“ (ğ‘¥) = ğ¿ 2 ğ‘¥2 We now analyze the stationary squared gradient of the quantized parameter used by the model. For any regime, define the (steady-state) metric := lim ğ‘¡ ğ”¼[ğ‘”(Ë†ğ‘¥ğ‘¡)2] = ğ¿2 lim ğ‘¡ ğ”¼[Ë†ğ‘¥2 ğ‘¡ ], (73) where Ë†ğ‘¥ğ‘¡ is the parameter seen by the forward/backward pass (quantized weights). C.1.1. SGDM with master weights Algorithm. We store full-precision master weight ğ‘¥ğ‘¡. Each step quantizes it for the gradient: then performs SGDM using Ë†ğ‘¥ğ‘¡: Ë†ğ‘¥ğ‘¡ = ğ‘(ğ‘¥ğ‘¡) = ğ‘¥ğ‘¡ + ğœ‰ğ‘¡, ğ‘šğ‘¡+1 = ğ›½ğ‘šğ‘¡ + (1 ğ›½)ğ¿Ë†ğ‘¥ğ‘¡, ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ ğœ‚ğ‘šğ‘¡+1. Linear form. Let ğ‘ := (1 ğ›½)ğ¿, and define Using Ë†ğ‘¥ğ‘¡ = ğ‘¥ğ‘¡ + ğœ‰ğ‘¡, we obtain ğ‘ := 1 ğœ‚ğ‘, ğ‘ := ğœ‚ğ›½, ğ‘‘ := ğ›½. ğ‘¥ğ‘¡+1 = ğ‘ğ‘¥ğ‘¡ + ğ‘ğ‘šğ‘¡ + (ğœ‚ğ‘) ğœ‰ğ‘¡, ğ‘šğ‘¡+1 = ğ‘ğ‘¥ğ‘¡ + ğ‘‘ğ‘šğ‘¡ + ğ‘ ğœ‰ğ‘¡. This matches (67) with (ğµ1, ğµ2) = (ğœ‚ğ‘, ğ‘). (74) (75) (76) (77) Stationary second moments. Let (ğ‘¢, ğ‘£, ğ‘¤) denote the stationary solution of (69)(71). Plugging ğµ1 = ğœ‚ğ‘ and ğµ2 = ğ‘ into Lemma C.1 and setting (ğ‘¢ğ‘¡+1, ğ‘£ğ‘¡+1, ğ‘¤ğ‘¡+1) = (ğ‘¢, ğ‘£, ğ‘¤) yields the linear system ğ‘¢ = ğ‘2ğ‘¢ + 2ğ‘ğ‘ğ‘£ + ğ‘2ğ‘¤ + ğœ‚2ğ‘2ğœ2, ğ‘£ = ğ‘ğ‘ ğ‘¢ + (ğ‘ğ‘‘ + ğ‘ğ‘) ğ‘£ + ğ‘ğ‘‘ ğ‘¤ ğœ‚ğ‘2ğœ2, ğ‘¤ = ğ‘2ğ‘¢ + 2ğ‘ğ‘‘ ğ‘£ + ğ‘‘2ğ‘¤ + ğ‘2ğœ2. We solve it by elimination. From (80) and ğ‘‘ = ğ›½, (1 ğ›½2)ğ‘¤ = ğ‘2(ğ‘¢ + ğœ2) + 2ğ‘ğ›½ğ‘£ = ğ‘¤ = ğ‘2(ğ‘¢ + ğœ2) + 2ğ‘ğ›½ğ‘£ 1 ğ›½ . (78) (79) (80) (81) ECO: Quantized Training without Full-Precision Master Weights Substitute (81) into (79). Using ğ‘ğ‘‘ + ğ‘ğ‘ = ğ›½(1 ğœ‚ğ‘) + (ğœ‚ğ›½)ğ‘ = ğ›½ 2ğœ‚ğ›½ğ‘ and ğ‘ğ‘‘ = ğ‘ğ›½ = ğœ‚ğ›½2, we rewrite (79) as ğ‘£ = ğ‘ğ‘ ğ‘¢ + ( ğ›½ 2ğœ‚ğ›½ğ‘) ğ‘£ ğœ‚ğ›½2ğ‘¤ ğœ‚ğ‘2ğœ2. (82) Move the ğ‘£ and ğ‘¤ terms to the left and substitute ğ‘¤ from (81). This yields single linear equation in ğ‘£ and ğ‘¢, which solves to Plugging (83) back into (81) gives ğ‘£ = ğ¿2ğœ‚ğœ2( ğ›½ 1) 2(1 + ğ›½) ğ¿ğœ‚(1 ğ›½) . ğ‘¤ = 2ğ¿2ğœ2(1 ğ›½) 2(1 + ğ›½) ğ¿ğœ‚(1 ğ›½) . Finally, substitute (83) and (84) into (78). Solving for ğ‘¢ yields ğ‘¢ = ğ”¼[ğ‘¥2] = ğ¿ğœ‚ğœ2(1 + ğ›½) 2(1 + ğ›½) ğ¿ğœ‚(1 ğ›½) . Limit of the squared gradient. The model uses Ë†ğ‘¥ = ğ‘¥ + ğœ‰ with ğ”¼[ğ‘¥ğœ‰] = 0. Hence Therefore the stationary squared gradient satisfies ğ”¼[Ë†ğ‘¥2] = ğ”¼[ğ‘¥2] + ğ”¼[ğœ‰2] = ğ‘¢ + ğœ2. Taking ğœ‚ 0 in (85) gives ğ‘¢ 0, so LMW = ğ¿2(ğ‘¢ + ğœ2). LMW = ğ¿2ğœ2. lim ğœ‚0 C.1.2. Naive master-weight removal Algorithm. We store only quantized weights Ë†ğ‘¥ğ‘¡. Each step: (83) (84) (85) (86) (87) (88) ğ‘šğ‘¡+1 = ğ›½ğ‘šğ‘¡ + (1 ğ›½)ğ¿Ë†ğ‘¥ğ‘¡, ğ‘¥ğ‘¡+1 = Ë†ğ‘¥ğ‘¡ ğœ‚ğ‘šğ‘¡+1, Ë†ğ‘¥ğ‘¡+1 = ğ‘(ğ‘¥ğ‘¡+1) = ğ‘¥ğ‘¡+1 + ğœ‰ğ‘¡+1. (89) Linear form. With ğ‘ = (1 ğ›½)ğ¿ and the same ğ‘, ğ‘, ğ‘‘ as above, Ë†ğ‘¥ğ‘¡+1 = ğ‘Ë†ğ‘¥ğ‘¡ + ğ‘ğ‘šğ‘¡ + 1 ğœ‰ğ‘¡+1, ğ‘šğ‘¡+1 = ğ‘Ë†ğ‘¥ğ‘¡ + ğ‘‘ğ‘šğ‘¡. (90) This matches (67) with (ğµ1, ğµ2) = (1, 0) and state ğ‘¥ğ‘¡ Ë†ğ‘¥ğ‘¡. Stationary second moments. Let (ğ‘¢, ğ‘£, ğ‘¤) denote the stationary solution for ğ‘¢ = ğ”¼[Ë†ğ‘¥2]. Plugging (ğµ1, ğµ2) = (1, 0) into Lemma C.1 and setting stationarity yields ğ‘¢ = ğ‘2ğ‘¢ + 2ğ‘ğ‘ğ‘£ + ğ‘2ğ‘¤ + ğœ2, ğ‘£ = ğ‘ğ‘ ğ‘¢ + (ğ‘ğ‘‘ + ğ‘ğ‘) ğ‘£ + ğ‘ğ‘‘ ğ‘¤, ğ‘¤ = ğ‘2ğ‘¢ + 2ğ‘ğ‘‘ ğ‘£ + ğ‘‘2ğ‘¤. (91) (92) (93) 23 ECO: Quantized Training without Full-Precision Master Weights From (93) and ğ‘‘ = ğ›½, (1 ğ›½2)ğ‘¤ = ğ‘2ğ‘¢ + 2ğ‘ğ›½ğ‘£ = ğ‘¤ = ğ‘2ğ‘¢ + 2ğ‘ğ›½ğ‘£ 1 ğ›½2 . (94) Substitute (94) into (92); as above, ğ‘ğ‘‘ + ğ‘ğ‘ = ğ›½ 2ğœ‚ğ›½ğ‘ and ğ‘ğ‘‘ = ğœ‚ğ›½2. This yields one linear equation in (ğ‘¢, ğ‘£), which solves to ğ‘£ = ğœ2(ğ¿ğœ‚ ğ›½ 1) ğœ‚(cid:0)2(1 + ğ›½) ğ¿ğœ‚(1 ğ›½)(cid:1) . (95) Plugging (95) into (94) gives ğ‘¤; substituting (ğ‘£, ğ‘¤) into (91) and solving for ğ‘¢ yields the closed form ğ‘¢ = ğ”¼[Ë†ğ‘¥2] = ğœ2 (1 ğ›½2) + 2ğ›½ğ¿ğœ‚ ğ¿ğœ‚(cid:0)2(1 ğ›½2) ğ¿ğœ‚(1 ğ›½)2(cid:1) . (96) Divergence as ğœ‚ 0. From (96), as ğœ‚ 0 the denominator is 2ğ¿ğœ‚(1 ğ›½2) + ğ‘œ(ğœ‚) while the numerator is (1 ğ›½2) + ğ‘œ(1), hence Therefore ğ”¼[Ë†ğ‘¥2] = ğœ2 2ğ¿ğœ‚ + ğ‘‚(1), ğœ‚ 0. LNaive = ğ¿2ğ”¼[Ë†ğ‘¥2] ğ¿ğœ2 2ğœ‚ ğœ‚0 . (97) (98) C.1.3. ECO: momentum injection eliminates the 1/ğœ‚ blow-up Algorithm. ECO uses the same SGDM step as the naive method to compute (ğ‘¥ğ‘¡+1, ğ‘šğ‘¡+1) from (Ë†ğ‘¥ğ‘¡, Ë†ğ‘šğ‘¡), then quantizes and injects the quantization error into momentum. Concretely: ğ‘šğ‘¡+1 = ğ›½ Ë†ğ‘šğ‘¡ + (1 ğ›½)ğ¿Ë†ğ‘¥ğ‘¡, ğ‘¥ğ‘¡+1 = Ë†ğ‘¥ğ‘¡ ğœ‚ ğ‘šğ‘¡+1, Ë†ğ‘¥ğ‘¡+1 = ğ‘(ğ‘¥ğ‘¡+1) = ğ‘¥ğ‘¡+1 + ğœ‰ğ‘¡+1. (99) Define the (post-quantization) error ğ‘’ğ‘¡+1 := ğ‘¥ğ‘¡+1 Ë†ğ‘¥ğ‘¡+1 = ğœ‰ğ‘¡+1. ECO then sets Ë†ğ‘šğ‘¡+1 = ğ‘šğ‘¡+1 + ğ›¼ğ‘’ğ‘¡+1 = ğ‘šğ‘¡+1 ğ›¼ğœ‰ğ‘¡+1, ğ›¼ = (cid:16) 1 ğœ‚ 1 (cid:17) 1 ğ›½ = ğ›½ 1 ğœ‚ğ›½ . Since ğ›½ (0, 1), ğ›¼ < 0. Define the positive injection gain ğ›¾ := ğ›¼ = 1 ğ›½ ğœ‚ğ›½ > 0, so that Ë†ğ‘šğ‘¡+1 = ğ‘šğ‘¡+1 + ğ›¾ğœ‰ğ‘¡+1. Linear form. With ğ‘ = (1 ğ›½)ğ¿ and the same ğ‘, ğ‘, ğ‘‘ as above, ECO becomes Ë†ğ‘¥ğ‘¡+1 = ğ‘Ë†ğ‘¥ğ‘¡ + ğ‘ Ë†ğ‘šğ‘¡ + 1 ğœ‰ğ‘¡+1, Ë†ğ‘šğ‘¡+1 = ğ‘Ë†ğ‘¥ğ‘¡ + ğ‘‘ Ë†ğ‘šğ‘¡ + ğ›¾ ğœ‰ğ‘¡+1, i.e., (67) with (ğµ1, ğµ2) = (1, ğ›¾). (100) (101) (102) 24 ECO: Quantized Training without Full-Precision Master Weights Stationary second moments. Applying Lemma C.1 to (102) and setting stationarity yields ğ‘¢ = ğ‘2ğ‘¢ + 2ğ‘ğ‘ğ‘£ + ğ‘2ğ‘¤ + ğœ2, ğ‘£ = ğ‘ğ‘ ğ‘¢ + (ğ‘ğ‘‘ + ğ‘ğ‘) ğ‘£ + ğ‘ğ‘‘ ğ‘¤ + ğ›¾ğœ2, ğ‘¤ = ğ‘2ğ‘¢ + 2ğ‘ğ‘‘ ğ‘£ + ğ‘‘2ğ‘¤ + ğ›¾2ğœ2. (103) (104) (105) We again eliminate ğ‘¤ using (105) (same algebra as before) and then eliminate ğ‘£ using (104). The resulting expressions simplify dramatically because ğ›¾ is coupled to (ğœ‚, ğ›½) by (101). Solving (103) (105) yields the closed form ğ‘¢ = ğ”¼[Ë†ğ‘¥2] = 2ğœ2 2(1 ğ›½2) ğ¿ğœ‚(1 ğ›½)2 . Finite noise floor as ğœ‚ 0. Taking ğœ‚ 0 in (106) gives ğ”¼[Ë†ğ‘¥2] = lim ğœ‚0 ğœ2 1 ğ›½2 , and therefore the stationary squared gradient satisfies lim ğœ‚ LECO = lim ğœ‚0 ğ¿2ğ”¼[Ë†ğ‘¥2] = ğ¿2ğœ2 1 ğ›½2 . (106) (107) (108) Interpretation. Comparing (98) and (108), naive master-weight removal yields stationary error that blows up like 1/ğœ‚, while ECO stabilizes the dynamics and yields finite noise floor controlled by the geometric factor 1/(1 ğ›½2)."
        }
    ],
    "affiliations": [
        "Google Research",
        "ISTA"
    ]
}