{
    "paper_title": "FARE: Fast-Slow Agentic Robotic Exploration",
    "authors": [
        "Shuhao Liao",
        "Xuxin Lv",
        "Jeric Lew",
        "Shizhe Zhang",
        "Jingsong Liang",
        "Peizhuo Li",
        "Yuhong Cao",
        "Wenjun Wu",
        "Guillaume Sartoretti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale $200m\\times130m$ building environment."
        },
        {
            "title": "Start",
            "content": "FARE: Fast-Slow Agentic Robotic Exploration Shuhao Liao1,2, Xuxin Lv1, Jeric Lew2, Shizhe Zhang2, Jingsong Liang2, Peizhuo Li2, Yuhong Cao2, Wenjun Wu1, Guillaume Sartoretti2 6 2 0 2 1 2 ] . [ 1 1 8 6 4 1 . 1 0 6 2 : r Fig. 1: Overall framework of FARE, where slow-thinking LLM module performs global reasoning, and fast-thinking RL module executes locally grounded, sensor-driven autonomous exploration under this global guidance. Abstract This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, hierarchical autonomous exploration framework that integrates large language model (LLM) for global reasoning with reinforcement learning (RL) policy for local decision making. FARE follows fast-slow thinking paradigm. The slow-thinking LLM module interprets concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into sequence of global waypoints through topological graph. To further improve reasoning efficiency, this module employs modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy 1Beihang University, China 2Department of Mechanical Engineering, National University of Singapore, Singapore FARE on hardware and validate it in complex, 200m 130m building environment. large scale I. INTRODUCTION Autonomous exploration is core capability for mobile robots operating in unknown environments, where the objective is to efficiently acquire geometric information. The robot acquires environmental data through sensors such as 3D LiDARs or cameras and incrementally builds representation of the environment, commonly encoded as an occupancy grid or voxel map. With the reliability of modern LiDAR odometry and SLAM (Simultaneous Localization and Mapping), recent work has increasingly focused on the exploration problem itself, without concerns about mapping or localization accuracy [1][7]. Yet, despite substantial progress, efficient exploration in complex real-world environments remains challenging. Existing planners struggle to leverage long-term structural information embedded in the evolving map and often rely on fixed heuristics or rigid decision rules that limit their ability to adapt exploration strategies to different environmental characteristics. Existing autonomous exploration approaches are largely divided into conventional methods and learning based methods. Conventional planners maintain geometric belief map and plan motions that trade off investigating new regions and further refining partially explored areas [1], [3], [8], [9]. To improve scalability in large and complex environments, recent conventional planners increasingly rely on hierarchical planning, which decouples exploration into low-resolution global path computed over coarse belief and high-resolution local path computed over the nearby detailed map [2], [5], [6], [10]. The key insight there is that fine reasoning is most effective locally, while coarse inference suffices for guiding long-term exploration. By contrast, learning-based methods formulate exploration as partially observable Markov decision process and use RL to learn policies that map observations to next waypoint [11] [14]. These methods improve exploration efficiency through reward designs that encourage informative observations and reduce redundant motions, often supported by neural architectures that capture spatial correlations in the evolving map. Despite these advances, existing methods still struggle to exploit long-term information and to adapt their exploration strategy in response to evolving environments. Conventional planners rely on fixed hyperparameters that determine how they balance path length with information gain or how they choose the resolution of the global representation. These parameters typically remain constant throughout deployment, which prevents the planner from adjusting its behavior in different environments. As result, the planner may act too conservatively in open spaces or spend unnecessary effort refining cluttered areas. Even hierarchical planners face similar constraints. Their performance depends on predefined coarse-to-fine decomposition of the map, which information can influence restricts how global structural local decisions. Learning-based planners encounter different but equally important limitation. The objective in autonomous exploration is to minimize the overall time or distance required to complete coverage, yet this objective only becomes observable at the end of an episode. The reward signal therefore becomes extremely sparse, making it difficult for the policy to learn behaviors that depend on long-term consequences. Existing methods address this issue by introducing dense rewards that provide immediate feedback proportional to newly observed information [11], [12], [15], [16]. Although this improves training stability, it also biases the optimization process toward short-term, immediately observable information gain. As result, longhorizon credit assignment becomes significantly more challenging in practice, making it difficult for the policy to reliably acquire behaviors such as long-distance backtracking or exploiting distant structural cues that only become relevant later in the mission. Consequently, both conventional and learning-based approaches still struggle to utilize longterm information effectively and to adjust their exploration strategy as environmental conditions change. Inspired by the impressive intelligence emerging in recent AI developments, we propose FARE, novel hierarchical fastslow thinking framework that separates global reasoning from local decision making. Our slow-thinking module operates at the global level and reasons over sparse belief graph derived from community detection and modularity-based clustering [7]. Given concise textual description of the environment, our slow-thinking module extracts high-level spatial characteristics and obstacle configurations, which are then used to synthesize an exploration strategy tailored to the overall map structure. This strategy is grounded onto the global belief graph to generate sequence of global waypoints that reflect long-range coverage priorities and adapt as the partial map is incrementally updated. Our fast-thinking module operates at the local level and uses reinforcement learning policy to select actions based on dense local map information, including frontiers and nearby geometric features. During deployment, the policy reacts in real time to local observations while also incorporating the global waypoints supplied by the slow-thinking module. To ensure that the learned policy can follow long-horizon guidance without relying on overly myopic reward shaping, the training process includes an additional reward term that encourages consistency with the global waypoints. This design allows the policy to remain responsive to local conditions while still pursuing long-term objectives. Through this structured interaction, our framework effectively integrates global reasoning with local decision, enabling the robot to exploit long-term information and to adapt its exploration behavior as the environment unfolds. As demonstrated in simulations and real-world robot validation, this design reduces unnecessary backtracking and yields more efficient exploration trajectories. II. RELATED WORK A. Autonomous Exploration Autonomous exploration has been widely studied using both conventional and learning-based approaches. Conventional planners are predominantly frontier-based, selecting the boundary between known and exploration targets at unknown regions, where early methods directly navigate to individual frontier points, leading to greedy and myopic behavior [17]. Later works reformulate exploration as viewpoint selection problem, in which candidate viewpoints are sampled and evaluated based on expected information gain and feasibility, enabling non-myopic planning through sampling-based strategies [9], [18]. To improve scalability in large environments, many works adopt hierarchical designs that combine fine-grained local planning with coarse global representations. Representative approaches construct sparse global frontier graphs or viewpoint graphs to guide long-range navigation, while local planners handle detailed viewpoint sampling and collision checking [2], [5]. While effective, these methods rely on manually designed global representations and planning heuristics, and require careful balancing between representation sparsity and coverage, which is difficult to achieve in environments with unknown or evolving structures. Learning-based exploration methods aim to alleviate manual design by training policies to guide exploration decisions. Some works employ supervised learning to explicitly predict occupancy or semantic maps from partial observations, and then integrate the predicted maps into conventional frontier-based planners [11], [15]. More recent approaches formulate exploration as sequential decision-making problem and use reinforcement learning to estimate long-term returns from observations. Many of these methods rely on convolutional neural networks (CNNs) to process fixed-size map representations or local occupancy grids [11], [15], [16], [19], with different action abstractions ranging from frontier or viewpoint selection [11], [15] to direct navigation commands such as velocity or steering control [12], [16]. To address the limited scalability and robustness of CNN-based planners, recent works adopt graph learning-based representations and formulate exploration as next-viewpoint selection problem on graph [7], [13], [14], enabling variable-sized environments. Despite these advances, existing methods still struggle to exploit longterm information and adapt their exploration behavior as the environment evolves, largely because exploration decisions are conditioned on local observations or fixed representations, without explicitly leveraging readily available, highlevel environment priors to guide planning. B. LLM-based Graph Reasoning LLM-based graph reasoning has advanced substantially in recent years, as an increasing body of work shows that large language models can exploit structured relational information when graphs are presented in suitable, structured formats. central challenge in this line of research lies in how graph structures are encoded and provided to LLMs in way that preserves relational semantics while remaining compatible with text-based or embedding-based model interfaces. To this end, widely adopted approaches introduce structured graph verbalizers, embedding projectors, or instruction-tuned schemas that convert nodes, edges, and local neighborhoods into sequential representations that LLMs can reliably process [20][23]. Building on such representations, another line of work focuses on strengthening LLM reasoning capabilities over graphs. Recent methods incorporate guided graph traversal, community-aware pruning, and explicitly structured reasoning steps to facilitate multi-hop inference and long-range dependency modeling on complex graphs [24] [26]. In parallel, hybrid approaches couple LLMs with dedicated graph encoders or multimodal modules, allowing symbolic or geometric graph features to complement language-based reasoning, as exemplified by GraphLLM and multimodal extensions such as GITA [27], [28]. Despite these advances, existing LLM-based graph reasoning methods are evaluated primarily on static, offline benchmarks, and rarely address the challenges posed by dynamic, partially observed, and incrementally evolving graph structures that arise in realworld robotic environments. III. PROBLEM FORMULATION Let R2 denote bounded and initially unknown environment, represented by two-dimensional occupancy grid. During exploration, the robot incrementally constructs partial map M, which is decomposed into known and unknown regions, = Mu Mk. (1) the environment The known region Mk is further partitioned into free and occupied space Mk = Mf Mo, where Mf denotes traversable space and Mo denotes occupied space. At the beginning of exploration, is completely unknown, i.e., = Mu. At each decision step, the robot acquires observations using an omnidirectional LiDAR with sensing range ds, and cells within the sensing range are classified as either free or occupied according to traversability. The objective of autonomous exploration is to compute collision-free trajectory that minimizes the total traversal cost while completing the exploration, τ = argmin L(τ ), τ s.t. Mk = Mg (2) where denotes the set of feasible trajectories and : τ R+maps trajectory to its total path length. While the ground-truth map Mg is not available during real-world deployment, it is accessible in simulation and benchmark settings for evaluation. In practice, exploration completion is commonly approximated by the absence of remaining unknown or frontier regions. IV. METHODOLOGY A. Hierarchical Robot Belief Graph In this work, we construct hierarchical robot belief graph through the community-based method proposed in [7]. It represents the neighboring area of the robot as dense local graph, which will serve as the input for the fast-thinking module, and the distant areas as sparse global graph, which will serve as the input for the slow-thinking module. 1) Local Belief Graph: The robot belief is represented by collision-free graph constructed from onboard sensor data. The robot trajectory is defined as sequence of viewpoints τ = (v0, v1, . . . ), where each waypoint vi Mf lies in free space. At each decision step t, set of candidate viewpoints Vt = {v0, v1, . . . }, with vi = (xi, yi) Mf , is uniformly sampled from the current free space following the approach in TARE. Each viewpoint is connected to its nearest neighbors, and edges intersecting occupied or unknown regions are removed, yielding collision-free graph Gt = (Vt, Et). The robot selects viewpoints sequentially, forming trajectory τi Vt. Each node vi is assigned utility value ui, defined as the number of observable frontiers within sensor range. frontier fj is observable from vi if the line segment L(vi, fj) is collision-free and fj vi ds. The utility is defined as ui = Fo,i, fj vi ds, fj Fo,i, L(vi, fj) (M Mf ) = . (3) where Fo,i denotes the set of observable frontiers at vi. Then we introduce square sliding window Wlocal of size d, centered at the robots current position vcur, and construct the local belief graph Glocal = (Vlocal, Elocal). Here, Vlocal Vt contains all candidate nodes within the window, and Elocal Et includes the edges connecting those nodes. 2) Global Belief Graph with Modularity-Based Pruning: We construct the global belief graph Gglobal = (Vglobal, Eglobal) by jointly performing community detection and modularity-based pruning on the collision-free graph Gt. Instead of retaining all detected communities, we explicitly select subset of structurally informative communities according to their modularity contribution, and only these communities are promoted as high-level nodes for global reasoning. Specifically, we first apply modularity-based community detection on Gt [29], where the graph modularity is defined as = 1 2m (cid:88) i,j (cid:20) Aij (cid:21) kikj 2m δ(ci, cj), (4) with denoting the total number of edges, Aij the adjacency matrix, ki the degree of node i, and δ() the community indicator. Let denote the set of communities detected from Gt. For simplicity, we ignore edge weights and directions and reorganize the modularity objective at the community level: = 1 2m (cid:88) (cid:88) (cid:20) Aij (cid:21) . (5) kikj 2m cC i,jc For each community c, let (cid:80) in denote the number of edges in c, and let (cid:80) tot denote the number of edges connected to c. Thus, we have: (cid:88) i,jc kikj 2m = ((cid:80) tot)2 2m , the modularity can be rewritten as = 1 2m (cid:88) (cid:20)(cid:88) cC in ((cid:80) tot)2 2m (cid:21) . Then, the modularity of community is Q(c) = (cid:88) in ((cid:80) tot)2 2m . (6) (7) (8) Rather than preserving all communities, we directly integrate pruning step into the construction of Gglobal by retaining only the top-k communities with the highest modularity contributions: (9) := argtopkcC Q(c). Each retained community is abstracted as node in Vglobal, while edges in Eglobal are induced by inter-community connectivity in Gt. In this way, the global belief graph is constructed directly from pruned set of structurally coherent communities, yielding compact yet informative high-level representation that significantly reduces reasoning complexity while preserving the dominant topological structure of the environment. B. Slow-Thinking Module 1) Environment Conditioned Strategy Generation: This part provides principled mechanism for translating highlevel environment descriptions into strategy-level guidance for autonomous exploration. Rather than directly prescribing actions or trajectories, it operates at the level of environmentconditioned reasoning, bridging semantic understanding and long-horizon planning objectives. According to the intuitive impression formed after brief exposure to the environment, we give concise natural language description that summarizes the environment type and its composition. The slow-thinking module first performs structured environment characterization using an LLM. To ensure interpretability and robustness, the analysis is constrained to predefined schema that decomposes the environment into three complementary aspects. Spatial characteristics capture the global layout and navigability of the environment, including openness, structural complexity, topological connectivity, and corridor width. Obstacle characteristics describe the distribution and structure of obstacles, such as their density, predictability, and vertical variation. Exploration challenges reflect task-level difficulties induced by the environment, including navigation difficulty, the likelihood of dead ends, and the necessity of backtracking. Environment Characterization Example spatial characteristics: openness: confined, complexity: moderate, connectivity: low, corridor width: narrow obstacle characteristics: density: moderate, predictability: irregular, height variation: flat exploration challenges: navigation difficulty: moderate, dead end probability: moderate, backtracking necessity: moderate Based on the extracted environment characteristics, we instantiate an exploration strategy by populating set of predefined strategy dimensions. These dimensions provide structured representation of agent-level exploration behavior and are designed to capture recurring decision patterns across diverse environments. Specifically, the strategy is parameterized along four complementary axes: spatial strategy, efficiency strategy, safety strategy, and task strategy. Each axis governs distinct aspect of exploration behavior, including coverage patterns and traversal order, energy and time trade-offs, risk sensitivity and obstacle handling, and completion objectives and information priorities. The environment characteristics directly condition the values assigned to these dimensions. For instance, environments exhibiting low topological connectivity increase tolerance for backtracking and enforce stronger awareness of escape routes, reflecting the higher risk of entrapment. Dense or irregular obstacles bias the strategy toward conservative motion, larger obstacle clearance, and cautious treatment of unexplored regions. Similarly, high navigation difficulty favors exploration behaviors that prioritize reliability and path quality over speed. In this way, environment semantics are systematically translated into structured exploration strategies that remain interpretable and explicitly grounded in environmental factors. Strategy Example description: Outdoor environment with natural obstacles and terrain variations, spatial: coverage strategy: boundary first, direction bias: perimeter following, depth strategy: balanced depth breadth, corridor handling: natural path , efficiency: ate, backtrack tolerance: moderate, revisit policy: avoid , safety: obstacle clearance: conservative, unknown area approach: standard, dead end handling: explore carefully, escape route awareness: always maintain , task: object detection, quality vs speed: balanced completion criteria: time limited, information priority: energy policy: conservative, time constraint: moder2) Graph Reasoning: Given the pruned global belief graph Gglobal, textual strategy prompt and the episode memory m, we perform iterative graph reasoning with the instance Π of LLMs. At reasoning depth i, the LLMs perform: τg = Π(Gglobal, x, m) (10) where τg = [vcur, v1, . . . , vm] is global path from the current node to an unexplored node. C. Fast-Thinking Module 1) Policy Network: The fast-thinking module operates on structured observation that integrates the local graph, utility, and global path. At each time step, the observation is defined as ot = (G, τt). Following [7], [13], [14], we build an informative graph = (V , El), which shares the same edge set and node positions with the local graph Gloacl. In addition to the position (xi, yi), each node = (xi, yi, ui, gi) in the informative graph has two more properties: the utility ui and guidepost gi (a binary signal that denotes whether the location of the node is in the global paths). The planner selects neighboring node vi (vcur ) as the next waypoint wt, and the robot executes the action at to move toward wt. Our policy network comprises attention-based encoder and decoder modules tailored to graph-structured inputs. We first compute query (qi), key (ki), and value (vi) vectors via learned linear transformations: , (11) qi = qh(q) , ki = kh(k,v) , vi = vh(k,v) i where hi Rdf denotes the input node feature vector assoi, q, k, Rdf df are learnable ciated with node projection matrices, and superscripts (q) and (k, v) indicate query or key/value projections, respectively. The scaled dotproduct attention scores are then computed between pairs of nodes as uij = kj and transformed into normalized df attention weights subject to edge constraints indicated by an adjacency-based mask matrix : where is the edge set defining the current adjacency relationships. Finally, the output node features j=1 wijvj aggregates the weighted values across neighbors. = (cid:80)n Fig. 2: Illustration of the environment state at timestep t, showing the robot at position pt, the policy-selected waypoint wt, and the globally guided next waypoint . 2) Instruction Following: Building upon the reward design in [7], we introduce an instruction following objective to encourage the fast-thinking policy to adhere to the long horizon guidance generated by the slow-thinking module. At decision step t, the fast-thinking policy is guided by global path τ , and selects local waypoint wt for execution. , whose next waypoint is denoted as At high level, following the global guidance reduces unnecessary detours and implicitly shortens the executed trajectory. While this objective can be expressed in terms of path length deviation, directly optimizing such quantities is impractical for reinforcement learning. Instead, we optimize smooth surrogate that preserves the monotonic relationship with deviation from the global guidance. We define the normalized deviation between the policy-selected waypoint and the globally guided waypoint as wt 2 4 node where node denotes the node resolution. The normalization constant corresponds to an upper bound on admissible deviation measured in grid units, ensuring scale invariance across environments. The instruction following reward is then defined as an exponential penalty on the normalized deviation, dt = (13) , rdev = edt 1 , (14) wij = exp(uij) (1 Mij) j=1 exp(uij) (1 Mij) (cid:80)n , Mij = (cid:40)0, (vi, vj) 1, (vi, vj) / (12) which yields rdev [1, 0]. This formulation assigns mild penalties for small deviations while increasingly suppressing large deviations from the global guidance, providing smooth and stable gradients for policy optimization. By this construction, the waypoint deviation dt is monotonically related to detours from the global path. Therefore, minimizing the (a) Indoor (b) Forest (c) Warehouse Fig. 3: Performance comparison between FARE and conventional baselines across 10 runs per method in each environment. TABLE I: Comparison results in three environment. (a) Indoor (b) Forest (c) Warehouse Distance (m) Time (s) Distance (m) Time (s) Distance (m) Time (s) DSVP TARE ARiADNE HEADER 1511(75) 1209(42) 1053(63) 1030(40) 931(61) 658(23) 610(24) 576(26) DSVP TARE ARiADNE HEADER 2058(92) 1363(43) 1320(81) 1230(72) 1083(60) 711(21) 790(62) 725(36) DSVP TARE ARiADNE HEADER 869(42) 652(31) 521(16) 492(17) 582(32) 366(22) 362(40) 286(16) FARE 1048(13) 590(10) FARE 1090(21) 680(10) FARE 441(15) 252(8) (a) Indoor (b) Forest (c) Warehouse Fig. 4: Demonstration of the exploration trajectories output by FARE in both indoor and outdoor Gazebo simulations. The Trajectory is color-coded to represent the robots movement over time. cumulative instruction-following penalty implicitly encourages shorter executed trajectories, while remaining fully compatible with efficient reinforcement learning. V. EXPERIMENTS We conduct set of experiments to evaluate the effectiveness and robustness of FARE. We first perform comparative evaluations in Gazebo simulation across three representative environmentsindoor, forest, and warehouseto assess exploration performance under diverse structural characteristics. We then deploy FARE on real mobile robot and validate its performance in large-scale campus environment. All experiments use the same trained model and identical system configurations unless otherwise specified. A. Comparison Analysis We integrate FARE into the robot operating system (ROS) and compare it with representative state-of-the-art exploincluding TARE [2], DSVP [10], ARiration planners, ADNE [14] and HEADER [7]. For fair comparison, ARiADNE is implemented with the same graph rarefaction strategy as in [14], and all graph-related settings are kept consistent across methods. The comparison is conducted in Gazebo simulation across three benchmark environments: an indoor environment, forest environment, and warehouse environment. All experiments are performed on fourwheeled differential-drive robot equipped with 16-channel 3D LiDAR, with maximum speed of 2 m/s. For different environments, FARE only adjusts the node resolution parameter node, which is varied from 1.2 to 2.8 m. The baseline planners generally require tuning of (a) Agilex Scout-mini (b) Indoor Teaching Building Map (c) Exploration Trajectory Fig. 5: Validations of FARE in real-world scenarios on wheeled robot. multiple parameters to achieve their best performance. In addition, FARE is provided with short environment description specific to each benchmark environment to condition the generation of the exploration strategy, with the slowthinking module instantiated as Qwen3-14B. The environment descriptions are as follows: Indoormodern indoor office building with long corridors, meeting rooms, and cubicle areas; Forestoutdoor forest environment with natural obstacles, trees, and uneven terrain; and Warehouseindoor warehouse composed of densely arranged box stacks forming narrow aisles. Each method is executed for 10 runs per environment. Travel distance and explored volume are reported in Fig. 3, and quantitative results are summarized in Table I. FARE achieves performance on par with other baselines in the indoor environment, where the environment is compact and lacks distinctive global-level structure. In the forest environment, FARE achieves clear reduction in travel distance and makespan, and this advantage further widens in the warehouse environment, where FARE obtains the shortest paths and fastest completion times among all methods. Specifically, as illustrated by the trajectory visualizations in Fig. 4, FARE does not rely solely on local frontier or utility signals. Instead, it systematically incorporates global structural cues during exploration. After reaching the boundary of the explored region, FARE tends to complete peripheral and corner areas early, rather than postponing them. In contrast, baseline methods often defer these regions and revisit them later, resulting in additional backtracking and reduced overall efficiency. We attribute FAREs performance gains to two key factors. First, our hierarchical design enables environment-adaptive planning, allowing FARE to adjust its global exploration strategy according to the environment character. Second, effective coordination between global guidance and local execution allows the agent to balance long-horizon objectives with local reactivity. The slow-thinking module provide global direction, while the fast-thinking module remain flexible to exploit nearby informative regions. B. Hardware Validation We validate FARE on an Agilex Scout-mini wheeled robot equipped with an onboard Jetson AGX Orin, which runs the LLM-based slow-thinking module using Qwen314B. An Ouster OS0-32 LiDAR is used for perception, and FastLIO2 [30] provides odometry and mapping. For all experiments, the maximum robot speed is set to 1 m/s, the sensor range to 8 m, and the fast-thinking module replanning frequency to 1 Hz. Real-world validation is conducted in 200m 130m indoor teaching building on campus. The environment consists of long corridors, rooms, and intersections, posing challenges in global reasoning and long-horizon exploration. We set the map resolution to map = 0.4 and the node resolution to node = 0.8 m. These parameters are selected prior to deployment and remain fixed throughout the experiment. During deployment, FARE successfully explores the entire building without manual intervention. The system maintains stable runtime performance during global guidance generation and local policy execution. This hardware experiment demonstrates that FARE can effectively operate with onboard LLM inference and transfer from simulation to real-world environments. VI. CONCLUSIONS reasoning from fast In this work, we propose FARE, hierarchical autonomous exploration framework that separates environmentconditioned global local decisionmaking. FARE translates concise natural language environment descriptions into structured, interpretable exploration strategies, and performs LLM-based graph reasoning on pruned global belief graph to generate adaptive global guidance. fast-thinking policy integrates local graph structure, utility signals, and global paths, and is explicitly trained to follow long-horizon guidance while retaining local flexibility. Together, these components enable coherent exploration behaviors that reduce redundant backtracking and improve efficiency, as validated in both simulation and real-world experiments. Future work will extend FARE to multi-robot exploration with explicit inter-agent coordination, and incorporate visionbased semantic perception to enable online detection of environment changes, better supporting hybrid environments with abrupt scene-type transitions. We also plan to investigate richer environment representations and three-dimensional action spaces to further enhance generality. [20] J. Guo, L. Du, H. Liu, M. Zhou, X. He, and S. Han, Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking, arXiv preprint arXiv:2305.15066, 2023. [21] J. Wang, J. Wu, Y. Wu, Y. Liu, M. Gao, and J. McAuley, Instructgraph: Boosting large language models via graph-centric instruction tuning and preference alignment. [22] R. Chen, T. Zhao, A. Jaiswal, N. Shah, and Z. Wang, Llaga: Large language and graph assistant, arXiv preprint arXiv:2402.08170, 2024. [23] Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S. Wang, D. Yin, W. Fan, H. Liu, et al., Exploring the potential of large language models (llms) in learning on graphs, ACM SIGKDD Explorations Newsletter, vol. 25, no. 2, pp. 4261, 2024. [24] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, L. M. Ni, H. Shum, and J. Guo, Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph, in ICLR. OpenReview.net, 2024. [25] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk, and T. Hoefler, Graph of thoughts: Solving elaborate problems with large language models, in AAAI. AAAI Press, 2024, pp. 17 682 17 690. [26] X. Liang and Z. Gu, Fast think-on-graph: Wider, deeper and faster reasoning of large language model on knowledge graph, in AAAI. AAAI Press, 2025, pp. 24 55824 566. [27] Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y. Yang, Graphllm: Boosting graph reasoning ability of large language model, CoRR, vol. abs/2310.05845, 2023. [28] Y. Wei, S. Fu, W. Jiang, Z. Zhang, Z. Zeng, Q. Wu, J. T. Kwok, and Y. Zhang, GITA: graph to visual and textual integration for visionlanguage graph reasoning, in NeurIPS, 2024. [29] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, Fast unfolding of communities in large networks, Journal of statistical mechanics: theory and experiment, vol. 2008, no. 10, p. P10008, 2008. [30] W. Xu, Y. Cai, D. He, J. Lin, and F. Zhang, Fast-lio2: Fast direct lidarinertial odometry, IEEE Transactions on Robotics, vol. 38, no. 4, pp. 20532073, 2022."
        },
        {
            "title": "REFERENCES",
            "content": "[1] M. Selin, M. Tiger, D. Duberg, F. Heintz, and P. Jensfelt, Efficient autonomous exploration planning of large-scale 3-d environments, IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 16991706, 2019. [2] C. Cao, H. Zhu, H. Choset, and J. Zhang, Tare: hierarchical framework for efficiently exploring complex 3d environments. in Robotics: Science and Systems, vol. 5, 2021, p. 2. [3] T. Dang, M. Tranzatto, S. Khattak, F. Mascarich, K. Alexis, and M. Hutter, Graph-based subterranean exploration path planning using aerial and legged robots, Journal of Field Robotics, vol. 37, no. 8, pp. 13631388, 2020. [4] O. Peltzer, A. Bouman, S.-K. Kim, R. Senanayake, J. Ott, H. Delecki, M. Sobue, M. Kochenderfer, M. Schwager, J. Burdick, et al., Fig-op: Exploring large-scale unknown environments on fixed time budget, arXiv preprint arXiv:2203.06316, 2022. [5] J. Huang, B. Zhou, Z. Fan, Y. Zhu, Y. Jie, L. Li, and H. Cheng, Fael: Fast autonomous exploration for large-scale environments with mobile robot, IEEE robotics and automation letters, vol. 8, no. 3, pp. 16671674, 2023. [6] S. Long, Y. Li, C. Wu, B. Xu, and W. Fan, Hphs: Hierarchical planning based on hybrid frontier sampling for unknown environments exploration, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024, pp. 12 05612 063. [7] Y. Cao, Y. Wang, J. Liang, S. Liao, Y. Zhang, P. Li, and G. Sartoretti, robot exploration via attention-based deep learning with expert-guided reward, arXiv preprint Header: Hierarchical reinforcement arXiv:2510.15679, 2025. [8] B. Yamauchi, frontier-based approach for autonomous exploration, in Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA97.Towards New Computational Principles for Robotics and Automation. IEEE, 1997, pp. 146151. [9] A. Bircher, M. Kamel, K. Alexis, H. Oleynikova, and R. Siegwart, Receding horizon next-best-view planner for 3d exploration, in 2016 IEEE international conference on robotics and automation (ICRA). IEEE, 2016, pp. 14621468. [10] H. Zhu, C. Cao, Y. Xia, S. Scherer, J. Zhang, and W. Wang, Dsvp: Dual-stage viewpoint planner for rapid exploration by dynamic expansion, in 2021 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2021, pp. 76237630. [11] F. Chen, S. Bai, T. Shan, and B. Englot, Self-learning exploration and mapping for mobile robots via deep reinforcement learning, in Aiaa scitech 2019 forum, 2019, p. 0396. [12] F. Chen, J. D. Martin, Y. Huang, J. Wang, and B. Englot, Autonomous exploration under uncertainty via deep reinforcement learning on graphs, in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 61406147. [13] Y. Cao, T. Hou, Y. Wang, X. Yi, and G. Sartoretti, Ariadne: reinforcement learning approach using attention-based deep networks for exploration, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 10 21910 225. [14] Y. Cao, R. Zhao, Y. Wang, B. Xiang, and G. Sartoretti, Deep learning-based large-scale robot exploration, IEEE reinforcement Robotics and Automation Letters, 2024. [15] Y. Xu, J. Yu, J. Tang, J. Qiu, J. Wang, Y. Shen, Y. Wang, and H. Yang, Explore-bench: Data sets, metrics and evaluations for frontier-based and deep-reinforcement-learning-based autonomous exploration, in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 62256231. [16] S. Zhu, J. Zhou, A. Chen, M. Bai, J. Chen, and J. Xu, Maexp: generic platform for rl-based multi-agent exploration, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 51555161. [17] H. H. Gonzalez-Banos and J.-C. Latombe, Navigation strategies for exploring indoor environments, The International Journal of Robotics Research, vol. 21, no. 10-11, pp. 829848, 2002. [18] Z. Xu, D. Deng, and K. Shimada, Autonomous uav exploration of dynamic environments via incremental sampling and probabilistic roadmap, IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 27292736, 2021. [19] F. Niroui, K. Zhang, Z. Kashino, and G. Nejat, Deep reinforcement learning robot for search and rescue applications: Exploration in unknown cluttered environments, IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 610617, 2019."
        }
    ],
    "affiliations": [
        "Beihang University, China",
        "Department of Mechanical Engineering, National University of Singapore, Singapore"
    ]
}