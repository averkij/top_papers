{
    "paper_title": "SAO-Instruct: Free-form Audio Editing using Natural Language Instructions",
    "authors": [
        "Michael Ungersböck",
        "Florian Grötschla",
        "Luca A. Lanzendörfer",
        "June Young Yi",
        "Changho Choi",
        "Roger Wattenhofer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative models have made significant progress in synthesizing high-fidelity audio from short textual descriptions. However, editing existing audio using natural language has remained largely underexplored. Current approaches either require the complete description of the edited audio or are constrained to predefined edit instructions that lack flexibility. In this work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of editing audio clips using any free-form natural language instruction. To train our model, we create a dataset of audio editing triplets (input audio, edit instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual editing pipeline. Although partially trained on synthetic data, our model generalizes well to real in-the-wild audio clips and unseen edit instructions. We demonstrate that SAO-Instruct achieves competitive performance on objective metrics and outperforms other audio editing approaches in a subjective listening study. To encourage future research, we release our code and model weights."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 5 9 7 2 2 . 0 1 5 2 : r SAO-Instruct: Free-form Audio Editing using Natural Language Instructions Michael Ungersböck ETH Zurich mungersboeck@ethz.ch Florian Grötschla ETH Zurich fgroetschla@ethz.ch Luca A. Lanzendörfer ETH Zurich lanzendoerfer@ethz.ch June Young Yi Seoul National University julianyi1@snu.ac.kr Changho Choi Korea University changho98@korea.ac.kr Roger Wattenhofer ETH Zurich wattenhofer@ethz.ch"
        },
        {
            "title": "Abstract",
            "content": "Generative models have made significant progress in synthesizing high-fidelity audio from short textual descriptions. However, editing existing audio using natural language has remained largely underexplored. Current approaches either require the complete description of the edited audio or are constrained to predefined edit instructions that lack flexibility. In this work, we introduce SAO-Instruct, model based on Stable Audio Open capable of editing audio clips using any free-form natural language instruction. To train our model, we create dataset of audio editing triplets (input audio, edit instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and manual editing pipeline. Although partially trained on synthetic data, our model generalizes well to real in-the-wild audio clips and unseen edit instructions. We demonstrate that SAO-Instruct achieves competitive performance on objective metrics and outperforms other audio editing approaches in subjective listening study. To encourage future research, we release our code and model weights."
        },
        {
            "title": "Introduction",
            "content": "Generative audio models have become increasingly popular, allowing users to generate high-fidelity long-form audio within seconds. While they have been successfully applied in various domains, including music [8, 10, 1, 5], speech [29, 26, 21, 4], and general audio [30, 24, 9, 13], the area of audio editing still remains largely unexplored. These models, especially when given short or ambiguous prompts, have freedom and flexibility in their outputs. While this enables diverse and creative generations, it may lead to outputs that deviate from the users original intent. similar issue exists in recorded audio, which often contains imperfections and typically requires manual editing before being suitable for real-world use. These modifications can range from subtle adjustments to stylistic and spatial effects, such as the birds should chirp louder, make it sound muffled, or add reverb to the fireworks. In addition to the challenges faced in generative audio, such as modeling the high-dimensional long-term temporal dependencies, audio editing introduces further complexities. Edits must selectively modify specific aspects of the provided audio while preserving the overall background context. User-specified edits can also vary dramatically in scope and typically do not follow rigid structures. Recent methods have made progress towards addressing some of these challenges. Zero-shot inversion approaches [33, 20] modify existing audio by conditioning on full target description. However, describing an audio clip with its unique sound characteristics in concise, unambiguous text is challenging and requires significant effort [1]. more intuitive alternative is instruction-based 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: Given an input audio clip and an edit instruction, SAO-Instruct outputs the edited audio while keeping the overall audio context intact. editing, where users specify only the intended change in natural language rather than the desired outcome. The AUDIT [44] model is one of the first to explore instruction-based editing, but only supports predefined set of editing tasks. Additionally, user instructions are often diverse and underspecified, which prevents them from aligning neatly with such fixed operations. We argue that enabling fully free-form instruction-based editing would allow more expressive transformations, significantly simplify user interaction, and broaden the applicability of generative audio models. In this work, we introduce SAO-Instruct, the first model for free-form instruction-based editing in the audio domain. We use Prompt-to-Prompt [16], DDPM inversion [33], and manual editing pipeline to generate data triplets of input audio, edit instruction, and the corresponding edited audio. As illustrated in Fig. 1, we show that the model learns to modify audio given free-form edit instruction. While partially fine-tuned on synthetic data, our experiments show that the model is able to generalize well to real in-the-wild audio and is able to follow diverse editing operations. Our key contributions can be summarized as follows: We present SAO-Instruct, the first fully free-form instruction-based audio editing model based on Stable Audio Open. Our model generalizes well to real in-the-wild audio clips and achieves competitive performance compared to zero-shot approaches that rely on full audio descriptions. We design novel pipeline to create dataset of audio editing triplets (input audio, edit instruction, output audio), combining LLM-based prompt generation, Bayesian Optimization, and filtering mechanism that addresses the limitations of current generative audio models. We create diverse dataset using three complementary approaches: fully synthetic generation via Prompt-to-Prompt, semi-synthetic samples via DDPM inversion, and real-world manual edits. We demonstrate their contribution to the performance of our editing model in an ablation study. Samples are available online: https://eth-disco.github.io/sao-instruct"
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Text-to-Audio Generation The area of generative audio has gained significant popularity in recent years. Applications range from music generation [8, 10, 1, 5] and speech synthesis [29, 26, 21, 4] to the broader general audio domain [30, 24, 9, 13]. The latter category encompasses everything from short, distinct sound effects (the sound of whip) to more complex scenes (frogs croaking at pond) and environmental ambient noise (light rain with distant thunder). Several architectures have been explored for general audio synthesis. Apart from autoregressive approaches [24], recent advancements in latent diffusion models have achieved state-of-the-art results. AudioLDM [30] uses latent diffusion architecture conditioned on Contrastive Language-Audio Pretraining (CLAP) [45] embeddings to learn the latent space of mel spectrograms. While such approaches require vocoder to reconstruct the waveform, which can introduce artifacts due to missing phase information, recent advances such as BigVGAN [28] have significantly improved reconstruction fidelity. To remove the need for vocoders, Stable Audio Open [9] uses variational autoencoder operating at 21.5 Hz latent framerate to encode stereo audio into continuous latent representation. diffusion transformer [38] then operates on this latent space, conditioned on the text prompt, timing information, and the current diffusion timestep. Finally, the output is decoded back into 44.1 kHz stereo audio of up to 47 seconds in length. By conditioning on timing information, users can specify the start and end points of the generated audio, while the model is trained to fill the remaining segments with silence. 2 Figure 2: Overview of our proposed method. Green indicates synthetic data. Audio datasets are used as the starting point for prompt generation. DDPM inversion and Prompt-to-Prompt use the input caption and generated output caption to create partial and fully synthetic dataset, respectively. For manual edits, predefined edit operation is sampled. In the fine-tuning stage, Stable Audio Open is trained on the combined generated samples and edit instructions. During inference, SAO-Instruct receives an audio clip and free-form edit instruction and produces the edited output. 2.2 Generative Editing Recent progress in generative models has enabled new capabilities in editing content across various domains. Approaches to generative editing can be broadly categorized into zero-shot methods, which adapt existing models without training, and supervised methods, which fine-tune generative models for specific editing tasks. Zero-shot Methods. common zero-shot editing strategy for diffusion models involves partially noising an input sample and then guiding the denoising process with modified description [36]. Other approaches [37, 19, 33, 20] use inversion techniques which first estimate the latent representation given an input sample and its description, and then generate the edited output by guiding the denoising process with modified prompt. However, these inversion approaches typically require explicit access to detailed input and output descriptions, which are often unavailable in real-world settings. The quality of results is also highly dependent on how these descriptions are phrased, making such approaches less intuitive for practical use. Supervised Methods. Another line of work trains generative models specifically for editing tasks. Diffusion models with an infilling objective learn to reconstruct masked audio regions based on the surrounding context and target text description [43]. more intuitive alternative is instructionbased editing, where users specify only the intended change in natural language rather than the desired outcome. In the image domain, InstructPix2Pix [3] fine-tunes diffusion model on synthetic data triplets (input image, edit instruction, output image), generated using an LLM and Promptto-Prompt [16], which enables image edits by selectively injecting attention maps of the original prompt during the generation of the modified prompt. In the general audio domain, AUDIT [44] was among the first to enable instruction-based editing. It supports the following five core operations: addition, removal, replacement, inpainting, and super-resolution of audio. InstructME [14] introduced related approach for music editing that also considers the harmonic consistency while executing edits. Fugatto [42] further explores free-form audio generation and transformation through specialized multitasks datasets. While such models demonstrate broader instruction-following capabilities, they are not specifically tailored for high-fidelity audio editing using natural free-form instructions."
        },
        {
            "title": "3 Method",
            "content": "Free-form audio editing with supervised learning approach requires dataset of triplets consisting of an input audio clip, an edit instruction, and the corresponding edited audio. Since no such dataset is readily available, we construct one using combination of LLM-based prompt generation and partially synthetic audio, inspired by recent work in the image domain [3]. As illustrated in Fig. 2, our approach first prepares dataset of prompts that contain an input caption, an edit instruction, and an output caption. The corresponding paired audio samples are created using three complementary methods: fully synthetic audio generated via Prompt-to-Prompt [16], semi-synthetic audio via DDPM 3 inversion [33], and manually edited real audio clips. These generated edit instructions and audio pairs are then combined to form the full training set. Finally, Stable Audio Open (SAO) [9] is fine-tuned on this dataset to obtain SAO-Instruct, which can edit given audio clip based on free-form natural language instruction. 3.1 Prompt Generation To train our model for free-form editing, we need samples of triplets (input caption, edit instruction, and output caption). For the input captions, we use the captioning datasets AudioCaps [22] and WavCaps [35]. AudioCaps contains 50k human-written captions paired with audio clips sourced from AudioSet [12]. WavCaps consists of 400k audio-caption pairs collected from multiple sources, including FreeSound 1, AudioSet-SL [15] and the BBC Sound Effects 2 library. We use GPT-4o to generate synthetic dataset of structured prompts. Given an input caption, the LLM is prompted to generate fitting edit instruction and corresponding output caption. For example, starting from the caption Birds chirping and water flowing, it may generate the instruction Remove the water flowing and the output caption Birds chirping, with the output caption derived by applying the edit instruction to the input caption. The LLM also generates additional metadata, including negative prompts and count of distinct audible elements, which we use to improve sample quality for audio synthesis and to enable downstream filtering. More details can be found in Section A. 3.2 Prompt-to-Prompt significant challenge in audio editing is applying targeted modifications while preserving the overall audio context, including background sounds and overall atmosphere. Since there is no such dataset available, we adapt the Prompt-to-Prompt [16] method, originally developed for image editing, to the audio domain. Prompt-to-Prompt enables localized edits of synthesized audio by injecting attention maps from the input prompt into the generation process of the edited prompt. We use Stable Audio Open [9] as the underlying generative model, due to its high-fidelity 44.1 kHz stereo output and flexibility of generating audio up to 47 seconds in length. As Stable Audio Open sometimes omits sounds present in captions with connectors, such as Helicopter taking off with wind blowing and dogs barking, we fine-tune it on AudioCaps to improve prompt adherence on general audio. While fine-tuning improved the alignment between prompts and outputs, as detailed in Section D, achieving more consistent high-quality results also requires selecting suitable combination of seed and Classifier-Free Guidance (CFG) [18] values. To this end, our approach consists of two stages, as shown in Fig. 3. First, our method explores different combinations of seed and CFG values to identify configuration that produces satisfactory results for given input/output caption pair. Next, Prompt-to-Prompt is applied to generate the edited audio pair. Candidate Search. The process for finding suitable candidate configuration (seed and CFG value) for each prompt pair is illustrated in Fig. 3, section (a) Candidate Search. We use Stable Audio Open to generate seven audio pairs, each with different configuration. Specifically, each pair is generated using 50 denoising steps, randomly chosen seed, and CFG value sampled between 3 and 9. To assess the similarity between the generated audio and corresponding caption, we use the Gemini 2.0 Flash API. We instruct Gemini to perform perceptual quality check by analyzing all audible elements in the provided audio and assigning score between 1 and 10, based on how closely the generated audio resembles the provided caption. Samples that score above threshold of 6 proceed to the next stage, in which the CLAP similarity is calculated. Finally, we select the configuration that passes the Gemini filter and has the highest mean CLAP similarity across input and output. Sample Generation. After finding suitable configuration for prompt, we move on to the sample generation as outlined in section (b) of Fig. 3. There are various parameters that can be configured in Prompt-to-Prompt. The injection fraction λattn frac configures the extent to which attention maps from the input audio influence the generation of the output audio. fraction of 0 means that no attention maps from the input audio are injected. As result, the output is not directly influenced by the input, apart from shared initial noise and sampling noise. In contrast, an injection fraction of 1 enforces strong similarity such that the output audio closely resembles the input audio. Intermediate values allow 1https://freesound.org/ 2https://sound-effects.bbcrewind.co.uk/ 4 Figure 3: Pipeline for Prompt-to-Prompt audio generation. In (a), various seeds and CFG value combinations are tested and filtered using Gemini and CLAP to identify suitable configurations for prompts. In (b) Stable Audio Open (SAO) with Prompt-to-Prompt generates audio pairs using the seed and CFG configuration found in (a). Bayesian Optimization process suggests Prompt-toPrompt parameters and resulting samples are evaluated using an objective function. For clarity, only 3 candidate pairs and 2 Bayesian Optimization trials are shown. for balance between flexibility and audio similarity during generation. The injection delay λattn delay controls at which point the attention injection begins, expressed as fraction of the total number of attention maps. value of 0 indicates that the injection starts from the first attention map, while higher values delay the injection. For example, value of 0.3 skips the first 30% of attention maps before the injection begins. As constraint, it follows that λattn delay 1. Attention reweighting λattn weight specifies multiplier that increases attention to tokens that differ between the input and output captions. These tokens are identified by comparing the tokenized versions of both captions, and their corresponding cross-attention maps are scaled by the given factor. For example, value of 1.5 increases the attention on changed tokens or newly added tokens by 50%, which helps the model pay closer attention to proposed edits. frac + λattn Using fixed Prompt-to-Prompt configuration often leads to suboptimal results. While some instructions involve only subtle changes (e.g., add an echo effect), others require more extensive modifications (e.g., remove the people talking). To handle this variability, we assign unique parameter configuration to each prompt pair. Since manual tuning is infeasible at scale, we rely on Bayesian Optimization [2] to automatically select suitable parameters. This setup requires an objective function that evaluates both the quality of the generated audio pair and the edit accuracy. Objective Function. We define an objective function that evaluates the quality of the generated audio pair and how well the edit was executed, using combination of multiple metrics. Building on insights from prior work, particularly InstructPix2Pix [3], as well as our own experiments, we identified the following metrics as suitable. The output CLAP similarity out CLAP measures the cosine similarity between the CLAP embeddings of the generated output audio and the output caption. dir CLAP measures the direction of change in the embedded CLAP space, as initially introduced for the image domain [11]. It calculates the cosine similarity between the difference of the CLAP embedded audio pair and the CLAP embedded prompt pair. The CLAP audio similarity sim CLAP measures the cosine similarity between the CLAP embedding of the input and output audio. The mel spectrogram audio similarity sim MEL calculates the multi-scale mel spectrogram loss [25] between the generated audio pair. The objective is computed as weighted sum and is maximized during Bayesian Optimization. Note that the multi-scale mel spectrogram loss is subtracted as lower values indicate better perceptual alignment. Lobj = ω1 out CLAP ω4 sim MEL CLAP + ω3 sim CLAP + ω2 dir (1) Since not all metrics contribute equally, we conducted small-scale listening study to determine effective weightings. Using our pipeline, we generated around 100 audio pairs with different weightings used for the objective function. Listeners were presented with an input audio clip, the edit 5 instruction, and two output audio samples selected by the objective functions initialized with different weightings. An ELO rating system was used to rank the different weighting configurations based on their effectiveness in selecting high-quality, relevant edits. We found the following weightings to be effective: ω1 = 8, ω2 = 14, ω3 = 0.5, ω4 = 1.5. Optimization. We run the Bayesian Optimization process for 10 trials for each caption pair. Specifically, we optimize λattn weight [1.0, 1.8]. Stable Audio Open is configured to use 50 denoising steps during each trial, while the final selected audio pair is generated using 100 denoising steps. Using fewer denoising steps during optimization significantly reduces runtime without affecting the relative ranking of generated samples, while the final 100 steps maximize audio fidelity once the optimal parameters are found. delay [0.0, 0.6], and λattn frac [0.3, 0.9], λattn 3.3 DDPM Inversion In addition to the fully synthetic Prompt-to-Prompt dataset, we create semi-synthetic dataset of audio pairs using DDPM inversion to increase data diversity. The main benefit of this approach is that the input audio is real audio clip, while only the output audio is generated. We use the zero-shot text-based audio (ZETA) [33] implementation of DDPM inversion with Stable Audio Open as the underlying generative model. We provide the model with an existing input audio and caption, with the output caption generated as outlined in Section 3.1. Sample Generation. To use ZETA in an automated manner, several parameters need to be configured per sample. The CFGsrc and CFGtar parameters control the CFG strength of the input and output prompt, respectively. Unlike traditional inversion techniques, which are applied to the full denoising process, ZETA uses the parameter Tstart to specify up to which timestep the inversion is performed. Lower values for Tstart result in high consistency with the input audio, while higher values enable more editing flexibility. As the required edit strength varies across instructions, fixed value would lead to underor over-editing. We therefore apply Bayesian Optimization with the objective function as defined in Section 3.2. Optimization. For each caption pair, we run Bayesian Optimization for 7 trials. We search for optimal values within the following ranges: CFGsrc [1, 3], CFGtar [3, 10], and Tstart [18, 65]. Throughout optimization, Stable Audio Open is configured to use 70 denoising steps. Compared to Prompt-to-Prompt, we use fewer optimization trials and denoising steps to balance quality and runtime, given the higher computational cost of DDPM inversion. 3.4 Manual Edits We further extend our dataset using suite of twelve manually implemented operations, inspired by the approach from AUDIT [44]. We refer to these operations as manual edits, as each edit is implemented using standard, deterministic, and interpretable audio effects. The twelve editing tasks are ADD, REPLACE, DROP, SWAP, LOOP, PITCH, SPEED, LOW_PASS, HIGH_PASS, INPAINT, SUPER_RES, and DENOISE. Each task has set number of inputs, certain constraints on those inputs, and an optional controllable parameter. The input audio clips may be constrained by the number of unique audio elements, as analyzed in Section 3.1, or by their duration. Some tasks accept parameters that introduce variability in their edits. For instance, the PITCH task allows control over the amount of pitch shifting, while the LOOP task accepts how many times the input audio should get repeated. The final output of each operation is an audio pair consisting of the input and edited audio clip. All tasks, input constraints, and parameters are detailed in Section B. Sample Generation. To create an audio editing triplet, we first sample one of the twelve tasks with equal probability. Based on the tasks constraints, input audio clips are filtered before suitable candidates are randomly selected. After applying the edit operation, we synthesize the natural language instruction by passing the captions of the input audio clips, alongside the optional parameter value, to task-specific LLM-based instruction generation stage built on GPT-4.1 mini. Each generation process is provided with description of the task and few-shot examples, allowing the model to produce tailored task-specific instructions. To increase instruction diversity and better reflect how real-world users might phrase requests, we apply two post-processing steps using custom stages based on o4-mini. Each post-processing step is executed with 50% probability, independently. In the 6 Table 1: An example three-stage instruction generation pipeline for the ADD task, where the selected base audio People talking in roadside cafe is mixed with the target audio chirping bird. First, the initial stage combines the base and target audio captions to produce an edit instruction. Then, the variation stage rewrites it with alternate phrasing to increase diversity. Finally, the minimization stage compresses it into its most concise form. The last two stages are applied with 50% probability. Generation Stage Applied Instruction 1. Initial 2. Variation 3. Minimization 100% Add the sound of bird chirping to the people talking in roadside cafe 50% Add some chirping birds to the chatter in roadside cafe 50% Add bird sounds variation stage, the instruction is rewritten without altering its meaning. In the minimization stage, the instruction is compressed into its most concise form. An example of this three-stage pipeline for the ADD task is shown in Table 1."
        },
        {
            "title": "4 Experimental Setup",
            "content": "4.1 SAO-Instruct Dataset. To generate the dataset of audio editing triplets we use the three approaches outlined in Section 3. For Prompt-to-Prompt, input captions are taken from the AudioCaps [22] training split and random subset of FreeSound from WavCaps [35]. For DDPM inversion, the input audio and caption are sourced from random subset of AudioSet-SL [15]. Edit instruction and output captions are generated as described in Section 3.1. For the manual edits, we choose different subset of FreeSound from WavCaps to avoid overlaps with Prompt-to-Prompt. The final audio samples are stored as 44.1 kHz stereo WAV files. The computational resources used for the dataset generation are listed in Section C. Fine-tuning and Inference. We start from the open-source weights of Stable Audio Open and fine-tune the model on our created dataset of triplets: input audio, output audio, and edit instruction. SAO-Instruct uses three types of conditioning: (1) the text prompt is replaced with free-form edit instruction, (2) the timing condition is set to the length of the provided input audio, (3) an additional audio condition for the input audio is concatenated to the models input channels. During training, the model learns to modify the provided input audio based on the edit instruction, such that the resulting output matches the reference output audio. During inference, we encode the input audio into the latent space of the diffusion model and add Gaussian noise. This noised latent is used as the initial starting point for the denoising process. Unless specified otherwise, we use 100 denoising steps and CFG value of 5. More fine-tuning and inference details are found in Section and E. 4.2 Evaluation Metrics Objective Metrics. We use several metrics for objective evaluation that capture both distributional similarity and perceptual quality. To measure how closely the edited samples match the distribution of real audio, we compute the Fréchet Distance (FD) [17], the log spectral distance (LSD), and the Kullback-Leibler (KL) divergence. To evaluate perceptual quality, we use the Inception Score (IS) [39]. The FD, KL, and IS metrics utilize the PANNs [23] classifier. The LSD calculates the distance between spectrograms of original and target audio clips. To compute these metrics, we use the evaluation pipeline provided by AudioLDM [30]. Additionally, we evaluate how well edits were performed using the CLAP [45] score, which measures the cosine similarity between the generated audio and the target caption in the CLAP embedding space. As CLAP is also used in our data generation pipeline to filter samples, it can bias results towards its embedding space. We mitigate this by not training SAO-Instruct with CLAP loss and by reporting multiple objective and subjective metrics. Subjective Metrics. For the subjective evaluation, we conducted listening study with 13 participants. Each participant was presented with 10 randomly selected 10-second audio clips from the AudioCaps test subset, including the original caption, an edit instruction, and the corresponding 7 Table 2: Ablation study comparing the influence of different generated datasets on edit relevance and audio quality. FD, LSD, and KL are shown for both original and regenerated audio. Metrics reported with indicate mean and standard deviation across evaluation samples. Outputs from SAO-Instruct fine-tuned on the combined dataset balance the strengths of the individual approaches, achieving both accurate edits and faithfulness to the input audio. Original Ref. Regenerated Ref. Training Dataset Samples FD LSD KL FD LSD KL IS CLAP Prompt-to-Prompt DDPM Inversion Manual Edits Combined Combined-Large 50k 50k 50k 50k 150k 18.71 1.50 20.50 1.34 14.60 1.42 19.11 1.41 18.38 1.36 1.32 18.29 2.68 0.86 20.72 2.75 0.58 21.21 2.76 1.02 19.24 2.72 0.93 18.97 2.72 1.77 7.940.72 0.380.14 1.87 6.820.73 0.340.15 1.89 7.500.68 0.350.15 1.74 7.690.76 0.380.14 1.76 7.591.00 0.380. edited audio clips generated by the models. Model names were hidden from participants and the order of outputs was randomized on per-sample basis. We use similar metrics to AudioEditor [20], where participants were tasked to rate each clip using the Mean Opinion Score (MOS) on discrete scale between 1 and 5. Ratings were collected for three categories: Quality, the perceptual quality of the edited audio compared to the original input audio, Relevance, how well the edit was performed, and Faithfulness, the similarity between the input audio and the edited audio. The input audio refers to the original, unedited clip provided to the model for editing. The final MOS for each category was computed by averaging ratings across all participants and samples. Section shows the evaluation interface and the instructions given to participants."
        },
        {
            "title": "5 Evaluation",
            "content": "We evaluate the performance of SAO-Instruct in an ablation study that measures the impact of the different methods for dataset generation, and compare it with audio editing baselines. We evaluate on 1k 10-second samples from the AudioCaps test subset, where edit instructions and output captions are generated using the approach described in Section 3.1. For the FD, LSD, and KL metrics we use two types of references: the original AudioCaps clips and synthetic audio generated from the target captions using Stable Audio Open. The original reference tries to capture realism with natural audio, while the regenerated reference serves as proxy to measure how well the edits were performed. Notably, lower FD, LSD, and KL scores do not necessarily reflect better editing performance, as these metrics primarily measure the similarity to the reference audio. For instance, edited audio that is perceptually close to the original audio clips may score well, even if the edit instruction was ignored or only partially followed. By including both types of references, we aim to capture both naturalness and edit accuracy of generated outputs. 5.1 Ablation To identify the effectiveness of the different dataset generation techniques, we fine-tune Stable Audio Open separately on datasets generated using Prompt-to-Prompt, DDPM inversion, and manual edits, each consisting of 50k samples. Additionally, we evaluate two combined variants: one with 50k samples with equal contribution from each of the three generation methods, and larger version containing 150k samples with 50k samples from each method. Results. The results of this ablation study are shown in Table 2. Edits from SAO-Instruct fine-tuned on Prompt-to-Prompt showcase high similarity with the regenerated reference. This indicates that the performed edits may be more accurate, while being less faithful to the original qualities of the input audio. In contrast, fine-tuning on DDPM inversion and manual edits, which are built on partial or fully real audio, may produce edits that are less precise while better preserving the characteristics and quality of the input audio. Overall, the combined approaches perform robustly across all metrics, balancing the advantages from the individual datasets. 8 Table 3: Comparison with zero-shot audio editing baselines. Results (mean standard deviation) are shown for applicable objective and all subjective metrics. Inf. denotes the inference time per sample in seconds, measured on single NVIDIA A6000 GPU with batch size of 1. Original Ref. Regenerated Ref. Subjective Metrics Models Inf. (s) FD LSD KL FD LSD KL IS CLAP Quality Relevance Faithfulness AudioEditor ZETATstart=50 ZETATstart=75 SAO-Instruct 79.49 15.31 17.78 9.94 17.21 1.73 24.65 2.27 27.91 2.69 18.38 1.36 1.40 25.97 2.32 1.64 17.01 2.55 1.86 18.88 2.59 0.93 18.97 2.72 1.46 10.010.60 0.480.12 3.221.01 3.331.35 9.060.67 0.380.13 3.560.83 3.251.25 1.26 9.070.72 0.360.13 3.281.00 3.041.24 1.36 7.591.00 0.380.14 3.540.93 3.831.00 1. 2.750.99 2.951.06 2.751.12 3.990.74 5.2 Comparison with Baselines We compare SAO-Instruct with prior zero-shot audio editing baselines. The ZETA baseline [33] uses Stable Audio Open with 100 denoising steps as the underlying generative model, which was trained on FreeSound and the Free Music Archive (FMA) [6]. We evaluate two variants by adjusting the edit strength via the Tstart parameter, set to 50 and 75. ZETA has access to the full original and target audio description. We also compare our model to AudioEditor [20], which uses Auffusion [46] as its underlying model trained on several audio datasets, including AudioCaps [22], WavCaps [35], and MACS [34]. Depending on the edit type, AudioEditor conditions on either the original or target description, along with the indices of words that changed between the descriptions. SAO-Instruct is trained on combined dataset of 150k audio editing triplets, consisting of 50k samples each from Prompt-to-Prompt, DDPM inversion, and manual edits. Unlike the baselines, SAO-Instruct only has access to the edit instruction, conveying significantly less information than full audio descriptions. Results. The results of the comparison with baselines are shown in Table 3. SAO-Instruct has the fastest inference time, editing audio in just under 10 seconds, making it nearly 8x faster than AudioEditor. On the FD, LSD, and KL metrics, the models perform similarly, with no model dominating across both reference proxies. AudioEditor has the highest CLAP and Inception scores by significant margin, likely due to its additional information available as the full target audio caption and longer inference time. However, this also leads to more aggressive edits that reduce the similarity with the original audio clip, reflected in its lower faithfulness score. SAO-Instruct shows strong performance in the subjective listening test and outperforms the other models in both edit relevance and faithfulness to the original audio, while maintaining high audio quality. Using only free-form instruction, SAO-Instruct demonstrates competitive performance and, in several cases, exceeds the results of models conditioned on full audio descriptions. 5.3 Limitations While SAO-Instruct shows promising results, some limitations remain. The generation process for our Prompt-to-Prompt and DDPM inversion datasets is computationally expensive and constrained by the capabilities of the underlying generative model. Examples of failure cases are shown in Section G.3. While our work focuses on editing general audio, our approach could be applied to music editing, provided appropriate generative music models exist and triplets are constructed from music datasets. Future work could explore handling multi-step edits and extending support to languages other than English. See Section for discussion on broader impacts."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce SAO-Instruct, the first fully free-form instruction-based audio editing model. SAOInstruct can perform wide-range of edit instructions, while preserving the overall context and coherence of the provided input audio. We propose novel data generation pipeline utilizing Promptto-Prompt [16], DDPM inversion [33], and manual edits [44] to create diverse dataset of audio editing triplets. Our evaluations show that SAO-Instruct outperforms existing zero-shot editing baselines in subjective listening tests and achieves competitive performance on objective metrics. While prior baselines require information from both the input and target audio captions to guide the editing process, SAO-Instruct requires only single free-form edit instruction. We believe free-form instruction-based audio editing unlocks new possibilities for creative audio workflows and hope that our released model weights and code will encourage further research in this area."
        },
        {
            "title": "References",
            "content": "[1] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, et al. MusicLM: Generating music from text. arXiv preprint arXiv:2301.11325, 2023. URL https://arxiv.org/abs/2301.11325. [2] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama. Optuna: next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019. [3] T. Brooks, A. Holynski, and A. A. Efros. InstructPix2Pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [4] S. Chen, S. Liu, L. Zhou, Y. Liu, X. Tan, J. Li, S. Zhao, Y. Qian, and F. Wei. VALL-E 2: neural codec language models are human parity zero-shot text to speech synthesizers. arXiv preprint arXiv:2406.05370, 2024. [5] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Defossez. Simple and controllable music generation. In Advances in Neural Information Processing Systems, volume 36, pages 4770447720. Curran Associates, Inc., 2023. [6] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson. FMA: dataset for music analysis. In International Society for Music Information Retrieval Conference, 2017. [7] A. Défossez, N. Usunier, L. Bottou, and F. Bach. Demucs: Deep extractor for music sources with extra unlabeled data remixed. arXiv preprint arXiv:1909.01174, 2019. [8] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever. Jukebox: generative model for music. arXiv preprint arXiv:2005.00341, 2020. [9] Z. Evans, J. D. Parker, C. Carr, Z. Zukowski, J. Taylor, and J. Pons. Stable Audio Open, 2024. URL https://arxiv.org/abs/2407.14358. [10] S. Forsgren and H. Martiros. Riffusion - stable diffusion for real-time music generation. 2022. URL https://riffusion.com/about. [11] R. Gal, O. Patashnik, H. Maron, A. H. Bermano, G. Chechik, and D. Cohen-Or. StyleGANNADA: Clip-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):113, 2022. [12] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter. AudioSet: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. [13] J. Hai, Y. Xu, H. Zhang, C. Li, H. Wang, M. Elhilali, and D. Yu. EzAudio: Enhancing text-toaudio generation with efficient diffusion transformer, 2024. URL https://arxiv.org/abs/ 2409.10819. [14] B. Han, J. Dai, W. Hao, X. He, D. Guo, J. Chen, Y. Wang, Y. Qian, and X. Song. InstructME: An instruction guided music edit and remix framework with latent diffusion models. arXiv preprint arXiv:2308.14360, 2023. [15] S. Hershey, D. P. Ellis, E. Fonseca, A. Jansen, C. Liu, R. C. Moore, and M. Plakal. The benefit of temporally-strong labels in audio event classification. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 366370. IEEE, 2021. [16] A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-or. Prompt-toPrompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023. [17] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 10 [18] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [19] I. Huberman-Spiegelglas, V. Kulikov, and T. Michaeli. An edit friendly DDPM noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1246912478, 2024. [20] Y. Jia, Y. Chen, J. Zhao, S. Zhao, W. Zeng, Y. Chen, and Y. Qin. AudioEditor: training-free diffusion-based audio editing framework. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [21] Z. Ju, Y. Wang, K. Shen, X. Tan, D. Xin, D. Yang, Y. Liu, Y. Leng, K. Song, S. Tang, Z. Wu, T. Qin, X.-Y. Li, W. Ye, S. Zhang, J. Bian, L. He, J. Li, and S. Zhao. NaturalSpeech 3: Zero-shot speech synthesis with factorized codec and diffusion models, 2024. URL https: //arxiv.org/abs/2403.03100. [22] C. D. Kim, B. Kim, H. Lee, and G. Kim. AudioCaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, 2019. [23] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley. PANNs: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:28802894, 2020. [24] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. Défossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi. AudioGen: Textually guided audio generation. In The Eleventh International Conference on Learning Representations, 2023. [25] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar. High-fidelity audio compression with improved RVQGAN. Advances in Neural Information Processing Systems, 36:27980 27993, 2023. [26] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems, 36, 2024. [27] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey. SDRhalf-baked or well done? In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 626630. IEEE, 2019. [28] S.-g. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon. BigVGAN: universal neural vocoder with large-scale training. arXiv preprint arXiv:2206.04658, 2022. [29] Y. A. Li, C. Han, V. Raghavan, G. Mischler, and N. Mesgarani. StyleTTS 2: towards humanlevel text-to-speech through style diffusion and adversarial training with large speech language models. Advances in Neural Information Processing Systems, 36:1959419621, 2023. [30] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2145021474. PMLR, 2023. [31] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [32] Y. Luo and N. Mesgarani. TasNet: time-domain audio separation network for real-time, singlechannel speech separation. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 696700. IEEE, 2018. [33] H. Manor and T. Michaeli. Zero-shot unsupervised and text-based audio editing using DDPM inversion. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 3460334629. PMLR, 2127 Jul 2024. 11 [34] I. Martín-Morató and A. Mesaros. What is the ground truth? reliability of multi-annotator data for audio tagging. In 2021 29th European Signal Processing Conference (EUSIPCO), pages 7680. IEEE, 2021. [35] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, and W. Wang. WavCaps: chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. [36] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [37] R. Mokady, A. Hertz, K. Aberman, Y. Pritch, and D. Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60386047, 2023. [38] W. Peebles and S. Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 41954205, October 2023. [39] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training GANs. Advances in neural information processing systems, 29, 2016. [40] U.-H. Shin, S. Lee, T. Kim, and H.-M. Park. Separate and reconstruct: Asymmetric encoderdecoder for speech separation. Advances in Neural Information Processing Systems, 37: 5221552240, 2024. [41] A. Tjandra, Y.-C. Wu, B. Guo, J. Hoffman, B. Ellis, A. Vyas, B. Shi, S. Chen, M. Le, N. Zacharov, et al. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv preprint arXiv:2502.05139, 2025. [42] R. Valle, R. Badlani, Z. Kong, S.-g. Lee, A. Goel, S. Kim, J. F. Santos, S. Dai, S. Gururani, A. Aljafari, et al. Fugatto 1: Foundational generative audio transformer opus 1. In The Thirteenth International Conference on Learning Representations, 2025. [43] A. Vyas, B. Shi, M. Le, A. Tjandra, Y.-C. Wu, B. Guo, J. Zhang, X. Zhang, R. Adkins, W. Ngan, et al. Audiobox: Unified audio generation with natural language prompts. arXiv preprint arXiv:2312.15821, 2023. [44] Y. Wang, Z. Ju, X. Tan, L. He, Z. Wu, J. Bian, and S. Zhao. AUDIT: Audio editing by following instructions with latent diffusion models. In Advances in Neural Information Processing Systems, volume 36, pages 7134071357. Curran Associates, Inc., 2023. [45] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15, 2023. [46] J. Xue, Y. Deng, Y. Gao, and Y. Li. Auffusion: Leveraging the power of diffusion and large language models for text-to-audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. [47] R. Yamamoto, E. Song, and J.-M. Kim. Parallel WaveGAN: fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In ICASSP 20202020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 61996203. IEEE, 2020."
        },
        {
            "title": "A Prompt Generation",
            "content": "Figure 4: Pipeline for prompt generation. caption is taken from dataset and passed to an LLM, which produces an edit instruction and corresponding output caption. Additional metadata is generated for downstream filtering and for improving sample quality for synthetic audio generation. Generation and Filtering. As illustrated in Fig. 4, we use GPT-4o to create dataset of structured prompts. Before generating outputs, the LLM reasons about the provided input caption to identify all audible elements and suitable edit instruction. The LLM also generates additional metadata, including count of distinct audio elements of the input caption. Captions vary in complexity, with some containing only single audible element (A cat meows), while others may have multiple distinct elements, such as man speaks during heavy traffic with emergency sirens going by. Current generative audio models often struggle to accurately synthesize these complex captions, which can result in incomplete generations, with parts of the caption missing, or outputs with substantial audible artifacts. To address this, we filter out prompts that contain more than two distinct audio elements. Table 4: Examples of prompt generation. The input caption is taken from an existing captioning dataset, while the rest is generated using an LLM. The reasoning step is not shown for brevity. Input Caption Instruction Output Caption Elem. Neg. Input Neg. Output man speaks as birds chirp Remove the birds chirping man speaks Thunder and rain add distant wind Thunder and rain with distant wind car accelerates Change it to motorcycle motorcycle accelerates plane takes off it should be further away plane takes off in the distance 2 1 1 birds chirp distant wind motorcycle car Negative Prompts. Depending on the edit instruction, the LLM also generates optional negative captions, which contain descriptions of audio elements that should be excluded from the input and output. These negative prompts help improve sample quality during synthetic audio generation by ensuring that elements are properly removed or added. For example, given the input caption man speaks as birds chirp, the model generates the edit instruction Remove the birds chirping (cf. Table 4). Since this instruction removes sound, the negative input caption is empty, while the negative output caption contains birds chirp. If the edit instruction instead adds new sound, the negative input caption would contain that new element, while the negative output caption remains empty. Both negative input and output prompts are used during synthetic sample generation with Prompt-to-Prompt. For DDPM inversion, only the negative output prompt is used, since the input audio is not generated."
        },
        {
            "title": "B Manual Edit Tasks",
            "content": "We define twelve editing tasks in total: ADD, REPLACE, DROP, SWAP, LOOP, PITCH, SPEED, LOW_PASS, HIGH_PASS, INPAINT, SUPER_RES, and DENOISE. These tasks differ in the number of inputs they require, the constraints on those inputs, and whether they accept controllable parameters. 13 Table 5: The four editing tasks that accept parameters. The function len() returns the audio length in seconds. Task/Inputs Parameter Description and Constraints Example Instruction ADD/2 Position Mixes target audio into base audio. ℓ = len(target) = len(base) {start, middle, end} [0, ℓ] Add the sound of barking dog to the beginning of the street ambience. LOOP/ Count PITCH/1 Semitones SPEED/1 Factor Repeats an audio multiple times. Z>0 st. len(result) 47s Shifts pitch up or down. [12, 12] Changes speed without affecting pitch. LogUnif (1/3, 3) len(result) 47s Repeat five times. Make the voice sound deeper by three notes. Slow this clip down by about 30 percent. Input Constraints. While some tasks operate on multiple audio clips, others only require one. For example, ADD overlays two audio clips, while HIGH_PASS processes single clip by removing its low-frequency region. In addition to the number of inputs, each task may also impose constraints based on the relative length of the inputs or the number of distinct audio elements as analyzed in Section 3.1. These constraints ensure that the edited audio does not exceed the 47 second limitation of Stable Audio Open and avoids producing samples that contain an excessive number of distinct audio elements. For instance, DROP is configured to only remove single element from composite audio. Parameters. Tasks can be separated into two groups based on whether they accept controllable parameters. The tasks ADD, LOOP, PITCH, and SPEED each accept one controllable parameter as detailed in Table 5. The values of these parameters are all numerical, except for ADD, which accepts the insertion location either as floating point timestamp or as one of the keywords start, middle, or end, which are automatically converted into their corresponding timestamp values. These parameters also come with some restrictions. For example, the insertion location of the ADD tasks cannot insert the target audio before the start or beyond the length of the base audio, while the pitch shift of the PITCH tasks is restricted to full octave in either direction. For tasks such as LOOP that change the length of given audio clip, the number of loops is constrained to ensure that the final audio does not exceed 47 seconds. The parameter values are sampled uniformly, unless stated otherwise, within the allowed values. The REPLACE, DROP, SWAP, LOW_PASS, HIGH_PASS, INPAINT, SUPER_RES, and DENOISE tasks do not accept parameters and are outlined in Table 6. For example, HIGH_PASS simply removes the low-frequency region of given audio."
        },
        {
            "title": "C Dataset Generation",
            "content": "We outline the computational resources used for generating the dataset of audio editing triplets via the three approaches introduced in Section 3. While the dataset was generated using various GPUs, we report the average inference times based on single NVIDIA A6000 GPU for consistency. For the Prompt-to-Prompt dataset, we generate seven candidate pairs to find suitable seed and CFG values and filter them using Gemini and CLAP. For each prompt pair, this step takes on average 2.2 minutes. After finding suitable candidate, we perform the Prompt-to-Prompt sample generation using Bayesian Optimization with 10 trials, which takes approximately 3.4 minutes per audio pair. For DDPM inversion, we use 7 Bayesian Optimization trials to find the optimal inversion parameters, taking 1.8 minutes per audio pair. To generate 100k samples (50k from each method), we used total of 6.2k GPU hours. In contrast, generating manual edits is comparatively lightweight, as no GPUs are required. The full pipeline, including audio manipulation and generating the edit instruction, takes approximately 5.1 seconds per sample on single CPU core. 14 Table 6: The eight editing tasks that take no parameters. The function len() returns the audio length in seconds and elem() counts the number of audible elements as generated in Section 3.1. Task/Inputs REPLACE/3 DROP/ SWAP/2 LOW_PASS/1 HIGH_PASS/1 INPAINT/1 SUPER_RES/1 DENOISE/ Description and Constraints Example Instruction Swaps out one element (target) in composite audio (base + target) with another (replace). elem(target) = elem(base) = 1 len(target), len(replace) len(base) Drops sound (target) from composite audio (base + target). elem(target) = 1 len(target) len(base) Reorders two audio clips. elem(first) = elem(second) = 1 len(first) + len(second) 47s Removes high-frequency region. cutoff at 8000 Hz Removes low-frequency region. cutoff at 1000 Hz Fills in silent region. α U(0, 95) randomly blank out α% of input Replace the engine hum with the sound of propeller plane. Remove the rain sounds from this outdoor recording. Swap the order of sounds. these two Filter out the high-pitched noise from the recording. Remove the bass rumble from the audio. Restore the missing audio in the middle of this clip. Reconstruct high-frequency region. input is resampled to 1/4th of its sample rate Enhance the quality of this lowfrequency audio. Removes noise from the signal. gaussian noise (0, 0.01) is added to input Remove the background hiss from this audio. Table 7: Performance of Stable Audio Open (SAO) and its fine-tuned variant on AudioCaps. The AudioCaps test subset was used for evaluation. Models FD KL IS CLAP 8.560.47 0.260.14 SAO SAO + AudioCaps 20.85 1.42 10.050.51 0.460.11 41.64 2.19 Fine-tuning To fine-tune Stable Audio Open on both AudioCaps and our generated audio editing triplets, we follow the guidelines in the official repository 3 and adopt the default optimizer and inverse learning rate scheduler with exponential warmup. Specifically, the AdamW [31] optimizer is configured with learning rate of 5e 5, (β1, β2) = (0.9, 0.999), and weight decay of 1e 3. In both cases, we keep the autoencoder frozen and only train the diffusion transformer. Stable Audio Open on AudioCaps. To improve prompt adherence of Stable Audio Open, we fine-tune the model on both the training and validation splits of AudioCaps [22] using total of 47k samples. The model is trained for 15 epochs with batch size of 64 on two NVIDIA A100 GPUs for 30 hours. For evaluation, we provide the models with captions from the test subset of AudioCaps and generate audio using 100 denoising steps and CFG of 6. As shown Table 7, the fine-tuned version followed prompts more closely. SAO-Instruct. We train the diffusion transformer on two NVIDIA A6000 GPUs with batch size of 16 for 4 epochs. Models trained on the individual datasets (50k samples each) were trained for 30 hours, while the final model on the large combined dataset (150k samples) was trained for 80 hours. 3https://github.com/Stability-AI/stable-audio-tools 15 Table 8: Comparing two inference configurations of SAO-Instruct. Starting from an encoded input audio with added Gaussian noise preserves more input audio characteristics while still providing enough editing flexibility. Original Ref. Regenerated Ref. Initial Latent FD LSD KL FD LSD KL IS CLAP Pure Noise 29.73 2.23 Audio + Noise 18.38 1. 2.59 19.17 2.61 0.93 18.97 2.72 2.27 8.100.69 0.320.15 1.76 7.591.00 0.380.14 Table 9: Comparison with zero-shot audio editing baselines on Production Quality (PQ) and Production Complexity (PC) from AudioBox Aesthetics [41]. Model PQ PC AudioEditor ZETATstart=50 ZETATstart=75 SAO-Instruct 5.380.91 6.060.94 6.040.91 5.610.89 3.020.72 2.760.69 2.730.66 3.230."
        },
        {
            "title": "E Inference",
            "content": "We compare two inference configurations for SAO-Instruct in Table 8: sampling from pure noise and sampling from the Gaussian-noised latent of the encoded input audio. Starting from the noised latent substantially improves metrics computed against the original audio clips, which indicates better preservation of the input audios characteristics. It also achieves higher CLAP score and lower FD/KL relative to the target caption-conditioned regenerated reference, showing accurate instruction following. Overall, sampling from the noised latent preserves more input audio characteristics while still providing enough flexibility to perform the required edits."
        },
        {
            "title": "F Listening Study",
            "content": "The evaluation interface for the subjective listening study is shown in Fig. 5. The 13 participants were volunteers and received no compensation. They were given the following instructions to rate the model outputs: Quality: How good is the sound quality of the edited audio compared to the input? 1 = Poor quality with strong artifacts, 5 = Same quality as the input audio Relevance: How well does the edited audio match the given instruction? 1 = Completely irrelevant to instruction, 5 = Perfectly follows instruction Faithfulness: How similar does the edited audio sound to the input audio? 1 = Completely different from the input audio, 5 = Same as input audio"
        },
        {
            "title": "G Results",
            "content": "G.1 Comparison with Baselines To compare our model with zero-shot editing baselines, we also evaluate on objective metrics designed for automatic quality assessment of audio. Production Quality (PQ) measures technical aspects of audio quality, while Production Complexity (PC) focuses on the complexity of the audio scene [41]. The results are shown in Table 9. We observe that SAO-Instruct slightly underperforms ZETA in production quality, while slightly outperforming ZETA in production complexity. These results reflect the findings from our subjective listening study. 16 Figure 5: Evaluation interface for the subjective listening study comparing SAO-Instruct with audio editing baselines. G.2 Deterministic Editing We compare SAO-Instruct with the baselines on deterministic manual editing tasks using 100 samples per edit type drawn from the AudioCaps test set. As evaluation metrics, we provide the STFT loss, the Multi-Resolution STFT loss (MR-STFT) [47], the Multi-Resolution Mel-Spectrogram loss (MR-MEL) [25], the Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) [27], and the Scaleinvariant Signal-to-Noise Ratio (SI-SNR) [27, 32]. Together, these metrics provide perceptualand signal-level view of the performance on these tasks. The input caption, edit instruction, and target captions were generated using an LLM based on the selected edit task, its optional parameter, and the captions from the selected input audio clips from AudioCaps. The edit types ADD and REPLACE are not evaluated as their target audio is ambiguous and not deterministic. For all other edit types the input audio and target audio can be created similarly as during dataset generation (cf. Section 3.4) and evaluated using common timeand frequency-based metrics [7, 40, 25]. summary of the results averaged over all deterministic manual editing tasks is given in Table 10, while the full per-task results are shown in Table 11. G.3 Qualitative Results Fig. 6 demonstrates the editing capabilities of SAO-Instruct across range of instruction types. In the first example, ambient noise is introduced in speech recording without distorting the primary signal. The second example shows global transformation where the volume of rain is reduced. The 17 Table 10: Average performance across all deterministic manual editing tasks. Model STFT MR-STFT MR-MEL SI-SDR SI-SNR AudioEditor 6.064.16 6.024.20 7.532.54 -54.5511.74 -54.5011.57 ZETATstart=50 4.733.43 4.653.53 5.612.12 -28.7513.68 -28.8913.65 ZETATstart=75 5.214.41 5.134.51 5.912.19 -30.6514.85 -30.6714.83 SAO-Instruct 2.911.82 2.821.83 3.471.33 -21.6212.81 -21.7912.65 third example shows more localized operation, in which the sound of crumpling paper is replaced with typing, while the original speech remains intact. Notably, the model performs all edits based solely on the input audio and free-form edit instruction, with no access to either the original or target audio captions. Figure 6: Edits performed by SAO-Instruct. The model has only access to the input audio and the edit instruction. It is able to perform global operations and local operations while keeping the overall background context intact. Failure Cases While the performance of SAO-Instruct can be further improved by per-sample adjustments, such as tuning the CFG scale or the amount of noise applied to the initial encoded audio, some limitations remain. In Fig. 7, we observe that the phrasing of edit instruction can influence the edit quality and accuracy of the model. The model also occasionally struggles to reconstruct coherent speech and may produce edits with audible artifacts. In Fig. 8, when adding elements, the newly added sounds sometimes fail to naturally blend in with the background and instead appear overlaid on existing sound elements. Additionally, if clip contains many distinct elements, the model may be unable to alter sounds or confuses them, leading to unintended edits. These limitations primarily stem from insufficient data diversity and could be mitigated by training on larger and more diverse datasets."
        },
        {
            "title": "H Broader Impacts",
            "content": "Our work introduces SAO-Instruct, model that enables free-form instruction-based audio editing. It enables an intuitive and accessible way for users to edit audio using natural language instructions. However, it also introduces potential risks of misuse, such as the manipulation of audio clips for deceptive or harmful purposes. To mitigate these concerns, we do not scrape data from arbitrary sources and only use well-established datasets that have been widely adopted in prior research. We encourage future work to further explore responsible deployment practices for instruction-based audio models and methods for detecting synthetically edited audio. 18 Table 11: Performance on all deterministic manual editing tasks. Task Model STFT MR-STFT MR-MEL SI-SDR SI-SNR DROP SWAP LOOP PITCH SPEED AudioEditor 6.596.63 6.586.69 6.372.67 -51.249.26 -51.209.28 ZETATstart=50 5.587.22 5.577.63 5.512.17 -20.2116.19 -20.3816.56 ZETATstart=75 6.278.88 6.299.41 5.932.31 -23.2818.21 -23.0617.93 SAO-Instruct 6.1211.63 6.0611.68 4.443.08 -13.7716.86 -13.6916.75 AudioEditor 6.221.13 6.071.11 11.782.71 -60.7013.89 -59.9413.77 ZETATstart=50 4.391.34 4.291.31 6.702.30 -57.5210.98 -58.0110.28 ZETATstart=75 4.561.58 4.461.56 6.451.72 -56.459.86 -56.539.65 SAO-Instruct 4.381.05 4.251.04 7.322.17 -54.1911.35 -54.3011.36 AudioEditor 6.342.23 6.212.19 10.393.90 -55.3112.19 -55.1911.99 ZETATstart=50 4.701.85 4.551.83 8.013.41 -18.1312.79 -18.1012.75 ZETATstart=75 5.112.29 4.952.27 8.263.34 -19.6114.10 -19.5814.28 SAO-Instruct 2.010.58 1.940.58 2.360.93 -11.9113.78 -11.8913.76 AudioEditor 5.734.04 5.734.20 5.791.97 -51.7311.31 -51.4010.97 ZETATstart=50 5.447.75 5.428.32 4.781.55 -46.7813.71 -46.7913.79 ZETATstart=75 5.958.82 5.949.44 5.081.85 -48.6614.11 -48.5914.21 SAO-Instruct 2.630.70 2.490.71 3.110.96 -41.3914.74 -41.3114.42 AudioEditor 6.494.85 6.454.98 7.023.03 -50.9613.55 -50.6012.52 ZETATstart=50 4.942.82 4.832.83 5.842.44 -45.9313.81 -46.7513.66 ZETATstart=75 5.332.97 5.232.99 6.222.54 -47.2812.97 -47.3912.85 SAO-Instruct 3.531.23 3.431.26 5.131.37 -45.1213.79 -46.1513. LOW_PASS AudioEditor 4.942.17 4.922.16 5.921.90 -53.4010.43 -53.9910.64 ZETATstart=50 3.411.27 3.321.27 4.871.76 -16.0713.30 -15.9913.30 ZETATstart=75 3.791.54 3.701.48 5.281.95 -17.7615.49 -17.7815.47 SAO-Instruct 1.630.45 1.560.47 1.890.59 -2.2610.95 -2.2810.95 HIGH_PASS AudioEditor 6.957.75 6.947.74 6.521.46 -62.2812.08 -62.2512.08 ZETATstart=50 4.893.42 4.823.43 5.151.77 -31.0512.92 -30.9813.01 ZETATstart=75 5.825.53 5.715.46 5.652.12 -34.6415.85 -34.2315.52 SAO-Instruct 1.750.38 1.680.40 2.080.59 -28.4812.66 -28.4812.66 INPAINT AudioEditor 6.503.80 6.433.91 9.814.40 -54.4012.39 -54.7111.92 ZETATstart=50 4.613.45 4.543.44 5.332.13 -22.3417.05 -22.5117.09 ZETATstart=75 5.626.32 5.536.28 5.131.74 -23.4916.82 -24.2117.06 SAO-Instruct 2.491.00 2.420.99 3.312.24 -13.9813.20 -14.6112. SUPER_RES AudioEditor 5.562.66 5.532.66 6.661.75 -52.8312.23 -52.9312.31 ZETATstart=50 4.513.02 4.433.02 4.681.63 -14.1911.74 -14.1811.75 ZETATstart=75 4.753.47 4.673.47 4.961.70 -16.9714.71 -16.9614.74 -1.679.69 SAO-Instruct 2.160.71 2.100.71 2.360.81 -1.689.70 DENOISE AudioEditor 5.306.38 5.296.38 5.001.66 -52.6510.11 -52.7910.24 ZETATstart=50 4.792.17 4.702.17 5.282.02 -15.3114.27 -15.1614.28 ZETATstart=75 4.912.70 4.832.70 6.102.59 -18.3716.37 -18.3416.62 SAO-Instruct 2.360.50 2.300.48 2.740.56 -3.3911.03 -3.5011.02 Figure 7: An example failure case where the phrasing of an instruction impacts edit quality and accuracy. While remove the alarm fails to suppress the alarm, the alarm should be silent! is more successful. Figure 8: Examples of failure cases where newly added sounds fail to blend naturally with the background and where edits in complex audio scenes lead to unintended edits."
        },
        {
            "title": "I Licenses",
            "content": "We provide the licenses of datasets and models that we build upon in our work below. We ensure that our use of these assets fully complies with their license and terms of use. AudioCaps [22] is released under the MIT License. AudioSet [12] and AudioSet-SL [15] are available under the CC BY 4.0 license. WavCaps [35] is available for academic use and includes audio data from multiple sources. We follow the licensing terms for the BBC Sound Effects 4 library and respect the licenses associated with each audio clip from FreeSound.5 We comply with the Stability AI Community License Agreement 6, which allows the use of Stable Audio Open [9] for research purposes. CLAP [45] is licensed under the Creative Commons CC0 1.0 Universal license, which allows unrestricted use and distribution. 4https://sound-effects.bbcrewind.co.uk/licensing 5https://freesound.org/help/faq/#licenses 6https://huggingface.co/stabilityai/stable-audio-open-1."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Korea University",
        "Seoul National University"
    ]
}