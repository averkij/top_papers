{
    "paper_title": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction",
    "authors": [
        "Huiwon Jang",
        "Sihyun Yu",
        "Jinwoo Shin",
        "Pieter Abbeel",
        "Younggyo Seo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once."
        },
        {
            "title": "Start",
            "content": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction Huiwon Jang1 Sihyun Yu1 Jinwoo Shin1 Pieter Abbeel2 Younggyo Seo 1KAIST 2 UC Berkeley 4 2 0 2 2 2 ] . [ 1 2 6 7 4 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Efficient tokenization of videos remains challenge in training vision models that can process long videos. One promising direction is to develop tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, video tokenizer that learns mapping from coordinatebased representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes video into factorized triplane representations and reconstructs patches that correspond to randomly sampled (x, y, t) coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode 128-frame video with 128128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of diffusion transformer that can generate 128 frames at once. 1. Introduction Efficient tokenization of videos remains challenge in developing vision models that can process long videos. While recent video tokenizers have achieved higher compression ratios [1, 2, 9, 52, 61, 62] compared to using image tokenizers for videos (i.e., frame-wise compression) [43, 67], the vast scale of video data still requires us to design more efficient video tokenizer. One promising direction for efficient video tokenization is enabling video tokenizers to exploit the temporal coherence of videos. For instance, video codecs [26, 28, 32, 41] extensively utilize such coherence for video compression by Project website: huiwon-jang.github.io/coordtok Correspondence to mail@younggyo.me. extracting keyframes and encoding the difference between them. In fact, there have been several recent works based on similar intuition that train tokenizer to encode videos into factorized representations [19, 64, 65]. However, key limitation is that existing tokenizers are typically trained to encode short video clips because of high training cost, but it is more likely that tokenizers can better exploit the temporal coherence when they are trained on longer videos. For instance, because tokenizers are trained to reconstruct all the frames at once, their training cost increases linearly with the length of videos (see Figure 1a). This makes it difficult to train tokenizers that can encode long videos and thus capture the temporal coherence of videos (see Figure 1b). In this paper, we aim to design video tokenizer that can be easily scaled up to encode long videos. To this end, we draw inspiration from recent works that have successfully trained large 3D generative models in compute-efficient manner [16, 17, 22, 27]. Their key idea is to train model that learns mapping from randomly sampled (x, y, z) coordinates to RGB and density values instead of training with all the possible coordinates at once. In particular, we ask: can we utilize similar idea to design scalable video tokenizer? Actually, there have been recent studies that formulate the video reconstruction as problem of learning the mapping from (x, y, t) coordinates to RGB values [6, 20]. However, they rather focus on compressing each individual video instead of training video tokenizer that can encode diverse set of videos. We introduce CoordTok: Coordinate-based patch reconstruction for long video Tokenization, scalable video tokenizer that learns mapping from coordinate-based representations to the corresponding patches of input videos. The key idea of CoordTok is to encode video into factorized triplane representations [20, 64] and reconstruct patches that correspond to randomly sampled (x, y, t) coordinates (see Figure 2). This enables the training of large tokenizers directly on long videos without excessive memory and computational requirements (see Figure 1a). To investigate whether training video tokenizer on long video clips indeed leads to more efficient tokenization, we compare CoordTok with other baselines [9, 50, 61, 64] on the UCF-101 dataset [40]. Our experiments show that, by 1 (a) Maximum batch-size when training video tokenizers on 128 128 resolution videos with varying lengths, measured with single NVIDIA 4090 24GB GPU. (b) Inter-clip reconstruction consistency of video tokenizers. Existing video tokenizers [9, 50, 64] show the pixel-value inconsistency between short clips (16 frames). In contrast, Our tokenizer shows the temporally consistent reconstruction. Figure 1. Limitation of existing video tokenizers. (a) Existing video tokenizers [9, 50, 64] are often not scalable to long videos because of excessive memory and computational demands. This is because they are trained to reconstruct all video frames at once, i.e., giant 3D array of pixels, which incurs huge computation and memory burden in training especially when trained on long videos. For instance, PVDM-AE [64] becomes out-of-memory when trained to encode 128-frame videos when using single NVIDIA 4090 24GB GPU. (b) As result, existing tokenizers are typically trained to encode up to 16-frame videos and struggle to capture the temporal coherence of videos. exploiting the temporal coherence of videos, CoordTok significantly reduces the number of tokens for encoding long videos compared to baselines. For instance, CoordTok encodes 128-frame video with 128128 resolution into only 1280 tokens, while baselines require 6144 or 8192 tokens to achieve similar encoding quality. We also show that efficient tokenization with CoordTok enables memory-efficient training of diffusion transformer [25, 29] that can generate 128-frame video at once. Finally, we provide an extensive analysis on the effect of various design choices. We summarize the contributions of this paper below: We introduce CoordTok, scalable video tokenizer that learns mapping from coordinate-based representations to the corresponding patches of input videos. We show that CoordTok can leverage the temporal coherence of videos for tokenization, drastically reducing the number of tokens required for encoding long videos. We show that efficient video tokenization with CoordTok enables memory-efficient training of diffusion transformer [25, 29] that can generate long videos at once. 2. Method In this section, we present CoordTok, scalable video tokenizer that can efficiently encode long videos. In nutshell, CoordTok encodes video into factorized triplane representations [20, 64] and learns mapping from randomly sampled (x, y, t) coordinates to pixels from the corresponding patches. We provide the overview of CoordTok in Figure 2. Problem setup Let be video and be dataset consisting of videos. Our goal is to train video tokenizer that encodes video into tokens (or low-dimensional latent vector) and decodes into x. In particular, we want the tokenizer to be efficient so that it can encode videos into fewer number of tokens as possible but still can decode tokens to the original video without loss of information. 2.1. Encoder Given video x, we divide the video into non-overlapping space-time patches. We then add learnable positional embeddings and process them through series of transformer layers [48] to obtain video features e. We further encode video features into factorized triplane representations [4, 64], i.e., = [zxy, zyt, zxt], where zxy captures the global content in across time (e.g., layout and appearance of the scene or object), zyt and zxt capture the underlying motion in across two spatial axes (see Figure 9 for visualization). This design is efficient because it represents video with three 2D latent planes instead of 3D latents widely used in prior approaches [9, 52, 61]. 0 , zxt We implement our encoder based on the memoryefficient design of recent 3D generation work [16] that introduces learnable embeddings and translates them to triplane representations. Specifically, we first introduce learn0 , zyt able embeddings z0 = [zxy 0 ]. We then process them through series of cross-self attention layers, where each layer consists of (i) cross-attention layer that attends to the video features and (ii) self-attention layer that attends to its own features. In practice, we split each learnable embedding into four smaller equal-sized embeddings. We then use them as inputs to the cross-self encoder, because we find it helps the model to use more computation by increasing the length of input sequence. Finally, we project the outputs into triplane representations to obtain = [zxy, zyt, zxt]. 2 Figure 2. Overview of CoordTok. We design our encoder to encode video into factorized triplane representations = [zxy, zyt, zxt] which can efficiently represent the video with three 2D latent planes. Given the triplane representations z, our decoder learns mapping from (x, y, t) coordinates to RGB pixels within the corresponding patches. In particular, we extract coordinate-based representations of sampled coordinates by querying the coordinates from triplane representations via bilinear interpolation. Then the decoder aggregates and fuses information from different coordinates with self-attention layers and project outputs into corresponding patches. This design enables us to train tokenizers on long videos in compute-efficient manner by avoiding reconstruction of entire frames at once. 2.2. Decoder Given the triplane representation = [zxy, zyt, zxt], we implement our decoder to learn mapping from (x, y, t) coordinate to the pixels of corresponding patch. We first divide the video into non-overlapping space-time patches. We note that the configuration of patches, e.g., patch sizes, may differ from the one used in the video encoder. We then randomly sample (x, y, t) coordinates by sampling patches and extracting the center coordinates of sampled patches. We find that sampling only 3% of video patches can achieve strong performance (see Table 4 for the effect of sampling). As inputs to the decoder, we use coordinate-based representations that are obtained by querying each coordinate from triplane representations via bilinear interpolation. Specifically, let (i, j, k) be one of sampled coordinates. We extract hxy by querying (i, j) from zxy, hyt by querying (j, k) from zyt, and hxt by querying (i, k) from zxt. We concatenate them to get coordinate-based representations h. Given coordinate-based representations [h1, ..., hN ], our decoder processes them through series of selfattention layers, enabling each hi to attend to other representations hj. This allows the decoder to aggregate and fuse the information from different coordinates. We then use linear projection layer to process the output from each hi to pixels of the corresponding patch. Finally, we update the parameters of our encoder and decoder to minimize an ℓ2 loss between the reconstructed pixels and original pixels. To further improve the quality of reconstructed videos, we introduce an additional fine-tuning phase where we train our tokenizer with both ℓ2 loss and LPIPS loss [66]. Specifically, instead of sampling coordinates, we randomly sample few frames and use all coordinates within the sampled frames for fine-tuning. This enables the tokenizer to compute and minimize LPIPS loss, which requires reconstructing the entire frame. While we find that sampling frames instead of coordinates from the beginning of the training is harmful due to the lack of diversity in training data (see Table 4), we find that fine-tuning with sampled frames improves the quality of reconstructed videos. 3. Experiments We design experiments to investigate following questions: Can CoordTok efficiently encode long videos? Does encoding long videos lead to efficient video tokenization? (Figures 3 and 4 and Table 1) Can CoordTok learn meaningful tokens that can be used for downstream tasks such as video generation? (Figure 6 and Table 2) Can efficient video tokenization improve video generation models? (Table 3 and Figure 5) What is the effect of various design choices? (Figure 7 and Table 4) 3.1. Experimental Setup Implementation details We conduct all our experiments on the UCF-101 [40] dataset. Following the setup of prior works [9, 61], we use the train split of the UCF-101 dataset for training. For preprocessing videos, we resize and centercrop the frames to 128 128 resolution. We train our tokenizer using the AdamW optimizer [23] with batch size of 3 Figure 3. 128-frame, 128128 resolution video reconstruction results from CoordTok (Ours) and baselines [50, 64] trained on the UCF-101 dataset [40]. For each frame, we visualize the ground-truth (GT) and reconstructed pixels within the region highlighted in the red box, where CoordTok achieves noticeably better reconstruction quality than other baselines. setup of StyleGAN-V [37] that reports FVD measured with 2048 video clips. We provide more details of evaluation metrics in Appendix B. 3.2. Long video tokenization Setup To investigate whether training CoordTok to encode long videos at once leads to efficient tokenization, we consider setup where tokenizers encode 128-frame videos. Because existing tokenizers cannot encode such long videos at once, we split videos into multiple 16-frame video clips, use baseline tokenizers to encode each of them, and then concatenate the tokens from entire splits. For CoordTok, we train our tokenizer to encode 128-frame videos at once. We provide more details in Appendix A. Baselines We mainly consider tokenizers used in recent image or video generation models as our baselines. We first consider MaskGIT-AE [5], an image tokenizer, as our baseline to evaluate the benefit of using video tokenizers for encoding videos. Moreover, we consider PVDM-AE [64], which encodes video into factorized triplane representations and decodes all frames at once, as another baseline. Comparison with PVDM-AE enables us to evaluate the benefit of our decoder design because it shares the same latent structure with CoordTok. We further consider recent video tokenizers that encode videos into 3D latents, i.e., TATS-AE [9], MAGVIT-AE-L [61], and LARP [50], as our baselines. For fair comparison, we train all baselines from scratch on UCF-101 or use the model weights trained on UCF-101 following their official implementations. We provide more details of each baseline in Appendix C. Figure 4. CoordTok can efficiently encode long videos. rFVD scores of video tokenizers, evaluated on 128-frame videos, with respect to the token size. indicates lower values are better. 256, where each sample is randomly sampled 128-frame video. We use = 1024 coordinates for main training and = 4096 for fine-tuning. For the main experimental results, we train CoordTok for 1M iterations and further fine-tune it with LPIPS loss for 50k iterations. For analysis and ablation studies, we train CoordTok for 200k iterations and further fine-tune it with LPIPS loss for 10k iterations. For model configurations such as embedding dimension and number of layers, we mostly follow the architectures of vision transformers (ViTs; [7]). We provide more detailed implementation details in Appendix A. Evaluation For evaluating the quality of reconstructed videos, we follow the setup of MAGVIT [61] that reports reconstruction Frechet video distance (rFVD; [46]), peak signal-to-noise ratio (PSNR), LPIPS [66], and SSIM [53]. We use 10000 video clips of length 128 for evaluation. For evaluating the quality of generated videos, we follow the 4 Table 1. Frame-wise reconstruction quality of image and video tokenizers. We report metrics that measure the quality of reconstructed frames: PSNR, LPIPS, and SSIM, computed using the 128128 resolution frames reconstructed by image and video tokenizers trained on the UCF-101 dataset [40]. Total # tokens denotes the number of tokens required for encoding 128-frame videos. # Frames denotes number of frames in video used for training tokenizers. and denotes whether lower or higher values are better, respectively. Model configuration Reconstruction quality Method Token shape Total # tokens # Frames PSNR LPIPS SSIM MaskGIT-AE [5] 2D TATS-AE [9] MAGVIT-AE-L [61] LARP [50] PVDM-AE [64] CoordTok (Ours) 3D 3D 1D Triplane Triplane 8192 8192 8192 8192 6144 1280 16 16 16 16 128 21.4 23.2 21.8 24.3 26.5 28.6 0. 0.213 0.113 0.142 0.120 0.066 0.667 0.792 0.690 0.806 0.859 0.892 Table 3. Video generation efficiency. We report time (s) and memory (GB) required for synthesizing 128-frame video using single NVIDIA 4090 24GB GPU. We use the DDIM sampler [39] with 200 sampling steps for PVDM-L and HVDM and use the Euler-Maruyama sampler [25] with 250 sampling steps for our method. Table 2. FVDs of video generation models on the UCF-101 dataset (128-frame, 128128 resolution). indicates lower values are better. Method MoCoGAN [45] + StyleGAN2 [18] MoCoGAN-HD [44] DIGAN [63] StyleGAN-V [37] PVDM-L [64] HVDM [19] FVD 3679.0 2311.3 2606.5 2293.7 1773.4 505.0 549.7 Method TATS [9] LARP [50] PVDM-L [64] HVDM [19] Time (s) Mem (GB) 180.7 114.3 116.9 52.1 9. 9.8 3.1 4.0 3.9 4.5 Figure 5. Efficient video tokenization improves video generation. We report FVDs of SiT-L/2 models trained upon CoordTok with token sizes of 1280 and 3072. indicates lower values are better. CoordTok-SiT-L/2 (Ours) 369. CoordTok-SiT-L/2 (Ours) Results For qualitative evaluation, we provide videos reconstructed by CoordTok and other baseline tokenizers in Figure 3. We report the rFVD of CoordTok and other tokenizers with varying number of token sizes in Figure 4. Notably, we find that CoordTok efficiently encodes 128-frame videos into only 1280 tokens. In contrast, baselines achieve significantly worse reconstruction quality when they use similar number of tokens to CoordTok. For instance, CoordTok can encode 128-frame videos to 1280 tokens with rFVD score of 103, while PVDM-AE achieves >1000 rFVD score when using 1152 tokens. This highlights the benefit of our decoder design, which enables the tokenizer to exploit the temporal coherence of long videos better for efficient tokenization. Moreover, Table 1 shows CoordTok outperforms baseline tokenizers across diverse metrics that assess the quality of reconstructed frames. 3.3. Long video generation Setup To investigate whether CoordTok can encode long videos into meaningful tokens, we consider an unconditional video generation setup where we train model to produce 128-frame videos. Videos of length 128 are often considered too long to be generated at once, so several works use techniques such as iterative generation [64] for generating long videos. However, because CoordTok can efficiently encode long videos, we train our model to generate 128-frame videos at once. Specifically, we encode 128frame videos into 1280 tokens with CoordTok and train SiT-L/2 model [25], recent flow-based transformer model, for 600K iterations with batch size of 64. We then use the model to generate 128-frame videos using the EulerMaruyama sampler with 250 sampling steps. We provide more implementation details in Appendix A. Baselines We consider recent video generation models that can generate 128-frame videos as baselines, i.e., MoCoGAN [45], MoCoGAN-HD [44], DIGAN [63], StyleGANV [37], PVDM-L [64], and HVDM [19]. We provide more details of each baseline in Appendix C. Results Table 2 provides the quantitative evaluation of our model, i.e., CoordTok-SiT-L/2, and other video generation models. We find that CoordTok-SiT-L/2 achieves the best FVD score of 369.3, outperforming previous baselines. This is an intriguing result considering that CoordTok-SiTL/2 can generate 128-frame videos much faster than other 5 Figure 6. Unconditional 128-frame, 128128 resolution video generation results from CoordTok-SiT-L/2 trained on 128-frame videos from the UCF-101 dataset [40]. We provide more examples of generated videos in Appendix D. (a) Effect of Model size (b) Effect of Triplane size (spatial) (c) Effect of Triplane size (temporal) Figure 7. Analysis on the effect of (a) model size, (b) spatial dimensions of triplane representations, and (c) temporal dimensions of triplane representations. For our main experiments, we use CoordTok-L with triplane representations of 1616 spatial dimensions and 32 temporal dimensions. and denote whether lower or higher values are better, respectively. baselines, as shown in Table 3. Moreover, to investigate whether efficient video tokenization improves video generation, we evaluate the FVD scores of SiT-L/2 models trained with CoordTok using token sizes of 1280 and 3072. Figure 5 shows that SiT-L/2 trained with the token size of 1280 achieves consistently low FVD scores, even though there is no significant difference in the reconstruction quality of CoordTok with 1280 and 3072 tokens (see Appendix E). This is likely because the reduced number of tokens makes it easier to train the SiT model. For qualitative evaluation, we provide videos from CoordTok-SiT-L/2 in Figure 6. 3.4. Analysis and ablation studies Effect of model Size In Figure 7a, we investigate the scalability of CoordTok with respect to model sizes. We evaluate three variants of CoordTok: CoordTok-S, CoordTok-B, and CoordTok-L. Each variant has different size for the encoder and decoder (see Appendix for detailed model configurations). We find that the quality of reconstructed videos improves as the model size increases. For instance, CoordTok-B achieves PSNR of 25.2 while CoordTok-L achieves PSNR of 26.9. Effect of triplane size In Figure 7b and Figure 7c, we investigate the effect of spatial and temporal dimensions in triplane representations. We evaluate CoordTok with varying spatial dimensions, i.e., 1616, 3232, and 6464, and varying temporal dimensions, i.e., 8, 16, and 32. In general, we find that using larger planes improves the quality of reconstructed videos, as the model can better represent details within videos using more tokens. This result suggests there is trade-off between the number of tokens and the reconstruction quality. In practice, we find that reducing the spatial dimensions to 1616 while using high temporal dimension of 32 strikes good balance, achieving good quality of reconstructed videos with relatively low number of tokens. Effect of triplane representations We now examine the effect of one of our key design choices: encoding videos 6 (a) Effect of triplane representations. (b) Effect of coordinate-based representations Figure 8. Analysis on the effect of (a) triplane representations and (b) coordinate-based representations. (a) We measure the Pearson correlation between the reconstruction quality and dynamics metric that measures how dynamic each video is. video with larger dynamics magnitude indicates more dynamic video. We find that the correlation is stronger for CoordTok compared to TATS-AE [9] and MaskGIT-AE [5], which encode videos into 3D latents. We hypothesize this is because it is difficult to decompose dynamic videos into contents (zxy) and motions (zyt, zxt). (b) We measure the Pearson correlation between the reconstruction quality and frequency metric that measures the fineness of video details [58]. video with larger frequency magnitude indicates finer-grained video. In this case, we find that the correlation is weaker for CoordTok compared to other tokenizers. We hypothesize this is because CoordTok explicitly learns mapping from each coordinate-based representation to pixels within the corresponding patch. into triplane representations rather than 3D latents. We hypothesize that CoordTok may struggle to encode dynamic videos, as decomposing video to its content (zxy) and motion components (zyt, zxt) becomes difficult. To investigate this, in Figure 8a, we provide scatter plot where the x-axis represents metric for video dynamics and the axis represents the PSNR score. As metric for video dynamics, we use the mean ℓ2-distance between pixel values of consecutive frames (see Appendix for more details). As expected, we find that the correlation between reconstruction quality and the magnitude of dynamics is strong (-0.87) for CoordTok, compared to the weaker correlations for TATSAE (-0.40) and MaskGIT-AE (-0.59), both of which use 3D latent structures. This is one of the limitations of CoordTok, and addressing this by adopting techniques from video codecs, such as introducing multiple keyframes, could be an interesting future direction. Effect of coordinate-based representations We further examine the effect of our design that trains using coordinate-based representations. Our hypothesis is that the reconstruction quality of CoordTok is less sensitive to how fine-grained each video is, because CoordTok learns mapping from each coordinate to pixels. To investigate this, we measure the correlation between the PSNR score and frequency metric proposed in Yan et al. [58] that utilizes Sobel edge detection filter, where larger frequency magnitude indicates finer-grained video (see Appendix for details). As shown in Figure 8b, the correlation between reconstruction quality and the frequency metric is weak (- 0.37) for CoordTok, compared to stronger correlations for Table 4. Effect of sampling. We report rFVD and the maximum batch size (Max BS) measured with single NVIDIA 4090 24GB GPU, with different sampling schemes. Random patch uses center coordinates of randomly selected patches for training, while Random frame uses all coordinates from few randomly sampled frames for training. Ratio (%) indicates the proportion of sampled coordinates relative to all possible coordinates within video. indicates lower values are better. Sampling Ratio (%) rFVD Max BS Random frame Random patch Random patch 3.125 1.563 3.125 479 401 238 21 13 TATS-AE (-0.85) and MaskGIT-AE (-0.75). Effect of sampling We investigate two coordinate sampling schemes: (i) Random patch, which uses center coordinates of randomly sampled patches, and (ii) Random frame, which uses all coordinates from few randomly sampled frames. As shown in Table 4, Random patch outperforms Random frame when sampling the same number of coordinates. We hypothesize this is because Random frame fails to provide the tokenizer with sufficiently diverse training data. For instance, sampling 3.125% of video patches corresponds to sampling only 4 frames out of 128 in the Random frame scheme. In contrast, Random patch uniformly samples patches from all 128 frames, which helps provide more diverse training data. For Random patch, we find that sampling fewer coordinates reduces the training memory requirement but also degrades performance. 7 Figure 9. Illustration of factorized triplane representations = [zxy, zyt, zxt] of CoordTok trained on the UCF-101 dataset [40]. We note that zxy captures the global content in the video across time, e.g., layout and appearance of the scene or object, and zyt, zxt capture the underlying motion in the video across two spatial axes. 4. Related Work Video tokenization Many recent works have explored the idea of using video tokenizers to encode videos into lowdimensional latent tokens. Initial attempts proposed to directly use image tokenizers for videos [8, 31, 47] via framewise compression. However, this approach overlooks the temporal coherence of videos, resulting in inefficient compression. Thus, recent works have proposed to train tokenizer specialized for videos [1, 2, 9, 15, 51, 52, 56, 57, 59, 61, 62]. They typically extend image tokenizers by replacing spatial layers with spatiotemporal layers (e.g., 2D convolutional layers to 3D convolution layers). More recent works have introduced efficient tokenization schemes with careful consideration of redundancy in video data. For instance, several works proposed to encode videos into factorized triplane representations [19, 64, 65], and another line of works proposed an adaptive encoding scheme that utilizes the redundancy of videos for tokenization [50, 58]. However, they still train the tokenizer through reconstruction of entire video frames, so training is only possible with short video clips split from the original long videos. Our work introduces video tokenizer that can directly handle much longer video clips by removing the need for decoder to reconstruct entire video frames during training. By capturing the global information present in long videos, we show that our tokenizer achieves more effective tokenization. Latent video generation Instead of modeling distributions of complex and high-dimensional video pixels, most recent video generation models focus on learning the latent distribution induced by video tokenizers, as it can dramatically reduce memory and computation bottlenecks. One approach involves training autoregressive models [9, 21, 56] in discrete token space [31, 47]. Another line of research [49, 57, 60] also considers discrete latent space but has trained masked generative transformer (MaskGiT; [5]) for generative modeling. Finally, many recent works [1, 11, 13, 19, 24, 35, 64, 68] have trained diffusion models [14, 38] in continuous latent space, inspired by the success of latent diffusion models in the image domain [33]. Despite their efforts, the models are typically limited to processing only short video clips at time (usually 16-frame clips), which makes it difficult for the model to generate longer videos. In this paper, we significantly improve the limited contextual length of latent video generation models by introducing an efficient video tokenizer. 5. Conclusion In this paper, we have presented CoordTok, scalable video tokenizer that learns mapping from coordinate-based representations to the corresponding patches of input videos. CoordTok is built upon our intuition that training tokenizer directly on long videos would enable the tokenizer to leverage the temporal coherence of videos for efficient tokenization. Our experiments show that CoordTok can encode long videos using far fewer number of tokens than existing baselines. We also find that this efficient video tokenization enables memory-efficient training of video generation models that can generate long videos at once. We hope that our work further facilitates future researches on designing scalable video tokenizers and efficient video generation models. Limitations and future directions One limitation of our work is that our tokenizer struggles more with dynamic videos than with static videos, as shown in Figure 8. We hypothesize this is due to the difficulty of learning to decompose dynamic videos into global content and motion. One interesting future direction could involve introducing multiple content planes across the temporal dimension. Moreover, future work may introduce an adaptive method for deciding the number of such content planes based on how dynamic each video is, similar to techniques in video codecs [12, 28, 42, 54] or an adaptive encoding scheme designed for recent video tokenizer [58]. Lastly, we are excited about scaling up our tokenizer to longer videos from larger datasets and evaluating it on challenging downstream tasks such as long video understanding and generation."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 1, 8 [2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. OpenAI Blog, 2024. 1, 8 [3] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2 [4] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 2 [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. MaskGIT: Masked generative image transformer. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 4, 5, 7, 8, 3 [6] Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vidit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong Wang. VideoINR: Learning video implicit neural representation for continuous space-time super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 4 [8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In IEEE transformers for high-resolution image synthesis. Conference on Computer Vision and Pattern Recognition, 2021. 8, 3 [9] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In European Conference on Computer Vision, 2022. 1, 2, 3, 4, 5, 7, 8 [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014. 3 [11] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision, 2024. 8 [12] Jingning Han, Bohan Li, Debargha Mukherjee, Ching-Han Chiang, Adrian Grange, Cheng Chen, Hui Su, Sarah Parker, Sai Deng, Urvang Joshi, et al. technical overview of av1. Proceedings of the IEEE, 109(9):14351462, 2021. 8 [13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 8 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 8 [15] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale pretraining for text-tovideo generation via transformers. In International Conference on Learning Representations, 2023. 8 [16] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to In International Conference on Learning Representa3D. tions, 2024. 1, 2 [17] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. Shap-e: GeneratarXiv preprint [18] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 5, 3 [19] Kihong Kim, Haneol Lee, Jihye Park, Seyeon Kim, Kwanghee Lee, Seungryong Kim, and Jaejun Yoo. Hybrid video diffusion models with 2d triplane and 3d wavelet repIn European Conference on Computer Vision, resentation. 2024. 1, 5, 8, 3 [20] Subin Kim, Sihyun Yu, Jaeho Lee, and Jinwoo Shin. Scalable neural video representations with learnable positional features. In Advances in Neural Information Processing Systems, 2022. 1, 2 [21] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. In International Conference on Machine Learning, 2023. [22] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. In Advances in Neural Information Processing Systems, 2024. 1 [23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018. 3, 1, 2 [24] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: General-purpose video diffusion transformers via mask modeling. In International Conference on Learning Representations, 20244. 8 [25] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, 2024. 2, 5 9 [26] Detlev Marpe, Thomas Wiegand, and Gary Sullivan. The h. 264/mpeg4 advanced video coding standard and its applications. IEEE communications magazine, 2006. [27] Takeru Miyato, Bernhard Jaeger, Max Welling, and Andreas Geiger. GTA: geometry-aware attention mechanism for In International Conference on multi-view transformers. Learning Representations, 2024. 1 [28] Debargha Mukherjee, Jingning Han, Jim Bankoski, Ronald Bultje, Adrian Grange, John Koleszar, Paul Wilkins, and Yaowu Xu. technical overview of vp9the latest opensource video codec. SMPTE Motion Imaging Journal, 2015. 1, 8 [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE International Conference on Computer Vision, 2023. 2 [30] Prajit Ramachandran, Barret Zoph, and Quoc Le. arXiv preprint Searching for activation functions. arXiv:1710.05941, 2017. 3 [31] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In Advances in Neural Information Processing Systems, 2019. [32] Karel Rijkse. H. 263: Video coding for low-bit-rate communication. IEEE Communications magazine, 1996. 1 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 8 [34] Karen Simonyan. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. 2 [35] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In International Conference on Learning Representations, 2023. 8 [36] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Advances in Neural Information Processing Systems, 2020. [37] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. StyleGAN-V: continuous video generator with the price, image quality and perks of StyleGAN2. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 4, 5, 2, 3 [38] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International Confernonequilibrium thermodynamics. ence on Machine Learning, 2015. 8 [39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 5 [40] Soomro. UCF101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 1, 3, 4, 5, 6, 8, 2 [41] Gary Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high efficiency video coding (hevc) standard. IEEE Transactions on circuits and systems for video technology, 2012. 1 [42] Vivienne Sze, Madhukar Budagavi, and Gary Sullivan. In Integrated cirHigh efficiency video coding (hevc). cuit and systems, algorithms and architectures, page 40. Springer, 2014. [43] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1 [44] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris Metaxas, and Sergey Tulyakov. good image generator is what you need for high-resolution video syntheIn International Conference on Learning Representasis. tions, 2021. 5, 3 [45] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: Decomposing motion and content for video generation. In IEEE Conference on Computer Vision and Pattern Recognition, 2018. 5, 3 [46] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. FVD: new metric for video generation, 2019. 4, 2 [47] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems, 2017. 8 [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017. 2 [49] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. [50] Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, and Abhinav Shrivastava. LARP: Tokenizing videos with arXiv preprint learned autoregressive generative prior. arXiv:2410.21264, 2024. 1, 2, 4, 5, 8, 3 [51] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. OmniTokenizer: joint imagevideo tokenizer for visual generation. In Advances in Neural Information Processing Systems, 2024. 8 [52] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, 2, 8 [53] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 2004. 4, 2 [54] Thomas Wiegand, Gary Sullivan, Gisle Bjontegaard, and Ajay Luthra. Overview of the h. 264/avc video coding standard. IEEE Transactions on circuits and systems for video technology, 13(7):560576, 2003. 8 [55] Yuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision, 2018. 3 [56] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 8 10 [57] Wilson Yan, Danijar Hafner, Stephen James, and Pieter Abbeel. Temporally consistent transformers for video generation. In International Conference on Machine Learning, 2023. 8 [58] Wilson Yan, Matei Zaharia, Volodymyr Mnih, Pieter Abbeel, ElasticTok: AdaparXiv preprint Aleksandra Faust, and Hao Liu. tive tokenization for image and video. arXiv:2410.08368, 2024. 7, 8, [59] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. CogvideoX: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 8 [60] Jaehoon Yoo, Semin Kim, Doyup Lee, Chiheon Kim, and Seunghoon Hong. Towards end-to-end generative modeling of long videos with memory-efficient bidirectional transformers. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 8 [61] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 1, 2, 3, 4, 5, 8 [62] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. In International Conference on Learning Representations, 2024. 1, 8 [63] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In International Conference on Learning Representations, 2022. 5, 3 [64] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 1, 2, 4, 5, 8, [65] Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, and Anima Anandkumar. Efficient video diffusion models via content-frame motion-latent decomposition. In International Conference on Learning Representations, 2024. 1, 8 [66] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In IEEE Conference on Computer Vision and Pattern Recognition, 2018. 3, 4, 2 [67] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 1 [68] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022. 8 11 Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. Long video tokenization We train CoordTok via AdamW optimizer [23] with constant learning rate of 104, (β1, β2) = (0.9, 0.999), and weight decay 0.001. We use batch size of 256, where each sample is randomly sampled 128-frame video. CoordTok is trained in two stages: main training and fine-tuning. In the main training stage, we reconstruct = 1024 randomly sampled coordinates and update the model using ℓ2 loss. In the fine-tuning stage, we reconstruct 16 randomly sampled frames (i.e., = 4096 coordinates) and update the model using combination of ℓ2 loss and LPIPS loss with equal weights. To speed up training, we use mixedprecision (fp16). For the main experimental results, we train CoordTok for 1M iterations and fine-tune it for 50K iterations. For analysis and ablation studies, we train CoordTok for 200K iterations and fine-tune it for 10K iterations. Architecture CoordTok consists of transformer encoder that extracts video features from raw videos, crossself encoder that processes video features into triplane representations via cross-attention between learnable parameters and video features, and transformer decoder that learns mapping from coordinate-based representations into corresponding patches. In what follows, we describe each component in detail. Transformer encoder consists of Conv3D patch embedding, learnable positional embedding, and transformer layers, where each transformer layer comprises selfattention and feed-forward layers. Cross-self encoder consists of plane-wise Conv2D patch embeddings, transformer layers, and plane-wise linear projectors, where each transformer layer comprises crossattention, self-attention, and feed-forward layers. Transformer decoder consists of linear patch embedding, learnable positional embedding, transformer layers, and linear projector, where each transformer layer comprises self-attention and feed-forward layers. We provide the detailed architecture configurations for each model size in Table 5. Detailed description of coordinate-based patch reconstruction We describe the coordinate-based patch reconstruction process of CoordTok during training, which involves: (1) constructing the inputs and target outputs for the decoder, (2) obtaining coordinate-based representations from the inputs, and (3) reconstructing the target outputs 1 Table 5. Model configurations of CoordTok for each model size. Model size Module #layers Hidden dim. #heads Large Base Small Transformer Encoder Cross-self Encoder Transformer Decoder Transformer Encoder Cross-self Encoder Transformer Decoder Transformer Encoder Cross-self Encoder Transformer Decoder 8 24 24 8 12 12 8 8 8 1024 1024 1024 768 768 512 512 512 16 16 16 12 12 12 8 8 8 using these representations. Decoder input and target. Given video x, we patchify it and set patch coordinates as inputs and their corresponding patch values as target outputs. Formally, we start by patchifying the video x, i.e., Patchify(x) := {(m, x(m))}M m=1. We then convert each patch index into the coordinates (i, j, k) representing the center position of the patch along each axis relative to the entire video x, i.e., i, j, [0, 1]. Finally, we randomly sample patches. We note that we use entire patches for evaluation. Decoder input: Target output: [(i1, j1, k1), , (iN , jN , kN )] [x(i1j1k1), , x(iN jN kN )] (1) We also use triplane representation = [zxy, zyt, zxt] as input of decoder, where the planes have the shape of , , and , respectively. Coordinate-based representations. We use coordinatebased representations [h1, , hN ] obtained by triplane representation as input of the transformer decoder. Formally, let (i, j, k) [0, 1]3 be one of the sampled input coordinates. We extract hxy by querying (i, j) from zxy, hyt by querying (j, k) from zyt, and hxt by querying (i, k) from zxt as follows: hxy = Bilinear((i, j); zxy hyt = Bilinear((j, k); zyt hxt = Bilinear((i, k); zxt lm, zxy mn, zyt ln, zxt l,m+1, zxy m,n+1, zyt l,n+1, zxt l+1,m, zxy m+1,n, zyt l+1,n, zxt l+1,n+1) l+1,m+1) m+1,n+1) (2) lm, zyt where (l, m, n) = (i (H 1), (W 1), (T 1)), (zxy ln) indicates the latent vector in at coordinates (l, m, n), and Bilinear(; ) indicates the bilinear interpolation operation at the input coordinate between given vectors. We then concatenate them mn, zxt to get coordinate-based representation of (i, j, k), i.e., := Concat(hxy, hyt, hxt). Patch reconstruction. Given coordinate-based representations [h1, , hN ], the transformer decoder processes them through self-attention layers to reconstruct the patches [ˆx(i1j1k1), , ˆx(iN jN kN )]. Before the transformer layers, we add positional embeddings corresponding to the coordinates (i1, j1, k1), , (iN , jN , kN ) to each patch. This allows the self-attention layers to effectively distinguish each patch. A.2. Long video generation We implement CoordTok-SiT-L/2 based on the original SiT implementation [25]. The inputs of SiT-L/2 are the normalized triplane representation obtained by tokenizing video clips of length 128 with CoordTok. To normalize the triplane representation, we randomly sample 2048 video clips of length 128 and calculate the mean and standard deviation for each plane. We train SiT-L/2 via AdamW optimizer [23] with constant learning rate of 104, (β1, β2) = (0.9, 0.999), and no weight decay. We use batch size of 64. We train the model for 600K iterations and we update an EMA model with momentum parameter 0.9999. Architecture We use the same structure as SiT, except that our patch embedding and final projection layers are implemented separately for each plane. To train the unconditional video generation model, we assume the number of classes as 1, and we set the class dropout ratio to 0. We provide the detailed architecture configurations in Table 6. Table 6. Model configurations of CoordTok-SiT-L/2. SiT-L/2, #token = SiT-L/2, #token = 3072 Input dim. (zxy) Input dim. (zyt) Input dim. (zxt) # layers Hidden dim. # heads 16168 16328 16328 24 1024 16 32328 32328 3232 24 1024 16 Sampling For sampling, we use the Euler-Maruyama sampler with 250 sampling steps and diffusion coefficient wt = σt. We use the last step of the SDE sampler as 0.04. B. Evaluation Details B.1. Long video reconstruction For our CoordTok, we tokenize and reconstruct 128-frame videos all at once. Specifically, we encode the video into triplane representation and then reconstruct the video by passing all patch coordinates through the transformer deIn contrast, the baselines can only handle coder at once. videos of much shorter lengths (e.g., 16 frames for PVDMAE [64]). Therefore, to evaluate the reconstruction quality of 128-frame videos for the baselines, we split the videos into short clips and tokenize and reconstruct them. To be specific, we first split 128-frame video into shorter clips suitable for each tokenizer. We then tokenize and reconstruct each short clip individually using the tokenizer. Finally, we concatenate all the reconstructed short clips to obtain the 128-frame video. For evaluating the quality of reconstructed videos, we follow the setup of MAGVIT [61]. We randomly sample 10000 video clips of length 128, and then measure the reconstruction quality using the metrics as follows: rFVD [46] measures the feature distance between the distributions of real and reconstructed videos. It uses the I3D network [3] to extract features, and it computes the distance based on the assumption that both feature distributions are multivariate Gaussian. Specifically, we compute the rFVD score on video clips of length 128. PSNR measures the similarity between pixel values of real and reconstructed images using the mean squared error. For videos, we compute the PSNR score for each frame and then average these frame-wise PSNR scores. LPIPS [66] measures the perceptual similarity between real and reconstructed images by computing the feature It agdistance using pre-trained VGG network [34]. gregates the distance of features extracted from various layers. For videos, we compute the LPIPS score for each frame and then average these frame-wise LPIPS scores. SSIM [53] measures the structural similarity between real and reconstructed images by comparing luminance, contrast, and structural information. For videos, we compute the SSIM score for each frame and then average these frame-wise SSIM scores. B.2. Long video generation For sname-SiT-L/2, we generate the tokens corresponding to 128-frame video all at once and then decode these tokens using CoordTok. In contrast, baselines iteratively generate 128-frame videos. For instance, PVDM and HVDM generate the next 16-frame video conditioned on the previously generated 16-frame video clip. For evaluating the quality of generated videos, we strictly follow the setup of StyleGAN-V [37] that calculates the FVD scores [46] between the distribution of real and generated videos. To be specific, we use 2048 video clips of length 128 for each distribution, where the real videos are sampled from the dataset used to train generation models (i.e., the UCF-101 dataset [40]). B.3. Analysis Dynamics magnitude. To measure how dynamic each video is, we use the pixel value differences between con2 C.2. Long video generation We describe the main idea of baseline methods that we used for the evaluation. MoCoGAN [45] proposes video generative adversarial network (GAN; [10]) that has separate content generator and an autoregressive motion generator for generating videos. MoCoGAN-HD [44] also proposes video GAN with motion-content decomposition but uses strong pretrained image generator (StyleGAN2 [18]) for highresolution image synthesis. DIGAN [63] interprets videos as implicit neural representation (INR; [36]) and trains GANs to generate such INR parameters. StyleGAN-V [37] also introduces an INR-based video GAN with computation-efficient discriminator. PVDM-L [64] proposes latent video diffusion model that generates videos in projected triplane latent space. HVDM [19] proposes latent video diffusion model that generates videos with 2D triplane and 3D wavelet representation. D. Additional Qualitative Results In Figure 10, we provide additional video reconstruction reIn addition, in Figure 11, we prosults from CoordTok. vide additional unconditional video generation results from CoordTok-SiT-L/2. E. Additional Quantitative Results We provide the reconstruction quality of CoordTok with 1280 and 3072 tokens in Table 8. Although there is no significant difference in the reconstruction quality between CoordTok with token sizes of 1280 and 3072, training SiT-L/2 with the 1280 tokens results in substantially better generation quality (see Section 3.3). Table 8. Reconstruction quality of CoordTok trained on the UCF101 dataset [40]. #tokens rFVD PSNR LPIPS SSIM 1280 3072 102.9 100.5 28.6 28.7 0.066 0.065 0.892 0. secutive frames. To be specific, we compute the dynamics magnitude for each pair of consecutive frames, calculate the mean of these values, and then take the logarithm. Here, dynamics magnitude of two frames 1 and 2 of resolution can be defined as follows: d(f 1, 2) ="
        },
        {
            "title": "1\nHW",
            "content": "H (cid:88) (cid:88) h=1 w=1 d2(f 1 hw, hw), (3) where hw denotes the RGB values at coordinates (h, w) of frame and d2 denotes ℓ2-distance of RGB pixel values. In Figure 8a, we standardize the video dynamics score into range of 0 to 100. Frequency magnitude. To measure the frequency magnitude, we use the metric proposed in Yan et al. [58] that utilizes Sobel edge detection filter. To be specific, to get the frequency magnitude, we apply both horizontal and vertical Sobel filters to each frame to compute the gradient magnitude at each pixel. We then calculate the average of these magnitudes across all pixels. C. Baselines C.1. Long video reconstruction We describe the main idea of baseline methods that we used for the evaluation. We also provide the shape of tokens of baselines in Table 7. MaskGiT-AE [5] uses 2D VQ-GAN [8] that encodes an image into 2D discrete tokens. TATS-AE [9] introduces 3D-VQGAN that compresses 16-frame video clip both temporally and spatially into 3D discrete tokens. MAGVIT-AE-L [61] also introduces 3D-VQGAN but improves architecture design (e.g., uses deeper 3D discriminator rather than two shallow discriminators for 2D and 3D separately, uses group normalization [55] and Swish activation [30]) and scales up the model size. PVDM-AE [64] encodes 16-frame video clip into factorized triplane representations. LARP [50] encodes videos into 1D arrays by utilizing next-token prediction model as prior model. Table 7. Token shapes of video tokenization baselines Method Input shape Token shape MaskGiT-AE [5] 1281283 88 TATS-AE [9] MAGVIT-AE-L [61] PVDM-AE [64] LARP [50] 161281283 161281283 161281283 16128128 41616 41616 (1616) 3 1024 3 Figure 10. Additional 128-frame, 128128 resolution video reconstruction results from CoordTok (Ours) trained on the UCF-101 dataset [40]. For each frame, we visualize the ground-truth (GT) and reconstructed pixels from CoordTok. Figure 11. Additional unconditional 128-frame, 128128 resolution video generation results from CoordTok-SiT-L/2 trained on 128-frame videos from the UCF-101 dataset [40]."
        }
    ],
    "affiliations": [
        "KAIST",
        "UC Berkeley"
    ]
}