{
    "paper_title": "Factorized Learning for Temporally Grounded Video-Language Models",
    "authors": [
        "Wenzheng Zeng",
        "Difei Gao",
        "Mike Zheng Shou",
        "Hwee Tou Ng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a \"grounding then answering with evidence referencing\" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm."
        },
        {
            "title": "Start",
            "content": "Factorized Learning for Temporally Grounded Video-Language Models Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng National University of Singapore wenzhengzeng@u.nus.edu, {daniel.difei.gao, mike.zheng.shou}@gmail.com, nght@nus.edu.sg 5 2 0 2 0 3 ] . [ 1 7 9 0 4 2 . 2 1 5 2 : r Figure 1. (a) Performance: Our method outperforms SOTA methods across various tasks (here we draw the maximum performance across methods, detailed in Sec. 6). (b) Model: We propose new framework D2VLM, where we decompose the generation objective into grounding then answering with evidence referencing paradigm and introduce evidence tokens to emphasize explicit event-level visual semantic capture. (c) Training Algorithm: We introduce Factorized Preference Optimization (FPO) that explicitly addresses both temporal grounding and textual response. factorized data synthesis approach is also designed to support FPO."
        },
        {
            "title": "Abstract",
            "content": "Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in coupled manner without clear logical structure, leading to sub-optimal objectives. We address this from factorized learning perspective. We first propose D2VLM, framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt grounding then answering with evidence referencing paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce novel factorized Corresponding authors 1 preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm. 1. Introduction Recent advances in video-language models, especially those built upon large language models (video LLMs), have enabled remarkable progress in video understanding [10, 21, 23, 25, 31, 44]. Through their flexible video&text-intext-out nature, video LLMs demonstrate great potential as general-purpose solvers, unifying various tasks (e.g., temporal grounding [9], dense captioning [47], and question answering [7]) into one generic video question answering framework. Despite their effectiveness, existing video LLMs still struggle with accurate temporal grounding for event perception and localization [10, 11, 23, 31, 31], crucial capability not only for grounding tasks themselves but also for relevant tasks that need textual answering. We notice that for video understanding, temporal eventlevel grounding and textual response are two primary tasks that exhibit distinct characteristics yet maintain strong logical dependencies. Specifically, temporal grounding focuses on precisely locating temporal events (evidence) that support answering, while textual response emphasizes accurate interpretation from the grounded evidence and generating coherent textual answers. However, existing methods [4, 13, 14, 23, 31] typically handle these two tasks in coupled manner, with two major limitations: (1) Various special tokens are designed for temporal grounding [4, 14, 23], but their generation is mixed with text token generation without clear logical structure, leading to coupled learning objectives. (2) More importantly, these special tokens mainly focus on timestamp representation for precise timestamp output, lacking the explicit capture of visual semantics from grounded events. In contrast, we argue that such event-level visual semantics should not be overlooked, as it could inherently serve as crucial contexts for subsequent textual answer generation, especially under the next-token prediction paradigm. Based on the aforementioned observations, we propose to address this from factorized learning perspective. We first introduce new framework D2VLM that decouples the learning of temporal evidence grounding and textual answering, while preserving and even strengthening their inherent dependency. Specifically, as shown in Fig. 1 (b), we decompose the model response into two sequential stages: (1) pure temporal grounding that aims to localize and capture essential visual evidence for response, followed by (2) interleaved text-evidence answer generation, where both the textual answer and temporal information are produced in an evidence-referencing manner to establish consistency with the previously grounded evidence. Technically, we introduce evidence tokens, special token type dedicated to temporal evidence grounding. Different from existing designs of grounding tokens that focus on its special category and timestamp representation, our evidence tokens not only aim to determine the temporal location of the grounded event, but also emphasize the capture of event-level visual semantics, which serves as crucial context for subsequent answer generation. During the subsequent interleaved text-evidence generation, the evidencereferencing process is achieved by generating evidence tokens that align with those from the previous grounding stage. This ensures that the output information in the final response remains consistent with the initially grounded evidence while reinforcing logical coherence across stages. Besides providing decoupled and clearer task objectives, our design naturally fits well with the teacher-forcing autoregressive training paradigm, as subsequent textual response generation is conditioned on the correctly grounded evidence, enabling learning shortcut for more stable training. Our experiments demonstrate that both the designed sequence generation objective and event-level visual semantic capture are essential for performance improvement, offering valuable insights for future model design. To further facilitate the learning of these two tasks, we propose novel Factorized Preference Optimization (FPO) algorithm. Unlike standard preference optimization, FPO incorporates probabilistic temporal grounding modeling into the optimization objective, enabling explicit preference learning for temporal grounding in addition to standard textual preference. Meanwhile, another obstacle is that existing video preference datasets do not account for such temporal grounding aspects, making it infeasible to directly apply our proposed FPO algorithm. To overcome this limitation, we construct synthetic dataset by introducing factorized perturbations into the original preferred response sequence. These perturbations are applied at the sub-video event level, considering two main factors: temporal grounding and textual response, along with multiple possible sub-factors. This approach ensures structured and controllable noise generation process, where the cause and type of noise are precisely known without any manual annotation. As result, our data synthesis process is both reliable and scalable, making it possible to effectively conduct the proposed factorized preference optimization. To the best of our knowledge, we are the first to use preference learning involving explicit temporal grounding, and moreover, in factorized manner, as illustrated in Fig. 1 (c). We conduct extensive experiments to demonstrate the superiority of our proposed framework. An intuitive comparison is shown in Fig. 1 (a), where our 3.8B model outperforms existing SOTA methods (typically ranging from 3.8B to 13B in model size) across various tasks. 2. Related Work 2.1. Video LLMs The recent development of video LLMs has shown great potential as generic video understanding assistants [21, 23, 25, 44], primarily due to their flexible video&textin-text-out capability. Despite the effectiveness, existing video LLMs still struggle with accurate temporal eventlevel perception and localization (i.e., temporal grounding) [10, 13, 23, 31, 39], crucial capability not only for grounding tasks but also for generating correct textual answers that are heavily based on grounding ability. Various efforts have been made to enhance the temporal grounding ability of video LLMs [4, 10, 11, 14]. Besides adding targeted data for temporally sensitive finetuning [13, 20, 31, 37, 43], recent work usually designs different special tokens for precise temporal representation [10, 14, 23, 29, 36], where some work further combines such special tokens with extra grounding decoder [4, 11]. Despite their effectiveness, we argue that there still exist two potential limitations. First, in the generation sequence, special time tokens are interleaved with text tokens, leading to highly coupled learning objective between temporal grounding and textual answering. Second, these special tokens are primarily designed for timestamp representations to output timestamp, lacking the ability to capture the visual semantics of grounded events. In this work, we aim to enhance temporally grounded video LLMs from factorized learning perspective. We first decompose the task into learning of temporal evidence grounding and textual response while also strengthening their inherent dependency. We also propose factorized preference learning paradigm to further facilitate the twotask learning. 2.2. Preference Optimization Reinforcement learning from human feedback (RLHF) [28, 33] or from AI feedback (RLAIF) [3, 15] has been widely used to align LLMs behavior with human preference, with PPO [27, 32, 33, 48] and DPO [30] as common implementations. While this spirit has extended to the multimodal domain [17, 35, 45], it remains underexplored for video LLMs [18, 46], especially regarding explicit temIn this work, we introduce factorized poral grounding. preference optimization (FPO) algorithm that explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual responses. We also construct synthetic dataset to address the lack of suitable datasets for factorized preference learning for explicit temporal grounding. 3. Method Here we will present the proposed D2VLM framework. 3.1. Preliminary We begin by first illustrating the general formulation of temporally grounded video-language models, particularly in the context of video LLMs. Given video and textual question (or prompt) Q, the goal is to answer the question based on the given video, where the answer is typically composed of textual response together with temporal grounding information (i.e., localized temporal interval). For video LLMs, separate visual and text encoders will be adopted first to encode the raw inputs into hidden space, in the format of tokens. Specifically, input video will be encoded into video tokens FV R(T Nv)C, where is the Figure 2. Conceptual demonstration of the D2VLM framework. Figure 3. The visual semantic capture process of <evi> token. number of frames, Nv is the number of frame-level visual tokens, and is the number of channels. Without loss of generality, here we set Nv = 1 (i.e., FV RT C, one token per frame) for illustration purposes. Similarly, input question will be encoded into text tokens FQ RNtC, where Nt is the number of tokens. Then, both FV and FQ will feed into the video-aware LLM decoder, where the response will be generated in an autoregressive manner. 3.2. D2VLM We notice that temporal event-level grounding and textual response are two primary tasks in temporally grounded video understanding tasks, where they exhibit distinct characteristics yet maintain strong logical dependencies: Temporal grounding focuses on precisely locating temporal events (evidence) that support answering, while textual response emphasizes accurate evidence interpretation and generating coherent textual answers. However, existing methods typically handle these two tasks in coupled manner, with two major limitations: (1) Various special tokens are designed for temporal grounding, but are mixed with text token generation without clear logical structure, leading to coupled learning objectives. (2) More importantly, these special tokens mainly focus on timestamp representation to output timestamp, lacking the explicit capture of visual semantics from grounded events. In contrast, we argue that such visual semantics can actually serve as crucial contexts for subsequent textual answer generation, especially under the next-token prediction paradigm. Based on this insight, we propose D2VLM, where our core motivation is to decouple the tied learning objective while also preserving and even strengthening the inherent dependency between these two tasks. An overall conceptual demonstration is shown in Fig. 2. Specifically, encoded video tokens FV and question textual tokens FQ are fed 3 into the LLM decoder, which will generate response tokens autoregressively. Here, we enforce the video LLM to first conduct pure grounding task before providing the actual answer, where the grounded events are expected to serve as essential evidence for the subsequent response. Then, the model will give the actual answer in an interleaved textevidence manner which will be explained later. Evidence token for grounding and more. Technically, we introduce evidence token <evi>, special token type dedicated to temporal grounding. Unlike existing designs that mainly focus on category and timestamp representation, the proposed <evi> token not only identifies the temporal location of the grounded event but also emphasizes the explicit capture of event-level visual semantics. Specifically, as shown in Fig. 3, once the video LLM generates token classified as an <evi> token, we first calculate the similarity between the <evi> token and each frame-level LLM-processed video token FV RT (shares the same dimension as the input FV ). Video tokens with high similarity to the <evi> token are then regarded as salient tokens. We aggregate the visual semantics from these salient tokens into the <evi> token with simple implementations (i.e., average pool the salient tokens and then add it to the <evi> token). Such an operation explicitly endows the <evi> token with rich visual semantics of the corresponding grounded event, enabling it to truly serve as solid evidence for the subsequent response based on it. Experiments in Sec. 6 demonstrate that both the design of event-level modeling and visual semantic capture are crucial for performance improvement. Interleaved text-evi token response. After the pure grounding stage, the generation objective switches to answer the input question based on the grounded evidence. The answer is typically composed of textual response together with temporal grounding information (e.g., You capture the dog at 10s-15s and 50s-60s.). Here, we formulate the response objective as an interleaved text-evidence token generation process, where <evi> tokens are generated again to represent the required temporal output information in manner akin to evidence referencing, rather than using standard textual tokens to denote timestamps only. The exact timestamps for answering can be directly obtained from the similarity calculation between the <evi> token and the frame-level video tokens FV , using the frame indices of the salient video tokens  (Fig. 3)  . To make the model aware of such transition between tasks, we introduce another special token, </evi>, to indicate the end of the evidence grounding stage and encourage the model to begin generating the interleaved text-evidence answer. Ideally, the generated <evi> token during this stage should also match the one in the evidence grounding phase (i.e., the final response should be consistent with the previous grounded one). We introduce an explicit constraint to enhance this consistency: (cid:88) Lcons ="
        },
        {
            "title": "1\nK",
            "content": "k=1 (cid:12) (cid:12)F S1 (cid:12) <evi>k S2 <evi>k (cid:12) (cid:12) (cid:12) , (1) where S1, S2 denote the pure evidence grounding stage and interleaved text-evi response stage, respectively. is the kth <evi> token in each stage. The <evi> tokens are generated in temporal order, each aligned with distinct interval corresponding to ground-truth event. Such constraint enforces strong consistency across stages, preserving logical coherence and strengthening the alignment between evidence grounding and answer generation. Loss function. We adopt the following components to supervise network learning: = Lsf + Lgnd + Lcons, (2) where Lsf is the standard token classification loss for supervised fine-tuning [22, 24, 38], Lgnd is the average grounding loss for generated <evi> tokens across both stages, with each formulated as: L<evi> gnd = 1 (cid:88) t=1 BCE (cid:0)yt, sim t(cid:1) , (3) where BCE is the binary cross-entropy loss, yt is the framelevel ground-truth (i.e., 1/0 for foreground/background), and simt is the normalized dot-product similarity between <evi> token and frame-level video token feature V . 4. Factorized Preference Optimization To further enhance the model learning on both temporal event grounding and textual response, we introduce novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling explicit preference learning on both temporal grounding and textual response. Here, we start with macroscopic formulation that captures the essence of preference optimization and gradually elaborate on our new design. Inspired by the existing efficient preference learning algorithm DPO [30], we adopt similar principle: encouraging the model to assign higher probability of generating good response than generating bad response, which can be defined as: = (cid:2)log σ (cid:0)log π (R) log π (cid:0) R(cid:1)(cid:1)(cid:3) , (4) where π (R) and π (cid:0) R(cid:1) are the joint generation probabilities of preferred and dis-preferred response, respectively. Under the autoregressive paradigm, such joint generation log-probability can be further detailed as: log π(R) = (cid:88) RiR log (Ri V, Q, R1:i1), (5) 4 Figure 4. The proposed data synthesis pipeline, where factorized perturbation is imposed to form the dispreferred data. where Ri is the ith token of the whole model response R. and are the input video and question, respectively. One of our goals is to perform preference optimization not only for textual responses but also with an explicit emphasis on temporal grounding. However, achieving such multi-factor preference learning presents two key challenges: (1) The lack of suitable preference data that explicitly considers temporal grounding while maintaining factorized structure. We will elaborate more on this issue and introduce our newly proposed synthetic dataset as solution in Sec. 5. (2) The challenge on building suitable objectives for explicit preference optimization on temporal grounding, particularly in quantifying the models grounding probability (i.e., measuring the likelihood that the model will ground specific temporal event). Many existing works introduce special tokens and regression heads to enhance the grounding ability of video LLMs. Despite their effectiveness, this deviates from the standard discrete text token prediction paradigm, making it non-trivial to model grounding likelihood for preference optimization. In this work, we notice that our designed D2VLM framework can naturally model such event grounding probability, in different and explicit way compared with the standard token prediction probability. Specifically, as grounding is achieved by frame-level feature similarity calculation between the <evi> token and each video frame, we directly use such similarity as the frame-level grounding probability and multiply these probabilities across frames to obtain the interval-level grounding probability. Formally, for each generated <evi>k token during model response, its corresponding grounding probability pg([sk, ek]) for given time interval [sk,ek] (sk, ek denote the starting and ending time index respectively) can be formulated as: pg([sk, ek]) = (cid:89) t= (cid:40) simt 1 simt ifsk ek otherwise (6) Here, is the total number of input video frames, simt is the normalized dot-product similarity between <evi>k and frame-level video token feature V . Based on such event grounding modeling, the log-probability formulation within our proposed factorized preference optimization is given as: log π(R) = (cid:88) log (Ri V, Q, R1:i1) RiR (cid:88) + Rk=<evi> log pg ([sk, ek] V, Q, R1:k1), (7) where the first term corresponds to the standard textual token prediction modeling, while the second term represents the proposed explicit modeling on temporal event grounding. By substituting Equation (7) into Equation (4), we obtain the final formulation of our Factorized Preference Optimization (FPO). Next, we explain the process of constructing multi-factor preferred (R) and dis-preferred ( R) data pairs, which serve as the training data for FPO. 5. Factorized Preference Data Synthesis As previously mentioned, one key challenge is the lack of suitable preference data that captures multi-factor preferencesnot only for textual responses but also for explicit temporal grounding. Particularly, in the video domain, preference data is typically constructed by providing video LLM with either complete or degraded video inputs (e.g., blurred or cropped images/videos) and treating its responses as preferred or dispreferred, respectively [18]. The underlying assumption is that the model will produce inferior responses when given degraded or incomplete visual inputs. However, existing works using this approach often face some major issues: (1) The reliability of the generated preference data is relatively less controlled. Specifically, the generated preferred sample may not actually be the preferred one, and the generated dispreferred data can sometimes be as good as or even better than the preferred data [18]. As result, post-filtering is usually required, which involves additional models and ad-hoc human interventions, increasing costs but still cannot fully ensure 5 quality. (2) This lack of control also makes it difficult to properly factorize the generated dispreferred data, including identifying the source or the reason why it is considered dispreferred. (3) Last but not least, existing video preference data mainly focuses on generating responses in pure textual format (e.g., video caption) [18, 46], while generating preference response that explicitly contains temporal grounding information (e.g., temporal event position) remains underexplored. Overall, these issues make factorized preference optimization difficult to achieve. To address the aforementioned limitations, we propose an automatic factorized preference data synthesis framework, which introduces factorized perturbations on the original response at the sub-video event level. As shown in Fig. 4 (a), the overall process consists of the following steps: Random selection of factors and sub-video events. For each given sample (video-question-answer pair), we first randomly select factor that determines the exact type of the perturbation. As shown in Fig. 4 (b), the factors are broadly categorized into two types and each can be further divided into multiple sub-types: (1) Temporal grounding: This type of perturbation aims to reveal/simulate model response inaccuracies related to temporal event grounding. It can be further divided into sub-types, including temporal localization shift, randomly adding/deleting grounded events (corresponding to false positives and missed detections), and merging multiple events into one (corresponding to lack of fine-grained distinction in event boundaries). (2) Textual response: This type of perturbation modifies the It includes semantic correctness of the textual response. sub-types such as distorting key information, which disrupts critical content, and repeating responses, common failure mode observed in video LLMs. Next, we randomly select some sub-video events to apply factorized perturbation. sub-video event refers to semantically meaningful segment of the video that can be determined based on the given sample (e.g., its question or ground-truth answer). For instance, in dense video captioning task, an event naturally corresponds to specific captioned interval, while in an action localization task, an event can align with single action instance. Without loss of generality, we take the dense video caption task as an illustration example (as shown in Fig. 4 (c)), as this task emphasizes both temporal grounding and textual description. Overall, such an event-level design aligns naturally with the inherent video structure, ensuring that perturbations are introduced in way that respects the videos temporal and semantic coherence. Moreover, it enables fine-grained perturbation, allowing controlled modifications at the event level rather than applying coarse, less precisely controlled noise to the entire response sequence. Injecting factorized perturbations. Once the factors and sub-video events are determined, localized perturbation is introduced based on the selected factors and sub-video events. If temporal grounding factor is selected, we modify the timestamps of the selected events based on the determined sub-factor (e.g., shift the interval, delete the interval, etc.). For the factor of textual response, for each selected event, we prompt off-the-shelf LLM (e.g., Qwen [41, 42]) to generate distorted response based on the original correct event-level response. If the repeat sub-factor is selected, we can easily achieve this by setting the response of the current event to its previous one to simulate such response repeating phenomenon. synthesized example is shown in Fig. 4 (c). After the perturbation synthesis, we combine the newly obtained noisy response with the original one to form the preference sample pair for factorized preference optimization. In practice, our preference data is synthesized based on the E.T. Instruct 164K dataset [23]. Discussion. Compared to existing preference data synthesis approaches, our framework ensures that the generated dispreferred response contains specific, well-factorized noise rather than arbitrary, unpredictable degradation, and thus we can better identify the noise source without manual costs. To our knowledge, we are also the first to emphasize explicit temporal grounding in preference data, and moreover, in multi-factorized manner. The overall design makes our factorized preference optimization in Sec. 4 feasible. 6. Experiments 6.1. Experimental Setup Implementation details. Following the practice of [23], we adopt the pre-trained ViT-G/14 from EVA-CLIP [8, 34], followed by Q-Former-like feature compressor [6, 16, 23], and use Phi-3-Mini-3.8B [1] as the base LLM. Also following [23], our model is initialized from the stage-2 model of [23] and [19], and we also only fine-tune our model on the E.T. Instruct 164K dataset [23] using LoRA [12]. We conduct training on 4 NVIDIA H100 GPUs within 1 day. More details are provided in the supplementary material. (2) E.T. Bench Dense Captioning [23]. Evaluation tasks and datasets. We evaluate our method across various datasets and tasks. (1) E.T. Bench Grounding [23]. It consists of 5 different grounding-related tasks (e.g., moment retrieval, action localization, summarization/highlight detection, etc.) for comprehensive evaluaIt is comtion. posed of tasks like dense video caption and step localization, where both key event grounding and textual description are required. Besides, we also conduct experiments on two widely used individual datasets, (3) Charades-STA [9] and (4) YouCook2 [47], for independent evaluation on moment retrieval and dense video captioning, respectively. 6 Method Year E.T. Bench Grounding E.T. Bench Dense Captioning TVGF 1 EPMF 1 TALF 1 EVSF 1 VHDF 1 AvgF 1 DVCF 1 DVCSim SLCF 1 SLCSim AvgF 1 AvgSim Video-ChatGPT-7B [26] ACL24 Video-LLaVA-7B [21] LLaMA-VID-7B [19] Video-LLaMA-2-7B [5] PLLaVA [40] VTimeLLM-7B [13] TimeChat-7B [31] LITA-13B [14] VTG-LLM-7B [10] Qwen2.5-VL-7B* [2] E.T.Chat-3.8B [23] D2VLM-3.8B (Ours) EMNLP24 ECCV24 arXiv24 arXiv24 CVPR24 CVPR24 ECCV24 AAAI25 arXiv25 NeurIPS24 ICCV25 7.0 7.0 5.5 0.1 6.9 7.6 26.2 22.2 15.9 46.6 38.6 60. 1.3 1.9 1.2 0.0 1.1 1.9 3.9 4.6 3.7 9.3 10.2 14.4 15.1 15.0 8.0 0.0 5.7 18.2 10.1 18.0 14.4 32.2 30.8 33.4 8.4 0.3 1.4 0.0 0.3 15.9 29.1 29.7 26.8 19.9 25.4 35.2 28.8 28.9 30.0 1.5 28.9 28.9 40.5 23.9 48.2 68.6 62.5 68.2 12.1 10.6 9.2 0.3 8.6 14.5 22.0 19.7 21.8 35.3 33.5 42.3 8.8 28.0 27.1 0.6 13.3 12.4 16.6 39.7 40.2 39.0 38.4 48. 11.3 15.0 12.6 14.5 10.6 13.1 12.5 17.2 18.6 21.4 19.7 25.5 5.7 0.9 5.2 0.0 9.7 8.7 5.6 21.0 20.8 25.5 24.4 26.7 10.2 8.3 11.1 15.2 11.8 6.4 9.2 12.2 14.4 15.1 14.6 18.1 7.3 14.5 16.2 0.3 11.5 10.6 11.1 30.4 30.5 32.2 31.4 37.5 10.8 11.7 11.9 14.9 11.2 9.8 10.9 14.7 16.5 18.2 17.2 21.8 Table 1. Performance comparison on E.T. Bench-Grounding dataset (5 sub tasks: TVG: Temporal Video Grounding; EPM: Episodic Memory; TAL: Temporal Action Localiztion; EVS: Extractive Video Summarization; VHD: Video Highlight Detection) and E.T. BenchDense Captioning dataset (2 sub tasks: DVC: Dense Video Captioning; SLC: Step Localization and Captioning). *: Implemented by us, with more details in the supplementary material. Method Year R@1(IoU=0.5) R@1(IoU=0.7) TimeChat-7B [31] VTimeLLM-7B [13] VTimeLLM-13B [13] Momentor-7B [29] HawkEye-7B [37] VTG-LLM-7B [10] TRACE-7B [11] VideoChat-T-7B [43] E.T.Chat-3.8B [23] D2VLM-3.8B (Ours) CVPR24 CVPR24 CVPR24 ICML24 arXiv24 AAAI25 ICLR25 ICLR25 NeurIPS24 ICCV 32.2 27.5 34.3 26.6 31.4 33.8 40.3 48.7 45.9 50.3 13.4 11.4 14.7 11.6 14.5 15.7 19.4 24.0 20.0 26.0 Table 2. Performance comparison on Charades-STA. Method Year CIDEr SODA TimeChat-7B [31] VTG-LLM-7B [10] TRACE-7B [11] D2VLM-3.8B (Ours) CVPR24 AAAI25 ICLR25 ICCV25 12.6 17.5 22.4 26.4 3.4 5.0 8.1 10. 1.2 1.5 2.2 3.2 Table 3. Performance comparison on YouCook2. 6.2. Main Benchmarking Results E.T. Bench grounding. The results are shown in the left part of Tab. 1, where we compare our method with the recently published SOTA methods. It can be observed that the proposed D2VLM significantly outperforms recently published SOTA methods across various tasks, achieving an average F1 score improvement of at least 7.0%. Meanwhile, our model is relatively lightweight (3.8B parameters) compared to most state-of-the-art methods (ranging from 3.8B to 13B, with the majority being 7B parameters). This further demonstrates the superiority of our proposed method. E.T. Bench dense captioning. The performance comparIt can be seen ison is shown in the right part of Tab. 1. that our method also shows noticeable superiority in dense captioning-related tasks, where it outperforms recent SOTA methods in both temporal event grounding (at least 5.3% on average F1 score) and textual description (at least 3.6% on average Sim score). This essentially demonstrates the generalizability of our method across various scenarios. Charades-STA. From Tab. 2, it can be seen that our method still outperforms recent SOTA methods by large margins. Specifically, our 3.8B model outperforms counterpart [23] with the same base model by 4.4% on R@1(IoU=0.5), and outperforms methods published in 2025 with 7B model size [10, 11, 31] by at least 1.6% on R@1(IoU=0.5). This further demonstrates the superiority of our method in both effectiveness and efficiency. YouCook2. As shown in Tab. 3, our method achieves state-of-the-art performance compared to recent SOTA approaches. Specifically, it outperforms them by at least 4% in temporal event grounding (F1) while also demonstrating superior textual description capacity, with gains of at least 2.5% on CIDEr and 1.0% on SODA c. This further highlights the superiority of our approach in both temporal grounding and text generation. 6.3. Ablation Studies Here we conduct ablation studies on E.T. Bench grounding and E.T. Bench Dense Captioning datasets to analyze the effect of the proposed components, in both temporal eventlevel grounding and textual description. For each dataset (i.e., grounding, dense captioning), we report the average performance across multiple sub-tasks. In the following, we first analyze the component effect of the newly designed generation objective and evidence token, where we do not take the proposed FPO into account. Finally, we analyze the effect of the proposed FPO. Generation objective of D2VLM. Here, we analyze the effectiveness of our newly designed generation objective within D2VLM. From Tab. 4, it can be seen that: (1) Row-1 7 Decomposed Objective Interleaved Text-evi Generation Consistency Constraint Grounding AvgF Dense Captioning AvgSim AvgF 1 21.2 28.9 35.6 39.5 14.3 23.1 34.3 35. 11.3 16.0 19.8 21.2 FPO Grounding Dense Captioning AvgF 1 AvgF 1 AvgSim 39.5 42.3 35. 37.5 21.2 21.8 Table 4. Component effect of the designed generation objective. Table 5. Effect of the proposed FPO. Method Grounding Dense Captioning AvgF 1 AvgF 1 AvgSim w/o event-level modeling w/o visual semantic capture Full design 26.1 37.1 39.5 33.4 27.5 35.0 16.2 17.7 21.2 Table 6. Component effect within evidence token design. v.s. Row-2: Decompose the coupled grounding-textual response objective into grounding (by our designed <evi> token) then answer from grounded event can substantially improve the performance in both grounding (i.e., 7.7% averaged F1 on grounding tasks and 8.8% averaged F1 on dense captioning tasks) and textual description (i.e., 4.7% averaged sentence similarity on dense captioning tasks). (2) Row-2 v.s. Row-3: When we shift the answering stage from pure textual token generation into interleaved text-evi token generation, the performance can be further enhanced by large margins (at least 6.7% on grounding and 3.8% on textual description). The reason is that under such cases, the overall generation objective can be interpreted as grounding then answer with evidence referencing, where such referencing strengthens the relationship between event grounding and textual answering. (3) Row-3 v.s. Row-4: The introduced consistency constraint further boosts the performance by noticeable margin, as it explicitly enhances the consistency between the two stages. Overall, the results validate the effectiveness of our design. Design of evidence token. Here we analyze the impact of the core components within the <evi> token design (i.e., the event-level modeling and the explicit visual semantic capture). The results are shown in Tab. 6, where w/o eventlevel modeling means each <evi> token only captures the information of individual frame (a common design in existing works [14, 23, 29]), rather than an interval-level event (e.g., 10s-15s). w/o visual semantic capture indicates visual semantics from the video tokens is not explicitly integrated into the <evi> token through the operation illustrated in Sec. 3.2 (also common design in most of the existing works [4, 14, 29]). From Tab. 6, it can be summarized that: (1) The performance drops significantly without event-level modeling. We argue that this is because compared to individual timestamp-level modeling, event-level modeling more naturally aligns with the characteristics of event-level evidence grounding and can capture more robust event-level semantics in global perspective. (2) Explicit visual semantic capture is also crucial for enhancing model effectiveness, as it enables <evi> tokens to truly serve as solid context for subsequent answer generation, which also fits well with the autoregressive generation process. Notably, compared with grounding-only tasks, such an effect is more pronounced on dense captioning tasks, as textual description generation can truly benefit from the captured event-level visual semantics. Effect of factorized preference optimization. Tab. 5 demonstrates that the proposed FPO algorithm consistently enhances performance on both temporal grounding and captioning-related tasks, with more significant improvement in temporal grounding. This indeed validates the effectiveness of our factorized optimization objective design. 7. Conclusion and Limitations In this work, we uncover some key limitations of existing temporal grounded video-language models: (1) the use of coupled learning objective for temporal grounding and textual response generation, and (2) the overlooking of inherent dependency between these two tasks, especially the event-level visual semantics from the intermediate grounding result. We tackle this from factorized learning perspective, where we propose D2VLM that decomposes the generation objective into grounding then answering with evidence referencing paradigm, and introduce evidence tokens to emphasize explicit visual semantic capture beyond timestamp representation. Furthermore, we design factorized preference learning algorithm, coupled with factorized synthetic dataset, to enhance the learning of both tasks. Extensive experiments demonstrate the clear superiority of our approach across various scenarios. Despite achieving new state-of-the-art results, our approach still has room for improvement (e.g., F1 scores of only 14.4% on the episodic memory task and 26.4% on the YouCook2 dense video captioning task). Besides, while our factorized data generation offers good factorized controllability and directly contributes to factorized optimization, it focuses only on generating negative (dis-preferred) samples. However, generating more diverse positive samples to further enrich the preference data is also meaningful, and we leave this for future work."
        },
        {
            "title": "Acknowledgment",
            "content": "This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG3-RP-2022-030). We would like to acknowledge that computational work involved in this research work is partially supported by NUS ITs Research Computing group under grant number NUSRECHPC-00001."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 6 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 7 [3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. 3 [4] Qirui Chen, Shangzhe Di, and Weidi Xie. Grounded multihop videoqa in long-form egocentric videos. arXiv preprint arXiv:2408.14469, 2024. 2, 3, 8 [5] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 7 [6] Wenliang Dai, Junnan Li, DONGXU LI, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Instructblip: Towards general-purpose and Steven Hoi. vision-language models with instruction tuning. In Advances in Neural Information Processing Systems, pages 49250 49267. Curran Associates, Inc., 2023. 6 [7] Shangzhe Di and Weidi Xie. Grounded question-answering in long egocentric videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1293412943, 2024. [8] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1935819369, 2023. 6 [9] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 1, 6 [10] Yongxin Guo, Jingyu Liu, Mingda Li, Dingxin Cheng, Xiaoying Tang, Dianbo Sui, Qingbin Liu, Xi Chen, and Kevin Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding. arXiv preprint arXiv:2405.13382, 2024. 1, 2, 3, 7 [11] Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Qingbin Liu, and Xi Chen. Trace: Temporal grounding video llm via causal event modeling. In International Conference on Learning Representations, 2025. 2, 3, 7 [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [13] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1427114280, 2024. 2, 3, 7 [14] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In European Conference on Computer Vision, pages 202218. Springer, 2024. 2, 3, 7, 8 [15] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. 3 [16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 6 [17] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665, 2023. 3 [18] Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy. Temporal preference optimization for longform video understanding. arXiv preprint arXiv:2501.13919, 2025. 3, 5, 6 [19] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2024. 6, [20] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, YiQing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Vu Tu, et al. Groundinggpt: Language enhanced multi-modal grounding model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66576678, 2024. 3 [21] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, 2024. 1, 2, 7 [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2023. 4 [23] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. E.T. Bench: Towards open-ended 9 In The Thirtyevent-level video-language understanding. eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 1, 2, 3, 6, 7, 8 [24] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pages 22631 22648. PMLR, 2023. [25] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1258512602, 2024. 1, 2 [26] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. 7 [27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 3 [28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 3 [29] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, TatSeng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning. arXiv preprint arXiv:2402.11435, 2024. 3, 7, 8 [30] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 3, [31] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 1, 2, 3, 7 [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3 [33] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. 3 [34] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 6 [35] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large mulIn Annual timodal models with factually augmented rlhf. Meeting of the Association for Computational Linguistics, 2024. 3 [36] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temarXiv poral grounding in video large language models. preprint arXiv:2410.03290, 2024. 3 [37] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training videoarXiv preprint text llms for grounding text in videos. arXiv:2403.10228, 2024. 3, [38] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 4 [39] Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question In Proceedings of the IEEE/CVF Conference answering. on Computer Vision and Pattern Recognition, pages 13204 13214, 2024. 2 [40] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 7 [41] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 6 [42] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 6 [43] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, et al. Timesuite: Improving mllms for long video understanding via grounded tuning. In International Conference on Learning Representations, 2025. 3, 10 [44] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 1, 2 [45] Mengxi Zhang, Wenhao Wu, Yu Lu, Yuxin Song, Kang Rong, Huanjin Yao, Jianbo Zhao, Fanglong Liu, Haocheng Feng, Jingdong Wang, et al. Automated multi-level preference for mllms. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 3 [46] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. 3, 6 [47] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 1, 6 [48] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. URL https://arxiv. org/abs, page 14, 1909. 3 Factorized Learning for Temporally Grounded Video-Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. More Implementation Details Method Year TEM (Rec) GVQ (Rec) Here we provide more implementation details in addition to the main paper. Following existing practice [9, 11], we adopt 1 FPS frame sampling for both training and testing. Frames are resized to 224 224 before being fed into the network. To determine the salient tokens for grounding and explicit event-level visual semantic capture, we treat frames within the ground-truth interval as salient during training. During inference, if the feature similarity between framelevel video token and <evi> exceeds 60% of the maximum similarity between the current <evi> token and all frame tokens, the corresponding frame-level video token is considered salient and included. Sec. C.4 demonstrates the performance robustness to threshold variation. Before similarity calculation, the <evi> token is first projected through 2-layer MLP. This projection helps to distinguish its two functional roles: serving as generation token during autoregressive decoding (via standard LM classification head), and acting as query token for similarity-based grounding and visual semantic aggregation. This design facilitates the joint learning of these related but functionally distinct tasks. For the performance comparison among different methods, most reported numbers are directly taken from the original papers, except for Qwen2.5-VL [1] on E.T. Bench [11], which we re-implemented due to the absence of official results. We found that the performance is highly sensitive to the prompt and pixel configurations, which aligns with findings discussed in the context of video temporal grounding on GitHub1. We combine the official cookbook from Qwen2.5-VL and the practice from lmms-eval2, resulting in considerably higher performance compared with directly using the official cookbook which may not be tailored for benchmarking purposes. We report the highest performance of Qwen2.5-VL that we were able to achieve in our paper. B. Data Annotation Formats Here, we also provide the annotation formats for model training, which offer an intuitive and clear understanding of D2VLMs generation objective. Based on the input-output format, we categorize different tasks into three main types: (1) Grounding-focused task (e.g., temporal video grounding, action localization, etc.), which can involve single- (2) Dense event grounding and multi-event grounding. 1https://github.com/QwenLM/Qwen2.5VL/issues/ 837 2https://github.com/EvolvingLMMs-Lab/lmms-eval Video-ChatGPT-7B [12] Video-LLaVA-7B [10] LLaMA-VID-7B [9] Video-LLaMA-2-7B [3] PLLaVA [14] VTimeLLM-7B [6] VTG-LLM-7B [5] TimeChat-7B [13] LITA-13B [7] E.T. Chat-3.8B [11] D2VLM-3.8B (Ours) ACL24 EMNLP24 ECCV24 arXiv24 arXiv24 CVPR24 AAAI25 CVPR24 ECCV24 NeurIPS24 ICCV25 15.9 7.5 7.0 0.0 4.1 6.8 8.9 18.0 16.0 16.5 29. 0.0 0.1 0.9 0.1 1.2 1.9 1.4 1.5 2.2 3.7 7.1 Table 1. Performance comparison on TEM (Temporal Event Matching) and GVQ (Grounded Video Question answering). captioning-related task, which requires grounding multiple events throughout the entire video while also providing textual description for each grounded event. (3) temporally grounded video question answering, which involves answering the users open-ended questions while also providing the temporal position of the answer (evidence). The examples are shown in Fig. 1. Here, we provide the definitions of some important keys in the annotation files. The conversations key is the main component, which consists of two sub-parts: from human and from gpt. The value corresponding to the from human part represents the input prompt, which mainly includes the video (represented here as place-holder <image>, but will be actually replaced by video frames) and the user question (instruction). The second part, from gpt, represents the desired model response sequence, which typically consists of two stages: the pure evidence grounding stage and the interleaved text-evidence token generation stage. These two stages are separated by the </evi> token, which the model should also generate to indicate the end of the evidence grounding stage and the beginning of the interleaved response. Another important key is time gt, which indicates the ground-truth temporal event position. This is used to supervise the similarity calculation between the <evi> token and frame-level tokens, as mentioned in this paper. Here, the ground-truth annotations for the evidence grounding stage and the interleaved response stage are the same, based on the natural assumption that the grounded evidence should be consistent with the answer. C. More Experimental Results C.1. Performance on E.T. Bench Complex Dataset Here we also compare the performance on E.T. Bench Comtemporal plex dataset [11] that involves two sub-tasks: 12 Method MVBench Video-MME (w/o subs) Video-LLaVA-7B [10] E.T. Chat-3.8B [11] D2VLM-3.8B (Ours) 43.0 36.4 43.9 39.9 34.5 43.9 Table 2. Performance comparison on general video-questionanswering benchmarks. Threshold Grounding AvgF 1 Dense Captioning AvgSim AvgF 0.4 0.5 0.6 0.7 0.8 40.9 41.7 42.3 42.1 39.7 36.2 37.1 37.5 35.6 31.5 20.9 21.3 21.8 21.2 19.9 Table 3. Threshold analysis on E.T. Bench data. event matching and grounded video question answering. The results are shown in Tab. 1. It can be seen that our approach also outperforms the existing state of the art by large margins, further demonstrating its superiority. C.2. Extension to General QA Tasks We test our model on general video question answering benchmarks (MVBench [8] and Video-MME [4]). To enhance basic instruction-following capability, we incorporate automatically constructed multiple-choice questions during the proposed factorized preference optimization process. Due to our proposed factorized preference data synthesis, we can easily generate diverse distractor options based on different causes of failure and combine them with the original correct answer to form multiple-choice questions, without requiring additional external data sources. As shown in Tab. 2, our method outperforms the grounding-focused counterpart E.T. Chat [11] and achieves results comparable to some general video understanding models (e.g., Video-LLaVA [10]) trained on large-scale generic data, but are usually less effective on grounding. We attribute the performance gap between our model and recent SOTA methods [1, 2] to the absence of large-scale generic pretraining and the relatively smaller model size. Incorporating such data and scaling up the model could further improve our framework. Meanwhile, it is also worth exploring how to train model that can simultaneously achieve strong general reasoning and accurate temporal grounding. C.3. Cost of Frame-Wise Similarity Calculation Since the designed <evi> token involves additional framewise feature similarity computation for temporal grounding and visual semantic aggregation beyond the standard autoregressive decoder, it is natural to evaluate the associated computational cost. Such frame-wise similarity calculation process is actually lightweight, taking less than 0.4 ms per token generation on single 3090 GPUonly 1.4% of the total network forwarding time (29 ms). C.4. Sensitivity Analysis on Similarity Threshold As shown in Tab. 3, performance is relatively robust across different threshold values for salient frame identification during inference, and the intuitive choice of 0.5 already yields acceptable results. Overall, an overly high threshold causes information loss, while an overly low one introduces less relevant context. The best performance is achieved at threshold of 0.6. D. More about the Factorized Data Synthesis As mentioned in the main paper, we mainly focus on two main factors: temporal event grounding and textual response, where each factor can be further categorized into multiple sub-factors. For temporal event grounding aspects, sub-factors include temporal localization shift, randomly adding or deleting grounded events (corresponding to the simulation of false positives and missed detection), and merging multiple events into one (corresponding to the simulation of lack of fine-grained distinction in event boundaries). full demonstration example can be found at Fig. 2. For textual response aspect, this type of perturbation modifies the semantic correctness of the textual response. It includes sub-types such as distorting key information, which disrupts critical content, and repeating responses, common failure mode observed in video LLMs. Except for the repeating factor, we prompt an off-the-shelf LLM [15] to generate distracted response based on the original correct event-level response. E. Visualization Results Here we provide qualitative results to better demonstrate the capability of our approach. Based on the input-output format, we categorize different tasks into three main types: (1) Dense captioning related task, which requires grounding multiple events throughout the entire video while also providing textual description for each grounded event. (2) Grounding-focused task (e.g., temporal video grounding, action localization, etc.), which includes single-event grounding and multi-event grounding. (3) Temporally grounded video question answering, which involves answering the users open-ended questions while also providing the temporal position of the answer (evidence). We also visualize the prediction result from the recent SOTA method [11] for comparison. Note that in the response from D2VLM, all temporal information is derived from the generated <evi> token through the conversion process illustrated in the main paper. For each input, D2VLM will first perform pure evidence grounding, followed by interleaved 13 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1427114280, 2024. [7] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In European Conference on Computer Vision, pages 202218. Springer, 2024. [8] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. [9] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: In An image is worth 2 tokens in large language models. European Conference on Computer Vision, pages 323340. Springer, 2024. [10] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, 2024. [11] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. E.T. Bench: Towards open-ended In The Thirtyevent-level video-language understanding. eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [12] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1258512602, 2024. [13] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. [14] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [15] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. text-evidence generation (here we denote this as its actual response part). We show the converted time-involved text for both stages, where the actual response stage begins after the Answer: marker. Dense captioning task. From Fig. 3, we can observe that: (1) Compared with the recent counterpart, our method can better localize the individual events. (2) Our method also generates more coherent and meaningful textual description, whereas the compared method often fails to do so and repeatedly generates similar content. These results essentially demonstrate the superiority of our approach in both event grounding and textual generation. Grounding-focused task. The qualitative examples are shown in Fig. 4. It can be observed that: (1) Compared with recent SOTA method [11], our approach can localize the desired temporal event position more accurately (Fig. 4 (a)). (2) Our method also better distinguishes the boundaries between individual events, demonstrating its fine-grained discrimination capability (Fig. 4 (b)). Temporally grounded video question answering. The example is shown in Fig. 5. It can be observed that: (1) Our method can correctly answer the question, while the compared method fails to follow the instruction given by the user (i.e., only responding to the temporal evidence position without answering the question). (2) Our method can also provide more reliable temporal evidence grounding."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, and Mike Zheng Shou. Livecc: Learning video llm with streamIn Proceedings of the ing speech transcription at scale. Computer Vision and Pattern Recognition Conference, pages 2908329095, 2025. [3] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [4] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. [5] Yongxin Guo, Jingyu Liu, Mingda Li, Dingxin Cheng, Xiaoying Tang, Dianbo Sui, Qingbin Liu, Xi Chen, and Kevin Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding. arXiv preprint arXiv:2405.13382, 2024. [6] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Figure 1. Qualitative examples for grounding-focused task, dense captioning-related task, and temporally grounded video question answering task. 15 Figure 2. An illustrative example of the data synthesis approach. Figure 3. qualitative example for dense captioning task. 16 Figure 4. Qualitative examples for grounding-focused task. Figure 5. qualitative example for temporally grounded video question answering task."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}