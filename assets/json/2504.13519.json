{
    "paper_title": "Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering",
    "authors": [
        "Yipeng Sun",
        "Linda-Sophie Schneider",
        "Mingxuan Gu",
        "Siyuan Mei",
        "Chengze Ye",
        "Fabian Wagner",
        "Siming Bayer",
        "Andreas Maier"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable self-supervised single-image denoising framework -- Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through a lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted post-training for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce a novel downsampling shuffle strategy with a new self-supervised loss function that extends the concept of Noise2Noise to a single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at https://github.com/sypsyp97/Filter2Noise.git ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . e [ 1 9 1 5 3 1 . 4 0 5 2 : r Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering Yipeng Sun1, Linda-Sophie Schneider1, Mingxuan Gu1, Siyuan Mei1, Chengze Ye1, Fabian Wagner2, Siming Bayer1, and Andreas Maier1 1 Friedrich-Alexander-University Erlangen-Nuremberg, Erlangen, Germany 2 Siemens Healthineers AG, Forchheim, Germany yipeng.sun@fau.de Abstract. Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and selfsupervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable selfsupervised single-image denoising frameworkFilter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted posttraining for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce novel downsampling shuffle strategy with new self-supervised loss function that extends the concept of Noise2Noise to single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at https://github.com/sypsyp97/Filter2Noise.git. Keywords: Self-Supervised Learning Single-Image Denoising Computed Tomography"
        },
        {
            "title": "Introduction",
            "content": "Low-dose computed tomography (CT) reduces radiation exposure but increases image noise, potentially compromising diagnostic accuracy [2,26]. Effective denoising is essential for preserving diagnostic quality and ensuring optimal patient outcomes despite the reduced dose. Traditional methods, such as Non-Local Means (NLM) [1] and BM3D [3], often face challenges in clinical applications due to their high computational cost, sensitivity to parameter tuning, and assumptions about noise characteristics [28,27,4]. Deep learning has demonstrated 2 Y. Sun et al. superior performance, with supervised methods achieving remarkable results [13,29,21,17]. However, their dependence on large, paired datasets presents significant limitation, particularly in the low-dose CT setting, where obtaining such data is both ethically and practically difficult [33,26]. Self-supervised and unsupervised learning offer solution by eliminating the need for clean data [23,6,12,29,18]. Noise2Noise [10] and Noise2Void [9] are promising examples, but Noise2Noise requires multiple noisy views of the same scene (often clinically unavailable), while the assumptions of Noise2Void about noise properties do not hold true for low-dose CT [11,13,16]. Many self-supervised methods, while reducing the need for clean data, still require substantial amounts of noisy data. Furthermore, the black-box nature of most deep learning models raises concerns in medical imaging, where data integrity is critical [8,17,7]. Interpretability and user control are therefore crucial. To tackle these challenges, we present Filter2Noise (F2N), an interpretable and self-supervised framework for single-image denoising in low-dose CT scans. Our contributions are threefold: First, we create transparent alternative to complex deep networks through our novel Attention-Guided Bilateral Filter (AGBF). AGBF learns to predict spatially varying filter parameters, adapting the denoising to the specific noise characteristics in each image. Unlike traditional bilateral filters with fixed parameters [25], AGBF dynamically responds to image content, simultaneously improving denoising performance and providing visual insight into the filtering process. Second, we provide post-training user control over the denoising strength through simple parameter adjustments. This is critical feature for targeted refinement in clinical settings, allowing medical professionals to adjust results as needed for different diagnostic purposes. Finally, for single-image selfsupervised learning, we develop novel multi-scale self-supervised loss combined with downsampling strategy enhanced by our Euclidean Local Shuffle (ELS) technique, designed to address spatially correlated noise. ELS disrupts noise correlations while preserving local image structures, preventing the model from collapsing into trivial identity mapping. Filter2Noise is engineered to achieve high performance on real low-dose CT datasets, while offering transparency, user control, and parametric efficiency compared to existing self-supervised single-image methods. To the best of our knowledge, this work presents the first interpretable, self-supervised framework for single-image denoising of low-dose CT images."
        },
        {
            "title": "2 Methodology",
            "content": "Self-supervised single-image denoising methods often construct noisy image pairs from noisy image via downsampling [13,6,11], using network to map one to the other (following the Noise2Noise principle [10]): Eypnoisy (cid:2)fθ(g1(y)) g2(y)2(cid:3) , min θ (1) Interpretable Self-Supervised Single LDCT Image Denoising 3 Fig. 1: (a) The Filter2Noise (F2N) denoising pipeline. (b) The downsampling strategy, following ZS-N2N [13]. (c) Our proposed Euclidean Local Shuffle (ELS). where is the noisy image, g1 and g2 are downsampling operators, and fθ is neural network. However, this network-based approach is often black-box [13,6,11], and highly correlated noise can cause the network to learn an identity mapping instead of effective denoising. To overcome these issues, we propose the Filter2Noise (F2N) framework, which integrates Attention-Guided Bilateral Filtering (AGBF) as its core denoising mechanism. The following sections detail both the AGBF and the training strategy for F2N."
        },
        {
            "title": "2.1 Attention-Guided Bilateral Filtering (AGBF)",
            "content": "AGBF is modified bilateral filter for interpretable and adaptive denoising. Standard bilateral filters [22] use fixed global parameters, limiting their adaptability to spatially varying noise. AGBF fundamentally changes this by rendering the filter parameters spatially dependent and conditioned on the input image content. This allows the filter to adaptively adjust its smoothing behavior based on local image features and noise levels. Figure 1a depicts the architecture of AGBF, which employs dual-attention module to estimate the spatially varying parameters σ(n) (range and spatial standard deviations) for each image patch at every denoising stage n. In contrast to conventional bilateral filters that apply uniform smoothing across the image, AGBF dynamically adjusts to local image characteristics, thereby enhancing spatial adaptivity. , and σ(n) , σ(n) To provide context for our modifications, we first present an overview of the standard bilateral filter [22]. It computes denoised pixel fθ(y)x,y as weighted average of neighboring pixels. yi,j represents the intensity at the position i, j: 4 Y. Sun et al. fθ(y)x,y ="
        },
        {
            "title": "1\nWx,y",
            "content": "x+k (cid:88) y+k (cid:88) i=xk j=yk yi,j ws(i, j; x, y) wr(i, j; yx,y), (2) where ws and wr are spatial and range weights. These weights are determined by fixed spatial standard deviations (σx, σy) and range standard deviation (σr). Wx,y is normalization factor, is the kernel size: Wx,y = x+k (cid:88) y+k (cid:88) i=xk j=yk ws(i, j; x, y) wr(i, j; yx,y). (3) The proposed AGBF (Figure 1a) extends the standard bilateral filter by replacing fixed parameters with spatially varying values: σ(n) . These parameters are estimated for each small image patch through lightweight dualattention module. To balance local adaptivity with computational efficiency, we implement patch size of = 8 at each denoising stage n. , and σ(n) , σ(n) Input RBCHW is divided into patches, flattened, and processed by dual-attention module consisting of Feature Attention and Sigma Attention. Feature Attention: This step extracts patch-wise features to understand the context of each image patch. It uses linear layers to create query (Q), key (K), and value (V ) vectors: = Wqxp, = Wkxp, = Wvxp, (4) where xp RBN (P 2C) is the flattened patch data, = (H/P ) (W/P ) counts the patches, and Wq, Wk, Wv are trainable weights. These vectors are fed into scaled dot-product attention [24], which computes contextualized representation of each patch for further processing. , σ(n) Sigma Attention: This step predicts the standard deviations (σ(n) ) using features from Feature Attention. Distinct linear layers (Wqσ, Wkσ, Wvσ) form query (Qσ), key (Kσ), and value (Vσ) vectors tailored for this task. These are processed via attention, normalized with Layer Normalization, and projected. For instance, the range standard deviation is calculated as follows: , σ(n) σ(n) = Softplus (cid:16) Wσ Attention(Qσ, Kσ, Vσ) (cid:17) , (5) where Wσ projects the output, and Softplus ensures non-negative constraint. Spatial standard deviations (σ(n) ) are calculated following the same method. The predicted standard deviation values are then used to compute spatially , σ(n) varying weighting kernels. At stage n, the spatial weighting kernel is: w(n) (i, j; x, y) = exp (i x)2 (cid:16) σ(n) (cid:17)2 , (6) (j y)2 (cid:17)2 (cid:16) σ(n) 2 Interpretable Self-Supervised Single LDCT Image Denoising 5 and the range weighting kernel is: w(n) (i, j; x, y) = exp yi,j yx,y2 σ(n) (cid:17)2 (cid:16) . (7) These kernels are applied in Equation 2 to obtain the denoised output (n)(y)x,y. The kernel size adapts per patch at stage using = 2max(σ(n) )+1. This rounds up the maximum spatial standard deviation plus one, doubled for symmetry, balancing the effective filtering with computational cost. , σ(n) 2."
        },
        {
            "title": "Interpretability and User Control",
            "content": "A further advantage of AGBF is its inherent interpretability. As the denoising process is controlled by learned, spatially varying standard deviations (σ(n) , , σ(n) σ(n) ), visualizing these parameters offers insight into the denoising behavior. Higher σr values indicate stronger smoothing in areas with greater noise intensity. Moreover, AGBF allows users to adjust the standard deviation maps post-training, enabling region-specific denoising, such as increasing σ(n) for suspected lesions, thereby enhancing diagnostic confidence and transparency. Last but not least, users can define upper limits for filter parameters to prevent excessive blurring. These limits can be tailored to anatomical regions, accommodating different noise properties or radiologist preferences. r"
        },
        {
            "title": "2.3 Training Strategy for Filter2Noise (F2N)",
            "content": "The F2N framework (Figure 1) leverages multi-stage AGBF. Our training strategy exploits image self-similarity across scales and addresses spatially correlated noise. We introduce multi-scale reconstruction loss, edge preservation regularization, and the proposed Euclidean Local Shuffle (ELS). Downsampling Following Zero-Shot Noise2Noise (ZS-N2N) [13], two downsampled images, g1(y) and g2(y), are generated from the noisy input with 2 2 convolution (stride 2) using following kernels: F1 = (cid:20) 0 0.5 (cid:21) 0.5 0 , F2 = (cid:21) (cid:20)0.5 0 0 0.5 . (8) These kernels average different pixel combinations, as illustrated in Figure 1b. Euclidean Local Shuffle (ELS) To address spatially correlated noise in lowdose CT, we present ELS (Figure 1c), grounded in Rudermans local statistical invariance [19]. While downsampling alone struggles to decorrelate noise, ELS disrupts local patterns by rearranging pixels in 2 2 blocks with minimum Y. Sun et al. Euclidean distance, maintaining local statistics while breaking noise correlation. For 2 2 block with pixel values (a, b, c, d), the Euclidean distances are: dab = b, dbc = c, dac = c, dbd = d, dad = d, dcd = d. (9) After determining the minimum distance dmin = min(dab, dac, dad, dbc, dbd, dcd), the corresponding pixels are swapped. This operation, denoted as E(y), is applied to the downsampled images, generating E(g1(y)) and E(g2(y)). Loss Function For self-supervised single-image training, the total loss Ltotal combines reconstruction loss, Lrec, and weighted regularization term λLreg, where λ is the weighting factor: Ltotal = Lrec + λLreg. (10) Reconstruction Loss (Lrec): As shown in Figure 1a, this term ensures consistency across resolutions. It includes resolution loss between denoised outputs of shuffled, downsampled images; cross-scale loss with the downsampled versions of the denoised full image; and denoise loss for consistency: Lrec = 1 3 fθ(E(g1(y))) fθ(E(g2(y)))1 + 1 3 +fθ(E(g2(y))) g2(fθ(y))1) + (fθ(E(g1(y))) g1(fθ(y))1 1 g1(fθ(y)) g2(fθ(y))1. (11) The key idea is to ensure that clean image content remains consistent across scales and transformations during the denoising process. Regularization Loss (Lreg): This term preserves edges and prevents excessive blurring by penalizing differences between the absolute Difference of Gaussian (DoG) filtered noisy and denoised images: Lreg = DoG(y) DoG(fθ(y)) 1 , (12) where DoG is defined as the difference of two Gaussian filters with standard deviations s1 and s2 (s1 < s2). We empirically set s1 = 9 and s2 = 10 to enhance robustness against noise. Equations for DoG are omitted for brevity."
        },
        {
            "title": "3 Experiments and Discussions",
            "content": "Our experiments evaluate Filter2Noise (F2N) for denoising low-dose CT images against other self-supervised single-image methods, assessing denoising performance, parametric efficiency, learned parameter interpretability, and key design impacts through ablation studies. Interpretable Self-Supervised Single LDCT Image Denoising 7 Table 1: Results on the Mayo Low-Dose CT Challenge. B30/D45: reconstruction kernels. F2N-S1/S2: one/two AGBF layers. Inference time per slice is measured on an NVIDIA RTX 4070 Super GPU. Two-sided paired t-tests [5] compare each method with F2N-S2. indicates no significant difference (p > 0.05). Mayo-2016 B30 Mayo-2016 D45 Mayo-2020 GPU Time # Params. Method PSNR (dB) SSIM PSNR (dB) SSIM PSNR (dB) SSIM BM3D (TIP2007) [3] DIP (CVPR2018) [23] N2V (CVPR2019) [9] NB2NB (TIP2022) [6] ZS-N2N (CVPR2023) [13] F2N-S1 w/o ELS (Ours) F2N-S2 w/o ELS (Ours) F2N-S1 (Ours) F2N-S2 (Ours) 37.15 37.94 33.63 36.63 35.13 35.11 35.62 39.54 39. 89.41 90.74 88.45 89.79 84.33 84.24 85.08 91.35 91.78 35.48 36.23 35.78 37.70 38.01 37.05 37.14 37.79 38.03 87.80 84.97 86.73 89.89 89.24 87.33 87.41 89.17 89. 36.50 36.69 33.56 36.97 37.21 36.98 36.44 37.19 37.28 88.27 85.92 83.68 86.09 88.43 86.92 86.97 89.92 90.09 3 sec. 3 min. 10 min. 90 sec. 22 sec. 10 sec. 20 sec. 11 sec. 20 sec. 2.2M 2.2M 1.3M 22k 1.8k 3.6k 1.8k 3.6k (a) Mayo-2016 B30 (b) Mayo-2016 D45 (c) Mayo-2020 Fig. 2: Denoising Comparison Across Different Noise Conditions."
        },
        {
            "title": "3.1 Experimental Configuration",
            "content": "To evaluate F2N, we utilized the Mayo-2016 and Mayo-2020 low-dose CT datasets from the NIH AAPM-Mayo Clinic Grand Challenge [14,15]. This decision considered the crucial impact of reconstruction kernels on noise characteristics [30,31,32]. Smoother kernels like B30 produce correlated noise, while sharper kernels like D45 generate more random noise. We divided Mayo-2016 into B30 and D45 subsets (526 slices/1mm thickness each, same patient) and used the Mayo-2020 dataset (641 slices, two patients, weighted FBP reconstruction [20]) to cover different noise profiles. All 512 512 pixel images were denoised using self-supervised single-image methods, requiring no training data, and all compared methods were trained on single NVIDIA A6000 GPU using the Adam optimizer with learning rate of 1 103 until convergence. 8 Y. Sun et al."
        },
        {
            "title": "3.2 Experimental Results",
            "content": "Table 1 and Figure 2 compare F2N to BM3D [3], DIP [23], Noise2Void (N2V) [9], Neighbor2Neighbor (NB2NB) [6], and Zero-Shot Noise2Noise (ZS-N2N) [13]. While BM3D effectively reduces noise, it tends to over-smooth fine details. ZSN2N and NB2NB perform well on Mayo-2016 D45 and Mayo-2020 but degrade on Mayo-2016 B30 (lower PSNR/SSIM), likely because the correlated noise challenges their downsampling strategy. Filter2Noise consistently achieves superior performance. The two-stage Filter2Noise (F2N-S2) achieves the highest PSNR and SSIM values in most cases, demonstrating the effectiveness of AGBF and our training strategy, while using significantly fewer parameters than other methods. The single-stage Filter2Noise (F2N-S1) also performs strongly, close to F2N-S2, indicating that even single AGBF network can denoise effectively. Regarding computational efficiency, F2N benefits from its lightweight design to offer faster inference than other self-supervised single-image approaches, although it does not match the processing speed of BM3D. compared to σ(2) Interpretability Analysis Figure 3a visualizes the spatially varying standard deviation parameters, providing insight into the denoising behavior of F2N-S2. The larger σ(1) indicates that the first stage performs the primary noise reduction, with the second stage refining the result. The consistently larger σx compared to σy suggests preferential smoothing along the x-direction. The spatial variation within each illustration demonstrates the adaptive nature of the filtering, reflecting local image content and noise. Ablation Study We focused our ablation study on the regularization weight λ (Equation 10) and the presence of ELS, as performance was most sensitive to variations in these factors. λ controls the trade-off between noise reduction and edge preservation. Figure 3b shows results for different λ values on F2N-S2: lower values lead to blurring (weaker edge preservation), while higher values result in sharper images but with more residual noise. Optimal tuning of λ is therefore essential. ELS is also critical; without it, performance degrades significantly, especially with spatially correlated noise (B30, Table 1)."
        },
        {
            "title": "4 Conclusion",
            "content": "This paper introduced Filter2Noise (F2N), an interpretable framework for selfsupervised single-image low-dose CT denoising. Our Attention-Guided Bilateral Filter (AGBF) adaptively predicts spatially varying parameters, providing transparency through visualizable maps and user control. Experiments on Mayo datasets demonstrated F2N outperforms state-of-the-art self-supervised methods while using only 3.6k parameters. Our two-stage F2N achieved PSNR improvements up to 4.59 dB compared to ZS-N2N, with consistent performance across various reconstruction kernels. Future work will focus on accelerating inference through custom CUDA kernel implementation and extending F2N to other medical imaging modalities. Interpretable Self-Supervised Single LDCT Image Denoising 9 (a) Visualization of the spatially varying standard deviation maps from F2N-S2. (b) Ablation study on λ (Equation 10). PSNR and SSIM values are shown. Fig. 3: Interpretability Analysis and Ablation Study of F2N-S2. Acknowledgements This research was financed by the Verbundprojekt 05D2022 - KI4D4E: Ein KI-basiertes Framework für die Visualisierung und Auswertung der massiven Datenmengen der 4D-Tomographie für Endanwender von Beamlines. Teilprojekt 5. (Grant number: 05D23WE1)."
        },
        {
            "title": "References",
            "content": "1. Buades, A., Coll, B., Morel, J.M.: Non-local means denoising. Image Processing On Line 1, 208212 (2011). https://doi.org/10.5201/ipol.2011.bcm_nlm 2. Chen, H., Zhang, Y., Zhang, W., Liao, P., Li, K., Zhou, J., Wang, G.: Low-dose ct denoising with convolutional neural network. In: 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). pp. 143146. IEEE (2017) 3. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Transactions on Image Processing 16(8), 20802095 (2007). https://doi.org/10.1109/TIP.2007.901238 4. Hendriksen, A.A., Pelt, D.M., Batenburg, K.J.: Noise2inverse: Self-supervised deep convolutional denoising for tomography. IEEE Transactions on Computational Imaging 6, 13201335 (2020) 5. Hsu, H., Lachenbruch, P.A.: Paired test. Wiley StatsRef: statistics reference online (2014) 6. Huang, T., Li, S., Jia, X., Lu, H., Liu, J.: Neighbor2neighbor: Self-supervised denoising from single noisy images. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1478114790 (2021) 7. Huang, Y., Preuhs, A., Lauritsch, G., Manhart, M., Huang, X., Maier, A.: Data consistent artifact reduction for limited angle tomography with deep learning prior. In: International workshop on machine learning for medical image reconstruction. pp. 101112. Springer (2019) 8. Izadi, S., Sutton, D., Hamarneh, G.: Image denoising in the deep learning era. Artificial Intelligence Review 56(7), 59295974 (2023) 9. Krull, A., Buchholz, T.O., Jug, F.: Noise2void-learning denoising from single noisy images. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 21292137 (2019) 10 Y. Sun et al. 10. Lehtinen, J.: Noise2noise: Learning image restoration without clean data. arXiv preprint arXiv:1803.04189 (2018) 11. Lequyer, J., Philip, R., Sharma, A., Hsu, W.H., Pelletier, L.: fast blind zero-shot denoiser. Nature Machine Intelligence 4(11), 953963 (2022) 12. Li, T., Wang, L., Xu, Z., Zhu, L., Lu, W., Huang, H.: Positive2negative: Breaking the information-lossy barrier in self-supervised single image denoising. arXiv preprint arXiv:2412.16460 (2024) 13. Mansour, Y., Heckel, R.: Zero-shot noise2noise: Efficient image denoising without any data. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1401814027 (2023) 14. McCollough, C.H., Bartley, A.C., Carter, R.E., Chen, B., Drees, T.A., Edwards, P., Holmes III, D.R., Huang, A.E., Khan, F., Leng, S., et al.: Low-dose ct for the detection and classification of metastatic liver lesions: results of the 2016 low dose ct grand challenge. Medical physics 44(10), e339e352 (2017) 15. Moen, T.R., Chen, B., Holmes III, D.R., Duan, X., Yu, Z., Yu, L., Leng, S., Fletcher, J.G., McCollough, C.H.: Low-dose ct image and projection dataset. Medical physics 48(2), 902911 (2021) 16. Pang, T., Zheng, H., Quan, Y., Ji, H.: Recorrupted-to-recorrupted: Unsupervised deep learning for image denoising. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 20432052 (2021) 17. Prakash, M., Delbracio, M., Milanfar, P., Jug, F.: Interpretable unsupervised diversity denoising and artefact removal. arXiv preprint arXiv:2104.01374 (2021) 18. Quan, Y., Chen, M., Pang, T., Ji, H.: Self2self with dropout: Learning selfsupervised denoising from single image. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 18901898 (2020) 19. Ruderman, D., Bialek, W.: Statistics of natural images: Scaling in the woods. Advances in neural information processing systems 6 (1993) 20. Stierstorfer, K., Rauscher, A., Boese, J., Bruder, H., Schaller, S., Flohr, T.: Weighted fbpa simple approximate 3d fbp algorithm for multislice spiral ct with good dose usage for arbitrary pitch. Physics in Medicine & Biology 49(11), 2209 (2004) 21. Tian, C., Fei, L., Zheng, W., Xu, Y., Zuo, W., Lin, C.W.: Deep learning on image denoising: An overview. Neural Networks 131, 251275 (2020) 22. Tomasi, C., Manduchi, R.: Bilateral filtering for gray and color images. In: Proceedings of the 1998 IEEE International Conference on Computer Vision. pp. 839846. Bombay, India (January 1998) 23. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Deep image prior. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 94469454 (2018) 24. Vaswani, A.: Attention is all you need. Advances in Neural Information Processing Systems (2017) 25. Wagner, F., Thies, M., Gu, M., Huang, Y., Pechmann, S., Patwari, M., Ploner, S., Aust, O., Uderhardt, S., Schett, G., et al.: Ultralow-parameter denoising: trainable bilateral filter layers in computed tomography. Medical physics 49(8), 51075120 (2022) 26. Wagner, F., Thies, M., Pfaff, L., Maul, N., Pechmann, S., Gu, M., Utz, J., Aust, O., Weidner, D., Neag, G., et al.: Noise2contrast: Multi-contrast fusion enables self-supervised tomographic image denoising. In: International Conference on Information Processing in Medical Imaging. pp. 771782. Springer (2023) 27. Wang, J., Di, S., Chen, L., Ng, C.W.W.: Noise2info: Noisy image to information of noise for self-supervised image denoising. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1603416043 (2023) Interpretable Self-Supervised Single LDCT Image Denoising 11 28. Wang, Z., Cun, X., Bao, J., Zhou, W., Liu, J., Li, H.: Uformer: general u-shaped transformer for image restoration. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1768317693 (2022) 29. Xie, Y., Wang, Z., Ji, S.: Noise2same: Optimizing self-supervised bound for image denoising. Advances in neural information processing systems 33, 2032020330 (2020) 30. Zeng, R., Lin, C.Y., Li, Q., Jiang, L., Skopec, M., Fessler, J.A., Myers, K.J.: Performance of deep learning-based ct image denoising method: Generalizability over dose, reconstruction kernel, and slice thickness. Medical physics 49(2), 836853 (2022) 31. Zhao, H., Gu, Y., Zhao, Z., Du, B., Xu, Y., Yu, R.: Wia-ld2nd: Wavelet-based image alignment for self-supervised low-dose ct denoising. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 764774. Springer (2024) 32. Zhou, L., Zhou, Z., Huang, X., Zhang, X., Wang, H., Li, G.: Neighboring slice noise2noise: Self-supervised medical image denoising from single noisy image volume. arXiv preprint arXiv:2411.10831 (2024) 33. Zhou, S., Yu, L., Jin, M.: Supervised and unsupervised deep learning methods for low-dose ct image denoising. In: 2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC). p. 13. IEEE (October 2020). https://doi.org/10.1109/nss/mic42677.2020.9508074, http://dx.doi.org/10. 1109/nss/mic42677.2020."
        }
    ],
    "affiliations": [
        "Friedrich-Alexander-University Erlangen-Nuremberg, Erlangen, Germany",
        "Siemens Healthineers AG, Forchheim, Germany"
    ]
}