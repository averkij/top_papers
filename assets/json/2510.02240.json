{
    "paper_title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning",
    "authors": [
        "Sicheng Feng",
        "Kaiwen Tuo",
        "Song Wang",
        "Lingdong Kong",
        "Jianke Zhu",
        "Huan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 0 4 2 2 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "REWARDMAP: TACKLING SPARSE REWARDS IN FINEGRAINED VISUAL REASONING VIA MULTI-STAGE REINFORCEMENT LEARNING Sicheng Feng1, Kaiwen Tuo1,2, Song Wang3, Lingdong Kong4, Jianke Zhu3, Huan Wang1, 1Westlake University 4National University of Singapore Dataset & Toolkit: https://fscdc.github.io/RewardMap Equal contribution. Corresponding author. 3Zhejiang University 2Tongji University"
        },
        {
            "title": "ABSTRACT",
            "content": "Fine-grained visual reasoning remains core challenge for multimodal large language models (MLLMs). The recently introduced REASONMAP highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct REASONMAP-PLUS, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose REWARDMAP, multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. REWARDMAP incorporates two key designs. First, we introduce difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on REASONMAP and REASONMAP-PLUS demonstrate that each component of REWARDMAP contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with REWARDMAP achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities."
        },
        {
            "title": "INTRODUCTION",
            "content": "Fine-grained visual reasoning over structured visual inputs remains significant challenge for multimodal large language models (MLLMs) (Bai et al., 2025; OpenAI, 2025; Zhang et al., 2025b). Recently, REASONMAP (Feng et al., 2025b) was introduced as benchmark on high-resolution transit maps in the real world, where tasks (e.g., route planning) combine visual understanding with spatial reasoning, jointly constituting the fine-grained visual reasoning challenge. This task is not only of practical value for real-world navigation and transportation systems, but also of fundamental scientific interest as it exposes reasoning gaps in current MLLMs. Despite steady advances in visionlanguage pre-training (Liu et al., 2023; 2024; Bai et al., 2025), existing models consistently struggle with the visual and spatial reasoning demands in REASONMAP. This gap motivates us to investigate how reinforcement learning (RL) (Zhang et al., 2025a; Shao et al., 2024; Guo et al., 2025a) can be adapted to enhance fine-grained visual reasoning abilities in structured visual domains such as transit maps. However, directly applying standard RL methods (Rafailov et al., 2023; Shao et al., 2024) to complex tasks such as REASONMAP is highly challenging, as supervision signals are inherently sparse (i.e.,"
        },
        {
            "title": "Preprint",
            "content": "success is typically judged only at the final answer after long reasoning chain (Quadros et al., 2025; Cao et al., 2024)). The difficulty of the tasks further amplifies this sparsity, which in turn destabilizes optimization and hinders effective exploration (Wang et al., 2025a). While classical approaches like Supervised Fine-Tuning (SFT) (Liu et al., 2023; Wei et al., 2025) offer dense supervision, they fall short in equipping models for the long-chain decision-making intrinsic to visual reasoning tasks. This mismatch between task complexity and supervision signal forms critical bottleneck in leveraging RL for fine-grained visual reasoning. To address this issue, we first construct REASONMAP-PLUS, an extended dataset that introduces dense reward signals for further cold-start training. Tasks in REASONMAP-PLUS are organized along natural difficulty continuum, from simple Visual Question Answering (VQA) that enhances perception to progressively harder visual tasks reflecting the complexity of fine-grained visual reasoning queries. We further introduce REWARDMAP, multi-stage RL framework with detailed reward design. It consists of two key components: (1) reward scheme that, beyond basic format and correctness rewards, incorporates detail rewards to mitigate sparsity in supervision for hard samples and adopts difficulty-aware design to account for task complexity; and (2) cold-start strategy that departs from SFT-based initialization (Xu et al., 2024; Guo et al., 2024; Wei et al., 2025) by directly employing RL, ensuring alignment between reward signals and task objectives from the outset. Training data are organized from easy to hard across multiple stages, with dense and accessible rewards at lower levels supporting effective cold-start training. This staged strategy systematically bridges perception and reasoning within unified RL framework. We conduct extensive experiments on REASONMAP and REASONMAP-PLUS to evaluate the effectiveness of REWARDMAP. Results indicate that each component yields measurable gains, with their integration delivering the best overall performance. Moreover, beyond the targeted benchmarks, models trained with REWARDMAP achieve consistent improvements (3.47% average) across six benchmarks (Wu & Xie, 2024; Wang et al., 2024; Li et al., 2024; Wang et al., 2025d; Masry et al., 2022; Chen et al., 2024a) covering spatial reasoning, fine-grained visual reasoning, and general tasks, suggesting enhanced general visual perception and reasoning capabilities. In summary, this work makes the following contributions: (1) We introduce REASONMAP-PLUS, an extended dataset organized from easy to hard, providing dense supervision for multi-stage RL training; (2) We propose REWARDMAP, multi-stage RL framework that integrates cold-start curriculum data (i.e., easy hard) with difficulty-aware detail reward design; (3) Extensive experiments demonstrate that REWARDMAP not only improves performance on REASONMAP and REASONMAP-PLUS but also enhances performance across broader visual benchmarks beyond transit maps. Together, these contributions establish principled approach to overcoming sparse reward challenges in visual reasoning, advancing the capabilities of MLLMs in structured visual tasks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Visual Reasoning in MLLMs. The development of MLLMs has rapidly progressed from foundational models that bridge vision and language encoders, such as Flamingo (Alayrac et al., 2022), to those enhanced by visual instruction tuning like LLaVA (Liu et al., 2023; 2024). To elicit more complex, step-by-step reasoning, researchers have adapted Chain-of-Thought (CoT) (Wei et al., 2022) prompting from the language domain to the multimodal context (MCoT) (Zhang et al., 2023). However, key limitation of early MCoT is its reliance on purely textual rationales (Wang et al., 2025e), which can be bottleneck for expressing fine-grained visual logic (Zhang et al., 2025b). More recent work has thus focused on developing vision-centric reasoning processes (Dong et al., 2025; Man et al., 2025), such as generating intermediate visual representations as perception tokens to aid the reasoning chain (Bigverdi et al., 2025). Despite these advances, significant performance gap persists. Benchmarks specifically designed to test abstract, spatial, and logical reasoning, such as VisuLogic (Xu et al., 2025) and REASONMAP (Feng et al., 2025b), reveal that even state-ofthe-art models struggle with tasks that require high-fidelity visual and topological understanding, underscoring the need for new methods tailored for structured visual domains, such as transit maps. Reinforcement Learning for Visual Reasoning. Reinforcement Learning (RL) (Wu et al., 2025; Sarch et al., 2025; Feng et al., 2025a) offers powerful paradigm for enhancing the reasoning capabilities of MLLMs beyond the static nature of Supervised Fine-Tuning (SFT) (Liu et al., 2023; Wei et al., 2025). The application of RL in this domain has evolved from methods like Reinforce-"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of REASONMAP-PLUS. REASONMAP-PLUS comprises 4,018 questions from 5 extended question types and maps from 30 cities across 13 countries. ment Learning from Human Feedback (RLHF) (Bai et al., 2022) to more direct and stable policy optimization algorithms such as Direct Preference Optimization (DPO) (Rafailov et al., 2023) and Group Relative Policy Optimization (GRPO) (Shao et al., 2024). While SFT on CoT datasets can activate models latent reasoning abilities (Xu et al., 2024; Guo et al., 2024; Wei et al., 2025), it often leads to overfitting and cognitive rigidity, limiting generalization (Chu et al., 2025; Shen et al., 2025; Wang et al., 2025b). RL provides dynamic alternative by enabling models to learn reasoning policy through exploration and reward (Guo et al., 2025a; Zhang et al., 2025a). However, its direct application to complex visual reasoning is often hindered by sparse rewards (Quadros et al., 2025; Cao et al., 2024), where signal is only provided upon completion of long reasoning chain (Wang et al., 2025a). This motivates us to develop framework that can effectively apply RL to structured visual tasks by mitigating sparse reward problems from the outset. Spatial Reasoning on Maps. Spatial reasoning on transit maps (Wu et al., 2020) has traditionally been approached with multi-stage computer vision systems. These methods first employ Optical Character Recognition (OCR) (Memon et al., 2020; Liu et al., 2024) to extract textual information such as station names, and then use specialized image processing and graph algorithms to identify key topological elements like stations (nodes) and the routes connecting them (edges) (Cherry et al., 2006). Pathfinding is subsequently performed on this extracted graph representation of the transit network using classical search algorithms (Noto & Sato, 2000; Deng et al., 2012). While logical, these systems are often brittle, as errors in early stages can propagate and lead to incorrect final paths. MLLMs offer promising end-to-end alternative, yet benchmarks consistently show they fail at fundamental spatial tasks like judging relative positions and orientation (Xing et al., 2025). These failures are often attributed to architectural limitations in how vision encoders process positional information (Chen et al., 2024b). When applied directly to map-based planning, MLLMs struggle to comprehend environmental constraints and execute the multi-hop reasoning required (Feng et al., 2025b). Our work addresses above concerns by constructing cold-start data for fine-grained understanding and through multi-stage reinforcement learning training."
        },
        {
            "title": "3 REASONMAP-PLUS CONSTRUCTION",
            "content": "In this section, we first introduce the construction pipeline of REASONMAP-PLUS as shown in Figure 1, which builds on REASONMAP (Feng et al., 2025b), and comprises three stages: (1) data collection and preprocessing, (2) design-guided construction of questionanswer pairs, and (3) quality control. We report comprehensive statistical overview of REASONMAP-PLUS in Appendix A.1."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of REWARDMAP. The framework enhances fine-grained visual understanding and reasoning in MLLMs through reinforcement learning with Group Relative Policy Optimization (GRPO). It consists of two key components: (1) difficulty-aware reward design (Section 4.2), which combines format, correctness, and detail rewards with difficulty-based weighting; and (2) multi-stage RL curriculum (Section 4.3), which schedules training data from simple perception tasks to complex reasoning tasks, ensuring effective optimization tackling sparse rewards. 3.1 CONSTRUCTION PIPELINE OF REASONMAP-PLUS We follow the construction pipeline of REASONMAP on stages (1) and (3) for REASONMAP-PLUS. Specifically, for (1) data collection and preprocessing, we reuse the collected high-resolution transit maps and annotated line-stop information (refer to the Metro Data) in REASONMAP to drive the following question-answer generation; for (3) quality control, we manually review the automatically generated question-answer pairs to verify correctness and, when necessary, adjust the question distribution to maintain diversity and balanced difficulty. We then present the details of stage (2). Construction of Question-Answer Pairs in REASONMAP-PLUS. We extend the planning question in REASONMAP to 5 related categories covering counting and True or False questions (Figure 1), preserving consistency between REASONMAP-PLUS and REASONMAP. For each category, we construct questions based on predesigned question templates (see Appendix A.2) and automatically derive answers from the Metro Data to form question-answer pairs. We consider the following question types: (1) Global Counting assesses global fine-grained visual understanding by asking for the number of lines in map. This type is sparse, and each map yields only one question; (2) Local Counting evaluates local fine-grained visual understanding through two variants: counting the intermediate stops between two specified stops, and counting the number of lines that pass through specified stop; (3) True or False probes fine-grained visual understanding through two variants: judging the spatial relation between two specified stops, and between one stop and one line. We balance yes and no answers to prevent models from exploiting label frequency. Difficulty Annotation. For map difficulty, we follow the manual assignment in REASONMAP, which uniformly categorizes all maps into three levels (e.g., easy, middle, and hard). For question difficulty, these questions probe basic visual understanding of transit maps rather than the complex visual reasoning required by the planning questions in REASONMAP. Accordingly, we define question difficulty by the corresponding map difficulty label."
        },
        {
            "title": "4 METHOD",
            "content": "In this section, we propose REWARDMAP to enhance fine-grained visual understanding and reasoning in MLLMs. We begin by presenting the overview of our target tasks, baseline, and our proposed REWARDMAP. We then introduce the difficulty-aware reward design and illustrate multi-stage reinforcement learning with the Group Relative Policy Optimization (GRPO) process."
        },
        {
            "title": "4.1 OVERVIEW",
            "content": "Target Tasks. We investigate two target tasks in this paper: (1) fine-grained visual understanding in REASONMAP-PLUS (Section 3) and (2) fine-grained visual reasoning in REASONMAP (Feng et al., 2025b). Both are cast as Visual Question Answering (VQA), given high-resolution image and an instructional question Q, the model must produce an answer Aformatted that conforms to the required output format and correctly addresses Q. Unlike conventional benchmarks, our tasks foreground fine-grained perception to assess models ability to exploit high-resolution details. Additionally, the route planning task in REASONMAP further evaluates spatial reasoning. Baseline. We conduct baseline experiments under standard setup with training data from the original REASONMAP. We train the model with GRPO reinforcement learning with basic reward function consisting of format reward and correctness reward. However, training on this setting exhibits sparse rewards under high task difficulty (see the results of Qwen2.5-VL-3/7B-Instruct in Table 1). In GRPO, given an input and group = {yi}K i=1 of sampled outputs with returns {ri}K i=1, the centered group advantage drives policy updates: ˆAi = ri"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) j=1 rj, max θ L(θ) = (cid:88) i=1 ˆAi log πθ(yi x). With sparse rewards, most ri 0, so ˆAi either collapses to near zero (all failures) or becomes highly skewed (rare positives), yielding low-signal or high-variance gradients and thus slowing convergence. REWARDMAP. To mitigate the sparse reward issue observed in the baseline, we propose REWARDMAP, which consists of two components: (1) detail-oriented reward with difficulty-aware weighting, and (2) multi-stage RL regimen that exploits dense-reward questions from REASONMAPPLUS for effective cold start. Implementation details are provided below. 4.2 DIFFICULTY-AWARE REWARD DESIGN Our reward function comprises three terms: format reward, correctness reward, and detail reward, scaled by difficulty factor and weighting coefficient for the detail reward: = Wdifficulty(Rformat + Rcorrectness + α Rdetail) , where Rformat, Rcorrectness, and Rdetail denote the format, correctness, and detail rewards, respectively; α > 0 controls the relative strength of the detail term (α > 0 is set to 0.5 for subsequent training); and Wdifficulty > 0 scales the overall reward according to difficulty. Format Reward. The format reward enforces compliance with task-specific output conventions: for REASONMAP-PLUS, answers are elicited within boxed{} to localize the output; for REASONMAP, the reward is computed using the benchmarks original formatting specification1. Correctness Reward. For training data in REASONMAP-PLUS, we use exact-match scoring to compute rewards, as these questions have single deterministic ground truth (e.g., numerals or yes/no). For training data in our proposed REASONMAP, the correctness reward is computed using the benchmarks official evaluation algorithm2. Detail Reward. To alleviate sparse rewards caused by the difficulty of planning tasks, we add detail reward that grants partial credit for correct items of the answer. Specifically, we reward/penalize correctness of the origin and destination stops, route names, transfer stations, and the number of route segments (see the computation pipeline in Algorithm 1). Additionally, we modulate this influence of detail reward on training with weighting coefficient α. Difficulty-Aware Weighting. To incorporate problem difficulty, we scale the sum of the three rewards by difficulty-aware weight. We consider two fine-grained factors: (1) Map difficulty (for all training data from REASONMAP and REASONMAP-PLUS), with weights assigned by the three levels (e.g., easy, medium, hard, see Appendix A.1); and (2) Question difficulty (for the training data in REASONMAP), with weights determined by the transfer count of the required route. The weighting 1Please refer to Appendix A.1 in REASONMAP paper for more details. 2See Appendix B.1 of the REASONMAP paper (Feng et al., 2025b)."
        },
        {
            "title": "Preprint",
            "content": "scheme is defined as follows: Wdifficulty = Wmap + Wquestion, Wmap = γe, map difficulty = easy γm, map difficulty = medium , γh, map difficulty = hard Wquestion = (cid:26)β0, β1, transfer count = 0 transfer count 1 ."
        },
        {
            "title": "4.3 MULTI-STAGE GRPO-BASED REINFORCEMENT LEARNING",
            "content": "To effectively exploit REASONMAP-PLUS and alleviate the sparse-reward issue in complex tasks such as route planning in REASONMAP, we design multi-stage curriculum built upon GRPO-based reinforcement learning. The core idea is to progressively schedule training data in principled manner, ensuring smoother optimization process and more stable reward propagation. We adhere to two complementary principles: (1) Global curriculum principle. We impose coarse-to-fine learning schedule by partitioning tasks into distinct stages according to both question type (from binary judgment counting planning) and target task (from fine-grained visual understanding fine-grained visual reasoning). This global ordering ensures that the agent acquires fundamental perceptual skills before engaging in more abstract reasoning, as illustrated in Figure 2. (2) Local stochasticity principle. Within each stage, we avoid strict deterministic ordering by introducing randomness, i.e., shuffling training samples instead of ranking them solely by heuristic difficulty metrics (e.g., map or question complexity). This stochasticity prevents overfitting to fixed curriculum trajectory and enhances robustness. By jointly applying these principles, the proposed multi-stage RL scheme transforms curriculum training into structured combination of reward shaping and task scheduling, thereby enabling effective reinforcement learning on inherently sparse-reward visual reasoning problems."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETUPS Training Details. We conduct training experiments under various settings with the training data from REASONMAP (696 samples, Rtrain) (Feng et al., 2025b) and our proposed REASONMAP-PLUS (2, 570 samples, RP lustrain) on Qwen2.5-VL models (Bai et al., 2025). For GRPO (Shao et al., 2024) RL training, we use AdamW with an initial learning rate of 1.0 106 and KL divergence coefficient of 1.0 103. The global batch size is 16, and we sample 8 responses per query. Besides RL training, we conduct baseline experiments using SFT with training data from REASONMAP-PLUS. We apply LoRA (Hu et al., 2022) to the language blocks and the text-vision projector with an initial learning rate of 1.0 104. For implementation, we adopt LLaMA-Factory (Zheng et al., 2024) and VeRL (Sheng et al., 2024). All experiments were conducted on 8 NVIDIA H800 GPUs. Inference Details. All evaluations use greedy decoding (temperature=0). For open-source models, we cap the maximum output length at 2, 048 tokens and retain all other settings from the official HuggingFace configurations; deployments use PyTorch with Transformers3 on 8 NVIDIA H800 GPUs. For closed-source models, we evaluate through official APIs using default settings. The evaluated models include: Kimi-VL-A3B-Insturct/Thinking (Team et al., 2025), Qwen2.5-VL3/7/32/72B-Instruct (Bai et al., 2025), and Seed1.5-VL (Guo et al., 2025b). Evaluation Datasets. We first evaluate on the test sets of REASONMAP and REASONMAP-PLUS, and we report the metrics adjusted by the difficulty-aware weighting scheme (see details in Appendix C.1). To further assess the capability gains brought by REWARDMAP, we next employ six widely-used benchmarks spanning three dimensions (e.g., spatial reasoning, fine-grained visual reasoning, and general tasks): SEED-Bench-2-Plus (Li et al., 2024), SpatialEval (Wang et al., 2024), VBench Wu & Xie (2024), HRBench (Wang et al., 2025d), ChartQA (Masry et al., 2022), and MMStar (Chen et al., 2024a) (see Appendix C.2 for details). All evaluations are conducted with VLMEvalKit4. 3https://github.com/huggingface/transformers 4https://github.com/open-compass/VLMEvalKit"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Evaluations of reference models and fine-tuned models on REASONMAP and REASONMAPPLUS. S. represents results for short questions, while L. denotes results for long questions. Bold indicates the best results among fine-tuned models, while underline represents the second best. Model Training Data REASONMAP (S./L.) REASONMAP-PLUS Weighted Acc. Weighted Map Score Weighted Acc. Weighted Acc. (Count / TorF) - Kimi-VL-A3B-Thinking - Kimi-VL-A3B-Instruct Qwen2.5-VL-3B-Instruct - Qwen2.5-VL-32B-Instruct - Qwen2.5-VL-72B-Instruct - - Seed1.5-VL Reference Models 5.47% / 5.47% 12.76% / 12.33% 8.68% / 7.99% 16.49% / 15.71% 26.65% / 24.22% 34.20% / 38.02% 2.44 / 3.17 3.30 / 5.37 2.75 / 3.70 3.88 / 6.84 5.09 / 8.80 5.25 / 11.96 Baseline & REWARDMAP Qwen2.5-VL-7B-Instruct + RL (baseline) + SFT + SFT RL + RL (baseline) + REWARDMAP 13.28% / 7.12% - 26.22% / 26.04% Rtrain RP lustrain 13.63% / 9.11% RP lustrain + Rtrain 28.82% / 30.38% RP lustrain + Rtrain 29.51% / 29.51% RP lustrain + Rtrain 31.51% / 31.77% 4.01 / 5.74 5.52 / 9.52 4.09 / 6.25 5.88 / 10.62 6.00 / 10.41 6.21 / 11.22 33.95% 32.55% 37.61% 58.32% 53.21% 73.58% 44.21% 44.64% 57.93% 60.53% 67.61% 74.25% 18.17% / 50.39% 14.75% / 51.08% 22.68% / 53.16% 46.96% / 70.14% 43.46% / 63.36% 65.26% / 82.23% 37.39% / 51.32% 37.57% / 52.01% 50.73% / 65.44% 55.38% / 65.90% 68.37% / 66.82% 72.18% / 76.42% Table 2: Evaluation of reference models and fine-tuned models on various benchmarks. Bold indicates the best results among fine-tuned models, while underline represents the second best. , , $, , denote the results from the technical report or the official HuggingFace repository (see result sources in Appendix C.3), while all other results are obtained from our own experiments. Model Spatial Reasoning SEED-Bench-2-Plus SpatialEval Fine-grained Visual Reasoning General Task HRBench ChartQA MMStar Kimi-VL-A3B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-32B-Instruct Qwen2.5-VL-72B-Instruct Seed1.5-VL 58.49% 58.86% 72.10%$ 73.00%$ - Reference Models 52.64% 55.04% 57.99% - - 59.16% 58.12% 82.72% 86.40% 89.00% 55.38% 66.25% 63.50% - - Baseline & REWARDMAP 87.08% 84.00% 64.90%$ 89.50% 87.40% 61.70% 55.90% 69.50% 70.80% 76.20% Avg. 62.41% 63.03% 68.45% - - Qwen2.5-VL-7B-Instruct + SFT RL + REWARDMAP 60.97% 61.59% 0.62% 61.96% 0.99% 57.30% 78.01% 68.75% 86.12% 61.67% 68.80% 69.06% 11.76% 78.01% 0.00% 71.00% 2.25% 86.92% 0.80% 62.27% 0.60% 71.48% 2.68% 70.81% 13.51% 80.10% 2.09% 71.25% 2.50% 87.24% 1.12% 62.27% 0.60% 72.27% 3.47% 5.2 MAIN RESULTS Results on REASONMAP and REASONMAP-PLUS We evaluate the proposed REWARDMAP on REASONMAP and REASONMAP-PLUS, both of which provide fine-grained difficulty annotations to assess visual understanding, visual reasoning, and spatial reasoning. In addition to the baselines introduced in Section 4.1, we compare against two widely-used settings: (1) SFT RL baseline, which applies SFT on REASONMAP-PLUS followed by RL on REASONMAP, and (2) RL baseline, which performs RL on the combined training data from REASONMAP and REASONMAP-PLUS. For both baselines, the reward design includes only format and correctness terms. As shown in Table 1, REWARDMAP consistently outperforms all baselines across different question templates in REASONMAP and question types of REASONMAP-PLUS. On REASONMAP, it substantially surpasses the best open-source result (Qwen2.5-VL-72B-Instruct) and approaches the performance of the closed-source model (Seed1.5-VL). On REASONMAP-PLUS, REWARDMAP not only exceeds all open-source models but also outperforms Seed1.5-VL. Results on Other Benchmarks We further evaluate the generalization ability of our method on six benchmarks spanning three task categories introduced in Section 5.1. Table 2 reports the results of reference models, the SFT RL baseline, and our proposed REWARDMAP. Across all six benchmarks, REWARDMAP achieves consistent improvements, with the most substantial gain of 13.51% observed on SpatialEval. While the SFT RL baseline also yields stable improvements, its performance remains inferior to REWARDMAP. These results highlight the significant contributions of both REWARDMAP and the datasets (both REASONMAP and REASONMAP-PLUS) to enhancing model general capability."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Qualitative comparisons among reference models, baseline, and our proposed REWARDMAP. We crop and zoom in on the transit map for clearer presentation. Table 3: Ablation on Reward Design and Multi-Stage Design of REWARDMAP. S. represents results for short questions, while L. denotes results for long questions. Model Training Data REASONMAP (S./L.) REASONMAP-PLUS Weighted Acc. Weighted Map Score Weighted Acc. Weighted Acc. (Count / TorF) 26.22% / 26.04% Rtrain RL (baseline) Rtrain 29.08% / 29.95% RL + Reward Design RP lustrain + Rtrain 29.51% / 29.51% RL (baseline) RP lustrain + Rtrain 30.56% / 30.38% RL + Reward Design RL + Multi-Stage Design RP lustrain + Rtrain 30.64% / 31.51% RP lustrain + Rtrain 31.51% / 31.77% REWARDMAP 5.52 / 9.52 5.88 / 10.53 6.00 / 10.41 6.12 / 10.62 6.08 / 10.88 6.21 / 11.22 44.64% 45.16% 67.61% 71.07% 73.12% 74.25% 37.57% / 52.01% 37.79% / 52.84% 68.37% / 66.82% 67.61% / 74.67% 69.52% / 76.88% 72.18% / 76.42% 5.3 QUALITATIVE RESULTS Figure 3 presents qualitative comparison between reference models, the baseline RL model, and our proposed REWARDMAP. Across diverse maps, we observe that reference models and the baseline RL model often suffer from visual confusion (e.g., mistaking the route or stop as shown by Example & in Figure 3) or even hallucination (e.g., repeating the same route many times in Figure 3 (b)). In contrast, REWARDMAP consistently produces the correct target routes. These examples highlight the effectiveness of REWARDMAP in handling visually complex maps (e.g., Example & in Figure 3) and reducing both visual confusions and hallucinations. 5.4 DIAGNOSTIC EXPERIMENTS We provide extensive ablation studies with Qwen2.5-VL-7B-Instruct in this section, along with verification of REWARDMAP tackling sparse reward and across different model scales (e.g., Qwen2.5VL-3B-Instruct). Ablation on Reward Design and Multi-Stage Design of REWARDMAP. We ablate the reward design and multi-stage design of REWARDMAP under two training data settings. For the configuration using only REASONMAP training data, where the multi-stage design cannot be applied, we ablate the"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Ablation on REASONMAP-PLUS. S. represents results for short questions, while L. denotes results for long questions. Model Training Data REASONMAP (S./L.) REASONMAP-PLUS Weighted Acc. Weighted Map Score Weighted Acc. Weighted Acc. (Count / TorF) RL (baseline) Rtrain 26.22% / 26.04% RL (baseline) RP lustrain + Rtrain 29.51% / 29.51% SFT RL RP lustrain + Rtrain 28.82% / 30.38% 5.52 / 9.52 6.00 / 10.41 5.88 / 10.62 44.64% 67.61% 60.53% 37.57% / 52.01% 68.37% / 66.82% 55.38% / 65.90% Table 5: Ablation on Granularity of Multi-Stage Design. S. represents results for short questions, while L. denotes results for long questions. Model REASONMAP (S./L.) REASONMAP-PLUS Training Data Weighted Acc. Weighted Map Score Weighted Acc. Weighted Acc. (Count / TorF) RP lustrain + Rtrain 29.51% / 29.51% RL (baseline) RL (coarse-grained Multi-Stage) RP lustrain + Rtrain 29.60% / 30.21% RL (Multi-Stage) = REWARDMAP RP lustrain + Rtrain 31.51% / 31.77% 6.00 / 10.41 5.98 / 10.83 6.21 / 11.22 67.61% 70.30% 74.25% 68.37% / 66.82% 66.02% / 74.76% 72.18% / 76.42% Table 6: Evaluation of REWARDMAP across Model Scales. S. represents results for short questions, while L. denotes results for long questions. Model Training Data REASONMAP (S./L.) REASONMAP-PLUS Weighted Acc. Weighted Map Score Weighted Acc. Weighted Acc. (Count / TorF) Qwen2.5-VL-3B-Instruct - + RL (baseline) + REWARDMAP 8.68% / 7.99% Rtrain 11.46% / 10.50% RP lustrain + Rtrain 19.36% / 15.89% 2.75 / 3.70 3.81 / 6.09 4.79 / 7.53 37.61% 38.29% 65.91% 22.68% / 53.16% 22.06% / 55.19% 63.58% / 68.34% reward design alone. As shown in Table 3, enabling either component individually yields performance gains on both REASONMAP and REASONMAP-PLUS, while combining them achieves the best results, confirming the effectiveness and complementarity of the two components. Ablation on REASONMAP-PLUS. Next, we assess the impact of REASONMAP-PLUS under two training strategies (e.g., SFT and RL). As shown in Table 4, conducting cold-start training with REASONMAP-PLUS consistently improves performance on both REASONMAP and REASONMAPPLUS, regardless of the chosen strategy. Ablation on Granularity of Multi-Stage Design. We further conduct an ablation study on the granularity of the multi-stage design by comparing coarse-grained variant with our proposed REWARDMAP. As shown in Table 5, switching to the coarse strategy leads to performance degradation while still outperforming the baseline, thereby reinforcing the effectiveness of the multi-stage design. Effectiveness of Tackling Sparse Reward. We validate the effectiveness of REWARDMAP to address sparse rewards. As shown in Figure 4, we compare reward trajectories of REWARDMAP with baseline RL trained on REASONMAP, aligned at the planning stage. The results show that REWARDMAP alleviates reward sparsity, further confirming its effectiveness. Effectiveness of REWARDMAP across Model Scales. In the end, we evaluate the effectiveness of REWARDMAP across different model scales. Due to training cost constraints, we adopt Qwen2.5-VL-3B-Instruct as the base model and compare the baseline RL with REWARDMAP. As shown in Table 6, REWARDMAP achieves the promising results, demonstrating its robustness and effectiveness."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Figure 4: Comparison of training rewards between baseline RL and REWARDMAP. The yellow curve denotes the reward trajectory of REWARDMAP, while the blue curve corresponds to the baseline RL trained solely on REASONMAP. In this work, we address the challenge of applying reinforcement learning to fine-grained visual reasoning, where sparse rewards and long reasoning horizons often hinder effective optimization."
        },
        {
            "title": "Preprint",
            "content": "Building upon the REASONMAP benchmark, we introduce REASONMAP-PLUS, an extended dataset that organizes tasks along difficulty continuum, providing dense supervision to facilitate cold-start training. Furthermore, we propose REWARDMAP, multi-stage reinforcement learning framework that combines curriculum-style task scheduling with difficulty-aware reward design. Our experiments demonstrate that each of these components contributes to stable and effective training, and that their integration yields the strongest improvements. REWARDMAP not only advances performance on REASONMAP and REASONMAP-PLUS but also enhances robustness across broader visual reasoning benchmarks, indicating improved perceptual and reasoning capabilities of multimodal models."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work does not involve human subjects or sensitive personal data. REASONMAP-PLUS is constructed from publicly available transit maps with automatically generated questionanswer pairs, ensuring no privacy or security concerns. The datasets are released exclusively for academic research under the Apache License 2.0 on HuggingFace, and all reported results are fully reproducible with the released code and configurations."
        },
        {
            "title": "REFERENCES",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 35:2371623736, 2022. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda Shapiro, and Ranjay Krishna. Perception tokens enhance visual reasoning in multimodal language models. In CVPR, pp. 38363845, 2025. Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, and Lei Meng. Beyond sparse rewards: Enhancing reinforcement learning with language model critique in text generation. arXiv preprint arXiv:2401.07382, 2024. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? NeurIPS, 37:2705627087, 2024a. Weizhe Chen, Sven Koenig, and Bistra Dilkina. Why solving multi-agent path finding with large language model has not succeeded yet. arXiv preprint arXiv:2401.03630, 2024b. Christopher Cherry, Mark Hickman, and Anirudh Garg. Design of map-based transit itinerary planner. Journal of Public Transportation, 9(2):4568, 2006. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Yong Deng, Yuxin Chen, Yajuan Zhang, and Sankaran Mahadevan. Fuzzy dijkstra algorithm for shortest path problem under uncertain environment. Applied Soft Computing, 12(3):12311237, 2012. Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. In CVPR, pp. 90629072, 2025."
        },
        {
            "title": "Preprint",
            "content": "Sicheng Feng, Gongfan Fang, Xinyin Ma, and Xinchao Wang. Efficient reasoning models: survey. arXiv preprint arXiv:2504.10903, 2025a. Sicheng Feng, Song Wang, Shuyi Ouyang, Lingdong Kong, Zikai Song, Jianke Zhu, Huan Wang, and Xinchao Wang. Can mllms guide me home? benchmark study on fine-grained visual reasoning from transit maps. arXiv preprint arXiv:2505.18675, 2025b. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b. Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge. arXiv preprint arXiv:2401.13601, 2024. Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, and Zhiding Yu. Argus: Vision-centric reasoning with grounded chain-of-thought. In CVPR, pp. 1426814280, 2025. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Jamshed Memon, Maira Sami, Rizwan Ahmed Khan, and Mueen Uddin. Handwritten optical character recognition (ocr): comprehensive systematic literature review (slr). IEEE Access, 8: 142642142668, 2020. Masato Noto and Hiroaki Sato. method for the shortest path search by extended dijkstra algorithm. In Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics.cybernetics evolving to systems, humans, organizations, and their complex interactions(cat. no. 0, volume 3, pp. 23162320. IEEE, 2000. OpenAI. OpenAI o3 and o4-mini System Card. https://cdn.openai.com/pdf/ 2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card. pdf, 2025. Andre Quadros, Cassio Silva, and Ronnie Alves. Llm-driven intrinsic motivation for sparse reward reinforcement learning. arXiv preprint arXiv:2508.18420, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 36:5372853741, 2023. Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael Tarr, Aviral Kumar, and Katerina Fragkiadaki. Grounded reinforcement learning for visual reasoning. arXiv preprint arXiv:2505.23678, 2025."
        },
        {
            "title": "Preprint",
            "content": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, and Ke Wang. Harnessing uncertainty: Entropy-modulated policy gradients for long-horizon llm agents. arXiv preprint arXiv:2509.09265, 2025a. Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models. NeurIPS, 37:7539275421, 2024. Song Wang, Gongfan Fang, Lingdong Kong, Xiangtai Li, Jianyun Xu, Sheng Yang, Qiang Li, Jianke Zhu, and Xinchao Wang. Pixelthink: Towards efficient chain-of-pixel reasoning. arXiv preprint arXiv:2505.23727, 2025b. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025c. Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In AAAI, volume 39, pp. 79077915, 2025d. Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025e. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 35: 2482424837, 2022. Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, and Weiran Huang. Advancing multimodal reasoning via reinforcement learning with cold start. arXiv preprint arXiv:2505.22334, 2025. Hsiang-Yun Wu, Benjamin Niedermann, Shigeo Takahashi, Maxwell Roberts, and Martin Nollenburg. survey on transit map layoutfrom design, machine, and human perspectives. In Computer Graphics Forum, volume 39, pp. 619646. Wiley Online Library, 2020. Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In CVPR, pp. 1308413094, 2024. Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, and Mike Zheng Shou. Reinforcement learning in vision: survey. arXiv preprint arXiv:2508.08189, 2025. Shuo Xing, Zezhou Sun, Shuangyu Xie, Kaiyuan Chen, Yanjia Huang, Yuping Wang, Jiachen Li, Dezhen Song, and Zhengzhong Tu. Can large vision language models read maps like human? arXiv preprint arXiv:2503.14607, 2025."
        },
        {
            "title": "Preprint",
            "content": "Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025a. Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025b. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Yaowei Zheng, Richong Zhang, Junhao Zhang, YeYanhan YeYanhan, and Zheyan Luo. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 400410, 2024."
        },
        {
            "title": "APPENDIX",
            "content": "In Appendix A, we provide statistical overview of REASONMAP-PLUS and its question templates. Appendix outlines the computation pipeline of the detail reward. Appendix describes evaluation settings, including difficulty-aware weighting, benchmark datasets, and result sources in Table 2. Finally, Appendix presents the statement on LLM usage. Dataset Construction Details A.1 Statistical Overview . A.2 Question Template . . . . . REWARDMAP Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Computation Pipeline of Detail Reward . . . . . . . . . . . . . . . . . . . . . . . Evaluation Details C.1 Details of Difficulty-Aware Weighting . . . . . . . . . . . . . . . . . . . . . . . . C.2 Details of Evaluation Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Result Source Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Large Language Model Usage Statement"
        },
        {
            "title": "A DATASET CONSTRUCTION DETAILS",
            "content": "A.1 STATISTICAL OVERVIEW 14 14 14 15 15 15 15 16 16 REASONMAP-PLUS mirrors REASONMAP in image composition, comprising high-resolution transit maps from 30 cities. It contains 4, 018 questions across five categories (e.g., Local Counting 1 - 1, 050; Local Counting 2 - 970; Global Counting - 30; True or False 1 - 1, 039; True or False 2 - 929), with difficulty distribution of 1, 259 easy, 1, 342 middle, and 1, 417 hard. To preserve difficulty balance and diversity, we adopt the same split as REASONMAP, using questions from 11 cities for the test set (1, 448) and the remainder for the training set (2, 570). A.2 QUESTION TEMPLATE We present the question templates of REASONMAP-PLUS as follows. Local Counting 1 Please solve the multiple choice problem and put your answer (one of ABCD) in one boxed. According to the subway map, how many intermediate stops are there between stop 1 and stop 2 (except for this two stops)? A) B) C) D) Local Counting 2 Please solve the problem and put your answer in one boxed. According to the subway map, how many lines pass through stop 1? Global Counting Please solve the problem and put your answer in one boxed. According to the subway map, how many subway (metro) lines are there in total?"
        },
        {
            "title": "Preprint",
            "content": "True or False 1 Please solve the problem and put your answer (only answer yes or no) in one boxed. According to the subway map, is it true that stop 1 is the same line as stop 2? True or False 2 Please solve the problem and put your answer (only answer yes or no) in one boxed. According to the subway map, is it true that stop 1 is on the line x?"
        },
        {
            "title": "B REWARDMAP DETAILS",
            "content": "B.1 COMPUTATION PIPELINE OF DETAIL REWARD We present the complete computation pipeline of Detail Reward in Algorithm 1. Algorithm 1: Detailed Reward for Planning Questions in REASONMAP Initialize score 0; if route data is empty or format is wrong then return score; if departure stop of first segment = stop 1 or arrival stop of last segment = stop 2 then score score +2; foreach segment si in predicted route do if current transfer times > question transfer count then score score 5; if current transfer times = 0 and is correct(route name) then score score +4; if departure stop stations and arrival stop stations then if segment si is not the last then if arrival stop = departure stop of next segment then score score +1; score min(score, 10); return score;"
        },
        {
            "title": "C EVALUATION DETAILS",
            "content": "C.1 DETAILS OF DIFFICULTY-AWARE WEIGHTING For REASONMAP, we follow the weighting scheme described in the original paper (see Appendix B.3 therein). For REASONMAP-PLUS, weights are assigned solely based on map difficulty, with values of 1.0, 1.5, and 2.0 for easy, medium, and hard maps, respectively. C.2 DETAILS OF EVALUATION DATASETS We provide brief description of the six evaluation benchmarks used in our paper: 1. SEED-Bench-2-Plus (Map) (Li et al., 2024) describes three categories (Charts, Maps, Webs) with human-verified multiple-choice items, from which we use the Map slice. 2. SpatialEval (Wang et al., 2024) targets spatial intelligence across relationships, position, counting, and navigation."
        },
        {
            "title": "Preprint",
            "content": "3. Bench (Wu & Xie, 2024) evaluates fine-grained attribute recognition and spatial relationships on high-resolution images. 4. HRBench (Wang et al., 2025d) evaluates MLLMs on 4K/8K high-resolution images and introduces training-free enhancement baseline. 5. ChartQA (Masry et al., 2022) benchmarks QA over charts with visual and logical reasoning. 6. MMStar (Chen et al., 2024a) offers 1,500 human-curated, vision-indispensable samples covering 6 core capabilities and 18 axes. C.3 RESULT SOURCE SUMMARY We present sources of results in Table 2: , , $, , correspond to the Seed1.5-VL technical report (Guo et al., 2025b), the Qwen2.5-VL technical report (Bai et al., 2025), the InternVL3.5 technical report (Wang et al., 2025c), the Kimi-VL technical report Team et al. (2025), and the official Hugging Face repository of Qwen2.5-VL-32B-Instruct5, respectively."
        },
        {
            "title": "D LARGE LANGUAGE MODEL USAGE STATEMENT",
            "content": "Large Language Models (LLMs) were used only for surface-level editing of the manuscript (e.g., polishing grammar and style, rephrasing for clarity, and making minor LATEX adjustments). They were not involved in generating ideas, methods, algorithms, code, experiments, figures, tables, or citations. All research design, implementation, data processing, and analysis were performed by the authors. LLM use was limited to de-identified text snippets, with no proprietary data or unpublished results shared, and all outputs were manually reviewed and revised. This limited assistance does not affect reproducibility, as every reported result is fully reproducible from the released code and configurations. 5https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Tongji University",
        "Westlake University",
        "Zhejiang University"
    ]
}