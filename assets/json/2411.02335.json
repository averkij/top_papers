{
    "paper_title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity",
    "authors": [
        "Yuqi Luo",
        "Chenyang Song",
        "Xu Han",
        "Yingfa Chen",
        "Chaojun Xiao",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 5 3 3 2 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Under peer review",
            "content": "SPARSING LAW: TOWARDS LARGE LANGUAGE MODELS WITH GREATER ACTIVATION SPARSITY Yuqi Luo, Chenyang Song, Xu Han Yingfa Chen, Chaojun Xiao, Zhiyuan Liu, Maosong Sun Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China luo-yq23@mails.tsinghua.edu.cn, scy22@mails.tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs), such as computation acceleration and model interpretability. Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-p% sparsity, precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions (i.e., ReLU and SiLU) exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1 sparsity ratio) evolves as convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below certain bottleneck point, indicating the potential advantage of deeper architecture at fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable. Materials related to this work (i.e., codes and checkpoints) are available at https://github.com/thunlp/SparsingLaw."
        },
        {
            "title": "INTRODUCTION",
            "content": "Activation sparsity refers to the phenomenon where considerable elements within the output of neural layer (typically activation functions, as shown in Figure 1) are zero or low values and thus contribute weakly to the final model output given specific input. As prevalent property of many language and vision modeling architectures (Li et al., 2022), activation sparsity has wide practical values, such as inference acceleration (Liu et al., 2023; Song et al., 2023; Xue et al., 2024; Song et al., 2024a), training acceleration (Zhang et al., 2024b), and LLM interpretation (Sajjad et al., 2022; Zhang et al., 2023). Generally, model with greater sparsity ratio (i.e., the ratio of inactivated elements) has more potential in these scenarios. However, this raises an underexplored problem: how to obtain an LLM with greater activation sparsity? simple solution is to design constraints at the model architecture level that force the model to have predefined large sparsity ratio. For example, mixture-of-experts (MoE), the most popuCorresponding author: Xu Han (thu.hanxu13@gmail.com) and Zhiyuan Liu (liuzy@tsinghua.edu.cn). Equal Contributions: Yuqi Luo and Chenyang Song."
        },
        {
            "title": "Under peer review",
            "content": "Figure 1: typical case of activation sparsity (with sparsity ratio of 60%) in gated feed-forward network of LLMs, where considerable elements weakly contribute to the outputs within the activation scores. Figure 2: The PPL-activation Pareto curve of the 0.1B MoE with different expert numbers versus the 0.1B vanilla decoderonly Transformer. lar architecture-constrained design, typically uses token-level top-k parameter selection router to assign fixed sparsity ratio for each token at each layer (Fedus et al., 2022; Zoph et al., 2022). However, these constraints often sacrifice model flexibility and performance. Recent works reveal the potential performance degradation caused by such inflexible sparsity assignment (Huang et al., 2024; Liu et al., 2024). Moreover, to inspect the impact of such constraints, we plot the PPL-activation (PPL denotes perplexity) Pareto curve (activation ratio = 1 sparsity ratio) of MoE in Figure 2 and compare it with vanilla decoder-only Transformer (Touvron et al., 2023) of the same parameter scale and amount of training data1. MoE has significantly worse performance-sparsity trade-off. The best sparsity ratio is also hard to predefine, since too-high or too-low sparsity ratio may lead to more severe performance degradation or substantial unnecessary computation, respectively. To avoid negative impacts on flexibility and performance, we focus on the intrinsic activation sparsity within decoder-only Transformer-based LLMs in this paper, such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023). Different from the sparsity enforced through architectural constraints, intrinsic activation sparsity (Zhang et al., 2024a; Song et al., 2024a) arises self-adaptively during the pre-training stage, without unified or predefined sparsity ratios among different tokens or layers. We provide comprehensive study on the quantitative scaling properties and influential factors of this intrinsic property. These findings can help us formulate generalizable method to control and promote activation sparsity without harming flexibility and performance. To rigorously study the influential factors of activation sparsity, we need metric to precisely reflect the sparsity level of an LLM. Conventional works rely on prior experience to set fixed global threshold for recognition of weakly-contributed neurons2, which is empirical, inflexible, and performance-unaware (see Section 2.2). We propose an improved metric, named PPL-p% sparsity, with three advantages: versatility across model architectures, performance-awareness, and the precise recognition of weakly-contributed neurons. For versatility, PPL-p% sparsity follows Zhang et al. (2024a) and recognizes the most weakly-contributed neurons, which are then inactivated, by introducing layer-wise adaptive thresholds and comparing the magnitudes of neuron outputs with the layer-specific threshold. As magnitude-based metric, it is not bound to the activation functions that output exactly non-negative elements (e.g., ReLU) and is applicable to any LLMs with activation layers. Moreover, considering the trade-off between performance and sparsity (Song et al., 2024a), performance-awareness is required, indicating whether metric can comprehensively reflect the sparsity under each desired performance level. To this end, PPL-p% sparsity explicitly incorporates PPL as performance proxy, computed as the ratio of inactivated neurons when the output PPL raises just by p% after the layer-wise thresholds are applied, compared to the dense setting with all neurons activated. Finally, the precise recognition of weakly-contributed neurons indicates that our metric obtains good trade-off between performance and sparsity ratio (see Section 4.1). Based on the above metric, we systematically study the correlation between the activation sparsity and influential factors, including the amount of training data, the activation function, the width1MoE models of different sparsity are obtained by tuning the number of activated experts, while for the vanilla setting, we adjust the CETT value proposed by Zhang et al. (2024a). 2A neuron denotes certain row or column within the parameter matrices."
        },
        {
            "title": "Under peer review",
            "content": "depth ratio (i.e., the ratio of the hidden dimension to the layer number), and the parameter scale. Through comprehensive experiments, we obtain the following observations: 1. There is an increasing power-law (SiLU-activated LLMs) or decreasing logspace powerlaw (ReLU-activated LLMs) relationship between the activation ratio and the amount of training data. Both laws are convergent with certain limit sparsity ratio as the amount of data approaches infinity. Note that the increasing sparsity-data trend indicates that ReLUactivated LLMs are more efficient in improving activation sparsity with more data. 2. Given the same parameter scale, the sparsity obtained by ReLU-activated LLMs always surpasses that of SiLU-activated LLMs, while their performance is comparable. 3. Given the same parameter scale, the activation ratio linearly increases with the width-depth ratio under bottleneck point (i.e., deeper models are sparser), above which the activation fluctuates around fixed level. However, considering the performance issue, the best widthdepth ratio should be just within certain interval that ensures the best performance. 4. Given similar width-depth ratios, the limit of activation sparsity is weakly correlated to the scale of LLMs. On the other hand, the convergence speed to the limit is much faster in smaller models. We try to explain these phenomena in Section 4.4. The above empirical laws can provide comprehensive instructional values for designing and pretraining an LLM with greater activation sparsity, which offers more significant potential in producing more efficient and interpretable LLMs. Moreover, our work enables the training-time prediction of the future sparsity ratio, and the evolving trend of activation sparsity with the amount of training data potentially provides lens for the progress of neuron specialization."
        },
        {
            "title": "2.1 PRELIMINARIES OF ACTIVATION SPARSITY",
            "content": "Activation sparsity is prevalent property existing in neuron networks with activation layers, indicating the existence of considerable parameters, which correspond to the activation outputs with zero or low values and thus have limited impact on final network outputs given specific inputs. These weakly-contributed parameters are often defined as inactivated parameters and can be skipped in computation. Due to the activation layers in feed-forward networks (FFNs), mainstream LLMs also present remarkable activation sparsity (Li et al., 2022; Zhang et al., 2022; Song et al., 2024a). Owing to activation sparsity, we can improve LLMs in many aspects, such as efficiency, interpretability, and robustness. For instance, recent works manage to exploit activation sparsity for inference acceleration, mainly by predicting the activation patterns, wisely allocating hardware resources, and saving redundant resources associated with inactivated parameters (Liu et al., 2023; Song et al., 2023; Xue et al., 2024). Zhang et al. (2024b) reveal the existence of activation sparsity throughout the majority of the LLM pre-training process, and then utilize this sparsity for pretraining acceleration. Besides acceleration, activation sparsity also helps improve the interpretability (Agarap, 2018; Dai et al., 2022; Cuadros et al., 2022; Sajjad et al., 2022) and robustness (Ahmad & Scheinkman, 2019; Muthukumar & Sulam, 2023) of LLMs, which are also important properties in producing reliable and well-performing LLMs."
        },
        {
            "title": "2.2 METRICS OF ACTIVATION SPARSITY",
            "content": "While considerable works call for greater activation sparsity due to the significant merits brought by sparsity (e.g., more efficient computing and better model interpretability) (Mirzadeh et al., 2023; Song et al., 2024a;b), it is nontrivial to frame satisfactory metric for measuring sparsity. For the convenience of demonstrations, we formally introduce the following notations for the computation process of FFNs (also see Figure 1). By denoting the hidden dimension and intermediate dimension as dh and df respectively, gated FFN (a common FFN form adopted in most mainstream LLMs (Dauphin et al., 2017; Shazeer, 2020)) processes the inputs as follows: = σ(Wgatex), FFN(x) = Wout[s (Winx)], (1)"
        },
        {
            "title": "Under peer review",
            "content": "where Rdh , Rdf , σ, and denote the input hidden states, the activation scores, the activation function, and the element-wise multiplication, respectively. Wgate, Win Rdf dh and Wout Rdhdf are learnable parameters. Next, we decompose the parameters of FFN along the dimension of df into df neurons. The output of the i-th neuron ni is calculated by si = σ(Wgate i,: x), ni = Wout :,i [si (Win i,:x)], FFN(x) = df (cid:88) i=1 ni, (2) where Wgate Wout, respectively. The FFN outputs can be formalized as the sum of all neuron outputs. :,i are the i-th row of Win, the i-th row of Wgate, and the i-th column of i,:, Wout , Win i,: Activation sparsity is measured by the ratio of inactivated neurons, namely D/df , where is the index set of inactivated neurons. However, different sparsity metrics can differ in determining whether specific neuron is inactivated. straightforward metric, naturally adopted in ReLUactivated models, regards those neurons with zero activation scores in Eq. (2) as inactivated, namely = {isi = 0}. To prune more non-zero weakly contributed activations, works by Kurtz et al. (2020) and Mirzadeh et al. (2023) introduce positive threshold or bias. In this case, the set of inactivated neurons changes to = {isi < ϵ}, where ϵ > 0 is the threshold or bias. The major drawback of this straightforward definition is the lack of versatility. Concretely, it is unsuitable for activation functions that have considerable non-negligible negative outputs, such as SiLU (Elfwing et al., 2018). In these cases, the straightforward metric can lose considerable negative neuron outputs and harm performance. quick fix is to use the absolute value, = {isi < ϵ}, but global threshold across layers is hard to determine. To this end, Zhang et al. (2024a) adaptively searches the layer-wise thresholds by introducing the cumulative errors of tail truncation (CETT). Defined as the L2 norm relative error caused by inactivated neurons, CETT is computed as: CETT = (cid:80) iD ni2 FFN(x)2 , = {ini2 < ϵ}, (3) where 2 is the L2 norm operator. Notably, as CETT increases monotonically with ϵ, for each layer, we can use binary search to find threshold ϵ that makes CETT just equal predefined value. Meeting the versatility demands, the CETT paradigm is still not friendly enough, as CETT does not directly reflect the model performance. In real-life deployment, we often need to utilize activation sparsity under certain tolerance of performance degradation. Therefore, in this paper, we introduce more performance-aware metric named PPL-p% sparsity, which explicitly reflects the sparsity ratio under target PPL level by binary-searching an appropriate CETT value."
        },
        {
            "title": "2.3 SCALING PROPERTIES OF ACTIVATION SPARSITY",
            "content": "Despite the importance of activation sparsity, few works conduct comprehensive studies on how it scales with the increase of parameter scale and training data, as well as the impact of other influential factors within architecture designs. Speculating that activation sparsity comes from the training dynamic in the optimization process, Li et al. (2022) finds an increasing trend of sparsity with larger scale, depth, and width in T5 series (Raffel et al., 2020). Zhang et al. (2024a) dives into this problem from the aspect of activation functions. Song et al. (2024a) discovers that LLMs tend to be sparser on more formatted datasets such as codes and multiple choices. Other works have discussed the scaling properties of parameter-sparse models (Frantar et al., 2023), fine-grained MoE models (Krajewski et al., 2024), and sparse autoencoders (Gao et al., 2024). To the best of our knowledge, we present the first comprehensive quantitative study on the scaling properties of activation sparsity and the impact of architecture factors. The work most similar to ours is done by Li et al. (2022), but they only conduct qualitative analyses and focus on the conventional encoder-decoder Transformer (e.g., T5) rather than decoder-only LLMs. Besides, they adopt the most straightforward metric for activation sparsity, limited to ReLU-activated models."
        },
        {
            "title": "3.1 METRIC OF ACTIVATION SPARSITY",
            "content": "We propose PPL-p% sparsity, which improves the CETT metric (Zhang et al., 2024a) by directly reflecting the sparsity ratio at different requirements of PPL (as proxy of performance). Firstly, given pre-trained LLM, we introduce the dense setting with all its neurons activated (i.e., = 0), whose outputs are theoretically the most accurate. Next, we conduct binary search for an appropriate CETT value CETTk, which makes the average PPL increase just by p% compared to the dense setting. The search algorithm is described in Appendix C. Finally, based on CETTk, we can determine the activation thresholds for each layer and obtain the sparsity value, according to Eq. (3). Our experiments in Section 4.1 will demonstrate the rationality of this metric in achieving the greatest sparsity under the same output PPL. 3."
        },
        {
            "title": "INFLUENTIAL FACTORS",
            "content": "As we mainly study the effect of potentially influential factors on activation sparsity, it is necessary to specify how to compute the involved influential factors as follows: Amount of training data: The number of tokens passed during the pre-training process. To obtain more comprehensive dynamics and more precise estimations of the limits, the number of training tokens is no less than 80 times the scale of non-embedding parameters. Activation function: ReLU and SiLU are incorporated as the two most widely adopted activation functions with FFNs. Parameter scale: The number of parameters included in model excluding the embeddings. In this work, we incorporate 5 scales: 0.1B, 0.2B, 0.4B, 0.8B, and 1.2B. Width-depth ratio: The ratio of the hidden dimension (i.e., dh) to the number of hidden layers (denoted as ). We sample 9 values of width-depth ratio ranging from 14.2 to 597.3, covering both the ordinary and extreme conditions."
        },
        {
            "title": "3.3 SETTING OF SCALING ANALYSIS",
            "content": "Model settings We adopt the same architecture as MiniCPM (Hu et al., 2024), which adopts Tensor Program (Yang et al., 2022) for training stability and shares the input and output embeddings. Training settings We mainly focus on the activation sparsity of foundation models, which only undergo the pre-training stage. However, before we evaluate models on task-specific benchmarks, we follow MiniCPM (Hu et al., 2024) to conduct decay stage, where instruction-tuning data is additionally added for training. Thereby, we can obtain more reasonable results on benchmarks. Besides, considering the influence of various training hyper-parameters, we follow the optimal batch sizes, optimal learning rates, and the WSD learning rate scheduler of MiniCPM (Hu et al., 2024). Evaluation Settings We introduce tiny validation dataset and two groups of benchmarks, including commonsense reasoning and reading comprehension, for evaluation. We also test our model on more complex coding or knowledge benchmarks but fail to obtain performance significantly surpassing the random level (See Appendix H), mainly due to the small parameter scales. For the measurement of sparsity, to eliminate the impact of stochastic factors (especially the sparsity fluctuations during the early stage), we use sparsity stabilizing strategy to obtain smoother sparsity-data curves (see Appendix for more details). If there are no special statements, the training loss, validation loss, and perplexity are all calculated on models that only complete pre-training (the latter two metrics are computed on the validation data), and the task-specific performance is evaluated on checkpoints after the decay stage. See Appendix and for more details about datasets and training settings, respectively."
        },
        {
            "title": "Under peer review",
            "content": "Figure 3: The PPL-activation Pareto curve of our PPL-p% sparsity versus two baselines within models of different scales. Straightforward ReLU is only applicable to ReLU-activated models. Table 1: The average evaluation scores (%) on two task groups, where C.R. refers to commonsense reasoning and R.C. refers to reading comprehension. The second column represents settings with different p% values, with dense indicating the most accurate case where = 0. 0.1B 0.4B ReLU SiLU ReLU SiLU ReLU SiLU ReLU SiLU ReLU SiLU 0.8B 0.2B 1.2B C.R. R.C. dense 49.6 PPL-1% 49.1 PPL-5% 49.2 PPL-10% 49.4 dense 28.2 PPL-1% 28.4 PPL-5% 26.9 PPL-10% 26. 49.5 49.9 49.0 48.7 27.7 28.0 26.5 24.8 52.0 51.7 51.7 51.6 40.7 39.7 38.6 38.6 52.2 52.4 52.0 51.9 40.2 39.6 36.8 34. 54.7 54.6 54.3 54.9 44.0 42.9 40.8 39.9 55.8 55.8 55.1 55.2 41.8 40.9 38.2 35.3 56.8 55.9 56.3 55.6 44.8 43.2 42.2 40. 57.6 57.6 57.1 56.4 43.3 44.3 40.7 38.8 60.0 59.6 59.3 59.3 53.2 53.3 53.3 52.9 59.6 59.6 58.8 59.3 54.8 55.4 52.6 51."
        },
        {
            "title": "4.1 RATIONALITY OF PPL-p% SPARSITY",
            "content": "As stated in Section 1, rational metric for activation sparsity should have: versatility across model architectures, performance-awareness, and the precise recognition of weakly-contributed neurons. While the former two properties are already demonstrated in Section 1, we mainly discuss whether PPL-p% sparsity has the third one in this section. Specifically, PPL-p% sparsity should wisely recognize more weakly-contributed neurons while minimizing performance degradation simultaneously. In other words, our metric should strike better trade-off between performance and sparsity. We introduce the following baselines for comparison: (1) Straightforward ReLU: The most simple setting that uses the zero threshold and is only applicable to ReLU: = {isi = 0}. (2) Top-k sparsity, widely adopted in the MoE architectures (Fedus et al., 2022; He, 2024), enforces each layer to consistently maintain activated neurons, whose absolute values of activation scores rank in the top-k ones among all the neurons of that layer. Obviously, we have = df k, and the Top-k sparsity method holds constant sparsity ratio across all layers. (3) FAT-ϵ sparsity (Kurtz et al., 2020) (FAT denotes forced activation threshold) similarly introduces global hyper-parameter ϵ as the threshold shared by all layers, namely = {isi < ϵ}. Note that this is slightly different from the original FATReLU by Kurtz et al. (2020) as we compute the absolute values of activation scores to accommodate SiLU. As demonstrated by Figure 3, PPL-p% sparsity obtains the best trade-off between sparsity and performance, with the lowest PPL given target sparsity ratio. This is brought by its wise recognition of inactivated neurons through adaptive thresholds. Moreover, the performance-aware definition of PPL-p% sparsity makes us explicitly grasp the sparsity ratio at each desired perplexity. Another problem lies in how the hyper-parameter p% influences task-specific performance. To inspect this, we evaluate models of different scales on the benchmarks in Section 3.3 and tune their sparsity through distinct p% values. As shown in Table 1, with the value of p% increasing (intrinsi-"
        },
        {
            "title": "Under peer review",
            "content": "Figure 4: The trend of activation ratios (hereinafter using PPL-1% sparsity) of models with different scales and activation functions during the pre-training stage. The fitted curves are plotted in brown. The number of training tokens is no less than 190 times the scale of non-embedding parameters. cally promoting greater sparsity), the reading comprehension performance is considerably impaired, corresponding to the trade-off between sparsity and performance. Notably, in both task groups, the average performance of PPL-1% is comparable to that of the theoretically most accurate dense setting. Therefore, we assume PPL-1% sparsity as reliable performance-unimpaired metric and employ it to compute sparsity in the following discussions."
        },
        {
            "title": "FUNCTIONS",
            "content": "To obtain the scaling relationship between the activation sparsity and the amount of training data, we pre-train models with different numbers of parameters and two activation functions (i.e., ReLU and SiLU), respectively, and then evaluate the sparsity level of their checkpoints using PPL-1%. After careful attempts, we find that the curve of activation ratios to the amount of training data is easier to fit than that of the sparsity ratio. Therefore, we will frequently use activation ratios instead of sparsity ratios in the following sections, whose trend is plotted in Figure 4. For ReLU models, we observe logspace power-law relationship between the activation ratio AReLU (D) and the amount of training data D, expressed in the following formula: AReLU (D) = exp(cDα + b) + A0, (4) where A0 > 0 is the limit activation ratio with infinite training data and we have c, α > 0. Obviously, this is convergent decreasing function, indicating that more training data can potentially make ReLU models more sparsely activated. By contrast, the activation ratio ASiLU (D) of SiLU models exhibit vanilla power-law relationship: ASiLU (D) = Dα + A0, (5) where similarly, A0 > 0 is the limit activation ratio and c, α > 0. Note that this is convergent increasing function, and thus more training data will impair the activation sparsity of SiLU models. See Appendix for the algorithm of curve fitting and the results (i.e., coefficients). As for the selection of activation functions, by comparing the sparsity dynamics, we can conclude that the activation sparsity achieved by ReLU is significantly greater than that of SiLU. Besides, the task-specific performance in Table 1 and the trend of training loss in Appendix reveal the comparable performance between ReLU and SiLU activations. Based on the above observations, ReLU is more competent as the activation function than SiLU due to three advantages: an increasing trend of sparsity, significantly higher sparsity ratio, and comparable performance."
        },
        {
            "title": "4.3 ACTIVATION SPARSITY WITH THE WIDTH-DEPTH RATIO",
            "content": "The width-depth ratio, defined as the ratio of the hidden dimension to the layer number, reflects the shape of Transformer and is key architectural property that potentially influences activation sparsity. To inspect its influence on the activation sparsity, we conduct experiments on the 0.1B ReLU-activated model and select 9 different width-depth ratios. The limit activation ratio and the limit training loss are plotted in Figure 5 and Figure 6 respectively. As demonstrated by Figure 5, under bottleneck point (about 114 for 0.1B), the activation ratio linearly increases with the width-depth ratio. However, given width-depth ratio greater than this bottleneck, the activation ratio fluctuates around 8%. From the sparsity aspect, smaller widthdepth ratio is definitely more helpful. However, the dynamics shown in Figure 6 demonstrate that there exists the best interval of width-depth ratio for the lowest training loss (from 74 to 282 for 0.1B). Therefore, to maintain the best performance while promoting greater activation sparsity, the best width-depth ratio should fall on the smallest point of this interval (i.e., around 74 for 0.1B). Figure 5: The limit activation ratios on 0.1B ReLU-activated models. Figure 6: The limit training loss on 0.1B ReLUactivated models. Figure 7: The limit activation ratio for pretrained models with different scales and activation functions. Figure 8: The derivative trends of the sparsitydata curve with the increase of data-scale ratio, within ReLU/SiLU models of distinct scales."
        },
        {
            "title": "4.4 ACTIVATION SPARSITY WITH THE PARAMETER SCALE",
            "content": "To obtain comprehensive scaling properties of activation sparsity with the increase of scales (i.e., the number of non-embedding parameters), we obtain the limit activation ratio of the above pre-trained models with 5 distinct scales but similar width-depth ratios. From the results plotted in Figure 7, we can reach the first observation that under similar width-depth ratios, the limit activation ratio as the amount of training data approaches infinity is weakly related to the parameter scale. For SiLU settings, the activation ratio decreases slightly by 2.7 points from 0.1B to 1.2B. By contrast, for ReLU settings, the activation ratio marginally increases by 1.7 points from 0.1B to 1.2B."
        },
        {
            "title": "Under peer review",
            "content": "To reflect the evolving dynamics of sparsity, we compute the derivatives of the sparsity-data curve as fitted in Section 4.2 and plot the trend of derivatives with the increase of data-scale ratio3. The results in Figure 8 clearly demonstrate that smaller models tend to converge faster than larger models to the limit, as the absolute values of their derivatives are much larger. We try to explain the above observations as follows. Observation: Neurons within models of different scales present similar activation patterns. To support this point, we conduct two experiments from the aspect of dataset-wise and token-wise activation patterns respectively. Figure 9: The distribution of the neuron activation frequencies within models of distinct scales. Four datasets from the pre-training data are involved. Figure 10: The activation ratio (%) distributions of 71,549 tokens sampled from the vocabulary. We conduct pair-wise comparison of the average activation ratio of each token within models of different scales. Note that the red line is the = curve. To inspect the dataset-wise activation distributions under different parameter scales, we consider four datasets, each subset of the pre-training data: Code, Wikipedia, Math, and Chinese. Next, we compute the distribution of activation frequencies (i.e., the times that neuron is activated divided by the total number of tokens) among the neurons within models of different scales. As demonstrated by Figure 9, for all the datasets, the distribution patterns of neuron activation frequencies are similar across different scales. While this observation holds on average, special cases exist in certain layers (see Appendix G). For the token-wise activation, we sample 71,549 tokens from the vocabulary and count their activation ratios on sufficiently large amount of data. Next, we compare the activation ratios of each token among models of different scales in pair-wise manner. Figure 10 clearly shows that most tokens maintain close activation ratio across models of various scales. 3The data-scale ratio means the ratio of the number of training tokens to the parameter scale. We choose this variable as previous works have demonstrated the roughly proportional relationship between the optimal amount of training data and the parameter scale (Hoffmann et al., 2022; Besiroglu et al., 2024)."
        },
        {
            "title": "Under peer review",
            "content": "From different aspects, the above two experiments both support the insensitiveness of the activation pattern to the parameter scale. This can potentially provide one explanation for why the activation sparsity is quite weakly correlated with the model sizes. Assumption: Neurons within models of different scales present similar specialization. The neurons in FFNs tend to specialize into certain functions during the training progress (Li et al., 2022; Zhang et al., 2023). However, few works have studied how such specialization differs in models of distinct scales. As stated above, both the dataset-wise and token-wise activation patterns are insensitive to the parameter scale. In other words, the numerical distribution of neurons activated for certain function (e.g., specific category of datasets or syntactic elements) is similar. Therefore, it is reasonable to assume that the specialization of neurons is also scale-insensitive. Deduction: Smaller models converge faster to the limit activation ratio mainly due to their small amount of neurons. To simplify this problem, we model the specialization of neurons as grouping process, where each neuron can be placed into zero or more groups (considering the potential existence of dead neurons and versatile neurons). Suppose the df neurons should specialize into groups, each of them having t1, t2, ..., tG neurons respectively. Based on the assumption of similar activation patterns and neuron specialization, the ratio of neurons placed in each group (i.e., 0 < ti/df 1, = 1, 2, ..., G) should be shared across different parameter scales. We can obtain the number of all the possible grouping results (df ) easily, (df ) = (cid:89) i="
        },
        {
            "title": "C ti\ndf",
            "content": "= (cid:89) i=1 df ! ti!(df ti)! , (6) where ti is the combinatorial number, the number of possibilities to select ti neurons from df df ones. Obviously, (df ) grows in factorial speed with df , much faster than the linear function. For larger models, the number of neuron specialization possibilities is significantly greater than that of smaller ones. Therefore, it takes more training expenses for larger models to form stable neuron specialization and approach the limit activation ratio."
        },
        {
            "title": "5 DISCUSSION",
            "content": "In this section, we mainly discuss how this work can help accomplish our key target: how to obtain an LLM with greater activation sparsity. Architectural design Before kicking off the pre-training stage, wiser design of the LLM architecture can effectively improve the limit activation sparsity. Based on the discoveries in this paper, we propose to replace SiLU with ReLU as the activation function to promote greater sparsity and leverage its increasing trend of sparsity with more training data. Moreover, deeper model can potentially exhibit higher sparsity given fixed computation budget. Nevertheless, as model with an extreme width-depth ratio can violate the performance, the best setting for the width and depth still deserves careful studies, combined with the quantitative study on the relationship between performance and the width-depth ratio. Training-time predictable sparsity Another important value of our work lies in the prediction of sparsity during the pre-training stage. By fitting the power-law or logspace power-law between activation sparsity and the amount of training data, model developers can either predict the theoretical upper/lower bound sparsity of model to evaluate its potential (e.g., in inference acceleration), or estimate the number of tokens required to achieve desired sparsity ratio. Lens for the convergence of neuron specialization Generally, the loss curve is one of the most important signs of the training state (e.g., at which point the model converges). However, if we compare the loss curve in Figure 11 and the sparsity (activation ratio) curve in Figure 4, we will find that the convergence speed of activation sparsity is much slower than loss, indicating the ongoing of neuron specialization even when the training loss changes little. Despite the wide recognition of the neuron specialization phenomenon (Li et al., 2022; Zhang et al., 2023), it is still unclear when such specialization converges and how to inspect this progress. Besides, the loss curve"
        },
        {
            "title": "Under peer review",
            "content": "is often not good standard for convergence, especially for modern LLMs with trillion-level pretraining data. We believe that the trend of activation sparsity can provide lens for inspecting the progress of neuron specialization as well as the training convergence. We leave the study on the correlation between activation sparsity and neuron specialization for future work."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we first propose precise, versatile, and performance-aware metric for activation sparsity, called PPL-p% sparsity, whose rationality is demonstrated in striking better trade-off between performance and sparsity. Next, we conduct comprehensive and quantitative study on how the activation sparsity scales with the amount of training data and parameter scales. The influence of two key architectural settings, namely the activation function and the width-depth ratio, are also well evaluated. Through extensive experiments, we figure out the quantitative sparsity-data relationship, substantiate the advantage of ReLU activation, find small width-depth ratio helpful in promoting sparsity, observe and then explain the insensitiveness of sparsity to scale. These can better instruct LLM developers to build models with greater activation sparsity and leverage the merits of sparsity."
        },
        {
            "title": "LIMITATIONS",
            "content": "One limitation lies in the lack of experiments on even larger LLMs, such as models with more than 7B parameters. Such experiments will cost considerable computation resources and time but may potentially encounter unexpected observations (e.g., emergent properties). Another drawback of our study is the absence of computation (e.g., FLOPS) in some analyses, especially the experiments for width-depth ratios. In Section 4.3, we find smaller width-depth ratio potentially produces sparser model. However, with the substantial increase in the number of layers, the training efficiency is significantly decreased, as we have observed in the training process. Therefore, in addition to performance, the computation costs of model also deserve consideration. Similarly, considering the values of activation sparsity in acceleration, it may be interesting to involve the inference computation as variable in our study. Finally, an obvious limitation of our PPL-p% metric (as well as all the sparsity metrics relying on validation dataset) is the sensitivity to different data distributions. Intuitively, the same model can have different sparsity ratios on distinct datasets or tasks. The correlation between sparsity and influential factors (e.g., the form of power laws) can also have dataset-unique characteristics. piece of evidence already presented in our paper is in Table 1, where the performance on commonsense reasoning tasks is insensitive to p%, largely different from the results on reading comprehension tasks. Moreover, the data mixing policies for pre-training can also have considerable impact on the activation sparsity, which we leave for future work."
        },
        {
            "title": "REFERENCES",
            "content": "Abien Fred Agarap. Deep learning using rectified linear units (ReLU). arXiv preprint arXiv:1803.08375, 2018. URL https://arxiv.org/pdf/1803.08375.pdf. Subutai Ahmad and Luiz Scheinkman. How can we be so dense? The benefits of using highly sparse representations. arXiv preprint arXiv:1903.11257, 2019. URL https://arxiv.org/pdf/ 1903.11257.pdf. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. URL https://arxiv.org/pdf/2108. 07732.pdf. Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: replication attempt. arXiv preprint arXiv:2404.10102, 2024. URL https://arxiv.org/pdf/2404. 10102."
        },
        {
            "title": "Under peer review",
            "content": "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical In Proceedings of the AAAI conference on artificial intelcommonsense in natural language. ligence, volume 34, pp. 74327439, 2020. URL https://ojs.aaai.org/index.php/ AAAI/article/view/6239/6095. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, URL https://proceedings.neurips.cc/paper_files/paper/2020/ 2020. file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https:// arxiv.org/pdf/2107.03374.pdf. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936, 2019. URL https://aclanthology.org/N19-1300.pdf. Jonathan Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454470, 2020. URL https://aclanthology.org/2020.tacl-1.30.pdf. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv. org/pdf/2110.14168.pdf. Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre FT Martins, Fabrizio Esposito, Vera Lucia Raposo, Sofia Morgado, et al. SaulLM-7B: pioneering large language model for law. arXiv preprint arXiv:2403.03883, 2024. URL https://arxiv.org/pdf/2403.03883. Xavier Suau Cuadros, Luca Zappella, and Nicholas Apostoloff. Self-conditioning pre-trained language models. In International Conference on Machine Learning, pp. 44554473. PMLR, 2022. URL https://proceedings.mlr.press/v162/cuadros22a/cuadros22a.pdf. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained Transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 84938502, 2022. URL https:// aclanthology.org/2022.acl-long.581.pdf. Yann Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International Conference on Machine Learning, pp. 933941. PMLR, 2017. URL https://proceedings.mlr.press/v70/dauphin17a/dauphin17a. pdf. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. URL https://arxiv.org/pdf/ 2305.14233.pdf. Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neuNeural networks, 107: ral network function approximation in reinforcement 311, 2018. URL https://www.sciencedirect.com/science/article/pii/ S0893608017302976. learning."
        },
        {
            "title": "Under peer review",
            "content": "William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. URL https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf. Elias Frantar, Carlos Riquelme Ruiz, Neil Houlsby, Dan Alistarh, and Utku Evci. Scaling laws for sparsely-connected foundation models. In The Twelfth International Conference on Learning Representations, 2023. URL https://openreview.net/pdf?id=i9K2ZWkYIP. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. URL https: //arxiv.org/pdf/2101.00027.pdf. Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. URL https://arxiv.org/pdf/2406.04093. Xu Owen He. Mixture of million experts. arXiv preprint arXiv:2407.04153, 2024. URL https: //arxiv.org/pdf/2407.04153. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. URL https://arxiv.org/pdf/2009.03300.pdf. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, the 36th et al. Information Processing Systems, pp. 3001630030, International Conference on Neural URL https://papers.neurips.cc/paper_files/paper/2022/file/ 2022. c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf. Training compute-optimal large language models."
        },
        {
            "title": "In Proceedings of",
            "content": "Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. MiniCPM: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Liwei Chen, Songfang Huang, and Yansong Feng. Harder tasks need more experts: Dynamic routing in MoE models. arXiv preprint arXiv:2403.07652, 2024. URL https://arxiv.org/pdf/2403. 07652. Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. The Stack: 3 TB of permissively licensed source code. Transactions on Machine Learning Research, 2022. URL https://openreview.net/pdf?id=pxpbTdUEpD. Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pioro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Krol, Tomasz Odrzygozdz, Piotr Sankowski, et al. Scaling laws for fine-grained mixture of experts. arXiv preprint arXiv:2402.07871, 2024. URL https: //arxiv.org/pdf/2402.07871. Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage Moore, Nir Shavit, and Dan Alistarh. Inducing and exploiting activation sparsity for fast inference on deep neural networks. In International Conference on Machine Learning, pp. 55335543. PMLR, 2020. URL https://proceedings.mlr.press/ v119/kurtz20a/kurtz20a.pdf. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. StarCoder: may the source be with arXiv preprint arXiv:2305.06161, 2023. URL https://arxiv.org/pdf/2305. you! 06161.pdf."
        },
        {
            "title": "Under peer review",
            "content": "Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. The lazy neuron phenomenon: On emergence In The Eleventh International Conference on Learning of activation sparsity in Transformers. Representations, 2022. URL https://openreview.net/pdf?id=TJ2nxciYCk-. Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, and Weizhu Chen. GRIN: GRadient-INformed MoE, 2024. URL https://arxiv.org/abs/2409.12136. Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja Vu: Contextual sparsity for efficient LLMs at inference time. In International Conference on Machine Learning, pp. 2213722176. PMLR, 2023. URL https://proceedings.mlr.press/v202/liu23am/liu23am.pdf. Donald Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the society for Industrial and Applied Mathematics, 11(2):431441, 1963. Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, and Mehrdad Farajtabar. ReLU strikes back: Exploiting acarXiv preprint arXiv:2310.04564, 2023. URL tivation sparsity in large language models. https://arxiv.org/pdf/2310.04564.pdf. Ramchandran Muthukumar and Jeremias Sulam. Adversarial robustness of sparse local Lipschitz predictors. SIAM Journal on Mathematics of Data Science, 5(4):920948, 2023. URL https: //epubs.siam.org/doi/full/10.1137/22M1478835. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15251534, 2016. URL https://aclanthology.org/P16-1144.pdf. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-totext Transformer. Journal of machine learning research, 21(140):167, 2020. URL https: //www.jmlr.org/papers/volume21/20-074/20-074.pdf. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011. URL https://cdn.aaai.org/ocs/2418/2418-10878-1-PB.pdf. Hassan Sajjad, Nadir Durrani, and Fahim Dalvi. Neuron-level interpretation of deep NLP modTransactions of the Association for Computational Linguistics, 10:1285 els: survey. 1303, 2022. URL https://direct.mit.edu/tacl/article-pdf/doi/10.1162/ tacl_a_00519/2060745/tacl_a_00519.pdf. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 87328740, 2020. URL https://cdn.aaai.org/ojs/ 6399/6399-13-9624-1-10-20200517.pdf. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. SocialIQA: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, 2019. URL https: //aclanthology.org/D19-1454.pdf. Noam Shazeer. GLU variants improve Transformer. arXiv preprint arXiv:2002.05202, 2020. URL https://arxiv.org/pdf/2002.05202.pdf."
        },
        {
            "title": "Under peer review",
            "content": "Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. URL https://arxiv.org/pdf/2402.00159. Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, and Maosong Sun. ProSparse: Introducing and enhancing intrinsic activation sparsity within large language models. arXiv preprint arXiv:2402.13516, 2024a. URL https://arxiv.org/pdf/2402.13516.pdf. Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. PowerInfer: Fast large language model serving with consumer-grade GPU. arXiv preprint arXiv:2312.12456, 2023. URL https: //arxiv.org/pdf/2312.12456.pdf. Yixin Song, Haotong Xie, Zhengyan Zhang, Bo Wen, Li Ma, Zeyu Mi, and Haibo Chen. Turbo Sparse: Achieving LLM SOTA performance with minimal activated parameters. arXiv preprint arXiv:2406.05955, 2024b. URL https://arxiv.org/pdf/2406.05955. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. URL https://arxiv.org/pdf/2210.09261.pdf. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. URL https: //arxiv.org/pdf/2302.13971.pdf. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with OSS-Instruct. In Forty-first International Conference on Machine Learning, 2024. URL https://arxiv.org/pdf/2312.02120. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. WizardLM: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. URL https://arxiv.org/pdf/2304.12244. Zhenliang Xue, Yixin Song, Zeyu Mi, Le Chen, Yubin Xia, and Haibo Chen. PowerInfer-2: Fast large language model inference on smartphone. arXiv preprint arXiv:2406.06282, 2024. URL https://arxiv.org/pdf/2406.06282. Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs V: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. URL https://arxiv.org/pdf/2203.03466. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, 2019. URL https://aclanthology.org/ P19-1472.pdf. Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. MoEfication: In Findings of the Association for Transformer feed-forward layers are mixtures of experts. Computational Linguistics: ACL 2022, pp. 877890, 2022. URL https://aclanthology. org/2022.findings-acl.71.pdf. Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Chaojun Xiao, Xiaozhi Wang, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, and Jie Zhou. Emergent modularity in pre-trained Transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 40664083, 2023. URL https://aclanthology.org/2023.findings-acl.250.pdf. Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, and Maosong Sun. ReLU2 wins: Discovering efficient activation functions for sparse LLMs. arXiv preprint arXiv:2402.03804, 2024a. URL https://arxiv.org/ pdf/2402.03804.pdf."
        },
        {
            "title": "Under peer review",
            "content": "Zhengyan Zhang, Chaojun Xiao, Qiujieli Qin, Yankai Lin, Zhiyuan Zeng, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, and Jie Zhou. Exploring the benefit of activation sparsity in pretraining. In Forty-first International Conference on Machine Learning, 2024b. URL https: //openreview.net/pdf?id=KfXXPCcobh. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. URL https://arxiv.org/pdf/2304. 06364.pdf. Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. ST-MoE: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. URL https://arxiv.org/pdf/2202.08906."
        },
        {
            "title": "A TRAINING LOSS DYNAMICS",
            "content": "To present the comprehensive training dynamics of our pre-trained models, we plot the trend of loss with the increase of training data in Figure 11. As can be clearly observed, larger models have smaller training loss. Besides, we also plot the limit values of the training loss with infinite training tokens, shown in Figure 12. As demonstrated in the above two figures, SiLU and ReLU models are well comparable from the loss aspect. Figure 11: The trend of pre-training loss for models with different scales and activations. Figure 12: The limits of the training loss with the amount of training data approaches infinity."
        },
        {
            "title": "B SPARSITY STABILIZING STRATEGY",
            "content": "We find that the stochastic factors in gradient descent during pre-training have significant impact on the metric of activation sparsity. Especially, during the early training stage, the model is far from convergence with considerable sparsity fluctuations, and the magnitude-based sparsity metric can become unstable, making the sparsity-data curve not smooth enough. To eliminate the influence of these unstable factors to facilitate smoother sparsity metric, we first drop the sparsity points during the warmup stage for curve fitting. Moreover, we mainly apply the PPL-p% on the last several checkpoints (specifically, the last five pre-trained checkpoints) as whole, binary-searching CETT value that controls the average PPL of these checkpoints to just increase by p%. Then this CETT value is applied to all the checkpoints of this pre-training process to measure the sparsity."
        },
        {
            "title": "C BINARY SEARCH ALGORITHM FOR CETT",
            "content": "Given list of checkpoints, validation dataset, and hyper-parameter p%, we employ Algorithm 1 to find the CETT value that just makes the average PPL of these checkpoints on the validation dataset rise by exactly p%, compared to the dense setting with all the neurons activated. Note that this algorithm can be applied to either single checkpoint or multiple checkpoints, as adopted in the strategy described in Appendix B."
        },
        {
            "title": "D FITTING ALGORITHM AND RESULTS",
            "content": "We employ the Levenberg-Marquardt method (Marquardt, 1963) to fit the activation-data curves. To improve the stability of curve fitting, we divide the number of tokens passed (i.e., the amount of training data) by 109 to normalize its magnitude. All the results we obtained from fitting Eq. (4) (for ReLU-activated models) and Eq. (5) (for SiLU-activated models) are shown in Table 2."
        },
        {
            "title": "Under peer review",
            "content": "Algorithm 1 Find the CETT value for PPL-p% sparsity Input: The input list of checkpoints CkptList. Input: The validation dataset alidSet. Input: The hyper-parameter p%. Input: The error tolerance eps. Output: The CET that just makes the average PPL of CkptList on alidSet rise by p%. 0, 1 while > eps do mid (l + r)/2 LRatioList [ ] for Ckpt CkptList do lossdense Ldense(Ckpt, alidSet) losssparse Lsparse(Ckpt, alidSet, cett = mid) LRatio exp(losssparse lossdense) LRatioList.append(P LRatio) end for eanP LRatio Mean(P LRatioList) if eanP LRatio < 1 + p% then mid else mid end if end while CET (l + r)/2 return CET Table 2: Coefficients of activation-data (logspace) power-laws obtained from curve fitting. The curves of ReLU-activated and SiLU-activated models follow Eq. (4) and Eq. (5) respectively. α 0.1B 1.01 1001 1.51 1002 0.2B 4.49 1001 3.05 10+00 0.4B 6.83 1001 3.46 10+00 1.01 10+00 3.49 10+00 0.8B 1.33 10+00 3.89 10+00 1.2B 0.1B 4.79 1001 0.2B 8.44 1001 1.03 10+00 0.4B 0.8B 9.95 1001 1.2B 9.67 1001 - - - - - 3.20 10+00 2.86 1001 7.90 1002 7.97 1003 9.03 1004 1.02 1001 2.08 1001 4.20 1001 5.62 1001 5.38 1001 A0 6.14 1002 6.74 1002 6.90 1002 7.21 1002 7.82 1002 4.09 1001 3.90 1001 3.85 1001 3.83 1001 3.82 10 ReLU SiLU"
        },
        {
            "title": "E DATASETS AND BENCHMARKS",
            "content": "Training data The pre-training data is mixture of various corpus, including cleaned version of CommonCrawl, Dolma (Soldaini et al., 2024), C4 (Raffel et al., 2020), Pile (Gao et al., 2020), the Stack (Kocetkov et al., 2022), StarCoder (Li et al., 2023), and other collected raw corpus. In contrast, the decay data contains additional instruction-tuning data, such as UltraChat (Ding et al., 2023), SlimOrca (Colombo et al., 2024), OssInstruct (Wei et al., 2024), EvolInstruct (Xu et al., 2023), and other collected datasets. Validation data To measure the PPL-p% sparsity more precisely, we introduce tiny validation dataset, which shares the same distribution as the pre-training data. We conduct deduplication to eliminate any intersections between validation and pre-training data."
        },
        {
            "title": "Under peer review",
            "content": "Evaluation benchmarks To evaluate the task-specific performance of models, we introduce the following two groups of benchmarks: (1) Commonsense reasoning: We compute the average 0-shot accuracies on PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2020), and COPA (Roemmele et al., 2011). (2) Reading comprehension: We report the average 0-shot accuracies on BoolQ (Clark et al., 2019), LAMBADA (Paperno et al., 2016), and TyDi QA (Clark et al., 2020). We also evaluate our model on more complex tasks but fail to obtain performance above the random level. These include: the average pass@1 scores on HumanEval (0-shot) (Chen et al., 2021) and MBPP (3-shot) (Austin et al., 2021), the average accuracies on GSM8K (8-shot) (Cobbe et al., 2021), MMLU (5-shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3-shot) (Suzgun et al., 2022), and AGI-Eval (0-shot) (Zhong et al., 2023)."
        },
        {
            "title": "F DETAILED TRAINING SETTINGS",
            "content": "We utilize the MiniCPM (Hu et al., 2024) architecture and adopt its hyper-parameter policies, along with the WSD learning rate scheduling method. Across all parameter scales, the ratio of df to dh equal to 2.5 consistently, the number of query heads always matches that of key and value heads, and the width-depth ratios range from 48 to 56, generally similar across different scales. The specific number of parameters of various settings are shown in Table 3. We employ the following pretraining hyper-parameters across all settings: peak learning rate lr = 0.01, β1 = 0.9, β2 = 0.95, weight decay = 0.1. The batch size depends on the parameter scale, as presented in Table 3. Table 3: Hyper-parameters across various parameter scales. Parameter Scale # non-embedding parameters batch size 0.1B 1.08 108 3.27 105 0.2B 2.41 108 5.90 0.4B 4.52 108 7.86 105 0.8B 7.60 108 1.18 106 1.2B 1.18 109 1.57 106 DATASET-WISE ACTIVATION PATTERN Although the overall distribution patterns of activation frequencies are similar in terms of the average scenario, they exhibit difference when focusing on neurons in specific layers, such as the first, the last, or the exact middle layer. As shown in Figure 13, models with varying parameter scales have diverse neuron activation frequency distributions in the first layer and the last layer, while the patterns on the middle layer are still largely scale-insensitive."
        },
        {
            "title": "H PERFORMANCE ON INDEPENDENT BENCHMARKS",
            "content": "In Table 1, we already provide the average performance on the two groups of commonsense reasoning and reading comprehension. In this section, we present the evaluation scores on independent benchmarks of these two task groups, as shown in Table 4 and Table 5, respectively. From these tables, it can be observed that in commonsense reasoning benchmarks, as the number of model parameters increases from 0.1B to 1.2B, the average evaluation score of the ReLU settings rises from 49.6 to 60.0, while the average score of the SiLU settings increases from 49.5 to 59.6. Similarly, in reading comprehension benchmarks, the score of ReLU settings goes from 28.2 to 53.2, and the score of SiLU settings rises from 27.7 to 54.8. Additionally, models with these two distinct activation functions demonstrate comparable performance at the same parameter scale. Moreover, under the PPL-1% setting, the models are generally comparable to the dense setting with all neurons activated, whereas under the PPL-5% setting, they tend to suffer from significant performance on reading comprehension tasks, but the commonsense reasoning scores almost remain unaffected, which is phenomenon worth studies. We also evaluate our models on several more complex tasks. However, due to the limited number of parameters, we are unable to obtain reliable results above the random level. The evaluation results for this part are shown in Table 6."
        },
        {
            "title": "Under peer review",
            "content": "Figure 13: The distributions of average activation frequencies across three individual layers at different positions within models of distinct scales, including four datasets from the pre-training data."
        },
        {
            "title": "Under peer review",
            "content": "Table 4: Evaluation scores (%) on commonsense reasoning benchmarks. PIQA SIQA HellaSwag WinoGrande COPA 0.1B 0.2B 0.4B 0.8B 1.2B ReLU SiLU ReLU SiLU ReLU SiLU ReLU SiLU ReLU SiLU dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% acc 62.8 62.7 63.1 63.0 64.3 64.3 63.5 63. 66.3 66.3 66.2 66.0 67.6 68.2 67.4 66.8 68.8 68.8 68.3 68.1 69.0 68.7 68.9 68.7 70.1 69.8 69.9 69.6 70.4 70.3 69.9 69. 71.6 71.1 70.8 70.2 71.8 71.8 71.8 71.6 acc 37.8 37.4 37.6 38.0 37.6 37.5 38.4 38.1 38.3 38.1 38.1 37. 39.0 39.2 38.2 38.8 39.9 39.7 39.9 40.4 39.6 39.4 39.4 39.3 41.8 41.8 41.8 41.8 40.9 41.4 41.3 40.7 44.1 44.7 43.9 43. 41.2 40.9 41.3 41.3 acc 30.5 30.5 30.3 30.5 30.9 30.7 30.5 30.4 37.1 37.2 37.1 37.0 37.8 37.7 37.7 37. 42.7 42.9 42.7 42.6 44.5 44.6 44.6 44.9 50.4 50.2 49.7 50.0 50.6 50.6 51.0 50.6 57.7 58.0 57.8 57.1 57.8 57.8 57.9 58. acc 53.0 52.6 51.1 51.5 52.8 53.0 51.5 51.3 53.1 52.7 52.2 51.9 51.8 52.0 51.8 52.1 51.9 51.8 52.5 53. 51.9 52.2 51.5 51.0 53.6 52.8 52.3 51.8 54.0 53.9 54.1 53.2 56.4 55.3 54.9 53.7 56.1 57.3 55.9 55.5 acc 64.0 62.0 64.0 64.0 62.0 64.0 61.0 60.0 65.0 64.0 65.0 65.0 65.0 65.0 65.0 64.0 70.0 70.0 68.0 70.0 74.0 74.0 71.0 72. 68.0 65.0 68.0 65.0 72.0 72.0 69.0 68.0 70.0 69.0 69.0 72.0 71.0 70.0 67.0 70.0 Avg. 49.6 49.1 49.2 49. 49.5 49.9 49.0 48.7 52.0 51.7 51.7 51.6 52.2 52.4 52.0 51.9 54.7 54.6 54.3 54.9 55.8 55.8 55.1 55.2 56.8 55.9 56.3 55. 57.6 57.6 57.1 56.4 60.0 59.6 59.3 59.3 59.6 59.6 58.8 59."
        },
        {
            "title": "Under peer review",
            "content": "Table 5: Evaluation scores (%) on reading comprehension benchmarks. Avg. 28.2 28.4 26.9 26.2 27.7 28.0 26.5 24.8 40.7 39.7 38.6 38.6 40.2 39.6 36.8 34. 44.0 42.9 40.8 39.9 41.8 40.9 38.2 35.3 44.8 43.2 42.2 40.3 43.3 44.3 40.7 38.8 53.2 53.3 53.3 52.9 54.8 55.4 52.6 51. BoolQ LAMBADA TyDiQA TyDiQA F1 17.9 19.9 17.9 16.4 18.5 19.1 18.0 16.6 38.0 36.8 36.3 37.4 36.3 35.3 31.6 28. 43.6 42.1 39.9 39.2 41.1 40.5 38.1 35.0 42.6 41.0 40.0 37.8 41.0 43.3 37.5 34.6 54.3 55.0 56.3 56.8 55.2 56.1 53.1 53. acc 4.1 4.5 3.4 3.9 4.5 5.5 5.5 5.0 30.0 30.0 28.6 30.2 28.2 27.5 24.3 20.9 28.0 26.6 23.4 22. 25.4 23.4 20.4 17.7 27.3 24.6 24.1 21.1 22.1 24.8 18.2 15.0 42.5 42.7 45.2 44.5 47.3 47.5 43.9 43.4 0.1B 0.2B 0.4B 0.8B 1.2B ReLU SiLU ReLU SiLU ReLU SiLU ReLU SiLU ReLU SiLU dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% acc 60.8 60.6 60.6 60.1 56.5 56.2 53.6 51.9 56.3 56.2 56.4 55.9 57.5 57.5 55.2 54.5 61.7 61.6 60.8 60. 57.6 56.6 55.2 52.7 62.1 61.7 60.9 59.8 63.1 63.1 62.5 62.7 63.3 63.4 62.1 62.6 63.2 63.7 62.2 60.2 acc 30.1 28.5 25.6 24.6 31.4 31.1 28.9 25.7 38.4 35.8 33.0 30.8 38.7 38.3 36.0 34.0 42.9 41.3 39.1 37.8 43.0 43.1 39.2 35. 47.3 45.7 43.8 42.5 46.9 46.0 44.7 43.0 52.5 52.2 49.5 47.7 53.4 54.2 51.2 47."
        },
        {
            "title": "Under peer review",
            "content": "Table 6: Evaluation scores (%) on other more complex benchmarks. AGIEval HumanEval MBPP GSM8K MMLU BBH 0.1B 0.2B 0.4B 0.8B 1.2B ReLU SiLU ReLU SiLU ReLU SiLU ReLU SiLU ReLU SiLU dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% dense PPL-1% PPL-5% PPL-10% acc 23.4 23.3 23.5 23.4 23.6 23.5 23.6 23. 23.2 22.8 22.7 23.0 24.2 24.2 23.9 23.2 24.6 24.3 24.6 25.0 24.4 24.6 24.5 24.2 25.4 25.7 25.3 25.8 25.4 25.1 25.1 24. 26.6 26.5 25.8 25.9 26.2 27.0 25.7 25.6 acc 26.3 26.5 26.3 26.4 26.1 25.6 25.8 25.8 27.2 26.9 27.1 26. 25.7 25.2 25.0 24.2 26.1 26.2 26.6 26.5 24.9 25.8 25.3 24.6 26.3 26.3 26.5 26.4 24.7 24.8 24.5 24.2 33.4 33.9 34.3 34. 32.6 32.2 31.0 30.7 acc 29.3 29.5 28.7 29.7 29.2 28.5 30.6 29.0 28.8 30.3 29.7 30.1 29.6 29.1 29.0 28. 30.3 30.1 30.2 29.8 30.6 29.4 29.6 30.1 30.1 30.0 29.8 29.2 28.9 29.7 29.4 28.8 29.9 30.3 30.2 30.6 30.9 30.4 30.0 30. pass@1 pass@1 acc 0.3 0.3 0.1 0.2 0.8 0.4 0.3 0.4 1.5 1.2 1.0 1. 1.0 1.8 1.6 0.5 2.3 3.1 2.9 2.7 3.2 3.7 2.9 2.3 5.3 5.8 5.4 5.0 4.7 4.6 3.8 3.9 6.2 7.8 7.4 6. 9.0 8.9 8.5 6.9 1.8 1.7 1.9 1.4 1.6 2.1 1.4 1.4 1.6 2.1 1.6 2.1 2.2 2.0 1.4 2.4 2.1 1.9 2.2 2. 2.6 3.3 3.8 2.7 4.2 4.5 4.5 4.0 4.1 4.0 3.6 3.0 6.4 7.7 6.3 5.9 5.2 5.8 5.1 4.0 0.6 0.6 0.6 0. 0.6 0.6 0.6 1.2 2.4 2.4 2.4 2.4 4.3 4.3 5.5 3.0 6.7 7.9 7.9 7.3 5.5 5.5 6.1 4.9 9.2 9.2 8.5 8. 9.2 7.9 7.3 7.3 7.3 9.8 7.9 7.3 9.8 11.0 7.9 9.2 23 Avg. 13.6 13.7 13.5 13. 13.7 13.4 13.7 13.5 14.1 14.3 14.1 14.2 14.5 14.4 14.4 13.6 15.3 15.6 15.7 15.6 15.2 15.4 15.4 14.8 16.7 16.9 16.7 16. 16.1 16.0 15.6 15.3 18.3 19.3 18.6 18.4 18.9 19.2 18.0 17."
        }
    ],
    "affiliations": [
        "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China"
    ]
}