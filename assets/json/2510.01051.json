{
    "paper_title": "GEM: A Gym for Agentic LLMs",
    "authors": [
        "Zichen Liu",
        "Anya Sims",
        "Keyu Duan",
        "Changyu Chen",
        "Simon Yu",
        "Xiangxin Zhou",
        "Haotian Xu",
        "Shaopan Xiong",
        "Bo Liu",
        "Chenmien Tan",
        "Chuen Yang Beh",
        "Weixun Wang",
        "Hao Zhu",
        "Weiyan Shi",
        "Diyi Yang",
        "Michael Shieh",
        "Yee Whye Teh",
        "Wee Sun Lee",
        "Min Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 1 5 0 1 0 . 0 1 5 2 : r GEM: Gym for Agentic LLMs GEM: GYM FOR AGENTIC LLMS Zichen Liu12, Anya Sims13*, Keyu Duan12*, Changyu Chen14*, Simon Yu69, Xiangxin Zhou1* Haotian Xu7, Shaopan Xiong8, Bo Liu2, Chenmien Tan9, Chuen Yang Beh2, Weixun Wang8 Hao Zhu5, Weiyan Shi6, Diyi Yang5, Michael Shieh2, Yee Whye Teh3, Wee Sun Lee2, Min Lin1 1Sea AI Lab 2NUS 3Oxford 4SMU 5Stanford 6Northeastern 7OpenRLHF 8ROLL 9RL2 ABSTRACT The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), whichunlike GRPOis compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both singleand multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as convenient evaluation toolkit besides training environment. We hope this framework can help accelerate future agentic LLM research1. Figure 1: Learning curves of Qwen3-based agents across diverse environments of 5 categories: game (language games); rg (ReasoningGym); code (coding tasks); math (python-integrated math questions); qa (searchintegrated general questions). All agents are learned via simple yet general multi-turn algorithm based on REINFORCE (Algorithm 1). The comparison between two curves in each subplot illustrate the effectiveness of Return Batch Normalization (ReBN)."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement learning (RL) (Sutton and Barto, 2018) has emerged as powerful paradigm for improving the reasoning capabilities of large language models (LLMs) (OpenAI, 2024; Guo et al., 2025). By collecting experience in interactive environments, RL allows agents to learn complex, Equal contribution with random order. Work done during their associate membership at Sea AI Lab. 1Code is available at: https://github.com/axon-rl/gem. 1 GEM: Gym for Agentic LLMs open-ended tasks without direct supervision (Silver and Sutton, 2025). This approach promises to create powerful agents for variety of domains. For instance, an agent could develop entire software modules by writing, testing, and debugging code, while also adapting to integration failures or evolving requirements. Similarly, in scientific discovery, an agent could be trained to develop hypotheses, design relevant experiments, and adjust its long-term strategy based on the results. However, current research on RL for LLMs has largely focused on single-turn tasks, such as answering math questions or retrieving specific data (Lambert et al., 2024; Guo et al., 2025). While these tasks are valuable starting point, they significantly oversimplify multi-turn interactions (Liu et al., 2025a). This oversimplification means that algorithms which excel in the single-turn setting (e.g., GRPO (Shao et al., 2024)) are fundamentally inapplicable to full multi-turn problems. If the goal is to train agentic LLMs capable of long-horizon planning, trial-and-error, iterative refinement etc, it is crucial to transition to testbeds that support these more complex multi-turn interactions. To facilitate this next step, we introduce GEM (General Experience Maker), an open-source environment framework for diverse, multi-turn, long-horizon tasks. Motivated by OpenAI-Gym (Brockman et al., 2016) which catalyzed research in traditional RL by providing unified interface and standardized environments, GEM aims to provide analogous foundational infrastructure for LLM agents. GEM offers diverse suite of environments spanning singleand multi-turn (over 100 turns) tasks (including tool integrated responses, reasoning games etc), flexible observation and action wrappers, asynchronous parallel execution, and rich set of tools (python, search, and external MCP compatible tools). Additionally, GEM includes validated baselines and single-file training scripts showcasing seamless integration with five popular RL training frameworks (Oat, Verl, OpenRLHF, ROLL, and RL2see Section 4.5). Besides introducing the GEM framework, this paper also presents and discusses simple yet effective algorithmic variant of REINFORCE (Williams, 1992) which incorporates Return Batch Normalization (ReBN), useful technique similar to advantage normalization (Andrychowicz et al., 2021; Liu et al., 2025b) that brings consistent improvements (Figure 1). Unlike GRPO and its variants, REINFORCE with ReBN is fully compatible with the multi-turn RL setting, including turn-level dense rewards and arbitrary discount factors. We further compare REINFORCE-based algorithms with multi-turn PPO (Schulman et al., 2017) and GRPO, showing its theoretical connections and empirical tradeoffs. We also provide case studies on the impact of the discount factor γ on multi-turn learning, extensive results of tool-integrated RL, and performance benchmarks on terminal and MCP usage of strong LLMs using GEM as unified evaluation toolkit. We hope this framework will accelerate RL research on agentic LLMs and advance progress toward more capable and autonomous AI systems."
        },
        {
            "title": "2 GEM ENVIRONMENTS",
            "content": "This section introduces GEMs core functionality, covering its main interface (Section 2.1), the environment design (Section 2.2), and advanced features such as asynchronous vectorization and modular wrappers (Sections 2.3 and 2.4). INTERFACE 2.1 GEM employs standardized environment interface closely following the well-established OpenAI Gym API with the main functions being reset() and step(). basic agent-environment interaction loop is as follows (multi-agent interface shown in Section D.3): # (1) Agent acting: action = env.sample_random_action() # action = agent.act(observation) # real acting by LLM sampling 1 import gem 2 # gem.print_envs() # to list all available environments 3 env = gem.make(\"game:GuessTheNumber-v0\") 4 observation, info = env.reset() 5 6 while True: 7 8 9 10 11 12 13 14 15 16 17 18 # (3) Agent learning: # agent.learn(observation, action, reward) observation = next_obs if terminated or truncated: break # (2) Environment execution: next_obs, reward, terminated, truncated, info = env.step(action) 2 GEM: Gym for Agentic LLMs 2.2 TASKS AND TOOLS GEMs core environment components are tasks and tools. Each combination of task and an optional set of tools constitutes an environment that tests complex capabilities such as reasoning, multi-step planning, and tool use. These environments can therefore be used to benchmark LLMs and to test and develop new algorithms. GEM currently features seven main categories of tasks: Math: Solve math problems with chain-of-thought reasoning. Math with image: Solve geometry math problems with images using chain-of-thought reasoning. Code: Generate code to solve competitive programming problems. Game: Multi-turn text-based games adapted from TextArena (Guertler et al., 2025). QA: General, potentially knowledge-intensive questions (useful for testing search tool capability). ReasoningGym: unified interface of ReasoningGym (Stojanovski et al., 2025) which provides 100+ single-turn verifiable tasks. Terminal: Perform complex tasks through containerized terminal environment. GEMs modular design simplifies task integration. Math (with images), code, and QA tasks can be integrated by simply providing new dataset. Terminal tasks require new Docker file, instructions, and test cases. New games and other custom tasks can be added by inheriting from GEMs environment base class and defining their state transition and reward logic. In addition, tasks can be augmented with any combination of tools. GEM currently supports: Python: Parses and executes code blocks, returning the stdout or execution error. Search: Parses query, executes search against an external engine, and returns the results. MCP: General tool calling to any external servers that conform to the model context protocol. The use of tools converts single-turn tasks, like Math or ReasoningGym, into multi-turn tasks in which an agent can learn to call tools and adapt based on their output. 2.3 ASYNCHRONOUS VECTORIZATION AND AUTORESET To facilitate efficient agent RL training, we support parallel execution of vectorized environments via asynchronous tool calls to collect episodes in batches. In addition to the latency reduction, the use of vectorized environments with autoreset streamlines the experience collection logic. Users can run single .reset() at the initialization stage and simply continue with .step() in the following agent-environment loop for continuous data generation. In addition, the user code can use the returned terminated flag to prevent value bootstrapping across episode boundaries, ensuring the correctness of critic learning. An illustration of the autoreset mechanism can be found in Figure 2. Figure 2: Illustration of autoreset in vectorized environments. Autoresetting resets the environment automatically after termination, allowing users to collect batches of episodes by simply running .step() without needing more complicated logic such as keeping track of whether individual episodes have terminated. 2.4 WRAPPERS Like in OpenAI-Gym, GEM uses wrappers for easy extensibility. Observation wrappers, for example, control how the episode is converted into an observation. Options include observing just the most recent environment output, concatenation of all previous environment outputs, concatenation of all previous environment outputs and actions, or some parsed/summarized version of this. The 3 GEM: Gym for Agentic LLMs Figure 3: The illustration of different view of agentic RL. Green nodes denote tokens responsible for loss. Python interpreter or database/web search tools are also formulated as wrappers which can be added on top of any specified task environment."
        },
        {
            "title": "3 REINFORCEMENT LEARNING WITH GEM",
            "content": "In this section, we begin by describing the main RL formulations for LLMs, including their respective flexibilities and limitations (Section 3.1). Motivated by this, we then present our baseline algorithm which is applicable to the more flexible RL formulation (Section 3.2). 3.1 PRELIMINARY: LLMS AS AGENTS There are three main ways of treating LLM-environment interactions in RL algorithms which each have different limitations and strengths: Action = Single token (Figure 3(a)): The first approach is to treat each token generated by the LLM as an individual action (Ziegler et al., 2019). This, however, means that episodes are typically very long (thousands of tokens), and it also requires specifying the reward for the addition of every token, which is difficult to evaluate. Successful applications of RL in this formulation tend to use sparse outcome reward with discount factor γ = 1 (Guo et al., 2025). Action = Response (Figure 3(b)): To avoid these complications the second approach is to treat whole response (a sequence of tokens until an EOS) as single action2 (Ahmadian et al., 2024; Liu et al., 2025a). In answering math problems for examplecurrently the most common testbed for RL for LLMseach episode contains question and response. With this view all episodes therefore have length 1 and the RL problem essentially degenerates to contextual bandits (Abe et al., 2003). This is convenient as it means sample-based advantage estimation methods such as GRPO (Shao et al., 2024) can be applied efficiently, and these have been demonstrated to be highly effective. Extending to multi-turn episodes (e.g. for games or tool use), however, results in an issue: Multi-turn interactions have episode lengths > 1, meaning sample-based advantage estimation methods (e.g., Kazemnejad et al. (2025)) become infeasible (since they require collecting multiple episode completions from each turn (state) in the episode, leading to exponential complexity). Action = Whole interaction (Figure 3(c)): One approach to make GRPO applicable to multi-turn interactions is to treat the whole interaction as single action while masking the loss on tool outputs. This view again degenerates the full RL problem back to one-step RL or contextual bandits, meaning GRPO etc. can be applied. However, it requires two compromises: Firstly, it effectively fixes the discount factor at γ = 1, thus removing the incentive to solve problems quickly. This is significant, for example in Section 4.2 where we show how the optimal search algorithm is only recovered when setting γ < 1. Secondly, this approach is limited to single trajectory-level rewards, losing fine-grained per-turn credit assignment. Many prior works make these concessions and use GRPO in multi-turn LLM RL (Cao et al., 2025; Jiang et al., 2025; Chen et al., 2025a; Jin et al., 2025; Feng et al., 2025a). However, to develop an algorithm compatible with the full RL setting, we go back to the second view (action=response) and employ simple variant of REINFORCE with Return Batch Normalization (ReBN). Unlike GRPO, this algorithm is compatible with per-step dense rewards and arbitrary discount factors (γ 1), thus making it significantly more flexible for optimizing LLMs in complex, multi-turn interactive settings. 2Ignoring token-level PPO clipping which has no effect if the updates are on-policy. 4 GEM: Gym for Agentic LLMs ), n=1 (1) (cid:88) s(n) G(n) (n)1 (cid:88) log πθ(a(n) JREINFORCE(θ) = 3.2 BASELINE ALGORITHMS We start from the foundational on-policy3 policy-gradient method REINFORCE (Williams, 1992), which optimizes the following objective: 1 t=0 where is the batch size, [s0, a0, s1, ..., aT 1] is sequence of states and actions making up trajectory in which each st and at is itself sequence of tokens, and Gt = (cid:80)T 1 k=t γktrk is the return. Though initially designed for single-turn problems (i.e., (n) = 1), GRPO can be extended to multi-turn tasks by sampling group of trajectories per initial state and normalizing the trajectory-level reward for each group4: 1 GRPO = ((cid:80)T 1 n=1 mean(R))/std(R) with = {(cid:80)T where A(n,m) }m[1,...,M ]. However, this approach has poor credit assignment for multi-turn problems because all turns in the trajectory share the same advantage estimation, and improving it typically requires tree-like sampling which leads to combinatorial explosion. To bypass the expensive sampling from each turn, we can learn value function to estimate the return Gt, known as critic (Sutton and Barto, 2018), which in turn guides the policy learning in the actor-critic architecture. We can compute GAE (Schulman et al., 2015) for the advantage actor-critic (A2C) objective: 1 t=0 r(n,m) log πθ(a(n,m) t=0 r(n,m) (n,m)1 (cid:88) JGRPO(θ) = A(n,m) GRPO s(n,m) (cid:88) (cid:88) m=1 (2) t=0 ), JA2C(θ) = 1 (cid:88) (n)1 (cid:88) n=1 t=0 GAE,t log πθ(a(n) A(n) s(n) ). (3) To retain the benefits of fine-grained and stable advantage estimation without the combinatorial explosion or learning an additional critic, we instead use Return Batch Normalization (ReBN). For ReBN the per-transition returns Gi are normalized over the whole batch of transitions: 1 mean(G))/std(G), with = {G(n) where A(n) }n[1,...,N ],t[1,...,T (n)1]. Each of these algorithms trains the agent by iterating between two main phases: (A) data collection and (B) policy update. We present the RL loop of Equation (4) in Algorithm 1 in Section due to space constraint. ReBN,t log πθ(a(n) A(n) JREINFORCE+ReBN(θ) = ReBN,t = (G(n) (n)1 (cid:88) s(n) (cid:88) (4) n=1 t=0 ), t"
        },
        {
            "title": "4 EMPIRICAL STUDIES WITH GEM",
            "content": "In this section, we demonstrate how GEM can facilitate RL research on agentic LLMs through series of empirical studies. These include comprehensive apples-to-apples algorithm benchmarking across eight GEM environments (Section 4.1); analyses of the effects of the discount factor γ and tool integration (Sections 4.2 and 4.3); an examination of cross-task generalization (Section 4.4); and, finally, demonstration of GEMs compatibility with five RL training frameworks along with their easily accessible infrastructure benefits (Section 4.5). RL results in vision-language environment and analysis of multi-agent environment can be found in Sections D.2 and D.3. 4.1 BENCHMARKING RL ALGORITHMS FOR LLMS Benchmarking has been critical for the progress of RL, with OpenAI-Gym providing standardized environments that enabled systematic evaluation of algorithms (Raffin et al., 2021; Huang et al., 2022). Following this paradigm, GEM offers unified testbed for agentic LLMs, where prior work often relied on bespoke tasks that complicate fair comparison. We benchmark all baseline algorithms introduced in Section 3.2 (GRPO, PPO5, REINFORCE, ReBN) across eight GEM environments under unified experimental protocol. All algorithms are implemented using Oat (Liu et al., 2024) with hyperparameters detailed in Section F. Results are evaluated by mean episode return, sample efficiency, and stability. 3Orthogonally, we can also utilize proximal updates (Schulman et al., 2017) to improve sample efficiency. 4This is not the original GRPO because we fixed the length bias as noted by Liu et al. (2025b). 5PPO in this work generally refers to turn-level PPO instead of token-level PPO commonly seen in single-turn dialogue scenarios (Ouyang et al., 2022). 5 GEM: Gym for Agentic LLMs Figure 4: Algorithm benchmarking using eight representative environments from GEM. All agents are trained from Qwen3-{scale}-Base models, with scale specified in each plot. rg refers to single-turn reasoning tasks from ReasoningGym; game consists of long-horizon language games; qa and math are tool-integrated multi-turn environments. We present all learning curves in Figure 4. We first observe that in all three single-turn environments (labeled with rg), GRPO performs reasonably well, defending its effectiveness in single-step RL with verifiable rewards. However, GRPO falls short when it comes to multi-turn environments (GuessTheNumber and Sudoku), where dense per-turn rewards are available and more finegrained credit assignment is necessary for efficient policy learning, due to constant advantage estimation across all steps. Such effects are the most profound when the environments reward structure is inherently non-sparse (qa and math is less so). In contrast to GRPO, REINFORCE and PPO are natively suitable for multi-turn RL. We find that vanilla REINFORCE is readily strong baseline in most environments, but it might suffer from suboptimal convergence (e.g., two Sudoku environments). We hypothesize that this might be because the raw return calculation of vanilla REINFORCE can be sensitive to reward shaping, thus hindering exploration; we defer an in-depth ablation study to Section D.1. On the other hand, PPO is generally performant, attaining the best episode return in the complex and long-horizon Sudoku environment. This performance advantage can be attributed to well-learned critic, but it is also deemed difficult to robustly learn an accurate critic (Van Hasselt et al., 2018; Kazemnejad et al., 2025) (as evidenced by the poor performance of PPO in Minesweeper), inviting future works to go in this direction. Finally, we investigate the proposed REINFORCE variant, which incorporates simple Return Batch Normalization (ReBN) technique. Results in both Figures 1 and 4 show that ReBN consistently improves on vanilla REINFORCE by large margin, suggesting the empirical benefits of adaptive normalization of policy gradient coefficients. Moreover, ReBN outperforms or is comparable with PPO and GRPO in all evaluated environments, rendering it the strongest baseline without expensive computations, such as critic learning or extensive rollouts. 4.2 DISCOUNT FACTOR γ MATTERS Figure 5: (a) Average number of turns and episode return when trained with different discount factors. (b) Comparative experiment results on tool availability. Next, we investigate the effect of the discount factor γ. key motivation for REINFORCE+ReBN over GRPO is its compatibility with arbitrary discount factors. To investigate the effect of this 6 GEM: Gym for Agentic LLMs we trained the Qwen3-1.7B-Base model (Yang et al., 2025) using REINFORCE+ReBN on the GuessTheNumber environment. In this environment the agent must guess hidden number randomly selected between 1 and 50. At each turn the agent may guess, and receives feedback as to whether the hidden number is larger or smaller. The optimal strategy is therefore binary search. As shown in Figure 5(a), as expected, smaller γ values naturally encourage solutions with fewer turns and drive convergence to the optimal turn count (log2(50) 5.6)achievable only through binary search. Example interactions are included in Section B. As discussed in Section 3.2, the natural efficiency incentive from γ < 1 is not compatible with GRPO. Instead, prior works using GRPO hyperparameter tune the environments maximum number of turns to get efficient agent behavior (Xue et al., 2025). 4.3 TOOL-INTEGRATION IN MATH AND QUESTION-ANSWERING TASKS GEM is designed with modular support for external tools, enabling seamless integration into range of tasks. To empirically assess the impact of tool use, we focus on two domains: Math and Question-Answering (QA). Table 1: Math benchmark scores for four agents, evaluated with and without tool access and RL training. Note: scores should be interpreted relative to other values here due to sensitivity to the grader code (see Section 4.3). Qwen3-4B-Base Base (no tool) Base (with tool) Base + RL (no tool) Base + RL (with tool) AIME24 AMC MATH500 MinervaMath OlympiadBench Average 10.0 39.8 61.0 36.4 29.5 35.3 6.7 50.6 62.4 30.1 31. 36.2 16.7 49.4 67.4 40.1 33.5 41.4 30.0 67.5 71.0 40.4 39.9 49.8 We first investigate the effect of GEMs Python tool on Math tasks. Starting from the base model Qwen3-4B-Base, we finetune on the math:Orz57K environment, training two variants: one with Python tool integration and one without. The base model and both finetuned models are then evaluated across five distinct math environments. Hyperparameter details are provided in Section F, with the training curve shown in Figure 5(b), and Pass@1 accuracy reported in Table 1. The math grader used for reward and evaluation is based on HuggingFaces math_verify library6. We found that even minor differences in grading logic across codebases yields substantial variation in reported performance. Thus, all results should be interpreted comparativelywithin consistent evaluation frameworkrather than as absolute values. This further highlights the need for unified benchmarking, as provided by GEM. Results in Table 1 reveal clear and consistent pattern: across all environments, performance improves substantially after RL training compared to the base model. Furthermore, the model with access to the Python tool achieves higher final performance in every setting. Table 2: QA benchmark scores for the base agent and agents trained with different RL configurations. and * denote single-hop and multi-hop datasets, respectively. Qwen3-4B NQ TriviaQA PopQA HotpotQA* 2wiki* Musique* Bamboogle* Average Base (no tool) Base + RL (no tool, single env) Base + RL (no tool, mixed env) Base + RL (with tool, single env) Base + RL (with tool, mixed env) 6.1 35.4 11.3 11.1 10.0 2.9 17. 10.2 15.4 43.4 19.0 21.1 26.8 4.7 28.8 22.7 15.8 44.9 19.9 22.1 30.1 5.5 28.8 23.9 35.0 69.0 47.1 43.2 44.5 17.6 49. 43.7 37.3 71.9 48.1 45.5 46.7 19.9 48.8 45.5 6github.com/huggingface/Math-Verify 7 GEM: Gym for Agentic LLMs We also perform parallel analysis for QA tasks, this time integrating the Search tool. We train on two environment compositions: qa:HotpotQA alone, and mixture of both qa:HotpotQA and qa:NaturalQuestions. All other setting are the same as for the Math experiments (see above). Evaluation spans seven diverse QA environments. Results, summarized in Table 2, mirror those from the math domain: RL finetuning markedly improves performance, and models equipped with the Search tool achieve the highest accuracy in every scenario. The consistency of these findings across both domains (mathematics and QA), tools (Python and Search), and multiple evaluation environments underscores the flexibility and robustness of GEMs approach to RL LLM with tool integration. 4.4 STUDYING GENERALIZATION Figure 6: Training on the game:sudoku-v0-easy environment generalizes to ReasoningGym. GEMs environments can be used for both training and evaluation. This makes it ideal for investigating cross-environment generalization. For instance, we demonstrate training on the game:sudoku-v0-easy environment, while periodically evaluating on three different environments, with some encouraging initial generalization results shown in Figure 6. 4. INTEGRATION WITH TRAINING FRAMEWORKS Finally, we demonstrate that GEMwhich takes care of the environment sidecan be easily integrated with five popular frameworks that handle the training side. There has been proliferation of frameworks focusing on the training side of LLM RL. These often rely heavily on multiple other libraries (such as vLLM for response generation (Kwon et al., 2023), and DeepSpeed for optimization (Rasley et al., 2020)). The diverse range of features and design choices make it challenging for researchers to select and adapt suitable training framework to their specific needs. To address this GEM comes with complete, single-file training scripts showing clean integration into five widely used LLM RL frameworks: Oat (Liu et al., 2024), Verl (Sheng et al., 2024), OpenRLHF (Hu et al., 2024), ROLL (Wang et al., 2025a), and RL2 (Tan et al., 2025). These are validated in Figure 7(a) where we show the training curve for each. Despite minor differences due to underlying design choices of the frameworks (e.g., different LLM generation engines) and RL stochasticity, all curves exhibit similar trends, demonstrating that GEM is agnostic to training frameworks and validating their implementation equivalence. Furthermore, supporting wide range of frameworks allows us to effortlessly access their advanced features. For example, enabling the asynchronous rollout in RL2 gives an immediate 2 gain in wall-clock efficiency (Figure 7(b)). Figure 7: (a) Training curves on two environments showing successful integration of GEM into five existing frameworks. (b) Asynchronous rollout improves wall-clock efficiency of training Sudoku-solving agents based on Qwen3-4B-Base. 8 GEM: Gym for Agentic LLMs"
        },
        {
            "title": "5 AGENT EVALUATION WITH GEM",
            "content": "In addition to RL training, GEM can serve as unified evaluation interface to test LLM agents performance. In this section, we present two example use cases where we evaluate agents powered by strong LLMs (GPT-5 (OpenAI, 2025), Gemini-2.5-Pro (Gemini Team, 2025) and ClaudeSonnet-4 (Anthropic, 2025a)) on two complex tasks: database operation via model context protocol (MCP) (Anthropic, 2025b) and terminal interaction via docker containers, both of which have been added to GEM following Section A. 5.1 GENERAL TOOL USE VIA MODEL CONTEXT PROTOCOL Modern LLM agents often need to interact with external tools, such as search engines, APIs, and code interpreters. To facilitate this, GEM is designed to be compatible with the MCP, which is an open protocol that provides standardized way for LLMs to communicate with external tools and data sources. The MCP architecture consists of an MCP host (the LLM application), an MCP client, and an MCP server (the external tool). By adopting this protocol, GEM allows for \"plug-and-play\" tool usage, where any tool that implements the MCP server interface can be used by an agent in GEM environment without custom integration. This significantly simplifies the process of creating tool-augmented LLM agents and opens up vast ecosystem of potential tools. Figure 8: Benchmark results on MCPMark (Postgres subset) and Terminal-Bench (subset) using GEM as unified evaluation toolkit. Using PostgreSQL MCP tool, we assess the agents tool-augmented reasoning capabilities using 20 database tasks taken from MCPMark (Team, 2025a). We report the average success rate and the average number of turns required to complete the tasks in the left panel of Figure 87. GPT-5 attains the best success rate with the fewest interactions, while Gemini-2.5-Pro and Claude-Sonnet-4 have slightly lower and varied performance. 5.2 TERMINAL ENVIRONMENT VIA DOCKER CONTAINER To support wider range of tasks, especially those involving complex software dependencies and interactions with the operating system, GEM includes support for environments running inside docker containers. The integrated terminal environment provides sandboxed unix operating system where agents can learn to perform tasks using shell commands. This approach provides high degree of isolation and reproducibility, ensuring that the environment is consistent across different machines. We assess the terminal mastery of LLM agents on 57 tasks sampled from Terminal-Bench (Team, 2025b), without any scaffolding. The right panel of Figure 8 reports the average success rate and the number of turns required to complete the tasks. GPT-5 attains the highest success rate with the most efficient interaction, followed by Claude-Sonnet-4 and Gemini-2.5-Pro. The evaluation leverages the same interaction loop used for RL training, highlighting GEMs role as unified framework for both reinforcement learning and standardized evaluation."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "GEM aims to accelerate agentic LLM research by providing decoupled and clean library that is agnostic to training frameworks, unified agent-environment interface and suite of standardized environments. In this paper, we introduced the design choices of GEM, the current suite of task domains and tools, features like vectorized environment execution, simple yet general multi-turn REINFORCE algorithm implemented in five training frameworks, comprehensive algorithm benchmarking evaluation, and in-depth analysis on several algorithmic details. We invite the community to enter the era of experience for LLM agent learning, and join us in both using and continuing to develop the GEM framework. 7Our evaluation relies on the basic response generation API rather than agent frameworks (e.g., LangChain, OpenAI Agent SDK), which may lead to deviations from the original benchmark results. 9 GEM: Gym for Agentic LLMs"
        },
        {
            "title": "REFERENCES",
            "content": "Naoki Abe, Alan Biermann, and Philip Long. Reinforcement learning with immediate rewards and linear hypotheses. Algorithmica, 37(4):263293, 2003. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphaël Marinier, Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What matters for on-policy deep actor-critic methods? large-scale study. In International conference on learning representations, 2021. Anthropic. System card: Claude opus 4 & claude sonnet 4. https://www-cdn.anthropic. com/07b2a3f9902ee19fe39a36ca638e5ae987bc64dd.pdf, 2025a. Anthropic. Model context protocol. https://github.com/modelcontextprotocol/ modelcontextprotocol, 2025b. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. Shiyi Cao, Sumanth Hegde, Dacheng Li, Tyler Griggs, Shu Liu, Eric Tang, Jiayi Pan, Xingyao Wang, Akshay Malik, Graham Neubig, Kourosh Hakhamaneshi, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-v0: Train real-world long-horizon agents via reinforcement learning, 2025. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to reason with search for llms via reinforcement learning, 2025a. Wentse Chen, Jiayu Chen, Hao Zhu, and Jeff Schneider. Context-lite multi-turn reinforcement learning for LLM agents. In ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 2025b. URL https://openreview.net/forum?id=6CE5PLsZdW. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms, 2025a. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025b. Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Leon Guertler, Bobby Cheng, Simon Yu, Bo Liu, Leshem Choshen, and Cheston Tan. Textarena. arXiv preprint arXiv:2504.11442, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and João G.M. Araújo. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274):118, 2022. URL http://jmlr.org/papers/v23/21-1342.html. GEM: Gym for Agentic LLMs Dongfu Jiang, Zhuofeng Li, Yi Lu, Zhiheng Lvu, Ping Nie, Wenhu Chen, Tianyu Pang, and Chao Du. Verltool, 2025. URL https://github.com/TIGER-AI-Lab/verl-tool. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Refining credit assignment in rl training of llms. In International conference on machine learning, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, et al. Spiral: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025a. Zichen Liu, Changyu Chen, Xinyi Wan, Chao Du, Wee Sun Lee, and Min Lin. Oat: researchfriendly framework for llm online alignment. https://github.com/sail-sg/oat, 2024. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. In Conference on Language Modeling (COLM), 2025b. OpenAI. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. OpenAI. Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):18, 2021. URL http://jmlr.org/papers/v22/ 20-1364.html. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506, 2020. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 11 GEM: Gym for Agentic LLMs Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 1, 2025. Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas Köpf. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards. arXiv preprint arXiv:2505.24760, 2025. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. Chenmien Tan, Simon Yu, Lanbo Lin, Ze Zhang, Yuanwu Xu, Chenhao Jiang, Tianyuan Yang, Sicong Xie, and Guannan Zhang. Rl2: Ray less reinforcement learning. https://github.com/ ChenmienTan/RL2, 2025. GitHub repository. The MCPMark Team. Mcpmark: Stress-testing comprehensive mcp use. https://github. com/eval-sys/mcpmark, 2025a. The Terminal-Bench Team. Terminal-bench: benchmark for ai agents in terminal environments, Apr 2025b. URL https://github.com/laude-institute/terminal-bench. Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018. Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025a. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning, 2025b. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, et al. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Zejun Ma, and Bo An. Simpletir: End-toend reinforcement learning for multi-turn tool-integrated reasoning. https://simpletir. notion.site/report, 2025. Notion Blog. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 12 GEM: Gym for Agentic LLMs"
        },
        {
            "title": "A ENVIRONMENT REGISTRATION",
            "content": "GEM enables rapid development of new RL environments. In this section, we illustrate two scenarios: (i) integrating additional datasets into an existing task and (ii) defining custom task, followed by the procedure for registering these environments for use. The following code snippet shows how to add new dataset for math environment, where the answer verification logic is predefined by GEM and can be reused. 1 import gem 2 from gem.envs.registration import register 3 4 register( 5 6 7 8 9 10 ) 11 12 env = gem.make(\"math:GSM8K-Example\") # ready to use \"math:GSM8K-Example\", \"gem.envs.math_env:MathEnv\", dataset_name=\"axon-rl/GSM-8k\", # HuggingFace or local dataset path question_key=\"problem\", answer_key=\"answer\", Next, we demonstrate how to build new environment from scratch by defining the initial state distribution (in .reset()) and the transition and reward functions (in .step()) as follows. 1 from gem.core import Env 2 from gem.envs.registration import register 3 from gem.utils.constants import TERMINAL_STATE 4 from gem.utils.parsing import extract_last_boxed_answer 5 6 class ReverseStringEnv(Env): 7 8 9 10 11 12 13 super().__init__() self.str_len = str_len def __init__(self, str_len: int = 5): def _get_instructions(self): return ( \"You are tasked to reverse given string.n\" \"You may provide your response in any manner. Only the content wrapped inside boxed{} will be considered as your final answer.n\" super().reset(seed) characters = string.ascii_letters + string.digits # A-Z, a-z, 0-9 self.gt_str = \"\".join(random.choices(characters, k=self.str_len)) return self._get_instructions(), {} ) f\"Please reverse the string: {self.gt_str}.n\" def step(self, action): def reset(self, seed=None): 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # Register your environment 34 register(\"custom:ReverseString\", ReverseStringEnv) 35 36 env = gem.make(\"custom:ReverseString\") return TERMINAL_STATE, reward, True, True, {} clean_action = extract_last_boxed_answer(action) if clean_action is None: reward = 0 else: reward = float(clean_action[::-1] == self.gt_str)"
        },
        {
            "title": "B CASE STUDIES OF LANGUAGE GAMES",
            "content": "This section illustrates example interactions of game-playing agents trained on GuessTheNumber under different discount factors γ. With near-undiscounted setting (γ = 0.999), the agent fails to complete the game within the trial budget, as it lacks incentive to obtain the terminal correctness reward as earlier as possible (see example below). 13 GEM: Gym for Agentic LLMs (γ = 0.999, the agent exhausts all 10 trials) State: You are playing Guess The Number. You have to guess the number between 1 and 50 (inclusive) within 50 turns. As you play, the history of your guesses will be appended below. Use the information to complete the game before you run out of guesses. Enter your first guess to start the game. At turn 1, you guessed 14, and the target number is higher than 14. At turn 2, you guessed 22, and the target number is higher than 22. . . . At turn 10, you guessed 39, which has been already guessed before. Enter your next guess. Action: Lets break down the problem step by step: 1. **Initial Guess (Turn 1):** The target number is higher than 14. This means the target number is between 15 and 50 (inclusive). 2. **Initial Guess (Turn 2):** The target number is higher than 22. This means the target number is between 23 and 50 (inclusive). . . . From the final two steps, the target number is between 37 and 50 (inclusive). The final answer is boxed{38}. In contrast, when using more myopic return estimate (γ = 0.9), the agent is encouraged to minimize the number of turns, discovers the binary search strategy, and converges to the optimal policy. (γ = 0.9, the agent learns to conduct binary search) State: You are playing Guess The Number. You have to guess the number between 1 and 50 (inclusive) within 50 turns. As you play, the history of your guesses will be appended below. Use the information to complete the game before you run out of guesses. Enter your first guess to start the game. At turn 1, you guessed 25, and the target number is lower than 25. At turn 2, you guessed 12, and the target number is higher than 12. At turn 3, you guessed 18, and the target number is higher than 18. At turn 4, you guessed 21, and the target number is higher than 21. At turn 5, you guessed 23, and the target number is lower than 23. Enter your next guess. Action: To determine the target number between 1 and 50, we can use binary search strategy, which is effective for narrowing down range when given hints like higher or lower. Step-by-Step Reasoning: 1. **Initial Guess (Turn 1):** - The target number is lower than 25. - This means the target number is in the range [1, 24]. 2. **Second Guess (Turn 2):** - The target number is higher than 12. - This means the target number is in the range [13, 24]. 3. **Third Guess (Turn 3):** - The target number is higher than 18. - This means the target number is in the range [19, 24]. 4. **Fourth Guess (Turn 4):** - The target number is higher than 21. - This means the target number is in the range [22, 24]. 5. **Fifth Guess (Turn 5):** - The target number is lower than 23. - This means the target number is in the range [22, 22]. Final Guess: The target number is in the range [22, 22], which means the target number is 22. Final Answer: boxed{22} 14 GEM: Gym for Agentic LLMs"
        },
        {
            "title": "C ALGORITHM",
            "content": "For completeness we include the full algorithm of ReBN in Algorithm 1. Algorithm 1 Multi-turn REINFORCE with Return Batch Normalization (ReBN) Reset batch buffer while do // Multi-turn episode collection Sample the initial state s0 ρ for turn = 0, 1, . . . , 1 until terminate do Require: Policy πθ, Environment = (S, A, P, R, γ, ρ), Batch size 1: while not converged do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for for = 0, 1, . . . , 1 do Gt (cid:80)T 1 k=t γktrk Add (st, yt, Gt) to yt πθ(st) at extract_action(yt) rt R(st, at) st+1 (st, at) end for end while 17: 18: // Return Batch Normalization Gi (Gi mean(G)) / std(G) Generate reasoning + action Compute discounted return 19: 20: 21: end while // Policy optimization Update θ using Monte Carlo policy gradient (cid:80)B Or proximal update for data reuse Giθ log πθ(yisi) i="
        },
        {
            "title": "D EXTENDED EMPIRICAL STUDIES WITH GEM",
            "content": "(a-b) The average success rate of two Figure 9: Learning curves of different reward shaping strategies. environments. (c-d) The corresponding average number of turns taken to solve the tasks, equal to the number of tool calls minus one. D.1 IMPROVING LEARNING EFFICIENCY VIA RETURN BATCH NORMALIZATION (REBN) As briefly discussed in Section 4.1, while REINFORCE demonstrates strong performance across most environments, its convergence can be suboptimal in certain cases. To investigate this further, we present an in-depth ablation study here. Following minimalist principles, we began with the vanilla REINFORCE algorithm and simple reward scheme: = 1 for correct answers and = 0 otherwise. This approach has been shown effective for single-turn RL training (Singh et al., 2023; Xiong et al., 2025). However, as shown in Figure 9(c) (w.o ReBN), it failed to induce tool usage in multi-turn settings, despite significant amount of initial attempts. We hypothesize that this failure arises from the absence of negative gradients under 0/1 reward shaping, which are crucial for efficient learning and exploration. To address this, we introduced negative gradients in two ways: (i) assigning fixed negative rewards (r = 1 for correct and = 1 for incorrect answers, denoted as Neg rew in Figure 9); and (ii) applying Return Batch Normalization with GEM: Gym for Agentic LLMs 0/1 rewards, where Monte Carlo returns in REINFORCE are normalized as described in Algorithm 1 (denoted as ReBN in Figure 9). While both 0/1 and 1 reward schemes theoretically induce the same optimal policy, they might exhibit markedly different learning dynamics in practice. Notably, ReBN demonstrates strong and consistent performance across environmentsnot only in math and QA tasks (Figure 9) but also in all other settings (Figure 1). We also observe that models can be sensitive to fixed reward shaping: for example, Neg rew fails to improve tool use in math:Orz57K, yet leads to tool overuse in qa:HotpotQA, both of which are suboptimal behaviors. D.2 RL ON VISION-LANGUAGE ENVIRONMENTS In addition to text-only environments, we support visual elements as part of the observation for the agent to understand and take actions. As demonstrative example, we build visual-language environment based on Geometry3k dataset8 for training reasoning agent to solve geometry math problems with images input. We RL-tune Qwen2.5-VL-3B/7B-Instruct (Bai et al., 2025) using Dr. GRPO (Liu et al., 2025b), and the learning curves are shown in Figure 10. An example reasoning trace is shown in Figure 11. Figure 10: Learning curves of vision-language agents. We RL-tune Qwen2.5-VL-3B/7B-Instruct using Dr. GRPO on the math:Geometry3K environment and track their training rewards (left) and validation scores (right). Figure 11: An example problem and the response of trained agent based on Qwen2.5-VL-7B-Instruct. 8https://huggingface.co/datasets/hiyouga/geometry3k. 16 GEM: Gym for Agentic LLMs Figure 12: Multi-agent evaluation on TAU-bench retail. Stronger user simulators (rows) consistently improve agent performance (columns) across model strengths. D.3 MULTI-AGENT ENVIRONMENTS GEM supports multi-agent settings where multiple agents interact within the same environment. This capability enables the development of agents that can collaborate, compete, or simulate realistic interactions with other entities. Interface design. GEM provides MultiAgentEnv base class that extends the standard Gym API to support multiple agents. The step() and reset() functions operate on dictionaries keyed by agent identifiers: 1 from gem.envs.multiagent import MultiAgentEnv 2 3 env = MyMultiAgentEnv() 4 observations, infos = env.reset() # Dict[agent_id -> observation] 5 6 while not done: 7 8 9 actions = {agent_id: agent.act(obs) for agent_id, obs in observations.items()} observations, rewards, terminations, truncations, infos = env.step(actions) done = all(terminations.values()) To implement custom environment, users inherit from MultiAgentEnv and implement observe(agent) and _process_actions(actions). The framework handles agent lifecycle management and cumulative rewards tracking. Turn coordination is managed via AgentSelector, which supports two modes: sequential (agents act one at time in roundrobin order) and parallel (all agents act simultaneously). The selector determines which agents are active at each step and automatically advances turns, enabling flexible multi-agent interaction patterns without manual bookkeeping. TAU-bench retail integration. We demonstrate this API by integrating the TAU-bench retail benchmark (Yao et al., 2024), which evaluates conversational agents on customer service tasks. We formulate this as two-agent environment: an assistant agent using tools (order lookup, product search) and user agent simulating customer behavior via an LLM. The user simulator is initialized with task instructions and generates queries; the assistant must satisfy these requests before episode termination. Impact of user model strength. key question in multi-agent RL is: how does simulated user agent capability affect trainable assistant agent learning? We vary both user and assistant models across three levels: weak (Gemini-2.0-Flash-Lite), medium (GPT-4o-mini), and strong (GPT-4o), yielding 9 configurations to study user-assistant model interactions. 17 GEM: Gym for Agentic LLMs Evaluating across all 115 tasks from the TAU-bench retail test set (Figure 12), we find that stronger user agents consistently improve overall success rates across all assistant agent model strengths. Notably, the strongest assistant (GPT-4o) exhibits the largest absolute performance gains (20% from weak to strong user), achieving 61.7% success with strong user simulator. Interestingly, strong user paired with weak assistant (44.3%) outperforms weak user paired with strong assistant (41.7%), demonstrating that improving the user agent is crucial for robust conversational task completion. These results motivate us to develop multi-agent RL to co-evolve user and assistant agents to achieve scalable and autonomous learning."
        },
        {
            "title": "E RELATED WORKS",
            "content": "There is significant body of work on tool-integrated language modelsincluding SkyRL-v0 (Cao et al., 2025), VerlTool (Jiang et al., 2025), ReCall and ReSearch (Chen et al., 2025a), Search-R1 (Jin et al., 2025), ReTool (Feng et al., 2025a), and SimpleTIR (Xue et al., 2025). common design pattern in these methods is to collect multi-turn agent-environment interactions as single continuous sequences of tokens of agent actions interleaved with environment outputs. Training then simply involves masking the environment outputs from the loss calculation. However, this single-sequence approach presents two significant limitations. First, the state observation is rigidly defined as the complete history of actions and outputs. This restricts the ability to use alternative state representations, such as pruning thinking tokens or summarizing the history to avoid exceeding context lengths. Second, this formulation inherently limits the reward structure to single, trajectory-level signal, preventing the use of finer-grained, per-step rewards, and effectively fixing the discount factor at γ = 1. In Section 4.2 we demonstrate that γ < 1 is crucial for obtaining the optimal fastest search behavior. By contrast, with trajectory-level rewards, the natural speed incentive from γ < 1 is lost, and hence other works, such as SimpleTIR, must tune and enforce strict turn-limit to get this behavior. To address this, our framework, GEM, is designed for maximum flexibility by collecting trajectories as sequence of individual transitions (i.e., state, action, reward, next state) as in the full, unsimplified RL formulation. This design choice enables arbitrary state observation constructions (using observation wrappers), and also preserves compatibility with per-turn rewards and arbitrary discount factors γ 1. The verl-agent framework (Feng et al., 2025b) also adopts this transition-wise approach, which enables its implementation of GiGPO (Feng et al., 2025b), an RL method that utilizes turn-level rewards. While GiGPO collapses to trajectory-level GRPO when observations are unique, it is an example of type of algorithm that is now straightforward to implement with GEMs infrastructure. There are multiple popular frameworks that focus on the agent training side (e.g., Oat (Liu et al., 2024), Verl (Sheng et al., 2024), OpenRLHF (Hu et al., 2024), ROLL (Wang et al., 2025a), and RL2 (Tan et al., 2025)). Currently, many works that build on these, including verl-agent, RAGEN (Wang et al., 2025b), Verlog (Chen et al., 2025b), and many of the works above, add environments by directly modifying the source code. This results in tight coupling between training and environments, and makes it difficult to maintain and reuse the environments for future research. As result, each codebase tends to support only small, ad-hoc collection of environments, making it hard to compare different methods. Even environments with the same name are often inconsistent between codebases. GEM addresses this by dealing with all the environment infrastructure, including providing diverse suite of environments, and corresponding baselines. This makes it easy to keep training and environments decoupled, with the aim of freeing researchers from cumbersome environment development and setup, and thus enabling quicker prototyping and evaluation of new ideas. 18 GEM: Gym for Agentic LLMs"
        },
        {
            "title": "F EXPERIMENTAL SETTINGS",
            "content": "All our experiments are performed on 8 A100 GPUs and finished in about one day. The detailed experimental configurations are shown in Table 3. Table 3: Hyperparameter configurations used in all experiments. Parameter Maximum response length per turn Sampling temperature, train Sampling temperature, evaluation (top P, top k) Value ACTOR 4096 tokens 1.0 0.0 (1.0, -1) LEARNER Optimizer Adam parameters (β1, β2) Weight decay Gradient norm clipping Learning rate scheduler Learning rate Inner proximal update epoch KL loss coefficient KL penalty coefficient Policy clipping parameter Discount factor GAE λ Steps AdamW (0.9, 0.95) 0.0 1.0 Constant 1 106 2 0.0 0.0 0.2 0.9 (game,qa); 1.0 (otherwise) 0.95"
        }
    ],
    "affiliations": [
        "NUS",
        "Northeastern",
        "OpenRLHF",
        "Oxford",
        "RL2",
        "ROLL",
        "SMU",
        "Sea AI Lab",
        "Stanford"
    ]
}