{
    "paper_title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
    "authors": [
        "Kaining Ying",
        "Hengrui Hu",
        "Henghui Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work addresses motion-guided few-shot video object segmentation (FSVOS), which aims to segment dynamic objects in videos based on a few annotated examples with the same motion patterns. Existing FSVOS datasets and methods typically focus on object categories, which are static attributes that ignore the rich temporal dynamics in videos, limiting their application in scenarios requiring motion understanding. To fill this gap, we introduce MOVE, a large-scale dataset specifically designed for motion-guided FSVOS. Based on MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings. Our results reveal that current methods struggle to address motion-guided FSVOS, prompting us to analyze the associated challenges and propose a baseline method, Decoupled Motion Appearance Network (DMA). Experiments demonstrate that our approach achieves superior performance in few shot motion understanding, establishing a solid foundation for future research in this direction."
        },
        {
            "title": "Start",
            "content": "MOVE: Motion-Guided Few-Shot Video Object Segmentation Kaining Ying* Hengrui Hu* Henghui Ding (cid:0) Fudan University, China https://henghuiding.com/MOVE/ 5 2 0 2 9 2 ] . [ 1 1 6 0 2 2 . 7 0 5 2 : r Figure 1. We propose new benchmark for MOtion-guided Few-shot Video object sEgmentation (MOVE). In this example, given two support videos showing distinct motion patterns (S1: Cristiano Ronaldos signature celebration [19], S2: hugging), our benchmark aims to segment target objects in the query video that perform the same motions as in the support videos. MOVE provides platform for advancing few-shot video analysis and perception by enabling the segmentation of diverse objects that exhibit the same motions."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction This work addresses motion-guided few-shot video object segmentation (FSVOS), which aims to segment dynamic objects in videos based on few annotated examples with the same motion patterns. Existing FSVOS datasets and methods typically focus on object categories, which are static attributes that ignore the rich temporal dynamics in videos, limiting their application in scenarios requiring motion understanding. To fill this gap, we introduce MOVE, large-scale dataset specifically designed for motion-guided FSVOS. Based on MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings. Our results reveal that current methods struggle to address motion-guided FSVOS, prompting us to analyze the associated challenges and propose baseline method, Decoupled Motion-Appearance Network (DMA). Experiments demonstrate that our approach achieves superior performance in few-shot motion understanding, establishing solid foundation for future research in this direction. *Equal contribution. (cid:0) Henghui Ding (henghui.ding@gmail.com) is the corresponding author with the Institute of Big Data, College of Computer Science and Artificial Intelligence, Fudan University, Shanghai, China. Few-shot video object segmentation (FSVOS) [3, 37, 40, 42, 52] aims to segment objects of unseen classes in videos using only few annotated examples. FSVOS is relatively underexplored yet promising field. By requiring only minimal supervision, FSVOS significantly reduces the need for extensive labeled datasets while enabling rapid adaptation to new object classes. With these advantages, it shows great potential in autonomous driving, robotics, surveillance, augmented reality, and media production [72]. Previous FSVOS methods [3, 51, 54] are semanticcentric and primarily focus on object categories, associating query videos with support sets based on object class. For example, given support images containing pandas, these methods aim to segment all pandas in the query video regardless of their individual characteristics. This semanticcentric paradigm, similar to the widely-studied few-shot image segmentation (FSS) [57, 59, 60, 69], largely overlooks the crucial temporal dynamics inherent in videos, such as object motions and temporal dependencies, thus limiting the advancement of FSVOS research. We emphasize the fundamental role of motion patterns in videos, which cannot be adequately captured by static image-based segmentation approaches. Consider Cristiano Ronaldos celebration motion shown in Figure 1 (S1), such Decoupled Motion-Appearance (DMA) module for extracting temporally decoupled motion-appearance prototypes, enabling the model to focus more on object motions rather than object appearance. Experiments demonstrate that our proposed DMA helps the model learn motion-centric features, thereby effectively improving model performance. We conduct comprehensive evaluation of 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings in 2 backbones, demonstrating the superiority of the proposed DMA in few-shot motion understanding. In summary, this work makes the following three main contributions: i) We introduce MOVE, motion-guided fewshot video object segmentation dataset that shifts the focus from static object categories to dynamic motion understandii) We propose DMA, method based on decoupled ing. motion and appearance, which demonstrates effective fewshot motion understanding and achieves strong performance on the proposed MOVE. iii) We conduct comprehensive experiments, benchmarking 6 baselines on MOVE, providing solid foundation for future research. 2. Related Work 2.1. Video Object Segmentation Video Object Segmentation (VOS) [12, 14, 20, 21, 46] aims to track and segment the corresponding objects in video sequence, given the object mask in the initial frame. Early deep neural network (DNN)-based methods, such as OSVOS [2] and MoNet [58], fine-tuned network parameters during inference to model inter-frame correlations. Methods like OSMN [62] and LML [1] used the first frame with mask as prompt to generate prototype for pixel-level matching with subsequent frames. Recent trends have shifted toward memory-based methods. STM [43] introduced memory modules to store historical frame information, while STCN [7] enhanced memory usage efficiency. XMem [6] and Cutie [8] further improved memory mechanisms with multiple granularities and object-specific storage. Recently, SAM2 [47] emerged as large video model extending SAM [26], achieving significant performance improvements. While previous VOS methods have achieved milestone progress, their feasibility and scalability are still constrained by the need for large volumes of densely annotated masks. Additionally, many methods struggle with out-of-domain inputs. In contrast to conventional VOS settings, we focus on fewshot settings to significantly reduce annotation costs and improve generalization to wider range of scenarios. 2.2. Few-Shot Video Object Segmentation Few-shot video object segmentation (FSVOS) [3, 37, 40, 42, 52] has emerged as promising solution to address the heavy dependency on pixel-wise annotations in traditional Figure 2. Given motion of interest, our approach enables retrieval and indexing of relevant videos and their corresponding objects from the internet or personal collections. Notably, these motions of interest can be novel actions that are difficult to describe accurately using single-frame images or text alone. dynamic patterns can only be properly represented through video sequences. Unlike previous few-shot video segmentation methods that focus on object categories, e.g., robot or mouse in the support videos of Figure 1, we explore to segment objects based on their motion patterns, e.g., hugging, regardless of their object categories. This enables us to recognize and segment objects performing the same motions, which facilitates novel motion-based object-level retrieval in video, as shown in Figure 2. Recent referring video object segmentation (RVOS) [11, 13, 18, 30, 36, 61, 68] methods explore motion-guided expressions to segment target objects in videos. However, these methods face inherent limitations when dealing with novel or complex motions that are difficult to describe textually. In contrast, such motions can be effectively characterized by providing reference video example, such as the distinctive dance sequence from the movie in Figure 2. While widely recognized actions eventually receive distinctive names, e.g., CR7s celebration and Jokers dance, this does not contradict our approach. video is worth thousand words, particularly for newly emerging actions that have not yet gained widespread recognition. In light of this, we propose MOVE, new large-scale motion-guided FSVOS dataset containing 224 motion categories, 4,300 videos with 261,920 frames in total, and 314,619 high-quality segmentation masks annotating 5,135 objects across 88 object categories. This dataset is designed to capture diverse motion patterns, facilitating the development and evaluation of motion-centric FSVOS methods. By adapting existing methods [3, 11, 54] to MOVE, we find that MOVE presents great challenges in understanding and matching motions between support and query videos. Understanding motions in support videos requires comprehensive analysis of the entire video sequence, rather than relying on static single-frame semantic recognition. Furthermore, effectively extracting motion-related prototypes presents another significant challenge, as existing methods primarily focus on extracting semantic features while overlooking the dynamic information inherent in support videos. To address these challenges, we propose 2 Table 1. Comparison of related few-shot video object segmentation/detection datasets [3, 16, 51] with our proposed dataset MOVE. Dataset FSVOD-500 [16] YouTube-VIS [3, 63] MiniVSPW [51] MOVE (ours)"
        },
        {
            "title": "Venue",
            "content": "[ECCV22] [CVPR21] [IJCV25] [ICCV25] Label Type Annotation Support Type Categories Videos Objects Frames Masks 104,495 97,110 - 314,619 96,609 61,845 541,007 261, Object Object Object Motion Image Image Image Video 4,272 2,238 2,471 4,300 4,663 3,774 - 5,135 Box Mask Mask Mask 500 40 20 VOS. Given an annotated support set, FSVOS aims to segment novel object categories unseen during training in query videos with only few prompt images and masks. Recent FSVOS methods mainly focus on prototype learning [3, 37, 54] or affinity calculation [52]. DANet [3] first defined FSVOS and proposed sampled query agents for attention. TTI [52] and VIPMT [37] focused on temporal consistency through prototypes at different granularities. HPAN [41] and CoCoNet [40] further improved temporal modeling with graph attention and optimal transport respectively. While these studies advance few-shot segmentation for novel object categories, they are inherently categorycentric, limiting their real-world applicability. In contrast, we propose motion-centric approach that prioritizes the motion over the object category. In our new task, MOVE, the support set consists of videos and masks specifying particular motion, and the model segments objects performing the same motion in query videos regardless of their categories, enabling generalization to both novel motions and objects. 2.3. Motion-centric Tasks Motion understanding has evolved as core research direction in video analysis, progressing from early humancentered action recognition [49, 56, 73] to more complex tasks including action detection [50, 64, 71], spatiotemporal localization [5, 28, 44]. Recent LVLMs [22, 38, 67] also pay attention to temporal motion-related tasks. However, these methods require extensive training data and cannot accurately segment target objects in videos. Recently, referring video object segmentation [11, 18, 25, 30] has explored using motion-related expressions to segment target objects, but expressions often fail to accurately describe novel motions. Our proposed DMA can learn novel actions with minimal data to segment target objects in query videos, enabling broader applications across diverse real-world scenarios. 3. MOVE Benchmark 3.1. Task Setting Revisit of FSVOS. Few-Shot Video Object Segmentation (FSVOS) [3, 51, 54] aims to learn segmentation model that can generalize to novel object categories with limited labeled examples. The framework operates on two disjoint data splits: base class training set Dtrain and novel class test set Dtest. The evaluation protocol involves episodes, t,c)}T , k,c)}K t=1, where where each episode comprises support set and query set Q. Specifically, the support set encompasses pairs k, of images and their corresponding masks {(I k=1 extracted from separate videos, with denoting the k-th support image and k,c representing its associated mask for class c. The query set contains video with frames {(I indicates the t-th frame and t,c denotes its ground truth mask. The objective of FSVOS is to leverage the visual and semantic cues from support samples to accurately segment target objects in query video frames. Extension. The proposed MOVE focuses on motion categories rather than object categories. Since static images inherently lack the capacity to represent temporal dynamics, we extend the support set to contain video-mask pairs {(V k=1 sampled from different videos, where each video clip demonstrates motion pattern with corresponding mask sequence k,c of motion-class c. The query set remains as video sequence of frames {(I t=1. This formulation shifts the focus from static appearances to temporal modeling, emphasizing motion as the core feature for video understanding. k,c)}K , , t,c)}T 3.2. Dataset Annotation Vocabulary Collection. Following previous video recognition datasets [9, 24, 53], we build hierarchical vocabulary set with four areas: daily actions, sports, entertainment activities, and special actions. Each category follows three criteria: fine-grained, mutual exclusion (clear semantic boundaries), and novelty (not well covered in existing datasets). This systematic classification lays the foundation for motion-guided few-shot video segmentation tasks. Video Clip Collection. Videos in MOVE are sourced from i) public action recognition datasets [4, 9, 15, two parts: 23, 27, 29, 31, 32, 45, 70] and ii) internet videos under Creative Commons License. During the selection process, we followed these criteria: videos should have clear motion boundaries, diverse scenes, and varied subject categories. Mask Annotation. For videos without preexisting masks, we recruited well-trained annotators to label high-quality masks with the assistance of state-of-the-art VOS segmentation model [48] on an interactive annotation platform. 3.3. Data Statistics and Analysis As shown in Table 1, our MOVE benchmark contains 224 action categories across four domains (daily actions, sports, 3 Figure 3. Overview of our proposed method. Figure 4. Decoupled Motion-Appearance (DMA) Module. entertainment activities, and special actions), with 4,300 video clips, 5,135 moving objects, 261,920 frames, and 314,619 mask annotations. Compared to existing objectcentric datasets, MOVE features video-level support samples and motion-based categories, while maintaining comparable scale in terms of videos and annotations. Each video clip is equipped with high-quality pixel-level mask annotations, capturing diverse scenes, subjects (person, vehicle, animal, etc.), and motion complexities. For more statistics, please refer to the supplementary materials. 4. Methodology 4.1. Overview As shown in Figure 3, the proposed method consists of five main components: 1) shared encoder for extracting multi-scale features from both support and query video frames, 2) proposal generator for obtaining coarse mask proposals of the query video, 3) shared DMA module for extracting decoupled motion-appearance prototypes, 4) prototype attention module for facilitating interaction between support and query prototypes, and 5) mask decoder for generating the final segmentation masks of the query video. In the following sections, we describe each component in detail. For simplicity, we describe our method in the 1-way-1-shot setting, although it can be easily extended to N-way-K-shot scenarios. Given support video clip with Ts frames {I t=1 and corresponding mask sequence }Ts {M t=1, along with query video clip containing Tq }Tq frames {I t=1, our goal is to segment out the target object mask sequence { ˆM t=1 in the query video that exhibits the same motion pattern as the object in the support video. }Tq }Ts 4.2. Encoder and Proposal Generator Encoder. Our encoder combines backbone [17, 39] with feature pyramid network [34] to extract multi-scale features from both the support and query videos as follows: Fl1,t, Fl2,t, Fl3,t, Fl4,t = E(It), = 1, . . . , T, (1) 4 where Fli,t is the i-th layer feature of the t-th frame It. denotes the total number of frames. Fl1,t, Fl2,t, Fl3,t, and Fl4,t correspond to features at resolutions of 1/4, 1/8, 1/16, and 1/32 of input resolution, respectively. Proposal Generator. This module processes multi-scale query features {F l3, l4} to generate coarse mask proposals. It employs three convolutional blocks at different scales (1/32, 1/16, and 1/8 resolution) with residual connections. Features are progressively refined through upsampling and fusion, with final predictions generated by lightweight convolutional head that outputs single-channel proposals. This approach effectively balances multi-scale information utilization and computational efficiency. l2, l1, 4.3. Decoupled Motion-Appearance Module As shown in Figure 4, DMA module extracts decoupled motion-appearance prototypes for both query and support branches. The module takes the 1/4 resolution features Fl1 and corresponding object masks as input, where the support branch utilizes pre-annotated support masks while the query branch leverages mask proposals generated by the proposal generator. Appearance Prototype. DMA first extracts appearance prototypes Pa by applying mask pooling on feature Fl1: Pa = (cid:80) h,w Fl1 (cid:80) h,w RT d, (2) where denotes element-wise multiplication. Motion Prototype. DMA then extracts motion prototypes by calculating temporal differences between adjacent frames features, where the temporal difference at the last time step is padded with zeros: Dl1,t = Fl1,t+1 Fl1,t, = 1, . . . , 1, Pm = Pooling(Conv3D(Dl1)) RT d, (3) where Conv3D denotes 3D convolutional layers for temporal feature enhancement, and Pooling is spatial pooling operation that aggregates the motion features across the spatial dimensions into motion prototypes Pm. To guide the learning of discriminative and complementary motion and appearance prototypes, we introduce two auxiliary classification heads: po = Classifiero(AvgPool(Pa)) RCo, pm = Classifierm(AvgPool(Pm)) RCm, (4) (5) where Co is the number of predefined object categories and Cm is the number of motion categories. These classification tasks explicitly guide Pa to encode object-specific appearance information, while Pm focuses on motion-specific temporal dynamics. This decoupled supervision ensures that the two prototype branches learn complementary features for appearance and motion, respectively. The extracted motion prototypes Pm are further refined using transformer-based architecture. As shown in Figure 4, this architecture processes learnable queries Qdma and special [CLS] token through multiple transformer layers. Each transformer layer consists of cross-attention modules attending to motion prototypes Pm and appearance prototypes Pa, followed by self-attention modules and feedforward networks (FFN). This process produces the final decoupled motion-appearance prototypes Pdma along with the [CLS] token used for prototype matching. 4.4. Prototype Attention and Mask Decoder dma Prototype Attention. To fuse prototype features from both support and query videos, we introduce prototype attention module that consists of multiple transformer layers. Given the decoupled motion-appearance prototypes and dma, this module performs cross-attention where dma serves as queries while dma serves as keys and values, followed by self-attention on the enhanced dma features. Through multiple transformer layers, this iterative process refines the prototypes, facilitating effective information exchange while preserving their distinctive characteristics. The enhanced prototypes, denoted as qs dma, are subsequently used for mask generation in Mask Decoder. Mask Decoder. The Mask Decoder generates segmentation masks by fusing multi-scale features under the guidance of prototypes qs It enhances features at different scales dma. via cross-attention with prototypes, enabling the features to focus on motion-centric information. These enhanced features are then progressively fused in top-down manner. This hierarchical design together with motion prototype guidance ensure that both high-level semantic information and low-level spatial details are effectively leveraged, contributing to the accurate prediction of the final mask. Matching Score. To determine whether the query instance exhibits the same motion as the support instance, we compute matching score based on the [CLS] tokens from both branches. The matching score is calculated as: Smatch = cos([CLS]s, [CLS]q), (6) Table 2. Necessity study of the proposed MOVE benchmark. Methods Type Support Query YTVIS [3] MOVE SCCAN [59] HPAN [3] HPAN* LMPM [11] FSS FSVOS FSVOS RVOS Image Image Video Text Image Video Video Video 62.3 63.0 62.7 62.5 40.6 44.4 46.3 41.8 where cos(, ) represents the cosine similarity between the [CLS] tokens from support and query branches. The matching score Smatch ranges from -1 to 1, with higher values indicating that the query instance is performing the same motion as the support instance, and lower values suggesting different motions. 5. Experiment Evaluation Metrics. Following prior works [3, 12, 54], we use &F to evaluate segmentation quality, with and measuring IoU and contour accuracy, respectively. For robustness evaluation, we include query samples with empty foreground and adopt N-Acc and T-Acc metrics [35] to measure accuracy on empty and non-empty target samples. Dataset Settings. MOVE contains 224 motion categories. For cross-validation, we split these into 4 folds with two strategies: Overlapping Split (OS) and Non-overlapping Split (NS) based on node-level motion distribution. Please refer to supplementary materials for more details. Implementation Details. Our backbone uses ResNet50 [17] pre-trained on ImageNet [10] and VideoSwinTiny [39] pre-trained on Kinetics-400 [24]. Following previous work [3, 54, 65, 66], we employ both crossentropy and IoU losses for mask prediction and proposal generation. Additionally, we use cross-entropy loss for the auxiliary classification head and matching score prediction. We use learning rate of 1e-5 with cosine annealing scheduler for optimization. For our main experiments, we train for 240,000 episodes on 3 folds and test on the remaining fold with 20,000 episodes, using both 2-way-1-shot and 5-way-1-shot settings as our primary configurations. Unless otherwise specified, ablation studies are conducted with 150,000 episodes, training on 2 folds and testing on the remaining 2 folds, using the 2-way-1-shot setting on OS. All the experiments are conducted on 4 NVIDIA RTX A6000 (48GB) GPUs. 5.1. Benchmark Necessity Study To demonstrate the necessity of our MOVE benchmark, we conduct experiments comparing state-of-the-art methods across different areas on both the common-used YouTubeVIS (YTVIS) [63] and our proposed MOVE datasets, as shown in Table 2. The use of YouTube-VIS strictly follows the few-shot setting in [3]. Notably, when evaluated on YTVIS, the image-based FSS method SCCAN [59] achieves 62.3% &F, comparable to HPAN [3] (63.0% 5 Table 3. Main results on MOVE benchmark with overlapping split (OS) setting. Bold and underlined indicate the largest and second largest values under the same backbone, respectively. VSwin-T indicates VideoSwin-T backbone [39]. Methods Venue Type Backbone Mean (2-way-1-shot) &F (2-way-1-shot) Mean (5-way-1-shot) &F (5-way-1-shot) &F T-Acc N-Acc 1 2 3 4 &F T-Acc N-Acc 2 3 4 LMPM [11] CyCTR [69] SCCAN [59] DANet [3] HPAN [54] TTI [51] DMA (Ours) DANet [3] DMA (Ours) [ICCV23] RVOS ResNet50 ResNet50 FSS [ECCV24] ResNet50 FSS [ECCV24] [CVPR21] FSVOS ResNet50 [CSVT24] FSVOS ResNet50 [IJCV25] FSVOS ResNet50 [ICCV25] FSVOS ResNet50 [CVPR21] FSVOS VSwin-T [ICCV25] FSVOS VSwin-T 41.8 34.4 40.6 45.4 44.4 45.2 50.1 49.8 51. 93.1 98.4 93.9 97.1 97.3 97.6 98.6 93.4 98.9 5.3 1.2 5.8 8.2 7.2 9.4 11.5 16.5 21.2 45.2 42.1 40.7 39.1 32.8 34.4 35.7 34.5 47.5 37.1 40.5 37.4 41.4 44.7 47.1 48.2 48.4 45.2 43.4 40.8 45.8 43.9 43.7 47.4 51.2 46.2 54.3 48.6 49.3 47.5 52.5 49.9 51.1 48.6 56.3 50.0 26.3 22.5 28.6 25.4 34.0 35.6 40.2 36.1 41.4 98.3 99.2 97.3 77.2 99.1 70.6 99.5 37.2 99.8 2.6 0.1 2.8 28.0 3.1 26.2 28.7 30.3 31. 27.5 31.7 22.7 23.3 23.1 21.5 20.8 24.7 27.7 32.5 27.2 27.1 27.4 23.5 25.8 25.0 37.6 34.8 34.6 29.1 33.8 35.9 34.8 37.8 40.7 38.9 41.3 39.7 34.8 34.3 38.3 37.1 41.5 39.8 42.7 41.1 Methods Venue Table 4. Main results on MOVE benchmark with non-overlapping split (NS) setting. &F (2-way-1-shot) Mean (5-way-1-shot) Mean (2-way-1-shot) Type Backbone &F (5-way-1-shot) &F T-Acc N-Acc 1 2 4 &F T-Acc N-Acc 1 2 3 LMPM [11] CyCTR [69] SCCAN [59] DANet [3] HPAN [54] TTI [51] DMA (Ours) DANet [3] DMA (Ours) [ICCV23] RVOS ResNet50 ResNet50 FSS [ECCV24] ResNet50 FSS [ECCV24] [CVPR21] FSVOS ResNet50 [CSVT24] FSVOS ResNet50 [IJCV25] FSVOS ResNet50 [ICCV25] FSVOS ResNet50 [CVPR21] FSVOS VSwin-T [ICCV25] FSVOS VSwin-T 38.8 28.2 34.5 44.6 39.1 43.6 46.0 47.4 49.0 94.8 98.0 92.3 97.7 96.3 97.2 98.2 97.2 98.0 4.4 1.0 5.9 2.5 1.4 2.4 7.8 1.2 8.8 45.5 34.5 36.5 38.5 31.2 25.7 33.0 22.7 41.8 32.7 29.2 34.3 48.4 36.9 49.5 43.6 49.1 34.9 40.0 32.3 47.2 33.4 50.0 43.9 47.8 37.9 48.0 50.3 53.2 37.4 48.3 50.9 54.4 37.4 48.5 55. 29.8 23.4 27.8 29.9 30.2 32.7 34.7 30.0 35.4 96.6 95.3 96.0 96.6 99.1 98.3 99.6 74.8 97.4 2.4 3.2 4.3 4.2 1.1 0.9 5.0 2.9 9.3 28.9 26.4 37.6 26.1 23.6 21.2 27.6 21.3 31.7 31.4 25.8 22.2 29.5 24.0 31.0 35.3 35.3 28.5 30.3 26.6 35.0 29.8 35.7 30.2 35.6 31.5 37.0 34.6 34.6 26.5 32.1 26.8 37.9 29.9 36.6 37.2 &F) which is specifically designed for FSVOS. This suggests that YTVIS primarily relies on category-based object association, requiring minimal temporal understanding between support and query samples. However, the performance landscape changes dramatically on MOVE. SCCANs performance drops significantly to 40.6% &F, substantially lower than HPANs 44.4% &F. This stark contrast highlights the critical role of temporal information in MOVE. Furthermore, we enhance HPAN (denoted as HPAN*) by incorporating temporal modeling during prototype extraction through simple self-attention mechanism across frames. This modification yields notable improvement from 44.4% to 46.3% &F, further emphasizing the importance of motion understanding in our benchmark. Furthermore, we benchmark the referring video object segmentation method LMPM [11] on our MOVE dataset by converting support set into referring expressions with the template The one [motion category]. While LMPM achieves competitive performance of 62.5% &F on YTVIS, only 0.2% &F lower than the temporallyenhanced HPAN*, its performance drops significantly to 41.8% &F on MOVE, substantially underperforming compared to HPAN*s 46.3% &F. We attribute this performance gap to the presence of fine-grained, novel, and specialized motion patterns in our MOVE dataset like mutations and moonwalks, which are difficult to describe clearly using text. These findings underscore the unique challenges posed by MOVE and emphasize its necessity in advancing motion-centric few-shot video understanding. 5.2. Main Results As shown in Table 3 and Table 4, we benchmark referring video object segmentation (RVOS) [11], few-shot image segmentation (FSS) [59, 69], and few-shot video object segmentation (FSVOS) methods [3, 51, 54] across 2-way1-shot and 5-way-1-shot test settings on both OS and NS data splits with two different backbones, ResNet50 [17] and VideoSwin-T [39]. Our proposed DMA consistently outperforms all competing methods across all metrics and settings, demonstrating its superior few-shot motion understanding and segmentation capabilities. For the &F metric with ResNet50 backbone, DMA achieves significant improvements over the second-best method, reaching 50.1% (vs. 45.4%) in 2-way-1-shot and 40.2% (vs. 35.6%) in 5-way-1-shot under the OS setting. This substantial performance gap highlights the limitations of existing methods in effectively modeling motion patterns. When using VideoSwin-T backbone, which provides better temporal feature extraction, our method further improves to 51.5% and 41.4% in respective settings, indicating the importance It of temporal modeling in motion-centric segmentation. is worth noting that performance on the NS setting (46.0% &F with ResNet50) is lower than OS (50.8%), reflecting its greater challenge as more realistic scenario where test categories have completely different parent classes from training categories. Regarding robustness metrics, while DMA maintains high target accuracy (T-Acc) of 98.6% 6 Table 5. Ablation study on motion extractor. Table 7. Ablation study on auxiliary classification. ID II III Motion Extractor Mask Pooling Mask Adapter Differencing &F 41.3 43.4 46.8 T-acc 98.0 98.4 99.8 N-acc 6.8 6.6 12.3 Table 6. Ablation study on DMA prototype extractor. ID II III Appear. Motion &F 36.5 43.8 46.8 T-acc 80.1 95.5 99.8 N-acc 30.4 10.7 12. ID II III IV Object Motion &F 43.8 44.2 43.5 46.8 T-acc 97.2 87.6 83.2 99.8 N-acc 5.2 9.6 7.2 12. Table 8. Oracle results on motion category and mask. ID II Motion Mask &F 63.6 74.3 T-acc 100.0 73. N-acc 100.0 100.0 and achieves better non-target accuracy (N-Acc) of 11.5% with ResNet50 compared to baselines, the generally low NAcc scores across all methods suggest common challenge in effectively modeling background information to reduce false positives. This limitation points to promising direction for future research in MOVE. 5.3. Ablation Studies Ablation study on motion extractor. As shown in Table 5, our proposed differencing-based motion extractor achieves better performance compared to baseline approaches such as mask pooling and mask adapter [33], improving &F from 41.3% (mask pooling) and 43.4% (mask adapter) to 46.8%. The explicit motion modeling through frame differencing enables more effective extraction of motion-centric prototypes, leading to enhanced motion pattern recognition and segmentation performance. Ablation study on DMA prototype extractor. As shown in Table 6, we analyze the contribution of appearance and motion prototypes in our DMA prototype extractor. Using only appearance prototypes (I) achieves 36.5% &F, while using only motion prototypes (II) results in 43.8% &F. These results suggest that while appearance features provide static cues for target object recognition, they are insufficient for motion-centric video understanding in MOVE. In contrast, motion features capture temporal dynamics, enhancing the distinction between different motion categories. When combining both prototypes (III), our model achieves the best performance of 46.8% &F, demonstrating the complementary nature of static appearance and dynamic motion information in our DMA mechanism. Ablation study on auxiliary classification. As shown in Table 7, applying auxiliary classification supervision separately to appearance and motion prototypes yields the best performance of 46.8% &F. We attribute this improvement to explicit supervision of object and motion, which effectively enhances the decoupling of motion and appearance features, resulting in overall performance gains. Orcale results. As shown in Table 8, we conduct oracle experiments to analyze the performance upper bound of our model. When provided with perfect motion category labels, t-SNE [55] visualization of prototypes in our model Figure 5. (a) w/o decoupling and (b) w/ decoupling. Different colors and different shapes represent the object categories (e.g., cat) and motion categories (e.g., surfing), respectively. The proposed DMA effectively extracts the motion-centric prototypes and makes those having the same motions closer in feature space. the model achieves 63.6% &F, while using ground truth masks yields higher &F of 74.3%. The results indicate significant room for improvement in both motion understanding and mask prediction capabilities. t-SNE Visualzation of Prototypes. As shown in Figure 5, we visualize the decoupled motion-appearance prototypes Pdma using t-SNE [55]. Without our DMA approach for prototype extraction, prototypes cluster according to object categories, i.e., colors. In contrast, with our proposed DMA approach, prototypes cluster based on motion categories, i.e., shapes, highlighting the effectiveness of our method in capturing motion-centric representations rather than appearance-based features. 5.4. Qualitative Results Figure 6 presents several representative examples comparing our DMA with the baseline methods DANet [3] and HPAN [54]. In case (a), we showcase challenging scenario where objects of different categories perform the same action: cat playing the drums and person playing the flute in the support videos while person playing the drums in the query video. Baseline methods fail by segmenting based on the same object category of person, whereas our method correctly segments the target based on the shared motion pattern, playing the drums. This demonstrates 7 Figure 6. Qualitative comparison of representative cases from MOVE between baseline methods, DANet [3] and HPAN [54], and our proposed DMA. (a) shows different object categories of cat (Support 1) and person (Query) performing the same action, playing drums. (b) presents temporally correlated motions: fingers transitioning from pinching to opening (Support 1) and from opening to pinching (Support 2 & Query videos). (c) is misleading background in the Query video, playing football on the basketball court. the effectiveness of our DMA design in prioritizing motion cues over object class identity. In case (b), we highlight scenario with strong temporal correlations between frames in the support set: fingers transitioning from pinching to opening and from opening to pinching. While baseline methods struggle with fine-grained action discrimination due to insufficient temporal modeling, our proposed method effectively captures subtle temporal dependencies, leading to precise motion recognition and object segmentation. In case (c), our model correctly segments the target object, whereas the baseline methods are misled by the background context of playing football on the basketball court, and fail to capture the specific motion category. These examples demonstrate the superiority of our approach in few-shot motion understanding. Additional failure cases are provided in the supplementary material. 6. Conclusion We introduce MOVE, new benchmark for motion-guided few-shot video object segmentation. Unlike existing FSVOS datasets that segment objects based on object the proposed MOVE emphasizes temporal categories, dynamics by segmenting objects according to motion categories that correlate with support and query videos. Experimental results show that MOVE poses significant challenges to current state-of-the-art methods, motivating us to propose DMA that decouples motion and appearance prototypes for more robust and effective motion prototype extraction. MOVE provides foundation for advancing research in motion-centric few-shot video understanding and temporal modeling for segmentation tasks. Future Directions. The MOVE benchmark opens up several promising research directions that need further investigation. We highlight some key areas for future exploration: i) decomposing complex motions into meta-motions for more general and efficient motion prototype learning, ii) modeling relational motions that involve interactions between multiple objects, iii) improving fine-grained motion discrimination through extracting more robust motion prototypes, iv) handling long-term temporal motions spanning multiple seconds through efficient temporal modeling, and v) learning discriminative background prototypes to suppress false positives in complex scenes better. 8 Acknowledgement. This project was supported by the National Natural Science Foundation of China (NSFC) under Grant No. 62472104."
        },
        {
            "title": "References",
            "content": "[1] Goutam Bhat, Felix Jaremo Lawin, Martin Danelljan, Andreas Robinson, Michael Felsberg, Luc Van Gool, and Radu Timofte. Learning what to learn for video object segmentation. In Eur. Conf. Comput. Vis., 2020. 2 [2] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, and Luc Van Gool. Oneshot video object segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2 [3] Haoxin Chen, Hanjie Wu, Nanxuan Zhao, Sucheng Ren, and Shengfeng He. Delving Deep Into Many-to-Many Attention In IEEE Conf. for Few-Shot Video Object Segmentation. Comput. Vis. Pattern Recog., 2021. 1, 2, 3, 5, 6, 7, 8 [4] Jun Chen, Ming Hu, Darren Coker, Michael Berumen, Blair Costelloe, Sara Beery, Anna Rohrbach, and Mohamed Elhoseiny. Mammalnet: large-scale video benchmark for mammal recognition and behavior understanding. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 3 [5] Lei Chen, Zhan Tong, Yibing Song, Gangshan Wu, and Limin Wang. Efficient video action detection with token dropout and context refinement. In Int. Conf. Comput. Vis., 2023. 3 [6] Ho Kei Cheng and Alexander Schwing. Xmem: Longterm video object segmentation with an atkinson-shiffrin memory model. In Eur. Conf. Comput. Vis., 2022. [7] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethinking space-time networks with improved memory coverage for efficient video object segmentation. Adv. Neural Inform. Process. Syst., 2021. 2 [8] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, and Alexander Schwing. Putting the object back into In IEEE Conf. Comput. Vis. video object segmentation. Pattern Recog., 2024. 2 [9] Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, and Chi-Keung Tang. Haa500: Human-centric atomic In Int. Conf. Comput. action dataset with curated videos. Vis., 2021. 3 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., 2009. 5 [11] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. MeViS: large-scale benchmark for video segmentation with motion expressions. In Int. Conf. Comput. Vis., 2023. 2, 3, 5, 6 [12] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. MOSE: new dataset for video object segmentation in complex scenes. In Int. Conf. Comput. Vis., 2023. 2, [13] Henghui Ding, Song Tang, Shuting He, Chang Liu, Zuxuan Wu, and Yu-Gang Jiang. Multimodal referring segmentation: survey. arXiv, 2025. 2 more challenging dataset for video object segmentation in complex scenes. arXiv, 2025. 2 [15] Hongdong Li Dongxu Li. Wlasl (world level american sign language) video, 2022. 3 [16] Fan, Qi, Tang, Chi-Keung, Tai, and Yu-Wing. Few-Shot Video Object Detection. In Eur. Conf. Comput. Vis., 2022. 3 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conf. Comput. Vis. Pattern Recog., 2016. 4, 5, [18] Shuting He and Henghui Ding. Decoupling static and hierarchical motion perception for referring video segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2, 3 [19] Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, et al. Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills. arXiv preprint arXiv:2502.01143, 2025. 1 [20] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: benchmark for long-term video object segmentation. In Int. Conf. Comput. Vis., 2023. 2 [21] Lingyi Hong, Zhongying Liu, Wenchao Chen, Chenzhi Tan, Yuang Feng, Xinyu Zhou, Pinxue Guo, Jinglun Li, Zhaoyu Chen, Shuyong Gao, et al. Lvos: benchmark for largescale long-term video object segmentation. arXiv preprint arXiv:2404.19326, 2024. 2 [22] Wenyi Hong, Yean Cheng, Zhuoyi Yang, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, and Jie Tang. Motionbench: Benchmarking and improving fine-grained video motion understanding for vision language models. In IEEE Conf. Comput. Vis. Pattern Recog., 2025. 3 [23] HE Jian and WANG Weidong. Visual recognition of chinese traffic police gestures based on spatial context and temporal features. Acta Electronica Sinica, 2020. 3 [24] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The arXiv preprint kinetics human action video dataset. arXiv:1705.06950, 2017. 3, 5 [25] Seongchan Kim, Woojeong Jin, Sangbeom Lim, Heeji Yoon, Hyunwook Choi, and Seungryong Kim. Referring video object segmentation via language-aligned track selection. arXiv preprint arXiv:2412.01136, 2024. [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Int. Conf. Comput. Vis., 2023. 2 [27] Yu Kong, Yunde Jia, and Yun Fu. Learning human interaction by interactive phrases. In Eur. Conf. Comput. Vis., 2012. 3 [28] Okan Kopuklu, Xiangyu Wei, and Gerhard Rigoll. You only watch once: unified cnn architecture for realarXiv preprint time spatiotemporal action localization. arXiv:1911.06644, 2019. 3 [14] Henghui Ding, Kaining Ying, Chang Liu, Shuting He, YuGang Jiang, Philip HS Torr, and Song Bai. MOSEv2: [29] Hildegard Kuehne, Hueihan Jhuang, Estıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: large video 9 database for human motion recognition. Comput. Vis., 2011. 3 In Int. Conf. [30] Ge Li, Hanqing Sun, Aiping Yang, Jiale Cao, and Yanwei Pang. Motion expressions guided video segmentation via IEEE Trans. Emerg. effective motion information mining. Topics Comput. Intell., 2025. 2, 3 [31] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Int. Conf. Comput. Vis., 2021. 3 [32] Yixuan Li, Lei Chen, Runyu He, Zhenzhi Wang, Gangshan Wu, and Limin Wang. Multisports: multi-person video dataset of spatio-temporally localized sports actions. In Int. Conf. Comput. Vis., 2021. [33] Yongkang Li, Tianheng Cheng, Wenyu Liu, and Xinggang Wang. Mask-adapter: The devil is in the masks for openvocabulary segmentation. arXiv preprint arXiv:2412.04533, 2024. 7 [34] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 4 [35] Chang Liu, Henghui Ding, and Xudong Jiang. GRES: In IEEE Generalized referring expression segmentation. Conf. Comput. Vis. Pattern Recog., 2023. 5 [36] Chang Liu, Xudong Jiang, and Henghui Ding. Primitivenet: decomposing the global constraints for referring segmentation. Visual Intelligence, 2024. 2 [37] Nian Liu, Kepan Nan, Wangbo Zhao, Yuanwei Liu, Xiwen Yao, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Junwei Han, and Fahad Shahbaz Khan. Multigrained Temporal Prototype Learning for Few-shot Video Object Segmentation. In Int. Conf. Comput. Vis., 2023. 1, 2, [38] Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi Shao, et al. Convbench: multi-turn conversation evaluation benchmark with hierarchical ablation capability for large In Adv. Neural Inform. Process. vision-language models. Syst. D&B, 2024. 3 [39] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 4, 5, 6 [40] Naisong Luo, Yuan Wang, Rui Sun, Guoxin Xiong, Tianzhu Zhang, and Feng Wu. Exploring the Better Correlation for Few-Shot Video Object Segmentation. IEEE Trans. Circuits Syst. Video Technol., 2024. 1, 2, 3 [41] Naisong Luo, Yuan Wang, Rui Sun, Guoxin Xiong, Tianzhu Zhang, and Feng Wu. Holistic prototype attention network for few-shot video object segmentation. IEEE Trans. Circuits Syst. Video Technol., 2024. 3 [42] Binjie Mao, Xiyan Liu, Linsu Shi, Jiazhong Yu, Fei Li, and Shiming Xiang. Few-shot video object segmentation with prototype evolution. Neural Computing and Applications, 2024. 1, 2 [43] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In Int. Conf. Comput. Vis., 2019. [44] Junting Pan, Siyu Chen, Mike Zheng Shou, Yu Liu, Jing Shao, and Hongsheng Li. Actor-context-actor relation network for spatio-temporal action localization. In Int. Conf. Comput. Vis., 2021. 3 [45] Chirag Parikh, Rohit Saluja, CV Jawahar, and Ravi Kiran Idd-x: multi-view dataset for egoSarvadevabhatla. relative important object localization and explanation in In IEEE Int. Conf. Robot. dense and unstructured traffic. Autom., 2024. 3 [46] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video In IEEE Conf. Comput. Vis. Pattern object segmentation. Recog., 2016. 2 [47] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 2 [48] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 3 [49] Paul Scovanner, Saad Ali, and Mubarak Shah. 3dimensional sift descriptor and its application to action recognition. In ACM Int. Conf. Multimedia, 2007. [50] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed videos via multi-stage cnns. In IEEE Conf. Comput. Vis. Pattern Recog., 2016. 3 [51] Mennatullah Siam. Temporal transductive inference for fewshot video object segmentation. Int. J. Comput. Vis., 2025. 1, 3, 6 [52] Mennatullah Siam, Konstantinos Derpanis, and Richard Wildes. Temporal transductive inference for few-shot video arXiv preprint arXiv:2203.14308, object segmentation. 2022. 1, 2, 3 [53] Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 3 [54] Yin Tang, Tao Chen, Xiruo Jiang, Yazhou Yao, Guo-Sen Xie, and Heng-Tao Shen. Holistic prototype attention network for few-shot video object segmentation. IEEE Trans. Circuit Syst. Video Technol., 2023. 1, 2, 3, 5, 6, 7, 8 [55] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. J. Mach. Learn. Res., 2008. 7 [56] Heng Wang and Cordelia Schmid. Action recognition with In Int. Conf. Comput. Vis., pages improved trajectories. 35513558, 2013. 3 [57] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot image semantic In Int. Conf. segmentation with prototype alignment. Comput. Vis., 2019. 1 [58] Huaxin Xiao, Jiashi Feng, Guosheng Lin, Yu Liu, and Maojun Zhang. Monet: Deep motion exploitation for video 10 [73] Wangjiang Zhu, Jie Hu, Gang Sun, Xudong Cao, and Yu Qiao. key volume mining deep framework for action In IEEE Conf. Comput. Vis. Pattern Recog., recognition. 2016. 3 object segmentation. Recog., 2018. 2 In IEEE Conf. Comput. Vis. Pattern [59] Qianxiong Xu, Wenting Zhao, Guosheng Lin, and Cheng Long. Self-calibrated cross attention network for few-shot segmentation. In Int. Conf. Comput. Vis., 2023. 1, 5, 6 [60] Qianxiong Xu, Guosheng Lin, Chen Change Loy, Cheng Long, Ziyue Li, and Rui Zhao. Eliminating feature ambiguity for few-shot segmentation. In Eur. Conf. Comput. Vis., 2024. 1 [61] Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Hao Dong, Zhongjiang He, and Peng Gao. Referred by multi-modality: unified temporal transformer for video object segmentation. In AAAI, 2024. [62] Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, Efficient video object In IEEE Conf. and Aggelos Katsaggelos. segmentation via network modulation. Comput. Vis. Pattern Recog., 2018. 2 [63] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In Int. Conf. Comput. Vis., 2019. 3, 5 [64] Le Yang, Houwen Peng, Dingwen Zhang, Jianlong Fu, and Junwei Han. Revisiting anchor mechanisms for temporal action localization. IEEE Trans. Image Process., 2020. 3 [65] Kaining Ying, Zhenhua Wang, Cong Bai, and Pengfei Zhou. Isda: Position-aware instance segmentation with deformable attention. In IEEE Int. Conf. Acoust. Speech Signal Process., 2022. 5 [66] Kaining Ying, Qing Zhong, Weian Mao, Zhenhua Wang, Hao Chen, Lin Yuanbo Wu, Yifan Liu, Chengxiang Fan, Yunzhi Zhuge, and Chunhua Shen. CTVIS: Consistent training for online video instance segmentation. In Int. Conf. Comput. Vis., 2023. [67] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. MMT-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI. In Int. Conf. Mach. Learn., 2024. 3 [68] Kaining Ying, Henghui Ding, Guangquan Jie, and Yu-Gang Jiang. Towards omnimodal expressions and reasoning in referring audio-visual segmentation. In Int. Conf. Comput. Vis., 2025. 2 [69] Gengwei Zhang, Guoliang Kang, Yi Yang, and Yunchao Few-shot segmentation via cycle-consistent transWei. former. Adv. Neural Inform. Process. Syst., 2021. 1, 6 [70] Yifan Zhang, Congqi Cao, Jian Cheng, and Hanqing Lu. Egogesture: new dataset and benchmark for egocentric IEEE Trans. Multimedia, 2018. hand gesture recognition. 3 [71] Chen Zhao, Ali Thabet, and Bernard Ghanem. Video selfstitching graph network for temporal action localization. In Int. Conf. Comput. Vis., 2021. 3 [72] Tianfei Zhou, Fatih Porikli, David Crandall, Luc Van Gool, and Wenguan Wang. survey on deep learning technique IEEE Trans. Pattern Anal. Mach. for video segmentation. Intell., 2022."
        }
    ],
    "affiliations": [
        "Fudan University, China"
    ]
}