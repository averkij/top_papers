{
    "paper_title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation",
    "authors": [
        "Chenkai Xu",
        "Xu Wang",
        "Zhenyi Liao",
        "Yishun Li",
        "Tianqi Hou",
        "Zhijie Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as a notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify a unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), a qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce a trajectory segmentation strategy and a curriculum learning procedure to improve the training convergence. Empirically, in text-to-image generation, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo exhibits a 1.5x speedup without significantly sacrificing performance. The code is available at https://github.com/zhijie-group/Show-o-Turbo."
        },
        {
            "title": "Start",
            "content": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation Chenkai Xu1*, Xu Wang1*, Zhenyi Liao1, Yishun Li3, Tianqi Hou2, Zhijie Deng1 3Tongji University 1Shanghai Jiao Tong University 2Huawei {132435xck,wangxu60,zhijied}@sjtu.edu.cn 5 2 0 2 8 ] . [ 1 5 1 4 5 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce trajectory segmentation strategy and curriculum learning procedure to improve the training convergence. Empirically, in text-toimage generation, Show-o Turbo displays GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Showo Turbo exhibits 1.5x speedup without significantly sacrificing performance. The code is available at https: //github.com/zhijie-group/Show-o-Turbo. 1. Introduction Multimodal large models like LLaVA [35] and Stable Diffusion [13] have shown promise across variety of multimodal understanding (e.g., image/video to text) and generation (e.g., text to image/video) tasks. Recently, the attention has been shifted from dedicated multimodal models to unified, versatile one, with Chameleon [55], Transfusion [72], Emu3 [57] and Show-o [61] as popular examples. Among them, Show-o is distinguished by its outstand- *Equal contribution. Corresponding author. ing capabilities given the small model size (of only 1.3B parameters) as well as its open-source nature. In short, Show-o integrates the discrete diffusion modeling of image tokens (yielded by an image tokenizer like MAGVIT-v2 [68]) and discrete autoregressive (AR) modeling of text tokens into one single transformer. The discrete diffusion can boil down to masked autoregressive formula in practice. As result, the inference of Show-o involves progressively denoising image tokens and autoregressively decoding the text tokens. This, undesirably, causes tens to hundreds of sampling steps for both image and text generation and hence high serving cost. Although prior studies on accelerating diffusion models [46, 52] or large language models (LLMs) [25, 28, 43] can be separately applied to remediate such issues, the question remains whether more unified approach exists to enhance the efficiency of Show-o. This work introduces Show-o Turbo to reply to the question. We first advocate exploring parallel decoding algorithms of text tokens [23, 47, 51] to establish unified view for the generation of both images and text. Basically, such algorithms invoke the language model to refine (n > 1) text tokens in parallel and iterate until the fixed point. The sampling trajectory exhibits gradual noise removal pattern (see Figure 2). This way, we can unify the generation of both text and images from denoising view, and the learning of Show-o Turbo reduces to the shortening of the multimodal denoising trajectories. Drawing inspiration from diffusion acceleration literature [37, 46, 48, 52], we resort to the promising consistency distillation (CD) [52] technique to realize this. Concretely, we train Show-o Turbo to consistently map an arbitrary point on the sampling trajectory of Show-o to the same endpoint. Such an objective aids in pushing Show-o Turbo to generate meaningful content as fast as possible [39, 49, 52]. Conceptually, our approach forms an empirical generalization of CD, which was originally defined on ODE trajectories, to general (deterministic) discrete sampling trajectories, and stands as cross-modal extension of consistency LLMs (CLLMs) [23]. We can simply initialize Show-o Turbo as the pre-trained Show-o and perform consistency distillation based on distri1 Figure 1. 512 512 images generated by Show-o Turbo given various text prompts. From top to bottom, the images are generated by Show-o Turbo in 8, 4, and 2 sampling steps without reliance on classifier-free guidance [20]. butional disparity (e.g., KL divergence) due to the discrete nature of the modeling. Following [18, 62, 71], we further introduce trajectory segmentation and curriculum learning strategies, where multiple training stages with decreasing numbers of segments of the entire sampling trajectory are employed and CD is performed within each segment. This helps improve the model convergence. We perform extensive studies to evaluate the effectiveness of Show-o Turbo. For text-to-image generation, we evaluate on various metrics including GenEval [16], Human Preference Score (HPS) [55], ImageReward (IR) [28], and CLIP Score (CS) [19]. We show that the 4-step Show-o Turbo without classifier-free guidance (CFG) [20] can attain better GenEval, HPS, IR, and CS scores than the 8step Show-o with CFG. For image-to-text generation, we evaluate Show-o Turbo on the image description benchmarks Flickr30K [41, 67] and NoCaps [1], observing 1.5x inference speedup without significant performance drop. Show-o Turbo also shows reasonable performance on multimodal understanding (MMU) tasks that rely on one-token responses, including POPE [27], MME [14], and MMMU [69]. We also conduct ablation experiments for deep understanding of Show-o Turbo. 2. Related Work Multimodal Large Models. There has been much effort in exploring multimodal large models for image generation [42, 45, 54, 64] and understanding [3, 35, 65, 73]. For image generation tasks, text-conditioned Diffusion-based models [7, 8, 30, 42, 45, 50] gradually remove Gaussian noise in latent space [45] to generate images aligning with prompts. For multimodal understanding tasks, LLaVA family [26, 31, 3436, 74] employs vision encoder to encode the image, integrating it with large language model (LLM) architecture to facilitate image-to-text understanding. Recent unified multimodal models [9, 12, 58, 59, 70] have emerged that aim to handle both image and text tasks simultaneously. For example, Chameleon [55] and Emu3 [57] autoregressively predict the next token on both tasks, while Transfusion [72] combines the autoregressive and continuous diffusion generation methods to handle different tasks. Similar to the Transfusion, Show-o [61] applies the autoregressive text generation but uses the discrete diffusion methods in image generation process. Acceleration of Diffusion Model. Diffusion models (DMs), such as Stable Diffusion [42, 45], inherently suffer from slow generation speeds due to iterative sampling. In this context, numerous acceleration techniques [5, 37, 44, 46, 48, 66] have emerged in recent years, with the most influential being the Consistency Model (CM) family [39, 52]. This family introduces the concept of consistency, mapping any two points on trajectory to the same endpoint and supporting fast one-step generation. Subsequent works built on them introduce multi-step consistency [18, 56, 62, 71]. Their idea of segmentation trajectories reduces the learning difficulty and enhances the effect of consistency distillation. However, these methods focus on continuous diffusion models, which makes them inconsistent with the discrete diffusion process. More importantly, they do not integrate acceleration for both text and image generation. Acceleration of LLMs. The acceleration of LLMs [4, 10, 11, 24, 53] has been popular research area, including de2 Figure 2. Illustration of the sampling trajectories of text and image tokens in Show-o. As shown, they both display denoising pattern. In particular, the trajectory of text generation is yielded by Jacobi Decoding [47]. The black line denotes the unified abstraction of the multimodal trajectory, and the red lines illustrate the objective of our Show-o Turboto map an arbitrary point on the sampling trajectory to the endpoint. Note that we omit the trajectory segmentation strategy here for brevity. creasing the size of KVcache [21, 33, 38] and the parallel decoding methods [15, 28, 29]. For example, speculative decoding [25] involves training draft model to predict tokens while the LLM verifies them. The target LLM may generate multiple tokens in single inference process, but training such draft model proves to be challenging. Given the success of consistency distillation on diffusion models, CLLM [23] extends this idea to the acceleration of LLMs. Based on Jacobi decoding [51], CLLM collects Jacobi trajectories during the LLM generation process and then uses consistency distillation principles similar to CM to achieve acceleration. It proves that the idea of consistency distillation can effectively accelerate language models, and serves as an inspiration for our work on applying similar strategies in multimodal scenarios. 3. Preliminary: Show-o This section provides brief overview of Show-o, unified generative model for both images and text. Image Tokenization. Show-o opts to model the distribution of discrete image tokens via discrete diffusion [2]. To this end, it exploits MAGVIT-v2 [68] to convert highdimensional continuous images into discrete token sequences and employs unified large vocabulary to represent both text and image tokens. This enables using single transformer to jointly characterize these two modalities. Training Objectives. Show-o adopts an autoregressive (AR) modeling for text tokens following the principle of Next Token Prediction (NTP). For image modeling, the discrete diffusion paradigm can be equivalently simplified to Mask Token Prediction (MTP) objective [6]. Formally, let := {u1, u2, , um} and := {v1, v2, , vn} denote sequence of image tokens and sequence of text tokens, Show-o maximizes these objectives for training: LNTP := (cid:88) log pθ(viv1, , vi1, u), (1) LMTP := (cid:88) log pθ(uju, u2, , u, um, v), (2) where pθ is the prediction distribution represented by Showo, refers to the special mask token [MASK], and and traverse all text tokens and mask image tokens respectively. Inference. The text generation of Show-o is based on the naive AR strategy. For image generation, Show-o adheres to the methodology outlined in MaskGIT [6]. This involves predefining progressively decreased mask ratio schedule over steps, initializing sequence of full mask tokens u0, and iteratively reducing the number of mask tokens according to the schedule. Specifically, letting uk denote the sequence containing partial mask tokens at k-th iteration, the model yields the prediction distribution for each mask token in uk and use sample from the distribution to replace each mask token. After that, the model follows the mask schedule to replace the low-confidence predictions back as mask tokens, yielding uk+1. The sampling trajectory {u0, u1, . . . , uK} are illustrated in Figure 2. Besides, it is shown that Classifier-Free Guidance (CFG) [20] can be incorporated into Show-o to improve the sample quality [61]. However, CFG introduces an additional evaluation of the model, thereby increasing the sampling cost. 4. Method This section introduces Show-o Turbo to reduce the sampling steps of Show-o for inference acceleration. 3 4.1. View Text Generation as Denoising We first establish unified perspective for the generation of both images and text in Show-o so that concise and general acceleration strategy can apply. We notice the gap between the generation of images and text is mainly that the (mask) image tokens are decoded in parallel but text tokens emerge in an autoregressive manner. This motivates us to resort to fixed-point iteration algorithms that decode multiple text tokens in parallel [23, 47, 51] to bridge the gap. Jacobi Decoding [47] is representative fixed-point iteration algorithm for parallel text decoding. Given that, for text generation, Show-o equals regular language model, we can directly apply Jacobi decoding to Show-o. Starting from sequence of randomly initialized text tokens, denoted as v0 := {v0 n}, Jacobi decoding iteratively refines the token sequence until stable output (i.e. fixed point). At k-th iteration, the refinement corresponds to simultaneously solving the following problems: 1, . . . , v0 vk+1 1 = arg max pθ(vvk 1 , u), vk+1 2 = arg max ... vk+1 = arg max pθ(vvk 1 , vk 2 , u), (3) pθ(vvk 1 , . . . , vk n1, u). They can be solved simultaneously with only one forward pass of Show-o using casual attention mask, which takes roughly identical time as decoding one new token. Note that the greedy sampling strategy is used here. Abusing to denote the number of iterations to reach the fixed point vK, it is easy to see + 1 because there is at least one token being correctly predicted in each iteration. Refer to Figure 2 for visualization of the sampling trajectory {v0, . . . , vK}, which displays gradual noise removal pattern. Although empirical studies in Table 2 show that applying Jacobi decoding to Show-o cannot witness considerable inference speedup (because Show-o is originally trained to predict the next token instead of decoding multiple tokens concurrently), Jacobi decoding offers us unified denoising view of the generation of images and text. 4.2. Show-o Turbo Given the above discussion, the problem of accelerating Show-o amounts to shortening the multimodal denoising trajectories. Drawing inspiration from the diffusion acceleration community [37, 46, 48, 52], we propose to adapt the qualified consistency distillation (CD) strategy [52] to Show-o. In particular, we aim at learning Show-o Turbo model, denoted as pϕ, to consistently map any point on the trajectory of pθ to the same endpoint. Such an objective can drive Show-o Turbo to generate meaningful content as fast 1By correctness, we mean the generated tokens equal to those generated by regular AR decoding. as possible [39, 49, 52]. In practice, we initialize ϕ with the parameter θ of the teacher Show-o. We elaborate on the algorithmic details in the following. Consistency Loss. We use the Jacobi iteration algorithm and the image sampling algorithm detailed in Section 3 to collect the trajectories of the original Show-o pθ on both text-to-image and image-to-text tasks. Then, the consistency loss on image trajectories takes the form of: (cid:16) (cid:17) Lu = EkU (0,K)d pϕ (uK , v), pϕ(uk, v) , (4) where ϕ denotes the frozen version of ϕ and indicates divergence measure. aggregates the disparity between categorical prediction distributions (e.g., measured by KL divergence) over the mask image tokens as in Equation 2. The consistency loss on text trajectories, denoted as Lv , can be similarly defined. Recall that uK refers to the endpoint of the trajectory, i.e., the final generation, so Lu corresponds to global consistency loss, which is empirically proven superior over the local one (operating on the adjacent points of the trajectory) for the acceleration of text generation [23]. Conceptually, our objective forms an empirical generalization of the original CD defined on ODE trajectories and cross-modal extension of [23]. Besides, when collecting the trajectories, we disable the randomness in the sampling process of both modalities by applying greedy strategy, which makes the discrete sampling trajectories deterministic and is likely to remediate training instability. Despite being trained with deterministic trajectories, Show-o Turbo is empirically evidenced to be compatible with the random sampling method for inference (see Table 4). We also clarify that the trajectories corresponding to the image tokens are collected with the involvement of CFG to guarantee the quality of uK. Regularization. Training with only the consistency loss can drive pϕ to trivial convergence (e.g., always yielding the same outputs for arbitrary inputs). To avoid this, we introduce regularizations for both modalities. On the text side, we demand pϕ to fit the endpoint text tokens vK with an objective similar to LN , which ensures Show-o Turbo excels in image-to-text modeling. On the image side, we record the prediction distributions of the recovered image tokens at each sampling step during the trajectory collection procedure. The concentration of these distributions contains rich information about the generation process of the teacher Show-o, such as the easy-to-difficult hierarchy, so we advocate using them as another guidance for pϕ(uk, v)."
        },
        {
            "title": "We use Lv",
            "content": "REG and Lu REG to represent these two regularizations respectively. We also include an AR loss LAR on pure text to maintain the language modeling capacity following the original Show-o. That said, the total loss is = Lu + αLv + βLu REG + γLv REG + δLAR, (5) where α, β, γ, and δ are the trade-off coefficnets. 4 Steps Model CFG GenEval HPS IR CS Time (sec) AVG TO CT CL SO CA 8 4 2 Show-o Show-o Show-o Turbo Show-o Turbo Show-o Show-o Show-o Turbo Show-o Turbo Show-o Show-o Show-o Turbo Show-o Turbo Show-o Show-o Show-o Turbo Show-o Turbo 10 5 0 0 10 5 0 0 10 5 0 0 10 5 0 0 0.674 0.823 0.647 0.288 0.838 0.984 0.463 0.672 0.778 0.666 0.293 0.835 0.991 0.468 0.649 0.793 0.644 0.253 0.809 0.956 0.440 0.646 0.818 0.597 0.218 0.827 0.984 0. 0.578 0.631 0.519 0.235 0.811 0.991 0.280 0.580 0.647 0.584 0.225 0.766 0.984 0.275 0.642 0.788 0.631 0.253 0.787 0.981 0.413 0.638 0.813 0.541 0.250 0.814 0.991 0.420 0.277 0.270 0.266 0.273 0.257 0.255 0.264 0.273 0.992 0.318 0.885 0.318 0.768 0.315 0.925 0.318 0.672 0.313 0.632 0.313 0.800 0.315 0.963 0.318 0.353 0.237 0.325 0.095 0.540 0.863 0.060 0.396 0.298 0.334 0.158 0.572 0.925 0.088 0.596 0.692 0.553 0.218 0.758 0.978 0.375 0.625 0.770 0.553 0.245 0.806 0.978 0. 0.197 -0.560 0.283 0.207 -0.300 0.294 0.633 0.312 0.249 0.934 0.318 0.269 0.181 0.025 0.131 0.008 0.327 0.588 0.008 0.251 0.051 0.188 0.038 0.442 0.778 0.010 0.459 0.407 0.422 0.148 0.668 0.925 0.185 0.557 0.614 0.478 0.180 0.793 0.972 0.305 0.140 -1.756 0.246 0.152 -1.456 0.260 0.201 -0.259 0.295 0.680 0.312 0.247 1.39 1.39 0.77 0.77 0.76 0.76 0.46 0.46 0.44 0.44 0.30 0. 0.29 0.29 0.22 0.22 Table 1. Comparison of 512 512 T2I performance on GenEval, HPS, IR, and CS. AVG: average, TO: Two Object, CT: Counting, P: Position, CL: colors, SO: Single Object, CLA: Color Attr. Trajectory Segmentation and Curriculum Learning. We empirically ascertain that imposing long-range consistency may introduce unnecessary learning challenges, potentially impeding model convergence and ultimately limiting acceleration capabilities. Consequently, we suggest splitting the entire training process into multiple stages with decreasing numbers of segments of the sampling trajectory. We enforce consistency solely between the points within segment and the endpoint of that segment. The efficacy of such strategy is also supported by recent advances in consistency distillation [18, 62, 71]. As the training proceeds, the trajectory of the student Show-o Turbo may deviate considerably from that of the teacher Show-o. Thus, persisting in utilizing Show-os trajectory for distillation purposes could constrain the ultimate acceleration effect. To mitigate this, we suggest using the acquired model from the past stage as teacher to construct new trajectories. Doing so encourages the final Show-o Turbo to learn consistency mapping over long distances. In the inference phase, Sampling Strategy. the original Show-o generates image tokens with polynomial sampling [61]. However, we find that for the learned Showo Turbo model with few sampling steps, there is significantly higher uncertainty in the prediction distribution of the mask tokens. We empirically identify that incorporating the top-k sampling strategy, which is widely used in language models, can alleviate this issue, substantially improving the sampling quality in 2-4 steps (see Table 4). 5. Experiments In this section, we evaluate on multiple text-to-image (T2I) generation and multimodal understanding (MMU) tasks to inspect the efficacy of Show-o Turbo. 5.1. Implementation Details Datasets. We leverage three types of data for the training of Show-o Turbo: the captions in the train split of COCO 2017 [32] and the LLaVA instruction tuning dataset [36] are used to generate the image and text trajectories respectively; the RefinedWeb text dataset [40] is used to maintain the language modeling ability. Training Details. We separate the training process into two stages. For 256 resolution, in the first stage, we get image trajectories from the original Show-o with CFG scale of 10 and = 16. We split each trajectory into 4 segments to train the student model, denoted as Show-o Turbo. In the second stage, we initialize the teacher and student model using Show-o Turbo. We sample image trajectories with CFG scale of 1.5, = 8, and the number of segments as 2. The text trajectories are collected similarly. We employ Jacobi decoding to iteratively produce 16 tokens in each round to finally form lengthy text, which proves to yield good acceleration performance while preserving the generative modeling capabilities [23]. In terms of loss coefficients, we set α = 10 according to the relative values of the losses, set β = 20 and γ = 100 according to the ablation study in Table 3, and set δ = 2 following [61]. For 512 resolution,"
        },
        {
            "title": "Method",
            "content": "Show-o Show-o Turbo Show-o Turbo Decoding Speed (tokens/s) POPE MME MMMU Flickr30K NoCaps"
        },
        {
            "title": "Jacobi\nJacobi",
            "content": "40.3 36.9 49.93 61.1 83.2 83.2 81.8 78.4 1042.5 1042.5 1003.6 865. 24.6 24.6 25.4 26.3 26.6 26.6 20.3 20.4 38.9 38.9 29.6 30. Table 2. Comparison of 512 512 MMU performance on multiple benchmarks. Note that Flickr30K and NoCaps evaluate the ability of image description, and POPE, MME, and MMMU measure question-answering ability. Figure 3. The text sampling trajectory of Show-o Turbo in MMU cases. Show-o Turbo realizes acceleration by predicting multiple successive tokens in one iteration and correctly guessing the later tokens. we set to 32, CFG scale to 15 and number of segments to 8 in the first stage, and set to 16, CFG scale to 1.75 and number of segments to 4 in the second stage. Besides, we set α = 10, β = 40, γ = 200 and δ = 8. We follow the original Show-o regarding the mask schedule. We use an AdamW optimizer and 8 RTX 4090 GPUs to train each stage for 18 hours, with constant learning rate of 105. 5.2. Main Results Benchmarks. For T2I generation, we evaluate the generated images with Human Preference Score v2 (HPS) [60], ImageReward (IR) [63], and CLIP Score (CS) [19] metrics, based on test prompts in Human Preference Dataset v2 (HPD) [60]. Additionally, following Show-o, we evaluate also Show-o Turbo on GenEval [17]. For MMU, we evaluate Show-o Turbo on the description benchmarks Flickr30K [41, 67] and NoCaps [1] measured by the bleu4 score, and calculate the accuracy on question answering benchmarks POPE [27], MMEMME [14], and MMMU [69]. Baselines. For T2I generation, we compare Show-o Turbo without CFG to Show-o with CFG across various sampling steps to fully demonstrate the effectiveness of our method. We consider CFG scale of 5 and 10 for Show-o following [61]. For MMU, we compare Show-o Turbo with the original Show-o in terms of both inference speed and accuracy, where the speed is measured on single RTX 4090 GPU. Quantitative Results. Table 1 displays the results for T2I generation. We observe that in 2-8 step sampling, Show-o Turbo comprehensively outperforms Show-o, even without 6 Show-o (CFG=10) Show-o Turbo"
        },
        {
            "title": "2 Steps",
            "content": "A cybernetic owl perched on neon-lit branch, its mechanical feathers reflecting holographic patterns... modern electric guitar with flame maple top, its wood grain catching studio lights... small succulent plant in ceramic pot, its leaves forming perfect geometric pattern... traditional wooden chess piece on marble board, its polished surface reflecting soft light... detailed macro shot of dragonfly perched on thin blade of grass, its wings iridescent in the sunlight... single, colorful autumn leaf floating on the surface of calm pond... Figure 4. Comparison between Show-o and Show-o Turbo on 512 resolution in T2I generation. The former crashes in two-step sampling, while the latter maintains good performance. using CFG, particularly at 2 and 4 steps. In 16-step sampling, Show-o Turbo is comparable to Show-o on GenEval, HPS, IR, and CS. Moreover, the 4-step sampling of Showo Turbo without CFG outperforms the 8-step sampling of Show-o, and the results of the 2-step Show-o Turbo fall between the 4-step and 8-step Show-o with CFG, highlighting the effectiveness of our method in acceleration. Besides, we can observe that Show-o Turbo outperforms Show-o Turbo, demonstrating the efficacy of curriculum learning. Additionally, we demonstrate the reliance of Show-o on CFG, and find that CFG can further enhance Show-o Turbo, which is shown in Appendix B. Table 2 shows the performance of Show-o Turbo in MMU tasks. We evaluate the text tokens generation speed on NoCaps, witnessing 1.5x speedup on average. Besides, we notice that Show-o Turbo achieves comparable description performance to Show-o on Flickr30K and NoCaps. The slight performance drop also implies trade-off between 7 Settings #IT POPE MME IR CS"
        },
        {
            "title": "Model",
            "content": "Steps Top-k HPS IR CS"
        },
        {
            "title": "4 Segments\n2 Segments\n1 Segment",
            "content": "10.57 12.48 11.71 72.6 69.8 74.1 803.4 595.8 675.3 0.586 0.307 0.500 0.306 0.270 0.304 Show-o Turbo Full-parameter Tuning vs. LoRA Full-parameter LoRA 10.57 13.14 72.6 78.1 803.4 881.2 0.586 0.307 0.472 0.304 Show-o 4 4 2 2 4 4 2 2 - 200 - 10 - 200 - 10 0.245 0.252 0.216 0.240 0.228 0.230 0.169 0. 0.621 0.706 0.027 0.529 0.306 0.309 0.291 0.306 0.301 0.219 0.286 0.302 -1.257 0.254 -1.263 0."
        },
        {
            "title": "Regularization",
            "content": "β = 0, γ = 0 2.85 β = 10, γ = 50 12.71 β = 20, γ = 100 10.57 0.0 74.8 72.6 4.91 798.4 803.4 -2.278 0.184 0.483 0.307 0.586 0.307 Table 4. Comparison on sampling strategy on 256 resolution. Top-k sampling is beneficial to Show-o Turbo compared to regular multinomial samples, but the benefits for the original Show-o are minor. Table 3. Ablation studies regarding various aspects on 256 resolution. #IT represents the number of iterations required by Jacobi decoding to decode 16 tokens. Refer to the text for more details. the performance and acceleration effect on these tasks. Performing distillation with more advanced MMU corpora can be possible remedy to this. On the other hand, we find that Show-o Turbo maintains strong performance on questionanswering benchmarks POPE, MME, and MMMU which rely on one-token responses. Qualitative Results. Figure 4 provides visual comparison between Show-o with CFG scale of 10 and Show-o Turbo without CFG across various sampling steps on 512 resolution. It can be observed that the images generated by Showo with 2 steps are collapsed, while our model addresses this issue. More results of our Show-o Turbo are provided in Figure 1. These studies prove that Show-o Turbo has the ability to perform effective sampling with fewer steps. Figure 3 visualizes the text sampling trajectory of Showo Turbo for several MMU cases. As shown, Show-o Turbo can complete the prediction of 16 tokens in fewer than 10 iterations, due to the ability to predict multiple successive tokens in one iteration and correctly guess the later tokens. We also showcase the performance of Show-o Turbo in image inpainting and extrapolation in Appendix A. Show-o Turbo can effectively complete both tasks in just four steps without requiring additional fine-tuning. 5.3. Ablation Studies To analyze the influence of each part in our method, we conduct comprehensive ablation study on 256 resolution in this subsection. Unless otherwise specified, we report the results of the model after the first training stage (i.e., Showo Turbo) and the T2I generation is done with 4 sampling steps. Number of Segments. As shown in Table 3, models trained in two segments and without trajectory segmentation (i.e., using one segment) can exhibit suboptimal performance and degraded acceleration effect. This result reflects the effectiveness of our trajectory segmentation strategy for improving convergence speed and model performance. Full-parameter Training. We study the influence of the training module on Show-o Turbo. Table 3 shows the performance of full-parameter training and LoRA trianing [22] on several benchmarks. Although the LoRA method maintains good performance on the MMU task, its acceleration effect on MMU and T2I tasks is inferior to that of fullparameter training. Therefore, we ultimately opt for full parameter training to balance between accuracy and speed. Regularization. As shown in Table 3, training without regularization constraints (i.e., β = 0, γ = 0) tends to make the model collapse rapidly. Besides, smaller regularization weights can lead to inferior performance, highlighting the importance of regularization in constraining the distribution of Show-o Turbo in training. Top-k Sampling. Table 4 shows the results with different sampling strategies. We observe that top-k significantly improves the performance of Show-o Turbo on both 2-step and 4-step sampling. In contrast, the benefit of top-k to Show-o is minor. This is probably because there is higher uncertainty in the output distribution of Show-o Turbo. 6. Conclusion This work introduces Show-o Turbo, unified approach for accelerated multimodal understanding and generation. To achieve this, with unified denoising perspective for the image and text generation process, we extend the consistency distillation to the multimodal denoising trajectories. Additionally, the trajectory segmentation strategy and curriculum learning are introduced to improve the model convergence and performance. Extensive experiments demonstrate significant improvement in generation speed for both tasks, with minimal decrease in performance."
        },
        {
            "title": "References",
            "content": "[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 89488957, 2019. 2, 6 [2] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. 3 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2 [4] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. 2 [5] Clement Chadebec, Onur Tasar, Eyal Benaroche, and Benjamin Aubin. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. arXiv preprint arXiv:2406.02347, 2024. 2 [6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 3 [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [8] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-δ: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024. [9] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. 2 [10] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 2 [11] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 2 [12] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. 9 In Forty-first International Conference on Machine Learning, 2024. 1 [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 2, 6 [15] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024. 3 [16] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, 2019. 2 [17] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment, 2023. [18] Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. 2, 5 [19] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. 2, 6 [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 2, 3 [21] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. 3 [22] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 8 [23] Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models. arXiv preprint arXiv:2403.00835, 2024. 1, 3, 4, [24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. 2 [25] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274 19286. PMLR, 2023. 1, 3 [26] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [27] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 2, 6 [28] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-2: Faster inference of language models with dynamic draft trees. arXiv preprint arXiv:2406.16858, 2024. 1, 2, [29] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. 3 [30] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 2 [31] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning, et al. Moe-llava: Mixture of experts for large visionlanguage models. arXiv preprint arXiv:2401.15947, 2024. 2 [32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 5 [33] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024. 3 [34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2 [35] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 1, 2 [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, 5 [37] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusionbased text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. 1, 2, 4 [38] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. 3 [39] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 1, 2, [40] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 5 nik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. IJCV, 123 (1):7493, 2017. 2, 6 [42] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations. 2 [43] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5: 606624, 2023. 1 [44] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. 2 [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [46] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 1, 2, 4 [47] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodol`a. Accelerating transformer inference for translation via parallel decoding. arXiv preprint arXiv:2305.10427, 2023. 1, 3, 4 [48] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2025. 1, 2, 4 [49] Yang Song and Prafulla Dhariwal. niques for training consistency models. arXiv:2310.14189, 2023. 1, Improved techarXiv preprint [50] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [51] Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In International Conference on Machine Learning, pages 97919800. PMLR, 2021. 1, 3, 4 [52] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 1, 2, 4 [53] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model serving with consumergrade gpu. arXiv preprint arXiv:2312.12456, 2023. 2 [54] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [41] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazeb- [55] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, 10 similarity metrics for semantic inference over event descriptions. TACL, 2:6778, 2014. 2, 6 [68] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 1, 3 [69] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 2, [70] Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong Wang. Monoformer: One transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280, 2024. 2 [71] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024. 2, 5 [72] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 1, 2 [73] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 [74] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Llava-phi: Efficient multi-modal arXiv preprint language model. Mou, and Jian Tang. assistant with small arXiv:2401.02330, 2024. Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 2 [56] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu arXiv preprint Liu, et al. arXiv:2405.18407, 2024. 2 Phased consistency model. [57] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, 2 [58] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 2 [59] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [60] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 6 [61] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1, 2, 3, 5, 6 [62] Qingsong Xie, Zhenyi Liao, Zhijie Deng, Shixiang Tang, Haonan Lu, et al. Mlcm: Multistep consistency distillation of latent diffusion model. arXiv preprint arXiv:2406.05768, 2024. 2, 5 [63] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai ImagereLi, Ming Ding, Jie Tang, and Yuxiao Dong. ward: Learning and evaluating human preferences for textto-image generation, 2023. 6 [64] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [65] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: large language model with Revolutionizing multi-modal In Proceedings of the IEEE/CVF modality collaboration. Conference on Computer Vision and Pattern Recognition, pages 1304013051, 2024. [66] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 2 [67] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New 11 Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Inpainting and Extrapolation Figure 5 shows that Show-o Turbo can efficiently fill in missing parts of an image with high quality in just 2 to 4 steps, based on the given prompt. Meanwhile, Figure 6 demonstrates that Show-o Turbo can smoothly complete image extrapolation in just 4 steps. Figure 5. Visualization of image inpainting by Show-o and Show-o Turbo on 256 resolution. From left to right are the 2, 4, and 8 steps sampling."
        },
        {
            "title": "Model",
            "content": "Steps CFG HPS IR CS Show-o Show-o Turbo 16 4 2 16 8 4 0 10 0 10 0 10 0 10 0 1 0 1 0 1 0 1 0.174 0.254 0.181 0.249 0.178 0.228 0.159 0.169 0.258 0.258 0.255 0.255 0.252 0.252 0.240 0.235 -1.097 0.739 -0.916 0.665 -0.877 0.219 -1.661 -1.257 0.752 0.816 0.738 0.782 0.706 0.731 0.529 0. 0.272 0.310 0.276 0.308 0.276 0.301 0.234 0.254 0.310 0.310 0.309 0.310 0.309 0.309 0.306 0.302 Table 5. Results with different CFG on 256 resolution. proper CFG can enhance the performance of Show-o and Show-o Turbo. Figure 6. Visualization of image extrapolation by Show-o and Show-o Turbo on 256 resolution. From top to bottom are the 4, 8, and 16 steps sampling. B. Settings of CFG As shown in Table 5, the performance of Show-o drops significantly without CFG, resulting in images that lack semantic information. Additionally, the appropriate use of CFG further enhances the sampling performance of Show-o Turbo, particularly for sampling steps of 4 or more. 1 Figure 7. Visualization of regularization label for image trajectory distillation. denotes the regularization label at time step k. pθ is the prediction distribution represented by teacher model, and lθ denotes the output logits of teacher model. Steps Model CFG GenEval HPS IR CS Time (sec) AVG TO CT CL SO CA 16 8 2 Show-o Show-o Show-o Turbo Show-o Turbo Show-o Show-o Show-o Turbo Show-o Turbo Show-o Show-o Show-o Turbo Show-o Turbo Show-o Show-o Show-o Turbo Show-o Turbo 10 5 0 10 5 0 0 10 5 0 0 10 5 0 0 0.591 0.692 0.478 0.165 0.859 0.978 0.378 0.571 0.631 0.469 0.155 0.846 0.994 0.333 0.543 0.593 0.447 0.130 0.814 0.953 0.323 0.562 0.689 0.366 0.140 0.814 0.991 0.373 0.540 0.578 0.428 0.145 0.838 0.969 0.285 0.530 0.558 0.441 0.133 0.825 0.972 0.255 0.518 0.518 0.400 0.123 0.809 0.972 0.285 0.552 0.669 0.353 0.128 0.817 0.963 0.385 0.425 0.333 0.334 0.100 0.700 0.950 0.135 0.429 0.351 0.369 0.078 0.707 0.947 0.120 0.504 0.513 0.375 0.130 0.787 0.962 0.257 0.523 0.664 0.303 0.103 0.801 0.959 0. 0.254 0.253 0.251 0.258 0.249 0.247 0.250 0.255 0.228 0.228 0.245 0.252 0.739 0.310 0.642 0.309 0.586 0.307 0.752 0.310 0.665 0.308 0.602 0.308 0.597 0.307 0.738 0.309 0.219 0.301 0.225 0.302 0.586 0.307 0.706 0. 0.206 0.046 0.140 0.033 0.330 0.678 0.010 0.229 0.068 0.122 0.023 0.378 0.763 0.020 0.439 0.358 0.313 0.075 0.755 0.941 0.193 0.494 0.530 0.334 0.093 0.787 0.959 0.260 0.169 -1.257 0.254 0.182 -0.917 0.263 0.174 0.302 0.224 0.529 0.306 0.240 0.44 0.44 0.27 0.27 0.24 0.24 0.15 0.15 0.14 0.14 0.09 0.09 0.08 0.08 0.06 0. Table 6. Comparison of 256 256 T2I performance on GenEval, HPS, IR, and CS. Show-o Turbo refers to the model after the first stage of training. AVG: average, TO: Two Object, CT: Counting, P: Position, CL: colors, SO: Single Object, CLA: Color Attr."
        },
        {
            "title": "Method",
            "content": "Show-o Show-o Turbo Show-o Turbo Decoding Speed (tokens/s) Flickr30K NoCaps TextCaps POPE MME MMMU"
        },
        {
            "title": "Jacobi\nJacobi",
            "content": "41.8 38.2 61.3 64.5 15.0 15.0 12.1 14.6 25.8 25.8 19.2 21. 12.3 12.3 9.7 10.6 73.8 73.8 72.6 73.2 948.4 948.4 803.4 872. 25.1 25.1 27.0 25.8 Table 7. Comparison of 256 256 MMU performance on multiple benchmarks. Note that Flickr30K, NoCaps, and TextCaps evaluate the ability of image description, and POPE, MME, and MMMU measure question-answering ability. 2 C. Regularization Loss Details The regularization loss for text trajectories is straightforward to compute because we only need pϕ to fit the endpoint text tokens vK, which is similar to the objective LN . However, for the regularization loss of image trajectories, we need to construct regularized logits labels by recording the predicted distribution of teacher model at each sampling step during the trajectory collection process. As shown in Fig. 7, the regularization label at time step is denoted as k, with the initialization 0 = 0. During the iteration process of the trajectory uk, we simultaneously iterate k. For the known regions in uk, we directly assign the corresponding positions in k1 to k. For the masked regions in uk, we record the logits output of the teacher model and assign them to the corresponding positions. Through this iterative process, we can obtain the regularization labels at the segmentation points, which are used to compute Lu REG. D. Results of 256 resolution Table 6 and Table 7 show the performance of Show-o Turbo on T2I and MMU tasks at 256-resolution respectively. It can be observed that Show-o Turbo can also achieve the effect of 8 steps of the original model in 4step sampling without CFG in 256-resolution image generation, and also achieves about 1.5 times acceleration in 256-resolution image understanding. E. Additional Image Results Figure 8 and Figure 9 show the image generation results for 512 and 256 resolutions respectively. Show-o Turbo can generate high-quality images with rich details with only 2 to 4 sampling steps and without CFG. Figure 8. 512 512 images generated by Show-o Turbo. From left to right, the images are generated by Show-o Turbo in 2, 4, 8 and 16 sampling steps without CFG. 4 Figure 9. 256 256 images generated by Show-o Turbo. From left to right, the images are generated by Show-o Turbo in 2, 4, 8 and 16 sampling steps without CFG."
        }
    ],
    "affiliations": [
        "Huawei",
        "Shanghai Jiao Tong University",
        "Tongji University"
    ]
}