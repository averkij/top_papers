{
    "paper_title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?",
    "authors": [
        "Subhajit Maity",
        "Killian Hitsman",
        "Xin Li",
        "Aritra Dutta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effectiveness in diverse machine learning (ML) tasks, such as vision, remains questionable. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep network architectures, including advanced architectures such as vision Transformers (ViTs). In this paper, we are the first to design a general learnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate on any choice of basis. However, the computing and memory costs of training them motivated us to propose a more modular version, and we designed particular learnable attention, called Fourier-KArAt. Fourier-KArAt and its variants either outperform their ViT counterparts or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures' performance and generalization capacity by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior, and contrast them with vanilla ViTs. The goal of this paper is not to produce parameter- and compute-efficient attention, but to encourage the community to explore KANs in conjunction with more advanced architectures that require a careful understanding of learnable activations. Our open-source code and implementation details are available on: https://subhajitmaity.me/KArAt"
        },
        {
            "title": "Start",
            "content": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Subhajit Maity 1 Killian Hitsman 2 Xin Li 2 Aritra Dutta 2 1 5 2 0 2 3 1 ] . [ 1 2 3 6 0 1 . 3 0 5 2 : r Abstract Kolmogorov-Arnold networks (KANs) are remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effectiveness in diverse machine learning (ML) tasks, such as vision, remains questionable. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep network architectures, including advanced architectures such as vision Transformers (ViTs). In this paper, we are the first to design general learnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate on any choice of basis. However, the computing and memory costs of training them motivated us to propose more modular version, and we designed particular learnable attention, called Fourier-KArAt. FourierKArAt and its variants either outperform their ViT counterparts or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures performance and generalization capacity by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior, and contrast them with vanilla ViTs. The goal of this paper is not to produce parameterand compute-efficient attention, but to encourage the community to explore KANs in conjunction with more advanced architectures that require careful understanding of learnable activations. Our open-source code and implementation details are available on: subhajitmaity.me/KArAt 1Department of Computer Science, University of Central Florida, Orlando, FL, USA 2Department of Mathematics, University of Central Florida, Orlando, FL, USA. Correspondence to: Subhajit Maity <Subhajit@ucf.edu>, Killian Hitsman <killian.hitsman@ucf.edu>, Xin Li <xin.li@ucf.edu>, Aritra Dutta <aritra.dutta@ucf.edu>. Figure 1. Model parameters vs. Top-1 Test accuracy in ImegeNet-1K training of vanilla ViTs (Dosovitskiy et al., 2020), Vision KAN (DeiT+KAN) by (Chen et al., 2024), ViT+KAN and Kolmogorov-Arnold Transformer (KAT) by (Yang & Wang, 2025). 1. Introduction Artificial general intelligence has become rapidly growing research direction, and Kolmogorov-Arnold Network (KAN) (Liu et al., 2024) marks remarkable innovation to that. KANs with learnable activation functions can potentially capture more complex relationships and facilitate meaningful interaction between the model and human intuition. After training KAN on specific problem, researchers can extract the learned univariate functions the model uses to approximate complex multivariable functions. By studying these learned functions, researchers can gain insights into the underlying relationships from the data and refine the model. KANs exhibit state-of-the-art performance in finding symbolic function representations (Yu et al., 2024), continual learning of one-dimensional functions (Liu et al., 2025). KANs were integrated with neural network architectures, the primary of them being conventional MLPs or convolution neural networks (CNNs). E.g., (Ferdaus et al., 2024; Bodner et al., 2024; Drokin, 2024; Abueidda et al., 2025; Wang et al., 2025) combine KAN with CNNs, (Li et al., 2024) combine KAN with U-Net, etc. Interestingly, (Yu et al., 2024) claimed to make the first fairer comparison between KANs and MLPs on multiple ML tasks on small-scale 1 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? datasets. By setting the same parameter counts in both networks, (Yu et al., 2024) concluded that KANs with B-Spline basis significantly underperform compared to their MLP counterparts in all ML tasks except symbolic formula representation; similar claim reinstated in (Azam & Akhtar, 2024) for vision. But KANs exploration of more advanced architectures such as Transformers, remains limited. (Chen et al., 2024) replace the MLP layers inside the encoder blocks of data-efficient image Transformers (DeiTs) (Touvron et al., 2021) with KANs and proposed DeiT+KAN, (Yang & Wang, 2025) proposed two variants; ViT+KAN that replaces the MLP layers inside ViTs encoder blocks, and Kolmogorov-Arnold Transformer (KAT), albeit similar to ViT+KAN but with refined group-KAN strategy. While KANs were claimed to be not superior to simple MLPs in vision tasks (Yu et al., 2024; Azam & Akhtar, 2024), we were curious to find out how ViTs can see images through the lens of KANs. In that effort, Figure 1 demonstrates the performance of vanilla ViTs (ViT tiny, ViT small (Wu et al., 2022; Steiner et al., 2022), and ViT base (Dosovitskiy et al., 2020)) and their KAN counterparts on ImageNet1K (Deng et al., 2009) training. While different variants of DeiT+KAN (Chen et al., 2024) and ViT+KAN (Yang & Wang, 2025) show 5%18% drop in the Top-1 accuracy compared to vanilla ViTs and DeiTs in ImageNet-1K, the KATs (Yang & Wang, 2025) show approximately 1%2.5% gain in Top-1 accuracy compared to vanilla ViTs and DeiTs by keeping about the same number of parameters. These results show KANs require further investigation in more advanced architectures such as Transformers. Nevertheless, first, we want to understand why there is discrepancy in these two seemingly equivalent implementations. In search for an answer, we realized that while DeiT+KAN and ViT+KAN replace the MLP layer with KAN in the Transformers encoder block, KAT implements sophisticated group-KAN strategy that reuses learnable function among group of units in the same layer and chooses different bases for different encoder blocks. This strategy helps improve the accuracy by keeping almost similar parameter counts. Therefore, simply replacing MLPs with KANs might not guarantee better generalizability, but properly designed KAN could. This perspective on the KANs performance in conjunction with Transformers opens many research questions. Importantly, we also note that the community remains put from designing learnable multihead attention module, the heart of the Transformers. Therefore, in the rise of second-generation Transformers, such as Googles TITAN (Behrouz et al., 2024) and SAKANA AIs Transformer2 (Sun et al., 2025) that mimic the human brain, we want to ask: Is it worth deploying learnable multi-head self-attention to the (vision) Transformers? Finally, we note that there is line of research that proposes efficient attention mechanisms in terms of sparse attention (Rahimian et al., 2024; Yun et al., 2020; Shi et al., 2021; Kovaleva et al., 2019; Zhang et al., 2021; Zaheer et al., 2020; Guo et al., 2019) or linear/kernelized attention (Han et al., 2025; Nguyen et al., 2023; 2021; 2022b; Lu et al., 2021; Nguyen et al., 2022a), to remedy computational and memory complexities of attention calculation. In contrast, this paper tries to understand how learnable multi-head self-attention (MHSA) modules can perform over regular self-attention used in ViTs, defining technology in vision in the last six years. We list our contributions as follows: Designing Learnable Attention Module for ViTs. Since language is significantly more complicated, less structured, and has higher information entropy than an image, we limit this pilot study to the ViTs (Dosovitskiy et al., 2020) for image classification. In 3, we propose general learnable Kolmogorov-Arnold multi-head self-attention mechanism or KArAt for vanilla ViTs that can operate on any basis. However, the computing and memory costs of training them motivated us to propose more modular version, and we designed particular MHSA mechanism using Fourier basis, called Fourier KArAt and its variants; see 4 for details. Benchmarking, Evaluation, and Analysis. In 5, we benchmark different variants of Fourier KArAt on CIFAR10, CIFAR-100, (Krizhevsky et al., 2009) and ImageNet-1K (Deng et al., 2009) datasets and evaluate their performance against vanilla ViTsViT tiny, ViT small (Wu et al., 2022; Steiner et al., 2022), and ViT base (Dosovitskiy et al., 2020). We dissect these architectures performance and generalization capacity by analyzing the loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior and contrast them with vanilla ViTs. 2. Background Multi-layer Perceptrons (MLPs) consist of layers, where the lth-layer is defined for input x[l1] Rdl1 as a[l] = Φ[l](Wlx[l1] + bl), Wl Rdldl1, bl Rdl. By convention, a[0] = x[0] denotes the input data. In MLP, the nonlinear activation functions, Φ() are fixed (Bengio et al., 2017). For supervised tasks, given training dataset with elements of the form (input, ground-truth) pairs, {(xi, i=1, the loss, L(X, W) = dY (a[L], ) (metric induced by the space RdL) is calculated in the forward pass. The layerwise weights, {(Wl, bl)}l[L] are learned by minimizing L(X, W). )}N Kolmogorov-Arnold Network (KAN) (Liu et al., 2025) is neural network involving learnable activations parametrized by chosen set of basis functions defined on set of grid points or knots. The idea for this network stems from the Kolmogorov-Arnold Representation Theorem. 2 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Figure 2. (a) The standard softmax self-attention for ith head in the jth encoder block. (b) The Kolmogorov-Arnold Attention (KArAt) replaces the softmax with learnable function Φi,j. (c) The ideal KArAt is with an operator matrix, Φi,j with learnable units that act on each row of Ai,j. (d) While Φi,j RN is impossible to implement due to computational constraints, our architecture uses an operator (cid:98)Φi,j with learnable units , followed by linear projector with learnable weights RrN . Theorem 1 (Kolmogorov-Arnold Representation Theorem). (Kolmogorov, 1956) For any multivariate continuous function, : [0, 1]n R, there exists finite composition of continuous single-variable functions, ϕq,p : [0, 1] R, Φq : such that (x) = (x1, x2, . . . , xn) = 2n+1 (cid:88) (cid:18) (cid:88) Φq (cid:19) ϕq,p(xp) . q=1 p=1 Theorem 1 describes an exact finite expression using 2 layers, but, this can be extended to multi-layer KAN; it is analogous to the Universal Approximation Theorem (Hornik et al., 1989). For input x[0], an L-layer KAN is given as KAN(x[0]) = ΦL Φl Φ1(x[0]). In each KAN layer, the activation function Φ is learnable, and Φ[l] = [Φ[l] ij ] operates on x[l1] Rdl1 to produce x[l] Rdl , such that: = Φ[l] x[l] i: (x[l1]) = dl1 (cid:88) j=1 ij (x[l1] Φ[l] ). Originally, (Liu et al., 2025) chose the B-spline basis functions. That is, the activation functions were defined to be ϕ(x) = w(b(x) + spline(x)), where b(x) = silu(x) = 1+ex and spline(x) = (cid:80) ciBi(x), with Bi() being one of the k-th degree B-spline basis functions parametrized by the grid points on uniform grid, [I, I]. In this case, the representation for each function is given as: ij (x[l1] Φ[l] ) = w[l] ij (silu(x[l1] G+k1 (cid:88) )+ m= ijmBijm(x[l1] c[l] )). inspired by the Transformer proposed in (Vaswani et al., 2017). For simplicity, we do not mention the details of the layer normalization and other technicalities. Our central focus is the MHSA architecture. Let RHW be an input image of resolution . ViT uses an patch and generates = HW input p2 tokens. Each token is vectorized along channels to produce matrix XT RN p2C, where each row, XTi,: , represents flattened patch across channels. learnable embedding matrix, Rp2Cd projects each row XT i,: to an embedding vector in Rd such that After appending the ground-truth class, xG and adding the positional encoding matrix, PE R(N +1)d of compatible size, the final input matrix, XI = [xG; XE] + PE. The matrix XI is layer normalized and further projected on the row spaces of three learnable weight matrices, WQ, WV , Wk Rdd to generate the query, key, and value matrices, Q, K, and , respectively, via = XI WQ, = XI WK, and = XI WV . These matrices are further divided into pard titions (also, called heads), Qi, i, RN , such that each partition generates self-attention matrix for the ith head, Ai = QiKi . Finally, we project onto the column space of σ(Ai) as σ(Ai)V i, where σ is the Softmax operator applied row-wise. The outputs of different heads are further concatenated into large maσ(Ah)V h(cid:3) O, posttrix (cid:2)σ(A1)V 1 σ(A2)V 2 multiplied by the output weight matrix Rdd. This process is sequentially performed in encoder blocks; see Figure 2(a). d/h ij ])}L ijm], [w[l] The weights {([c[l] l=1 are learned layerwise, similar to MLPs, by minimizing by final loss for supervised learning task. However, the bases for the activations could be Fourier (Mehrabian et al., 2024; Xu et al., 2024), wavelet basis functions (Bozorgasl & Chen, 2024), fractals (Yang & Wang, 2025) etc. Multi-head self-attention (MHSA) in ViTs (Dosovitskiy et al., 2020). The encoder-only vanilla ViT architecture is 3. How Can We Design Learnable Attention? In this Section, we design learnable multi-head selfattention (MHSA) module for ViTs that operate inside its encoder blocks. Last year witnessed surge in embedding KANs in different DNN architectures (Genet & Inzirillo, 2024; Ferdaus et al., 2024; Li et al., 2024). To our knowledge, the work closely Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? related to ours is KAT, where (Yang & Wang, 2025) replace the MLP layers in the ViTs with KAN layers; also, see (Chen et al., 2024). We later show that deploying learnable attention is complicated and much-involved process. Let Ai,j RN be the attention matrix for ith head in the jth encoder block. For [N ], the softmax activation function, σ : RN (0, 1)N , operates on the kth row vector, Ai,j k,: R1N of Ai,j to produce component-wise output . Instead of using the softmax function, σ(Ai,j k,l) = (cid:80) n=1 i,j k,l i,j k,n we can use learnable activation function, σ on the row vectors of each attention head Ai,j. With any choice of the basis functions (e.g., B-Spline, Fourier, Fractals, Wavelets, etc.), the activated attention row vector, σ(Ai,j k,:) for [N ] can be written as (cid:105) (cid:104) (Ai,j k,:) σ = ϕ11() ϕ21() ... ϕ12() ϕ22() ... ϕN 1() ϕN 2() k,:)(cid:105)(cid:17) Φi,j (cid:104) (Ai,j , (cid:16) = ϕ1N () ϕ2N () ... ... ... . . . ... ϕN () Ai,j k,1 Ai,j k,2 ... Ai,j k,N (1) pq ] RN and each matrix entry, ϕpq, where Φi,j = [ϕi,j is referred to as learnable unit, represented using set of basis functions {ψi,j }m=1. The coefficients associated with the basis functions are called the learnable parameters. Note that, in our convention, Ai,j k,: R1N is row vector. We apply the transpose operation to each row vector, to adopt the convention used in KAN layers. As result, Φi,j (cid:104) RN 1, and we transpose it to ob- (Ai,j k,:)(cid:105) tain learnable attention row vectors σ 2(b)-(c). (cid:105) (cid:104) (Ai,j k,:) ; see Figures Projection on to the probability simplex. The softmax function acts on each row of the attention matrix to create probability distribution; the learnable attention does not ensure that. To ensure that each row vector of the learned attention matrix lies on probability simplex, we project them onto the ℓ1-unit ball. That is, for each attention matrix, Ai,j RN , and for each [N ], we want to have (cid:104) (Ai,j 0 for [N ]. In the k,l) σ following, we cast it as sparse approximation problem, whose variants have been well-studied in the past decade and arise frequently in signal processing (Bryan & Leise, 2013; Donoho, 2006; Dutta, 2016) and matrix approximation (Boas et al., 2017; Stewart, 1993). 1 = 1 and σ (Ai,j k,:) (cid:105) (cid:104) (cid:105) sparse approximation problem. Let Rm be given vector. If we want to approximate with vector, Rm, with positive components and x1 = 1, we can write the (a) Attention matrix Ai,j before softmax activation. (b) Attention matrix σ(Ai,j) after softmax activation. Figure 3. Spectral analysis of the attention matrices before and after softmax shows that they are low-rank. For this experiment, we use all 3 heads in the last encoder block of ViT tiny on 5 randomly sampled images from the CIFAR-10 validation set. We plot all 15 singular vectors (each of 197 dimensions) where the singular values are arranged in non-increasing order. constrained optimization problem as: = arg min xRm y 1 2 subject to xi 0 for [m], and x1 = 1. (2) For completeness, we describe the solution to problem (2) in A. Algorithm 1 provides pseudocode for ℓ1 projection, (cid:105) (cid:104) (Ai,j and by setting = σ and = for each [N ] k,:) in Algorithm 1, we obtain the projected attention vector. 4. Our Architecture k,: of Ai,j, and produces the output σ(Ai,j Section 3 outlines generic learnable attention module. However, we experience computation bottleneck in implementing it. As in (1), for learnable activation function, σ : RN RN , an operator matrix, Φi,j RN acts on each row Ai,j k,:). These operations are extremely compute-heavy and have large memory footprints. E.g., ViT-tiny (Dosovitskiy et al., 2020; Wu et al., 2022; Steiner et al., 2022) has 5.53M parameters and requires nearly 0.9 GB of GPU memory to train. Implementing the learnable attention (1) in ViT-tiny training with B-splines of order 5 and grid size 10 and Fourier basis with grid size 10 increases the parameter count to 30.68M and 39.06M, respectively. We could not train the version with B-splines for its humongous memory requirements, and B-spline computations are non-parallelizable. The one with Fourier basis is parallelizable but it takes approximately 60 GB of GPU memory when computing with batch of one image. The computational bottleneck is agnostic of the basis functions. How can we remedy this? Deep neural networks exhibit low-rank structure (Oja, 1982; Jain et al., 2016). Recently, (Kwon et al., 2024) showed across various networks, the training largely occurs within low-dimensional subspace. Therefore, we postulate that the attention matrices also have an underlying low-rank structure. 4 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? their overall implementation results in slower, sparse, nonscalable, and complicated GPU execution; also, see (Xu et al., 2024; Pal & Das, 2024). In B.7, our extensive experiments with B-spline basis in KAN for image classification tasks verify their poor generalizability on medium-scale datasets (e.g., CIFAR-10 and CIFAR-100). So, what could be an attractive basis? fundamental question in function approximation is whether the function converges pointwise almost everywhere. Carleson in 1966 proved the following fundamental result for the Fourier approximation of Lp periodic functions. Theorem 2. (Carleson, 1967) Let be an Lp periodic function for (1, ], with Fourier coefficients ˆf (n). Then limN ˆf (n)einx = (x), for almost every x. (cid:80) nN In particular, if function is continuously differentiable, its Fourier series converges to it everywhere. In the past, (Xu et al., 2024; Mehrabian et al., 2024) used Fourier basis in KAN, (Dong et al., 2024) proposed Fourier analysis network (FAN) to capture periodic phenomena. Motivated by these, we use the Fourier basis to approximate the effect of the smooth softmax function. For our architecture, we employ the Fourier basis, {sin (), cos ()} with gridsize to design learnable attention. Algorithm 1 can be pq ] RrN , we used optionally. For the operator (cid:98)Φi,j = [ (cid:98)ϕi,j have (cid:98)Φi,j : RN Rr. Hence, each row Ai,j k,:, with [N ], (cid:104) (Ai,j transformed into (cid:98)Φi,j k,q), via the Fourier bases such that pq (Ai,j (cid:98)ϕi,j k,q) + bpqm sin (mAi,j apqm cos (mAi,j k,:)(cid:105) pq (Ai,j q=1 (cid:98)ϕi,j = (cid:80)N k,q) = k,q), (cid:88) m=1 where refers to the mth grid point on uniform grid m=1, i,j] are of size G. The weights, [{[apqm], [bpqm]}G updated in the backpropagation. If we use Fourier basis in (cid:98)Φ RrN , the parameter complexity of MHSA becomes O(2N rhGL). Regardless of the basis used, the complexity of linear projection is O(N 2rhL). We note that for any Transformer, and are fixed. So, one may omit them from the complexity results. Blockwise and Universal operator configuration. We consider two configurations for updating the operator (cid:98)Φi,j. (a) Blockwise: In this configuration, each encoder block involves learning the attention through distinct operators (cid:98)Φ for each of the heads, totaling hL operators; see Figure 4 (a). Like the MHSA architecture in ViTs, the blockwise configuration is designed to learn as many different data representations as possible. (b) Universal: The motivation behind this configuration comes from the KAT (Yang & Wang, 2025). In KAT, the MLP head is replaced with different variations of KAN headKAN and Group-KAN. In Group-KAN, the KAN layer shares rational base functions and their coefficients among the edges. Inspired by Figure 4. Different configurations to update (cid:98)Φ:(a) Blockwise configuration, where Φi,1 = Φi,2 = = Φi,L for all = 1, 2, ..., (b) universal configuration, where Φi,1 = Φi,2 = = Φi,L = Φi for all = 1, 2, ..., h. To validate this, we perform spectral analysis of the attention matrices before and after the softmax activation; see Figures 3a-3b. The scree test (Cattell, 1966) shows that for vanilla attention without softmax, there are 8 significant singular values; after softmax activation this count increases to 16. The above observation verifies that with or without the nonlinear activation, the attention matrix Ai,j possesses an underlying low-rank structure. This motivates us to use lower-dimensional operator, (cid:98)Φ for the learnable attention calculation with = 12. In B.3, we perform an ablation to find the best r. Instead of using an operator, Φi,j, we use reduced sized operator (cid:98)Φi,j RrN such that , and the new learned activation is r-dimensional for [N ]. That is, (cid:98)Φi,j down-projects each attention row vector of Ai,j to lower dimensional subspace. This process significantly reduces the computational overhead. Next, we postmultiply using another learnable weight matrix, i,j RN to project them back to their original dimension. For each [N ], this operation results in computing k,:)(cid:105)(cid:105) (cid:98)σ(Ai,j i,j (cid:98)Φi,j (cid:104) ; see Figure 2(d). k,:) = (Ai,j (cid:104) Fourier Kolmogorov-Arnold Attention. Although the default basis for KANs is B-Splines in (Liu et al., 2025), the number of MHSA parameters is much more than that of an MLP. Specifically, for encoder blocks, each with attention heads, the parameter complexity is O(N 2Lh(G + k)), for Φ RN and O(N rLh(G + k)), for (cid:98)Φ RrN , if the model uses B-Splines of degree k. Moreover, as (Yang & Wang, 2025) mentioned, B-splines are localized functions, not standard CUDA functions. Although efficient CUDA implementations for cubic B-Splines exist (Ruijters & Thevenaz, 2012; Ruijters et al., 2008), 5 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Figure 5. Training loss and test accuracy of vanilla ViTs and their Fourier KArAt versions on CIFAR-10, CIFAR-100 and ImageNet-1K. this, in our update configuration, all encoder blocks share the same operators; (cid:98)Φi,j = (cid:98)Φi for = 1, 2, . . . , L; see Figure 4 (b). Rather than learning attention through hL operators, this configuration only uses operators. Here, we share all learnable units and their parameters in each head across blocks. We postulate that blockwise mode with more operators captures more nuances from the data. In contrast, the universal mode is suitable for learning simpler decision boundaries from the data. We also note that these two modes can be used with (1) as shown in Figure 2. Finally, Algorithm 2 in Appendix gives pseudocode of applying Fourier-KArAt in ViT encoder block. 5. Empirical Analysis Baselines and Datasets. For our benchmarking, we chose three popular vision TransformersViT-tiny, ViTsmall (Wu et al., 2022; Steiner et al., 2022), and ViTbase (Dosovitskiy et al., 2020). We incorporate FourierKArAt in them by replacing their softmax function with learnable activation. We perform our benchmarking on CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009) and ImageNet-1K (Deng et al., 2009) datasets. Implementing Fourier KArAt. Our implementation of MHSA using the Fourier basis has one hyperparameter, the grid size, G, and two configurations, blockwise or universal. Using different permutations of these factors, we can design multiple variants of Fourier-KArAt. We vary our grid sizes from the set = {1, 3, 5, 7, 9, 11}. For each grid value, we perform universal and blockwise updates; see Figure 4. With this formalization, GnB denotes Fourier-KArAt with grid size and weights updated in blockwise mode, and GnU denotes Fourier-KArAt with grid size and weights updated in universal mode. See the detailed ablation study of these different variants in the Appendix. We also note Figure 6. Weight distribution of (top to bottom) ViT-Tiny, ViTTiny+Fourier KArAt, ViT-Base, ViT-Base+Fourier KArAt. The columns (left to right) represent weights at initialization, at epoch 50, at epoch 100, and their superposition. that projecting the learned attention rows to probability simplex degrades the performance; see B.5. We dispense ℓ1 projection for the experiments in the main paper. See implementation details in B.1. 5.1. Training Performance and Model Quality Table 1 presents the performance of the best-performing Fourier KArAt variants. We also present the training loss and test accuracy curves on all three datasets; see Figure 5. Fourier KArAt is easily optimized in the initial training phase, gaining at par or better loss value and accuracy than the vanilla ViTs. However, in the later training phase, except for the fewer parameter variants (ViT Tiny+Fourier KArAt), the loss curve flattens faster. From Figure 5 we infer that 6 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Table 1. Performance of the best-performing Fourier KArAt models compared to the conventional vanilla baselines. The best and the second best Top-1 accuracies are given in red and blue, respectively. The and arrows indicate the relative loss and gain, respectively, compared to the base models."
        },
        {
            "title": "MODEL",
            "content": "CIFAR-10 CIFAR-100 IMAGENET-1K ACC.@1 ACC.@5 ACC.@ ACC.@5 ACC.@1 ACC.@5 VIT-BASE + G1B + G1U 83.45 81.81 (1.97%) 80.75(3.24%) VIT-SMALL + G3B + G3U 81.08 79.78 (1.60%) 79.52(1.92%) VIT-TINY + G3B + G3U 72.76 76.69(5.40%) 75.56 (3.85%) 99.19 99.01 98.76 99.02 98.70 98.85 98.14 98.57 98. 58.07 55.92(3.70%) 57.36 (1.22% ) 53.47 54.11 (1.20%) 53.86(0.73%) 43.53 46.29(6.34%) 46.75(7.40%) 83.70 82.04 82.89 82.52 81.02 81.45 75.00 77.02 76. 72.90 68.03(6.68%) 68.83 (5.58%) 70.50 67.77 (3.87%) 67.76(3.89%) 59.15 59.11 (0.07%) 57.97(1.99%) 90.56 86.41 87.69 89.34 87.51 87.60 82.07 82.01 81."
        },
        {
            "title": "PARAMETERS",
            "content": "85.81M 87.51M (1.98% ) 85.95M (0.16% ) 22.05M 23.58M (6.94%) 22.18M (0.56%) 5.53M 6.29M (13.74%) 5.59M (1.08%) (i) Distribution of the Weights. (Zhang et al., 2022) considered an invariant measure perspective to describe the training loss stabilization of the neural networks. Based on this idea, we study the distribution of the weights of the smallest and largest models of the ViTs, Tiny and Base, and their Fourier KArAt variants. Figure 6 shows the distribution of the weights of these models during different training phases. The evolution of the weights distributions for respective models remains invariant, and we can guarantee the convergence of their loss values; also, see Figure 5. However, as explained by (Zhang et al., 2022), this perspective cannot shed light on the generalization capacity and structural differences between different neural networks. (ii) Spectral Analysis of Attention. Although by studying the distribution of the weights we cannot comment on the generalizability, from Table 1 we realize all the KArAt variants have more parameters than their vanilla counterparts. Therefore, it would be interesting to see how their attention matrices behave. As discussed in 4, the attention matrices in vanilla MHSA have low-rank structure. Following that study, we investigated if Fourier KArAts learned attention matrices are also low-rank. We use all 3 heads in the last encoder block of ViT-Tiny on 5 randomly sampled images from the CIFAR-10 validation set. There are total of 15 singular vectors (each of 197 dimensions) where the singular values are arranged in non-increasing order. Let σi be the ith singular value across all heads and samples (it is permutation invariant). For each [197], we plot [ln(σmin )], where σi is the average of ith-indexed singular value across all samples and heads. From Figure 7, we observe that vanilla softmax attention has significantly larger singular values than its KArAt variant, G3B. Fourier KArAt with more parameters shows faster drop in significant singular values. This observation indicates that Fourier KArAt variants are highly overparam- ), ln( σi), ln(σmax Figure 7. Spectral decomposition of the attention matrix for ViT-Tiny on CIFAR-10 dataset with vanilla softmax attention and our learnable Fourier attention. for the majority of the scenarios involving Fourier-KArAt. We also notice that larger models like ViT-Base are more susceptible to slight changes in the parameters, making the training process difficult to manage, which we try to investigate in the rest of the paper using several analytical studies. To conclude, ViT-Tiny+Fourier KArAt variants outperform ViT-Tiny on CIFAR-10 and CIFAR-100 by 5.40% and 7.40%, respectively, and on ImageNet-1K it achieves almost similar accuracy. ViT-Small and -Base models with Fourier KArAt variants can barely outperform the vanilla models and the accuracy differences increase. Comparing the Top-1 accuracies from Table 1 across blockwise and universal modes on the datasets does not have enough evidence to support the claim that one mode is better than the other. These tabulated accuracies are the results of applying the two variants on only two grid sizes. 5.2. Analysis of Performance In this Section, we dissect the performance of Fourier KArAt through diverse set of analyses. 7 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Figure 8. Loss landscape for ViT-Tiny and ViT-Base (the smallest and the largest model) along the two largest principal component directions of the successive change of model parameters. The top row provides 3D-visulaization of the loss surface including the local optima and saddle points. The bottom row shows the loss contours along with the trajectory of the optimizers. Figure 9. Visualizing attention maps for Vit-Tiny. The original images used for inference are on the left, and on the right, we show the attention score map and the image regions of the dominant head (Top row: Fourier KArAt, bottom row: Vanilla MHSA). eterized and have much lower rank structure compared to the vanilla ViTs. (iii) Loss Landscape Analysis. To understand the reason behind the slower convergence in the later training phase and the lower generalizability of KArAts, we visualize the loss landscape of the networks. We plot the loss surface along the directions in which the gradients converge. Following (Li et al., 2018), we perform principal component analysis (PCA) of the change in parameters over training progression to understand the major directions of parameter convergence. Considering the fully trained model as the minimum in the loss hyperplane and the two principal component directions as and axes, we plot the loss values over the validation set of CIFAR-10 along the axis for ViT-Tiny and ViTBase for the vanilla MHSA and KArAt (G3B and G1B, respectively). These visualizations in Figure 8 show that having Fourier KArAt in ViT architectures significantly impacts the smoothness of the loss surfaces. ViT-Tiny with the least parameters has the smoothest loss landscape and modest generalizability. In contrast, ViTTiny+Fourier KArAts loss landscape is spiky; it indicates the model is full of small-volume minima (Huang et al., 2020). However, the model is modest in the number of parameters, so the gradient descent optimizer can still find an optimized model with better generalizability than the vanilla ViT-Tiny; hence it gains better test accuracy; see Table 1. ViT-Base, however, has more parameters than Tiny and its loss surface is much spikier than ViT-Tiny. Finally, the loss surface of ViT-Base+Fourier KArAt is most spikey, making it narrow margin model with sharp minima in which small perturbations in the parameter space lead to high misclassification due to their exponentially larger volume in high dimensional spaces; see Figure 8(d). Moreover, ViT-Base+G3B has 14 times more parameters than ViTTiny+G1B. With learnable activations, gradient descent 8 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? optimizers fail to find the best-optimized model, as small differences in the local minima translate to exponentially large disparities. The slow convergence in the later phases of the models is bacause the increasing number of local minima can slow down convergence. (iv) Attention Visualization. MHSA in ViTs captures region-to-region interaction in images. DINO (Caron et al., 2021) provides an innovative way to explain the dominant regions in an image that contribute towards the inference decision. It maps the self-attention values of the CLS token at the last encoder layer which is directly responsible for capturing the information related to the class label. While the computer vision community predominantly uses this attention visualization, it is not directly suitable for our case. Unlike softmax, Fourier KArAt does not inherently ensure learned attention values in [0, 1], as we dispense the ℓ1 projection. Therefore, we cannot interpret its performance similarly. Nonetheless, pre-softmax or pre-KArAt values in attention matrix Ai,j are supposed to capture token-totoken interactions, and we adapt the attention maps to ignore the negative values for the sake of visualization. We use the ImageNet-1K trained models of ViT-Tiny (with vanilla MHSA and Fourier KArAt) and plot the attention maps of the last layers in Figure 9. Fourier KArAts tend to concentrate high interaction scores on the concerned objects dominant features compared to having interactions throughout the object regions like traditional ViTs (Dosovitskiy et al., 2020; Touvron et al., 2021). 6. Conclusion and Future Direction This paper proposes learnable MHSA for vision Transformers that can theoretically operate on any choice of basis. The central design theme of this learnable attention module is based on the famous Kolmogorov-Arnold representation theorem (Kolmogorov, 1956) and the Kolmogorov-Arnold network proposed by (Liu et al., 2025). This endeavor raises many interesting questions regarding KArAts computing scalability and the resulting models generalization capacity. In the future, with the advent of large multimodal models (LMMs), we plan to extend this study beyond ViTs for image classification and check the resilience of KArAt in the language processing domain and on open-source LMMs. KANs are parameter efficientthis claim does not hold for learnable KArAt selft-attention design. However, our spectral analysis and loss-landscape experiments suggest that overparameterized KArAts weights lie in much lower-dimensional subspace. Nevertheless, how to navigate through KArAts parameter-heavy loss landscape to find the optimized model with better generalizability remains an open question that we encourage the community to focus on. Finally, we note that the data specificity of MHSA plays key role in their high performance. Therefore, we aim to explore its scope with KArAts with the possibility of pitfalls due to long-range dependency (Park & Kim, 2022)."
        },
        {
            "title": "References",
            "content": "Abueidda, D. W., Pantidis, P., and Mobasher, M. E. DeepOKAN: Deep operator network based on Kolmogorov Arnold networks for mechanics problems. Computer Methods in Applied Mechanics and Engineering, 436:117699, 2025. Azam, B. and Akhtar, N. Suitability of KANs for Computer Vision: preliminary investigation. arXiv preprint arXiv:2406.09087, 2024. Behrouz, A., Zhong, P., and Mirrokni, V. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Bengio, Y., Goodfellow, I., Courville, A., et al. Deep Learning, volume 1. MIT press Cambridge, MA, USA, 2017. Boas, T., Dutta, A., Li, X., Mercier, K., and Niderman, E. Shrinkage function and its applications in matrix approximation. The Electronic Journal of Linear Algebra, 32: 163171, 2017. Bodner, A. D., Tepsich, A. S., Spolski, J. N., and Pourteau, S. Convolutional Kolmogorov-Arnold Networks. arXiv preprint arXiv:2406.13155, 2024. Bozorgasl, Z. and Chen, H. Wav-KAN: Wavelet arXiv preprint Kolmogorov-Arnold Networks. arXiv:2405.12832, 2024. Bryan, K. and Leise, T. Making Do with Less: An Introduction to Compressed Sensing. Siam Review, 55(3): 547566, 2013. Carleson, L. On convergence and growth of partial sums of Fourier series. Matematika, 11(4):113132, 1967. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging Properties in Self-Supervised Vision Transformers. In ICCV, pp. 9650 9660, 2021. Cattell, R. B. The scree test for the number of factors. Multivariate behavioral research, 1(2):245276, 1966. Chen, Z., Gundavarapu, and DI, W. Vision-KAN: Exploring the Possibility of KAN Replacing MLP https://github.com/ in Vision Transformer. chenziwenhaoshuai/Vision-KAN.git, 2024. Condat, L. Fast Projection onto the Simplex and the ℓ1 Ball. Mathematical Programming, 158(1):575585, 2016. 9 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and FeiFei, L. ImageNet: Large-Scale Hierarchical Image Database. In CVPR, pp. 248255, 2009. Dong, Y., Li, G., Tao, Y., Jiang, X., Zhang, K., Li, J., Su, J., Zhang, J., and Xu, J. FAN: Fourier Analysis Networks. arXiv preprint arXiv:2410.02675, 2024. Donoho, D. L. Compressed sensing. IEEE Transactions on information theory, 52(4):12891306, 2006. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR, 2020. Drokin, I. Kolmogorov-Arnold convolutions: Design arXiv preprint principles and empirical studies. arXiv:2407.01092, 2024. Dutta, A. Weighted Low-Rank Approximation of Matrices: Some Analytical and Numerical Aspects. PhD thesis, University of Central Florida, 2016. Ferdaus, M. M., Abdelguerfi, M., Ioup, E., Dobson, D., Niles, K. N., Pathak, K., and Sloan, S. KANICE: Kolmogorov-Arnold Networks with Interactive Convolutional Elements. arXiv preprint arXiv:2410.17172, 2024. Genet, R. and Inzirillo, H. TKAN: Temporal KolmogorovArnold Networks. arXiv preprint arXiv:2405.07344, 2024. Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., and Zhang, Z. Star-Transformer. arXiv preprint arXiv:1902.09113, 2019. Han, D., Pu, Y., Xia, Z., Han, Y., Pan, X., Li, X., Lu, J., Song, S., and Huang, G. Bridging the Divide: Reconsidering Softmax and Linear Attention. In NeurIPS, volume 37, pp. 7922179245, 2025. Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359366, 1989. Huang, W. R., Emam, Z., Goldblum, M., Fowl, L., Terry, J. K., Huang, F., and Goldstein, T. Understanding Generalization Through Visualizations. In Proceedings on Cant Believe Its Not Better! at NeurIPS Workshops, volume 137, pp. 8797, 2020. Kingma, D. P. and Ba, J. Adam: Method for Stochastic Optimization. In ICLR, 2015. Kolmogorov, A. N. On the representation of continuous functions of several variables as superpositions of continuous functions of smaller number of variables. Doklady Akademii Nauk SSSR, 1956. Kovaleva, O., Romanov, A., Rogers, A., and Rumshisky, A. Revealing the Dark Secrets of BERT. In EMNLPIJCNLP, 2019. Krizhevsky, A., Hinton, G., et al. Learning Multiple Layers of Features from Tiny Images. 2009. Kwon, S. M., Zhang, Z., Song, D., Balzano, L., and Qu, Q. Efficient Low-Dimensional Compression of Overparameterized Models. In AISTATS, pp. 10091017, 2024. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 1998. LeCun, Y., Cortes, C., and Burges, C. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. Li, C., Liu, X., Li, W., Wang, C., Liu, H., Liu, Y., Chen, Z., and Yuan, Y. U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation. arXiv preprint arXiv:2406.02918, 2024. Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. Visualizing the Loss Landscape of Neural Nets. In NeurIPS, volume 31, 2018. Liu, Z., Ma, P., Wang, Y., Matusik, W., and Tegmark, M. KAN 2.0: Kolmogorov-Arnold Networks Meet Science. arXiv preprint arXiv:2408.10205, 2024. Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Soljaˇcic, M., Hou, T. Y., and Tegmark, M. KAN: Kolmogorov-Arnold Networks. In ICLR, 2025. Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., Xu, C., Xiang, T., and Zhang, L. Soft: Softmax-free Transformer In NeurIPS, volume 34, pp. with Linear Complexity. 2129721309, 2021. Mehrabian, A., Adi, P. M., Heidari, M., and HaciImplicit Neural Representations with haliloglu, I. Fourier Kolmogorov-Arnold Networks. arXiv preprint arXiv:2409.09323, 2024. Jain, P., Jin, C., Kakade, S. M., Netrapalli, P., and Sidford, A. Streaming PCA: Matching Matrix Bernstein and NearOptimal Finite Sample Guarantees for Ojas Algorithm. In COLT, pp. 11471164, 2016. Nguyen, T., Suliafu, V., Osher, S., Chen, L., and Wang, B. FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field attention. In NeurIPS, volume 34, pp. 2944929463, 2021. 10 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Nguyen, T., Nguyen, T., Do, H., Nguyen, K., Saragadam, V., Pham, M., Nguyen, K. D., Ho, N., and Osher, S. Improving Transformer with an Admixture of Attention Heads. In NeurIPS, volume 35, pp. 2793727952, 2022a. Nguyen, T., Pham, M., Nguyen, T., Nguyen, K., Osher, S., and Ho, N. FourierFormer: Transformer Meets Generalized Fourier Integral Theorem. In NeurIPS, volume 35, pp. 2931929335, 2022b. Nguyen, T. M., Nguyen, T., Bui, L., Do, H., Nguyen, D. K., Le, D. D., Tran-The, H., Ho, N., Osher, S. J., and Baraniuk, R. G. Probabilistic Framework for Pruning Transformers Via Finite Admixture of Keys. In ICASSP, pp. 15, 2023. Oja, E. Simplified Neuron Model as Principal Component Analyzer. Journal of mathematical biology, 15:267273, 1982. Pal, A. and Das, D. Understanding the Limitations of BSpline KANs: Convergence Dynamics and Computational Efficiency. In NeurIPS, 2024. Park, N. and Kim, S. How Do Vision Transformers Work? In ICLR, 2022. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An Imperative Style, HighPerformance Deep Learning Library. In NeurIPS, volume 32, 2019. Rahimian, A. K., Govind, M. K., Maity, S., Reilly, D., Kummerle, C., Das, S., and Dutta, A. Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads. arXiv preprint arXiv:2406.19391, 2024. Ruijters, D. and Thevenaz, P. GPU Prefilter for Accurate Cubic B-spline Interpolation. The Computer Journal, 55 (1):1520, 2012. Ruijters, D., ter Haar Romeny, B. M., and Suetens, P. Efficient GPU-Based Texture Interpolation using Uniform B-splines. Journal of Graphics Tools, 13(4):6169, 2008. Shi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., and Kwok, J. T.-Y. SparseBERT: Rethinking the Importance In ICML, pp. 95479557, Analysis in Self-attention. 2021. Steiner, A. P., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., and Beyer, L. How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. TMLR, 2022. ISSN 2835-8856. Stewart, G. W. On the Early History of the Singular Value Decomposition. SIAM review, 35(4):551566, 1993. Sun, Q., Cetin, E., and Tang, Y. Transformer2: SelfAdaptive LLMs. arXiv preprint arXiv:2501.06252, 2025. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transformers & distillation through attention. In ICML, pp. 1034710357, 2021. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention Is All You Need. In NeurIPS, volume 30, 2017. Wang, Z., Zainal, A., Siraj, M. M., Ghaleb, F. A., Hao, X., and Han, S. An intrusion detection model based on Convolutional Kolmogorov-Arnold Networks. Scientific Reports, 15(1):1917, 2025. Wu, K., Zhang, J., Peng, H., Liu, M., Xiao, B., Fu, J., and Yuan, L. TinyViT: Fast Pretraining Distillation for Small Vision Transformers. In ECCV, pp. 6885, 2022. Xiao, H., Rasul, K., and Vollgraf, R. Fashion-MNIST: novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. Xu, J., Chen, Z., Li, J., Yang, S., Wang, W., Hu, X., and Ngai, E. C.-H. FourierKAN-GCF: Fourier KolmogorovArnold NetworkAn Effective and Efficient Feature Transformation for Graph Collaborative Filtering. arXiv preprint arXiv:2406.01034, 2024. Yang, X. and Wang, X. Kolmogorov-Arnold Transformer. In ICLR, 2025. Yu, R., Yu, W., and Wang, X. KAN or MLP: Fairer Comparison. arXiv preprint arXiv:2407.16674, 2024. Yun, C., Chang, Y.-W., Bhojanapalli, S., Rawat, A. S., Reddi, S., and Kumar, S. O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers. In NeurIPS, volume 33, pp. 1378313794, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big Bird: Transformers for Longer Sequences. In NeurIPS, volume 33, pp. 1728317297, 2020. Zhang, J., Li, H., Sra, S., and Jadbabaie, A. Neural Network Weights Do Not Converge to Stationary Points: An Invariant Measure Perspective. In ICML, pp. 2633026346, 2022. Zhang, P., Dai, X., Yang, J., Xiao, B., Yuan, L., Zhang, L., and Gao, J. Multi-Scale Vision Longformer: New Vision Transformer for High-Resolution Image Encoding. In ICCV, pp. 29983008, 2021. Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Algorithm 1 Projection on ℓ1 ball (Condat, 2016) Input: Rm Sort: as y(1) y(2) ... y(m) Calculate: ρ max (cid:110) [m] y(i) 1 Set: λ = 1 ρ (cid:18) ρ (cid:80) i=1 (cid:19) y(i) (cid:18) (cid:80) j=1 (cid:19)(cid:111) y(j) 1 Output: = (y λ)+ max (y λ, 0) Algorithm 2 Fourier-KArAt in jth Vision Transformer block Input: RN d, Fourier basis {sin (), cos ()} Output: RN m=1, i,j], O,j Parameters: Hyperparameters: Blockwise, universal, Algorithm 1 (ℓ1 projection), encoder blocks for each head {1, . . . , h} do: , G, [{[apqm], [bpqm]}G Rddh , dh = , Q, Qi,j XW i,j i,j XW i,j i,j XW i,j Ai,j = Qi,j (Ki,j ) dh end for for each head {1, . . . , h} do: for each row {1, . . . , } do: q=1 (cid:98)ϕi,j k,:] (cid:80)N (cid:98)Φi,j[(Ai,j) if ℓ1 projection then pq (Ai,j k,q) = (cid:80)G m=1 apqm cos (mAi,j k,q) + bpqm sin (mAi,j k,q) Execute Algorithm 1 else pass (cid:98)σ(Ai,j k,:) (cid:104) end for i,j (cid:98)Φi,j[(Ai,j k,:)] (cid:105) end for (cid:2) (cid:98)σ(A1,j)V 1,j if Blockwise Mode then (cid:98)σ(A2,j)V 2,j"
        },
        {
            "title": "Return O",
            "content": "(cid:98)σ(AN,j)V N,j(cid:3) O,j else if universal Mode then pass learnable units and parameters to (j + 1)th encoder block Return Organization. We organized the Appendix as follows: Section contains the solution to problem (2). We also provide the pseudocode to implement Fourier-KArAt in ViTs encoder block; see Algorithm 2. Section discusses additional numerical results, including implementation details, hyperparameter tuning, computation time analysis, and diverse ablation studies. A. Solution to Problem (2) First, we rewrite the constrained problem (2) as an unconstrained problem using the Lagrange multipliers as: L(x, λ, µ) = 1 2 y2 + λ (cid:18) (cid:88) i= (cid:19) xi 1 µT x, (3) where λ, µ are Lagrange multipliers. Using the Karush-Kuhn-Tucker (KKT) stationarity condition on (3), we find 0 L(x, λ, µ), which implies, xi yi + λ = µi. The complementary slackness gives, µixi = 0 for [m]. Further, for [m], the primal and dual feasibility conditions are xi 0, xi = 1, and µi 0, respectively. The stationarity and complementary slackness conditions give xi yi + λ 0, (xi yi + λ)xi = 0. If λ yi, then (yi λ) = 1 implying λ = 1 xi = 0. Otherwise, we have that yi 1). xi = (cid:80) i= (cid:80) i=1 ( (cid:80) i=1 (cid:80) i=1 12 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Note that, in Algorithm 1 one can replace with (cid:98)Φi,j[(Ai,j) 0 is satisfied for the low-rank attention matrices in Fourier-KArAt. This algorithm is also kept as an optional sub-process for Algorithm 2. k,:] so that the constraint (cid:98)σ 1 = 1 and (cid:98)σ (cid:104) (cid:105) (Ai,j k,:) (cid:104) (cid:105) (Ai,j k,l) B. Addendum to Empirical Analysis This section complements our empirical results in 5. B.1. Implementation Details We implement the framework in Python using PyTorch (Paszke et al., 2019) and use the training strategy of DEiT (Touvron et al., 2021). We train all models for 100 epochs with the ADAM optimizer (Kingma & Ba, 2015) with base learning rate of 3.125 105 and batch-size 32, except for the experiments on ImageNet-1K that use learning rate of 1.25 104 with batch-size 128. All experiments use warm-up of 5 epochs and cosine scheduler with weight decay of 5 102. The experiments were performed on two 80 GB NVIDIA H100 GPUs. All the hyper-parameter settings are given in Table 2. Table 2. Hyper-parameter settings for all the experiments conducted in this work."
        },
        {
            "title": "Learning Rate Schedule\nLearning Rate\nWarmup LR\nMin LR\nEpochs\nDecay Epochs\nWarmup Epochs\nDecay Rate",
            "content": "Exponential Moving Average (EMA) EMA Decay Random Resize & Crop Scale and Ratio Random Flip Color Jittering Auto-augmentation Mixup Cutmix Mixup, Cutmix Probability Mixup Mode Label Smoothing 224 224 0.9 128 for Imagenet-1K and 32 for CIFAR-10 & CIFAR-100 AdamW 1 106 0.9 0.05 1.0 Cosine 5 104 Batch Size 512 1 106 1 105 100 1 5 0.988 True 0. (0.08, 1.0), (0.67, 1.5) Horizontal 0.5; Vertical 0.0 0.4 rand-m15-n2-mstd1.0-inc1 True True 0.5, 0.5 Batch 0.1 B.2. Computation Time and Throughput The overall computation for Fourier KArAt variants is higher than their conventional softmax MHSA, and we have delineated the same in Figure 10. Primarily, the Fourier KArAt variants have longer training time. We show the training time comparison between the vanilla MHSA and its Fourier KArAt versions for 100 epochs on CIFAR-10, CIFAR-100, and Imagenet-1K datasets for all the models (ViT-Tiny, ViT-Small, ViT-Base) in Figures 10a, 10b, 10c, respectively. We 13 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? (a) Training time on CIFAR-10 dataset. (b) Training time on CIFAR-100 dataset. (c) Training time on Imagenet1K dataset. (d) Throughput comparison during inference. Figure 10. detailed comparison of computing requirements. We compare the training times for 100 epochs with the hyperparameter settings given in Table 2 for all the datasets CIFAR10, CIFAR100, and ImageNet1K. We also compare the throughputs of different models on ImageNet1k; the throughput results will be similar for other datasets as the input size is 224 224. only compare the best-performing Fourier KArAt models (G1B and G1U for ViT-Base, and G3B and G3U for ViT-Tiny & ViT-Small) with their softmax MHSA counterparts. We also observe that universal mode GnU training times are consistently slightly less than the blockwise modes GnB. During the training, we monitored the GPU memory requirements, and as expected, Fourier KArAt variants utilize significantly more memory than vanilla MHSA. In particular, the GPU memory requirements scale by 2.5, 4.25, and 6 in case of ViT-Tiny, ViT-Small, and ViT-Base, respectively, compared to the vanilla softmax MHSA. We also compare the throughput during inference in Figure 10d and see slightly faster inference in universal mode than blockwise except for ViT-Base. While there is massive training time discrepancy between vanilla ViTs and Fourier KArAt ViTs, the inference speeds for Fourier KArAt variants are comparable to their vanilla counterparts. Although there is minor difference in throughput between universal mode and blockwise mode during inference, theoretically, both variants for any model with the same grid size should involve the same quantity of FLOPs. B.3. Ablation with the Hidden Dimension, While avoiding the computational overhead for computing Φi,j RN , we make use of the low-rank structure that the attention heads show (see Figure 3) by comparing different values of r. Particularly, we consider the values, = 8, 12, 24, on the Fourier KArAt variant of ViT-Base model, (cid:98)Φi,j RrN ; see Table 4 for results on CIFAR-10. In this ablation, we observe that changing the hidden dimension has minimal impact on the models performance. This can be explained by the sudden drop in the singular values, as shown in Figure 7. As long as remains greater than the sudden drop index, the model should not be impacted by the changing of except for changes in computational requirement; higher would incur higher compute time as the size of the operator (cid:98)Φ scales with r. B.4. Ablation Study on the Impact of grid size, KANs are highly dependent on certain hyperparameters, and Fourier KAN and Fourier-KArAt have only one hyperparameter to tune the performance grid size G. Thus, we perform extensive experimentation involving grid size and present in Table 3. We observe that each of the particular ViT models, in conjunction with particular Fourier-KArAt variants, has typical value that brings out its best performance, and there is no universal value of to follow. When performing validation with ViT-Base+ variants on CIFAR-10 and CIFAR-100, the accuracy drops as the grid size passes size after 5. However, this behavior is not persistent with the ViT-Small/Tiny+ variants; see Table 31. B.5. Does Fourier KArAt Require ℓ1-Projection? To ensure that each row vector of the learned attention matrix lies on probability simplex, we project them onto the ℓ1-unit ball. We use Algorithm 1 to project learned attention vectors to the probability simplex and compare Top-1 accuracies to the baseline model. We note that using ℓ1 projection does not increase the training time substantially. However, from 1We did not perform gridsize 5, 7, 9, and 11 experiments with ViT-Tiny+KArAt as it was already outperforming the base ViT-Tiny with gridsize 1 and 3, and the experiments are extensively resource intensive. 14 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Table 3. Ablation on grid size for ViT-Tiny, ViT-Small and ViT-Base. We find particular grid size suitable for each of the models."
        },
        {
            "title": "MODEL",
            "content": "VIT-BASE + G1B + G1U + G3B + G3U + G5B + G5U + G11B + G11U VIT-SMALL + G1B + G1U + G3B + G3U + G5B + G5U + G11B + G11U VIT-TINY + G1B + G1U + G3B + G3U + G5B + G5U + G7B + G7U + G9B + G9U + G11B + G11U ACC.@1 CIFAR-10 CIFAR83.45 81.81 80.75 80.09 81.00 79.80 81.17 50.47 40.74 81.08 79.00 66.18 79.78 79.52 78.64 78.75 77.39 78.57 72.76 75.75 74.94 76.69 75.56 75.85 74.71 75.11 74.45 74.85 73.97 74.52 73.58 58.07 55.92 57.36 56.01 57.15 54.83 56.38 42.02 39.85 53.47 53.07 53.86 54.11 53.86 53.42 54.21 52.62 53.35 43.53 45.77 46.00 46.29 46.75 Table 4. Ablation on hidden dimension on CIFAR-10 with ViTBase. Here, we compare the values of {8, 12, 24}."
        },
        {
            "title": "MODEL",
            "content": "VIT-BASE + G3B + G3U + G5B + G5U + G3B + G3U + G5B + G5U + G3B + G3U + G5B + G5U 24 24 24 24 12 12 12 12 8 8 8 8 ACC.@1 ON CIFAR-10 58.07 80.54 80.81 77.99 80.52 80.09 81.00 79.80 81.17 80.76 80.40 79.79 80.83 Table 5. Comparing performance of Fourier KArAt for ViT-Tiny and ViT-Base with or without using ℓ1 projection in Algorithm 1. ViT-Tiny and ViT-Base use softmax and do not require ℓ1 projection."
        },
        {
            "title": "MODEL",
            "content": "ℓ1 PROJECTION ACC.@1 ON CIFAR-10 VITTINY + G3B + G3U + G3B + G3U VIT-BASE + G3B + G3U + G3B + G3U 72.76 41.99 40.85 76.69 75.56 83.45 47.44 46.11 80.09 81.00 Table 5, we observe that incorporating this algorithm in the Fourier-KArAt does not improve its performance; instead, the performance degrades significantly. Without applying Algorithm 1, the weights in each learnable unit can easily adjust and update accordingly. B.6. Alternate Variants of Fourier KArAt In this section, we experiment with different attention variants on Fourier KArAt. B.6.1. ALTERNATIVE APPROACHES TO THE LOWER RANK ATTENTION STRUCTURES In the main paper, we approximated the effect of the operator, Φi,j RN casting it as the product of two rank-r operatorsone operator with learnable activation, (cid:98)Φ, and the other is the learnable the linear transformation matrix, such that Φ = (cid:98)ΦW . Here, we abuse the notations for simplicity. However, natural question is how many different configurations are possible with (cid:98)Φ and such that they can approximate the effect of Φ. Specifically, we use the following configurations: (i)2 (cid:98)Φ(A), where (cid:98)Φ RrN and RN r, and (ii) (cid:98)Φ(W A), where RrN and (cid:98)Φ RN r. In the first configuration, (cid:98)Φ() acts on each row of to produce r-dimensional vector, and then projects it back to an -dimensional subspace. In the second configuration, , first projects each row of to produce r-dimensional vector, and then (cid:98)Φ with learnable activation produces -dimensional vectors. Primarily, we started with the full-rank operator Φ RN and found their calculations are prohibitively expensive, 2This configuration was used in the main paper. 15 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? Table 6. Ablation study on the operator variants in Fourier KArAt. Note that (cid:98)Φ(W A), (cid:98)Φ(A) refer to order of using and (cid:98)Φ, and Φ refers to the full-rank operator."
        },
        {
            "title": "MODEL",
            "content": "VIT-BASE + G11B + G11U VIT-TINY + G11B + G11U ACC.@1 ON CIFAR-10 (cid:98)Φ(W A) (cid:98)Φ(A) Φ(A) 41.02 50.47 30.14 47. 67.04 70.98 75.41 33.09 73.67 36.98 Table 7. Performance using softmax and variants of Fourier KArAt in separate heads. MODEL VIT-BASE VIT-BASE + G11B VIT-TINY VIT-TINY + G11B VIT-TINY + G11U LOW RANK CONFIGURATIONS NO. OF DISTINCT HEADS ACC.@1 ON (FOR FOURIER KARAT) T FOURIER KARAT CIFAR-10 ˆσ = (cid:98)Φ(W A) ˆσ = (cid:98)Φ(W A) ˆσ = (cid:98)Φ(A) ˆσ = Φ(A) ˆσ = Φ(A) ˆσ = Φ(A) ˆσ = Φ(A) ˆσ = Φ(A) ˆσ = Φ(A) ˆσ = Φ(A) ˆσ = Φ(A) 12 0 6 0 0 6 3 0 1 2 0 1 0 12 6 12 12 6 0 3 2 1 3 2 1 83. 41.02 47.60 50.47 30.14 30.24 72.76 33.09 32.51 34.73 36.98 33.12 70.57 Table 8. Experimental results on multi-layer KANs organized similarly to an MLP. These experiments involve B-Splines as the basis functions, as mentioned in Section 2. (a) Experiments on CIFAR-10 and CIFAR-100. (b) Experiments on MNIST and Fashion-MNIST."
        },
        {
            "title": "SPLINE ORDER GRIDS GRID RANGE LAYERS",
            "content": "ACC.@1 CIFAR-10 CIFAR-"
        },
        {
            "title": "SPLINE ORDER GRIDS GRID RANGE LAYERS",
            "content": "ACC.@1 MNIST FMNIST 1 3 4 10 10 10 10 20 (-4, 4) (-4, 4) (-4, 4) (-1, 1) (-2, 2) (-4, 4) (-4, 4) (-4, 4) (-6, 6) (-8, 8) (-4, 4) (-4, 4) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (20, 40) (40, 40) (10, 10) (10, 10) (10, 10) (10, 10) 92 94 94 89 90 95 96 96 95 95 93 85 86 86 83 95 86 87 88 86 86 85 1 3 4 5 10 40 10 20 40 10 20 40 (-4, 4) (-4, 4) (-6, 6) (-8, 8) (-4, 4) (-6, 6) (-8, 8) (-4, 4) (-4, 4) (-6, 6) (-8, 8) (-4, 4) (-6, 6) (-8, 8) (-4, 4) (-1, 1) (-2, 2) (-4, 4) (-4, 4) (-4, 4) (-6, 6) (-8, 8) (-4, 4) (-4, 4) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (10, 10) (20, 20) (40, 40) (10, 10) (10, 10) (10, 10) (10, 10) 22 21 22 22 22 22 22 22 22 39 22 29 31 21 10 10 22 40 45 39 10 21 1 4 4 4 1 1 1 4 4 4 1 1 1 1 4 1 4 4 9 7 9 1 regardless of the choice of the basis. Next, we conduct an ablation study to see which operator configuration works better; see Table 6. Our experiments show that the operator configuration (ii) demonstrates an inferior performance. We postulate that by down-projecting the dimensional attention row vector to smaller dimension, loses adequate token-to-token interaction, and this fails to capture dependencies from significant attention units within row. After that, from this limited information, learnable activation cannot significantly help the models performance. B.6.2. FOURIER KARAT AND T XAN HYBRID VERSION We consider another variant where we mix the learnable activation and pre-defined activation in each head. E.g., the user can have 2 of the 3 attention heads in each encoder block in ViT-Tiny activated by Softmax and the third activated by the Fourier KArAt. With the idea of incorporating KAN as way to replace softmax activation, we are curious to see if the ViT 16 Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers? model performs better with hybrid mode of activation; see Table 7 for results. B.6.3. MORE COMBINATIONS We have also experimented with more configurations of Fourier KArAt in various permutations of the strategies mentioned above and have not found any significant combination in terms of performance. We also carefully design particular training strategy where the attention operators ((cid:98)Φ, , and Φ wherever applicable) of Fourier KArAt are trained with separate learning rate to alleviate the problem of the mismatch of the data passing through these learnable layers, as they consider each row of an attention matrix as an input. This also attends to our observation of the models having gradient and loss explosion during training. However, we found this strategy to be of no improvement over the performance without the same. B.7. B-Spline experiments We test the performance of the B-spline KANs in the classification task on the CIFAR10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), MNIST (LeCun et al., 1998; 2010), and Fashion-MNIST (Xiao et al., 2017) datasets. We performed extensive experiments to see if there are any benefits to choosing B-Splines for the basis functions in KAN layers. Table 8 shows the Top-1 accuracies from different variants of Deep KAN. In this set of experiments, we closely followed earlier works (Yu et al., 2024; Liu et al., 2025). Hyperparameters involved in these experiments include the order of the B-spline k, grid size G, grid range [I, I], layers, and width. Although the B-spline KANs yield high accuracies in the small-scale MNIST and Fashion-MNIST datasets, they fail to generalize over larger datasets."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Central Florida, Orlando, FL, USA",
        "Department of Mathematics, University of Central Florida, Orlando, FL, USA"
    ]
}