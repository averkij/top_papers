{
    "paper_title": "BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities",
    "authors": [
        "Shaozhe Hao",
        "Xuantong Liu",
        "Xianbiao Qi",
        "Shihao Zhao",
        "Bojia Zi",
        "Rong Xiao",
        "Kai Han",
        "Kwan-Yee K. Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce BiGR, a novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that unifies generation and discrimination within the same framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a binary transcoder for binary code prediction. Additionally, we introduce a novel entropy-ordered sampling method to enable efficient image generation. Extensive experiments validate BiGR's superior performance in generation quality, as measured by FID-50k, and representation capabilities, as evidenced by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization across various vision tasks, enabling applications such as image inpainting, outpainting, editing, interpolation, and enrichment, without the need for structural modifications. Our findings suggest that BiGR unifies generative and discriminative tasks effectively, paving the way for further advancements in the field."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 2 7 6 4 1 . 0 1 4 2 : r Preprint. BIGR: HARNESSING BINARY LATENT CODES FOR IMAGE GENERATION AND IMPROVED VISUAL REPRESENTATION CAPABILITIES Kai Han1 Shaozhe Hao1 Xuantong Liu2 Xianbiao Qi3 Shihao Zhao1 Bojia Zi4 Rong Xiao3 1The University of Hong Kong 3Intellifusion {szhao,shzhao,kykwong}@cs.hku.hk qixianbiao@gmail.com kaihanx@hku.hk Project page: https://haoosz.github.io/BiGR Code and models: https://github.com/haoosz/BiGR Kwan-Yee K. Wong1 2Hong Kong University of Science and Technology 4The Chinese University of Hong Kong Figure 1: BiGR generates high-quality images while improving the discriminative capabilities of the representations. Left: Generated 512512 samples, 256256 samples, and classconditional editing samples. Right: BiGR vs. LlamaGen (Sun et al., 2024). We visualize image features from 100 classes in ImageNet-1K validation split using t-SNE (van der Maaten & Hinton, 2008), where the same color indicates the same class. Our model produces features with greater discriminative separability and enhances both generative and discriminative performance."
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce BiGR, novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that unifies generation and discrimination within the same framework. BiGR features binary tokenizer, masked modeling mechanism, and binary transcoder for binary code prediction. Additionally, we introduce novel entropy-ordered sampling method to enable efficient image generation. Extensive experiments validate BiGRs superior performance in generation quality, as measured by FID-50k, and representation capabilities, as evidenced by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization across various vision tasks, enabling applications such as image inpainting, outpainting, editing, interpolation, and enrichment, without the need for structural modifications. Our findings suggest that BiGR unifies generative and discriminative tasks effectively, paving the way for further advancements in the field. Project lead Corresponding authors 1 Preprint."
        },
        {
            "title": "INTRODUCTION",
            "content": "Image generation is experiencing revolutionary growth driven by the advancements in diffusion models (Ho et al., 2020; Rombach et al., 2022; Peebles & Xie, 2023) and autoregressive models (Esser et al., 2021; Sun et al., 2024; Tian et al., 2024). While these models have demonstrated impressive performance, their representation capabilities are under-studied. As revealed by Balestriero & LeCun (2024), reconstruction-based learning often produces visually compelling results but fails to provide strong latent representations for perception. It has been long-desired goal of the research community to design good image generator which can also serve as strong feature extractor. Centered around this goal, previous studies (Chen et al., 2020a; Li et al., 2023a) on representation capabilities of generative models have primarily focused on unconditional generation. Despite conditional generation (Peebles & Xie, 2023; Sun et al., 2024; Li et al., 2024) has emerged as recent research trend and garnered much attention, investigations of the representation capabilities of conditional generative models remain limited. In conditional image generation, conditions are added to guide the generation process. However, this guidance is absent in downstream discriminative tasks. This weakens the relationship between features and categories, and thereby diminishes the representation capabilities of the features. We validate this limitation using the latest class-conditional image generation model (Sun et al., 2024) (see Fig. 1 (right)), and stress the necessity of improving the representation capabilities of conditional generative models. We introduce BiGR, novel conditional image generation model that utilizes compact Binary latent codes for Generative tasks with improved Representation capabilities. BiGR is trained exclusively through generative process by reconstructing tokens without relying on any discriminative losses. We compress an image into sequence of binary codes using lookup-free quantization (Yu et al., 2024; Wang et al., 2023) and utilize our model to predict these binary codes. We emphasize that BiGR is the first conditional image generation model that unifies generative and discriminative tasks, achieving improved performance across both. Below, we describe our model design, generative and discriminative use, and zero-shot generalized applications. Our framework, built upon the language model architecture, has three major components, namely (1) binary tokenizer that converts pixel-level image into sequence of binary latent codes, (2) decoder-only transformer equipped with full bidirectional attention, and (3) binary transcoder that transforms continuous features into Bernoulli-distributed binary codes. We train BiGR using the masked modeling approach (Bao et al., 2022; Chang et al., 2022; Li et al., 2023a). This modification, deviating from the typical autoregressive approach, expands token interaction without altering the structure of Llama. Paired with tailored inference process and inherent visual representations, BiGR can perform both generative and discriminative tasks. For generative purpose, we design sampling method that iteratively unmask tokens in sequence, with the order determined by the binary entropy magnitude from the predicted Bernoulli distribution probabilities. This approach requires only small number of sampling iterations which significantly accelerates the inference process. As result, we achieve high efficiency in image generation compared with diffusion models, which require multiple steps to remove noise, and autoregressive models, which predict each token sequentially. Through extensive experiments, we show that BiGR performs on par with, or even surpasses, existing baselines in quantitative metrics. For discriminative purpose, we perform average pooling on the intermediate features in BiGR. By this straightforward operation, BiGR exhibits significantly stronger representation capabilities than comparable models, which has been empirically validated through linear probe evaluation. Due to the compactness of binary codes and the global information from masked modeling, the feature representations produced by BiGR can more effectively linearly separate visual categories in downstream discriminative tasks. Moreover, we explore the zero-shot generalization capabilities of BiGR within the generation domain. Unlike autoregressive models that must predict tokens in raster order, the masked modeling mechanism offers huge flexibility during inference, allowing for the design of task-specific strategies. As result, BiGR can perform various vision tasks in zero-shot manner, without requiring any structural changes or parameter fine-tuning. In this paper, we showcase applications of our model in image inpainting, outpainting, editing, interpolation, and enrichment. We believe that further applications of BiGR can be unlocked through community efforts. 2 Preprint. To summarize, our BiGR possesses the following prominent advantages: (i) Uniformity - BiGR is the first conditional image generation model that unifies generative and discriminative tasks within the same model. By modeling compact binary latent codes, BiGR delivers strong performance in both tasks compared to existing models. (ii) Efficiency - BiGR generates images at low time cost, attributed to the small number of sampling steps required in the iterative unmasking process, while still maintaining high generation quality. (iii) Flexibility - BiGR can be flexibly employed for various vision applications, such as inpainting, outpainting, editing, interpolation, and enrichment in zero-shot manner, without the need for task-specific structural changes or parameter fine-tuning. (iv) Scalability - BiGR demonstrates scalability in both generative and discriminative tasks, as evidenced by comprehensive evaluations of both generation quality and linear-probe performance."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Binary latent code modeling Binary latent code, also known as hashing (Wang et al., 2017), has been largely demonstrated effective for visual representations due to its compactness and discreteness (Cakir et al., 2019; Jiang & Li, 2018; Shen et al., 2015; Wei et al., 2021; Wu et al., 2019). In the realm of visual generation, the study of binary tokenizers has recently attracted notable attention, referred as look-up free quantization in Yu et al. (2024) and as binary autoencoder in Wang et al. (2023). Binary tokenizers can enhance the codebook utilization for vector-quantization methods (Esser et al., 2021; Van Den Oord et al., 2017), facilitating image and video generation. Wang et al. (2023) introduces Bernoulli diffusion process that operates on Bernoulli-distributed variables to generate binary latents. Our work studies this type of tokenizers and we propose novel generative framework for uniform conditional generation and visual representation. Generative representation learning Representation learning has long been an important topic, with self-supervised methods (He et al., 2020; Chen et al., 2020b; Caron et al., 2020; Grill et al., 2020; Caron et al., 2021; Zhou et al., 2022) dominating the field in the past few years. Some works learn visual representations through generative modeling. For example, iGPT (Chen et al., 2020a) predicts pixels in manner similar to GPTs (Brown et al., 2020), while MAE (He et al., 2022) and MAGE (Li et al., 2023a) reconstruct masked image regions. ViT-VQGAN (Yu et al., 2022a) studies the representation capabilities of unsupervised generative models. However, these methods involve specialized designs for discriminative tasks and are not directly suited for conditional image generation. Our work broadens this scope by proposing conditional image generation framework that consistently delivers both high-quality generation and strong representation capabilities. Conditional image generation Conditional image generation has gained significant attention recently. Existing works on this topic can be broadly grouped into two categories: diffusion models (Ho et al., 2020; Song et al., 2021; Rombach et al., 2022; Peebles & Xie, 2023; Chen et al., 2024a), which gradually denoise random Gaussian noise, and autoregressive models (Esser et al., 2021; Yu et al., 2022b;a; Sun et al., 2024; Tian et al., 2024), which predict the next tokens similarly to language models. The models based on masked prediction (Chang et al., 2022; Li et al., 2023a; Chang et al., 2023) can be classified as autoregressive models, as discussed in (Li et al., 2024). In this paper, for clarity, we use autoregressive to specifically refer to models that use causal attention and next-token prediction, and mask to refer to models using masked modeling. Although conditional generative models can produce visually compelling images, their representation capabilities have rarely been studied. Our work aims to bridge this gap."
        },
        {
            "title": "3 METHOD",
            "content": "Our framework is based on masked language model that operates directly on binary latent codes derived from images. We train the model by masking portion of the input tokens and learning to unmask them using predicted output tokens. The prediction is achieved through Bernoulli diffusion process (Wang et al., 2023), which is well-suited for generating binary latent codes. In sampling, we determine the order of tokens to be unmasked based on the magnitude of entropy computed from the predicted Bernoulli distribution probabilities. To obtain latent representations, we perform average pooling on the intermediate features of our model. We present the overview of BiGR in Fig. 2. We describe the details of each of its components below. 3 Preprint. Figure 2: Overview of BiGR. For simplicity, we display only 1 bit for each token, although each token actually consists of bits in length. Left: We outline the training of BiGR. Starting with binary codes from binary tokenizers, we append condition token and mask partial tokens. These tokens are projected into continuous embeddings and processed by the Llama backbone. The outputs undergo Bernoulli denoising process in the binary transcoder to generate probabilities, penalized by the weighted binary cross-entropy loss (wBCE) at masked positions. Right: We illustrate the generation process (detailed in Sec. 3.3) and the representation acquisition via average pooling. 3.1 PRELIMINARY We first review the binary tokenizer and the Bernoulli diffusion process that underpin our model. Binary tokenizer An image tokenizer can encode an image R3HW into sequence of latent codes {ζ 1, ζ 2, , ζ n} = (x), where each ζ represents the code at specific spatial position. Binary tokenizers (Yu et al., 2024; Wang et al., 2023), also known as lookup-free quantization, transform the code into binary format by and corresponding token index ri can be computed by zi = sign(ζ i) = 1{ζ > 0}, ri = (cid:88) k=1 2k1 zi[k], (1) (2) where zi[k] denotes the k-th bit of the binary code zi, and is the number of binary bits (i.e., code dimension), resulting in total of 2K token indices. Using Eq. (2), (Yu et al., 2024) index image tokens with the binary code zi and build vocabulary of size 2K for generative purposes. In contrast, our approach focuses on directly modeling the sequence of binary codes {zi}n i=1. Bernoulli diffusion We generate binary codes through Bernoulli diffusion process (Wang et al., 2023), which effectively models Bernoulli-distributed variables. Specifically, Bernoulli diffusion process adds Bernoulli noise from the starting point q(z0): q(ztzt1) = (cid:0)zt; zt1(1 βt) + 0.5βt(cid:1) = 1, 2, , T. (3) Here, denotes Bernoulli distribution, and the timestep out of the total is denoted as superscript. We model the denoising process by p(zt1zt), which predicts the Bernoulli distribution probabilities for the binary code at the previous timestep. By iterating the denoising process, starting with coin toss B(0.5), we can finally generate binary codes that follow Bernoulli distributions. 3.2 MASKED MODELING ON BINARY LATENT CODES Backbone We build our method upon the transformer-based language model Llama (Dubey et al., 2024; Touvron et al., 2023b;a). Unlike language, an image is not naturally modeled as causal sequence of tokens, but instead, each token should have access to all others to better capture global visual information. Therefore, we replace the causal attention commonly used in language models with bidirectional attention, and let the model predict masked tokens instead of next tokens. 4 Preprint. Input projection In the input space, instead of looking up an embedding vector with token index, we use simple linear layer that projects the binary code onto the embedding space. This technique has recently been explored for continuous-valued tokenizers in Tschannen et al. (2023), and we find that it also works well for binary-valued tokenizers. We maintain standard conditional embeddings and mask embeddings, where the conditional embedding is appended at the start of the sequence, and the mask embedding replaces inputs at masked positions. mi Mask-token prediction During training, we simply mask portion of image tokens with learnable mask token [M]. The fraction of masked tokens follows cosine schedule, as used in Li et al. (2023a). We compute losses only for the masked positions, where the model predicts the values of the masked tokens. Formally, let fθ represent the language model, and {zi }n i=1 denote the sequence of binary codes that are partially masked. Here, = {mi}n i=1 indicates whether the i-th position is masked (mi = 1) or left unmasked (mi = 0). We obtain outputs at the masked positions }n from the language model {hi}mi=1 = fθ({zi i=1), which are distributed in continuous space. Binary transcoder We transform the model outputs into binary codes1 through Bernoulli diffusion process (Wang et al., 2023). In particular, we learn denoising network gϕ with Sigmoid function to model pϕ(zt1zt) = (cid:0)zt1; S(gϕ(zt, t, h))(cid:1) , which predicts the probabilities of the Bernoulli distribution conditioned on the intermediate feature h. Consequently, binary codes can be generated by sampling from these probabilities. Following Wang et al. (2023); Ho et al. (2020), the training target is the binary residual, i.e., zt z0 where represents the element-wise XOR operation. The training objective is simply an element-wise weighted binary cross-entropy (wBCE) loss, expressed as (4) mi yk = (zt z0)[k] {0, 1} pk = S(gϕ(zt, t, h))[k] [0, 1], = 1 (cid:88) k=1 wk (yk log pk + (1 yk) log(1 pk)) , where wk = (1 yk) (cid:88) k=1 yk/K + yk (1 (cid:88) k= yk/K) + 1/K. (5) (6) (7) Here, yk represents the target, and pk is the predicted probability for the k-th bit in the binary code. The element-wise loss weight wk is applied to mitigate the imbalance between 0s and 1s, calculated based on their respective ratios in K-dimensional code. The constant term 1/K is added to prevent nearly-zero weights that could impede training. In training, we jointly optimize the language model fθ and the denoising network gϕ using the loss defined in Eq. (6). Visual representation Once trained, our model inherently possesses strong visual representations. Given an image, we input it into the model without any masks, along with an unconditional token appended. We then perform average pooling on the continuous-valued features to derive the global representation of the given image. We observe that the most discriminative representation originates not from the final layer but from the middle layers within the transformer blocks, in line with the findings in Yu et al. (2022a); Chen et al. (2020a). As result, we use the intermediate features as the final image representation. 3.3 ENTROPY-ORDERED GENERATIVE SAMPLING For image generation, we design sampling strategy for our model, enabling it to iteratively predict tokens from fully masked sequence. Unlike in training, where mask positions are randomly chosen at each step, during sampling, the order in which tokens are unmasked follows predefined criterion. We arrange the masked tokens according to the binary entropy magnitude calculated from the predicted probabilities. The binary entropy is defined as: = 1 (cid:88) k=1 pk log2 pk + (1 pk) log2(1 pk), (8) 1Since the operation is position-wise, we omit the superscript of positions for simplicity. 5 Preprint. which ranges from 0 to 1. Here, low value indicates high prediction confidence (i.e., when pk is closer to either 1 or 0). Therefore, confidence score can be derived from 1 H, illustrating the models confidence in this prediction. Following Li et al. (2023a), we add noise sampled from random Gumbel distribution multiplied by the temperature τ to the confidence score. At each sampling iteration, we select and unmask proportion of masked positions with the highest confidence scores. To unmask each token, we obtain its binary codes by performing Bernoulli sampling from the distribution B(pk). The unmasking ratio follows cosine schedule as used in Chang et al. (2022); Li et al. (2023a). This process operates over sampling iterations. When the mask ratio drops to zero, the sampling progresses to the last iteration where all tokens are unmasked, marking the completion of the generation process."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Model configuration We use the binary autoencoder (B-AE) introduced by Wang et al. (2023) as our binary tokenizer. The downsampling rate of the autoencoder is 16, projecting 256256 image into 1616 token sequence. We train four variants of the binary autoencoders designed with four different binary code dimensions, namely 16, 20, 24, and 32. With the four binary tokenizers, we train our BiGR of three different sizes based on Llama (Touvron et al., 2023a), namely (316M), XL (743M), and XXL (1.38B). For the binary transcoder, we follow Li et al. (2024) to employ an MLP gϕ with an adaptive LayerNorm, with sizes of 20M, 56M, and 104M respectively. For clarity, we denote the S-sized variant with B-dim tokenizer as BiGR-S-dB, e.g., BiGR-L-d16. Training details We train all models for 400 epochs, with L-sized models using batch size of 1024 and the others using batch size of 512. Our L/XL-sized models are trained on 8 A800 GPUs, while XXL-sized models are trained on 32 A800 GPUs. We maintain consistent training settings across all compared models based on the model size. Sampling Our model inherently supports classifier-free guidance (CFG) (Ho & Salimans, 2022) through the Bernoulli diffusion process. Within our sampling process, four hyperparameters are involved: CFG scale, Gumbel temperature (τ ), the number of sampling iterations (N ), and the number of Bernoulli denoising steps (T ). We identify the optimal hyperparameter setting for all models. We set CFG to 2.5 for all quantitative evaluations, which has shown to be effective across all our models. We set to 100 as default for all models. See more details in Appendix A. 4.2 UNIFORM PERFORMANCE Evaluation We evaluate the uniformity of BiGR by concurrently comparing generative and discriminative performance. We evaluate generation quality on ImageNet-1K 256256 by reporting Frechet Inception Distance (FID) as the main metric, along with Inception Score (IS), sFID, Precision (Pre.), and Recall (Rec.) as auxiliary metrics. All metrics are obtained using 50K generated samples. We assess representation capabilities through linear-probe evaluation, reporting the top-1 and top-5 accuracies, abbreviated as ACC1 and ACC5, on ImageNet-1k 256256 validation split. We follow standard practice (He et al., 2022) by using parameter-free BatchNorm (Ioffe, 2015) layer and linear classifier layer to classify the model features. We use the intermediate features from the 10-th layer for L-sized models, the 15-th layer for XL-sized models, and the 32-nd layer for XXL-sized models, as experiments on d16 models demonstrate these configurations yield the best performance. Additionally, we compare the inference speed, specifically the time taken to generate each image using one A100 GPU with batch size of 256. Comparison Starting from the latest autoregressive generation baseline LlamaGen (Sun et al., 2024), we comprehensively analyze two major components in this paper, namely (1) training objectives, specifically categorical loss (cat.) and binary loss (bin.), and (2) modeling types, including masking and autoregressive (AR) approaches. In total, we compare five models in Tab. 1, training four modelsS0, S1, S2, and S3with different configurations, excluding LlamaGen. For LlamaGen, we use the generative performance metrics reported in their paper and conduct our own evaluation of linear-probe performance using their pretrained model. The inference time of all models is tested on the same machines by us. 6 Preprint. Table 1: Uniformity comparison. We compare the generative and discriminative performance of our model against LlamaGen (Sun et al., 2024) and three other settings, varying by tokenizers, training objectives, and modeling types. Generative Discriminative Model Tokenizer Objective Type Time FID IS sFID Pre. Rec. ACC1 ACC5 LlamaGen VQGAN S0 S1 S2 S3 (Ours) B-AE B-AE B-AE B-AE Cat. Cat. Cat. Bin. Bin. AR AR Mask AR Mask 1.10 1.09 0.10 1.04 0.69 3. 3.21 3.85 7.50 3.17 248.28 239.17 261.81 164.31 262.14 8. 5.38 6.10 6.56 5.59 0.83 0.83 0.85 0.85 0.86 0. 0.54 0.47 0.41 0.50 40.5 23.8 61.1 45.2 64.3 64. 44.2 83.2 69.3 85.4 Table 2: Binary transcoder comparison. Generative Discriminative Binary objective FID IS sFID Pre. Rec. ACC1 ACC5 w/o Bernoulli denoising Direct BCE 5.84 212.34 9.89 0.78 0.52 63. 84.8 w/ Bernoulli denoising Predict z0 Predict zt z0 (Ours) 4.39 3.17 274.26 262.14 9.07 5. 0.87 0.86 0.44 0.50 62.0 64.3 83.9 85.4 Table 3: Sampling order comparison. We include the autoregressive variant for reference. Type Order Time FID IS sFID Pre. Rec. AR Raster Mask Raster Mask Rand. Ours Mask 1.04 8.81 0.69 0.69 7.50 4.51 7.12 3.17 164.31 191.10 174.11 262.14 6.56 6.49 11.85 5.59 0.85 0.80 0.76 0. 0.41 0.54 0.55 0.50 Observation As shown in Tab. 1, our model significantly outperforms other methods across all main evaluation metrics. In addition, we have the following observations. (1) By comparing LlamaGen and S0, using binary autoencoder provides better generative performance and worse discriminative performance compared to VQGAN. (2) For generation, AR modeling is better suited for categorical loss, while masked modeling is more appropriate for binary loss. (3) For discrimination, masked modeling drastically outperforms AR modeling for both losses, with binary loss further enhancing performance. (4) Masked modeling achieves significantly faster inference speed compared to AR modeling due to its fewer sampling iterations, with the binary objective taking more time resulting from the diffusion process. To conclude, BiGR, which employs masked modeling on binary latent codes, achieves the best uniform performance on both generative and discriminative tasks, accompanied by an efficient inference runtime. 4.3 MODEL ANALYSIS We analyze each component of our proposed method below. All experiments are conducted on BiGR-L-d16 unless otherwise specified. Binary transcoder We apply Bernoulli denoising process (Wang et al., 2023) as our binary transcoder to generate probabilities of Bernoulli distributions, from which the binary codes are sampled. We experiment with two variants, namely (1) predicting the initial clean latent z0, and (2) predicting the element-wise exclusive OR (XOR) value between the latent at the t-th timestep zt and z0. We find empirically the latter performs better, and thus, we adopt this setting for all of our models. Alternatively, naıve approach involves using direct binary cross-entropy (BCE) loss to train the model, replacing the Bernoulli denoising process. We compare these three variants in Tab. 2. Our method outperforms the other two variants across all main metrics. We observe that using direct BCE generates very smooth images which harms the generative performance. XOR prediction yields better generative and discriminative performance compared to z0 prediction. Sampling strategy In this paper, we propose simple entropy-ordered sampling strategy tailored for the masked training paradigm. We compare our method with two alternative sampling orders, namely (1) raster-scan order similar to the autoregressive approach, and (2) random order. Like our strategy, both compared methods are applied to the same trained model. The comparison results of the generative evaluation are reported in Tab. 3. The results indicate that the proposed sampling strategy is the best fit for our models generative purposes. Inference hyperparameters We evaluate the impact of two hyperparameters specific to our model on its performance. (1) We first present the FID results and sample time for different numbers of sampling iterations on the left side of Fig. 3. We observe that larger models generally achieve lower FID values, although they also increase sample time. In addition, more sampling iterations do not guarantee better performance, as different-sized models have varying optimal sampling iterations. For example, the L-sized model achieves its best performance with 20 iterations, rather than with larger numbers. (2) On the right side of Fig. 3, we present the results for different numbers of diffusion timesteps . The results indicate that diffusion timesteps have marginal impact on 7 Preprint. Figure 3: Relationships between FID-50K and sample time across varying inference hyperparameters. We compare different numbers of sampling iterations (N ) on the left and varying diffusion timesteps (T ) on the right for three model sizes. Figure 4: Evaluation of generative and discriminative performance across different model sizes. We report results for all tested tokenizers across four different dimensions of binary codes. We include the reconstruction FID (rFID) for each binary tokenizer for reference (grey points). generative performance, suggesting that our model can achieve comparable generation quality with fewer diffusion timesteps. This can significantly accelerate generation speed, especially for larger models. For example, with 10 diffusion timesteps, the XXL-sized model can achieve FID of 2.73 at speed of 0.85s per image. Model size and code dimension We validate that our model is scalable by testing the performance of different-sized models using tokenizers with various code dimensions. Note that the dimension of the binary codes only alters the number of parameters in the input and output linear projections, resulting in minimal effects on the overall model size. The evaluation results of both generative and discriminative performance are shown in Fig. 4. Our model generally performs better with larger sizes across all code dimensions, as indicated by both generative and discriminative metrics. Besides, we have the following observations from Fig. 4. (1) When the model size is small, it becomes challenging to model large-dimensional codes, such as dimension of 32 for the L-sized model, especially for generative purpose. (2) In contrast, as the model size increases, the improvement for smaller-dimensional codes is relatively modest, indicating that these codes are easier to model and can be effectively handled by smaller-sized models. (3) An exception arises in the linearprobe evaluation of models with 32-dimensional codes, where our XL-sized model outperforms the XXL-sized model. We hypothesize that this may be due to the optimal transformer layer for feature representation identified in the 16-dimensional model, which might not be the best choice for 32-dimensional models of the XXL size. Unconditional training Our model is class-conditional generative model. Intuitively, conditional generative training adds condition guidance that is absent in downstream discriminative tasks, which can diminish the representation capabilities of the model. We validate this conjecture by comparing the linear-probe performance of our model with that of its unconditional counterpart. We train the unconditional model by replacing the class conditional tokens with single unconditional token, and keep the inference process unchanged. We evaluate BiGR-L-d20 alongside its unconditional counterpart, and report the results in Tab. 4. The unconditional counterpart demonstrates better representation capabilities than our conditional model, indicating that discriminative tasks are more challenging for conditional generative models. Resolution of 512512 Using binary autoencoder that projects 512512 image into 3232 binary latent codes, we enable our model to generate 512512 images by increasing the input se8 Preprint. Figure 5: Generated 512512 samples. Table 4: Linear-probe evaluation of conditional and unconditional counterparts. Training ACC1 ACC5 Cond. Uncond. 67.5 68.3 87.5 88.4 Table 5: Generative performance comparison on 256256 ImageNet-1K benchmark. Table 6: Linear-probe top-1 accuracy on ImageNet-1K. : our evaluation results. Type Model #Params. FID IS Type Method #Tokens Params ACC1 Diff. DiT-L/2 (Peebles & Xie, 2023) DiT-XL/ Mask MaskGIT (Chang et al., 2022) AR VAR MAR AR Ours VQGAN (Esser et al., 2021) VQGAN ViT-VQGAN (Yu et al., 2022a) RQTran. (Lee et al., 2022) VAR-d16 (Tian et al., 2024) VAR-d20 VAR-d24 VAR-d30 MAR-B (Li et al., 2024) MAR-L MAR-H LlamaGen-B (Sun et al., 2024) LlamaGen-L LlamaGen-XL LlamaGen-XXL LlamaGen-3B BiGR-L-d24 BiGR-XL-d24 BiGR-XXL-d32 458M 675M 227M 227M 1.4B 1.7B 3.8B 310M 600M 1.0B 2.0B 208M 479M 943M 111M 343M 775M 1.4B 3.1B 336M 799M 1.5B 5.02 2.27 6.18 18.65 15.78 4.17 7.55 3.30 2.57 2.09 1.92 2.31 1.78 1.55 5.46 3.81 3.39 3.09 3. 2.71 2.49 2.36 167.2 278.2 182.1 80.4 74.3 175.1 134.0 274.4 302.6 312.9 323.1 281.7 296.0 303. 193.6 248.3 227.1 253.6 222.3 275.7 278.8 277.2 Con. MIM Gen. Cond. gen. MoCo (He et al., 2020) SimCLR (Chen et al., 2020b) SwAV (Caron et al., 2020) DINO (Caron et al., 2021) BYOL (Grill et al., 2020) CAE (Chen et al., 2024b) CMAE (Huang et al., 2023) iBOT (Zhou et al., 2022) BEiT (Bao et al., 2022) MAE (He et al., 2022) MAGE (Li et al., 2023a) BigBiGAN (Brock, 2018) iGPT-L (Chen et al., 2020a) iGPT-L ViT-VQGAN-B (Yu et al., 2022a) ViT-VQGAN-L RCG (Li et al., 2023b) l-DAE (Chen et al., 2024c) LlamaGen-L (Sun et al., 2024) MAR-B (Li et al., 2024) MAR-L MAR-H BiGR-L-d20 (Ours) BiGR-XL-d32 (Ours) - - - - - - - - 1616 1414 1616 - 3232 4848 3232 3232 1616 - 1616 1616 1616 1616 1616 1616 375M 375M 93M 85M 375M 304M 86M 304M 307M 304M 328M 344M 1.4B 1.4B 650M 1.7B 304M 304M 343M 208M 479M 943M 336M 799M 68.6 76.5 75.3 75.3 78.6 78.1 73. 81.0 73.5 75.8 78.9 61.3 60.3 65.2 65.1 73.2 77.6 75.0 40.5 57.9 59.1 60.0 67.5 69.8 quence length to 1024. We train such binary autoencoder with code dimension of 32 and train our model to accommodate this sequence length. We showcase the generated samples in Fig. 5, with additional samples available in Appendix C. 4.4 SYSTEM-LEVEL COMPARISONS We re-emphasize that the goal of this work is to propose uniform conditional generative model that can produce high-quality generations while maintaining strong representation capabilities. Therefore, surpassing state-of-the-art models across all metrics is not within the scope of this research. We provide more comprehensive comparison in Appendix B. Conditional image generation We present comparison of the generative performance of our model with leading generative systems in Tab. 5. Our model maintains top-tier generative quality among the first echelon of approaches. Besides, BiGR significantly outperforms LlamaGen. Visual representation We compare the linear-probe results of our model and the previous methods specifically designed for discriminative tasks. The results are shown in Tab. 6. We categorize the compared models into several types: contrastive (Con.), masked image modeling (MIM), generative (Gen.), and conditional generative (Cond. gen.). This classification is not entirely precise, as some models may use multiple losses for training, like MAGE (Li et al., 2023a). Our model is fairly compared to the conditional generative models, which solely rely on plain reconstruction loss without discriminative designs, such as specialized losses, augmentations, or additional data. For LlamaGen (Sun et al., 2024) that has the same model architecture as ours, we use the feature from the same layer for linear layer training. For MAR (Li et al., 2024), since their structure largely resembles MAE (He et al., 2022), we follow MAEs approach and train the linear layer on top of the encoder outputs. BiGR significantly outperforms the other conditional generative models. 4.5 ZERO-SHOT GENERALIZED APPLICATIONS The nature of the masked modeling mechanism allows the use of BiGR in wide range of applications in zero-shot manner, without the need for task-specific structural changes or parameter fine-tuning. We present the results of BiGR applied across various tasks in Fig. 6. 9 Preprint. Figure 6: Zero-shot generalization. We present samples of inpainting, outpainting, editing, interpolation, and enrichment. The original image is marked with purple border, with pink box highlighting the masked region. Images without the purple borders are generated by our model. Inpainting & Outpainting Given an image with mask, we use the unmasked regions to initialize the model inputs, enabling it to generate the remaining masked tokens. This generation process is guided by an unconditional token, which ensures that no additional information is introduced, allowing the model to focus solely on the existing image information. This approach enables highquality and diverse inpainting and outpainting. Class-conditional editing Unlike inpainting and outpainting, class-conditional editing is guided by specific class condition, allowing the model to edit the masked region with designated class object. Other operations remain consistent with inpainting and outpainting. Class interpolation We interpolate between two class conditions by calculating weighted sum in the embedding space. We then use the resulting interpolated embedding to guide the generation process. This interpolation process demonstrates that our model can generalize visual characteristics across different classes rather than merely memorizing each class. Image enrichment Our model can also enrich visual details in low-resolution image, process we call image enrichment. Specifically, we first upsample 128128 image to resolution of 256256 and encode it into sequence of 1616 tokens. This approach leverages the models generative capabilities to enrich images from low-resolution inputs."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce BiGR as the first conditional generative model that unifies generative and discriminative tasks within the same framework. Through extensive experiments, we highlight its uniformity, efficiency, flexibility, and scalability. Our results demonstrate that BiGR achieves decent performance in both generation quality and linear separability. Additionally, we showcase its application in various zero-shot generalized tasks. We believe BiGR has the potential to be adapted for broader range of applications in the future. Limitations (1) Our sampling strategy involves numerous hyperparameters to tune, resulting in substantial search space; thus, the reported models may not represent the optimal settings. (2) The models sequence length is fixed during training, making it inflexible to accommodate inputs of varying lengths. Consequently, generating higher-resolution images requires re-training the model. 10 Preprint. Acknowledgement This work is partially supported by the Hong Kong Research Grants Council - General Research Fund (Grant No.: 17211024)."
        },
        {
            "title": "REFERENCES",
            "content": "Randall Balestriero and Yann LeCun. How learning by reconstruction produces uninformative features for perception. In ICML, 2024. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers. In ICLR, 2022. Andrew Brock. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. Fatih Cakir, Kun He, Sarah Adel Bargal, and Stan Sclaroff. Hashing with mutual information. IEEE TPAMI, 2019. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Patrick Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers. In ICML, 2023. Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-$alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024a. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020a. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020b. Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. IJCV, 2024b. Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. arXiv preprint arXiv:2401.14404, 2024c. Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In NeurIPS, 2019. Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. In ICLR, 2017. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 11 Preprint. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. Olivier Henaff. Data-efficient image recognition with contrastive predictive coding. In ICML, 2020. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Zhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui IEEE Shen, and Jiashi Feng. Contrastive masked autoencoders are stronger vision learners. TPAMI, 2023. Sergey Ioffe. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Qing-Yuan Jiang and Wu-Jun Li. Asymmetric deep supervised hashing. In AAAI, 2018. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In CVPR, 2023. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In CVPR, 2023a. Tianhong Li, Dina Katabi, and Kaiming He. Self-conditioned image generation via generating representations. arXiv preprint arXiv:2312.03701, 2023b. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH, 2022. Fumin Shen, Chunhua Shen, Wei Liu, and Heng Tao Shen. Supervised discrete hashing. In CVPR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. 12 Preprint. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinite-vocabulary transformers. arXiv preprint arXiv:2312.02116, 2023. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 2008. Jingdong Wang, Ting Zhang, Nicu Sebe, Heng Tao Shen, et al. survey on learning to hash. IEEE TPAMI, 2017. Ze Wang, Jiang Wang, Zicheng Liu, and Qiang Qiu. Binary latent diffusion. In CVPR, 2023. Xiu-Shen Wei, Yang Shen, Xuhao Sun, Han-Jia Ye, and Jian Yang. A2-net: Learning attribute-aware hash codes for large-scale fine-grained image retrieval. In NeurIPS, 2021. Dayan Wu, Qi Dai, Jing Liu, Bo Li, and Weiping Wang. Deep incremental hashing network for efficient image retrieval. In CVPR, 2019. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR, 2022a. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. TMLR, 2022b. Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In ICLR, 2024. Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image BERT pre-training with online tokenizer. In ICLR, 2022. 13 Preprint."
        },
        {
            "title": "A ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "Model configuration The configuration settings for the model architecture, training and inference of BiGR across different model sizes are provided in Tab. 7. Unless otherwise specified, our model uses this default setting in the main paper. Table 7: The default configuration settings of three models: BiGR-L, BiGR-XL, BiGR-XXL. BiGR-L BiGR-XL BiGR-XXL Config Value Config Value Config Value Architecture Transformer layers Transformer heads Transformer dimensions MLP layers MLP dimensions Training Batch size Epochs Weight decay Learning rate Total diffusion timesteps Inference CFG scale Sampling iterations Gumbel temperature Diffusion timesteps 24 16 1024 3 1024 400 2e-2 1e-4 256 2.5 20 0.17 100 Architecture Transformer layers Transformer heads Transformer dimensions MLP layers MLP dimensions Training Batch size Epochs Weight decay Learning rate Total diffusion timesteps Inference CFG scale Sampling iterations Gumbel temperature Diffusion timesteps 36 20 1280 6 1280 512 400 2e-2 1e-4 256 2.5 25 0.25 100 Architecture Transformer layers Transformer heads Transformer dimensions MLP layers MLP dimensions Training Batch size Epochs Weight decay Learning rate Total diffusion timesteps Inference CFG scale Sampling iterations Gumbel temperature Diffusion timesteps 48 24 1536 8 512 400 2e-2 1e-4 256 2.5 25 0.30 100 Binary transcoder After producing Bernoulli distribution probabilities through Bernoulli denoising, there are two ways to obtain binary codes: deterministic and non-deterministic. For deterministic method, values are set to 1 if the probability exceeds 0.5, and 0 otherwise. In contrast, for non-deterministic methods, we sample directly from the Bernoulli distribution to obtain 0 and 1 values. We empirically compare these two methods, as shown in Tab. 8 and find that the nondeterministic approach slightly outperforms its deterministic counterpart. As result, we adopt the non-deterministic approach for all models presented in the main paper. Table 8: Comparison of deterministic and non-deterministic sampling. Determ. (Ours) FID IS sFID Pre. Rec. 3.19 3.17 239.79 262.14 6.25 5. 0.84 0.86 0.52 0.50 Sampling strategy In our sampling strategy, the implementation of Eq. (8) in the main paper may encounter nan issue caused by the logarithmic operation. Since we only need to compare the relative magnitudes of different entries, we can instead use value with the same trend to mimic the exact confidence. We use 2 pk 0.5 as the final confidence value in our implementation. Adaptive LayerNorm We empirically find that adaptive LayerNorm (adaLN) has marginal effect on performance. Following the approach in Tian et al. (2024), we implement shared adaLN that uses single MLP to obtain the shift, scale, and gate values for all transformer layers, which adds only minimal number of parameters. Linear probe Following the linear-probe evaluation protocol outlined in (He et al., 2022), we use the LARS optimizer with momentum of 0.9. We train the linear head for 100 epochs, using batch size of 256 along with 8 gradient accumulation steps. We use warm-up period of 10 epochs and set the learning rate to 0.1. An extra BatchNorm layer is added before the linear classifier, without affine transformation. We refrain from using mixup, cutmix, drop path, or color jittering, and the weight decay is set to zero. We use the same linear-probe setting for all compared models in the main paper. Preprint. Table 9: Model comparison of generative performance on ImageNet-1K. Metrics include Frechet inception distance (FID), inception score (IS), precision (Pre.) and recall (Rec.). All models are tested on 256256 ImageNet-1K benchmark. The suffix -re denotes the use of rejection sampling. Type Model #Params. FID IS Pre. Rec. GAN Diffusion BigGAN (Brock, 2018) GigaGAN (Kang et al., 2023) StyleGanXL (Sauer et al., 2022) LDM-4 (Rombach et al., 2022) DiT-L/2 (Peebles & Xie, 2023) DiT-XL/ Mask. MaskGIT (Chang et al., 2022) MaskGIT-re VQGAN (Esser et al., 2021) VQGAN VQGAN-re ViT-VQGAN (Yu et al., 2022a) ViT-VQGAN-re RQTran. (Lee et al., 2022) RQTran.-re VAR-d16 (Tian et al., 2024) VAR-d20 VAR-d24 VAR-d30 MAR-B (Li et al., 2024) MAR-L MAR-H LlamaGen-B (Sun et al., 2024) LlamaGen-L LlamaGen-XL LlamaGen-XXL LlamaGen-3B BiGR-L-d24 BiGR-XL-d24 BiGR-XXL-d24 AR VAR MAR AR Ours 112M 569M 166M 400M 458M 675M 227M 227M 227M 1.4B 1.4B 1.7B 1.7B 3.8B 3.8B 310M 600M 1.0B 2.0B 208M 479M 943M 111M 343M 775M 1.4B 3.1B 336M 799M 1.5B 6.95 3.45 2.30 3.60 5.02 2.27 6.18 4.02 18.65 15.78 5.20 4.17 3.04 7.55 3. 3.30 2.57 2.09 1.92 2.31 1.78 1.55 5.46 3.81 3.39 3.09 3.05 2.71 2.49 2.36 224.5 225.5 265.1 247.7 167.2 278. 182.1 355.6 80.4 74.3 280.3 175.1 227.4 134.0 323.7 274.4 302.6 312.9 323.1 281.7 296.0 303.7 193.6 248.3 227.1 253.6 222.3 275.7 278.8 277. 0.89 0.84 0.78 - 0.75 0.83 0.8 - 0.78 - - - - - - 0.84 0.83 0.82 0.82 0.82 0.81 0. 0.83 0.83 0.81 0.83 0.80 0.84 0.84 0.83 0.38 0.61 0.53 - 0.57 0.57 0.51 - 0.26 - - - - - - 0.51 0.56 0.59 0.59 0.57 0.60 0.62 0.45 0.52 0.54 0.53 0.58 0.53 0.55 0.55 ADDITIONAL SYSTEM-LEVEL COMPARISON We provide more comprehensive comparison of different leading models. We compare generative performance in Tab. 9 and discriminative performance in Tab. 10."
        },
        {
            "title": "C ADDITIONAL GENERATED SAMPLES",
            "content": "We provide additional 512512 samples and 256256 samples generated by our model in Fig. 7. We also include uncurated generated samples from various classes in Fig. 8 to 19."
        },
        {
            "title": "D ETHICS STATEMENT",
            "content": "We recognize the ethical risks of image generation, such as potential misuse for harmful content. Our research aims to promote positive uses like creativity and education, with commitment to responsible application. Safeguards and continuous ethical oversight are strongly encouraged. 15 Preprint. Table 10: Linear-probe top-1 accuracy on ImageNet-1K. MIM denotes masked image modeling. : our evaluation results. Method #Tokens Params ACC1 CPC v2 (Henaff, 2020) MoCo (He et al., 2020) SimCLR (Chen et al., 2020b) SwAV (Caron et al., 2020) DINO (Caron et al., 2021) BYOL (Grill et al., 2020) CAE (Chen et al., 2024b) CMAE (Huang et al., 2023) I iBOT (Zhou et al., 2022) BEiT (Bao et al., 2022) MAE (He et al., 2022) MAGE (Li et al., 2023a) BiGAN Donahue et al. (2017) t v r G BigBiGAN (Donahue & Simonyan, 2019) BigBiGAN iGPT-L (Chen et al., 2020a) iGPT-L ViT-VQGAN-B (Yu et al., 2022a) ViT-VQGAN-L RCG (Li et al., 2023b) l-DAE (Chen et al., 2024c) LlamaGen-L (Sun et al., 2024) MAR-B (Li et al., 2024) MAR-L MAR-H BiGR-L-d20 (Ours) BiGR-XL-d32 (Ours) . . C - - - - - - - - - 1616 1414 16 - - - 3232 4848 3232 3232 1616 - 1616 1616 1616 1616 1616 1616 303M 375M 375M 93M 85M 375M 304M 86M 304M 307M 304M 328M 138M 86M 344M 1.4B 1.4B 650M 1.7B 304M 304M 343M 208M 479M 943M 336M 799M 71.5 68.6 76.5 75.3 75.3 78.6 78.1 73.9 81.0 73.5 75.8 78.9 31.0 56.6 61.3 60.3 65.2 65.1 73.2 77.6 75.0 40.5 57.9 59.1 60.0 67.5 69.8 16 Preprint. Figure 7: Additional generated 256256 and 512512 samples. 17 Preprint. Figure 8: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Jay (17) Figure 9: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Loggerhead, loggerhead turtle, Caretta caretta (33) Preprint. Figure 10: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Common iguana, Iguana, Iguana iguana (39) Figure 11: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Macaw (88) 19 Preprint. Figure 12: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Golden retriever (207) Figure 13: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Siberian husky (250) 20 Preprint. Figure 14: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Otter (360) Figure 15: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Madagascar cat, ring-tailed lemur, Lemur catta (383) Preprint. Figure 16: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Altar (406) Figure 17: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Space Shuttle (812) 22 Preprint. Figure 18: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Alp (970) Figure 19: Uncurated 256256 samples. Model: BiGR-XXL-d32 Class label = Volcano (980)"
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong"
    ]
}