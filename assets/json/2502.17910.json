{
    "paper_title": "Scaling LLM Pre-training with Vocabulary Curriculum",
    "authors": [
        "Fangyuan Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size. Our method alternates between entropy-guided vocabulary expansion and model optimization, enabling models to learn transferable representations across diverse tokenization granularities. This approach naturally gives rise to an optimal computation allocation pattern: longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts. Experiments on small-scale GPT models demonstrate improved scaling efficiency, reinforcing the effectiveness of dynamic tokenization. We release our code to support further research and plan to extend our experiments to larger models and diverse domains."
        },
        {
            "title": "Start",
            "content": "Scaling LLM Pre-training with Vocabulary Curriculum Fangyuan Yu Temus"
        },
        {
            "title": "Abstract",
            "content": "Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size. Our method alternates between entropy-guided vocabulary expansion and model optimization, enabling models to learn transferable representations across diverse tokenization granularities. This approach naturally gives rise to an optimal computation allocation pattern: longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts. Experiments on small-scale GPT models demonstrate improved scaling efficiency, reinforcing the effectiveness of dynamic tokenization. We release our code to support further research and plan to extend our experiments to larger models and diverse domains."
        },
        {
            "title": "Introduction",
            "content": "5 2 0 2 5 2 ] . [ 1 0 1 9 7 1 . 2 0 5 2 : r Figure 1: Scaling better with vocabulary curriculum Modern language model pre-training relies on static vocabularies, fixed before training and detached from the models learning dynamicsunlike human language acquisition. This fixed approach limits Preprint. Under review. models ability to adapt to different levels of linguistic granularity, potentially hindering efficiency and performance. While humans acquire language hierarchically, starting with basic units before building more complex representations, language models typically operate with predetermined tokenization schemes. Our approach dynamically merges predictable tokens, enabling the model to allocate computational resources more efficiently and shift focus toward harder-to-predict patterns. This results in an adaptive curriculum that evolves alongside the models capabilities. The vocabulary curriculum learning strategy begins with basic units (characters) and progressively expands to more complex representations, allocating more capacity to regions of high modeling entropy and refining the models understanding of difficult linguistic structures. digram of our approach is provided in 2 Empirical results from pre-training GPT models [10] on the enwiki8 dataset [11] highlight two key advantages of our vocabulary curriculum learning approach: 1. It improves model performance across various vocabulary sizes, consistently achieving lower bits-per-character (BPC) compared to traditional fixed-vocabulary training. 2. It enhances scaling efficiencymodels trained with vocabulary curriculum exhibit shallower slope (0.109 vs. 0.147) in log-scale vocabulary size vs. BPC plots, indicating more effective utilization of larger vocabularies. As shown in Figure 1, models trained with incremental vocabulary curriculum learning (red) exhibit steeper improvement curve compared to compute-matching baselines (blue). The log-scale vocabulary size vs. bits-per-character (BPC) plot reveals that vocabulary curriculum learning achieves slope of -0.147, meaning it leverages larger vocabularies more effectively than compute-matching learning, which only reaches -0.109. Additionally, we observe that the curated vocabulary naturally forms hierarchical structure, where longer tokens become increasingly predictable (lower BPC), while shorter tokens remain harder to predict (higher BPC). This structural organization emerges organically from our training process, reinforcing the effectiveness of our dynamic tokenization strategy. Our key contributions are: dynamic vocabulary creation system that adapts based on model entropy curriculum learning approach for tokenization that improves scaling efficiency Evidence that hierarchical token organization emerges naturally from our approach While our focus is on language modeling, we believe this scaling effect can generalize to other modalities and domains, as byte sequences serve as the fundamental building blocks of digital data."
        },
        {
            "title": "2 Relevant Work",
            "content": "2.1 Tokenization Methods and Limitations Standard tokenization approaches like Byte Pair Encoding (BPE) [8], [9] rely on static co-occurrence statistics detached from model learning. This creates representational limitations, particularly evident in early language models struggles with mathematical operations [10]. Naive BPE tokenization produces inconsistent representations of numbersfor example, \"711\" might be encoded as single token while \"703\" requires multiple tokens. This inconsistency makes it harder for models to learn arithmetic operations compared to specialized approaches that assign unique tokens to all 1-3 digit integers [7]. Even with fixed vocabulary, different encoding strategies can produce varying segmentations of the same text. BPE-dropout [2] leverages this property by introducing stochasticity during training, showing improvements in neural machine translation. More recent work exploits segmentation equivariance during inference to enhance reasoning through self-consistency [3]. Additionally, research [5] has established the existence of optimal vocabulary sizes for BPE-style tokenization, which correlate with model size, log-linear relationship is observed between perplexity and vocabulary size. 2 Figure 2: Scaling better with vocabulary curriculum 2.2 Curriculum Learning Curriculum learning [13] progressively increases task difficulty during training to improve model performance. While successful in LLM post-training [15, 21], effective curriculum strategies for pretraining remain challenging [16]. Previous attempts at vocabulary-based curricula for decoder-only models found no improvements [19], highlighting the difficulty of designing effective curricula for language model pre-training. Our work addresses these limitations with novel adaptive approach to vocabulary curriculum. 2.3 Entropy Aware Tokenization Recent work has begun exploring entropy-aware tokenization. The Byte Latent Transformer [4] builds tokenization vocabularies using separately trained small language models. However, this approach creates vocabularies that are detached from the actual models entropy patterns and cannot be dynamically updated during training. Our work differs by integrating vocabulary building directly into the training process, allowing the tokenization scheme to evolve with the models developing understanding of the text. This creates true curriculum that adapts to the specific learning trajectory of each model, rather than imposing static or pre-computed vocabulary structure."
        },
        {
            "title": "3 Approach",
            "content": "Given text corpus consisting of numerous character sequences, where each sequence x1:m consists of characters xi (or bytes). vocabulary and an encoding function e(x1:mV) together define tokenization scheme that converts character sequences to token sequences s1:n. Language modeling then focuses on predicting the next token: p(sns1:n1) through minimizing entropy H(sts1:t1). We propose dynamic tokenization framework that jointly learns the vocabulary and encoding strategy alongside the language model. Our approach consists of two key components: (1) entropy-guided vocabulary update and (2) vocabulary curriculum learning. 3.1 Entropy-Guided Vocabulary Update Given trained language model , we identify mergeable token sequences based on their predictability. For sequence (s1, s2, . . . , sn), we compute the entropy H(sts1:t1) for each token. sequence is considered mergeable if all tokens after the first position exhibit monotonically decreasing entropy below threshold Ïµ: 3 mergeable(s1:n) > 1 : H(sts1:t1) < H(st1s1:t2) H(sts1:t1) < Ïµ The vocabulary update process can either increase or decrease the vocabulary size: Vk+1 = (cid:26)add(Vk, f, D) reduce(Vk, ntarget) for vocabulary expansion for vocabulary reduction where ntarget is the target vocabulary size. For each new token added to Vk+1, we expand the models embedding layer WE RVd and language modeling head WL RVd: WE[vnew] = h(L) , WL[vnew] = WL[vt] (1) where h(L) represents the final hidden state for the merged sequence. Figure 3: Token grouping process based on entropy patterns from trained character-level language model Unlike BPE which prohibits merges across space characters, our encoding function e(x1:mV) allows unrestricted merging. The encoding process identifies longest valid token sequences using sliding window approach, optimized through trie structure for efficient prefix matching. To speed up encoding speed for long sequences, we employ batch encoding with additional tokenization at batch boundaries, considering connection sequences of length up to maxvV v. The vocabulary management preserves several invariants: (1) non-leaf tokens are preserved during removal to maintain dependencies, (2) token indices reflect merge dependencies where tokens with smaller indices cannot be merges of tokens with larger indices, and (3) token indices align with rows in WE and WL, enabling vocabulary reduction through prefix slicing. 3.2 Vocabulary Curriculum Learning The curriculum learning process starts with the base vocabulary V0 = (the character alphabet) and alternates between model optimization and vocabulary updates. At each stage k: 1. Model Training: Train the language model with current vocabulary Vk using cross-entropy loss: Lk = (cid:88) (cid:88) x1:mD log p(sts1:t1; Vk) where s1:n = e(x1:mVk) is the encoded sequence. 2. Vocabulary Update: Based on the trained models entropy patterns, either expand the vocabulary through entropy-guided merging or reduce it through prefix slicing as defined in the previous section: Vk+1 = (cid:26)add(Vk, f, D) reduce(Vk, ntarget) for expansion phase for reduction phase 4 This iterative process continues until reaching the desired model performance or vocabulary size constraints."
        },
        {
            "title": "4 Experiments",
            "content": "We investigate two key questions: (1) Is learning transferable across different vocabularies? and (2) Does vocabulary curriculum improve model performance? 4.1 Experimental Setup We evaluate our approach on cleaned version of enwiki8 dataset using small GPT architecture (context length 512, 6 layers, 6 attention heads, embedding dimension 384, 10M parameters). The initial vocabulary V0 consists of 92 characters, and the model is trained with dropout 0.2 without bias terms. For vocabulary updates, we set the entropy threshold Ïµ = 0.3 and limit per-iteration vocabulary growth to 3K tokens. 4.2 Incremental Vocabulary Curriculum Our primary experiment consists of 5 iterations of vocabulary expansion, starting from base model with minimal vocabulary (92) and progressively training models with larger vocabularies (4359, 7941, 11382, 14819, 18276). Each iteration uses the previous models checkpoint for vocabulary addition. We cap the vocabulary at 18K based on compute-matching experiments showing performance deterioration beyond this size, aligning with observations in [5] that optimal vocabulary size correlates with model size. Figure 4: Incremental vocabulary learning shows noticeable improvement which scales with vocabulary size in log-linear fashion To isolate curriculum effects from training duration, we compare against compute-matching baselines where models are trained from scratch with equivalent total iterations. As shown in Figure 4, training with progressively increasing vocabulary reveals log-linear relationship between vocabulary size and Bits Per Character (BPC), with curriculum learning demonstrating steeper improvement curve compared to baseline training, detailed BPC at each iteration is documented in 1 5 Table 1: Comparison of BPC Values Across Different Vocabulary Sizes Method Vocabulary Size 92 4,359 7,941 11,382 14,819 18, incre_vocab_curriculum 1.7141 1.7141 compute_matching 1.5131 1.5303 1.4385 1.5103 1.4032 1.5035 1.3853 1.4780 1.3764 1. % Improvement 0.00% 1.12% 4.75% 6.67% 6.27% 5.96% Figure 5: Longer tokens has smaller BPC, contributing to smaller global BPC 4.3 Analysis of Improvement Mechanisms To understand the source of these improvements, we analyze per-token BPC distributions across different checkpoints. Figure 5 shows that at vocabulary size 4359, longer tokens consistently achieve better compression rates, validating our entropy-aware token addition approach. Further analysis across iterations (Figure 6 and Table 2) reveals two key patterns: 1. Newly created tokens are progressively longer and achieve lower BPC 2. Original shorter tokens become more challenging to model, showing slight BPC increases Token Group Iter 0 tokens Iter 1 tokens Iter 2 tokens Iter 3 tokens Iter 4 tokens Iter 5 tokens Iter 0 2.26 - - - - - Iter 1 4.20 1.11 - - - - Iter 2 4.24 1.22 0.78 - - - Iter 3 4.47 1.30 0.83 0.77 - - Iter 4 4.60 1.41 0.88 0.80 0.76 - Iter 5 4.82 1.47 0.91 0.84 0.80 0.76 Table 2: BPC values across training iterations. Each row represents tokens introduced at specific iteration, while columns show how their BPC values change in subsequent iterations. Note the increasing BPC trend for early tokens (top rows) and lower initial BPC for tokens introduced later (bottom rows). This suggests that our curriculum enables the model to effectively learn hierarchical patterns, with longer tokens capturing predictable sequences while shorter tokens specialize in harder-to-predict contexts. 6 Figure 6: Longer tokens has smaller BPC, contributing to smaller global BPC Notably, when testing decremental vocabulary curriculum (reducing vocabulary size over time), we observe performance comparable to direct training but without additional improvements, suggesting that the benefits of curriculum learning are specifically tied to the incremental approach. 4.4 Implication and Future work Optimal vocabulary size is correlated with model size [5], following this insight, we suspect the scaling improvement might be better for bigger model size. Well work on extending our experiments therein. The effectiveness of incremental vocabulary learning suggests its potential application in other modality than text, for instance, in bGPT [12] all digital files can be converted into byte sequences, where the scaling power of vocabulary curriculum could be leveraged to compress the context, as well as improve modeling accuracy."
        },
        {
            "title": "References",
            "content": "[1] R. D. Martinez, Z. Goriely, H. McGovern, C. Davis, A. Caines, P. Buttery, and L. Beinborn, CLIMB: Curriculum Learning for Infant-inspired Model Building, arXiv preprint arXiv:2311.08886, 2023. [2] I. Provilkov, D. Emelianenko, and E. Voita, BPE-Dropout: Simple and Effective Subword Regularization, Association for Computational Linguistics, pp. 18821892, 2020. [3] A. Sathe, D. Aggarwal, and S. Sitaram, Improving Self Consistency in LLMs through Probabilistic Tokenization, ICML2024, 2024. [4] A. Pagnoni, R. Pasunuru, P. Rodriguez, J. Nguyen, B. Muller, M. Li, C. Zhou, L. Yu, J. Weston, L. Zettlemoyer, G. Ghosh, M. Lewis, A. Holtzman, and S. Iyer, Byte Latent Transformer: Patches Scale Better Than Tokens, arXiv: 2412.09871, 2024. [5] C. Tao, Q. Liu, L. Dou, N. Muennighoff, Z. Wan, P. Luo, M. Lin, and N. Wong, Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies, NeurIPS 2024, 2024. [6] R. Tian, B. Li, X. Weng, Y. Chen, E. Schmerling, Y. Wang, B. Ivanovic, and M. Pavone, Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving, arXiv preprint arXiv:2407.00959, 2024. 7 [7] A. K. Singh and D. J. Strouse, Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs, arXiv: 2402.14903, 2024. [8] P. Gage, New Algorithm for Data Compression, The Users Journal, 1994. [9] R. Sennrich, B. Haddow, and A. Birch, Neural Machine Translation of Rare Words with Subword Units, in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 17151725, 2016. [10] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, Language Models are Unsupervised Multitask Learners, OpenAI Blog, 2019. [11] M. Hutter, The Human Knowledge Compression Prize, 2006. [12] S. Wu, X. Tan, Z. Wang, R. Wang, X. Li, and M. Sun, Beyond Language Models: Byte Models are Digital World Simulators, arXiv: 2402.19155, 2024. [13] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, Curriculum learning, in International Conference on Machine Learning, 2009. [14] Z. Lin, Z. Gou, Y. Gong, X. Liu, Y. Shen, R. Xu, C. Lin, Y. Yang, J. Jiao, N. Duan, and W. Chen, Rho-1: Not All Tokens Are What You Need, arXiv:2404.07965, 2025. [15] F. Yu, H. S. Arora, and M. Johnson, Iterative Graph Alignment, arXiv:2408.16667, 2024. [16] D. Campos, Curriculum learning for language modeling, arXiv:2108.02170, 2021. [17] A. Warstadt, L. Choshen, A. Mueller, A. Williams, E. Wilcox, and C. Zhuang, Call for Papers The BabyLM Challenge: Sample-efficient pretraining on developmentally plausible corpus, arXiv: 2301.11796, 2023. [18] F. Chollet, On the Measure of Intelligence, arXiv preprint arXiv:1911.01547, 2019. [19] S. Y. Feng, N. D. Goodman, and M. C. Frank, Is Child-Directed Speech Effective Training Data for Language Models?, arXiv preprint arXiv:2408.03617, 2024. [20] W. Chen, Z. Li, S. Xin, and Y. Wang, Squid: Long Context as New Modality for EnergyEfficient On-Device Language Models, arXiv preprint arXiv:2408.15518, 2024. [21] N. Lee, T. Wattanawong, S. Kim, K. Mangalam, S. Shen, G. Anumanchipalli, M. W. Mahoney, K. Keutzer, and A. Gholami, LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement, arXiv preprint arXiv:2403.15042, 2024."
        }
    ],
    "affiliations": []
}