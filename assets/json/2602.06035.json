{
    "paper_title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
    "authors": [
        "Sirui Xu",
        "Samuel Schulter",
        "Morteza Ziyadi",
        "Xialin He",
        "Xiaohan Fei",
        "Yu-Xiong Wang",
        "Liangyan Gui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 5 3 0 6 0 . 2 0 6 2 : r InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions Sirui Xu1 Samuel Schulter2 Morteza Ziyadi2 Xialin He1 Xiaohan Fei2 Yu-Xiong Wang1 1 University of Illinois Urbana-Champaign Liang-Yan Gui1 2 Amazon Equal Advising https://sirui-xu.github.io/InterPrior Figure 1. InterPrior is versatile generative controller instantiated as goal-conditioned policy that controls simulated humanoid to follow goal guidance and interact with objects in physics-based simulator. Three core, composable capabilities enable pursuing (I) longhorizon snapshot goals, (II) trajectory goals, and (III) contact goals (Top). Yellow, blue, and red dots respectively denote human, object, and contact goals. It demonstrates failure recovery (Bottom Left) from unsuccessful grasps. InterPrior enables steering control from human operator and can be applied to humanoid robot embodiments (Bottom Right). More demo videos are provided in the webpage."
        },
        {
            "title": "Abstract",
            "content": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, scalable framework that learns unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills fullreference imitation expert into versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into valid manifold, yielding motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment. 1. Introduction Human-object interaction (HOI) is inherently hierarchical: humans plan at high level with sparse intentions, while detailed limb coordination, balance, and contact emerge through fast, intuitive motor responses [61]. For instance, when reaching for bottle, we plan the hands target and object motion, while the rest of the body follows through subconscious coordination. Motion imitation policies [87] 1 have scaled to large HOI skills but rely on explicit planners for dense full-body and object references. In contrast, an interaction motor prior should sample feasible locomanipulation behaviors from distribution conditioned on sparse goals, e.g., next-second hand contact, rather than simply mimicking deterministic, fully specified trajectories. To model distribution over feasible loco-manipulation behaviors, early work [15, 44] learns generative controller via adversarial distributional matching and then uses reinforcement learning (RL) to promote task achievement under it. These methods can expand motion coverage beyond demonstrations, but are hard to scale due to unstable optimization, discriminator mode collapse, and handcrafted task objectives. An alternative is to distill reference imitation policies [37], with goal conditioning [59] achieved without task-specific design. While these approaches can absorb large-scale data, they can be brittle when reference coverage lags far behind the configuration spaceas in loco-manipulation, where even few object degrees of freedom can induce combinatorial explosion of contact modes and relative poses with different geometries. To address these limitations, we introduce InterPrior, physics-based HOI controller that is scalable along four (I) task coverage: single policy supaxes (Figure 1). ports multiple goal formulations, e.g., sparse targets and their compositions; (II) skill coverage: the same training recipe scales to large HOI data and enables affordance-rich interactions beyond simple grasping; (III) motion coverage: it generates expressive trajectories instead of merely reconstructing demonstrations; and (IV) dynamics coverage: it maintains task success under varied physical properties. Our key insight is that RL finetuning is essential for turning distillation from data reconstruction into robust, generalizable policy. Distillation alone cannot cover the full HOI configuration space, yet RL applied in isolation often drifts toward unnatural reward-hacking behaviors. We therefore use distillation to provide strong, natural initialization, and apply RL as local optimizer that improves robustness while remaining anchored to the pretrained model. Concretely, we leverage distillation to inherit broad skills from large-scale HOI demonstrations, by training masked conditional variational policy to reconstruct motor control from sparse, multimodal goals, distilled from reference imitation expert. We then RL finetune this policy to consolidate its latent skills into valid interaction manifold. The finetuning optimizes two objectives: improving success on unseen goals and initializations, and preserving pretrained knowledge through regularization. It leverages the pretrained base policy to synthesize natural in-between motions, with failure states to acquire recovery behaviors, e.g. re-approach and re-grasp. Together, these steps transform reconstructed latent skills into stable, continuous manifold that generalizes beyond the training trajectories. Our contributions are fourfold. (I) We present InterPrior, generalizable generative controller for physicsbased human-object interaction, encompassing diverse skills rather than fixed procedural routines (e.g., approach, (II) We develop an grasp, place) typical of prior work. RL finetuning strategy that enables robust failure recovery and goal execution across varied configurations while maintaining human-like coordination. The resulting controller supports mid-trajectory command switching, re-grasps after failures, and remains stable under perturbations. (III) We show that our finetuning strategy naturally extends to novel objects and interactions, functioning as reusable prior. (IV) We demonstrate embodiment flexibility by training on the G1 humanoid [64] with sim-to-sim evaluation and enabling real-time control via keyboard interfaces. 2. Related Work Data-driven human interaction animation has progressed from kinematic models assuming simplified object dynamics [66, 99, 103] to methods generating whole-body motions with dynamic objects [5, 8, 13, 14, 16, 19, 21, 22, 25, 33, 45, 49, 50, 74, 76, 83, 84, 88, 92, 97]. However, these kinematic approaches often exhibit implausible contact drift and interpenetration. Such limitations partly arise from existing HOI datasets [3, 18, 20, 26, 28, 31, 34, 40, 78, 81, 95, 96, 98, 102], which contain spatial or physical inconsistencies that impede the learning of realistic interactions. Physicsbased methods seek to address this gap but often rely on early curated datasets [56] focusing on limited yet highfidelity hand-centric manipulations [37, 59, 71]. Recent advances in humanoid hardware [2, 10, 24, 55, 104] have begun to bridge the virtual and physical domains, though typically without too much agility. Together, these developments highlight the need for scalable HOI priors, models capable of generalizing across tasks, remaining robust to imperfect data, and synthesizing physically realistic HOIs. 2.1. Physics-based Character Animation Physics-based character animation learns simulated controllers via RL, e.g., tracking reference motions [46, 101]. Scalability has been improved through multi-clip trackers with reference planners [23, 69, 72] without or with closedloop schemes [60, 82]. Nevertheless, such controllers remain constrained by their reference motion planners, making them fragile when the planned motions are dynamically unstable, very common issue in HOI, where kinematic planners often neglect physical feasibility. Learned generative priors address this limitation by encoding physically plausible motor memory encoded into policies. One line of research employs adversarial imitation with discriminators [47] to learn the motor prior, and later extends to skill embeddings [48] and conditional control [9, 57]. These approaches promote motion diversity but remain sample2 inefficient and challenging to scale. complementary line distills motor skills into compact latent codes. Earlier work adopts model learning to train variational autoencoder (VAE) [27] based controller [11, 73, 89, 90], while recent studies pretrain universal trackers [35] and distill them into latent priors [36], masked policies [58], or offline training with diffusion models [17, 63, 75]. Yet, these methods are often limited by the expert converage. Our InterPrior synergizes the strength of both lines: it first distills large-scale motion imitators and finetunes it via RL, bridging generative controller with versatile conditions while enhancing the control by alleviating out-of-distribution brittleness. 2.2. Physics-based Human-Object Interaction Advances in physics-based character control have progressively expandeded the scope of HOI animation. Early approaches primarily focus on simple object dynamics, such as striking or sitting [4, 6, 43, 48, 77], whereas recent developments have extended to complex, scenario-specific sports and games [1, 30, 38, 67, 68, 71, 79, 93]. Progress has also been observed in generalizable tasks, such as object carrying and rearrangement [7, 12, 15, 29, 42, 44, 54, 70, 94, 100], predominantly enabled by adversarial imitation learning, while most systems remain skill-specific, relying on fixed procedural routines (e.g. approach, grasp, place with regular-shaped objects). They struggle to adapt to objects that require careful affordances and fine-grained interaction skills (e.g., grasping chair bar with one hand). To address these limitations, HOI motion imitation [76, 80, 87, 91] has emerged as promising paradigm for scaling skill repertoires and capturing fine-grained interactions, as it directly emphasizes precision and stability. Distilling such imitation policies therefore represents crucial step toward establishing versatile HOI controller. However, existing efforts often exhibit narrow task coverage, emphasizing single-object proficiency [91] or relying on curated dataset with lowdynamic and hand-centric skills [37, 39, 59]. Our InterPrior provides principled solution for generalizing generative controller for agile whole-body loco-manipulation. 3. Methodology Task Formulation. We aim to learn policy π that operates in physics simulator and produces human-object interaction motion from high-level goals rather than full reference. Such goals can be extracted from human user (e.g., steering control), HOI kinematic motion generator (see Sec. F), or keypoints from Motion Captured (MoCap) data. The policy π conditions on the current human-object state and recent history together with these goals, and samples control signals from its learned distribution to drive the simulated human or humanoid to interact with the object. The outcome is rollout motion sequence that is physically simulated, follows the provided goals where available, and Figure 2. Overview of the proposed InterPrior framework. It consists of: (I) full-reference imitation expert training on large-scale human-object interaction data; (II) distillation of the expert into variational policy with structured latent space for skill embeddings; and (III) post-training of the variational policy to enhance generalization. Blue modules denote the final policy used at inference; green and red modules are training-only components, and red arrows denote supervision signals (rewards/losses). remains diverse and natural in aspects that are not specified. Overview. Figure 2 illustrates our three-stage paradigm. First, we train an expert policy πE for large-scale HOI motion imitation, incorporating data augmentation, physical perturbations, and shaped rewards to promote stable wholebody coordination and precise grasping across diverse configurations (Sec. 3.2). Second, we distill the expert into masked conditional variational policy π that maps sparse goal inputs to multi-modal distribution (Sec. 3.3). Third, we finetune this policy π using RL to enhance robustness under unseen configurations, employing failure-state resets to encourage recovery behaviors (Sec. 3.4). Each stage is modeled as Markov Decision Process (MDP), which shares consistent input formulation comprising observations and goal conditioning, as well as an output action corresponding to low-level actuation commands (Sec. 3.1). 3.1. Policy States and Actions Observation. The policy input at time includes an observation that aggregates human kinematics, object kinematics, and their interaction and contact states, xt = (cid:3). Here, the su- (cid:2) rh , θ (cid:124) , ro (cid:124) , θh , Dt, Ct (cid:124) (cid:123)(cid:122) (cid:125) (cid:125) interaction , rh (cid:123)(cid:122) human , ro (cid:123)(cid:122) object , θo , θ (cid:125) perscripts and denote human and object quantities, respectively. and θ denote positions and orientations, respectively; the dotted terms indicate linear and angular velocities. The interaction terms include signed distances from body segments to object surfaces Dt and binary contacts Ct derived from simulator contact forces, following [87]. All continuous quantities are normalized in human root-centric and local heading frame for invariance to global placement. The human-related terms contain 52 components for the SMPL humanoid [35] and 39 for the 3 Unitree G1 robot [64]. Each rigid body contributes one element to human-related variables in xt, including Dt and Ct, e.g., Dt R393 for G1. Objects are all rigid. Goal Conditioning. The policy is also conditioned on set of future goals that specify desired human-object configurations at different horizons. During training, we extract goals from reference, where each reference yt shares the same state space as observation xt, including human, object, and contact components. corresponding binary mask mt indicates which components of the reference are provided to the policy [58]. To capture both near-term and distant intentions, we employ two types of goal conditioning: (I) short-horizon preview sequence and (II) long-horizon snapshot. Let denote the maximum prediction horizon, {1, . . . , H} set of short-horizon offsets, and long-horizon offset. The long-horizon offset is initialized randomly, decremented by one at each timestep, and re-sampled when it reaches zero. For each {L}, we retrieve (yt+k, mt+k), where the mask mt+k is sampled to cover every possible condition e.g., end-effector pose, object pose, human-object contacts, their combination, etc. (see Sec. for details of the sampling). Each goal is represented using masked residual encoding: (cid:1), Gt = { (yt+k, mt+k) yt+k = mt+k (cid:0)yt+k, xt {L} }, where denotes elementwise masking and applies log-map to rotational components and subtraction to Euclidean quantities. During inference, user-specified or model generated sparse targets can be supplied by filling only the informed components, setting the corresponding mask to one, and zeroing the rest. Action. The policy outputs an action vector at, defining the actuation as at R513 for SMPL [32, 51] and at R29 for the G1 humanoid [64]. Each action represents joint position target expressed in the exponential map, which is subsequently converted into joint torques via proportionalderivative (PD) control. The resulting torques are applied to the corresponding joints in the physics simulator, driving the human-object interactions and generating the next state xt+1 according to the simulators dynamics. 3.2. InterMimic+: Full-Reference Imitation Expert Serving as the teacher for the final policy π, we formulate large-scale co-tracking of human and object motions following InterMimic [87]. At each timestep t, the expert policy πE receives the observation along with future references, which contain complete information without masking. The policy outputs low-level actuation commands at and is trained using Proximal Policy Optimization (PPO) [53] to maximize composite reward function: = rtrack renergy, where rtrack promotes alignment between the reference yt and simulation state xt, and renergy encourages physically plausible and efficient behaviors. This formulation enforces strict adherence to the reference. The policy from the original InterMimic achieves highfidelity imitation and broad loco-manipulation coverage. However, in practice, we observe key issues due to the policys strong reliance on references, which we address with our advanced version. (I) The policy shows degradation of precision when interacting with thin or small objects, as it tends to rigidly follow reference trajectories (See Figure 3) (II) without utilizing fine-grained hand-object relations. This limitation is more severe if the rollout deviates from reference trajectories. To mitigate these issues, we expand reference scope and introduce reference-free rewards. Expanding Reference Scope. To reduce reliance on reference trajectories, we apply randomization, perturbation, and augmentation. We initialize each episode from reference frames with random variations in human-object poses. During rollouts, we apply sparse impulses, i.e., random velocity perturbations to the pelvis and object, to induce deviations from the references. We augment object shapes and randomize physical properties such as mass density, centerof-mass offsets, inertia, and friction, with details presented in Sec. E. This exposes the policy to diverse dynamics, without alternating the reference. Unlike common sim-to-real practices, we do not randomize actuation parameters or add observation noise, as these do not directly enhance state or dynamics coverage. However, perturbations alone are insufficient; it is necessary to introduce termination penalty that discourages the policy from entering failure under perturbation. We define rter = wter cter, where cter is triggered by human fall or large deviations in states from references, following [87], and wter is scaling coefficient. Reference-Free Reward. key challenge in precise hand grasping under randomization and perturbation is that strict reference-based tracking becomes unreliable. To address this, we introduce hand reward rh that encourages the hand to target and wrap around the object based on the current simulation state, rather than relying on reference trajectories. Details of the formulation can be found in Sec. D. When combined with the reference imitation reward, it serves as corrective term that guides the hand to orient, align, and close around the actual object, potentially deviated from the reference due to perturbations, rather than strictly following the reference trajectory. The full reward is defined as rt = (rtrack renergy rh) + rter. 3.3. InterPrior: Variational Distillation Given an imitation expert policy πE (Sec. 3.2) trained to master motor skills for HOI, our objective is to distill it into variational policy π. Unlike the expert policy πE, which operates under densely supervised and fully observed reference trajectories, the variational policy π must preserve naturalness and diversity with sparse cues. This is achieved by sampling from latent skill distribution, which endows π with the capacity to generate plausible variations in action 4 space. Our framework builds upon [58, 85] with two new designs: (I) multi-modal conditioning, including contact for versatile human-object conditioning, and (II) prior shaping and bounding regularization for robustness. Model. We model the policy π with latent zt Rdz to for multi-modality. As shown in Fig. 2, π consists of: Prior: Encoder: Decoder: pψ(zt xtℓ:t, Gt), qϕ(zt xt, Gt, yt:t+H , yt+L), fθ(at xtℓ:t, zt). The encoder is an MLP used only during training; given the full future reference, it outputs Gaussian (µq, Σq). In parallel, prior Transformer encodes recent history, with history length ℓ, and sparse goal, producing Gaussian (µp, Σp). Following [58], we form residual posterior (µp + µq, Σq). During training we sample the latent skill via reparameterization: zt = (µp + µq) + Σ1/2 ϵ, ϵ (0, I), and hold ϵ fixed within an episode to promote temporally consistency [89]. During inference, only the prior is used to sample zt (µp, Σp). The decoder MLP maps the latent and observation to the action. The decoder also includes an auxiliary head during training that reconstructs the masked entries of the goal, encouraging meaningful latent space by learning to complete intent from context. Bounding the Latent. To improve robustness and prevent unnatural behaviors induced by out-of-distribution latents, after sampling we project zt zt/zt so that the policy operates on hypersphere, following [48]. This simple normalization stabilizes skill learning by limiting the rare latent draws while preserving directional variability for multimodal behaviors. Note that we apply the projection after sampling, thus KL regularization can still be computed on the Gaussian pψ and qϕ before projection. Online Distillation and Regularization. We utilize an online distillation framework following DAgger [52], where the student policy π learns from mixture of expert πE and self-generated rollouts. Training begins with trajectories fully controlled by the expert πE, and the ratio of student-driven states is gradually increased as learning progresses. At each step, the expert provides its action output as supervision for the student. The policy is optimized using composite objective consisting of multiple loss terms: Ltotal = LELBO + λscale Lscale + λtc Ltc. The primary objective, LELBO, is weighted evidence lower bound [27] that combines three components: (I) an imitation loss encouraging the student to reproduce expert actions, (II) goal reconstruction loss promoting accurate completion of masked goal entries to align with the ground truth, and (III) KL regularization loss that penalizes divergence between the posterior (µp + µq, Σq) and the prior distribution (µp, Σp). We introduce two auxiliary losses to further shape the latent. Lscale constrains the prior mean µp to maintain unit magnitude, preventing degeneracy given hypersphere normalization. Ltc encourage consecutive prior distributions to remain similar across time steps. Details of these losses are provided in Sec. D. 3.4. InterPrior: Post-Training Beyond Reference The distilled policy π (Sec. 3.3) exhibits goal following, yet it is brittle when the goal or human-object state drifts off the dataset distribution, e.g., during transitions between skills. Unlike human-only motion [36] or small-object grasping [59], loco-manipulation tasks with coupled affordances span far larger configuration space that references alone cannot cover. This follows from the learning dynamtraining proceeds by replaying dataset ics of distillation: trajectories. Our key observation is that the pretrained π provides strong and natural initialization for RL finetuning as local optimizer that expands its scope along three axes: (I) recover from near-failure or failure states, (II) explore unseen yet plausible configurations without trajectory replay, and at the same time (III) preserve the naturalness of behaviors encoded by the pretrained policy. natural alternative is to sample novel multi-frame trajectories that combine diverse human, object, and contact configurations and then train the policy to track them [37], but this requires strong trajectory sampler, which is particularly challenging at loco-manipulation scale. Instead, we target singleframe goals: composing goals observed in data can induce unseen configurations, and we further combine such goals with randomized initializations and offsets to systematically broaden the state distribution encountered during RL. In-Betweening for Finetuning. To mitigate the cost of exhaustive trajectory sampling, we formulate finetuning as an in-betweening task, where the policy tracks from randomly sampled initial configuration toward single-frame goal randomly drawn from the dataset. The policy is rewarded for progressing toward this sampled goal. The reward is defined as, = (cid:0)renergy rh rPT (cid:40)rsucc, if (cid:13) 0, rgoal = otherwise. (cid:1) + rgoal + rter, (cid:13)mt+L (yt+L, xt)(cid:13) (cid:13)1 < τ, where the terms renergy, rter, and rh are defined in Sec. 3.2. Since the goal is arbitrary by the random masking, we do not use dense distance-based reward. The goal reward rgoal provides sparse success signal that activates when the masked feature distance between the current state xt and target yt+L falls below threshold τ . rsucc is constant. Learning New Skills. As shown in Figure 1, our RL finetuning can expand the distilled policy by handling two common regimes. (I) In-distribution extensions reuse and compose behaviors already supported by the demonstrations. representative example is regrasping, which arises nat5 training the urally from goal-conditioned in-betweening: policy to reach goals from diverse initializations and perturbed states encourages self-correction from near-failure (II) Out-ofoutcomes without additional supervision. distribution skills must be learned explicitly when the required behavior is absent from the dataset. representative example is getting up. Following prior practice [44, 65], we append learnable token to the (Sec. 3.3) to indicate this new subtask and add an auxiliary reward that encourages upright posture and center-of-mass elevation (Sec. D). Prior Preservation. During finetuning, rather than freezing network components to mitigate catastrophic forgetting as in prior work [44, 65], we adopt simple multi-objective schedule. Specifically, we maintain subset of environments that continue optimizing the original distillation objective (Sec. 3.3), while the remaining environments perform RL finetuning (Sec. D). This anchors the policy to the pretrained prior during adaptation without restricting model capacity. Given the environment mixtures and the joint execution of RL and distillation, we distribute tasks across multiple GPUs and aggregate gradients via map-reduce scheme. Further details are provided in Sec. D. 4. Experiments We evaluate InterPrior on two tasks: (I) full-reference tracking and (II) sparse goal following. The evaluation covers snapshot, trajectory, and contact specification, as well as their compositions. Since our goal representation is formed by masking arbitrary subsets of targets, these settings subsume broad family of task formulations, ranging from single-frame constraints to multi-step trajectories over different joints and contacts. We further study InterPrior as reusable prior for novel objects, and for tracking trajectories generated by kinematic models (Sec. F). Datasets. We employ the InterAct [86] dataset with its preprocessing, which features diverse daily interactions encompassing wide range of subjects and objects. Following [87], we use the OMOMO subset [28] repaired by their teacher rollout. To assess generalizability, we apply InterPrior to other InterAct subsets including selected data from BEHAVE [3] and HODome [95]. We exclude interactions dominated by soft-body dynamics (e.g., backpack shoulder straps) when choosing evaluation examples. Baselines and Tasks. We focus on baselines that cover diverse objects and skills and therefore omit methods that are for single object or task-specific proficiency [44, 71, 91]. (I) Full-reference tracking. We compare against the original InterMimic [87], with InterPrior, which supports fullreference imitation by removing masks. Evaluations target challenging regimes involving thin-object interactions and initialization noise. (II) Sparse goal following. We evaluate the complete InterPrior framework against adapted MaskedMimic [58, 59], to our task under identical goals, following Figure 1: (a) Snapshot goals: ground truth frame specifies few human joints or object position in the long term; (b) Trajectory goals: sequence of ground-truth keyframes defines the few joints or object trajectories; (c) Contact goals: contact schedule specifies the desired active contact regions on objects, which will be converted to goals for human joints; (d) Multi-goal chaining: To evaluate longhorizon robustness, we concatenate three randomly sampled ground-truth subgoals, each canonicalized with respect to the preceding one. The concatenated sequence may include mixture of snapshot, trajectory, and contact-following segments, with randomized goal transitions. For consistency, the same goals are used across all baselines; (e) Random initialization: To test motion coverage, we initialize the humanoid within five meters of the object and define the task as lifting the object by 0.5 meters from its initial position. Metrics. (I) Full-reference tracking. Following [87], we report the following metrics: (a) Success Rate (SR): the proportion of rollouts completed without violating the earlytermination criteria; (b) Human Position Error Eh (m): the mean per-joint positional deviation between the simulated and reference humans, excluding hands due to the missing ground truth from the dataset; and (c) Object Position Error Eo (m): the mean positional deviation between the simulated and reference objects. (II) Sparse goal following. The evaluation metrics include: (a) Success Rate (SR); (b) Human and Object Errors (Eh, Eo): the deviation from the target goal state, computed over the unmasked region; and (c) Failure Rate (Fail): proportion of rollouts that directly fail e.g., fall. More details are presented in Sec. F. Implementation Details. All control policies operate at 30 Hz in IsaacGym [41]. The imitation expert policy, along with the encoder and decoder used during distillation, are implemented as MLPs with hidden layers of (1024, 1024, 512). The prior network is four-layer Transformer encoder, and the critics use the same MLP architecture for expert training and RL finetuning. We retrain InterPrior on the G1 embodiment using our three-stage paradigm. During the first stage, we incorporate additional rewards and domain randomization to enhance stability on G1 and facilitate robust sim-to-sim transfer. All auxiliary rewards are multiplied with the imitation reward in exponential form exp( ), except for the termination term, which is added directly. The formulation of each G1-specific reward term is provided in Table C, and the dynamics randomization ranges used during training are summarized in Table D. We exclude thin-geometry objects for G1 because we do not include dexterous hands supporting single-hand grasps. 4.1. Quantitative Results (I) Full-reference tracking. Table 2 shows that InterPrior achieves higher success rates under thin-geometry interactions and initialization noise. While InterMimic attains 6 Figure 3. Qualitative comparison of same reference imitation between InterMimic [87] (top) and our InterMimic+ (bottom). InterMimic strictly follows the reference humanoid motion but fails to grasp the thin cloth stand when initialized with perturbations. Figure 5. Zero-shot qualitative results. single InterPrior model trained from OMOMO [28] demonstrates generalization to unseen objects and interactions from BEHAVE [3] and HODome [95]. Figure 4. Qualitative results on multi-object task. The model input is shifted to the second object once the first object is released. lower position error by strictly tracking the reference, InterPrior sometimes yields slightly higher human position error because it intentionally deviates when needed to re-align contact, trading strict tracking for interaction completion. (II) Goal-conditioned tasks. Under identical goal specifications  (Table 1)  , InterPrior consistently improves success and reduces errors, with the largest gains on long-horizon multi-goal chaining and random-initialization stress tests. Distillation-based policies (including InterPrior pre-RL) fit the demonstration-induced state distribution; long rollouts with goal switching can enter under-covered intermediate states, causing drift and failure. RL finetuning directly trains the policy to reach sparse targets from diverse initializations, improving interpolation across goal sequences and recovery from off-distribution states. The position error trends follow goal-sparsity continuum: broader state coverage benefits sparse goals more, and the gap narrows as goals densify. With full-reference tracking  (Table 2)  , InterMimic for strict tracking achieves the lowest errors. 4.2. Qualitative Results (I) Full-reference tracking. Figure 3 shows that InterMimic rigidly follows the reference but often fails to acquire or maintain contact on thin geometries under perturbations. In contrast, our tracking policy allows small, targeted deviations to correct hand-object alignment, producing stable grasps and more reliable completion. (II) Longhorizon tasks. Figures 4 and 1 show that InterPrior sustains minute-long whole-body interaction with multiple objects 7 Figure 6. Qualitative results on sim-to-sim from IsaacGym [41] to MuJoCo [62] with object trajectory as condition, showing sustained interaction involving box pickup, pushing, and kicking. and smooth transitions across skills (e.g., approach, grasp, lift, reposition). When drift begins (contact or balance), InterPrior self-corrects instead of compounding errors, consistent with the robustness induced by RL finetuning. (III) Novel objects and interactions. Figures 5 and 7 demonstrate zero-shot generalization to unseen objects and interaction styles. Guided only by sparse snapshot goals, InterPrior complete unspecified degrees of freedom and converge to feasible contact, even the original data in BEHAVE [3] and (IV) SimHODome [95] is for different human shape. to-sim transfer. Figure 6 illustrates transfer from IsaacGym [41] to MuJoCo [62]: InterPrior maintains coherent long-horizon interactions under object-conditioned goals, showing the potential to transfer to the real world. 4.3. Ablation Study We conduct cumulative ablation study reported in Table 1. Starting from MaskedMimic baseline with an InterMimic expert, we progressively enable the components of InterPrior: upgrading to an InterMimic+ expert, incorporating the latent shaping loss, bounding both latent and observation spaces, and finally applying RL finetuning. Impact of Latent Shaping and Bounding. Introducing the latent shaping loss yields modest improvements on indistribution tasks but provides clear gains for long-horizon behavior and under random initialization. This indicates that well-shaped and properly bounded latent is essential for mitigating drift in challenging, contact-rich interactions. Effectiveness of Finetuning. Comparing the full InterPrior model with the variant before finetuning shows that RL finetuning chiefly enhances robustness. The improvement is also more pronounced on stress tests, suggesting that fineFigure 7. Qualitative comparison between InterMimic [87] (left, full reference), MaskedMimic [58] (middle), and our InterPrior (right) on unseen and imperfect interactions from the BEHAVE [3] dataset. InterPrior can recover from data imperfection and continue the rollout. Table 1. Quantitative evaluation and ablation study on in-distribution goal-conditioned tasks, including snapshot, trajectory, contact (Figure 1), plus out-of-distribution stress tests on challenging scenerio, such as long-horizon multi-goal chains and object lifting under random human initialization. For the random initialization, only the object is assigned goal, thus the human error is omitted. Method Snapshot Trajectory Contact Chain Rand Init Variant Additions (cumulative) Succ Eh Eo Fail Succ Eh Eo Fail Succ Ec Eo Fail Succ Eh Eo Succ Eo MaskedMimic [58] InterMimic [87] as Expert 64.2 29.3 22.1 12.6 88. InterPrior (Ours) InterMimic+ as Expert + Latent Shaping Loss + Bounded Latent & Observations + RL Finetuning (= full) 71.4 74.9 89.1 90.0 18.6 11.7 11.0 20.4 15.5 10.6 8.9 11.7 6.0 3.7 9.5 13.6 92.7 92.4 93.6 94.6 9. 8.2 7.9 8.1 7.9 8.1 7.7 6.6 6.6 6.9 8.5 5.2 5.3 4.6 2.5 52. 49.2 25.7 13.9 29.1 40.2 43.9 31.7 26.8 69.3 71.9 88.5 90. 9.7 25.6 18.2 26.7 15.3 11.9 8.1 5.4 17.0 2.9 15.9 9.9 33.9 40.0 45.1 68.8 37.1 39.6 37.0 40.8 31.5 37.2 30.2 35.7 30.1 30.9 41.1 88.6 22.1 13.9 19.6 11.9 Table 2. Quantitative evaluation of full-reference imitation on OMOMO with thin objects and initialization perturbations, and adaptation to novel object and interaction skills, evaluated before and after finetuning on new data. For novel interactions, Eh and Eo not directly comparable since InterPrior now uses random sparse goals. Results show that InterPrior functions as reusable prior with stronger adaptation capability than the full-reference imitator. OMOMO [28] select BEHAVE [3] HODome [95] Method SR Eh Eo InterMimic [87] InterMimic + finetuning InterPrior InterPrior + finetuning 63.9 / 83.2 / 7.1 / 8.9 / 11.4 / 11.7 / SR 10.7 38.9 27.4 52.0 SR 27.8 55.5 40.1 72.4 tuning helps the policy exploring the feasible motion space and recover from distributional shift, while maintaining the policy with similar precision on standard tasks. Impact of Finetuning on Trajectory Following. As discussed in Sec. 3.4, our in-betweening finetuning is applied only on snapshot goals rather than full trajectories, which may raise concerns about degrading trajectory-following performance. However, as shown in Table 1, trajectory following is well preserved for two reasons: (I) the finetuning procedure does not alter the model under trajectoryconditioned inputs, which are explicitly protected by concurrent distillation loss; and (II) we redefine snapshot goal if deviations from the target trajectory appears, and thus trajectory-following can implicitly benefit from the RL finetuning on snapshot goal following. Scalable Prior. Beyond the generalization results in Figure 5, Table 2 and Figure 7 further demonstrate that InterPrior scales more robustly to novel objects and interactions, with or without finetuning, compared to the full-reference InterMimic baseline. key factor is the prevalent dataset imperfections. For example, in Figure 7, baselines fail as contact artifacts cause failure initialization, whereas InterPrior can re-establish contact and continue the task. This flexibility allows the learned model to better absorb additional interaction data, even when such data are imperfect. Failure Cases. Despite its improved robustness over the baselines, InterPrior still exhibits failure modes, as shown in Figure A. The human loses contact and moves without the object, whereas the baseline demonstrates significantly higher failure rate, often resulting in human fall. We find typical failure scenarios include: (I) challenges with extremely thin or elongated objects that were unseen during training; and (II) partial goal completion in multi-goal chaining, where canonicalization introduces large alignment discrepancies, leading the policy to favor maintaining balance over achieving precise goal configurations. 5. Conclusion We present InterPrior, physics-based generative motion controller that scales human-object interaction by combining large-scale imitation distillation with reinforcement finetuning. Using distilled, goal-conditioned latent policy and optimizing it with RL yields controller that maintains natural whole-body coordination while substantially improving robustness and competence. It composes loco-manipulation skills, transitions smoothly, and recovers from failures across diverse contact and dynamic conditions. This decoupled recipe broadens task, skill, and dynamics coverage while enabling interactive control and can be applied to different embodiments. We hope this scalable paradigm to provide practical recipe for humanoid locomanipulation. Future directions include integrating perception, language-conditioned goals, and richer affordances to advance InterPrior toward robust sim-to-real assistive manipulation and teleoperation."
        },
        {
            "title": "References",
            "content": "[1] Jinseok Bae, Jungdam Won, Donggeun Lim, Cheol-Hui Min, and Young Min Kim. Pmp: Learning to physically interact with environments using part-wise motion priors. In SIGGRAPH, 2023. 3 [2] Donghoon Baek, Amartya Purushottam, Jason Choi, and Joao Ramos. teleoperation with multi-stage object parameter estimation for arXiv preprint wheeled humanoid locomanipulation. arXiv:2508.09846, 2025. 2 Whole-body bilateral [3] Bharat Lal Bhatnagar, Xianghui Xie, Ilya Petrov, Cristian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll. BEHAVE: Dataset and method for tracking human object interactions. In CVPR, 2022. 2, 6, 7, 8, 1, 3 [4] Yu-Wei Chao, Jimei Yang, Weifeng Chen, and Jia Deng. Learning to sit: Synthesizing human-chair interactions via hierarchical control. In AAAI, 2021. 3 [5] Peishan Cong, Ziyi Wang, Yuexin Ma, and Xiangyu Yue. Semgeomo: Dynamic contextual human motion generation with semantic and geometric guidance. In CVPR, 2025. 2 [6] Jieming Cui, Tengyu Liu, Nian Liu, Yaodong Yang, Yixin Zhu, and Siyuan Huang. AnySkill: Learning openvocabulary physical skill for interactive agents. In CVPR, 2024. [7] Zekai Deng, Ye Shi, Kaiyang Ji, Lan Xu, Shaoli Huang, and Jingya Wang. Human-object interaction via automatically designed vlm-guided motion policy. arXiv preprint arXiv:2503.18349, 2025. 3 [8] Christian Diller and Angela Dai. CG-HOI: Contact-guided 3d human-object interaction generation. In CVPR, 2024. 2 [9] Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, and Wenping Wang. ase: Learning conditional adversarial skill embeddings for physics-based characters. In SIGGRAPH Asia, 2023. 2 [10] Yuhui Fu, Feiyang Xie, Chaoyi Xu, Jing Xiong, Haoqi Yuan, and Zongqing Lu. DemoHLM: From one demonstration to generalizable humanoid loco-manipulation. arXiv preprint arXiv:2510.11258, 2025. 2 [11] Levi Fussell, Kevin Bergamin, and Daniel Holden. Supertrack: Motion tracking for physically simulated characters using supervised learning. ACM Transactions on Graphics (TOG), 40(6):113, 2021. 3 [12] Jiawei Gao, Ziqin Wang, Zeqi Xiao, Jingbo Wang, Tai Wang, Jinkun Cao, Xiaolin Hu, Si Liu, Jifeng Dai, and Jiangmiao Pang. CooHOI: Learning cooperative humanobject interaction with manipulated object dynamics. In NeurIPS, 2024. 3 [13] Zichen Geng, Zeeshan Hayder, Wei Liu, and Ajmal Saeed Mian. Auto-regressive diffusion for generating 3d humanobject interactions. In AAAI, 2025. [14] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. IMoS: Intentdriven full-body motion synthesis for human-object interactions. In Computer Graphics Forum, 2023. 2 [15] Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, and Xue Bin Peng. Synthesizing phys9 ical character-scene interactions. In SIGGRAPH, 2023. 2, 3 [16] Wenkun He, Yun Liu, Ruitao Liu, and Li Yi. Syncdiff: Synchronized motion diffusion for multi-body human-object interaction synthesis. In ICCV, 2025. 2 Jean Pierre Sleiman, [17] Xiaoyu Huang, Takara Truong, Yunbo Zhang, Fangzhou Yu, Jessica Hodgins, Koushil Sreenath, and Farbod Farshidian. Diffuse-cloc: Guided diffusion for physics-based character look-ahead control. ACM Transactions on Graphics (TOG), 44(4):112, 2025. 3 [18] Yinghao Huang, Omid Taheri, Michael J. Black, and Dimitrios Tzionas. InterCap: Joint markerless 3D tracking of humans and objects in interaction. In GCPR, 2022. 2 [19] Kai Jia, Tengyu Liu, Mingtao Pei, Yixin Zhu, and Siyuan Huang. PrimHOI: Compositional human-object interaction via reusable primitives. In ICCV, 2025. 2 [20] Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Yixin Chen, He Wang, Yixin Zhu, and Siyuan Huang. CHAIRS: Towards full-body articulated human-object interaction. In ICCV, 2023. 2 [21] Nan Jiang, Zimo He, Zi Wang, Hongjie Li, Yixin Chen, Siyuan Huang, and Yixin Zhu. Autonomous characterIn SIGscene interaction synthesis from text instruction. GRAPH Asia, 2024. 2 [22] Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, and Siyuan Huang. Scaling up dynamic human-scene interaction modeling. In CVPR, 2024. 2 [23] Jordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin SuperPADL: Scaling language-directed physicsIn Peng. based control with progressive supervised distillation. SIGGRAPH, 2024. 2 [24] Dvij Kalaria, Sudarshan Harithas, Pushkal Katara, Sangkyung Kwak, Sarthak Bhagat, Shankar Sastry, Srinath Sridhar, Sai Vemprala, Ashish Kapoor, and Jonathan Chung-Kuan Huang. DreamControl: Human-inspired whole-body humanoid control for scene interaction via guided diffusion. arXiv preprint arXiv:2509.14353, 2025. 2 [25] Hyeonwoo Kim, Sangwon Beak, and Hanbyul Joo. DAViD: Modeling dynamic affordance of 3d objects usarXiv preprint ing pre-trained video diffusion models. arXiv:2501.08333, 2025. 2 [26] Jeonghwan Kim, Jisoo Kim, Jeonghyeon Na, and Hanbyul Joo. ParaHome: Parameterizing everyday home activities towards 3d generative modeling of human-object interactions. arXiv preprint arXiv:2401.10232, 2024. 2 [27] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3, 5 [28] Jiaman Li, Jiajun Wu, and Karen Liu. Object motion guided human motion synthesis. ACM Transactions on Graphics (TOG), 42(6):111, 2023. 2, 6, 7, [29] Yitang Li, Mingxian Lin, Zhuo Lin, Yipeng Deng, Yue Cao, and Li Yi. Learning physics-based full-body human reaching and grasping from brief walking references. In CVPR, 2025. 3 [30] Libin Liu and Jessica Hodgins. Learning to schedule control fragments for physics-based characters using deep qlearning. ACM Transactions on Graphics (TOG), 36(3): 114, 2017. 3 [31] Yun Liu, Chengwen Zhang, Ruofan Xing, Bingda Tang, Bowen Yang, and Li Yi. Core4d: 4d human-objecthuman interaction dataset for collaborative object rearrangement. In CVPR, 2025. 2 [32] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. SMPL: skinned multi-person linear model. ACM transactions on graphics, 2015. 4, 1 [33] Jintao Lu, He Zhang, Yuting Ye, Takaaki Shiratori, Sebastian Starke, and Taku Komura. CHOICE: Coordinated interaction in cluttered environments for human-object pick-and-place actions. arXiv preprint arXiv:2412.06702, 2024. 2 [34] Jiaxin Lu, Chun-Hao Paul Huang, Uttaran Bhattacharya, Qixing Huang, and Yi Zhou. HUMOTO: 4d dataset of mocap human object interactions. In ICCV, 2025. 2 [35] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for real-time simulated avatars. In ICCV, 2023. [36] Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris Kitani, and Weipeng Xu. Universal humanoid motion representations for physics-based control. arXiv preprint arXiv:2310.04582, 2023. 3, 5 [37] Zhengyi Luo, Jinkun Cao, Sammy Christen, Alexander Winkler, Kris Kitani, and Weipeng Xu. Grasping diverse objects with simulated humanoids. In NeurIPS, 2024. 2, 3, 5 [38] Zhengyi Luo, Jiashun Wang, Kangni Liu, Haotian Zhang, Chen Tessler, Jingbo Wang, Ye Yuan, Jinkun Cao, Zihui Lin, Fengyi Wang, et al. SMPLOlympics: Sports environments for physically simulated humanoids. arXiv preprint arXiv:2407.00187, 2024. 3 [39] Zhengyi Luo, Chen Tessler, Toru Lin, Ye Yuan, Tairan He, Wenli Xiao, Yunrong Guo, Gal Chechik, Kris Kitani, Linxi Fan, et al. Emergent active perception and dexterity of simulated humanoids from visual reinforcement learning. arXiv preprint arXiv:2505.12278, 2025. 3 [40] Xintao Lv, Liang Xu, Yichao Yan, Xin Jin, Congsheng Xu, Shuwen Wu, Yifan Liu, Lincheng Li, Mengxiao Bi, Wenjun Zeng, et al. HIMO: new benchmark for full-body human interacting with multiple objects. In ECCV, 2024. 2 [41] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. In NeurIPS, 2021. 6, 7, 1, 2 [42] Josh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval Tassa, Leonard Hasenclever, Vu Pham, Tom Erez, Greg Wayne, and Nicolas Heess. Catch & carry: reusable neural controllers for vision-guided whole-body tasks. ACM Transactions on Graphics (TOG), 39(4):391, 2020. 3 [43] Liang Pan, Jingbo Wang, Buzhen Huang, Junyu Zhang, Haofan Wang, Xu Tang, and Yangang Wang. Synthesizing physically plausible human motions in 3d scenes. 3DV, 2024. 3 In [44] Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, and Jingbo Wang. TokenHSI: Unified synthesis of physical human-scene interactions through task tokenization. In CVPR, 2025. 2, 3, 6 [45] Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, and Huaizu Jiang. HOI-Diff: Text-driven synthesis of 3d human-object interactions using diffusion models. arXiv preprint arXiv:2312.06553, 2023. 2 [46] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. ACM Transactions On Graphics (TOG), 37(4):114, 2018. 2, 4 [47] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. ACM Transactions on Graphics (ToG), 40(4):120, 2021. [48] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. ACM Transactions On Graphics (TOG), 41(4):117, 2022. 2, 3, 5 [49] Ilya Petrov, Vladimir Guzov, Riccardo Marin, Emre Aksan, Xu Chen, Daniel Cremers, Thabo Beeler, and Gerard Pons-Moll. ECHO: Ego-centric modeling of human-object interactions. arXiv preprint arXiv:2508.21556, 2025. 2 [50] Ilya Petrov, Riccardo Marin, Julian Chibane, and Gerard Pons-Moll. Tridi: Trilateral diffusion of 3d humans, objects, and interactions. In ICCV, 2025. 2 [51] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. ACM Transactions on Graphics, 36(6), 2017. 4, 1 [52] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction In Proceedings of the fourto no-regret online learning. teenth international conference on artificial intelligence and statistics, pages 627635. JMLR Workshop and Conference Proceedings, 2011. 5, 4 [53] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 4 [54] Yutong Shen, Hangxu Liu, Lei Zhang, Penghui Liu, Ruizhe Xia, Tianyi Yao, and Tongtong Feng. Detach: Crossdomain learning for long-horizon tasks via mixture of disentangled experts. arXiv preprint arXiv:2508.07842, 2025. [55] Wandong Sun, Luying Feng, Baoshi Cao, Yang Liu, Yaochu Jin, and Zongwu Xie. Ulc: unified and finegrained controller for humanoid loco-manipulation. arXiv preprint arXiv:2507.06905, 2025. 2 [56] Omid Taheri, Nima Ghorbani, Michael Black, and Dimitrios Tzionas. GRAB: dataset of whole-body human grasping of objects. In ECCV, 2020. 2 10 [57] Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, and Xue Bin Peng. Calm: Conditional adversarial latent models for directable virtual characters. In SIGGRAPH, 2023. 2 [58] Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, and Xue Bin Peng. Maskedmimic: Unified physics-based character control through masked motion inpainting. ACM Transactions on Graphics (TOG), 43(6):121, 2024. 3, 4, 5, 6, 8, 1, 2 [59] Chen Tessler, Yifeng Jiang, Erwin Coumans, Zhengyi Luo, Gal Chechik, and Xue Bin Peng. MaskedManipulator: Versatile whole-body control for loco-manipulation. arXiv preprint arXiv:2505.19086, 2025. 2, 3, 5, 6, [60] Guy Tevet, Sigal Raab, Setareh Cohan, Daniele Reda, Zhengyi Luo, Xue Bin Peng, Amit Bermano, and Michiel van de Panne. CLoSD: Closing the loop between simulation and diffusion for multi-task character control. In ICLR, 2025. 2 [61] Emanuel Todorov and Michael Jordan. Optimal feedback control as theory of motor coordination. Nature neuroscience, 5(11):12261235, 2002. 1 [62] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In IROS, 2012. 7, 1 [63] Takara Everest Truong, Michael Piseno, Zhaoming Xie, and Karen Liu. Pdp: Physics-based character animation via diffusion policy. In SIGGRAPH Asia, 2024. 3 [64] Unitree. Unitree g1 humanoid agent ai avatar. https: //www.unitree.com/g1/. 2, 4, [65] Ron Vainshtein, Zohar Rimon, Shie Mannor, and Task Tokens: flexible approach to arXiv preprint Chen Tessler. adapting behavior foundation models. arXiv:2503.22886, 2025. 6 [66] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. Sceneaware generative network for human motion synthesis. In CVPR, 2021. 2 [67] Jiashun Wang, Jessica Hodgins, and Jungdam Won. Strategy and skill learning for physics-based table tennis animation. In SIGGRAPH, 2024. 3 [68] Jiashun Wang, Yifeng Jiang, Haotian Zhang, Chen Tessler, Davis Rempe, Jessica Hodgins, and Xue Bin Peng. Hil: Hybrid imitation learning of diverse parkour skills from videos. arXiv preprint arXiv:2505.12619, 2025. [69] Tingwu Wang, Yunrong Guo, Maria Shugrina, and Sanja Fidler. Unicon: Universal neural controller for physicsbased character motion. arXiv preprint arXiv:2011.15119, 2020. 2 [70] Wenjia Wang, Liang Pan, Zhiyang Dou, Zhouyingcheng Liao, Yuke Lou, Lei Yang, Jingbo Wang, and Taku Komura. SIMS: Simulating human-scene interactions with real world script planning. In ICCV, 2025. 3 [71] Yinhuai Wang, Jing Lin, Ailing Zeng, Zhengyi Luo, Jian Zhang, and Lei Zhang. PhysHOI: Physics-based imitation of dynamic human-object interaction. arXiv preprint arXiv:2312.04393, 2023. 2, 3, 6 [72] Jungdam Won, Deepak Gopinath, and Jessica Hodgins. scalable approach to control diverse behaviors for physi11 cally simulated characters. ACM Transactions on Graphics (TOG), 39(4):331, 2020. [73] Jungdam Won, Deepak Gopinath, and Jessica Hodgins. Physics-based character controllers using conditional vaes. ACM Transactions on Graphics (TOG), 41(4):112, 2022. 3 [74] Lin Wu, Zhixiang Chen, and Jianglin Lan. HOI-Dyn: Learning interaction dynamics for human-object motion diffusion. arXiv preprint arXiv:2507.01737, 2025. 2 [75] Yan Wu, Korrawe Karunratanakul, Zhengyi Luo, and Siyu Tang. UniPhys: Unified planner and controller with diffusion for flexible physics-based character control. In ICCV, 2025. 3 [76] Zhen Wu, Jiaman Li, Pei Xu, and Karen Liu. Humanobject interaction from human-level instructions. In ICCV, 2025. 2, 3 [77] Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, and Jiangmiao Pang. Unified human-scene interaction via prompted chain-of-contacts. In ICLR, 2024. 3 [78] Xianghui Xie, Jan Eric Lenssen, and Gerard Pons-Moll. InterTrack: Tracking human object interaction without object templates. In 3DV, 2024. 2 [79] Zhaoming Xie, Sebastian Starke, Hung Yu Ling, and Michiel van de Panne. Learning soccer juggling skills with layer-wise mixture-of-experts. In SIGGRAPH, 2022. 3 [80] Zhaoming Xie, Jonathan Tseng, Sebastian Starke, Michiel van de Panne, and Karen Liu. Hierarchical planning arXiv preprint and control for box loco-manipulation. arXiv:2306.09532, 2023. [81] Liang Xu, Chengqun Yang, Zili Lin, Fei Xu, Yifan Liu, Congsheng Xu, Yiyi Zhang, Jie Qin, Xingdong Sheng, Yunhui Liu, et al. Perceiving and acting in first-person: dataset and benchmark for egocentric human-object-human interactions. In ICCV, 2025. 2 [82] Michael Xu, Yi Shi, KangKang Yin, and Xue Bin Peng. Physics-based augmentation with reinforcement Parc: learning for character controllers. In SIGGRAPH, 2025. 2 [83] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan InterDiff: Generating 3d human-object interactions Gui. with physics-informed diffusion. In ICCV, 2023. 2, 5 [84] Sirui Xu, Ziyin Wang, Yu-Xiong Wang, and Liang-Yan Gui. Interdreamer: Zero-shot text to 3d dynamic human-object interaction. In NeurIPS, 2024. 2 [85] Sirui Xu, Yu-Wei Chao, Liuyu Bian, Arsalan Mousavian, Yu-Xiong Wang, Liangyan Gui, and Wei Yang. Dexplore: Scalable neural control for dexterous manipulation from reference scoped exploration. In CoRL, 2025. 5 [86] Sirui Xu, Dongting Li, Yucheng Zhang, Xiyan Xu, Qi Long, Ziyin Wang, Yunzhi Lu, Shuchang Dong, Hezi Jiang, Akshat Gupta, Yu-Xiong Wang, and Liang-Yan Gui. InterAct: Advancing large-scale versatile 3d human-object interaction generation. In CVPR, 2025. 6 [87] Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, and Liang-Yan InterMimic: Towards universal whole-body control In CVPR, Gui. for physics-based human-object interactions. 2025. 1, 3, 4, 6, 7, 8, 2 [101] Ziyu Zhang, Sergey Bashkirov, Dun Yang, Michael Taylor, and Xue Bin Peng. ADD: Physics-based motion imitation with adversarial differential discriminators. arXiv preprint arXiv:2505.04961, 2025. 2 [102] Chengfeng Zhao, Juze Zhang, Jiashen Du, Ziwei Shan, IM Junye Wang, Jingyi Yu, Jingya Wang, and Lan Xu. HOI: Inertia-aware monocular capture of 3d human-object interactions. In CVPR, 2024. 2 [103] Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and Siyu Tang. Synthesizing diverse human motions in 3d indoor scenes. In ICCV, 2023. 2 [104] Siheng Zhao, Yanjie Ze, Yue Wang, Karen Liu, Pieter Abbeel, Guanya Shi, and Rocky Duan. ResMimic: From general motion tracking to humanoid whole-body arXiv preprint loco-manipulation via residual learning. arXiv:2510.05070, 2025. 2 [88] Mengqing Xue, Yifei Liu, Ling Guo, Shaoli Huang, and Changxing Ding. Guiding human-object interactions with rich geometry and relations. In CVPR, 2025. [89] Heyuan Yao, Zhenhua Song, Baoquan Chen, and Libin Liu. Controlvae: Model-based learning of generative controllers for physics-based characters. ACM Transactions on Graphics (TOG), 41(6):116, 2022. 3, 5 [90] Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, and Libin Liu. MoConVQ: Unified physics-based motion control via scalable discrete representations. arXiv preprint arXiv:2310.10198, 2023. 3 [91] Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, and Qifeng Chen. Skillmimic-v2: Learning robust and generalizable interaction skills from sparse and noisy demonstrations. In SIGGRAPH, 2025. 3, 6 [92] Ling-An Zeng, Guohong Huang, Yi-Lin Wei, Shengbo Gu, Yu-Ming Tang, Jingke Meng, and Wei-Shi Zheng. ChainHOI: Joint-based kinematic chain modeling for humanobject interaction generation. In CVPR, 2025. 2 [93] Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue Bin Peng, and Kayvon Fatahalian. Learning physically simulated tennis skills from broadcast videos. ACM Transactions on Graphics (TOG), 42(4):114, 2023. 3 [94] Haozhuo Zhang, Jingkai Sun, Michele Caprio, Jian Tang, Shanghang Zhang, Qiang Zhang, and Wei Pan. HumanoidVerse: versatile humanoid for vision-language arXiv preprint guided multi-object rearrangement. arXiv:2508.16943, 2025. 3 [95] Juze Zhang, Haimin Luo, Hongdi Yang, Xinru Xu, Qianyang Wu, Ye Shi, Jingyi Yu, Lan Xu, and Jingya Wang. NeuralDome: neural modeling pipeline on multi-view human-object interactions. In CVPR, 2023. 2, 6, 7, 8, 1, 3 [96] Juze Zhang, Jingyan Zhang, Zining Song, Zhanhe Shi, Chengfeng Zhao, Ye Shi, Jingyi Yu, Lan Xu, and Jingya Wang. Hoi-mˆ 3: Capture multiple humans and objects interaction within contextual environment. In CVPR, 2024. [97] Jinlu Zhang, Yixin Chen, Zan Wang, Jie Yang, Yizhou Wang, and Siyuan Huang. InteractAnything: Zero-shot human object interaction synthesis via llm feedback and object affordance parsing. In CVPR, 2025. 2 [98] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Ilya Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo Perez-Pellitero, and Gerard Pons-Moll. FORCE: Dataset and method for intuitive physics guided human-object interaction. In 3DV, 2024. 2 [99] Xiaohan Zhang, Sebastian Starke, Vladimir Guzov, Zhensong Zhang, Eduardo Perez Pellitero, and GerSCENIC: Scene-aware semantic naviard Pons-Moll. arXiv preprint gation with instruction-guided control. arXiv:2412.15664, 2024. 2 [100] Yunbo Zhang, Deepak Gopinath, Yuting Ye, Jessica Hodgins, Greg Turk, and Jungdam Won. Simulation and retargeting of complex multi-character interactions. In SIGGRAPH, 2023. 3 12 InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary, we provide additional details of our InterPrior framework with extended experiments: (i) Sec. describes the organization of the demo video. (ii) Sec. details the overall simulation configuration. (iii) Sec. provides additional information on our goal representation, e.g., how snapshot, trajectory, and contact goals are constructed at training and evaluation time with the masks. (iv) Sec. gives comprehensive explanation on: (I) the detailed formulation of the reference-free hand reward; (II) the losses used for variational distillation and latent shaping, and (III) RL finetuning. (v) Sec. specifies additional implementation details, including network architectures, training schedules, and how we apply data augmentation to expert training, as well as additional techniques we use during G1 training for sim-to-sim experiments. (vi) Sec. presents further qualitative results, e.g., the integration of InterPrior with kinematic HOI generators, additional details of metrics, and failure cases. (vii) Sec. examines the limitations of our current system and its potential societal implications."
        },
        {
            "title": "Contents",
            "content": "A. Demo Video B. Simulation C. Goal Formulation C.1. Horizon for Goals . . . . . C.2. Stochastic Mask Sampling during Training . . . . . C.3. Task Definition for Inference . . . . . . . . . . . . . . D. Additional Details on Methodology D.1. InterMimic+: Full-Reference Imitation Expert . . . . D.2. InterPrior: Variational Distillation . D.3. InterPrior: Post-Training Beyond Reference . . E. Implementation Details F. Additional Experimental Results G. Discussion A. Demo Video 1 1 2 2 2 2 3 3 3 3 4 5 The demo video on the webpage visualizes behaviors produced by InterPrior across settings detailed in the follow1 ing. All sequences are rendered from the physics simulator [41, 62] using the same SMPL [32, 51] and G1 [64] model as for training. No post-processing is applied other than camera selection and cropping for visualization. Core Capability. We show examples of snapshot, trajectory, and contact-conditioned control corresponding to the scenarios illustrated in Figure 1 of the main paper, for objects with diverse shapes. Failure Recovery and Regrasping. We visualize rollouts perturbed or initialized from failure states. The video highlights re-approaching, re-grasping, and recovery from falls as described in Sec. 3.4. Long-Horizon Multi-Goal Chains. We include long sequences where three canonicalized sub-goals are chained (Sec. 4, Chain tasks) and the policy must transition smoothly between different interaction while maintaining task success. Diverse Task Execution from the Same Goal. We show that our model is able to control the simulated human achieving the same task with different execution. Baseline Comparison. We demonstrate that InterPrior achieves superior performance compared to existing baseline methods [58, 59, 87]. Novel Interaction Generalization. We visualize qualitative results on BEHAVE [3] and HODome [95], as complementary to Figure 5 and Figure 7 in the main paper. Interaction with multiple objects. We showcase that InterPrior supports human interactions with multiple objects, without requiring any task-specific training. Sim-to-Sim for G1. We include more examples of the G1 humanoid with sim-to-sim transfer, as complementary to Figure 6, for controlling humanoid only based on object future snapshot goal. Interactive Steering Control. Finally, we show real-time keyboard control where user steers high-level goals and InterPrior produces coherent whole-body motion online. B. Simulation All experiments are performed in IsaacGym [41] with the GPU PhysX backend. Control policies run at 30Hz, while the simulator is stepped at 60Hz with two internal substeps per control step. The main simulation hyperparameters are summarized in Table A. We introduce small object rest offset to reduce humanobject interpenetration, especially for thin geometries. Although this slightly enlarges the effective collision boundary, it avoids the substantial cost associated with increasing Table A. Simulation hyperparameters used in IsaacGym [41]. We largely follow the settings from prior work [71, 87]."
        },
        {
            "title": "Hyperparameter",
            "content": "Simulation step Control step t"
        },
        {
            "title": "Physics substeps per control step\nPosition solver iterations\nVelocity solver iterations\nContact offset\nRest offset\nMax depenetration velocity",
            "content": "Object & ground restitution Object & ground friction Object density Max convex hulls per object Object rest offset"
        },
        {
            "title": "Value",
            "content": "1/60 1/30 2 4 1 0.02 0.0 100 0.7 0.9 200 64 0.01 solver accuracy to compensate for collision handling. C. Goal Formulation This section details the construction of snapshot, trajectory, and contact goals and the associated masks used. Specifically, goal state yt shares the same structure as the observation xt, and binary mask mt indicates which components of yt are provided to the policy. C.1. Horizon for Goals Short-Horizon Preview. We use small set of offsets = {1, 2, 4, 16} to provide short-horizon previews relative to the current timestep t. For each offset K, we construct goal pair (yt+k, mt+k). Long-Horizon Snapshot. long-horizon offset sampled by [1, 128] defines single far-future goal (yt+L, mt+L). During training, is initialized randomly at the start of each episode and then decremented each timestep, being resampled once it reaches zero. Although termed long-horizon snapshot, its value naturally decreases at each step and may temporarily fall below the short-horizon offsets. C.2. Stochastic Mask Sampling during Training During training, masks are not tied to specific tasks (snapshot; trajectory; contact). Instead, we randomly decide which parts of the future state are revealed to the policy, so that the policy is exposed to wide variety of partial and sparse goals, following [58]. We operate at the level of rigid bodies, including objects with following three rules: Body-Wise Masking. Visibility is enforced at the body level. For each rigid body, we maintain single binary variable. If it is false, all all state features associated with that 2 body at time t+k are masked out, positions, orientations, and linear and angular velocities. The same rule applies to the entries in the interaction vectors Dt+k and the contact state Ct+k, defined in Sect. 3.1, which are masked or revealed together. Independent Sampling in Rigid Bodies. At each horizon offset k, each body is sampled independently according to fixed Bernoulli distribution: human-state and interaction components are revealed with probability 0.1, and object components with probability 0.5. This procedure produces diverse, randomly constructed combinations of visible and masked human, object, and contact features, rather than relying on any task-specific mask templates. Temporal Consistency of Masks. To avoid flickering visibility, masks evolve over time with high probability of staying the same and small probability of being resampled. Concretely, for > 1 we define first-order Markov process: (cid:40) mt+k = mt+k1, Bernoulli(pvis), with probability preset. with probability 1 preset, Here preset = 0.01 ensures that once body is masked or unmasked, it tends to remain in that state for multiple steps, while occasional resets still diversify the masks. The visibility probabilities pvis follow the design above. C.3. Task Definition for Inference During inference, masks are constructed according to the target task. For given task, the visibility pattern remains fixed throughout the rollout. The only exception is the multi-goal chaining setting, where we resample new mask whenever the controller transitions to the next sub-goal. Snapshot-Conditioned Control. We unmask the longhorizon snapshot. We still apply the consistent per-body sampling to determine which body or object components are revealed. All short-horizon preview are fully masked. Trajectory-Conditioned Control. We unmask the shorthorizon preview. Following the same per-body sampling, we reveal only subset of the joint or object components. The long-horizon snapshot goal is retained. Contact-Conditioned Control. Contact goals are implemented as special case of snapshot conditioning in which we reveal only contact-related information. Specifically, we unmask the contact entries of Ct, the associated signeddistance fields Dt (defined in Sec. 3.1), and the relevant human body parts. To avoid ambiguity in the target, we additionally unmask the object pose in the snapshot frame. Multi-Goal Chaining. For multi-goal chains, we extract data by concatenating different data sequences. Specifically, we canonicalize each subsequent first frame with respect to the previous last frame. Canonicalization is performed by aligning the human root position (excluding height), and heading, i.e., rotation around the vertical zaxis only, rather than the full SO(3) orientation. Because this transformation is applied with respect to the human frame only, the object frame may become partially misaligned after canonicalization. As result, we do not expect the policy to perfectly satisfy all chained goals, especially when object-relative alignment becomes extremely inconsistent. Nevertheless, the presence of long horizon makes the policy possibly compensate for canonicalization artifacts. entries of the goal, i.e., those that were hidden from the policy input. Formally, the goal reconstruction loss is (cid:2)(cid:13) (cid:0)1 mt+k (cid:13) Lgoal = Et,k (cid:1) (cid:0) (cid:98)yt+k yt+k (cid:1)(cid:13) 2 (cid:13) 2 (cid:3), where denotes element-wise multiplication and 1 is an all-ones vector. This loss encourages the latent zt to capture intent and context sufficient to reconstruct the missing parts of the goal, given only the visible subset provided by the mask. In practice, we reconstruct short future with = 1. D. Additional Details on Methodology D.3. InterPrior: Post-Training Beyond Reference This section expands the reward and loss formulations, as well as additional details for the three stages of our framework: (I) InterMimic+ expert training (extending Sec. 3.2), (II) variational distillation (extending Sec. 3.3), and (III) RL post-training (extending Sec. 3.4). D.1. InterMimic+: Full-Reference Imitation Expert Reference-Free Reward for Expert. Here we introduce the detailed formulation of the hand reward rh. Let pT denote the position of the thumb fingertip and {pj}jS the positions of the other fingertips, with qT and {qj}jS being their respective nearest surface points on the object. We define unit bearing vectors from the object surface toward the fingertips as uT = (pT qT )/pT qT and uj = (pjqj)/pj qj, S. The reward is defined as rh = exp(wheh), where eh = 1 1 , and wh increases as the hand-object distance decreases, activating only when the reference indicates an upcoming interaction. This reward encourages all five fingers to maximize upcoming surface contact with the object. 1u uj 2 (cid:80) jS D.2. InterPrior: Variational Distillation Here we introduce the formulation for our proposed losses for variational Distillation. Let µp,t and Σp,t denote the priors mean and covariance at time t, i.e., (µp,t, Σp,t) pψ(zt xtℓ:t, Gt). (I) Scale loss. We regularize the prior mean to lie on the unit hypersphere. This is to prevent the output mean from collapsing or exploding, with the use of latent normalization: Lscale = Et (cid:2)(cid:0)µp,t2 1(cid:1)2(cid:3). (II) Temporal consistency loss. To obtain smooth latent prior over time, we use Ltc to penalize changes in the prior distribution across consecutive timesteps using the squared 2-Wasserstein distance between Gaussians. (III) Goal reconstruction loss. The decoder includes an additional head that predicts future goal features conditioned on the latent. Let (cid:98)yt+k denote the predicted goal at offset and mt+k the input mask used to construct the masked residual goal. We train this head to complete the masked Get-Up Training. To learn the get-up behavior, in addition to the new learnable token as discussed in Sec. 3.4, we introduce an auxiliary reward that becomes active, with episodes initialized from fallen state. The reward encourages both elevation of the pelvis and reorientation of the torso toward an upright configuration: rgetup = wheight σ(cid:0)ht htarget (cid:1) + wupright σ(cid:0)nt nup (cid:1), (1) where ht is the pelvis height, htarget is set as 0.7, nt is the torsos up vector, nup is the world up direction, and σ() denotes clipped linear shaping function. Distributed Training. To mitigate catastrophic forgetting, we divide the parallel simulation environments into three (I) RL environments, optimized solely with the groups: post-training reward rPT ; (II) Distillation environments, optimized using the ELBO objective and supervised by the expert policy, as described in Sec. 3.3. The policy parameters are shared across all environments. Gradients are aggregated synchronously to update the shared policy. Mask Prompt Engineering during Inference. To further enhance robustness during inference without additional learning, we apply lightweight mask-based prompting over the goal specification Gt (Sec. 3.1): (I) When following trajectory and the state lags behind, we remove the trajectory goal but redefine the nearest waypoint as the snapshot goal. (II) For snapshot goals with distant target joints (>1 m), we retain only the root translation goal while masking out all other components, prompting locomotion be- (III) When human-object targets fore fine manipulation. are contradictory, e.g., both are moving but no grasp is established, we set the human root goal to the current object position while maintaining root height, masking all other joints. This encourages natural re-approach and regrasping behaviors. These inference-time edits operate solely on the goal Gt, while the policy parameters remain fixed. Finetuning on Additional HOI Datasets. The same finetuning mechanism naturally extends to absorbing new interaction datasets. Given any additional HOI corpus (e.g., BEHAVE [3] or HODome [95] in Sec. 4), states from such new dataset are treated as additional sources of long-horizon goals and initializations for RL rollouts, while the distillation group continues to regularize the policy toward the original prior. This allows InterPrior to incrementally acquire new object categories and interaction styles without retraining from scratch. E. Implementation Details This section summarizes key implementation details, including network configurations, hyperparameters, randomization settings used for expert training, and additional techniques used during G1 training for sim-to-sim experiments. PPO Setup. For both the expert and RL finetuning stages, we use PPO with generalized advantage estimation (GAE) and clipped surrogate objective, and train with Adam. Following common practice [46], we keep the PPO discount factor γ, GAE parameter λ, clip ratio, and entropy regularization as shown in Table B, and apply gradient clipping. InterMimic+: Full-Reference Imitation Expert. The InterMimic+ expert policy and critic are MLPs with three hidden layers of sizes (1024, 1024, 512), using ReLU activations. Actor and critic are parameterized separately, and the critic outputs scalar value with full observation and reference as input. Please refer to [87] for more details. InterPrior: Variational Distillation. The encoder and decoder used for variational distillation share the same MLP backbone with hidden sizes (1024, 1024, 512). The prior pψ is implemented as 4-layer Transformer encoder with 4 attention heads, latent dimension of 512, and feedforward width of 1024. For the distillation objective (Sec. D), we use unit weight for the action reconstruction loss, and assign weight of 103 to all auxiliary terms (goal reconstruction, scale loss, and temporal consistency loss). The KL regularizer follows β-VAE style schedule: the KL weight β is annealed from 103 to 1.0 over the course of training. We first perform 500 epochs of warm-up using only teacher-controlled rollouts, and then gradually increase the fraction of student-controlled rollouts [52] until epoch 10, 000, at which point 95% of environments are driven by the student policy while the remaining 5% always use the teacher for fresh expert trajectories. InterPrior: Post-Training Beyond Reference. For the post-training stage, we retain the same loss weights used for the distillation branch, and combine with the PPO loss weights specified in Table for the RL branches. Inference Efficiency. The runtime breakdown is: observation 20.16,ms, physics 19.02,ms, policy inference 0.43,ms, SDF 0.134,ms, and other overheads 0.057,ms, highlighting the policys potential for real-world deployment. F. Additional Experimental Results In this section, we introduce metric details, provide supplementary qualitative results, and discuss failure cases. Additional Details on Evaluation Metrics. For trajectoryfollowing tasks, we evaluate the policy at each timestep by Table B. Hyperparamters for training teacher and student policies."
        },
        {
            "title": "Hyperparameters",
            "content": "Discount factor γ Generalized advantage estimation λ Learning rate Action loss weight Critic loss weight Action bounds loss weight Minibatch size Horizon length Maximum episode length value 0.99 0.95 2e-5 1 5 10 16384 32 300 Table C. Additional reward terms for G1 used in Stage expert training. Here, τ denotes the vector of joint torques with elementwise limits [τ min, τ max]; and are joint degrees and velocities with limits [qmin, qmax]; at is the control action at time t; ω and are the base (root) angular and linear velocities; feet is the vertical ground-reaction force at the feet; vfeet is the tangential (ground-plane) velocity of the feet; dfeet is the horizontal distance between the two feet, with desired bounds [dmin, dmax]; gfeet xy is the projection of the gravity direction onto the foot frames ground plane; 1() and 1termination are indicator functions. All norms and 2 are Euclidean. TERM Penalty: Torque limits DoF position limits Energy Termination Regularization: DoF velocity Action rate Torque Angular velocity Base velocity Foot slip Feet distance reward Feet orientation EXPRESSION WEIGHT 1(τ / [τ min, τ max]) 1(q / [qmin, qmax]) τ 1termination q2 2 at2 2 τ ω2 1(F feet > 5.0) (cid:112)vfeet 1 2 exp (100 max(dfeet dmin, 0.5)) + 1 2 exp (100 max(dfeet dmax, 0)) gfeet xy (cid:113) 2 5 104 30 4 104 0.1 2 103 0. 0.1 0.03 0.5 1 comparing the rollout state with the corresponding reference, and compute pose and object errors only over the unmasked components. For snapshot goal-following tasks, there is no time-aligned reference trajectory. Instead, we compute the error between the rollout state and the snapshot goal at every timestep and report the minimum of this distance over the rollout. This reflects whether the policy is capable of reaching the target configuration. Diverse Behaviors Under the Same Goal. Beyond the examples shown in the main paper, Figure illustrates how InterPrior behaves diversely given the same goal, showing Figure A. Additional qualitative comparisons with baseline method [58, 59] (Top). Our InterPrior shows higher success rate under the same task goal. Table D. Range of dynamics randomization. default refers to the parameter value from the unitree G1 official 29DoF model. vxy is the planar (horizontal) push velocity. Term Range / Value Dynamics randomization Friction coefficient Base CoM offset Base mass offset gain scaling gain scaling U(1.0, 3.0) U(0.05, 0.05) U(3.0, 3.0) kg U(0.8, 1.2) default U(0.8, 1.2) default External perturbation Push robot interval = 4 s, vxy = 1 m/s Figure B. Qualitative results given the same goal. Our framework produces multiple valid yet distinct interaction trajectories. that our learned latent space is meaningful and is able to capture diverse behaviors. Integration with Kinematic HOI Generators. To demonstrate that InterPriors generalization, we integrate it with InterDiff [83] that produces physically unconstrained interaction trajectories. The integration proceeds as follows: (I) the kinematic generator produces 25 frames of humanobject poses given the past 15 frames following [83]; (II) we convert these sequences into our goal representation by extracting snapshot and trajectory goals; and (III) we feed these goals into InterPrior. The result is shown in Figure C. 5 Figure C. Qualitative results of InterPrior following the targets generated by InterDiff [83] (yellow and red dots). InterPrior adaptively completes the task without strictly adhering to the targets, using only sparse inputs of wrist, feet, and object target. G. Discussion Limitations and Future Work. InterPrior is still bounded by the coverage and quality of its training data: highly corrupted or unseen interaction patterns are not reliably recovered, and in such cases the policy often defaults to conservative strategies, maintaining balance without fully solving the task. Our model is tailored to rigid object, and we still observe occasional artifacts such as shallow interpenetrations, foot skating, or failure cases such as object drop over long rollouts. The current hand and contact representation is also not designed for fine-grained finger dexterity or in-hand manipulation. Finally, our three-stage training introduces additional complexity and hyperparameters. Future work includes expanding dataset diversity, incorporating richer hand models, and simplifying or unifying the training scheme. Societal and Ethical Considerations. InterPrior enables more general-purpose, physically grounded humanoid controller, which can be beneficial for animation, simulation, and robotics, but also raises potential risks. More capable humanoid controllers could be deployed in unsafe settings or for applications that conflict with societal norms (e.g., surveillance or coercive scenarios). We therefore encourage careful consideration of safety mechanisms, usage policies, and ethical guidelines when applying this type of model beyond controlled research environments."
        }
    ],
    "affiliations": [
        "Amazon",
        "University of Illinois Urbana-Champaign"
    ]
}