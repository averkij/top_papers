{
    "paper_title": "Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models",
    "authors": [
        "Sina Tayebati",
        "Divake Kumar",
        "Nastaran Darabi",
        "Dinithi Jayasuriya",
        "Ranganath Krishnan",
        "Amit Ranjan Trivedi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used in safety-critical applications, yet their opaque decision-making complicates risk assessment and reliability. Uncertainty quantification (UQ) helps assess prediction confidence and enables abstention when uncertainty is high. Conformal prediction (CP), a leading UQ method, provides statistical guarantees but relies on static thresholds, which fail to adapt to task complexity and evolving data distributions, leading to suboptimal trade-offs in accuracy, coverage, and informativeness. To address this, we propose learnable conformal abstention, integrating reinforcement learning (RL) with CP to optimize abstention thresholds dynamically. By treating CP thresholds as adaptive actions, our approach balances multiple objectives, minimizing prediction set size while maintaining reliable coverage. Extensive evaluations across diverse LLM/VLM benchmarks show our method outperforms Least Ambiguous Classifiers (LAC) and Adaptive Prediction Sets (APS), improving accuracy by up to 3.2%, boosting AUROC for hallucination detection by 22.19%, enhancing uncertainty-guided selective generation (AUARC) by 21.17%, and reducing calibration error by 70%-85%. These improvements hold across multiple models and datasets while consistently meeting the 90% coverage target, establishing our approach as a more effective and flexible solution for reliable decision-making in safety-critical applications. The code is available at: {https://github.com/sinatayebati/vlm-uncertainty}."
        },
        {
            "title": "Start",
            "content": "Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models Sina Tayebati1, Divake Kumar1, Nastaran Darabi1, Dinithi Jayasuriya1, Ranganath Krishnan2, Amit Ranjan Trivedi1 1University of Illinois at Chicago, 2Intel Labs 5 2 0 2 8 ] . [ 1 4 8 8 6 0 . 2 0 5 2 : r AbstractLarge Language and Vision-Language Models (LLMs/VLMs) are increasingly used in safety-critical applications, yet their opaque decision-making complicates risk assessment and reliability. Uncertainty quantification (UQ) helps assess prediction confidence and enables abstention when uncertainty is high. Conformal prediction (CP), leading UQ method, provides statistical guarantees but relies on static thresholds, which fail to adapt to task complexity and evolving data distributions, leading to suboptimal trade-offs in accuracy, coverage, and informativeness. To address this, we propose learnable conformal abstention, integrating reinforcement learning (RL) with CP to optimize abstention thresholds dynamically. By treating CP thresholds as adaptive actions, our approach balances multiple objectives, minimizing prediction set size while maintaining reliable coverage. Extensive evaluations across diverse LLM/VLM benchmarks show our method outperforms Least Ambiguous Classifiers (LAC) and Adaptive Prediction Sets (APS), improving accuracy by up to 3.2%, boosting AUROC for hallucination detection by 22.19%, enhancing uncertainty-guided selective generation (AUARC) by 21.17%, and reducing calibration error by 70%85%. These improvements hold across multiple models and datasets while consistently meeting the 90% coverage target, establishing our approach as more effective and flexible solution for reliable decision-making in safety-critical applications. The code is available at https://github.com/sinatayebati/vlm-uncertainty. Index TermsLarge Language Models, Conformal Abstention, Uncertainty Estimation, Policy Search I. INTRODUCTION are and"
        },
        {
            "title": "Language",
            "content": "rapidly becoming indispensable Vision-Language Models in (LLMs/VLMs) safety-critical applications, from autonomous systems [1] to healthcare diagnostics [2]. Their ability to process and information across visual and textual modalities interpret presents unprecedented opportunities for complex decisionmaking. However, their internal workings remain opaque, making it challenging to identify biases, vulnerabilities, and unintended consequences, which hinders effective risk assessment and mitigation. Traditional risk management frameworks, designed for static systems with well-defined rules, struggle to keep pace with the evolving capabilities and emergent behaviors of these models [3]. As decision support systems increasingly rely on LLM/VLM, equipping them with robust mechanisms to identify and manage their prediction risks has become crucial. Uncertainty quantification (UQ) of LLM/VLM has therefore gained significant attention for assessing prediction reliability and enabling abstentionallowing models to defer decisions when uncertainty is high. However, state-of-the-art UQ methods like conformal prediction (CP), while providing statistical guarantees, rely on static thresholds that fail to adapt to varying task complexities or evolving data distributions. Abstention strategies built on these methods therefore remain inflexible, treating abstention as binary choicepredict or abstain [4] without adapting to context. Consequently, state-of-the-art methods such as Least Ambiguous Classifiers (LAC) [5] tend to produce overly narrow prediction sets, sacrificing coverage, while Adaptive Prediction Sets (APS) [6] generate excessively large sets. To address these limitations, we propose framework for learnable conformal abstention, where models dynamically adjust abstention decisions based on task complexity and evolving data distributions. By integrating reinforcement learning with conformal prediction, our approach enables adaptive thresholding that surpasses static methods in accuracy, coverage, and reliability. Extensive evaluations across diverse benchmarks demonstrate its effectiveness in improving risk management, selective abstention, and overall decision-making in safety-critical LLM/VLM applications. In particular, our learned policy boosts hallucination detection by up to 22%, improves uncertainty-guided selective generation by more than 20% in certain scenarios, and reduces expected calibration error by 70%-85% compared to standard conformal baselines. Notably, it also sustains at least 90% coverage while reducing the average prediction set size. II. BACKGROUND Uncertainty Quantification (UQ) of Prediction Models: Several approaches have been explored to capture and manage uncertainty in machine learning models. Conformal prediction [7][9] provides distribution-free, model-agnostic framework for generating prediction sets with statistical guarantees, with advances such as inductive, split, and cross-conformal prediction addressing different calibration strategies. Extensions include handling distribution shifts, sequential data, and active learning. Evidential learning captures uncertainty by modeling distributions over parameters or predictions, with applications in deep evidential regression, image classification, and reinforcement learning, and connections to belief function theory. Bayesian deep learning offers alternatives like Monte Carlo dropout [10], variational inference [11], deep ensembles [12], and trainable calibration [13], with recent work focusing on scalable inference for large models and flexible posterior distributions using normalizing flows. Additionally, calibration techniques such as test-time augmentation, temperature scaling, and Platt scaling further refine confidence estimates [14], [15]. However, these methods primarily focus on UQ rather than actionable risk mitigation. Conformal Prediction (CP): We characterize the proposed framework within CP, distribution-free, model-agnostic UQ approach [7][9]. CP transforms model uncertainty estimates into statistically rigorous measures, generating prediction set that includes the true label with predefined error probability. The sets size reflects model uncertainty, with larger sets indicating higher uncertainty. Formally, given classification model mapping input to one of classes = {1, . . . , K}, CP constructs prediction set C(Xt) for test instance Xt satisfying: (Yt C(Xt)) 1 α, where α (0, 1) is the target error rate. Coverage is determined using calibration dataset Dcal = {(X (i) i=1. conformal score function s(X, ) is defined, where higher values indicate greater uncertainty. Calibration scores are computed as s1, . . . , sn, and the threshold ˆq is set using the (1 α)-quantile: , (i) )}n ˆq = quantile (n+1)(1α) {s1, . . . , sn}. The final prediction set for Xt is: C(Xt) = {Y : s(Xt, ) ˆq}. common uncertainty heuristic is the softmax score, which estimates class probabilities but often misaligns with true uncertainty. Two CP-based scoring functions have been proposed to address this: (i) Least Ambiguous Classifiers (LAC) [5], [16] use: s(X, ) = 1f (X)Y , where (X)Y is the softmax score for . LAC minimizes prediction set size but may yield overly narrow or broad sets. (ii) Adaptive Prediction Sets (APS) [6], [16] define: s(X, ) = (cid:80) {Y :f (X)Y (X)Y } (X)Y , summing softmax scores of classes ranked above or equal to . APS mitigates LACs limitations but often produces larger prediction sets, reducing informativeness. III. LEARNING CONFORMAL ABSTENTION POLICIES We propose novel framework for learning an abstention policy that leverages conformal prediction to generate uncertainty-aware prediction sets with statistical guarantees. The proposed framework, conformalized abstention policy (CAP), allows three possible outcomes per query: single prediction, set of plausible predictions, or abstention, balancing informativeness and risk based on prediction confidence. We formulate this as reinforcement learning (RL) problem, optimizing the CP hyperparameters (α and β) as actions using the REINFORCE [17]. First, to quantify the uncertainty of LLM/VLM responses, we use nonconformity measure based on softmax probabilities. Given an input x, the model produces logits ℓ = [ℓ1, ℓ2, . . . , ℓK] for classes, which are converted to probabilities pi(x) using softmax. Using calibration dataset Dcal = {(X (i) i=1, where yi is the ground-truth label, we compute the nonconformity score for each sample as score(xi) = 1 pyi(xi). This score quantifies nonconformity, with higher values indicating greater uncertainty. Traditional conformal prediction defines single threshold as the (1 α)- quantile of the calibration scores: cal, (i) cal )}n ˆq = Quantile1α (cid:0){si}n i=1 (cid:1). We extend this by introducing two thresholds, ˆqpredict and ˆqabstain, computed as: ˆqpredict = quantile {s1, . . . , sn}, (cid:18) ˆqabstain = quantile {s1, . . . , sn}, (cid:18) (n + 1)(1 α) (n + 1)(1 β) (cid:19) (cid:19) , ."
        },
        {
            "title": "These thresholds partition the nonconformity score of each",
            "content": "test sample into three regimes: 1) score(x) < ˆqpredict Single best prediction. 2) ˆqpredict score(x) < ˆqabstain Set prediction. 3) score(x) ˆqabstain Abstain. Action Probabilities and Stochastic Decisions: We extend the deterministic three-regime decision with stochastic policy that maps the nonconformity score to action probabilities. Let: s(x) = score(x) = 1 maxi pi(x). Using thresholds ˆqpredict and ˆqabstain, the action probabilities are defined as: (cid:0)s(x)(cid:1) = σ(cid:0) (cid:2)s(x) ˆqpredict (cid:3)(cid:1), (cid:3)(cid:1), (cid:0)s(x)(cid:1) = σ(cid:0) (cid:2)s(x) ˆqabstain where σ(z) = 1/(1 + ez) is the sigmoid function, and > 0 is scaling constant. The probability of set prediction is: (cid:0)s(x)(cid:1). For each test (cid:0)s(x)(cid:1) pabstain pset point, we stochastically select from {single, set, abstain} based on {psingle, pset, pabstain}, capturing model uncertainty. (cid:0)s(x)(cid:1) = 1 psingle psingle pabstain Reinforcement Learning and Abstention Policy: To dynamically adjust the confidence levels (α, β) for optimal performance, we employ policy-based reinforcement learning approach using the REINFORCE algorithm. The policy network πθ(α, β) learns distribution over these parameters. We treat α and β as actions sampled from multivariate Gaussian distribution defined by the policy network. Let: (µθ, σθ) = fθ(s), where is the current state, and fθ is neural network mapping to the mean and standard deviation vectors (µθ, σθ). Specifically, (cid:1), α (cid:0)µ(α) β (cid:0)µ(β) (cid:1). , σ(β) 2 θ θ , σ(α) 2 θ θ At each iteration, the process involves: (1) sampling α, β from the learned distribution, (2) computing the thresholds ˆqpredict, ˆqabstain on calibration set, (3) evaluating performance on test set, and (4) using the performance-based cost as reward signal to update πθ via REINFORCE. Cost Function and Reward Design: We define scalar cost function C(α, β) to balance multiple objectives: maximizing accuracy while ensuring well-calibrated uncertainty estimates, avoiding unnecessary set predictions or abstentions, and maintaining coverage guarantees. Let acc be the fraction of correct predictions, abstention the fraction of abstained samples, and avgSet the average prediction set size. Additionally, coverage = 1 abstention, and div is an entropybased term quantifying the balance among single predictions, set predictions, and abstentions. The cost function is defined as: C(α, β) = (1 acc) + λ1 avgSet + λ2 abstention λ3 coverage λ4 div. where λ1, . . . , λ4 are hyperparameters controlling trade-offs between these objectives. The corresponding reward is simply the negative cost: R(α, β) = C(α, β). REINFORCE Update: We employ Monte Carlo policy gradient method to update the policy parameters by maximizing the expected reward J(θ) = Eτ πθ [R(τ )], where τ is trajectory of states and actions, and R(τ ) is the corresponding reward. Each episode corresponds to evaluating the doubly conformalized prediction with specific set of actions on the test set. For batch of sampled actions {αt, βt}, the gradient of the expected reward is: θJ(θ) = E(α,β)πθ (cid:2)θ log πθ(α, β) R(α, β)(cid:3). This expectation is approximated by sampling trajectories (or averaging over minibatch of α, β) and updating the policy parameters as: θt+1 = θt + η Rtθ log πθ(atst), where η is the learning rate, Rt is the reward after action at in state st, and log πθ(atst) is the log-probability of taking at under the policy. The learned ˆα and ˆβ minimize the cost with respect to coverage, set size, accuracy, and abstentions, coupling conformal prediction thresholds to an RL objective and enabling principled trade-offs between predictive certainty and abstention for LLM outputs. algorithm 1 in Appendix summarizes the training of our proposed adaptive conformal method and abstention policy. IV. EXPERIMENTS AND RESULTS We conducted thorough empirical evaluation to benchmark our proposed CAP framework against the comparative least ambiguous set-valued classifier (LAC) and adaptive prediction sets (APS). The experiments focus on multiple-choice question answering (MCQA) tasks, assessing six key metrics: confidence ranking for hallucination detection, uncertaintyguided selective generation, coverage, set size, calibration, and accuracy. This evaluation systematically measures the effectiveness of the proposed abstention policy and the reliability of uncertainty estimates. A. Experimental Settings Datasets: We used diverse collection of ten benchmark LLM/VLM datasets. These datasets are designed for multiplechoice question-answering (MCQA) across various reasoning tasks and uncertainty scenarios. For VLMs, we employ five datasets: (i) MMBench [18], multi-modal benchmark with 4,000 questions spanning perception and reasoning tasks, standardized to four options; (ii) OODCV-VQA [19], focusing on out-of-distribution instance counting via its Digits subset, expanded to four options; (iii) ScienceQA [20], containing 3,952 image-based questions across natural and social sciences; (iv) SEEDBench [21], evaluating visual understanding (e.g., object localization) with 14,233 questions; and (v) AI2D [22], featuring 15,000 diagram-based science questions extended to six options. All datasets are reformatted to multiplechoice questions (MCQA) with four or six options to assess uncertainty handling. For LLMs, we evaluate on five tasks: (i) MMLU [23], question-answering benchmark spanning 57 academic subjects; (ii) CosmosQA [24], focusing on reading comprehension requiring contextual inference; (iii) HellaSwag [25], assessing commonsense inference for event followup prediction; (iv) HaluDial [26], evaluating dialogue response selection from knowledge-grounded conversations; and (v) HaluSum [26], testing document summarization on news articles. Each dataset is standardized to six options (including dont know and None of the above) to align with uncertainty-aware evaluation protocols. This selection ensures diverse assessment of LLM capabilities in knowledge recall, reasoning, and abstention under ambiguity. Models: We evaluated on diverse set of LLM/VLM models with parameter scales ranging from 2.7B to 34B. For VLMs, the main body of the paper includes results for the LLaVA-v1.6 series (34B, 13B, and 7B parameters) [27]. Additional state-of-the-art VLMssuch as the lightweight MoELLaVA-Phi2 2.7B [28], Monkey-Chat 7B [29], InternLMXComposer2-VL 7B [30], Yi-VL 6B [31], CogAgent-VQA 7B [32], MobileVLMV2 Appendix C. For LLMs, the main body presents results for the Yi 34B model [31] and the Qwen series (7B and 14B parameters) [33]. Results for the Llama-2 foundation model series (7B and 13B parameters) are included in Appendix C. Evaluation Metrics: CAP is evaluated using the following metrics that assess both prediction quality and UQ, capturing its ability to produce single predictions, set predictions, or abstentions. The same metrics are applied to baseline conformal methods, including APS and LAC, following [16], [34]: Accuracy: For test input Xt with true label Yt, let C(Xt) denote the generated prediction set. If single prediction ˆYt is produced (e.g., in confident scenarios under ATCP), accuracy is binary: 1 if ˆYt = Yt, and 0 otherwise. For set predictions, accuracy is computed fractionally, inversely proportional to the size of C(Xt) when Yt C(Xt). Coverage: Coverage measures the fraction of instances where the correct is included in the models outputeither as single prediction or within prediction set. In label TABLE I: Comparison of CAP (Ours) with Least Ambiguous Classifiers (LAC) [5] and Adaptive Prediction Sets (APS) [6]. Models include VLMs and LLMs, assessed across datasets using AUROC (Hallucination Detection) and AUARC (UncertaintyGuided Selective Generation). Best values are in bold. Models VLMs LLaVA-v1.6-34B LLaVA-v1.6-13B LLaVA-v1.6-7B LLMs Yi-34B Qwen-14B Qwen-7B Method AUROC (Hallucination Detection) AUARC (Uncertainty guided selective generation) MMB OOD SQA SB AI2D Avg. MMB OOD SQA SB AI2D Avg. APS LAC Ours APS LAC Ours APS LAC Ours APS LAC Ours APS LAC Ours APS LAC Ours 0.7173 0.7837 0.8041 0.4930 0.6835 0.6382 0.6961 0.6849 0. 0.6962 0.7003 0.7849 0.5901 0.5919 0.7070 0.3424 0.4836 0.5643 0.7244 0.8000 0.8606 0.5281 0.5990 0.6663 0.6093 0.5555 0. 0.5566 0.5626 0.6512 0.4854 0.5038 0.6103 0.5699 0.4988 0.5919 0.8404 0.8476 0.8989 0.7775 0.7475 0.8083 0.8247 0.6930 0. 0.7070 0.7388 0.8000 0.5748 0.6251 0.6860 0.6085 0.5832 0.6480 0.9583 0.9412 0.9791 0.9566 0.9258 0.9761 0.9575 0.9212 0. 0.9138 0.9021 0.9717 0.8759 0.8512 0.9592 0.8712 0.8125 0.9253 0.9275 0.9099 0.9813 0.9444 0.8945 0.9565 0.9147 0.8671 0. 0.9155 0.8830 0.9441 0.9307 0.8791 0.9343 0.9239 0.8730 0.9353 0.9283 0.9192 0.9913 0.9142 0.8956 0.9838 0.8952 0.8691 0. HSwg HDial CQA HSum MMLU HSwg HDial CQA HSum MMLU 0.9109 0.9487 0.9726 0.8442 0.9182 0.9397 0.5638 0.4646 0.6380 0.5089 0.5650 0. 0.5296 0.4799 0.6175 0.3437 0.3542 0.4958 0.8370 0.9287 0.9649 0.7852 0.9132 0.9286 0.6107 0.7777 0.8037 0.5643 0.4181 0. 0.2611 0.1269 0.3510 0.2612 0.1654 0.4429 0.5883 0.6832 0.7425 0.6426 0.5445 0.6450 0.4829 0.4643 0.6053 0.6819 0.7087 0. 0.6125 0.5965 0.6964 0.4525 0.4452 0.5971 0.9735 0.9700 0.9973 0.9828 0.9748 0.9924 0.6853 0.6603 0.9325 0.7334 0.7140 0. 0.8326 0.8015 0.9323 0.7645 0.7275 0.9213 0.9373 0.9336 0.9963 0.9732 0.9657 0.9923 0.9000 0.8825 0.9831 0.7864 0.7529 0. 0.6266 0.5737 0.7146 0.5113 0.4754 0.6817 0.8806 0.8590 0.9669 0.8554 0.8216 0.9494 0.7459 0.7133 0.9450 0.9287 0.9111 0. 0.9244 0.8892 0.9620 0.9125 0.8686 0.9478 0.8622 0.8459 0.9700 0.8541 0.8275 0.9162 0.7214 0.6918 0.8927 TABLE II: Coverage (%) evaluation: Comparison of CAP (Ours) with LAC [5] and APS [6]. CAP meets the 90% coverage guarantee, underlined, in instances."
        },
        {
            "title": "Method",
            "content": "LLaVA-v1.6-34B LLaVA-v1.6-13B LLaVA-v1.6-7B"
        },
        {
            "title": "LLMs",
            "content": "Qwen-7B Qwen-14B Yi-34B APS LAC Ours APS LAC Ours APS LAC Ours APS LAC Ours APS LAC Ours"
        },
        {
            "title": "APS\nLAC\nOurs",
            "content": "Coverage (%)"
        },
        {
            "title": "MMB OOD SQA",
            "content": "SB AI2D Avg. 98.26 90.73 93.97 98.99 90.18 95.57 98.45 89.26 92. 94.87 91.42 93.25 96.20 91.00 92.48 97.89 89.10 91.63 98.08 88.67 93.07 99.29 89.28 92.06 97.88 89.83 90. 95.81 90.23 91.41 97.36 89.84 90.67 96.74 90.19 91.23 97.48 90.21 95.46 98.86 90.47 95.14 96.19 89.65 93. 96.90 90.25 93.43 98.14 90.15 93.18 97.43 89.61 91.94 HSwg HDial CQA HSum MMLU Avg. 92.12 89.64 91.96 99.82 91.98 94. 99.88 93.90 96.48 95.24 90.90 91.70 94.22 90.42 90.96 95.24 90.02 92.56 98.92 90.44 95.68 99.46 92.10 95. 99.68 94.40 96.40 90.18 90.12 90.17 90.56 89.70 90.32 92.08 89.32 90.82 96.24 90.66 91.32 95.72 90.46 91. 97.30 89.78 93.34 94.54 90.35 92.16 95.96 90.93 92.68 96.84 91.49 93.92 setups with abstention, it also accounts for instances where the model successfully avoids making an incorrect explicit guess. This metric ensures the ground truth is not excluded from predictions. key aspect of conformal prediction is meeting predefined coverage guarantee, set at 90% in our experiments. Set Sizes (SS): Set Sizes measure the average number of labels in prediction sets, excluding single predictions and abstentions. This metric reflects model uncertainty, with larger sets indicating higher uncertainty and smaller sets implying greater confidence. Area Under the Receiver Operating Characteristic (AUROC): AUROC [35] curve evaluates the models ability to rank predictions by confidence. It measures how effectively the model distinguishes correct from incorrect predictions, with higher AUROC indicating more reliable confidence-based ranking."
        },
        {
            "title": "Area Under",
            "content": "the Accuracy-Rejection Curve (AUARC): AUARC illustrates the trade-off between accuracy and the retained fraction of predictions after abstaining from uncertain ones. For the proposed framework, AUARC quantifies how well the models uncertainty estimates align with true prediction difficulty. higher AUARC indicates better identification and abstention from difficult cases while maintaining high accuracy on confident predictions. Expected Calibration Error (ECE) [36]: ECE quantifies how well the models confidence estimates align with empirical correctness rates. Given nbins confidence bins, it is defined as: ECE = nbins(cid:88) b=1 Bb (cid:12)acc(Bb) conf(Bb)(cid:12) (cid:12) (cid:12), where Bb is the set of samples whose confidence scores fall into bin b, acc(Bb) is the mean accuracy, and conf(Bb) is the mean predicted confidence in that bin. Lower ECE indicates better calibration, meaning the models confidence estimates closely match empirical correctness rates. Prompting Strategies: For VLMs, we adapt the multiplechoice Question Answering (VQA) template from LLaVA [37]. Each prompt starts with the attached image, followed by the question text and any relevant hints. Six answer options are then listed line by line, each prefixed with letter (A-F). The prompt ends with the explicit instruction: Answer with the options letter from the given choices directly. To ensure compatibility, we use model-specific templates sourced from TABLE III: Evaluation of accuracy (%) and set sizes: Comparative analysis of CAP (Ours) with standard Least Ambiguous set-valued Classifiers (LAC) [5], and Adaptive Prediction Sets (APS) [6] methods. The table highlights that our proposed method achieves the highest average accuracy across datasets, while maintaining balance in set sizes that avoids overly narrow or broad predictions observed in the baseline methods. Highest accuracy values are in bold and balanced set size values are underlined. Models VLMs LLaVA-v1.6-34B LLaVA-v1.6-13B LLaVA-v1.6-7B LLMs Yi-34B Qwen-14B Qwen-7B Method Accuracy (%) SS MMB OOD SQA SB AI2D Avg. MMB OOD SQA SB AI2D Avg. 87.73 86.75 88.57 82.29 81.75 82.66 81.36 80.60 82. 87.42 86.47 88.19 80.02 80.47 80.79 81.14 79.89 81.20 84.38 83.53 86.46 78.08 77.91 79.08 74.98 75.03 75. 81.72 81.39 81.64 77.83 77.37 77.50 76.96 76.65 76.34 83.22 82.55 88.36 80.39 79.95 84.87 77.44 77.03 81. 84.89 84.14 86.64 79.72 79.49 81.38 78.38 77.84 79.38 2.6501 1.2499 1.6519 3.1275 1.5573 2.6249 3.1540 1.5811 1. 1.6744 1.3101 1.6210 2.6857 1.6842 2.2271 2.9613 1.7250 2.2982 2.7269 1.2883 1.8447 3.2180 1.6884 2.1796 3.0303 1.8690 2. 2.6556 1.5854 1.9937 3.1280 1.8606 2.2776 3.1102 1.9445 2.3464 2.6386 1.4683 2.1755 3.0165 1.6505 2.3135 2.9752 1.7617 2. HSwg HDial CQA HSum MMLU HSwg HDial CQA HSum MMLU 95.21 93.90 96.17 93.75 91.98 94.02 72.46 72.12 73.79 83.99 83.17 85. 81.91 82.42 83.09 74.47 75.63 75.81 95.74 94.40 96.12 93.95 92.06 94.32 88.38 87.65 90.06 81.20 80.98 83. 62.86 64.22 57.59 52.48 52.91 47.75 80.64 80.44 82.90 74.43 74.26 76.13 67.47 68.07 72.25 87.76 86.98 88. 81.38 80.59 81.03 71.85 71.68 71.93 3.0254 1.0000 1.4790 3.0120 1.0000 1.3774 2.3844 2.0564 2.6116 2.0548 1.3992 2. 2.4050 1.4634 1.8742 2.9366 2.0014 2.8832 2.5868 1.0000 1.5664 2.7242 1.0008 1.3270 3.1336 1.1790 1.9172 1.8630 1.3934 1. 2.6036 2.3154 2.3764 3.0076 2.9220 2.5734 2.8206 1.5886 2.1220 2.9640 2.1026 2.5508 3.5344 2.4890 3.1820 2.4691 1.3804 1. 3.0351 1.6882 2.3245 3.0462 1.7763 2.2382 2.4701 1.2762 1.8186 2.7418 1.5764 1.9012 3.1993 2.1296 2.6335 APS LAC Ours APS LAC Ours APS LAC Ours APS LAC Ours APS LAC Ours APS LAC Ours their official GitHub repositories. Templates for CogAgent and InternLM-XComposer2 are obtained from Hugging Face. Due to the common constraint of single-image input in many VLMs, we exclude few-shot demonstrations. For more details on prompt template refer to Appendix C-B. For LLMs, we use the Base Prompt strategy, following [38]. This method concatenates the question with all answer options as the input prompt. The LLM is instructed to output the correct option using the prefix Answer:, ensuring standardized and straightforward input format for evaluation. For more details on prompt template please refer to Appendix C-B. B. Evaluation and Results We evaluate VLMs and LLMs with parameter sizes ranging from 7B to 34B, with detailed results presented in the following sections. Hallucination Detection: We evaluated the effectiveness of conformal methods in detecting hallucinations in VLM/LLM responses to multiple choice QA tasks. Here, hallucination refers to confidently asserted yet incorrect and arbitrary claims made by the model. Detecting them is framed as binary classification task: distinguishing correct from hallucinated (incorrect) responses using uncertainty estimates. AUROC serves as the primary evaluation metric [39], with higher scores indicating better separation of correct and incorrect predictions based on model confidence. As shown in Table I, our proposed CAP method consistently achieves higher AUROC scores than APS and LAC across various models and datasets. Notably, this improvement reaches up to 10.17% TABLE IV: Evaluation of Expected Calibration Error (ECE): Comparative analysis of the proposed CAP framework (Ours) with standard LAC [5], and APS [6] methods. The results show that the CAP method achieves significantly lower ECE values, in bold, compared to baseline."
        },
        {
            "title": "Method",
            "content": "ECE"
        },
        {
            "title": "MMB OOD SQA",
            "content": "SB AI2D Avg. LLaVA-v1.6-34B LLaVA-v1.6-13B LLaVA-v1.6-7B APS 0.1277 0.1261 0.2082 0.1356 0.2353 0.1666 LAC 0.0738 0.1124 0.1143 0.1312 0.1626 0.1109 0.0085 0.0302 0.0309 0.0342 0.0385 0.0285 Ours APS 0.1593 0.2218 0.1902 0.1607 0.2747 0.2013 LAC 0.1300 0.1698 0.1618 0.1759 0.1908 0.1657 0.0218 0.0159 0.0445 0.0601 0.0252 0.0335 Ours APS 0.1576 0.2439 0.2128 0.1704 0.2641 0.2098 LAC 0.1314 0.1974 0.1865 0.1797 0.1987 0.1787 0.0419 0.0252 0.0498 0.0581 0.0148 0.0380 Ours"
        },
        {
            "title": "LLMs",
            "content": "HSwg HDial CQA HSum MMLU Avg. Qwen-7B Qwen-14B Yi-34B APS 0.3470 0.2978 0.2327 0.4099 0.4032 0.3381 LAC 0.3222 0.2680 0.1479 0.4381 0.3474 0.3047 0.0807 0.0265 0.0772 0.1409 0.0485 0.0748 Ours APS 0.0901 0.1972 0.0996 0.2949 0.2729 0.1909 LAC 0.0156 0.1644 0.0278 0.3360 0.2273 0.1542 0.0134 0.0307 0.0266 0.1271 0.0170 0.0429 Ours APS 0.1111 0.3240 0.1554 0.2163 0.2479 0.2109 LAC 0.0514 0.2718 0.1030 0.1887 0.1727 0.1575 0.0522 0.1528 0.0990 0.0542 0.0337 0.0784 Ours on average and 22.19% in specific instances, demonstrating CAPs effectiveness in reliably detecting hallucinations. Uncertainty-Guided Selective Generation: We assess the conformal models ability to abstain from responses when uncertainty is high. Following [39] and [40], we evaluate this Fig. 1: Accuracy vs. Expected Calibration Error (ECE) comparison of CAP, APS, and LAC across various VLMs and five datasets: MMBench, ScienceQA, OODCV, SEEDBench, and AI2D. An ideal model has high accuracy and low ECE (upperleft). ATCP shows significant ECE improvement over baselines. Please refer to Figure 4 in Appendix C-D3 for complete list of figures. Fig. 2: Accuracy versus Expected Calibration Error (ECE) comparison between ATCP, APS and LAC methods across different LLMs and five datasets i.e. CosmosQA, HaluDial, HaluSum, HellaSwag, MMLU. The ideal model should have high accuracy and low ECE, indicating accurate predictions with well calibrated uncertainty quantification (upper-left of the plot). The ECE of ATCP shows significant improvement compared to baseline methods. Please refer to Figure 6 in Appendix C-D3 for the complete list of figures. using AUARC, which quantifies the trade-off between accuracy and abstention. Higher AUARC indicates better alignment of uncertainty estimates with prediction difficulty, enabling effective abstention in challenging examples while maintaining accuracy on confident predictions. As shown in Table I, our CAP method consistently outperforms APS and LAC in AUARC, with average improvements of up to 9.43% and peak gains of 21.17%. This demonstrates CAPs effectiveness in allowing better abstention policies and selective generation. Coverage Guarantee: CAP ensures that the true label is included in the prediction set (or as single prediction when confidence is high) with predefined probability (1 α, set to 90%). This target represents the minimum probability of capturing the true label, corresponding to maximum error rate of 10%. As shown in Table II, CAP consistently achieves at least 90% coverage across all datasets and VLMs. Larger prediction sets indicate greater uncertainty while preserving statistical guarantees. Compared to baselines, CAP effectively balances coverage and prediction set size. APS attains higher coverage rate but at introducing the cost of excessively large sets, greater uncertainty and reducing practical utility (Table III). Conversely, LAC produces smaller sets but frequently falls short of the 90% coverage target, compromising reliability (Table II). CAP optimally bridges these extremes, reliably maintaining the 90% guarantee while keeping prediction sets well-controlled. This balance ensures statistically valid and practically useful estimates. Accuracy and Set Size: Prediction set size is key indicator of model uncertainty, with smaller sets generally reflecting lower uncertainty. However, as shown in Table III, the relationship between accuracy and set size is not always straightforward. While accuracy remains relatively consistent Fig. 3: Performance comparison of CAP (Ours), APS, and LAC on Llava-v1.6-34B (VLM) and Yi-34B (LLM) across four metrics: (i) accuracy, (ii) set size, (iii) AUROC, and (iv) AUARC. Each figure shows model performance across ten benchmark datasets, illustrating the impact of conformal method on uncertainty metrics. across APS, LAC, and our method, set sizes vary significantly. APS achieves competitive accuracy but often produces overly large prediction sets, indicating higher uncertainty and less informative outputs. In contrast, LAC generates the smallest sets but at the cost of compromised coverage rates and slightly lower accuracy in some cases (Table III). Our CAP method achieves the highest average accuracy across datasets by leveraging the trainable adaptive threshold mechanism. It generally outperforms APS in accuracy, with only minor exceptions. In terms of set sizes, CAP consistently strikes balanceproducing smaller, more controlled sets than APS while avoiding LACs overly narrow sets that lead to under-coverage. As shown in Table III, the balanced set sizes of CAP are underlined. This ensures effective uncertainty management, which is key strength of our approach. Importantly, these findings demonstrate that accuracy alone is insufficient for evaluating conformal methods. Significant variations in set sizes, despite similar accuracy, highlight the importance of set size as distinct measure of performance and uncertainty. Accuracy and Expected Calibration Error: Calibration measures how well models confidence estimates reflect actual correctness. We assess this using expected calibration error (ECE), where lower values indicate better alignment between confidence and accuracy. As shown in Table IV, our CAP method consistently achieves significantly lower ECE than APS and LAC across all models and datasets. Notably, CAP improves calibration without compromising accuracy (Table III, Figure 1, Figure 2). Compared to APS, CAP reduces ECE by an average of 82.9% across all VLMs and LLMs (83.1% for VLMs, 82.7% for LLMs). Against LAC, CAP achieves an average ECE reduction of 74.8% (74.4% for VLMs, 75.2% for LLMs), demonstrating superior calibration. The combination of lower ECE and improved accuracy underscores CAPs advantage: it delivers not only accurate predictions but also reliable uncertainty estimates. High-confidence predictions correspond to higher likelihood of correctness, while uncertain predictions more accurately reflect the models limitations. Figure 1 and Figure 2 illustrate CAPs effectiveness, positioning it in the upper-left region of the accuracy-ECE space. Discussion and Limitations: Our extensive evaluations show that CAP can significantly improve over static uncertainty quantification methods by leveraging reinforcement learning to dynamically adjust thresholds, optimizing the trade-off between accuracy, coverage, and prediction set size. Empirically, CAP outperforms APS and LAC in hallucination detection (AUROC), selective generation (AUARC), and calibration error while maintaining valid coverage guarantees. As shown in Figure 3, CAP achieves higher accuracy than static baselines, balances prediction set sizes to prevent underor over-coverage, and significantly improves uncertainty-aware metrics. Additional results across are provided in Appendix C-D. However, integrating CP with RL introduces challenges. Learned policies may overfit, bias abstention strategies, or distort CPs theoretical guarantees. CAP also introduces additional parameters and relies on well-tuned reward functions, which may require careful optimization for different data distributions. Extreme distribution shifts or limited calibration data can further impact performance if the calibration set fails to capture relevant uncertainty signals. These risks need careful investigation and can be mitigated by enforcing distribution-aware regularization, calibrating policies through out-of-sample validation, and constraining reward functions to align with conformal principles. V. CONCLUSION In this work, we propose reinforcement learning-based approach to adaptively configure conformal prediction thresholds for selective abstention in large language and visionlanguage models. By dynamically adjusting the decision boundary between single-label, set-valued predictions, and abstentions, our method overcomes the limitations of static conformal approaches, such as rigid coverageuncertainty trade-offs and suboptimal confidence calibration. Extensive evaluations across diverse tasksfrom multiple-choice QA to image-based reasoningdemonstrate that our learned conformal abstention policy (CAP) outperforms APS and LAC, achieving higher accuracy, maintaining coverage guarantees, shrinking prediction sets, and reducing calibration error. Notably, CAP enhances hallucination detection and uncertaintyguided selective generation, highlighting the potential of coupling conformal prediction with adaptive policies for robust risk management in foundation models."
        },
        {
            "title": "REFERENCES",
            "content": "[1] X. Zhou, M. Liu, E. Yurtsever, B. L. Zagar, W. Zimmer, H. Cao, and A. C. Knoll, Vision language models in autonomous driving: survey and outlook, IEEE Transactions on Intelligent Vehicles, 2024. [2] N. Yildirim, H. Richardson, M. T. Wetscherek, J. Bajwa, J. Jacob, M. A. Pinnock, S. Harris, D. Coelho De Castro, S. Bannur, S. Hyland et al., Multimodal healthcare ai: identifying and designing clinically relevant vision-language applications for radiology, in Proceedings of the CHI Conference on Human Factors in Computing Systems, 2024. [3] S. Abdali, R. Anarfi, C. Barberan, and J. He, Securing large language models: Threats, vulnerabilities and responsible practices, arXiv preprint arXiv:2403.12503, 2024. [4] Y. A. Yadkori, I. Kuzborskij, D. Stutz, A. Gyorgy, A. Fisch, A. Doucet, I. Beloshapka, W.-H. Weng, Y.-Y. Yang, C. Szepesvari et al., Mitigating llm hallucinations via conformal abstention, arXiv preprint arXiv:2405.01563, 2024. [5] M. Sadinle, J. Lei, and L. Wasserman, Least ambiguous set-valued classifiers with bounded error levels, Journal of the American Statistical Association, vol. 114, no. 525, pp. 223234, 2019. [6] Y. Romano, M. Sesia, and E. Candes, Classification with valid and adaptive coverage, Advances in Neural Information Processing Systems, vol. 33, pp. 35813591, 2020. [7] V. Vovk, A. Gammerman, and G. Shafer, Algorithmic learning in random world. Springer, 2005, vol. 29. [8] V. Balasubramanian, S.-S. Ho, and V. Vovk, Conformal prediction for reliable machine learning: theory, adaptations and applications. Newnes, 2014. [9] A. N. Angelopoulos and S. Bates, gentle introduction to conformal prediction and distribution-free uncertainty quantification, arXiv preprint arXiv:2107.07511, 2021. [10] Y. Gal and Z. Ghahramani, Dropout as Bayesian approximation: Representing model uncertainty in deep learning, International Conference on Machine Learning, pp. 10501059, 2016. [11] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, Weight uncertainty in neural network, in International conference on machine learning. PMLR, 2015. [12] B. Lakshminarayanan, A. Pritzel, and C. Blundell, Simple and scalable predictive uncertainty estimation using deep ensembles, Advances in Neural Information Processing Systems, vol. 30, 2017. [13] R. Krishnan and O. Tickoo, Improving model calibration with accuracy versus uncertainty optimization, Advances in Neural Information Processing Systems, vol. 33, 2020. [14] J. Gawlikowski, C. R. N. Tassi, M. Ali, J. Lee, M. Humt, J. Feng, A. Kruspe, R. Triebel, P. Jung, R. Roscher et al., survey of uncertainty in deep neural networks, Artificial Intelligence Review, vol. 56, no. Suppl 1, pp. 15131589, 2023. [15] A. Kumar, P. S. Liang, and T. Ma, Verified uncertainty calibration, Advances in Neural Information Processing Systems, vol. 32, 2019. [16] F. Ye, M. Yang, J. Pang, L. Wang, D. F. Wong, E. Yilmaz, S. Shi, and Z. Tu, Benchmarking llms via uncertainty quantification, arXiv preprint arXiv:2401.12794, 2024. [17] J. Zhang, J. Kim, B. ODonoghue, and S. Boyd, Sample efficient reinforcement learning with reinforce, in Proceedings of the AAAI conference on artificial intelligence, vol. 35, no. 12, 2021, pp. 10 887 10 895. [18] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu et al., Mmbench: Is your multi-modal model an all-around player? in European conference on computer vision. Springer, 2025, pp. 216233. [19] B. Zhao, S. Yu, W. Ma, M. Yu, S. Mei, A. Wang, J. He, A. Yuille, and A. Kortylewski, Ood-cv: benchmark for robustness to outof-distribution shifts of individual nuisances in natural images, in European conference on computer vision. Springer, 2022, pp. 163 180. [20] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan, Learn to explain: Multimodal reasoning via thought chains for science question answering, Advances in Neural Information Processing Systems, vol. 35, pp. 25072521, 2022. [21] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, Seed-bench: Benchmarking multimodal llms with generative comprehension, arXiv preprint arXiv:2307.16125, 2023. [22] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi, diagram is worth dozen images, in Computer Vision ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14. Springer, 2016, pp. 235251. [23] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, Measuring massive multitask language understanding, arXiv preprint arXiv:2009.03300, 2020. [24] L. Huang, R. L. Bras, C. Bhagavatula, and Y. Choi, Cosmos qa: Machine reading comprehension with contextual commonsense reasoning, arXiv preprint arXiv:1909.00277, 2019. [25] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [26] J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, Halueval: large-scale hallucination evaluation benchmark for large language models, arXiv preprint arXiv:2305.11747, 2023. [27] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, 2023. [28] B. Lin, Z. Tang, Y. Ye, J. Cui, B. Zhu, P. Jin, J. Zhang, M. Ning, and L. Yuan, Moe-llava: Mixture of experts for large vision-language models, arXiv preprint arXiv:2401.15947, 2024. [29] Z. Li, B. Yang, Q. Liu, Z. Ma, S. Zhang, J. Yang, Y. Sun, Y. Liu, and X. Bai, Monkey: Image resolution and text label are important things for large multi-modal models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 26 76326 773. [30] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, X. Wei, S. Zhang, H. Duan, M. Cao et al., Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model, arXiv preprint arXiv:2401.16420, 2024. [31] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang et al., Yi: Open foundation models by 01. ai, arXiv preprint arXiv:2403.04652, 2024. [32] W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding et al., Cogagent: visual language model for gui agents, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 14 28114 290. [33] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, Qwen-vl: versatile vision-language model for untext reading, and beyond, arXiv preprint derstanding, arXiv:2308.12966, vol. 1, no. 2, p. 3, 2023. localization, [34] V. Kostumov, B. Nutfullin, O. Pilipenko, and E. Ilyushin, Uncertaintypreprint vision-language models, arXiv aware for evaluation arXiv:2402.14418, 2024. [35] J. Davis and M. Goadrich, The relationship between precision-recall and roc curves, in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 233240. [36] M. P. Naeini, G. Cooper, and M. Hauskrecht, Obtaining well calibrated the AAAI probabilities using bayesian binning, in Proceedings of conference on artificial intelligence, vol. 29, 2015. [37] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, Advances in neural information processing systems, vol. 36, 2024. [38] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing et al., Judging llm-as-a-judge with mt-bench and chatbot arena, Advances in Neural Information Processing Systems, vol. 36, pp. 46 59546 623, 2023. [39] S. Farquhar, J. Kossen, L. Kuhn, and Y. Gal, Detecting hallucinations in large language models using semantic entropy, Nature, vol. 630, no. 8017, pp. 625630, 2024. [40] R. Krishnan, P. Khanna, and O. Tickoo, Enhancing trust in large language models with uncertainty-aware fine-tuning, arXiv preprint arXiv:2412.02904, 2024. [41] X. Chu, L. Qiao, X. Zhang, S. Xu, F. Wei, Y. Yang, X. Sun, Y. Hu, X. Lin, B. Zhang et al., Mobilevlm v2: Faster and stronger baseline for vision language model, arXiv preprint arXiv:2402.03766, 2024. [42] Q. Ye, H. Xu, J. Ye, M. Yan, A. Hu, H. Liu, Q. Qian, J. Zhang, and F. Huang, mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13 04013 051. [43] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023."
        },
        {
            "title": "APPENDIX A\nFORMAL PROOF OF CONFORMAL COVERAGE GUARANTEE",
            "content": "the random permutation. Note that (cid:101)Yn+1 belongs to C( (cid:101)Xn+1) precisely if its nonconformity score We provide here classic proof of the coverage property for standard (single-threshold) conformal prediction under i.i.d. assumptions. In the main text, this lays the foundation for our two-threshold extension (see Section III), where an additional threshold is introduced to distinguish between single-label predictions, set-valued predictions, and abstentions. Despite that extension, the core argument below underpins the claimed coverage guarantee at level 1 α. Theorem 1 {(Xi, Yi)}n+1 partitioned into: (Conformal Coverage Guarantee). Let i=1 be i.i.d. samples from an unknown distribution, calibration set of size n: {(Xi, Yi)}n test point (Xn+1, Yn+1). i=1. Suppose nonconformity score function s(, ) assigns realvalued score s(Xi, Yi) to each calibration sample, capturing how atypical or nonconforming the pair (Xi, Yi) appears relative to prediction model. Denoting si = s(Xi, Yi), = 1, . . . , n, let ˆq be the (1 α)-quantile of these calibration scores: ˆq = Quantile (cid:16) {s1, . . . , sn}, 1 α (cid:17) . Then we define the conformal prediction set for the test point (Xn+1, ) as C(cid:0)Xn+1 (cid:1) = (cid:110) (cid:111) : s(cid:0)Xn+1, y(cid:1) ˆq . Under the i.i.d. assumption, this set satisfies (cid:16) Pr Yn+1 C(cid:0)Xn+1 (cid:1)(cid:17) 1 α. Proof. Because the samples {(Xi, Yi)}n+1 i=1 are assumed exchangeable (i.i.d.), any permutation of the + 1 points is equally likely. Consider random permutation π of the indices {1, . . . , + 1}, and let ( (cid:101)Xi, (cid:101)Yi) = (cid:0)Xπi, Yπi (cid:1) be the permuted data. We then treat the first permuted samples as calibration set, computing their nonconformity scores, (cid:101)si = s(cid:0) (cid:101)Xi, (cid:101)Yi (cid:1), = 1, . . . , n, and defining (cid:16) (cid:101)q = Quantile {(cid:101)s1, . . . , (cid:101)sn}, 1 α (cid:17) . The point ( (cid:101)Xn+1, (cid:101)Yn+1) is then the test sample in this permuted view, for which the conformal set is C(cid:0) (cid:101)Xn+1 (cid:1) = (cid:8) : s(cid:0) (cid:101)Xn+1, y(cid:1) (cid:101)q(cid:9). We must show that Pr(cid:0) (cid:101)Yn+1 C( (cid:101)Xn+1)(cid:1) 1 α with respect to the randomness of both the original samples and (cid:101)sn+1 = s(cid:0) (cid:101)Xn+1, (cid:101)Yn+1 (cid:1) does not exceed the (1 α)-quantile (cid:101)q. Equivalently, (cid:101)sn+1 is at most the (n + 1)(1 α)-th largest among {(cid:101)s1, . . . , (cid:101)sn+1}. By symmetry, (cid:101)sn+1 is equally likely to appear in any rank among the + 1 scores (cid:101)s1, . . . , (cid:101)sn+1. Hence, the probability that (cid:101)sn+1 falls above that critical rank is at most α. Therefore, (cid:101)Yn+1 / C( (cid:101)Xn+1) α, (cid:17) (cid:16) Pr (cid:16) and so Pr (cid:101)Yn+1 C( (cid:101)Xn+1) (cid:17) 1 α. Reversing the permutation π simply reverts the data to its original indexing. Because all permutations are equally likely, we conclude that, for the original test point (Xn+1, Yn+1), Pr(cid:0)Yn+1 C(Xn+1)(cid:1) 1 α. Interpretation in the Context of Two-Threshold Policies. Although Theorem 1 is stated for single threshold ˆq, the rank-based argument holds equally under mild modifications when additional thresholds are introduced. In the main text, we exploit two thresholds to partition nonconformity scores into regimes that yield single-label predictions, set-valued predictions, or abstentions. The coverage requirement is preserved provided that the relevant thresholds are computed against {(cid:101)s1, . . . , (cid:101)sn} (the calibration scores) and remain within the same unified conformal scoring framework. As result, the final coverage probability for the true label Yn+1 remains at least 1 α, up to the statistical deviations governed by the i.i.d. assumption on {(Xi, Yi)}. In our method (see Section III in the main paper), we further optimize these thresholds via reinforcement learning to improve accuracy, set size, and abstention outcomes. Nonetheless, the conformal criterion ensures that the proportion of samples for which the correct label lies outside the conformal set remains bounded by α."
        },
        {
            "title": "APPENDIX B\nTRAINING VIA REINFORCEMENT LEARNING",
            "content": "algorithm 1 summarizes the training of our proposed adaptive conformal environment and abstention policy."
        },
        {
            "title": "APPENDIX C\nEXPERIMENTAL DETAILS",
            "content": "A. Datasets This section provides details about the datasets used in our evaluation. We focus on two groups of datasets: one for VisionLanguage Models (VLMs) on multiple-choice visual question answering (MCQA) tasks, and another for Language Models (LLMs) across multiple tasks. Below, we describe the VLM datasets in detail."
        },
        {
            "title": "Datasets for VLMs",
            "content": "the evaluation of VisionFor Language Models, we focus on multiple-choice visual question Algorithm 1 Conformalized Abstention Policy with Reinforcement Learning Input: Calibration dataset Dcal, LLM/VLM model , learning rate η, policy network πθ, cost function C(α, β) Output: Optimized thresholds ˆqpredict, ˆqabstain for each episode do Sample α (µ(α) Compute nonconformity scores si = 1 pyi(xi) for all ) and β (µ(β) , σ(α)2 θ , σ(β)2 θ ) θ θ (xi, yi) Dcal Calculate quantile thresholds: ˆqpredict = Quantile({si}, (n + 1)(1 α)/n) ˆqabstain = Quantile({si}, (n + 1)(1 β)/n) for each test sample do Compute s(x) = 1 maxi pi(x) Compute action probabilities: psingle = σ(c[s(x) ˆqpredict]) pabstain = σ(c[s(x) ˆqabstain]) pset = 1 psingle pabstain Sample action {single, set, abstain} based on these probabilities end for Evaluate performance and compute cost C(α, β) Compute reward R(α, β) = C(α, β) Update policy parameters: θ θ + η R(α, β)θ log πθ(α, β) end for answering (MCQA) tasks. The following datasets are used, each addressing specific aspects of visual understanding and reasoning: Comprehensive Visual Understanding and Reasoning The MMBench dataset [18] evaluates models ability to perform tasks across 20 distinct capability dimensions, organized into two broad categories: perception and reasoning. It includes approximately 3,000 multiple-choice questions in the test set and 4,000 in the development set. Since the test set lacks ground truth answers, we use the development set for evaluation. Questions have between two to four answer options, and we standardize them to four options by adding randomly sampled incorrect answers when necessary. Out-of-Distribution Instance Counting The OODCVVQA dataset [19], part of safety evaluation benchmark, focuses on out-of-distribution instance counting tasks. We specifically use the Digits subset, where each question involves counting objects in images and has two answer options. To ensure consistency, we augment the options to four by randomly sampling incorrect digits not included in the original options."
        },
        {
            "title": "Scientific Reasoning with Visual Context",
            "content": "The ScienceQA dataset [20] tests models ability to answer scientific questions across three subjects: natural science, language science, and social science. We use the validation and test portions, selecting only image-based questions with closedchoice answers. The number of options ranges from two to five, and we standardize them to four by adding or removing incorrect options as needed, resulting in 3,952 questions. Multimodal Scene and Instance Understanding The SEED-Bench dataset [21] evaluates models capabilities across 12 dimensions, including spatial and temporal understanding. For our evaluation, we focus on dimensions 1-9, which are related to image modality and assess tasks such as scene understanding, instance identity, and instance location. We use 14,233 questions from this benchmark, each with four answer options."
        },
        {
            "title": "Diagram Understanding and Reasoning",
            "content": "The AI2D dataset [22] contains over 5,000 diagrams from elementary school science topics, accompanied by more than 15,000 multiple-choice questions. These questions test models ability to understand and reason about information presented in diagrams. Each question already includes four answer options, so no modifications are required. Standardization of Options To ensure consistency across all datasets, we append two additional choices (I dont know and None of the above) to the list of options for each question, expanding the total number of options to six. This provides uniform evaluation framework for all tasks. Datasets for LLMs: To comprehensively evaluate the capabilities of Language Models, we focus on five key natural language processing (NLP) tasks: question answering, reading comprehension, commonsense inference, dialogue response selection, and document summarization. Each task is formulated as multiple-choice question answering (MCQA) task, where the model must select the correct answer from six possible options (A, B, C, D, E, and F). Below, we describe the datasets used for each task. Question Answering (QA) For the question answering task, we use the MMLU dataset [23]. MMLU evaluates an LLMs ability to leverage its extensive world knowledge to answer questions across 57 diverse subjects, including elementary mathematics, US history, computer science, and law. These subjects are grouped into four broad categories: humanities, social sciences, STEM, and others (e.g., business, health, and miscellaneous topics). We sample 2,500 instances from each category, resulting in total of 10,000 questions for evaluation. Reading Comprehension (RC) The reading comprehension task assesses an LLMs ability to understand and analyze textual contexts, infer meanings, and draw conclusions based on the provided information. For this task, we use the CosmosQA dataset [24]. CosmosQA focuses on reasoning beyond explicit text spans, requiring models to interpret everyday narratives and infer implicit meanings. Since ground truth labels for the test set are unavailable, we sample 10,000 instances from the training and development sets for evaluation. Commonsense Inference (CI) Commonsense inference evaluates an LLMs ability to reason about relationships between concepts and events using background knowledge and commonsense understanding. We employ the HellaSwag dataset [25] for this task. HellaSwag focuses on natural language inference, requiring models to select the most plausible continuation of given event description. Similar to CosmosQA, we sample 10,000 instances from the training and development sets of HellaSwag for evaluation. Dialogue Response Selection (DRS) The dialogue response selection task tests an LLMs ability to understand conversational contexts and select appropriate responses that maintain coherence and relevance. For this task, we use the dialogue data from the HaluEval benchmark [26], specifically the HaluDial subset. HaluDial is derived from OpenDialKG (Moon et al., 2019), knowledge-grounded dialogue dataset, and consists of exactly 10,000 instances for evaluation. Document Summarization (DS) Document summarization evaluates an LLMs ability to comprehend and condense lengthy documents into concise summaries that capture the main ideas and key information. For this task, we use the summarization data from the HaluEval benchmark [26], specifically the HaluSum subset. HaluSum is derived from the CNN/Daily Mail dataset (See et al., 2017), which focuses on summarizing news articles, and contains exactly 10,000 instances for evaluation."
        },
        {
            "title": "Standardization of Options",
            "content": "To ensure consistency across all datasets, we standardize the number of answer options. While MMLU, CosmosQA, and HellaSwag originally provide four options per question, HaluDial and HaluSum the include only two options. For the latter, we augment options by randomly selecting additional choices from other questions within the same dataset. Additionally, we append two universal options, dont know and None of the above, to every question, resulting in six possible options for all datasets. B. Prompting Templates This section describes the prompting strategies and templates used for evaluating VLMs and LLMs. The templates are designed to ensure consistent and effective evaluation across different model families. Prompting Templates for VLMs: For Vision-Language Models, we adopt standardized prompting strategy tailored for multiple-choice visual question answering (MCQA) tasks. The template is inspired by the approach used in LLaVA (Liu et al., 2024) and is designed to maximize compatibility across various VLM architectures. The prompt structure is as follows: The prompt begins with an attached image, serving as the primary visual input for the model. The question text follows, optionally including hint if available. Six answer options are presented line by line, each prefixed with its corresponding letter (A-F). Additional choices such as dont know and None of the above are also included to account for uncertainty. The prompt concludes with an explicit instruction: Answer with the options letter from the given choices directly. For models requiring specific multimodal token format, the image is prepended with designated image token, such as <image> or model-specific tokens like DEFAULT_IMAGE_TOKEN, ensuring compatibility with different VLM architectures. Depending on the model type, the prompt is wrapped within structured conversational template. Examples include Vicuna-style conversation for LLaVA, structured input for CogVLM, Yi-VL, and Qwen-VL, ensuring consistency in processing. To accommodate the constraints of single-image input in many VLMs, we intentionally exclude few-shot demonstrations from the prompts. The templates are adapted for specific model families, including LLaVA, Yi-VL, Qwen, Monkey, MoE-LLaVA, mPLUG-Owl, and MobileVLM, using their respective official repositories. For CogAgent and InternLMXComposer2, the templates are sourced from their Hugging Face repositories."
        },
        {
            "title": "Below is the base prompt template format utilized in our",
            "content": "experiments: Image: {<Image>} Question: {Question Text} Hint: {Optional Hint Text} Choices: A. {Content of option A} B. {Content of option B} C. {Content of option C} D. {Content of option D} E. dont know F. None of the above Answer with the options letter from the given choices directly. TABLE V: This table presents the structured prompt template used for multiple-choice question answering in VLMs. Each prompt consists of an attached image, question (optionally with hint), and six answer choices, including uncertainty options (I dont know and None of the above). To maintain consistency across different VLM architectures, model-specific input tokens (e.g., <image> or DEFAULT_IMAGE_TOKEN) are included when necessary. The prompt concludes with direct instruction for the model to answer using the letter corresponding to its chosen option. This template ensures consistent format for evaluating VLMs across diverse datasets and tasks. The inclusion of six options (A-F) standardizes the evaluation process, while the explicit instruction at the end guides the model to provide direct response. Prompting Templates for LLMs: For Language Models, we employ base prompting strategy without any strategy such as shared instruction or task-specific instruction prompt in order maintain standardized approach across evaluations. This prompt is designed to evaluate several model performances across multiple tasks, including question answering (QA), reading comprehension (RC), commonsense inference (CI), dialogue response selection (DRS), and document summarization (DS). The prompt template is designed to provide consistent structure for all tasks while accommodating taskspecific information. The structure of the base prompt is as follows: The prompt begins with the task-specific context, dialogue, or document: For QA tasks, no background information is included. For RC and CI tasks, the keyword Context introduces the relevant background information. For DRS tasks, the keyword Dialogue incorporates the dialogue history. For DS tasks, the keyword Document includes the document content. The question is presented next, followed by list of six answer options: Four standard options (A-D) with task-specific content. Two additional options: dont know and None of the above. The model is instructed to provide the letter corresponding to the correct answer."
        },
        {
            "title": "Below is the base prompt template format utilized in our",
            "content": "experiments: Context/Dialogue/Document: {The context or dialogue history or document corresponding to the following question} Question: {Question} Choices: A. {Content of option A} B. {Content of option B} C. {Content of option C} D. {Content of option D} E. dont know F. None of the above Answer with the options letter from the given choices directly. TABLE VI: This table presents the structured prompt template used for multiple-choice question answering in LLMs. In the QA setting, no additional background information is included. For the RC and CI tasks, the keyword Context is introduced to incorporate relevant background information. Similarly, the keywords Dialogue and Document are used for DRS and DS tasks, respectively, to integrate necessary context. This template ensures standardized format for evaluating LLMs across diverse tasks. For instruction-finetuned LLMs, the entire prompt input is treated as the users message, and the apply chat template function is used to transform the prompt into chat format, ensuring compatibility with chatbased models. C. Additional Models Evaluated This appendix provides additional details on the VisionLanguage Models (VLMs) and Large Language Models (LLMs) that complement those discussed in the main body of the paper. These models were evaluated to broaden the scope of our analysis across different architectures and parameter scales. For VLMs, we include results for several additional models. Monkey-Chat 7B [29] is vision-language model optimized for multimodal chat-based reasoning. InternLMXComposer2-VL 7B [30] enhances vision-language interaction through structured prompts, while Yi-VL 6B [31] is smaller variant of the Yi-VL series, designed for effective image-text understanding. CogAgent-VQA 7B [32] focuses on visual question answering with robust reasoning capabilities. MobileVLMV2 7B [41] is lightweight VLM tailored for mobile and edge applications. Additionally, mPLUGOwl2 7B [42] offers strong image-text understanding capabilities, and Qwen-VL-Chat 7B [33] is designed for dialoguedriven multimodal interactions. For LLMs, we also present results for the Llama-2 7B and 13B models [43], which serve as foundation models with strong text generation and reasoning capabilities. The inclusion of these models extends the scope of our evaluation, providing comprehensive comparison across diverse architectures and parameter scales. D. Additional Results 1) Results of VLMs Additional results in Table VII, Table VIII, and Table IX demonstrate the performance of multiple VLMs mentioned in Appendix in terms of uncertainty quantification i.e. AUROC vs AUARC, coverage rate vs set size, and accuracy vs expected calibration error respectively. As shown in these tables, our CAP model outperforms other methods in hallucination detection and uncertainty guided selective generation while satisfying the minimum coverage rate of 90% in all instances and maintaining the middle ground in set size balancing this for all cases. 2) Results of LLMs Additional results in Table X, Table XI, and Table XII demonstrate the performance of Llama-2 series models (7B and 13B) discussed in Appendix in terms of uncertainty quantification i.e. AUROC vs AUARC, coverage rate vs set size, and accuracy vs expected calibration error respectively. As shown in these tables, our CAP model outperforms other methods in hallucination detection and uncertainty guided selective generation while satisfying the minimum coverage rate of 90% in all instances and maintaining the middle ground in set size balancing this for all cases. 3) Accuracy vs ECE: Figure 5 shows the results of accuracy vs ECE achieved using CAP versus APS and LAC across multiple VLMs. Lower ECE values indicate better calibration, signifying that confidence scores are more reliable indicators of prediction accuracy. As shown in these figures, CAP was able to improve the accuracy while significantly reducing the expected calibration error. Moreover, Figure 7 shows the same trend in LLMs consistently reducing ECE while improving accuracy across all tasks and datasets. 4) Effect of Model Scale To examine the impact of model scale, we analyzed the performance of our CAP method across models of varying TABLE VII: Evaluation of uncertainty quantification: Comparative analysis of the proposed CAP (Ours) meth with standard Least Ambiguous set-valued Classifiers (LAC) [5], and Adaptive Prediction Sets (APS) [6] methods (the best values are in bold). The comparison includes different datasets and VLM models, with quality of uncertainty quantification evaluated using the Area Under the Receiver Operating Characteristic (AUROC) and the Area Under the Accuracy-Rejection Curve (AUARC). Best values are in bold."
        },
        {
            "title": "Method",
            "content": "AUROC (Hallucination Detection) AUARC (Uncertainty guided selective generation)"
        },
        {
            "title": "SQA",
            "content": "SB AI2D Avg."
        },
        {
            "title": "SQA",
            "content": "SB AI2D Avg. Monkey-Chat-7B InternLM-XComposer2-VL-7B CogAgent-VQA-7B MobileVLM-v2-7B mPLUG-Owl2-7B Qwen-VL-Chat-7B Yi-VL-6B MoE-LLaVA-Phi2-2.7B"
        },
        {
            "title": "APS\nLAC\nOurs",
            "content": "0.6360 0.6855 0.7241 0.6648 0.6861 0.7068 0.6416 0.7003 0.7432 0.7646 0.7168 0.7368 0.5347 0.6575 0.6920 0.6230 0.6557 0. 0.6094 0.6785 0.7432 0.6359 0.6864 0.7360 0.2994 0.4151 0.5182 0.5000 0.5275 0.6295 0.3448 0.3396 0.5175 0.3836 0.3963 0. 0.4550 0.5069 0.6316 0.4610 0.4057 0.5348 0.3616 0.4638 0.6284 0.5785 0.5614 0.7147 0.4916 0.6501 0.6739 0.7010 0.7524 0. 0.4930 0.5693 0.6355 0.5652 0.6777 0.6672 0.3855 0.4828 0.5766 0.5156 0.5394 0.6079 0.5674 0.5780 0.6446 0.5248 0.4810 0. 0.5304 0.4596 0.5550 0.4731 0.4810 0.5773 0.5274 0.4844 0.5346 0.4153 0.4617 0.5695 0.3421 0.3692 0.5169 0.4990 0.4624 0. 0.4747 0.4387 0.5471 0.4199 0.4849 0.5772 0.7662 0.6716 0.7340 0.6421 0.6429 0.7035 0.5341 0.4245 0.4867 0.4867 0.3539 0. 0.4862 0.3432 0.4792 0.6786 0.6511 0.6990 0.4486 0.4246 0.5331 0.4282 0.4142 0.5352 0.5447 0.5764 0.6410 0.5962 0.6180 0. 0.5082 0.5036 0.5835 0.5231 0.5213 0.5916 0.4407 0.4719 0.5793 0.5554 0.5429 0.6161 0.4923 0.5167 0.6193 0.5175 0.5256 0. 0.9285 0.8988 0.9652 0.9267 0.9001 0.9667 0.9240 0.8996 0.9746 0.9610 0.9307 0.9682 0.9625 0.9247 0.9650 0.8882 0.8593 0. 0.9517 0.9198 0.9676 0.9446 0.9070 0.9655 0.7640 0.7137 0.9174 0.7999 0.7807 0.9219 0.7469 0.7015 0.9264 0.8712 0.8196 0. 0.8706 0.8383 0.9244 0.6872 0.6425 0.9171 0.8790 0.8461 0.9228 0.7610 0.7360 0.9477 0.8950 0.8646 0.9686 0.9537 0.9301 0. 0.8448 0.8130 0.9608 0.9503 0.9133 0.9698 0.9111 0.8677 0.9415 0.8052 0.7851 0.9313 0.9012 0.8606 0.9551 0.8522 0.8083 0. 0.8579 0.8028 0.9335 0.8642 0.8322 0.9261 0.8741 0.8251 0.9471 0.9296 0.8673 0.9194 0.9134 0.8447 0.9051 0.7918 0.7616 0. 0.9023 0.8501 0.9187 0.8815 0.8298 0.9342 0.8635 0.8413 0.9747 0.8879 0.8624 0.9624 0.7828 0.7483 0.9553 0.8508 0.7866 0. 0.8628 0.7949 0.9066 0.8536 0.8292 0.9688 0.8747 0.8276 0.9312 0.8061 0.7576 0.9284 0.8618 0.8242 0.9519 0.8865 0.8611 0. 0.8345 0.7975 0.9528 0.9126 0.8635 0.9369 0.9041 0.8541 0.9285 0.8052 0.7755 0.9407 0.9018 0.8608 0.9391 0.8491 0.8077 0. TABLE VIII: Evaluation of coverage rate (%) and set size: Comparative analysis of the proposed CAP (Ours) meth with standard LAC [5], and APS [6] methods. The comparison includes different datasets and VLM models, show casing the satisfied coverage rate and balanced set sizes produced by our method with underlined values."
        },
        {
            "title": "Method",
            "content": "Coverage (%) SS"
        },
        {
            "title": "SQA",
            "content": "SB AI2D Avg. MMB"
        },
        {
            "title": "OOD",
            "content": "Monkey-Chat-7B InternLM-XComposer2-VL-7B CogAgent-VQA-7B MobileVLM-v2-7B mPLUG-Owl2-7B Qwen-VL-Chat-7B Yi-VL-6B MoE-LLaVA-Phi2-2.7B"
        },
        {
            "title": "APS\nLAC\nOurs",
            "content": "97.85 89.45 93.33 96.57 89.17 93.51 98.54 90.68 94.15 97.99 89.63 92.78 99.27 89.81 92.65 96.21 88.44 93. 98.63 90.22 93.38 99.50 89.26 92.10 96.27 88.75 91.35 92.48 88.96 90.01 95.64 90.37 92.12 96.27 90.86 91. 95.08 89.52 91.28 93.46 88.75 90.37 95.43 89.94 91.35 93.95 89.17 91.63 98.84 90.44 94.69 98.74 89.58 92. 97.47 90.14 93.53 99.04 89.07 94.18 98.18 91.40 91.91 92.01 88.11 90.44 98.13 89.78 92.41 97.07 90.84 92. 96.50 89.22 92.03 94.46 89.90 90.21 95.94 89.36 93.59 97.67 89.49 91.53 97.09 89.94 91.94 92.97 89.21 91. 95.94 89.84 90.11 97.60 89.66 91.34 97.28 90.98 94.36 96.28 89.87 92.43 93.83 90.65 94.25 95.87 90.23 90. 95.81 90.34 89.52 96.70 89.69 93.94 96.77 91.01 90.99 96.50 90.08 90.84 97.35 89.77 93.15 95.71 89.50 91. 96.28 90.24 93.53 97.37 89.86 92.04 97.09 90.20 91.46 94.27 88.84 91.76 96.98 90.16 91.61 96.92 89.80 91. 3.787 1.611 2.383 3.479 1.966 2.763 2.997 1.665 2.175 3.439 1.629 2.159 3.365 1.727 2.080 3.413 1.990 2. 3.326 1.621 2.082 3.669 2.181 2.987 2.575 1.819 2.457 2.944 1.971 2.757 3.074 2.153 2.623 2.485 1.689 2. 3.589 3.049 3.673 2.506 1.536 1."
        },
        {
            "title": "SQA",
            "content": "3.455 1.656 2.567 3.383 1.443 1.926 2.833 1.895 2.506 3.610 1.625 2.329 3.346 2.070 2.401 3.349 2.451 3. 3.503 2.009 2.574 SB AI2D Avg. 4.013 2.505 3.285 3.578 2.584 3. 2.996 1.975 3.015 3.494 2.106 2.567 3.431 2.432 2.753 3.692 2.945 3.504 3.116 2.106 2.522 4.040 2.346 3. 3.673 2.358 2.902 3.240 2.640 3.652 3.866 2.925 3.448 3.379 2.624 2.934 3.796 2.394 3.061 3.491 2.514 2. 3.793 2.060 2.848 3.338 2.034 2.634 3.002 2.030 2.821 3.497 2.088 2.625 3.201 2.109 2.446 3.568 2.566 3. 3.189 1.957 2.414 3.4961 1.5843 2.0461 2.2651 1.5204 2.1280 3.2969 2.0976 2.6631 3.3834 2.0021 2.5178 3.3425 2.4891 2. 3.1568 1.9387 2.4613 TABLE IX: Evaluation of accuracy (%) and ECE: Comparative analysis of the proposed CAP (Ours) meth with standard LAC [5], and APS [6] methods. The comparison includes different datasets and VLM models, demonstrating the significant reduction in expected calibration error while improving overall accuracy."
        },
        {
            "title": "Method",
            "content": "Accuracy (%) ECE"
        },
        {
            "title": "SQA",
            "content": "SB AI2D Avg. MMB"
        },
        {
            "title": "SQA",
            "content": "SB AI2D Avg. Monkey-Chat-7B InternLM-XComposer2-VL-7B CogAgent-VQA-7B MobileVLM-v2-7B mPLUG-Owl2-7B Qwen-VL-Chat-7B Yi-VL-6B MoE-LLaVA-Phi2-2.7B"
        },
        {
            "title": "APS\nLAC\nOurs",
            "content": "81.40 81.26 84.03 76.72 77.30 78.46 81.07 80.55 83.29 80.79 80.85 82.12 78.94 78.67 79.78 76.71 76.78 79. 80.54 80.70 81.82 79.51 79.80 81.62 76.75 77.06 78.61 77.04 77.88 78.07 76.86 76.91 79.72 74.86 75.23 75. 80.48 79.91 80.88 62.45 63.91 67.14 81.23 80.67 81.05 82.14 81.16 83.06 79.27 80.89 82.37 81.73 82.72 84. 75.98 76.16 79.25 78.11 78.90 79.66 73.87 74.53 75.28 70.04 71.12 73.18 74.40 74.77 76.03 72.74 74.08 74. 72.33 72.58 74.57 70.80 71.72 72.15 76.03 75.48 76.18 74.30 74.28 73.95 70.54 69.76 69.88 66.86 67.42 69. 74.59 74.18 73.99 74.51 74.86 75.86 73.58 74.53 78.70 72.52 73.13 75.02 67.99 67.98 69.60 63.37 63.41 64. 65.42 64.91 65.96 71.25 72.10 76.11 67.98 68.29 69.72 66.56 66.66 69.02 76.67 77.26 79.66 75.76 76.55 77. 75.59 75.42 77.61 74.29 74.53 75.18 73.85 73.55 74.36 69.46 70.27 73.20 75.75 75.72 76.52 75.89 75.71 76. 0.2134 0.1480 0.0159 0.1805 0.1284 0.0341 0.2310 0.1608 0.0134 0.1460 0.1202 0.0464 0.1578 0.1453 0.0473 0.2350 0.1653 0. 0.1694 0.1263 0.0308 0.2067 0.1377 0.0224 0.3583 0.3042 0.0336 0.2179 0.1871 0.0173 0.3614 0.3407 0.0470 0.2230 0.2239 0. 0.1858 0.1574 0.0352 0.3944 0.3684 0.0316 0.1720 0.1594 0.0332 0.2863 0.2385 0.0991 0.2696 0.1857 0.0190 0.1727 0.1073 0. 0.3043 0.2340 0.0366 0.1790 0.1363 0.0306 0.2260 0.2093 0.0439 0.2487 0.2249 0.0403 0.2553 0.2136 0.0287 0.2687 0.2212 0. 0.2825 0.2494 0.0336 0.2203 0.2093 0.0593 0.2327 0.2151 0.0246 0.1691 0.1927 0.0780 0.1982 0.2384 0.0863 0.2937 0.2739 0. 0.1837 0.2019 0.0650 0.2476 0.2100 0.0333 0.3237 0.2608 0.0207 0.2417 0.1776 0.0289 0.3148 0.2912 0.0110 0.2838 0.2932 0. 0.2588 0.2857 0.0668 0.3211 0.2510 0.0087 0.2621 0.2565 0.0504 0.3266 0.2825 0.0238 0.2895 0.2296 0.0246 0.2066 0.1620 0. 0.2889 0.2484 0.0265 0.2002 0.1933 0.0539 0.2053 0.2072 0.0559 0.2986 0.2567 0.0270 0.2085 0.1915 0.0416 0.2672 0.2180 0. TABLE X: Evaluation of uncertainty quantification: Comparative analysis of the proposed CAP (Ours) meth with standard Least Ambiguous set-valued Classifiers (LAC) [5], and Adaptive Prediction Sets (APS) [6] methods (the best values are in bold). The comparison includes different datasets and LLM models, with quality of uncertainty quantification evaluated using the Area Under the Receiver Operating Characteristic (AUROC) and the Area Under the Accuracy-Rejection Curve (AUARC). Best values are in bold."
        },
        {
            "title": "Method",
            "content": "AUROC (Hallucination Detection) AUARC (Uncertainty guided selective generation)"
        },
        {
            "title": "HSum MMLU",
            "content": "Avg."
        },
        {
            "title": "HSum MMLU",
            "content": "Avg. Llama2-7B Llama2-13B"
        },
        {
            "title": "APS\nLAC\nOurs",
            "content": "0.4884 0.4079 0.7066 0.6225 0.4685 0.6396 0.4646 0.2623 0.7040 0.3460 0.2007 0.5043 0.6378 0.5490 0.7724 0.5186 0.6377 0. 0.6353 0.7205 0.7672 0.4092 0.3478 0.6255 0.4495 0.3594 0.6324 0.4132 0.3808 0.5572 0.5351 0.4598 0.7165 0.4619 0.4071 0. 0.3473 0.3395 0.8681 0.5788 0.5591 0.9254 0.3301 0.2891 0.8354 0.5065 0.4801 0.8134 0.5296 0.5185 0.9599 0.7893 0.7710 0. 0.2962 0.2923 0.9078 0.4709 0.4580 0.8986 0.5774 0.5496 0.8935 0.7455 0.6950 0.9177 0.4161 0.3978 0.8929 0.6182 0.5926 0. TABLE XI: Evaluation of coverage rate (%) and set size: Comparative analysis of the proposed CAP (Ours) meth with standard LAC [5], and APS [6] methods. The comparison includes different datasets and LLM models, show casing the satisfied coverage rate and balanced set sizes produced by our method with underlined values."
        },
        {
            "title": "Method",
            "content": "Coverage (%) SS"
        },
        {
            "title": "CQA HSum MMLU",
            "content": "Avg."
        },
        {
            "title": "CQA HSum MMLU",
            "content": "Avg. Llama2-7B Llama2-13b"
        },
        {
            "title": "APS\nLAC\nOurs",
            "content": "90.02 90.66 90.38 89.70 89.88 90.11 90.44 89.96 90.42 90.32 90.62 90.41 91.78 90.08 91.22 97.06 90.52 94. 89.72 89.22 89.78 90.26 89.98 90.30 92.50 90.54 91.04 95.86 89.18 93.62 90.89 90.09 90.56 92.64 90.03 91. 3.346 3.253 3.378 2.801 2.497 3.071 3.257 3.251 3.252 2.571 2.535 2.537 2.661 2.275 2.316 2.881 1.568 2. 3.227 3.423 3.360 2.306 2.117 2.122 3.319 3.021 3.191 3.320 2.578 3.104 3.162 3.044 3.099 2.776 2.259 2. TABLE XII: Evaluation of accuracy (%) and ECE: Comparative analysis of the proposed CAP (Ours) meth with standard LAC [5], and APS [6] methods. The comparison includes different datasets and LLM models, demonstrating the significant reduction in expected calibration error while improving overall accuracy."
        },
        {
            "title": "Method",
            "content": "Accuracy (%) ECE"
        },
        {
            "title": "CQA HSum MMLU",
            "content": "Avg."
        },
        {
            "title": "HSum MMLU",
            "content": "Avg. Llama2-7B Llama2-13b"
        },
        {
            "title": "APS\nLAC\nOurs",
            "content": "54.86 55.05 61.00 70.38 70.46 73.31 50.54 50.24 57.78 66.84 66.69 64.28 74.43 74.10 80.32 83.42 83.26 85. 60.78 59.23 68.36 72.59 72.34 73.44 59.33 59.11 64.13 65.48 64.79 68.11 59.99 59.55 66.32 71.74 71.49 73. 0.5720 0.5784 0.0606 0.4165 0.4183 0.0814 0.5934 0.5927 0.0572 0.4207 0.4373 0.0721 0.5085 0.4915 0.1953 0.3462 0.2808 0. 0.6176 0.6127 0.1693 0.4666 0.4638 0.1559 0.4894 0.4703 0.0414 0.3930 0.3444 0.0203 0.5562 0.5491 0.1048 0.4086 0.3889 0. Fig. 4: Accuracy vs. Expected Calibration Error (ECE) comparison of CAP, APS, and LAC across various VLMs and five datasets: MMBench, ScienceQA, OODCV, SEEDBench, and AI2D. An ideal model has high accuracy and low ECE (upperleft). ATCP shows significant ECE improvement over baselines. sizes. As shown in Figure 8, larger models generally achieve higher accuracy, with the most significant gains observed when scaling from 13B to 34B parameters. Prediction set size inversely correlates with model scale, as larger models produce smaller sets, reflecting greater precision and reduced uncertainty. Additionally, AUROC and AUARC improve consistently with increasing model scale, indicating that larger models are not only more accurate but also less prone to hallucinations and better at abstaining when uncertainty is high. To examine the impact of model scale, we analyzed the performance of our CAP method across models of varying sizes. larger models generally achieve higher accuracy and produce smaller set sizes while showing better performance in avoiding hallucinations and uncertainty guided selective generation. As shown in Figure 8, Figure 10, and Figure 12, the most significant gains observed when scaling the model size from 13B to 34B parameters. Prediction set size inversely correlates with model scale, as larger models produce smaller sets, reflecting greater precision and reduced uncertainty. Additionally in Figure 9, we can see slight gains in all metrics comparing VLMs with 7B parameters against Yi-VL with 6B parameters. However, since size differences in VLMs in this particular benchmark are not very different, part of the gap in results between model would be associated with different finetuning methods used for these models and the pre-trained model under the hood. Fig. 5: Accuracy versus Expected Calibration Error (ECE) comparison between CAP, APS and LAC methods across different VLMs and five datasets i.e. MMBench, ScienceQA, OODCV, SEEDBench, AI2D. The ideal model should have high accuracy and low ECE, indicating accurate predictions with well calibrated uncertainty quantification (upper-left of the plot). The ECE of ATCP shows significant improvement compared to baseline methods. E. Size Distribution of Predicted Set The distribution of prediction types provides insights into our models decision-making behavior across different visionlanguage tasks. As shown in Figure 13, LLaVA-1.6-34B demonstrates preference for set predictions across all benchmarks, with set prediction rates ranging from 55.4% (AI2D) to 62.4% (ScienceQA). This suggests that the model frequently identifies multiple plausible answers rather than committing to single prediction due to underlying uncertainty in VLM response. Single predictions constitute substantial portion of responses, varying between 31.5% to 38.8%, indicating scenarios where the model exhibits high confidence in unique answer. The abstention rates show notable variation across datasets, from 6.1% on ScienceQA to 12.8% on AI2D, reflecting the models ability to recognize and acknowledge uncertainty in different visual reasoning contexts. This trend repeats for Yi-34B LLM across five different tasks as well. This distribution pattern demonstrates that our selective prediction approach effectively captures different levels of model uncertainty, allowing for more nuanced and reliable responses across diverse vision-language tasks. Fig. 6: Accuracy versus Expected Calibration Error (ECE) comparison between ATCP, APS and LAC methods across different LLMs and five datasets i.e. CosmosQA, HaluDial, HaluSum, HellaSwag, MMLU. The ideal model should have high accuracy and low ECE, indicating accurate predictions with well calibrated uncertainty quantification (upper-left of the plot). The ECE of ATCP shows significant improvement compared to baseline methods. Fig. 7: Accuracy versus Expected Calibration Error (ECE) comparison between ATCP, APS and LAC methods across different LLMs and five datasets i.e. CosmosQA, HaluDial, HaluSum, HellaSwag, MMLU. The ideal model should have high accuracy and low ECE, indicating accurate predictions with well calibrated uncertainty quantification (upper-left of the plot). The ECE of ATCP shows significant improvement compared to baseline methods. Fig. 8: Performance comparison of VLMs with different model sizes (2.7B to 34B) across various metrics. Figures from left to right represents the performance of four models on one of the four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we have drawn the performance of models across five datasets in VLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics. Fig. 9: Performance comparison of additional VLMs with different model sizes (6B to 7B) across various metrics. Figures from left to right represents the performance of four models on one of the four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we have drawn the performance of models across five datasets in VLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics. Fig. 10: Performance comparison of LLMs with different model sizes (7B to 34B) across various metrics. Figures from left to right represents the performance of four models on one of the four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we have drawn the performance of models across five datasets in LLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics. Fig. 11: Performance comparison of Llama-2 series LLMs with different model sizes (7B and 13B) across various metrics. Figures from left to right represents the performance of two models on four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we show the performance of models across five datasets in LLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics. Fig. 12: Performance comparison of Llama-2 series LLMs with different model sizes (7B and 13B) across various metrics. Figures from left to right represents the performance of two models on four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we show the performance of models across five datasets in LLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics. Fig. 13: Distribution of CAPs prediction types for LLaVA-1.6-34B (VLM) and Yi-34B (LLM). The models responses are categorized into single predictions (confident single answers), set predictions (multiple possible answers), and abstentions (declining to answer)."
        }
    ],
    "affiliations": [
        "Intel Labs",
        "University of Illinois at Chicago"
    ]
}