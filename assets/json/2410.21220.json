{
    "paper_title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines",
    "authors": [
        "Zhixin Zhang",
        "Yiyuan Zhang",
        "Xiaohan Ding",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the user's question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, a novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 2 ] . [ 1 0 2 2 1 2 . 0 1 4 2 : r Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines Zhixin Zhang1 Yiyuan Zhang1,2 Xiaohan Ding3 Xiangyu Yue1 1 MMLab, CUHK 2 Shanghai AI Laboratory 3 Tencent https://github.com/cnzzx/VSA"
        },
        {
            "title": "Abstract",
            "content": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the users question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs visual understanding capabilities and web agents real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs. 1. Introduction The advent of Large Language Models (LLMs) [1, 3, 13, 34, 38, 41] has significantly enhanced the human capacity to acquire unfamiliar knowledge through powerful zero-shot Question-Answering (QA) capabilities. Building upon these advancements, techniques such as RetrievalAugmented Generation (RAG) [39, 42, 47] have further reinforced LLMs in knowledge-intensive, open-domain QA tasks. Concurrently, recent progress in visual instruction tuning [29, 30, 50] has led to the development of large Vision-Language Models (VLMs) that aim to equip LLMs with visual understanding capabilities. By scaling model parameters and training on extensive text-image datasets, VLMs such as LLaVA-1.6-34B [29], Qwen2-VL-72B [5], and InternVL2-76B [11] have achieved state-of-the-art performance on the OpenVLM leaderboard1. However, LLMs and VLMs are subject to the limitations imposed by their knowledge cutoff dates. They may provide incorrect answers when asked about events or concepts that occurred after their knowledge cutoff dates (Figure 1) To overcome this limitation for LLMs, they are often connected to web agents [4, 10, 14, 32, 33], which enable internet access and information retrieval, allowing them to obtain the most up-to-date data and improve the accuracy of their responses. Such agents are designed to interpret natural language instructions, navigate complex web environments, and extract relevant textual information from HTML documents, thereby enhancing the accessibility and utility of vast amounts of web-based textual data for wide range of applications. However, for VLMs facing unseen images and novel concepts, their ability to learn and use up-to-date multimodal knowledge from the internet remains pressing challenge. As the existing web agents predominantly rely on searching the users question and summarizing the returned HTML content, they present notable limitation when handling tasks involving images or other visual content: the visual information is often overlooked or inadequately processed. In this work, we enable the VLM to answer question regarding an unseen image or novel concept, which behaves like human searching the Internet. It 1) understands the query, 2) decides which objects in the image it should look at and infers the correlations among the objects, 3) respectively generates the texts to search, 4) analyzes the contents returned from the search engine based on the query and inferred correlations, and 5) judges if the obtained visual and textual information is sufficient for generating the answer 1https : / / huggingface . co / spaces / opencompass / open_vlm_leaderboard Figure 1. Vision Search Assistant acquires unknown visual knowledge through web search. An intuitive comparison of answering the users question with an unseen image. The proposed Vision Search Assistant is developed based on LLaVA-1.6-7B, and its ability to answer the question on unseen images outperforms the state-of-the-art models including LLava-1.6-34B [29], Qwen2-VL-72B [5], and InternVL2-76B [11]. or it should iterate and refine the above process. Regarding the concrete designs of such framework, we make contributions by answering the following three questions that remained unanswered in the literature What to search: shall we search the descriptions of the whole image or some critical objects? How to search: shall we search once and summarize huge amount of returned content or search progressively to obtain more related content? By what to conclude: shall the final answer be generated with the eventually summarized web knowledge or all the knowledge acquired through the entire searching process? By exploring such three aspects, we propose Vision Search Assistant, framework based on VLM-agent collaboration, which empowers an arbitrary VLM to become multimodal automatic search engine. We integrate VLMs into web agents to understand what the user wants, where to look, what to search, how to learn from the returned multimodal information, and whether to conclude or search another time. More specifically, Vision Search Assistant conducts three steps (Figure 3): Visual Content Formulation ( 3.1) is proposed to represent the visual content with VLM-generated textural descriptions of critical visual objects and their underlying correlations. Through this step, we obtain correlated formulation for each critical object, which is textual representation that considers its correlations with other objects. Web Knowledge Search ( 3.2) is novel algorithm that drives the search process. It generates multiple subquestions with the web agent regarding the user prompt and the correlated formulation of each critical object. Each of such sub-questions can be viewed as node in directed graph. For each correlated formulation and each sub-question, we construct the search query by combining the correlated formulation and sub-question and use the LLM to analyze and select useful contents returned from the search engine, then summarize the web knowledge from the answers obtained with all such sub-questions. After that, we iterate the above step by proposing more sub-questions based on the previous subquestions and known web knowledge, which can be seen Figure 2. Comparsion with Closed-Source Models including GPT-4o [34], Gemini [37], Claude 3.5 Sonnet [3] with Vision Search Assistant shows that Vision Search Assistant satisfies users needs better even if the image is novel. as expanding the directed graph. We use the LLM to judge if the latest iteration has obtained sufficient web knowledge to answer the users question and terminate the process if so. Collaborative Generation ( 3.3)) is proposed to use the VLM to generate the eventual answer with all the critical objects in the image, the initial question, all of their correlated formulations, and the web knowledge obtained in every iteration. As shown in Figure 2, Vision Search Assistant can generate more precise answers than powerful closed-source models such as GPT-4o [34], Gemini [37], and Claude 3.5 Sonnet [3], which further validates the necessity and promising improvement of VLM-Agents collaboration in tackling the growing complexity of multimodal web data and the rapid influx of novel visual content. 2. Related Work Vision-Language Models. Pioneering models such as Flamingo [2], BLIP-2 [28], LLaVA [30], and MiniGPT4 [50] have been instrumental in training vision-language models for the tasks such as image captioning and visual question answering. Recent works focus on higher-quality datasets [17] and developing lightweight, trainable models [16] to enhance efficiency and accessibility. Further progress includes extending large language models (LLMs) to additional modalities and domains, such as audio processing [8, 21], and more modalities [15, 20, 48]. Additionally, KOSMOS-2 [35], InternVL2 [11], MiniGPT-2 [9], and LLaVA-1.5 [29] incorporate region-level information by encoding visual regions to embeddings of language models. However, despite scaling model parameters and training data, VLMs ability to handle unseen images remains limited, as they heavily rely on previously seen textimage pairs. To overcome this, we propose to enhance VLMs performance on novel data by improving generalization without relying solely on extensive training pairs. Web Search Agents. The development of web search agents has progressed through integrating advanced learning techniques, enhancing autonomy, and optimizing efficiency in web automation. Early models like WebGPT [33] and WebGLM [32] primarily focused on retrieving information for question-answering tasks, while newer models, such as AutoWebGLM [27], address deployment challenges with compact designs. Despite their strong web navigation skills, larger models such as WebAgent [18] are constrained by size. Incorporating reinforcement learning [4] and behavior cloning [49] has further boosted the efficiency of web agents, as demonstrated by MindAct [14], which integrates cognitive functionalities for complex task execution. While these advances are leading to more scalable and versatile solutions for real-world use, current web agents still struggle with processing visual content directly from the web. We introduce Vision-Language Models to enable web agents to effectively interpret and interact with visual data, significantly expanding their capabilities in handling complex, multimodal tasks. We hope it can make web agents more powerful and adaptable in real-world applications. Retrieval-Augmented Generation. Integrating retrieval from large corpora into language models has become essenFigure 3. Overview of Vision Search Assistant. We first identify the critical objects and generate their descriptions considering their correlations, named Correlated Formulation, using the Vision Language Model (VLM). We then use the LLM to generate sub-questions that leads to the final answer, which is referred to as the Planning Agent. The web pages returned from the search engine are analyzed, selected, and summarized by the same LLM, which is referred to as the Searching Agent. We use the original image, the users prompt, the Correlated Formulation together with the obtained web knowledge to generate the final answer. Vision Search Assistant produces reliable answers, even for novel images, by leveraging the collaboration between VLM and web agents to gather visual information from the web effectively. tial for knowledge-intensive tasks like open-domain question answering. Instead of relying solely on pre-trained data, the retriever-reader architecture [7, 19] enables models to fetch relevant information based on an input query, which the language model then uses to generate accurate predictions. Recent research has enhanced retrievers [25, 26, 36, 43, 44], improved readers [6, 12, 23, 45], jointly fine-tuned both components [22, 24, 40, 46], and integrated retrieval directly within language models [39, 42, 47]. Therefore, we propose the Vision Search Assistant framework, which introduces an open-world retrievalaugmented generation framework that extends beyond textbased retrieval to operate across both vision and language It enables VLMs to access realmodalities on the web. time, dynamic information, improving their ability to handle novel, cross-modal queries. By pushing the boundaries of retrieval beyond static knowledge sources, we address the challenge of incorporating web-based, multimodal data into generative tasks, offering more adaptable and scalable solution for RAG. 3. Vision Search Assistant 3.1. Visual Content Formulation The Visual Content Formulation is proposed to extract the object-level descriptions and correlations among objects in an image. Given the input image XI , we first use the openvocab detector Fdet() [31] to obtain regions of interests in the original image, {X (i) }N i=1 = Fdet(XI ), (1) where indicates the i-th region (i) in the image XI . Then we employ the pretrained VLM 2 Fvlm(, ) to caption these regions {X (i) i=1 conditioned on the tokenized users textual prompt XT , and obtain the visual caption }N 2Our experiments are conducted with LLaVA-1.6-Vicuna-7B model, which is publicly available at https : / / huggingface . co / liuhaotian/llava-v1.6-vicuna-7b. (i) for the i-th region: (i) = Fvlm(X (i) , XT ). (2) }N In this way, we make the regional captions {X (i) i=1 contain specific visual information obtained based on the users interests. To formulate the visual content more comprehensively, we further correlate these visual regions to obtain precise descriptions of the whole image. More specifically, for each region, we concatenate its corresponding caption and the captions of all the other regions. The resultant text, denoted by [X (i) }j=i], encodes the underlying correlations. It is fed into the VLM together with the image region (i) . The output is referred to as the correlated formulation of each region {X (i) i=1. , {X (j) }N (i) = Fvlm(X (i) , [X (i) , {X (j) }j=i])). (3) We will use the correlated formulations of such regions to perform the following web search. 3.2. Web Knowledge Search: The Chain of Search The core of Web Knowledge Search is an iterative algorithm named Chain of Search, which is designed to obtain the comprehensive web knowledge of the correlated formui=1. We take an arbitrary i-th region (i) lations {X (i) to elaborate on the Chain of Search algorithm and drop the superscript (i) for convenience. }N We use the LLM in our VLM to generate sub-questions that lead to the final answer, which is referred to as the Planning Agent. The web pages returned from the search engine are analyzed, selected, and summarized by the same LLM, which is referred to as the Searching Agent. In this way, we can obtain web knowledge regarding the visual content. Then, based on each of such sub-questions, the Planning Agent generates more sub-questions, and the Searching Agent obtains web knowledge for the next iteration. Formally, we define directed graph to represent this process, which is = V, E, where = {V0} is the set of nodes, V0 is the initial node, and = is the set of edges. node represents set of known information so that V0 should represent what we know about the region before any web search, i.e., the correlated formulation Xc. This is formulated as V0 Xc. When we search with sub-question, we will update the graph with new node representing the web knowledge gained through the sub-question. For the 1-st update, we generate sub-questions based ) = is the number of sub-questions, i.e., on V0 and denote the generated sub-questions by (X 1 {(X 1 the number of new nodes. i=1, where 1 )i}N Let be the index of the sub-question, the new node is child of V0, which corresponds to search with (1) Figure 4. The Chain of Search algorithm ( 3.2). We deduce the update of the directed graph when = 1, 2, , and the web knowledge is progressively extracted from each update. sub-question (X 1 )j. The returned set of web pages are formatted as HTML documents. The Searching Agent uses the LLM in our VLM, which is denoted by Fllm(), to judge their relevance to the parent node V0 and the corresponding sub-question (X 1 )j and select those of the highest relevance. The selected web page index τ 1 can be formulated = Fllm([V0, (X 1 τ )j]) . (4) }. We derive the search response R1 We use τ 1 to select subset of the HTML documents at the 1-st update, and those selected for sub-question are denoted by {P 1 for sub-question at the 1-st update by summarizing the selected pages with the LLM, which is R1 }). By the definition of the directed graph, the new node (1) . We add (1) should represent R1 R1 into the node set and (V0, (1) ) into the edge set. In this paper, (1) is synonymous with the search response R1 obtained with sub-question (X 1 , that is, (1) = Fllm({P j Then, we summarize the search responses of all the 1 nodes at the 1-st update and obtain the comprehensive web knowledge (1) )j. , which is denoted by = Fllm([R1 (1) 1, R1 For the following updates with > 1, we expand the 2, , R1 1 (5) ]) . graph similarly but with minor differences: For each node at update k1, we use the LLM to generate further sub-questions, just like how we expand V0 at the 1-st update. When we select the most relevant web pages for node , we analyze their relevance to not only V0 and the )j (just like the 1-st upV (k) corresponding sub-question (X date), but also the search response of its parent node. When we summarize the comprehensive web knowledge (k) , except for the search responses of all the nodes at the current update, we also use all the known comprehensive web knowledge {X (i) }k1 i=1 and the search responses m}{m=N ,n=k1} of all the previous nodes {Rn {m=1,n=1} . Figure 5. Open-Set Evaluation: We conduct human expert evaluation on open-set QA tasks. Vision Search Assistant significantly outperformed Perplexity.ai Pro and GPT-4o-Web across three key objectives: factuality, relevance, and supportiveness. Model LLava-1.6-7B (Baseline) LLava-1.6-7B (naive search) LLava-1.6-7B (w/ 3.2) Vision Search Assistant Conversation (%) Detail (%) Reasoning (%) Overall (%) 72.9 70.3 72.6 73.3 (+0.4) 76.5 76.7 78.9 79.3 (+2.8) 84.2 85.8 89.8 95.0 (+10.8) 78.5 78.9 82.7 84.9 (+6.4) Table 1. Closed-Set Evaluation on the LLaVA-W benchmark. We use GPT-4o (0806) for evaluation. Naive search here denotes the VLM with Google image search. Formally, when > 1: =Fllm([V0, (X τ =Fllm({X (i) )j, Rk1 i=1 , {Rn ]), m}{m=N (k) ,n=k1} }k1 }N i=1). (6) At each update, the search agent uses the LLM to judge if the knowledge currently obtained is sufficient to answer the initial question. If so, we terminate the process. {m=1,n=1} , {Rk 3.3. Collaborative Generation We use the original image XI , the users initial prompt }N i=1 together i=1 to collaboraXT , and the Correlated Formulations {X (i) with the obtained web knowledge {X (i) }N tively generate the final answer with the VLM: = Fvlm(XI , {X (i) }N i=1, {X (i) }N i=1, XT ) . (7) 4. Experiments 4.1. Open-Set Evaluation Setup. In the Open-Set Evaluation, we performed comparative assessment by 10 human experts evaluation, which involved questions of 100 image-text pairs collected from the news from July 15th to September 25th covering all fields on both novel images and events. Human experts conducted the evaluations across three critical dimensions: factuality, relevance, and supportiveness. Results and Analysis. As illustrated in Figure 5, Vision Search Assistant demonstrated superior performance across all three dimensions compared to Perplexity.ai Pro and GPT-4-Web: 1) Factuality: Vision Search Assistant scored 68%, outperforming Perplexity.ai Pro (14%) and GPT-4-Web (18%). This significant lead indicates that Vision Search Assistant consistently provided more accurate and fact-based answers. 2) Relevance: With relevance score of 80%, Vision Search Assistant demonstrated substantial advantage in providing highly pertinent answers. In comparison, Perplexity.ai Pro and GPT-4-Web achieved 11% and 9%, respectively, showing significant gap in their ability to maintain topicality with the web search. 3) Supportiveness: Vision Search Assistant also outperformed the other models in providing sufficient evidence and justifications for its responses, scoring 63% in supportiveness. Perplexity.ai Pro and GPT-4-Web trailed with scores of 19% and 24%, respectively. These results underscore the superior performance of Vision Search Assistant in open-set tasks, particularly in delivering comprehensive, relevant, and well-supported answers, positioning it as an effective method for handling novel images and events. 4.2. Closed-Set Evaluation Setup. We conduct the closed-set evaluation on the LLaVAW [29] benchmark, which contains 60 questions regarding the Conversation, Detail, and Reasoning abilities of VLMs in the wild. We use the GPT-4o(0806) model for evaluation. We use LLaVA-1.6-7B as our baseline model, that has been evaluated in two modes: the standard mode and naive search mode that utilizes simple Google Image search component. Additionally, an enhanced version Figure 6. Comparisons among Qwen2-VL-72B, InternVL2-76B, and Vision Search Assistant. We compare the open-set QA results on both novel events (the first two rows) and images (the last two rows). Vision Search Assistant excels in generating accurate and detailed results. Figure 7. Ablation Study on What to Search. We use the object description to avoid the visual redundancy of the image. of LLaVA-1.6-7B, equipped with improvements outlined in section 3.2, is also evaluated. Results and Analysis. As shown in Table 1, the Vision Search Assistant demonstrates the strongest performance across all categories. Specifically, it achieves 73.3% score in the conversation category, representing modest gain of +0.4% compared to the LLaVA models. In the detail category, the Vision Search Assistant stands out with score of 79.3%, outperforming the highest-performing LLaVA variation by +2.8%. When it comes to reasoning, our method brings out +10.8% above the best-performing LLaVA model. This suggests that the Vision Search Assistants advanced integration of visual and textual search greatly enhances its reasoning capabilities. The overall performance of the Vision Search Assistant is 84.9%, marking an improvement of +6.4% over the baseline model. This shows that the Vision Search Assistant excels in both conversational and reasoning tasks, giving it clear advantage Figure 8. Ablation Study on How to search. We propose the Chain of Search ( 3.2) to obtain related web knowledge for VLMs progressively. Figure 9. Ablation Study on Complex Scenarios. We use visual correlation to improve the ability in multiple-object scenarios. for in-the-wild abilities. 4.3. Ablation Study What to search: Object-Level Descriptions. As illustrated in Figure 7, if we use the image-based caption, the search agent can not precisely focus on the key information (the handbag in this figure), meanwhile, the image contains visual redundancy, which obstacles the textual description to drive web agent and retrieve the most relevant web pages, therefore, we use the object-level description in the following ablation study. Complex Scenarios of Search: Visual Correlation. We find that the caption can not fully support the search ability in multiple-object scenarios. As shown in Figure 9, the caption of Biden can not answer the questions on the groupwise debate, the visual correlation (debate in this demo) between Trump can effectively improve the answer quality. How to search: Chain of Search ( 3.2). The trivial idea to incorporate web search with VLMs is to introduce Google search engine and re-rank the large-scale related pages. As shown in Figure 8, we found it difficult to directly obtain the required knowledge since the page-rank method prefers more hyper-link pages instead of exact relevance. The VLM is also limited to its context length to summarize large number of pages. Therefore, we propose the chain of search and enable the progressive summary of web knowledge aiming to answer the users questions. 5. Conclusion and Discussion In this paper, we seek to improve the generalization ability of VLMs of novel images and extend the capacity of web agents to solve visual content tasks. Through the synergistic collaboration between VLMs and web agents, we find that VLMs can generate more reliable answers regarding novel images with the help of real-time web knowledge retrieval, and web agents can solve more challenging tasks than HTML documents only. Meanwhile, there are also some limitations inside the Vision Search Assistant framework such as the exact inference speed of VLMs, the web condition of web agents, and the retrieval efficiency. We hope this paper can inspire more research to address the challenges of VLMs in user experience and improve the automation abilities of web agents across diverse modalities."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 3 [3] Anthropic. Introducing the next generation of claude. 2024. 1, 3 [4] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-thewild device-control agents with autonomous reinforcement learning. arXiv preprint arXiv:2406.11896, 2024. 1, 3 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1, 2 [6] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 22062240. PMLR, 2022. [7] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18701879, 2017. 4 [8] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160, 2023. 3 [9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: arXiv preprint for vision-language multi-task learning. arXiv:2310.09478, 2023. 3 [10] Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agentflan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881, 2024. 1 [11] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 1, 2, 3 [12] Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Unitedqa: hybrid approach for open domain question answering. arXiv preprint arXiv:2101.00178, 2021. [13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 1 [14] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024. 1, 3 [15] Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, and Ying Shan. Unireplknet: universal perception large-kernel convnet for audio video point cloud time-series and image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55135524, 2024. 3 [16] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 3 [17] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: vision and lanarXiv preprint guage model for dialogue with humans. arXiv:2305.04790, 2023. 3 [18] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. realworld webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. [19] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pretraining. In International conference on machine learning, pages 39293938. PMLR, 2020. 4 [20] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26584 26595, 2024. 3 [21] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023. 3 [22] Gautier Izacard and Edouard Grave. Distilling knowledge arXiv from reader to retriever for question answering. preprint arXiv:2012.04584, 2020. 4 [23] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282, 2020. [24] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. ArXiv, abs/2208.03299, 2022. 4 [25] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wentau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. 4 [26] Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. Few-shot reranking for multiarXiv preprint hop qa via language model prompting. arXiv:2205.12650, 2023. 4 [27] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: Bootstrap and reinforce large language model-based web navigating agent. arXiv preprint arXiv:2404.03648, 2024. 3 [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 3 [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 1, 2, 3, 6 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, [31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 4 [32] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. Webglm: Towards an efficient web-enhanced question answering system with human preferences. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 45494560, 2023. 1, 3 [33] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. 1, 3 [34] OpenAI. Hello gpt4-o. 2024. 1, 3 [35] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 3 [36] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191, 2020. 4 [37] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [38] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language models for dialogue. OpenAI blog, 2022. 1 [39] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023. 1, 4 [40] Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of multi-document reader and retriever for open-domain question answering. Advances in Neural Information Processing Systems, 34: 2596825981, 2021. 4 [41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [42] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Interleaving retrieval with chain-ofAshish Sabharwal. thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022. 1, [43] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. 4 [44] Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad, Wentau Yih, Sebastian Riedel, Douwe Kiela, et al. Answering complex open-domain questions with multi-hop dense retrieval. arXiv preprint arXiv:2009.12756, 2020. 4 [45] Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering. arXiv preprint arXiv:2110.04330, 2021. 4 [46] Wenhao Yu. Retrieval-augmented generation across heterogeneous knowledge. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, pages 5258, 2022. 4 [47] Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, Improving language models arXiv preprint feedback. and Ashish Sabharwal. via plug-and-play retrieval arXiv:2305.14002, 2023. 1, [48] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Metatransformer: unified framework for multimodal learning. arXiv preprint arXiv:2307.10802, 2023. 3 [49] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. 3 [50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 3 Figure 10. series of demos of Vision Search Assistant on novel images, events, and in-the-wild scenarios. Vision Search Assistant delivers promising potential as powerful multimodal engine."
        }
    ],
    "affiliations": [
        "MMLab, CUHK",
        "Shanghai AI Laboratory",
        "Tencent"
    ]
}