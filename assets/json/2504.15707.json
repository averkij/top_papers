{
    "paper_title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark",
    "authors": [
        "Yannic Neuhaus",
        "Matthias Hein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/RePOPE ."
        },
        {
            "title": "Start",
            "content": "RePOPE: Impact of Annotation Errors on the POPE Benchmark"
        },
        {
            "title": "Matthias Hein",
            "content": "Tubingen AI Center University of Tubingen 5 2 0 2 2 2 ] . [ 1 7 0 7 5 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/ RePOPE. 1. Introduction The POPE [7] dataset has become standard benchmark for object hallucinations in vision large language models (VLMs) and is frequently used by the research community, e.g. as part of the OpenVLM Leaderboard [4]. The most commonly used version of the POPE benchmark relies on the MSCOCO [14] image dataset which provides exhaustive annotations for 80 different object classes. It is known that image datasets such as MSCOCO contain significant amounts of annotation errors [12]. In this work, we identify these errors and examine how they influence the results of the POPE benchmark by evaluating corrected version which we denote as RePOPE. Our contributions are: We assess annotation quality for the MSCOCO images used in the POPE benchmark. We provide RePOPE, corrected label set for the benchmark, and show that the errors significantly impact the results. 2. POPE POPE [7] evaluates object hallucinations as binary classification task, prompting VLMs with the question Is there object in the image?. The most common variant of POPE is based on 500 randomly selected images from the validation set of MSCOCO [14] which contain at least 3 objects according to the annotations. For each image, 6 questions are constructed, three with ground truth Yes and three with ground truth No. While the three Yes-questions can be directly derived from the MSCOCO annotations, questions with answer No are built by sampling from the non-annotated objects for the corresponding image. Note that all 80 MSCOCO object classes were exhaustively annotated for all images, i.e. all objects that are not annotated can be assumed to be not present in the image. There are three sampling strategies proposed in [7], resulting in 3 variants of the benchmark. Note that all three variants share the same images as well as the same set of questions with answer Yes, and differ only in the set of questions with No. The three strategies for sampling those objects are: Random Sampling: 3 objects are randomly sampled from all objects that are not annotated for the image Popular Sampling: the 3 most frequent objects in the image dataset which are not annotated for this image Adversarial Sampling: the 3 objects co-occuring most frequently with the objects that are actually present in the image In total, POPE consists of 3 sets of image-question pairs, each containing 1500 pairs with answer Yes and 1500 pairs with answer No. 3. RePOPE The construction of POPE relies on the original MSCOCO annotations. We re-annotate all 500 images and assign the labels: Yes if the object is visible in the image, No if the object is not visible in the image, Ambiguous for corner cases where it is not clear whether the object is present or not, based on consensus decision of two human labelers. Fig. 1 presents examples for our re-labeling in . For the positive set of POPE, i.e. the images with ground truth Yes, most of the observed label errors are due to the presence of visually similar or related object, e.g. scooter mistaken for motorcycle, mouse and keyboard labeled as laptop, parsley on plate as broccoli, and kite carried over the shoulder as an umbrella. On the other hand, annotaFigure 1. RePOPE annotation examples The first row displays images that do not contain the object but are incorrectly labeled as Yes in POPE. The second row shows images where the object is present but mistakenly labeled as No. The objects presence is highlighted with red box. The third row illustrates cases of inconsistent labeling in POPE, which we annotate as ambiguous. Examples include teddy bear being categorized as bear, motorcycle being considered motorized bicycle, and airport vehicles being classified as cars. Since these categorizations are subjective and MSCOCO labels are inconsistent, we exclude such cases from the benchmark. tion errors on the negative set, i.e. images with ground truth No, occur due to the very subtle presence of the object. The original annotators missed persons in the background or behind glass, the tennis player occludes the chairs in the background and the cole slaw contains only small visible stripe of carrot. For some objects, the COCO annotations are highly inconsistent likely due to differing definitions of those objects used by the original annotators. The classification of teddy bear as bear, motorcycle as motorized bicycle, or an airport vehicle as car depends on specific definitions, leading to inconsistencies in POPE ground truth annotations. Therefore, we annotate the corresponding image-question pairs as ambiguous. Tab. 1 presents the results of our re-labeling. We observe much higher error rate for the positive questions, i.e. the ones with answer Yes, with 9.3% annotation errors and 13.8% ambiguous labels. In contrast, negative questions show lower error rate, with 1.7% labeling errors and 4.3% ambiguous labels. The increasing error rate across the three subsets aligns with the expected occurrence frequency in the benchmark design where random includes randomly selected objects, popular consists of frequently occurring objects, and adversarial features objects that frequently co-occur. POPE: Yes POPE: No RePOPE Labels Yes No Amb. Yes No Amb. Random Popular Adversarial 76.9% 9.3% 13.8% 0.3% 98.4% 1.3% 2.6% 93.0% 4.4% 2.2% 90.5% 7.3% Total 76.9% 9.3% 13.8% 1.7% 94.0% 4.3% Table 1. Results of the re-annotation The positive questions are identical for all three POPE variants. Among the questions with POPE answer Yes, we find 9.3% label errors and 13.8% ambiguous cases where the correct label is not clear. For the questions with POPE answer No, only 1.7% of the questions have wrong label and 4.3% are ambiguous. 4. Experiments For our corrected benchmark RePOPE, we correct all ground truth labels where the original annotations disagree with our re-labeling (yes/no or no/yes) and remove all image-question pairs that were annotated as ambiguous. We evaluate models on both label sets and compare the resulting metrics, either considering the values on the individual splits (random, popular, adversarial) or the mean over all three (mean). 4.1. Models We evaluate range of open-weight models on POPE and RePOPE covering different architectures and model Included are also some of the top models for sizes. POPE on OpenVLM Leaderboard [4]: InternVL2.5 [3] (8B/26B/38B/78B and 8B-MPO/26B-MPO), LLaVANeXT [9] (Vicuna[11]/Mistral[6]/Llama[5]), LLaVAOneVision [8], Ovis2 [10] (1B/2B/4B/8B), PaliGemma3B [2] and PaliGemma2 [13] (3B/10B). 4.2. Results In this section, we investigate how the biased distribution of label errors impacts the results on the POPE benchmark. In Fig. 2, we show how the number of true positives (TP) and false positives (FP) changes after the relabeling (as there is almost no variance across the positve image-question pairs of the three variants, we only report the mean over all three for TPs). While TP counts drop significantly, FP changes follow more nuanced pattern. On the random subset, the number of FP almost doubles for most models, i.e. half of the objects that are falsely recognized by the models are covered by annotation errors on this POPE variant. This questions how reliable this kind of error can be measured on this split and suggests that the negative set is saturated on POPE random. For the adversarial variant, the number of FPs even decreases, most likely due to higher prevalence of label errors on the negative set, i.e. by selecting images of frequently co-occurring objects it gets more likely that the object of interest is also in the image. Note that the rankings are relatively stable considering these counts. This is also true when considering precision and recall. In general, the models show an improved recall on RePOPE while their precision decreases but the rankings stay roughly similar for both metrics and reflect the ranking according to the yes ratio (see Fig. 3). Nevertheless, the relative shifts significantly impact the ranking according to F1 scores (POPEs main metric) as shown in Fig. 2. On the random subset, the top models of the RePOPE ranking, Ovis2-4B and - 8B, are aligned with the top models for both POPE and RePOPE on the popular and adversarial subsets, indicating that the larger number of false positives in the subset enables better measurement of hallucinations. Some models that achieve some of the best F1 scores in the POPE ranking, e.g. InternVL2.5-8B or -26B, drop to the bottom of the ranking after evaluating on the RePOPE labels. similar pattern holds for the accuracy scores (as shown in Fig. 4). However, as the corrected labels are not balanced anymore with respect to the amount of positive and negative samples, acccuracy values on RePOPE might be biased. We provide full results tables for POPE (App. B) and RePOPE (App. C) in the appendix. 5. Conclusion In this work, we explored the impact of annotation errors in the MSCOCO image dataset on the results of the POPE object hallucination benchmark. We observe substantial True Positives (TP) and False Positives (FP) F1 Scores Figure 2. POPE vs RePOPE: Due to the high error rate on the positive labels, the number of TP is significantly reduced across all models. Regarding FP, we observe different patterns across the three subsets: on the random subset, the number of false positives almost doubles for most of the models, results on popular are relatively stable, and on adversarial we observe slight reduction of false positives. The ranking according to the F1 score is heavily impacted by the relabeling. The top models (Ovis2-4B/-8B) on the popular and adversarial split for POPE, also achieve the top ranks on random for RePOPE. larger amount of label errors on the positive set of POPE (answer Yes) compared to the negative set (answer No) which translates into significant change in the F1 score rankings after re-labeling the images. This significant influence of the identified annotation errors on the benchmark results highlights the importance of data quality. To enable more robust measurement of vulnerability to object hallucinations, we provide the corrected labels under the name RePOPE. Note that this re-labeling has only limited effect on the saturation of the benchmark (many models acchieve true positive rates as well as true negative rates of more than 90%. To overcome this, other benchmarks need to be evaluated complimentary, e.g. DASH-B [1] which follows similar design as POPE with harder negative set."
        },
        {
            "title": "References",
            "content": "[1] Augustin et al. of systematic hallucinations of vlms. arXiv:2503.23573, 2025. 4 Dash: Detection and assessment arXiv preprint [2] Beyer et al. PaliGemma: versatile 3B VLM for transfer, 2024. 3 source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [4] Duan et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 1, 3 [5] Dubey et al. The llama 3 herd of models, 2024. 3 [6] Jiang et al. Mistral 7b, 2023. 3 [7] Li et al. Evaluating object hallucination in large visionlanguage models, 2023. 1 [8] Li et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3 [9] Liu et al. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 3 [10] Lu et al. Ovis: large language model. Structural embedding alignment arXiv preprint for multimodal arXiv:2405.20797, 2024. 3 [11] Peng et al. Instruction tuning with gpt-4, 2023. 3 [12] Schubert et al. Identifying label errors in object dearXiv preprint tection datasets by loss inspection. arXiv:2303.06999, 2023. 1 [13] Steiner et al. Paligemma 2: family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555, 2024. 3 [14] Tsung-Yi et al. Microsoft coco: Common objects in context, [3] Chen et al. Expanding performance boundaries of open2015. 1 A. Appendix: Additional Plots We show additional scatter plots including precision, recall, true negative rates and yes ratios in Fig. 3 as well as accuracy over all POPE subsets in Fig. 4. B. Appendix: Full Results POPE We show all results for all models on the random (Tab. 3), popular (Tab. 4), and adversarial (Tab. 5) split as well as the mean over all three subsets (Tab. 2). C. Appendix: Full Results RePOPE We show all results for all models on the random (Tab. 7), popular (Tab. 8), and adversarial (Tab. 9) split as well as the mean over all three subsets (Tab. 6). Figure 3. POPE vs. RePOPE: Precision and Recall The models precision decreases on RePOPE while the true positive rate (TPR) improves. Effects on the true negative rate (TNR) are small but sufficient to change the ranking, and due to the larger amount of label errors in the positive questions, the models yes rates decrease on the relabeled dataset. Figure 4. POPE vs. RePOPE: Accuracy We observe similar pattern as for the F1 score. However, note that for RePOPE the number of positive and negative samples is not balanced anymore. Thus, accuracy needs to be interpreted with care. Pos. Model F1 TP FP TN FN Precision Recall ACC Yes Ratio InternVL2.5-26B InternVL2.5-8B InternVL2.5-8B-MPO 1 2 Ovis2-8B 3 Ovis2-4B 4 5 6 Ovis2-2B 7 LLaVa-NeXT-Llama 8 LLaVa-NeXT-Mistral 9 Ovis2-1B 90.1% 1402.0 90.0% 1290.0 89.9% 1294.0 89.7% 1381.0 89.4% 1390.0 89.4% 1330.0 89.3% 1368.0 89.3% 1335.0 88.8% 1314.0 88.8% 1318.0 InternVL2.5-26B-MPO 88.8% 1439.0 88.7% 1246.0 InternVL2.5-78B 87.9% 1225.0 87.4% 1195.0 86.9% 1222.0 86.6% 1177.0 85.7% 1160.0 10 LLaVa-NeXT-Vicuna 11 12 13 LLaVA-OneVision 14 15 16 17 PaliGemma2-3B InternVL2.5-38B PaliGemma2-10B PaliGemma-3B 216.7 76.0 86.0 204.0 224.0 149.0 197.3 155.7 145.7 152.7 310.7 64.0 63.3 40.0 90.3 42.7 46.0 1283.3 1424.0 1414.0 1296.0 1276.0 1351.0 1302.7 1344.3 1354.3 1347.0 1189.3 1436.0 1436.7 1460.0 1409.7 1457.3 1454.0 98.0 210.0 206.0 119.0 110.0 170.0 132.0 165.0 186.0 182.0 61.0 254.0 275.0 305.0 278.0 323.0 340.0 87.1% 94.5% 93.9% 87.7% 86.7% 90.2% 87.7% 89.8% 90.2% 89.9% 83.0% 95.2% 95.2% 96.8% 93.3% 96.5% 96.2% 93.5% 89.5% 86.0% 90.5% 86.3% 90.3% 92.1% 89.2% 92.7% 88.9% 88.7% 89.4% 91.2% 89.0% 89.0% 89.3% 87.6% 88.9% 87.9% 88.8% 95.9% 87.6% 83.1% 89.4% 81.7% 88.7% 79.7% 88.5% 81.5% 87.7% 78.5% 87.8% 77.3% 87.1% 54.0% 45.5% 46.0% 52.8% 53.8% 49.3% 52.2% 49.7% 48.7% 49.0% 58.3% 43.7% 42.9% 41.2% 43.7% 40.7% 40.2% Table 2. POPE - Mean Pos. Model F1 TP 94.7% 1402 InternVL2.5-26B 1 94.5% 1390 InternVL2.5-8B-MPO 2 InternVL2.5-26B-MPO 94.5% 1439 3 94.3% 1381 4 InternVL2.5-8B 92.6% 1368 5 LLaVa-NeXT-Llama 92.4% 1330 6 Ovis2-2B 92.0% 1335 7 LLaVa-NeXT-Mistral 91.8% 1294 8 Ovis2-4B 91.7% 1290 9 Ovis2-8B 91.7% 1314 10 Ovis2-1B 91.6% 1318 11 LLaVa-NeXT-Vicuna 90.4% 1246 12 InternVL2.5-78B 89.3% 1225 13 LLaVA-OneVision 89.1% 1222 14 88.4% 1195 15 87.5% 1177 16 86.9% 1160 17 InternVL2.5-38B PaliGemma2-3B PaliGemma2-10B PaliGemma-3B FP 60 51 107 48 88 49 67 24 22 53 60 12 20 20 10 12 11 TN FN Precision Recall ACC Yes Ratio 1440 1449 1393 1452 1412 1451 1433 1476 1478 1447 1440 1488 1480 1480 1490 1488 1489 98 110 61 119 132 170 165 206 210 186 182 254 275 278 305 323 340 95.9% 96.5% 93.1% 96.6% 94.0% 96.4% 95.2% 98.2% 98.3% 96.1% 95.6% 99.0% 98.4% 98.4% 99.2% 99.0% 99.1% 93.5% 94.7% 92.7% 94.6% 95.9% 94.4% 92.1% 94.4% 91.2% 92.7% 88.7% 92.7% 89.0% 92.3% 86.3% 92.3% 86.0% 92.3% 87.6% 92.0% 87.9% 91.9% 83.1% 91.1% 81.7% 90.2% 81.5% 90.1% 79.7% 89.5% 78.5% 88.8% 77.3% 88.3% 48.7% 48.0% 51.5% 47.6% 48.5% 46.0% 46.7% 43.9% 43.7% 45.6% 45.9% 41.9% 41.5% 41.4% 40.2% 39.6% 39.0% Table 3. POPE - Random Pos. Model F1 TP 1 Ovis2-8B 2 LLaVa-NeXT-Llama 3 Ovis2-4B 4 LLaVa-NeXT-Mistral 5 InternVL2.5-26B 6 Ovis2-2B InternVL2.5-8B 7 8 LLaVa-NeXT-Vicuna InternVL2.5-78B 9 10 InternVL2.5-8B-MPO 11 Ovis2-1B 12 13 LLaVA-OneVision 14 15 16 17 90.1% 1290 89.9% 1368 89.8% 1294 89.7% 1335 89.6% 1402 89.2% 1330 89.1% 1381 89.0% 1318 88.6% 1246 88.5% 1390 88.4% 1314 InternVL2.5-26B-MPO 88.3% 1439 87.8% 1225 87.2% 1195 86.7% 1222 86.7% 1177 85.6% 1160 PaliGemma2-3B InternVL2.5-38B PaliGemma2-10B PaliGemma-3B FP 74 176 88 142 229 153 218 144 68 250 160 319 67 46 97 39 49 TN FN Precision Recall ACC Yes Ratio 1426 1324 1412 1358 1271 1347 1282 1355 1432 1250 1340 1181 1433 1454 1403 1461 1451 210 132 206 165 98 170 119 182 254 110 186 61 275 305 278 323 340 94.6% 88.6% 93.6% 90.4% 86.0% 89.7% 86.4% 90.2% 94.8% 84.8% 89.1% 81.9% 94.8% 96.3% 92.6% 96.8% 95.9% 86.0% 90.5% 91.2% 89.7% 86.3% 90.2% 89.0% 89.8% 93.5% 89.1% 88.7% 89.2% 92.1% 88.8% 87.9% 89.1% 83.1% 89.3% 92.7% 88.0% 87.6% 88.5% 95.9% 87.3% 81.7% 88.6% 79.7% 88.3% 81.5% 87.5% 78.5% 87.9% 77.3% 87.0% 45.5% 51.5% 46.1% 49.2% 54.4% 49.4% 53.3% 48.7% 43.8% 54.7% 49.1% 58.6% 43.1% 41.4% 44.0% 40.5% 40.3% Table 4. POPE - Popular Pos. Model F1 TP InternVL2.5-78B 88.3% 1290 1 Ovis2-8B 88.0% 1294 2 Ovis2-4B 87.2% 1246 3 86.6% 1225 4 LLaVA-OneVision 86.6% 1195 PaliGemma2-3B 5 86.5% 1314 6 Ovis2-1B 86.5% 1330 7 Ovis2-2B 86.3% 1335 8 LLaVa-NeXT-Mistral 85.9% 1402 InternVL2.5-26B 9 85.8% 1318 10 LLaVa-NeXT-Vicuna 85.6% 1368 11 LLaVa-NeXT-Llama 85.6% 1381 InternVL2.5-8B 12 85.5% 1177 PaliGemma2-10B 13 85.2% 1390 InternVL2.5-8B-MPO 14 85.0% 1222 InternVL2.5-38B 15 84.7% 1160 PaliGemma-3B 16 InternVL2.5-26B-MPO 83.5% 1439 17 FP 132 146 112 103 64 224 245 258 361 254 328 346 77 371 154 78 506 TN FN Precision Recall ACC Yes Ratio 1368 1354 1388 1397 1436 1276 1255 1242 1139 1246 1172 1154 1423 1129 1346 1422 210 206 254 275 305 186 170 165 98 182 132 119 323 110 278 340 61 90.7% 89.9% 91.8% 92.2% 94.9% 85.4% 84.4% 83.8% 79.5% 83.8% 80.7% 80.0% 93.9% 78.9% 88.8% 93.7% 74.0% 86.0% 88.6% 86.3% 88.3% 83.1% 87.8% 81.7% 87.4% 79.7% 87.7% 87.6% 86.3% 88.7% 86.2% 89.0% 85.9% 93.5% 84.7% 87.9% 85.5% 91.2% 84.7% 92.1% 84.5% 78.5% 86.7% 92.7% 84.0% 81.5% 85.6% 77.3% 86.1% 95.9% 81.1% 47.4% 48.0% 45.3% 44.3% 42.0% 51.3% 52.5% 53.1% 58.8% 52.4% 56.5% 57.6% 41.8% 58.7% 45.9% 41.3% 64.8% Table 5. POPE - Adversarial Pos. Model F1 TP FP TN FN Precision Recall ACC Yes Ratio InternVL2.5-78B InternVL2.5-38B PaliGemma2-10B 1 Ovis2-4B 2 3 Ovis2-8B 4 PaliGemma2-3B 5 LLaVA-OneVision 6 Ovis2-1B 7 8 9 Ovis2-2B 94.2% 1123.7 94.1% 1101.3 94.1% 1116.0 92.9% 1062.3 92.8% 1078.0 92.2% 1128.3 92.1% 1073.7 92.0% 1044.3 91.8% 1131.3 91.3% 1167.7 91.1% 1114.3 91.1% 1153.7 91.1% 1120.0 90.7% 1023.0 90.6% 1158.7 89.8% 1133.0 InternVL2.5-26B-MPO 88.2% 1177. 10 InternVL2.5-26B 11 LLaVa-NeXT-Vicuna 12 InternVL2.5-8B 13 LLaVa-NeXT-Mistral PaliGemma-3B 14 15 InternVL2.5-8B-MPO 16 LLaVa-NeXT-Llama 17 84.0 60.0 77.3 44.3 66.0 141.3 78.7 46.3 154.3 214.7 152.7 202.7 161.3 53.0 222.7 212.7 318.7 1464.7 1488.7 1471.3 1504.3 1482.7 1407.3 1470.0 1502.3 1394.3 1334.0 1395.7 1346.0 1387.3 1495.7 1326.0 1336.0 1230.0 56.0 78.3 63.7 117.3 101.7 51.3 106.0 135.3 48.3 12.0 65.3 26.0 59.7 156.7 21.0 46.7 2.7 93.1% 94.9% 93.6% 96.0% 94.3% 89.0% 93.3% 95.8% 88.2% 84.8% 88.1% 85.4% 87.6% 95.1% 84.3% 84.4% 79.2% 95.3% 94.8% 93.4% 94.9% 94.6% 94.8% 90.1% 94.1% 91.4% 93.8% 95.7% 92.9% 91.0% 93.2% 88.5% 93.3% 95.9% 92.5% 99.0% 91.6% 94.5% 92.0% 97.8% 91.6% 94.9% 91.9% 86.7% 92.3% 98.2% 91.0% 96.1% 90.5% 99.8% 88.2% 44.3% 42.6% 43.8% 40.6% 41.9% 46.6% 42.3% 40.0% 47.2% 50.7% 46.5% 49.8% 47.0% 39.5% 50.7% 49.4% 54.9% Table 6. RePOPE - Mean Pos. Model F1 TP InternVL2.5-78B InternVL2.5-26B InternVL2.5-8B-MPO InternVL2.5-8B 1 Ovis2-4B 2 Ovis2-8B 3 4 5 6 7 Ovis2-1B 8 Ovis2-2B 9 LLaVA-OneVision PaliGemma2-3B 10 11 InternVL2.5-38B 12 LLaVa-NeXT-Vicuna PaliGemma2-10B 13 14 LLaVa-NeXT-Mistral 15 16 LLaVa-NeXT-Llama PaliGemma-3B 17 95.9% 1113 95.9% 1109 95.5% 1090 95.0% 1150 94.9% 1142 94.9% 1138 94.8% 1118 94.7% 1121 94.3% 1072 94.2% 1057 93.9% 1063 93.6% 1104 93.4% 1041 93.3% 1110 InternVL2.5-26B-MPO 92.8% 1157 92.5% 1122 92.1% 1018 FP 49 45 33 111 105 101 81 87 42 29 41 96 29 110 177 146 34 TN FN Precision Recall ACC Yes Ratio 1566 1570 1582 1504 1510 1514 1534 1528 1573 1586 1574 1519 1586 1505 1438 1469 1581 46 50 69 9 17 21 41 38 87 102 96 55 118 49 2 37 141 95.8% 96.1% 97.1% 91.2% 91.6% 91.8% 93.2% 92.8% 96.2% 97.3% 96.3% 92.0% 97.3% 91.0% 86.7% 88.5% 96.8% 96.0% 96.6% 95.7% 96.6% 94.0% 96.3% 99.2% 95.7% 98.5% 95.6% 98.2% 95.6% 96.5% 95.6% 96.7% 95.5% 92.5% 95.3% 91.2% 95.3% 91.7% 95.1% 95.3% 94.6% 89.8% 94.7% 95.8% 94.3% 99.8% 93.5% 96.8% 93.4% 87.8% 93.7% 41.9% 41.6% 40.5% 45.5% 45.0% 44.7% 43.2% 43.5% 40.2% 39.1% 39.8% 43.3% 38.6% 44.0% 48.1% 45.7% 37.9% Table 7. RePOPE - Random Pos. Model F1 TP InternVL2.5-78B PaliGemma2-10B 94.1% 1131 1 Ovis2-4B 93.9% 1121 2 Ovis2-8B 93.7% 1106 3 92.6% 1083 4 LLaVA-OneVision 92.4% 1065 PaliGemma2-3B 5 91.8% 1080 6 InternVL2.5-38B 91.7% 1139 7 Ovis2-2B 91.7% 1046 8 91.6% 1135 9 Ovis2-1B 91.4% 1121 10 LLaVa-NeXT-Vicuna 91.3% 1124 11 LLaVa-NeXT-Mistral 91.1% 1179 InternVL2.5-26B 12 90.9% 1164 13 InternVL2.5-8B 90.5% 1141 14 LLaVa-NeXT-Llama 90.2% 1025 PaliGemma-3B 15 90.1% 1170 InternVL2.5-8B-MPO 16 InternVL2.5-26B-MPO 88.2% 1190 17 FP 80 73 62 64 47 80 152 43 149 139 145 216 205 187 54 235 315 TN FN Precision Recall ACC Yes Ratio 1454 1461 1472 1470 1487 1454 1382 1491 1385 1394 1389 1318 1329 1347 1480 1299 1219 62 72 87 110 128 113 54 147 58 72 69 14 29 52 168 23 3 93.4% 93.9% 94.7% 94.4% 95.8% 93.1% 88.2% 96.1% 88.4% 89.0% 88.6% 84.5% 85.0% 85.9% 95.0% 83.3% 79.1% 94.8% 94.8% 94.0% 94.7% 92.7% 94.5% 90.8% 93.6% 89.3% 93.6% 90.5% 92.9% 95.5% 92.4% 87.7% 93.0% 95.1% 92.4% 94.0% 92.3% 94.2% 92.2% 98.8% 91.6% 97.6% 91.4% 95.6% 91.2% 85.9% 91.9% 98.1% 90.5% 99.7% 88.3% 44.4% 43.8% 42.8% 42.1% 40.8% 42.5% 47.3% 39.9% 47.1% 46.2% 46.5% 51.2% 50.2% 48.7% 39.6% 51.5% 55.2% Table 8. RePOPE - Popular Pos. Model F1 TP InternVL2.5-78B 1 2 Ovis2-4B 3 Ovis2-8B PaliGemma2-3B 4 5 LLaVA-OneVision PaliGemma2-10B 6 7 InternVL2.5-38B 8 Ovis2-1B 9 10 Ovis2-2B 11 LLaVa-NeXT-Mistral 12 LLaVa-NeXT-Vicuna InternVL2.5-26B 13 InternVL2.5-8B 14 15 InternVL2.5-8B-MPO 16 LLaVa-NeXT-Llama 17 93.1% 1108 92.5% 1127 92.4% 1118 92.2% 1065 91.5% 1079 91.0% 1046 90.6% 1078 90.1% 1132 89.8% 1026 89.1% 1134 88.6% 1126 88.4% 1118 87.7% 1174 87.5% 1159 86.9% 1164 86.5% 1136 InternVL2.5-26B-MPO 83.5% 1184 PaliGemma-3B FP 85 123 114 57 92 67 115 194 71 224 229 223 317 302 328 305 464 TN FN Precision Recall ACC Yes Ratio 1412 1374 1383 1440 1405 1430 1382 1303 1426 1273 1268 1274 1180 1195 1169 1192 1033 79 60 69 122 108 141 109 55 161 53 61 69 13 28 23 51 3 92.9% 90.2% 90.7% 94.9% 92.1% 94.0% 90.4% 85.4% 93.5% 83.5% 83.1% 83.4% 78.7% 79.3% 78.0% 78.8% 71.8% 93.3% 93.9% 94.9% 93.2% 94.2% 93.2% 89.7% 93.3% 90.9% 92.5% 88.1% 92.3% 90.8% 91.7% 95.4% 90.7% 86.4% 91.4% 95.5% 89.7% 94.9% 89.2% 94.2% 89.1% 98.9% 87.7% 97.6% 87.7% 98.1% 86.9% 95.7% 86.7% 99.7% 82.6% 44.4% 46.6% 45.9% 41.8% 43.6% 41.5% 44.4% 49.4% 40.9% 50.6% 50.5% 50.0% 55.6% 54.4% 55.6% 53.7% 61.4% Table 9. RePOPE - Adversarial"
        }
    ],
    "affiliations": [
        "Tubingen AI Center, University of Tubingen"
    ]
}