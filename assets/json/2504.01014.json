{
    "paper_title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction",
    "authors": [
        "Junhao Cheng",
        "Yuying Ge",
        "Yixiao Ge",
        "Jing Liao",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 4 1 0 1 0 . 4 0 5 2 : r AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction Junhao Cheng1,2 Yuying Ge1, (cid:66) Yixiao Ge1 Jing Liao2 Ying Shan 1ARC Lab, Tencent PCG 2City University of Hong Kong https://howe125.github.io/AnimeGamer.github.io/ Figure 1. An example of infinite anime life simulation by our AnimeGamer. Users can continuously interact with the game world as the character Sosuke (the main character of the film Ponyo on the Cliff) through open-ended language instructions. AnimeGamer generates consistent multi-turn game states, consisting of dynamic animation shots (i.e., videos) with contextual consistency (e.g., the purple car and the forest background), and updates to character states including stamina, social, and entertainment values."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/ TencentARC/AnimeGamer. 1. Introduction Recent advances in generative models have significantly enhanced anime production, particularly in character design and the creation of character-centric images and videos [23, 74, 76]. This progress raises an intriguing question: Can we transcend static content generation to create infinite anime 1 games by transforming characters from anime films into interactive, playable entities? Imagine experiencing the life of characters crafted by Hayao Miyazaki within dynamic anime world. Users can continuously interact with this world using open-ended language instructions, while the model consistently generates game states. These states encompass dynamic animation shots and updates to character attributes such as stamina, social, and entertainment values, as illustrated in Figure 1. The concept of Anime Life Simulation falls under the category of infinite games, as explored in recent research [41]. In these games, all behaviors and graphics are generated through AI models, eliminating the need for predefined game rules and pre-designed graphics. By leveraging the capabilities of generative models, we can create immersive and ever-evolving gaming experiences that allow players to experience the lives of their favorite anime characters in unprecedented ways. Some recent works utilize generative models to generate the next frame in existing games [1, 69, 75] or opendomain game scenarios [5, 9, 78] by taking the previous game frames and user controls (mouse or keyboard) as the input. However, these approaches are constrained by limited command inputs (e.g., directional controls) and exploration within predefined environments, which categorizes them as finite games. The pioneering work Unbounded [41] addresses the challenge of infinite anime life simulation by employing an LLM as router to translate multi-turn textonly dialogues into language captions for static image generation, as illustrated in Figure 2. However, this approach neglects historical visual context, which is crucial for maintaining continuity and coherence in gameplay. Furthermore, it is limited to generating static images, which fails to represent the dynamic interactions and movements essential for an engaging gaming experience (imagine game world where characters remain completely motionless). To overcome the above limitations, in this work, we propose AnimeGamer, which leverages an MLLM to generate game states for infinite anime life simulation. We introduce novel action-aware multimodal representations, which effectively capture the intricacies of animation shots. These representations can be seamlessly decoded into high-quality video clips using video diffusion model. By utilizing historical multimodal representations and character state updates as input, AnimeGamer can predict subsequent game states, ensuring that the generated animation shots are contextually consistent with satisfactory dynamics, and update character states reasonably. In addition, we propose an automatic data collection pipeline from anime films, empowering players to experience the life of their favorite characters in an infinite game through training models on their customized data. To evaluate the effectiveness of our model, we tailor related SOTA methods to this task and design eval- (a) LLM-based methods (e.g., Unbounded [41]) (b) AnimeGamer Figure 2. Comparison of AnimeGamer with previous methods. Unbounded employs an LLM to translate multi-turn text-only dialogues into language descriptions for static image generation, with an additional condition based on reference images. In contrast, AnimeGamer utilizes an MLLM to predict multimodal representations by incorporating historical multimodal context as input. These generated representations can then be directly decoded into consistent dynamic clips using video diffusion model. uation metrics including both automated and human assessments. The evaluation results demonstrate that our model performs favorably in terms of instruction following, contextual consistency and overall gaming experience. We make the following contributions in this work: We propose AnimeGamer for infinite anime life simulation. Powered by an MLLM, our model takes multimodal context as input to predict the next game state, including dynamic animation shots and character state updates, providing an immersive gaming experience. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using video diffusion model. By taking multimodal representations as input to predict subsequent representations, our approach ensures contextual consistency and satisfactory dynamics throughout gameplay. We conduct both qualitative and quantitative evaluations, including user studies, to demonstrate the effectiveness of AnimeGamer. 2. Related Works Generative Games Finite Games Generation. Finite games are defined by James P. Carse as games that are played for the purpose of winning with boundaries, fixed rules, and definitive endpoint [8]. Existing efforts for generative finite games can be primarily categorized into two main approaches: partly generated and fully generated. The partly generated meth2 ods rely on pre-existing games or hard-coded systems, with AI assisting in generating specific game components. Some efforts have focused on using AI to design game interfaces [17, 21] or develop AI-driven games [18, 68]. Other approaches have been explored to generate dynamic game content [42, 56, 58, 63] or rules [35] with concept maps [67], conceptual expansion [25, 26], Markov Chains [59, 60], Bayes Nets [24], and LSTMs [54, 62]. Some works attempt to leverage the advantage of generative models to create game levels environments based on GANs [37, 38, 55, 70] or diffusion models [64, 83]. While some recent researches try to use LLM or MLLM to design and generate the game mechanics and environments [2, 15, 29, 46, 61, 66, 79], or act as an agent to take part in playing and simulation [19, 34, 43, 49, 82]. However, these methods are limited by their reliance on pre-defined, hard-coded systems and rules, which may potentially stifle innovation. On the other hand, the fully generated approaches use AI to create all aspects of game behavior. These efforts mainly focus on replicating specific scenes from existing games such as Minecraft [1], Mario [75], and DOOM [69], or on open-domain scenarios [5, 9, 78]. However, they only support limited commands within predefined environment, thus cannot generalize to perform infinite games. Infinite Games Generation. Carse defines infinite games as those played for the purpose of continuing the play [8]. The latest work Unbounded [41] introduces the concept of generative infinite game, with an LLM to generate text responses and pre-trained T2I model enhanced with LoRA [30] for character-consistent image generation. However, since LLM takes only in-context text information as input, this can lead to decrease in visual coherence in the final generated results. This issue becomes more pronounced when the images need to be further converted into video outputs. In our work, we utilize an MLLM to predict multimodal representations by incorporating historical multimodal context as input. Multi-turn Image&Video Generation. Multi-turn T2I and T2V generation require models to generate coherent visual outputs based on human instructions for various applications such as content design and storytelling [27, 45]. Benefiting from the in-context learning and generation capabilities [4] of LLM and MLLM, existing approaches are usually driven by them and can be primarily categorized into off-the-shelf approaches and end-to-end methods. The former utilize pre-trained LLM as router to transform dialogue into character information [13, 41, 44], layouts [12, 71], or captions [6, 40] for generative models. These approaches ignore in-context visual information when generating next game states. As result, they underperform in terms of contextual consistency and visual coherence. The latter methods [72, 76, 80] leverage MLLM for end-to-end generation. They take account of both text and visual in-context information to predict next image features. However, when applied to infinite anime life simulation that require video output, these methods necessitate an additional video conversion process, which can disrupt incontext coherence and entail additional computational and time costs. In our work, AnimeGamer predicts the next animation shot representations, which can then be decoded into videos with controllable character movements and motion scope. 3. Methods 3.1. Task Formulation We focus on the challenging infinite anime life simulation task in this paper. Following prior works [8, 41], we define round of infinite game as consisting of multiple game states s, which serve as feedback to players. Each is composed of two parts: 1) Dynamic animation shot: an anime clip demonstrating the action of the character; 2) Character state: visualization of characters stamina, social, and entertainment values to represent their mood and physical health. Models are required to receive open-ended language instructions from players to generate multi-turn game states. 3.2. AnimeGamer Overview. The overview of our AnimeGamer is illustrated in Figure 3. We model an animation shot as action-aware multimodal representation by training an animation shot encoder Ea, with an animation shot decoder Da based on video diffusion models to decode the representation into highquality video clips. Next, we introduce an MLLM to predict each game state representation with multimodal input. We further enhance the quality of decoded animation shots from the MLLM via an adaptation phase, where the decoder is fine-tuned by taking MLLMs predictions as input. Animation Shot Tokenization and Detokenization. The alignment of characters visual features and actions in an anime clip with player instructions is crucial for the gaming experience. However, existing MLLM-based methods primarily predict text-only [72] or image-only [80] representations to align with generative diffusion models. They are limited by the significant loss of visual and motion information in video clip, resulting in inconsistency in gameplay. To address this, we model an animation shot as action-aware multimodal representation sa that serve as bridge for the MLLM and Da. As illustrated in Figure 3, we decompose an animation shot into the following three parts: 1) Overall visual reference fv, which is captured by CLIP [51] embeddings of the first frame of an anime clip; 2) Action description fmd, short motion prompt focusing on the characters action in the video (e.g., Softly talk), which is represented by T5 [52] text embeddings; 3) Motion scope fms, we repFigure 3. Overview of our AnimeGamer. The training process consists of three phases: (a) We model animation shots using action-aware multimodal representations through an encoder and train diffusion-based decoder to reconstruct videos, with the additional input of motion scope that indicates action intensity. (b) We train an MLLM to predict the next game state representations by taking the history instructions and game state representations as input. (c) We further enhance the quality of decoded animation shots from the MLLM via an adaptation phase, where the decoder is fine-tuned by taking MLLMs predictions as input. resent the intensity of characters action in video by optical flow1. As depicted in Figure 4, the animation shot is encoded by Ea as follows: sa = Ea(fmd, fv) Ea = Concat(LN(MLP(x)), LN(MLP(y))), (1) where MLP stands for multi-layer perception for dimension alignment, LN stands for layer normalization to align feature scale, and Concat represents the concatenation operation along the token dimension. Finally, fms will serve as an additional condition for Da to control the motion scope in the output dynamic animation shot. To decode the multimodal representation into highquality video, we introduce decoder Da upon video diffusion model CogvideoX [77] by replacing the original text features with the action-aware multimodal representation. In addition, we introduce fms as an additional generation condition to control action intensity. As illustrated in Figure 4, fms is embedded using sinusoidal functions and several fully-connected (FC) layers activated by SiLU [28], which is then added to the timestep embedding ft. As for training, we first align sa with the input space of Da by optimizing only Ea as warm-up. We initially encode an input video into latent code using the 3DVariational Autoencoder from [77]. Next, the noisy latent code zt at timestep serves as the input for the denoising DiT ϵθ with text condition and sa. The training objective for this process is defined as follows: = Ez,c,sa,ϵN (0,1),t (cid:2)ϵ ϵθ(zt, t, c, sa)2 2 (cid:3) , (2) where ϵ represents random noise sampled from standard Gaussian distribution. Then, we jointly train Ea and Da, and the training loss remains consistent with Equation 2. Game State Prediction with MLLM. Recent advancements in MLLM have demonstrated significant progresses in unified comprehension and generation [14, 22, 76, 86]. Inspired by this, we utilize MLLM as game engine to perform infinite anime life simulation by next game state prediction. As shown in Figure 3, AnimeGamer takes multimodal historical context and the current instruction as inputs to generate the next game state. For sa, we employ learnable queries as input and continuously output actionaware multimodal representations from the MLLM with full attention. Here, we set = 226 to align the pre-trained model of Da to reduce consumption costs. For sc, we predict the three character states, as well as fms as an additional generation control. We treat these as discrete targets and add special tokens2 to format the generation after sa. During training, we sample random-length subset from the multi-turn data for each iteration. We initialized AnimeGamer with the weight of Mistral-7B [32] and task the model to continuously output the next sa with MSE loss, and perform next-token prediction to genearte sc and fms, which as optimized with Cross Entropy loss. The overall 1Detailed in Appendix A. 2See Appendix for more details. 4 Figure 4. Architecture of animation shot encoder and decoder. The action-aware multimodal representation integrates visual features of the first frame with textual features of action description, and serve as the input to the modulation module of the decoder. Additional motion scope indicating action intensity is injected using condition module. training loss is as follows: = LCE + αLMSE, (3) where α is the weight of the loss term LMSE. Decoder Adaptation. The separate training of the MLLM and Da conserves memory but risks potential misalignment between the latent spaces of the MLLM output and Da, which may lead to artifacts in the generated videos. To mitigate this issue, we conduct adaptation training where only Da is trained. Conditioned on the output embeddings of the MLLM, Da is expected to generate anime shots that are pixel-level aligned with the ground truth. Inference. During the inference process, the historical action-aware multimodal representations are projected into the input space of the MLLM using linear resampler. To enable theoretically infinite generation, we follow previous works [53] to adopt the sliding window technique for multimodal generation with train-short-test-long scheme. 3.3. Dataset Construction Training AnimeGamer requires multi-turn character-centric video data with contextual coherence. However, existing anime datasets [33, 36, 48, 57] mainly focus on single scene or are closed-sourced, which limits their application to this challenging task. Noticing that anime films are an ideal data source due to their sufficient time span, narrative coherence and easy accessibility, we construct pipeline to obtain the required training data from them. Specifically, we collect 10 popular anime films and split them into approximately 20,000 video clips, each containing 16 frames at 480 720 resolution. We uniformly sample 4 frames from each video clip as input for InternVL [11], prompting it to obtain character movement, background, and character states in the video. Additionally, we collect images of the main characters and prompt InternVL to label them in each frame to ensure character consistency. Players can customize their favorite characters following this pipeline3. 4. Experiments 4.1. Baselines To the best of our knowledge, there is lack of open-source approaches for this challenging task. For comparison, we tailor related SOTA models to this task. We use Gemini1.5 [65] as router LLM to comprehend dialogues and generate character states and generation instructions. Based on this, we construct three baseline methods as follows: GC: We fine-tune T2V model CogvideoX to generate animation shot output. GFC: We fine-tune T2I model Flux [39] and further process the image results using pre-trained I2V model CogvideoX-I2V to render the final video. GSC: We integrate CogvideoX-I2V into the story visualization model StoryDiffusion [85] as tuning-free method for comparison. We follow the task setting of Unbounded [41] in infinite game generation, which trains models with custom characters and evaluates them in closed domains. All baselines are trained on the same dataset as our AnimeGamer for fair comparison. See Appendix for details. 4.2. Evaluation Benchmark To evaluate the quality of infinite game generation, we construct an evaluation benchmark using GPT-4o [47]. We randomly select characters from our training data and prompt GPT-4o to simulate multiple infinite games, with each game 3See Appendix for pipeline construction details. 5 Table 1. Quantitative comparison with baseline models on automatic metrics. Bold indicate the best performance. Model Character Consistency CLIP-I DreamSim CLIP-T CLIP-TE ACC-F MAE-F ACC-S MAE-S Semantic Consistency Motion Quality State Update Inference Time (s/turn) GSC GFC GC AnimeGamer 0.7862 0.7662 0.7960 0.8132 0.5019 0.5797 0.6416 0.7403 0.3331 0.3325 0.3339 0. 0.3142 0.3123 0.3158 0.4012 0.3163 0.2923 0.4249 0.6744 0.8263 1.0212 0.7223 0.4238 0.6773 0.6771 0.6779 0.6773 0.5888 0.5888 0.5888 0.5872 50 63 25 Table 2. Quantitative comparison with baseline models on MLLM judgement and human evaluation. Bold indicate the best performance. Model Overall Instruction Following Contexual Consistency Chracter Consistency Style consistency State Update GPT-4V Human GPT-4V Human GPT-4V Human GPT-4V Human GPT-4V Human GPT-4V Human GSC GFC GC AnimeGamer 5.35 4.96 6.42 8.36 2.29 4.27 7.38 10.00 6.13 5.51 7.29 9.14 2.96 3.57 7.37 9. 5.44 4.73 6.58 8.41 2.71 3.20 6.89 9.95 5.33 6.22 7.49 9.11 2.96 3.76 7.55 9.86 5.57 4.84 6.57 7.52 5.77 3.62 6.10 9. 8.38 8.38 8.39 8.39 9.92 9.92 9.94 9.94 containing 10 rounds of instructions. We prompt GPT-4o to provide instructions that include characters, movement descriptions, and the environment, along with the corresponding ground-truth character states for each turn. The benchmark comprises 2,000 rounds, featuring 20 characters, 940 distinct movements, and 133 unique environments. See Appendix for details. 4.3. Metrics We use automatic metrics CLIP-I [51], DINO-I [7] and DreamSim [20] to evaluate character consistency by mapping the detected generated characters to the ground-truth, in line with prior works [13, 41, 80]. For semantic consistency, we employ CLIP to calculate cosine similarity between the generated video and the input text prompt and environment prompt, denoted as CLIP-T and CLIP-TE, respectively. We further utilize an optical flow detection model [16] to detect the action intensity of the generated video and calculate Mean Absolute Error (MAE) and Accuracy (ACC) with the ground truth motion scope, denoted as MAE-F and ACC-F, respectively. To assess updates in character states, we report both MAE and ACC, denoted as MAE-S and ACC-S, respectively. Furthermore, some researches [41, 76, 81] employ more advanced MLLM as judges to assess the outputs of different models. In this study, we utilize GPT-4v as the evaluation MLLM to score the models from various aspects. We also conduct user studies, adhering to previous game generation works [2, 26, 31]. Please refer to Appendix and for more details. mantic consistency, and motion control within the generated animation shots. This can be attributed to the actionaware multimodal representation of animation shots, which enhances controllability and generalizability. Additionally, AnimeGamer performs favorably in contextual consistency and style consistency, due to the multimodal comprehension and generation capabilities of MLLM. In contrast, other baseline models only consider text context, resulting in decline across all metrics. When it comes to character state updates, AnimeGamer performs similarly to Gemini-1.5. However, using the API of an LLM incurs additional time costs, giving AnimeGamer an advantage in inference time. 4.5. Qualitative Comparisons We compare infinite anime life simulation results4 of AnimeGamer with GC and GFC in Figure 5. GC and GFC neglect historical visual information, leading to deficiency in contextual consistency. Additionally, they underperform at generalize interactions between characters from different anime films (the two characters in rounds 1 and 2 are from two distinct anime films) and character actions (in round 3, the action of flying on broomstick is exclusive to Qiqi in the training set). In contrast, AnimeGamer considers multimodal context in the generation process, thus delivering more coherent and immersive game experience. Moreover, the generalization ability of the MLLM makes AnimeGamer perform well in character-centric commands. The tuning-free method GSC fails to achieve character consistency, which is crucial for the gaming experience, thus is unsuitable for this task. 4.4. Quantitative Comparisons 4.6. Ablation Study The performance comparison results based on automatic metrics, MLLM judgement and human evaluation are presented in Table 1 and Table 2. AnimeGamer outperforms all baseline models in terms of character consistency, seWe randomly select one anime film within our dataset to conduct ablation studies. See Appendix for details. 4See Appendix for character images and more qualitative results. 6 O F u C G O F u C G O F Round Pazu and Kiki Peacefully stand together Room Round 2 Pazu and Kiki Quietly research the book Room Round 3 Pazu Steadily fly on broomstick Meadow Round Pazu Carefully read map Cave Round 5 Pazu and Sheeta Gently talk to each other Cave Figure 5. Visualization of infinite anime life simulations. 7 Qiqi Excitedly laugh Sky a / c / s Figure 6. Visualization of ablation study on game state prediction and decoder adaptation. Our method outperforms in terms of character consistency and movement following. Ursala Quickly read paper Home r u G a . r / a e / i d / t - r / - w / f / u Figure 7. Results of ablation study on anime shot tokenization and de-tokenization. Our method outperforms in terms of image consistency and movement following. Ablation on Animation Shot Tokenization and Detokenization. Ea plays crucial role in encoding anime clips into action-aware multimodal representations. We demonstrate its efficiency by comparing it with four variTable 3. Results of the ablation study of our AnimeGamer, where the columns in the table above pertain to the ablation experiments on the tokenizer and de-tokenizer (w/o MLLM). Name w/o MLLM Image Consistency Semantic Consistency Motion Quality CLIP-I DreamSim CLIP-T CLIP-TE ACC-F MAE-F w/ rand. frame w/ less para w/ addition w/ cross-attn w/o warm-up w/o fms Ours w/o adapt w/ Lcos Ours 0.8446 0.8406 0.7684 0.7264 0.8306 0.8533 0.8672 0.6831 0.7628 0.7856 0.4500 0.4481 0.6173 0.7084 0.5107 0.6894 0.7928 0.4937 0.5966 0.6084 0.2480 0.2450 0.2311 0.2328 0.2447 0.2829 0. 0.1889 0.2228 0.2212 0.2270 0.2274 0.232 0.2333 0.2382 0.2518 0.2523 0.1898 0.2117 0.2203 0.4744 0.3649 0.4671 0.3284 0.7028 0.1824 0.7293 0.3649 0.6649 0.6722 0.5620 0.6934 0.6058 0.8102 0.4582 1.2189 0. 0.8467 0.4467 0.4883 ants5, as well as removing the warm-up training phase. The results presented in Table 3 and Figure 7 indicate that reducing the learnable parameters of Ea or combining fv with fmd via element-wise addition or cross-attention leads to decline in all metrics and visual quality. This can be attributed to the disruption of spatial positional information within the visual feature. Additionally, using random frame instead of first frame to obtain fv or remove the warm-up training phase also leads to decrease in consistency between the generated animation shot and the reference character. This can be caused by the increased difficulty in training. Finally, we remove fms in Da, which results in decline in motion control quality. This indicates that relying solely on text to control the motion scope is unreliable. Ablation on Next Game State Prediction. Some studies [73] employ Cosine Similarity Loss for MLLM when training to fit continuous features. We adopt this approach in our AnimeGamer, denoted as w/ Lcos. Results in Table 3 and Figure 6 show that the impact of Lcos is marginal. Ablation on Decoder Adaptation. We conduct ablation by removing the decoder adaptation training phase, denoted as w/o adapt. As illustrated in Table 3 and Figure 6, removing the decoder adaptation training phase leads to artifacts in the generated videos. These disadvantages may lead to unsatisfactory gaming experiences. 5. Conclusion and Limitation In this paper, we propose AnimeGamer for infinite anime life simulation. Users can continuously interact with the game world as anime characters through open-ended language instructions. AnimeGamer generates multi-turn game states that consist of dynamic animation shots and updates to character states, including stamina, social, and entertainment values. Through modeling animation shots using action-aware multimodal representations, we train MLLM to predict the next animation shot representations by taking the history instructions and multimodal representations as the input. Evaluation through both automated 5Detailed in Appendix C. metrics and human evaluation shows that AnimeGamer outperforms baseline methods across various gaming aspects. Our focus has been on developing an effective method for transforming characters into interactive, playable entities within infinite games, without further exploration of the extension to open domains. Our task setting aligns with the most recent work in infinite game generation, which emphasizes training models with custom characters and evaluating them in closed domains. In future work, we will explore the generalization to unseen characters."
        },
        {
            "title": "References",
            "content": "[1] Decart AI. Oasis: The future engine of virtual worlds. 2024. 2, 3 [2] Asad Anjum, Yuting Li, Noelle Law, Megan Charity, and Julian Togelius. The ink splotch effect: case study on chatgpt as co-creative game designer. In Proceedings of the 19th International Conference on the Foundations of Digital Games, pages 115, 2024. 3, 6 [3] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. 14 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 3 [5] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 2, 3 [6] Qingxing Cao, Junhao Cheng, Xiaodan Liang, and Liang Lin. Visdiahalbench: visual dialogue benchmark for diagnosing hallucination in large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1216112176, 2024. 3 [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [8] JP Carse. Finite and infinite games: vision of life as play and possibility.[kindle ipad version]. Available from Amazon. com, 1986. 2, 3 [9] Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769, 2024. 2, 3 [10] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 13 [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 5, 13 [12] Junhao Cheng, Xi Lu, Hanhui Li, Khun Loun Zai, Baiqiao Yin, Yuhao Cheng, Yiqiang Yan, and Xiaodan Liang. Autostudio: Crafting consistent subjects in multi-turn interactive image generation, 2024. 3 [13] Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, and Xiaodan Liang. Theatergen: Character management with llm for consistent multi-turn image generation, 2024. 3, [14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. 4 [15] John Joon Young Chung and Max Kreminski. Patchview: Llm-powered worldbuilding with generative dust and magnet visualization. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pages 119, 2024. 3 [16] Qiaole Dong and Yanwei Fu. Memflow: Optical flow esIn Proceedings of timation and prediction with memory. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1906819078, 2024. 6, 13 [17] Mirjam Eladhari, Anne Sullivan, Gillian Smith, and Josh McCoy. Ai-based game design: Enabling new playable experiences. UC Santa Cruz Baskin School of Engineering, Santa Cruz, CA, 2011. 3 [18] Taha-Yassine Ennabili. comparison of traditional game design vs. ai-driven game design. 2023. 3 [19] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35: 1834318362, 2022. 3 [20] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 6 [21] Swen Gaudl, Mark Nelson, Simon Colton, Rob Saunders, Edward Powley, Peter Ivey, Blanca Perez Ferrer, and Michael Cook. Exploring novel game spaces with fluidic games. arXiv preprint arXiv:1803.01403, 2018. 3 [22] Yuying Ge, Yizhuo Li, Yixiao Ge, and Ying Shan. Divot: Diffusion powers video tokenizer for comprehension and generation. arXiv preprint arXiv:2412.04432, 2024. 4 [23] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 1 [24] Matthew Guzdial and Mark Riedl. Game level generation from gameplay videos. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pages 4450, 2016. 3 [25] Matthew Guzdial and Mark Riedl. Automated game design via conceptual expansion. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pages 3137, 2018. 3 [26] Matthew Guzdial and Mark Riedl. Conceptual game expansion. IEEE Transactions on Games, 14(1):93106, 2021. 3, 6 [27] Huiguo He, Huan Yang, Zixi Tuo, Yuan Zhou, Qiuyue Wang, Yuhang Zhang, Zeyu Liu, Wenhao Huang, Hongyang Chao, and Jian Yin. Dreamstory: Open-domain story visualization by llm-guided multi-subject consistent diffusion, 2025. 3 [28] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 4 [29] Chengpeng Hu, Yunlong Zhao, and Jialin Liu. Game generation via large language models. In 2024 IEEE Conference on Games (CoG), pages 14. IEEE, 2024. 3 [30] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [31] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu. survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024. 6 [32] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume LamarXiv preprint ple, Lucile Saulnier, et al. Mistral 7b. arXiv:2310.06825, 2023. 4 [33] Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Bingwen Zhu, Jixuan Xu, et al. Exploring the frontiers of animation video generation in the sora era: Method, dataset and benchmark. arXiv preprint arXiv:2412.10255, 2024. 5 [34] Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, Lyfe agents: Generative agents for and Andrew Ahn. arXiv preprint low-cost real-time social arXiv:2310.02172, 2023. 3 interactions. [35] Ahmed Khalifa, Michael Cerny Green, Diego PerezLiebana, and Julian Togelius. General video game rule generation. In 2017 IEEE Conference on Computational Intelligence and Games (CIG), pages 170177. IEEE, 2017. 3 [36] Kangyeol Kim, Sunghyun Park, Jaeseong Lee, Sunghyo Chung, Junsoo Lee, and Jaegul Choo. Animeceleb: Largescale animation celebheads dataset for head reenactment. In European Conference on Computer Vision, pages 414430. Springer, 2022. [37] Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, and Sanja Fidler. Learning to simulate dynamic environments with gamegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12311240, 2020. 3 [38] Vikram Kumaran, Bradford Mott, and James Lester. Generating game levels for multiple distinct games with common latent space. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pages 102108, 2019. 3 [39] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 5 [40] Zeqiang Lai, Xizhou Zhu, Jifeng Dai, Yu Qiao, and Wenhai Wang. Mini-dalle3: Interactive text to image by prompting large language models. arXiv preprint arXiv:2310.07653, 2023. [41] Jialu Li, Yuanzhen Li, Neal Wadhwa, Yael Pritch, David Jacobs, Michael Rubinstein, Mohit Bansal, and Nataniel Ruiz. Unbounded: generative infinite game of character life simulation. arXiv preprint arXiv:2410.18975, 2024. 2, 3, 5, 6 [42] Jialin Liu, Sam Snodgrass, Ahmed Khalifa, Sebastian Risi, Georgios Yannakakis, and Julian Togelius. Deep learning for procedural content generation. Neural Computing and Applications, 33(1):1937, 2021. 3 [43] Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, and Jiaya Jia. Rl-gpt: Integrating reinforcement learning and code-as-policy. arXiv preprint arXiv:2402.19299, 2024. 3 [44] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videodrafter: Content-consistent multi-scene video generation with llm. arXiv preprint arXiv:2401.01256, 2024. 3 [45] Xiangyang Luo, Junhao Cheng, Yifan Xie, Xin Zhang, Tao Feng, Zhou Liu, Fei Ma, and Fei Yu. Object isolated attention for consistent story visualization, 2025. 3 [46] Muhammad Nasir and Julian Togelius. Practical pcg through large language models. In 2023 IEEE Conference on Games (CoG), pages 14. IEEE, 2023. [47] OpenAI. Openai models. 2023. 5 [48] Zhenglin Pan, Yu Zhu, and Yuxuan Mu. dataset: Scaling up cartoon research. arXiv:2405.07425, 2024. 5 Sakuga-42m arXiv preprint [49] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. 3 [50] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 14 [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 6 [52] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 3 [53] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 5 [54] Anurag Sarkar and Seth Cooper. Blending levels from different games using lstms. In AIIDE Workshops, 2018. 3 [55] Frederik Schubert, Maren Awiszus, and Bodo Rosenhahn. Toad-gan: flexible framework for few-shot level generation in token-based games. IEEE Transactions on Games, 14(2): 284293, 2021. 3 [56] Noor Shaker, Julian Togelius, and Mark Nelson. Procedural content generation in games. 2016. 3 [57] Li Siyao, Yuhang Li, Bo Li, Chao Dong, Ziwei Liu, and Chen Change Loy. Animerun: 2d animation visual correspondence from open source 3d movies. Advances in Neural Information Processing Systems, 35:1899619007, 2022. 5 [58] Gillian Smith. The future of procedural content generation In Proceedings of the AAAI Conference on Arin games. tificial Intelligence and Interactive Digital Entertainment, pages 5357, 2014. [59] Sam Snodgrass and Santiago Ontanon. Experiments in map generation using markov chains. In FDG, 2014. 3 [60] Sam Snodgrass and Santiago Ontanon. Learning to generate video game maps using markov models. IEEE transactions on computational intelligence and AI in games, 9(4):410 422, 2016. 3 [61] Shyam Sudhakaran, Miguel Gonzalez-Duque, Matthias Freiberger, Claire Glanois, Elias Najarro, and Sebastian Risi. Mariogpt: Open-ended text2level generation through large language models. Advances in Neural Information Processing Systems, 36, 2024. 3 [62] Adam Summerville and Michael Mateas. Super mario as string: Platformer level generation via lstms. arXiv preprint arXiv:1603.00930, 2016. 3 [63] Adam Summerville, Sam Snodgrass, Matthew Guzdial, Christoffer Holmgard, Amy Hoover, Aaron Isaksen, Andy Nealen, and Julian Togelius. Procedural content generation via machine learning (pcgml). IEEE Transactions on Games, 10(3):257270, 2018. [64] Yuqian Sun, Zhouyi Li, Ke Fang, Chang Hee Lee, and Ali Asadipour. Language as reality: co-creative storytelling game experience in 1001 nights using generative ai. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pages 425434, 2023. 3 [65] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 5 [66] Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, and Julian Togelius. Level generation through large language models. In Proceedings of the 18th International Conference on the Foundations of Digital Games, pages 18, 2023. 3 [67] Mike Treanor, Bryan Blackford, Michael Mateas, and Ian Bogost. Game-o-matic: Generating videogames that represent ideas. In Proceedings of the The third workshop on Procedural Content Generation in Games, pages 18, 2012. 3 [68] Mike Treanor, Alexander Zook, Mirjam Eladhari, Julian Togelius, Gillian Smith, Michael Cook, Tommy Thompson, Brian Magerko, John Levine, and Adam Smith. Ai-based game design patterns. 2015. [69] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. 2, 3 [70] Vanessa Volz, Jacob Schrum, Jialin Liu, Simon Lucas, Adam Smith, and Sebastian Risi. Evolving mario levels in the latent space of deep convolutional generative adversarial network. In Proceedings of the genetic and evolutionary computation conference, pages 221228, 2018. 3 [71] Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, and Chunhua Shen. Autostory: Generating diverse Internastorytelling images with minimal human efforts. tional Journal of Computer Vision, pages 122, 2024. 3 [72] Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, et al. Pandora: Towards general world model arXiv with natural preprint arXiv:2406.09455, 2024. 3 language actions and video states. [73] Junfei Xiao, Feng Cheng, Lu Qi, Liangke Gui, Jiepeng Cen, Zhibei Ma, Alan Yuille, and Lu Jiang. Videoauteur: arXiv preprint Towards long narrative video generation. arXiv:2501.06173, 2025. 8 [74] Chenshu Xu, Yangyang Xu, Huaidong Zhang, Xuemiao Xu, and Shengfeng He. Dreamanime: Learning style-identity textual disentanglement for anime and beyond. IEEE Transactions on Visualization and Computer Graphics, 2024. [75] Mingyu Yang, Junyou Li, Zhongbin Fang, Sheng Chen, Yangbin Yu, Qiang Fu, Wei Yang, and Deheng Ye. Playable game generation. arXiv preprint arXiv:2412.00887, 2024. 2, 3 [76] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal arXiv long story generation with large language model. preprint arXiv:2407.08683, 2024. 1, 3, 4, 6 [77] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 4 [78] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325, 2025. 2, 3 [79] Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, and Mohit Bansal. Envgen: Generating and adapting environments via llms for training embodied agents. arXiv preprint arXiv:2403.12014, 2024. 3 [80] Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. 3, 6 [81] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 6 [82] Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Steve-eye: Equipping llm-based embodied agents arXiv preprint Lu. with visual perception in open worlds. arXiv:2310.13255, 2023. 3 [83] Hongwei Zhou, Jichen Zhu, Michael Mateas, and Noah Wardrip-Fruin. The eyes, the hands and the brain: What can text-to-image models offer for game design and visual creativity? In Proceedings of the 19th International Conference on the Foundations of Digital Games, pages 113, 2024. 3 [84] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. Advances in Neural Information Processing Systems, 37: 110315110340, 2024. 14 [85] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. Advances in Neural Information Processing Systems, 37: 110315110340, 2025. [86] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan. Vl-gpt: generative pre-trained transformer for vision and arXiv preprint language understanding and generation. arXiv:2312.09251, 2023."
        },
        {
            "title": "Overview",
            "content": "In this appendix, we present the following: Details of our AnimeGamer in Section A. Dataset Construction Pipeline in Section B. Implementation details of AnimeGamer and other baselines in Section C. Details of the evaluation benchmark in Section D. Human Evaluation in Section E. Additional visualization results in Section F. A. Details of AnimeGamer stinema value, <SC></SC> for Special Tokens in MLLM. We add special token to the tokenizer of our MLLM to generate game states and formulate the output. Specifically, we use <MS></MS> to represent the start and end of motion scope, <ST></ST> for social value, <ET></ET> for entertainment value. To continuous generate the action-aware multimodal representations, We add 226 learnable query tokens <IMAGE i> to stimulate continuous generation, and <VS></VS> to represent start and end of the animation shot representation. Motion Scope. We employ Memflow [16] to compute the optical flow transformation for each frame within the video. Subsequently, we convert them into absolute values to represent the motion scope. filtering threshold of r=0.2 is adopted to filter out the background information. After that, we calculate the average value of the remaining part to denote the motion scope of an animation shot. Next, we divide the range into five levels, which serve as discrete targets for the MLLM to fit. B. Dataset Construction Details Video Pre-processing. Taking an anime film as an example, we first download the film and crop its borders. Then, we resize it to the corresponding size and divide it into several segments using scene - detection model [10]. Next, each segment is split according to fixed time period (2 seconds). In this way, we obtain the video training data arranged in timestamp order. In addition, we download the reference images of each protagonist to locate the characters in each video segment. Captioning. We utilize Intern-VL-26B [11] to generate captions for each animation shot. We input the protagonist reference images and eight evenly-sampled frames from an anime clip as visual input. To match the character, we employ the following prompt: Image-1: <image>Image-2: <image>According to the characters index in Image-1, your task is to answer how many characters are in Image-2 (4 frames from anime video) and what their [Num]<number indices are. Response format: of characters>[ID]<index of the character>; Response example: If no characters are detected, please respond with [Number]0[Index]0. Your response: [Number]2[Index]1,3. To acquire descriptions of motion, the environment, and character states, we utilize the following prompt: Image-1: <image>Image-1 is from an anime clip, Your task is to extract structured description based on the information. First, will give you the movement level <ML>respectively. The movement level is categorized into five levels: Level 1: very small movement amplitude, almost imperceptible; Level 2: small movement amplitude, slight swaying or adjustments; Level 3: moderate movement amplitude, appropriate movement or adjustments; Level 4: large movement amplitude, noticeable and significant; Level 5: very large movement amplitude, extremely obvious and intense. Next, you need to generate the following information: (1) subject <S>, motion description <MD>and environment <EV>. Please use single word for subject and background, use simple phrase for motion in the present simple tense. (2) Movement adverb <MA>: Based on <ML>, give <MD>a fitting adverb. (3) Social interaction <SC>: If there are two or more characters interacting socially in the scene, such as talking, hugging, walk together or kissing, use 1; otherwise, use 0 to indicate no social action. (4) Entertainment <ET>: If the protagonist is engaged in entertainment activities, sports or relaxing, such as reading, riding, flying, swimming, whispering, archery... use 1; otherwise, use 0 to indicate no entertainment action. (5) Stamina <ST>: Stamina can be restored through actions lying, sleeping, hugging, like eating, drinking, If the <MD>are restoring stamina, treatment... fill Example output: <S>Girl</S><MD>run</MD><EV>Forest </EV><MA>slowly</MA><SC>0</SC><ET> 1</ET><ST>-1</ST>. Your response: in 1; otherwise, use -1. C. Implementation Details In this section, we present the implementation details of our AnimeGamer in Section C.1, baseline methods in Sec13 Figure 8. Four variants of our animation shot encoder. 1) We use random frame instead of first frame to obatain the action-aware multimodal representation, denoted as w/ rand. frame; 2) We replace the MLP module in Ea with single Linear layer to reduce learnable parameters, denoted as w/ less para; 3) We combine fv with fmd via element-wise addition, denoted as w/ addition; 4) We combine fv with fmd via cross-attention, denoted as w/ cross-attn. tion C.2 and the ablation studies in Section C.3. C.1. AnimeGamer Animation Shot Encoding and Decoding. In this phase, we initialize the parameters of our animation shot decoder using CogvideoX-2B6. We apply LoRA to the 3D-Attention with rank of 64. The learning rate is set to 2e-4. To enhance generalization capabilities, we initially pretrain using 100k samples from the WebVid [3]. Subsequently, we start with warm-up phase where Ea trained for 10,000 steps. This is followed by joint training phase of Ea and Da which extends for an additional 80,000 steps. Next Game State Prediction. For the MLLM, we initialize our model with the weight of Mistral-7B and train it using LoRA, facilitated by the peft library. The LoRA rank is set to 32, with lora-alpha also set to 32. The learning rate is 5e-5, and the training is carried out for 15,000 steps. Decoder Adaptation. In this stage, we fine-tune only Da. The learning rate is 5e-5, and the training is executed for 10,000 steps. C.2. Baselines GSC. We utilize StoryDiffusion [84] based on SDXL [50], where the instructions for 10-round game are input simultaneously to generate the corresponding images. Then, we use the Cogvideox-5B-I2V7 model to convert these images into animation shots. During this process, action instructions are provided as prompts to the pretrained I2V model. GFC. We fine-tune the T2I model FlUX8 using LoRA. For training, we pair the first frame of each animation shot with its corresponding instruction to form image-text pairs. We employ LoRA with rank of 32 and train for 200,000 steps. During testing, we convert images to video using the same method as in GSC. 6THUDM/CogVideoX-2b 7THUDM/CogVideoX-5b-I2V 8black-forest-labs/FLUX.1-dev GC. We fine-tune the CogvideoX-2B model using LoRA, employing the same configuration as used for training Da. C.3. Ablation Study In the ablation study, we randomly selected movie Qiqis Delivery Service from the training dataset as the training data. We split approximately 2,000 training samples into training set and test set with an 8:2 ratio. The ablation on animation shot tokenization and de-tokenization does not incorporate the MLLM, in order to focus on the reconstruction ability of Ea and Da for animation shots. Ablation on Animation Shot Tokenization and Detokenization. We construct four variants for Ea, as shown in Figure 8. 1) We use random frame instead of the first frame as fv, denoted as w/ rand. frame. 2) We replace the MLP with simpler Linear layer to align features, denoted as w/ less para. 3) We use element-wise addition to unify fv and fmd, denoted as w/ addition. 4) We use cross-attention to unify fv and fmd, denoted as w/ crossattn. Ablation on Next Game State Prediction. In this ablation study, we incorporate the Cosine Loss into the training process. The overall training loss is combination given by: = LCE + αLMSE + βLcos, (4) where the hyperparameters α and β are set to 0.5. D. Evaluation Benchmark Construction MLLM as benchmark constructor. We use the following prompt for GPT-4o to generate our evaluation benchmark. 14 You are world model for an anime life simulation. You can generate stories of the character living in the world. The stories should sound like game and leave space for user interaction. Now, you need to generate 10-panel story (simulation game) with [Character] as the main character. For each turn, you should generate the following components: 1) Characters <S>: The main character must appear in each panel, and 0-1 additional characters can be included as supporting characters, chosen from [Characters]. 2) Motion Description <MD>: Describe the main characters action with simple phrase. 3) Environment <EV>: Describe the current environment with one word. 4) Main characters state: you need to generate the following information: (1) Motion Level <ML>: The movement level is categorized into number 1-5: 1: very small movement amplitude 2: small movement amplitude, slight swaying or adjustments 3: moderate movement amplitude, appropriate movement or adjustments 4: large movement amplitude, noticeable and significant 5: very large movement amplitude, extremely obvious and intense (2) Movement adverb <MA>: Based on <ML>, give <MD>a fitting adverb. (3) Social interaction <SC>: If there are two or more characters interacting socially in the scene, such as talking, hugging, walking together, or kissing, use 1; otherwise, use 0 to indicate no social action. (4) Entertainment <ET>: If the protagonist is engaged in entertainment activities, sports, or relaxing, such as reading, riding, flying, swimming, whispering, archery, use 1; otherwise, use 0 to indicate no en- (5) Stamina <ST>: Stamina tertainment action. can be restored through actions like eating, drinking, lying, sleeping, hugging, treatment... If the <MD>are restoring stamina, use 1; otherwise, use -1. For the entire story, here are some instructions you need to follow: 1) Ensure continuity between different panels as much as possible. Encourage different actions in the same scene or return to previous scene in subsequent panels. 2) Keep it realistic and as close to life simulation game scenario as possible. Please use common scenes and easily representable actions, and avoid including tiny, difficult-to-generate objects. 3) Output format: Each line represents one turn, using the following format: <S>Characters</S><MD>Motion Description</MD><EV>Environment</EV><ML >Motion Level</ML><MA>Movement adverb </MA><SC>Social interaction</SC><ET> Entertainment</ET><ST>Stamina</ST> MLLM as judge. We use the following prompt for GPT4o to assess the output of the models. Please act as an impartial judge and evaluate the quality of the generation story video contents provided by AI agents. Heres some instructions you need to follow: 1) Story Composition: Each story consists of 5 scenes, and will provide you with their respective prompts. 2) Evaluation: For each AI agents output, will present you with an image composed of 5 frames extracted from the videos. The image in the i-th row represent 5 frames extracted from the generated video corresponding to scene i. 3) Evaluation Criteria: You need to score each AI agents output based on Overall Quality <OA>: The overall gaming experience. Text Alignment <TA>: The alignment between the prompt and the generated results. Contextual Coherence <ConC>: Whether the content of each scene can connect naturally, Character Consistency <ChaC>: Are the characters in each scene consistent with the provided reference characters? Emotional Consistency <EC>: The consistency between the expression of the scenes and the emotional statements in the prompt. Visual Coherence <VC>: Are the colors, styles, and compositions of the scenes consistent? The score range for these criteria is from 1 to 10, with higher scores indicating better overall performance. 4) Output Format: Your output should contain four lines, each starting with the evaluation criteria code such as <OA>, followed by numbers representing the scores for each of the agents, separated by spaces. Finally, provide brief explanation of your evaluation on new line. 5) Evaluation Requirements: Avoid any bias, ensure that the order of presentation does not affect your decision. Do not let the length of the response influence your evaluation. Do not favor certain agent names. E. Human Evaluation For the human evaluation, we recruit 20 participants who hold at least bachelors degree and have prior experience in image or video generation. total of 9-round games with 50 samples are presented to the participants. We showcase the animation shots and character states generated by various models to the participants in the form of PowerPoint presentation and ask them to fill out an Excel spreadsheet. They are required to rate the performance of different models for each metric in every game. Subsequently, we convert the rankings into absolute scores: 10 points for the first15 ranked model, 7 points for the second, 4 points for the third, and 1 point for the fourth. Finally, we calculate the average performance of each model. F. Additional Qualitative results We present the image of characters appeared in our paper in Figure 9. We present the infinite game generation results of AnimeGamer and other baselines in our homepage: https : / / howe125 . github . io / AnimeGamer . github.io/."
        },
        {
            "title": "Sheeta",
            "content": "Figure 9. Image of characters in the paper."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "City University of Hong Kong"
    ]
}