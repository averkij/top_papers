{
    "paper_title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
    "authors": [
        "Xiang Chen",
        "Yuling Shi",
        "Qizhen Lan",
        "Yuchao Qiu",
        "Xiaodong Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments."
        },
        {
            "title": "Start",
            "content": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents 1Zhejiang University Xiang Chen1 Yuling Shi2 Qizhen Lan3 Yuchao Qiu1 Xiaodong Gu2 2Shanghai Jiao Tong University chenxianghz@zju.edu.cn, yuling.shi@sjtu.edu.cn, Qizhen.Lan@uth.tmc.edu, 12532006@zju.edu.cn, xiaodong.gu@sjtu.edu.cn 3UTHealth Houston 5 2 0 2 ] . [ 1 0 7 8 8 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectorylevel rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, Federated Self-Evolution framework for LLM agents. Fed-SE establishes local evolutionglobal aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments."
        },
        {
            "title": "1 Introduction",
            "content": "LLM-based agents have demonstrated significant potential in complex interactive tasks, ranging from embodied intelligence to online service systems (Zitkovich et al., 2023; Belkhale et al., 2024; Li et al., 2025b; Peng et al., 2025; Shi et al., 2024; OpenAI et al., 2024; DeepSeekAI et al., 2025). The enhancement of agent capabilities typically relies on the accumulation 1Our code is available at https://github.com/ Soever/Federated-Agents-Evolution. 1 Figure 1: Motivation. Static federated methods limit agent adaptation. While directly introducing online learning into FL suffers from high variance and gradient conflicts. Fed-SE resolves this by stabilizing learning via trajectory filtering and robust subspace aggregation. of experience through continuous interaction with environments (Liu et al., 2025; Chen et al., 2025b; Fang et al., 2025; Cai et al., 2025), facilitating the refinement of decision logic, tool usage strategies, and long-horizon planning. However, in real-world deployments, platform compliance requirements, and business risk controls often preclude the centralized aggregation of raw interaction data (Yang et al., 2019; Li et al., 2021; Kairouz et al., 2021). Consequently, agents are often optimized in isolation within single environments (Cheng et al., 2024; Silagadze, 2023). This isolation necessitates additional online adaptation when migrating to new environments, thereby significantly increasing the cost of cross-scenario reuse and limiting the formation of generalizable capabilities (Zala et al., 2024; He et al., 2025; Chen et al., 2025a). Federated Learning (FL) (McMahan et al., 2023) offers collaborative training paradigm to achieve knowledge aggregation by aggregating local updates from clients without exporting raw data. Despite its success with static offline corpora (Jajoo, 2025; Wu et al., 2024a), extending FL to the online interaction trajectories of LLM-based agents remains underexplored. Meanwhile, alternative schemes involving context exchange (Shi et al., 2025; Wu et al., 2024c) avoid parameter training but struggle to consolidate interaction experience into model parameters. Therefore, establishing parameter-level collaborative framework for LLM-agent represents significant gap in the current landscape. However, extending FL to this dynamic setting is non-trivial. Unlike conventional training based on static datasets, online decision sequences generated by agents in heterogeneous environments exhibit significant distributional discrepancies (Jin et al., 2022; Hwang and Hong, 2025). straightforward integration of standard Reinforcement Learning (RL) into the federated paradigm introduces training instability due to high variance under sparse rewards (Li et al., 2025c; Bjorck et al., 2022) and gradient conflicts arising from heterogeneous dynamics (Zhang et al., 2024b; Jiang et al., 2025). Furthermore, prohibitively high communication overheads in full-parameter updates hinder practical deployment (QI et al., 2024; Singhal et al., 2025a). To address these challenges, Federated Self-Evolution (Fed-SE) framework is proposed. This paradigm synergizes local self-evolution with the global aggregation of lightweight adapters to decouple general reasoning capabilities from environment-specific dynamics. Specifically, clients perform local optimization on filtered successful trajectories to stabilize gradients against sparse rewards, while the server aggregates updates within low-rank subspace to mitigate cross-task negative transfer. The main contributions are summarized as follows: We introduce communication-efficient framework that systematically addresses gradient instability in federated agent training. By integrating trajectory filtering, experience replay, and low-rank aggregation, the framework enables stable selfevolution under sparse rewards, achieving robust cross-environment capability transfer without sharing raw interaction data. decentralized training mechanism based 2 on parameter-efficient fine-tuning is designed. By keeping the shared base model frozen and updating lightweight adapters, environment-specific improvements are distilled into generalizable parameter knowledge, while significantly reducing coldstart costs and data requirements on the edge side. Empirical evaluations across five heterogeneous environments indicate that Fed-SE improves the average task success rate by approximately 18% over the Federated Averaging baseline, validating the effectiveness and performance superiority of the proposed method in handling diverse and complex interactive tasks."
        },
        {
            "title": "2.1 LLM Agents and Self-Evolution",
            "content": "LLM agents have evolved from promptingbased systems to autonomous entities capable of planning, tool use, and multi-step reasoning (Yao et al., 2023; Schick et al., 2023). Recent research focuses on enabling agents to improve autonomously without human supervision through self-evolution (Tao et al., 2024; ang Gao et al., 2025). Self-improvement training has emerged as dominant paradigm, encompassing self-rewarding (Yuan et al., 2025), self-play (Chen et al., 2024), and selfrefinement (Madaan et al., 2023) mechanisms. Frameworks like AgentGym (Xi et al., 2024) integrate these mechanisms with diverse environments and evolutionary methods to achieve continuous improvement. Memory and experience accumulation further support persistent learning, with methods ranging from verbal reinforcement through reflective memory (Shinn et al., 2023) to autonomous experience extraction (Zhao et al., 2024) and continual learning via causal abstractions (Majumder et al., 2023). For cross-environment generalization, modular architectures (Yin et al., 2024), self-evolving curricula (Qi et al., 2025), and cross-task experience sharing (Yang et al., 2024) have shown promising results."
        },
        {
            "title": "Despite",
            "content": "these advances, existing selfevolution approaches universally assume centralized access to agent trajectories, which becomes problematic when agents are deployed Figure 2: Overview of the Fed-SE Framework. The framework operates through two distinct phases: local agent self-evolution and global knowledge aggregation. Parallel client agents interact with diverse environments to optimize local low-rank adapters (LoRA) using filtered successful trajectories stored in privacy-preserving experience buffers. The central server aggregates these distributed adapter parameters to construct global model with generalized reasoning capabilities, which is subsequently synchronized across all clients for the next communication round. across distributed, privacy-sensitive environments."
        },
        {
            "title": "2.2 Federated Learning for Large\nLanguage Models and Agents",
            "content": "Federated learning for LLMs has focused on parameter-efficient fine-tuning (PEFT) to mitigate high communication costs (Singhal et al., 2025b; Sun et al., 2024; Koo et al., 2025; Li et al., 2025a). FedIT (Zhang et al., 2024a) and FedPETuning (Zhang et al., 2023) established the efficacy of federated instruction tuning. Heterogeneous LoRA methods, such as FlexLoRA (Bai et al., 2024), address client resource constraints through dynamic rank adjustment. Moving beyond instruction tuning, recent research has begun exploring federated alignment and agent learning. While FedRLHF (Wu et al., 2024b) establishes convergence-guaranteed framework for privacy-preserving policy optimization via client-specific feedback, it remains limited to single-turn preference alignment, lacking the multi-step reasoning capabilities required for autonomous agents. Meanwhile, FICAL (Wu et al., 2024c) pioneers federated in-context agent learning by transmitting knowledge compendiums, yet it relies on frozen models without parameter updates. Consequently, there is lack of frameworks for the continuous, autonomous self-evolution of agents across heterogeneous environments under privacy constraints. To bridge this gap, we propose FedSE, federated self-evolution framework that enables agents to continuously improve their reasoning and planning capabilities across distributed environments without raw data sharing."
        },
        {
            "title": "3 Preliminaries and Problem Setup",
            "content": "Agent-environment interaction is modeled as POMDP. At timestep t, given an instruction and historical context Ht1, the policy πθ generates reasoning chain ht and an action at . The probability of generating trajectory τ decomposes as: pθ(τ e, u) = (cid:89) (cid:16) t=1 πθ(ht u, Ht1, ot) (1) πθ(at u, Ht1, ot, ht) (cid:17) Consider federated system with clients, each holding private environment ek. Under sparse binary rewards R(τ ) {0, 1} (indicating 3 task success or failure), the local objective for client is to maximize the expected return: Jk(θ) = Eτ πθ(ek,u)[R(τ ek)] (2) The global goal is to optimize the weighted sum of local objectives via decentralized updates, strictly prohibiting raw trajectory sharing: max θ J(θ) = (cid:88) k=1 ωkJk(θ) (3) where ωk denotes the aggregation weight such that (cid:80)K k=1 ωk = 1."
        },
        {
            "title": "4 Methodology",
            "content": "The Federated Self-Evolution (Fed-SE) framework is proposed to enable collaborative agent training under privacy constraints. As illustrated in Figure 2, Fed-SE decouples general reasoning capabilities from environmentspecific adaptability, achieving knowledge transfer via local trajectory self-evolution and global low-rank aggregation. The training process operates iteratively over communication In each round, the procedure is dirounds. vided into two distinct phases: Local Agent Self-Evolution (Section 4.1), where clients optimize adapters using filtered high-quality trajectories, and Global Knowledge Aggregation (Section 4.2), where the server unifies distributed knowledge into generalizable global model."
        },
        {
            "title": "4.1 Local Agent Self-Evolution",
            "content": "To mitigate the high gradient variance characteristic of sparse reward settings, the standard RL objective is optimized via surrogate lower bound. By leveraging importance sampling and treating the policy from the previous iteration as the reference distribution, the maximization of expected return is theoretically approximated by performing Maximum Likelihood Estimation (MLE) solely on the distribution of successful trajectories D+ (see Appendix for detailed derivation). Consequently, the optimization problem is formulated as: max θ Eτ D+[log πθ(τ )], (4) guided by this formulation, the local evolution process proceeds as follows. Exploration and Filtering. The agent first interacts with the local environment using the current policy to generate exploration trajectories. Based on the binary reward signal, the subset of successful trajectories Dsucc is filtered k,t out: Dsucc k,t = {τ τ πΘ,ϕt, R(τ ) = 1}, (5) where Dsucc k,t denotes the set of newly discovered successful trajectories by client in round t. Experience Accumulation. To alleviate distribution shift and catastrophic forgetting, cumulative experience buffer is explicitly maintained. The training set Dtrain is constructed by merging historical experience with the new successful trajectories: k,t Dtrain k,t = Dtrain k,t1 Dsucc k,t , (6) k,t where Dtrain the current round, and Dtrain buffer from the previous round. is the complete training set for k,t1 represents the Parameter-Efficient Fine Tuning. While keeping the base model Θ frozen, only the lightweight adapter parameters ϕ are optimized to minimize the negative log-likelihood loss on the accumulated dataset: L(ϕ) = τ Dtrain k,t log πΘ,ϕ(aj u, Hj1, hj) (cid:21) (cid:20) τ (cid:88) j=1 (7) where τ is the trajectory length, aj is the action taken at step j, and context Hj1 includes history prior to step j."
        },
        {
            "title": "4.2 Global Knowledge Aggregation",
            "content": "The global phase distills environment-specific experiences into generalizable capabilities under strict communication constraints. Low-Rank Subspace Aggregation. To mitigate the prohibitive bandwidth costs of fullparameter transmission and, more crucially, to address negative transfer arising from task heterogeneity, aggregation is performed strictly within the low-rank adapter space. The server computes the global consensus via unweighted averaging to prevent bias toward environments with abundant easy trajectories: ϕt ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 ϕt,k, (8) 4 where ϕt,k denotes the local LoRA matrices. This constrained averaging effectively cancels environment-specific noise while preserving consistent reasoning logic across tasks. Global Parameter Synchronization. To mitigate client drift caused by heterogeneous dynamics, periodic re-anchoring mechanism is enforced. By resetting local parameters to the global consensus, all agents initiate the next cycle from unified point: ϕt+1,k = ϕt, {1, . . . , K}. (9) This synchronization acts as an implicit hard regularization, preventing local policies from diverging toward environment-specific optima and ensuring alignment with the global objective. The overall procedure is summarized in Algorithm 1. Algorithm 1 Federated Self-Evolution Require: Frozen base model Θ, Initial LoRA parameters ϕ0, Clients set = {1, . . . , K}, Initial dataset Dtrain k, Total rounds Ensure: Optimized global LoRA parameters ϕT 1: for = 0 to 1 do 2: 3: Server: Broadcast global parameters ϕt to all clients k,1 = D0 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: Client-side Parallel Execution: for all do // 1. Exploration and Filtering Generate exploration trajectories Dexp policy πΘ,ϕt Dsucc Filter successful trajectories: k,t {τ Dexp R(τ ) = 1} // 2. Experience Accumulation if = 0 then Dtrain k,t D0 Dsucc k,t else Dtrain k,t Dtrain k,t1 Dsucc k,t end if // 3. Local PEFT ϕt+1,k arg minϕ (cid:18) (cid:20) τ Dtrain k,t (cid:80)τ j=1 log πΘ,ϕ(aj u, Hj1, hj) Upload update ϕt+1,k to server (cid:21)(cid:19) end for 19: 20: 21: 22: 23: 24: end for Server: // 4. Global Knowledge Aggregation Collect parameters {ϕt+1,k}K Update global parameters: (cid:80)K ϕt+1 1 k=1 ϕt+1,k k= using"
        },
        {
            "title": "5 Experimental Results",
            "content": "5.1 Experiments Setting Environment and Tasks. The proposed Fed-SE framework is evaluated across five heterogeneous environments that cover diverse sequential decision-making capabilities: BabyAI (Chevalier-Boisvert et al., 2019) (embodied control and grounding), WebShop (Yao et al., 2022) (web interaction), TextCraft (Prasad et al., 2024) (hierarchical planning), MAZE (Abdulhai et al., 2025) (longhorizon memory), and Wordle (Abdulhai et al., 2025) (iterative reasoning). This selection ensures comprehensive assessment of agent generalization across distinct task dynamics. Baselines. To isolate the contributions of cross-environment collaboration and online selfevolution, Fed-SE is compared against three baselines: (1) Local Instruction Tuning (Local): Agents are fine-tuned independently using only their local static behavioral cloning datasets. This baseline assesses agent learning capabilities in the complete absence of crossenvironment knowledge sharing. (2) Centralized Instruction Tuning (Centralized): Static datasets from all environments are aggregated onto central server for joint instruction tuning. This setting represents standard multitask learning paradigm where data privacy constraints are disregarded. (3) Federated Averaging (FedAvg) (McMahan et al., 2023): Standard collaborative training on static datasets without the online self-evolution mechanism, used to verify the specific gain from dynamic experience accumulation. Implementation Details. All experiments are conducted on NVIDIA A6000 GPUs using Llama-2-Chat-7B as the base model. The training process spans 20 communication rounds. To ensure fair comparison between the proposed method and baselines, hyperparameters such as learning rate and optimizer settings remain consistent across all experiments."
        },
        {
            "title": "5.2 Main Results",
            "content": "As detailed in Table 1 and Figure 3, Fed-SE achieves an average success rate of 0.66, significantly outperforming FedAvg (0.56), Local (0.53), and Centralized (0.49). It establishes substantial advantages in reasoningheavy tasks like BabyAI (0.92) and Maze (0.80). Figure 3: Comparative performance evolution across heterogeneous tasks. The plots illustrate the test success rate trajectories over 20 communication rounds. The solid curves represent the continuous improvement of federated methods (Fed-SE and FedAvg), while the horizontal dashed lines indicate the converged baseline performance of static approaches (Local and Centralized). Fed-SE (Blue) exhibits robust growth, consistently breaking the performance ceilings of static baselines and significantly outperforming FedAvg (Red), particularly in complex reasoning environments like Maze. Table 1: Effectiveness of our method on different tasks. Performance is measured by success rate. Method Local Centralized FedAvg Fed-SE (Ours)"
        },
        {
            "title": "BabyAI Webshop Textcraft Maze Wordle Average",
            "content": "0.64 0.62 0.70 0.92 0.68 0.59 0.61 0.66 0.34 0.36 0.49 0.52 0.20 0.36 0.28 0.80 0.08 0.00 0.16 0.16 0.53 0.49 0.56 0. The results highlight three key mechanisms: Mitigation of Gradient Conflicts and Negative Transfer. Centralized fine-tuning performs poorly (0.49), failing completely on Wordle (0.00), which underscores the severity of negative transfer caused by conflicting gradients in heterogeneous data mixing. Conversely, FedAvg (0.56) outperforms the Local baseline, suggesting that aggregation in parameter space effectively decouples optimization trajectories. This approach avoids direct conflicts and preserves general capabilities, facilitating positive cross-task transfer by maintaining robust instruction-following abilities. Self-Evolution Breaking Static Imitation Limits. Fed-SE outperforms FedAvg by 17.9%, validating the critical role of online interaction in overcoming the distribution shifts inherent in static behavioral cloning. By incorporating filtered successful trajectories, the framework enables self-correction. This is most evident in the Maze task, where FedSE boosts the success rate from 0.28 to 0.80. Figure 3 illustrates distinct capability breakthrough after round 10, demonstrating the mastery of complex sequential decision-making beyond static data coverage. Performance Trade-offs in Specific Environments. While the Local method marginally leads in WebShop (0.68) due to the tasks high domain specificity benefiting from isolated specialization, Fed-SE (0.66) remains highly competitive. Crucially, it significantly surpasses Centralized (0.59), indicating that Fed-SE effectively mitigates cross-task interference while retaining domain-specific expertise, Figure 4: Impact of Key Components on Final Performance. Removing the success filter results in catastrophic performance drop (-26%), while excluding history or using weighted averaging also degrades the robust baseline (66%). achieving robust balance between global generalization and local adaptation."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "To verify the effectiveness of key components in the Fed-SE framework, we constructed three variants for ablation studies while retaining the initial behavioral cloning dataset: (1) w/o History: Removes the experience accumulation mechanism, fine-tuning only with new data from the current round; (2) w/o Filtering: Removes the success filter, including failed trajectories in training; (3) w/ Weighted Avg: Uses weighted averaging based on the number of successful trajectories during aggregation. Overall Performance Analysis. As shown in Figure 4, the full Fed-SE achieves the highest average success rate (66.1%), outperforming all ablation variants. Notably, removing the success filter results in the most severe performance decay, with the average success rate plummeting to 40.5%, while removing history experience and changing the aggregation strategy also lead to varying degrees of performance decline. Importance of Cumulative History. While w/o History (64.1%) achieves competitive average, this masks critical deficiencies in complex long-horizon tasks. In Maze (Figure 5a), performance plateaus at 40.0%, far below Fed-SEs 80.0%. This significant gap indicates that relying solely on fresh data leads to the loss of prior capabilities during distribution adaptation. The historical buffer acts as (a) Performance Evolution on Maze (b) Performance Evolution on Wordle Figure 5: Evolution Process Analysis. (a) The Maze task shows that removing history accumulation (w/o History) leads to suboptimal convergence. (b) The Wordle task demonstrates that removing the success filter (w/o Filtering) causes catastrophic performance collapse due to noise injection. an Experience Replay mechanism, effectively suppressing Catastrophic Forgetting and stabilizing policy oscillation against online distribution shifts, thereby sustaining the evolution of long-horizon planning. Necessity of Success Filtering. Removing the success filter results in the most drastic 7 decay. In Wordle (Figure 5b), success rates collapse to zero after round 8. This confirms that failed trajectories act as misleading signals under behavioral cloning assumptions. Without strict filtering, the model erroneously imitates failure, rapidly contaminating global parameters and diverting policy optimization from the true objective. Robustness of Aggregation Strategy. Weighted averaging (59.8%) underperforms simple averaging. This suggests that under highly heterogeneous distributions, trajectory quantity does not correlate with gradient quality (e.g., BabyAI generates abundant simple samples). Weighted aggregation risks biasing the global model toward simpler tasks, weakening generalization on difficult ones. Therefore, simple averaging demonstrates superior robustness in balancing multi-task heterogeneity."
        },
        {
            "title": "5.4 Communication Efficiency",
            "content": "Communication overhead in Federated Learning is critical constraint for deployment. The Fed-SE framework achieves parameter-efficient transmission through LoRA adapters, making communication overhead linearly related to the rank (r). Figure 6: Trade-off between model performance and communication cost across different LoRA ranks. Trade-off between Performance and Cost. Figure 6 illustrates the trade-off between performance and communication cost under different ranks. The analysis reveals typical trend of Diminishing Returns: increasing from 4 to 8 significantly boosts the success rate from 51.8% to 57.5% (+5.7%), indicating insufficient model capacity at = 4. However, further increasing from 8 to 16 yields 8 only marginal gain from 57.5% to 59.1% (+1.6%), while doubling the communication overhead (76.3 MB 152.5 MB). This comparison compellingly demonstrates that = 8 represents the Optimal Trade-off Point between performance and communication cost. Deployment Feasibility As agents evolve, successful trajectories lengthen, increasing dynamic memory demands. Larger ranks (e.g., = 16) yield negligible gains but consume static memory needed for processing these long sequences. Thus, = 8 balances model capacity with memory constraints, preventing out-of-memory errors during later evolutionary stages."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents Fed-SE, novel framework that bridges the gap between agent selfevolution and privacy-constrained collaborative learning. By synergizing local trajectory optimization with global low-rank aggregation, Fed-SE mitigates gradient conflicts and reward sparsity inherent in distributed training. Empirical results demonstrate an 18% gain over static baselines and robust capability transfer in complex scenarios. These findings indicate the feasibility of distilling generalist agent capabilities from decentralized, self-evolved interaction experiences without raw data sharing, providing viable paradigm for scalable and privacy-preserving agent learning."
        },
        {
            "title": "Limitations",
            "content": "While transmitting adapter parameters prevents raw data exposure, the framework does not currently incorporate cryptographic techniques such as Differential Privacy or Homomorphic Encryption. This design choice prioritizes the high parameter precision required for complex reasoning but may leave the system vulnerable to advanced gradient reconstruction attacks. Additionally, the current reliance on synchronous Federated Averaging assumes consistent client connectivity. In real-world edge deployments, device heterogeneity and network instability could induce straggler effects, potentially hindering convergence efficiency. Furthermore, the global aggregation mechanism relies on standard element-wise averaging; advanced aggregation strategies specifically tailored for optimizing the fusion of low-rank adapters remain unexplored in this work."
        },
        {
            "title": "References",
            "content": "Marwa Abdulhai, Isadora White, Charlie Victor Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. 2025. LMRL gym: Benchmarks for multi-turn reinforcement learning with language models. In Forty-second International Conference on Machine Learning. Huan ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, and 8 others. 2025. survey of self-evolving agents: On path to artificial super intelligence. Preprint, arXiv:2507.21046. Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, and Yaliang Li. 2024. Federated finetuning of large language models under heterogeneous tasks and client resources. Preprint, arXiv:2402.11505. Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quan Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. 2024. RT-H: action hierarchies using language. In Robotics: Science and Systems XX, Delft, The Netherlands, July 15-19, 2024. Johan Bjorck, Carla P. Gomes, and Kilian Q. Weinberger. 2022. Is high variance unavoidable in rl? case study in continuous control. Preprint, arXiv:2110.11222. Zhicheng Cai, Xinyuan Guo, Yu Pei, JiangTao Feng, Jiangjie Chen, Ya-Qin Zhang, Wei-Ying Ma, Mingxuan Wang, and Hao Zhou. 2025. Flex: Continuous agent evolution via forward learning from experience. Preprint, arXiv:2511.06449. Arthur Chen, Zuxin Liu, Jianguo Zhang, Akshara Prabhakar, Zhiwei Liu, Shelby Heinecke, Silvio Savarese, Victor Zhong, and Caiming Xiong. 2025a. Grounded test-time adaptation for llm agents. Preprint, arXiv:2511.04847. Silin Chen, Shaoxin Lin, Xiaodong Gu, Yuling Shi, Heng Lian, Longfei Yun, Dong Chen, Weiguo Sun, Lin Cao, and Qianxiang Wang. 2025b. Sweexp: Experience-driven software issue resolution. arXiv preprint arXiv:2507.23361. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024. Self-play finetuning converts weak language models to strong language models. Preprint, arXiv:2401.01335. Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. 2024. Exploring large language model based intelligent agents: Definitions, methods, and prospects. Preprint, arXiv:2401.03428. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2019. Babyai: platform to study the sample efficiency of grounded language learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, Zhaochun Ren, Nikos Aletras, Xi Wang, Han Zhou, and Zaiqiao Meng. 2025. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. Preprint, arXiv:2508.07407. Zhitao He, Zijun Liu, Peng Li, Yi R. Fung, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. 2025. Advancing language multi-agent learning with credit re-assignment for interactive environment generalization. Preprint, arXiv:2502.14496. Ukjo Hwang and Songnam Hong. 2025. Federated reinforcement learning in heterogeneous environments. Preprint, arXiv:2507.14487. Gautam Jajoo. 2025. Federated learning with heterogeneous llms: Integrating small student client models with large hungry model. In AAAI25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pages 2958129583. AAAI Press. Wenzheng Jiang, Ji Wang, Xiongtao Zhang, Weidong Bao, Cheston Tan, and Flint Xiaofeng Fan. 2025. Fedhpd: Heterogeneous federated reinforcement learning via policy distillation. Preprint, arXiv:2502.00870. Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2022. Federated reinforcement learning with environment heterogeneity. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 1837. PMLR. Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary 9 Charles, Graham Cormode, Rachel Cummings, Rafael G. L. DOliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri`a Gascon, Badih Ghazi, Phillip B. Gibbons, and 40 others. 2021. Advances and open problems in federated learning. Preprint, arXiv:1912.04977. Jabin Koo, Minwoo Jang, and Jungseul Ok. 2025. Towards robust and efficient federated low-rank adaptation with heterogeneous clients. Preprint, arXiv:2410.22815. Chuan Li, Qianyi Zhao, Fengran Mo, and Cen Chen. 2025a. Fedcot: Communication-efficient federated reasoning enhancement for large language models. Preprint, arXiv:2508.10020. Han Li, Yuling Shi, Shaoxin Lin, Xiaodong Gu, Heng Lian, Xin Wang, Yantao Jia, Tao Huang, and Qianxiang Wang. 2025b. Swe-debate: Competitive multi-agent debate for software issue resolution. arXiv preprint arXiv:2507.23348. Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. 2021. Federated learning on non-iid data silos: An experimental study. Preprint, arXiv:2102.02079. Wenyun Li, Wenjie Huang, and Chen Sun. 2025c. Shaping sparse rewards in reinforcement learning: semi-supervised approach. Preprint, arXiv:2501.19128. Jiaqi Liu, Kaiwen Xiong, Peng Xia, Yiyang Zhou, Haonian Ji, Lu Feng, Siwei Han, Mingyu Ding, and Huaxiu Yao. 2025. Agent0-vl: Exploring selfevolving agent for tool-integrated vision-language reasoning. Preprint, arXiv:2511.19900. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Selfrefine: Iterative refinement with self-feedback. Preprint, arXiv:2303.17651. Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. 2023. Clin: continually learning language agent for rapid task adaptation and generalization. Preprint, arXiv:2310.10134. H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera Arcas. 2023. Communication-efficient learning of deep networks from decentralized data. Preprint, arXiv:1602.05629. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, and 262 others. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Weihan Peng, Yuling Shi, Yuhang Wang, Xinyun Zhang, Beijun Shen, and Xiaodong Gu. 2025. Swe-qa: Can language models answer repositoryarXiv preprint level arXiv:2509.14635. code questions? Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2024. ADaPT: Asneeded decomposition and planning with language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 42264252, Mexico City, Mexico. Association for Computational Linguistics. Jiaxing QI, Zhongzhi Luan, Shaohan Huang, Carol Fung, Hailong Yang, and Depei Qian. 2024. Fdlora: Personalized federated learning of large language model via dual lora tuning. Preprint, arXiv:2406.07925. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, and Yuxiao Dong. 2025. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. Preprint, arXiv:2411.02337. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can 2023. teach themselves Preprint, to use tools. arXiv:2302.04761. Yuling Shi, Songsong Wang, Chengcheng Wan, Min Wang, and Xiaodong Gu. 2024. From code to correctness: Closing the last mile of code generation with hierarchical debugging. arXiv preprint arXiv:2410.01215. Zitong Shi, Guancheng Wan, Wenke Huang, Guibin Zhang, Jiawei Shao, Mang Ye, and Carl Yang. 2025. Privacy-enhancing paradigms within federated multi-agent systems. Preprint, arXiv:2503.08175. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Relanguage agents with verbal reinforceflexion: ment learning. In Advances in Neural Information Processing Systems, volume 36, pages 86348652. Curran Associates, Inc. Z.K. Silagadze. 2023. On arxiv moderation system. Journal of Informetrics, 17(3):101433. 10 Raghav Singhal, Kaustubh Ponkshe, and Praneeth Vepakomma. 2025a. Fedex-lora: Exact aggregation for federated and efficient fine-tuning of foundation models. Preprint, arXiv:2410.09432. Raghav Singhal, Kaustubh Ponkshe, and Praneeth Vepakomma. 2025b. Fedex-lora: Exact aggregation for federated and efficient fine-tuning of foundation models. Preprint, arXiv:2410.09432. Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding. 2024. Improving lora in privacy-preserving federated learning. Preprint, arXiv:2403.12313. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. 2024. survey on self-evolution of large language models. Preprint, arXiv:2404.14387. Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, and Jing Gao. 2024a. Fedbiot: LLM local fine-tuning in federated learning without full model. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024, pages 33453355. ACM. Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Lu Su, and Jing Gao. 2024b. Towards federated rlhf with aggregated client preference for llms. arXiv preprint arXiv:2407.03038. Panlong Wu, Kangshuo Li, Junbao Nan, and Fangxin Wang. 2024c. Federated in-context llm agent learning. Preprint, arXiv:2412.08054. Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, and 1 others. 2024. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151. Chen Yang, Chenyang Zhao, Quanquan Gu, and Dongruo Zhou. 2024. Cops: Empowering llm agents with provable cross-task experience sharing. Preprint, arXiv:2410.16670. Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine learning: Concept and applications. Preprint, arXiv:1902.04885. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable real-world web interaction with grounded language agents. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing reasoning and acting in language models. Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. 2024. Agent lumos: Unified and modular training for open-source language agents. Preprint, arXiv:2311.05657. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2025. Self-rewarding language models. Preprint, arXiv:2401.10020. Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, and Mohit Bansal. 2024. Envgen: Generating and adapting environments via llms for training embodied agents. Preprint, arXiv:2403.12014. Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Yufan Zhou, Guoyin Wang, and Yiran Chen. 2024a. Towards building the federated gpt: Federated instruction tuning. Preprint, arXiv:2305.05644. Xinyu Zhang, Weiyu Sun, and Ying Chen. 2024b. Tackling the non-iid issue in heterogeneous federated learning by gradient harmonization. Preprint, arXiv:2309.06692. Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. 2023. FedPETuning: When federated learning meets the parameter-efficient tuning methods of pretrained language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 99639977, Toronto, Canada. Association for Computational Linguistics. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024. Expel: Llm agents are experiential learners. Preprint, arXiv:2308.10144. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, and 35 others. 2023. RT-2: vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 21652183. PMLR."
        },
        {
            "title": "Objective",
            "content": "This section provides the detailed derivation of the surrogate objective function used in the Local Agent Self-Evolution phase. The goal is to demonstrate that maximizing the expected return under sparse binary rewards can be theoretically approximated by performing Maximum Likelihood Estimation (MLE) on filtered 11 objective transforms into maximizing the loglikelihood of successful trajectories discovered by the reference policy. successful trajectories.Let πΘ,ϕ denote the current policy parameterized by the frozen base model Θ and the trainable adapter ϕ. Let πold represent the reference policy (the policy at the beginning of the current optimization step). The standard Reinforcement Learning objective is to maximize the expected return: J(ϕ) = Eτ πΘ,ϕ[R(τ )] = (cid:90) p(τ Θ, ϕ)R(τ )dτ (10) Since sampling directly from the evolving policy πΘ,ϕ is computationally expensive, Importance Sampling (IS) is employed to estimate the expectation using trajectories generated by the reference policy πold: J(ϕ) = Eτ πold (cid:20) πΘ,ϕ(τ ) πold(τ ) (cid:21) R(τ ) (11) To construct tractable lower bound, the inequality 1 + log (derived from the concavity of the logarithm function log 1) is applied. Let = πΘ,ϕ(τ ) πold(τ ) . Substituting this into Equation (2) yields: J(ϕ) Eτ πold (cid:20)(cid:18) 1 + log πΘ,ϕ(τ ) πold(τ ) (cid:19) (cid:21) R(τ ) = Eτ πold[R(τ )] + Eτ πold [R(τ ) log πΘ,ϕ(τ )] Eτ πold [R(τ ) log πold(τ )] (12) In this inequality, the first term Eτ πold[R(τ )] and the third term Eτ πold[R(τ ) log πold(τ )] depend solely on the fixed reference policy πold and the reward function. With respect to the optimization variable ϕ, these terms are constants. Thus, maximizing the lower bound of J(ϕ) is equivalent to maximizing: Jsurrogate(ϕ) = Eτ πold[R(τ ) log πΘ,ϕ(τ )] (13) Under the specific setting of sparse binary rewards, where R(τ ) {0, 1}, the term R(τ ) log πΘ,ϕ(τ ) is non-zero only when R(τ ) = 1. Consequently, the expectation over the entire distribution πold can be reduced to an expectation over the distribution of successful trajectories D+ = {τ τ πold, R(τ ) = 1}. The optimization problem simplifies to: max ϕ Eτ D+[log πΘ,ϕ(τ )] (14) This derivation confirms that under the assumption of binary rewards and using the logarithmic lower bound approximation, the RL"
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "UTHealth Houston",
        "Zhejiang University"
    ]
}