{
    "paper_title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
    "authors": [
        "Sai Kumar Dwivedi",
        "Dimitrije Antić",
        "Shashank Tripathi",
        "Omid Taheri",
        "Cordelia Schmid",
        "Michael J. Black",
        "Dimitrios Tzionas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce a novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multi-view rendering, (2) trains a novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose a new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enabling richer interaction modeling. InteractVLM outperforms existing work on contact estimation and also facilitates 3D reconstruction from an in-the wild image. Code and models are available at https://interactvlm.is.tue.mpg.de."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 3 0 3 5 0 . 4 0 5 2 : r InteractVLM: 3D Interaction Reasoning from 2D Foundational Models Sai Kumar Dwivedi1 Dimitrije Antic2 Shashank Tripathi1 Omid Taheri1 Cordelia Schmid3 Michael J. Black1 Dimitrios Tzionas2 1Max Planck Institute for Intelligent Systems, Tubingen, Germany 2University of Amsterdam (UvA), the Netherlands 3Inria, Ecole normale superieure, CNRS, PSL Research University, France Figure 1. We present InteractVLM, novel method for estimating contact points on both human bodies and objects from single in-thewild image, shown here as red patches. Our method goes beyond traditional binary contact estimation methods by estimating contact points on human in relation to specified object. We do so by leveraging the broad visual knowledge of large Visual Language Model."
        },
        {
            "title": "Abstract",
            "content": "We introduce InteractVLM, novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multiview rendering, (2) trains novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enInteractVLM outperabling richer interaction modeling. forms existing work on contact estimation and also facilitates 3D reconstruction from an in-the-wild image. To estimate 3D human and object pose, we infer initial body and object meshes, then infer contacts on both of these via InteractVLM, and last exploit these for fitting the meshes to image evidence. Results show that our approach performs promisingly in the wild. Code and models are available at https://interactvlm.is.tue.mpg.de. 1. Introduction People interact with objects routinely. Reconstructing human-object interaction (HOI) in 3D is key for many applications, from robots to mixed reality. However, doing so from single images is challenging due to depth ambiguity, occlusions, and the diverse shape and appearance of objects. There are methods that estimate 3D human bodies and methods that estimate 3D objects, but few that put these together. Knowing the contacts between these could significantly improve joint reconstruction. Our goal is to infer contact points on both humans and objects from single inthe-wild images, and then use these contacts for jointly reconstructing humans and objects. However, there is lack of in-the-wild training images paired with ground-truth contact labels for both 3D humans and objects. Acquiring such data is challenging and existing methods do not scale. 1 The problem gets more challenging due to the complexity of real-world interactions. Humans often contact multiple objects simultaneously; e.g., using laptop while sitting on club chair. Yet, current approaches treat contact prediction as simple binary classification; i.e. detecting whether body part is in contact with any object. This simplified assumption fails to capture the rich semantic relationships of multi-object interactions. To address this, we introduce novel Semantic Human Contact estimation task. This involves predicting the contact points on the body related to particular object given single in-the-wild image. To tackle this, as well as to overcome data scarcity, we propose new paradigm for scaling and reasoning in the wild. Specifically, we observe that large Vision-Language Models (VLMs) can reason about in-the-wild images because they are trained on internet-scale data and possess broad visual knowledge about humans and their interactions with the world. We also observe that this knowledge can be re-purposed for novel tasks by fine-tuning these large models on small datasets. Thus, we exploit VLMs for developing novel framework, called InteractVLM. At the heart of InteractVLM lies reasoning module based on VLM; see Fig. 2. We enrich this with skills for 3D human-object understanding, by extending the VLM with LoRA [28] adaptation layer. As result, given color image, this module can be asked to produce reasoning tokens that facilitate 3D contact localization. However, exploiting these tokens to localize contact is non-trivial. natural choice would be to employ foundational localization model [33] that takes these tokens as guidance, and highlights the 3D contacts. But there exists key practical problem. Existing foundation models inherently operate only in 2D space, while we need them to do so in 3D. To tackle this, we need to re-cast our problem so it is appropriate for 2D foundation models. To this end, we develop novel Render-Localize-Lift (RLL) framework that has three main steps; see Fig. 2: (1) We render the 3D shape of canonical SMPL+H [55] body and an object as 2D images from multiple viewpoints. Regarding the object, 3D mesh is efficiently retrieved from large-scale 3D database [13] through OpenShape [44]. (2) We pass the above images into foundation model to predict 2D contact masks for both the body and the object. (3) We lift the predicted 2D contact points to 3D points via back-projection, i.e., performing the inverse of the first step. However, even after recasting the problem to 2D by rendering multi-view images, existing foundation models are still not 3D aware, i.e. they treat each view independently, ignoring multi-view consistency. This means contact detections in one view do not necessarily agree with ones in adjacent views. To tackle this, just appending camera parameters to multi-view renderings is insufficient. Instead, we build novel Multi-view Localization model that we call Figure 2. Overview of InteractVLM. Given color image, our VLM performs the core reasoning, and guides novel MV-Loc model to localize contacts on both bodies and objects in 3D. Here we show only the body; for details, and object contact, see Fig. 3. MV-Loc. This has two steps: (1) It transforms the reasoning token provided by the VLM with the camera parameters used to render the multi-view images. (2) It ensures multi-view consistency, by lifting the inferred 2D contacts in each view to 3D and computing 3D loss. Our method, InteractVLM, utilizes VLM in tandem with our novel multi-view localization model, MV-Loc, to perform 3D contact prediction for humans and objects. See Fig. 1 for some examples. We quantitatively evaluate the efficacy of our method for in-the-wild 3D contact prediction on bodies and on objects, using the DAMON [60] and PIAD [67] datasets, respectively. For bodies, we evaluate both for the traditional binary contact estimation, and for our new task of semantic contact estimation. For all tasks, we find that InteractVLM outperforms the prior work. Finally, we demonstrate how InteractVLMs estimated contact improves 3D HOI recovery from in-the-wild images; this is highly ill-posed task due to depth ambiguities and occlusions. To address this, we develop an optimization-based method that fits SMPL-X body mesh and an OpenShape-retrieved object mesh to the image, using InteractVLMs inferred contacts as constraints to anchor human and object meshes w.r.t. each other. To the best of our knowledge, this is the first approach for estimating 3D HOI for in-the-wild images using inferred contacts. In summary, we make four key contributions: 1. We build InteractVLM, novel method that facilitates HOI reconstruction from an in-the-wild image by detecting 3D contacts on both bodies and objects. 2. We demonstrate way to minimize reliance on 3D contact annotations via exploiting the broad visual knowledge of Vision-Language Models. 3. We build novel Multi-view Localization model that helps in estimating contacts in 3D by transforming the reasoning of foundation models from 2D to 3D. 4. We introduce the novel Semantic Human Contact task for inferring body contacts conditioned on object labels. Our code and trained models are available for research at https://interactvlm.is.tue.mpg.de. 2 2. Related Work 2.3. 3D Human-Object Interaction 2.1. Large Vision-Language Models Recent advancements in large language models (LLMs) have led to the development of multimodal models that integrate vision and language reasoning. Models like Flamingo [2] and BLIP-2 [38] use cross-attention mechanisms and visual encoders to align image features with text, supporting variety of vision-language tasks. More recent works, such as VisionLLM [62] and Kosmos-2 [52], use grounded image-text data to enhance spatial understanding, while GPT4RoI [70] introduces spatial box inputs for finer alignment. However, these models typically lack endto-end segmentation capabilities. To address this limitation, LISA [36] combines vision foundation segmentation models like SAM [33] with multimodal embeddings, enabling language-guided segmentation. PARIS3D [15] extends this approach to referential 3D segmentation by processing multi-view object renders through both SAM and LLaVA [43] for spatially-aware segmentation. Taking inspiration from these approaches, we exploit language-guided segmentation model for the task of predicting human and object contact in 3D. However, unlike PARIS3D, we process single RGB image with LLaVA and multi-view renders of the human mesh and the object mesh with SAM. Moreover, we introduce feature-lifting technique that extends LLaVAs 2D features into 3D using camera parameters, thus guiding SAMs multi-view segmentations. This approach ensures multi-view consistency and efficiently predicts contact affordances, extending the use of multimodal models in 3D reasoning tasks focused on human-object interaction. 2.2. 3D Human and Object from Single Images Estimating 3D human pose and shape from single image has evolved from optimization-based methods to learningbased approaches. Optimization-based methods fit parametric body models like SMPL [47], SMPL-X [51], or GHUM [66] to 2D cues such as keypoints [9], silhouettes [50], or segmentation masks [50]. Learning-based methods either regress body parameters from images or videos [5, 16, 17, 31, 34, 35, 39] or estimate non-parametric bodies as vertices [37, 42], implicit surfaces [48, 56], or dense points [58]. Transformer-based methods [18, 23, 41, 57] have further improved robustness. For 3D object reconstruction from single image, regression-based methods predict geometry using meshes, voxels, or point clouds. Diffusion-based models [27] utilize large 3D datasets like Objaverse [13] or 2D diffusion models [40, 45, 46, 53] to guide reconstruction and optimization methods [4] use render and compare. Retrieval-based methods, such as OpenShape [44] and Uni3D [71], have demonstrated some robustness in cases with occlusions. Understanding 3D human-object interactions is essential for modeling realistic scenes. Early works focused on handobject interactions, such as ObMan [26] and FPHA [22], with more recent studies like ARCTIC [19] and HOLD [20] providing more detailed data and reconstruction for hands. For full-body interactions, initial studies involve interactions with scenes, as in PROX [24], and with objects, as in BEHAVE [7], GRAB [59], and InterCap [30]. BEHAVE and GRAB use motion capture that are accurate but nonscalable, while InterCap uses multi-camera setups that are more scalable but less accurate. Both of these approaches are limited in capturing diverse and realistic interactions. As proxy for 3D reconstruction, recent methods like DECO [60] and HOT [11] infer contact on body meshes and image pixels, respectively, for which contact annotations are crowdsourced. Predicting contact on objects is more challenging due to varying object shapes, while also currently there exists no dataset of object contact for in-thewild images. Thus, we estimate 3D object affordances as proxy for contact. 3D-AffordanceNet [14] introduces affordances that are not grounded in images, capturing the likelihood of humans interacting with specific parts of an object for given affordance (e.g., sit on chair). PIAD [67] curates RGB images depicting object affordances and trains network to estimate them. LEMON [68] extends object affordance prediction of PIAD to include human contact estimation. However, these methods require human contact vertices paired with object affordances for training, limiting the number of categories they handle to 21 ones. In contrast, we learn from unpaired human and object interaction data enabling human interaction reasoning for 80 categories and object affordance prediction for 32 categories. 2.4. Joint 3D Human-Object Reconstruction Joint reconstruction of humans and objects in 3D has been tackled with both regression and optimization methods. Regression-based methods directly predict 3D humanobject meshes, as in HDM [64] and CONTHO [49], while other methods first predict contact points and then use testtime optimization to fit human and object meshes, as in CHORE [63] and PHOSA [69]. Since regression methods rely on limited training data, optimization methods are preferred for in-the-wild scenarios, such as PHOSA [69]. Optimization-based methods either assume known contacts or infer contacts to fit meshes to the image, but their success heavily relies on the quality of contact. Our method improves upon these by providing more accurate contact predictions, which in turn facilitate better fitting of human and object meshes to image evidence, improving the realism and accuracy of 3D human-object reconstructions from single images. 3 Figure 3. Method overview. Given single in-the-wild color image, our novel InteractVLM method estimates 3D contact points on both humans and objects (a). Then, we reconstruct 3D human and object in interaction by exploiting these contacts (b). More specifically: (a) Contact estimation. Given an image, I, and prompt text, Tinp, our VLM, Ψ, produces contact tokens for humans and objects, <HCON> and <OCON>, which are projected (Γ) into feature embeddings, EH and EO. These guide Multi-View [contact] Localization model. This renders the 3D human and object geometry via cameras, K, into multi-view 2D renders and passes these to encoder, Θ, while decoders, ΩH, ΩO, estimate and highlight 2D contacts in these renders. Then, the FeatLift module, Φ, transforms the VLMs features (EH, EO) to become 3D-aware (EH 3D) by exploiting the camera parameters, K. final module lifts the detected 2D contacts to 3D. (b) 3D HOI reconstruction. For joint human-object reconstruction, we use InteractVLMs inferred contacts in an optimization framework. 3D, EO 3. Method 3.1. Input Representation Given an image, RHW3, InteractVLM estimates 3D contacts for both human bodies and objects. The human is represented by SMPL+H [55] 3D body mesh, H, with vertices R104753. The body is posed in canonical star shape (see details in Sec. 3.4). The human contacts are binary per-vertex labels, CH {0, 1}. The object is represented by 3D point cloud (or mesh), RN3, with points. As there are no datasets of natural images paired with 3D contacts for objects, we use largescale 3D affordance dataset [67], instead, as affordances are closely related to contact. Specifically, they represent the likelihood of contact on 3D object areas for various purposes. Therefore, for objects we use the terms affordance and contact interchangeably. The object contacts are continuous per-point values, CO [0, 1]. During inference, we retrieve 3D object shape from large database [13] via OpenShape [44] conditioned on image I. 3.2. Overview of InteractVLM The biggest challenge for learning 3D contact prediction in the wild is the limited 3D contact data for humans and objects. To go beyond existing limited datasets, we introduce novel method, called InteractVLM, that harnesses the commonsense knowledge of large vision-language models. Specifically, InteractVLM  (Fig. 3)  has two main components: Vision Language Model (VLM) and novel MultiView contact Localization model (MV-Loc). MV-Loc highlights parts that are in contact for both humans and objects with the VLMs guidance. The input to the VLM (Sec. 3.3) is an image, I, and prompt, Tinp, that asks the VLM to detect contact. The input to MV-Loc (Sec. 3.4) is the 3D geometry of humans and objects, and O, respectively. 3.3. Interaction Reasoning through VLM The VLM module, Ψ, conducts the core interaction reasoning. It takes an input image, I, and prompt text, Tinp, and outputs prompt text, Tout = Ψ(I, Tinp). Inspired by the recent LISA [36] model, we expand the VLMs vocabulary with two specialized tokens, <HCON> and <OCON>, for contact information for humans and objects, respectively. To denote contact, Ψ produces prompt that includes the above tokens. To aid MV-Loc in localizing contact, we extract the last-layer embeddings of the VLM corresponding to these tokens and pass them through projection layer, Γ, to obtain the feature embeddings, EH or EO. Let Tgt be the ground-truth text, and Tpred be the predicted one. Then, our token-prediction loss is defined as cross-entropy loss: Ltoken = (cid:80)N i=1(T (i) gt log(T (i) pred)). (1) 3.4. Interaction Localization through MV-Loc We develop novel MV-Loc module that has shared image encoder, Θ, and separate decoders, ΩH and ΩO, for humans and objects. MV-Loc performs contact localization by using novel Render-Localize-Lift (RLL) framework. To this end, it takes three steps; (1) rendering the 3D shape of humans and objects in 2D, (2) predicting 2D contact maps for both of these, and (3) lifting the 2D contact maps to 3D. RLL step #1: Render 3D2D. The input is human and object geometry, namely and O, respectively. Both serve as canvas for painting detected contacts on these. The body has default SMPL+H shape in canonical star-shape pose to minimize self-occlusions when rendering. The object geometry (initialized in Sec. 3.1) is normalized to unit sphere. Each geometry is rendered from fixed views with camera parameters, K, to form multi-view renderings, RH,O = {R j}J j=1, so that the entire 3D geometry is captured. Since our geometries do not have texture, we color the meshes with normals and point clouds using the NOCS map [61]. This enhances cross-view correspondence, making renderings resemble real images to image encoder, Θ. RLL step #2: Localize in 2D. The rendered geometry, RH,O, is first sent to the image encoder, Θ, and then passed to decoders, so that the final contact masks, MH and MO, get highlighted on it. However, MV-Loc requires spatial and contextual cues for highlighting the contact region. To this end, we use the feature embeddings (Sec. 3.3), i.e. EH and EO to guide the contact localization. However, since the VLM reasons in 2D, these features are not 3D aware. This is problem, because MV-Loc needs 3D awareness to localize contact consistently across multiview renderings. Thus, we transform the features to lift them to 3D to better guide multi-view localization. In detail, we design lifting network, Φ, which takes camera parameters, K, and the 2D EH,O, and lifts the latter to 3D as EH,O = Φ(EH,O, K). Contact masks are defined as: 3D MH,O = ΩH,O(cid:0)Θ(RH,O), EH,O (2) (cid:1). 3D Below, the and superscripts are dropped for brevity. We calculate losses only on valid regions, i.e., the areas within the outline of rendered geometry; we denote this as M. To encourage overlap between the predicted masks, M, and the ground-truth ones, (cid:98)M, particularly in sparse contact regions, we use focal-weighted BCE loss and Dice loss: LBCE = α(1 pM)γ log(pM) (1 α)pγ 2 (cid:80) (cid:98)M + ϵ (cid:80) + (cid:80) (cid:98)M + ϵ LDice = 1 log(1 pM), (3) , (4) where pM is the predicted mask probability, α controls class balance, γ adjusts the focus on hard examples, and ϵ is residual term to prevent division by zero. 5 RLL step #3: Lift 2D3D. To lift the inferred 2D contact points to 3D points, we follow the inverse of step #1. Normally, 2D points backproject to 3D lines due to depth ambiguities. In our case the lines intersect with the known 3D geometry that produced the multi-view renders. So, 2D points are lifted to 3D points, and by extension 2D contact masks, MH,O, are lifted to 3D contact areas, CH,O. We use human contact loss, LH , that combines focal loss with sparsity regularization, to encourage precise true-positive predictions in valid contact regions while discouraging false positives in non-contact areas: LH = α(1 phC)γ log(phC) + λCH1, (5) where phC is the contact probability, and λ, α and γ are scalar weights. We also use an object contact loss, LO , that combines Dice and Mean Squared Error (MSE) loss: LO = LDice(CO, (cid:98)CO) + βCO (cid:98)CO2 2, (6) where β is weighting factor, and (cid:98)CO denotes the groundtruth 3D contacts for objects. 3.5. Implementation Details Architecture. We use LLaVA [43] as our VLM and SAM [33] for our MV-Loc, with weights pre-trained by LISA [36] for segmentation [36]. The feature-lifting network, Φ, contains spatial-understanding network (two fully-connected layers of size 128 with ReLU activation) followed by viewspecific (256-dimensional) transformations and sigmoid activation. For 3D contact prediction, our MV-Loc model converts 2D masks to 3D contact points via 2D-to-3D pixelto-vertex mappings that are precomputed during MV-Locs rendering step. For details, see Sup. Mat. Training. To efficiently fine-tune our VLM, we employ LoRA [28] with rank 8. The separate decoders for human and object contact prediction are trained without LoRA, while keeping the image encoder frozen. For training, we use DeepSpeed [3] with mixed precision training (bfloat16), and batch size of 8. We train on 4 Nvidia-A100 GPUs for 30 epochs. For more details, please refer to Sup. Mat. Datasets. We focus on two tasks, i.e., 3D human contact and 3D object affordance prediction, using two inthe-wild datasets, i.e., DAMON [60] and PIAD [67], respectively. For human contact we train and evaluate on DAMON [60]. For 3D object affordances, we train and evaluate on PIAD [67]. We find that exploiting textual descriptions for the contacting body parts, and the object type in contact, helps training. Similarly, adding Visual Question-Answering (VQA) data generated by GPT4o for the training images also helps. For details, see Sup. Mat. Unlike LEMON [68], which requires paired humanobject geometry for training using the 3DIR dataset [68], we use unpaired data. This enables us to scale for many human and object categories not addressed by prior work. For Method F1 (%) Precision (%) Recall (%) Geodesic (cm) POSAPIXIE [25] [21] BSTRO [29] DECO [60] InteractVLM 31.0 46.0 55.0 75.6 42.0 51.0 65.0 75.2 34.0 53.0 57.0 76. 33.00 38.06 21.32 2.89 Methods PMF [72] ILN [10] PFusion [65] XMF [1] IAGNet [67] PIAD-Seen [67] PIAD-Unseen [67] SIM AUC aIOU MAE (%) (%) (%) SIM AUC aIOU MAE (%) (%) (%) 42.5 42.7 43.2 44.1 54.5 75.05 75.84 77.50 78.24 84.85 10.13 11.52 12.31 12.94 20.51 1.41 1.37 1.35 1.27 0. 0.81 33.0 32.5 33.0 34.2 35.2 41.4 60.25 59.69 61.87 62.58 71.84 75.45 4.67 4.71 5.33 5.68 7. 8.50 2.11 2.07 1.93 1.88 1.27 0.99 InteractVLM 62.7 86.47 21. Table 1. Evaluation for Binary Human Contact prediction on the DAMON dataset [60]. We compare our InteractVLM model (trained only for this task) with the state of the art. Semantic-DECO [60] (Baseline) InteractVLM Object Categories F1 (%) Prec. (%) Rec. (%) Geo (cm) F1 (%) Prec. (%) Rec. (%) Geo (cm) Accessory Daily Obj Food Furniture Kitchen Sports Transport 40.1 26.5 11.7 24.5 27.7 36.4 52.0 30.7 20.1 19.4 15.8 24.7 30.4 39.1 75.6 52.3 12.9 83.7 37.2 80.1 93.7 21.88 60.34 49.61 29.17 52.34 79.21 31.78 61.1 68.6 66.4 60.5 71.8 77.9 77. 72.9 71.4 66.1 64.2 71.3 76.7 80.5 65.6 78.0 77.9 60.1 81.1 83.2 78.9 6.91 7.46 7.17 6.21 7.61 7.98 7.97 Table 2. Evaluation for Semantic Human Contact prediction on the DAMON [60] dataset. For results on each class, see Sup. Mat. The Semantic-DECO baseline extends DECO for our new task. the final joint human-object reconstruction task, we combine DAMON [60], PIAD [67], 3DIR [68] and all textual descriptions about body parts, contact type and HOI. Evaluation metrics. For human contact prediction, following Tripathi et al. [60], we report the F1, precision, and recall scores using threshold of 0.5, and geodesic distance measuring spatial accuracy. For object contact prediction, following Yang et al. [67], we report the Similarity (SIM), Mean Absolute Error (MAE), Area Under ROC Curve (AUC), and average Intersection over Union (IOU). 4. Experiments 4.1. Binary Human Contact Estimation This task refers to estimating contact areas on the body via binary classification of its vertices, ignoring the number or type of objects involved. We train and evaluate on the DAMON [60] dataset, and report results in Tab. 1. InteractVLM significantly outperforms all previous methods achieving 20.6% improvement in F1 score. Although, here, InteractVLM is trained on the same data as DECO, it goes beyond this by harnessing the commonsense knowledge of large foundation model. In Sup. Mat. we also evaluate on the 3DIR [68] dataset and compare with the LEMON method [68]. Even though LEMON uses paired human-object data, InteractVLM performs on-par with it despite training on human-only data. We also evaluate InteractVLMs performance for binary human contact prediction across different body parts; for more details please refer to Sup. Mat. Table 3. Evaluation for Object Affordance Prediction on the PIAD [67] dataset. We compare our InteractVLM model (trained only for this task) with the state of the art. 4.2. Semantic Human Contact Estimation In real-life interactions, multiple objects can be contacted by different body areas concurrently. Thus, we introduce novel task called Semantic Human Contact prediction. We evaluate on DAMON [60] and report results in Tab. 2; for finer-grained version of this table, see Sup. Mat. To establish baseline, we adapt the DECO model [60] that detects binary contacts and turn it into multi-class prediction model, called Semantic-DECO. Due to DAMONs limited training data, this has poor performance. Instead, as discussed in Sec. 4.1, InteractVLM learns effectively from this data, by also leveraging the commonsense of foundation models. Thus, it significantly outperforms SemanticDECO. Qualitative results reflect this finding; see Fig. 4. Our model captures detailed, accurate contact regions, whereas Semantic-DECO often highlights false-positive areas that differ from the actual contact regions. 4.3. Object Affordance Prediction We evaluate the performance of our InteractVLM model on predicting object affordances. This involves identifying regions on objects of possible contact for certain interaction intent, such as sitting on chair or moving it. We train and evaluate on the PIAD [67] dataset. We compare against SotA methods, and report results in Tab. 3. Note that we evaluate on object instances that are both seen during training (PIAD-Seen column) and unseen (PIAD-Unseen column). Our model demonstrates significant improvement over SotA methods in terms of similarity (SIM), area under the curve (AUC), and mean absolute error (MAE) for both seen and unseen objects. Thus, our method is effective not only for estimating human contact (Secs. 4.1 and 4.2), but also for object affordances. 4.4. Reliance on 3D Annotations To analyze InteractVLMs efficiency regarding 3D supervision, we train multiple versions of it with varying amounts of training data from the DAMON dataset, and report performance in Fig. 5. Remarkably, InteractVLM achieves an F1 score of 0.53 with just 1% of the data, nearly matching the 0.55 F1 score of DECO that uses 100% of the data. With 6 Figure 4. Semantic Human Contact estimation (Sec. 4.2). Given an image and an object label, InteractVLM infers body contacts for this object. InteractVLM outperforms Semantic-DECO [60] baseline. Objects are shown in green circles, and contacts as red patches. Ablations. For study on the influence of InteractVLM components, including mask resolution, MV-Loc variants, loss functions, training data, the influence of the VLM, and the effect of text prompts, please refer to Sup. Mat. 5. Joint Human-Object Reconstruction We demonstrate the usefulness of 3D contacts inferred by InteractVLM for reconstructing 3D human and object in interaction from single in-the-wild image, I. Initializing 3D body pose & shape, object shape. We use OSX [41] to estimate 3D SMPL-X body mesh, H, and OpenShape [44] to retrieve 3D object mesh, O, from the Objaverse [13] database that best matches the image. Initializing 3D object pose. We apply InteractVLM on image to predict 3D body and object contact vertices, CH, CO. Then, we solve for object pose, {RO, tO} by applying the ICP [6] algorithm to snap the 3D object contact points, CO, onto body ones, CH. To avoid false correspondences, the 3D normals of contact points must be compatible, i.e., should have similar angles but opposite directions. Optimizing 3D object pose. Given the above initialization, we optimize over object rotation, RO, translation, tO, and scale, sO, via render-and-compare by minimizing: = EM + λC EC, EM = IoU( (cid:98)M, M) + (cid:99)Mc Mc2, (cid:88) (cid:88) EC = 1 CHCO iH jO CH CO H O 2, (7) (8) (9) where EM is mask loss, EC is contact loss, IoU is intersection-over-union, (cid:98)M, are hypothesis and groundtruth masks, respectively, (cid:99)Mc, Mc are the corresponding 7 Figure 5. InteractVLMs reliance on 3D annotations. We evaluate performance for binary human contact (F1 score, Y-axis) for models trained on varying percentage of DAMON [60] training data (X-axis). The DECO baseline trains on 100% of DAMON. Instead, InteractVLM trains on varying (smaller) portion of this dataset. Yet, it achieves significantly higher performance, by leveraging the broad visual knowledge of foundation models. only 5% of the data, InteractVLM surpasses DECO, reaching an F1 score of 0.58. This performance gap widens as training data increases, ultimately achieving 0.75 F1 score with 100% of the data. This is compelling finding, highlighting InteractVLMs efficient use of 3D supervision by leveraging the rich visual understanding of foundation models. Note that the strong performance with limited training data has high practical value, because obtaining 3D annotations is expensive. Figure 6. 3D HOI reconstruction (Sec. 5). We build an optimization method that fits SMPL-X body and OpenShape-retrieved object to an in-the-wild image. We evaluate against the SotA method PHOSA [69]. Reconstruction is guided by InteractVLM-inferred contacts. mean mask pixels, CH, CO are the number of contact vertices on the human and object, H, are the number of , mesh vertices, are the i-th and j-th human and object vertices, while CO , CH indicate whether vertices are in contact or not. Given the image, we extract the object mask, M, via SAM [33], and depth map, D, via Depth Pro [8]. Intuitively, we use the EM loss to align the 3D object to the image, while in EC the predicted 3D contacts anchor the 3D object onto the body so they interact realistically. We perform iterative optimization via Adam [32]. OSX produces reasonable bodies, so we keep these fixed. The object is updated in each iteration we render depth, (cid:98)D, and masks, (cid:98)M, via PyTorch3D [54] differentiable renderer. Qualitative Results. We reconstruct 3D human-object interaction from an image. We show results in Fig. 6, and compare with PHOSA [69], the most related SotA method. InteractVLMs reconstructions look more realistic. Note that PHOSA uses handcrafted contacts on humans and bodies. Instead, InteractVLM infers 3D contacts on both human bodies and objects from the single in-the-wild image. These play crucial role for guiding 3D reconstruction under occlusions and depth ambiguities. Perceptual Study. There exists no in-the-wild dataset with 3D ground truth for HOI, so we conduct perceptual study via Amazon Mechanical Turk for evaluation. Specifically, we evaluate the realism of our reconstructions against ones of PHOSA. We randomly select 55 images for which PHOSA has handcrafted contact annotations. For each image, participants are shown (with random swapping) reconstructions generated by our method and by PHOSA, and are asked to select the one that best represents the image. Our reconstructions are preferred 62% of the time. 6. Conclusion We develop InteractVLM, novel method for estimating 3D contacts on both humans and objects from single natural image. InteractVLM has reduced reliance on expensive 3D contact annotations for training, by leveraging the broad knowledge of Vision-Language Models. Specifically, we introduce novel Render-Localize-Lift (RLL) framework and novel multi-view localization model (MV-Loc) to adapt 2D foundation models for 3D contact estimation. We outperform existing work on contact estimation and introduce new Semantic Human Contact estimation task for inferring body contacts conditioned on object labels. This goes beyond traditional binary contact estimation, which fails to capture rich semantic relationships of multi-object interactions. Last, we develop the first approach that uses inferred contact points on both bodies and objects for joint 3D reconstruction from single in-the-wild images. Acknowledgments: We thank Alpar Cseke for his assistance with evaluating joint human-object reconstruction. We also thank Tsvetelina Alexiadis and Taylor Obersat for MTurk evaluation, Yao Feng, Peter Kultis and Markos Diomataris for their valuable feedback and Benjamin Pellkofer for IT support. SKD is supported by the International Max Planck Research School for Intelligent Systems (IMPRS-IS). The UvA part of the team is supported by an ERC Starting Grant (STRIPES, 101165317, PI: D. Tzionas). Disclosure: DT has received research gift fund from Google. For MJB see https://files.is.tue.mpg.de/black/CoI CVPR 2025.txt"
        },
        {
            "title": "References",
            "content": "[1] Emanuele Aiello, Diego Valsesia, and Enrico Magli. Crossmodal learning for image-guided point cloud shape completion. Conference on Neural Information Processing Systems (NeurIPS), 35:3734937362, 2022. 6 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In Conference on Neural Information Processing Systems (NeurIPS), 2022. 3 [3] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, A. A. Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He. DeepSpeed inference: Enabling efficient inference of transformer models at unprecedented scale. International Conference for High Performance Computing, Networking, Storage and Analysis, 2022. 5 [4] Dimitrije Antic, Georgios Paschalidis, Shashank Tripathi, Theo Gevers, Sai Kumar Dwivedi, and Dimitrios Tzionas. SDFit: 3D object pose and shape by fitting morphable SDF to single image. arXiv:2409.16178, 2025. 3 [5] Fabien Baradel*, Matthieu Armando, Salma Galaaoui, Romain Bregier, Philippe Weinzaepfel, Gregory Rogez, and Thomas Lucas*. Multi-HMR: Multi-person whole-body human mesh recovery in single shot. In European Conference on Computer Vision (ECCV), 2024. 3 [6] Paul J. Besl and Neil D. McKay. method for registration of 3D shapes. Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 14:239256, 1992. [7] Bharat Lal Bhatnagar, Xianghui Xie, Ilya Petrov, Cristian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll. BEHAVE: Dataset and method for tracking human object In Computer Vision and Pattern Recognition interactions. (CVPR), 2022. 3 [8] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen Koltun. Depth Pro: Sharp monocular metric depth in less than second. arXiv:2410.02073, 2024. 8 [9] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from single image. In European Conference on Computer Vision (ECCV), 2016. 3 [10] Honghua Chen, Zeyong Wei, Yabin Xu, Mingqiang Wei, and Jun Wang. ImLoveNet: Misaligned image-supported registration network for low-overlap point cloud pairs. In International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH), pages 19, 2022. 6 [11] Yixin Chen, Sai Kumar Dwivedi, Michael J. Black, and Dimitrios Tzionas. Detecting human-object contact in images. In Computer Vision and Pattern Recognition (CVPR), 2023. 3 [12] Alpar Cseke, Shashank Tripathi, Sai Kumar Dwivedi, Arjun S. Lakshmipathy, Agniv Chatterjee, Michael J. Black, and Dimitrios Tzionas. PICO: Reconstructing 3D people in contact with objects. In Computer Vision and Pattern Recognition (CVPR), 2025. 15 [13] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3D objects. In Computer Vision and Pattern Recognition (CVPR), pages 1314213153, 2023. 2, 3, 4, [14] Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, and Kui Jia. 3D AffordanceNet: benchmark for visual object affordance understanding. In Computer Vision and Pattern Recognition (CVPR), 2021. 3 [15] John Doe, Paris3d: Jane Smith, and Alice Brown. Language-guided 3d segmentation with multimodal models. arXiv:2406.08394, 2024. 3 [16] Sai Kumar Dwivedi, Nikos Athanasiou, Muhammed Kocabas, and Michael J. Black. Learning to regress bodies from images using differentiable semantic rendering. In International Conference on Computer Vision (ICCV), 2021. 3 [17] Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, and Dimitrios Tzionas. POCO: 3D pose and shape estimation using confidence. In International Conference on 3D Vision (3DV), 2024. 3 [18] Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng, and Michael J. Black. TokenHMR: Advancing human mesh reIn Computer covery with tokenized pose representation. Vision and Pattern Recognition (CVPR), 2024. 3 [19] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J. Black, and Otmar Hilliges. ARCTIC: dataset for dexterous bimanual handobject manipulation. In Computer Vision and Pattern Recognition (CVPR), 2023. [20] Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen, Michael Black, and Otmar Hilliges. HOLD: Category-agnostic 3D reconstruction In Computer of interacting hands and objects from video. Vision and Pattern Recognition (CVPR), 2024. 3 [21] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. Collaborative regression of expressive bodies using moderation. In International Conference on 3D Vision (3DV), 2021. 6 9 [22] Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, and Tae-Kyun Kim. First-person hand action benchmark with RGB-D videos and 3D hand pose annotations. In Computer Vision and Pattern Recognition (CVPR), 2018. 3 [23] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4D: Reconstructing and tracking humans with transformers. In International Conference on Computer Vision (ICCV), 2023. 3 [24] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael J. Black. Resolving 3D human pose ambiguities with 3D scene constraints. In International Conference on Computer Vision (ICCV), pages 22822292, 2019. 3 [25] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael J. Black. Populating 3d scenes In Proceedings of by learning human-scene interaction. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1470814718, 2021. 6 [26] Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, and Cordelia Schmid. Learning joint reconstruction of hands and manipulated objects. In Computer Vision and Pattern Recognition (CVPR), 2019. [27] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to 3D. arXiv: 2311.04400, 2023. 3 [28] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. International Conference on Learning Representations (ICLR), 2021. 2, 5 [29] Chun-Hao Huang, Hongwei Yi, Markus Hoschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael J. Black. Capturing and inferring In Computer Vision dense full-body human-scene contact. and Pattern Recognition (CVPR), pages 1326413275, 2022. 6, 12, 13 [30] Yinghao Huang, Omid Taheri, Michael J. Black, and Dimitrios Tzionas. InterCap: Joint markerless 3D tracking of humans and objects in interaction from multi-view RGB-D International Journal of Computer Vision (IJCV), images. 132(7):25512566, 2024. 3 [31] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exemplar fine-tuning for 3D human pose fitting towards in-thewild 3D human pose estimation. In International Conference on 3D Vision (3DV), pages 4252, 2021. 3 [32] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. 8 [33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and In International Ross B. Girshick. Segment anything. Conference on Computer Vision (ICCV), pages 39924003, 2023. 2, 3, 5, 8, [34] Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black. VIBE: Video inference for human body pose and shape estimation. In Computer Vision and Pattern Recognition (CVPR), pages 52525262, 2020. 3 [35] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, and Michael J. Black. PARE: Part attention regressor for 3D human body estimation. In International Conference on Computer Vision (ICCV), pages 1112711137, 2021. 3 [36] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. LISA: Reasoning segmentation via large language model. In Computer Vision and Pattern Recognition (CVPR), pages 95799589, 2024. 3, 4, 5 [37] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. HybrIK: hybrid analytical-neural inverse kinematics solution for 3D human pose and shape estimation. In Computer Vision and Pattern Recognition (CVPR), pages 33833393, 2021. 3 [38] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning (ICML), pages 1973019742, 2023. 3 [39] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan. CLIFF: Carrying location information in full frames into human pose and shape estimation. In European Conference on Computer Vision (ECCV), 2022. 3 [40] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D content creation. Computer Vision and Pattern Recognition (CVPR), 2023. 3 [41] Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li. One-stage 3D whole-body mesh recovery with component aware transformer. In Computer Vision and Pattern Recognition (CVPR), 2023. 3, [42] Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh In International Conference on Computer Vigraphormer. sion (ICCV), 2021. 3 [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Conference on Neural Information Processing Systems (NeurIPS), 2023. 3, 5, 13 [44] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao Su. OpenShape: Scaling up 3D shape representation towards open-world understanding. In Conference on Neural Information Processing Systems (NeurIPS), 2023. 2, 3, 4, 7 [45] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3D mesh in 45 seconds without perIn Conference on Neural Information shape optimization. Processing Systems (NeurIPS), 2023. 3 [46] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3D object. International Conference on Computer Vision (ICCV), 2023. [47] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multiperson linear model. Transactions on Graphics (TOG), 34 (6):248:1248:16, 2015. 3 10 [48] Marko Mihajlovic, Shunsuke Saito, Aayush Bansal, Michael Zollhoefer, and Siyu Tang. COAP: Compositional articulated occupancy of people. In Computer Vision and Pattern Recognition (CVPR), 2022. 3 [49] Hyeongjin Nam, Daniel Sungho Jung, Gyeongsik Moon, and Kyoung Mu Lee. Joint reconstruction of 3D human and object via contact-based refinement transformer. In Computer Vision and Pattern Recognition (CVPR), 2024. 3 [50] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter Gehler, and Bernt Schiele. Neural body fitting: Unifying deep learning and model based human pose and shape estimation. In International Conference on 3D Vision (3DV), 2018. 3 [51] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from single image. In Computer Vision and Pattern Recognition (CVPR), pages 1097510985, 2019. 3 [52] Bingbing Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv:2306.14824, 2023. [53] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One image to high-quality 3D object generation using both 2D and 3D diffusion priors. In International Conference on Learning Representations (ICLR), 2024. 3 [54] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3D deep learning with PyTorch3D. arXiv:2007.08501, 2020. 8 [55] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. Transactions on Graphics (TOG), 36(6), 2022. 2, 4, 14 [56] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. PIFuHD: Multi-level pixel-aligned implicit function for high-resolution 3D human digitization. In Computer Vision and Pattern Recognition (CVPR), 2020. 3 [57] Qingping Sun, Yanjun Wang, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiyi Mei, Chi Sing Leung, Ziwei Liu, Lei Yang, and Zhongang Cai. AiOS: All-in-one-stage expressive human pose and shape estimation. In Computer Vision and Pattern Recognition (CVPR), 2024. 3 [58] Istvan Sarandi and Gerard Pons-Moll. Neural localizer fields for continuous 3D human pose and shape estimation. In Conference on Neural Information Processing Systems (NeurIPS), 2024. [59] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dimitrios Tzionas. GRAB: dataset of whole-body human grasping of objects. In European Conference on Computer Vision (ECCV), 2020. 3 [60] Shashank Tripathi, Agniv Chatterjee, Jean-Claude Passy, Hongwei Yi, Dimitrios Tzionas, and Michael J. Black. DECO: Dense estimation of 3D human-scene contact in the wild. In International Conference on Computer Vision (ICCV), pages 80018013, 2023. 2, 3, 5, 6, 7, 12, 13, 14 [61] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J. Guibas. Normalized object coordinate space for category-level 6D object pose and size In Computer Vision and Pattern Recognition estimation. (CVPR), 2019. 5 [62] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. VisionLLM: Large language model is also an open-ended decoder for vision-centric tasks. In Conference on Neural Information Processing Systems (NeurIPS), 2023. 3 [63] Xianghui Xie, Bharat Lal Bhatnagar, and Gerard Pons-Moll. CHORE: Contact, human and object reconstruction from single RGB image. In European Conference on Computer Vision (ECCV), 2022. 3 [64] Xianghui Xie, Bharat Lal Bhatnagar, Jan Eric Lenssen, and Gerard Pons-Moll. Template free reconstruction of humanobject interaction with procedural interaction generation. In Computer Vision and Pattern Recognition (CVPR), 2024. 3 [65] Danfei Xu, Dragomir Anguelov, and Ashesh Jain. PointFusion: Deep sensor fusion for 3D bounding box estimation. In International Conference on Computer Vision (ICCV), pages 244253, 2018. [66] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, Bill Freeman, Rahul Sukthankar, and Cristian Sminchisescu. GHUM & GHUML: Generative 3D human shape and articulated pose models. In Computer Vision and Pattern Recognition (CVPR), 2020. 3 [67] Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo Luo, and Zheng-Jun Zha. Grounding 3D object affordance from 2D interactions in images. In International Conference on Computer Vision (ICCV), pages 1087110881, 2023. 2, 3, 4, 5, 6, 12, 14, 16 [68] Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, and Zheng-Jun Zha. LEMON: Learning 3D human-object interaction relation from 2D images. In Computer Vision and Pattern Recognition (CVPR), 2024. 3, 5, 6, 12, 14 [69] Jason Y. Zhang, Sam Pepose, Hanbyul Joo, Deva Ramanan, Perceiving 3D Jitendra Malik, and Angjoo Kanazawa. human-object spatial arrangements from single image in In European Conference on Computer Vision the wild. (ECCV), 2020. 3, 8 [70] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. GPT4RoI: Instruction tuning large language model on region-ofinterest. arXiv:2307.03601, 2023. 3 [71] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uni3D: Exploring unified 3D representation at scale. In International Conference on Learning Representations (ICLR), 2024. [72] Zhuangwei Zhuang, Rong Li, Kui Jia, Qicheng Wang, Yuanqing Li, and Mingkui Tan. Perception-aware multi-sensor fusion for 3D lidar semantic segmentation. In International Conference on Computer Vision (ICCV), 2021. 6 11 InteractVLM: 3D Interaction Reasoning from 2D Foundational Models Supplementary Material S.1. Human Contact Prediction S.1.1. Evaluation on the 3DIR Dataset We evaluate our method against several state-of-the-art approaches for human contact prediction on the 3DIR dataset [67] as shown in Tab. S.1. Our method outperforms methods that are trained on 3D training data for only humans, while it is on par with methods that use 3D data for both humans and objects. Moreover, by eliminating the requirement for paired human-object contact training data, our method can be trained on more categories than prior work, as unpaired datasets are more varied. This makes our method more practical for real-world applications. S.1.2. Contact Estimation Across Body Parts We extend our binary contact estimations evaluation to measure our methods performance across different human body parts to ensure it captures nuanced interactions effectively. As shown in Tab. S.2, our method (InteractVLM) significantly outperforms DECO [60] across all body parts, including the head, torso, hands, arms, feet, and legs. The results demonstrate our method excels at detecting contacts across diverse body parts, making it well-suited for realworld scenarios. S.1.3. Semantic Human Contact per Object Class We evaluate our methods performance on semantic human contact prediction across diverse set of object categories from the DAMON dataset, as shown in Tab. S.3. Results for high-level categories are presented in the main paper. We compare our method against Semantic-DECO, which is our extension of the existing DECO [60] model for this new task. Our method significantly outperforms SemanticIt also DECO in terms of F1-score for all categories. demonstrates strong performance across wide range of object categories, from large objects like furniture (couch: 62.1% F1, chair: 70.3% F1) to small objects for sports (baseball glove: 93.6% F1, tennis racket: 82.3% F1). S.2. Ablation Studies We conduct extensive ablation studies to evaluate the contribution of InteractVLMs main components, including the influence of the VLM and prompt design choices. The results are summarized in Tab. S.4, where we use alphabet numbering to refer to each variant for clarity. Below, we discuss the key findings from these experiments. Mask Resolution and MV-Loc Components. Increasing the mask resolution from 512 512 (variant (a)) to Method 3D Supervision Obj. Human F1 (%) Prec. (%) Rec. (%) Geo. (cm) BSTRO [29] DECO [60] LEMON-P [68] LEMON-D [68] InteractVLM 55.0 69.0 77.0 78.0 78.4 57.0 70.0 76.0 78.0 82.5 58.0 72.0 81.0 82.0 76. 28.58 15.25 9.02 7.55 6.73 Table S.1. Evaluation for Binary Human Contact prediction on the 3DIR dataset [68]. Note that LEMON is trained with paired Instead, for this human-object contact data from 3DIR dataset. task, InteractVLM is only trained with human contact data from the same dataset. Method Head Torso Hips Hands Arms DECO [60] Ours 20.0 56.0 46.1 87. 66.6 95.7 74.3 93.5 22.2 71.5 Feet 94.4 96.9 Legs 66.6 68.3 Table S.2. F1 scores for human contact estimation w.r.t. body parts 1024 1024 (variant (b)) yields significant improvement of 4.9% in F1 score, highlighting the importance of finegrained spatial information for contact detection. For the MV-Loc feature embedding, using our FeatLift network (variant (e)) outperforms simply concatenating camera parameters (variant (d)) by 3.7%, demonstrating its effectiveness in incorporating viewpoint information. Removing camera parameters entirely (variant (c)) further degrades performance, emphasizing their role in the pipeline. However, the performance significantly drops when we replace MV-Loc with 2 layer MLP (variant (f)). Loss Functions. Using only the valid mask regions for training (variant (h)) improves performance by 3.3% compared to using the whole mask (variant (g)). The addition of our 3D contact loss (variant (i)) further boosts the F1 score by 3%, underscoring the importance of explicitly modeling 3D contact cues during training. Data and VLM Influence. The choice of training data significantly impacts performance. Using only 3D contact datasets (variant (j)) results in relatively low F1 score of 65.9%. Adding contact parts in text form (variant (k)) improves performance by 8.9%, while further incorporating HOI-VQA data (variant (l)) achieves the best results. This demonstrates the value of leveraging textual and contextual cues for contact localization. The VLM plays critical role in the pipeline. Removing the VLM entirely (variant (m)) drastically reduces performance, while using VLM with only image input (variant (n)) serves as strong baseline. Fine-tuning the VLM (variant (b)) is crucial, as the non-fine-tuned version (variant (o)) shows significant drop in performance. Interestingly, 12 Object # Semantic-DECO [60] InteractVLM (Ours) Categories Skateboard Surfboard Snowboard T. Racket Cell phone Couch Bicycle Chair Bench Motorcycle Book Skis Bed Laptop Backpack Umbrella Knife Frisbee D. Table B. Glove Remote Banana Kite Toothbrush Boat Sports ball B. Bat Apple Handbag Tie Suitcase Wine glass Spoon Fork Keyboard Teddy bear Clock Cake Scissors Cup Car Pizza Carrot Truck Bottle Airplane Toilet Hot dog Donut Mouse Vase F. Hydrant 85 70 49 45 43 38 37 36 35 33 27 25 24 24 24 23 19 15 11 10 10 10 9 8 8 8 8 7 7 6 6 5 5 5 5 5 4 4 4 4 4 4 3 3 3 2 2 2 2 1 1 F1 (%) Prec. (%) Rec. (%) 30.3 23.1 38.2 57.0 42.4 31.4 62.1 23.2 19.0 60.4 48.0 36.5 29.1 36.9 37.2 51.5 63.3 33.9 19.6 71.4 0.2 6.1 65.3 2.9 33.5 36.0 36.7 6.3 12.1 39.8 26.7 5.5 61.1 1.5 3.2 17.5 23.3 0.0 0.2 7.2 0.0 19.4 0.0 0.0 0.0 0.0 0.0 7.0 19.6 0.0 0.0 0.0 19.3 14.2 25.7 42.0 27.8 19.7 48.0 14.6 11.2 45.5 33.8 25.0 19.1 24.9 24.3 36.1 54.0 22.0 14.1 63.3 1.0 7.1 51.8 4.7 23.9 34.1 60.8 17.4 7.0 28.1 24.0 8.4 48.5 1.6 6.2 15.7 14.8 0.0 0.2 11.2 0.0 19.0 0.0 0.0 0.0 0.0 0.0 23.0 30.7 0.0 0.0 0.0 91.3 98.4 92.2 99.6 99.6 89.2 98.1 87.1 92.1 99.1 99.7 93.4 82.9 94.4 87.2 99.2 84.4 99.4 67.1 81.9 0.1 6.4 95.9 2.1 83.7 39.4 27.2 3.9 46.2 87.2 30.7 5.0 89.9 1.3 3.1 45.0 58.1 0.0 0.2 5.4 0.0 35.1 0.0 0.0 0.0 0.0 0.0 4.1 14.8 0.0 0.0 0. Geo. (cm) 99.95 101.22 108.29 64.25 51.73 17.07 29.89 36.05 29.51 19.24 53.59 104.07 20.71 45.73 12.10 67.20 31.55 69.43 42.56 41.58 82.16 67.19 50.50 56.38 46.24 60.54 26.00 45.69 26.61 7.24 87.44 70.32 15.35 75.47 70.41 24.70 46.42 83.99 87.88 69.03 49.13 46.43 90.22 61.65 91.14 87.52 86.55 46.32 42.47 82.03 91.96 88.18 F1 (%) Prec. (%) Rec. (%) Geo. (cm) 71.5 79.7 84.2 82.3 70.6 62.1 81.5 70.3 63.0 76.6 74.1 83.0 54.0 54.0 59.2 82.3 77.0 68.7 35.2 93.6 70.6 76.6 85.4 77.3 71.3 64.4 82.8 69.3 31.8 49.6 79.2 66.4 67.5 64.9 60.8 43.8 37.1 52.4 28.7 68.6 66.7 44.3 59.7 81.2 59.2 76.4 32.5 81.3 73.6 40.7 68.5 85.5 67.0 76.3 83.1 80.8 73.1 62.5 84.4 73.6 70.7 78.6 75.2 81.4 56.7 54.0 71.1 83.7 74.9 71.5 44.9 98.6 77.4 74.3 86.0 82.6 75.3 74.0 81.2 62.9 27.1 32.8 65.9 68.5 62.8 66.2 69.1 61.6 68.9 41.9 21.4 71.4 67.7 44.1 62.4 84.9 55.1 69.3 35.7 84.0 90.1 27.0 59.3 82.7 83.5 78.9 84.0 86.3 74.3 60.5 81.9 68.8 64.4 77.7 80.1 83.7 48.8 68.6 54.8 86.4 86.6 84.5 63.4 89.1 82.7 81.7 85.4 74.8 63.1 83.8 87.8 77.7 40.4 60.8 83.4 69.4 78.5 76.5 74.0 68.8 75.0 82.2 73.1 76.2 73.3 71.4 77.6 77.5 81.2 85.2 71.1 78.9 65.6 82.9 81.0 88.5 0.90 0.80 0.20 0.20 7.00 2.10 2.50 1.60 4.00 0.90 1.10 0.80 2.70 4.70 3.50 1.00 0.10 1.00 6.60 0.10 0.50 2.80 0.30 5.40 1.40 5.30 1.60 4.20 4.10 7.60 0.80 4.40 5.50 2.20 0.50 11.60 3.30 10.60 40.10 1.70 5.30 29.20 0.20 3.10 0.10 3.60 3.30 4.10 12.00 0.10 0.20 0.00 Table S.3. Evaluation for Semantic Human Contact prediction on the DAMON [60] dataset for different object categories in the test set. The number of samples for each category is shown in the second column. Semantic-DECO is our extension of the existing DECO [60] model for this new task. Zero metrics indicate no correct predictions for the class. reducing the VLM size from 13B to 7B parameters (variant (p)) has minimal impact, suggesting that the model can maintain strong performance even with fewer parameters. Prompt Design. The design of the text prompt significantly influences the results. Using fine-grained contact parts (variant (q)) outperforms coarse segmentation (variant (r)) by 6.4% in F1 score, indicating that finer granularity in body part labeling is beneficial. Removing the object name from the prompt (variant (s)) also degrades accuracy, highlighting the importance of explicit object context Variants F1 (%) Prec. (%) Rec. (%) Masks (a) Size 512 512 (b) Size 1024 MV-Loc Losses Data VLM Prompt (c) No CamParams (d) Concat CamParams (e) FeatLift (Φ) (f) No MV-Loc (g) Whole Mask (h) Valid Mask (i) + 3D Contact Loss (j) 3D Contact Datasets (k) + Contact Parts (text) (l) + HOI-VQA (m) No VLM (n) VLM-13B-Img (o) VLM-13B-NoFT (p) VLM-7B (q) Contact parts (fine) (r) Contact parts (coarse) (s) No object name 70.7 75.6 69.4 71.9 75.6 62. 69.3 72.6 75.6 65.9 74.8 75.6 32.3 67.2 64.8 73.3 74.8 68.4 71.5 70.1 75.2 68.0 72.0 75.2 60. 68.7 71.2 75.2 64.8 74.5 75.2 30.8 68.5 65.3 76.8 74.5 69.0 72.1 71.4 76.0 71.1 71.8 76.0 63. 70.0 74.0 76.0 67.0 75.1 76.0 43.0 66.0 64.2 73.5 75.1 67.8 70.9 Table S.4. Ablation study for the effect of different InteractVLM components. We evaluate for Binary Human Contact prediction on the DAMON dataset [60]. in guiding the VLMs predictions. Our ablation studies demonstrate the importance of finegrained spatial information, effective feature embedding, 3D contact modeling, and well-designed prompts in achieving robust contact localization. The VLMs role, particularly its fine-tuning and input modalities, is also critical to the overall performance. S.3. Impact of RLL Render-Localize-Lift (RLL) is central to our method. Traditional approaches, like DECO [60] and RICH [29], rely on fully supervised learning with limited 3D GT data to predict 3D contacts. While effective for scenarios encountered during training, these methods fail to generalize to inthe-wild cases. To address this limitation, we leverage the broad visual knowledge of VLM to learn from the limited data. However, effectively utilizing VLM requires reformulating our 3D problem into 2D representation, making it compatible with VLM, which we achieve through RLL. As demonstrated in the main paper, by using RLL with VLM, we surpass the state-of-the-art method for human contact estimation while training on only 5% of the data. S.4. Implementation Details S.4.1. Architecture InteractVLM has two major blocks; reasoning module, Ψ, based on LLaVA-v1 [43] and novel multi-view localization model, MV-Loc, based on SAM [33]. MV-Loc has 13 2 components; shared encoder, Θ and two separate 2D contact decoders, ΩH and ΩO, for humans and objects respectively. Θ, ΩH, and ΩO have the same architecture as SAM. Given an RGB image, I, and prompt text, Tinp, the VLM produces contact tokens, <HCON> and <OCON>, for humans and objects, respectively. To aid MV-Loc in localizing contact, we extract the last-layer embeddings of the VLM corresponding to these tokens and pass them through projection layer, Γ. The latter, Γ, is multi-layer perceptron with 2 layers each of size 256 and ReLU activation. S.4.2. Training Before the start of training, we render multiple views of the human mesh and object point cloud. We also compute the ground-truth contact mask. S.4.2.1. Human Mesh Rendering The human mesh rendering pipeline uses comprehensive multi-view approach using the SMPL+H [55] parametric body model. We initialize the model in neutral shape, positioning the body in Vitruvian pose. This specific pose ensures optimal visibility of potential contact surfaces. We use PyTorch3D for rendering. We select 4 camera viewpoints to capture the complete body geometry: top-front (elevation 45, azimuth 315), top-back (45, 135), bottomfront (315, 315), and bottom-back (315, 135). Each viewpoint is positioned at distance of 2 units from the subject with slight horizontal translations to optimize coverage. We use FoV-Perspective projection model rendered at 10241024 resolution, with blur-radius and faces-perpixel settings set as 0.0 and 1, respectively. For realistic appearance, we use point lights positioned at [0, 0, 3] coordinates relative to the mesh. The lighting settings such as ambient, diffuse, and specular are set at 0.5, 0.3, 0.2, respectively. This creates balanced illumination that highlights surface details. Surface normals are computed per vertex and are used as vertex colors. Crucially, InteractVLM maintains precise correspondence between 2D rendered pixels and 3D mesh vertices. For each rendered view, it generates: (1) pixel-to-vertex mapping matrix storing the indices of mesh vertices visible at each pixel. (2) Barycentric coordinates for accurate interpolation within mesh faces. (3) Binary contact masks for regions with at least three neighboring vertices in contact. This comprehensive multi-view representation, combined with precise pixel-to-vertex correspondences, enables accurate lifting of 2D contact predictions back to the 3D mesh space. Our model processes each view as separate channels in 3 tensor shape during training, where is the batch size and is the number of views. S.4.2.2. Object Point Cloud Rendering The object rendering pipeline uses point clouds to capture object affordances in multiple views. The point cloud preprocessing begins with normalization, where each object is centered at its geometric centroid and scaled to fit within unit sphere, ensuring consistent scale across different objects. Since the point clouds do not have color, we use the NOCS representation for coloring, namely for every point we assign color derived from its normalized spatial NOCS coordinates (scaled to [0.1, 0.9] for better contrast). Our rendering pipeline uses PyTorch3D with four viewpoints: front-left (elevation 45, azimuth 315), front-right (45, 45), back-left (330, 135), and back-right (330, 225). Each view is rendered at 10241024 resolution using FoVPerspective camera positioned at distance of 2 units from the object center. We use fixed point cloud radius of 0.05. For the rasterization settings: we use 10 points per pixel and 50,000 points per bin to handle dense point clouds effectively. An alpha compositor is used for the final rendering. For affordance heatmaps, we generate rendered view with continuous values, [0, 1], representing the affordance likelihood. For each view, we create pixelto-point mapping for lifting 2D affordance heatmaps to 3D affordance points. S.4.3. Additional Text Data for Training S.4.3.1. Data from GPT4o To enhance our models understanding of human-object interactions (HOI), we build comprehensive Visual Question-Answering (VQA) data generation pipeline using GPT-4V (GPT4o). The pipeline processes images from three datasets, namely DAMON [60], LEMON [68], and PIAD [67], generating structured textual descriptions that capture multiple aspects of HOI. For each image, we query GPT-4V to describe five key aspects: (1) the humans visual appearance including clothing and distinctive features, (2) specific body parts in contact with the object, (3) the nature of the interaction, (4) the objects physical characteristics, and (5) the specific parts of the object in contact with the human. To ensure efficient processing while maintaining visual fidelity, images are resized to 256256 pixels. These generated VQA data enrich the training signal with detailed descriptions of interactions. This additional supervision helps our model develop more nuanced understanding of the relationship between visual features and contact regions, ultimately contributing to improved performance in contact prediction tasks. We format the collected data as JSON files to seamlessly integrate these with our VLM training pipeline, allowing the model to leverage these rich textual descriptions during the learning process. Regarding objects, our method faces challenges inherent to the training paradigm. Since there exists no dataset of in-the-wild images with ground-truth 3D contact annotations for objects, we train on affordance data, which represents likelihood of contact rather than actual contact points. However, the distinction between actual contacts and affordances can be ambiguous, particularly for large objects like chairs, as shown in Fig. S.1. In highly occluded or visually ambiguous scenarios, our approach can face challenges due to object lookup failure. The object lookup is also limited by the richness and diversity of underlying object database. However, since our method performs retrieval within predefined object categories, it consistently retrieves an object instance belonging to the correct semantic category, even if exact geometric matches are not always guaranteed. We provide qualitative examples highlighting these limitations in Fig. S.2. S.6. Qualitative Results We present qualitative results for our InteractVLM method for three different tasks. First, in Fig. S.3 we show the object affordance prediction results, where our method more accurately identifies plausible contact regions on objects compared to the state-of-the-art IAGNet method. Second, we show semantic human contact prediction results in Fig. S.4, where our method successfully identifies contact regions on human bodies specific to different object categories, even in complex scenarios. Finally, in Fig. S.5, we demonstrate 3D HOI reconstruction from in-the-wild images, where we leverage the inferred contacts on both human bodies and objects to generate physically plausible 3D reconstructions; this is done for the first time for in-the-wild images. S.7. Future Work Our approach follows two-stage process for 3D HOI: it first predicts human and object contacts, and then uses the inferred contacts in optimization for joint 3D reconstruction. In the future, we will explore learning to perform both 3D contact prediction and 3D reconstruction in an end-toend fashion. This could lead to more coherent predictions by learning and exploiting direct relationships between contact points and physical constraints. Moreover, currently our approach learns on disjoint image datasets of body contacts and object affordances. In the future, we will also exploit recent datasets of images paired with contact annotations for both bodies and objects [12]. Figure S.1. Contact Estimation Failure Cases. Our method struggles with unusual human poses (left). For objects (right), training on affordances rather than actual contacts can sometimes lead to ambiguous contact predictions, especially for large objects like chairs. However, no dataset exists for 3D object contacts for in-the-wild images. Figure S.2. Object Retrieval Failure Cases. The retrieved object meshes (right) differ notably from the actual objects in the input images (left), especially in cases of significant occlusion, atypical object instances, or limited database coverage. Despite these inaccuracies, the retrieval consistently selects objects within the correct semantic category. S.4.3.2. Converting 3D contact vertices to text To establish precise mapping between 3D contact vertices and natural-language descriptions, we leverage the SMPL body models semantic segmentation. The body is divided into 15 semantically meaningful parts including the torso, head, hands, feet, arms, legs, thigh and forearm. For training our VLM, we employ diverse set of natural-language prompts that query about body part contacts with objects. This structured approach creates strong bridge between geometric contact information and natural-language understanding, enabling the model to learn the relationship between visual features, contact regions, and their semantic descriptions. S.5. Failure Cases Despite the overall strong performance, our method has certain limitations. For human contact prediction, our method occasionally struggles with unusual or ambiguous poses that deviate significantly from common interaction patterns. For example, in Fig. S.1 the person is sleeping in an unusual pose on the bed. 15 Figure S.3. Object Affordance Prediction. Here we compare our InteractVLM method trained for object affordance prediction on PIAD [67] dataset with the state-of-the-art IAGNet method. We train for affordance detection because there exists no dataset of in-the-wild images paired with ground-truth 3D contacts for objects. Note that given an image of person performing an action like sit or grasp, the affordance prediction task estimates contact possibilities on the object. 16 Figure S.4. Semantic Human Contact estimation. Here we show results for semantic human contact estimation from in-the-wild images. Each row shows person in contact with multiple objects. Note how InteractVLM estimates contact on bodies that is specific to the object. Figure S.5. 3D HOI reconstruction. Here we show results of our InteractVLM method for 3D HOI reconstruction from in-the-wild images. We use the InteractVLMs inferred contacts on both bodies and objects for joint 3D reconstruction."
        }
    ],
    "affiliations": [
        "Inria, Ecole normale superieure, CNRS, PSL Research University, France",
        "Max Planck Institute for Intelligent Systems, Tubingen, Germany",
        "University of Amsterdam (UvA), the Netherlands"
    ]
}