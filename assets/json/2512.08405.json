{
    "paper_title": "Learning Robot Manipulation from Audio World Models",
    "authors": [
        "Fan Zhang",
        "Michael Gienger"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 5 0 4 8 0 . 2 1 5 2 : r a"
        },
        {
            "title": "LEARNING ROBOT MANIPULATION FROM AUDIO\nWORLD MODELS",
            "content": "Fan Zhang and Michael Gienger Honda Research Institute EU Email: firstname.lastname@honda-ri.de"
        },
        {
            "title": "ABSTRACT",
            "content": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent developments in world modeling offer new possibilities for addressing challenges in robotic manipulation. Research in this domain has primarily concentrated on the following directions: 1) video-based models (Liang et al., 2025; Assran et al., 2025) that predict future visual frames from present observations, encoding the causal dependencies critical for physical interaction. Achieving high-fidelity visual prediction often necessitates large-scale generative models, which in turn impose considerable computational demands and latency. 2) To mitigate these limitations, recent state-ofthe-art work explores latent vision-language world models (Zheng et al., 2025) that learn internal representations of future states, thereby bypassing the need for explicit full-frame reconstruction. In this work, we explore another line of research, audio-driven world models, where audio provides rich information for robotic manipulation but has remained largely underexplored in prior studies. Existing multimodal robot learning approaches typically incorporate audio signals as an auxiliary sensory modality to enhance performance compared to vision-only policies, in tasks such as bagel flipping (Liu et al., 2024) or food scooping (Mejia et al., 2024). Nevertheless, in many scenarios, audio is not merely supplementary, as it can provide essential information for robot learning, particularly when visual cues are scarce or unclear. For example, as shown in Fig. 1, when filling bottle with water, visual observations may remain largely unchanged, making it necessary to reason over the temporal evolution of audio cues, such as pitch patterns, to determine whether the bottle is full. Unlike action-free vision-based world models that can generate wide variety of interactive environments (Zhu et al., 2025), the audio cues in these tasks often reflect certain intrinsic physical dynamics, such as pitch and rhythmic patterns. Consequently, predicting future audio observations plays critical role in informing robot action policies. In this paper, we propose generative model that predicts future audio states in conjunction with robot action policy. Flow matching (Lipman et al., 2022; Rouxel et al., 2024) has emerged as promising alternative to diffusion models, offering faster and higher-quality sampling. As illustrated in Fig. 1, our framework employs transformer-based flow matching approach in the latent space to efficiently generate temporally consistent audio representations. Across both simulation and realworld experiments, we demonstrate that leveraging audio predictions for robot action generation leads to superior performance compared to methods without future lookahead. 1 Figure 1: Overview of the proposed method. The source audio is first encoded into latent representation. Given the current audio segments, flow-matching transformer estimates the generating vector field from noisy audio latents. This vector field is then used to solve the corresponding ODE, producing the future audio latents. The resulting sequence of future audio latents is decoded into audio spectrograms. Finally, robot policy is trained using both the current and predicted future audio spectrograms along with image observations."
        },
        {
            "title": "2 METHODS",
            "content": "Fig. 1 presents the structure of our method. Given source audio sL:0 RLds of length L, our method generates sequence of future audios s1:L and the corresponding robot action chunk a. Our proposed model consists of three phases. First, we pre-train an autoencoder for an expressive and smooth audio latent space. Next, we employ flow matching as the backbone of our world model to generate sequence of future audio latents, given the driving conditions. The generated future latents are decoded to the actual audios. Lastly, policy model is used to predict the robot actions from the synthesized audio frames. Audio Latent Autoencoder We represent audio using spectrograms, as they provide rich timefrequency information that effectively captures the signals temporal and spectral features. For example, in the water-filling task, the spectrogram in Fig. 1 clearly indicates the button pressing and releasing actions, as well as the increasing pitch during the water-filling process. We train AudioMAE (Zhu et al., 2025) to encode and decode the spectrogram with the reconstruction loss. Flow Matching in Latent Space We employ flow matching (Lipman et al., 2022) for the latent audio generation by reconstructing target vector field computed from the corresponding audio latents. We regress on vector field vt(xt, ct; θ) where xt is the sample at the timestep [0, 1], and ct represents driving conditions of consequent audio frames. We adopt the vector field predictor proposed in (Ki et al., 2024), which is modified from DiT (Peebles & Xie, 2023), and decouples frame-wise conditioning from time-axis attention mechanism for temporally consistent latents generation. For training, we choose audio latents ws1:L, and construct the target vector field ut(xws1:L) with noisy input φt(x0) = (1 t)x0 + tws1:L(t U[0, 1] and x0 (01:L, I)). For smooth transitions of sequences longer than the window length L, we incorporate the last audio feature latents wsL :0 from the preceding window as additional input. Thus the flow matching objective Lfm(θ) is defined by Lfm(θ) = vL:L (1) where xt = [wsL :LφL:L (x0)] R(L+L)d, ct R(L+L)h is the concatenated driving condition consisting of [t, wsL:0, ws1:L]. We also incorporate velocity loss to supervise temporal consistency: (xt, ct; θ) ut(xwsL :L ), Lv(θ) = vt ut, where vt and ut are the frame-wise difference along the time-axis for the prediction. The total objective L(θ) is (2) L(θ) = λfmLfm(θ) + λvLv(θ), where λfm and λv are coefficients. Additionally, we apply dropout to the preceding audio latents with probability of 0.5 to ensure smoother transition in the initial window. (3) For the inference procedure, random waypoints are sampled from the source distribution and then flowed into the target audio latents by estimating the flow from = 0 to = 1 over steps. We could 2 use multiple steps 1/t for inference: xt+t = xt + tv1:L , for [0, 1] (4) Robot Policy We leverage the current and predicted audio frames to generate robot action with various manipulation policies. We adopt flow matching policy proposed in (Zhang & Gienger, 2024), conditioned on the current audio, current image observation representation, and the future predicted audios. The robot policy is end-to-end trained in supervised manner using the flow prediction MSE loss on future robot end-effector velocity of 16 steps. Learning Details We decouple the audio flow matching world model from the robot policy training. Following the findings of (Liang et al., 2025), which investigates video generation and robotic control, multi-stage training paradigm yields significantly higher performancein terms of average success ratethan the end-to-end training. This modular architecture also allows independent adaptation and replacement of individual components. As described above, we employ AudioMAE for spectrogram encoding and reconstruction, and flow matching for the robot policy. In the subsequent experimental sections, we further demonstrate the flexibility of this framework in the piano-playing simulation task, where components can be interchangedfor instance, substituting AudioMAE with MusicVAE (Roberts et al., 2018) for encoding and decoding Musical Instrument Digital Interface (MIDI) signals, and replacing the flow-matching robot policy module with Soft Actor-Critic (SAC) reinforcement learning policy. We use the ground truth future audio frames for training the robot policy. During evaluation, we use the generated audios from the latent world model to infer robot actions, based on the trained robot policy. We train with batch size of 256 and AdamW with learning rate of 1.5e-4. Learning rate is cosine decayed after 500 warmup steps, with 3000 training epochs. We incorporate weight decay of 1e-6 on all trainable parameters. The overall model is relatively lightweight: training the entire pipelinecomprising the autoencoder, audio flow matching, and robot policyon 20,000 spectrograms requires approximately one day on single NVIDIA H100 GPU."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "We conduct two robot experiments: 1) real-world water filling task, and 2) piano playing task in simulation. 3.1 FILLING WATER Our water-filling experiments are conducted using Kinova Gen3 robotic arm, which operates the water dispenser by pressing and releasing its control button. RGB video data is captured using an Intel RealSense D410 camera mounted on the robot, while audio is recorded with MAONO omnidirectional USB lapel microphone at 24-bit depth and 192 kHz sampling rate. Haption Virtuose 6D haptic device is employed to teleoperate the robot during data collection. The audio waveform, sampled at 44.1 kHz, is transformed into log Mel filterbank representation using the Kaldi implementation with Hanning window, 128 Mel bins, and frame shift of 10 ms. This process captures the perceptually relevant spectral features of the signal while suppressing noise through the use of Mel scaling and energy exclusion. The filterbank features are linearly normalized to range [1, 1]. At each step, the last 128 128 spectrogram signals (roughly around 1.28 seconds) are used as the input to generate the future spectrogram with size of 256 128 spectrogram (roughly around 2.56 seconds) using our proposed audio world model. Then we use the generated and observed spectrograms, together with resized 224x224 resolution of synchronized image, to generate 6-DoF Cartesian space robot end-effector velocity commands with horizon of 16 timesteps using the flow matching policy. The whole model prediction costs around 50 milliseconds at each step and operates in closed-loop manner. Fig. 2 shows the ground-truth and closed-loop generated spectrograms. The generated future data clearly captures the onset, offset, and gradually increasing pitch of the activity. During testing, we also observed that the predicted button-releasing activity (the second yellow line shown in Fig. 2) consistently appeared in the generated spectrogram just before the bottle was actually filled. Our method achieved 100% success rate over 30 trials, further demonstrating its capability to predict future audio states based on the underlying physical dynamics and pitch patterns. Figure 2: Experimental results. We respectively show the ground truth and the world model generation of water filling spectrogram, music spectrogram and MIDI data. The water filling spectrogram is predicted in closed-loop manner during robot evaluation. Music pieces are generated autoregressively based on previous pieces."
        },
        {
            "title": "3.2 PIANO PLAYING IN SIMULATION",
            "content": "In our second experiment, we simulate piano duet scenario in which the robot must anticipate the forthcoming, yet-unheard musical notes from auditory input before performing its own part. We employ the simulated piano-playing environment introduced by (Zakka et al., 2023). Musical pieces are represented using the Musical Instrument Digital Interface (MIDI) standard, which encodes music as sequence of time-stamped messages corresponding to note-on and note-off events. Following the approach in (Zakka et al., 2023), we convert each MIDI file into time-indexed note trajectory (also known as piano roll), where each note is encoded as one-hot vector. This trajectory serves as the agents goal representation, specifying which keys should be pressed at each time step. We use Soft-Actor-Critic (SAC) policy for robot joint action prediction at frequency of 20 Hz. As pointed in (Zakka et al., 2023), to perform effectively, the agent must anticipate upcoming goals several seconds in advance; therefore, the goal state is stacked over lookahead horizon. The goal state here consists of vector of key goal positions obtained by indexing the piano roll at the current time step, along with discrete vector indicating which fingers should be used. Instead of providing the SAC policy with the complete, predefined song as goal states, we employ our proposed audio world model to generate music dynamically, which then serves as the goal states for the RL agent. Our audio world model adopts MusicVAE to encode 64 time steps (around 8 seconds) of MIDI data and generate the next 64 steps. This process can be repeated autoregressively, using each newly generated segment as input to extend the sequence over time. The resulting MIDI is then converted into robot goal states for the SAC policy. We use the MIDI data from (Zakka et al., 2023) and PIG (Nakamura et al., 2020) to train the audio world model. For the robot policy, we train and test on two songs (Twinkle Twinkle Little Star and Chopin Nocturne in Flat Major Op.9). We use the F1 score for evaluation, widely adopted accuracy metric in the audio information retrieval literature. Our proposed world modelbased learning framework, which incorporates generated future audio, is compared against basic RL baseline without future lookahead. We observe improved performance when future goal states are incorporated into the observation. This result is intuitive, as access to future notes allows the policy to plan more effectivelyfor instance, by pre-positioning non-finger joints (e.g., the wrist) to reach upcoming notes more promptly. Fig. 2 shows, respectively, the ground-truth and autoregressively generated spectrograms, along with the corresponding MIDI data of the Nocturne piece."
        },
        {
            "title": "4 CONCLUSIONS",
            "content": "We have proposed latent flow matchingbased audio generation model that facilitates robot policy learning. It is important to emphasize that this is not merely multi-modal learning problem; rather, the audio cues in our tasks often capture intrinsic physical dynamics, such as pitch and rhythmic patterns. Consequently, constructing an audio-based world model plays critical role in informing robot action policies. Extending this framework to more complex tasks that require fine-grained and dexterous manipulation represents an important direction for future research."
        },
        {
            "title": "REFERENCES",
            "content": "Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. Taekyung Ki, Dongchan Min, and Gyeongsu Chae. Float: Generative motion latent flow matching for audio-driven talking portrait. arXiv preprint arXiv:2412.01064, 2024. Junbang Liang, Pavel Tokmakov, Ruoshi Liu, Sruthi Sudhakar, Paarth Shah, Rares Ambrus, and Carl Vondrick. Video generators are robot policies. arXiv preprint arXiv:2508.00795, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Benjamin Burchfiel, and Shuran Song. Maniwav: Learning robot manipulation from in-the-wild audio-visual data. arXiv preprint arXiv:2406.19464, 2024. Jared Mejia, Victoria Dean, Tess Hellebrekers, and Abhinav Gupta. Hearing touch: Audio-visual pretraining for contact-rich manipulation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 69126919. IEEE, 2024. Eita Nakamura, Yasuyuki Saito, and Kazuyoshi Yoshii. Statistical learning and estimation of piano fingering. Information Sciences, 517:6885, 2020. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck. hierarchical latent vector model for learning long-term structure in music. In International conference on machine learning, pp. 43644373. PMLR, 2018. Quentin Rouxel, Andrea Ferrari, Serena Ivaldi, and Jean-Baptiste Mouret. Flow matching imitation learning for multi-support manipulation. In 2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids), pp. 528535. IEEE, 2024. Kevin Zakka, Philipp Wu, Laura Smith, Nimrod Gileadi, Taylor Howell, Xue Bin Peng, Sumeet Singh, Yuval Tassa, Pete Florence, Andy Zeng, et al. Robopianist: Dexterous piano playing with deep reinforcement learning. arXiv preprint arXiv:2304.04150, 2023. Fan Zhang and Michael Gienger. Affordance-based robot manipulation with flow matching. arXiv preprint arXiv:2409.01083, 2024. Ruijie Zheng, Jing Wang, Scott Reed, Johan Bjorck, Yu Fang, Fengyuan Hu, Joel Jang, Kaushil Kundalia, Zongyu Lin, Loic Magne, et al. Flare: Robot learning with implicit world modeling. arXiv preprint arXiv:2505.15659, 2025. Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek Gupta. Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets. arXiv preprint arXiv:2504.02792, 2025."
        }
    ],
    "affiliations": [
        "Honda Research Institute EU"
    ]
}