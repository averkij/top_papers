{
    "paper_title": "Beyond Release: Access Considerations for Generative AI Systems",
    "authors": [
        "Irene Solaiman",
        "Rishi Bommasani",
        "Dan Hendrycks",
        "Ariel Herbert-Voss",
        "Yacine Jernite",
        "Aviya Skowron",
        "Andrew Trask"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 1 0 7 6 1 . 2 0 5 2 : r Beyond Release: Access Considerations for Generative AI Systems Irene Solaiman Hugging Face Rishi Bommasani Stanford University Dan Hendrycks Center for AI Safety Ariel Herbert-Voss RunSybil Yacine Jernite Hugging Face Aviya Skowron EleutherAI Andrew Trask OpenMined"
        },
        {
            "title": "Abstract",
            "content": "Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy."
        },
        {
            "title": "Introduction",
            "content": "Release decisions for generative AI systems raise ongoing discourse, debates, and regulatory questions. Sometimes framed as an open versus closed debate, release considerations [105] are discussed in media [56], U.S. government executive order [52], and research on release decisions [32]. central issue is determining responsible release that balances tensions of benefits and risks along the gradient of fully open and fully closed systems [103]. Research and policy have focused on how model is released, availability of model weights [18], and more recently, what system components are released [28, 66, 9], such as model weights, training data, and training code. Discourse and analysis has been too narrowly scoped; instead, focus should include aspects of access. Release analysis cannot only consider whether system and its components are made available, but also must consider how accessible each component is. Release and access to AI systems are often used interchangeably, but access includes the resources and qualities needed for stakeholders to engage with system components. Overall access, beyond release, more concretely determines outcomes. Generative AI release decisions for whether system component is made available and to whom it is available do not provide the full information for long-term risk-benefit considerations. Once components are released, making those components accessible determines whether they can be used and First author. All subsequent authors listed in alphabetical order by last name. Inclusion as an author does not entail endorsement of all aspects of the paper. Correspondence to: irene@huggingface.co Preprint. Under review. enables scaling usage and reach. For example, due to resourcing, compute-intensive open-weight model may not be accessible to many researchers. Due to technical usability, user interfaces can make both openand closed-weight models more accessible to malicious actors with low computer literacy. Due to utility, less comprehensive documentation can limit research. Accessibility makes systems more scalable, affecting who has access and resources needed for risk management. In this paper, we explain how analyzing variables beyond release is an effective means of weighing generative AI benefits and risks, in order to inform deployer release decisions, policymaker actions, and further research. We differentiate system release and availability of components from how system and its components are made accessible. Access goes beyond what is available. The three axes of access shed light on how people and society can be helped or harmed by generative AI systems: resourcing, usability, and utility. Each subsection examines risks and benefits per system component. We examine who can access what component and how scaling access affects managing deployment."
        },
        {
            "title": "2 Previous and Related Work",
            "content": "Existing work to distinguish between how system is released and how it interacts with the public includes defining release as making model components available and deployment as the vector of impact [53]. Research to prevent AI misuse explains the misuse-use tradeoff, showing misuse chain where harm occurs after release [23]. Laying out the model pipeline gives another approach to considering certain post-release variables [44]. Examining openness must also include the resourcing and materials around system [118]. Governance proposals specific to open foundation models highlight downstream use considerations and the impact of open systems on the overall AI ecosystem [30]. Recent papers analyze open systems in sectoral contexts, spanning from national security and defense priorities [40] to fact-checking organizations [120]. Motivations to make systems more open cite scientific reproducibility, product reliability and data security, and economic diversification. Motivations toward closedness per component cite inability to monitor models with downloadable weights and less control to intervene or revoke access to given component. There has been lack of consensus on key terms such as open source, misuse of the term, and popularization of the term openness [61]. Works toward concrete terminology lay out openness dimensions [28], create an openness framework [117], and propose formal open source AI definition [9]. Further work explores open foundation model marginal risk [59], safety specific to open foundation models [77], and risk mitigation for open foundation models [106]."
        },
        {
            "title": "3 From Available to Accessed",
            "content": "Release methods commonly address what is released or made available. The gradient of release lays out availability: whether or not system and its components are released along spectrum [103]. Whether systems components are released or made open is distinct from how accessible the system is; system may be fully openly released, but not accessible to the many groups affected. When system components are released, they are not useful until they are made accessible. Fig. 1 distinguishes availability and accessibility and shows the three subsections of access. Release can be viewed as the initial conditions for whether component is available before accounting for making the components accessible to an external stakeholder. Accessible subsections grapple with tradeoffs per component, with high level risks and benefits noted in Figure 1. Figure 1: From Available to Accessible with High Level Tradeoffs 2 Access variables set the grounding necessary for scaling access; from individuals to broader audience, ensuring components are accessible enables usage. The scale of distribution of components is critical factor for better understanding the landscape of accessible system components, both for research and product. Who and how many people are able to access and deploy systems influences scale-related tradeoffs, such as how to manage potential harmful usage. See Appendix for additional figures and visual representations."
        },
        {
            "title": "4 Breaking Down Access",
            "content": "We most closely examine further considerations for system accessibility. Who is granted access and how they are able to use given component shapes who is able to benefit and risk for malicious use. In order to examine who can access which component, more granularity is needed. The three subsections of access are resourcing, usability, and utility, as shown in Figure 2 with the respective variables per component. Figure 2: Categories of Access and Respective Variables In each section, we highlight system components, as defined in existing frameworks for an AI system [28], namely product; model components of datasets, code, and model weights; infrastructure, documentation, and licensing. We compare the following highest performing language models: Llama 3.1 405B Instruct (Meta, open-weight) DeepSeek v3 (DeepSeek, open-weight) GPT-4 (OpenAI, closed-weight) Claude 3.5 Sonnet (Anthropic, closed-weight) 2Note: GPT-4o surpasses GPT-4, but GPT-4 is selected for this paper as the most comparable model. 3 We select these models based on publicly available data on evaluation results and comparative performance; they are the top scoring openand closed-weight models on Stanford Universitys HELM Lite leaderboard [11] as of early 2025. Information about each model is selected from their respective documentation cards [7], technical reports [81, 42], blog announcements [26], and product pages [15, 14]. 4.1 Resourcing Resourcing refers to if and how broad and diverse population can host and serve system and its components, particularly around infrastructure and costs. Computational infrastructure is notable barrier; outside of training, AI systems require varying levels of compute for hosting, inference, and functions such as fine-tuning. Compute resources skew heavily toward industry organizations [20] as costs remain high. The high cost of resourcing has been scrutinized as reducing accessibility of open-weight systems [118]. Local hosting refers to hosting model on independent, local hardware, self-hosting can refer to hosting model locally or renting external cloud instances, and external or managed hosting is when another organization is responsible for infrastructure and hosting costs. External or managed hosting includes dedicated deployments with dedicated infrastructure and serverless deployments with shared infrastructure. Costs for locally and self-hosting can be ambiguous and highly dependent on how hardware and cloud service providers are chosen. Most popular models are available for free limited use via interface when externally hosted by the developer or hosting platform [12]. 4.1.1 Model Ability to acquire infrastructure includes hardware and local infrastructure as well as cloud compute credits needed to locally, self-host, or serve model. The type of hardware needed will differ by model, with larger models running better on only graphics processing units (GPUs) than the more accessible central processing units (CPUs), and some smaller models running on GPU-CPU mixed infrastructure or even CPUs. The global GPU shortage and compute demand skews who is able to acquire hardware. Renting cloud instances is often more accessible option. Developers and deployers often offer free credits (e.g. Google Cloud new customer credits, OpenAI Dev Day credits, HF ZeroGPU). Benefits: Acquiring infrastructure allows users to access benefits from local and self-hosting models. Free credits can help low-resource researchers and developers. Risks: Infrastructural needs and related costs may change for newer models. Sectoral gaps in who is able to obtain compute resources can disadvantage groups such as academia. Free credits can be misused by malicious actors. Storage and hosting costs include costs to locally or self-host model. For local hosting, this includes cost of hardware, storage capacity for storing and loading models, electricity costs, and operational costs such as setup and maintenance. Memory requirements depend on the model and hardware. High upfront setup costs for locally hosting often leads to preferencing self-hosting or using API alternatives. Self-hosting requires user accounts with cloud providers and hosting platforms. Benefits: Local or self hosting model gives more controllability and privacy to the user, especially for sensitive data. Long-term usage costs can be lower, depending on the model and usage. Local hosting ensures stability of access and ensures access to the same version of the model, with no external dependencies. It also enables unrestricted fine-tuning and other adaptations for better customization. Risks: Building and managing local infrastructure has high upfront cost and necessitates hardware investment, in addition to the technical knowledge of setting up and optimizing resources. Energy costs may also be high, and add to potential costs for general operation, from human hours in setup and maintenance to cooling and security. Locally hosted models are less monitorable or not monitorable, especially compared to models hosted by deployer organizations; malicious actors can locally host model and adapt or output harmful content while unmonitored. 4 Inference costs for locally or self-hosted models are determined based on cloud instance selected and instance cost per hour, whereas externally hosted models costs are often charged per token. Depending on infrastructure, locally or self-hosted models do not have limits and tokens can be generated as needed. For externally hosted models, this is usually measured in cost per token or output. Some high performance tasks aggregate multiple calls to model [37], using multiple weaker model calls to match stronger model, although research shows limitations to this approach [108]. Often model sizes correlate to popular hardware limitations for node. Inference optimization methods can be conducted at the data, model, and hardware levels, and aim to make memory, storage, and computation more efficient. For popular decoder-only language models, continuously inputting lengthy token sequences is costly and is often mitigated using key-value cache (KV cache) that stores and reuses previous key and value pairs [126]. Compression techniques such as model quantization [72] can reduce the computational cost of inference, allowing models to run on less computing power. Other popular techniques include model distillation, where smaller model is trained from larger model to emulate behavior and performance, and sparsification, such as via pruning techniques, which remove some weights. Chosen techniques can result in performance losses [127], depend on tasks, and can be combined [121]. Benefits: Local or self-hosted task-specific small models can achieve high performance at relatively low cost while reducing latency [55]. Per token or per output cost is easier for users without or unable to invest in local infrastructure, or short-term users. Costs for large generations or high quality outputs can disincentivize malicious actors. Risks: Large and compute-intensive models are generally costly for inference and will have higher latency. High costs can disincentivize users and be inequitable for who is able to benefit. Powerful on-device models are hard to monitor. Adaptability costs include costs for fine-tuning or reinforcement learning with human feedback. In addition to computing power, human labor and time are often costly. As opposed to classic finetuning, narrow approaches to fine-tuning can be more efficient and less costly. Approaches such as parameter-efficient fine-tuning (PEFT) [122], from low-rank adaptation (LoRA) [54] to quantized low-rank adaptation (QLoRA) [43], can have performance trade-offs. See Ability to adapt or finetune in Technical Usability for usage considerations. Benefits: The relatively small amount of data and compute needed compared to training is more accessible. Risks: Lowering the barrier to adapt models can enable harmful model behavior. 4.1.2 Code Access to libraries includes the ability for users to run code that includes libraries that may be only available to certain organization. Benefits: Full access to all parts of the code can empower researchers to reproduce and build on existing work. Lack of access can prevent malicious actor reproduction. Risks: Without access to all libraries included, the code is largely unrunnable, preventing scientific reproduction. 4.1.3 Training Data Storage and hosting ability refers to capacity to host datasets, especially large datasets and those with modalities with high storage requirements. Benefits: Being able to store datasets such as training data enables additional research and the ability to improve data quality. The Pile dataset at 1346GB in practice is relatively accessible to host [47] and has contributed to many other datasets and models including LLaMA [113]. Risks: For sensitive datasets, secure storage and sharing is needed to prevent misuse or malicious use. 4.1.4 Comparing models Comparing hosting and inference costs for open-weight models depends highly on utilization, task, latency, and hardware used, and varies heavily. Given numbers are broad approximations, and change with hardware, usage, and other factors. Precision affects cost, with many popular models trained with 32 or 16 bits of precision. Memory needed for inference tends to be around 20% higher than model memory [25]. While hardware memory varies, we use NVIDIA H100 GPUs, which have 80 GB of VRAM, and NVIDIA H200 GPUs, which have 141 GB of HBM3e memory. These GPUs cost approximately $25,000 and $32,000 each, respectively. Costs for hardware, renting GPUs, energy, and production models fluctuate heavily over time. See Appendix Resourcing Calculations for more information on determining resourcing for Llama 3.1 405B Instruct.. Llama 3.1 405B Instruct to run locally, uses at minimum 8 NVIDIA H100 GPUs and 405 GB VRAM to load the model in lower 8-bit precision (FP8) and at least 810 GB VRAM and 11 H100 GPUs to load it in its original 16-bit precision (FP16). To reach the 128K token maximum context length, cache memory requirements are 123.05 GB, which would bring hardware requirements to 12 H100 GPUs. Full fine-tuning requires 3.25 TB memory [7]. This results in an upfront cost of at least $200,000 for GPUs in FP8 and $300,000 in FP16, plus additional costs for servers. DeepSeek v3 to run locally is recommended to use 8 NVIDIA H200 GPUs to deploy in FP8. The model is trained in FP8, but can be run in BF16 [42]. Due to its mixed precision training framework, the model is relatively small for its performance, at about 1.3 TB. To deploy the BF16 variant doubles memory requirements to 2.5 TB [88]. At minimum, base infrastructure costs $256,000, plus costs for servers. Distilled versions of DeepSeeks reasoning model, DeepSeek-R1, which is trained on DeepSeek v3, boast relatively high performance and can run on one GPU with small models, such as the 8B parameter model, able to run on personal computer [83]. GPT-4 is available via the OpenAI API, where the developer carries storage, hosting, and maintenance costs. As closed-weight model, it is not possible to host locally or self-host. Information on hosting costs is unavailable publicly. Claude 3.5 Sonnet is available via the Anthropic API, with the developer carrying related hosting costs. It is also closed-weight without publicly available hosting information. Hosting Infrastructure Hosting Cost Inference Cost API Cost Per Token Llama 3.1 12 H100 GPUs, Server, Maintenance 12 H100 GPUs, Energy, Maintenance (12 H100 GPUs GPU / hour) /response rate Input: $5 / 1M Output: $16 / 1M Resourcing: Comparing Models GPT-4 N/A (deployer supplied) DeepSeek v3 8 H200 GPUs, Server, Maintenance Claude 3.5 Sonnet N/A (deployer supplied) 8 H200 GPUs, Server, Energy, Maintenance (8 H200 GPUs GPU / hour) /response rate $0.07 (cache hit) or $0.27 (cache miss) / 1M Output: $1.10 / 1M N/A (deployer supplied) N/A (deployer supplied) N/A (deployer supplied) N/A (deployer supplied) Input: $10 / 1M Output: $30 / 1M Input: $3 / 1M Output: $15 / 1M 4.2 Technical Usability Usability determines whether and how broad and diverse population can technically access and use an AI system and its components. For an average user seeking to query model, well-designed and easy user interfaces have enormously lowered the barrier for people outside the AI field to engage with these systems. However, reaching more people also carries risk of enabling malicious uses from larger set of users. 4.2.1 Model End user interfaces, including interfaces to connect inputs and outputs and chat with model, lower the barrier to interact with model and allow broader populations to access model. These can be built and maintained by hosting organizations, and projects [80] with open repositories can ease interface building for locally hosted models. Interfaces are also built for tasks such as training, fine-tuning [27], and evaluating models, also lowering those barriers. Benefit: Well-designed interfaces surpass the need for technical expertise, and enable users without technical background to complete technical tasks [96]. These interfaces can make models more 6 enjoyable to use by reducing friction, and help users more easily understand how to effectively interact with model. Effective interfaces, user experience, and design can help models such as ChatGPT appeal to wider audiences [112]. Risk: While interfaces enable subject matter experts to use AI, they can also enable less technical malicious actors to generate harmful material. Popular interfaces have been used to consult in terrorist attacks [34, 101]. Early interfaces for Bard and ChatGPT were used to generate malicious code [85]. Deepfake applications that build an application-specific interface on top of image generative models have been weaponized for non-consensual intimate imagery (NCII) and used by and against minors [79]. Additionally, without full transparency, an interface can obfuscate potential layers on top of model that user could be interacting with. This includes content filters which can skew perception of model performance. Interfaces also can often not be built upon. Underlying models can change without notice. AI system functions such as remote APIs can include logit access. APIs bridge systems, providing easier model access to builders. Model releases, whether openor closed-weight, may or may not also have an accompanying API hosted by the developer. The level of access that an API provides, such as fine-tuning, can vary as well. Benefits: APIs enable users to call model and build more easily, streamlining and simplifying model integration, and helping projects scale easier. Remote APIs also can be monitored and rate limited, providing means of intervention in deployment. Users can be authenticated and blocked at any time. Misuse can be shut down [82]. Risks: APIs can introduce security vulnerabilities such as stolen or leaked API keys, and can be vector for attacks such as stealing model information [35, 46] or distributed denial of service attacks. Sensitive data and sensitive inputs can also be leaked. API stability can vary by provider and high demand can slow response rates. Models and versions available by API can change without notice. Concerns around API monitoring include surveillance and privacy, especially for inputting sensitive or proprietary data. Ability to adapt or fine-tune grants access to adapt model toward set of determined tasks or behaviors. This can be enabled via API. Adapting models is possible for open-weight models for those with adequate compute and technical skills, and can be provided for hosted, closed-weight models. Benefits: Adapting or fine-tuning model allows users to tailor models to use cases, products, and contexts. Customizability can improve models for specific use cases. Risks: Fine-tuning can override safety techniques and guardrails even in closed-weight but finetunable models [49], even without malicious intent [89]. Personal eligibility includes the user age, countries, regions, and other restrictions that may be part of model and systems availability. This can be limited by gating or simply blocking IPs from unsupported regions. Restrictions can be technically implemented or legally such as with terms of use. Benefits: Age restrictions can prevent minors from inappropriate content. Regional access restrictions can prevent misuse in or from those regions. For platforms that deploy or host models, user accounts can aid in monitoring. Risks: Collecting user data for permissions can raise both verification issues and privacy concerns. Blocking usage in some regions can prevent populations from benefiting and can lead to technological skill asymmetry. Verification for legal mechanisms vary in reliability and terms of use can be difficult to enforce. 7 Assisting tools, libraries, courses, and frameworks aid in running, deploying, and adapting models. This includes features and prewritten code on collaborative platforms like Hugging Face. Examples of libraries are PyTorch and Tensorflow, frameworks include Ollama, LangChain, and LlamaIndex. Benefits: Many free and available resources train users and simplify deploying pre-trained models. This can close sectoral and resource gaps, especially for underrepresented groups, low-resource developers, and startups. Risks: Malicious actors have equal access to open tools. Technical skill refers to the expertise needed to run, host, adapt, conduct research on, and deploy model. Benefits: Layperson attackers will be unable to conduct sophisticated attacks without upskilling. Running open weight models will require understanding the needed infrastructure. Conducting attacks and and surpassing model safeguards [91] requires knowing potential existing work [36] and proven techniques. Risks: The technical proficiency needed to meaningfully work with model limits who is able to benefit. Educational resources can also aid attackers seeking to upskill. Open-weight models without existing infrastructure such as an interface or API require technical knowledge to deploy, from setting up the right environment to configuring model. Latency refers to the time needed to process inputs and generate an output. Factors such as infrastructure, model capability, and input size and complexity can affect latency. Benefits: Locally hosting models can reduce latency and model usage friction, improving user experience [65]. Risks: Powerful models hosted on external infrastructure, such as via API or hosted interfaces, can face high latency with high demands to their infrastructure. This can make building beneficial projects harder. Low-latency, especially for locally hosted models, can allow malicious actors to generate larger volumes of harmful content faster. 4.2.2 Code Technical skill includes the ability to run code accompanying system releases. It is related to legibility and quality of documentation, which refers to how easily code that accompanies models can be understood and used. Legacy code can become too convoluted for easy use by external actors. Benefits: High quality code release better incentivizes reuse. Risks: Poor quality code can make additional research more difficult, with barriers to onboarding researchers and parsing past decisions. 4.2.3 Training Data Technical skill refers to the ability to conduct research and analysis on datasets, in addition to using datasets accompanying system releases for development. Benefits: Data research can give insights into model performance and safety. Risks: Datasets can contain harmful and illegal material. Access types and eligibility to access datasets is similar to that for models, such as restrictions by user age and region. It can be further affected by legal constraints, from intellectual property (IP) and personally identifiable information (PII) access. Types of access include direct access, API access to certain functions such as the ROOTS tool [86], API access to statistics about data, legal (e.g. GDPR) requests, and court discovery. Benefits: Limiting dataset access can prevent unauthorized users, such as minors, from accessing inappropriate material. Legal compliance for sensitive data protects data subjects. Risks: Restricting access to datasets limits who can conduct research. 8 4.2.4 License Customized licenses can range from permissive use for researchers but restricted for commercial applications. Permissive use as allowed by licenses for models, code, and data [117], with the types of licenses and applicability differing by system component. For models, use may not be tightly correlated with weights specifically and instead with the system usage overall. For data, rights attached to data are an important factor. Benefits: Allowing access to researchers can enhance scientific integrity. Customized licenses can vary with size of model; permissive licenses can be staged to allow open usage for smaller parameter count models and tailored, more restrictive licenses for larger models, as seen with Alibabas Qwen2 model family [123]. Permissive licenses can grant users security. Risks: Licenses, especially when customized, can be difficult to understand for an average user with limited legal experience. Increasingly customized licenses are often more complex legally and can be difficult to enforce. 4.2.5 Comparing models Many popular models will have some level of free interface to generate outputs, often with limitations (rate, filters), and hosted by the developer or third party sources. Latency will vary, depending on infrastructure and setup for open-weight self-hosted models, and depending on demand for APIserved models. Technical Usability: Comparing Models Llama 3.1 Yes: Third-Party Yes: Third-Party Developer, Yes DeepSeek v3 Yes: Third-Party Yes: Third-Party Yes Developer, Developer, GPT-4 Yes: Developer Claude 3.5 Sonnet Yes: Developer Developer, Yes: Third-Party Yes Developer, Yes: Third-Party Yes Customized Customized Customized Customized User Interface API Ability to FineTune License 4.3 Utility Where usability refers to whether broad and diverse population can technically access system, utility refers to whether populations can gain utility from accessible capabilities of the system. This does not cover overall or task-specific model performance, which is noted in Non-Access Considerations. 4.3.1 Model Multilingualilty considers high quality outputs in multiple languages. Benefits: High quality language performance, especially for non-English and low-resource language speakers, opens new markets to AI and allows native speakers of many languages [17, 115] to more comfortably use, build, and deploy. Risks: More language availability and quality gives access to attackers in that language and in regions where that language is more popularly spoken. Some languages may be more spoken in areas with lower resourcing for monitoring or interventions. This disparate resourcing has proved dire for social media monitoring failures [107]. Multimodality covers the modalities for model inputs and outputs, such as text and code, image, audio, and video. Benefits: Certain modalities, e.g. text, fit better for given use cases and deployment contexts, e.g. summarization. Risks: Different modalities require different approaches to monitoring and safeguarding. Some modalities are more prone to certain attacks than others, such as images for NCII generation. 9 Context length is the possible size of an input to model. Benefits: Larger input capacities can improve output relevance, especially for inputs with large spread or complex information. Long context lengths can enable more use cases and improve output accuracy and user experience. Risk: Long context windows can be exploited to jailbreak LLMs [24] and can be more expensive to use. Maximum output refers to the possible size of the output generated, often measured in tokens. Benefits: Longer outputs can improve some use cases. Risks: Longer outputs can aid in large-scale attacks that require more content, such as spreading disinformation. Knowledge cutoff is the timeframe from when training data is stopped. Benefits: Later knowledge cutoffs can provide better updated results in modern contexts. Risks: Earlier model cutoffs can lead to inaccurate or misleading results for current events. 4.3.2 Code Reproducibility includes sharing code for training models and other tasks. Comparisons between open systems and open software continue to influence tradeoff analysis [63]. Benefit: Code release has proven helpful scientifically for reproducibility and collaboration [87], leading to higher citations for publications with accompanying code [125]. Risk: Malicious actors can use code to reproduce components. 4.3.3 Training data Sensitive data includes IP and copyrighted material as well as private and PII. Benefits: For some use cases in controlled settings with appropriate compliance, training on sensitive data can produce helpful and tailored outcomes. Risks: Including sensitive data in training data risks leakage, stealing, and harm to data subjects. 4.3.4 Documentation Comprehensiveness is the robustness and legibility of documentation for system components and processes. Documentation can give insights into system components such as from where public training data was sourced. In addition to documentation differing in findability and consumability, many processes and aspects of model training and release are not documented or shared publicly; for example release decisions processes are rarely if ever shared [31]. Benefits: More comprehensive documentation that includes information such as evaluation results and uncertainty increases transparency [68] and trust. Useful details in technical papers increase reproducibility for scientific integrity. Risks: Incomplete dataset documentation or documentation with sparse sections hurts documentation quality [124]. The same details in technical papers can be misused by malicious actors to reproduce components. 4.3.5 Comparing models Utility across the four selected models are largely comparable, with strong multilinguality availability in DeepSeek v3. Notable differences between the below openand closed-weight models are in input modality, where GPT-4 and Claude 3.5 Sonnet are vision language models and allow image inputs. 10 Utility: Comparing Models Multilingual Llama 3.1 English, German, French, Italian, Portuguese, Hindi, Spanish, Thai Modalities Input: Text, Code Output: Text, Code DeepSeek v3 Albanian, Afrikaans, English, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Cambodian, Burmese, Catalan, Croatian, Czech, Danish, Dutch, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Kyrgyz, Laotian, Latvian, Lithuanian, Macedonian, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian Bokmal, Polish, Portuguese, Punjabi, Rhaeto-Romance, Romanian, Russian, Serbian, Simplified Chinese, Singhalese, Slovak, Slovenian, Spanish, Swahili, Swedish, Tamil, Telugu, Thai, Traditional Chinese, Turkish, Ukrainian, Urdu, Vietnamese, Zulu Input: Text, Code Output: Text, Code Persian, GPT-4 English, Italian, Afrikaans, Spanish, German, French, Indonesian, Russian, Polish, Ukrainian, Greek, Latvian, Mandarin, Arabic, Turkish, Japanese, Swahili, Welsh, Korean, Icelandic, Bengali, Urdu, Nepali, Thai, Punjabi, Marathi, Telugu Claude 3.5 Sonnet English, German, Spanish, French, Hindi, Italian, Japanese, Korean, Polish, Portuguese, Russian, Turkish, Chinese Input: Image, Text, Code Output: Text, Code 128k Input: Image, Text, Code Output: Text, Code 200k Context Length (In Tokens) Maximum Output Tokens Knowledge Cutoff 128k 128k 8000 4096 December 2023 October 2023 December 8192 April"
        },
        {
            "title": "5 Scaling Access: Access to Whom",
            "content": "Who can and should be able to access components often centers on motivation. Some model licenses allow research access and not commercial deployment, while some models are broadly open to any usage. Additionally, the ability to store, host, and deploy artifacts may differ by factors such as geographic location. For sensitive artifacts such as some datasets, the ability to accommodate regional laws [57] is crucial. As systems gain reach, the scale of who can access model has positive relationship with both who can benefit, and potential malicious actors. The scale also influences deployer organizations ability to manage risk and intervene in violations. Making system components accessible sets the foundation for scaling who is given access. Increased access will add computing power demands and must consider means of distribution. This can affect variables such as latency. Depending on the diversity of users, usability and utility variables may update as well, such as increasing language compatibility when deploying in new regions. Figure 3 shows the flow from release, to becoming accessible, to scale considerations. Figure 3: Flow of Access and Scale with High Level Tradeoffs 11 5.1 Individualized Release and Depth of Access One of the central questions of access is which actors qualify for access, especially in limited releases. Calls for independent scrutiny of closed systems, such as structured access [99], safe harbor evaluations [69], and third party audits [2], must ask what components an external actor has access to, and what is the permissive ability to access each component. Mostly targeted toward researchers and auditors, depth of access enables scrutiny outside of the developers expertise or incentives. For product and commercial-oriented actors, depth enables better tailored models for given application. Per system component, the amount of access, or the depth, given to an external actor, researcher, or auditor depends on the use case and research area [33]. The depth of access overlaps with what is made available, and is sometimes framed as completeness. Some research often requires model weights, such as interpretability [29]. Safe information sharing mechanisms can encourage collaboration and transparency [114]. Some data may not be fit for public release, often due to legal or security constraints, such as trade secrets, IP, PII, and national security information. Existing frameworks, such as the Five Safes, help manage safe research access to data [97]. For systems developed with researcher access, more work is needed to understand the most urgent types of research that should be conducted on that system and the correlating level of access needed. Curated external access to closed-weight system faces challenges of appropriate due diligence for determining the right users to whom access should be granted. Curating access by select priority expertise areas may overlook potential unforeseen contributions. 5.2 Release Distribution Methods Who can and will access system and its components is tied to how system is distributed. Distribution considers the method of distribution and the means by which components are made available. This can include developer and deployer platforms, hosting platforms, curated access to individuals, social media links, and more rudimentary methods such as USB sticks or QR codes [90]. Staging releases with set time frame for making model weight available [105] can aid both understanding of threats and assuring trust in the release process. Developer and deploy platforms can advertise, influencing who is incentivized to access the platform. Platforms that host model weights can include the model developers sites, hosting platforms like Hugging Face, and partner platforms. For example, Llama models are available on Meta official websites, Hugging Face repositories, Kaggle, and partners for different model sizes [6]. Hosting platforms can enable broad access to systems, which can be controlled and managed via gating and user access. Varied platform visibility for proprietary, private, or sensitive systems can allow organizations to selectively share components, such as internal only organizational access. Content guidelines and terms of use are applied at the model level. Distribution of content affects how impactful model and its outputs are, for beneficial and malicious purposes. For example, AI generated disinformation is only as impactful as its distribution, with studies showing concerns about AI disinformation are overstated [100]. Concerns can of course change with time and major events, such as elections, with distribution remaining key factor [104]. The perception of heavy distribution can have harmful effects: survey of 100,000 people across 47 countries showed 59% of people are worried about false news content [78]. Integrating AI into distribution platforms, such as news media, has been met with public skepticism: the majority of respondents in the same survey in the U.S. and UK noted they would be uncomfortable with news produced mostly with AI [78]. Scale of deploying systems requires heavy investment in maintaining the means of deployment, often an API, user interface, or other application. From rapid adoption by daily users [73], to increasing usage by researchers [67], to deployment by startups and businesses [10], reaching more people via deployment also means meeting infrastructure, legal, and management demands. These demands tend to differ by user group. 5.3 Managing Scale Manageability includes the ability to establish what constitutes misuse and identify misconduct; to monitor and intervene on misconduct; to reduce the reach of system; and the cost to manage. 12 This can mean managing the release of artifacts themselves as well as managing deployed systems, especially when models are deployed by third-party commercial entities. The distribution of release and breadth of access affects how monitorable and manageable system is: widely, globally used system across release availability will have different demands as user scale increases. Use in different e.g. languages can affect manageability. Misuse risks may not be fully addressed in either open-weight or closed-weight settings. Access controls largely depend on method of release, such as the ability to create an account to download model weights for open-weight releases or adhering to API policies for API-deployed closed-weight models. Open-weight releases can be downloaded and kept by users with the appropriate hardware, leading to inability to monitor if run locally. Reducing reach by deprecating model weights from distribution and hosting platforms [1] can limit malicious actor access. Proposed methods for preventing harmful use via access span from structural, such as access control, to technical, such as task blocking [48]. When models are deployed via API or hosted medium, technical interventions can be ineffective or the deployer organization must be able to meet the demands of high user traffic. This can have high management costs. Interventions such as query refusals and false positives in content filters can disincentivize users from engaging with that model or deployer. Variables that affect management include competitive pressure towards faster deployment, wider access, and less friction for users."
        },
        {
            "title": "6 Access-Adjacent Considerations",
            "content": "In addition to the risk-benefit tradeoffs for variables under each access category, overarching factors post-release strongly influence threat and utility landscapes. How and with what intentions system is deployed influences reach and subsequently system risks-benefit tradeoffs. Accessibility is linked to deployability; whether commercial entity is able to technically and legally deploy system depends on variables such as licensing and other governance mechanisms. 6.1 Changes Over Time The rapid pace of change in the AI landscape shifts the weight of some variables. Notably, changes in capabilities [22], cost [39], and data availability [70] are some of the most influential recent factors. Emergent abilities [116] and how to measure and validate them [94] raise research questions on AI system future capabilities and risks. Advancement and competitive pressure from releases such as DeepSeek-R1, showcasing low-cost, open-weight, high performance, are already shifting potential for all three access categories. Within weeks, releases such as Mistral Small 3 [13] contribute to trends towards smaller models. The Allen Institute for AI (AI2) model Tulu 3 405B model [62] quickly boasted high benchmarks but also shared lessons on emphasis on reinforcement learning. Beyond overall system performance increasing over time, some aspects of model components have improved. Tokenization leaps for non-English languages has increased language availability, although pricing disparities remain [19]. Coheres Command R+s tokenizer compressed non-English language text better than comparable models at its time of release [93], and models such as DeepSeek v3 [42] show significant improvements in non-English and low resource languages, especially compared to previous models. Changes in hardware and hardware costs include advances in memory capacity, better computational price-performance ratio, and also potential walls [50]. Inference cost for existing and newer models can change [38] with commercial needs and energy limitations. Popularization of reasoning models can put more computing pressure on inference, where thinking tokens are more demanded. Decline in data availability affects the utility of data collection over time, which can advantage organizations with better access to data resources. 6.2 Modality Influence As referenced in Utility, modalities for model inputs, model outputs, and datasets impact threat models. Some modalities have higher malicious use potential and comparably less research on safeguards. For example, voice cloning can be helpful in medical settings for patients who have lost their voices to disease [75], but raises serious misuse concerns such as scams and fraud, prompting government response [4]. Modalities that can more closely infringe on personal likeness, such 13 as audio and image, navigate safeguards for consent. Many audio generation platforms for voice cloning restrict users from usage without consent of the data subject, but consent mechanisms are flawed and most do not require proof of consent [92]. Safeguard effectiveness differs by modality as well, as seen with watermarking and content moderation. While some modalities have more flexible use cases and some product use cases only function with certain modalities, some modalities are more conducive to certain attacks. All modalities can be weaponized for egregious misuse such as child sexual abuse material (CSAM), and some modalities such as image are used more often [110]. Some modalities are better suited for distribution per distribution platform, such as Instagram primarily distributing images. 6.3 Smaller Models In addition to independent projects that enable users to run models locally, such as llamafile [51], larger model families often offer small model options. High costs for larger models incentivize research to maintain high performance in smaller models, as exemplified by models such as DeepSeekR1, Qwen2 72B Instruct, Llama 3 70B, and Mistral Large 2 128B. On-device deployment means models can be hosted without cloud servers and on everyday hardware such as laptops and phones. Apples OpenELM [74] models ranging from 270M to 3B parameters, can run on Macbooks and some iPhones. Microsofts Phi models [16] released 3.8B parameter model, phi-3-mini, that can run on phone and boasts comparable benchmark results to GPT-3.5. On-device hosting sidesteps the security risks associated with using the internet. Additional models in this category are Googles Gemma and Mistrals Les Ministraux model families. 6.4 Application and Actualization models usefulness or harmfulness is directly informed by the application in which it is deployed. Due to many generative systems generality in task performance, general risk assessments are unable to capture all possible risks and magnitudes of risks. Assessing model safety is best done in the context of the operational domain [60]. Building application-specific infrastructure and optimizing for usability can be weaponized, as seen with deepfake NCII applications [79]. Application-specific infrastructure can also be misused, where AI tools are integrated into platforms such as animation and modeling interfaces [119]. For certain risks, such as chemical and biological attacks, material and physical needs and equipment are needed to actualize harm. Creating harmful toxins and pathogens relies on the feasibility, both operationally and biologically, of developing high risk attack [76]. For high risk threats, effective threat modeling should consider how operationalizable, targeted, and scalable, attacks can be. 6.5 Non-Access Considerations Some release considerations are adjacent to but not directly related to model accessibility. This includes overall and task-specific performance, scientific and market impacts, and usefulness outside of access variables such as agentic behavior [98]. As discussed in Changes Over Time, capability increases are often centered in risk discourse, with advances with scaling [58] informing some deployment decisions [3]. High performance for high risk tasks, whether from large generalized model or task-specific model, can include output quality. Output quality can range based on task. The realism and human-like quality of outputs affects usefulness. AI outputs close to or indistinguishable from human-generated outputs that are not labeled or shared as AI outputs contribute to clouding our information ecosystem. Task-specific performance, especially for high risk tasks, and training on dangerous information such as weapons manufacturing, add to potential threats. Mitigating these threats includes sorting data and redacting dangerous information or blacklisting potentially dangerous data sources. Scientific and economic factors are popularly referenced in release decisions, with themes of broader community feedback and the need for openness in scientific integrity and innovation [102]. Releasing training techiniques, as with DeepSeek-R1 [41], can influence broader investment in for example, reinforcement learning. Concentration of power [111] in the market and research field is commonly cited concern for who is able to access and build systems. This includes sectoral compute gaps and the ability for 14 lower resource actors to compete commercially. Unexpected risks and ongoing safety research areas such as deception capabilities [84] also affect release decisions. related variable is the state of tamper-proof or tamper resistant safeguards [109] per modality, such as image watermarking [21]. In addition to robustness, public trust in safeguards affects release receptivity."
        },
        {
            "title": "7 Limitations and Future Work",
            "content": "While access considerations broaden insights into release, tangential artifacts and variables as well as non-access variables require more scrutiny. System-adjacent components that are not inherently part of development may not be included in model releases. Evaluations and test datasets, which can be developed by model developers or other research groups, can be withheld out of concern for contaminating training data [45]. Model releases that do include evaluations include AI2s OLMo [8], where the code and data used to produce OLMo 2s results are on the model release page. Modalities with comparably less research background and less access demand may have risks and benefits that evolve differently than other modalities. Incentives to reduce prices for popular commercial modalities such as text may differ from video. The limited data and publicly available documentation on some modality releases, such as audio and video, make effective release strategies difficult to meaningfully compare. The pace of advancements and ecosystem influence shifts costs and incentives in timeframes that are difficult to reliably model. Pricing changes as new model service providers enter the market leads to competitive pricing. Related to Changes Over Time, and similar to demands by modalities differing, ecosystem variables such as demand for access and economic influences in willingness to adjust to costs [64]. Low-cost, open-weight models are more easily able to long-term embed in digital infrastructure, which can encourage broader integration due to version stability. As influenced by DeepSeek-R1s trend of high performance in low-cost, small models, examining potential thresholds for preferencing open-weight model integration is critical open question that affects global and geopolitical influence. Differentiating research versus commercial access can be blurry; research access can inform and evolve into product and commercial ideas. More insight is needed on potentially distinctive uses and needs by sector."
        },
        {
            "title": "8 Conclusion",
            "content": "Aspects of access beyond system release provide more clarity to release benefits, risks, and tradeoffs. Examining system accessibility and possible scale of access can more accurately inform release and policy decisions and research. We show through granular analysis and model comparisons that similar considerations exist across the release spectrum and for both openand closed-weight models. We also show barriers without usable infrastructure, and high risk via accessible interfaces and fine-tuning. As accessibility positively impacts scale, reaching more users broadens usage but also reaches more malicious actors and affects ability to intervene. The resourcing, technical usability, and utility subsections of access exemplify how to more meaningfully weigh usefulness and threats."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "Thank you to Hugging Face for supporting this research. Thank you to Stella Biderman, Jeff Boudier, Liv Erickson, Nathan Lambert, Philipp Schmid, and anonymous reviewers for their thoughtful feedback on earlier versions of this paper. Any errors remain the first authors responsibility."
        },
        {
            "title": "References",
            "content": "[1] 2022. ykilcher/gpt-4chan Hugging Face. https://huggingface.co/ykilcher/ gpt-4chan [2] 2023. How to audit an AI model owned by someone else (part 1). https://blog. openmined.org/ai-audit-part-1/ [3] 2023. Responsible Scaling Policies (RSPs). METR Blog (Sept. 2023). https://metr.org/ blog/2023-09-26-rsp/ [4] 2024."
        },
        {
            "title": "Approaches",
            "content": "to Address AI-enabled Voice Cloning. https: //www.ftc.gov/policy/advocacy-research/tech-at-ftc/2024/04/ approaches-address-ai-enabled-voice-cloning [5] 2024. Average Energy Prices, San Francisco-Oakland-Hayward August 2024. https://www.bls.gov/regions/west/news-release/averageenergyprices_ sanfrancisco.htm [6] 2024. Llama 3.2. https://www.llama.com/docs/getting-the-models/ 405b-partners [7] 2024. meta-llama/Llama-3.1-405B-Instruct-FP8 Hugging Face. https://huggingface. co/meta-llama/Llama-3.1-405B-Instruct-FP [8] 2024. OLMo release notes Ai2. https://allenai.org/olmo/release-notes [9] 2024. The Open Source AI Definition 1.0. https://opensource.org/ai/ open-source-ai-definition [10] 2024. The state of AI in early 2024 McKinsey. https://www.mckinsey.com/ capabilities/quantumblack/our-insights/the-state-of-ai [11] 2025. Holistic Evaluation of Language Models (HELM). https://crfm.stanford.edu/ helm/lite/latest/#/leaderboard [12] 2025. HuggingChat. https://huggingface.co/chat [13] 2025. Mistral Small 3 Mistral AI. https://mistral.ai/news/mistral-small- [14] 2025. Models. https://docs.anthropic.com/en/docs/about-claude/models [15] 2025. OpenAI Platform. https://platform.openai.com [16] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio Cesar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong 16 Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. https://doi.org/10.48550/arXiv.2404.14219 arXiv:2404.14219. [17] David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel Dsouza, Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen H. Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin, Tendai Marengereke, and Salomey Osei. 2021. MasakhaNER: Named Entity Recognition for African Languages. Transactions of the Association for Computational Linguistics 9 (2021), 1116 https://doi.org/10.1162/tacl_a_00416 Place: Cambridge, MA Publisher: 1131. MIT Press. [18] National Telecommunication and Information Administration. 2024. Dual-Use Foundation Models with Widely Available Model Weights Report. Technical Report. https://www. ntia.gov/issues/artificial-intelligence/open-model-weights-report [19] Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and Yulia Tsvetkov. 2023. Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Singapore, 99049923. https://doi.org/10.18653/v1/2023.emnlp-main.614 [20] Nur Ahmed, Muntasir Wahed, and Neil C. Thompson. 2023. The growing influence of industry in AI research. Science 379, 6635 (March 2023), 884886. https://doi.org/10. 1126/science.ade2420 Publisher: American Association for the Advancement of Science. [21] Bang An, Mucong Ding, Tahseen Rabbani, Aakriti Agrawal, Yuancheng Xu, Chenghao Deng, Sicheng Zhu, Abdirisak Mohamed, Yuxin Wen, Tom Goldstein, and Furong Huang. 2024. WAVES: Benchmarking the Robustness of Image Watermarks. https://doi.org/10. 48550/arXiv.2401.08573 arXiv:2401.08573. [22] Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, Cullen OKeefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, and Kevin Wolf. 2023. Frontier AI Regulation: Managing Emerging Risks to Public Safety. https://doi.org/10.48550/arXiv.2307.03718 arXiv:2307.03718. [23] Markus Anderljung and Julian Hazell. 2023. Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted? https://arxiv.org/abs/2303.09377v [24] Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford, Francesco Mosconi, Rajashree Agrawal, Rylan Schaeffer, Naomi Bashkansky, Samuel Svenningsen, Mike Lambert, Ansh Radhakrishnan, Carson Denison, Evan Hubinger, Yuntao Bai, Trenton Bricken, Timothy Maxwell, Nicholas Schiefer, Jamie Sully, Alex Tamkin, Tamera Lanham, Karina Nguyen, Tomasz Korbak, Jared Kaplan, Deep Ganguli, Samuel Bowman, Ethan Perez, Roger Grosse, and David Duvenaud. [n. d.]. Many-shot Jailbreaking. ([n. d.]). [25] Quentin Anthony, Stella Biderman, and Hailey Schoelkopf. 2023. Transformer Math 101. https://blog.eleuther.ai/transformer-math/ 17 [26] Anthropic. 2024. Claude 3.5 Sonnet Model Card Addendum. https: //www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/ Model_Card_Claude_3_Addendum.pdf [27] Ali Asaria. 2023. Transformer Lab: Experiment with Large Language Models. https: //github.com/transformerlab/transformerlab-app [28] Adrien Basdevant, Camille Francois, Victor Storchan, Kevin Bankston, Ayah Bdeir, Brian Behlendorf, Merouane Debbah, Sayash Kapoor, Yann LeCun, Mark Surman, Helen KingTurvey, Nathan Lambert, Stefano Maffulli, Nik Marda, Govind Shivkumar, and Justine Tunney. 2024. Towards Framework for Openness in Foundation Models: Proceedings from the Columbia Convening on Openness in Artificial Intelligence. https://doi.org/10. 48550/arXiv.2405.15802 arXiv:2405.15802 [cs]. [29] Leonard Bereska and Efstratios Gavves. 2024. Mechanistic Interpretability for AI Safety Review. https://doi.org/10.48550/arXiv.2404.14082 arXiv:2404.14082. [30] Rishi Bommasani, Sayash Kapoor, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Daniel Zhang, Marietje Schaake, Daniel E. Ho, Arvind Narayanan, and Percy Liang. 2024. Considerations for governing open foundation models. Science 386, 6718 (Oct. 2024), 151 153. https://doi.org/10.1126/science.adp1848 Publisher: American Association for the Advancement of Science. [31] Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, and Percy Liang. 2023. The Foundation Model Transparency Index. https://arxiv.org/abs/2310.12941v1 [32] Zoe Brammer, Anthony Aguirre, Markus Anderljung, Chloe Autio, Ramsay Brown, Chris Byrd, Gaia Dempsey, David Evan Harris, Vaishnavi J. J., Landon Klein, Sebastien Krier, Jeffrey Ladish, Nathan Lambert, Aviv Ovadya, Elizabeth Seger, and Deger Turan. 2023. How Does Access Impact Risk? Technical Report. https://securityandtechnology.org/ ai-foundation-model-access-initiative/how-does-access-impact-risk/ Bucknall [33] Benjamin for nical structured-access-for-third-party-research-on-frontier-ai-models Access Frontier Techhttps://www.governance.ai/research-paper/ 2023. AI Models Third-Party Report. and Research Robert on GovAI. Structured Trager. [34] Dell Cameron. 2025. Before Las Vegas, ers Were Turning to AI. Wired (Jan. 2025). las-vegas-bombing-cybertruck-trump-intel-dhs-ai/ Section: tags. Intel Analysts Warned That Bomb Makhttps://www.wired.com/story/ [35] Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Itay Yona, Eric Wallace, David Rolnick, and Florian Tram`er. 2024. Stealing Part of Production Language Model. https://doi.org/10.48550/arXiv.2403.06634 arXiv:2403.06634. [36] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, and Eric Wong. 2024. Jailbreaking Black Box Large Language Models in Twenty Queries. https: //doi.org/10.48550/arXiv.2310.08419 arXiv:2310.08419. [37] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024. Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems. https://doi.org/10.48550/arXiv.2403.02419 arXiv:2403.02419 [cs]. [38] Lingjiao Chen, Matei Zaharia, and James Zou. 2023. FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. https://doi.org/10. 48550/arXiv.2305.05176 arXiv:2305.05176. [39] Ben Cottier, Robi Rahman, Loredana Fattorini, Nestor Maslej, and David Owen. 2024. The rising costs of training frontier AI models. https://doi.org/10.48550/arXiv.2405. 21015 arXiv:2405.21015. 18 [40] Masao Dahlgren. 2024. Defense Priorities in the Open-Source AI Debate. (Aug. 2024). https://www.csis.org/analysis/defense-priorities-open-source-ai-debate [41] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. https://arxiv.org/abs/2501.12948 eprint: 2501.12948. [42] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2025. DeepSeek-V3 Technical Report. https://doi.org/10.48550/arXiv.2412.19437 arXiv:2412.19437 [cs]. [43] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. Advances in Neural Information Processing Systems 36 (Dec. 2023), 1008810115. https://proceedings.neurips.cc/paper_files/paper/ 2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html [44] Francisco Eiras, Aleksandar Petrov, Bertie Vidgen, Christian Schroeder, Fabio Pizzati, Katherine Elkins, Supratik Mukhopadhyay, Adel Bibi, Aaron Purewal, Csaba Botos, Fabro Steibel, Fazel Keshtkar, Fazl Barez, Genevieve Smith, Gianluca Guadagni, Jon Chun, Jordi Cabot, Joseph Imperial, Juan Arturo Nolazco, Lori Landay, Matthew Jackson, Phillip H. S. Torr, Trevor Darrell, Yong Lee, and Jakob Foerster. 2024. Risks and Opportunities of Open-Source Generative AI. https://doi.org/10.48550/arXiv.2405.08597 arXiv:2405.08597. [45] Aparna Elangovan, Jiayuan He, and Karin Verspoor. 2021. Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics, Online, 13251335. https://doi.org/10.18653/v1/2021.eacl-main. 113 [46] Matthew Finlayson, Xiang Ren, and Swabha Swayamdipta. 2024. Logits of API-Protected LLMs Leak Proprietary Information. https://doi.org/10.48550/arXiv.2403.09539 arXiv:2403.09539 [cs]. [47] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. https://doi.org/ 10.48550/arXiv.2101.00027 arXiv:2101.00027 [cs]. [48] Peter Henderson, Eric Mitchell, Christopher Manning, Dan Jurafsky, and Chelsea Finn. 2023. Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society (AIES 23). Association for Computing Machinery, New York, NY, USA, 287296. https://doi. org/10.1145/3600211.3604690 [49] Peter Henderson, Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, and Prateek Mittal. 2024. Safety Risks from Customizing Foundation Models via Finetuning. (Jan. 2024). https://hai.stanford.edu/sites/default/files/2024-01/ Policy-Brief-Safety-Risks-Customizing-Foundation-Models-Fine-Tuning. pdf [50] Marius Hobbhahn. 2023. Trends in Machine Learning Hardware. https://epochai.org/ blog/trends-in-machine-learning-hardware [51] Stephen Hood and Justine Tunney. 2023. Introducing llamafile Mozilla Hacks - the Web developer blog. https://hacks.mozilla.org/2023/11/introducing-llamafile [52] The White House. 2023. worthy Development whitehouse.gov/briefing-room/presidential-actions/2023/10/30/ executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/ Executive Order on the Safe, Secure, and Trusthttps://www. and Use of Artificial Intelligence. [53] Jeremy Howard. 2024. What policy makers need to know about AI (and what goes wrong if they dont). https://www.answer.ai/posts/2024-06-11-os-ai.html [54] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. https://doi.org/10.48550/arXiv.2106.09685 arXiv:2106.09685 [cs]. 20 [55] Chandra Irugalbandara, Ashish Mahendra, Roland Daynauth, Tharuka Kasthuri Arachchige, Jayanaka Dantanarayana, Krisztian Flautner, Lingjia Tang, Yiping Kang, and Jason Mars. 2024. Scaling Down to Scale Up: Cost-Benefit Analysis of Replacing OpenAIs LLM with Open Source SLMs in Production. https://doi.org/10.48550/arXiv.2312.14972 arXiv:2312.14972. [56] Mike Isaac. 2024. What to Know About the Open Versus Closed Software Debate. The New York Times (May 2024). https://www.nytimes.com/2024/05/29/technology/ what-to-know-open-closed-software.html [57] Yacine Jernite, Huu Nguyen, Stella Biderman, Anna Rogers, Maraim Masoud, Valentin Danchev, Samson Tan, Alexandra Sasha Luccioni, Nishant Subramani, Isaac Johnson, Gerard Dupont, Jesse Dodge, Kyle Lo, Zeerak Talat, Dragomir Radev, Aaron Gokaslan, Somaieh Nikpoor, Peter Henderson, Rishi Bommasani, and Margaret Mitchell. 2022. Data Governance in the Age of Large-Scale Data-Driven Language Technology. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT 22). Association for Computing Machinery, New York, NY, USA, 22062222. https: //doi.org/10.1145/3531146.3534637 [58] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. https://doi.org/10.48550/arXiv.2001.08361 arXiv:2001.08361. [59] Sayash Kapoor, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Peter Cihon, Aspen Hopkins, Kevin Bankston, Stella Biderman, Miranda Bogen, Rumman Chowdhury, Alex Engler, Peter Henderson, Yacine Jernite, Seth Lazar, Stefano Maffulli, Alondra Nelson, Joelle Pineau, Aviya Skowron, Dawn Song, Victor Storchan, Daniel Zhang, Daniel E. Ho, Percy Liang, and Arvind Narayanan. 2024. On the Societal Impact of Open Foundation Models. https://arxiv.org/abs/2403.07918v1 [60] Heidy Khlaaf. 2023. Toward Comprehensive Risk Assessments and Assurance of AI-Based Systems. (2023). [61] Nathan Lambert. 2023. Why we disagree on what open-source AI should be. https: //www.interconnects.ai/p/flavors-of-open-source-ai [62] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2024. Tulu 3: Pushing Frontiers in Open Language Model Post-Training. (2024). [63] Max Langenkamp and Daniel N. Yue. 2022. How Open Source Machine Learning Software Shapes AI. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES 22). Association for Computing Machinery, New York, NY, USA, 385395. https: //doi.org/10.1145/3514094.3534167 [64] Kai-Fu Lee. 2024. Think Lighter. how-do-you-get-to-artificial-general-intelligence-think-lighter/ tion: tags. Intelligence? https://www.wired.com/story/ Secto Artificial General How Do You Get 2024). Wired (Nov. [65] Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, Rose E. Wang, Minae Kwon, Joon Sung Park, Hancheng Cao, Tony Lee, Rishi Bommasani, Michael Bernstein, and Percy Liang. 2024. Evaluating Human-Language Model Interaction. https: //doi.org/10.48550/arXiv.2212.09746 arXiv:2212.09746. [66] Percy Liang, Rishi Bommasani, Kathleen Creel, and Rob Reich. 2022. The Time Is Now https://crfm. to Develop Community Norms for the Release of Foundation Models. stanford.edu/2022/05/17/community-norms.html 21 [67] Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D. Manning, and James Y. Zou. 2024. Mapping the Increasing Use of LLMs in Scientific Papers. https://doi.org/10.48550/arXiv.2404.01268 arXiv:2404.01268. [68] Q. Vera Liao and Jennifer Wortman Vaughan. 2024. AI Transparency in the Age of LLMs: Human-Centered Research Roadmap. Harvard Data Science Review Special Issue 5 (May 2024). https://doi.org/10.1162/99608f92.8036d03b Publisher: The MIT Press. [69] Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Borhane Blili-Hamelin, Yangsibo Huang, Aviya Skowron, Zheng-Xin Yong, Suhas Kotha, Yi Zeng, Weiyan Shi, Xianjun Yang, Reid Southen, Alexander Robey, Patrick Chao, Diyi Yang, Ruoxi Jia, Daniel Kang, Sandy Pentland, Arvind Narayanan, Percy Liang, and Peter Henderson. 2024. Safe Harbor for AI Evaluation and Red Teaming. https://doi.org/ 10.48550/arXiv.2403.04893 arXiv:2403.04893 [cs]. [70] Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William Brannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman, Christopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh, Caroline Chitongo, Da Yin, Damien Sileo, Deividas Mataciunas, Diganta Misra, Emad Alghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kush Tiwary, Lester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu Minh Chien, Xuhui Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, Daphne Ippolito, Sara Hooker, Jad Kabbara, and Sandy Pentland. 2024. Consent in Crisis: The Rapid Decline of the AI Data Commons. https://doi.org/10.48550/arXiv.2407.14933 arXiv:2407.14933. [71] Sasha Luccioni, Yacine Jernite, and Emma Strubell. 2024. Power Hungry Processing: Watts Driving the Cost of AI Deployment?. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT 24). Association for Computing Machinery, New York, NY, USA, 8599. https://doi.org/10.1145/3630106. [72] Yu Mao, Weilan Wang, Hongchao Du, Nan Guan, and Chun Jason Xue. 2024. On the Compressibility of Quantized Large Language Models. https://doi.org/10.48550/arXiv. 2403.01384 arXiv:2403.01384 [cs]. [73] Colleen McClain. 2024. Americans use of ChatGPT is ticking up, but few trust its https://www.pewresearch.org/short-reads/2024/03/26/ election information. americans-use-of-chatgpt-is-ticking-up-but-few-trust-its-election-information/ [74] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. 2024. OpenELM: An Efficient Language Model Family with Open Training and Inference Framework. https://doi.org/10.48550/arXiv.2404.14619 arXiv:2404.14619. [75] Amanda Morris, Alexa Juliana Ard, and Szu Yu Chen. 2023. Patients were told their voices could disappear. They turned to AI to save them. https://www.washingtonpost.com/ wellness/interactive/2023/voice-banking-artificial-intelligence/ [76] Christopher A. Mouton, Caleb Lucas, and Ella Guest. 2024. The Operational Risks of AI in Large-Scale Biological Attacks: Results of Red-Team Study. Technical Report. RAND Corporation. https://www.rand.org/pubs/research_reports/RRA2977-2.html [77] Terrence Neumann and Bryan Jones. 2024. PRISM: Design Framework for Open-Source Foundation Model Safety. https://arxiv.org/abs/2406.10415v1 [78] Nic Newman, Richard Fletcher, Craig T. Robertson, Amy Ross Arguedas, and Rasmus Kleis Nielsen. 2024. Reuters Institute digital news report 2024. Technical Report. Reuters Institute for the Study of Journalism. https://doi.org/10.60625/RISJ-VY6N-4V57 22 [79] Anna North. 2024. AI has created new form of sexual abuse. https://www.vox.com/ 24145522/ai-deepfake-apps-teens-ban-laws [80] oobabooga. 2024. oobabooga/text-generation-webui. https://github.com/oobabooga/ text-generation-webui original-date: 2022-12-21T04:17:37Z. [81] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C. J. Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report. https://doi.org/10.48550/arXiv.2303.08774 arXiv:2303.08774 [cs]. [82] Kyle Orland. 2025. nAI. viral-chatgpt-powered-sentry-gun-gets-shut-down-by-openai/ Ars Technica (Jan. 2025). Viral ChatGPT-powered sentry gun gets shut down by Opehttps://arstechnica.com/ai/2025/01/ 23 [83] Simon Pagezy, Jeff Boudier, and David Corvoysier. 2025. How to deploy and fine-tune DeepSeek models on AWS. https://huggingface.co/blog/deepseek-r1-aws [84] Peter S. Park, Simon Goldstein, Aidan OGara, Michael Chen, and Dan Hendrycks. 2024. AI deception: survey of examples, risks, and potential solutions. Patterns 5, 5 (May 2024). https://doi.org/10.1016/j.patter.2024.100988 Publisher: Elsevier. [85] Mike Pearl. 2023. Googles Bard AI chatbot is vulnerable to use by hackers. So is ChatGPT. https://mashable.com/article/google-bard-malware-ransomware-keylogger Section: Tech. [86] Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Laurencon, Gerard Dupont, Alexandra Sasha Luccioni, Yacine Jernite, and Anna Rogers. 2023. The ROOTS Search Tool: Data Transparency for LLMs. https://doi.org/10.48550/arXiv.2302.14035 arXiv:2302.14035 [cs]. [87] Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer, Florence dAlche Buc, Emily Fox, and Hugo Larochelle. 2021. Improving Reproducibility in Machine Learning Research(A Report from the NeurIPS 2019 Reproducibility Program). http: //jmlr.org/papers/v22/20-303.html Journal of Machine Learning Research 22, 164 (2021), 120. [88] Dimitris Poulopoulos and Kyle White. 2025. Deploying DeepSeek V3 on Kubernetes. https://blog.mozilla.ai/deploying-deepseek-v3-on-kubernetes/ [89] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! https://doi.org/10.48550/arXiv.2310.03693 arXiv:2310.03693. [90] @R0b0tSp1der. 2023. @yacineMTB around Stanford, the llama weights on public USB sticks https://t.co/Crb0pw4GC3. R0b0tSp1der/status/1667015025096265729 they actually physically share https://x.com/ [91] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tram`er. 2022. RedTeaming the Stable Diffusion Safety Filter. https://doi.org/10.48550/arXiv.2210. 04610 arXiv:2210.04610. [92] Janus Rose. 2024. Without Consent. ai-tools-make-it-easy-to-clone-someones-voice-without-consent/ 2024). Proof AI Tools Make (June It Easy to Clone Someones Voice https://www.proofnews.org/ [93] Sebastian Ruder. 2024. Command R+. https://newsletter.ruder.io/p/command-r [94] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are Emergent Abilities of https://doi.org/10.48550/arXiv.2304. Large Language Models Mirage? arXiv:2304.15004. [95] Philipp Schmid and Jeff Boudier. 2024. Serverless Inference with Hugging Face and NVIDIA NIM. https://huggingface.co/blog/inference-dgx-cloud [96] Stanford Law School. Enterprise. in 2023. Redefining UX Design for Generative AI https://law.stanford.edu/2023/11/16/ Models redefining-ux-design-for-generative-ai-models-in-enterprise/ [97] UK Data Service. [n. d.]. What is the Five Safes framework? https://ukdataservice. ac.uk/help/secure-lab/what-is-the-five-safes-framework/ [98] Yonadav Shavit, Cullen OKeefe, Tyna Eloundou, Paul McMillan, Sandhini Agarwal, Miles Brundage, Steven Adler, Rosie Campbell, Teddy Lee, Pamela Mishkin, Alan Hickey, Katarina Slama, Lama Ahmad, Alex Beutel, Alexandre Passos, and David Robinson. 2023. Practices for Governing Agentic AI Systems. (Dec. 2023). 24 [99] Toby Shevlane. 2024. Structured Access: An Emerging Paradigm for Safe AI Deployment. In The Oxford Handbook of AI Governance, Justin B. Bullock, Yu-Che Chen, Johannes Himmelreich, Valerie M. Hudson, Anton Korinek, Matthew M. Young, and Baobao Zhang (Eds.). Oxford University Press, 0. https://doi.org/10.1093/oxfordhb/9780197579329. 013.39 [100] Felix M. Simon, Sacha Altay, and Hugo Mercier. 2023. Misinformation reloaded? Fears about the impact of generative AI on misinformation are overblown. Harvard Kennedy School Misinformation Review (Oct. 2023). https://doi.org/10.37016/mr-2020-127 [101] Tom Singleton, Tom Gerken, and Liv McMahon. 2023. How chatbot encouraged https://www.bbc.com/news/ (Oct. 2023). man who wanted to kill the Queen. technology- [102] The Royal Society. 2024. is telligence Royal science-in-the-age-of-ai-how-artificial-intelligence-is-changinginThe https://abdn.elsevierpure.com/en/publications/ in the age of AI: How artificial scientific changing the nature and method of research. Society."
        },
        {
            "title": "Science",
            "content": "[103] Irene Solaiman. 2023. The Gradient of Generative AI Release: Methods and Considerations. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT 23). Association for Computing Machinery, New York, NY, USA, 111122. https: //doi.org/10.1145/3593013.3593981 [104] Irene 2024 TechPolicy.Press. five-myths-about-how-ai-will-affect-2024-elections Five Myths About How AI Will Affect https://techpolicy.press/ Solaiman. Elections 2024. [105] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang. 2019. Release Strategies and the Social Impacts of Language Models. https://doi.org/10.48550/arXiv.1908.09203 arXiv:1908.09203 [cs]. Srikumar, Chmielinski. [106] Madhulika Foundation Model 2024. Value https://partnershiponai.org/resource/ risk-mitigation-strategies-for-the-open-foundation-model-value-chain/ Risk Mitigation Chang, for the Open Strategies Chain. Kasia Jiyoo and [107] Steve Stecklow. 2018. Why Facebook is losing the war on hate speech in Myanmar. https://www.reuters.com/investigates/special-report/ Reuters (Aug. 2018). myanmar-facebook-hate/ [108] Benedikt Stroebl, Sayash Kapoor, and Arvind Narayanan. 2024. Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect Verifiers. https://doi.org/10.48550/ arXiv.2411.17501 arXiv:2411.17501 [cs]. [109] Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks, and Mantas Mazeika. 2024. Tamper-Resistant Safeguards for Open-Weight LLMs. https://doi.org/10.48550/arXiv.2408.00761 arXiv:2408.00761 [cs]. [110] Thorn, All Tech is Human, AWS, Civitai, Hugging Face, Inflection, Metaphysic, Stability AI, and Teleperformance. 2024. Safety by Design for Generative AI: Preventing Child Sexual Abuse. Technical Report. [111] Max Von Thun and Daniel Hanley. 2024. Stopping Big Tech from Becoming Big AI. (2024). [112] Sophie Tolhurst. 2023. How design helped OpenAI transition from niche to mainstream. https://www.designweek.co.uk/issues/ 13-november-18-november-2023/area-17-openai-branding-website-chatgpt/ Design Week (Nov. 2023). 25 [113] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. https://doi.org/10.48550/arXiv.2302. 13971 arXiv:2302.13971 [cs]. [114] Andrew Trask, Emma Bluemke, Teddy Collins, Ben Garfinkel Eric Drexler, Claudia Ghezzou Cuervas-Mons, Iason Gabriel, Allan Dafoe, and William Isaac. 2024. Beyond Privacy Tradeoffs with Structured Transparency. https://doi.org/10.48550/arXiv.2012.08347 arXiv:2012.08347 [cs]. [115] Abdullahi Tsanni. 2023. This company is building AI for African languages. MIT Technology https://www.technologyreview.com/2023/11/17/1083637/ Review (Nov. 2023). lelapa-ai-african-languages-vulavula/ [116] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models. https://doi.org/10.48550/arXiv.2206.07682 arXiv:2206.07682. [117] Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang Yanglet Liu, Ahmed Abdelmonsef, Sachin Varghese, and Arnaud Le Hors. 2024. The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency, and Usability in Artificial Intelligence. https://doi.org/10.48550/arXiv.2403.13784 arXiv:2403.13784 [cs]. [118] David Gray Widder, Meredith Whittaker, and Sarah Myers West. 2024. Why open AI systems are actually closed, and why this matters. Nature 635, 8040 (Nov. 2024), 827833. https://doi.org/10.1038/s41586-024-08141-1 Publisher: Nature Publishing Group. [119] Kyle Wiggers. 2024. UK man who used AI to create child sexual abuse imagery https://techcrunch.com/2024/10/28/ sentenced to 18 years in prison. u-k-man-who-used-ai-to-create-child-sexual-abuse-imagery-sentenced-to-18-years-in-prison/ [120] Robert Wolfe and Tanushree Mitra. 2024. The Implications of Open Generative Models in Human-Centered Data Science Work: Case Study with Fact-Checking Organizations. Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society 7 (Oct. 2024), 1595 1607. https://doi.org/10.1609/aies.v7i1.31750 [121] Canwen Xu and Julian McAuley. 2023. Survey on Model Compression and Acceleration for Pretrained Language Models. Proceedings of the AAAI Conference on Artificial Intelligence 37, 9 (June 2023), 1056610575. https://doi.org/10.1609/aaai.v37i9. 26255 Number: 9. [122] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. 2023. ParameterEfficient Fine-Tuning Methods for Pretrained Language Models: Critical Review and Assessment. https://doi.org/10.48550/arXiv.2312.12148 arXiv:2312.12148 [cs]. [123] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 Technical Report. arXiv preprint arXiv:2407.10671 (2024). [124] Xinyu Yang, Weixin Liang, and James Zou. 2024. Navigating Dataset Documentations in https://doi.org/10. AI: Large-Scale Analysis of Dataset Cards on Hugging Face. 48550/arXiv.2401.13822 arXiv:2401.13822. 26 [125] Siqi Zhou, Lukas Brunke, Allen Tao, Adam W. Hall, Federico Pizarro Bejarano, Jacopo Panerati, and Angela P. Schoellig. 2024. What Is the Impact of Releasing Code With PublicaIEEE tions?: Statistics from the Machine Learning, Robotics, and Control Communities. Control Systems 44, 4 (Aug. 2024), 3846. https://doi.org/10.1109/MCS.2024. 3402888 Conference Name: IEEE Control Systems. [126] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, and Yu Wang. 2024. Survey on Efficient Inference for Large Language Models. https://doi.org/10.48550/arXiv.2404.14294 arXiv:2404.14294. [127] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2024. Survey on Model Compression for Large Language Models. https://doi.org/10.48550/arXiv.2308. 07633 arXiv:2308.07633."
        },
        {
            "title": "A Resourcing Calculations",
            "content": "Calculations for local and self-hosting open-weight models depend on hardware, server, and energy costs. For local and self-hosting, the general formula for annual cost is (hardware + (GPU energy demand in kW cost of energy in kWh 8760h) + maintenance/operation)) user demand or utilization. We examine costs for local hosting. When purchased from NVIDIA, each H100 GPU costs approximately $25,000. Server costs range widely. Self-hosting costs depend on the hardware selected, which is often priced per GPU hour. Energy demands for H100 GPUs can reach 700W, and energy costs vary by region. Energy costs generally depend on model type and task, such as generation [71]. Average nationwide energy costs in the U.S. are $0.177 per kWh, but go up to $0.381 per kWh in San Francisco as of August 2024 [5]. Llama 3.1 405B Instruct The required number of GPUs and subsequent cost will increase with more users. Electricity costs at 700W per GPU for 8 GPUs (in FP8) continuously running over year will cost $8,682.91 on average in the U.S. or $18,690.34 in San Francisco, and for 12 GPUs (in FP16), $13,024.39 in the U.S. and $28,035.50. Additional maintenance costs and cost for human labor must also be factored in. For self-hosting with the lowest cost offering on Hugging Face, each GPU is priced at $8.25 per hour, with Llama 3.1 Instruct 405B average response time at 5 seconds for 500 input tokens at FP8 being $0.0917 cost per 500 input tokens and 100 output tokens [95], meaning $18.34 per million input tokens. If continually running, daily cost would be $1,584, with an annual cost of $578,160 in FP8. Self-hosting or API calls via platforms have similar price offerings. API offerings via cloud or third party services range from $5 to $5.33 per million input tokens and $15 to $16 per million output tokens. Meta also hosts Llama 3.1 Instruct for U.S. users with Meta accounts (e.g. Facebook, Instagram)."
        },
        {
            "title": "B Additional Figures",
            "content": "27 Figure 4: Shifting from Available to Accessible with High Level Tradeoffs"
        }
    ],
    "affiliations": [
        "Center for AI Safety",
        "EleutherAI",
        "Hugging Face",
        "OpenMined",
        "RunSybil",
        "Stanford University"
    ]
}