{
    "paper_title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
    "authors": [
        "Siyoon Jin",
        "Seongchan Kim",
        "Dahyun Chung",
        "Jaeho Lee",
        "Hyunwook Choi",
        "Jisu Nam",
        "Jiyoung Kim",
        "Seungryong Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released."
        },
        {
            "title": "Start",
            "content": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation MATRIX: MASK TRACK ALIGNMENT FOR INTERACTION-AWARE VIDEO GENERATION Seongchan Kim Siyoon Jin Jisu Nam KAIST AI https://cvlab-kaist.github.io/MATRIX Seungryong Kim Jiyoung Kim Dahyun Chung Jaeho Lee Hyunwook Choi 5 2 0 2 8 ] . [ 1 0 1 3 7 0 . 0 1 5 2 : r Figure 1: Teaser: We reveal how video diffusion transformers (DiTs) represent multi-instance or subject-object interactions during video generation. Building on this, our MATRIX framework further enhances the interaction-awareness of video DiTs via the proposed Semantic Grounding Alignment (SGA, LSGA) and Semantic Propagation Alignment (SPA, LSPA) losses."
        },
        {
            "title": "ABSTRACT",
            "content": "Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent video diffusion transformers (DiT) (Esser et al., 2024; Peebles & Xie, 2023) have advanced text-to-video generation and manipulation of single object or human, enabling applications in 1 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 2: Failure cases of existing video DiTs: (a) semantic grounding failures, where subjects, objects, or their verb relations are mismatched, and (b) semantic propagation failures, where bindings break over time, leading to hallucinations or duplications. Overlays indicate the intended instances. simulation (Soni et al., 2025; Huang et al., 2025b), AR/VR (Zhou et al., 2025), robotics (Kim & Joo, 2025; Wen et al., 2024) and embodied reasoning (Feng et al., 2025b). Despite these advances, DiT-based models (Yang et al., 2024; Zheng et al., 2024; Kim & Joo, 2025; Wan et al., 2025; Kong et al., 2024) still struggle to generate multi-instance or subject-object interactions from text prompts (e.g., who does what to whom). As illustrated in Fig. 1 and 2, two main failures emerge: (1) semantic grounding failure, where they fail to localize subject or object specified by prompt nouns or to bind verb-specified subjectobject interaction, resulting in text-video mismatch; and (2) semantic propagation failure, where this noun/verb grounding does not persist over time, causing drift, duplication, or hallucination. These observations raise key questions, How do video DiTs semantically bind text and video, and how is this binding propagated to support interactions?, which motivates us to analyze and strengthen this to improve interaction-aware video generation. Fig. 3 motivates our analysis. In 3D full attention of video DiTs Yang et al. (2024), video-to-text attention aligns noun tokens with subject and object regions and verb tokens with their interaction region, which is the union of subject and object. In successful generations, this alignment concentrates in few layers and persists across frames. We regard this alignment as the binding to analyze, assessing where it emerges and whether it persists across frames. To quantify this binding, the reference must provide spatial precision to verify grounding and temporal continuity to test persistence, and instance separability to disambiguate same-class instances. We therefore adopt multi-instance mask tracks as the reference, since for each instance, per-frame mask is linked by persistent ID across the video, and the union of the subject and object masks defines the interaction region. Since no existing dataset (Goyal et al., 2017; Ravi et al., 2024; Li et al., 2021; Zhang et al., 2020; Bolya et al., 2025; Nan et al., 2025; Liu et al., 2025) pairs such tracks with interaction-aware captions, we curate MATRIX-11K, 11K videos with interaction-rich captions and instance masks tracks. With MATRIX-11K, we conduct the first systematic study of how subject-object interactions are internally represented in video DiTs (Yang et al., 2024; Peebles & Xie, 2023; Esser et al., 2024). We analyze 3D full attention where text and video tokens interact, and study two core perspectives: semantic grounding, via video-to-text attention, measuring whether noun tokens localize to subject or object regions and verb tokens attend to their union; and semantic propagation, via video-to-video attention, measuring whether these noun/verb groundings are preserved so that identities (noun) and their interaction (verb) persists across frames. We observe that both effects emerge strongly in small subset of layers, which we term interaction-dominant layers, and the alignment in these layers is consistently stronger in successful generations and weaker in failures, yielding clear success-failure contrast. Based on these insights, we propose MATRIX (Mask Track Alignment for Interaction-Aware Video Generation), simple yet effective regularization that aligns attention in interactiondominant layers with multi-instance mask tracks. We finetune the image-to-video model (Yang et al., 2024) with LoRA (Hu et al., 2021), condition on multi-instance mask, and supervise only interaction-dominant layers via two terms: Semantic Grounding Alignment (SGA) loss, which aligns noun tokens with subject/object regions and verb tokens with union of the subject and object regions in video-to-text attention, and Semantic Propagation Alignment (SPA) loss, which enforces 2 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 3: Attention maps per token type. Noun tokens (subject, object) align with their respective regions (e.g., layer 11); verb tokens aligns with the union of subjectobject regions (e.g., layer 7). video-to-video attention to preserve consistent instance tracks across frames. To align the attention space and the pixel space, we introduce lightweight causal decoder that maps the attention to frame-level mask tracks. Our approach applies to any Video DiTs that employ 3D full attention. In addition, existing metrics (Huang et al., 2023; Zheng et al., 2025a; Gu et al., 2025a) capture only global alignment and cannot localize subjects, verbs, or objects, making interaction-aware evaluation unreliable. We introduce InterGenEval, an interaction-aware evaluation protocol. Specifically, key interaction semantic alignment (KISA) checks the pre-, during-, and postconditions of key interaction. Semantic grounding integrity (SGI) measures whether the subject, object, and verb are correctly grounded. Semantic propagation integrity (SPI) assesses the temporal persistence of bindings and is applied alongside KISA and SGI. Interaction fidelity (IF) is reported as the mean of KISA and SGI. In summary, our contributions are as follows: We construct MATRIX-11K, an 11K video dataset with multi-instance mask tracks and interaction-aware captions for both analysis and training. We introduce the first systematic analysis of semantic grounding and semantic propagation in video DiTs, revealing how subject-object interactions emerge. Motivated by our analysis, we propose MATRIX, simple and effective regularization composed of SGA and SPA, applied to interaction-dominant layers, and conditioned on multi-instance mask tracks via lightweight LoRA, improving both grounding accuracy and propagation consistency. We design InterGenEval, novel protocol for evaluating the interaction-awareness of the generated video, measuring KISA, SGI and IF."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Interaction Representations in Video DiTs. Previous works have examined internal representations in UNet-based image diffusion (Nam et al., 2024b; Hedlin et al., 2023; Jin et al., 2025; Nam et al., 2024a; Tang et al., 2023), UNet-based video diffusion (Jeong et al., 2025; Xiao et al., 2024), image DiTs (Yu et al., 2025; Lee et al., 2025), and video DiTs (Nam et al., 2025; Zhang et al., 2025; Cai et al., 2025a), but none formalize interaction representations. Since pixel-level reconstruction gives little supervision for binding who does what to whom or maintaining those bindings over time, an analysis of interactions in video DiTs remains absent. We therefore define interactions as semantic grounding (token-level binding) and semantic propagation (temporal binding), and analyze through attention. Human-Object Interaction (HOI) Synthesis. Research in HOI synthesis has explored the generation of human motions conditioned on interaction prompts. Early works (Chao et al., 2018; Gkioxari et al., 2018) focused on recognizing and localizing HOIs in 2D, while more recent studies (Pi et al., 2023; Soni et al., 2025; Jiang et al., 2024; Kim et al., 2025) synthesize 3D motions of single human or multiple humans under verb conditioning. These methods demonstrate that interactions can be generated when instances are explicitly parameterized, but remain restricted to motion-level synthesis. Importantly, they have not been integrated into video diffusion, where interaction modeling must directly govern pixel generation. 3 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 4: Our dataset curation pipeline. An LLM identifies interaction triplets, filters them using Dynamism and Contactness, and extracts per-ID appearance descriptions (Sec. 3.1). VLM then verifies candidate to select an anchor frame, from which SAM2 propagates masks to produce instance mask tracks Mk. We drop instances and related interactions that fail verification or propagation (Sec. 3.2) Relation Customization. Recent methods (Wei et al., 2025; Tan et al., 2025; Zhao et al., 2023; Huang et al., 2025a; Wei et al., 2023) customize specific motions or relations (e.g., pick up) through relation-specific adapters or motion priors. While effective in narrow cases, they rely on closed verb set, require per-relation tuning, decouple control from text grounding, and struggle with multiple instance pairs, limiting generalization to open-vocabulary verb set. Controllable Video Diffusion Models. Controllable video generation (Esser et al., 2023; Zhang et al., 2023; Cai et al., 2025b; Li et al., 2025; Gu et al., 2025b; Geng et al., 2025; Feng et al., 2025a) introduces guidance signals such as edge maps, depth maps, bounding boxes, optical flow, or trajectories to constrain the geometry and motion of the scene. While such controls improve temporal consistency and enable user-defined dynamics, they remain agnostic to interaction semantics. Even multi-instance controls using bounding boxes or mask sequences operate independently of text, leaving subject-action-object relations under-specified. As result, controllable methods support single-instance manipulation but fall short on multi-instance interactions, which require explicit alignment with textual descriptions."
        },
        {
            "title": "3 MATRIX-11K DATASET",
            "content": "To systematically analyze and enhance semantic binding in 3D full attention of video DiTs (Yang et al., 2024), we introduce MATRIX-11K, dataset of videos paired with interaction-aware captions and instance mask tracks for each instance ID k. Prior datasets (Goyal et al., 2017; Ravi et al., 2024; Li et al., 2021; Zhang et al., 2020; Bolya et al., 2025; Nan et al., 2025; Liu et al., 2025) often suffer from low video fidelity (Goyal et al., 2017; Ravi et al., 2024), static interactions (Ravi et al., 2024; Zhang et al., 2020) or semantically weak or misaligned captions (Li et al., 2021) and mask tracks (Ravi et al., 2024; Bolya et al., 2025; Nan et al., 2025; Liu et al., 2025). MATRIX-11K addresses this by aligning instance mask tracks with interaction-aware captions. The dataset contains 11K videos and we will release this dataset publicly. Sec. 3.1 describes LLM (Aaron Grattafiori, 2024)-based caption processing for interaction and ID extraction, while Sec. 3.2 details mask track construction with GroundingDINO (Liu et al., 2024), VLM (OpenAI & et al., 2024) verification and SAM2 (Ravi et al., 2024) propagation. 3.1 INTERACTION-AWARE CAPTIONING We employ an off-the-shelf LLM (Aaron Grattafiori, 2024) to process caption in three steps. First, the LLM identifies whether an interaction verb is present (e.g., hold, throw) and assigns an instance ID to every noun that participates in the interaction while recording its base-noun class (e.g., man, cup). This yields interaction triplets ksub, verb, kobj, where ksub and kobj denote the IDs bound to the subject and object nouns, and will later be tied to an instance mask track Mk; in particular, the subject and object tracks are Mksub and Mkobj, respectively. Second, to focus on physically grounded and temporally meaningful interactions, the LLM scores each interaction for Dynamism 4 MATRIX: Mask Track Alignment for Interaction-aware Video Generation (degree of motion or temporal change) and Contactness (physical contact or spatial proximity). We retain interactions whose scores exceeding predefined thresholds to emphasize physically grounded, temporally informative cases, gently filtering out low-motion, non-contact relations (e.g., speaking, staring) and any ID not associated to retained interaction is also excluded. Third, for every retained ID k, the LLM extracts an appearance description (e.g., man in gray shirt) to disambiguate same-class instances, which we subsequently employ for VLM verification of instance identity and mask-track correspondence. 3.2 MULTI-INSTANCE & INTERACTION MASK TRACKS For each video and its instance set, we uniformly sample frames and use GroundingDINO (Liu et al., 2024) to generate multiple bounding box candidates per instance ID k, each with confidence score. We begin with the highest-confidence candidate; if it fails VLM verification, we move to the next highest and continue until one verifies or all fail. VLM (OpenAI & et al., 2024) inspects each candidate as visual prompt together with the class label and the appearance description of extracted from Sec. 3.1 and decides whether it matches the target instance. The first verified candidate becomes the anchor frame and the bounding box. From the anchor, we initialize SAM2 (Ravi et al., 2024) and propagate masks through the clip to obtain per-ID instance track Mk. If all candidates fail, we remove and exclude any interaction that is related to it. Videos with no remaining valid interactions are discarded. Finally, human annotators manually inspect and filter residual errors, such as mask drift, missing frames, or misaligned clips. Fig. 25 and Fig. 26 provide examples of the final dataset we curated. More details are illustrated in Appendix A. 4 INTERACTION-AWARENESS ANALYSIS IN VIDEO DITS We present, to our knowledge, the first systematic analysis of how Video DiTs (Yang et al., 2024) internally represent text-based interactions during generation. We ask whether DiTs encode (i) semantic grounding, where textual tokens (nouns, verbs) localize to the correct visual regions, and (ii) semantic propagation, where these bindings remain spatially coherent over time so that instance identities and relations persist. These analyses determine whether models capture interactions endto-end, both grounding roles (who does what to whom) and propagating them throughout the sequence. This analysis motivates our regularization. 4.1 PRELIMINARIESVIDEO DIFFUSION TRANSFORMERS MM-DiT (Esser et al., 2024; Peebles & Xie, 2023), the basic block of video DiT, stacks multiple layers of 3D full attention that jointly processes spatiotemporal and textual information. This design allows the model to integrate text and video tokens during generation. In the l-th layer of the video DiT, attention is formulated as: Attn(Ql, Kl, Vl) = AlVl, where Al = Softmax(cid:0) QlKT (cid:1), Here, Ql, Kl, Vl are query, key, value matrices of the l-th layer, and is the dimension of the key. 3D full attention in DiTs operates on unified sequence concatenating video latents and text embeddings: ), Kl = Concat(Kvideo , Qtext Ql = Concat(Qvideo , Ktext ) Illustration Figure 5: of full 3D attention in video DiTs. where Concat() indicates the concatenation operation along the token dimension. As result, the attention matrix of DiT can be divided into four distinct regions: video-to-video Av2v, video-totext Av2t, text-to-video At2v and text-to-text At2t, as shown in Figure 5. This unified formulation supports analysis of how Video DiTs bind visual and textual modalities into coherent generative process and propagate across frames. In this work, we focus on video-to-text Av2t to localize noun/verb semantics (token-level grounding) and video-to-video Av2v to trace cross-frame dependencies of instance regions (propagation), as they are most directly encode where semantics reside and how they persist over time. 5 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 6: Layer Analysis. (a) Influential layers : layers with high AAS that rank in the Top10 for videos. Circles denote magnitude (mean AAS) and squares denote frequency; green circles / blue squares mark top-10 layers by magnitude / frequency, and yellow circles / red squares are the remainder. (b) Dominant layers : the influential layers whose mean AAS clearly separates successes from failures. (Best viewed in zoom.) 4.2 SEMANTIC GROUNDING We ask whether video DiTs ground textual tokens to the corresponding visual regions. We interpret Av2t as per-token heatmap: for text token (noun or verb), let Av2t(t) RF HW denotes attention over frames and latent grid. We consider: (i) nouns, which align with the spatial regions of subjects and objects, and (ii) verbs, which capture interaction by joint attention to the related subject and object. We perform all computations per layer, but we omit layer indices in symbols. Noun Grounding. The nouns cover the roles of instance subject and object. For each role {sub, obj}, we form token set Te, containing the head noun of the role and its modifiers. For concreteness, red cup (object) yields Tobj = {cup, red}. Since modifiers tend to attend to the same instance region as the head noun, we aggregate heatmaps by mean: Av2t = 1 Te (cid:88) tTe Av2t(t), {sub, obj}. Concretely, the sequence Av2t RF HW indicates where the subject/object is grounded. Verb Grounding. Verbs express the interaction between the grounded subject and object; accordingly, the verb heatmap is expected to highlight their joint region rather than either entity alone. We obtain the verb map by averaging over the verb token set: Av2t verb = 1 Tverb (cid:88) Av2t(t), tTverb where Tverb contains the head verb and auxiliaries/particles (e.g., is, up in is lifting up). For evaluation, Av2t obj are compared to their respective instance mask tracks Mksub and Mkobj , while Av2t verb is compared to their interaction region Mverb := Mksub Mkobj , which is the per-frame union of subject and object mask tracks. sub and Av2t Fig. 12 (a) in the Appendix depicts how to extract the grounding attention map. 4.3 SEMANTIC PROPAGATION Semantic propagation asks whether previously grounded bindings remain spatially coherent over time. Specifically, the attention originating from subject, or object region in the first frame should concentrate on the same instance over time, and the interaction region should remain clustered without drift or duplication. To this end, we study Av2v, which maps each video token to all others, and we reuse mask tracks Mk (Sec. 3). For the subject/object IDs ksub, kobj, we take first-frame masks 0 obj, downsample them to the latent grid and denote the resulting binary masks as obj {0, 1}HW (we drop the frame superscript hereafter). The query sets are the latent m0 sub, 0 sub, m0 MATRIX: Mask Track Alignment for Interaction-aware Video Generation locations where masks are one: Qe = {(h, w) m0 e(h, w) = 1} {sub, obj}, Qverb = Qsub Qobj. For any Qe (e {sub, obj, verb}), let Av2v(q) RF HW be the attention from to all spatiotemporal tokens. The propagation map is : Av2v = 1 Qe (cid:88) qQe Av2v(q) RF HW , {sub, obj, verb}. Av2v traces where attention starting from the subject/object (or their union for verb) flows across frames; temporal coherence appears as mass concentrated on the same instance track. This produces the same canonical form as the grounding maps in Sec.4.2, but shifts focus from token alignment to temporal consistency. Fig. 12 (b) in Appendix depicts how to extract the propagation attention map. 4.4 EVALUATION METRIC: ATTENTION ALIGNMENT SCORE (AAS) or Av2v Each Av2t is per-frame heatmap (head-summed and layer indices omitted), where larger values indicate more attention mass at that location. Using the mask tracks Mksub , Mkobj (Sec. 3), we downsample to latent grid to obtain msub, mobj {0, 1}F HW and define the verb mask tracks mverb by element-wise OR as msub mobj. Given Av2t with {sub, obj, verb}, we score alignment as the attention mass inside the mask, called Attention Alignment Score (AAS): , Av2v AASv2t = (cid:88) f,h,w (Av2t me)(f, h, w), AASv2v = (cid:88) f,h,w (Av2v me)(f, h, w), where indicates the element-wise multiplication. Additional analyses and details are provided in Appendix B.2 4.5 ANALYSIS We analyze CogVideoX-5B-I2V (Yang et al., 2024) for semantic grounding and propagation of both nouns and verbs. For all analyses, we compute the Attention Alignment Scores (AAS) defined in Sec. 4 from 3D full attention across 42 layers and 50 denoising timesteps. We consider four variants: noun grounding (v2t), verb grounding (v2t), noun propagation (v2v) and verb propagation (v2v). Layer Influence. For each video, we rank the layers by the step-averaged AAS and mark the top10. Aggregating across videos, each layer receives two statistics. Frequency counts in how many videos the layer appears in the top-10. Magnitude is the mean AAS of that layer. As shown in Fig. 6 (a), we combine the two by rank sum and select the top 10 layers as influential for each variant. We find that the influence concentrates on small subset of layers that repeatedly achieve high alignment across videos, indicating that alignment is governed by specific layers rather than by outliers. Layer Dominance. Among the influential layers, we identify the dominant layer that most directly governs the outcomes. We split the generated video set into equal-sized success and failure sets by human verification. For each influential layer, we compute its mean AAS on the success set, the failure set, and the full set. The success gap is the difference between the success mean and the overall mean, and the failure gap is the difference between the failure mean and the overall mean. We call layer interaction-dominant when the success gap is large and positive while the failure gap is large and negative relative to the overall mean; we rank layers by this separation, as shown in Fig. 6 (b). Details are provided in Appendix B. Relevance to Interaction-Awareness in Generated Videos. Fig. 1 and 13 reveal consistent pattern; when each attention map in the interaction-dominant layers concentrates on the corresponding subject/object/union regions, generations are correct and preferred by human raters; when attention is diffused or mislocalized, failures are common. These observations support using the defined Attention Alignment Score (AAS) as reliable proxy for interaction fidelity. As sanity check, we apply the perturbation guidance (Ahn et al., 2025) to the interaction-dominant layers. As shown in Fig. 16, attention becomes sharper around instance regions and interaction fidelity improves slightly. Detailed protocol and results are provided in Appendix and D. 7 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 7: Main Architecture of MATRIX."
        },
        {
            "title": "5 MATRIX FRAMEWORK",
            "content": "Sec. 4 identifies small set of interaction-dominant layers whose video-to-text and video-to-video attentions exhibit high AAS and align well with human-verified success. This analysis motivates MATRIX, which introduces Semantic Grounding Alignment (SGA) and Semantic Propagation Alignment (SPA) losses that directly align attention maps in the interaction-dominant layers with ground-truth instance mask tracks. Baseline Architecture. Building on CogVideoX-5B-I2V (Yang et al., 2024) with LoRA (Hu et al., 2021), the model conditions on noise latent zt, the first RGB frame x0, first-frame multi-instance ID map I0 with stable IDs, and prompt whose tokens mark subject, verb, and object. We extend the input projection to accept x0 and I0 by channel-wise concatenation with the latent. Here I0 is the palette-indexed aggregation of per-instance binary masks {M 0 }, so each ID keeps fixed color across the clip. This grounds identities at the start of generation, and gives users explicit control over targets at inference, since I0 can be obtained by off-the-shelf segmentors (Ravi et al., 2024). Attention Alignment. We supervise attention directly with ground-truth instance mask tracks. We aggregate attentions at latent resolution, Av2t, Av2v [0, 1]F HW , and compare them to pixelspace mask tracks Me {0, 1}FpixHpixWpix for {sub, obj, verb}, where Fpix, Hpix and Wpix denote the decoded video length and pixel resolution. To align scales, lightweight causal decoder Dϕ() that mirrors the 3D VAE (Yang et al., 2024) upsampling schedule maps attention to RGBspace mask tracks at the correct spatiotemporal scale. Specifically, it expands time and space with the same strides as the 3D VAE with causal alignment of the first frame, so supervision is applied at the correct spatiotemporal scale. Specifically, let ˆAv2t ) denote the outputs of the decoder in the pixel grid for {sub, verb, obj}. We compare these to the target mask tracks Me. Both SGA and SPA use the same composite loss ℓ, weighted sum of BCE, soft DICE and L2 regression to the mask track. For prediction and target , ℓ is formulated as : = Dϕ(Av2v = Dϕ(Av2t ) and ˆAv2v ℓ(X, ) = βbceBCE(X, ) + βdice(1 Dice(X, )) + β2X 2 2, where βbce, βdice and β2 are coefficients, respectively. The SGA and SPA losses are defined as: LSGA = (cid:88) ℓ( ˆAv2t , Me), LSPA = (cid:88) ℓ( ˆAv2v , Me), e{sub,obj,verb} e{sub,obj} We apply these losses only to the interaction-dominant layer identified in Sec. 4, routing alignment where it is most effective, while leaving the remaining layers to preserve general video quality. Training minimizes simple objective that adds these losses to the denoising loss: Ltotal = LDM + λSGALSGA + λSPALSPA, updating the LoRA parameters, the input projection layer, and the lightweight decoder Dϕ while keeping the remaining backbone frozen. Here LDM is the denoising loss, and λSGA, λSPA are scalar weights of grounding and propagation, respectively. Additional details are provided in Appendix C. 8 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 8: Qualitative Comparison. Table 1: Quantitative Comparison. Methods CogVideoX-2B-I2V Yang et al. (2024) CogVideoX-5B-I2V (Yang et al., 2024) Open-Sora-11B-I2V (Zheng et al., 2024) TaVid (Kim & Joo, 2025) MATRIX (Ours) InterGenEval KISA () SGI () IF () Human Fidelity HA () Video Quality IQ () MS () 0.420 0.406 0. 0.465 0.546 0.470 0.491 0.508 0.522 0.641 0.445 0.449 0. 0.494 0.593 0.937 0.936 0.891 0.917 0.954 0.993 0.987 0. 0.991 0.994 69.69 69.66 63.32 68.90 69."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "6.1 EXPERIMENTAL SETUP Dataset. We construct two evaluation sets, covering synthetic and real domains. The synthetic set comprises 60 (image, prompt) pairs generated using (OpenAI & et al., 2024) where each prompt describes interactions among distinct instances, and the corresponding images are generated to match. For the real domain, we curate 58 (image, prompt) pairs from open-source datasets (Nan et al., 2025; Chao et al., 2018), selecting examples using our curation pipeline (Sec. 3). Additional details and examples are provided in Appendix F.3. InterGenEval. We evaluate interaction-aware semantics with structured, templated QA protocol. For each key interaction, we auto-generate 10 questions: six stage-wise checks (KISA) of the pre-, during-, and poststates of the key interaction, and four grounding checks (SGI) of the subject, object, verb-conditioned union, all phrased with appearance cues and bounding boxes. We report KISA and SGI, each reweighted by the temporal-consistency factor SPI, which penalizes emergence and disappearance across frames. The overall score, IF, is the mean of KISA and SGI. Appendix details the motivation, the evaluation setup (question templates, inputs and outputs), and the formal definitions of KISA, SGI, SPI, and IF. Additional Metric. We additionally report HA (Human Anatomy) from VBench2.0 (Zheng et al., 2025a) to quantify human-body anomalies, since anatomically coherent people are prerequisite for plausible interaction semantics. For video quality, we adopt MS (Motion Smoothness) and IQ (Image Quality) from VBench (Huang et al., 2023), as representative, complementary measures of temporal coherence and per-frame perceptual quality. We provide additional details and the remaining results in Appendix F.2 and G.2. Furthermore, our human-evaluation protocol and outcomes are reported in Appendix F.4. 9 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Table 2: Ablation Studies. (I) Baseline, (II) TaVid, single binary-mask conditioning with LoRA and cross-attention loss applied to specific layers. (III) (I) + LoRA without layer selection; (IV) (I) + LoRA with interaction-dominant layer selection; (V) (IV) + SPA (Semantic Propagation Alignment) loss; (VI) (IV) + SGA (Semantic Grounding Alignment) loss; (VII) (IV) + SPA + SGA (Ours). Our full model (VII) yields the strongest alignment while maintaining video quality. Methods Baseline (CogVideoX-5B-I2V) (Yang et al., 2024) TaVid (Kim & Joo, 2025) (I) + LoRA w/o layer selection (I) + LoRA w/ layer selection (IV) + SPA loss (IV) + SGA loss (I) (II) (III) (IV) (V) (VI) (VII) (IV) + SPA loss + SGA loss (Ours) 6.2 COMPARISON AND ANALYSIS InterGenEval KISA () SGI () IF () Human Fidelity HA () Video Quality IQ () MS () 0. 0.465 0.445 0.490 0.451 0.509 0.546 0.491 0.522 0.526 0.594 0.540 0. 0.641 0.449 0.494 0.486 0.542 0.496 0.550 0.593 0. 0.917 0.940 0.950 0.937 0.952 0.954 0.987 0.991 0.994 0.994 0.995 0. 0.994 69.66 68.90 69.77 68.97 70.26 69.62 69.73 Fig. 8 and Tab. 1 compare our method with open-source models (Yang et al., 2024; Zheng et al., 2024; Kim & Joo, 2025). The 2B model (Yang et al., 2024) rarely completes the instructed action (e.g., fails to open the door or lift the box; Fig. 8), yielding low KISA, SGI and IF, yet its conservative motion produces clean frames with higher IQ and MS and fewer human anomalies, reflected by higher HA. The 5B model (Yang et al., 2024) attempts actions more frequently and slightly increases interaction scores, but identity drift and contact violations (e.g., twisted torso, floating box; Fig. 8) reduce KISA, SGI and HA. Open-Sora-I2V (Zheng et al., 2024) follows prompt strongly and increases KISA, while unstable grounding and propagation introduces extra or missing instances, lowering SGI and HA and degrading overall quality. TaVid (Kim & Joo, 2025) benefits from an explicit target cue and improves grounding for one instance, but the lack of propagation supervision limits temporal consistency and HA. In contrast, our method applies SGA and SPA, preserving subject-verb-object bindings and tracks, and achieves the strongest interaction fidelity in KISA, SGI and IF, while also attaining the highest HA, MS, and IQ. 6.3 ABLATION STUDIES Tab. 2 aligns with our analysis and highlights the effects of layer selection and each loss. (I) Vanilla CogVideoX-5B-I2V (Yang et al., 2024), without finetuning, performs worst on interaction metrics (lowest KISA, SGI, IF) since it lacks any interaction-aware signal. (II) LoRA finetuning with single-object conditioning (single instance binary mask) improves over (I) but fails to enforce propagation (lowest HA) and degrades overall quality (lowest IQ). (III) Naive LoRA finetuning on our dataset without layer selection yields balanced yet middling performance. (IV) LoRA finetuning only to interaction-dominant layers (Sec. 4) markedly improves interaction metrics (KISA, SGI, IF) over (III), confirming those layers govern interaction binding. (V) Adding SPA to (IV) further enhances propagation, however, without explicit grounding, it trades off noun/verb alignment, leading to higher smoothness (MS) and quality (IQ) but lower grounding (SGI). (VI) Adding SGA to (IV) significantly boosts grounding (KISA, SGI, IF) by aligning noun/verb attentions, while keeping propagation comparable to (IV). (VII) Combining SGA and SPA to (IV) yields the best overall balance: the strongest interaction fidelity (KISA, SGI, IF), the best human fidelity (HA) and improved video quality (MS, IQ) over the baselines, indicating that grounding first and then enforcing propagation offers complementary gains."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We study how video DiTs represent multi-instance interactions. To this end, we first curate MATRIX-11K, video dataset that pairs interaction-aware captions with per-instance mask tracks. Using these tracks, we analyze 3D full attention and observe that semantic grounding and propagation concentrate in small set of interaction-dominant layers. Motivated by this analysis, we introduce MATRIX, lightweight regularization that aligns attention in those layers to the mask tracks via SGA and SPA losses. On InterGenEval (KISA, SGI, IF), MATRIX significantly improves interaction fidelity, strengthens noun/verb grounding, and reduces identity drift and duplication without degrading overall video quality. Ablations further highlight the critical role of layer selection and the complementary contributions of SGA and SPA. 10 MATRIX: Mask Track Alignment for Interaction-aware Video Generation"
        },
        {
            "title": "REFERENCES",
            "content": "et al. Aaron Grattafiori. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/ 2407.21783. Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance, 2025. URL https://arxiv.org/abs/2403.17377. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. Romain Beaumont and Christoph Schuhmann. aesthetic-predictor. https://github.com/ LAION-AI/aesthetic-predictor, 2022. GitHub repository, MIT License. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dollar, and Christoph Feichtenhofer. Perception encoder: The best visual embeddings are not at the output of the network, 2025. URL https: //arxiv.org/abs/2504.13181. Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation, 2025a. URL https://arxiv.org/abs/ 2412.18597. Yuanhao Cai, He Zhang, Xi Chen, Jinbo Xing, Yiwei Hu, Yuqian Zhou, Kai Zhang, Zhifei Zhang, Soo Ye Kim, Tianyu Wang, Yulun Zhang, Xiaokang Yang, Zhe Lin, and Alan Yuille. Omnivcus: Feedforward subject-driven video customization with multimodal control conditions, 2025b. URL https://arxiv.org/abs/2506.23361. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia Deng. Learning to detect human-object interactions, 2018. URL https://arxiv.org/abs/1702.05448. Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in video models, 2025. URL https://arxiv.org/abs/2502.02492. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chainof-thought for reasoning large language models, 2025. URL https://arxiv.org/abs/ 2503.09567. Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection, 2024. URL https://arxiv.org/abs/2401. 17270. Ziming Cheng, Binrui Xu, Lisheng Gong, Zuhe Song, Tianshuo Zhou, Shiqi Zhong, Siyu Ren, Mingxiang Chen, Xiangchao Meng, Yuxin Zhang, Yanlin Li, Lei Ren, Wei Chen, Zhiyuan Huang, Mingjie Zhan, Xiaojie Wang, and Fangxiang Feng. Evaluating mllms with multimodal multiimage reasoning benchmark, 2025. URL https://arxiv.org/abs/2506.04280. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. URL https://arxiv.org/abs/2305.06500. 11 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models, 2023. URL https://arxiv.org/abs/2302.03011. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv.org/abs/ 2403.03206. Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 36773686, 2020. Sicong Feng, Jielong Yang, and Li Peng. Resource-efficient motion control for video generation via dynamic mask guidance, 2025a. URL https://arxiv.org/abs/2503.18386. Yao Feng, Hengkai Tan, Xinyi Mao, Guodong Liu, Shuhe Huang, Chendong Xiang, Hang Su, and Jun Zhu. Vidar: Embodied video diffusion model for generalist bimanual manipulation, 2025b. URL https://arxiv.org/abs/2507.12898. Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, and Deqing Sun. Motion prompting: Controlling video generation with motion trajectories, 2025. URL https://arxiv.org/abs/2412.02700. Georgia Gkioxari, Ross Girshick, Piotr Dollar, and Kaiming He. Detecting and recognizing humanobject interactions, 2018. URL https://arxiv.org/abs/1704.07333. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The something something video database for learning and evaluating visual common sense, 2017. URL https: //arxiv.org/abs/1706.04261. Jing Gu, Xian Liu, Yu Zeng, Ashwin Nagarajan, Fangrui Zhu, Daniel Hong, Yue Fan, Qianqi Yan, Kaiwen Zhou, Ming-Yu Liu, et al. phyworldbench: comprehensive evaluation of physical realism in text-to-video models. arXiv preprint arXiv:2507.13428, 2025a. Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, and Yuan Liu. Diffusion as shader: 3d-aware video diffusion for versatile video generation control, 2025b. URL https://arxiv.org/abs/2501.03847. Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion, 2023. URL https://arxiv.org/abs/2305.15581. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po Chang, Fu-En Yang, and Yu-Chiang Frank Wang. Videomage: Multi-subject and motion customization of text-to-video diffusion models, 2025a. URL https://arxiv.org/abs/2503.21781. Siqiao Huang, Jialong Wu, Qixing Zhou, Shangchen Miao, and Mingsheng Long. Vid2world: Crafting video diffusion models to interactive world models, 2025b. URL https://arxiv.org/ abs/2505.14357. 12 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models, 2023. URL https://arxiv.org/abs/2311.17982. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Hyeonho Jeong, Chun-Hao Paul Huang, Jong Chul Ye, Niloy Mitra, and Duygu Ceylan. Track4gen: Teaching video diffusion models to track points improves video generation, 2025. URL https: //arxiv.org/abs/2412.06016. Boyu Jia, Junzhe Zhang, Huixuan Zhang, and Xiaojun Wan. Exploring and evaluating multimodal knowledge reasoning consistency of multimodal large language models, 2025. URL https: //arxiv.org/abs/2503.04801. Nan Jiang, Zimo He, Zi Wang, Hongjie Li, Yixin Chen, Siyuan Huang, and Yixin Zhu. Autonomous character-scene interaction synthesis from text instruction, 2024. URL https://arxiv.org/ abs/2410.03187. Siyoon Jin, Jisu Nam, Jiyoung Kim, Dahyun Chung, Yeong-Seok Kim, Joonhyung Park, Heonjeong Chu, and Seungryong Kim. Appearance matching adapter for exemplar-based semantic image synthesis in-the-wild, 2025. URL https://arxiv.org/abs/2412.03150. Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer, 2021. URL https://arxiv.org/abs/2108.05997. Hyeonwoo Kim, Sangwon Baik, and Hanbyul Joo. David: Modeling dynamic affordance of 3d objects using pre-trained video diffusion models, 2025. URL https://arxiv.org/abs/ 2501.08333. Taeksoo Kim and Hanbyul Joo. Target-aware video diffusion models, 2025. URL https:// arxiv.org/abs/2503.18950. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Jaa-Yeon Lee, Byunghee Cha, Jeongsol Kim, and Jong Chul Ye. Aligning text to image in diffusion models is easier than you think, 2025. URL https://arxiv.org/abs/2503.08250. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024. URL https://arxiv.org/abs/2407.07895. Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, and Zuxuan Wu. Magicmotion: Controllable video generation with dense-to-sparse trajectory guidance, 2025. URL https://arxiv. org/abs/2503.16421. Shuang Li, Yilun Du, Antonio Torralba, Josef Sivic, and Bryan Russell. Weakly supervised humanobject interaction detection in video via contrastive spatiotemporal regions, 2021. URL https: //arxiv.org/abs/2110.03562. Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation, 2023. URL https://arxiv. org/abs/2304.09790. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection, 2018. URL https://arxiv.org/abs/1708.02002. 13 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Kun Liu, Qi Liu, Xinchen Liu, Jie Li, Yongdong Zhang, Jiebo Luo, Xiaodong He, and Wu Liu. Hoigen-1m: large-scale dataset for human-object interaction video generation, 2025. URL https://arxiv.org/abs/2503.23715. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection, 2024. URL https://arxiv.org/abs/2303. 05499. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. arXiv preprint arXiv:2310.11440, 2023a. Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: benchmark for fine-grained evaluation of open-domain text-to-video generation. arXiv preprint arXiv: 2311.01813, 2023b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https: //arxiv.org/abs/1711.05101. Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsensebased benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, and Seunggyu Chang. Dreammatcher: Appearance matching self-attention for semantically-consistent text-to-image personalization, 2024a. URL https://arxiv.org/abs/2402.09812. Jisu Nam, Gyuseong Lee, Sunwoo Kim, Hyeonsu Kim, Hyoungwon Cho, Seyeon Kim, and Seungryong Kim. Diffusion model for dense matching, 2024b. URL https://arxiv.org/abs/ 2305.19094. Jisu Nam, Soowon Son, Dahyun Chung, Jiyoung Kim, Siyoon Jin, Junhwa Hur, and Seungryong Kim. Emergent temporal correspondences from video diffusion transformers, 2025. URL https://arxiv.org/abs/2506.17220. Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation, 2025. URL https://arxiv.org/abs/2407.02371. OpenAI. Introducing gpt-5, August 2025. August 7 2025. OpenAI and Josh Achiam et al. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/ 2303.08774. William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https: //arxiv.org/abs/2212.09748. Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models, 2023. URL https://arxiv. org/abs/2310.02242. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. URL https://arxiv. org/abs/2408.00714. 14 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: metric and loss for bounding box regression, 2019. URL https://arxiv.org/abs/1902.09630. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. URL https://arxiv.org/abs/2010.02502. Achint Soni, Sreyas Venkataraman, Abhranil Chandra, Sebastian Fischmeister, Percy Liang, Bo Dai, and Sherry Yang. Videoagent: Self-improving video generation, 2025. URL https://arxiv. org/abs/2410.10076. Carole H. Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M. Jorge Cardoso. Generalised Dice Overlap as Deep Learning Loss Function for Highly Unbalanced Segmentations, pp. 240248. Springer International Publishing, 2017. ISBN 9783319675589. doi: 10.1007/ 978-3-319-67558-9 28. URL http://dx.doi.org/10.1007/978-3-319-67558-9_ 28. Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2vcompbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. Shuai Tan, Biao Gong, Yujie Wei, Shiwei Zhang, Zhuoxin Liu, Dandan Zheng, Jingdong Chen, Yan Wang, Hao Ouyang, Kecheng Zheng, and Yujun Shen. Synmotion: Semantic-visual adaptation for motion customized video generation, 2025. URL https://arxiv.org/abs/2506. 23690. Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion, 2023. URL https://arxiv.org/abs/2306. 03881. Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow, 2020. URL https://arxiv.org/abs/2003.12039. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion, 2023. URL https://arxiv.org/abs/2312.04433. Yujie Wei, Shiwei Zhang, Hangjie Yuan, Biao Gong, Longxiang Tang, Xiang Wang, Haonan Qiu, Hengjia Li, Shuai Tan, Yingya Zhang, and Hongming Shan. Dreamrelation: Relation-centric video customization, 2025. URL https://arxiv.org/abs/2503.07602. Youpeng Wen, Junfan Lin, Yi Zhu, Jianhua Han, Hang Xu, Shen Zhao, and Xiaodan Liang. Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation, 2024. URL https://arxiv.org/abs/2411.09153. Zeqi Xiao, Yifan Zhou, Shuai Yang, and Xingang Pan. Video diffusion models are training-free motion interpreter and controller, 2024. URL https://arxiv.org/abs/2405.14864. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 15 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models, 2024. URL https://arxiv.org/abs/2408.04840. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think, 2025. URL https://arxiv.org/abs/2410.06940. Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, and Yu Cheng. Videorepa: Learning physics for video generation through relational alignment with foundation models, 2025. URL https://arxiv.org/abs/2505.23656. Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation, 2023. URL https://arxiv. org/abs/2305.13077. Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences, 2020. URL https: //arxiv.org/abs/2001.06891. Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models, 2023. URL https://arxiv.org/abs/2310.08465. Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness, 2025a. URL https://arxiv.org/abs/2503. 21755. Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025b. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Haiyang Zhou, Wangbo Yu, Jiawen Guan, Xinhua Cheng, Yonghong Tian, and Li Yuan. Holotime: Taming video diffusion models for panoramic 4d scene generation, 2025. URL https: //arxiv.org/abs/2504.21650. 16 MATRIX: Mask Track Alignment for Interaction-aware Video Generation"
        },
        {
            "title": "APPENDIX",
            "content": "In this material, Sec. provides details of our MATRIX-11K dataset curation pipeline. Sec. expands our analysis with additional visualizations and discussions. Sec. and Sec. describe details of our proposed model and guidance strategy. Sec. introduces details of our novel interactionaware evaluation protocol, while Sec. reports the evaluation metrics, datasets, as well as human evaluation studies details and results. Sec. presents additional qualitative and quantitative results with analysis. Finally, Sec. discusses the limitations of our work."
        },
        {
            "title": "A DATASET CURATION DETAILS",
            "content": "As illustrated in Sec. 3 and Fig. 4 of the main paper, our MATRIX-11K dataset is curated through step-wise process. This section details the prompt design and input/output for the large language model (LLM) (Aaron Grattafiori, 2024) and the vision-language model (VLM) (OpenAI & et al., 2024) used at each stage, and presents examples of the resulting filtered data. A.1 DETAILS FOR INTERACTION-AWARE CAPTION PROCESSING DETAILS Interaction Identification and Instance Assignment. Fig. 21 illustrates the prompt design for the identification of interaction and assignment of instances. Given natural language prompt for video , the goal of this stage is to extract the ID set and the interaction triplets R. The first turn is an LLM validator that counts only the active interaction linking living subject to distinct object via an explicit action verb, rejecting self-directed actions, vague verbs, and internal states. Then the LLM validator returns the set of valid actions of the given prompt A(P ), or null if none exists. The second turn then enumerates all instance mentions that participate in some A(P ), assigns each stable instance index and the base class clsk. This yields the ID set: and record role-type relation as: = {(k, clsk) participates in some a}, = {(a, ksub, kobj) A(P ), (ksub, ), (kobj, ) K}. In practice, the LLM-validator returns interaction information per interaction including the form idX verb idY, subject and object IDs, an interaction-type label (multi-subject relation or functional action), and the exact source sentence span. The outputs and serve as the formal supervision for all subsequent interaction-aware curation and evaluation steps. Interaction Scoring and Filtering. Fig. 22 presents the prompt design for interaction scoring and filtering. For each extracted interaction triplet (a, ksub, kobj) from the prompt , an LLM rater (Aaron Grattafiori, 2024) consumes the full textual context including the prompt , idX verb idY, and noun descriptors of each ID, and returns two integer scores {1, ..., 5}: Contactness quantifies the degree of physical contact or tight spatial coupling implied by the action (1 = no contact, 3 = indirect/uncertain, 5 = direct/certain contact). Dynamism measures the degree of motion or temporal change (1 = static relation, 3 = low/moderate movement or readiness, 5 = strong action/state change). For auditability, the rater also provides brief natural language justification and self-reported confidence to discard uncertain cases. Interactions judged to exhibit sufficient contact and motion are retained, and instances that do not appear in any retained triplet are pruned. Instance Description Extraction. Fig. 23 shows the prompt design to obtain the description of the instance. Given the prompt , selected interaction triplet (a, ksub, kobj), and the base nouns clsk for the participating IDs, the LLM rater (Aaron Grattafiori, 2024) produces, for every referenced instance K, compact descriptor desck = (noun, app, spatial). Here noun is short, visually discriminable noun phrase (e.g., man in blue shirt), app is one-sentence summary of salient appearance or physical attributes, and spatial is one-sentence statement of location or role in the scene. The descriptors are canonicalized, coverage-complete (one per ID), and linked to (k, clsk), redefining the ID set as = {(k, clsk, desck)}. We use this set to support grounding and to verify detected bounding boxes or masks by matching appearance and spatial cues, improving disambiguation among same-class entities. 17 MATRIX: Mask Track Alignment for Interaction-aware Video Generation A.2 DETAILS FOR INTERACTION-AWARE MULTI-INSTANCE MASK TRACKS WITH VERIFICATION Interaction-aware multi-instance tracks with verification. Fig. 24 illustrates the prompt design for vision-language verification, which checks the consistency between the bounding-box visual prompts and the appearance of the instance. We generate tracks in four steps. (1) Class-only proposals. Given video , its prompt and its ID set = {(k, clsk, desck)}, we uniformly sample frames and run GroundingDINO (Liu et al., 2024) with clsk only. For each frame i, it returns up to candidate bounding boxes (bi,j is box coordinate and ci,j [0, 1] is the class-conditioned confidence for the given class clsk. Thus, for each id k, the video yields at most JT candidates across the sampled frames. This class-only setting provides high recall, but cannot disambiguate same-class instances and may still miss the intended target on difficult frames. ), where bi,j , ci,j , ci,j (2) Anchor selection and VLM verification. For each noun ID k, we collect at most candidates {(bi,j )} on the sampled frames. We sort them by confidence and define anchor as the highest = bi,j scoring pair as: (i, j) = arg maxi,j ci,j . We then query vision-language model (VLM) (OpenAI & et al., 2024) with inputs, including the frame i, the crop from k, the class name clsk and the descriptor desck, and ask whether the crop matches the description of ID k. If the VLM verifies the match, we accept as the final box for ID and initialize the SAM2 (Ravi et al., 2024) propagation from that frame to obtain the instance mask track of the ID. If not, we move on to the next candidate in descending order ci,j and repeat the aforementioned process. When no candidate is verified, the ID is dropped; if both subject and object are removed, the clip is excluded. with When multiple IDs share the same class (e.g., man with blue shirt and another man with green shirt), verification is one-to-one: once candidate box is accepted for an ID, it is removed from the pools of the other IDs of the same class. This mutual exclusion pruning prevents duplicate assignments and reduces verification cost from naive O(KJT ) scan to much smaller set of checks in practice, while keeping recall high and disambiguation accurate. (3) Human verification. As final check, we run lightweight but explicit quality control pass on the verified tracks. For each clip, annotators review 10 frames, including the first verified frame, the last valid frame, and eight uniformly spaced interior frames. They view the RGB frames, instance mask tracks and boxes, union mask tracks, and the triplet descriptor. Each track is labeled Accept (clean and consistent), Fix (minor boundary/jitter; quick snap/smooth), or Drop (identity drift, duplication, hallucination or clear temporal gaps). clip is used for supervision only if both subject and object are Accept after any minor fixes; otherwise, it is excluded. For same-class IDs, we enforce one-toone assignment by dropping the worst of any substantially overlapping tracks. Effect of the proposed VLM verification. As described in Sec. 3 of the main paper, we employ VLM (OpenAI & et al., 2024) to verify and refine the error of GroundingDINO (Liu et al., 2024). Fig. 9 illustrates why this step is necessary. With only class name (e.g., person, man, cake), GroundingDINO frequently returns multiple instances of the same class over pre-defined threshold and cannot single out the intended target (e.g., the man outside the shop, the person being photographed in Fig. 9). In Fig. 9, (a) captures both the person inside and outside the shop. (b) captures the photographer, the person being photographed and even reflection in phone. (c) captures every cake in view, and (d) captures both the stylist and the client. straightforward solution is to add appearance phrases (e.g., the man outside the shop) to figure out the intended target. However, it is unreliable, since GroundingDINO often latches onto partial tokens and ignores modifiers. For instance, in (a) it selects the man inside the shop by focusing on man and shop while missing outside, and in (b), it selects the person taking the photo instead of the intended person being photographed. In (c), it selects the wrong cake rather than the blue cake on the table, and in (d), it still captures both people, failing to disambiguate the stylist from the client. Rather than directly injecting appearance phrases into GroundingDINO, we use it purely as classconsistent proposal generator, since with class names alone, it reliably enumerates candidate bounding boxes but cannot disambiguate same-class instances. Motivated by recent results (Cheng et al., 2025; Jia et al., 2025; Chen et al., 2025) showing that VLMs (Bai et al., 2023; Li et al., 2024; Ye et al., 2024; OpenAI & et al., 2024; Dai et al., 2023) excel at image and multi-image reasoning, we 18 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 9: Effects of Our VLM Verification. The first row (GroundingDINO with class) and the second row (GroundingDINO with appearance) often pick wrong same-class instance. The third row (Ours) verifies candidates with VLM and keeps exactly one box per noun, resolving (a) to (d). Best viewed in zoom. introduce VLM verification stage that cross-checks each candidate against descriptors, including appearance cues, and selects exactly one box per noun. If no candidate satisfies the verifier, we drop that instance and remove the clip from the supervision. This preserves high recall from GroundingDINO while delegating fine-grained disambiguation to the VLM, yielding cleaner per-instance tracks. As presented in the last row of the Fig. 9, VLM evaluates candidates against the provided appearance descriptor and selects the final bounding box that matches the cue. A.3 DATASET EXAMPLES AND STATISTICS We provide more dataset examples in Fig. 25 and Fig. 26. Furthermore, Fig. 10 shows the overall statistics of our curated dataset. In Fig. 10, (a) summarizes the distribution of video-text sources we used in our study. Our primary source is HOIGen (Liu et al., 2025), whose captions explicitly describe humans, human-object interaction, human action, and scene descriptions. Therefore, the text provides dense cues for extracting interactions. Since HOIGen collects videos from diverse sources, it spans from everyday to highly specific scenarios and offers abundant interaction instances. To improve generalization and ensure data quality, we further incorporate PE-Video (Bolya et al., 2025), high-quality, carefully annotated collection that covers wide range of categories. (b) reports the joint distribution of the contact and dynamism score in our curated corpus. We score contact on 1-5 scale (none-contact-rich) and dynamism on 1-5 scale (static-highly dynamic). While the corpus includes static or non-contact cases, it is enriched for dynamic, contact-rich interactions. Crucially, within each contact level(from 1 to 5), dynamism spans broad range, ensuring diverse motion intensities conditioned on contact level. Additionally, (c) summarizes the distribution of per-video counts of interactions (1-8) and identities (1-10). The mass concentrates in the 1-5 range for both, with clear modes at two interactions and two identities, indicating that pairwise subject-object settings dominate. Motivated by this distribution, we cap instance identities at = 5 per clip: the annotator collects up to five tracks and the model predicts up to five instance mask tracks. This choice balances coverage and computation while remaining extensible, raising only increases the number of track slots without altering the rest of the pipeline. Moreover, clips with more than five interactions or instances are empirical outliers in Fig. 10 (c), providing evidence that such highly crowded cases are rare. When they do occur, we either split the video into shorter sub-clips or retain the top-k salient instances and aggregate metrics at the original video level. Considering (a) and (b), these statistics indicate corpus 19 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 10: Dataset Statistics. (a) Source composition from two primary dataset. (b) Joint distributions of Contact and Dynamism, covering the full spectrum with tilt toward contact-heavy, high motion events. (c) Per-clip counts of interaction triplets and tracked instance IDs; most clips are modest in complexity, motivating fixed track budget of = 5. (d) Frequency of interaction verbs and (e) frequency of instance nouns, indicating broad lexical coverage. with dense interactions yet not overcrowded, aligned with our modeling in Sec. 5 and evaluation design in Sec. 4. Finally, the dataset exhibits strong linguistic coverage. In Fig. 10 (d), we plot the top-50 interaction verbs. Since contact frequently entails hold, that verb dominates. Excluding hold, the remaining verbs follow comparatively balanced distribution, indicating broad action diversity rather than reliance on handful of predicates. Fig. 10 (e) shows the top-50 ID nouns. As interaction typically involves at least one human subject, nouns such as man, person, and woman are frequent. Nevertheless, object nouns are broadly distributed, reflecting diverse targets and scenes. Together, (d) and (e) indicate wide linguistic coverage over actions and instances, supporting robust training and evaluation of interaction-aware models."
        },
        {
            "title": "B ANALYSIS DETAILS",
            "content": "B.1 ANALYSIS EVALUATION DATASET To faithfully evaluate interaction-aware video generation, we curate dedicated analysis evaluation dataset rather than relying on real-world videos. Using real videos for reconstruction is problematic due to inversion errors (Song et al., 2022), imperfect prompt-video alignment, and distributional drifts, making it difficult to isolate model behavior. To circumvent these issues, we curate controlled analysis evaluation dataset designed to simulate the generation process itself. By fixing random seeds during synthesis, we approximate near-perfect reconstruction conditions. Human annotators further verify the output, ensuring that only videos with high overall fidelity and consistent interactions are retained. Each video in the benchmark has resolution of 480 720, contains 49 frames, and the final dataset consists of 50 carefully validated prompt-video pairs. Scenario design. The curation process begins with scenario design proposed by (OpenAI & et al., 2024), where we systematically specify the conditions of interaction to ensure diversity and cover20 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 11: Analysis Dataset Pairs Example. age. Specifically, we distinguish between unidirectional interactions, where subject acts upon an object (e.g., person pushing box), and bidirectional interactions, where both subject and object mutually influence each other (e.g., two people shaking hands). We then vary the number of participating instances, ranging from simple subject-object pairs to multi-party settings with three or more instances, which introduce additional ambiguity in role assignment. Interactions are further categorized into contact (e.g., touching, holding), force (e.g., pushing, pulling), transport (e.g., handling over, carrying), manipulation (e.g., cutting, opening) and social (e.g., hugging, waving), thereby covering broad spectrum of physical and social dynamics. Finally, we ensure class diversity by including human-object, human-human, human-animal, human-nature interactions, encouraging generalization beyond human-centric scenarios. Together, these design choices allow us to construct structured prompts that specify the instances, their roles, and their relations, ultimately yielding balanced set of interaction scenarios for evaluation. Prompt Construction. Given scenario, we then construct prompts that specify instance identities (IDs), class labels, and concise descriptors, along with the intended interaction, following the same principles as our dataset curation process described in Sec. 3.1. We first compose an image prompt that captures the static scene and instance attributes. Next, we derive motion-aware video prompt by adding action and relation clauses (subject-verb-object) with temporal qualifiers (e.g., contact). To improve synthesis stability and phrasing consistency, we apply VLM (OpenAI & et al., 2024)-based prompt enhancement while preserving instance IDs and interaction roles. For controlled synthesis and verification, we generate videos with fixed random seeds and standardized rendering settings, holding resolution and length constant. Human annotators review each promptvideo pair for overall visual quality, semantic fidelity to the prompt, and interaction plausibility. Only pairs passing all criteria are retained in the analysis dataset. Fig. 11 (a) presents an example produced by our prompt-construction procedure. Pseudo Ground-Truth Mask Tracks Generation. Finally, to quantitatively evaluate semantic grounding and semantic propagation, we produce pseudo ground-truth mask tracks for each instance, since synthesized videos do not contain ground-truth supervision. Following the same grounding-and-verification procedure used in dataset curation as Sec. 3.2, we first extract candidate bounding boxes using GroundingDINO (Liu et al., 2024), verify them with vision-language model (OpenAI & et al., 2024) to eliminate irrelevant detections, and propagate the validated boxes using SAM2 (Ravi et al., 2024) to obtain per-instance mask tracks. final human verification step ensures the correctness of both instance identity and mask track quality, yielding high-quality mask tracks that serve as supervision for interaction analysis. Fig. 11 (b) shows an example constructed by our pseudo ground-truth generation. As result of the above systematic and precise procedure, we obtain images, prompts, and perinstance mask tracks for each instance ID. We use this analysis evaluation dataset to evaluate semantic grounding and semantic propagation, as presented in Sec. 4. 21 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Real-domain analysis evaluation set. To validate whether our findings are valid beyond controlled synthesis, we additionally curate real-domain set using PE-Video (Bolya et al., 2025) and OpenVid (Nan et al., 2025). As discussed in Sec. B, reconstructing real videos via prompt inversion is prone to inversion errors (Song et al., 2022) since accurate text prompts are difficult to recover and the real-video distribution differs from the training distribution. Consequently, we select video-text pairs whose captions instantiate our interaction schema, extract the captions to our ID, role, and action format, and reconstruct each clip with an image-to-video (Yang et al., 2024) using the paired caption. Human annotators then verify that the generated clip preserves the intended interaction, roles, and overall appearance; only verified pairs are retained. The same rater used in scoring and filtering provides contactness, dynamism and brief justification with confidence, and pseudo mask tracks are produced with the grounding and verification pipeline (Sec. 3) and checked for instance identity and mask track quality. B.2 ADDITIONAL ANALYSIS Figure 12: Attention Map Details for Grounding and Propagation. Fig. 12 visualizes the procedures used in Sec. 4 and Sec. 5 in the main paper to extract grounding attention maps from video-to-text attention and propagation attention maps from video-to-video attention. Metric Choice. In Sec. 4 in the main paper, we introduce the Attention Alignment Score (AAS) as the primary analysis metric. Our goal is to test whether the models attention encodes who does what to whom at the level of targeted instances and whether the targeted spatial region for each instance is preserved consistently over time. In other words, attention should be concentrated on the intended instances (e.g., subject, object) along their mask tracks, providing spatial binding within frames and temporal persistence across frames. We define AAS as the spatio-temporal inner product of Av2t , where {sub, obj, verb} and mask track, which measures how much attention mass is placed on the exact support of the instance over space and time. , Av2v e This formulation is driven by the evaluation goal and by the normalization behavior of 3D full attention. Queries attend the concatenation of visual and text keys, and the softmax normalization is taken across that union. In our setting, the text stream contributes roughly 226 tokens, whereas the video stream contributes about 1350 13 = 17550 visual tokens (1350 spatial locations per latent frame across 13 latent frames). Even when video-to-text attention is correctly localized, the relative scale of attention to text tokens might be compressed by this large cardinality imbalance. Preserving raw magnitude is therefore informative since it quantifies how much attention mass is allocated on the instances track versus how much is allocated to non-target tokens and regions, rather than merely indicating whether there is any overlap with the binary mask track. AAS integrates raw attention on the mask track without thresholding or calibration, so it remains comparable across layers and is robust to this token-count imbalance. straightforward alternative is to treat Av2t , Av2v {sub, obj, verb} as soft segmentae tion sequence and measure overlap with its corresponding mask track using standard segmentation scores (Rezatofighi et al., 2019; Lin et al., 2018; Sudre et al., 2017). For example, one can threshe 22 MATRIX: Mask Track Alignment for Interaction-aware Video Generation old instance attention maps to obtain binary sequence and compute IoU (Rezatofighi et al., 2019) against mask track, or use threshold-free scores such as BCE or Dice. However, these options either introduce sensitivity to an arbitrary threshold or discard absolute magnitude and retain only shape overlap. The loss of magnitude is particularly limiting under 3D full attention, where cross-modal competition suppresses text-side scales. At the opposite extreme, simply aggregating raw attention over the whole scene preserves magnitude, but no longer tests whether attention lies on the intended instance trajectory. AAS provides direct measure of what we seek to evaluate. Consequently, we use AAS in Sec. 4 to locate interaction-dominant layers and to link attention concentration with semantic grounding and semantic propagation. Figure 13: Visualization of Propagation Attention Maps. (a) In human-rated success cases, perquery propagation attention map remains stable across frames. (b) In human-rated failure cases, the maps are unstable and drift over time. This is in line with human judgments, supporting propagation attention map as meaningful signal for interaction fidelity. Qualitative link between attention alignment and generation quality. We provide qualitative evidence that attention-mask alignment is related to interaction fidelity. In the teaser 1 (b), videoto-text (v2t) grounding improves generation when noun and verb attentions align with the subject, object and union regions, whereas misalignment coincides with failures. Fig. 13 visualizes videoto-video (v2v) propagation maps. For each instance (e.g., boy, girl), first frame mask pixels serve as query points. As detailed in Sec. 4 of the main paper and Fig. 12 in the appendix, we extract per-query video-to-video attention over all spatial tokens across frames, reshape the result into map, and overlay it on the video. In successful examples, attention initialized within the instance mask remains compact, follows the same instance through time, and yields clean, consistent clips. In failure cases, even with accurate first-frame grounding, propagation diffuses within the mask, leaks outside, or jumps to other regions, producing identity drift and hallucinated parts. These observations indicate that both semantic grounding and semantic propagation alignment matter for generation quality, which motivates them to be explicitly supervised with SGA and SPA. Layer-wise Analysis. Fig. 14 compares, for each noun and verb token, the attention maps from the 42 layers of naive CogVideoX-5B-I2V (Yang et al., 2024). Two patterns emerge. First, small subset of layers shows strong alignment with the instance mask region for nouns and with the subjectobject union for verbs. Second, many other layers exhibit grid-like responses consistent with positional embedding effects rather than semantic binding. This heterogeneity implies that layers play distinct roles, so fine-tuning every layer can dilute or damage the layers that carry useful semantics. Notably, even vanilla CogVideoX-5B-I2V already displays alignment in several layers highlighted by our analysis (e.g., layers 7, 8, 9, 10 and 11), further motivating our focus on interaction-dominant layers. 23 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 14: Layer-wise Comparison of Semantic Grounding Maps. 24 MATRIX: Mask Track Alignment for Interaction-aware Video Generation"
        },
        {
            "title": "C MATRIX FRAMEWORK DETAILS",
            "content": "C.1 ARCHITECTURAL DETAILS We build on the pretrained CogVideoX-5B-I2V (Yang et al., 2024) and retain its transformer blocks and 3D VAE except for the input projection layer and small set of parameter-efficient adapters. Our network requires (i) the noise latent, (ii) first RGB frame and (iii) instance masks that supervise attention alignment. These signals are concatenated along the channel dimension and projected by the input projection layer of the backbone. To preserve the pretrained capability at initialization, we copy the original weights into the slice that corresponds to the original channels and zero-initialize the newly added channel kernels. This keeps the base behavior unchanged at step 0, while allowing the added channels to learn during finetuning. We attach adapters to the query, key, value, and output projections inside the corresponding attention modules while leaving all other weights frozen. To manipulate internal attentions without overfitting, we adopt LoRA (Hu et al., 2021) on minimal set of layers identified by our analysis. Layer 7 and layer 11 are used for semantic grounding based on video-to-text attentions and layer 12 is used for semantic propagation based on video-to-video attentions. These are the only transformer weights that receive trainable LoRA parameters and the rest of the backbone remains fixed. With these adapters, we supervise attention directly rather than supervising proxy features. We add two lightweight decoders, grounding head and propagation head that read the query-key product scores from the targeted layers, such as layer 7, 11 and 12, and convert them into alignment scores trained against binary ground truth mask tracks in RGB space while the generator remains unchanged. For semantic grounding, it uses the video-to-text attention where video tokens act as queries and instance token in the text act as keys. For semantic propagation, it uses the video-to-video attention that links each location in one frame to matching locations in the next few frames and checks whether the same instance persists over time. After computing the query key product, we reshape the result to the backbone spatio-temporal token grid so that each value aligns with patch and frame. We then take simple mean across attention heads and feed the resulting map to lightweight decoder. The decoder serves as supervised readout that turns token space attention into dense alignment scores against binary mask tracks. This separation allows the alignment loss update only the query and key projections in the adapted layers, preserves the pretrained behavior at initialization, and allows the grounding and propagation decoders to be removed at inference when only generation is needed. Both decoders follow the upsampling strategy and the time causality used in 3D VAE (Yang et al., 2024) while remaining lightweight. standard 3D VAE temporally compresses several frames into one latent which places attention on shorter temporal lattice than the ground truth instance mask tracks. In CogVideoX, the VAE temporally compresses frames from 1 + 4F to 1 + , reducing the effective frame rate of the latent sequence by factor of 4. This places attention on shorter temporal lattice than the ground truth binary mask tracks. In our setup, the latent attention sequence spans 13 steps, whereas the ground-truth instance mask tracks span 49 frames. The most straightforward solution to address this gap is to compress supervision by taking an element-wise OR over every 4 consecutive frames so that each group maps to one latent step. However, this ignores temporal ordering and inflates foreground regions, which weakens alignment under motion and degrades identity precision. Instead, we upsample the attention to the mask frame rate. The lightweight decoder mirrors the VAE temporal up path and causally expands the 13-step attention sequence to 49 frames without using future frames. The supervision is then applied at the original frame rate against the binary instance mask tracks. This preserves temporal ordering and sharp instance boundaries, avoids foreground inflation, and leaves the generator unchanged while confining updates to the query and key projections in the adapted layers. C.2 IMPLEMENTATION DETAILS We use the CogVideoX-5B-I2V (Yang et al., 2024) as our base image-to-video diffusion model, and generate output videos at resolution of 480 720 with total of 49 frames. The trainable parameters are limited to the selected LoRA (Hu et al., 2021) layers (layer 7, 11, 12), the input projection layer, and lightweight decoders for grounding and propagation, respectively. For model finetuning, we adopt LoRA (Hu et al., 2021) with rank of 128 and α = 64. We optimized only 25 MATRIX: Mask Track Alignment for Interaction-aware Video Generation the selected LoRA layers, input projection layer and lightweight decoders while keeping the other parts of the model frozen. Training was conducted on our curated dataset, MATRIX-11K, using an AdamW (Loshchilov & Hutter, 2019) optimizer with cosine learning rate decay schedule. The model is trained about 4, 000 steps, which takes approximately 32 hours on single NVIDIA A6000 GPU. We apply Semantic Grounding Alignment (SGA) loss and Semantic Propagation Alignment (SPA) loss selectively. The SGA loss supervises video-to-text attention in blocks 7 and 11. The SPA loss supervises video-to-video attention in block 12. This selective strategy concentrates updates on the query and key projections of the adapted layers, stabilizes optimization under motion, and preserves the pretrained generator at initialization and inference. TRAINING-FREE CROSS-MODAL GUIDANCE DETAILS Figure 15: Guidance Mechanism Details. (a) Cross-Attention Guidance (CAG) zeros out cross- (b) Cross-Modal Guidance (CMG) zeros out cross-modal frame video-to-video (v2v) attention. attentions, including text-to-video (t2v) and video-to-text (v2t) attention. The mechanisms differ in the location of zeroing. Our analysis shows that semantic grounding (video-to-text) and semantic propagation (video-tovideo) are concentrated in small subset of interaction-dominant layers. To validate whether these layers provide effective handles for improving interaction fidelity, we design zero-shot guidance strategy applied only at the identified layers. Specifically, we introduce Cross-Modal Guidance (CMG), our novel approach to enhance grounding, and adopt Cross-Attention Guidance (CAG) (Nam et al., 2025) for propagation. CMG perturbs token-to-entity attention maps at dominant video-to-text layers to simulate degraded grounding, and then guides the model away from these perturbed predictions, reinforcing semantic alignment. In parallel, CAG applies the same perturband-guide principle to cross-frame attention, reinforcing temporal consistency without additional training. Fig. 15 shows the architectural details of CAG and CMG. D.1 ARCHITECTURAL DETAILS Cross-Attention Guidance (CAG). Inspired by PAG (Ahn et al., 2025), which enhances image fidelity by transforming selected self-attention maps into identity matrices, we extend this idea to the video DiT architecture. In PAG, identity matrices are created by multiplying diagonal mask into the attention map before the softmax operationdiagonal elements set to 0, off-diagonal to yielding an identity matrix after softmax. naive extension to video assigns to crossframe positions, but this undesirably suppresses self-frame and text-frame scales. To address this, (Nam et al., 2025) zero out only the cross-frame values after softmax in Av2v , producing modified t,l maps ˆAv2v t,l that preserve other interactions. Cross-Modal Guidance (CMG). Analogous to CAG, CMG applies the perturb-and-guide strategy to video-to-text attention. At interaction-dominant layers, we simulate degraded grounding by 26 MATRIX: Mask Track Alignment for Interaction-aware Video Generation zeroing out tokeninstance alignments after softmax. For noun tokens, attention weights to instance regions are suppressed; for verb tokens, attentions capturing subjectobject unions are removed. This produces modified maps ˆAv2t t,l where semantic grounding is intentionally weakened, while other attentions remain intact. The diffusion model is then guided away from these degraded predictions, reinforcing correct grounding without retraining or auxiliary conditions. Both can be formulated as: ϵθ(zvideo,t, ztext, t) = ϵθ(zvideo,t, ztext, t) + (ϵθ(zvideo,t, ztext, t) ˆϵθ(zvideo,t, ztext, t)), where ϵθ() is the noise prediction from standard pass at timestep t, conditioned on the text, and ˆϵθ() indicates the noise prediction from perturbed forward pass. is the perturbation guidance scale and the final prediction ϵθ() is guided away from the degraded predictions. D.2 IMPLEMENTATION DETAILS For CAG, we adopt the 1 interaction-dominant video-to-video layers (e.g., layer 12 in CogVideoX5B-I2V) identified by our analysis, and apply guidance across all sampling steps. For CMG, we similarly select the 2 interaction-dominant video-to-text layers (e.g., layer 7 and 11 in CogVideoX-5B-I2V) and apply zero-shot guidance at every timestep. Both guidance scales are set following PAG (Ahn et al., 2025), and no additional parameters or training are introduced. D.3 EXPERIMENTAL RESULTS Figure 16: Effects of Guidance. In Fig.16, we diagnose the failures through attention. In the first row of (a) for woman cuts cake, noun attention for woman leaks onto the man and the verb cut focuses on him rather than the union of the woman-cake region, so the action is assigned to the wrong agent. In the second row of (a), noun attention to the subject man is weak and diffuse and video-to-video attention does not carry stable subject track forward, so the motion does not start. These cases show that when grounding is weak, propagation also breaks. We then apply perturbation guidance only to the interaction-dominant layers identified by our analysis and leave all other layers unchanged. The guidance biases video-to-text attention Av2t toward the intended subject, object, and their union and stabilizes the carry-over in video-to-video attention with small weight to avoid appearance drift. Under this setting, many borderline cases flip from failure to success. In the first row of (b), this sharpening results in the woman executing the cut with contact maintained across frames, and in the second row of (b), the man is cleanly localized from the first frame, and the motion initiates and proceeds without drift. The fact that lightweight in-layer perturbation cleans up video-to-text and video-to-video attention and improves plausibility, frequently turning failures into successes, shows that these layers are the dominant handles for attention sharpening as well as for grounding and propagation. However, critical limitations remain. CMG is zero-shot guidance that amplifies existing attention at selected layers, but does not inject region-level or ID-level supervision. When the initial noun map is severely ambiguous, when the verb is not well approximated by the subject-object union, or MATRIX: Mask Track Alignment for Interaction-aware Video Generation Table 3: Comparison of evaluation protocols. Existing benchmarks assess quality, compositionality, or physics, but only InterGenEval targets interaction-level semantic alignment. Target Semantic Granularity Temporal Semantics Semantic Alignment Protocol VBench VBench-2.0 EvalCrafter FETV T2V-CompBench PhyGenBench PhyWorldBench Visual Quality Faithfulness Quality & Alignment Attributes Compositionality Physics Physics Global (frame/clip) Global / Semantic Global (entity cues) Entity (attributes) Relation (multi-object) Event (physics) Event (physics) InterGenEval (ours) Interaction Fidelity Interaction-level Partial Global appearance Human, controllability, physics Basic visual-text alignment Attribute-level alignment Multi-object relations Physical plausibility Physical plausibility Interaction-level alignment under heavy occlusion, sharpening may be insufficient or may over-concentrate attention and subtly degrade appearance. Moreover, increasing the guidance scale to compensate the limitation often saturates attention and collapses diversity. Therefore, these observations motivate our mask-track alignment losses that provide explicit grounding and propagation signals, as depicted in the Sec. 5 in the main paper."
        },
        {
            "title": "E EVALUATION PROTOCOL DETAILS",
            "content": "E.1 RELATED WORKS Figure 17: Limitations of Existing Semantic Alignment Metrics using BLEU and CLIP. (a) Cross-model comparison: despite clear human preference for one model, BLEU and CLIP favor the other, assigning high scores to implausible or semantically misaligned results. (b) Within-model frames: frames preferred by humans receive lower scores than other frames from the same clip, showing insensitivity to instance-level grounding and temporal consistency. This gap motivates InterGenEval, an interaction-aware evaluation. Early evaluations of video generation primarily relied on the Inception Score (IS) (Salimans et al., 2016), Frechet Inception Distance (FID) (Heusel et al., 2017), and Frechet Video Distance (FVD) (Unterthiner et al., 2018), which measure distributional fidelity and diversity but fail to capture semantic correctness. To address this, recent benchmarks introduced multi-dimensional protocols. VBench (Huang et al., 2024) decomposes the quality of generation into 16 dimensions, including frame aesthetics, temporal consistency, and prompt adherence, and validates alignment with human judgments. EvalCrafter (Liu et al., 2023a) further integrates large prompt suite and combines multiple automatic metrics with human preference weighting. FETV (Liu et al., 2023b) emphasizes attribute-level evaluation, scoring static and temporal quality as well as fine-grained alignment. These works expand coverage beyond single-number scores, but remain global or attribute-focused. Many of these benchmarks often rely heavily on CLIP-based models to assess semantic similarity. However, as shown in Fig. 17, CLIP score and the BLIP-BLEU score from EvalCrafter fail to capture interaction-level granularity, highlighting their limitations in evaluating the interaction modeling capabilities of the generated videos. Other benchmarks target narrower capabilities. T2V-CompBench (Sun et al., 2024) measures compositionality over relations, attributes, and actions through VLM-based and detection-based metrics. PhyGenBench (Meng et al., 2024) and PhyWorldBench (Gu et al., 2025a) evaluate physical commonsense and causal plausibility with structured protocols, while VBench-2.0 (Zheng et al., 2025b) expands toward intrinsic faithfulness, covering human fidelity, controllability, and physics. These 28 MATRIX: Mask Track Alignment for Interaction-aware Video Generation efforts highlight compositional and physical reasoning, but still do not capture whether models realize prompt-specified interactions. In particular, existing protocols assess global semantics or object attributes but lack interactionaware semantic alignment: whether the correct subject acts on the correct object, contact occurs, and causal unfolding matches the prompt. Our proposed InterGenEval addresses this gap by treating interactions as the evaluation unit and introducing grounded criteria for the roleand time-sensitive alignment and Fig. 18 presents the overall InterGenEval pipeline. Figure 18: InterGenEval Pipeline. E.2 OVERVIEW InterGenEval focuses on interaction aware semantic alignment between the video and the prompt, measured by two metrics: Key Interaction Semantic Alignment(KISA) and Semantic Grounding Integrity(SGI). Specifically, after extracting key interactions from the prompt, KISA verifies-step by step-whether the subject actually performs the specified action on the object, while SGI assesses the grounding accuracy of the subject and object. When multiple key interactions are present in prompt, each key interaction is evaluated to obtain its corresponding KISA and SGI, which are then averaged across all interactions to produce the final KISA and SGI. This enables evaluation in multi-interaction and multi-instance scenarios. Meanwhile, maintaining the temporal consistency of interaction and grounding is also crucial. To account for this, we introduce Semantic Propagation Integrity(SPI) as sub-metric. SPI captures whether any instance suddenly appears or disappears throughout the video, providing measure of temporal consistency. SPI is then applied to KISA and SGI, injecting temporal consistency into both metrics by penalizing inconsistent instance propagation over time. The mean of KISA and SGI is then defined as the final Interaction Fidelity(IF) score. For clarity, we denote the unadjusted scores as KISAraw and SGIraw, and SPI applied scores as KISA and SGI. Setup. InterGenEval leverages multimodal foundation model GPT-5(OpenAI, 2025), utilizing its strong visual understanding and reasoning capabilities throughout the evaluation process. KISA and SGI are computed using question-answering framework, which verifies whether the subject actually performs the intended action and whether the subject and object are correctly grounded. Additionally, SPI is derived through an instruction-based evaluation procedure, where GPT-5 is used to detect the emergence and disappearance of each instance across frames assessing temporal consistency. InterGenEval uses sequence of frames where each instance involved in the interaction is visually annotated with bounding box of distinct color. We generate these annotated frames using SAM2(Ravi et al., 2024), which allows us to extract precise bounding boxes for each instance. This visual representation enables GPT-5 (OpenAI, 2025) to clearly identify each instance and focus on fine-grained interaction details. Each evaluation frame sequence includes the first and last frames 29 MATRIX: Mask Track Alignment for Interaction-aware Video Generation of video, while intermediate frames are uniformly sampled using fixed stride. In this paper, the stride is set to 5. E.3 EVALUATION METRICS Question Generation. Since KISA and SGI are derived from question-answering framework, it is important to construct well-structured set of questions that reflect whether each step of the interaction is performed and whether all instances are correctly grounded. To this end, we use GPT-5 to automatically generate 10 yes/no questions per key interaction, guided by task-specific instruction. As input, GPT-5 receives the text prompt, list of instances, their corresponding appearance descriptions, and the assigned bounding box colors. Based on this input, GPT-5 first identifies key interactions described in the prompt. Then, for each key interaction, it generates 10 questions that align with the evaluation goals of KISA and SGI. Each question explicitly refers to instances using both appearance and bounding box color(e.g., woman in green jacket (red bounding box)). The first six questions (Q1-Q6) are used to compute KISAraw, as they assess whether the interaction progresses through its expected stages. The remaining four questions (Q7-Q10) focus on verifying instance grounding and are used to compute SGIraw. Further details on the structure of these questions and the computation of KISA and SGI are provided in the following section. Key Interaction Semantic Alignment (KISA). KISA evaluates an interaction by decomposing it into three temporal stages: pre-, during-, and postinteraction. Question 1 corresponds to the pre-interaction stage and checks whether the subject and object are in the expected initial state prior to any engagement. Question 2 through 5 cover the during-interaction stage, where the model verifies the progression of the action across multiple steps. Finally, Question 6 focuses on the post-interaction stage, assessing whether the expected outcome of the interaction has been visibly achieved. For example, in the case of the interaction lifts B, the six questions will be constructed as follows. Q1. Is not touching before interaction? , Q2. Is move his/her hands toward B?, Q3. Does make contact with B? Q4. Does hold with his/her hand? Q5. Does start to lift B? Q6. Is visibly off the ground and being held by A? KISAraw is then computed as the proportion of Yes responses among these six questions, indicating how successfully the interaction is executed across all expected stages. Semantic Grounding Integrity (SGI). SGI evaluates whether the subject and object are correctly grounded within the interaction. To this end, it comprises four questions. Question 7 verifies whether the subject is correctly identified as the actor of the interaction. Question 8 checks whether the subject performs the specified action on the intended object. Question 9 evaluates whether the object is being acted upon by the specified subject. Question 10 assesses whether the object is indeed the correct recipient of the action. For example, in the case of the interaction lifts B, the four grounding questions will be constructed as follows. Q7. Is taking action? Q8. Is the one lifting B? Q9. Is being lifted by A? Q10. Is being lifted? SGIraw is then computed as the proportion of Yes responses among these four questions, capturing the accuracy of instance level semantic grounding within the interaction. Semantic Propagation Integrity (SPI). SPI measures the temporal consistency of each instance throughout the video. The first frame is used as an anchor, and the remaining frames are compared against it to detect any changes. We provide GPT-5 with list of instances, their bounding box colors, and the bounding box visualized frame sequence as input. GPT-5 then outputs the detection results for emergence and disappearance for each frame. Specifically, emergence is defined as the appearance of new instance-belonging to class listed in the instance list-that does not appear in the anchor frame but emerges in later frames. Disappearance occurs when an instance annotated with bounding box in the anchor frame is no longer visible in subsequent frames. To compute the SPI score, we first calculate the ratio of frames in which emergence or disappearance is detected. Each of these ratio is multiplied by penalty weight λ, and the result is subtracted from 1 to obtain the emergence score and the disappearance score, respectively. The final SPI score is defined as the average of these two scores. In this paper, we set λ= 5. Overall Scoring. As previously mentioned, we use SPI to incorporate temporal consistency into KISA and SGI. SPI ranges within (-4,1], with higher values indicating better temporal consistency MATRIX: Mask Track Alignment for Interaction-aware Video Generation of instances. To strongly penalize videos with poor temporal consistency, we multiply SPI with both KISAraw and SGIraw to obtain their reweighted final values. KISA = KISAraw SPI, SGI = SGIraw SPI. The final Interaction Fidelity (IF) score is then computed as the average of the reweighted KISA and SGI. IF = KISA + SGI . IF combines KISA, SGI, and SPI to provide quantitative score that reflects interaction aware semantic alignment with temporal consistency. This formulation offers an interpretable and consistent metric for assessing interaction quality. As result, InterGenEval functions as practical evaluation framework that gives precise feedback on the quality of interaction-aware video generation. Fig. 27 and Fig. 28 illustrate the prompt design for KISA, SGI and SPI."
        },
        {
            "title": "F EVALUATION",
            "content": "F.1 COMPARISON MODELS We compare our approach against several recent open-source image-to-video diffusion models including CogVideoX-2B-I2V (Yang et al., 2024), CogVideoX-5B-I2V (Yang et al., 2024), TaVid (Kim & Joo, 2025) and Open-Sora (Zheng et al., 2024). CogVideoX-2B-I2V is lightweight version with approximately 2 billion parameters, designed for efficient video synthesis. In contrast, CogVideoX-5B-I2V scales to 5B parameters and offers stronger generative capacity through larger model size and broader training coverage. TaVid simply conditions on single binary mask and performs LoRA fine-tuning with cross-attention alignment loss applied only to subset of layers. Finally, we include Open-Sora (11B) as fully open-source alternative, widely adopted as community benchmark. Collectively, these comparison models span spectrum of scales, training regimes, and accessibility levels, enabling us to evaluate both the absolute quality of our method and its relative efficiency against existing models. F.2 ADDITIONAL METRICS In addition to our proposed protocol, we adopt several metrics from VBench (Huang et al., 2024) and VBench-2.0 (Zheng et al., 2025b) to provide broader evaluation of video quality. VBench decomposes video quality into temporal and frame-wise aspects. For temporal quality, Subject Consistency measures whether the main subject maintains stable appearance across frames, computed via DINO (Caron et al., 2021) feature similarity. Background Consistency evaluates the stability of the background using CLIP (Radford et al., 2021) feature similarity. Motion Smoothness quantifies whether motion is physically plausible and continuous, using motion priors derived from video interpolation model (Li et al., 2023). Dynamic Degree measures the amplitude of motion in the generated video, estimated with RAFT (Teed & Deng, 2020)-based optical flow. For frame-wise quality, Aesthetic Quality captures perceptual attractiveness such as composition and color harmony, evaluated with the LAION aesthetic predictor (Beaumont & Schuhmann, 2022). Imaging Quality assesses low-level fidelity by detecting distortions such as blur, noise, or overexposure using MUSIQ (Ke et al., 2021) trained on the SPAQ (Fang et al., 2020) dataset. From VBench-2.0, we additionally include Human Anatomy, which evaluates whether human instances are consistently maintained without abnormal merging, splitting, or deformation across frames. This is achieved by detecting humans, hands, and faces with YOLO-World (Cheng et al., 2024), and applying anomaly detectors trained on large-scale dataset of real and generated human samples. The final score is defined as the proportion of frames not flagged as abnormal. F.3 EVALUATION DATASET Fig. 29 and Fig. 30 illustrate the benchmark we used for the evaluation, consisting of 118 imageprompt pairs. These pairs were constructed by selecting images with varying number of instance 31 MATRIX: Mask Track Alignment for Interaction-aware Video Generation IDs (2, 3 or 4), and by categorizing motions from simple to complex based on levels of contact and dynamism, such as walking along the street (low contact and low dynamism) or hands over the cup (hight contact and hight dynamism). Each prompt was designed to include (1) main subjects and objects involved in the interaction, (2) the interaction or motion descriptions between the main subjects and objects, and (3) scene description specifying the appearance of the main instances. For all images, we used large language model (LLM) to generate prompts that satisfy these conditions, following the same guidelines used during our dataset curation process in Sec. 3 of the main paper and the analysis evaluation dataset curation process in Sec of Appendix. F.4 HUMAN EVALUATION Human evaluation details. We adopt Two-Alternative Forced Choice (2AFC) protocol (Blattmann et al., 2023; Chefer et al., 2025), where raters compare two videos sideby-side and select the better one. Two models are uniformly sampled from {CogVideoX-5BI2V (Yang et al., 2024), Open-Sora-I2V (Zheng et al., 2024), TaVid (Kim & Joo, 2025), Ours}, yielding all six model pairs. For each sampled pair, we randomly select textimage prompt from the InterGenEval evaluation set and generate one video per model using the same prompt. The left/right presentation order is randomized to avoid positional bias, and raters are not allowed to skip or assign ties. Figure 19: Human Evaluation Results. Each trial consists of five evaluation questions: (1) Interaction Accuracy correctness of the specified interaction (who interacts with whom and what they are doing); (2) Semantic Grounding inclusion of objects indicated in the image prompt as instructed by the text prompt; (3) Semantic Propagation temporal consistency and absence of hallucinated objects; (4) Semantic Alignment overall fidelity and naturalness of the interaction; (5) Overall Quality perceptual realism and visual plausibility. Each participant evaluated all six model pairs with two prompts per pair, resulting in 6 2 = 12 video comparisons (12 pairs) per participant. This design ensured equal comparison frequency across models, providing balanced and fair evaluation protocol. Participants. We recruited 31 participants, each responding to multiple trials to cover all pairwise comparisons under diverse prompts. Results are aggregated using the win rate across pairwise comparisons and criteria, following standard practice in perceptual evaluation. Fig. 31 illustrates the 2AFC setup. Human evaluation results. Fig. 19 summarizes the win rates. Our model (MATRIX) consistently exceeds 0.9 across all criteria, while its backbone CogVideoX-5B-I2V remains around 0.360.44. This demonstrates substantial improvements in interaction-aware semantic alignmentcovering interaction accuracy, grounding, propagation, and alignmentas well as perceptual quality. Other baselines, such as Open-Sora and TaVid, show even lower performance. Overall, MATRIX not only inherits the strengths of CogVideoX but also delivers robust interaction fidelity and perceptual realism, validating the core contribution of our approach."
        },
        {
            "title": "G ADDITIONAL RESULTS",
            "content": "G.1 ADDITIONAL QUALITATIVE RESULTS Fig. 32 and Fig. 33 present the additional qualitative results comparing our method with others, including CogVideoX-2 B-I2V (Yang et al., 2024), CogVideoX-5B-I2V (Yang et al., 2024), OpenSora-I2V (Zheng et al., 2024) and TaVid (Kim & Joo, 2025). 32 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Table 4: Additional Quantitative Comparison. Methods CogVideoX-2B-I2V (Yang et al., 2024) CogVideoX-5B-I2V (Yang et al., 2024) Open-Sora-I2V (Zheng et al., 2024) TaVid (Kim & Joo, 2025) MATRIX (Ours) Human Fidelity HA () 0.937 0.938 0.893 0. 0.954 Video Quality SC () BC () MS () DD () AQ () IQ () 0.969 0.946 0.926 0. 0.962 0.962 0.942 0.937 0.939 0.956 0.993 0.986 0.992 0. 0.994 0.152 0.556 0.762 0.727 0.492 0.602 0.582 0.495 0. 0.587 69.69 69.66 63.32 68.90 69.73 G.2 ADDITIONAL QUANTITATIVE RESULTS AND ANALYSIS Tab. 4 reports additional quantitative comparisons across CogVideoX-2B-I2V (Yang et al., 2024), CogVideoX-5B-I2V (Yang et al., 2024), Open-Sora-I2V (Zheng et al., 2024), TaVid (Kim & Joo, 2025) and MATRIX (Ours), across the standard metrics of VBench (Huang et al., 2024). SC (subject consistency) and BC (background consistency) are highest for CogVideoX-2B-I2V, but this stems from its near-static outputs rather than stronger modeling. As shown in Fig. 20, little or no motion inflates inter-frame consistency and keeps AQ and IQ high because there is minimal motion-induced degradation. The motion metric Dynamic Degree (DD) confirms this with very low values. Very high DD is not always desirable either, since excessive motion increases off-track drift and hallucination risk. In Fig. 20, when comparable motion is introduced, SC, BC and AQ drop sharply, while human raters still prefer results that express the intended motion with correct bindings. Thus, SC, BC, AQ and IQ do not reliably track human preference in these settings. These metrics should be integrated alongside motion-aware and interaction-aware measures such as DD, KISA, SGI and IF. Our method maintains moderate DD and higher interaction fidelity, which aligns better with human judgments. Figure 20: Limitations of VBench metrics."
        },
        {
            "title": "H LIMITATIONS AND DISCUSSION",
            "content": "Instance Scalability. Our current framework supports up to five instance mask tracks per scene. While this upper bound appear restrictive, it is in face well aligned with the distribution observed in our dataset (Sec. A). As illustrated in Fig. 10 in Appendix, scenes containing more than five interacting instances are rare, and most examples contain two to four distinct objects involved in interaction. This design choice thus reflects practical tradeoff between generality and simplicity, allowing the model to remain effective without introducing unnecessary architectural complexity. Nevertheless, extending support to larger number of instances remains feasible direction for future work. Small Mask Sensitivity. Another limitation arises when the instance mask occupies very small spatial region. In such cases, the visual grounding signal may become weak or even ambiguous, potentially degrading the models ability to generate accurate motion. Future improvements could involve strategies such as hierarchical mask encoding, spatially adaptive attention or resolution33 MATRIX: Mask Track Alignment for Interaction-aware Video Generation aware learning to enhance robustness against object size variations. We leave these directions for future exploration. THE USE OF LARGE LANGUAGE MODELS (LLMS) In accordance with the ICLR 2026 submission policy, we disclose that Large Language Models were used to assist in grammar correction, polishing of the writing in this paper and caption processing in our dataset curation pipeline. Figure 21: Prompt Design for Interaction Identification and Instance Assignment. 34 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 22: Prompt Design for Interaction Scoring and Filtering. Figure 23: Prompt Design for Instance Description Extraction. Figure 24: Prompt Design for Vision-Language Verification. 35 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 25: Dataset Examples. 36 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 26: Additional Dataset Example. 37 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 27: Prompt design for KISA and SGI in Evaluation Protocol. 38 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 28: Prompt design for SPI in Evaluation Protocol. 39 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 29: Generated Evaluation Dataset Pairs Example. Figure 30: Sampled Evaluation Dataset Pairs Example. MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 31: An example of human evaluation. 41 MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 32: Additional Qualitative Results. MATRIX: Mask Track Alignment for Interaction-aware Video Generation Figure 33: Additional Qualitative Results."
        }
    ],
    "affiliations": [
        "KAIST AI"
    ]
}