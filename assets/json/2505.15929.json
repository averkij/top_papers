{
    "paper_title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
    "authors": [
        "Hui Shen",
        "Taiqiang Wu",
        "Qi Han",
        "Yunta Hsieh",
        "Jizhou Wang",
        "Yuyue Zhang",
        "Yuxin Cheng",
        "Zijian Hao",
        "Yuansheng Ni",
        "Xin Wang",
        "Zhongwei Wan",
        "Kai Zhang",
        "Wendong Xu",
        "Jing Xiong",
        "Ping Luo",
        "Wenhu Chen",
        "Chaofan Tao",
        "Zhuoqing Mao",
        "Ngai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy respectively-performance gaps exceeding 29\\% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 2 9 5 1 . 5 0 5 2 : r PHYX: Does Your Model Have the Wits for Physical Reasoning? Hui Shen1,2, Taiqiang Wu1, Qi Han3, Yunta Hsieh2, Jizhou Wang4, Yuyue Zhang3, Yuxin Cheng1, Zijian Hao3, Yuansheng Ni5, Xin Wang6, Zhongwei Wan6, Kai Zhang6, Wendong Xu1, Jing Xiong1, Ping Luo1, Wenhu Chen5, Chaofan Tao1, Z. Morley Mao2, Ngai Wong1 1The University of Hong Kong, 2University of Michigan, 3Independent, 4University of Toronto, 5University of Waterloo, 6The Ohio State University"
        },
        {
            "title": "Abstract",
            "content": "Existing benchmarks fail to capture crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PHYX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PHYX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave & acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5%, 42.2%, and 45.8% accuracy respectivelyperformance gaps exceeding 29% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement an evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation. More details are available on our project page: phyx-bench.github.io. 1 Introduction Physics is the most fundamental and all-inclusive of the sciences. Richard Feynman State-of-the-art models [13] now can basically solve Olympiad-level mathematical problems with human-competitive accuracy on benchmarks including AIME [4], GPQA [5], MATH-500 [6] and OlympiadBench [7], etc.. Emerging multimodal large language models (MLLMs) like GPT-4o [8] and Claude-3.7-Sonnet [9] further offer promising pathways by combining visual understanding into reasoning capabilities. Recent advances in multimodal foundation models have spurred the development of benchmarks assessing disciplinary knowledge [10] and mathematical problems[11 13]. However, these evaluations overlook critical dimension of machine intelligence: physical reasoning, the ability to integrate disciplinary knowledge, symbolic operations, and understanding of real-world constraints. Preprint. Figure 1: Accuracies of three leading MLLMs, two leading LLM and human performance on our proposed PHYX across 6 physical reasoning types and 6 domains. Figure 2: Sampled PHYX examples from each domain. Physical problem-solving fundamentally differs from pure mathematical reasoning or science knowledge question answering by requiring models to: (1) decode implicit conditions in the questions (e.g., interpreting \"smooth surface\" in the question as the coefficient of friction equals to zero), (2) maintain physical consistency across the reasoning chains since the laws of physics do not change with different reasoning trajectories. These differences arise from the fundamental distinction between physical reasoning and the more textual or abstract forms of reasoning emphasized in prior science-related and math-related benchmarks. More importantly, the capacity of physical reasoning assesses the model to ground the abstract physical formulas in the real-world visionary scenarios. It typically demands tight integration of visual perception (\"Is the surface rough or smooth?\"), material properties (\"Will the wooden block float?\"), and dynamic simulations (\"How will dominoes cascade?\"). To address these gaps, we present PHYX, the first large-scale benchmark designed for evaluating physics-based reasoning via multimodal problem-solving with three core innovations: (1) 3,000 newly collected questions with realistic physical scenarios requiring integrated visual analysis and causal reasoning, (2) Expert-validated data design covering six fundamental physics domains with representative examples illustrated in Figure 2, and six distinct physical reasoning types, Physical Model Grounding Reasoning, Spatial Relation Reasoning, Multi-Formula Reasoning, Implicit Condition Reasoning, Numerical Reasoning, and Predictive Reasoning and (3) Strict unified three-step evaluation protocols, account for varying instruction-following capabilities across models and enables accurate assessment of reasoning. Each scenario undergoes rigorous validation by physics Ph.D. students to ensure scientific accuracy while eliminating dataset bias. 2 Figure 3: Comparison with existing physics benchmarks. Realistic refers to the extent to which the dataset contains visually realistic physical scenarios. Size indicates the number of physics questions with images in multimodal benchmarks or total physics questions in text-only benchmarks. For evaluation methods, R: rule-based, M: model-based. For question type, OE: Open-ended, MC: Multiple-choice, FB: Fill-in-the-blank, J: Judgement. Upon comparison, PHYX leads in all aspects. In addition to MLLMs, our benchmark supports to evaluate LLMs by translating the images into text descriptions, thereby enabling an assessment of LLMs on these visually-grounded tasks. Our evaluation of 16 foundation models reveals an unprecedented capability gap: While the worst performance group of physics undergraduates and graduates achieve 75.6% accuracy, the bestperforming MLLM (GPT-o4-mini) scores only 45.8%. This 30-point performance chasm persists across all physics domains, most notably in Modern Physics (human 86.7% vs. model 40.6%) and Wave/Acoustics (human 86.7% vs. model 52.7%), as shown in Figure 1. These results expose three critical shortcomings in current multimodal reasoning frameworks: (1) Visual reasoning errors (39.6%) indicate that models frequently misinterpret visual context, underscoring their limited capability in accurately extracting and reasoning from realistic physical scenarios. (2) The inconsistent performance across input variationsspecifically, Full-Text, Text-DeRedundancy, and Text-Minimaldemonstrates that MLLMs remain overly dependent on textual descriptions, failing to effectively leverage visual input for reasoning. (3) Comparing physical reasoning performance to mathematical reasoning benchmarks such as MathVerse [13] and MATH-V [11] reveals that physical reasoning poses significantly greater challenges, highlighting critical need for improved integration of abstract concepts and real-world knowledge. PHYX thus provides both diagnostic toolkit for model improvement and roadmap for developing physically-grounded AI systems. Our contributions can be summarized as follows: Novel Benchmark Design: We introduce PHYX, the first large-scale benchmark for evaluating the reasoning capabilities in the physical world for both multi-modal models and language models. Curated by experts, it spans 25 fine-grained domains and 6 reasoning types with realistic scenarios. Versatile Evaluation Framework: PHYX supports versatile evaluation frameworks, including assessment formats (multiple-choice vs. open-ended) and hierarchical answer judge (rule-based and model-based). It also seamlessly integrates with mainstream toolkits (e.g., VLMEvalKit) for reproducible benchmarking. Critical Insights on Reasoning: We provide granular performance analysis and reveal some interesting observations, which sheds light on the design of the future models that jointly consider the disciplinary knowledge, symbolic operations, and real-world constraints for high-level physical reasoning."
        },
        {
            "title": "2 The PhyX Benchmark",
            "content": "2.1 Overview of PHYX We introduce PHYX, novel benchmark meticulously curated to assess the physical reasoning capabilities of foundation models. PHYX consists of 3,000 visually-grounded physics questions, meticulously curated to cover six distinct physics domains including Mechanics (550), Electromagnetism (550), Thermodynamics (500), Wave/Acoustics (500), Optics (500), and Modern Physics (400). Each problem in PHYX is centered around realistic physical scenarios to robustly assess the models ability to reason the physical world. Detailed data statistics are summarized in Table 1, with representative question examples from each domains illustrated in Figure 2. To enable comprehensive 3 Figure 4: Existing benchmarks that contain physics-related questions suffer from information redundancy and abstract representation. In contrast, de-redundancy in PHYX benchmark increases the difficulty, as models can perceive concepts from ONE modality only. Additionally, realistic visuals provide authentic context that challenges models to accurately apply physical laws. Table 1: Key Statistics of PHYX. Statistic Number Total new questions - Multiple-choice questions - Open-ended questions 6,000 3,000 (50.0%) 3,000 (50.0%) Unique number of images Unique number of questions Maximum description length Maximum question length Maximum option length Average description length Average question length Average option length 3,000 3, 288 119 46 48.3 14.6 11.2 Figure 5: Fine-grained Distribution of PHYX. assessment, each question within PHYX has been categorized into six well-defined physical reasoning types: Physical Model Grounding Reasoning, Spatial Relation Reasoning, Multi-Formula Reasoning, Implicit Condition Reasoning, Numerical Reasoning, and Predictive Reasoning. Detailed definitions and illustrative examples of these reasoning types are provided in Appendix C.4. Through its carefully curated structure and extensive coverage of diverse reasoning dimensions, PHYX represents robust resource for systematically benchmarking and advancing the capabilities of foundation models in realistic physical reasoning tasks. 2.2 Data Curation Process Data Collection. To ensure high-quality data, we design four-stage data collection process. Firstly, we conducted an in-depth survey of core physics disciplines to determine the coverage of our benchmark. We selected diverse physics domains and subfields, and defined set of reasoning types. Secondly, we recruited team of graduate students in STEM fields to serve as expert annotators. Annotators are instructed to comply with copyright and licensing rules by avoiding content from sources that restrict copying or redistribution. To mitigate potential data contamination in foundation models, they are also advised to select questions for which answers are not immediately available alongside the problem, such as those found in separate materials or at the end of textbooks. Then, each open-ended question is required to be converted into multiple-choice version, and vice versa. We also constructed three parallel versions of each question: (1) the original version as it appears in the textbook; (2) concise version where redundant textual informationthose duplicated by the corresponding imagewas removed; and (3) question-only version that retains only the core question. Lastly, to support evaluation of LLMs and facilitate multi-modal understanding, we used 4 Figure 6: An real example of reasoning trajectory based on GPT-4o and the comparison of required capabilities when solving physical and mathematical problems. GPT-4o to generate descriptive captions for each image, aim to summarize the visual content in self-contained textual form. This data curation process results in diverse collection of 3,300 questions from various sources. The detailed annotation protocol is in Appendix F. Data Quality Control. To further control the quality of our data, we perform three-stage data cleaning process. First, we detect potentially duplicated questions by analyzing lexical overlap, followed by manual review from physics Ph.D. students to confirm and remove duplicates. Then, we filter out the shortest 10% of questions based on their textual length. This rigorous process plays crucial role in maintaining the quality and difficulty of PHYX. 2.3 Key Difference Compared to Existing Benchmarks Compared with Scientific Knowledge Benchmarks. From Figure 3, science benchmarks like MMMU [10] cover broad disciplinary reasoning but lack focus on deep reasoning capability. These benchmarks often rely on memorization and basic understanding of disciplinary knowledge, with tasks that prioritize factual recall or simple cross-modal association. In contrast, PHYX specializes in university-level hard questions through high-fidelity visual scenarios. Unlike generalist benchmarks, our tasks demand integration of visual cues with implicit physical laws, requiring models to surpass mere knowledge recall and perform nuanced, context-driven inference. This targeted design evaluates true multimodal reasoning about the physical world, exposing gaps in models ability to handle professional-level scientific challenges. Compared with Mathematical Reasoning Benchmarks. Mathematical reasoning benchmarks, such as MathVista [13], MathVerse [12], and MATH-V [11], focus on logical deduction with clear expressions and explicit conditions, representing subset of the challenges in physical reasoning. Physical reasoning, as evaluated by PHYX, extends beyond these by requiring models to model real-world contexts (e.g., dynamic physical systems), identify implicit conditions from visual cues (e.g., Figure 6), and integrate the application of physical laws with symbolic logic, which are key capabilities absent in purely mathematical tasks. This makes PHYX more comprehensive test of multimodal reasoning, capturing the complexity of real-world physics problems. Compared with Physics-related Benchmarks Existing benchmarks (e.g., PHYBench [14], UGPhysics [15], OlympiadBench [7]) prioritize text-based problems or schematic visuals, limiting their assessment of multimodal reasoning. In details, PHYBenchs problems and UGPhysicss questions rely heavily on textual descriptions, while OlympiadBenchs problems use simplified diagrams, as shown in Figure 4. These benchmarks mainly test disciplinary knowledge but overlook the integration of visual perception with implicit physical constraints. PHYX bridges these gaps by embedding high-fidelity visual scenarios that require models to decode complex visual cues, infer context5 Table 2: Accuracy scores on the testmini subset of PHYX. The highest scores of models in each section and the overall highest score are respectively highlighted in blue and red. Models Random Choice Human Expert (Worst) Human Expert (Medium) Human Expert (Best) Claude3.7-Sonnet Claude3.5-Sonnet Claude3.5-Haiku GPT-o4-mini GPT-4o InternVL3-78B Yi-VL-34B InternVL3-14B InternVL3-8B MiniCPM-o-8B LLaVA-OneVision-7B DeepSeek-VL2-4.5B Kimi-VL-A3B-Instruct-2.8B DeepSeek-R1 DeepSeek-V3 GPT-o3-mini Full-Text Text-DeRedundancy Text-Minimal Open-Ended Multi-Choice Open-Ended Multi-Choice Open-Ended Multi-Choice - - - - 44.4 40.2 7.9 49.0 33.9 35.9 3.5 9.0 6.3 7.1 7.2 11.4 15.6 51.8 40.7 36.9 25 - - - - 75.6 77.8 78.9 Multimodal Large Language Models 65.8 62.6 37.0 87.9 61.0 45.6 34.8 46.9 45.5 31.8 37.7 28.2 37.1 42.2 39.0 13.6 45.8 32.5 33.1 3.4 7.9 6.5 7.2 5.7 10.2 15.4 Large Language Models 63.1 70.8 78.5 51.2 36.3 31.5 25 - - - 64.5 63.5 37.5 86.9 57.6 46.9 34.1 47.5 44.9 31.6 37.3 27.8 38.7 62.9 67.5 76.9 - - - - 17.2 17.0 5.5 24.1 14.3 14.8 1.9 5.1 4.6 3.2 2.7 4.7 8.1 22.2 16.2 14.3 25 - - - 41.6 43.5 31.7 62.6 43.8 40.5 34.1 45.9 44.0 34.2 38.0 27.3 39.3 43.6 49.9 56.2 specific physical laws and then reasoning problems. Unlike existing datasets, PHYX mandates equal reliance on both modalities with information de-redundancy, providing rigorous evaluation of professional-level physical reasoning in multimodal large language models."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setup The testmini Subset PHYX comprises 3,000 high-quality visual physics problems and 18,000 corresponding test instances. To streamline evaluation and accelerate model development validation, we extract smaller representative subset named testmini including 1,000 problems and 6,000 instances. The construction of testmini involved proportional random sampling strategy across different physics domains of PHYX. The quantitative evaluations in all subsequent experiments were assessed on this testmini subset. Baselines. We include random chance as naive baselines. Additionally, we recruiting 15 undergraduate and graduate physics students to represent the expert performance baseline, each student was tasked with completing 18 questions. The students were divided into three groups of five, and the results of each group are reported separately. Then, we conduct experiments on (a) Reasoning MLLMs: GPT-o4-mini [16], Claude-3.7-Sonnet [9], LLaVA-OneVision-7B [17] MiniCPM-o [18], (b) General MLLMs: GPT-4o [8], Claude-3.5-Sonnet [19], Claude-3.5-Haiku [20], InternVL3 [21], Yi-VL-34B [22], (c) LLMs: o3-mini [23], DeepSeek-R1 [1], DeepSeek-V3 [24], Qwen-3-4B [25], augmented with image captions generated by GPT-4o. 3.2 Evaluation Protocols Our evaluation is conducted with Chain-of-Thought (CoT) prompting to assess the reasoning capability of models. For both open-ended (OE) and multiple-choice (MC) questions, the instructionfollowing capabilities of models can vary significantly. To this end, we design universal evaluation pipeline for all recent LLMs and MLLMs with different instruction-following capabilities: Table 3: Average scores by model across different domains of physics with open-ended text deredundancy questions. The highest scores of models in each section and the overall highest score are respectively highlighted in blue and red. Models Overall Mechanics Electromagnetism Thermodynamics Waves & Acoustics Optics Modern Physics Human Expert (Worst) Human Expert (Medium) Human Expert (Best) Claude3.7-Sonnet Claude3.5-Sonnet Claude3.5-Haiku GPT-o4-mini GPT-4o InternVL3-78B Yi-VL-34B InternVL3-14B InternVL3-8B MiniCPM-o-8B LLaVA-OneVision-7B DeepSeek-VL2-4.5B Kimi-VL-A3B-Instruct-2.8B DeepSeek-R1 DeepSeek-V3 GPT-o3-mini Qwen3-8B 75.6 77.8 78.9 42.2 39.0 13.6 45.8 32.5 33.1 3.4 7.9 6.5 7.2 5.7 10.2 15.4 51.2 36.3 31.5 27.5 76.5 94.1 76.5 60.0 53.3 86.7 66.7 60.0 73. Multimodal Large Language Models 58.2 53.5 18.8 52.3 45.9 48.8 1.8 12.4 10.6 11.8 10.6 16.5 20.6 36.7 27.8 8.9 43.2 24.3 27.2 3.5 8.88 6.5 6.5 4.1 7.1 10.1 Large Language Models 71.8 52.9 41.8 42.9 53.2 39.6 24.9 23. 31.5 33.3 11.5 41.8 26.1 25.5 4.8 4.2 3.6 6.1 6.1 10.3 13.3 41.8 28.5 23.6 21.2 86.7 93.3 86.7 46.7 49.7 18.8 52.7 53.9 43.0 2.4 8.5 4.9 7.3 7.3 13.3 20.0 53.9 36.4 32.1 35.8 69.2 76.9 69. 44.6 35.5 12.0 44.0 23.5 28.9 4.2 4.8 6.6 6.0 3.0 9.0 16.2 39.8 28.9 33.7 21.1 86.7 86.7 86.7 35.2 3.9 11.5 40.6 21.2 24.8 3.6 8.5 6.7 5.5 3.0 4.8 12.1 46.1 30.9 32.7 20.0 Step 1. Prediction Generation. Initially, the models generate prediction given the input query, which incorporates different problem description according to the specific settings, the question, and the image, using the template defined in Appendix D.1. Step 2. Answer Extraction. The raw predictions often contain reasoning steps, explanations, or irrelevant conversational filler. To precisely extract the definitive answer from these raw outputs, we separately employ rule-based answer extraction strategies, which are detailed in Appendix D.2. Step 3. LLM Judge. For OE questions, the next step is comparing the extracted answer against the ground truth to determine its correctness. Given that answers in OE physics questions can be expressed in myriad ways, we proposed an evaluation mechanism using LLM, such as DeepSeekV3 [24], as judge, using the template defined in Appendix D.3. We feeds the answer extracted and the ground truth to LLM multiple times and checks if LLM succeed in all attempts. preliminary study of 200 examples shows that DeepSeek-V3 can judge the answer with more than 99% accuracy with affordable costs. For MC questions, we first attempt to directly match the option letter. If this direct matching fails, we then use LLM as judge, using the template for OE questions. 3.3 Main Results In this section, we present comprehensive comparison of LLMs and MLLMs on PHYX benchmark, detailed in Table 2 and Table 3. Our key findings can be summarized as follows: Challenging Nature of PHYX. PHYX presents significant challenges for current models. Notably, even worst human experts achieve accuracy of 75.6%, significantly outperforming all the models included in our comparative analysis. This disparity demonstrates an existing gap between human expertise and current model capabilities, reflecting the demanding standards inherent in PHYX. Multiple-Choice Format Narrows the Performance Gap. The result reveals that multiple-choice questions reduce the performance gap across models, enabling weaker models to rely on surface-level cues. In contrast, open-ended questions demand genuine reasoning and precise answer generation, leading to greater differentiation between models. This suggests that the open-ended format provides higher discriminative power when evaluating multimodal reasoning capabilities. Model Performance across Different Domains. As shown in Table 3, in domains such as Waves/Acoustics and Mechanics, which typically include natural images and questions requiring 7 Figure 7: The error distribution over 90 annotated errors based on GPT-4o with typical visual reasoning error , which is easy for humans but challenging for GPT-4o. More examples can be found in the Appendix. relatively less reasoning, models tend to achieve higher performance. Conversely, in domains such as Thermodynamics and Modern Physics, where tasks frequently demand intricate visual perception and multi-step reasoning, models performance is generally lower. 3.4 Discussion Reasoning-oriented Models Perform Better. Leading reasoning-oriented models such as GPTo4-mini and DeepSeek-R1 achieve accuracies of 45.8% and 51.2%, respectively, significantly outperforming general-purpose models like GPT-4o and Claude3.7-Sonnet. The results highlight the advantage of models specifically optimized for reasoning tasks, suggesting that architectural and training differences play key role in bridging the multimodal reasoning gap. LLMs Achieve Competitive Results. Despite lacking direct visual input, LLMs such as DeepSeekR1 and GPT-o3-mini perform competitively with most multimodal models. The strong performance of LLMs suggests that, in many cases, the caption provides sufficient visual context for reasoning. This highlights both the impressive generalization capabilities of LLMs and the current limitations of MLLMs in leveraging raw visual signals for physical reasoning. MLLMs Physical Reasoning Relies More on Text. Our experiments show clear performance gradient across the three input variations: Full-Text, Text-DeRedundancy, and Text-Minimal, with decreasing accuracy in that order. This indicates that MLLMs rely heavily on detailed textual descriptions, highlighting their limited ability to reason purely from visual context. Physical Reasoning Poses Greater Challenges than Mathematical Reasoning. Comparing GPT4os performance on our physical reasoning dataset to its previously reported results on MathVista (63.8%) and MATH-V (63.8%), we observe notably lower accuracy in physical reasoning tasks. This finding emphasizes that physical reasoning inherently requires deeper integration of abstract concepts and real-world knowledge, presenting more substantial challenge for current models compared to purely mathematical contexts. 3.5 Error Analysis To dive into the reasoning capabilities and limitations of models, we meticulously inspected 96 randomly sampled incorrect predictions and performed an in-depth analysis based on GPT-4o. The objectives of this analysis were twofold: to identify current model weaknesses and to guide future enhancements in model design and training. The distribution of these errors is illustrated in Figure 7, and comprehensive case study of 30 notable cases is included in Appendix E. Visual Reasoning Errors (39.6%) arise from the models incorrect extraction, spatial relationships, or reasoning based on visual information from realistic physical questions included in PHYX. notable instance of this can be observed in Appendix 8, where the model misread the voltage value shown in the image, leading to numerical error in its calculation. Given the realistic nature of our images, visual reasoning errors constitute larger proportion of mistakes, posing significant new challenge to MLLMs compared to existing benchmarks. 8 Text Reasoning Errors (13.5%) are characterized by incorrect processing or interpretation of textual content. The model occasionally struggles with implicit conditions, or incorrectly handles logical relationships presented in text form. An example of this can be illustrated in Appendix 4, where the model overlooked the explicit instruction to ignore friction and instead reasoned that the coefficient of friction was required to solve the problem. This highlights areas for improved textual inference and contextual reasoning are critical to address these shortcomings. Lack of Knowledge (38.5%) reflects GPT-4os incomplete understanding of specific domain knowledge. As demonstrated in the example in Appendix 25, the model lacks the fundamental knowledge that difference in wave speeds across media invalidates direct geometric reasoning based on symmetric travel paths. Specifically, it ignores that the slower speed in the liver requires correction when estimating depth from the reflection geometry, leading to an overestimated result. Calculation Error (8.3%) refer to mistakes in arithmetic operations, formula application, or unit conversions. These errors indicate that the model has grasped the physical context and relevant concepts but fails in the final step of numerical computation."
        },
        {
            "title": "4 Related Work",
            "content": "Multi-modal Large Language Models. Multi-modal large language models (MLLMs) [9, 16, 3] have shown great potential in wide achieved excellent visual understanding ability by integrating both visual and textual data in wide range of multimodal tasks. Recent advances in LLMs have motivated efforts [26, 27] to explore MLLM reasoning. Despite such achievements, it remains unclear whether these models truly possess advanced reasoning abilities when solving the visual tasks, especially in the physical area that is closer to the real world. To bridge this gap and comprehensively evaluate the physical reasoning capabilities of MLLMs, we introduce PHYX, multimodal benchmark to evaluate the real reasoning ability of recent advanced MLLMs in physics. LLM Benchmarks. Several benchmarks [28, 29, 5, 30, 31] have been proposed to evaluate LLMs ability on various aspects. Among these works, the most related one is PHYBench [14], which also focuses in the physic reasoning area. Although evaluating the same discipline, their scope remains narrow since it includes only small number of questions, making it insufficient to fully assess models reasoning capabilities. Furthermore, PHYBench concentrates exclusively on evaluating the understanding of physics concepts by language models through text. However, in real-world scenarios, solving physics problems also requires visual perception and interpretation. MLLM Benchmarks. Recently, several MLLM scientific benchmark [10, 32, 7, 3335] have also been proposed. For example, PhysReason [34] includes multimodal subset of 972 physics problems with figures to evaluate the MLLMs. EMMA [35] composes 2,788 problems covering various scientific area such as mathematics, physics, and coding. However, all of these benchmarks only contain small subset of data in physics area, which still could not fully evaluate the MLLMs ability on reasoning and solving the advanced physics problems."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "Existing benchmarks have overlooked the critical task of physical reasoning, which requires integrating domain knowledge, symbolic reasoning, and real-world constraints. To address this, we present PHYX, the first large-scale benchmark for evaluating physical reasoning in multimodal, visually grounded scenarios. Through rigorous evaluation, we reveal that state-of-the-art models exhibit significant limitations in physical reasoning, relying predominantly on memorized knowledge, mathematical formulas, and superficial visual patterns, rather than genuine understanding of physical principles. Our findings highlight the urgent need for future models to improve deep physical reasoning over surface-level associations, guiding the development of more intelligent models. On the other hand, our benchmark focuses exclusively on English-language prompts and annotations. While this aligns with the dominant language used in most foundation models, it is not suitable for assessing models reasoning ability in other languages. Also, the images in our dataset depict physically realistic scenarios but are often schematic or textbook-style rather than real-world photographs. While suitable for evaluating conceptual reasoning, this may not fully capture the complexity of perception in natural environments."
        },
        {
            "title": "References",
            "content": "[1] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [2] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. [3] Gemini Team. Gemini 2.5: Our most intelligent ai model, 2025. URL https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025/ #gemini-2-5-thinking. [4] MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/ math-competitions/american-invitational-mathematics-examination-aime. [5] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [6] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [7] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, 2024. [8] OpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. [9] claude. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/ claude-3-7-sonnet, 2025. [10] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [11] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [12] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [13] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations. [14] Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, et al. Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv preprint arXiv:2504.16074, 2025. [15] Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, and Yang Wang. Ugphysics: comprehensive benchmark for undergraduate physics reasoning with large language models. arXiv preprint arXiv:2502.00334, 2025. 10 [16] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, 2025. [17] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [18] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [19] claude. Introducing claude 3.5 sonnet. https://www.anthropic.com/news/ claude-3-5-sonnet, 2024. [20] claude. Claude 3.5 haiku. https://www.anthropic.com/claude/haiku, 2024. [21] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [22] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. [23] OpenAI. Openai o3-mini: Pushing the frontier of cost-effective reasoning. https://openai. com/index/openai-o3-mini/, 2025. [24] DeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412. 19437. [25] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [28] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and In International Jacob Steinhardt. Measuring massive multitask language understanding. Conference on Learning Representations. [29] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. Scieval: multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1905319061, 2024. [30] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. 11 [31] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911. [32] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. In International Conference on Machine Learning, pages 5062250649. PMLR, 2024. [33] Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, et al. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. Advances in Neural Information Processing Systems, 37:1920919253, 2024. [34] Xinyu Zhang, Yuxuan Dong, Yanrui Wu, Jiaxing Huang, Chengyou Jia, Basura Fernando, Mike Zheng Shou, Lingling Zhang, and Jun Liu. Physreason: comprehensive benchmark towards physics-based reasoning. arXiv preprint arXiv:2502.12054, 2025. [35] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025."
        },
        {
            "title": "Table of Contents in Appendix",
            "content": "A Ethics Statement Broader Impacts More Dataset Details C.1 Question Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Introduction of Domain and Subfield . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Images by Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Physical Reasoning Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Evaluation Details D.1 CoT Prompting for Generating Answer . . . . . . . . . . . . . . . . . . . . . . . D.2 Rule-based Answer Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Prompt for Answer Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Prompt for Caption Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Prompt for Reasoning Type Labeling . . . . . . . . . . . . . . . . . . . . . . . . . Case Study Data Annotation Protocol F.1 Data Collection . . . F.2 General Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Data Format and Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Quality Control and Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5 Handling Ambiguities . F.6 Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.7 Data Contamination Considerations . . . . . . . . . . . . . . . . . . . . . . . . . 14 14 14 14 15 22 22 22 22 22 27 58 58 58 58 58 59"
        },
        {
            "title": "A Ethics Statement",
            "content": "Legal Compliance. All questions included in PHYX are sourced from publicly accessible materials. During data collection, annotators are instructed to strictly follow the copyright and licensing terms of the original platforms. Any content from sources that prohibit reuse or redistribution MUST be explicitly excluded. PHYX is non-commercial project, and its usage aligns with the principles outlined in Fair Use 107: \"the fair use of copyrighted work, including such use by ...... scholarship, or research, is not an infringement of copyright\", where fair use is determined by \"the purpose and character of the use, including whether such use is of commercial nature or is for nonprofit educational purposes\" and \"the effect of the use upon the potential market for or value of the copyrighted work.\" Dataset Intended Usage and License. The full details of the PHYX dataset are presented in this paper, and both the PHYX and code for reproducing results will be made publicly available. The PHYX dataset is not supposed to be used to train models for cheating. The primary goal is to support the research community in benchmarking and advancing physical reasoning in LLMs and MLLMs. We take full responsibility for any rights violation that may arise. Both the PHYX data and our open-source code are released under the MIT license."
        },
        {
            "title": "B Broader Impacts",
            "content": "Our benchmark aims to advance the evaluation of MLLMs in the domain of physical reasoning. By focusing on realistic visual scenarios grounded in physics, we hope to contribute toward the development of AI systems with stronger scientific reasoning capabilities, which is an essential step for applications in education, science tutoring, and automated scientific discovery. In particular, this benchmark may support the design of models that assist learners in understanding complex physical concepts through both text and visuals. Potential negative impacts are limited but worth noting. First, as our dataset is curated entirely in English, it may not generalize well to non-English-speaking contexts, inadvertently reinforcing language bias. Then, the scenarios in our dataset are schematic rather than real-world images, which may limit generalization to real-world physical perception tasks. We believe these concerns are manageable and do not diminish the broader positive potential of the benchmark in promoting robust, multimodal physical reasoning in foundation models."
        },
        {
            "title": "C More Dataset Details",
            "content": "C.1 Question Distribution All questions in PHYX are written in English. Figure 8 presents the distribution of word counts of questions in Text-DeRedundancy setting, demonstrating the variation in question lengths. The similarity between the median and average word counts suggests roughly symmetrical distribution. C.2 Introduction of Domain and Subfield As shown in Table 4, PHYX covers 6 core domains and 25 subdomains. Mechanics. Mechanics is the branch of physics concerned with the motion of objects and the forces that cause or change this motion. It encompasses both classical mechanics and key subfields such as Kinematics (e.g., velocity, acceleration, free fall), Dynamics (e.g., Newtons laws, force analysis, friction), Work and Energy (e.g., work-energy theorem, mechanical energy conservation), Momentum and Collisions (e.g., conservation of momentum, elastic and inelastic collisions), Rotational Motion (e.g., torque, angular acceleration, moment of inertia), and Statics (e.g., torque balance, structural analysis). Mechanics lays the groundwork for much of physics, enabling the understanding of how and why objects move or remain at rest in various physical systems. Electromagnetism. Electromagnetism explores the interactions between electric charges and magnetic fields. It includes the subfields of Electrostatics (e.g., Coulombs law, electric fields and potential), Electric Circuits (e.g., Ohms law, circuit analysis, RC circuits), Magnetism (e.g., magnetic 14 Figure 8: The distribution of the number of words per question in PHYX. fields, Lorentz force, Amp√®res law), Electromagnetic Induction (e.g., Faradays law, Lenzs law, inductance), and optionally, Maxwells Equations and Electromagnetic Waves for advanced topics. This domain underpins much of modern technology, including electric circuits, motors, and wireless transmission. Thermodynamics. Thermodynamics is the study of heat, energy, and their transformations. Its subtopics include Temperature and Heat Transfer (e.g., conduction, convection, radiation), Specific Heat and Calorimetry (e.g., phase changes, heat calculations), Laws of Thermodynamics (e.g., energy conservation, entropy), and Ideal Gases and Kinetic Theory (e.g., gas laws, internal energy, pressure). This domain is central to engines, thermal systems, and understanding natural processes. Wave/Acoustics. This domain investigates wave behavior and sound phenomena. Core subfields include Wave Properties (e.g., speed, frequency, wavelength, interference), Sound (e.g., pitch, loudness, Doppler effect, standing waves), and Resonance and Harmonics (e.g., resonant frequencies, vibrations in strings and air columns). These concepts are crucial in fields ranging from acoustics to telecommunications. Optics. Optics studies the behavior and properties of light. It includes Geometrical Optics (e.g., reflection, refraction, lens imaging, total internal reflection), Wave Optics (e.g., interference, diffraction, polarization), and Optical Instruments (e.g., microscopes, telescopes, image formation). Optics has broad applications in imaging, vision science, and photonics. Modern Physics. Modern Physics addresses phenomena beyond the scope of classical mechanics. Its key subfields include Relativity (e.g., time dilation, mass-energy equivalence), Quantum Phenomena (e.g., photoelectric effect, atomic models), Nuclear Physics (e.g., radioactivity, nuclear reactions, mass defect), and optionally Particle Physics (e.g., elementary particles, the Standard Model). These topics form the theoretical basis of contemporary physics and technology. C.3 Images by Domains In this section, we present images example from the physics problems in PHYX. Figure 9, Figure 10, Figure 11, Figure 12, Figure 13 and Figure 14 show images from the problems under the category of Mechanics, Electromagnetism, Thermodynamics, Wave/Acoustics, Optics, Modern Physics, respectively. We observe that the images in our dataset are highly realistic, often depicting concrete physical scenarios rather than stylized or abstract illustrations. While they are not real-world photographs, these visuals are grounded in plausible physical settings. This realism provides essential context for physical reasoning and helps bridge the gap between abstract physics principles and their real-world manifestations. Across domains, the visual characteristics vary in alignment with the nature of the physical concepts. Despite their domain-specific variations, unifying theme across all categories is the consistent use of realistic and context-rich imagery, which provides essential grounding for physical interpretation and distinguishes our benchmark from other datasets with overly synthetic or schematic visual content. 15 Figure 9: Examples of the visual context for the Mechanics domain. 16 Figure 10: Examples of the visual context for the Electromagnetism domain. Figure 11: Examples of the visual context for the Thermodynamics domain. 18 Figure 12: Examples of the visual context for the Wave/Acoustics domain. 19 Figure 13: Examples of the visual context for the Optics domain. Figure 14: Examples of the visual context for the Modern Physics domain. 21 Domain"
        },
        {
            "title": "Optics",
            "content": "Optical Instrument, Wave Optics, and Geometrical Optics Subfields"
        },
        {
            "title": "Electromagnetism",
            "content": "Electromagnetic Wave, Electric Circuits, Magnetism, Electromagnetic Induction, and Electrostatics"
        },
        {
            "title": "Mechanics",
            "content": "Momentum and Collisions, Work and Energy, Statics, Dynamics, Relational Motion, and Kinematics. Wave/Acoustics Sound, Resonance and Harmonics, and Wave Properties"
        },
        {
            "title": "Thermodynamics",
            "content": "Specific Heat and Calorimetry, Temperature and Heat Transfer, Ideal Gases and Kinetic Theory, and Laws of Thermodynamics"
        },
        {
            "title": "Modern Physics",
            "content": "Particle Physics, Nuclear Physics, Relativity, and Quantum Phenomena Table 4: Subfields included in each domain in PHYX. Figure 15: CoT prompting for generating answer. C.4 Physical Reasoning Definition Six physical reasoning types are defined in Table 5."
        },
        {
            "title": "D More Evaluation Details",
            "content": "We conduct all experiments on NVIDIA A100 80G GPUs. D.1 CoT Prompting for Generating Answer The CoT prompting for generating answer is shown in Figure D.1. D.2 Rule-based Answer Extraction The rule-based answer extraction strategies for MC and OE questions are shown in Figure 16 and Figure 17, respectively. D.3 Prompt for Answer Judge The prompt for answer judge is shown in Figure 18. D.4 Prompt for Caption Generation The prompt for caption generation is shown in Figure 19 D.5 Prompt for Reasoning Type Labeling The prompt for reasoning type labeling is shown in Figure 20 and Figure 21 Physical Reasoning"
        },
        {
            "title": "Spatial Relation\nReasoning",
            "content": "Multi-Formula Reasoning Implicit Condition Reasoning Numerical Reasoning Description This reasoning involves connecting the specific details of problem description to fundamental physical concepts, laws, and idealized models. Its the process of identifying which area of physics is relevant and selecting the appropriate simplified representations that allow the problem to be analyzed using established physical principles and equations. Essentially, it translates real-world or described scenario into solvable physics framework. This focuses on understanding and manipulating the geometric and directional aspects of physics problem. It involves visualizing the setup, determining the positions, orientations, distances, angles, and relative movements of objects. This often requires using coordinate systems, vectors (including resolving them into components), and geometric principles. This reasoning type is required when problem cannot be solved using single physics equation. It involves identifying multiple relevant formulas or principles and understanding how they interrelate. The process typically involves using the output of one formula as the input for another, or setting up and solving system of simultaneous equations derived from different physical laws. This involves recognizing and utilizing information or constraints that are not explicitly stated in the problem text but are implied by the context, standard physics assumptions, or specific keywords. Examples include understanding that \"starts from rest\" means the initial velocity is zero, \"smooth\" surface implies zero friction, \"light string\" or \"light pulley\" means its mass is negligible, or that an object reaching its maximum height has momentary vertical velocity of zero. This reasoning refers to problems where solving requires the application of advanced mathematical methods beyond basic algebra and trigonometry. This includes techniques such as calculus, solving differential equations that model the system, vector calculus, Fourier analysis, linear algebra for complex systems, or other higher-level mathematical procedures necessary to manipulate the physical formulas and arrive at solution. This applies when the mathematical technique itself is core part of solving the physics, regardless of whether the final answer is purely numerical or symbolic. Predictive Reasoning This involves using established physical laws and the initial conditions of system to forecast its future state or behavior. Based on the principles governing the situation, you calculate or deduce what will happen after certain time or interaction. Examples include predicting the trajectory of projectile, the final temperature of mixture after thermal equilibrium is reached, or the velocity of objects after collision. Table 5: Definitions of six physical reasoning categories in PHYX. Figure 16: Rule-based answer extraction strategy for MC questions. Figure 17: Rule-based answer extraction strategy for OE questions. 24 Figure 18: Rule-based answer extraction strategy for OE questions. Figure 19: Prompt template for caption generation. Figure 20: Prompt for reasoning type labeling (1). Figure 21: Prompt for reasoning type labeling (2)."
        },
        {
            "title": "List of Case Study Figures",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1 Mechanics 1: Correct Case . 29 2 Mechanics 2: Correct Case . 30 3 Mechanics 3: Visual Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 4 Mechanics 4: Text Reasoning Error 32 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Mechanics 5: Lack of Knowledge 33 6 Electromagnetism 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 7 Electromagnetism 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 8 Electromagnetism 3: Visual Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . 36 9 Electromagnetism 4: Text Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . 37 . . . . . . . . . . . . . . . . . . . . . . . . . 10 Electromagnetism 5: Lack of Knowledge 38 11 Thermodynamics 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 12 Thermodynamics 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 Thermodynamics 3: Visual Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . 40 14 Thermodynamics 4: Text Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . 41 42 15 Thermodynamics 5: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . 43 16 Wave/Acoustics 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 17 Wave/Acoustics 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 18 Wave/Acoustics 3: Visual Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . 46 19 Wave/Acoustics 4: Text Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . 47 20 Wave/Acoustics 5: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Optics 1: Correct Case . 49 22 Optics 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Optics 3: Visual Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 24 Optics 4: Text Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 52 25 Optics 5: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 26 Modern Physics 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 27 Modern Physics 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 28 Modern Physics 3: Visual Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . 56 29 Modern Physics 4: Text Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . 57 30 Modern Physics 5: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Domain Correct Mechanics 1, 2 Electromagnetism 6, 7 Thermodynamics Wave/Acoustics Optics Modern Physics 11, 12 16, 17 21, 22 26, 27 Visual Reasoning Error 3 8 13 18 23 28 Text Reasoning Error 4 9 14 19 24 Lack of Knowledge 5 10 15 20 25 30 Table 6: Table index of case study figures by domains with associated error categories. 27 Figure 1: sample correct case of Mechanics. Back to List of Figures Back to Table Index 28 Figure 2: sample correct case of Mechanics. Back to List of Figures Back to Table Index 29 Figure 3: sample error case of Mechanics. Error category: Visual Reasoning Error Back to List of Figures Back to Table Index 30 Figure 4: sample error case of Mechanics. Error category: Text Reasoning Error Back to List of Figures Back to Table Index 31 Figure 5: sample error case of Mechanics. Error category: Lack of Knowledge Back to List of Figures Back to Table Index 32 Figure 6: sample correct case of Electromagnetism. Back to List of Figures Back to Table Index Figure 7: sample correct case of Electromagnetism. Back to List of Figures Back to Table Index 34 Figure 8: sample error case of Electromagnetism. Error category: Visual Reasoning Error Back to List of Figures Back to Table Index 35 Figure 9: sample error case of Electromagnetism. Error category: Text Reasoning Error Back to List of Figures Back to Table Index 36 Figure 10: sample error case of Electromagnetism. Error category: Lack of Knowledge Back to List of Figures Back to Table Index 37 Figure 11: sample correct case of Thermodynamics. Back to List of Figures Back to Table Index 38 Figure 12: sample correct case of Thermodynamics. Back to List of Figures Back to Table Index 39 Figure 13: sample error case of Thermodynamics. Error category: Visual Reasoning Error Back to List of Figures Back to Table Index Figure 14: sample error case of Thermodynamics. Error category: Text Reasoning Error Back to List of Figures Back to Table Index 41 Figure 15: sample error case of Thermodynamics. Error category: Lack of Knowledge Back to List of Figures Back to Table Index Figure 16: sample correct case of Wave/Acoustics. Back to List of Figures Back to Table Index 43 Figure 17: sample correct case of Wave/Acoustics. Back to List of Figures Back to Table Index 44 Figure 18: sample error case of Wave/Acoustics. Error category: Visual Reasoning Error Back to List of Figures Back to Table Index 45 Figure 19: sample error case of Wave/Acoustics. Error category: Text Reasoning Error Back to List of Figures Back to Table Index 46 Figure 20: sample error case of Wave/Acoustics. Error category: Lack of Knowledge Back to List of Figures Back to Table Index 47 Figure 21: sample correct case of Optics. Back to List of Figures Back to Table Index 48 Figure 22: sample correct case of Optics. Back to List of Figures Back to Table Index 49 Figure 23: sample error case of Optics. Error category: Visual Reasoning Error Back to List of Figures Back to Table Index 50 Figure 24: sample error case of Optics. Error category: Text Reasoning Error Back to List of Figures Back to Table Index 51 Figure 25: sample error case of Optics. Error category: Lack of Knowledge Back to List of Figures Back to Table Index 52 Figure 26: sample correct case of Modern Physics. Back to List of Figures Back to Table Index 53 Figure 27: sample correct case of Modern Physics. Back to List of Figures Back to Table Index Figure 28: sample error case of Modern Physics. Error category: Visual Reasoning Error Back to List of Figures Back to Table Index 55 Figure 29: sample error case of Modern Physics. Error category: Text Reasoning Error Back to List of Figures Back to Table Index Figure 30: sample error case of Modern Physics. Error category: Lack of Knowledge Back to List of Figures Back to Table Index"
        },
        {
            "title": "F Data Annotation Protocol",
            "content": "This document outlines detailed procedure for annotating dataset of physics questions that include visual context. F.1 Data Collection Sources of Data. Data is collected from freely accessible online resources, textbooks, and other materials. Annotators are instructed to use wide range of sources rather than relying on just one. Types of Questions: Multiple-Choice Questions: These consist of question accompanied by four answer options, with only one being correct. For each multiple-choice question, annotators are also required to create corresponding open-ended version of the same problem. Open-Ended Questions: These include formats such as short-answer and calculationbased problems. Questions with excessively lengthy answers should be avoided. For each open-ended question, corresponding multiple-choice version should also be constructed. Image Types. The annotators should find images with realistic physical senarios. F.2 General Guidelines General Principles: Annotations should be accurate, uniform, and maintain high level of academic quality. Specific Instructions: All questions should be written in English. All questions must contain one physical images. All images in question should be realistic, in specific physical scenarios. The question should not be ambiguous and can be answered with one of the given options or short answer. Annotate all data fields, including the description, simplified description, question, answer options, the correct answer, image, and domain. F.3 Data Format and Structure JSON File Format: The structured JSON format will include fields for index number, description, simplified description, question, answer options, correct answer, and domain. Naming Conventions: Each collected sample will be stored in line into JSONL file. Image files following standard naming rule: {QuesNum}.png Interleaving Question with Images: The images should be inserted as file path in the question. F.4 Quality Control and Validation Annotators will cross-check each others work to ensure accuracy and compliance with the annotation guidelines. Periodic reviews of randomly selected samples from the dataset will be carried out to maintain consistent quality over time. F.5 Handling Ambiguities Any ambiguous or unclear data entries should be marked for thorough review. Such questions will be collectively discussed during team meetings to develop consistent and standardized annotation strategy. 58 F.6 Ethical Considerations Copyright and Licensing: Annotators must strictly follow all applicable copyright and licensing rules. Content from sources that restrict reproduction or redistribution will be excluded without exception. Data Privacy: Upholding data privacy and ethical standards is essential. Annotators should refrain from including any questions that involve personal or sensitive information. F.7 Data Contamination Considerations When developing benchmarks for evaluating foundation models, it is crucial to account for the potential risk of data contamination. To mitigate this, annotators should deliberately avoid simple questions with widely available answers. Instead, they should prioritize selecting problems whose solutions are embedded in less conspicuous placessuch as in supplementary materials or at the end of lengthy textbooks. This strategy helps ensure that the benchmark effectively challenges models to demonstrate genuine comprehension and reasoning across complex and less accessible content."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: We clearly present our contribution in abstract and introduction. The dataset, experiments, error analysis, and ablation studies support our contribution. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in Section 5. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] Justification: This paper mainly focus on dataset curation, benchmark construction, and experiments analysis. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have released our code and data for reproducibility. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? 61 Answer: [Yes] Justification: Since the submission is single-blinded, we have made our code and dataset publicly available to ensure reproducibility. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have presented all the details of experiments in the main paper Section 3 and Appendix. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the statistical results in Section 3. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). 62 It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We discuss the compute resources in Section 3 and Appendix D. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have carefully checked the code of ethics. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts of the paper in Section B. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 63 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We discussed the source of our data, and the benchmark is purely for academic usage. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the works such as VLMEvalKit. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. 64 If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage 65 Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] Justification: We use LLM to judge the extracted answer with ground truth, detailed in Section 2. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "Independent",
        "The Ohio State University",
        "The University of Hong Kong",
        "University of Michigan",
        "University of Toronto",
        "University of Waterloo"
    ]
}