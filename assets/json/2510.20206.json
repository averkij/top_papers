{
    "paper_title": "RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling",
    "authors": [
        "Bingjie Gao",
        "Qianli Ma",
        "Xiaoxue Wu",
        "Shuai Yang",
        "Guanzhou Lan",
        "Haonan Zhao",
        "Jiaxuan Chen",
        "Qingyang Liu",
        "Yu Qiao",
        "Xinyuan Chen",
        "Yaohui Wang",
        "Li Niu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present \\textbf{RAPO++}, a cross-stage prompt optimization framework that unifies training-data--aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In \\textbf{Stage 1}, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. \\textbf{Stage 2} introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback -- including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow -- yielding progressively improved video generation quality. \\textbf{Stage 3} leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at https://github.com/Vchitect/RAPO."
        },
        {
            "title": "Start",
            "content": "RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling Bingjie Gao, Qianli Ma, Xiaoxue Wu, Shuai Yang, Guanzhou Lan, Haonan Zhao, Jiaxuan Chen, Qingyang Liu, Yu Qiao, Xinyuan Chen, Yaohui Wang*, and Li Niu 1 5 2 0 2 3 2 ] . [ 1 6 0 2 0 2 . 0 1 5 2 : r AbstractPrompt design plays crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present RAPO++, cross-stage prompt optimization framework that unifies training-dataaligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In Stage 1, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. Stage 2 introduces Sample-Specific Prompt Optimization (SSPO), closed-loop mechanism that iteratively refines prompts using multi-source feedbackincluding semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flowyielding progressively improved video generation quality. Stage 3 leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as model-agnostic, cost-efficient, and scalable solution that sets new standard for prompt optimization in T2V generation. The code is available at this https URL. Index TermsText-to-Video Generation, Prompt Optimization, Test-Time Scaling."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "W ITH the rapid advancement of diffusion models [37], [68], [69], visual content creation has experienced remarkable progress in recent years. The generation of images, as well as videos from text prompts utilizing large-scale diffusion models, referred to as text-to-images (T2I) [39], [50], [70] and text-to-videos (T2V) [27], [72], [86] generation, have attracted significant interest due to the broad range of applications in real-world scenarios. Various efforts have been made to enhance the performance of these models, including improvements in model architecture [32], [32], [73], learning strategies [26], [74], and data curation [76], [77], [80]. Recent studies [22], [26], [61] have revealed that employing long, detailed prompts with pre-trained model typically produces superior quality outcomes compared to utilizing shallow descriptions provided by users. This has underscored the significance of prompt optimization as an important challenge in text-based visual content creation. The prompts provided by users are often brief and lack the essential details required to generate vivid images or videos. Simply attempting to optimize prompts by manually adding random descriptions can potentially mislead models and degrade the quality of generative results, resulting in outputs that may not align with user intentions. Therefore, B. Gao, S. Yang, Q. Ma, J. Chen, H. Zhao, Q. Liu, L. Niu are with Shanghai Jiao Tong University, Shanghai 200240, China. X. Wu, G. Lan, Y. Qiao, X. Chen, Y. Wang are with Shanghai Artificial Intelligence Laboratory, Shanghai 201112, China. Corresponding author. *Project lead. developing automated methods to enhance user-provided prompts becomes essential for improving the overall quality of generated content. Towards improving image aesthetics and ensuring semantic consistency, several attempts [25], [44], [45] have been made in previous T2I works for prompt optimization. These efforts primarily involve instructing pre-trained or fine-tuned Large Language Model (LLM) to incorporate detailed modifiers into original prompts, with the aim of enhancing spatial elements such as color and relationships. While these approaches have displayed promising outcomes in image generation, studies [22], [55] reveal that their impact on video generation remains limited, especially in terms of enhancing temporal aspects such as motion smoothness and minimizing temporal flickering. For T2V generation, recent efforts [1], [2], [26], [46] have explored prompt rewriting strategies, where user-provided prompts are reformulated to address variability in linguistic style, length, and expressivity. Such approaches aim to improve alignment between textual descriptions and video outputs by standardizing or enriching the input language. However, existing practices in T2V prompt engineering remain largely model-specific. There is still lack of generalizable optimization strategies that can systematically guide prompt refinement across diverse models and tasks. To address the above issue, some RLHF-based prompt optimization methods [22], [25] mitigate model-specific variability in user prompts by training dedicated prompt rewriter through two-stage procedure: supervised ini2 Figure. 1: Overview of RAPO++. The framework couples training-dataaligned prompt refinement with test-time scaling to enhance Text-to-Video (T2V) generation without altering the generative backbone. Stage 1 RAPO: Retrieval-Augmented Prompt Optimization (Sec. 3). User prompts are augmented via retrieval-based relation graph and refactored by finetuned LLM, while frozen LLM provides alternative rewrites. discriminator then selects the best candidate, ensuring prompts align with training distributions while preserving intent. Stage 2 SSPO: Sample-Specific Prompt Optimization at Test-time (Sec. 4). Multiple candidates are evaluated by VLM verifiers and task-specific metrics, with misalignments guiding iterative refinement. This process enhances temporal coherence, fidelity, and semantic alignment during inference, and also yields prompt pairs for LLM fine-tuning. Stage 3: LLM Fine-Tuning & Evaluation (Sec. 4.2& 5). The prompt pairs collected from Stage 2 are used to fine-tune the LLM, further enhancing its generalization and robustness across models. The fine-tuned LLM is then validated across different benchmarks, demonstrating consistent and transferable improvements in T2V generation. tialization on curated high-quality prompts followed by reinforcement learning (e.g., PPO and GRPO) against learned reward model that encodes human preferences or alignment metrics. This pipeline effectively enables exploration beyond hand-crafted templates without altering the generators weights. Recent variants [29], [38] further decouple the rewriter from the generator and incorporate chain-of-thought or evaluator-guided rewriting to improve generalization. However, these methods focus on T2I generation and extending this RLHF recipe T2V generation faces fundamental practical and methodological barriers. Video generation introduces heavy temporal structure and substantially higher inference cost, so RLHFs reliance on large numbers of generator rollouts for reward estimation and policy search becomes prohibitively expensive. To this end, naively scaling those approaches to T2V generation is computationally impractical without additional algorithmic designs that address temporal evaluation and inferencetime cost. In this paper, we propose RAPO++ as shown in Fig. 1, cross-stage prompt optimization framework that unifies training-dataaligned refinement with test-time iterative Figure. 2: Generation results under different iterations of prompt refinement at inference utilizing SSPO. The initial prompt is valkyrie riding flying horses through the clouds. As the number of iterations increases (from left to right), the generated video becomes more detailed and vivid, and more consistent with the users intent. scaling. The framework is organized into three complementary stages. In the first stage, the proposed retrievalaugmented prompt optimization method (RAPO) leverages training corpus statistics to guide prompt refinement. relation graph built from large-scale videotext data retrieves semantically relevant modifiers, which are merged into user prompts via word augmentation mechanism. These enriched prompts are then refactored through an instruction-tuned LLM to match the structural and stylistic distribution of training prompts, ensuring compatibility with the generative backbone. In parallel, an alternative rewriting branch produces candidate prompts directly from frozen LLM. discriminator LLM subsequently selects the most effective candidate, yielding optimized prompts that preserve user intent while aligning more closely with the data distribution. This stage systematically addresses the challenge of model-specificity by anchoring prompts in training-grounded semantics and structure, improving compositionality and multi-object fidelity. Building upon Stage 1, which aligns prompts with training data semantics and structure, Stage 2 introduces Sample-Specific Prompt Optimization (SSPO), test-time scaling mechanism that iteratively refines prompts through closed-loop reflection process. SSPO consists of three modules: Rewriter Initialization and Video Generation, Quality Assessment and Automatic Feedback, and Feedback and Contextbased Rewrite. Starting from the RAPO-refined prompt, the system generates an initial video and evaluates it using vision-language alignment checks, ensemble verifiers for spatial fidelity, temporal coherence, and alignment quality, and optional task-specific modules (e.g., optical flow or object counting) for physical plausibility. Multi-source feedback is stored in memory bank and used by large language model to rewrite prompts, progressively improving semantic alignment, temporal consistency, and motion realism. An average-ranking mechanism selects the best candidate for subsequent inference. Through this reflectiondriven loop, SSPO significantly enhances video quality without modifying the generative backbone, achieving finer temporal control, stronger compositional reasoning, and higher semantic fidelity. As shown in Fig. 2, we present an example of generative results under different iterations of prompt refinement at inference utilizing SSPO. With each successive iteration (from left to right), the generated video gains more detail and vividness, and aligns more closely with the users intent. Stage 3 consolidates these improvements through LLM fine-tuning, transforming the iterative optimization knowledge from Stage 2 into reusable capability. During SSPO, the system collects paired data of original prompts and their optimized versions, which are used to fine-tune the rewriter LLM via instruction tuning. This enables the model to internalize task-specific patterns, generalize beyond seen examples, and generate high-quality prompts even before inference, reducing test-time computation and improving optimization generalization. The fine-tuned LLM accelerates convergence and extends RAPO++ to diverse T2V architectures and downstream tasks. Together, Stages 2 and 3 complement the training-aligned refinement of Stage 1, forming unified pipeline that couples inference-time adaptation with model-level enhancement. This cross-stage design empowers RAPO++ to achieve substantial gains in compositional generation, temporal stability, and physics-aware realism, setting new benchmark for prompt optimization in text-to-video generation. Extensive experiments across five representative T2V models (LaVie, Latte, HunyuanVideo, CogVideoX, and Wan2.1) and five complementary benchmarks (VBench, T2V-CompBench, EvalCrafter, VideoPhy, and PhyGenBench) demonstrate the effectiveness and generalizability of RAPO++. Compared to existing prompt optimization methods, RAPO++ achieves consistent and significant improvements in semantic alignment, compositional reasoning, temporal stability, and physical plausibility. On VBench, RAPO++ attains total score of 82.65% with LaVie and 80.75% with Latte, while on T2V-CompBench it delivers state-of-the-art performance across challenging categories such as consistent attribute binding and object interactions. These results reflect RAPO++s ability to generate videos with sharper spatial details, smoother motion dynamics, and stronger textvideo alignment than baseline approaches. RAPO++ also demonstrates strong scalability and adaptability in task-specific settings. Integrating physicsaware evaluators into the SSPO loop enables substantial gains in physical consistency and semantic alignment on PhyGenBench and VideoPhy, with performance steadily improving over iterative refinement rounds. Analyses further reveal that optimized prompts produced by RAPO++ closely match the training distribution in length and structure, unlocking the full generative potential of T2V models. Fine-tuned LLMs significantly enhance multi-object fidelity and compositional generation, while inference-time scaling yields progressive gains across temporal consistency, visual quality, and factual alignment. Ablation studies confirm the complementary contributions of each module and the robustness of RAPO++ across different LLM backbones, establishing it as model-agnostic and cost-efficient solution for high-quality text-to-video generation. Difference from our conference version: This manuscript improves the conference version [6] substantially with new methodology, wider extension to more models and tasks, and broader analyses. 1) We extend the original RAPO into three-stage framework called RAPO++ (Section 4), which integrates prompt refinement with Sample-Specific Prompt Optimization (SSPO) and LLM finetuning, forming unified pipeline that enhances semantic fidelity, temporal coherence, and compositional reasoning without modifying the generative backbone. 2) We apply RAPO++ to broader range of T2V models and evaluate it on multiple benchmarks in Section 5, demonstrating its effectiveness, scalability, and strong generalization across architectures and tasks. 3) We conduct more comprehensive analyses in Section 5.5, including multi-object generation, prompt statistics, physical consistency, and inference-time scaling behavior. We also provide deeper discussions of concurrent works and inference-time scaling strategies in Section 2 to better position RAPO++ within the evolving landscape of T2V prompt optimization research."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Text-to-Video Generation. With the remarkable breakthroughs of diffusion models [37], [68], [69], the generations of 3D content [87], [88], [89], images [17], [36], [39], [50], and videos [8], [86], [90], [91] from text descriptions achieve rapid advancement. Text-to-Video (T2V) [27], [43], [78] Generation aims to automatically create videos that match given textual descriptions. This process generally involves comprehending the scenes, objects, and actions described in the text and converting them into sequence of cohesive visual frames, producing video that is logically and visually consistent. T2V generation is wildly used in applications, such as animations [62], [63], [85] and automatic movie generation [64], [65], [66], [84]. However, large T2V generative models [26], [27], [32] trained on large-scale dataset could not adequately demonstrate their potential in generation due to mismatch between training and inference. Prompt optimization. T2I and T2V generative models are sensitive to input prompts. However, the well-performed prompts are often model-specific and coherent with training prompts, misaligned with user input. Therefore, several studies [6], [22], [24], [25], [79] are conducted to explore the generative potential of T2I and T2V generative models. Hao et al. [22] propose learning-based prompt optimizing framework unitizing reinforce learning for generating more aesthetically pleasing images. Chen et al. [24] enhance user 4 prompts by leveraging the users historical interactions with the system. Mo et al. [25] propose Prompt Auto-Editing (PAE) method to decide the weights and injection time steps of each word without manual intervention. These methods primarily focus on prompts optimizing for T2I models and lack extension to T2V models. Yang et al. [26] use large language models (LLMs) to transform short prompts into more detailed ones, maintaining consistent visual structure. Polyak et al. [61] develop teacher-student distillation approach for prompt optimization to improve computational efficiency and reduce latency. However, the results of optimized prompts usually could not be wellaligned with training prompts due to the misleading of the LLMs and the lack of more refined guidance. Test-Time Scaling. Test-time scaling [3], [5] refers to increasing computational resources during inference to enhance model performance. By employing larger models or more sophisticated search strategies, it yields more accurate, coherent, and contextually relevant outputs. Leveraging extra compute after training allows models to refine predictions and better adapt to inputs, producing higher-quality results [4], [9], [13], [14], [15], [19], [75]. In large language models, it improves response quality and contextual relevance, and recent work has extended this concept to diffusion models [20], [21], [48]. Ma et al. [16] propose framework for test-time scaling in diffusion models, searching for better noise candidates during the diffusion sampling process, and the results show substantial quality improvements in image generation across different tasks and model sizes. Xie et al. [20] leverage efficient training, depth pruning, and Test-time scaling to enhance text-to-image generation quality while reducing computational costs. Oshima et al. [21] propose method called Diffusion Latent Beam Search (DLBS) with lookahead estimator to optimize the quality of generated videos by selecting better diffusion latents and calibrating rewards to enhance perceptual quality without model updates. However, there is few research focus on the testtime scaling for generative models via iteratively refining prompts. Long et al. [48] propose VISTA, multi-agent framework that iteratively refines prompts during test time to enhance T2V generation, jointly optimizing visual, audio, and contextual quality and achieving significant gains over prior methods."
        },
        {
            "title": "3 RAPO",
            "content": "As illustrated in Fig. 1, RAPO mainly consists of three parts, 1) word augmentation module, 2) sentence refactoring module, as well as 3) prompt selection module. Given user-provided prompt xi, firstly, the word augmentation module utilizes an interactive retrieval-merge mechanism between relation graph and LLM to augment the prompt by adding related subject, action and atmosphere modifiers. Then, fine-tuned LLM Lr is applied to refactor the entire sentence into xr. xr has more unified format which is consistent with the prompt length and format distribution in training data. Finally, discriminator in the prompt selection module decides between xr and naively augmented prompt xn obtained directly from LLM via instruction, as the most suitable augmented prompt for T2V 5 TABLE 1: Input template for retrieval-merge mechanism. This template specifies how frozen LLM iteratively merges user-provided prompt texts with relevant modifiers retrieved from relation graph, thereby enriching the prompts semantic content and aligning it with the training prompt structure for improved text-to-video synthesis. LLM Template for Retrieval-Merge Mechanism Suppose you are Text Merger. You receive two inputs from the user: description body and relevant modifier. Your task is to enrich the description body with relevant modifiers while retaining the description body. You should ensure that the output text is coherent, contextually relevant, and follows the same structure as the examples provided. Examples of prompt-pairs provided: = {ein Input description body and modifier are: {xm The merged prompt is: {xm+1 i=0}. ,pm }. }. Figure. 3: The construction of relation graph. Relation graph consists of multiple nodes (scenes acting as core nodes with modifiers connected as sub-nodes). For each prompt in database, LLM extracts scene and related modifiers. Based on whether the extracted scene is already in the graph or not, different methods are used to incorporate the new information into the graph. generation. We proceed to introduce each module in detail in the following sections."
        },
        {
            "title": "3.1 Word Augmentation Module\nGiven a user-provided prompt xi, the word augmentation\nmodule aims to enrich xi with more multiple, straightfor-\nward and relevant modifiers. It is achieved through retriev-\ning modifiers meeting the requirements from a built relation\ngraph G, and merging them into xi through L via instruc-\ntion. In this section, we first introduce the construction and\nretrieval of relation graph. And we introduce the instruction\nformat and retrieval-merge mechanism of L.\nRelation Graph G. As shown in Fig. 3, we construct relation\ngraph G based on training prompts database. For each train-\ning prompt, we utilize L to extract scene and corresponding\nrelated modifiers (subject, action, atmosphere descriptions).\nEach scene serves as a core node, with subject, action and\natmosphere modifiers connected as individual sub-nodes\nin relation graph. For each extracted scene, we first check\nwhether it exists in relation graph or not. If so the extracted\nrelated modifiers will be connected to the existing one. If not\nthe extracted scene becomes a new core node with related\nmodifiers connected. Finally, we can obtain a relation graph\ncovering diverse scenes with multiple modifiers connected.\nFor relation graph retrieval, we utilize a sentence trans-\nformer pre-trained model to extract features of prompt, and\nemploy the cosine similarity to measure similarity between\nsentence features. We first retrieve the top-k relevant scenes\nfrom G for xi. Then we retrieve all modifiers connected to\nthe retrieved scenes. We select the top-k relevant modifiers\n{pn|k−1\nn=0} from all retrieved modifiers, preparing for the\nretrieval-merge mechanism of L.",
            "content": "LLM L. We augment xi with retrieved modifiers {pnk1 n=0} from relation graph. We rename xi with x0 to illustrate the process of iterative merging. Specifically, the retrieved modifiers are merged into input prompt x0 one by one through prompting L, to maintain the information of the original input while adding relevant modifiers. xm+1 = (xm , pm ), (1) where = 0, 1, ..., 1. is function that combine xm and pm reasonably by L. For instance, merged prompt woman dressed in black suit representing funeral is resulted from merging the user-provided prompt woman representing funeral and retrieved modifier black suit. We prompt to perform general prompt merging in normal manner as the template in Tab. 1. In instruction, we provide some prompt pairs = {ein i=0} as examples, in which ei contains input prompt, modifier and corresponding merged result."
        },
        {
            "title": "3.2 Sentence Refactoring Module",
            "content": "Sentence refactoring module aims to refactor word augmented prompts from word augmentation module to be more consistent with prompt format in training data. It is achieved through fine-tuned LLM Lr named as refactoring model. In this section, we introduce the training data preparation and instruction tuning for Lr. Data preparation. We represent the required dataset for training refactoring model by {Dr = riN i=1}, in which ri involves pair of prompts and is the number of training prompts pairs. Specifically, ri = (wi, ci), in which wi targets to simulate world augmented prompt, and ci represents the target prompt, that is, training prompt for T2V models. wi and ci share similar semantics while different in the prompt format and length. Therefore, we generate wi automatically through rewriting ci utilizing via instruction to break the unified training prompt format but maintaining the original semantics. Instruction tuning for Lr. We employ instruction tuning for fine-tuning LLM on our constructed dataset of instructional prompts and corresponding outputs. The constructed dataset is based on {Dr = riN i=1} containing instructional prompts and corresponding outputs. The template of the instruction tuning dataset for Lr is shown as Tab. 2. TABLE 2: Instruction tuning dataset template for Lr. This template directs LLM fine-tuning to restructure augmented prompts by adjusting their format while preserving semantics, aligning them with the training datas style for improved T2V generation. Instruction Tuning Dataset for Lr Instruction. Refine format and word length of the sentence: wi. Maintain the original subject descriptions, actions, scene descriptions. Append additional straightforward actions to make the sentence more dynamic if necessary. Output: target prompt ci. TABLE 3: Instruction tuning dataset template for Ld. This template aims to train discriminator LLM that evaluates multiple refined prompts and selects the optimal one based on the inclusion of clear, straightforward modifiers and faithful semantic alignment. Instruction Tuning Dataset for Ld Instruction. Given user-provided prompt xi, select the better optimized prompt from xr and xn. The chosen prompt is required to contain multiple, straightforward, and relevant modifiers about xi while involving the semantics of xi. Output: discriminator label yd."
        },
        {
            "title": "3.3 Prompt Selection Module",
            "content": "As shown in Fig. 1, prompt selection module contains finetuned LLM Ld named prompt discriminator to select the better one between xr from sentence refactoring module, and naively augmented prompt xn obtained directly from LLM via instruction. In this section, we introduce the training data preparation and instruction tuning for Ld. Data preparation. We represent the required dataset for training refactoring model by {Dd = diN i=1}, in which di contains three prompts and is the number of training prompts triples. Specifically, di = (xi, xr, xn, yd), in which yd represents the discriminator label to select the better one for T2V generation from xr and xn given input prompt xi. To simulate the user-provided prompts, we collect diverse prompts from several T2V benchmarks and generate more utilizing via instruction. xr and xn can be obtained from the proposed RAPO as shown in Fig. 1 given xi. We determine yd through the evaluation of generated videos conditioned on xr and xn. Specifically, the evaluations of T2V models performance involves diverse dimensions. For collected or generated prompts, we need to determine the evaluation dimension according to prompt content. We automatically decide the evaluation dimension of input prompts utilizing L, then choose the corresponding metrics to evaluate generated videos. Instruction tuning for Ld. Similar to Lr, we employ instruction tuning for Ld based on {Dd = diN i=1}. The template of the instruction tuning dataset for Ld is shown as Tab. 3."
        },
        {
            "title": "4 RAPO++",
            "content": "As illustrated in Fig. 1, building upon RAPO, RAPO++ additionally introduces three-stage framework that integrates Stage 2 SSPO (Sample-Specific Prompt Optimization at Test-Time) and Stage 3 (LLM Fine-Tuning), forming unified pipeline for test-time refinement and model-level enhancement. We proceed to introduce each part in detail in the following sections."
        },
        {
            "title": "4.1 SSPO Mechanism",
            "content": "6 As shown in Fig. 1, the SSPO mechanism of RAPO++ consists of three parts, 1) Rewriter Initialization and Video Generation module, 2) Quality Assessment and Automatic Feedback module, as well as 3) Feedback and Context-based Rewrite module. Based on RAPO in Stage 1, Raw User Prompt ρu is first transformed into Refined Prompt ρr , which is then used to generate video vi through the T2V model. The generated video undergoes Quality Assessment and Automatic Feedback module, including misalignment detection between the Generated Video vi and Raw User Prompt ρu via Vision-Language Model (VLM) , ensemblebased video scores from different verifiers, and optional task-specific assessment (e.g., optical-flow prediction or object counting). These feedback signals are passed to the Feedback and Context-based Rewrite module, where Feedback Memory database stores prior evaluations and provides contextual feedbacks to the LLM. Large Language Model (LLM) then incorporates these contextual signals to rewrite the current Refined Prompt ρr , enabling reflection-driven optimization loop that progressively enhances temporal coherence, fidelity, and semantic alignment. An Average Ranking strategy is applied across multiple evaluation dimensions (e.g., semantic alignment, temporal coherence, and physical plausibility) to select the best-performing prompt from this candidate set, yielding the best-performing optimized prompt ρb . This reflection-driven optimization loop progressively enhances temporal coherence, visual fidelity, and semantic alignment while producing aligned prompt pairs {(ρu i=0 } for LLM finetuning in Stage 3, where denotes the total number of prompts contained in the prompt database. Rewriter Initialization and Video Generation. This module refines raw user prompt ρu through the RAPO framei work, which retrieves semantically relevant modifiers from relation graph and restructures them via fine-tuned Large Language Models (LLMs) to match the distribution of training data. The refined prompt ρr is then fed into Texti to-Video (T2V) model to generate the corresponding video vi. Both the RAPO rewriter and the T2V generative model are modular and can be replaced with other existing T2V models (e.g., HunyuanVideo [2], CogVideoX [26], Wan [1]), enabling flexible integration and generalization across different architectures. Quality Assessment and Automatic Feedback. This module evaluates the generated video vi through multiple complementary feedback signals that jointly capture its consistency with the input prompt ρu . Vision-Language Model (VLM) estimates the semantic misalignment between the raw user prompt ρu and the generated video vi, denoted as M(ρu k=1 assess the overall generation quality across various dimensions, including spatial fidelity, temporal coherence, and semantic alignment, producing video scores {sk}K k=1 that are aggregated into unified evaluation score S(vi) = 1 k=1 sk. In addition, Task-Specific Assessment branch can be flexibly designed to enhance the generalization ability of this , vi). Simultaneously, set of verifiers {Vk}K )n1 , ρb (cid:80)K module across diverse tasks. For instance, in physical-aware video generation, an optical flow prediction module O(vi) can be incorporated to evaluate motion dynamics and physical plausibility by analyzing flow consistency and object trajectories. All feedback signals {M(ρu , vi), S(vi), O(vi)} are passed to the next module and utilized as contextual information to guide subsequent prompt rewriting and iterative refinement. Feedback and Context-based Rewrite. This module leverages accumulated feedback to iteratively refine prompts through reflection-driven rewriting process. dedicated feedback memory database is designed to record multisource feedback signals {M(ρu , vi), S(vi), O(vi)}. Rather than processing each generation independently, the feedback memory database maintains historical record of previous assessments and refinement outcomes, allowing the system to capture temporal dependencies and longterm optimization patterns. During each iteration, contextual information retrieved from the feedback memory database provides the Large Language Model (LLM) with comprehensive understanding of prior errors, successful refinements, and evolving feedback trends. The LLM then performs context-based rewriting of the current refined prompt ρr , integrating historical and current feedback signals to produce an updated version. This reflection-driven mechanism enables the framework to progressively enhance temporal coherence, visual fidelity, and semantic alignment over multiple iterations, ensuring that prompt optimization remains adaptive, memory-informed, and dynamically responsive to generation quality. Average Ranking for Prompt Selection. To ensure that the optimized prompt achieves robust generalization across multiple evaluation dimensions, we introduce an Average Ranking mechanism to guide the selection of ρb . Instead of relying on single metric, each candidate refined prompt is evaluated using multiple criteria such as semantic alignment, spatial fidelity, temporal consistency, and physical plausibility. Each candidate receives rank for every metric, and an average rank score is computed as the mean of its ranks across all metrics. The candidate with the lowest average rank is then selected as ρb . This ranking-based approach mitigates bias from any single metric and ensures balanced performance across compositional, temporal, and physical dimensions. Instruction Template of Refining Prompts. To enable context-aware and memory-guided rewriting, we design an instruction template that guides the Large Language Model (LLM) in refining prompts based on prior feedback signals stored in the Feedback Memory. The template incorporates three key components: (1) the initial user prompt ρu , (2) previously optimized prompts with their corresponding misalignment assessments {(M(ρu t=0}, and (3) the unified evaluation score S(vi) derived from multiverifier feedback. By referencing these structured inputs, the LLM is able to infer patterns of improvement and identify the semantic gaps that most strongly influence video-text inconsistency. Through this reflection-driven instruction, the model generates refined prompt ρr i+1 that balances textual precision and generative controllability, leading to improved temporal coherence and visual-textual alignment in subsequent generations. The complete instruction template used , vt), ρu )i"
        },
        {
            "title": "Instruction Template",
            "content": "for TABLE 4: Context-Aware Feedback-Driven Prompt Refinement. This template guides the LLM to iteratively refine prompts by integrating historical and current feedback from the Feedback Memory, improving semantic alignment, temporal coherence, and perceptual fidelity. Instruction Template for Prompt Refinement )i1 You are prompt engineering expert using diffusion-based Textto-Video (T2V) model. Your task is to refine the current refined prompt ρr to improve the alignment between the generated video and the input textual semantics. You should consider both the historical and current feedback signals stored in the Feedback Memory, including the raw user prompt ρu , the historical feedback records {(M(ρu , vt), ρu t=0}, overall video scores S(vi), and taskspecific assessments O(vi). Please analyze these feedbacks together with the previous optimized prompts and their evaluation results to propose new, improved prompt. The goal is to generate refined prompt that minimizes semantic misalignment, enhances temporal and spatial coherence, and improves overall perceptual fidelity. Historical Feedback Records: {(M(ρu Raw User Prompt: ρu Current Refined Prompt: ρr Final Output (Updated Refined Prompt): ρr , vt), ρu )i1 t=0} i+1 TABLE 5: Input template for fine-tuning LLM. The initial prompt ρu is refined into detailed target prompt ρb through the incorporation of vivid descriptions, dynamic actions, and specific contextual enhancements such as camera language, lighting, and atmosphere. LLM Template for Fine-Tuning LLM You are prompt engineering expert and using diffusion model to generate video by giving prompt. Your task is to refine the prompt to add more related and vivid descriptions (Optional: camera language, light and shadow, atmosphere) for better generative performance. Conceive some additional actions to make the sentence more dynamic. Make sure it is fluent sentence, not nonsense. Initial Prompt: ρu . Target Optimized Prompt ρb . for prompt refinement is shown as Tab. 4. , ρb"
        },
        {
            "title": "4.2 LLM Fine-Tuning\ni )|n−1\nThe prompt pairs {(ρu\ni=0 } collected from Stage 2,\nwhere n denotes the total number of prompts contained\nin the prompt database, are then used to fine-tune the\nrewriter LLM, reinforcing its generalization capacity and\nmitigating local optima. The fine-tuned LLM strengthens\nthe overall refinement pipeline when deployed at inference.\nFine-tuning the LLM used in Stage 2 is both necessary and\nbeneficial, as it further improves the initial prompt opti-\nmization module and prevents optimization from falling\ninto local minima. We employ instruction tuning based on\nthese initial–optimized pairs {(ρu\ni=0 }. The template of\nthe instruction used for fine-tuning is shown in Tab. 5.",
            "content": "i )n1 , ρb"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In Section 5.1, we introduce the evaluation metrics, benchmarks, and comparative methods. In Section 5.2, we detail the technical configurations and parameter settings used in the experiments. In Section 5.3, we present performance 8 Figure. 4: Qualitative comparisons across dynamic and static dimensions. This figure showcases videos generated using LaVie with short prompts, GPT-4 and Open-sora prompt optimizations, and our RAPO method. Videos produced with RAPO exhibit significantly sharper spatial details, smoother temporal transitions, and closer semantic alignment with the input text. improvements through both quantitative and qualitative comparisons. In Section 5.5, we provide comprehensive analysis of compositionality, multi-object binding, temporal stability, and prompt statistics, complemented by attention visualization, inference-time scaling evaluation, and assessments of LLM fine-tuning and task-specific modules on semantic alignment and motion realism. In Section 5.6, we validate the importance of individual components through systematic ablation experiments."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Models & Evaluation. We applied RAPO++ on several open-source Text-to-Video (T2V) models and benchmarks, as listed below, to examine how the proposed framework enhances the generative performance of generated videos, such as semantic fidelity, compositional accuracy, and physical plausibility. LaVie [27]: cascaded latent unetbased T2V model. LaVie composes three modulesbase video diffusion, temporal interpolation, and video super-resolutionto generate visually high-fidelity and temporally coherent videos. Latte [32]: DiT-based T2V model that formulates video generation in tokenized latent space. It first extracts spatio-temporal tokens and then uses Transformer blocks to model the video distribution. HunyuanVideo [2]: HunyuanVideo emphasizes largescale joint training across image and video domains, efficient infrastructure for large-scale inference, and robust textvideo alignment. CogVideoX [26]: derivative of the CogVideo family, CogVideoX is an open-source T2V model that generates videos of moderate length (e.g. 10 seconds) from textual prompts. It offers multiple variants (e.g. 2B, 5B) and has been adopted in benchmark comparisons of T2V generation. Wan2.1 [1]: Wan2.1 is available in 14Bparameter version for 480P/720P output, as well as lighter 1.3B variant for more limited hardware. It also supports bilingual text generation (Chinese and English) in video frames. To better and more comprehensively assess the generalization ability of RAPO++, we evaluate it across five complementary benchmarks. These benchmarks probe different aspects of video generationsuch as visual quality, semantic alignment, compositional generalization, temporal consistency, and physical commonsenseand together provide more holistic view of RAPO++s strengths and limitations. Below we briefly summarize the key features and evaluation design of each benchmark: VBench [30]: hierarchical benchmark decomposing video quality into fine-grained dimensions (e.g., identity and background consistency, motion smoothness, temporal flicker, spatial relations) with tailored prompts and evaluation pipelines. T2V-CompBench [57]: compositional benchmark assessing T2V models ability to coherently combine objects, attributes, actions, spatial relations, interactions, and numeracy, structured into seven categories and evaluated using MLLM-based and detection/tracking metrics. EvalCrafter [56]: large-scale evaluation pipeline using 700 prompts and 17 metrics to comprehensively assess visual quality, content alignment, motion dynamics, and temporal consistency. VideoPhy [18]: benchmark testing whether generated motions follow physical commonsense principles such as momentum conservation, collision dynamics, and realistic trajectories. PhyGenBench [31]: physics-oriented benchmark with 160 prompts across 27 physical laws, using the hierarchical PhyGenEval framework and VLM/GPTbased reasoning to assess physical law adherence from single frames to full videos. Comparison to other methods.To validate the effectiveness of RAPO++, we compare it to five baseline strategies. These baselines cover spectrum from no prompt change to dynamic prompt editing, providing comprehensive comparison. Below we concisely introduce each: Naive Prompt: feed the original user prompt unchanged the simplest baseline. GPT-4 Refiner [47]: use GPT-4 to rewrite or enrich the prompt prior to generation, aiming to supplement missing details or disambiguate. Prompt Refiner [46]: controlled rewriting module (inspired by Open-Sora) that expands or adjusts prompts in semantically consistent way to improve granularity. Promptist [12]: learned prompt optimizer that explores variant prompts under reward function utilizing reinforce learning, selecting forms that better align with the models strengths. PAE [25]: dynamic editing method that refines prompts via reinforcement learning, adjusting token weights or insertion timing to maximize generation quality."
        },
        {
            "title": "5.2 Implementation Details",
            "content": "RAPO. The well-performed prompts are model-specific and aligned with the distribution of training prompts. We employ Vimeo25M [27], training dataset consisting of 25 million text-video pairs as our analysis dataset. At the same time, we choose LaVie [27] and Latte [32] as analysis T2V models, which belong to the diffusion-based and DiT architectures respectively and use Vimeo25M as one of training datasets. For relation graph construction, we utilize Mistral [28] to extract scenes with corresponding subject, action and atmosphere descriptions from Vimeo25M dataset, and use all-MiniLM-L6-v2 as sentence transformer pre-trained model. We filter about 2.1M valid sentences from from Vimeo25M dataset. For refactoring model training data, we prepare about 86k prompt-pairs following data preparation method in Section 3.2. For prompt discriminator training 9 data, we first generate 7K text captions using Mistral, covering all the dimensions in VBench [30]. We perform LoRA fine-tuning using LLaMA 3.1 [67], and fine-tune 8 epochs and 3 epochs for refactoring model and prompt discriminator respectively with single A100, using batch size of 32 and LoRA rank of 64. SSPO and LLM Finetuning. We utilize LLaVA-OneVision [11] to capture the misalignment between the initial prompt and the generated video. For user-provided prompts, we design about 12k prompts generated by GPT-4 [47] covering diverse scenes and actions. We choose LaVie [27] and Latte [32] as analysis T2V models. For the rewriting process, we adopt Qwen2.5-7B-Instruct [23] as the LLM to perform instruction-guided prompt refinement tailored to each sample. Additionally, for physical-aware video generation tasks, we conduct experiments on three representative DiT-based T2V models (WanX2.1, HunyuanVideo, and CogVideoX), and predict the optical flow of the generated videos to extract motion field information. This motion-aware feedback is integrated into the assessment module as an additional condition, enabling more accurate detection of physical violations (e.g., unrealistic momentum transfer, inconsistent motion trajectories) and guiding prompt optimization toward physics-consistent generations. In the LLM fine-tuning stage, we perform LoRA fine-tuning using LLaMA 3.1 for 8 and 3 epochs respectively, with batch size of 32 and LoRA rank of 64, on single A100 GPU within 5 hours per iteration."
        },
        {
            "title": "5.3 Evaluation Results",
            "content": "Quantitative comparisons. As shown in Tab. 6 and Tab. 7, RAPO and RAPO++ consistently achieve superior performance over all baseline methods across both static dimensions (e.g., visual quality, object class) and dynamic dimensions (e.g., human action, temporal flickering), demonstrating their robustness and versatility in diverse text-tovideo generation scenarios. While other methods attempt to enrich user prompts with additional scene and action details, these verbose and complex descriptions often lead to over-specification and confusion for the generation model, thereby limiting their effectiveness. In contrast, RAPO provides more structured and model-aware prompt refinements, resulting in substantial improvements across multiple benchmarks. In particular, RAPO significantly boosts compositional understanding and multi-entity reasoning: the multiple-objects score improves from 37.71% to 64.86% with LaVie and from 29.55% to 52.78% with Latte, highlighting its superior capacity to generate scenes involving multiple subjects and complex interactions. On T2VCompBench, RAPO and RAPO++ achieve state-of-the-art performance in challenging compositional dimensions such as consistent attribute binding and object interactions, validating their strength in capturing intricate spatial and semantic relationships. Moreover, RAPO++ further advances the overall performance on VBench, reaching total score of 82.65% with LaVie and 80.75% with Latte, and attaining the best or second-best results across almost all submetrics, including imaging quality, spatial relationships, and temporal stability. These results collectively demonstrate that RAPO and RAPO++ deliver substantial advantages TABLE 6: Quantitative comparisons on EvalCrafter [56] and T2V-CompBench [57]. The best performance among all methods for each metric is in bold, and the second best is underlined. RAPO and RAPO++ consistently outperform the baselines, achieving highest scores on both video quality and compositional benchmarks. 10 Method EvalCrafter T2V-CompBench Motion Quality Text-Video Alignment Visual Quality Temporal Consistency Consistent Attribute Binding Dynamic Attribute Binding Action Binding Object Interactions LaVie LaVie-GPT4 [47] LaVie-Prompt Refiner [46] LaVie-Promptist [12] LaVie-PAE [25] LaVie-RAPO LaVie-RAPO++ Latte Latte-GPT4 [47] Latte-Prompt Refiner [46] Latte-Promptist [12] Latte-PAE [25] Latte-RAPO Latte-RAPO++ 53.19 54.05 53.07 53.85 53.90 54.14 54.75 50.03 51.36 50.25 50.58 51.26 51.73 51. 69.60 65.51 71.38 70.64 70.37 74.38 75.62 55.49 53.65 57.32 56.12 56.89 60.86 61.92 64.81 64.96 65.26 64.72 65.12 66.62 66.95 57.65 58.02 58.71 58.06 58.43 59.24 60.25 60.87 61.22 61.41 61.25 61.22 61.29 66.80 53.94 54.65 55.47 54.45 55.17 55.26 55. 0.620 0.561 0.532 0.552 0.571 0.692 0.742 0.633 0.598 0.549 0.583 0.576 0.706 0.727 0.232 0.218 0.214 0.203 0.210 0.267 0.294 0.227 0.210 0.203 0.205 0.208 0.258 0.283 0.483 0.428 0.470 0.412 0.432 0.635 0.632 0.476 0.405 0.487 0.521 0.536 0.591 0. 0.760 0.620 0.698 0.615 0.631 0.839 0.849 0.792 0.688 0.743 0.687 0.695 0.847 0.856 TABLE 7: Quantitative comparisons on VBench [30]. The best performance among all methods for each metric is in bold, and the second best is underlined. RAPO++ lead across nearly all VBench submetrics (temporal flickering, object correctness, spatial relations, etc.), showing strong generalization and robust prompt optimization in text-to-video generation."
        },
        {
            "title": "Spatial\nRelationship",
            "content": "LaVie LaVie-GPT4 [47] LaVie-Prompt Refiner [46] LaVie-Promptist [12] LaVie-PAE [25] LaVie-RAPO LaVie-RAPO++ Latte Latte-GPT4 [47] Latte-Prompt Refiner [46] Latte-Promptist [12] Latte-PAE [25] Latte-RAPO Latte-RAPO++ 80.89% 79.69% 79.75% 79.13% 79.17% 82.38% 82.65% 77.03% 77.40% 77.23% 76.65% 76.83% 79.97% 80.75% 96.62% 96.14% 96.42% 96.63% 96.58% 96.86% 97.46% 97.10% 97.52% 97.67% 97.82% 97.78% 98.17% 97.93% 69.00% 70.27% 70.42% 70.08% 70.32% 71.40% 73.48% 63.38% 63.54% 64.19% 63.37% 63.52% 66.72% 67.84% 95.80% 92.09% 83.80% 88.73% 87.00% 91.29% 81.00% 71.04% 82.40% 73.23% 96.80% 96.91% 99.20% 98.78% 88.40% 83.86% 85.80% 78.32% 84.60% 83.60% 74.00% 79.18% 76.40% 81.52% 95.20% 96.47% 97.20% 93.82% 37.71% 36.23% 36.52% 43.97% 42.54% 64.86% 71.89% 29.55% 27.73% 30.00% 21.72% 23.42% 52.78% 55.38% 37.27% 50.55% 54.37% 37.76% 37.92% 59.15% 64.76% 40.63% 36.72% 35.12% 31.31% 33.49% 41.31% 46.87% over existing prompt optimization strategies, enabling more coherent, compositional, and semantically faithful text-tovideo generation. Qualitative comparisons. The qualitative examples in Fig. 4 and Fig. 5 vividly demonstrate that RAPO and RAPO++ produce more visually coherent and semantically faithful videos than baseline methods. Objects maintain consistent appearance and attributes across frames, motion trajectories are smooth and natural, and compositional interactions (such as multiple objects or relative spatial transitions) better reflect the intended prompt. RAPO++ in particular suppresses flickering, avoids sudden object deformation or disappearance, and handles complex conditions with greater fidelity, showing that the improvements observed in Tab. 6 and Tab. 7 indeed translate into tangible gains in visual realism and consistency."
        },
        {
            "title": "5.4 Extension to Physical-aware Video Generation",
            "content": "To further evaluate the effectiveness of Stage 2 SSPO (Sample-Specific Prompt Optimization at Test-Time) in handling task-specific scenarios, we extend our experiments to physical-aware video generation, where the generation quality is tightly linked to physical plausibility. This setting allows us to validate the impact of incorporating TaskSpecific Assessment into the SSPO framework, which introduces physics-based evaluators (e.g., physical consistency and semantic alignment) during the iterative prompt refinement process. We conduct experiments using three advanced T2V models (HunyuanVideo, CogVideoX-5B, and Wan2.1) on two physics-oriented datasets, PhyGenBench and VideoPhy. The experimental results, summarized in Tab. 8 and Tab. 9, demonstrate how performance evolves across iterative refinement rounds under this task-specific 11 Figure. 5: Qualitative comparisons using LaVie with initial prompts (left) and optimized prompts from RAPO++ (right). We present qualitative comparisons from the dynamic and static dimension. The videos generated by RAPO++ exhibit sharper details, smoother temporal transitions, and better alignment with the input text. evaluation setting. The results in Tab. 8 and Tab. 9 clearly show that integrating Task-Specific Assessment within SSPO leads to consistent and significant performance gains across all models and datasets. Here, one iteration refers to complete cycle of the SSPO process, where the generated video is evaluated with multi-source feedback (e.g., semantic alignment, temporal coherence, and physical plausibility), and the prompt is subsequently rewritten based on this feedback before generating new video. Repeating this iterative loop progressively improves the video quality across refinement rounds. For PhyGenBench, physical consistency (PC) and semantic alignment (SA) scores steadily improve with each iteration, with HunyuanVideo increasing from 0.38 to 0.57 in PC and from 0.24 to 0.42 in SA after four refinement rounds. Similar trends are observed for CogVideoX-5B and Wan2.1, confirming that iterative prompt optimization effectively enhances the physical realism and semantic alignment of generated videos. On the more challenging VideoPhy benchmark, improvements are consistent across all three interaction types (solid-solid, solid-fluid, and fluid-fluid). For example, HunyuanVideo achieves PC improvement from 0.28 to 0.40 and an SA increase from 0.41 to 0.65 in the solid-solid category, while similar upward trajectories are seen for the other categories and models. These results validate that the Task-Specific Assessment module in Stage 2 enables SSPO to adaptively guide prompt refinement toward physics-consistent video generation, thereby extending RAPO++s applicability to more complex, physically grounded scenarios."
        },
        {
            "title": "5.5 Analyses",
            "content": "Multiple objects. Synthesis quality of generated videos often declines when tasked with generating outputs that accurately represent prompts involving multiple objects. This issue is also prevalent in the T2I model, and several 12 Figure. 7: Visualization on attention map on multiple objects from different prompts. Adding description of the relative spatial position between objects can improve multiobject generation. TABLE 8: Iterative prompt optimization improves physical consistency on PHYGENBENCH. Video quality steadily improves over multiple refinement rounds, demonstrating that task-specific assessment in SSPO enhances physical consistency (PC) and semantic alignment (SA) across different T2V models. Round Model Metric 0 1 2 3 HunyuanVideo [2] CogVideoX-5B [26] Wan2.1 [1] PC SA PC SA PC SA 0.38 0.24 0.34 0.28 0.40 0.32 0.49 0.34 0.44 0.34 0.42 0. 0.53 0.37 0.49 0.36 0.44 0.40 0.55 0.41 0.51 0.38 0.48 0. 0.57 0.42 0.53 0.39 0.50 0.45 Figure. 6: Qualitative examples illustrating the limitation of RAPO++ in numeracy-related compositional tasks. Given prompts Five colorful parrots perch on tree branch (left) and Three majestic giraffes graze on the leaves of tall trees in the African savannah, their long necks reaching high, Salvador Dali style (right), the generated frames fail to accurately match the specified object counts, highlighting persistent challenges in precise numeracy understanding. studies [49], [50] have highlighted that the blended context created by the CLIP text encoder leads to improper binding. Meanwhile, some related works [53], [54] focus on image latents to address information loss, while the others [51], [55] pay more attention to text embedding to deal with the issue. However, few have explored optimizing prompts to improve the performance of multiple obejsts task. We apply our method to text-to-image using SD 1.4 [37], which uses the same text encoder with LaVie [27]. We test on prompts about multiple objects, and remove the irrelevant modifiers like action and atmosphere descriptions. As shown in Fig. 7, we can find the relevant spatial descriptions boost the performance of multiple objects. Statistical analysis of text. As shown in Fig. 8, we compared the word length distributions of prompts from the T2V training set, user prompts (simulated via VBench, EvalCrafter, and T2V-CompBench), and optimized prompts generated by various methods. The results show that the prompt length distribution produced by RAPO is closest to that of the training set, and this consistency unleashes Figure. 8: Prompt length distribution comparison among various methods. The distribution of RAPO-optimized prompts is more closer to the training prompts. the models generative potential to produce better videos. In contrast, user prompts are too short and lack necessary details, while other methods generate longer prompts that contain excessive details and complex vocabulary, which may be counterproductive, as shown in Tab. 6 and Tab. 7. Fine-tuning LLM. As shown in Tab.6 and Tab.7, the finetuned LLM significantly enhance generative performance of multiple objects and compositional T2V generation. For example, as shown in Fig. 9, the optimized prompt demonstrates significantly better accuracy and detail compared to the initial prompt. It more clearly instructs the model to generate an image of panda wearing red apron and name tag, working as cashier in Chinese New Year-themed supermarket, rather than defaulting to human cashier. The main reasons for this improvement are the prompts greater TABLE 9: Task-specific SSPO boosts physical awareness on VIDEOPHY. Across different interaction types, iterative refinement consistently improves physical consistency (PC) and semantic alignment (SA), showing that task-aware optimization generalizes well across complex physical dynamics. 13 Solid-Solid Solid-Fluid Fluid-Fluid Model Metric 0 HunyuanVideo [2] CogVideoX-5B [26] Wan2.1 [1] PC SA PC SA PC SA 0.28 0.41 0.27 0.54 0.26 0. 1 0.35 0.54 0.32 0.55 0.32 0.55 2 0.37 0. 0.35 0.57 0.37 0.59 3 0.39 0.64 0.38 0.58 0.39 0. 4 0.40 0.65 0.39 0.60 0.41 0.64 0 0.35 0. 0.38 0.67 0.31 0.62 1 0.44 0.74 0.48 0.69 0.42 0. 2 0.49 0.76 0.50 0.70 0.46 0.72 3 0.51 0. 0.52 0.70 0.48 0.74 4 0.52 0.78 0.54 0.71 0.49 0. 0 0.42 0.51 0.43 0.54 0.30 0.47 1 0.55 0. 0.53 0.63 0.44 0.61 2 0.58 0.66 0.57 0.64 0.50 0. 3 0.59 0.69 0.61 0.65 0.51 0.66 4 0.61 0. 0.62 0.65 0.53 0.67 refines the input prompt without relying on the internal structure of the T2V model, making it universally applicable to various architectures such as unet-based or DiT-based systems. By leveraging finetuned LLM, RAPO++ enhances video generation quality with minimal additional computational overhead, avoiding the need for expensive retraining while effectively aligning textual inputs with generated outputs. Its modular design also allows for seamless integration with existing prompt optimization methods, ensuring high compatibility across different frameworks. Together, these factors make RAPO++ model-unaware, cost-efficient, and highly compatible solution for improving T2V generation. Trade-offs between computational cost and performance. In our experiments, running several iterations at inference, each adding one extra pass through the T2V model plus VLM assessment, pushes inference time to roughly 3 that of single-pass baseline. Despite this overhead, RAPO++ delivers average gains of 3.5% on VBench (16 dimensions) and 18.1% on T2V-CompBench (4 dimensions) across LaVie and Latte, highlighting an efficient computeperformance trade-off. The additional 2 GB memory for LLaVAOneVision is negligible compared to the T2V models requirements."
        },
        {
            "title": "5.6 Ablation Study",
            "content": "We conduct ablation experiments on the VBench and T2VCompBench benchmark to examine the individual and combined effects of different modules in RAPO/RAPO++. Additionally, we perform ablation experiments on various configurations of rewriter LLM in Section 3. Owing to space constraints, additional visual results for the ablation study are available on our project website. Ablating each modules in RAPO. We directly obtain the related modifiers about input prompts utilizing GPT-4 [47], and merging them into inputs at one time as the comparison of word augmentation. We randomly select one of optimized prompts as the comparison of prompt selection. The optimal result is achieved by the full-fledged framework as shown in row (f). Ablation experiments on different L. We conduct ablation experiments on GPT-4 [47], Mistral [28] and LLaMA 3.1 [67]. As shown in Tab. 11, although GPT-4 achieves the best overall score, the differences are marginal, which suggests that RAPO is robust and effective across various LLMs in generating optimized prompts for T2V generation. Ablating SSPO mechanism and fine-tuning LLM in RAPO++. We conduct ablation experiments on the T2VFigure. 9: complex unusual example (a panda bear in red apron and name tag works as cashier in Chinese New Year-themed supermarket) generated by initial prompt (left) or optimized prompt (right). The generated video from optimized prompt is more consistent with initial prompt and user intention. specificity, clearer structure, stronger contextual emphasis, and explicit handling of the unusual concept (a panda taking on human role), all of which help the model better understand and produce the desired scene. Inference-time scaling performance. We further verify the inference-time scaling performance via iteratively prompt refinement. We conduct experiments on VideoScore [10] across temporal consistency, visual quality, T2V alignment, and factual consistency. We conduct experiments using LaVie [27], and use 2.2k T2V prompts provided in [7] as initial prompts. As shown in Fig. 10, each metric consistently increases across iterations, suggesting that RAPO++ leads to progressively refined outputs. Temporal Consistency and Visual Quality both show steady growth, reflecting improvements in coherent frame transitions and overall visual fidelity. T2V Alignment also demonstrates pronounced upward trend, indicating enhanced alignment between textual input and generated video content. Factual Consistency improves with each iteration, underscoring the systems growing ability to maintain accurate details throughout the generation process. Overall, these findings highlight the effectiveness of RAPO++ in bolstering multiple dimensions of video generation quality. Key Attributes of RAPO++. RAPO++ achieves its desirable properties through carefully designed iterative prompt optimization mechanism that operates independently of any specific T2V model architecture. The SSPO mechanism 14 Figure. 10: Inference-time scaling performance tested on temporal consistency, visual quality, T2V alignment, and factual consistency. We conduct experiments using LaVie [27] and utilize 2.2k T2V prompts provided in [7]. Each metric exhibits consistent upward trajectory as iteration count increases, underscoring the effectiveness of RAPO++ in enhancing generative performance. TABLE 10: Ablation studies of different modules in RAPO on VBench. Each module improves performance, while the combined use of all three leads to the highest evaluation score, confirming the synergistic effect of the full RAPO framework. word augmentation (a) (b) (c) (d) (e) (f) sentence refactoring prompt selection VBench Total Score 80.37% 79.75% 81.58% 81.75% 80.60% 82.38% TABLE 11: Ablation studies on different L. The results suggest that RAPO is robust and effective across various LLMs. GPT-4 Mistral"
        },
        {
            "title": "VBench Total Score",
            "content": "82.38% 82.25% 82.10% CompBench [57] to evaluate the impact of fine-tuning LLM Lo and SSPO mechanism. As shown in Tab. 12, either finetuning Lo or employing SSPO at inference improves performance across metrics such as consistent attribute binding, dynamic action binding, and object interaction. Combining both yields the best results."
        },
        {
            "title": "5.7 Limitation",
            "content": "Although RAPO++ achieves strong gains in compositionality, temporal stability, and physical plausibility, it still faces challenges in numeracy-related tasks. As shown in Fig. 6, when prompts explicitly specify object counts such as five parrots or three giraffes the generated videos often fail to match the intended number of entities. TABLE 12: Ablation results on T2V-CompBench [57] using LaVie [27]. The evaluation results verify the effectiveness of fine-tuning Lo and SSPO mechanism. The best is in bold. Method Consistent Attribute Binding Dynamic Attribute Binding w/o fine-tuning Lo, w/o SSPO w/o fine-tuning Lo, w/ SSPO w/ fine-tuning Lo, w/o SSPO w/ fine-tuning Lo, w/ SSPO 0.620 0.629 0.659 0.742 0.232 0.236 0.253 0.294 Action Binding 0.483 0.542 0.552 0.632 Object Interactions 0.760 0.778 0.835 0.849 This limitation stems from current T2V models tendency to blur numerical information with broader semantics and from SSPOs lack of fine-grained, count-aware feedback. Future work could integrate specialized counting verifiers and numeracy-sensitive assessment modules to better detect and penalize count mismatches, thereby improving number grounding and enhancing RAPO++s robustness in tasks requiring precise quantitative understanding."
        },
        {
            "title": "6 CONCLUSION AND FUTURE WORK",
            "content": "In this work, we propose RAPO++, three-stage prompt optimization framework that boosts T2V generation without changing the backbone by refining prompts (Stage 1), iteratively improving them with feedback (Stage 2), and finetuning the LLM for better generalization (Stage 3), achieving superior compositionality, dynamics, and physical realism over existing methods. In the future, we plan to make RAPO++ more efficient for real-time inference and extend it beyond T2V to tasks like controllable video editing, multimodal scene synthesis, and text-to-3D generation, establishing prompt optimization as core capability for future generative video systems."
        },
        {
            "title": "REFERENCES",
            "content": "[1] T. Wan, A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang et al., Wan: Open and advanced large-scale video generative models, arXiv preprint arXiv:2503.20314, 2025. [2] W. Kong, Q. Tian, Z. Zhang, R. Min, Z. Dai, J. Zhou, J. Xiong, X. Li, B. Wu, J. Zhang et al., Hunyuanvideo: systematic framework for large video generative models, arXiv preprint arXiv:2412.03603, 2024. [3] Q. Zhang, F. Lyu, Z. Sun, L. Wang, W. Zhang, W. Hua, H. Wu, Z. Guo, Y. Wang, N. Muennighoff et al., survey on test-time scaling in large language models: What, how, where, and how well? arXiv preprint arXiv:2503.24235, 2025. [4] B. Gao, Q. Zhou, and Y. Deng, Hie-edt: Hierarchical interval estimation-based evidential decision tree, Pattern Recognition, vol. 146, p. 110040, 2024. [5] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Cand`es, and T. Hashimoto, s1: Simple test-time scaling, arXiv preprint arXiv:2501.19393, 2025. [6] B. Gao, X. Gao, X. Wu, Y. Zhou, Y. Qiao, L. Niu, X. Chen, and Y. Wang, The devil is in the prompts: Retrieval-augmented prompt optimization for text-to-video generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 31733183. [7] Y. Wang, Z. Tan, J. Wang, X. Yang, C. Jin, and H. Li, Lift: Leveraging human feedback for text-to-video model alignment, arXiv preprint arXiv:2412.04814, 2024. [8] Z. Huang, F. Zhang, X. Xu, Y. He, J. Yu, Z. Dong, Q. Ma, N. Chanpaisit, C. Si, Y. Jiang et al., Vbench++: Comprehensive and versatile benchmark suite for video generative models, arXiv preprint arXiv:2411.13503, 2024. [9] B. Gao, Q. Zhou, and Y. Deng, Bim-afa: Belief information measure-based attribute fusion approach in improving the quality of uncertain data, Information Sciences, vol. 608, pp. 950969, 2022. [10] X. He, D. Jiang, G. Zhang, M. Ku, A. Soni, S. Siu, H. Chen, A. Chandra, Z. Jiang, A. Arulraj et al., Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation, arXiv preprint arXiv:2406.15252, 2024. [11] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu et al., Llava-onevision: Easy visual task transfer, arXiv preprint arXiv:2408.03326, 2024. [12] Y. Hao, Z. Chi, L. Dong, and F. Wei, Optimizing prompts for textto-image generation, Advances in Neural Information Processing Systems, vol. 36, pp. 66 92366 939, 2023. [13] M. Uehara, Y. Zhao, C. Wang, X. Li, A. Regev, S. Levine, and T. Biancalani, Inference-time alignment in diffusion models with reward-guided generation: Tutorial and review, arXiv preprint arXiv:2501.09685, 2025. [14] G. Wang, Y. Schiff, S. S. Sahoo, and V. Kuleshov, Remasking discrete diffusion models with inference-time scaling, arXiv preprint arXiv:2503.00307, 2025. [15] X. Zhang, H. Lin, H. Ye, J. Zou, J. Ma, Y. Liang, and Y. Du, Inference-time scaling of diffusion models through classical search, arXiv preprint arXiv:2505.23614, 2025. [16] N. Ma, S. Tong, H. Jia, H. Hu, Y.-C. Su, M. Zhang, X. Yang, Y. Li, T. Jaakkola, X. Jia et al., Inference-time scaling for diffusion models beyond scaling denoising steps, arXiv preprint arXiv:2501.09732, 2025. [17] Q. Ma, X. Ning, D. Liu, L. Niu, and L. Zhang, Decouple-thenmerge: Finetune diffusion models as multi-task learning, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 23 28123 291. [18] H. Bansal, Z. Lin, T. Xie, Z. Zong, M. Yarom, Y. Bitton, C. Jiang, Y. Sun, K.-W. Chang, and A. Grover, Videophy: Evaluating physical commonsense for video generation, arXiv preprint arXiv:2406.03520, 2024. [19] B. Han, Q. Xu, S. Bao, Z. Yang, K. Zi, and Q. Huang, Lightfair: Towards an efficient alternative for fair t2i diffusion via debiasing pre-trained text encoders, arXiv preprint arXiv:2509.23639, 2025. [20] E. Xie, J. Chen, Y. Zhao, J. Yu, L. Zhu, Y. Lin, Z. Zhang, M. Li, J. Chen, H. Cai et al., Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer, arXiv preprint arXiv:2501.18427, 2025. [21] Y. Oshima, M. Suzuki, Y. Matsuo, and H. Furuta, Inference-time text-to-video alignment with diffusion latent beam search, arXiv preprint arXiv:2501.19252, 2025. [22] Y. Hao, Z. Chi, L. Dong, and F. Wei, Optimizing prompts for textto-image generation, Advances in Neural Information Processing Systems, vol. 36, 2024. [23] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [24] Z. Chen, L. Zhang, F. Weng, L. Pan, and Z. Lan, Tailored visions: Enhancing text-to-image generation with personalized prompt rewriting, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 77277736. [25] W. Mo, T. Zhang, Y. Bai, B. Su, J.-R. Wen, and Q. Yang, Dynamic prompt optimizing for text-to-image generation, in CVPR, 2024. [26] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng et al., Cogvideox: Text-to-video diffusion models with an expert transformer, arXiv preprint arXiv:2408.06072, 2024. [27] Y. Wang, X. Chen, X. Ma, S. Zhou, Z. Huang, Y. Wang, C. Yang, Y. He, J. Yu, P. Yang et al., Lavie: High-quality video generation with cascaded latent diffusion models, arXiv preprint arXiv:2309.15103, 2023. [28] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al., Mistral 7b, arXiv preprint arXiv:2310.06825, 2023. 15 [30] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit et al., Vbench: Comprehensive benchmark suite for video generative models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 21 80721 818. [31] F. Meng, J. Liao, X. Tan, W. Shao, Q. Lu, K. Zhang, Y. Cheng, D. Li, Y. Qiao, and P. Luo, Towards world simulator: Crafting physical commonsense-based benchmark for video generation, arXiv preprint arXiv:2410.05363, 2024. [32] X. Ma, Y. Wang, G. Jia, X. Chen, Z. Liu, Y.-F. Li, C. Chen, and Y. Qiao, Latte: Latent diffusion transformer for video generation, arXiv preprint arXiv:2401.03048, 2024. [33] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural information processing systems, vol. 33, pp. 68406851, 2020. [34] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [35] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, Score-based generative modeling through stochastic differential equations, arXiv preprint arXiv:2011.13456, 2020. [36] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo et al., Improving image generation with better captions, Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, vol. 2, no. 3, p. 8, 2023. [37] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 68410 695. [38] L. Wang, X. Xing, Y. Cheng, Z. Zhao, J. Tao, Q. Wang, R. Li, X. Li, M. Wu, X. Deng et al., Promptenhancer: simple approach to enhance text-to-image models via chain-of-thought prompt rewriting, arXiv preprint arXiv:2509.04545, 2025. [39] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. uller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., Scaling rectified flow transformers for high-resolution image synthesis, in Forty-first International Conference on Machine Learning, 2024. [40] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 38363847. [41] Z. Huang, K. C. Chan, Y. Jiang, and Z. Liu, Collaborative diffusion for multi-modal face generation and editing, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 60806090. [42] J. Li, W. Feng, T.-J. Fu, X. Wang, S. Basu, W. Chen, and W. Y. Wang, T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback, arXiv preprint arXiv:2405.18750, 2024. [43] H. Chen, Y. Zhang, X. Cun, M. Xia, X. Wang, C. Weng, and Y. Shan, Videocrafter2: Overcoming data limitations for highquality video diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 73107320. [44] N. Hei, Q. Guo, Z. Wang, Y. Wang, H. Wang, and W. Zhang, user-friendly framework for generating model-preferred prompts in text-to-image synthesis, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 3, 2024, pp. 21392147. [45] J. Zhan, Q. Ai, Y. Liu, Y. Pan, T. Yao, J. Mao, S. Ma, and T. Mei, Prompt refinement with image pivot for text-to-image generation, arXiv preprint arXiv:2407.00247, 2024. [46] Open-sora: Democratizing efficient video production for all, 2024. URL: https: //github.com/hpcaitech/Open-Sora. [47] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [48] D. X. Long, X. Wan, H. Nakhost, C.-Y. Lee, T. Pfister, and S. O. Arık, Vista: test-time self-improving video generation agent, arXiv preprint arXiv:2510.15831, 2025. [49] W. Feng, X. He, T.-J. Fu, V. Jampani, A. Akula, P. Narayana, S. Basu, X. E. Wang, and W. Y. Wang, Training-free structured diffusion guidance for compositional text-to-image synthesis, arXiv preprint arXiv:2212.05032, 2022. [29] M. Wu, L. Wang, P. Zhao, F. Yang, J. Zhang, J. Liu, Y. Zhan, W. Han, H. Sun, J. Ji et al., Reprompt: Reasoning-augmented reprompting for text-to-image generation via reinforcement learning, arXiv preprint arXiv:2505.17540, 2025. [50] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. uller, J. Penna, and R. Rombach, Sdxl: Improving latent diffusion models for high-resolution image synthesis, arXiv preprint arXiv:2307.01952, 2023. [51] C. Zhuang, Y. Hu, and P. Gao, Magnet: We never know how text-to-image diffusion models work, until we learn how visionlanguage models function, arXiv preprint arXiv:2409.19967, 2024. [52] T. H. S. Meral, E. Simsar, F. Tombari, and P. Yanardag, Conform: Contrast is all you need for high-fidelity text-to-image diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 90059014. [53] H. Chefer, Y. Alaluf, Y. Vinker, L. Wolf, and D. Cohen-Or, Attendand-excite: Attention-based semantic guidance for text-to-image diffusion models, ACM Transactions on Graphics (TOG), vol. 42, no. 4, pp. 110, 2023. [54] Q. Phung, S. Ge, and J.-B. Huang, Grounded text-to-image synthesis with attention refocusing, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 79327942. [55] C.-Y. Chen, L.-W. Tsao, C. Tseng, and H.-H. Shuai, cat is cat (not dog!): Unraveling information mix-ups in text-to-image encoders through causal analysis and embedding optimization, arXiv preprint arXiv:2410.00321, 2024. [56] Y. Liu, X. Cun, X. Liu, X. Wang, Y. Zhang, H. Chen, Y. Liu, T. Zeng, R. Chan, and Y. Shan, Evalcrafter: Benchmarking and evaluating large video generation models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 22 13922 149. [57] K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu, T2vcompbench: comprehensive benchmark for compositional textto-video generation, arXiv preprint arXiv:2407.14505, 2024. [58] V. Liu and L. B. Chilton, Design guidelines for prompt engineering text-to-image generative models, in Proceedings of the 2022 CHI conference on human factors in computing systems, 2022, pp. 123. [59] N. Dehouche and K. Dehouche, Whats in text-to-image prompt? the potential of stable diffusion in visual arts education, Heliyon, vol. 9, no. 6, 2023. [60] J. Oppenlaender, taxonomy of prompt modifiers for text-toimage generation. arxiv, arXiv preprint arXiv:2204.13988, 2022. [61] A. Polyak, A. Zohar, A. Brown, A. Tjandra, A. Sinha, A. Lee, A. Vyas, B. Shi, C.-Y. Ma, C.-Y. Chuang et al., Movie gen: cast of media foundation models, arXiv preprint arXiv:2410.13720, 2024. [62] Y. He, M. Xia, H. Chen, X. Cun, Y. Gong, J. Xing, Y. Zhang, X. Wang, C. Weng, Y. Shan et al., Animate-a-story: Storytelling with retrieval-augmented video generation, arXiv preprint arXiv:2307.06940, 2023. [63] Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai, Animatediff: Animate your personalized textto-image diffusion models without specific tuning, arXiv preprint arXiv:2307.04725, 2023. [64] C. Zhao, M. Liu, W. Wang, J. Yuan, H. Chen, B. Zhang, and C. Shen, Moviedreamer: Hierarchical generation for coherent long visual sequence, arXiv preprint arXiv:2407.16655, 2024. [65] X. Wu, B. Gao, Y. Qiao, Y. Wang, and X. Chen, Cinetrans: Learning to generate videos with cinematic transitions via masked diffusion models, arXiv preprint arXiv:2508.11484, 2025. [66] S. Zhuang, K. Li, X. Chen, Y. Wang, Z. Liu, Y. Qiao, and Y. Wang, Vlogger: Make your dream vlog, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 88068817. [67] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [68] W. Peebles and S. Xie, Scalable diffusion models with transformers, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 41954205. [69] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, Hierarchical text-conditional image generation with clip latents, arXiv preprint arXiv:2204.06125, vol. 1, no. 2, p. 3, 2022. [70] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans et al., Photorealistic text-to-image diffusion models with deep language understanding, Advances in neural information processing systems, vol. 35, pp. 36 47936 494, 2022. [71] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen, Glide: Towards photorealistic image generation and editing with text-guided diffusion models, arXiv preprint arXiv:2112.10741, 2021. 16 [72] P. Esser, J. Chiu, P. Atighehchian, J. Granskog, and A. Germanidis, Structure and content-guided video synthesis with diffusion models, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 73467356. [73] Y. Jin, Z. Sun, N. Li, K. Xu, H. Jiang, N. Zhuang, Q. Huang, Y. Song, Y. Mu, and Z. Lin, Pyramidal flow matching for efficient video generative modeling, arXiv preprint arXiv:2410.05954, 2024. [74] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni et al., Make-a-video: Texttext-video data, arXiv preprint to-video generation without arXiv:2209.14792, 2022. [75] B. Han, Q. Xu, Z. Yang, S. Bao, P. Wen, Y. Jiang, and Q. Huang, Aucseg: Auc-oriented pixel-level long-tail semantic segmentation, Advances in Neural Information Processing Systems, vol. 37, pp. 126 863126 907, 2024. [76] H. Qiu, M. Xia, Y. Zhang, Y. He, X. Wang, Y. Shan, and Z. Liu, Freenoise: Tuning-free longer video diffusion via noise rescheduling, arXiv preprint arXiv:2310.15169, 2023. [77] F.-Y. Wang, W. Chen, G. Song, H.-J. Ye, Y. Liu, and H. Li, Genl-video: Multi-text to long video generation via temporal codenoising, arXiv preprint arXiv:2305.18264, 2023. [78] D. J. Zhang, J. Z. Wu, J.-W. Liu, R. Zhao, L. Ran, Y. Gu, D. Gao, and M. Z. Shou, Show-1: Marrying pixel and latent diffusion models for text-to-video generation, International Journal of Computer Vision, pp. 115, 2024. [79] J. Zhan, Q. Ai, Y. Liu, J. Chen, and S. Ma, Capability-aware prompt reformulation learning for text-to-image generation, in Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024, pp. 21452155. [80] Y. Wang, T. Xiong, D. Zhou, Z. Lin, Y. Zhao, B. Kang, J. Feng, and X. Liu, Loong: Generating minute-level long videos with autoregressive language models, arXiv preprint arXiv:2410.02757, 2024. [81] R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro, J. Kunze, and D. Erhan, Phenaki: Variable length video generation from open domain textual descriptions, in International Conference on Learning Representations, 2022. [82] M. Ding, W. Zheng, W. Hong, and J. Tang, Cogview2: Faster and better text-to-image generation via hierarchical transformers, Advances in Neural Information Processing Systems, vol. 35, pp. 16 89016 902, 2022. [83] O. Ma nas, P. Astolfi, M. Hall, C. Ross, J. Urbanek, A. Williams, A. Agrawal, A. Romero-Soriano, and M. Drozdzal, Improving text-to-image consistency via automatic prompt optimization, arXiv preprint arXiv:2403.17804, 2024. [84] M. Yang, Y. Du, B. Dai, D. Schuurmans, J. B. Tenenbaum, and P. Abbeel, Probabilistic adaptation of text-to-video models, arXiv preprint arXiv:2306.01872, 2023. [85] X. Chen, Y. Wang, L. Zhang, S. Zhuang, X. Ma, J. Yu, Y. Wang, D. Lin, Y. Qiao, and Z. Liu, Seine: Short-to-long video diffusion model for generative transition and prediction, in The Twelfth International Conference on Learning Representations. Jing, D. Schnurr, [86] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, J. Taylor, T. Luhman, E. Luhman L. et al., Video generation models as world simulators. 2024, URL https://openai. com/research/video-generation-models-asworld-simulators, vol. 3, p. 1, 2024. [87] S. Yang, J. Tan, M. Zhang, T. Wu, Y. Li, G. Wetzstein, Z. Liu, and D. Lin, Layerpano3d: Layered 3d panorama for hyper-immersive scene generation, arXiv preprint arXiv:2408.13252, 2024. [88] C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang, K. Kreis, S. Fidler, M.-Y. Liu, and T.-Y. Lin, Magic3d: Highresolution text-to-3d content creation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 300309. [89] R. Chen, Y. Chen, N. Jiao, and K. Jia, Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 22 24622 256. [90] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, Video diffusion models, Advances in Neural Information Processing Systems, vol. 35, pp. 86338646, 2022. [91] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts et al., Stable video diffusion: Scaling latent video diffusion models to large datasets, arXiv preprint arXiv:2311.15127, 2023."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory, Shanghai 201112, China",
        "Shanghai Jiao Tong University, Shanghai 200240, China"
    ]
}