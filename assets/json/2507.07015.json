{
    "paper_title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation",
    "authors": [
        "Hui Li",
        "Pengfei Yang",
        "Juanyang Chen",
        "Le Dong",
        "Yanxin Chen",
        "Quan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge distillation as an efficient knowledge transfer technique, has achieved remarkable success in unimodal scenarios. However, in cross-modal settings, conventional distillation methods encounter significant challenges due to data and statistical heterogeneities, failing to leverage the complementary prior knowledge embedded in cross-modal teacher models. This paper empirically reveals two critical issues in existing approaches: distillation path selection and knowledge drift. To address these limitations, we propose MST-Distill, a novel cross-modal knowledge distillation framework featuring a mixture of specialized teachers. Our approach employs a diverse ensemble of teacher models across both cross-modal and multimodal configurations, integrated with an instance-level routing network that facilitates adaptive and dynamic distillation. This architecture effectively transcends the constraints of traditional methods that rely on monotonous and static teacher models. Additionally, we introduce a plug-in masking module, independently trained to suppress modality-specific discrepancies and reconstruct teacher representations, thereby mitigating knowledge drift and enhancing transfer effectiveness. Extensive experiments across five diverse multimodal datasets, spanning visual, audio, and text, demonstrate that our method significantly outperforms existing state-of-the-art knowledge distillation methods in cross-modal distillation tasks. The source code is available at https://github.com/Gray-OREO/MST-Distill."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 5 1 0 7 0 . 7 0 5 2 : r MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation Hui Li gray1y@stu.xidian.edu.cn Xidian University Xian, China Le Dong dongle@xidian.edu.cn Xidian University Xian, China Pengfei Yang pfyang@xidian.edu.cn Xidian University Xian, China Juanyang Chen jychen_324@stu.xidian.edu.cn Xidian University Xian, China Yanxin Chen 24031110051@stu.xidian.edu.cn Xidian University Xian, China Quan Wang qwang@xidian.edu.cn Xidian University Xian, China ABSTRACT Knowledge distillation as an efficient knowledge transfer technique, has achieved remarkable success in unimodal scenarios. However, in cross-modal settings, conventional distillation methods encounter significant challenges due to data and statistical heterogeneities, failing to leverage the complementary prior knowledge embedded in cross-modal teacher models. This paper empirically reveals two critical issues in existing approaches: distillation path selection and knowledge drift. To address these limitations, we propose MST-Distill, novel cross-modal knowledge distillation framework featuring mixture of specialized teachers. Our approach employs diverse ensemble of teacher models across both crossmodal and multimodal configurations, integrated with an instancelevel routing network that facilitates adaptive and dynamic distillation. This architecture effectively transcends the constraints of traditional methods that rely on monotonous and static teacher models. Additionally, we introduce plug-in masking module, independently trained to suppress modality-specific discrepancies and reconstruct teacher representations, thereby mitigating knowledge drift and enhancing transfer effectiveness. Extensive experiments across five diverse multimodal datasets, spanning visual, audio, and text, demonstrate that our method significantly outperforms existing state-of-the-art knowledge distillation methods in cross-modal distillation tasks. The source code is available at https://github.com/Gray-OREO/MST-Distill. CCS CONCEPTS Computing methodologies Machine learning. KEYWORDS Cross-modality, Knowledge distillation, Mixture of teachers Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. MM 25, October 2731, 2025, Dublin, Ireland 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX ACM Reference Format: Hui Li, Pengfei Yang, Juanyang Chen, Le Dong, Yanxin Chen, and Quan Wang. 2025. MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation. In Proceedings of the 33rd ACM International Conference on Multimedia (MM 25). ACM, New York, NY, USA, 15 pages. https://doi.org/XXXXXXX.XXXXXXX Figure 1: Illustration of two key challenges in cross-modal knowledge distillation: distillation path selection and knowledge drift. Top: Performance comparison of unimodal students under different teachers (multimodal & cross-modal) on VGGSound-50k (visual & audio), with red lines as baselines. Bottom: Grad-CAM comparison between multimodal and cross-modal teachers on CrisisMMD-V2 (visual & text)."
        },
        {
            "title": "1 INTRODUCTION\nWith the rapid advancement of sensor technologies and intelligent\ndevices, data acquisition methods have diversified significantly,\ngenerating abundant multimodal data across vision, audio, and text\ndomains. This multimodal data provides rich training resources\n[2] and offers a more comprehensive perspective [52] for artificial\nintelligence models. By leveraging complementary cross-modal\ninformation, multimodal approaches have achieved remarkable\nperformance in video understanding [41], cross-modal retrieval\n[40], and human-computer interaction [10].",
            "content": "The expansion of these multimodal applications has driven an increasing trend toward distributed and near-sensor computing MM 25, October 2731, 2025, Dublin, Ireland Li et al. paradigms. Edge computing, which deploys intelligent systems closer to data sources, offers substantial benefits including reduced latency, enhanced privacy, and decreased bandwidth requirements, making it ideal for time-sensitive and data-intensive applications [47]. However, these edge deployments face significant challenges from dynamic network conditions and heterogeneous sensor characteristics [5, 27]. While conventional multimodal fusion methods assume well-aligned cross-modal correlations [3], real-world scenarios often involve temporal misalignment and partial modality absence due to asynchronous data transmission, degrading performance [27]. In this context, cross-modal knowledge transfer emerges as promising solution for resource-constrained edge devices, enabling efficient inference by constructing shared semantic spaces that facilitate the transfer of complementary knowledge across modalities. Knowledge distillation (KD) offers an effective technique for model compression and knowledge transfer within teacher-student framework [15]. Based on their supervision mechanisms, distillation methods are categorized as response-based [15, 30, 32], featurebased [6, 32, 48], and relation-based [30, 37]. While these approaches have demonstrated success in computer vision [23] and natural language processing [11], they primarily address knowledge transfer within single modality. When applied to cross-modal scenarios, these methods encounter additional challenges from data and statistical heterogeneity [17], which violates the distributional consistency assumption [35] in traditional distillation approaches, resulting in misaligned representations [44] and unreliable knowledge transfer [17]. Building on these foundational challenges, we empirically identify two critical issues that remain underexplored in existing literature: (1) Path selection in cross-modal knowledge distillation. As illustrated in Figure 1 (top), modality imbalance [17] manifest as pervasive asymmetry and uncertainty in the process of cross-modal knowledge distillation. Knowledge from certain source modalities exhibits stronger transferability for specific tasks, while the reverse direction often yields poor performance. Moreover, even multimodal teachers, despite their potential to leverage complementary information across modalities, do not always provide effective supervisory signals, introducing significant challenges to designing efficient and generalizable distillation paradigms. (2) Knowledge drift between teacher and student models. fundamental challenge in cross-modal knowledge distillation stems from inductive bias mismatches between models trained on different data domains. Even multimodal teachers may exhibit unimodal bias [50], resulting in substantial discrepancies between the teachers attention regions and those of the unimodal student when processing identical inputs. As shown in Figure 1 (bottom), the Grad-CAM [33] visualizations clearly illustrate these differences, revealing knowledge drift that impacts model behavior and transfer effectiveness. We provide comprehensive empirical analysis and in-depth discussion of both challenges in Appendix A. To tackle the challenges of distillation path selection and knowledge drift in cross-modal knowledge distillation, we propose MSTDistill, generalized and adaptive framework. By integrating diverse teacher models with an instance-level routing network, MSTDistill enables the target-modality student to dynamically select optimal distillation paths during training, thereby facilitating robust and flexible knowledge transfer. In addition, we introduce plug-in MaskNet module that reconstructs teacher representations under the guidance of response consistency, encouraging behavioral alignment and mitigating knowledge drift across modalities. Our main contributions are summarized as follows: We point out two key challenges in cross-modal knowledge distillation: distillation path selection and knowledge drift. To address them, we propose MST-Distill, unified framework integrating instance-level dynamic routing with reconstruction-consistency-guided teacher specialization mechanisms. We construct mixture of teachers comprising both multimodal and cross-modal models, coupled with an instancelevel routing network that allows the student model to adaptability select the optimal distillation path. To mitigate knowledge drift caused by inductive bias discrepancies between the models, we introduce learnable MaskNet module that effectively suppresses modality-specific discrepancies while reconstructing teacher representations aligned with the students behavior. Extensive experiments across five datasets demonstrate the effectiveness and generalizability of MST-Distill in crossmodal knowledge distillation tasks."
        },
        {
            "title": "2 RELATED WORK\n2.1 Multimodal Learning\nMultimodal learning has emerged as a prominent research focus\nin computer vision [22] and natural language processing [46]. By\nintegrating heterogeneous modality information, these approaches\nenable more comprehensive representation learning [24, 29] and\nhave demonstrated efficacy in applications including sentiment\nanalysis [36], video understanding [31], and multimodal dialogue\nsystems [21].",
            "content": "However, multimodal learning inherently faces significant challenges due to its complexity, spanning network architecture design [7], cross-modal distributional differences [8], and optimization strategies [42]. During training, these complexities manifest primarily as two fundamental obstacles that impede effective knowledge integration: modality conflict [14] and unimodal bias [20, 43]. Modality conflict emerges when semantic and structural inconsistencies between modalities destabilize the optimization process, while unimodal bias [50] occurs when training dynamics favor dominant modalities, suppressing information from others. These training challenges represent distinct barriers to modeling effective cross-modal relationships, ultimately constraining performance on downstream tasks. Building upon these foundational studies, researchers have made significant advances in addressing multimodal learning challenges. Zhang et al. [50] revealed critical architectural limitations in late fusion models that promote unimodal bias, while Wei et al. [42] introduced MMPareto, an optimization framework that effectively mitigates early-stage gradient conflicts through dynamic systems approach. Complementing these efforts, Fan et al. [9] developed prototypical modal rebalancing method that strategically applies task-oriented unimodal constraints to counteract modality imbalance. MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation MM 25, October 2731, 2025, Dublin, Ireland Although these contributions provide valuable theoretical insights and practical strategies for improving multimodal architectures and training methodologies, significant challenges persist. Particularly, the intrinsic difficulty of balancing modality-specific knowledge within multimodal frameworks continues to limit the effectiveness of cross-modal knowledge distillation, with knowledge drift emerging as critical barrier to achieving optimal performance gains."
        },
        {
            "title": "2.2 Cross-Modal Knowledge Distillation\nKnowledge distillation proves highly effective for unimodal knowl-\nedge transfer, yet cross-modal scenarios present unique challenges\nfrom heterogeneous data formats and statistical discrepancies [3].\nCross-Modal Knowledge Distillation (CMKD) addresses these issues\nby specifically bridging the modality gap, enhancing both repre-\nsentation quality and performance in target modalities through\neffective cross-modal knowledge alignment.",
            "content": "Early CMKD research focused on knowledge transfer between visually similar modalities (RGB, depth, infrared) that share visual characteristics despite different sensing principles [25]. The field has since expanded to highly heterogeneous modality pairs including vision, audio, and text [19], driving interest in modality complementarity and collaborative learning. To address structural and semantic discrepancies between diverse modalities, recent work has introduced sophisticated strategies such as contrastive learning [53], modality decoupling [16], shared semantic representations [49], and meta-learning [28]. These approaches have shown considerable efficacy across multimodal applications including video understanding, emotion recognition, and cross-modal retrieval. Despite these advances, current CMKD approaches remain primarily constrained to specific scenarios and distillation configurations, limiting their adaptability across diverse modality combinations and task requirements. Addressing this constraint, Xue et al. [45] introduced MKE, demonstrating viable cross-modal knowledge exchange between unimodal and multimodal models under generalized conditions. They subsequently developed the modality focusing hypothesis [44], approaching CMKD through strategic construction and shaping of the teacher models feature space. In recent contribution, Huo et al. [17] proposed C2KD, an innovative framework leveraging soft label rank consistency to guide dynamic sample selection for optimized knowledge transfer. While these efforts have established robust theoretical and optimization foundations for CMKD, significant opportunities remain in leveraging diverse teacher models and developing learnable mechanisms for behavioral alignment. To address these opportunities, we propose MST-Distill, generalized framework for cross-modal knowledge distillation that systematically addresses distillation path selection and behavioral alignment between models. This approach enables robust and adaptive knowledge transfer across heterogeneous modalities, advancing the state-of-the-art in crossmodal knowledge distillation."
        },
        {
            "title": "3 METHOD\nIn this section, we introduce the detailed implementation of our pro-\nposed Mixture of Specialized Teachers for Cross-Modal Knowledge\nDistillation (MST-Distill). An overview of MST-Distill framework is",
            "content": "illustrated in Figure 2, which is composed of three sequential stages: Collaborative Initialization (S1), Specialized Teacher Adaptation (S2), and Dynamic Knowledge Distillation (S3). In the following subsections, we elaborate on the objectives, mechanisms, and technical implementations of each stage."
        },
        {
            "title": "3.1 Collaborative Initialization of",
            "content": "Modality-Specific Members As further step toward exploring the potential benefits of teacher diversity in cross-modal knowledge transfer, introducing set of diverse teacher models becomes natural and effective design choice. We begin by formally defining the cross-modal knowledge , 𝑥 (𝑠 ) distillation task. Let = {(𝑥 (𝑠 ) 𝑠=1 denote 2 1 multimodal dataset with 𝑆 samples, where each sample consists of 𝑀 data from different modalities and corresponding label. The 𝑖-th modality is denoted by 𝑚𝑖 , with 𝑖 = 0 indicating the multimodal case, where 𝑥0 = (𝑥1, 𝑥2, . . . , 𝑥𝑀 ) combines all modality data for joint inference. The network model corresponding to modality 𝑚𝑖 is denoted as 𝑓𝑚𝑖 . 𝑀 ; 𝑦 (𝑠 ) )}𝑆 , . . . , 𝑥 (𝑠 ) In the first stage of MST-Distill, we do not specify target student modality in advance. Instead, we treat all 𝑀 + 1 models equally as modality-specific members and train them jointly for the collaborative initialization. The training objective consists of two components: task loss ℓ𝑡𝑎𝑠𝑘 , which supervises all members using the ground-truth label, and an alignment loss ℓ𝑎𝑙𝑖𝑔𝑛, which encourages prediction consistency via bidirectional KullbackLeibler (KL) divergence between all modality pairs. For single training sample, the losses are defined as: ℓtask = 𝑀 𝑖=0 CE (cid:0)𝑓𝑚𝑖 (cid:0)𝑥𝑖 ; 𝜃𝑚𝑖 (cid:1) , 𝑦(cid:1) , ℓalign = (cid:16) (cid:104) KL 𝑃𝑚𝑖 𝑃𝑚 𝑗 (cid:17) (cid:16) + KL 𝑃𝑚 𝑗 𝑃𝑚𝑖 (cid:17)(cid:105) , 0𝑖< 𝑗 𝑀 (1) (2) 𝑃𝑚𝑖 = softmax (cid:0)𝑓𝑚𝑖 (cid:0)(𝑥𝑖 ; 𝜃𝑚𝑖 (cid:1) /𝜏 (cid:1) , 𝑖 {0, . . . , 𝑀 }, (3) where CE() denotes the cross-entropy loss, KL() is the KullbackLeibler divergence, 𝜃𝑚𝑖 is the parameter set of model 𝑓𝑚𝑖 , and 𝑃𝑚𝑖 is the softened output distribution with temperature 𝜏. Notably, we do not apply gradient detachment to the outputs of the teacher models, in contrast to conventional bidirectional distillation practices, which enables mutual gradient propagation among modality-specific members. The loss function in Stage 1 for minibatch of size 𝐵 is given by: LS1 = 1 𝐵 𝐵 (cid:16) 𝑏=1 ℓ (𝑏 ) task + ℓ (𝑏 ) align (cid:17) , (4) where ℓ (𝑏 ) represents the per-sample loss corresponding to the 𝑏-th instance in the minibatch."
        },
        {
            "title": "Adaptation",
            "content": "Inspired by the feature significance-based filtering strategy in [44], which suppresses non-salient features in teacher representations, we introduce learnable, plug-in module called MaskNet in the MM 25, October 2731, 2025, Dublin, Ireland Li et al. Figure 2: Overview of the MST-Distill framework in two-modality setting, consisting of three stages: Collaborative Initialization (CI), Specialized Teacher Adaptation (STA), and Dynamic Knowledge Distillation (DKD). the base architecture, calculated as: 𝑁 = 𝑁𝑇 ,𝑚0 + 𝑁𝐴,𝑚𝑖 = 𝑀 𝑖=1 𝑖𝑡 𝑁𝑚𝑖 , 𝑀 𝑖=0 𝑖𝑡 (5) where 𝑁𝑇 ,𝑚0 and 𝑁𝐴,𝑚𝑖 denote the number of selected reconstruction layers in the multimodal teacher and auxiliary teachers, respectively. To simplify notation, we uniformly denote the number of selected layers from each teacher as 𝑁𝑚𝑖 . The structure of MaskNet is illustrated in Figure 3. For given intermediate feature 𝑍𝑙 R𝑑𝑚 from the 𝑙-th layer of teacher model, MaskNet first projects it into latent space R𝑑𝑚 𝑑ℎ via the projector (a linear layer followed by reshaping), and 𝑑𝑚, 𝑑ℎ are the input and hidden dimensions, respectively. This is followed by multi-head self-attention (MHSA) block [38] and linear layer, with sigmoid activation to produce soft attention mask. The masked 𝑙 R𝑑𝑚 is then obtained by an element-wise Hadamard feature 𝑍 product between the input and the soft mask. The process is formally defined as: 𝑍 𝑙 = MaskNet (𝑍𝑙 ; 𝜃MN) = 𝜎 (Linear (MHSA (Projector (𝑍𝑙 )))) 𝑍𝑙 , (6) where 𝜃MN denotes the parameters of the MaskNet module, and represents the Hadamard product operator. Subsequently, all model parameters except those of MaskNet are frozen, and each MaskNet is trained independently to align the behavior of its corresponding specialized teacher with that of the target student, guided by response consistency. Specifically, let 𝑓 𝑗 𝑚𝛿 ( 𝑗 ) denote the 𝑗-th specialized teacher model with corresponding MaskNet module, where 𝛿 ( 𝑗) is an index mapping function that Figure 3: Overall architecture of MaskNet. soft mask is generated to reconstruct the intermediate feature maps of the teacher model through standard multi-head self-attention mechanism. second stage of MST-Distill. This module generalizes soft maskingbased reconstruction to arbitrary intermediate layers of teacher models, thereby enabling efficient behavioral alignment between teachers and the target student model. As illustrated in the STA module of Figure 2, the selection of the target modality 𝑚𝑡 determines the corresponding unimodal student, the multimodal teacher, and the auxiliary cross-modal teachers. To further enhance the diversity of the teacher ensemble, we insert independently parameterized MaskNet modules into selected intermediate layers of each teacher model. Specifically, for single base teacher model, we create multiple specialized versions by incorporating different MaskNet instances at the same intermediate layers, where each MaskNet has its own independent parameters. This approach allows us to derive 𝑁 specialized teacher while reusing MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation MM 25, October 2731, 2025, Dublin, Ireland identifies the modality source of the 𝑗-th teacher: Similar to [15], the task-specific classification loss for the student (cid:40) 𝑗 𝑖 (cid:41) 𝑁𝑚𝑘 , 𝑗 {1, . . . , 𝑁 } , (7) is computed as: 𝛿 ( 𝑗) = arg min 𝑖 {0,...,𝑀 } 𝑖𝑡 𝑘=0 where 𝑁𝑚𝑘 is the number of specialized teachers derived from modality 𝑚𝑘 . For each training sample, the discrepancy between the output distributions of the 𝑗-th teacher and the target-modality student is measured as: ℓ𝑗 = KL (cid:16) 𝑚𝑡 𝑄 𝑗 𝑄 𝑗 𝑚𝛿 ( 𝑗 ) (cid:17) , (8) 𝑄 𝑗 (cid:17) (cid:17) (cid:16) 𝑓 𝑗 𝑚𝑖 (cid:16) 𝑥𝑖 ; 𝜃𝑚𝑖 , 𝜃 𝑗 , /𝜏 MN 𝑖 {0, . . . , 𝑀 }, 𝑚𝑖 = softmax (9) 𝑚𝑖 is the 𝑗-th specialized teacher under modality 𝑚𝑖 , 𝑄 𝑗 where 𝑓 𝑗 𝑚𝑖 denotes its corresponding softened output distribution, and 𝜃 𝑗 MN is the associated MaskNet parameter. As result, the loss function in Stage 2 for the 𝑗-th teacher over minibatch of size 𝐵 is defined as: 𝑗 S2 = 1 𝐵 𝐵 𝑏=1 ℓ (𝑏 ) 𝑗 . (10)"
        },
        {
            "title": "Mixture of Specialized Teachers",
            "content": "As the final stage of MST-Distill, the dynamic knowledge distillation process focuses on adaptively selecting and leveraging specialized teachers for the target modality 𝑚𝑡 . Given an input data 𝑥𝑡 from modality 𝑚𝑡 , the corresponding student model 𝑓𝑚𝑡 produces logits vector 𝑍𝑜𝑢𝑡 , which is then passed into routing network (GateNet) to generate confidence scores 𝐶 R𝑁 over all specialized teachers: (cid:1) , 𝑡 {1, . . . , 𝑀 } , 𝑍𝑜𝑢𝑡 = 𝑓𝑚𝑡 (cid:0)𝑥𝑡 ; 𝜃𝑚𝑡 (11) 𝐶 = softmax (GateNet (𝑍𝑜𝑢𝑡 ; 𝜃GN)) , (12) where GateNet() is the routing network with parameters 𝜃GN, implemented as multi-layer perceptron (MLP) with 𝑁 output nodes. Based on the confidence scores 𝐶, we adopt the TopK rule to select the indices of the 𝑘 highest-scoring teachers in descending order of confidence: Ttop-𝑘 = TopK𝑘 (cid:16)(cid:8)𝐶 𝑗 (cid:9)𝑁 𝑗=1 (cid:17) , (13) where Ttop-𝑘 denotes the indices of the selected teachers used for subsequent knowledge distillation. Notably, this instance-wise selection allows the student to dynamically choose the teachers with the highest cross-modal transferability. Given the selected top-𝑘 teachers, the distillation loss for each training sample is computed by measuring the KL divergence between the softened outputs of the student and those of the selected teachers: ℓ𝐷𝐾𝐷 = 𝑗 Ttop-𝑘 (cid:16) 𝑄 𝑗 𝑚𝛿 ( 𝑗 ) 𝑃𝑚𝑡 (cid:17) , KL (14) where 𝑄 𝑗 𝑚𝛿 ( 𝑗 ) denotes the softened output distribution from the 𝑗-th specialized teacher of modality 𝑚𝛿 ( 𝑗 ) , and 𝑃𝑚𝑡 is the students output distribution. ℓ𝑆 = CE (cid:0)𝑓𝑡 (cid:0)𝑥𝑡 ; 𝜃𝑚𝑡 (cid:1) , 𝑦(cid:1) . (15) Furthermore, to prevent the routing network from converging to only small subset of teachers, we incorporate load balancing loss that promotes diverse teacher utilization. Specifically, for each mini-batch, we calculate the average confidence distribution across all samples and compare it to uniform distribution 𝑈 R𝑁 using the Kullback-Leibler divergence: 𝐶 = 1 𝐵 𝐵 𝑏=1 𝐶 (𝑏 ), L𝐿𝐵 = KL (cid:0)𝑈 𝐶(cid:1) , (16) (17) where each element in 𝑈 is set to 1 𝑁 . This encourages uniform utilization of all specialized teachers throughout training. And the final loss function for Stage 3 over mini-batch is defined as: LS3 = 1 𝐵 𝐵 (cid:16) 𝑏=1 ℓ (𝑏 ) 𝑆 + 𝜆1 ℓ (𝑏 ) 𝐷𝐾𝐷 (cid:17) + 𝜆2 L𝐿𝐵, (18) where 𝜆1 and 𝜆2 are decay-weighted hyperparameters that gradually decrease during training. See Appendix C.1 for the detailed pseudocode of MST-Distill."
        },
        {
            "title": "4.1 Multimodal Classification\nWe follow [17, 44] and conduct experiments on four visual-audio\nand image-text datasets: (1) AV-MNIST [39] is a visual–audio\ndataset for digit classification, covering 10 categories of paired\nhandwritten digits and spoken audio spectrograms. (2) RAVDESS\n[26] is a visual–audio dataset for emotion recognition, with 8 emo-\ntional categories expressed through aligned facial and vocal cues. (3)\nVGGSound-50k [4] is a visual–audio scene classification dataset\nspanning 141 real-world categories [53] with co-occurring sound\nand visual content. (4) CrisisMMD-V2 [1] is an image–text dataset\nfor humanitarian classification, comprising 8 crisis-related cate-\ngories based on image–text pairs from social media. Further details\nof these datasets are provided in Appendix B.1.",
            "content": "MM 25, October 2731, 2025, Dublin, Ireland Li et al. Table 1: Performance comparison on multimodal classification datasets. The evaluation metric is the average overall accuracy over five independent runs. Dashed lines separate baselines trained independently on each modality. CM and MM denote cross-modal and multimodal teachers, respectively. Distillation results with performance gains are highlighted in Blue. The top two performing results are shown in Red, with the best result further underlined for emphasis. AV-MNIST RAVDESS VGGSound-50k CrisisMMD-V Paradigm Response-Based Feature-Based Relation-Based Mutual-Learning Cross-Modal Method T-Config KD MLLD FitNets w/o KD MM CM MM CM MM CM MM CM MM CM MM CM Image Audio Visual Audio Visual 0.4080 0.7472 0.6322 0.4248 0.7521 0.6322 0.3855 0.7035 0.6328 0.4241 0.7549 0.6288 0.4044 0.7208 0.6327 0.3778 0.7382 0.6255 0.3742 0.6889 0.6201 0.3808 0.7333 0.6331 0.3861 0.7292 0.6334 0.4156 0.7486 0.6341 0.4092 0.7569 0.6337 0.3804 0.7681 0.6333 0.3537 0.7736 0.6281 0.4601 0.7660 MM+CM 0.6320 0.4375 0.7576 0.6329 0.4229 0.7754 0.6309 MST-Distill MM+CM 0.6359 0.4595 0.7868 0.6694 0.6979 0.6944 0.6930 0.6799 0.6438 0.6229 0.6570 0.6445 0.6792 0.6826 0.6993 0.7014 0.7202 0.7111 0.6500 0.7174 0.4372 0.4383 0.4377 0.4372 0.4380 0.4332 0.4292 0.4367 0.4380 0.3868 0.3850 0.4387 0.4389 0.4393 0.4389 0.4389 0.4381 DML MGDFR C2KD CM CM CRD RKD OFA Audio 0.5818 0.5786 0.5887 0.5868 0.5898 0.5598 0.5685 0.5821 0.5839 0.5629 0.5709 0.5826 0.5826 0.5915 0.5967 0.6080 0.6098 Image 0.5524 0.5505 0.5501 0.5549 0.5541 0.5538 0.5458 0.5509 0.5493 0.5499 0.5508 0.5559 0.5504 0.5465 0.5469 0.5528 0. Text 0.5405 0.5367 0.5377 0.5098 0.5127 0.5317 0.5165 0.5394 0.5420 0.5406 0.5419 0.5268 0.5256 0.5351 0.5390 0.5204 0.5466 Implementation. We adopt consistent preprocessing strategy following [17, 44]. For each dataset, we use customized but consistent teacher and student architectures across all methods, detailed in Appendix B.2. All distillation methods use identical training conditions: 100 epochs for single-stage methods, consistent sub-stage epochs for multi-stage methods (FitNets, MGDFR, and ours), with uniform batch size and loss decay schedules. In our method, MaskNets with three self-attention heads are inserted into intermediate and penultimate layers of teacher networks (using post-fusion features for multimodal teachers). We implement Top𝑘 dynamic distillation with 𝑘 = 1, setting initial 𝜆1 and 𝜆2 to 1, with decay schedules of halving every 30 epochs and 10% reduction every 10 epochs, respectively. Data is split 60%/20%/20% for training/validation/testing, and results are averaged over five runs using the best validation model. All experiments are conducted on server equipped with an Intel Xeon Gold 6248R CPU and an NVIDIA A100 GPU. Comparison Results. We compare our MST-Distill framework against several advanced knowledge distillation baselines under identical training settings. As shown in Table 1, our proposed framework MST-Distill achieves either the best or the second-best performance across all four multimodal datasets. The advantages are particularly evident on datasets with pronounced modality imbalance, such as AV-MNIST, RAVDESS, and VGGSound-50k, demonstrating the robustness and generalization capability of our method in diverse cross-modal scenarios. In comparison to wide range of traditional knowledge distillation methods and recent crossmodal approaches, MST-Distill consistently delivers superior performance. It is worth noting that DML, general-purpose mutual learning method originally designed for unimodal settings, also achieves competitive results in the cross-modal domain. This further confirms the importance of utilizing diverse and collaborative teacher signals to enhance knowledge transfer. Furthermore, we observe that feature-based methods including FitNets and OFA tend to underperform. This may be attributed to their reliance on feature-level similarity, which can be inadequate for capturing the complementary knowledge across heterogeneous modalities. In contrast, relation-based methods such as RKD and CRD exhibit better compatibility, as the structural relations among samples are relatively more stable across modalities, making them more suitable for cross-modal distillation tasks."
        },
        {
            "title": "4.2 Multimodal Semantic Segmentation\nWe further evaluate the generalization of MST-Distill on the multi-\nmodal semantic segmentation task, focusing on knowledge transfer\nbetween closely related modalities (RGB and depth). Following the\nprotocol in [44], we conduct experiments on the NYU-Depth-V2\ndataset [34], which consists of 1,449 aligned RGB–depth image pairs\nwith annotations for 40 semantic categories.",
            "content": "Implementation. Consistent with our standardized setup in Section 4.1, we employ FuseNet [13] as the multimodal teacher model and derive unimodal student models from its modality-specific branches. Unlike classification tasks that operate at the sample level, our distillation occurs at finer pixel-wise granularity. To enhance feasibility, we apply knowledge transfer at the encoder-decoder bottleneck, focusing on mid-level features with lower dimensionality. Notably, due to the dense prediction nature of semantic segmentation, many traditional knowledge distillation methods designed for classification cannot be directly applied here. Thus, our comparison includes only subset of representative response-based and feature-based methods. MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation MM 25, October 2731, 2025, Dublin, Ireland Table 2: Performance comparison on NYU-Depth-V2 using Overall Accuracy (OA), Average Accuracy (AA), and mean IoU (mIoU) for both RGB and Depth modalities. Method T-Config KD w/o KD MM CM MM CM OA 0.5189 0.5251 0.5163 0.5167 0.4919 MM+CM 0.5455 0.5068 0.5329 MST-Distill MM+CM 0.5396 DML MGDFR C2KD CM CM FitNets RGB AA 0.2272 0.2338 0.2202 0.2077 0.1636 0.2402 0.2004 0.2332 0.2410 mIoU 0.1453 0.1491 0.1406 0.1338 0.1011 0.1567 0.1279 0.1525 0.1620 OA 0.5301 0.5479 0.5427 0.5337 0.5442 0.5570 0.5328 0.5468 0.5572 Depth AA 0.2015 0.2493 0.2285 0.2006 0.2151 0.2357 0.2352 0.2076 0.2584 mIoU 0.1318 0.1674 0.1529 0.1310 0.1443 0.1590 0.1567 0.1386 0. Comparison Results. To further validate the effectiveness of MSTDistill in transferring knowledge across closely related modalities for dense prediction tasks, we conduct multimodal semantic segmentation experiments on the NYU-Depth-V2 dataset. As shown in Table 2, MST-Distill demonstrates superior performance across all evaluation metrics (OA, AA, and mIoU) for both RGB and depth modalities, ranking first in five of six metrics and second in the remaining one. Notably, MST-Distill attains the highest mIoU scores on both modalities, with 0.1620 for RGB and 0.1797 for depth, clearly outperforming all existing baseline methods. These improvements demonstrate the frameworks capability to effectively capture and transfer fine-grained, structured knowledge across similar modalities. Moreover, consistent with our classification results, the generalpurpose distillation method DML also performs competitively in this task, further confirming the advantage of utilizing diverse teacher models for cross-modal knowledge transfer. In contrast, cross-modal methods such as C2KD and MGDFR fall short of MSTDistill, partly due to the asymmetry in distillation effectiveness caused by reliance on single cross-modal teacher. Overall, these results reinforce the adaptability and effectiveness of MST-Distill across both classification and segmentation tasks. Table 3: Ablation study of the three stages in MST-Distill. Each value indicates the mean accuracy averaged over all modality-specific students and five runs across four multimodal classification datasets. Setting S1 S2 AV-MNIST RAVDESS baseline (a) (b) (c) (d) (e) (f) default 0.5347 0.5362 0.5364 0.5369 0.5362 0.5371 0.5370 0.7083 0.7372 0.7372 0.7445 0.7500 0.7504 0. VGGSound50k CrisisMMDV2 0.4949 0.5273 0.5279 0.5406 0.5271 0.5349 0.5347 0.5465 0.5404 0.5381 0.5393 0.5432 0.5449 0."
        },
        {
            "title": "4.3 Ablation and Sensitivity Studies\nTo comprehensively understand the effectiveness of the proposed\nframework, we conduct ablation and sensitivity studies from two\nperspectives. First, we perform systematic ablation experiments to\nevaluate the contribution of the three core stages in MST-Distill.\nFollowing this, we investigate various configurations of the Mix-\nture of Specialized Teachers (MST), such as modality diversity and",
            "content": "Figure 4: Average routing probabilities of specialized teachers from single run of MST-Distill for the visual student on the RAVDESS dataset. Solid and dashed lines indicate multimodal and cross-modal teachers, respectively. the number of top-𝑘 selected teachers, to analyze their impact on performance and robustness. 4.3.1 Component Impacts of MST-Distill. Given the demonstrated advantages of diverse specialized teachers in prior experiments, we proceed with targeted ablation studies to investigate the specific contributions of each stage within the MST-Distill framework. Based on the established teacher diversity, we selectively activate three core stages: Collaborative Initialization (S1), Specialized Teacher Adaptation (S2), and Dynamic Knowledge Distillation (S3), in order to analyze their individual and combined impact. The results are shown in Table 3, which reveal three key observations: (1) Cross-modal knowledge distillation performs better with strongly aligned modality pairs. Compared to the independently trained student (baseline), setting (a) shows that mean-based distillation from diverse teachers significantly improves performance on well-aligned tasks such as RAVDESS and VGGSound-50k, but provides limited or even negative effects on loosely aligned data such as AV-MNIST and CrisisMMD-V2. (2) The proposed dynamic knowledge distillation strategy relies heavily on early-stage collaborative training. As shown in setting (b), applying dynamic distillation directly on static diverse teachers leads to minimal improvements. In contrast, both S1 (setting (c)) and S2 (setting (d)) introduce notable performance gains when compared to setting (b), highlighting the importance of model-teacher alignment and specialization before applying dynamic distillation. Interestingly, the benefits exhibit dataset-specific tendencies (RAVDESS benefits more from S2, while VGGSound-50k favors S1). (3) The transferability benefits from CI and STA are decoupled yet complementary. Their combination in setting (e) brings further improvements over using either stage alone. Incorporating S3 in setting (f) achieves the best overall performance, confirming the effectiveness of the full three-stage framework. 4.3.2 Routing Dynamics in MST-Distill. To better understand the mechanism of MST under dynamic knowledge, we further tracked the average routing probabilities of each teacher during RAVDESS training run. As shown as Figure 4, both multimodal and crossmodal teachers demonstrated significant engagement with distinctive adaptive selection patterns throughout the training process. MM 25, October 2731, 2025, Dublin, Ireland Li et al. Notably, CM-T 2 initially showed minimal selection probability but gradually increased its contribution over time, clearly demonstrating the adaptive nature of our DKD strategy. This dynamic adjustment of teacher contributions validates the effectiveness of our approach in automatically identifying and leveraging the most valuable knowledge sources as training progresses. 4.3.3 Configurations on MST-Distill. To extend our analysis beyond basic component ablations, we conduct detailed hyperparameter study of the MST-Distill framework, focusing on the nuanced configuration aspects within the Mixture of Specialized Teachers module. We systematically vary teacher compositions (cross-modal, multimodal, and their combinations) and examine how different values of the top-𝑘 parameter affect the dynamic knowledge distillation process. These fine-grained experiments complement our main ablations by revealing the sensitivity of distillation effectiveness to specific parameter choices, providing practical insights for optimal deployment. Additional experimental results and detailed analyses can be found in Appendix C. Figure 5: Box plots of OA improvements under different teacher configurations in MST. Results are based on five independent runs conducted on two representative multimodal classification datasets. Different box colors represent different teacher configurations. Effect of Teacher Diversity Configurations. Experiments on two representative multimodal classification datasets (visual, audio, and textual modalities) demonstrate our approachs effectiveness. As Figure 5 shows, the combined teacher configuration (CM+MM) consistently outperforms individual cross-modal (CM) or multimodal (MM) settings, achieving higher median OA improvements with reduced variance. In RAVDESS, CM+MM delivers stable and significant gains, particularly for visual students, with compact interquartile ranges and minimal outliers. For CrisisMMD-V2, which features weakly aligned modalities, CM+MM maintains superior stability and effectiveness, especially for textual students. These findings confirm the robustness and generalizability of combining cross-modal and multimodal teacher guidance. Figure 6: Performance trends under different top-𝑘 values in dynamic knowledge distillation. Student performance across modalities is distinguished by color and marker. Effect of Top-𝑘 Teacher Selection. Our sensitivity analysis examines parameter 𝑘, which controls the number of modality-specific teachers selected per sample during dynamic knowledge distillation. With 𝑘 ranging from 1 to 4 (limited by available feature layers for MaskNet insertion), Figure 6 reveals that performance improves when 𝑘 is below the maximum value, while 𝑘 = 4 consistently underperforms. This occurs because the adaptive selection mechanism degrades into uniform averaging when all teachers are used, eliminating sample-specific discrimination. These findings validate our top-𝑘-style solution. We adopt 𝑘 = 1 as our default across all experiments, as it consistently delivers robust performance on all datasets without requiring dataset-specific tuning."
        },
        {
            "title": "5 CONCLUSION AND DISCUSSION\nThis paper proposes MST-Distill, a novel framework for cross-\nmodal knowledge distillation that effectively tackles two critical\nchallenges: distillation path selection and knowledge drift. Our\napproach incorporates a diverse ensemble of specialized teach-\ners from both multimodal and cross-modal domains, utilizing an\ninstance-level routing network to dynamically select optimal teach-\ners for each input sample and a plug-in MaskNet module to address\nknowledge drift through response consistency supervision. Com-\nprehensive evaluations across five diverse multimodal benchmarks\ndemonstrate MST-Distill’s superior performance and generalizabil-\nity, consistently outperforming state-of-the-art knowledge distilla-\ntion and mutual learning methods. While enhancing effectiveness\nfor loosely aligned modalities remains an open challenge, this work\nestablishes a foundation for leveraging teacher diversity in cross-\nmodal knowledge transfer, with future research exploring more\nsophisticated methods including knowledge disentanglement and\ngradient modulation techniques, as well as extending to scenarios\nwith three or more modalities.",
            "content": "ACKNOWLEDGMENTS This work was supported in part by the Shaanxi Key Technology R&D Program under Grant 2024GX-ZDCYL-02-15 and the Natural Science Funds for Distinguished Young Scholar of Shaanxi under Grant 2025JC-JCQN-079. MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation MM 25, October 2731, 2025, Dublin, Ireland REFERENCES [1] Firoj Alam, Ferda Ofli, and Muhammad Imran. 2018. CrisisMMD: Multimodal Twitter Datasets from Natural Disasters. In Proceedings of the International AAAI Conference on Web and Social Media, Vol. 12. [2] Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2018. Multimodal Machine Learning: Survey and Taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence 41, 2 (2018), 423443. [3] Lluis Castrejon, Yusuf Aytar, Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. 2016. Learning Aligned Cross-Modal Representations from Weakly Aligned Data. In 2016 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 29402949. [4] Honglie Chen, Wjournali Xie, Andrea Vedaldi, and Andrew Zisserman. 2020. VGGSound: Large-Scale Audio-Visual Dataset. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 721725. [5] Jun-Ho Choi and Jong-Seok Lee. 2019. EmbraceNet: Robust Deep Learning Architecture for Multimodal Classification. Information Fusion 51 (2019), 259270. [6] Inseop Chung, SeongUk Park, Jangho Kim, and Nojun Kwak. 2020. Feature-MapLevel Online Adversarial Knowledge Distillation. In International Conference on Machine Learning. 20062015. [7] Xinyi Ding, Tao Han, Yili Fang, and Eric Larson. 2023. An Approach for Combining Multimodal Fusion and Neural Architecture Search Applied to Knowledge Tracing. Applied Intelligence 53, 9 (2023), 1109211103. [8] Hao Dong, Ismail Nejjar, Han Sun, Eleni Chatzi, and Olga Fink. 2023. SimMMDG: Simple and Effective Framework for Multi-Modal Domain Generalization. Advances in Neural Information Processing Systems 36 (2023), 7867478695. [9] Yunfeng Fan, Wenchao Xu, Haozhao Wang, Junxiao Wang, and Song Guo. 2023. PMR: Prototypical Modal Rebalance for Multimodal Learning. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2002920038. [10] Ryan Khushan Ghamandi, Ravi Kiran Kattoju, Yahya Hmaiti, Mykola Maslych, Eugene Matthew Taranta, Ryan McMahan, and Joseph LaViola. 2024. Unlocking Understanding: An Investigation of Multimodal Communication in Virtual Reality Collaboration. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 116. [11] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. MiniLLM: Knowledge Distillation of Large Language Models. In Proceedings of International Conference on Learning Representations. [12] Zhiwei Hao, Jianyuan Guo, Kai Han, Yehui Tang, Han Hu, Yunhe Wang, and Chang Xu. 2023. One-for-All: Bridge the Gap between Heterogeneous Architectures in Knowledge Distillation. Advances in Neural Information Processing Systems 36 (2023), 7957079582. [13] Caner Hazirbas, Lingni Ma, Csaba Domokos, and Daniel Cremers. 2016. FuseNet: Incorporating Depth into Semantic Segmentation via Fusion-Based CNN Architecture. In Asian Conference on Computer Vision. Springer, 213228. [14] Xiao He, Chang Tang, Xin Zou, and Wei Zhang. 2023. Multispectral Object Detection via Cross-Modal Conflict-Aware Learning. In Proceedings of the 31st ACM International Conference on Multimedia. 14651474. [15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in Neural Network. arXiv:1503.02531 [16] Weipeng Hu, Bohong Liu, Haitang Zeng, Yanke Hou, and Haifeng Hu. 2022. Adversarial Decoupling and Modality-Invariant Representation Learning for Visible-Infrared Person Re-Identification. IEEE Transactions on Circuits and Systems for Video Technology 32, 8 (2022), 50955109. [17] Fushuo Huo, Wenchao Xu, Jingcai Guo, Haozhao Wang, and Song Guo. 2024. C2KD: Bridging the Modality Gap for Cross-Modal Knowledge Distillation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1600616015. [18] Ying Jin, Jiaqi Wang, and Dahua Lin. 2023. Multi-level logit distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2427624285. [19] Donghwa Kim and Pilsung Kang. 2022. Cross-Modal Distillation with Audio-Text Fusion for Fine-Grained Emotion Classification Using BERT and Wav2vec 2.0. Neurocomputing 506 (2022), 168183. [20] Michael Kleinman, Alessandro Achille, and Stefano Soatto. 2023. Critical Learning Periods for Multisensory Integration in Deep Networks. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2429624305. [21] Ke Li, Fuyu Dong, Di Wang, Shaofeng Li, Quan Wang, Xinbo Gao, and Tat-Seng Chua. 2024. Show Me What and Where Has Changed? Question Answering and Grounding for Remote Sensing Change Detection. arXiv:2410.23828 [22] Ke Li, Di Wang, Haojie Xu, Haodi Zhong, and Cong Wang. 2024. LanguageGuided Progressive Attention for Visual Grounding in Remote Sensing Images. IEEE Transactions on Geoscience and Remote Sensing (2024). [23] Zhihui Li, Pengfei Xu, Xiaojun Chang, Luyao Yang, Yuanyuan Zhang, Lina Yao, and Xiaojiang Chen. 2023. When Object Detection Meets Knowledge Distillation: Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 8 (2023), 1055510579. [24] Xiao Liang, Yanlei Zhang, Di Wang, Haodi Zhong, Ronghan Li, and Quan Wang. 2024. Divide and Conquer: Isolating Normal-Abnormal Attributes in Knowledge Graph-Enhanced Radiology Report Generation. In Proceedings of the 32nd ACM International Conference on Multimedia. 49674975. [25] Yupeng Liang, Ryosuke Wakaki, Shohei Nobuhara, and Ko Nishino. 2022. Multimodal Material Segmentation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1980019808. [26] Steven Livingstone and Frank Russo. 2018. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English. PloS one 13, 5 (2018), e0196391. [27] Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, and Xi Peng. 2023. Are Multimodal Transformers Robust to Missing Modality?. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1817718186. [28] Wenxuan Ma, Shuang Li, Lincan Cai, and Jingxuan Kang. 2024. Learning Modality Knowledge Alignment for Cross-Modality Transfer. In Proceedings of the 41st International Conference on Machine Learning. 3377733793. [29] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, Andrew Ng, et al. 2011. Multimodal Deep Learning. In International Conference on Machine Learning, Vol. 11. 689696. [30] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. 2019. Relational Knowledge Distillation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 39673976. [31] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. 2024. TimeChat: Time-Sensitive Multimodal Large Language Model for Long Video Understanding. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1431314323. [32] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2015. FitNets: Hints for Thin Deep Nets. In Proceedings of International Conference on Learning Representations. [33] Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. In Proceedings of the IEEE International Conference on Computer Vision. 618626. [34] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. 2012. Indoor Segmentation and Support Inference from RGBD Images. In European Conference on Computer Vision. Springer, 746760. [35] Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander Alemi, and Andrew Wilson. 2021. Does Knowledge Distillation Really Work?. In Advances in Neural Information Processing Systems, Vol. 34. 69066919. [36] Jun Sun, Shoukang Han, Yu-Ping Ruan, Xiaoning Zhang, Shu-Kai Zheng, Yulong Liu, Yuxin Huang, and Taihao Li. 2023. Layer-Wise Fusion with Modality Independence Modeling for Multi-Modal Emotion Recognition. In Annual Meeting of the Association for Computational Linguistics. 658670. [37] Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive Representation Distillation. In Proceedings of International Conference on Learning Representations. [38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. Advances in Neural Information Processing Systems 30 (2017). [39] Valentin Vielzeuf, Alexis Lechervy, Stéphane Pateux, and Frédéric Jurie. 2018. CentralNet: Multilayer Approach for Multimodal Fusion. In European Conference on Computer Vision Workshops. [40] Di Wang, Caiping Zhang, Quan Wang, Yumin Tian, Lihuo He, and Lin Zhao. 2022. Hierarchical Semantic Structure Preserving Hashing for Cross-Modal Retrieval. IEEE Transactions on Multimedia 25 (2022), 12171229. [41] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. 2024. InternVideo2: Scaling Foundation Models for Multimodal Video Understanding. In European Conference on Computer Vision. Springer, 396416. [42] Yake Wei and Di Hu. 2024. MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance. In International Conference on Machine Learning. PMLR, 5255952572. [43] Nan Wu, Stanislaw Jastrzebski, Kyunghyun Cho, and Krzysztof Geras. 2022. Characterizing and Overcoming the Greedy Nature of Learning in Multi-Modal Deep Neural Networks. In International Conference on Machine Learning. PMLR, 2404324055. [44] Zihui Xue, Zhengqi Gao, Sucheng Ren, and Hang Zhao. 2023. The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation. In Proceedings of International Conference on Learning Representations. [45] Zihui Xue, Sucheng Ren, Zhengqi Gao, and Hang Zhao. 2021. Multimodal Knowledge Expansion. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 854863. [46] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi PontTuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. 2023. Vid2Seq: Large-Scale Pretraining of Visual Language Model for Dense Video Captioning. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10714 10726. MM 25, October 2731, 2025, Dublin, Ireland Li et al. [47] Jiangchao Yao, Shengyu Zhang, Yang Yao, Feng Wang, Jianxin Ma, Jianwei Zhang, Yunfei Chu, Luo Ji, Kunyang Jia, Tao Shen, et al. 2022. Edge-Cloud Polarization and Collaboration: Comprehensive Survey for AI. IEEE Transactions on Knowledge and Data Engineering 35, 7 (2022), 68666886. [48] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. 2017. Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning. In 2017 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 41334141. [49] Li Zhang and Xiangqian Wu. 2022. Latent Space Semantic Supervision Based on Knowledge Distillation for Cross-Modal Retrieval. IEEE Transactions on Image Processing 31 (2022), 71547164. [50] Yedi Zhang, Peter Latham, et al. 2024. Understanding Unimodal Bias in Multimodal Deep Linear Networks. In International Conference on Machine Learning, Vol. 235. PMLR. [51] Ying Zhang, Tao Xiang, Timothy Hospedales, and Huchuan Lu. 2018. Deep Mutual Learning. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 43204328. [52] Jing Zhao, Xijiong Xie, Xin Xu, and Shiliang Sun. 2017. Multi-View Learning Overview: Recent Progress and New Challenges. Information Fusion 38 (2017), 4354. [53] Jinxing Zhou, Dan Guo, and Meng Wang. 2022. Contrastive Positive Sample Propagation along the Audio-Visual Event Line. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 6 (2022), 72397257. MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation MM 25, October 2731, 2025, Dublin, Ireland"
        },
        {
            "title": "A SUPPLEMENTARY ANALYSIS SUPPORTING",
            "content": "METHOD MOTIVATION A.1 Distillation Path Selection As discussed in the main text, cross-modal knowledge distillation suffers from inherent asymmetry and uncertainty, which manifest in two aspects: the knowledge transferability between unimodal models, and the transferability from multimodal to unimodal models. Further experiments and analysis are provided in the following sections. Figure 7: Performance comparison of multimodal and unimodal models trained from scratch across five multimodal datasets. Different colors are used to distinguish the performance of the multimodal model and various unimodal models. A.1.1 Potential advantages of multimodal teachers. We independently train the multimodal and unimodal models corresponding to the five datasets mentioned in the main text, in order to establish performance benchmarks for the teacher and student models. The results are visualized as grouped bar charts in Figure 7. It can be clearly observed that, in most cases, multimodal models effectively integrate information from different modalities and achieve better performance than unimodal modelsexcept on the VGGSound-50k dataset, where significant modality imbalance exists. This suggests that multimodal models are capable of learning complementary prior knowledge across modalities, which gives them potential advantage in cross-modal knowledge distillation tasks. This also motivates our utilization of multimodal teacher models within the conventional cross-modal knowledge distillation framework. A.1.2 Knowledge transferability differences among diverse teachers. We further conduct teacher diversity experiments using the classic logits-based knowledge distillation method across the same datasets. The overall accuracies of different teacher-student combinations are shown as bar charts in Figure 8. Multimodal teacher models demonstrate strong transferability across all modalities on the RAVDESS and NYU-Depth-V2 datasets, and exhibit good transferability specifically to the visual modality on VGGSound-50k. However, their effectiveness is not significant on the remaining datasets. In summary, while multimodal teachers Figure 8: Performance comparison between multimodal and cross-modal teachers under response-based knowledge distillation. Subfigures (a) and (b) present the global accuracies of target-modality student models under different teacher selection strategies. The performance of unimodal models trained from scratch is indicated in green for reference. show potential for cross-modal knowledge distillation, their success is not guaranteedreflecting the uncertainty previously discussed. This variability in transferability also exists among cross-modal teachers, and often manifests as asymmetry. For instance, on the RAVDESS and VGGSound datasets, knowledge distillation from audio teachers to visual students fails, whereas the reverse direction proves to be effective. It is worth noting that such disadvantages are difficult to overcome in conventional cross-modal distillation settings that rely solely on single cross-modal teacher. These observations highlight that relying on either single multimodal teacher or single cross-modal teacher alone is insufficient for consistently achieving optimal distillation outcomes, thereby giving rise to the distillation path selection problem in cross-modal knowledge distillation. A.2 Knowledge Drift As mentioned in the main text, the difference in inductive biases under different modality inputs not only arises between distinct unimodal models, but also exists between the target-modality branch of multimodal model and its corresponding unimodal model. To illustrate this, we present Grad-CAM visualizations (Figure 9) comparing the visual branch of multimodal teacher model and visual student model, both trained from scratch on the CrisisMMD-V2 dataset. MM 25, October 2731, 2025, Dublin, Ireland Li et al. Figure 9: Grad-CAM visualizations comparing the visual attention of multimodal teacher and unimodal visual student on the CrisisMMD-V2 dataset. Text inputs are shown above each image. The attention map of the better-performing model is highlighted in bold; both are bolded if their performance is similar. The behavioral differences observed between these models exhibit notable complexity and task-dependent variations. From performance perspective, the visual branch of the multimodal model demonstrates superior performance in certain scenarios. Specifically, in the first two columns, the multimodal visual branch accurately focuses on the target person within the image, whereas the unimodal visual student fails to achieve this precise localization. Conversely, the unimodal student exhibits superior performance in alternative contexts. As evidenced in the middle two columns, the multimodal model erroneously attends to irrelevant regions including background elements and non-target individuals, while the unimodal model maintains correct focus on the intended subject. In some instances, the performance differences between the two approaches are marginal, suggesting task-specific advantages rather than consistent superiority of either method. These findings indicate that such inconsistencies in model behavior occur at the instance level, which can result in knowledge drift between the teacher and student models. This further motivates the introduction of MaskNet, an instance-aware and learnable teacher feature reconstruction module designed to adaptively align the teachers guidance with the needs of each student instance."
        },
        {
            "title": "EXPERIMENTAL DETAILS",
            "content": "B.1 Dataset Descriptions For the multimodal datasets used in the main text, we provide summary of their key characteristics in Table 4, followed by detailed descriptions in the subsequent paragraphs. Table 4: Overview of the five multimodal datasets utilized in our experiments, including modality composition, data scale, number of classes, and target task for each dataset. Modality Dataset AV-MNIST Image, Audio RAVDESS (Speech) Video, Audio Video, Audio VGGSound-50k Image, Text CrisisMMD-V2 RGB images, Depth images NYU-Depth-V2 Scale Classes Task 70k 1,440 49k 16058 1,449 10 8 141 8 41 Handwritten digit recognition Emotion recognition Scene classification Social media comment prediction Indoor scene semantic segmentation AV-MNIST Dataset. The AV-MNIST dataset is multimodal benchmark for digit recognition integrating visual and auditory modalities, comprising 70,000 paired samples across 10 classes, where the visual modality consists of 28 28 MNIST images with 75% of their energy removed via principal component analysis (PCA) to simulate low-quality input, while the auditory modality is derived from 25,102 spoken digit audio samples of the Tidigits database, augmented by overlaying noise segments from the ESC-50 dataset, and represented as 112 112 spectrograms. RAVDESS Dataset. The RAVDESS dataset is multimodal benchmark for emotion recognition, comprising 1,440 audiovisual samples of emotional utterances. Both video and audio modalities are preprocessed to uniform duration of 3.6 seconds, with visual inputs derived from 15 uniformly sampled frames, further processed using the MTCNN to extract facial regions from video frames. The audio modality is represented by 15-dimensional Mel-frequency cepstral coefficients (MFCCs) extracted at sampling rate of 22,050 Hz. VGGSound-50k Dataset. The VGGSound-50k dataset constitutes high-quality multimodal benchmark comprising 48,755 YouTube video clips across 141 fine-grained audio-visual scene categories, derived from VGGSound-AVEL50k through rigorous quality filtering. Following AVELs methodology, we extract visual features using VGG19 at 16fps and audio features via VGGish with standard preprocessing, maintaining temporal alignment between modalities. With standardized feature dimensions (512D visual, 128D audio) and 10-second average clip duration, this dataset provides robust testbed for advancing research in multimodal scene understanding. CrisisMMD-V2 Dataset. The CrisisMMD-V2 dataset serves as multimodal benchmark for humanitarian crisis classification. It contains 16,058 annotated Twitter posts, each comprising paired image and text, spanning 8 distinct humanitarian categories. Each sample includes manually verified semantic alignment between the visual and textual modalities, supporting joint vision-language modeling. For feature extraction, we utilize pretrained ResNet-50 MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation MM 25, October 2731, 2025, Dublin, Ireland"
        },
        {
            "title": "C SUPPLEMENTARY MATERIALS FOR",
            "content": "11: (with the classification head removed) for images, and BERTweetbase for textual representations. NYU-Depth-V2 Dataset. The NYU-Depth-V2 dataset serves as widely adopted benchmark for indoor scene understanding, comprising 1,449 precisely aligned RGB-depth image pairs captured using Microsoft Kinect sensors in diverse indoor environments (e.g., homes, offices, and stores). Each image is meticulously annotated with 40 semantic categories (e.g., walls, floors, furniture) and dense pixel-level labels, facilitating research in semantic segmentation, depth estimation, and 3D reconstruction tasks. B.2 Model Configurations Across Datasets The network architectures of the multimodal model and the two corresponding unimodal models for the five multimodal datasets discussed in the main text are summarized in Table 5. For more implementation details, please refer to our released code. MST-DISTILL This section presents supplementary materials that validate key design choices, clarify implementation details, and provide additional evidence for the effectiveness of the proposed MST-Distill framework. C.1 MST-Distill: Pseudocode The overall procedure of MST-Distill, encompassing the three stages described in main text, is summarized in Algorithm 1. C.2 Feature Visualization Analysis To further understand the working mechanisms of our proposed methods, we conduct comprehensive visualization analyses on different datasets and components. For MaskNet, we perform t-SNE visualization on 2048 samples from the AV-MNIST validation set when the visual modality is selected as the target student modality. We extract features from the penultimate layer of the multimodal teacher before and after MaskNet processing during the early and late phases of the Specialized Teacher Adaptation (STA) stage, as shown in Figure 10. The results reveal that in early training phase, MaskNet only affects partial samples within few categories (e.g., class 3). As training progresses, MaskNets influence becomes more pronounced, exhibiting finer-grained instance-level reconstruction that leads to further intra-class cluster differentiation. This aligns with our intuition that modality differences exhibit sample-level distinctions, validating the modules effectiveness in suppressing inter-modal disparities. For MST, we conduct Grad-CAM visualization on easy and hard samples from the CrisisMMD-V2 test set, as shown in Figure 11. Compared to multimodal teachers trained from scratch, the Specialized Teachers within the MST-Distill framework consistently extract more diverse visual cues after integrating different MaskNet variants. This enriches the teachers representation and helps the distilled student model attend to broader informative regions than independently trained students. Algorithm 1 MST-Distill Framework for Cross-Modal Knowledge Distillation Require: Multimodal dataset = {(𝑥 (𝑠 ) 1 , . . . , 𝑥 (𝑠 ) Target student modality index 𝑡 {1, 2, ..., 𝑀 } 𝑀 ; 𝑦 (𝑠 ) )}𝑆 𝑠=1; Ensure: Trained student model 𝑓𝑚𝑡 with optimal parameters 𝜃 𝑚𝑡 1: Stage 1: Collaborative Initialization (CI) 2: Initialize {𝜃𝑚𝑖 }𝑀 3: {𝜃 4: Stage 2: Specialized Teacher Adaptation (STA) 𝑖=0 for all models and initialize {𝜃 𝑗 }𝑁 5: Load {𝜃 𝑗=1 6: Identify target student modality 𝑚𝑡 and corresponding student 𝑖=0 for all modality-specific models {𝑓𝑚𝑖 }𝑀 𝑖=0 LS1 {Equation (4)} 𝑖=0 arg min{𝜃𝑚𝑖 }𝑀 𝑖=0 𝑚𝑖 }𝑀 𝑚𝑖 }𝑀 MN }𝑁 𝑗=1 via MaskNet insermodel 𝑓𝑚𝑡 tion 7: Construct specialized teachers {𝑓 𝑗 𝑚𝛿 ( 𝑗 ) 8: Freeze all model parameters except {𝜃 𝑗 9: for each specialized teacher 𝑓 𝑗 𝑚𝛿 ( 𝑗 ) do 10: }𝑁 𝑗=1 MN Apply soft masking via MaskNet during forward pass 𝜃 𝑗 MN arg min𝜃 𝑗 {Equation (10)} 𝑗 2 MN 12: end for 13: Stage 3: Dynamic Knowledge Distillation (DKD) 14: Load 𝑓𝑚𝑡 with 𝜃 𝑚𝑡 from Stage 1 15: Load specialized teachers {𝑓 𝑗 𝑚𝛿 ( 𝑗 ) 𝑖=0,𝑖𝑘 from Stage 1 and MaskNet parameters {𝜃 𝑗 }𝑁 𝑗=1 with base parameters }𝑁 𝑗= MN 𝑚𝑖 }𝑀 {𝜃 from Stage 2 16: Initialize 𝜃GN for GateNet 17: Freeze all parameters except 𝜃𝑚𝑡 and 𝜃GN 18: for each minibatch 𝐵 from do 19: 20: Compute student logits 𝑍out = 𝑓𝑚𝑡 (𝑥𝑡 ; 𝜃𝑚𝑡 ) Select top-𝑘 teachers based on GateNet confidence scores {𝜃 21: 𝑚𝑡 22: end for 23: return 𝑓𝑚𝑡 (; 𝜃 } arg min𝜃𝑚𝑡 ,𝜃GN LS3 {Equation (18)} , 𝜃 GN 𝑚𝑡 ) C.3 Computational Efficiency Analysis To evaluate computational overhead, we analyze our method using AV-MNIST as representative benchmark, where computational patterns remain consistent across multimodal datasets. Training time measures effective iterations to convergence with early stopping, while peak memory indicates maximum GPU consumption. For single-teacher baselines, training time represents cumulative cost across multiple sessions due to separate teacher selection per modality. As shown in Table 6, MST-Distill requires 53.6 GB peak memory and 1942.2s training time, the highest among all methods. While this overhead stems from simultaneous multi-teacher processing and complex cross-modal knowledge transfer, it is justified by substantial performance gains. Since cross-modal distillation aims for high-performance student models with efficient inference, the one-time training cost is acceptable given persistent deployment benefits. Furthermore, MaskNets parameter count can be flexibly adjusted during deployment by selecting teacher reconstruction layers and hidden nodes, enabling performance-cost trade-offs with considerable optimization potential. MM 25, October 2731, 2025, Dublin, Ireland Li et al. Table 5: Model configurations for each dataset, including two unimodal models and one multimodal model. The suffix net_woHead indicates that the classification head of the corresponding unimodal network is removed, and the symbol denotes the feature concatenation operation. Here, I, V, and represent image, video, and audio modalities, respectively. Dataset Unimodal Model 1 AV-MNIST I: LeNet Unimodal Model 2 A: ThreeLayerCNN-2D RAVDESS V: ThreeLayerCNN-3D + MLP A: ThreeLayerCNN-1D + MLP VGGSound-50k V: ThreeLayerCNN-3D + MLP CrisisMMD-V2 NYU-Depth-V I: MLP RGB: FuseNet (RGB-Encoder) + Decoder_RGB D: FuseNet(D-Encoder) + Decoder_D FuseNet A: ThreeLayerCNN-1D + MLP T: MLP Multimodal Model Inet_woHead FiveLayerCNN-2D_woHead + MLP Vnet_woHead Anet_woHead + MLP Vnet_woHead Anet_woHead + MLP Inet_woHead Tnet_woHead + MLP Figure 10: t-SNE visualizations of teacher features before and after MaskNet processing on AV-MNIST dataset. (a) Early phase and (b) Late phase of the STA stage. Left: features colored by processing status; Right: features colored by class labels. Table 6: Computational overhead comparison across different knowledge distillation methods on AV-MNIST dataset. Table 7: Performance comparison between DML-style training and joint training across four multimodal datasets. The best performance in each dataset is highlighted in bold. Method KD FitNets Peak Memory (GB) Training Time (s) 9.1 442.3 13.2 1171. RKD MGDFR C2KD MST-Distill 13.6 597.1 53.6 1942.2 7.1 310.9 7.6 491.0 Setting DML-Style Joint Training (Ours) AV-MNIST RAVDESS VGGSound-50k CMMD-V 0.5363 0.5370 0.7410 0.7521 0.5326 0.5347 0.5479 0.5481 MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation MM 25, October 2731, 2025, Dublin, Ireland Figure 11: Grad-CAM visualizations of different models on four CrisisMMD-V2 samples. Each row shows the visualizations produced by specific model, with the ground-truth label shown atop each column. Green and red borders indicate correct and incorrect predictions, respectively. MM-T and UM-S denote the multimodal teacher and unimodal student, respectively. The prefix ORG denotes models trained from scratch, while Sindicates the Specialized Teachers within the MST-Distill framework. C.5 Loading Balance Loss To validate the choice of load balancing (LB) loss in our MST-Distill framework, we compare the commonly used Coefficient of Variation (CV) loss with our KullbackLeibler divergence-based design. While CV is widely adopted in Mixture-of-Experts (MoE) models to balance expert utilization, our KL loss is formulated based on KullbackLeibler divergence to encourage the teacher routing distribution to approximate uniform distribution. As shown in Table 8, the KL-based loss yields consistently better performance, demonstrating its effectiveness in facilitating dynamic knowledge distillation. Table 8: Performance comparison of different load balancing losses across four multimodal datasets. CV and KL denote the Coefficient of Variation loss and the KullbackLeibler divergence-based loss, respectively. LB Loss CV KL (Ours) AV-MNIST RAVDESS VGGSound-50k CMMD-V2 0.5370 0. 0.7514 0.7521 0.5326 0.5347 0.5449 0.5481 C.4 Collaborative Initialization Approaches We conduct experiments on the model collaborative initialization stage using two strategies: DML-style training with gradient detachment and joint training without it. The average performance of target unimodal student models across four multimodal classification datasets is summarized in Table 7. It can be observed that modality-specific models trained via joint training achieve better performance within the MST-Distill framework. This suggests that gradient accumulation along shared optimization paths benefits subsequent knowledge transfer."
        }
    ],
    "affiliations": [
        "Xidian University Xian, China"
    ]
}