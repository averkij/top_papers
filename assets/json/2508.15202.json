{
    "paper_title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models",
    "authors": [
        "Yuanchen Zhou",
        "Shuo Jiang",
        "Jie Zhu",
        "Junhui Li",
        "Lifan Guo",
        "Feng Chen",
        "Chi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce \\textbf{Fin-PRM}, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin."
        },
        {
            "title": "Start",
            "content": "Fin-PRM: Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models Yuanchen Zhou1*, Shuo Jiang1,2*, Jie Zhu1, Junhui Li3, Lifan Guo1, Feng Chen1, Chi Zhang1 1Qwen DianJin Team, Alibaba Cloud Computing 2Osaka University 3Soochow University 5 2 0 2 1 2 ] . [ 1 2 0 2 5 1 . 8 0 5 2 : r Abstract Process Reward Models (PRMs) have emerged as promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce Fin-PRM, domainspecialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9% in supervised learning, 5.2% in reinforcement learning, and 5.1% in test-time performance. These findings highlight the value of domainspecialized reward modeling for aligning LLMs with expertlevel financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, leading to their increasing application in specialized domains such as finance(Yang, Liu, and Wang 2023; Zhu et al. 2025). However, financial reasoning tasks like financial statement analysis, investment strategy formulation, and regulatory compliance assessment demand level of precision, factuality, and logical coherence that pushes the limits of current models. *These authors contributed equally to this work and conducted the research during their internship at Qwen DianJin Team. Corresponding Author Therefore, critical research direction is to align LLM reasoning pathways with expert cognitive processes monitoring tools, such as PRMs (Lightman et al. 2023a; Zhang et al. 2025; Setlur et al. 2024). PRMs selects the best one from multiple responses, often as part of test-time scaling strategies like Best-of-N (Khalifa et al. 2025; Liu et al. 2025), and gives scalarized reward signals to reinforcement learning progress (Zou et al. 2025; Cui et al. 2025). The central challenge for the successful implementation of PRM lies in the creation and curation of high-fidelity datasets, which must feature not only verified outcomes but also granular, stepby-step demonstrations of expert financial reasoning analysis(Lightman et al. 2023b; Zhang et al. 2025). We acknowledge the foundational work of OpenThoughts (Guha et al. 2025), which established systematic methodology for reasoning data synthesis and validated its principles through comprehensive ablation studies. Following their successful data synthesis framework, we generate reasoning trajectories from faithful data source(Placing our full trust in OpenThoughts recommendation, we have moved to single data source), Cflue, knowledge-based Chinese financial benchmark. Leveraging the advanced reasoning model, Deepseek-R1 (DeepSeek-AI et al. 2025), we obtained pairs of trace and solution and constructed reasoning dataset in finance. Regarding the challenge of obtaining trustworthy reward signals, recent work has been greatly inspired by the effectiveness of LLM-as-a-Judge (Gu et al. 2025). While accept the advancement of LLM-as-a-Judge, we contend that relying solely on this method for the financial domain is often opaque and uninterpretable. We introduce knowledge verification and verifiable regularization signal to aggregate reward labels, ensuring trained PRM is knowledge aware and interpretable. To enable PRM to effectively learn from the multi-dimensional scoring signals encompassing teacher model trajectory reasoning, large model assessment, and knowledge construction, we propose novel dual-level training paradigm that simultaneously optimizes step-wise and trajectory-wise evaluation capabilities. Within this framework, we trained our model, named FinPRM, which provides reward signals at both the step and trajectory levels. Fin-PRM used newly constructed, highquality 3k financial reasoning dataset for training and show great awareness of step correctness and trajectory logic. We demonstrate the effectiveness of Fin-PRM by performing experimental validation on three methods: Offline data selection for SFT (Muennighoff et al. 2025; Xia et al. 2024), Best-of-N selection (Liu et al. 2025; Snell et al. 2024) and online reinforcement learning (Uesato et al. 2022; Cui et al. 2025). In summary, our primary contributions include: High-Quality Financial Reasoning Dataset: We constructed and curated new dataset of 3,000 samples, which provides granular, step-by-step reasoning traces with trustful reward label in the financial domain. Novel Dual-Level Training Framework: We developed training paradigm that fuses reward signals at both the step and trajectory levels, enabling the PRM to learn from multi-dimensional feedback and validate its key components through ablation studies. Comprehensive Experimental Validation: We demonstrated the effectiveness of Fin-PRM by successfully applying it to three distinct tasksoffline data selection, Best-ofN selection, and online reinforcement learningproving its capability to enhance financial reasoning models"
        },
        {
            "title": "2.1 Process Reward Models",
            "content": "Process Reward Models (PRMs) have emerged as crucial framework for providing step-level supervision and interpretable reward signals in complex reasoning tasks. Stateof-the-art PRMs, exemplified by MathShepherd (Wang et al. 2024), Skywork-PRM (He et al. 2024), and Qwen2.5-MathPRM (Zhang et al. 2025), employ human-annotated supervision with synthetic reward generation to deliver evaluation capabilities across diverse reasoning domains including mathematical problem solving, scientific analysis, and programming. Recent exploratory works such as ReasonFlux-PRM (Zou et al. 2025) combines both steplevel and template-guided trace-level reward signals, OpenPRM (Zhang et al. 2024) leverages authoritative ORMs to reverse-engineer process-level supervision signals. In application, PRMs successfully integrated into Best-of-N sampling (Liu et al. 2025), offline data selection (Xie et al. 2023), and online reinforcement learning for model optimization (Bai et al. 2022). However, effective PRM evaluation should derive its reasoning assessment capabilities from concrete thinking trajectories rather than merely final solution correctness, and real-world vertical domain applications of PRMs impose critical requirements for deep domain knowledge mastery. Guided by these thoughts, we design domain-spcialized framework that integrates trajectoryaware evaluation with expert knowledge validation, enabling more reliable process-level assessment for financal domain."
        },
        {
            "title": "2.2 Data Synthesis for Reasoning Tasks.",
            "content": "High-quality data has proven fundamental to developing effective reasoning models (Gunasekar et al. 2023). Early approaches focused on expanded existing datasets through rule-based transformations and template-driven generation Figure 1: Total process from data construction to model training. Fin-PRM used our dataset with both step-level and trajectory-level reward signals. (Wei and Zou 2019). These methods improving data coverage, but lacked the sophistication required for complex reasoning task. LLMs has enabled more advanced synthesis paradigms with distillation-based approaches leveraging powerful teacher models to generate high-quality reasoning traces for training efficient student models (Mukherjee et al. 2023). Notable contributions include WizardMath (Luo et al. 2025) synthesizes reasoning data through instruction evolution, MetaMath (Yu et al. 2024) generates diverse problems with step-by-step solutions, and OpenThoughts (Guha et al. 2025) establishes systematic methodological foundations for reasoning data synthesis through comprehensive ablation studies, providing empirical evidence for data composition principles. Considered that domain-specific applications demand specialized knowledge while maintaining reasoning quality standards, we adopt CFLUE (Zhu et al. 2024) as data source and Deepseek-R1 as teacher model to obtain reasoning trace. Recent advanced approaches employ LLM-as-a-judge for automated reward labeling, which always challenged by inherent limitations including black-box evaluation processes and insufficient reproducibility (Zheng et al. 2023). Building upon these observations, we enhance our reward annotation methodology by integrating expertbased knowledge verification mechanisms, significantly improves the observability and trustworthiness of reward signals, thereby ensuring more reliable evaluation capabilities for complex financial reasoning tasks."
        },
        {
            "title": "3 Financial Reasoning Dataset Construction\nCapability of the PRM fundamentally rely on the quality\nof its training data(Ye et al. 2025). In the financial domain,\ntraining data must not only feature correct outcomes but also\ncapture the nuances of expert-level reasoning to process-\nbased supervision(Lightman et al. 2023b). This section de-",
            "content": "tails our construction of Chain-of-thought dataset for financial reasoning."
        },
        {
            "title": "3.1 Synthesizing Reasoning Trajectories",
            "content": "We select Cflue (Zhu et al. 2024), an authoritative Chinese financial benchmark, as our basic data source. Cflue consists of wide array of complex questions, accompanied by detailed analyses written by financial experts, which serves as valuable source of ground-truth knowledge. Inspired by the systematic data synthesis framework proposed by OpenThoughts (Guha et al. 2025), we leverage Deepseek-R1 to generate structured reasoning traces. The models output is structured as pair (s, a), where: = (s1, s2, . . . , sT ) represents the reasoning trace, sequence of intermediate steps. is the final, consolidated solution derived from the trace. Each triplet (x, s, a) serves as candidate for which we will later generate reward signals."
        },
        {
            "title": "3.2 Financial Knowledge Extraction",
            "content": "Financial reasoning is intensely knowledge-driven. To mitigate reward-hacking(Khalifa et al. 2025), we extract these knowledge as new base, K, directly from the analysis of Cflue. We employ Qwen3-235b-a22b extract key financial terms and their corresponding explanations. For example, from an analysis discussing company valuation, the model might extract: Term: Price-to-Book Ratio"
        },
        {
            "title": "4.1 Overview of Fin-PRM\nFin-PRM is designed to serve as an expert evaluator for rea-\nsoning processes generated by LLMs in financial domain.\nIt assesses the quality of a model’s thinking process, rather\nthan just the final answer. To formalize this, we first define\nthe core object of evaluation.",
            "content": "Problem Formulation. The input for our reward model is triplet (x, s, a). Its scoring function, Rϕ, can be applied to evaluate either individual reasoning steps or the entire trajectory. When applied at the step level, Fin-PRM assesses the local correctness and utility of an individual reasoning step st. The model produces step-level reward conditioned on the full context: Rstep = Rϕ(st x, s<t, a) (1) where s<t is the preceding reasoning history. This score addresses questions like, Is this calculation correct? or Is this logical inference?. When applied at the trajectory level, Fin-PRM assesses the global coherence, logical flow, and strategic soundness of the entire reasoning trace s. It produces holistic score for the complete process: Rtraj = Rϕ(s x, a) (2) This score addresses higher-level questions, such as Is this the right overall approach to solve the problem?. Training Objective. The goal of training is to learn the parameters ϕ of Rϕ such that its predictions align with groundtruth reward signals derived from expert knowledge and verification. The objective is to minimize the discrepancy between the predicted and target rewards: (cid:34) (cid:88) (cid:16) t=1 Rϕ(st x, s<t, a), (cid:35) (cid:17) (3) Explanation: financial ratio used to compare companys current market price to its book value. min ϕ E(x,s,a,{r t})D This knowledge base acts as the trusted external reference for the knowledge verification module during the reward annotation process, ensuring our reward signals are factually grounded. Based on Cflue dataset, our final dataset consists of (x, s, a, K, y, yanalysis), where is gold truth answer, yanalysis is the expert analysis of the question. We treat as sliver truth because the answer teacher model gives maybe wrong."
        },
        {
            "title": "Reward Model",
            "content": "In this section, we present the detailed architecture and training methodology of Fin-PRM. Our work is positioned to address the critical need for precise, factual, and interpretable evaluation of LLM reasoning in finance. We begin with high-level overview of the Fin-PRM framework. Then we detail our method for handling step-level reward signals and trajectory-level signals. Subsequently, we describe our systematic process for reward data construction. Finally, we formulate the training objective that integrates these components. where is our training dataset, ward, and is loss function. is the ground-truth re-"
        },
        {
            "title": "4.2 Step-level Reward Modeling\nTo capture the multifaceted nature of a good reasoning step,\nwe decompose our step-level reward into three distinct com-\nponents: Monte Carlo estimation score rimportance , LLM-\nas-a-judge score rqual , and an accuracy score racc that veri-\nfies its factual correctness.",
            "content": "rimportance quantifies the Importance Score rimportance: utility of step by evaluating its likelihood of being on correct reasoning path. For each step st in trace, we prompt Qwen2.5-7b-math to generate (in our case, = 8) continuous rollouts until final answer is reached. rimportance defined as the proportion of these rollouts that yield correct final answer. This provides soft-label score reflecting the potential of the current step, if soft-label is not 0, hard-label defined as 1. rimportance = 1 (cid:88) i=1 I(ξ(Rµ,i(st x, s<t, y))) (4) where Rµ,i(st) is the i-th generated completion starting from step st, ξ is the answer check progress and I() is the indicator function, which returns 1 if the final answer of the rollout is correct and 0 otherwise. Qualitative Score rqual: rqual captures the abstract quality of reasoning step. We leverage powerful LLM, Qwen3-235b-a22b(we also considered chat-gpt-4.1, but observed almost the same score as Qwen3 gives), to evaluate each step st from semantic coherence, logic soundness, and answer orientation, details of prompt can be found in appendix. rqual = Rθ(st x, s<t, a) (5) where the score is prompt to be scalar between 0 and 1. Against to prior works like Openthoughts and ReasonfluxPRM treat as golden truth, we consider that reasoning data constructed for SFT is not suitable for PRM training. Accuracy Score racc: racc provides robust, quantitative measure of steps factual and procedural correctness. It is organized into two parts as following exaplained, specifically designed to anchor the reward signal in ground truth and knowledge base K, aims to mitigate issues like LLM hallucination and reward hacking: Procedural Correctness (rstep correctness): This subscore assesses the procedural validity of given step st. We employ powerful LLM as verifier, prompting it to make binary assertion (1 for correct, 0 for incorrect) on whether the step st constitutes logically sound and relevant action towards reaching the known gold truth, y. Here, the difference between rstep correctness and rqual is that they use different base as their ground truth. Factual Accuracy (rknowledge acc): This sub-score measures the factual accuracy of the content within st. It systematically validates all identifiable claims and financial terms within the step against our knowledge base . This directly counteracts model hallucination by ensuring that the reasoning is built upon verified facts from the trusted expert analysis, yanalysis. The final accuracy score combines these two facets in weighted sum: racc = 0.5(rstep correctness(st, y)+ωkrknowledge acc(st, Kx)) (6) Here, the hyperparameter ωk allows us to adjust the relative importance of factual grounding versus procedural correctness. higher ωk would more heavily penalize factual inaccuracies, aligning the model with stricter standard of verifiability. In our experiments, we set ωk = 1.0, treating both types of correctness as equally critical. This composite score thereby ensures that highly-rated steps are both logically sound and factually impeccable."
        },
        {
            "title": "4.3 Trajectory-level Reward Modeling\nNotice that a trajectory consists of correct steps sometimes\nlead to wrong answer, and PRMs can easily fall into reward\nhacking. We introduce trajectory-level reward signal com-\nbines two parts: an outcome-based correctness score rout\nand a knowledge coverage score rcover.",
            "content": "Outcome correctness score rout . rout provides an assessment of the final answers correctness. For the tasks in our dataset typically require selecting final option (e.g., A, B, ACD), we extra models chosen option compared directly to the ground-truth correct option, yields strict binary signal, rout {0, 1}. Knowledge coverage score rcover . high-quality reasoning process should be comprehensive and well-supported by relevant domain knowledge. rcover measures the extent to which the reasoning trace and the final answer utilize the necessary knowledge terms, calculated as the ratio of relevant knowledge concepts mentioned in generation to the total number of concepts required. rcover = ϕext(s a) Kx Kx (7) Here, Kx is the subset of our knowledge base containing all terms deemed relevant to the prompt x. The function ϕext() represents the extraction process, implemented through LLMs. denotes string concatenation."
        },
        {
            "title": "4.4 Reward Data Construction\nWe construct ground-truth labels by aggregating the multi-\nple signals into a single score for each granularity, which is\nthen binarized.",
            "content": "Step-level Label Construction. To form single supervisory signal for each step st, we aggregate its three distinct reward componentsimportance, quality, and accuracyusing dynamic weighting scheme. This approach, based on the softmax function, adaptively emphasizes the score providing the strongest signal. The final continuous score for step t, denoted rstep , is calculated as: rstep = (cid:88) softmax(cid:0)rimp , rqual , racc (cid:1) rk k{imp,qual,acc} (8) In this formulation, rk is the raw score for component at step t. The softmax function converts the vector of raw scores into probability distribution, which serves as set of dynamic weights. The k-th element of this distribution, indicated by the subscript ()k, is then multiplied by its corresponding raw score rk . This method functions as an attention mechanism: score that is significantly higher than the others will receive proportionally larger weight in the final sum, allowing its signal to dominate. For instance, step with exceptionally high factual accuracy (racc ) will have its contribution amplified, even if other scores are moderate. This is more robust than fixed-weight average. Finally, this aggregated score rstep is binarized using 0.5 threshold to produce the final step label, Lstep . Trajectory-level Label Construction. For each trajectory, we combine its outcome and coverage scores into single score, Straj : Rtraj(s, a) = rout(a) + η rcover(s, a) Here, η are the weights for outcome correctness and knowledge coverage. We set η to 1.5. This weighting ensures that (9) the knowledge coverage score has meaningful impact on the final label. The trajectory score is also converted to binary label, Ltraj, using 1.25 threshold, we select these weights to balance the contribution of these two signals, by using the mean value of their weights as the threshold value, we give the ability to change the hard label to each reward signal."
        },
        {
            "title": "4.5 Training Objective\nTo train Fin-PRM effectively, we formulate a joint objective\nto train model through binary cross-entropy (BCE), learning\nto predict the correctness of both individual steps and entire\ntrajectories. The total loss Ltotal:",
            "content": "Ltotal = Lstep + λ Ltraj (10) where λ are hyperparameters that balance the contribution of each supervision signal. The step-level loss, Lstep, is the average loss over all steps in reasoning trace. It measures the discrepancy between the models prediction and the ground-truth step label, Lstep(st): Lstep = 1 T (cid:88) t=1 (cid:16) LBCE Rϕ(st x, s<t, a), Rstep (cid:17) (11) The trajectory-level loss, Ltraj, follows the same principle. It compares the models prediction for the entire trajectory against the ground-truth trajectory label, Ltraj(s, a): Ltraj = LBCE (cid:16) Rϕ(s, x), Rtraj (cid:17) (12) where σ() denotes the sigmoid function, which converts the models raw logit outputs into probabilities. LBCE(, ) denotes the BCE loss function. For ground-truth label {0, 1} and model logit output Rϕ, it is defined as LBCE(Rϕ, L) = [L log σ(Rϕ) + (1 L) log(1 σ(Rϕ))], By jointly optimizing this objective, Fin-PRM is trained to make judgments."
        },
        {
            "title": "5.1 Supervised Fine-tuning with Data Selection\nPRM can be used to identify and select only the most co-\nherent and correct examples from the whole dataset accord-\ning to the score it gives. Through comparing the capability\nof student model trained by different selected dataset exam-\nples, advancement of Fin-PRM be proved.",
            "content": "Figure 2: BoN test on Cflue dataset. Fin-PRM is the black line, outperforms all baselines in financial domain. Qwen2.5-Math-PRM-7B shows performance below majorvoting when the number of becomes larger, points crucial needs for domain-specific PRM. The figure below shows that Fin-PRM performs almost identically to proprietary mathematical PRM of the same size in the field of mathematics. We use Qwen3-8B to produce multiple distinct reasoning trajectories for each question, scoring each trajectory through Fin-PRM by the following function: ˆR = 1 (cid:88) t= ˆRstep(st x, s<t, a) + ζ ˆRtraj(s, x) (13) Here, ˆRstep and ˆRtraj are the reward scores generated by Fin-PRM or other PRMs. The hyperparameter ζ balances the contribution of fine-grained step correctness and overall trajectory quality; we found ζ = 1.0 to work well in practice. As shown in Table 1, the selection strategy has profound impact on model performance. Fine-tuning on randomly selected data degrades performance to 43.8%, highlighting the risk of using noisy synthetic data. In contrast, all PRMbased selection methods provide substantial boost over the 45.3% base model accuracy. Our domain-specialized FinPRM achieves the highest accuracy of 58.2%. This result not only demonstrates 12.9-point improvement over the base model but also confirms that finance-aware reward model is superior for curating high-quality reasoning data in this domain."
        },
        {
            "title": "5.2 Test-Time Scaling\nAnother key application is to enhance model performance\nat inference time through Best-of-N (BoN) selection. This\nmethod computes the score from Equation 13 for N candi-",
            "content": "Method Qwen2.5-7b-instruct Random Selection Math-PRM-7B Math-PRM-72B Fin-PRM (Ours) Accuracy (%) 45.3 43.8 56. 57.1 58.2 Table 1: Offline data selection comparison on the CFLUE benchmark. All SFT methods use 1,000 selected samples to fine-tune the Qwen2.5-7B-Instruct base model. The highest performance is in bold. date answers generated by policy model to find the best one. We evaluate performance in our target financial domain. Using Qwen2.5-7B-Instruct as the generator, we perform BoN selection on 1,000-sample subset of the CFLUE test set for values of 4, 8, and 16. Fin-PRM is compared against two baselines: strong, general-domain Qwen2.5Math-PRM-7B and major voting method. As presented in Figure 2, in the Cflue BoN test, Fin-PRM consistently leads to greater accuracy gains as increases, outperforming the majority-voting baseline by more than 5.1% at N=16. This result highlights its effectiveness for financial reasoning. To validate the generalization of Fin-PRMs, we conduct the same BoN experiment on an out-of-domain benchmark, Math500. Method Pass@N (Oracle) Majority Voting Fin-PRM (Ours) Qwen2.5-Math-PRM-7B Qwen2.5-Math-PRM-72B N=1 N=2 N=4 N=8 N=16 60.0 60.0 60.0 60.0 60. 63.7 56.6 60.5 60.4 62.5 66.1 61.0 61.6 62.0 63.1 67.1 63.0 63.1 63.2 65. 68.4 63.3 63.0 63.8 65.8 Table 2: Best-of-N performance on the out-of-domain Math500 benchmark. Fin-PRM shows better performance than major voting. Best selector performance is in bold. The results on Math500, shown in Table 2, Fin-PRM demonstrates respectable baseline capability, proving it does not completely fail on unfamiliar tasks. Its performance closely tracks that of the 7B-scale math-specialized PRM and remains competitive with majority voting. This indicates that while its expert knowledge is sharply honed for finance, Fin-PRM retains foundational ability to assess logical structure, allowing it to function as competent generalist evaluator in other domains and demonstrating robust generalization."
        },
        {
            "title": "5.3 Reward signals for RL training",
            "content": "Beyond offline data curation and test-time selection, FinPRMs most dynamic application is providing composite reward that guides policy optimization through step-aware supervision. We integrate Fin-PRM into the Group Relative Policy Optimization (Shao et al. 2024) (GRPO) framework. By default, GRPO optimizes for the outcome-level reward, which in our case is rout. To incorporate the nuanced, process-level supervision from Fin-PRM, we augment this reward with our holistic score ˆR (from Equation 13). The new composite Figure 3: Performance of GRPO policy optimization using different reward signals. The policy model is Qwen2.5-7BInstruct. We report mean accuracy over multiple runs. reward for given trace (s, a) is defined as: rrl = (1 δ) rout + δ ˆR where the hyperparameter δ controls the relative weight of the process-level reward. For group of responses, the advantage is: (14) Arl = rrl mean({rrl}N std({rrl}N j=1) j=1) (15) With the Fin-PRM derived advantage term, Acomp, the GRPO objective is updated to: JGRPO(θ) = Ex,{si}πθold (cid:110) πθ(si,tx,si,<t) (cid:34) 1 (cid:80)N i=1 1 Ti (cid:80)Ti t= (cid:16) min πθold (si,tx,si,<t) Arl,i, πθold (si,tx,si,<t) , 1 ϵ, 1 + ϵ(cid:1)Arl,i clip(cid:0) πθ(si,tx,si,<t) (cid:35) (cid:17) βKLDKL(πθπref ) (cid:111) (16) where Arl,i is the advantage for the i-th sample, ϵ is the clipping hyperparameter, and the term weighted by βKL is KL-divergence penalty against reference policy πref . Figure 3 presents the downstream reasoning performance after using different reward signals for GRPO policy optimization. We use Qwen2.5-7B-Instruct as the policy model and compare three reward sources: rule-based signal using only rout, the general-domain Qwen2.5-Math-PRM-7B, and our Fin-PRM. Across all evaluations, using Fin-PRM as the reward source consistently yields the best-performing policy. Integrating Fin-PRM boosts performance on CFLUE to 70.5% When ζ = 0, the ranking relies solely on step-level rewards. While this performs reasonably well, it is consistently suboptimal, indicating that step-level correctness alone is insufficient. Performance peaks at ζ 1.0, where step-level and trajectory-level rewards are given roughly equal importance. This suggests that the most effective evaluation considers both the granular correctness of individual steps and the holistic quality of the entire reasoning path. As ζ becomes very large, performance degrades. This implies that over-relying on the trajectory score while ignoring step-level details leads to poorer selection, likely because the model may select trajectories that seem plausible overall but contain critical local flaws. These results strongly validate our dual-granularity reward design, demonstrating that balanced integration of both local and global signals is essential for accurately identifying superior reasoning processes. More ablation study about each parameters we used in paper can be found in appendix."
        },
        {
            "title": "7 Discussion\nOur experimental results across SFT, Best-of-N, and RL ap-\nplications consistently demonstrate that Fin-PRM outper-\nforms general-purpose baselines. This success validates our\ncentral thesis: for high-stakes domains like finance, effec-\ntive process supervision requires a reward model that is not\njust logically coherent but deeply specialized and factually\ngrounded. The key to Fin-PRM’s performance is its dual-\nlevel, knowledge-aware architecture. By integrating verifi-\nable reward components (racc and rcover) grounded in an\nexpert-derived knowledge base, Fin-PRM moves beyond as-\nsessing mere logical plausibility to penalizing factual hal-\nlucinations. This confirms that for domains where truth is\nnon-negotiable, a hybrid approach combining LLM-based\nqualitative assessment with explicit knowledge verification\nis critical.",
            "content": "While this framework provides robust proof-of-concept, we acknowledge several limitations that open important avenues for future research: the construction of our 3k-sample dataset, while high-quality, was resource-intensive. To make domain-specific PRMs more accessible, future work should explore efficient, semi-automated methods for generating and annotating such specialized data at scale. Meanwhile, our knowledge base is static, which poses risk in dynamic field like finance where regulations and market conditions evolve. Integrating dynamic knowledge sources, such as real-time financial news feeds or regulatory update APIs, would be critical enhancement to prevent knowledge decay and ensure long-term reliability. We select the hyperparameters in our work through ablation study, which were set fixed. more advanced approach could involve learning these weights dynamically, perhaps through meta-learning framework. This would allow the model to adapt the reward composition to the specific demands of each problem, potentially yielding further performance gains. Addressing these challenges is key to moving from specialized model to an adaptive, continuously improving financial reasoning expert. Figure 4: Ablation study on the ranking score weight ζ. The plot shows Best-of-N accuracy on the CFLUE test set as function of ζ for different numbers of candidates (N). FinPRMs best performance appears in ζ = 1 and on FinQA to 62.8%, significant gain of 3.3 points on both benchmarks compared to the rule-based heuristic. Crucially, Fin-PRM also outperforms the strong Qwen2.5Math-PRM-7B baseline, highlighting that the domainspecific, factually-grounded process supervision it provides is more effective for guiding RL in specialized domain than general-purpose reward model. These results demonstrate that the high-quality learned reward signals from FinPRM substantially enhance policy optimization, leading to more capable financial reasoning models."
        },
        {
            "title": "6.2 Experimental Setup.",
            "content": "We perform BoN selection on our 1,000-sample CFLUE test set, using the same fine-tuned Qwen2.5-7B-Instruct model as the generator. For values of 2, 4, 8, and 16, we vary ζ across the range [0.0, 2.0] and plot the resulting accuracy."
        },
        {
            "title": "6.3 Results and Analysis.",
            "content": "As shown in Figure 4, the models performance is sensitive to the value of ζ. For larger (specifically N=8 and N=16), we observe clear and consistent trend: accuracy rises as ζ increases from 0, reaches peak near ζ = 1.0, and then gradually declines. This pattern reveals several key insights:"
        },
        {
            "title": "8 Conclusion\nIn this work, we introduced Fin-PRM, demonstrating that\na domain-specialized, knowledge-aware process reward\nmodel significantly enhances financial reasoning. We be-\nlieve this framework serves as a blueprint for developing\ntrustworthy AI evaluators in other high-stakes fields like law\nand medicine, promoting a shift towards a portfolio of spe-\ncialized, reliable models. Our work provides a solid foun-\ndation for building the next generation of LLMs capable\nof expert-level reasoning in specialized, real-world applica-\ntions.",
            "content": "References Bai, Y.; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; DasSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.; Joseph, N.; Kadavath, S.; Kernion, J.; Conerly, T.; ElShowk, S.; Elhage, N.; Hatfield-Dodds, Z.; Hernandez, D.; Hume, T.; Johnston, S.; Kravec, S.; Lovitt, L.; Nanda, N.; Olsson, C.; Amodei, D.; Brown, T.; Clark, J.; McCandlish, S.; Olah, C.; Mann, B.; and Kaplan, J. 2022. Training Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. arXiv:2204.05862. Cui, G.; Yuan, L.; Wang, Z.; Wang, H.; Li, W.; He, B.; Fan, Y.; Yu, T.; Xu, Q.; Chen, W.; Yuan, J.; Chen, H.; Zhang, K.; Lv, X.; Wang, S.; Yao, Y.; Han, X.; Peng, H.; Cheng, Y.; Liu, Z.; Sun, M.; Zhou, B.; and Ding, N. 2025. Process Reinforcement through Implicit Rewards. arXiv:2502.01456. DeepSeek-AI; Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; Zhang, X.; Yu, X.; Wu, Y.; Wu, Z. F.; Gou, Z.; Shao, Z.; Li, Z.; Gao, Z.; Liu, A.; Xue, B.; Wang, B.; Wu, B.; Feng, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; Dai, D.; Chen, D.; Ji, D.; Li, E.; Lin, F.; Dai, F.; Luo, F.; Hao, G.; Chen, G.; Li, G.; Zhang, H.; Bao, H.; Xu, H.; Wang, H.; Ding, H.; Xin, H.; Gao, H.; Qu, H.; Li, H.; Guo, J.; Li, J.; Wang, J.; Chen, J.; Yuan, J.; Qiu, J.; Li, J.; Cai, J. L.; Ni, J.; Liang, J.; Chen, J.; Dong, K.; Hu, K.; Gao, K.; Guan, K.; Huang, K.; Yu, K.; Wang, L.; Zhang, L.; Zhao, L.; Wang, L.; Zhang, L.; Xu, L.; Xia, L.; Zhang, M.; Zhang, M.; Tang, M.; Li, M.; Wang, M.; Li, M.; Tian, N.; Huang, P.; Zhang, P.; Wang, Q.; Chen, Q.; Du, Q.; Ge, R.; Zhang, R.; Pan, R.; Wang, R.; Chen, R. J.; Jin, R. L.; Chen, R.; Lu, S.; Zhou, S.; Chen, S.; Ye, S.; Wang, S.; Yu, S.; Zhou, S.; Pan, S.; Li, S. S.; Zhou, S.; Wu, S.; Ye, S.; Yun, T.; Pei, T.; Sun, T.; Wang, T.; Zeng, W.; Zhao, W.; Liu, W.; Liang, W.; Gao, W.; Yu, W.; Zhang, W.; Xiao, W. L.; An, W.; Liu, X.; Wang, X.; Chen, X.; Nie, X.; Cheng, X.; Liu, X.; Xie, X.; Liu, X.; Yang, X.; Li, X.; Su, X.; Lin, X.; Li, X. Q.; Jin, X.; Shen, X.; Chen, X.; Sun, X.; Wang, X.; Song, X.; Zhou, X.; Wang, X.; Shan, X.; Li, Y. K.; Wang, Y. Q.; Wei, Y. X.; Zhang, Y.; Xu, Y.; Li, Y.; Zhao, Y.; Sun, Y.; Wang, Y.; Yu, Y.; Zhang, Y.; Shi, Y.; Xiong, Y.; He, Y.; Piao, Y.; Wang, Y.; Tan, Y.; Ma, Y.; Liu, Y.; Guo, Y.; Ou, Y.; Wang, Y.; Gong, Y.; Zou, Y.; He, Y.; Xiong, Y.; Luo, Y.; You, Y.; Liu, Y.; Zhou, Y.; Zhu, Y. X.; Xu, Y.; Huang, Y.; Li, Y.; Zheng, Y.; Zhu, Y.; Ma, Y.; Tang, Y.; Zha, Y.; Yan, Y.; Ren, Z. Z.; Ren, Z.; Sha, Z.; Fu, Z.; Xu, Z.; Xie, Z.; Zhang, Z.; Hao, Z.; Ma, Z.; Yan, Z.; Wu, Z.; Gu, Z.; Zhu, Z.; Liu, Z.; Li, Z.; Xie, Z.; Song, Z.; Pan, Z.; Huang, Z.; Xu, Z.; Zhang, Z.; and Zhang, Z. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948. Gu, J.; Jiang, X.; Shi, Z.; Tan, H.; Zhai, X.; Xu, C.; Li, W.; Shen, Y.; Ma, S.; Liu, H.; Wang, S.; Zhang, K.; Wang, Y.; Gao, W.; Ni, L.; and Guo, J. 2025. Survey on LLM-as-aJudge. arXiv:2411.15594. Guha, E.; Marten, R.; Keh, S.; Raoof, N.; Smyrnis, G.; Bansal, H.; Nezhurina, M.; Mercat, J.; Vu, T.; Sprague, Z.; Suvarna, A.; Feuer, B.; Chen, L.; Khan, Z.; Frankel, E.; Grover, S.; Choi, C.; Muennighoff, N.; Su, S.; Zhao, W.; Yang, J.; Pimpalgaonkar, S.; Sharma, K.; Ji, C. C.-J.; Deng, Y.; Pratt, S.; Ramanujan, V.; Saad-Falcon, J.; Li, J.; Dave, A.; Albalak, A.; Arora, K.; Wulfe, B.; Hegde, C.; Durrett, G.; Oh, S.; Bansal, M.; Gabriel, S.; Grover, A.; Chang, K.- W.; Shankar, V.; Gokaslan, A.; Merrill, M. A.; Hashimoto, T.; Choi, Y.; Jitsev, J.; Heckel, R.; Sathiamoorthy, M.; Dimakis, A. G.; and Schmidt, L. 2025. OpenThoughts: Data Recipes for Reasoning Models. arXiv:2506.04178. Gunasekar, S.; Zhang, Y.; Aneja, J.; Mendes, C. C. T.; Giorno, A. D.; Gopi, S.; Javaheripi, M.; Kauffmann, P.; de Rosa, G.; Saarikivi, O.; Salim, A.; Shah, S.; Behl, H. S.; Wang, X.; Bubeck, S.; Eldan, R.; Kalai, A. T.; Lee, Y. T.; and Li, Y. 2023. Textbooks Are All You Need. arXiv:2306.11644. He, J.; Wei, T.; Yan, R.; Liu, J.; Wang, C.; Gan, Y.; Tu, S.; Liu, C. Y.; Zeng, L.; Wang, X.; Wang, B.; Li, Y.; Zhang, F.; Xu, J.; An, B.; Liu, Y.; and Zhou, Y. 2024. Skywork-o1 Open Series. https://huggingface.co/Skywork. Khalifa, M.; Agarwal, R.; Logeswaran, L.; Kim, J.; Peng, H.; Lee, M.; Lee, H.; and Wang, L. 2025. Process Reward Models That Think. arXiv:2504.16828. Lightman, H.; Kosaraju, V.; Burda, Y.; Edwards, H.; Baker, B.; Lee, T.; Leike, J.; Schulman, J.; Sutskever, I.; and Cobbe, K. 2023a. Lets Verify Step by Step. arXiv:2305.20050. Lightman, H.; Kosaraju, V.; Burda, Y.; Edwards, H.; Baker, B.; Lee, T.; Leike, J.; Schulman, J.; Sutskever, I.; and Cobbe, K. 2023b. Lets Verify Step by Step. arXiv:2305.20050. Liu, R.; Gao, J.; Zhao, J.; Zhang, K.; Li, X.; Qi, B.; Ouyang, W.; and Zhou, B. 2025. Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling. arXiv:2502.06703. Luo, H.; Sun, Q.; Xu, C.; Zhao, P.; Lou, J.; Tao, C.; Geng, X.; Lin, Q.; Chen, S.; Tang, Y.; and Zhang, D. 2025. WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. arXiv:2308.09583. Muennighoff, N.; Yang, Z.; Shi, W.; Li, X. L.; Fei-Fei, L.; Hajishirzi, H.; Zettlemoyer, L.; Liang, P.; Cand`es, E.; and Hashimoto, T. 2025. s1: Simple test-time scaling. arXiv:2501.19393. Mukherjee, S.; Mitra, A.; Jawahar, G.; Agarwal, S.; Palangi, H.; and Awadallah, A. 2023. Orca: Progressive Learning from Complex Explanation Traces of GPT-4. arXiv:2306.02707. Zhu, J.; Li, J.; Wen, Y.; and Guo, L. 2024. Benchmarking Large Language Models on CFLUE - Chinese Financial Language Understanding Evaluation Dataset. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Findings of the Association for Computational Linguistics: ACL 2024, 56735693. Bangkok, Thailand: Association for Computational Linguistics. Zou, J.; Yang, L.; Gu, J.; Qiu, J.; Shen, K.; He, J.; and Wang, M. 2025. ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs. arXiv:2506.18896. Setlur, A.; Nagpal, C.; Fisch, A.; Geng, X.; Eisenstein, J.; Agarwal, R.; Agarwal, A.; Berant, J.; and Kumar, A. 2024. Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning. arXiv:2410.08146. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y. K.; Wu, Y.; and Guo, D. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300. Snell, C.; Lee, J.; Xu, K.; and Kumar, A. 2024. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. arXiv:2408.03314. Uesato, J.; Kushman, N.; Kumar, R.; Song, F.; Siegel, N.; Wang, L.; Creswell, A.; Irving, G.; and Higgins, I. 2022. Solving math word problems with processand outcomebased feedback. arXiv:2211.14275. Wang, P.; Li, L.; Shao, Z.; Xu, R. X.; Dai, D.; Li, Y.; Chen, D.; Wu, Y.; and Sui, Z. 2024. Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. arXiv:2312.08935. Wei, J.; and Zou, K. 2019. EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. arXiv:1901.11196. Xia, M.; Malladi, S.; Gururangan, S.; Arora, S.; and Chen, D. 2024. LESS: Selecting Influential Data for Targeted Instruction Tuning. arXiv:2402.04333. Xie, S. M.; Pham, H.; Dong, X.; Du, N.; Liu, H.; Lu, Y.; Liang, P.; Le, Q. V.; Ma, T.; and Yu, A. W. 2023. DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. arXiv:2305.10429. Yang, H.; Liu, X.-Y.; and Wang, C. D. 2023. FinGPT: Open-Source Financial Large Language Models. arXiv:2306.06031. Ye, Y.; Huang, Z.; Xiao, Y.; Chern, E.; Xia, S.; and Liu, P. 2025. LIMO: Less is More for Reasoning. arXiv:2502.03387. Yu, L.; Jiang, W.; Shi, H.; Yu, J.; Liu, Z.; Zhang, Y.; Kwok, J. T.; Li, Z.; Weller, A.; and Liu, W. 2024. MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. arXiv:2309.12284. Zhang, K.; Zhang, J.; Li, H.; Zhu, X.; Hua, E.; Lv, X.; Ding, N.; Qi, B.; and Zhou, B. 2024. OpenPRM: Building Open-domain Process-based Reward Models with Preference Trees. ICLR 2024. Zhang, Z.; Zheng, C.; Wu, Y.; Zhang, B.; Lin, R.; Yu, B.; Liu, D.; Zhou, J.; and Lin, J. 2025. The Lessons of Developing Process Reward Models in Mathematical Reasoning. arXiv:2501.07301. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; JudgZhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. ing LLM-as-a-Judge with MT-Bench and Chatbot Arena. arXiv:2306.05685. Zhu, J.; Chen, Q.; Dou, H.; Li, J.; Guo, L.; Chen, F.; and Zhang, C. 2025. DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models. arXiv:2504.15716."
        },
        {
            "title": "Table of Contents",
            "content": "A. Details on Models and Datasets A.1 Model Implementations A.2 Dataset Details B. Details on Reward Signal Construction B.1 Prompt for Qualitative Score (rqual) B.2 Prompt for Knowledge Extraction C. Additional Experimental Setups C.1 Fin-PRM Training C.2 Downstream Tasks (SFT, BoN, GRPO) D. Case Study: Fin-PRM in Action A. Details on Models and Datasets B.1 A.1 Model Implementations Generator Models. Throughout our experiments, we utilize several powerful large language models as generators for reasoning traces and candidate solutions: Deepseek-R1: highly capable reasoning model used in our initial data synthesis phase to generate the base reasoning trajectories from the Cflue dataset. Qwen3-235b-a22b: state-of-the-art model used for two critical auxiliary tasks: (1) extracting the structured financial knowledge base from expert analyses, and (2) serving as the powerful LLM-as-a-judge to provide the qualitative score, rqual. Qwen2.5-7B-Instruct: versatile and efficient model used as the base for our Fin-PRM, as well as the student model in our SFT experiments and the policy model in our GRPO and BoN experiments. Reward Models (Baselines). To benchmark the performance of Fin-PRM, we compare it against strong, publicly available, general-purpose PRM: Qwen2.5-Math-PRM-7B: state-of-the-art PRM specialized for the mathematics domain. It is trained on vast corpus of math reasoning problems and serves as powerful baseline to highlight the benefits of domain specialization. Its strong performance in technical domain makes it challenging benchmark for our financespecific model. Qwen2.5-Math-PRM-72B: state-of-the-art PRM specialized for the mathematics domain. The larger 72billion parameter version, included to establish strong upper-bound for general-purpose PRM performance on our tasks. B.2 A.2 Dataset Details Primary Data Source. Our entire framework is built upon the CFLUE (Chinese Financial Language Understanding Evaluation) benchmark. We selected CFLUE because it is high-quality, knowledge-intensive dataset where questions are accompanied by detailed, human-written expert analyses. This unique feature provides the ground-truth knowledge necessary for our fact-checking reward components (racc). Synthesized Reasoning Dataset. We constructed new dataset of 3,000 samples for training Fin-PRM. Each sample in our dataset, D, is tuple (x, s, a, Kx, y, yanalysis) containing: x: The original question from CFLUE. s: The reasoning trace generated by Deepseek-R1. a: The final answer synthesized by Deepseek-R1. Kx: The subset of our global knowledge base relevant to question x. y: The ground-truth correct answer from CFLUE. yanalysis: The ground-truth expert analysis from CFLUE, used to construct K. This structure enables our multi-faceted reward signal construction and provides robust foundation for training knowledge-aware reward model. Prompt for reasoning trace and solution To synthesize the reasoning traces (s) and their corresponding solutions (a), we prompt the Deepseek-R1 model with detailed set of instructions. This prompt is designed to elicit long-form, step-by-step thought process, followed by clean, final answer. The full prompt is shown in Listing 1. The [Question Text] placeholder is then replaced with the actual question from the CFLUE dataset. Prompt for Knowledge Extraction To construct the ground-truth knowledge base (Kx) for each question, we prompt the LLM judge to act as domain expert. Its task is to read the trusted expert analysis (yanalysis) provided in the dataset and extract all key financial terms along with their definitions as presented in the text. This process creates structured, verifiable source of facts for the downstream accuracy and coverage rewards. The prompt template is detailed in Listing 2. B. Details on Reward Signal Construction This section provides further details on the prompts used to generate the multi-faceted reward signals described in the main paper. These prompts are designed to elicit structured, machine-readable outputs from powerful LLM judge (Qwen3-235b-a22b). C.1 B.1 Prompt for Qualitative Score (rqual) To assess the intrinsic quality of each reasoning step, we use structured prompt that asks the LLM judge to evaluate step (st) based on three criteria. The prompt takes the original question (x), the reasoning history (s<t), and the current step (st) as input. The LLM is instructed to provide score from 0.0 to 1.0 for each of the following aspects, ensuring the output is machine-parsable JSON object: Logical Soundness: How coherent and logically valid is the reasoning within this specific step? Listing 1: Prompt for Generating Reasoning Traces and Solutions from Deepseek-R1 1 Your role as an assistant involves thoroughly exploring questions through systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection , backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. 2 3 In the Thought section, detail your reasoning process using the specified format: 4 <begin_of_thought> 5 {thought with steps separated with nn } 6 <end_of_thought> 7 Each step should include detailed considerations such as analyzing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. 8 9 In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should maintain logical, accurate, concise expression style and detail necessary steps needed to reach the conclusion, formatted as follows: 10 <begin_of_solution> 11 {final formatted, precise, and clear solution} 12 <end_of_solution> 13 14 Now, try to solve the following question through the above guidelines: 15 [Question Text] Step Correctness: Is the information presented in the step factually or procedurally correct, independent of the overall strategy? Target Progression: How effectively does this step move the overall reasoning process closer to correct final answer? The template for this prompt is shown in Listing 3. Listing 2: Template for the Knowledge Extraction Prompt 1 You are financial knowledge extraction expert. Read the following expert analysis and identify all key financial terms and concepts. For each term, provide concise definition based on the text. 2 3 **Expert Analysis Text:** 4 [Expert Analysis from the Dataset] 5 6 --- 7 **Your Task:** 8 Output JSON list where each object represents key knowledge point. 9 10 **Output Format (JSON list only):** 11 [ 12 13 14 \"Term\": \"<Identified_Term_1>\", \"Explanation\": \"< { 15 16 17 18 Definition_of_Term_1>\" }, { \"Term\": \"<Identified_Term_2>\", \"Explanation\": \"< Definition_of_Term_2>\" } 19 20 ] C.2 B.2 Prompts for Verifiable and Knowledge-Based Rewards This subsection details the prompts used to generate rewards that are grounded in external, verifiable information, such as the ground-truth answer or our extracted knowledge base. These prompts are crucial for ensuring the factual correctness and anti-hallucination properties of Fin-PRM. Prompt for Accuracy Score (racc) The accuracy score is composite of two verifiable checks. We use distinct prompts for each to ensure grounded, factual evaluation. Procedural Correctness Prompt. This prompt verifies if step is valid move towards the known correct answer (y), assessing its logical utility in the context of correct solution path. Factual Accuracy Prompt. This prompt validates the claims within step against the extracted knowledge base (Kx), acting as direct anti-hallucination check. Prompt for Knowledge Coverage (rcover) To calculate the trajectory-level knowledge coverage score, this prompt asks the LLM to verify which of the required knowledge points (Kx) were used in the models full generated response (s and a). C. Additional Experimental Setups This section provides detailed configurations and hyperparameters for the training of Fin-PRM and its application in the three downstream tasks. Listing 3: Template for the Qualitative Score Prompt 1 You are an expert financial analyst. Given the question, the previous reasoning steps, and the current step , evaluate the quality of the ** current step**. Listing 4: Template for Procedural Correctness Prompt 1 You are logical verifier. Given the reasoning so far and the known correct answer, determine if the current step is logically sound and productive move towards that answer. 2 3 **Question:** 4 [Original Question Text] 5 6 **Reasoning History:** 7 [Reasoning History up to step t-1] 8 9 **Current Step to Evaluate:** 10 [Text of step t] 11 12 --- 13 **Your Task:** 14 Provide JSON object with your 15 1. 16 2. 17 3. evaluation based on three criteria: logical_soundness: How logical is the current step? step_correctness: Is the information in the step correct? target_progression: Does the step help solve the problem? 18 19 **Output Format (JSON only):** 20 { \"logical_soundness\": < float_from_0_to_1>, \"step_correctness\": <float_from_0_to_1 >, \"target_progression\": < float_from_0_to_1> 23 24 } D.1 C.1 Fin-PRM Training Details Fin-PRM was trained by fine-tuning the Qwen2.5-7BInstruct model on our newly constructed financial reasoning dataset. The training objective combined step-level and trajectory-level losses, as described in Equation 16. The key hyperparameters used for training are summarized in Table 3. All training was conducted on NVIDIA A100 GPUs. Table 3: Hyperparameters for Fin-PRM Training. Parameter Base Model Dataset Size Learning Rate Batch Size Gradient Accumulation Steps Max Sequence Length Epochs Optimizer LR Scheduler Warmup Steps Loss Weight (λ in Eq. 16) Value Qwen2.5-7B-Instruct 3,000 samples 2e-5 1(per device) 2 8192 3 AdamW Cosine with warmup 50 1.0 2 3 **Question:** 4 [Original Question Text] 5 6 **Reasoning History:** 7 [Reasoning History up to step t-1] 8 9 **Correct Final Answer:** 10 [Ground Truth Answer y] 11 12 **Current Step to Evaluate:** 13 [Text of step t] 14 15 --- 16 **Your Task:** 17 Is this step valid, logical progression towards the correct final answer? Respond with JSON object containing binary value. 18 19 **Output Format (JSON only):** 20 { \"procedural_correctness\": < _for_yes_or_0_for_no> } Listing 5: Template for Factual Accuracy Prompt 1 You are fact-checking agent. Verify every factual claim and financial term in the \"Current Step\" against the provided \"Knowledge Base\". 2 3 **Knowledge Base:** 4 - <Term_1>: <Definition_1> 5 - <Term_2>: <Definition_2> 6 7 **Current Step to Evaluate:** 8 [Text of step t] 9 10 --- 11 **Your Task:** 12 Are all claims and terms in the current step supported by the knowledge base? Respond with JSON object containing binary value. 13 14 **Output Format (JSON only):** 15 { \"factual_accuracy\": <1 _if_all_claims_are_supported_or_0_otherwise > } D.2 C.2 Downstream Task Setups This subsection details the experimental setups for the three application scenarios: Supervised Fine-Tuning (SFT), Bestof-N (BoN) selection, and Group Relative Policy Optimization (GRPO). Listing 6: Template for the Knowledge Coverage Prompt 1 You are verification agent. Your task is to check if the required financial knowledge points were used in the provided model response. 2 3 **Required Knowledge Points:** 4 1. <Term_1>: <Definition_1> 5 2. <Term_2>: <Definition_2> 6 ... 7 8 **Models Reasoning Trace and Answer:** 9 [Models Full Generated Response] 10 11 --- 12 **Your Task:** 13 Analyze the models response and determine how many of the required knowledge points were covered. Output JSON object with the count and indices of the covered points. 14 15 **Output Format (JSON only):** 16 { 17 18 \"coverage_number\": <integer>, \"coverage_index\": [< list_of_covered_indices>] 19 } SFT with Data Selection For the offline data selection task, we first used Fin-PRM to score and select the top 1,000 reasoning traces from larger pool of synthetic data. We then fine-tuned the Qwen2.5-7B-Instruct model on this curated subset. The SFT process used the same set of hyperparameters as the PRM training (see Table 3), ensuring fair comparison. Best-of-N (BoN) Selection In the test-time scaling experiments, the policy model (Qwen2.5-7B-Instruct) generated candidate responses for each question. Fin-PRM then scored each candidate using the composite ranking score from Equation 20. The candidate with the highest score was selected as the final answer. Based on our ablation study (Figure 4), the hyperparameter ζ was set to 1.0 to give equal weight to step-level and trajectory-level rewards. GRPO Reinforcement Learning For the online policy optimization, we integrated Fin-PRM into the GRPO framework. The policy model was Qwen2.5-7B-Instruct, initialized from the same base checkpoint as other experiments. The composite reward from Equation 22 was used to calculate advantages. Key GRPO-specific hyperparameters are listed in Table 4. D. Case Study: Data Sample in Detail To better illustrate the granular nature of our reward signals, this section presents complete data sample in format that explicitly links each reasoning step to its corresponding reward score. Question: Table 4: Hyperparameters for GRPO Policy Optimization. Parameter Value Policy Model Samples per Group (N) Reward Composition (δ in Eq. 22) KL Penalty Coefficient (βKL) PPO Clipping Parameter (ϵ) Epochs Batch Size Learning Rate Qwen2.5-7B-Instruct 6 0.5 0.001 0.2 1 32 1e-6 business of commercial bank that does not constitute on-balance-sheet assets or liabilities but generates non-interest income is called its ( Choices: A: Investment business, B: Asset business, C: Intermediary business, D: Liability business ). Thinking Trajectories Step 1: Analyze the Problem The problem requires identifying commercial banks business type. The key criteria are: (1) It does not constitute on-balance-sheet assets or liabilities, meaning it is an off-balance-sheet business. (2) It generates non-interest income, such as fees or commissions. The options are A. Investment, B. Asset, C. Intermediary, and D. Liability business. The reasoning must be based on the classification of commercial bank operations. [Step-level reward: 0.644] Step 2: Summarize Findings - Asset business (Option B): Involves the use of bank funds, forming on-balancesheet assets (e.g., loans, securities investments). It primarily generates interest income and does not meet the offbalance-sheet condition. - Liability business (Option D): Involves the banks sources of funds, forming on-balancesheet liabilities (e.g., deposits, borrowings). It primarily incurs interest expenses and does not fit the criteria. - Investment business (Option A): Typically refers to the bank investing in financial instruments (bonds, stocks), which is subset of asset business. It constitutes on-balance-sheet assets. - Intermediary business (Option C): Refers to the bank acting as service intermediary, not using its own funds. It is off-balance-sheet and its income comes from fees and commissions (non-interest income). This matches the [Step-level reward: 0.769] description. Step 3: Brainstorm New Ideas To test the robustness of the conclusion, this step considers potential edge cases. For instance, do certain investment activities like derivatives trading count? Or do credit card fees (non-interest income) from liability product complicate the definition? This deepens the analysis beyond superficial match. [Steplevel reward: 0.753] Step 4: Verify Step Accuracy This step cross-references the definition of intermediary business with formal regulations, such as countrys Commercial Bank Law or accounting standards. It confirms that these formal sources align with the conclusion from Step 2, providing an external, fac- [Step-level reward: tual grounding. 0.686] Step 5: Refine Errors The model considers and clarifies potential misconceptions. For example, it distinguishes between contingent liability (like letter of credit, common in intermediary business) which is off-balance-sheet until triggered, and direct on-balance-sheet liability. This refines [Stepthe understanding and prevents oversimplification. level reward: 0.839] Step 6: Revisit Previous Steps The final step involves full review of the reasoning chain. It confirms that the initial analysis in Step 1 was correct, the evaluation in Step 2 was sound, and the verifications in subsequent steps all support the same conclusion, ensuring the entire trajectory is logi- [Step-level reward: cally coherent. 0.837] Final Response Based on the thought process, the business is the commercial banks intermediary business. Reason: Intermediary business does not constitute onbalance-sheet assets or liabilities (i.e., is an offbalance-sheet business) but generates non-interest income (such as fees and commissions) by providing intermediary services (e.g., payment settlement, agency services, consulting). it Exclusion of other options: A. Investment business: Constitutes an on-balancesheet asset. B. Asset business: Directly forms an on-balance-sheet asset. D. Liability business: Forms an on-balance-sheet liability. Therefore, the correct answer is C. Intermediary Business. [Trajectory-level Label: 1 (Correct)]"
        }
    ],
    "affiliations": [
        "Osaka University",
        "Qwen DianJin Team, Alibaba Cloud Computing",
        "Soochow University"
    ]
}