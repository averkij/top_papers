{
    "paper_title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems",
    "authors": [
        "Felix Chen",
        "Hangjie Yuan",
        "Yunqiu Xu",
        "Tao Feng",
        "Jun Cen",
        "Pengwei Liu",
        "Zeying Huang",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite impressive performance across diverse tasks, Multimodal Large Language Models (MLLMs) have yet to fully demonstrate their potential in visual mathematical problem-solving, particularly in accurately perceiving and interpreting diagrams. Inspired by typical processes of humans, we hypothesize that the perception capabilities to extract meaningful information from diagrams is crucial, as it directly impacts subsequent inference processes. To validate this hypothesis, we developed FlowVerse, a comprehensive benchmark that categorizes all information used during problem-solving into four components, which are then combined into six problem versions for evaluation. Our preliminary results on FlowVerse reveal that existing MLLMs exhibit substantial limitations when extracting essential information and reasoned property from diagrams and performing complex reasoning based on these visual inputs. In response, we introduce MathFlow, a modular problem-solving pipeline that decouples perception and inference into distinct stages, thereby optimizing each independently. Given the perceptual limitations observed in current MLLMs, we trained MathFlow-P-7B as a dedicated perception model. Experimental results indicate that MathFlow-P-7B yields substantial performance gains when integrated with various closed-source and open-source inference models. This demonstrates the effectiveness of the MathFlow pipeline and its compatibility to diverse inference frameworks. The FlowVerse benchmark and code are available at https://github.com/MathFlow-zju/MathFlow."
        },
        {
            "title": "Start",
            "content": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems Felix Chen1 Hangjie Yuan1 Yunqiu Xu1 Tao Feng2 Zeying Huang3 Yi Yang1 Jun Cen4 Pengwei Liu 5 2 0 2 9 1 ] . [ 1 9 4 5 6 1 . 3 0 5 2 : r 1Zhejiang University 2Tsinghua University 4The Hong Kong University of Science and Technology 3Intelligent Learning"
        },
        {
            "title": "Abstract",
            "content": "Despite impressive performance across diverse tasks, Multimodal Large Language Models (MLLMs) have yet to fully demonstrate their potential in visual mathematical problem-solving, particularly in accurately perceiving and interpreting diagrams. Inspired by typical processes of humans, we hypothesize that the perception capabilinformation from diagrams ities to extract meaningful inference is crucial, as it directly impacts subsequent processes. To validate this hypothesis, we developed FlowVerse, comprehensive benchmark that categorizes all information used during problem-solving into four components, which are then combined into six problem Our preliminary results on versions for evaluation. FlowVerse reveal that existing MLLMs exhibit substantial limitations when extracting essential information and reasoned property from diagrams and performing complex In response, we reasoning based on these visual inputs. introduce MathFlow, modular problem-solving pipeline that decouples perception and inference into distinct stages, thereby optimizing each independently. Given the perceptual limitations observed in current MLLMs, we trained MathFlow-P-7B as dedicated perception model. Experimental results indicate that MathFlow-P-7B yields substantial performance gains when integrated with various closed-source and open-source inference models. This demonstrates the effectiveness of the MathFlow pipeline and its compatibility to diverse inference frameworks. The FlowVerse benchmark and code are available at https://github.com/MathFlow-zju/MathFlow. 1. Introduction By enabling seamless interaction between visual data and natural language, multimodal large language modCorresponding author: hj.yuan@zju.edu.cn. Figure 1. The Typical Process of Humans Solving Visual Mathematical Problems. We can summarize two key capabilities observed in the typical human problem-solving process: perception and inference. The perception capability involves extracting relevant information from both visual and textual inputs, ensuring accurate reasoning, which inspired the development of FlowVerse and MathFlow. els (MLLMs) [10, 30, 51] are excelling in various tasks, including captioning [34], visual question answering [6] and visual dialogue [36]. Despite their impressive performance across diverse tasks, MLLMs have yet to fully demonstrate their potential in visual mathematical problemsolving [9, 40], particularly in accurately perceiving and interpreting diagrams within these problems [79]. Unlike textual mathematical problems, the typical process of humans for solving visual mathematical problems consists of five sequential steps [28, 49, 55], as illustrated in Fig. 1. We categorize them into two key stages: 1) the perception stage and 2) the inference stage. The perception stage focuses on extracting relevant information from both visual and textual inputs that can be seamlessly integrated with the original problem statement [49, 55]. In contrast, the inference stage concentrates on mathematical reasoning. Regarding the perception stage, previous methods require MLLMs to perceive and interpret diagrams accurately. However, existing MLLMs often struggle with mathematical images [8, 40, 52] and extract unreliable information from visual contents, which inevitably hampers Figure 2. Six Versions of Problems in FlowVerse. FlowVerse begins by categorizing the original problem information into four distinct components: Descriptive Information (DI), Essential Information (EI), Only Question (OQ), and Reasoned Property (RP). The first three components are derived directly from the original problem statement, while RP is extracted from the solution and represents the inferences needed to solve the problem. In the Vision Centric version, we convert the EI into diagrams, while in the Vision Primary version, we convert both the EI and RP into diagrams. the subsequent inference stage. Regarding the inference stage, MLLMs have limited reasoning abilities when given limited or unreliable essential information derived from the perception stage during complex problem solving [79, 81]. Therefore, these models often struggle to generate coherent solutions [50]. We propose that prior methods limited capability to extract information during the perception stage bounds the overall problem-solving performance. To validate this assumption, we developed new benchmark named FlowVerse. FlowVerse categorizes all the information used during problem-solving into four parts [49, 50, 55, 79]: Descriptive Information (DI), which provides fundamental details describing the composition of the problem; Essential Information (EI), which involves inferring critical details from both diagrams and text; Only Question (OQ), representing the specific question posed in the text; and Reasoned Property (RP), which refers to essential properties derived from diagrams and text during the problem-solving process rather than explicitly being provided in the problem. We design six problem variants using different combinations of information types, as illustrated in Fig 2. Our preliminary results on FlowVerse reveal that existing MLLMs struggle significantly in extracting EI and RP from diagrams, as well as in carrying out complex reasoning on given images. Motivated by these observations, we introduce modular problem-solving pipeline, MathFlow, which explicitly decouples the problem-solving pipeline into two distinct stages: perception stage and an inference stage. As illustrated in Fig 3, in the perception stage, EI is first extracted from diagrams, followed by preliminary reasoning to derive RP. They are then converted into text-level representation, further concatenated with the original problems textual information to create unified, enriched input. The inference model can easily generate reasonable solution using the combined visual-derived and textual information as input. Preliminary experiments on FlowVerse highlight the critical importance of robust perception capabilities in deriving useful properties from visual mathematical diagrams. However, model as advanced as GPT-4V may exhibit deficiencies in this specific aspect [1, 16, 52, 63]. To address this, we propose MathFlow-P-7B, specialized model designed to extract EI and RP information from visual diagrams, thereby facilitating visual mathematical problem-solving. the The training of MathFlow-P-7B involves two stages: multi-task pretraining stage and the supervised fine-tuning stage. The multi-task pretraining stage includes two core taskscaptioning EI and RP. The supervised fine-tuning stage further refines the models perception capabilities for enhanced performance. Experimental results indicate that MathFlow-P-7B enables superior performance improvements when integrated with different inference models, underscoring the effectiveness of the MathFlow pipeline as well as its adaptability to various inference models, notably extending the capabilities of general-purpose LLMs to effectively solve visual mathematical problems. Our contributions can be summarized as follows: FlowVerse: We introduce comprehensive benchmark specifically designed to evaluate the visual mathematical problem-solving capabilities of MLLMs across its meticulously crafted six problem versions. MathFlow: We propose modular problem-solving pipeline that decouples the problem-solving process into perception and inference, not only enhancing the models Figure 3. The Overview of MathFlow Pipeline. To effectively train MLLMs for problem-solving, we decouple MLLMs into two sub-modules: the perception model and the inference model. The perception model is responsible for extracting and interpreting visual information, converting it into form that can be effectively processed. The inference model uses this extracted information, along with the original question, to reason and derive solutions. ability to extract and reason with multimodal information effectively, but also empowering LLMs to handle visual mathematical problems. MathFlow-P-7B: We develop perceptive mathematics model, an MLLM specifically optimized for visual mathematical problems, demonstrating SOTA performance across various benchmarks. 2. Related Works 2.1. Mathematics Evaluation Benchmarks The use of LLMs and MLLMs for solving visual mathematical problems has been extensively explored in several studies [15, 48, 65, 71]. GPTs [17, 44, 46], for example, demonstrated some capacity for solving basic arithmetic and algebraic problems. [13, 72] However, it was noted that GPTs often produced incorrect or misleading explanations, highlighting the limitations of these models in reliably handling complex mathematical problem [18, 63, 83]. To better assess the progress of LLMs and MLLMs in mathematical solving and to drive further improvement, number of mathematics-specific evaluation benchmarks have been introduced [39, 54, 75]. Some benchmarks [3, 19, 37, 38], such as MathQA [3], MATH [26], and MathBench [37], are designed to evaluate LLMs across wide spectrum of mathematical concepts, from basic arithmetic to advanced calculus and geometry. However, many mathematical problems include diagrams, which has led researchers to increasingly focus on developing multimodal large language models (MLLMs) that can effectively handle both textual and visual content [20, 32]. Hence, additional benchmarks have been proposed to address these challenges more effectively [40, 41, 79]. WeMath [50] aims to evaluate MLLMs visual mathematical reasoning by decomposing problems into sub-problems, thereby assessing knowledge acquisition and generalization Figure 4. The FlowVerse-CoT-E Strategy. abilities beyond end-to-end performance. In contrast, MathVista [40] integrates diverse mathematical and visual tasks to challenge models with fine-grained visual understanding and compositional reasoning across various contexts. To provide more comprehensive and in-depth evaluation of MLLMs, MathVerse [79] builds upon MathVista, focusing on eliminating textual redundancy to ensure genuine interpretation of visual diagrams rather than reliance on textual shortcuts. It also introduces Chain-of-Thought (CoT) evaluation strategy [11, 59, 67, 68] for detailed assessment of intermediate reasoning steps. However, our analysis reveals that these benchmark datasets still have limitations, such as insufficient coverage of complex multimodal mathematical reasoning. 2.2. Visual Instruction Tuning The advancement of LLMs and MLLMs has driven significant progress in artificial intelligence [70, 77]. visual instruction tuning has emerged as key technique for optimizing MLLMs to effectively perform visual reasoning tasks [27, 42, 62]. Research efforts such as the LLaMAAdapter [22, 78] and LLaVA series [30, 31, 36] have utilized attention mechanisms and projection layers to align vision encoders with LLMs. Models like Flamingo [2, 5] and Infimm-Math [25] further enhance visual representation capabilities through cross-attention mechanisms, improving the models understanding of visual data. Recent advancements in visual instruction tuning for mathematical problem-solving include works [20, 53, 82] like MAVIS [80], Math-LLaVA [58], and InfiMMWebMath-40B [25]. MAVIS employs three-stage training pipeline utilizing specialized datasets to refine visual and mathematical understanding. Math-LLaVA synthesizes diverse question-answer pairs to enhance models reasoning abilities, while InfiMM-WebMath-40B provides largescale dataset that integrates text and visual data to bolster comprehensive reasoning capabilities in mathematical domains. Apart from designing effective training pipelines and network structures, having appropriate datasets to support MLLMs instruction tuning is also crucial [43, 56, 73]. However, previous datasets have often been limited in scale and domain diversity [29, 33, 57]. Hence, researchers have Table 1. Mathematical Evaluation on Six Problem Versions in FlowVerse. DI, EI, RP, OQ infer to the textual or visual Descriptive Information, Reasoned Property, Essential Information, Only Question, respectively. The Text Plus Version does not involve image input. CoT-E or Acc denotes whether to employ the FlowVerse-CoT-E strategy or not. The highest accuracy for closed-source and opensource MLLMs is marked in red and blue respectively. The full table is provided in the Appendix B.5. Model DI + RP + EI + OQ RP + EI + OQ DI + RP + EI + OQ EI + OQ RP + EI + OQ RP + EI + OQ All Text Centric Text Limited Text Plus Vision Dense Vision Centric Vision Primary CoT-E Acc CoT-E Qwen-VL-Max [6] GPT-4o-mini [47] Claude-sonnet-3.5 [4] GPT-4o [47] GPT-4V [46] Gemini-1.5-pro-002 [60] InfiMM-Math [25] InternVL2.5-8B [12] Qwen2-VL-72B [64] InternVL2.5-78B [12] 45.2 55.0 55.5 56.9 64.2 64.5 37.8 46.3 52.8 61. 36.2 47.8 45.1 49.7 58.7 56.2 29.5 40.1 39.7 55.0 49.8 58.7 60.8 61.0 69.1 68.3 43.8 49.2 59.4 66. Acc 42.1 54.8 52.6 56.8 57.1 61.9 36.1 41.3 47.3 62.7 CoT-E 46.7 58.2 58.7 58.7 65.0 66.1 40.6 40.5 54.3 64.1 Acc 38.3 53.2 50.3 54.4 55.0 61.8 34.7 48.4 45.7 60.3 CoT-E 53.9 59.6 64.0 62.2 72.0 68.9 46.1 49.6 63.7 67.8 Acc 51.0 55.2 58.3 58.2 61. 64.1 40.1 42.7 50.0 64.7 CoT-E 38.6 41.1 45.0 45.2 58.1 52.1 28.8 38.4 40.8 48. Acc 29.2 26.0 25.4 30.0 37.3 37.1 19.6 24.2 25.3 34.3 CoT-E 42.7 57.4 56.5 58.6 61.8 65.7 39.6 51.0 50.9 63. Acc 33.2 53.0 48.0 52.6 46.3 57.9 30.3 45.9 39.1 58.8 CoT-E 39.6 54.7 48.1 56.1 59.0 65.0 28.1 48.8 47.6 59. Acc 23.5 45.0 36.0 46.3 39.8 54.9 16.1 38.2 36.9 49.3 proposed various new datasets that cater to the unique needs of visual mathematical reasoning [66, 74]. For instance, Math-LLaVA [58] proposed the MathV360K [58] dataset for synthesizing diverse question-answer pairs from highquality images to strengthen reasoning abilities. However, these approaches do not differentiate between the perception and inference capabilities of the models, nor do they train and optimize these aspects separately. 3. Method To validate MLLMs capabilities regarding perception and inference, we first propose FlowVerse, which categorizes problem information into several key components. The preliminary results on FlowVerse imply that MLLMs capabilities in perception and inference are crucial for accurate problem-solving. Hence, we introduce modular problemsolving pipeline, MathFlow, which comprises two stages. The perception stage is first used to extract information from visual inputs, which is then passed to the inference stage for reasoning. Finally, we develop MathFlow-P-7B, which captures relevant visual content from diagrams. 3.1. FlowVerse: Visual Mathematics Benchmark Dataset Composition. FlowVerse comprises 2,000 visual mathematical problems collected from real exam questions in both Chinese and English, encompassing wide range of mathematical subjects, diagram types, and question types, resulting in over 12,000 test samples. The dataset covers three fundamental mathematics subjects: plane geometry (1,450), algebra (208), and functions (342). Detailed statistics for the dataset composition are presented in Tab. 5 in the Appendix A.1. To guarantee the datasets quality and precision, we conducted thorough review to verify the accuracy of the answers and analyses, as well as to ensure consistency between the questions and their corresponding diagrams. Notably, to avoid any confusion, we named the initially collected dataset FlowVerse, while the subsequent dataset containing six versions of the problem is called FlowVerse. Furthermore, to achieve comprehensive evaluation, this meticulously curated dataset encompasses four key components of information flow: Descriptive Information (DI), Essential Information (EI), Reasoned Property (RP), and Only Question (OQ). Each of these components is carefully crafted to provide distinct insights into the factors that influence the perception capabilities of MLLMs. Descriptive Information (DI) provides fundamental details about the composition of figures, referring to directly observable and explicitly depicted elements within diagram, such as geometric shapes or intersection points in functions. These descriptions help establish context and frame the problem, enabling clear understanding of the visual components involved. Essential Information (EI) refers to the critical details required for problem-solving, such as specific values or relationships between geometric elements (e.g., = 45, AD BC). FlowVerse incorporates information directly from visual diagrams as part of EI, recognizing that in visual math problems, much of the essential information comes from the diagrams. Thus, accurately extracting EI from multiple modalities is crucial for MLLMs to solve problems effectively. Reasoned Property (RP) represents information inferred through higher-level visual abstraction and reasoning combined with relevant mathematical knowledge. Unlike EI, RP requires understanding beyond simple visual perception, as diagrams may not explicitly convey relationships between geometric elements. In FlowVerse, RP is often not directly present in the original problem but requires additional human annotation. These annotations serve as guiding tips to assist solvers, enhancing accuracy and enabling more comprehensive and fine-grained evaluation of MLLMs capabilities. Only Question (OQ) refers to the specific question posed in the text. Though small part of the problem, it is essential for determining what needs to be solved, without Table 2. Mathematical Evaluation on Six Problem Versions in MathVerses testmini Set. The All score is calculated without including the average of the Text Only version. CoT-E or Acc indicates the use of the proposed CoT evaluation strategy or not. denotes that MathFlow-P-7B functions as the perception model, while GPT4V signifies that GPT-4V serves as the inference model within the MathFlow pipeline. The highest accuracy for closed-source and open-source MLLMs is marked in red and blue respectively."
        },
        {
            "title": "Model",
            "content": "Qwen-VL-Plus [6] Gemini-Pro [60] Qwen-VL-Max [6] GPT-4V [46] MathFlow GPT4V SPHINX-MoE [23] InternLM-XC2 [14] MAVIS-7B [80] InfiMM-Math [25] Qwen2-VL-72B [64] MathFlow Qwen2VL72B All Text Dominant CoT-E Acc CoT-E 21.3 35.3 37.2 54.4 56.7 25.8 25.9 27.5 34.5 38.9 40.5 11.8 23.5 25.3 39.4 43. 15.6 16.5 - - 28.5 31.7 26.0 39.8 42.8 63.1 65.2 33.3 36.9 41.4 46.7 49.2 52.3 Acc 15.7 26.3 30.7 54.7 51.1 22.2 22.3 - - 35.4 39. Text Lite CoT-E 21.2 34.7 37.7 56.6 58.9 21.9 28.3 29.1 32.4 35.6 37.7 Acc 11.1 23.5 26.1 41.4 46. 16.4 17.0 - - 27.8 31.7 Text Only CoT-E 25.2 44.5 47.9 60.3 62.1 40.7 42.5 42.5 - 52.1 55.2 Vision Intensive Vision Dominant Vision Only Acc 14.5 27.3 28.9 48.7 48.5 18.3 16.5 - - 39.6 45.5 CoT-E 18.5 32.0 33.6 51.4 53.7 21.1 20.1 27.4 38.1 38.7 40.5 Acc 9.0 23.0 24.1 34.9 40.3 14.8 15.7 - - 26.4 30.3 CoT-E 19.1 36.8 35.9 50.8 52.1 19.6 24.4 24.9 32.4 35.1 37.2 Acc 13.0 22.3 24.1 34.4 37.4 12.6 16.4 - - 28.5 32.4 CoT-E 21.8 33.3 35.9 50.3 52.5 18.3 19.8 14.6 15.8 22.5 24.6 Acc 10.0 22.2 21.4 31.6 39.0 9.1 11.0 - - 13.2 17.1 which the solution process cannot proceed. Based on the four categories, expert annotators systematically remove different types of information within questions and progressively incorporate critical elements into the diagrams. As illustrated in Fig 2, we generate six versions of each problem based on four key components of information, resulting in 12,000 test instances. With this curated problem set, we provide comprehensive evaluation of the visual perception capabilities of MLLMs and assess whether these abilities can effectively support multimodal mathematical reasoning. The details of each problem version are as follows: Text Centric Version retains all content, including DI, RP, EI, and OQ. Notably, in this version, we manually convert all RP and EI components into textual form to effectively analyze their impact on the final reasoning process across different modalities. Text Limited Version removes the DI from the Text Centric Version while retaining the other components. Text Plus Version excludes the image entirely from the Text Centric Version, reducing the input from multimodal to purely text-based format. Vision Dense Version removes the RP from the Text Limited Version. Vision Centric Version converts the Essential Information component from text to image, starting with the Text Limited Version, thereby making it more visually focused. Vision Primary Version further converts the Reasoned Property from text to image, based on the Vision Centric Version, resulting in an entirely visual input for analysis. FlowVerse-CoT-E: Evaluation Strategy. As previously mentioned, several studies have evaluated the abilities of MLLMs from Chain-of-Thought (CoT) perspective [67]. However, these approaches typically rely on GPT or human intervention to decompose the models response into sequential steps and subsequently evaluate the correctness of each step. The variability of intermediate steps complicates this process, as GPTs interpretation and decomposition of questions can introduce errors (termed MathVerse-CoT-E), leading to significant noise and undermining the accuracy and consistency of subsequent evaluations [6, 14]. To address these challenges, we propose CoT-based evaluation strategy, termed FlowVerse-CoT-E, which sequentially integrates components of an authoritative problem-solving solution into the prompts. Specifically, experts first develop an authoritative problem-solving method for each question in FlowVerse and then decompose it into several solution steps. These steps are progressively incorporated into the prompts provided to the MLLMs for reasoning. As illustrated in Fig. 4, the results generated by MLLMs under different prompts are aggregated using weighted approach to derive the final outcome. The scoring process is formulated as follows: Scorefinal = α("
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Scorei) + (1 α)Score0, (1) where α defines as balancing factor between intermediate reasoning steps, setting α = 0.8 by default to highlight the importance of CoT reasoning. Scorei represents the MLLMs score up to step i, while Score0 reflects its score based solely on the final answer, without intermediate steps. 3.2. MathFlow: Modular Visual Mathematical Problem-Solving Pipeline The MathFlow pipeline consists of two stages to alleviate the limitations of MLLMs in visual mathematical problemsolving. In the perception stage, the model extracts critical information from visual data and converts it into text representation, seamlessly integrating it with the original problem statement. This enriched input is then passed to the inference stage. models perception and inference capabilities are inherently distinct, with strong perception being prerequisite for accurate inference. Consequently, we prioritize enhancing the models perception abilities to enable precise extraction and interpretation of visual information, thereby supporting more reliable inference. Then, the inference stage can utilize other state-of-the-art MLLMs/LLMs, allowing the best inference capabilities to complement the perception improvements brought by MathFlow. This modular methodology ensures flexibility and robustness in solving complex visual mathematical problems. For simplicity, we refer to the model in the perception stage as the perception model and the model in the inference stage as the inference model. Furthermore, MathFlows training strategy is divided into two stages: the multi-task pretraining stage and the supervised fine-tuning stage. Upon completion of training, we obtain fine-tuned MLLM (MathFlow-P-7B). Multi-Task Pretraining Stage. The training tasks in this stage primarily include the EI caption task and the RP caption task. On the one hand, the EI caption task aims to train the perception model to generate textual descriptions of essential elements directly from visual inputs. For this, we fine-tune pretrained Qwen2-VL-7B model [64] by utilizing datasets like MAVIS [80] and Geo170k [21], which contain many real or synthetic visual mathematics image-text pairs. On the other hand, the RP caption task is designed to extract higher-level abstractions and relationships, requiring the model to engage in abstract reasoningsuch as deducing relationships between geometric shapes or identifying intersection points in function. To support this task, we developed training dataset called MathFlow-RP, sampled from educational materials that include detailed problemsolving solutions. Details of MathFlow-RP are presented in the Appendix B.1. Specifically, we deconstructed each problems corresponding solution into multiple steps, using the previous step as context to guide the MLLM in predicting the next step of the solution. Finally, we performed mixed training on both tasks, ensuring that the data ratio between the two tasks was maintained at 3:1. During this stage, the LLM backbone is frozen, and training is focused on the perceiver resampler and vision encoder module to ensure that the visual components can effectively extract and process the necessary information. Supervised Fine-Tuning Stage. In this stage, our goal is to enhance the models response quality and adapt it more effectively to the current task context. To achieve this, we meticulously developed supervised fine-tuning dataset called MathFlow-SFT, focusing on retaining only specific numerical values, relationships between geometric elements, and essential information necessary for defining the solution space. Details of this process are provided in the Appendix B.2. During this stage, we freeze the vision Table 3. Performance Comparison of MLLMs on MathVerse and FlowVerse Datasets. FlowVerse indicates the raw version of the dataset. The full table is provided in the Appendix B.5. MathVerse FlowVerse Model Qwen2-VL-72B InternVL-2.5-78B GPT-4V Claude-sonnet-3.5 Gemini 1.5-pro MathFlow Qwen2VL72B MathFlow GPT4V MathFlow MathFlow InternVL2.578B Claudesonnet3. MathFlow Gemini1.5pro 38.9 43.2 54.4 57.4 59.9 48.1 56.7 56.8 60.8 62.4 52.3 54.7 56.2 64.0 68.9 58.3 59.3 60.1 68.1 70. encoder and concentrate on training the perceiver resampler and LLM backbone. This approach aims to improve the models ability to accurately interpret visual inputs and generate concise textual representations that are crucial for effective problem-solving. 4. Experiment 4.1. Experimental Setup and Evaluation Metrics Datasets. For the training of MathFlow, we used both publicly available datasets and internal data. For the EI caption task, we began by filtering out images with excessively high resolutions and those unrelated to mathematical content from the MAVIS [80], Geo170k [21], and GeoGPT4V [7] datasets. Ultimately, we obtained 650,000 image-text pairs. For the RP caption task, we first selected 40,000 problems from our custom question bank that contained detailed and unambiguous solution processes. We then decomposed each problems solution into individual steps and incorporated them into prompts, resulting in final dataset of 130,000 samples. The more detailed data processing steps have been provided in Appendix A.1. Baselines. To ensure comprehensive evaluation, we benchmarked MathFlow against several state-of-the-art (SOTA) MLLMs. For open-source models, we compared MathFlow against Qwen-VL-Plus [6], Qwen2-VL [64], InternVL-2.5 [12] and Qwen-VL-Max [6], which leverage advanced multimodal transformers, InfiMM-Math utilizing large-scale multimodal datasets, and Qwen2-VL-72B [64], an enhanced scalable model. Forclose-source models, we included Claude-sonnet-3.5 [4], Gemini1.5-Pro [60] and GPT-4V [46], known for their advanced vision-language alignment and reasoning capabilities, SPHINX-MoE [23] with its mixture-of-experts architecture, and InternLMXC2 [14], which focuses on fine-grained visual understanding in mathematical tasks. Implementation Details. All experiments were conducted in zero-shot setting to demonstrate the generalization Table 4. Ablation Analysis of MathFlow on FlowVerse. Bold numbers indicate the best performance. The full table is provided in the Appendix B.5. Perception Model for EI Perception Model for RP Inference Model COT-E (%) Qwen2-VL-2B Qwen2-VL-7B InternVL2.5-8B Qwen2.5-VL-7B MathFlow-P-7B GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V Qwen2-VL-2B Qwen2-VL-7B MathFlow-P-7B GPT-4V MathFlow-P-7B MathFlow-P-7B GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V 49.0 53.3 53.8 53.9 58.9 49.8 52.9 57.3 56.2 59.3 MathFlow-P-7B MathFlow-P-7B Claude-sonnet-3.5 68.1 69.6 MathFlow-P-7B MathFlow-P-7B 70.4 MathFlow-P-7B MathFlow-P-7B Gemini 1.5-pro MathFlow-P-7B MathFlow-P-7B DeepSeek-v3 [35] 73.4 75.6 MathFlow-P-7B MathFlow-P-7B DeepSeek-r1 [24] GPT-4o capabilities of MLLMs in mathematical reasoning without relying on few-shot prompting or any additional finetuning. By default, we employ th CoT-based) prompting technique [67], which encourages MLLMs to perform complete reasoning steps for fine-grained evaluation. We conduct all experiments on NVIDIA A100 GPUs. We have listed the generate config of MathFlow in the Appendix B.5. During the training of MathFlow, we consistently used DeepSpeed Zero2 with the AdamW optimizer, configured with cosine learning rate scheduler. In the multi-task pretraining stage, we adopted maximum learning rate of 1e-5, betas of (0.9, 0.95), and weight decay of 0.1. In the supervised fine-tuning stage, we used maximum learning rate of 5e-6, with the same betas and weight decay settings. 4.2. Evaluation Results Tab. 1 compares several state-of-the-art (SOTA) MLLMs on FlowVerse benchmark, including both closed-source and open-source models. Notably, in FlowVersewhere Reasoned Property (RP) and Essential Information (EI) information are manually annotatedMathFlow does not require additional extraction of these components. Therefore, we do not include MathFlow in direct comparisons for FlowVerse. We mainly analyze the performance by the FlowVerse-CoT-E evaluation and derive the following observations: MLLMs Rely More on Reading text than Seeing Diagrams. Similar to the findings in MathVerse [79], when comparing the Text Centric and Text Plus versions in FlowVerse, we observe that most MLLMs improve in performance when visual input is removed, such as +4.3% increase for Qwen2-VL-72B, +3.2% increase for Claudesonnet-3.5. Conversely, when comparing the Text Centric and Text Limited versions, we notice significant drop in performance, such as -4.1% decrease for GPT-4V. These results emphasize the necessity of both comprehensive textual descriptions and effective visual encoding for robust problem-solving in MLLMs. Additional results can be found in Tab. 11 of the Appendix B.5. Good MLLMs Excel at Perceiving EI. Incorporating the Essential Information (EI) within diagrams challenges MLLMs to accurately identify and interpret these critical components visually during mathematical problem-solving. The Vision Centric results show significant drop in performance for most MLLMs compared to their accuracy in the Text Limited version. In other words, we conclude that truly robust MLLM must effectively extract EI from diagrams and visual data. This ability to independently comprehend and interpret visual elements is crucial for successfully solving visual mathematical problems, indicating that enhanced perception capabilities are key factor in improving MLLMs overall performance. MLLMs Significantly Benefit from RP. By removing the Reasoned Property (RP) from the question text, we observe significant decline in accuracy (Text Limited Version vs Vision Dense Version) for most MLLMs, particularly in open-source models. This indicates that RP contains many inferred relationships that are essential for guiding the reasoning process and enhancing the overall problem-solving capabilities of MLLMs. Furthermore, when we convert RP from the text modality to the image modality, the performance of most MLLMs decreases substantially. This not only reinforces the observation that MLLMs rely more on reading text than interpreting diagrams but also highlights that MLLMs significantly benefit from the inclusion of RP. The FlowVerse-CoT-E Strategy Shows More Robust. In Fig. 5, we compare two different CoT evaluation strategies: MathVerse-CoT-E and FlowVerse-CoT-E. Specifically, we evaluated them using both MathVerse-CoT-E and FlowVerse-CoT-E methods on FlowVerse dataset, repeating 5 times. The results show that while MathVerse-CoTE can demonstrate the effectiveness of fine-grained assessment, its evaluation lacks robustness due to its reliance on GPT to perform key-step extraction on MLLM responses, In contrast, FlowVersefollowed by multi-step scoring. CoT-E uses manually deconstructed steps, resulting in more accurate and robust evaluations. 4.3. Model Analysis Tab. 2 shows the performance of MLLMs on MathVerses testmini set. MathFlow achieves the highest overall accuracy among existing open-source and closed-source models, with notable accuracy of 40.5% by the MathFlow Qwen2VL72B and 56.7% by the MathFlow like GPT4V, outperforming other models Figure 5. Comparison of Two Different CoT Evaluation Performances on FlowVerse. GPT-4V Figure 6. Problem-solving Comparison of MathFlow and GPT-4V. In terms of CoT-based evaluation (CoT-E), GPT-4V. MathFlow GPT4V also demonstrates consistent superiority. On the other hand, Tab. 3 shows the performance comparison of MLLMs on the FlowVerse and MathVerse datasets, where FlowVerse refers to the raw, unmodified version of FlowVerse. Notably, MathFlow Gemini1.5pro achieves the highest accuracy across both datasets, with score of 62.4% on MathVerse and 70.4% on FlowVerse. These results not only highlight MathFlows adaptability but also underscore the critical role of perception capabilities in solving problems accurately. Furthermore, Fig. 6 illustrates the problem-solving progress between MathFlow GPT4V and GPT-4V. The results indicate that MathFlow-P-7B demonstrates significantly better capability in understanding and addressing complex mathematical problems, showcasing more effective perception and inference abilities compared to GPT-4V. Error Analysis. To further investigate the causes behind incorrect problem-solving outcomes, we analyzed the errors occurring either within the reasoning process or in the final answer. We categorized these errors into three types: visual perception error, reasoning error, and knowledge error. Educational experts annotated the responses of multiple models on FlowVerse to identify these error types, which are visualized in Fig. 7. From the results, we observe that MathFlow not only shows an overall reduction in error rate Figure 7. Comparison of Error Distributions Across Models on FlowVerse. compared to the corresponding base model but also exhibits decrease in visual perception errors. Moreover, even with powerful inference model (MathFlow DeepSeekr1), significant number of perception errors still occur. This underscores our assertion that perception capability is equally crucial. Ablation Studies. Tab. 4 presents the ablation analysis of MathFlow on the FlowVerse dataset, examining the contributions of different perception models for EI and RP, as well as the impact of varying inference models. The analysis demonstrates that when GPT-4V is used as the inference model, MathFlow consistently outperforms other configurations, achieving high CoT-E score of 58.9%. Furthermore, we found that substituting other SOTA LLMs as inference models yielded even higher performance, with MathFlow reaching its peak performance of 75.6% when Deepseek-r1 serves as the inference model. This remarkable effectiveness of MathFlow stems from its explicit separation of perception and inference, allowing for independent optimization of the perception model and resulting in more accurate visual feature extraction. By converting complex visual information into textual representations that are readily consumable by inference models, MathFlow significantly enhances the reasoning capabilities of various LLMs/MLLMs. Notably, we emphasize that MathFlow enables state-of-the-art LLMs to solve visual mathematical problems without any additional training, effectively extending their capabilities beyond pure language processing to visual mathematical reasoning. 5. Conclusion To investigate the bottlenecks of MLLMs in visual mathematical problem-solving, we introduce FlowVerse, benchmark categorizing problem-solving information into six distinct versions, each designed to evaluate specific aspects of visual mathematical reasoning. Benchmark results reveal significant limitations in current MLLMs ability to accurately perceive and process visual information. Motivated by these findings, we propose MathFlow, which decouples the problem-solving pipeline into perception and inference stages. Given the poor perception performance, we developed MathFlow-P-7B using two-stage training strategy."
        },
        {
            "title": "References",
            "content": "[1] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024. 2 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 3 [3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Konceland Hannaneh Hajishirzi. interpretable math word problem arXiv preprint Kedziorski, Yejin Choi, Mathqa: solving with operation-based formalisms. arXiv:1905.13319, 2019. 3 Towards [4] Anthropic. claude-3-5-sonnet system card, 2024. 4, 6, 9 [5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 4, 5, 6, 9 [7] Shihao Cai, Keqin Bao, Hangyu Guo, Jizhi Zhang, Jun Song, and Bo Zheng. Geogpt4v: Towards geometric multi-modal large language models with geometric image generation. arXiv preprint arXiv:2406.11503, 2024. 6 [8] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. 1 [9] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746, 2022. 1 [10] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: arXiv preprint for vision-language multi-task learning. arXiv:2310.09478, 2023. 1 [11] Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, and Ji-Rong Wen. Chatcot: Toolaugmented chain-of-thought reasoning on chat-based large language models. arXiv preprint arXiv:2305.14323, 2023. 3 [12] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 4, [13] Tyler Cowen and Alexander Tabarrok. How to learn and teach economics with large language models, including gpt. 2023. 3 [14] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprearXiv preprint hension in vision-language large model. arXiv:2401.16420, 2024. 5, 6, 9 [15] Mohammed Elhenawy, Ahmad Abutahoun, Taqwa Alhadidi, Ahmed Jaber, Huthaifa Ashqar, Shadi Jaradat, Ahmed Abdelhay, Sebastien Glaser, and Andry Rakotonirainy. Visual reasoning and multi-agent approach in multimodal large language models (mllms): Solving tsp and mtsp combinatorial challenges. arXiv preprint arXiv:2407.00092, 2024. 3 [16] Tony Haoran Feng, Paul Denny, Burkhard Wuensche, Andrew Luxton-Reilly, and Steffan Hooper. More than meets the ai: Evaluating the performance of gpt-4 on computer graphics assessment questions. In Proceedings of the 26th Australasian Computing Education Conference, pages 182 191, 2024. 2 [17] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30: 681694, 2020. 3 [18] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. Mathematical capabilities of chatgpt. Advances in neural information processing systems, 36, 2024. [19] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. 3 [20] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 3 [21] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 6 [22] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 3 [23] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024. 5, 6, 9 [24] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [25] Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, et al. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. arXiv preprint arXiv:2409.12568, 2024. 3, 4, 5, 9 [26] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 3, 12 [27] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 3 [28] Jennifer Krawec. Problem representation and mathematical problem solving of students of varying math ability. Journal of Learning Disabilities, 47(2):103115, 2014. 1 [29] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35: 38433857, 2022. 3 [30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 3 [31] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [32] Zhihao Li, Yao Du, Yang Liu, Yan Zhang, Yufang Liu, Mengdi Zhang, and Xunliang Cai. Eagle: Elevating geometric reasoning through llm-empowered visual instruction tuning. arXiv preprint arXiv:2408.11397, 2024. 3 [33] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. 3 [34] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 1 [35] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 7 [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 3 [37] Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. Mathbench: Evaluating the theory and application proficiency of llms with arXiv preprint hierarchical mathematics benchmark. arXiv:2405.12209, 2024. [38] Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo Jiang, and Liang He. Cmm-math: chinese multimodal math dataset to evaluate and enhance the mathematics reasoning of large multimodal models. arXiv preprint arXiv:2409.02834, 2024. 3 [39] Yan Liu, Renren Jin, Ling Shi, Zheng Yao, and Deyi Xiong. Finemath: fine-grained mathematical evaluation bencharXiv preprint mark for chinese large language models. arXiv:2403.07747, 2024. 3 [40] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 1, 3 [41] Jingkun Ma, Runzhe Zhan, Derek Wong, Yang Li, Di Sun, Hou Pong Chan, and Lidia Chao. Visaidmath: BenchmarkarXiv preprint ing visual-aided mathematical reasoning. arXiv:2410.22995, 2024. 3 [42] Ben Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 1, 2020. 3 [43] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023. [44] OpenAI. Chatgpt. https://chat.openai.com, 2023. 3 [45] OpenAI. Introducing openai o1, 2023. 12 [46] OpenAI. GPT-4V(ision) system card, 2023. 3, 4, 5, 6, 9 [47] OpenAI. GPT-4o system card, 2024. 4, 9 [48] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. 3 [49] George Polya and George Polya. How to solve it: new aspect of mathematical method. Princeton university press, 2014. 1, 2 [50] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. 2, 3, 12 [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [52] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. arXiv preprint arXiv:2407.06581, 2024. 1, 2 [53] Amrutesh Saraf, Pooja Kamat, Shilpa Gite, Satish Kumar, and Ketan Kotecha. Towards robust automated math problem solving: survey of statistical and deep learning approaches. Evolutionary Intelligence, pages 138, 2024. 3 [54] Ankit Satpute, Noah Gießing, Andre Greiner-Petter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, and Bela Gipp. Can llms master math? investigating large language models on In Proceedings of the 47th Internamath stack exchange. tional ACM SIGIR Conference on Research and Development in Information Retrieval, pages 23162320, 2024. 3 [55] Alan Schoenfeld. Polya, problem solving, and education. Mathematics magazine, 60(5):283291, 1987. 1, 2 [56] Minghao Shao, Abdul Basit, Ramesh Karri, and Muhammad Shafique. Survey of different large language model architectures: Trends, benchmarks, and challenges. IEEE Access, 2024. [57] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 3 [58] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Mathllava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. 3, 4 [59] KaShun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with chain-of-thought from labeled data. arXiv preprint arXiv:2302.12822, 2023. 3 [60] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 4, 5, 6, 9 [61] Qwen Team. Qwen2.5-llm: Extending the boundary of llms, 2024. [62] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3 [63] Karen Wang, Eric Burkholder, Carl Wieman, Shima Salehi, and Nick Haber. Examining the potential and pitfalls of chatgpt in science and engineering problem-solving. In Frontiers in Education, page 1330486. Frontiers Media SA, 2024. 2, 3 [64] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 4, 5, 6, 9 You, and Hongxia Yang. Exploring the reasoning abilities of multimodal large language models (mllms): comprehensive survey on emerging trends in multimodal reasoning. arXiv preprint arXiv:2401.06805, 2024. 3 [66] Zengzhi Wang, Rui Xia, and Pengfei Liu. Generative ai for math: Part imathpile: billion-token-scale pretraining corpus for math. arXiv preprint arXiv:2312.17120, 2023. 4 [67] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 3, 5, 7 [68] Zhaolong Wu, Abul Hasan, Jinge Wu, Yunsoo Kim, Jason PY Cheung, Teng Zhang, and Honghan Wu. Chain-ofthough (cot) prompting strategies for medical error detection and correction. arXiv preprint arXiv:2406.09103, 2024. [69] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason stepby-step. arXiv preprint arXiv:2411.10440, 2024. 12 [70] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666, 2024. 3 [71] Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, Yuxiao Dong, and Jie Tang. Mathglm-vision: Solving mathematical problems with multi-modal large language model. arXiv preprint arXiv:2409.13729, 2024. 3 [72] Gokul Yenduri, Ramalingam, Chemmalar Selvi, Supriya, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Deepti Raj, Rutvij Jhaveri, Prabadevi, Weizheng Wang, et al. (generative pre-trained transformer)a comprehensive review on enabling technologies, potential applications, emerging challenges, and future directions. IEEE Access, 2024. 3 Gpt [73] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong Wang, Lu Sheng, Lei Bai, et al. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. Advances in Neural Information Processing Systems, 36, 2024. 3 [74] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. [75] Boning Zhang, Chengxi Li, and Kai Fan. Mario eval: Evaluate your math llm with your math llma mathematical dataset evaluation toolkit. arXiv preprint arXiv:2404.13925, 2024. 3 [76] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, et al. Llama-berry: Pairwise optimization for o1like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024. 12 [65] Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng [77] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601, 2024. 3 [78] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. [79] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. 1, 2, 3, 7, 4 [80] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024. 3, 5, 6 [81] Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek Wong, Xiaowei Huang, Qiufeng Wang, and Kaizhu Huang. Is your model really good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733, 2024. 2 [82] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. arXiv preprint arXiv:2408.08640, 2024. 3 [83] Mingyu Zong and Bhaskar Krishnamachari. Solving math word problems concerning systems of equations with gpt models. Machine Learning with Applications, 14:100506, 2023. 3 MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems"
        },
        {
            "title": "Supplementary Material",
            "content": "In this Appendix, we first present comprehensive details of FlowVerse (Sec. A), covering data curation, subject definitions, and data statistics. Following this, we provide additional information on the MathFlow-RP and MathFlowSFT datasets (Sec. B), which are used during the multi-task pretraining and supervised fine-tuning stages, respectively. We then present experiments on MathVision (Sec. B.3) to validate the effectiveness of MathFlow. Furthermore, we include additional qualitative examples (Sec. B.4) to illustrate MathFlows high performance specifically. Notably, comprehensive experimental results and detailed analyses (Sec. B.5), providing additional insights into model performance across different configurations and scenarios. Finally, we discuss FlowVerse and MathFlows limitations and potential directions for future work (Sec. C). A. Additional Details of FlowVerse A.1. Data Curation Data Collection. We comprehensively collect visual math problems from internal data1,2. Specifically, we focused on collecting problems related to plane geometry, functions, and algebra. To further enhance the diversity of the dataset, we introduced additional subcategories within each main category, capturing more nuanced types of problems. After the initial collection, we compiled approximately 2,000 visual math problems. Data Categorization and Review. Initially, human annotators categorized the collected problems into three main subjects: plane geometry, solid geometry, and functions. the problems were further subdiWithin each subject, vided into twelve fine-grained categories. Subsequently, we meticulously reviewed the dataset, manually correcting problems with incorrect answers and discarding those that contained multiple diagrams, visual solutions, or had content overly similar to other problems. After this careful curation, we preserved 2,000 high-quality math problems with paired diagrams for FlowVerse, covering wide range of subjects and subfields. Notably, to ensure the quality and richness of the dataset, we excluded problems with minimal Descriptive Information (DI) during the dataset creation process. Notably, to facilitate evaluation, all proofbased questions have been reformulated into questionand-answer format, ensuring consistency and comparability across different problem types. Transformation of Problem Versions. Given the four Table 5. Statistics of FlowVerse. Statistic Total Questions - Subjects/subfields - Multiple-choice questions - Free-form questions - Questions with solutions Multiple-choice question - Proportion of answer - Proportion of answer - Proportion of answer - Proportion of answer - Proportion of answer E&F Number of unique images Number of unique questions Number of unique answers Number of English questions Number of Chinese questions Maximum question length Maximum answer length Average question length Average answer length Number 2000 3/15 848 (62.4%) 1,152 (37.6%) 2,000 (100%) 171 (20.2%) 257 (30.3%) 198 (23.3%) 200 (23.6%) 22 (2.6%) 1,906 (95.3%) 2,000 (100%) 561 (28.1%) 400 (20%) 1600 (80%) 769 351 104.1 9.9 distinct types of information embedded within the questions, human annotators meticulously transform each problem into six different versions, as detailed in Sec.3.1 of the main paper. For this purpose, we utilize NetPad3 to annotate diagrams for the Text Dominant, Vision Centric, and Vision Primary versions. For the Text Centric version, as depicted in Fig. 8, where the raw data presents both EI and RP fully within the diagrams, we selectively remove portions of this information from the diagrams and integrate them into the text. This approach emphasizes the textual representation of the key elements. In the Vision Centric version, NetPad is employed to translate EI from text into visual representation, while the corresponding textual information is removed from the Text Centric version. This transformation aims to shift the balance of information representation towards visual content. For the Vision Primary version, NetPad is further utilized to convert RP from text into visual form based on the Vision Centric version, ultimately creating version in which all pertinent information is conveyed solely through diagrams. 1https://basic.smartedu.cn 2https://jiaoyan.100tal.com 3https://www.netpad.net.cn Figure 8. Manual Modification for EI in FlowVerse. For the original problems shown, we transfer some of the EI from diagrams to question texts (highlighted in green) to mark the Vision Centric version. A.2. Subject and Subfield Definition Plane Geometry. This foundational area studies the properties and relationships of points, lines, and surfaces within two-dimensional plane. It covers key concepts such as circles, triangles, parallelograms, and parallel lines, providing comprehensive context to evaluate the spatial reasoning and logical deduction abilities of MLLMs. Furthermore, expert annotators have categorized the problems into twelve fine-grained categories, as illustrated in Fig. 8 highlighting various dimensions of visual mathematical skills. Circles. This subfield involves understanding properties and relationships associated with circles, such as radius, diameter, tangents, arcs, chords, and inscribed angles. Evaluating MLLMs in this area tests their ability to comprehend and reason about the unique attributes and properties that define circular geometry. Triangles. This category examines various properties of triangles, including side lengths, angles, congruence, similarity, and the Pythagorean theorem. Assessing models of learning and language (MLLMs) on triangles helps evaluate their understanding of fundamental geometric principles and their ability to solve problems involving different types of triangles, such as equilateral, isosceles, and scalene triangles. Parallelograms. This subfield focuses on understanding the properties of parallelograms. It covers the relationships between opposite sides and angles, the concept that the diagonals bisect each other, and specific types of parallelograms such as rectangles, rhombuses, and squares. It also tests the understanding of the characteristics that define and differentiate these shapes. Parallel Lines. This subfield focuses on the study of parallel lines, especially the angles created when they are intersected by transversal. Key angles to examine include alternate interior angles and corresponding angles. It is essential to evaluate the ability of MLLMs to apply these properties in order to solve complex geometric problems. Similarity of Figures. This area examines the criteria for similarity between geometric figures, such as side ratios and corresponding angles. It requires MLLMs to apply proportional reasoning and recognize conditions under which two figures are similar, testing their understanding of geometric relationships in broader context. Primary Geometry. This subfield deals with fundamental concepts such as points, lines, and basic shapes. It lays the foundation for understanding more complex geometrical relationships, testing MLLMs ability to grasp and reason with the basic building blocks of geometry. Functions. This encompasses analyzing mathematical relationships between variables, ranging from simple evaluations of function values to more complex tasks like examining different function types and their behaviors. We assess MLLMs capabilities using four categories of function problems, as exemplified in Fig 9 . Inverse Proportional Functions: This category involves functions where the product of two variables remains constant. Evaluating MLLMs on inverse proportional functions helps assess their understanding of non-linear relationships and their ability to apply these principles in problem-solving. Quadratic Functions: Problems in this category include analyzing the properties of quadratic equations, such as identifying the vertex, roots, axis of symmetry, and the impact of changing coefficients. MLLMs are tested on their ability to understand and manipulate quadratic expressions to solve for function behavior. Linear Functions: Linear functions represent relationships with constant rate of change. This category focuses on understanding slope and y-intercept and interpreting graphical representations of linear equations, which are essential for evaluating MLLMs grasp of fundamental linear relationships. Coordinate Systems and Functions: This subfield deals with understanding how functions are represented within coordinate planes, including plotting points, interpreting graphs, and understanding shifts and transformations of basic functions. It assesses MLLMs skills in translating functional relationships into visual formats. Trigonometric Functions: Problems in this category involve understanding trigonometric relationships, such as sine, cosine, and tangent, and their applications to solve problems involving angles and periodic phenomena. Evaluating MLLMs on these functions helps gauge their understanding of trigonometric concepts and their application in different scenarios. Algebra. encompasses wide range of topics that focus on understanding mathematical relationships through symbols and expressions. It involves solving problems related to probability, data analysis, equations, and numerical expressions. We evaluate MLLMs skills using four distinct categories within algebra: Probability: This category covers fundamental probability concepts, including calculating the likelihood of events, understanding random experiments, and applying basic probability laws. It assesses MLLMs ability to handle uncertainty and predict outcomes based on given conditions. Data Analysis: Problems in this category involve interpreting and analyzing data sets, including calculating measures such as mean, median, mode, and range, and understanding data representations like charts and graphs. Evaluating MLLMs in this area tests their ability to draw meaningful conclusions from quantitative data. Equations and Inequalities: This subfield includes solving linear and non-linear equations, as well as inequalities involving one or more variables. MLLMs are evaluated on their ability to manipulate and solve algebraic expressions and apply them to real-world scenarios. Numbers and Expressions: This category deals with unTable 6. Length of Different Problem Versions in FlowVerse. Problem Version Character Text Centric & Text Plus - Maximum question length - Maximum answer length - Average question length - Average answer length Text Limited - Maximum question length - Maximum answer length - Average question length - Average answer length Vision Dense - Maximum question length - Maximum answer length - Average question length - Average answer length Vision Centric - Maximum question length - Maximum answer length - Average question length - Average answer length 962 351 153.3 9. 937 351 123.9 9.9 688 351 74.7 9.9 571 351 75.8 9.9 derstanding numerical properties, simplifying algebraic expressions, and performing arithmetic operations. It tests MLLMs grasp of numerical relationships and their ability to work with algebraic symbols effectively. A.3. Detailed Statistics of FlowVerse More Data Statistics. In Tab. 5, we present detailed breakdown of the FlowVerse dataset. It is important to note that all problems in FlowVerse are sourced from internet data rather than other datasets, and each question is annotated manually. Additionally, our evaluation aims to effectively showcase the reasoning capabilities of MLLMs with moderate-level mathematical knowledge without restricting their performance through overly complex domain-specific theorems or extensive commonsense knowledge. Consequently, we focus on problems at the high school level, deliberately excluding advanced college-level topics such as calculus and graph theory. Problem Length Variance. In Tab. 6, we present the variations in question and answer lengths across five versions of problems in FlowVerse, excluding the Vision Primary category, as it contains only the Only Question component. By removing pre-defined components such as Descriptive Information (DI), Reasoned Property (RP), and Essential Information (EI), the maximum and average lengths of questions decrease progressively, whereas the answer lengths remain unaffected. Fig. 10 visualizes the character-level variation in question length for four problem versions: Text Centric (blue), Text Limited (green), Vision Centric (red), and Vision Dense (yellow). As DI, EI, and RP are sequentially omitted from the Text Centric version, Figure 9. Subject Distribution of FlowVerse. ematics problems. FlowVerse-CoT-E utilizes structured prompt that guides the model step-by-step through the problem-solving process, ensuring more transparent evaluation of the models ability to logically infer and connect various aspects of the problem. This evaluation mechanism distinguishes between different reasoning paths taken by the model, thereby providing nuanced understanding of its intermediate problem-solving capabilities. Error Analysis of GPT-4 on FlowVerse. Fig. 11 illustrates GPT-4Vs Knowledge Error, Reasoning Error, and Visual Perception Error across different versions of problem representations. The Text Plus version demonstrates the lowest Knowledge and Reasoning Errors, along with smaller Visual Perception Error, as indicated by its relatively small bubble radius. This makes it the most effective approach for minimizing overall errors. In contrast, the Vision Centric, Vision Only, and Vision Dense versions show significantly higher Reasoning and Knowledge Errors, accompanied by the largest bubble radii, indicating substantial Visual Perception Error. This suggests that an overreliance on visual information increases the complexity of both perception and reasoning. The Text Centric and Text Limited versions fall in between, exhibiting moderate error rates and bubble sizes, which implies slight increase in perception difficulty as the representation shifts away from textual information. Overall, the analysis highlights that enriched textual representation, such as in the Text Plus version, is crucial for achieving optimal performance in minimizing Knowledge, Reasoning, and Visual Perception erFigure 10. Distribution of Question Length for Four Problem Versions. We present the distribution of question length for the four problem versions, with the horizontal axis representing question length in characters and the vertical axis depicting the corresponding probability distribution. we observe clear downward trend in both the distribution of question lengths and their average values. A.4. Details of Evaluation on FlowVerse Prompt Design for Response Generation. We employ two distinct types of prompts for free-form and multiplechoice questions, respectively, as per the guidelines established by MathVerse [79], which are detailed in Tab. 7. To effectively elicit the Chain-of-Thought (CoT) reasoning capabilities of MLLMs, we also incorporate the phrase first conduct reasoning to encourage more structured and logical approach to problem-solving. Prompt for CoT Evaluation. As discussed earlier, our proposed Chain-of-Thought evaluation, termed FlowVerseCoT-E, is specifically designed to assess the reasoning depth and perceptual accuracy of MLLMs in visual mathB. Additional Details of MathFlow B.1. Details of MathFlow-RP As mentioned in Sec.3.2, We designed MathFlow-RP to enhance the reasoned property of MathFlow during the multitask pretraining stage for the RP caption task, which focuses on extracting higher-level abstractions and relationships, requiring the model to engage in abstract reasoningsuch as deducing relationships between geometric shapes or identifying intersection points in function. Data Collection. As previously described, the MathFlowRP dataset is meticulously curated to enhance the models response quality and adapt it more effectively to the current task context. This dataset consists of 40,000 questions, each accompanied by detailed solution, thus providing robust foundation for enhancing the models capability to comprehend complex visual mathematical reasoning. Data Annotation and Filtering. Specifically, as illustrated in Fig. 15, Qwen2.5-72B [61] is employed to extract the key steps from these solutions, given the prohibitive cost of utilizing GPT-4 for the extraction across such large dataset. As result, we were able to compile total of 13,000 annotated instances, which serve as the core training data for enhancing the models problem-solving capabilities. Notably, since some problems involve multiple questions, we divide them into different instances for clarity. Training Strategy. Building on this approach, the extracted solution steps are sequentially appended to the original question, enabling the model to predict the subsequent solution step based on the cumulative information. This iterative process facilitates chain-of-thought reasoning mechanism by progressively integrating previous steps into the prompt, allowing the model to incrementally build upon earlier conclusions. Specifically, each step in the solution is derived using pertinent properties, such as congruent triangles or properties of parallel lines, ensuring that every next prediction is grounded in logical progression. By extracting, labeling, and integrating key steps, the model is trained to emulate human-like deductive reasoning, ultimately enhancing its problem-solving accuracy for complex visual mathematical tasks. B.2. Details of MathFlow-SFT As mentioned in Sec.3.2, the MathFlow-SFT (Supervised Fine-Tuning) dataset is meticulously curated to enhance the models response quality and adapt it more effectively to the current task context during the supervised fine-tuning stage of MathFlow. The dataset focuses on extracting and refining the key visual and textual elements necessary for accurate mathematical reasoning, with specific emphasis on retaining only the essential information to clearly define solution steps. Data Collection. The MathFlow-SFT dataset samples viFigure 11. Visualization of Different Error Type across Different Versions using GPT-4 on FlowVerse. The horizontal axis represents different problem versions, while the vertical axis indicates the error types. The radius of each bubble corresponds to the number of visual perception errors, with smaller radii indicating fewer visual perception errors. rors, underscoring the importance of balanced information representation for effective problem-solving. In conclusion, the result further confirms that the limited capabilities of prior methods to extract information during the perception stage restrict the overall problem-solving performance, which also guides us in the development of the modular problem-solving pipeline, MathFlow. A.5. Qualitative Examples Figs. 12-14 illustrate the differences across six versions of problem representations: Text Centric, Text Limited, Text Plus, Vision Centric, Vision Intensive, and Vision Primary. Each version provides distinct balance between textual and visual information, thereby influencing the accessibility and interpretability of the problem. The Text Centric version includes detailed descriptive information along with key reasoning steps in the text, while the Text Limited version reduces some descriptive elements to focus on core content. The Text Plus version enriches this further with added visual context for better comprehension. On the other hand, the Vision Centric and Vision Dense versions emphasize visual elements by gradually minimizing textual information, whereas the Vision Primary version conveys almost all the information through diagrams with minimal textual support. These examples highlight the impact of information representation on the models ability to comprehend and solve problems, demonstrating how variations in text and visual content can influence both perception and reasoning capabilities. Table 7. Input Prompt of MLLMs for Response Generation."
        },
        {
            "title": "Prompt",
            "content": "Free-form Question Multiple-choice Question Please first conduct reasoning, and then answer the question and provide the final value, e.g., 1, 2.5, 300, at the end. Question: {question} Please first conduct reasoning, and then answer the question and provide the correct option letter, e.g., A, B, C, D, at the end. Question: {question} Table 8. Configuration for the FlowVerse-CoT-E."
        },
        {
            "title": "Prompt",
            "content": "Question & Sequence of Solutions You will be provided with visual mathematics problem along with detailed solution procedure. Your task is to solve the problem by utilizing the provided solution steps as guidance. Here are examples: Question: XXX Solution: 1. XXX 2. XXX 3. XXX Final Solution: XXX Here is what you need to solve: Question: {question} Solution: {solution} Final Solution: sual mathematical problems with detailed solutions, which are sourced from diverse educational resources, including textbooks, standardized exam questions, and synthetic problems specifically designed for visual challenges. Also, MathFlow-SFT is collected to ensure coverage across different types of mathematical reasoning, such as plane geometry, algebra, and trigonometry, to provide comprehensive training base. Annotation of Key Elements. To ensure that the model effectively perceives and interprets diagrams, as Fig. 16 shows, we manually annotated the critical components of each problem based on both the descriptive information provided in the problem statement and the corresponding solution. The annotations include: Geometric Elements: Labels for points, lines, angles, and other geometric properties. Relationships: Specific relationships, such as congruency or parallelism, are highlighted to help the model understand the underlying structure. Reasoned Property Extraction: Abstract properties inferred from the given diagrams and textual context are labeled, emphasizing the reasoning process needed to progress through each solution. B.3. More Analysis of Experiment Results Tab. 13 compares the performance of various closed and open-source Multimodal Large Language Models (MLLMs) on the MathVision dataset across different mathematical subjects. The highest scores are highlighted in red for closed-source and blue for open-source models. MathFlow leads logical GPT4Turbo among closed-source models in arithmetic (Ari), reasoning (Log), and geometry (Angle), showing its strength in diverse mathematical domains. Among open-source models, MathFlow Qwen2VL72B performs well in algebra (Alg), arithmetic (Ari), and topology (Topo), demonstrating robust capabilities in visual mathematical tasks. Overall, MathFlow models perform competitively with both closed and open-source baselines, showcasing their adaptability to complex mathematical problem-solving by effectively integrating visual perception and logical reasoning. B.4. Qualitative Examples Figs. 17-19 provide qualitative comparisons between GPT4V and MathFlow across several geometry problems. These examples illustrate common error types, such as Visual Perception Errors and Reasoning Errors, which are highlighted in the respective model responses. Figure 12. Comparison of Six Problem Versions in FlowVerse. Figure 13. Comparison of Six Problem Versions in FlowVerse. Table 9. Generate Config of MathFlow. Hyper Parameter Value Temperature TopP TopK Repetition Penalty Num of Beams 0.3 0.7 1.0 1.0 1.0 In the first example, GPT-4V struggles with applying geometric properties correctly, leading to reasoning mistake while calculating the length of chord. In contrast, MathFlow arrives at the correct solution by following systematic application of the Pythagorean theorem and congruent relationships, demonstrating its advantage in maintaining logical accuracy throughout the problem-solving process. The second example highlights the challenge of dealing with circle theorems. GPT-4V incorrectly deduces the angle at the center due to misinterpretation of the chords properties, while MathFlow accurately follows through the geometric relationships to determine the correct angle measure. This example underscores MathFlows superior ability to navigate complex angle relationships and avoid error propagation in multi-step reasoning. The third example involves congruent triangles and corresponding parts. GPT-4V makes visual perception error by misidentifying the corresponding parts of congruent triangles, resulting in an incorrect response. MathFlow, on the Figure 14. Comparison of Six Problem Versions in FlowVerse. Figure 15. Data Annotation of the MathFlow-RP We first employ Qwen2.5-72B to extract the corresponding steps from the solvingproblem solution, then select step as the target for prediction. Subsequently, the preceding N-1 steps are provided as input within the prompt, enabling the MLLM to predict the next step based on this sequential context. other hand, correctly identifies and matches corresponding sides and angles, showcasing its ability to effectively manage both visual and logical components of geometric problems. perception and reasoning errors, resulting in more reliable problem-solving performance. B.5. More Detailed Experiment Results These qualitative examples demonstrate that MathFlow consistently outperforms GPT-4V in handling visual mathematical problems. MathFlow effectively reduces visual In this section, we first present the generation configuration of MathFlow in Table 9. These hyperparameters were carefully tuned to optimize the models generation performance Figure 16. Data Annotation of the MathFlow-SFT.We manually extract the corresponding EI and RP from the solving-problem solution and associated diagram. In this representation, the red-highlighted portions indicate EI, while the blue-highlighted sections represent RP. Table 10. Full Mathematical Evaluation on Six Problem Versions in FlowVerse. DI, EI, RP, OQ infer to the textual or visual Descriptive Information, Reasoned Property, Essential Information, Only Question, respectively. The Text Plus Version does not involve image input. CoT-E or Acc denotes whether to employ the FlowVerse-CoT-E strategy or not. The highest accuracy for closed-source and open-source MLLMs is marked in red and blue respectively. Model DI + RP + EI + OQ RP + EI + OQ DI + RP + EI + OQ EI + OQ RP + EI + OQ RP + EI + OQ All Text Centric Text Limited Text Plus Vision Dense Vision Centric Vision Primary CoT-E Acc CoT-E Qwen-VL-Plus [6] Gemini-Pro [60] Qwen-VL-Max [6] GPT-4o-mini [47] Claude-sonnet-3.5 [4] GPT-4o [47] GPT-4V [46] Gemini-1.5-pro-002 [60] SPHINX-MoE [23] InternLM-XC2 [14] InfiMM-Math [25] InternVL2.5-8B [14] Qwen2-VL-72B [64] InternVL2.5-78B [14] 34.4 39.0 45.2 55.0 55.5 56.9 64.2 64.5 32.7 34.0 37.8 46.3 52.8 61.6 27.1 23.5 36.2 47.8 45.1 49.7 58.7 56. 15.0 16.5 29.5 40.1 39.7 55.0 Acc 24.6 36.5 42.1 54.8 52.6 56.8 57.1 38.1 44.3 49.8 58.7 60.8 61.0 69.1 68.3 61. 39.2 42.1 43.8 49.2 59.4 26.2 32.3 36.1 41.3 47.3 66.1 62.7 CoT-E 35.7 40.2 46.7 58.2 58.7 58.7 65.0 66. 34.5 34.3 40.6 40.5 54.3 64.1 Acc 28.6 33.8 38.3 53.2 50.3 54.4 55.0 61.8 23.4 25.0 34.7 48.4 45.7 60.3 CoT-E 41.7 47.2 53.9 59.6 64.0 62.2 72. Acc 35.6 39.8 51.0 55.2 58.3 58.2 61.4 68.9 64.1 42.7 43.0 46.1 49.6 63.7 28.3 32.5 40.1 42.7 50. 67.8 64.7 CoT-E 28.5 32.5 38.6 41.1 45.0 45.2 58.1 52.1 25.1 27.2 28.8 38.4 40.8 48.7 Acc 20.9 23.3 29.2 26.0 25.4 30.0 37.3 37.1 14.8 17.4 19.6 24.2 25.3 34.3 CoT-E 32.6 36.1 42.7 57.4 56.5 58.6 61.8 65.7 28.6 29.4 39.6 51.0 50.9 63.0 Acc 26.0 27.3 33.2 53.0 48.0 52.6 46.3 57.9 13.5 14.1 30.3 45.9 39.1 58.8 CoT-E 30.0 33.8 39.6 54.7 48.1 56.1 59.0 65.0 26.2 28.0 28.1 48.8 47.6 59.6 Acc 22.2 24.9 23.5 45.0 36.0 46.3 39.8 54.9 11.2 13.9 16.1 38.2 36.9 49.3 while maintaining output stability and reliability. Second, we present comprehensive experimental results through three detailed analyses. First, we evaluate our models performance across different problem versions in FlowVerse through Tab. 10. This analysis spans six distinct versions incorporating various combinations of textual and visual components: Descriptive Information (DI), Essential Information (EI), Reasoned Property (RP), and Only Question (OQ). The Text Plus Version specifically examines performance without image input. We report both standard accuracy (Acc) and performance with the FlowVerseCoT-E strategy (CoT-E), highlighting the highest accuracies achieved by closed-source and open-source MLLMs in red and blue respectively. Third, Tab. 11 presents thorough ablation analysis of our perception model on FlowVerse. This analysis systematically examines the contribution of different components within our perception framework, with bold numbers indicating the best performance across different configurations. Finally, Tab. 12 provides comprehensive performance comparison of various MLLMs across both MathVerse and FlowVerse datasets. Here, FlowVerse represents the raw version of the dataset, allowing us to evaluate the models fundamental capabilities without additional enhancements. This comparison offers insights into the relative strengths of different architectures and approaches across diverse mathematical reasoning tasks. C. Limitation and Future Work While MathFlow and FlowVerse take step forward in the field of visual mathematical evaluation for MLLMs, it is important to recognize several limitations as follows. In FlowVerse, we initially categorized the collected Figure 17. Response Comparison of GPT-4V and MathFlow GPT4V Figure 18. Response Comparison of GPT-4V and MathFlow GPT4V Figure 19. Response Comparison of GPT-4V and MathFlow GPT4V Table 11. Full Ablation Analysis of MathFlow on FlowVerse. Bold numbers indicate the best performance. Perception Model for EI Perception Model for RP Inference Model COT-E (%) Qwen2-VL-2B InternLM-XC2 InfiMM-Math Qwen2-VL-7B InternVL2.5-8B Qwen2.5-VL-7B MathFlow-P-7B GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V Qwen2-VL-2B InternLM-XC2 InfiMM-Math Qwen2-VL-7B MathFlow-P-7B GPT-4V MathFlow-P-7B MathFlow-P-7B GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V GPT-4V 49.0 47.1 52.6 53.3 53.8 53.9 58.9 49.8 50.7 54.1 52.9 57.3 56.2 59.3 GPT-4o-mini MathFlow-P-7B MathFlow-P-7B 63.8 MathFlow-P-7B MathFlow-P-7B Claude-sonnet-3.5 68.1 67.9 MathFlow-P-7B MathFlow-P-7B 69.6 MathFlow-P-7B MathFlow-P-7B 69.6 MathFlow-P-7B MathFlow-P-7B 70.4 MathFlow-P-7B MathFlow-P-7B Gemini 1.5-pro 73.4 DeepSeek-v3 MathFlow-P-7B MathFlow-P-7B 75.6 DeepSeek-r1 MathFlow-P-7B MathFlow-P-7B Llama3-80B Qwen2.5-72B GPT-4o Table 12. Full Performance Comparison of MLLMs on MathVerse and FlowVerse Datasets. FlowVerse indicates the raw version of the dataset. Perception Model Inference Model MathVerse FlowVerse InternLM-XC2 InfiMM-Math Qwen-VL-MaX Qwen2-VL-72B InternVL-2.5-78B GPT-4V GPT-4o-mini GPT-4o Claude-sonnet-3.5 Gemini 1.5-pro MathFlow-P-7B InternLM-XC2 MathFlow-P-7B InfiMM-Math MathFlow-P-7B Qwen-VL-MaX MathFlow-P-7B Qwen2-VL-72B MathFlow-P-7B InternVL-2.5-78B MathFlow-P-7B MathFlow-P-7B MathFlow-P-7B MathFlow-P-7B Claude-sonnet-3.5 MathFlow-P-7B Gemini 1.5-pro GPT-4V GPT-4o-mini GPT-4o 25.9 34.5 36.2 38.9 43.2 54.4 52.7 57.9 57.4 59.9 30.2 38.1 43.3 48.1 56.8 56.7 58.9 59.5 60.8 62.4 39.6 47.1 48.2 52.3 54.7 56.2 59.6 62.2 64.0 68.9 43.5 48.9 54.3 58.3 60.1 59.3 63.8 69.6 68.1 70.4 mathematical problems based on subjects and subfields that reflect varying degrees of multimodal content. These classification methods enable comprehensive evaluation of MLLM capabilities across different dimensions. However, Table 13. Comparison of model performances across various mathematical subjects on the MathVision dataset. The highest accuracy for closed-source and open-source MLLMs is marked in red and blue respectively. Model Overall Qwen-VL-Plus Qwen-VL-Max Gemini Pro GPT-4Turbo MathFlow GPT4Turbo ShareGPT4V-13B SPHINX-MoE InternLM-XComposer2-VL Qwen2-VL-72B MathFlow Qwen2VL72B 10.72 15.59 17.66 30.26 32.01 11.88 14.18 14.54 25.9 28.14 Alg 11.3 10.7 15.1 37.7 43. 7.5 7.8 9.3 - 38.2 AnaG 17.9 19.1 10.7 33.3 38.7 15.5 17.9 15.5 - 36.5 Ari 14.3 20.0 20.7 46.4 48. 16.4 14.3 12.1 - 47.5 CombG Comb 12.7 16.9 20.1 25.0 28.2 10.7 15.6 15.3 - 27.1 4.8 12.5 11.9 28.6 29.0 8.9 9.5 11.3 - 23. Cnt 10.5 17.9 7.5 25.3 25.4 9.0 11.9 10.5 - 20.6 DescG GrphT 15.4 16.4 20.2 15.4 17. 11.5 12.5 14.4 - 22.2 8.9 12.2 21.1 27.8 30.6 8.9 15.6 22.2 - 21.5 Log 14.3 21.0 16.8 31.9 32.1 7.6 12.6 19.3 - 27. Angle Area 11.6 13.3 19.1 30.6 32.9 11.6 16.2 19.7 - 19.5 6.4 14.2 19.0 29.0 30.7 13.0 15.6 15.6 - 24. Len 10.0 19.8 20.0 31.9 35.1 17.4 17.8 15.0 - 28.3 SolG 14.3 11.5 14.3 28.7 31.5 10.3 13.5 11.9 - 27. Stat 6.9 20.7 13.8 37.9 40.0 8.6 12.1 15.5 - 33.0 Topo TransG 8.7 13.0 17.4 17.4 20. 8.7 8.7 26.1 - 35.0 11.31 17.3 20.8 23.2 26.0 12.5 16.1 15.5 - 18.1 an additional categorization by difficulty levelsimilar to the approach used in datasets like MATH [26] and WeMath [50], where problems are sorted by complexitywould provide deeper insights into model performance. This extra layer of differentiation could significantly enhance the models assessment, and we plan to explore this in our future work. Furthermore, the issues with FlowVerse primarily stem from the fact that they are mainly available in English and Chinese. This limitation restricts the models applicability to wider range of languages. By expanding the dataset to include greater diversity of languages, we could enhance the multilingual capabilities of MLLMs and create benchmarks that are more inclusive for users who speak languages other than English and Chinese. Within the MathFlow pipeline, although we have trained the perception model using the RP captioning task, the resulting reasoning capabilities remain relatively basic, making it challenging for the model to handle more complex reasoning tasks effectively. On the other hand, with the emergence of models [45, 69, 76] like GPT-o1 [45] and LLaMA-o1 [76], which demonstrate enhanced capabilities, we see an opportunity for further advancement. Consequently, in future work, we plan to build upon these advanced models to improve the reasoning capabilities of MathFlow."
        }
    ],
    "affiliations": [
        "Intelligent Learning",
        "The Hong Kong University of Science and Technology",
        "Tsinghua University",
        "Zhejiang University"
    ]
}