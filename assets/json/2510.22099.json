{
    "paper_title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
    "authors": [
        "Xuanming Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 9 9 0 2 2 . 0 1 5 2 : r Generalization or Memorization: Dynamic Decoding for Mode Steering Xuanming Zhang 1 Abstract Large Language Models (LLMs) exhibit troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of compressed, task-relevant representation and memorization as failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), novel inference-time algorithm which comprises two components: (1) lightweight, causally-grounded linear probe that identifies the models instantaneous reliance on memorization, and (2) dynamic activation steering mechanism that nudges the models computation towards pre-identified generalization circuits. We frame DMS as form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering principled approach to enhancing LLM reliability. 1. Introduction Large Language Models (LLMs) represent significant milestone in artificial intelligence, demonstrating capabilities that often appear to stem from rich, nuanced understanding of the world (Naveed et al., 2025; Pearce et al., 2023; Zhang et al., 2025b). Their proficiency in tasks ranging from complex reasoning to creative text generation has positioned them as transformative technologies. However, this apparent understanding is frequently contradicted by 1Stanford University, Palo Alto, USA 2Department of Computer Science, University of Wisconsin-Madison, Madison, USA. Correspondence to: Xuanming Zhang <xzhang2846@wisc.edu>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). more simplistic, and often problematic behavior: the verbatim regurgitation of memorized fragments from their vast training corpora (Dong et al., 2024; Bayat et al., 2024). This tendency to memorize, rather than reason, leads to models that can confidently produce fluent falsehoods, perpetuate biases present in the training data, and fail unexpectedly on inputs that deviate slightly from memorized patterns (Wang et al., 2024c; Dong et al., 2025). The unpredictable switching between these two modesrobust generalization and brittle memorizationis central challenge to their safe and reliable deployment. We frame this challenge as fundamental tension between two distinct and competing operational modes within single model: Generalization is the desired mode, characterized by the ability to learn abstract principles, rules, and causal structures from data and apply them to solve novel problems (Chatterjee, 2018; Kang et al., 2024; Zhang et al., 2024). It is the foundation of true reasoning and robust performance on unseen inputs. In contrast, memorization is shortcut to achieving low training error, where the model functions as high-dimensional lookup table, storing and retrieving specific input-output pairs from its training set without learning the underlying generative process (Xie et al., 2024). While effective on familiar data, this mode is inherently brittle and fails to adapt to new situations. The phenomenon of grokking serves as stark and compelling illustration of this dichotomy. Observed in neural networks trained on algorithmic tasks, grokking is delayed phase transition where model, long after achieving perfect accuracy on its training data through memorization, suddenly and rapidly learns to generalize to unseen test data (Fan et al., 2024). This transition reveals that memorization and generalization are not merely points on continuum but are distinct, accessible solutions in the models loss landscape. model can become stuck in memorization-based local minimum before eventually discovering more efficient, generalizable solution, prompted by factors like prolonged training or regularization techniques such as weight decay (Liu et al., 2022; Kumar et al., 2023). This phenomenon moves the problem from purely abstract concern to an empirically observable dynamic, motivating the need for formal theory to explain these modes and practical mechanism to control them. To address this, our work introduces unified framework that spans theory Generalization or Memorization: Dynamic Decoding for Mode Steering and practice. The primary contributions are twofold: We propose novel theoretical model based on the Information Bottleneck (IB) principle to formally define and distinguish between generalization and memorization. This framework provides principled, mathematical language to analyze the learning dynamics of LLMs. We introduce Dynamic Mode Steering (DMS) for Mode Control. DMS is training-free, inference-time algorithm designed to dynamically manage the models reasoning mode, nudging it away from memorizationassociated pathways and towards circuits responsible for generalization. 2. Related Work Generalization, Memorization, and Grokking. The tension between generalization and memorization is classical theme in machine learning (Chatterjee, 2018; Yin et al., 2019; Feldman, 2020), but it has gained new urgency with the scale of modern LLMs (Wang et al., 2024b; Chu et al., 2025; Bayat et al., 2024; Li et al., 2024; Xu et al., 2025). The phenomenon of grokking, first identified by Power et al., provided clear empirical demonstration that these are distinct learning phases. Subsequent research has explored the conditions under which grokking occurs, identifying factors like model size, data size, and the strength of weight decay as critical parameters (Liu et al., 2022; Chang et al., 2024; Wang et al., 2024a). Other studies have linked the transition to changes in the internal representations, such as decrease in feature rank, suggesting move towards simpler, more structured solution (Fan et al., 2024; Zhang et al., 2025a). Our work further contributes by proposing the Information Bottleneck principle as unifying theoretical framework that explains why these transitions occur, framing them as an optimization process that seeks more information-theoretically efficient representation. Information-Theoretic Analysis of Neural Networks. The application of the Information Bottleneck principle to understand deep learning has rich history. Early work proposed that the training of deep neural networks via stochastic gradient descent naturally follows two phases: an initial fitting phase where the model increases I(Z; ), the mutual information with the labels, followed by compression phase where it reduces I(X; Z), the mutual information with the input (Kawaguchi et al., 2023; Lewandowsky & Bauch, 2024). Recent work has focused on developing more tractable bounds for the IB objective and proving rigorous learning theory guarantees that connect the minimization of I(X; Z) to better generalization bounds (Westphal et al., 2025; Hellstrom et al., 2025). Our framework builds on this lineage, using the IB objective not just as an analytical tool but as the formal basis for defining and distinguishing the computational goals of generalization and memorization. Inference-Time Decoding and Activation Steering. Standard decoding methods like greedy search, beam search, and nucleus sampling primarily manipulate the final probability distribution (Shi et al., 2024). More advanced adaptive decoding methods can alter their strategy based on model confidence or other signals, for instance, by using early-exiting from intermediate layers or injecting phrases to encourage continued reasoning (Wei et al., 2025; Jin et al., 2025). Activation steering aims to reverse-engineer the specific computations and circuits within neural networks (Turner et al., 2023; Sharkey et al., 2025). Recent technique involves directly modifying activations at inference time to control high-level model behaviors like honesty or sentiment (OBrien et al., 2025; Zhang et al., 2025a; Wang et al., 2025). Our work adopts this as the self-contrastive control mechanism by making the steering dynamic and adaptive, triggered and scaled by real-time signal from targeted mode probe. 3. Theoretical View To move beyond descriptive account of generalization and memorization, the Information Bottleneck (IB) principle reframes the goal of learning not just as prediction, but as the creation of an efficient, compressed representation of the world (Tishby & Zaslavsky, 2015; Kawaguchi et al., 2023). The dichotomy between generalization and memorization can be understood as trade-off between the complexity and utility of models internal representations. 3.1. The Information Bottleneck Principle The IB principle posits that an optimal representation should compress an input variable as much as possible while retaining the maximum possible information about relevant target variable . In the context of neural network, is the model input, is the target label, and the representation corresponds to the activations of an intermediate layer l, such that = ϕl(X). The goal is to find an encoder, defined by the conditional probability distribution p(zx) that solves the following constrained optimization problem: min p(zx) I(X; Z) s.t. I(Z; ) Imin Here, I(X; Z) is the mutual information between the input and the representation Z, quantifying the complexity of the representation, that how much information it retains about the input. I(Z; ) is the mutual information between the representation and the target , quantifying the representations utility or predictive power. This constrained op2 Generalization or Memorization: Dynamic Decoding for Mode Steering timization can be solved by maximizing the IB Lagrangian: LIB = I(Z; ) βI(X; Z) where β is Lagrange multiplier that controls the trade-off between predictive utility and representational compression. 3.2. Formalizing Reasoning Modes As shown in Figure 1, the IB framework provides natural interpretation to formalize the distinction between generalization and memorization, arising from different strategies for navigating the complexity-utility trade-off defined by the IB Lagrangian. Figure 1. The information plane illustrating generalization and memorization modes, framing the two reasoning modes as distinct solutions within the IB optimization landscape. Generalization as an IB-Optimal Solution: generalizing model learns representation, Zg, that approximates minimal sufficient statistic. This means it aggressively compresses the input X, discarding spurious correlations, noise, and irrelevant distracting features, thereby achieving low complexity I(X; Zg). Simultaneously, it retains only the core information necessary to predict the target , resulting in high utility I(Zg; ). This solution is efficient and robust because it captures the underlying structure of the data-generating process which lies on or near the theoretical IB boundary representing maximally efficient encoding strategy. Memorization as an IB-Suboptimal Solution: memorizing model prioritizes fitting the training data perfectly 3 at the expense of representational efficiency. It learns representation, Zm, that fails to compress the input, instead creating high-fidelity copy. This leads to high complexity I(X; Zm), as the representation contains not only the taskrelevant signal but also all the noise and irrelevant details of the specific training examples. While this high-complexity representation can achieve high utility I(Zm; ) on the training set, it is an inefficient, brute-force solution. It corresponds to point on the IB plane that is far from the optimal trade-off curve, representing poor solution to the IB objective. This solution is brittle because it has not learned the underlying data structure and thus fails when presented with novel inputs. From Compression to Generalization: The link between compression and generalization can be established from IB objective. Using the chain rule for mutual information, we can decompose the complexity term I(X; Z) as follows: I(X; Z) = I(Y ; Z) + I(X; ZY ) Here, I(Y ; Z) represents the information in that is relevant to the task. I(X; ZY ) represents the information in about the input that is not explained by the target , which can be interpreted as the amount of task-irrelevant information (e.g., noise, stylistic details, instance-specific artifacts) that the representation preserves. Substituting this decomposition back into the IB Lagrangian gives: LIB(Z) = (1 β)I(Z; ) βI(X; ZY ) model that memorizes fails precisely because it retains high degree of task-irrelevant information which makes it suboptimal solution to the IB objective and leads to poor performance on unseen data. We can see why compression is principled path to generalization. Now critical question arises: if the IB principle describes properties of the learned model shaped during training, can we find an inference-time intervention that provides appropriate solution? The training process does not yield model that operates in single, fixed mode. Instead, it shapes static, highdimensional weight landscape that contains the circuits and representations for both generalization and memorization. The model learns general rules from patterns in the data, while also learns to store specific, high-frequency examples. These correspond to different regions or basins of attraction within the models vast state space. Inference is dynamic process. Given new input, the models forward pass constitutes trajectory through its internal activation space. The path of this trajectory determines which computational circuits are engaged and, ultimately, the final output. model may default to trajectory that leads to memorization-based solution, even if the capacity for generalization-based solution exists within its Generalization or Memorization: Dynamic Decoding for Mode Steering weights. This is because the memorized solution may be cheaper or more readily accessible path for certain inputs. We thus possible to find targeted nudge on the models state during the inference-time trajectory, bridging the gap by treating the trained model as system with multiple latent capabilities and providing control mechanism to select the desired one at runtime. We provide detailed Theoretical Foundations in Appendix A.2. 4. Methodology The Information Bottleneck theory provides formal understanding of generalization and memorization as distinct representational strategies. Building upon this foundation, practical algorithm is needed to actively influence which strategy the model employs at inference time. Dynamic Mode Steering (DMS) is proposed to achieve this control. DMS operates as two-stage, closed-loop process: it first identifies the models current reasoning mode and then intervenes to steer its computation toward the more desirable generalization mode. 4.1. Stage 1: Mode Identification To enable targeted intervention, reliable signal indicating the models internal state is required. For DMS, we design lightweight linear probing classifier which serves as simple, auxiliary model trained to predict memorization property {0,1} based on representations. Its simplicity reduces the risk that the probe itself learns complex capabilities independent of the host model; also, the information it detects is linearly decodable from the activations, suggesting it is explicitly represented. The core challenge is to generate labeled dataset of activations corresponding to each reasoning mode. We achieve this by leveraging the hypothesis that memorized outputs exhibit peaked occurrence output distribution in multiqueries, whereas generalized reasoning produces more diverse outputs. We create two distinct sets of prompts: Memorization-Eliciting Prompts (PM ): This set contains prompts designed to trigger verbatim recall. Examples include direct quotes from famous texts, trivia questions with single, well-known answer, and programming problems from benchmarks known to contaminate training sets (e.g., HumanEval). Generalization-Eliciting Prompts (PG): This set contains prompts that require novel, multi-step reasoning. Examples include complex mathematical word problems, abstract reasoning tasks, and commonsense questions posed in novel scenarios. For each prompt in PM PG, we generate set of output samples {s1, s2, ..., sN } using nucleus sampling (e.g., = 50, temperature = 0.8) to encourage diversity. We then compute the average pairwise token-level edit distance among all samples: If PM and the average edit distance is below τlow, indicating highly similar, repetitive outputs, we label the corresponding internal activations as is memorizing = 1. If PG and the average edit distance is above τhigh, indicating diverse, varied outputs, we label the corresponding internal activations as is memorizing = 0. Prompts that do not result in clear low or high diversity score are discarded to ensure clean training signal for the probe. With the labeled dataset of (activation vector, label) pairs, we train standard logistic regression classifier. At inference time, for new input, the trained probe is applied to the LLMs internal activation, outputting continuous memorization score, m. This score represents the probes confidence that the model is currently operating in memorization mode. While significant limitation of standard probing is that it only reveals correlations; probe may detect feature that is present in the activations but is not actually used by the model to produce its final output. To build robust control algorithm, the intervention must target components that are causally responsible for the behavior of interest. We design activation patching to identify the optimal layer for both probing and intervention. 4.2. Activation Patching Activation patching is causal method that isolates the functional role of specific model components by swapping activations between two carefully chosen model runs. The process is as follows: Define Inputs: xclean is Clean Input that reliably elicits correct, generalized response. For example, novel math problem from held-out portion of the GSM8K dataset where the model correctly performs multi-step reasoning. xcorr, as Corrupted Input, is designed to elicit specific, incorrect, memorized response. For example, question from TruthfulQA where the common, memorized answer is misconception. Cache Activations: The model is run on xclean, and all internal activations specifically the residual stream states after each transformer block are saved into cache clean. The output is verified to be the correct, generalized answer. The model is then run on xcorr, and its activations are saved Generalization or Memorization: Dynamic Decoding for Mode Steering to cache corr. The output is verified to be the incorrect, memorized answer. Patching Intervention: The model is run third time on the corrupted input xcorr while intervened at each layer during forward pass. At layer l, we replace the corrupted activation ϕl(xcorr) with the clean activation ϕl(xclean) from cache clean. The forward pass then continues from layer l+1 with this patched activation. We measure the effect of each patch on the models final output probability for the correct answer. The layer where patching the clean activation is sufficient to flip the models output from the incorrect, memorized answer to the correct, generalized answer is identified as causally critical junction point. This analysis reveals the layer where the computational circuits for generalization are most influential. This causally validated layer is then selected as the single, optimal location for both training the memorization probe and applying the steering intervention, ensuring our algorithm targets component that is demonstrably responsible for the desired behavior. 4.3. Stage 2: Mode Control Once the probe detects high likelihood of memorization, DMS intervenes using activation steering, directly modifies the models internal activations during the forward pass to influence its behavior without altering its weights. single, static generalization-steering vector vg is computed offline using the labeled activation dataset from Stage 1. This vector is defined as the difference between the mean activation vector for all examples in the generalization class and the mean activation vector for all examples in the memorization class, at the causally-identified layer l: vg = ExGeneralizing[ϕl (x)] ExMemorizing[ϕl (x)] This vector vg represents direction in the high-dimensional activation space that points away from the centroid of memorization-associated representations and towards the centroid of generalization-associated representations. During inference, this steering vector is added to the models residual stream at the target layer l. Crucially, the magnitude of the added vector is scaled by the real-time memorization score provided by the probe: ϕsteered = ϕoriginal + α vg vg Here, ϕoriginal is the original activation at layer l, ϕsteered is the modified activation, and α is global hyperparameter that controls the maximum steering strength. This dynamic scaling ensures that the intervention is proportional to the detected need. When the model is confidently generalizing (m 0), the intervention is negligible. When the model is strongly relying on memorization (m 1), full-strength correction is applied to guide its computation back towards more robust pathway. DMS can be viewed as performing more targeted, selfcontrastive operation, therefore formalized as novel adaptive decoding strategy. Instead of operating exclusively on the final logit distribution or contrasting against separate model, DMS contrasts the models default computational path against steered, pro-generalization path. The steering vector vg is itself derived from contrast between the two modes µg µm, then applies this contrastive signal to the models internal state, making the intervention more precise and effective. 5. Experiment To assess the efficacy of the DMS framework, series of experiments were designed to evaluate its impact on tasks that critically depend on the distinction between generalization and memorization. We aim to answer the following key questions: ❶ Does DMS improve performance on complex, multi-step reasoning tasks where rote memorization is insufficient for success? ❷ Does DMS reduce the models propensity to generate memorized falsehoods, thereby increasing its factual accuracy and faithfulness? ❸ How robust is the performance of DMS to variations in its key hyperparameters, such as the choice of intervention layer and the steering strength? 5.1. Experimental Setup Models. To evaluate the scalability and general applicability of DMS, experiments were conducted using the Llama-3 family (Grattafiori et al., 2024), specifically the 8B and 70B parameter variants. Benchmarks. curated set of benchmarks was selected to probe the two target behaviors. ❶ Reasoning and Commonsense: GSM8K (Cobbe et al., 2021): benchmark consisting of grade-school math problems. Success on GSM8K requires robust multi-step logical and arithmetic reasoning, making it strong test for generalization capabilities. Performance is measured by Pass@1 accuracy. HellaSwag (Zellers et al., 2019): commonsense reasoning benchmark that involves completing sentence by choosing the most plausible ending from options. It is designed to be difficult for models that rely on surface-level statistical patterns, thus testing for deeper Generalization or Memorization: Dynamic Decoding for Mode Steering generalization. Performance is measured by Accuracy. ❷ Faithfulness and Factual Accuracy: Table 1. Performance on Reasoning and Commonsense Benchmarks. TruthfulQA (Lin et al., 2022): benchmark designed to measure models truthfulness by testing its ability to avoid generating answers based on common human misconceptions and falsehoods prevalent on the internet. This directly tests the models ability to overcome memorized, incorrect information. Performance is measured by the % True & Informative metric, which rewards answers that are both factually correct and comprehensive. Baselines. The performance of DMS was compared against suite of strong and widely used decoding methods: Greedy Decoding: deterministic baseline that selects the token with the highest probability at each step. Nucleus Sampling: standard stochastic decoding method that samples from the smallest set of tokens whose cumulative probability exceeds certain threshold (p=0.9). It introduces diversity while avoiding highly improbable tokens. Contrastive Decoding: strong inference-time baseline which comprehensively compare the amateur model prediction distribution and adjust the expert model prediction, aiming at improving generation quality by contrasting model behaviors. See Appendix A.1 for Implementation Details including training data, causal layer, and hyperparameter selection. The experimental results demonstrate that Dynamic Mode Steering provides substantial improvements in both complex reasoning and factual accuracy, outperforming all baseline methods across both model scales. 5.2. Reasoning Task As shown in Table 1, on GSM8K, which demands robust, multi-step generalization, DMS yields the most significant gains. For Llama-3 8B, DMS achieves Pass@1 of 68.3 (+6.2%). This pattern is even more pronounced for the larger Llama-3 70B, where DMS achieves 86.7 (+5.2%), outperforming Contrastive Decoding by 3.6%. This result strongly supports the hypothesis that by actively identifying and steering the model towards its generalization circuits, DMS can unlock more reliable and accurate reasoning capabilities. The improvements on HellaSwag, while more modest, are consistent and further indicate an enhanced ability to handle commonsense inference. MODEL LLAMA-3 8B LLAMA-3 70B METHOD GSM8K HELLASWAG GREEDY NUCLEUS SAMPLING CONTRASTIVE DECODING DMS (OURS) GREEDY NUCLEUS SAMPLING CONTRASTIVE DECODING DMS (OURS) 62.1 60.8 64.5 68.3 81.5 80.2 83.1 86.7 85.3 84.9 86.1 87.5 92.4 92.1 93.0 94.2 5.3. Faithfulness Task As shown in Table 2, the results on TruthfulQA validate the second core claim of DMS to suppress memorized falsehoods. The task is specifically designed common, easily memorized answers while are often factually incorrect. Models relying on memorization strategy are thus penalized. DMS consistently achieves the highest scores, improving by 6.4% for the 8B model and 5.4% for the 70B model over baseline. This demonstrates that the DMS probe is successfully identifying instances where the model is likely to regurgitate common falsehood, and the steering mechanism is effectively intervening to guide the model towards more factually grounded and nuanced response. Table 2. Performance on Factual Accuracy and Faithfulness Benchmarks. MODEL LLAMA-3 8B LLAMA-3 70B METHOD TRUTHFULQA GREEDY NUCLEUS SAMPLING CONTRASTIVE DECODING DMS (OURS) GREEDY NUCLEUS SAMPLING CONTRASTIVE DECODING DMS (OURS) 55.4 54.9 57.2 61.8 68.9 68.1 70.5 74.3 6. Discussion 6.1. Ablation Study To validate the design of DMS, two key ablation studies were conducted. Intervention Layer: The performance of DMS on GSM8K was measured while varying the layer at which the probe and steering vector were applied. As shown in Figure 2, clear performance peak around the mid-to-post layers, with the absolute best performance occurring at layer 22, identified via the offline causal tracing. Applying the in6 Generalization or Memorization: Dynamic Decoding for Mode Steering tervention at early or post layers resulted in significantly degraded performance. This finding provides strong empirical validation for the causal analysis methodology, confirming that targeting the causally junction point is essential for the effectiveness. Figure 2. Impact of Intervention Layer on DMS Performance. The plot illustrates the Pass@1 accuracy of the DMS algorithm on GSM8K as function of the transformer layer at which the probing and steering interventions are applied for Llama-3 8B. Steering Strength (α): The steering strength hyperparameter α, was varied to assess its impact on performance. Figure 3 revealed concave relationship, as α grows from zero, indicating the benefit of the steering intervention, while after the optimal point, further increasing the steering strength leads to decline. This suggests that while steering is beneficial, an overly aggressive intervention can push the models activations into an out-of-distribution region of the manifold, disrupting its learned computations. This highlights the importance of tuning α and supports the adaptive nature of DMS, where the intervention magnitude is also scaled by the probes confidence. Figure 3. Impact of Steering Strength (α) on DMS Performance. 7 6.2. Qualitative Analysis Qualitative examples further illuminate the mechanisms by which DMS improves model behavior. 7. Conclusion This work has addressed the critical challenge of managing the dual-mode nature of Large Language Models, which unpredictably switch between robust generalization and brittle memorization. We first introduce formal theoretical framework based on the Information Bottleneck, providing principled explanation for empirical phenomena like grokking, recasting them as an optimization process on the IB landscape. Building on this theory, we propose Dynamic Mode Steering (DMS). DMS is novel, training-free, inferencetime control mechanism that operates in closed loop. It uses causally-grounded linear probe to dynamically identify the models reliance on memorization and then applies targeted activation steering intervention to guide the models computation towards more generalizable circuits. This entire process was formalized as an efficient, adaptive, selfcontrastive decoding strategy. Experimental validation on the Llama-3 family of models demonstrated that DMS significantly outperforms strong baselines on benchmarks for both complex reasoning (GSM8K) and factual faithfulness (TruthfulQA). Example 1: TruthfulQA Factual Faithfulness Prompt: What is the medical consensus on what happens if you crack your knuckles lot? Baseline (Greedy): Cracking your knuckles lot can lead to arthritis and other joint problems. It is harmful habit that should be avoided. DMS Output: The medical consensus is that cracking your knuckles does not cause arthritis. While the sound can be annoying to others and may lead to minor issues like reduced grip strength in some cases, there is no evidence linking it to serious joint damage like arthritis. Analysis: The baseline model retrieves widely memorized but factually incorrect piece of folk wisdom. The DMS probe likely assigned high memorization score (m) to this context. The subsequent steering intervention pushed the models internal state away from this simple, recalled association, prompting it to access more nuanced and accurate representation of the medical facts. Generalization or Memorization: Dynamic Decoding for Mode Steering Example 2: GSM8K Logical Reasoning Prompt: bakery sells muffins for $3 each. On Monday, they sold 45 muffins. On Tuesday, they sold 15 fewer muffins than on Monday. On Wednesday, they sold twice as many muffins as they did on Tuesday. What were the total earnings from muffins on all three days? Baseline (Contrastive Decoding): On Monday, they sold 45 muffins. On Tuesday, they sold 45 - 15 = 30 muffins. On Wednesday, they sold 2 * 45 = 90 muffins. Total muffins = 45 + 30 + 90 = 165. Total earnings = 165 * 3 = $495. The answer is 495. (Incorrect step for Wednesday) DMS Output: Lets break this down step-bystep. Monday sales: 45 muffins. Tuesday sales: 45 - 15 = 30 muffins. Wednesday sales: 2 * 30 = 60 muffins. Total muffins sold: 45 + 30 + 60 = 135 muffins. Total earnings: 135 * 3 =405. The final answer is 405. Analysis: The baseline model makes common reasoning error, incorrectly using the Monday sales figure for the Wednesday calculationa form of attentional slip that can be viewed as failure of robust generalization. DMS, by promoting more structured, step-by-step computational path associated with its generalization circuits, correctly executes each logical step, leading to the correct final answer. This highlights how DMS can improve not just factual recall but the procedural integrity of complex reasoning."
        },
        {
            "title": "Impact Statement",
            "content": "The ability to understand, identify, and control the internal reasoning modes of LLMs has significant implications for AI safety and reliability. By providing mechanism to preferentially activate generalization circuits, DMS offers path towards building more trustworthy AI systems that are less likely to regurgitate memorized falsehoods or fail on novel inputs. The framework presented here, which combines information theory with causal, mechanistic interventions, represents step towards more rigorous and engineeringdriven approach to model alignment. Rather than treating models as inscrutable black boxes, this work treats them as complex but ultimately understandable systems whose internal computations can be analyzed and controlled."
        },
        {
            "title": "References",
            "content": "Bayat, R., Pezeshki, M., Dohmatob, E., Lopez-Paz, D., and Vincent, P. The pitfalls of memorization: When In The Thirteenth memorization hurts generalization. International Conference on Learning Representations, 2024. Chang, H., Park, J., Ye, S., Yang, S., Seo, Y., Chang, D.-S., and Seo, M. How do large language models acquire factual knowledge during pretraining? Advances in neural information processing systems, 37:6062660668, 2024. Chatterjee, S. Learning and memorization. In International conference on machine learning, pp. 755763. PMLR, 2018. Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q. V., Levine, S., and Ma, Y. Sft memorizes, rl generalizes: comparative study of foundation model post-training. In Forty-second International Conference on Machine Learning, 2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dong, Y., Jiang, X., Liu, H., Jin, Z., Gu, B., Yang, M., and Li, G. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 1203912050, 2024. Dong, Y., Jiang, X., Zhang, X., Liu, H., Jin, Z., Gu, B., Yang, M., and Li, G. Generalization or memorization: Evaluating data contamination for large language models, Jan 2025. URL https://www.researchgate.net/ publication/387596869_Generalization_ or_Memorization_Evaluating_Data_ Contamination_for_Large_Language_ Models. Fan, S., Pascanu, R., and Jaggi, M. Deep grokking: Would deep neural networks generalize better? arXiv preprint arXiv:2405.19454, 2024. Feldman, V. Does learning require memorization? short tale about long tail. In Proceedings of the 52nd annual ACM SIGACT symposium on theory of computing, pp. 954959, 2020. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Generalization or Memorization: Dynamic Decoding for Mode Steering Hellstrom, F., Durisi, G., Guedj, B., Raginsky, M., et al. Generalization bounds: Perspectives from information theory and pac-bayes. Foundations and Trends in Machine Learning, 18(1):1223, 2025. Pearce, A., Ghandeharioun, A., Hussein, N., Thain, N., Wattenberg, M., and Dixon, L. Do machine learning models memorize or generalize. People+ AI Research, 2023. Jin, H., Yeom, J. W., Bae, S., and Kim, T. well, keep thinking: Enhancing LLM reasoning with adaptive injection decoding. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 998910018, 2025. Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V. Grokking: Generalization beyond overfitarXiv preprint ting on small algorithmic datasets. arXiv:2201.02177, 2022. Kang, K., Setlur, A., Ghosh, D., Steinhardt, J., Tomlin, C., Levine, S., and Kumar, A. What do learning dynamics arXiv reveal about generalization in llm reasoning? preprint arXiv:2411.07681, 2024. Sharkey, L., Chughtai, B., Batson, J., Lindsey, J., Wu, J., Bushnaq, L., Goldowsky-Dill, N., Heimersheim, S., Ortega, A., Bloom, J., et al. Open problems in mechanistic interpretability. arXiv preprint arXiv:2501.16496, 2025. Kawaguchi, K., Deng, Z., Ji, X., and Huang, J. How does information bottleneck help deep learning? In International conference on machine learning, pp. 1604916096. PMLR, 2023. Kumar, T., Bordelon, B., Gershman, S. J., and Pehlevan, C. Grokking as the transition from lazy to rich training dynamics. In The twelfth international conference on learning representations, 2023. Lewandowsky, J. and Bauch, G. Theory and application of the information bottleneck method, 2024. Li, J., Li, G., Zhang, X., Dong, Y., and Jin, Z. Evocodebench: An evolving code generation benchmark aligned with real-world code repositories. arXiv preprint arXiv:2404.00599, 2024. Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3214 3252, 2022. Liu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., and Williams, M. Towards understanding grokking: An effective theory of representation learning. Advances in Neural Information Processing Systems, 35:34651 34663, 2022. Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., and Mian, A. comprehensive overview of large language models. ACM Transactions on Intelligent Systems and Technology, 16 (5):172, 2025. OBrien, K., Majercak, D., Fernandes, X., Edgar, R. G., Bullwinkel, B., Chen, J., Nori, H., Carignan, D., Horvitz, E., and Poursabzi-Sangdeh, F. Steering language model refusal with sparse autoencoders. In ICML 2025 Workshop on Reliable and Responsible Foundation Models, 2025. Shi, C., Yang, H., Cai, D., Zhang, Z., Wang, Y., Yang, Y., and Lam, W. thorough examination of decoding methods in the era of llms. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 86018629, 2024. Tishby, N. and Zaslavsky, N. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), pp. 15. Ieee, 2015. Turner, A. M., Thiergart, L., Leech, G., Udell, D., Vazquez, J. J., Mini, U., and MacDiarmid, M. Steering language models with activation engineering. arXiv preprint arXiv:2308.10248, 2023. Wang, B., Yue, X., Su, Y., and Sun, H. Grokked transformers are implicit reasoners: mechanistic journey to the edge of generalization. In ICML 2024 Workshop on Mechanistic Interpretability, 2024a. Wang, M., Yao, Y., Xu, Z., Qiao, S., Deng, S., Wang, P., Chen, X., Gu, J.-C., Jiang, Y., Xie, P., et al. Knowledge mechanisms in large language models: survey and perspective. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 70977135, 2024b. Wang, W., YANG, J., and Peng, W. Semantics-adaptive activation intervention for llms via dynamic steering vectors. In The Thirteenth International Conference on Learning Representations, 2025. Wang, X., Antoniades, A., Elazar, Y., Amayuelas, A., Albalak, A., Zhang, K., and Wang, W. Y. Generalization vs memorization: Tracing language models capabilities back to pretraining data. In The Thirteenth International Conference on Learning Representations, 2024c. Wei, Z., Chen, W.-L., Zhu, X., and Meng, Y. Adadecode: Accelerating llm decoding with adaptive layer parallelism. In Forty-second International Conference on Machine Learning, 2025. 9 Generalization or Memorization: Dynamic Decoding for Mode Steering Westphal, C., Hailes, S., and Musolesi, M. generalized information bottleneck theory of deep learning. arXiv preprint arXiv:2509.26327, 2025. Xie, C., Huang, Y., Zhang, C., Yu, D., Chen, X., Lin, B. Y., Li, B., Ghazi, B., and Kumar, R. On memorization of large language models in logical reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. Xu, Y., Zhang, X., Yeh, M.-H., Dhamala, J., Dia, O., Gupta, R., and Li, Y. Simulating and understanding deceptive behaviors in long-horizon interactions. arXiv preprint arXiv:2510.03999, 2025. Yin, M., Tucker, G., Zhou, M., Levine, S., and Finn, C. Meta-learning without memorization. In International Conference on Learning Representations, 2019. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, 2019. Zhang, X., Chen, Y., Yuan, Y., and Huang, M. Seeker: Enhancing exception handling in code with llm-based multi-agent approach. arXiv preprint arXiv:2410.06949, 2024. Zhang, X., Chen, Y., Yeh, M.-H., and Li, Y. Cognition-ofthought elicits social-aligned reasoning in large language models. arXiv preprint arXiv:2509.23441, 2025a. Zhang, X., Chen, Y., Yeh, S., and Li, S. Metamind: Modeling human social thoughts with metacognitive multiagent systems. Advances in Neural Information Processing Systems, 2025b. 10 Generalization or Memorization: Dynamic Decoding for Mode Steering A. Appendix A.1. Implementation Details Probe Training Data: The linear probe for DMS was trained on dataset constructed from the training splits of the evaluation benchmarks. The generalization class consisted of activations from the Llama-3 models processing GSM8K problems. The memorization class consisted of activations from the models processing TruthfulQA questions, where the prompts are designed to trigger common, memorized falsehoods. To ensure label purity, we used the outputdiversity heuristic described in Section 4.1 to filter and label samples, retaining only those exhibiting clear low diversity (memorization) or high diversity (generalization). Causal Layer Identification: The activation patching analysis was performed on small, held-out set of prompts containing examples from both GSM8K and TruthfulQA to ensure the analysis covered both target behaviors. For the Llama-3 architecture, this analysis consistently identified mid-to-late layer (specifically, layer 22 for the 8B model and layer 55 for the 70B model) as being causally critical for switching between reasoned and recalled outputs. This layer was subsequently used for all probe training and vector steering interventions. Hyperparameter Tuning: The key hyperparameters for DMS, the steering strength α and the self-contrastive interpolation factor λ, were tuned on small validation set composed of examples from reasoning and faithfulness tasks. We performed grid search to identify the optimal value of α on the validation set, showing that α = 1.4 provides robust balance between effective guidance and over-intervention. A.2. Theoretical Foundations The Information Bottleneck principle begins as the constrained optimization problem : min p(zx) I(X; Z) s.t. I(Z; ) Imin To solve this, we employ the method of Lagrange multipliers. We rewrite the constraint as Imin I(Z; ) 0. The Lagrangian function is then defined as: (p(zx)) = I(X; Z) + β(Imin I(Z; )) where β 0 is the Lagrange multiplier. Minimizing is equivalent to solving the original problem. Since Imin is constant, minimizing is equivalent to minimizing I(X; Z) βI(Z; ). This is, in turn, equivalent to maximizing its negative, LIB = I(Z; ) βI(X; Z). This function is the IB Lagrangian used in the main text, which converts constrained optimization into an unconstrained problem with trade-off parameter β. As derived in Section 3.2, the core objective of the IB principle can be made more explicit. The complexity term I(X; Z) can be decomposed using the chain rule of mutual information: I(X; Z) = I(X, ; Z) I(Y ; ZX). Given the Markov chain Z, we have I(Y ; ZX) = 0. The joint mutual information can be further expanded: This gives the key decomposition: I(X, ; Z) = I(Y ; Z) + I(X; ZY ) I(X; Z) = I(Y ; Z) + I(X; ZY ) Substituting this into the IB Lagrangian LIB = I(Z; ) βI(X; Z) yields: LIB(Z) = I(Z; ) β(I(Y ; Z) + I(X; ZY )) = (1 β)I(Z; ) βI(X; ZY ) This result formally proves that optimizing the IB objective is not just about compressing the input in general, but specifically about compressing the task-irrelevant components of the input, as quantified by the conditional mutual information I(X; ZY ). representation that contains large amount of information about the input that is useless for predicting the output (e.g., noise, dataset artifacts) is penalized by the IB objective. This directly connects the informationtheoretic goal of compression with the statistical learning goal of generalization. The training of deep neural network via stochastic gradient descent (SGD) results in set of fixed weights, θ, that define static function. This function maps an input to an output through series of intermediate activations Zl. The IB theory 11 Generalization or Memorization: Dynamic Decoding for Mode Steering helps characterize the properties of these activations. trained model can be viewed as system that contains multiple potential computational pathways, learned from the training data. Some pathways may correspond to abstract, generalizable rules with low I(X; ZY ), while others may correspond to rote memorization of specific training examples with high I(X; ZY ). Inference is the process of passing specific input through this fixed system. The resulting activation trajectory, z1, z2, . . . , zL, is deterministic path for given x. The core premise of our work is that this default path may not be the optimal one. The model might be drawn into memorization pathway due to superficial features in the input, even though the weights also encode more robust generalization pathway. Inference-time intervention is justified because it operates on the state of the system, not its learned parameters. The model has already learned the necessary circuits for generalization; the challenge is to activate them reliably. Our steering vector vg represents direction in activation space that points from the centroid of memorization-associated states towards the centroid of generalization-associated states. By adding αmvg to the activation at causal layer, we apply small perturbation to the inference trajectory, nudging it out of memorization basin and towards generalization basin. This is targeted, computationally cheap method to leverage the full capabilities learned during training, without requiring costly retraining or fine-tuning. The information-theoretic concept of compression has concrete geometric analogue in the phenomenon of Neural Collapse. Neural collapse describes the observation that in the terminal phase of training, the hidden-layer representations of all inputs belonging to the same class collapse towards their class mean. This geometric collapse is directly linked to information compression: reduction in the within-class variance of representations means that many distinct inputs xi, xj from the same class are mapped to nearly identical representations zi zj. This mapping necessarily discards the instance-specific information that distinguishes xi from xj, thereby reducing the overall mutual information I(X; Z). More formally, the superfluous information in representation is bounded by the within-class variance. This provides powerful, tangible target for our intervention. The abstract goal of promoting generalization becomes the concrete geometric goal of promoting neural collapse. Our steering vector, vg = µg µm, is explicitly designed to push an activation away from the center of the memorization cluster (µm) and towards the center of the generalization cluster (µg). In doing so, it encourages the models internal state to adopt the collapsed geometric structure characteristic of compressed, generalizing representation. This connects our information-theoretic framework to mechanistic, geometric process that can be directly manipulated. A.3. Future Work The DMS framework presented in this paper opens several avenues for future research. key direction is the development of unsupervised methods for discovering behavior vectors. Instead of relying on labeled prompts, techniques like sparse dictionary learning or principal component analysis could be applied to model activations to automatically identify dominant computational axes corresponding to behaviors like memorization, generalization, or even more nuanced concepts like honesty and harmlessness. Another promising area is extending DMS to multi-attribute control, where multiple probes and steering vectors could be used simultaneously to fine-tune model behavior along several axes at once. Finally, further theoretical work is needed to deepen the connection between the Information Bottleneck principle and the geometry of activation space. Understanding how the IB trade-off is physically realized in the representational manifolds of transformers could lead to even more powerful and principled methods for model control."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Wisconsin-Madison, Madison, USA",
        "Stanford University, Palo Alto, USA"
    ]
}