{
    "paper_title": "An Empirical Study of Qwen3 Quantization",
    "authors": [
        "Xingyu Zheng",
        "Yuye Li",
        "Haoran Chu",
        "Yue Feng",
        "Xudong Ma",
        "Jie Luo",
        "Jinyang Guo",
        "Haotong Qin",
        "Michele Magno",
        "Xianglong Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 4 1 2 2 0 . 5 0 5 2 : r An Empirical Study of Qwen3 Quantization Xingyu Zheng*1, Yuye Li*2, Haoran Chu*1, Yue Feng*1, Xudong Ma1, Jie Luo1 , Jinyang Guo1 , Haotong Qin3 , Michele Magno3 , Xianglong Liu1 1Beihang University 2Xidian University 3ETH Zürich {zhengxingyu,23371505chr,fay777,macaronlin,luojie,jinyangguo,xlliu}@buaa.edu.cn liyueye541@gmail.com {haotong.qin,michele.magno}@pbl.ee.ethz.ch"
        },
        {
            "title": "Abstract",
            "content": "The Qwen series has emerged as leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents promising solution, yet its impact on Qwen3s performance remains underexplored. This study conducts systematic evaluation of Qwen3s robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on GitHub and Hugging Face."
        },
        {
            "title": "Introduction",
            "content": "Developed by Alibaba Group, the Qwen series [1; 17] has rapidly advanced as competitive open-source family of autoregressive large language models (LLMs) based on the Transformer architecture [15]. With its initial release in 2023, Qwen demonstrated exceptional scalability, with even its 7B parameter model rivaling larger proprietary models like GPT-3.5 in certain benchmarks. The recently launched Qwen31, available in configurations from 0.6B to 235B parameters, further elevates performance through refined pre-training on diverse, high-quality corpora. This positions the Qwen family among the most capable open-source LLMs, adaptable to diverse deployment scenarios. Despite its strengths, practical deployment of Qwen3 faces challenges due to high computational and memory demands. Low-bit quantization [16; 5; 9; 6] has emerged as critical technique to mitigate these issues, enabling efficient inference on resource-constrained devices. However, quantization often introduces performance degradation. Qwen3s state-of-the-art capabilities present timely opportunity to reassess quantization techniques, uncovering new insights into their efficacy and limitations for cutting-edge models. In this empirical study, we systematically evaluate Qwen3s robustness under quantization across Post-Training Quantization (PTQ) methods. We test 5 classic methods, including Round-To-Nearest (RTN), GPTQ [5], AWQ [10], SmoothQuant [16] and BiLLM [8] for PTQ, spanning bit-widths *Equal Contribution. Corresponding Author. 1 https://github.com/QwenLM/Qwen3 from 1 to 8 bits. Our evaluation covers diverse language tasks using benchmarks such as Perplexity (WikiText2 [12], C4 [13]), 0-shot Commonsense Reasoning (PIQA [2], ARC-Easy/Challenge [4], HellaSwag [19], Winogrande [14], BoolQ [3]), and 5-shot MMLU [7]. This study aims to: (1) benchmark quantization-induced performance trade-offs, (2) identify optimal methods for specific bit-widths, and (3) highlight unresolved challenges, particularly in ultra-low-bit regimes. We hope our findings will guide future research toward higher accuracy in compressed models, enhancing the practicality of Qwen3 and subsequent LLMs."
        },
        {
            "title": "2 Empirical Study",
            "content": "2.1 Experiment Settings We evaluate low-bit quantization across Qwen3s post-training models (0.6B, 1.8B, 4B, 7B, 14B, and 72B) as well as their pretraining versions (Qwen3-0.6/1.8/4/7/14B-Base), with pre-trained weights sourced from official repositories1. Quantization methods. To comprehensively assess Qwen3s quantization robustness, we select 5 influential post-training quantization (PTQ) methods representing diverse technical approaches. All implementations adhere to their original open-source codebases2. Experiments were conducted on 1NVIDIA A800 80GB GPU to ensure consistent evaluation conditions. Quantization protocol. To ensure fair comparison across all quantization methods, we maintain three key consistency measures: (1) all methods share identical calibration data (128 samples from C4 dataset [13] with sequence length 2048), (2) for per-group quantization, channel-wise grouping adopts 128 block size following established practices in LLM quantization, and (3) weight-only quantization is applied uniformly from 1 to 8 bits. These controlled variables enable direct comparison of quantization method performance while minimizing confounding factors. Meanwhile, in weightactivation quantization methods, activations are quantized to 4 or 8 bits, which are the most commonly used settings, since lower bit-widths typically result in significant performance degradation. Evaluation protocol. For comprehensive PTQ evaluation, we measure perplexity (PPL) on WikiText2 [12] and 256-sample subset of C4 [13] with sequence length of 2048. Zero-shot accuracy is evaluated across six established reasoning benchmarks: PIQA [2], Winogrande [14], ARC-Easy and ARC-Challenge [4], HellaSwag [19], and BoolQ [3]. Few-shot capability is further examined using 5-shot MMLU [7]. This multi-dimensional evaluation framework provides rigorous assessment of the quantized Qwen3s capabilities across various task types and difficulty levels. 2.2 PTQ results We present the detailed experimental results in Table 1,Table 2,Table 3 and Table 4, and provide intuitive visual illustrations based on the data from Table 2, Table 3 and Table 4, as shown in Figure 1, Figure 2, Figure 3 and Figure 4. Impact of weight-only quantization. At 8 bits, Qwen3 consistently maintains near lossless performance, indicating that high-bit quantization still holds strong potential for practical deployment. However, when the bit-width is reduced to 4 bits, all quantization methods exhibit noticeable performance degradation. For example, Qwen-8Bs MMLU score drops from 74.7 to 69.3, as shown in Table 4. As the bit-width further decreases to 3 bits, although AWQ still retains some capability, most of the original models advantages are lost. At 2 bits, only methods like GPTQ, which leverage calibration-based compensation, manage to preserve minimal level of performance. Meanwhile, we observe that Bi-LLM, binarization method, demonstrates relatively promising results, surpassing even the 3-bit AWQ in the 32B model, highlighting the potential of binarization. Impact of activation quantization. When applying SmoothQuant, one of the most classic activation quantization methods, we observe that even under the w8a8 setting, there is already noticeable degradation in performance compared to the fp model. As the bit-width decreases to w4a8, the model suffers significant performance drop, markedly worse than weight-only quantization. This result aligns with recent research findings, suggesting that large models may be particularly sensitive 2 https://github.com/IST-DASLab/gptq, https://github.com/mit-han-lab/llm-awq, https: //github.com/mit-han-lab/smoothquant, https://github.com/Aaronhuang-778/BiLLM Figure 1: PPL of per-group quantization on C4 of Qwen3-Base. Figure 2: Challenge/HellaSwag/Winogrande/BoolQ) of per-group quantization of Qwen3-Base. 0-shot Commonsense Reasoning Accuracy (Average of PIQA/Arc-Easy/Arcto activation quantization, possibly due to activation outliers, leading to substantial performance degradation. Comparison across varying parameter scales. We observe that larger models exhibit greater stability under quantization. Specifically, as shown in Table 4, Qwen3-14B incurs only 1% drop in MMLU performance under 4-bit GPTQ compared to the full-precision model, whereas Qwen3-0.6B suffers drop of around 10% under the same setting, highlighting the ability of larger parameter spaces to mitigate quantization noise. Comparison with LLaMA3. We previously conducted experiments on LLaMA3 using the classical methods [9]. Compared to the prior results on LLaMA3, Qwen3 exhibits more pronounced performance degradation under low-bit quantization (3 bits or fewer). Specifically, in LLaMA3-8B, AWQ with w3a16g128 quantization leads to PPL increase on C4 from 9.2 to only 11.6, whereas in Qwen3-8B-Base, the same AWQ setting increases the PPL from 10.4 to 23.8. This aligns with our previous empirical observations and hypotheses: more thorough pre-training process likely results in fewer redundant representations in stronger LLMs, making them more sensitive to quantization."
        },
        {
            "title": "3 Conclusion",
            "content": "The newly released Qwen3 series has emerged as one of the most capable open-source LLM families, garnering substantial attention from both academia and industry. In this study, we present the first systematic evaluation of Qwen3s robustness under various low-bit quantization schemes, with particular focus on post-training quantization methods. Our investigation seeks to establish practical boundaries for deploying Qwen3 in resource-constrained scenarios through comprehensive quantization analysis. Our experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios. These findings underscore two critical implications: (1) current quantization techniques require further innovation to better preserve Qwen3s advanced capabilities, and (2) the trade-offs between model compression and performance retention need careful reconsideration for state-of-the-art LLMs. We believe our empirical analysis provides valuable guidance for future research directions in LLM quantization, particularly in developing methods that can maintain high accuracy at ultra-low 3 Figure 3: PPL of per-group (AWQ, GPTQ, BiLLM) and per-channel (SmoothQuant) quantization methods on c4 of Qwen3. Figure 4: 0-shot Commonsense Reasoning Accuracy (Average of PIQA/Arc-Easy/ArcChallenge/HellaSwag/Winogrande/BoolQ) per-group (AWQ, GPTQ, BiLLM) and per-channel (SmoothQuant) quantization methods on c4 of Qwen3. bit-widths. As the field progresses, we anticipate these insights will contribute to more efficient deployment of powerful models like Qwen3, ultimately advancing the practical applications of large language models while reducing their computational overhead. Future Work. We plan to evaluate more advanced forms of quantization methods, such as channel reordering-based approaches [18] and rotation-based quantization strategies [11], to assess Qwen3s performance under these techniques, particularly regarding their impact on activation quantization. 4 1.7B-Base 0.6B-Base Table 1: 2 to 8-bits per-channel PTQ results of Qwen3-Base Models. We report ppl on Wikitext2 and c4, 0-shot reasoning tasks and 5-shot mmlu performance. #W denotes the weight quantization bit-width, and #A denotes the activation quantization bit-width. #W #A #G Wiki2() Model / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 4 / 4 / 3 / 2 / 2 / 2 / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 4 / 4 / 3 / 2 / 2 / 2 / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 4 / 4 / 3 / 2 / 2 / 2 / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 4 / 4 / 3 / 2 / 2 / 2 / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 4 / 4 / 3 / 2 / 2 / PiQA Arc Arc HellaS WinoG BoolQ Avg() MMLU 70.0 70.0 70.3 70.1 69.6 65.8 69.6 67.7 63.5 51.6 62.9 52.1 54.5 53.9 75.7 75.8 75.8 75.7 75 71.2 74.3 72.0 69.9 52.7 68.4 51.4 53.5 52.9 78.1 78.1 78.0 78.0 77.0 75.5 77.0 76.6 73.4 51.3 71.7 52.9 52.3 51.0 79.3 79.2 79.1 79.1 78.4 77.6 77.5 78.3 73.9 51.1 74.6 51.8 52.6 50.8 80.5 80.5 80.5 80.5 80.2 76.3 79.9 79.7 72.4 52.7 76.0 53.9 53.4 51.1 Method FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ SmoothQuant SmoothQuant AWQ RTN AWQ GPTQ FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ SmoothQuant SmoothQuant AWQ RTN AWQ GPTQ FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ SmoothQuant SmoothQuant AWQ RTN AWQ GPTQ FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ SmoothQuant SmoothQuant AWQ RTN AWQ GPTQ FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ SmoothQuant SmoothQuant AWQ RTN AWQ GPTQ c4() 17.1 17.1 17.1 17.1 17.5 30.8 20.1 22.9 39.1 4.21E4 49.7 NaN 8.95E6 NaN 13.4 13.4 13.4 13.4 13.8 25.3 14.8 14.9 28.3 2.44E4 22.3 1.65E7 5.39E6 9.49E3 11.6 11.6 11.6 11.6 11.9 14.4 12.6 12.4 18.2 1.13E4 17.7 2.67E7 2.06E7 1.07E4 10.4 10.4 10.4 10.4 11.1 14.5 11.2 11.0 18.4 1.44E4 14.4 7.08E6 8.60E6 2.56E3 9.68 9.69 9.69 9.69 10.0 19.1 10.3 10.3 46.0 2.1E6 13.1 4.62E6 1.19E7 1.14E3 12.7 12.7 12.7 12.7 13.0 24.0 15.6 18.2 31.5 5.25E4 48.9 NaN 1.05E7 5.81E4 9.39 9.41 9.39 9.39 9.65 16.8 10.7 11.0 19.9 2.83E4 17.8 1.96E7 3.35E6 2.27E4 7.90 7.90 7.90 7.89 8.13 10.2 8.79 8.70 13.2 1.22E4 15.0 2.8E7 1.90E7 2.01E4 6.99 7.00 6.99 6.99 7.43 9.90 7.73 7.63 12.3 1.56E4 11.4 9.32E6 1.34E7 4.75E3 6.38 6.38 6.38 6.37 6.62 14.0 7.00 7.11 39.6 1.54E6 9.95 5.3E6 1.41E7 5.69E3 56.5 56.6 56.4 56.4 54.9 51.1 54.8 52.3 45.7 37.8 47.5 36.9 36.9 37.1 65.6 64.2 64.0 63.9 63.7 57.2 61.0 61.9 55.3 36.8 54.5 37.0 36.3 36.8 68.7 68.9 68.9 69.0 68.5 64.0 67.4 67.1 59.6 35.7 55.2 37.4 37.62 36.3 71.3 71.3 71.4 71.4 71.9 66.7 70.6 70.4 61.9 35.5 60.8 38.0 36.4 36.3 73.7 73.6 73.8 73.7 73.2 66.6 72.0 71.8 61.5 35.9 64.9 36.8 36.9 36.4 65.6 65.8 65.0 65.9 65.6 58.9 64.1 59.0 53.6 26.9 50.1 24.7 25.9 25.8 73.2 74.0 73.9 73.8 73.8 64.7 69.1 72.1 61.8 27.2 61.4 24.1 25.1 24.0 79.0 79.0 79.0 79.0 78.3 73.0 78.6 78.4 66.9 26.6 66.2 25.2 24.5 26.3 82.1 81.8 81.8 81.6 80.9 78.5 81.2 81.9 73.1 26.7 72.7 23.8 25.3 24.1 83.5 83.6 83.5 83.5 82.8 76.4 82.2 81.5 69.6 26.0 73.0 24.1 24.7 26. 58.5 59.1 59.2 58.3 57.3 56.0 57.9 57.1 52.9 48.5 53.2 50.0 49.6 48.1 64.2 64.2 64.2 64.2 64 59.9 62.9 63.3 58.2 49.6 56.4 50.9 48.4 51.3 70.0 70.3 70.6 70.6 70.5 66.8 67.6 68.2 62.7 47.4 59.7 51.1 51.6 50.7 72.1 72.5 72.4 72.4 77.5 68.2 72.5 68.7 61.9 49.4 59.1 52.1 48.5 50.0 74.1 74.0 74.2 74.0 73.8 65.4 71.7 73.2 63.2 50.0 62.7 50.5 50.0 51.0 69.7 70.2 69.5 70.0 66.7 63.3 65.3 63.0 63.1 51.0 62.1 44.3 42.3 46.2 79.2 79.5 79.0 78.7 77.6 70.4 75.5 77.1 71.7 41.5 67.2 47.3 43.2 45.1 82.9 82.9 83.5 83.3 82.3 73.9 79.5 79.5 64.1 42.2 48.8 49.3 48.7 42.8 82.9 83.0 82.9 83.0 82.5 71.4 83.0 83.7 66.9 39.6 65.6 52.4 44.2 45.1 86.5 86.6 86.6 86.8 86.2 76.7 83.1 81.8 73.8 40.6 77.0 45.4 44.5 42.3 33.9 33.2 33.4 33.4 32.8 26.9 32.8 30.0 27.3 21.6 25.2 24.1 23.7 23.6 41.5 42.2 41.6 41.6 42.7 34.4 36.9 40.4 32.3 20.8 33.2 23.4 21.9 21.7 48.4 48.5 47.9 48.3 48.3 43.3 48.0 46.9 40.8 20.9 38.3 20.4 23.0 21.7 52.6 52.6 53.3 53.2 52.7 48.7 51.7 52.1 43.0 20.3 40.6 22.4 22.1 22.6 55.6 55.3 55.9 55.6 54.5 51.0 54.2 54.0 43.4 20.7 44.6 21.5 23.2 23.5 52.3 52.4 52.3 52.2 51.7 35.9 47.3 40.4 32.8 23.9 26.5 23.8 24.5 25.0 61.0 60.5 60.6 60.5 60.2 52.5 57.5 53.2 47.9 24.2 40.5 24.7 24.9 24.5 73.0 72.8 72.8 73.0 72.0 65.0 69.2 68.9 63.2 23.6 51.6 22.9 25.3 24.5 76.7 76.6 76.6 76.6 75.5 70.2 73.8 72.7 61.6 24.8 57.7 24.7 24.8 24.5 80.7 80.6 80.7 80.7 79.6 75.7 78.7 78.5 72.7 24.2 70.8 25.4 23.8 24.4 41.1 41.1 41.0 40.9 40.6 35.7 38.8 37.2 34.3 25.2 31.7 25.9 25.5 25.3 49.2 49.3 49.3 49.3 48.8 42.6 47.3 46.5 41.9 25.7 40.3 25.1 25.5 25.8 54.6 54.5 54.5 54.6 54.5 51.6 53.7 52.9 49.6 25.9 46.7 25.5 25.6 25.4 58.9 58.9 58.8 58.9 58.7 56.0 57.5 57.4 52.7 26.0 52.2 25.5 25.5 25.3 61.7 61.8 61.9 61.8 61.4 53.5 60.6 60.6 46.3 25.3 55.9 25.7 25.3 24.8 16 16 16 16 8 16 16 16 8 4 16 16 16 16 16 16 16 16 8 16 16 16 8 4 16 16 16 16 16 16 16 16 8 16 16 16 8 4 16 16 16 16 16 16 16 16 8 16 16 16 8 4 16 16 16 16 16 16 16 16 8 16 16 16 8 4 16 16 16 14B-Base 4B-Base 8B-Base 5 Model Method FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ SmoothQuant SmoothQuant AWQ RTN AWQ GPTQ FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ SmoothQuant SmoothQuant AWQ RTN AWQ GPTQ FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ SmoothQuant SmoothQuant AWQ RTN AWQ GPTQ FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ SmoothQuant SmoothQuant AWQ RTN AWQ GPTQ FP16 RTN AWQ GPTQ SmoothQuant RTN AWQ GPTQ AWQ SmoothQuant SmoothQuant RTN AWQ GPTQ FP16 RTN AWQ GPTQ RTN AWQ GPTQ AWQ RTN AWQ GPTQ 0.6B 1.7B 4B 8B 14B 32B Table 2: 2 to 8-bits per-channel PTQ results of Qwen3 Models #W #A #G Wiki2() / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 4 / 4 / 3 / 2 / 2 / 2 / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 4 / 4 / 3 / 2 / 2 / 2 / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 4 / 4 / 3 / 2 / 2 / 2 / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 4 / 4 / 3 / 2 / 2 / 2 / 16 / 8 / 8 / 8 / 8 / 4 / 4 / 4 / 3 / 4 / 4 / 2 / 2 / 2 / 16 / 8 / 8 / 8 / 4 / 4 / 4 / 3 / 2 / 2 / 2 PiQA Arc Arc HellaS WinoG BoolQ Avg() MMLU 67.3 68.2 67.8 67.5 66.8 63.3 65.1 62.9 61.6 50.5 60.0 51.5 52.7 51.9 72.5 72.3 72.5 72.2 71.3 68.4 71.5 68.2 70.8 52.0 63.5 52.3 54.8 53.7 75.0 74.9 75.1 72.2 73.9 73.2 73.8 74.4 71.5 51.6 66.4 52.0 49.9 52.9 76.4 76.6 80.2 76.7 76.4 75.4 79.4 76.0 73.4 50.5 71.9 52.9 52.6 51.8 80.0 80.0 76.8 80.1 78.6 77.9 76.0 78.8 72.8 75.8 51.2 53.5 52.9 52.2 80.9 80.8 81.3 81.1 71.9 79.8 79.7 75.3 53.2 52.6 52.6 20.9 20.9 20.9 20.9 21.3 37.5 25.8 33.0 49.5 3.35E4 80.4 9.090E7 3.86E7 7.19E5 16.7 16.7 16.7 16.8 16.4 28.7 19.2 21.0 29.6 3.28E4 29.0 1.65E7 6.71E6 1.43E5 13.7 13.6 13.6 16.8 13.7 17.6 16.6 14.5 22.6 9.91E3 33.4 9.31E6 6.53E6 2.06E4 9.71 9.69 9.73 9.70 9.62 12.0 10.5 10.3 12.5 3.36E4 14.9 NaN 4.16E7 4.66E3 8.64 8.63 8.63 8.63 8.69 9.98 9.59 9.16 12.4 11.0 2.16E5 2.05E6 1.26E7 2.28E3 7.61 7.60 7.60 7.60 38.5 8.21 8.33 12.3 4.53E7 2.95E6 1.07E4 c4() 25.4 25.4 25.4 25.4 25.7 42.4 29.9 37.3 52.8 2.29E4 72.9 5.66E7 3.43E7 5.91E5 19.2 19.2 19.3 19.3 19.0 27.8 21.2 22.1 30.2 4.86E4 32.5 1.63E7 7.97E6 3.89E3 16.6 16.6 16.6 19.3 16.6 20.3 18.9 17.5 25.5 1.04E4 29.9 8.17E6 5.55E6 1.25E4 13.3 13.3 13.3 13.3 13.3 15.6 14.2 13.8 16.7 2.29E4 18.5 NaN 2.89E7 2.93E3 12.0 12.0 12.0 12.0 12.1 13.9 13.1 12.6 15.2 15.8 1.99E5 2.75E6 1.18E7 8.75E2 10.8 10.8 10.8 10.8 35.0 11.3 11.4 14.9 5.3E7 2.56E6 3.58E3 52.9 53.1 52.8 52.9 51.2 47.9 50.0 47.8 43.8 37.5 43.2 36.8 37.7 36.2 63.25 61.4 61.6 61.6 58.6 54.4 58.4 58.3 55.8 36.9 48.1 37.6 37.2 37.0 68.2 68.2 68.4 61.6 67.4 64.2 65.3 65.5 58.4 35.8 53.5 37.1 36.8 45.3 71.08 71.2 72.7 71.3 74.0 66.5 73.3 68.6 64.5 37.1 62.8 37.8 36.7 36.1 74.4 74.4 71.2 74.3 74.9 70.7 69.1 72.9 59.6 67.8 37.6 36.4 39.4 36.6 74.4 74.5 74.5 74.3 64.6 73.1 72.5 64.6 38.2 38.1 36.1 47.1 47.0 46.9 47.0 46.3 37.3 43.1 40.0 30.8 24.8 26.4 24.4 25.5 24.5 60.0 59.7 60.0 59.9 58.9 47.9 53.9 52.8 44.1 24.6 35.6 24.4 24.2 24.4 69.7 69.8 69.5 59.9 69.3 63.0 66.0 65.8 59.2 24.6 43.9 24.0 24.7 24.6 74.7 74.6 74.7 74.6 74.0 68.2 71.9 71.6 63.2 24.8 52.2 24.4 24.7 24.2 78.5 78.5 78.5 78.4 77.8 74.7 76.3 75.9 67.1 71.3 24.9 24.5 25.0 24.0 81.2 81.2 81.3 81.3 78.4 79.9 79.1 70.0 25.5 23.6 24. 63.9 63.1 63.3 64.5 60.1 62.6 61.3 59.1 50.0 50.0 57.5 45.8 50.4 44.3 77.6 76.4 77.8 77.0 75.5 72.8 75.3 75.1 70.8 43.2 61.2 48.4 45.0 44.5 85.1 85.0 85.0 77.0 85.0 81.6 81.7 82.7 63.8 41.0 63.6 48.9 47.7 41.8 86.5 86.5 89.4 86.7 86.2 78.9 88.4 84.5 79.1 39.6 78.5 49.8 46.6 41.8 89.4 89.3 86.7 89.1 89.2 86.3 85.8 87.8 75.9 82.2 38.5 43.1 54.6 44.1 86.6 86.5 86.2 86.2 85.8 84.5 84.5 74.5 50.7 51.1 40.4 56.2 56.7 56.0 56.4 52.3 53.0 56.1 54.7 52.2 49.6 50.9 51.5 51.9 49.5 60.9 61.7 61.7 61.6 58.6 55.6 56.7 60.0 53.9 49.5 52.6 51.9 50.4 50.7 65.8 66.3 66.4 61.6 64.7 61.8 63.1 63.9 57.5 47.9 54.6 49.9 51.9 50.4 68.0 67.6 72.8 68.4 68.4 63.6 72.0 66.1 62.1 52.2 62.2 51.2 48.8 49.3 72.9 72.8 68.0 73.3 73.1 68.7 66.1 72.5 56.1 66.3 50.2 48.5 53.4 49.4 73.6 73.0 73.0 72.5 62.9 71.8 67.6 62.1 50.8 51.9 50.4 37.6 37.6 37.5 37.4 37.2 33.3 35.8 34.5 31.6 25.9 30.5 25.5 25.5 25.2 46.0 46.3 46.0 46.2 45.9 41.1 43.5 43.5 40.3 25.8 36.5 25.6 25.3 25.8 52.2 52.2 52.3 46.2 51.7 48.6 50.1 50.1 46.1 25.6 42.0 25.5 25.5 25.4 57.1 57.1 50.9 57.1 57.0 53.8 59.7 55.6 51.7 26.7 54.7 25.3 25.3 25.5 60.9 60.9 57.1 61.0 61.1 58.9 55.6 59.6 48.9 57.0 25.8 25.7 25.4 25.3 63.9 63.9 63.9 63.9 44.4 63.1 62.7 57.3 25.5 25.4 25.6 31.7 31.7 31.8 30.6 31.2 25.6 27.7 27.4 23.7 23.5 21.6 22.5 22.3 21.4 40.1 39.3 39.7 40.1 39.3 32.7 37.1 35.9 32.7 20.7 24.7 22.8 22.4 21.9 50.6 50.4 50.7 40.1 49.5 44.7 46.2 45.4 42.2 22.6 32.6 22.0 22.1 21.3 55.5 55.7 58.7 55.5 55.9 48.5 57.2 49.7 44.3 25.7 41.0 22.3 21.4 22.8 59.0 59.0 55.2 58.3 57.9 51.8 51.7 57.1 37.8 50.5 26.5 23.9 24.3 23.4 57.8 58.2 58.1 57.5 49.7 56.9 57.8 46.5 24.1 22.8 22.1 60.8 61.1 60.4 60.7 59.4 49.8 53.7 48.3 43.9 25.5 38.6 23.7 23.6 25.2 72.4 72.5 72.1 72.6 69.9 55.8 66.3 66.9 58.1 27.2 49.8 24.5 25.0 25.3 80.5 80.3 80.7 72.6 79.5 75.3 76.6 76.6 69.5 25.9 61.7 24.6 23.4 80.1 83.5 83.5 84.3 83.5 82.6 79.1 83.1 80.1 71.0 25.5 68.2 25.3 25.3 25.8 84.3 84.2 83.5 84.2 83.5 80.6 79.2 81.9 65.8 79.0 25.8 24.0 25.6 25.1 84.4 84.5 84.6 84.3 73.0 82.5 82.6 71.7 24.7 24.8 25.3 16 16 16 16 8 16 16 16 8 4 16 16 16 16 16 16 16 16 8 16 16 16 8 4 16 16 16 16 16 16 16 16 8 16 16 16 8 4 16 16 16 16 16 16 16 16 8 16 16 16 8 4 16 16 16 16 16 16 16 16 8 16 16 16 16 8 4 16 16 16 16 16 16 16 16 16 16 16 16 16 6 Table 3: 1 to 8-bits per-group PTQ results of Qwen3-Base Models. We report ppl on Wikitext2 and c4, 0-shot reasoning tasks and 5-shot mmlu performance. #W denotes the weight quantization bit-width, #A denotes the activation quantization bit-width, and #G denotes the group size. Model 0.6B-Base 1.7B-Base 4B-Base 8B-Base 14B-Base Method FP16 AWQ GPTQ AWQ GPTQ AWQ AWQ GPTQ Bi-LLM 1.06 16 FP16 8 AWQ 8 GPTQ 4 AWQ 4 GPTQ 3 AWQ 2 AWQ GPTQ 2 Bi-LLM 1.04 16 FP16 8 AWQ 8 GPTQ 4 AWQ 4 GPTQ 3 AWQ 2 AWQ GPTQ 2 Bi-LLM 1.07 16 FP16 8 AWQ 8 GPTQ 4 AWQ 4 GPTQ 3 AWQ 2 AWQ GPTQ 2 Bi-LLM 1.05 16 FP16 8 AWQ 8 GPTQ 4 AWQ 4 GPTQ 3 AWQ 2 AWQ GPTQ 2 Bi-LLM 1.05 #W #A #G Wiki2() / 16 128 8 128 8 128 4 128 4 128 3 128 2 128 2 128 / 128 128 128 128 128 128 128 128 / 128 128 128 128 128 128 128 128 / 128 128 128 128 128 128 128 128 / 128 128 128 128 128 128 128 128 12.7 12.7 12.7 16.6 14.9 85.9 6.42E7 7.5E3 8.64E4 9.39 9.39 9.38 11.4 9.99 41.8 1.13E7 2.23E2 3.26E3 7.90 7.90 7.89 9.39 8.19 26.3 7.53E6 1.13E2 153 6.99 6.99 6.99 8.11 7.22 22.6 1.66E7 53.1 48.2 6.38 6.38 6.37 7.35 6.65 19.2 2.68E7 27.9 20.9 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 c4() 17.1 17.1 17.1 21.3 19.7 89.3 9.85E7 3.8E3 2.85E4 13.4 13.4 13.4 15.8 14.0 40.4 1.02E7 1.52E2 2.35E3 11.6 11.6 11.6 13.3 11.9 26.4 5.94E6 79.0 99.6 10.4 10.4 10.4 11.6 10.6 23.8 1.31E7 36.6 41.4 9.68 9.69 9.68 10.7 9.90 23.1 2.18E7 24.5 20. PiQA Arc Arc HellaS WinoG BoolQ Avg() MMLU 70.0 70.2 69.9 68.3 69.5 60.0 53.2 52.0 53.6 75.7 75.7 75.8 72.9 74.6 63.3 52.4 54.3 52.2 78.1 78.0 78.1 75.7 78.1 62.4 53.4 55.4 59.0 79.3 79.2 79.3 77.9 78.9 67.7 52.6 57.4 62.5 80.5 80.6 80.6 78.4 80.6 69.7 53.0 59.5 69.7 56.5 56.3 56.4 54.3 54.8 43.8 37.1 36.5 39.0 65.6 63.9 64.0 60.1 63.3 46.3 36.9 38.0 34.5 68.7 69.7 68.9 64.4 67.4 47.8 36.7 37.8 41.9 71.3 71.3 71.4 69.3 71.2 51.5 37.0 40.5 47.9 73.65 73.7 73.7 70.1 72.7 54.8 37.1 43.3 57.8 69.7 69.4 69.9 66.2 69.2 57.1 46.3 41.4 47.6 79.2 78.9 79.4 64.5 78.7 52.4 48.5 49.4 45.7 82.9 83.0 83.5 74.6 76.9 54.6 46.8 46.0 56.9 82.9 82.8 82.8 80.7 82.4 60.1 44.6 48.7 63.9 86.5 86.7 86.8 77.1 85.7 60.7 46.2 52.5 73.5 58.5 58.9 58.6 58.2 56.8 51.1 50.8 51.6 51.5 64.2 64.3 64.0 62.4 64.6 52.8 49.4 49.3 50.3 70.0 70.3 70.6 66.1 70.1 55.9 47.5 48.7 52.7 72.1 72.3 72.8 69.2 71.0 54.0 50.0 52.5 53.0 74.1 74.3 73.7 72.2 73.7 53.7 50.7 51.5 65.2 41.1 41.0 40.9 38.1 39.0 29.3 25.7 26.2 26.4 49.2 49.3 49.3 45.4 47.5 34.4 25.6 27.6 26.5 54.6 54.5 54.5 52.3 53.8 40.1 25.8 28.3 30.3 58.9 58.8 58.9 56.7 58.5 43.3 25.5 33.4 34.3 61.7 61.9 61.8 59.3 61.3 47.6 25.3 36.8 41.8 33.9 33.2 33.4 31.5 32.4 22.1 21.9 22.8 24.4 41.5 41.4 41.6 40.5 40.4 24.9 21.7 18.9 23.6 48.4 48.0 48.0 44.3 47.8 27.9 22.2 19.9 23.9 52.6 53.2 52.9 50.3 53.8 30.4 22.6 20.5 25.9 55.6 55.7 56.0 52.8 52.6 36.9 23.0 22.2 35. 52.3 52.4 52.4 43.8 47.2 25.9 25.5 24.8 26.9 61.0 60.7 60.6 54.6 56.7 28.5 25.2 26.7 23.2 73.0 72.8 72.9 66.7 70.9 37.5 26.3 25.0 25.1 76.7 76.6 76.7 72.4 75.4 36.6 25.3 26.8 30.1 80.7 80.7 80.7 77.1 79.8 43.7 24.4 27.2 39.9 65.6 65.0 65.6 63.7 61.8 43.6 24.8 24.8 28.9 73.2 74.0 73.8 74.8 74.2 49.8 24.0 28.3 28.6 79.0 78.8 78.8 73.3 77.9 46.1 24.7 28.6 36.2 82.1 81.8 81.7 81.1 82.6 53.2 26.6 30.6 43.8 83.5 83.3 83.5 80.9 82.4 59.9 24.6 37.4 61.2 7 Model Method Table 4: 1 to 8-bits per-group PTQ results of Qwen3 Models 0.6B 1.7B 4B 8B 14B 32B FP16 AWQ GPTQ AWQ GPTQ AWQ AWQ GPTQ Bi-LLM 1.06 16 FP16 8 AWQ 8 GPTQ 4 AWQ 4 GPTQ 3 AWQ 2 AWQ GPTQ 2 Bi-LLM 1.04 16 FP16 8 AWQ 8 GPTQ 4 AWQ 4 GPTQ 3 AWQ 2 AWQ GPTQ 2 Bi-LLM 1.07 16 FP16 8 AWQ 8 GPTQ 4 AWQ 4 GPTQ 3 AWQ 2 AWQ GPTQ 2 Bi-LLM 1.05 16 FP16 8 AWQ 8 GPTQ 4 AWQ 4 GPTQ 3 AWQ 2 AWQ GPTQ 2 Bi-LLM 1.05 16 FP16 8 AWQ 8 GPTQ 4 AWQ 4 GPTQ 3 AWQ 2 AWQ GPTQ 2 Bi-LLM 1. #W #A #G Wiki2() / 16 128 8 128 8 128 4 128 4 128 3 128 2 128 2 128 / 128 128 128 128 128 128 128 128 / 128 128 128 128 128 128 128 128 / 128 128 128 128 128 128 128 128 / 128 128 128 128 128 128 128 128 / 128 128 128 128 128 128 128 128 20.9 20.9 20.9 26.9 25.3 2.2E2 1.21E7 2.38E4 5.87E4 16.7 16.8 16.7 19.6 19.5 84.0 7.52E6 6.55E2 7.54E4 13.7 13.6 13.6 18.1 13.5 91.0 1.38E7 1.95E2 285 9.71 9.72 9.70 11.3 9.96 27.5 1.21E7 52.1 90.4 8.64 8.63 8.63 9.48 8.88 19.4 6.06E6 25.5 29.1 7.61 7.60 7.61 8.55 7.86 19.1 NaN 38.4 17.1 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 c4() 25.4 25.4 25.4 31.6 31.1 1.61E2 8.82E6 8.27E3 1.63E4 19.3 19.2 19.2 22.3 20.8 80.0 7.66E6 3.5E2 5.40E4 16.6 16.6 16.6 19.6 16.8 68.9 1.44E7 1.38E2 148 13.3 13.3 13.3 14.8 13.5 29.2 8.14E6 43.5 62.1 12.0 12.0 12.0 13.3 12.2 22.4 5.24E6 26.3 24.1 10.8 10.8 10.8 11.6 11.0 20.7 NaN 27.1 17.6 PiQA Arc Arc HellaS WinoG BoolQ Avg() MMLU 67.3 67.6 67.6 64.3 64.7 56.3 53.9 53.2 53.1 72.5 72.6 72.1 69.4 70.6 58.4 53.4 53.2 52.7 75.0 74.9 75.0 72.7 74.0 61.7 52.6 53.6 56.7 76.4 76.8 76.7 75.5 76.0 65.2 51.4 55.5 60.3 80.0 80.0 80.1 77.3 79.1 68.2 52.3 60.4 69.4 80.9 80.8 80.8 79.1 79.8 71.0 54.8 60.6 73.2 52.9 52.8 53.1 50.1 49.1 40.8 36.8 36.8 37.5 63.25 61.5 60.7 56.1 60.0 41.1 37.1 38.0 37.3 68.2 68.1 68.3 63.6 65.7 45.3 37.6 37.2 39.7 71.08 71.2 71.3 67.8 70.7 48.8 36.9 38.8 46.3 74.4 74.3 74.4 70.7 73.8 52.9 36.8 43.4 61.5 74.4 74.4 74.5 71.8 73.6 54.3 37.0 43.4 65. 47.1 47.0 47.0 42.1 44.0 25.0 25.7 23.5 23.3 60.0 59.8 59.9 52.5 55.7 26.7 25.2 25.6 25.1 69.7 69.6 69.7 64.1 67.6 31.5 24.6 24.4 26.3 74.7 74.5 74.7 69.3 73.4 33.2 25.3 25.0 32.8 78.5 78.5 78.4 75.9 77.4 47.7 25.0 28.5 39.9 81.2 81.3 81.4 78.0 80.6 54.9 24.6 28.1 57.5 37.6 37.5 37.6 35.1 35.7 28.2 25.5 25.6 26.2 46.0 46.1 46.2 42.3 44.0 29.7 25.9 26.3 26.6 52.2 52.3 52.3 48.6 50.9 35.5 25.4 27.0 32.9 57.1 57.1 57.1 54.6 56.6 40.3 25.3 30.3 38.0 60.9 60.9 60.9 58.4 60.5 46.1 25.5 36.0 56.4 63.9 63.9 63.9 61.9 63.3 42.7 25.7 38.6 67.8 63.9 63.2 65.2 65.6 63.6 54.6 43.4 43.9 42.6 77.6 77.7 77.1 69.8 76.8 49.6 46.0 50.3 42.4 85.1 85.1 85.1 82.0 84.3 55.8 49.7 44.1 46.2 86.5 86.5 86.7 82.1 86.5 61.6 43.7 46.9 63.1 89.4 89.5 89.3 87.0 89.2 66.4 46.6 52.8 82.1 86.6 86.4 86.4 86.4 85.7 61.8 44.3 50.8 78.1 31.7 31.5 30.9 25.8 29.4 21.7 22.2 20.9 25.2 40.1 39.2 39.8 33.5 38.2 22.2 21.0 20.1 24.7 50.6 50.5 50.8 45.0 48.9 27.2 22.1 21.1 22.8 55.5 55.5 55.6 48.8 54.2 26.6 23.4 20.6 24.5 59.0 58.4 58.4 51.9 57.9 29.8 23.1 22.0 35.9 57.8 57.9 58.2 54.2 57.6 35.6 21.7 25.6 43.3 60.8 60.6 60.9 54.1 48.1 33.9 24.3 26.2 27.2 72.4 71.9 72.3 63.1 71.2 36.5 24.7 25.7 28.5 80.5 80.6 80.5 73.0 71.2 42.4 26.7 26.3 30.6 83.5 83.5 83.7 80.8 82.4 45.2 25.8 27.9 38.2 84.3 84.3 84.4 79.5 83.8 52.2 24.9 36.0 59.3 84.4 84.3 84.4 81.3 83.8 61.6 26.0 33.1 63.8 56.2 56.4 56.6 55.4 53.4 50.0 51.6 51.1 50.6 60.9 61.6 56.6 58.4 59.3 50.2 51.6 52.0 48.9 65.8 65.6 66.0 60.1 65.0 51.5 48.8 51.4 48.9 68.0 67.5 67.9 64.7 68.4 54.1 51.7 51.5 53.5 72.9 72.8 73.2 70.0 72.0 54.4 48.3 53.4 65.7 73.6 72.9 73.4 68.0 71.5 53.1 49.3 51.9 66."
        },
        {
            "title": "References",
            "content": "[1] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023) [2] Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al.: Piqa: Reasoning about physical commonsense in natural language. In: Proceedings of the AAAI conference on artificial intelligence. vol. 34, pp. 74327439 (2020) [3] Clark, C., Lee, K., Chang, M.W., Kwiatkowski, T., Collins, M., Toutanova, K.: Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044 (2019) [4] Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord, O.: Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018) [5] Frantar, E., Ashkboos, S., Hoefler, T., Alistarh, D.: Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 (2022) [6] Gong, R., Ding, Y., Wang, Z., Lv, C., Zheng, X., Du, J., Qin, H., Guo, J., Magno, M., Liu, X.: survey of low-bit large language models: Basics, systems, and algorithms. arXiv preprint arXiv:2409.16694 (2024) [7] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., Steinhardt, J.: Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020) [8] Huang, W., Liu, Y., Qin, H., Li, Y., Zhang, S., Liu, X., Magno, M., Qi, X.: Billm: Pushing the limit of post-training quantization for llms. arXiv preprint arXiv:2402.04291 (2024) [9] Huang, W., Zheng, X., Ma, X., Qin, H., Lv, C., Chen, H., Luo, J., Qi, X., Liu, X., Magno, M.: An empirical study of llama3 quantization: From llms to mllms. Visual Intelligence 2(1), 36 (2024) [10] Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., Han, S.: Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978 (2023) [11] Liu, Z., Zhao, C., Fedorov, I., Soran, B., Choudhary, D., Krishnamoorthi, R., Chandra, V., Tian, Y., Blankevoort, T.: Spinquant: Llm quantization with learned rotations. arXiv preprint arXiv:2405.16406 (2024) [12] Merity, S., Xiong, C., Bradbury, J., Socher, R.: Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843 (2016) [13] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research 21(1), 54855551 (2020) [14] Sakaguchi, K., Bras, R.L., Bhagavatula, C., Choi, Y.: Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM 64(9), 99106 (2021) [15] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017) [16] Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., Han, S.: Smoothquant: Accurate and efficient post-training quantization for large language models. In: International Conference on Machine Learning. pp. 3808738099. PMLR (2023) [17] Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al.: Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 (2024) [18] Yuan, Z., Niu, L., Liu, J., Liu, W., Wang, X., Shang, Y., Sun, G., Wu, Q., Wu, J., Wu, B.: Rptq: Reorder-based post-training quantization for large language models. arXiv preprint arXiv:2304.01089 (2023) [19] Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., Choi, Y.: Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830 (2019)"
        }
    ],
    "affiliations": [
        "Beihang University",
        "ETH Zürich",
        "Xidian University"
    ]
}