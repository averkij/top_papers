{
    "paper_title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs",
    "authors": [
        "Yipeng Du",
        "Tiehan Fan",
        "Kepan Nan",
        "Rui Xie",
        "Penghao Zhou",
        "Xiang Li",
        "Jian Yang",
        "Zhenheng Yang",
        "Ying Tai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite advancements in Multimodal Large Language Models (MLLMs), their proficiency in fine-grained video motion understanding remains critically limited. They often lack inter-frame differencing and tend to average or ignore subtle visual cues. Furthermore, while visual prompting has shown potential in static images, its application to video's temporal complexities, particularly for fine-grained motion understanding, remains largely unexplored. We investigate whether inherent capability can be unlocked and boost MLLMs' motion perception and enable distinct visual signatures tailored to decouple object and camera motion cues. In this study, we introduce MotionSight, a novel zero-shot method pioneering object-centric visual spotlight and motion blur as visual prompts to effectively improve fine-grained motion understanding without training. To convert this into valuable data assets, we curated MotionVid-QA, the first large-scale dataset for fine-grained video motion understanding, with hierarchical annotations including SFT and preference data, {\\Theta}(40K) video clips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models. In particular, for fine-grained motion understanding we present a novel zero-shot technique and a large-scale, high-quality dataset. All the code and annotations will be publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 4 7 6 1 0 . 6 0 5 2 : r MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs Yipeng Du1 Tiehan Fan1 Kepan Nan1,2 Rui Xie1,2 Penghao Zhou2 Xiang Li3 1 Nanjing University 2 ByteDance Jian Yang1 Zhenheng Yang2 Ying Tai1(cid:66) 3 Nankai University https://nju-pcalab.github.io/projects/MotionSight Figure 1: Motivation and approach overview. (a) Temporal dynamics inherent in motion distinguish videos from static images. (b) Existing MLLMs show limitations in fine-grained motion detection. (c) Our approach shows superior performance on MotionBench and FAVOR-Bench compared to SOTA. Abstract Despite advancements in Multimodal Large Language Models (MLLMs), their proficiency in fine-grained video motion understanding remains critically limited. They often lack inter-frame differencing and tend to average or ignore subtle visual cues. Furthermore, while visual prompting has shown potential in static images, its application to videos temporal complexities, particularly for fine-grained motion understanding, remains largely unexplored. We investigate whether inherent capability can be unlocked and boost MLLMs motion perception and enable distinct visual signatures tailored to decouple object and camera motion cues. In this study, we introduce MotionSight, novel zero-shot method pioneering object-centric visual spotlight and motion blur as visual prompts to effectively improve fine-grained motion understanding without training. To convert this into valuable data assets, we curated MotionVid QA, the first large-scale dataset for fine-grained video motion understanding, with hierarchical annotations including SFT and preference data, Θ(40K) video clips and Θ(87K) QAs. Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models. In particular, for fine-grained motion understanding we present novel zero-shot technique and large-scale, high-quality dataset. All the code and annotations will be publicly available. * Equal contributions. Ying Tai is the corresponding author. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Benefiting from high-quality video-text datasets [6, 20, 5, 34, 39] and large model parameters [50, 13, 2, 49, 9], Multimodal Large Language Models (MLLMs) have exhibited impressive performance on wide range of video understanding tasks. Unlike static images, video inherently possesses temporal dimension. This temporal dimension, characterized by continuous changes between frames over time, forms rich and varied motion, this change may originate from objects or the camera, thus making videos lively and narrative, distinguishing them from images as shown in Figure 1 (a). However, even with rapid progress and development in video understanding tasks, the task of finegrained video motion understanding still lacks necessary attention and exploration. While MLLMs acquire broad semantic knowledge from large-scale data pre-training [2, 11, 43, 38, 22, 46, 32, 47, 17, 35, 36, 45, 33, 8], their direct application to fine-grained motion understanding is often suboptimal. This stems from their tendency to process spatial regions with uniform importance and lack of inherent mechanisms to explicitly discern the subtle inter-frame variations critical for nuanced motion analysis. These architectural predispositions mean their potential for fine-grained understanding remains largely untapped [31] as shown in Figure 1 (b). Therefore, how can we boost the latent capabilities of MLLMs derived from large-scale data, to achieve fine-grained motion understanding to localized motion cues and enhance the modeling of nuanced inter-frame dynamics? Furthermore, even if we successfully enhance the fine-grained motion understanding capabilities of MLLMs and address this research question, their augmented understanding and insights primarily subsist as implicit representations. pivotal consideration, therefore, is how to explicitly and structurally extract this knowledge and transform it into valuable data assets. These assets would be instrumental for the training of other models and for in-depth analysis by researchers, thus posing crucial direction for fostering broader advancements in the field. Motivated by these questions, in this study, we conducted extensive experiments and explorations on how to boost MLLMs inherent fine-grained motion understanding capability through zero-shot approaches, without relying on additional training data. Previous studies in the image understanding domain [42, 25, 41, 40] have demonstrated strong interest in visual prompting techniques, but their extension to address the intricate temporal complexities of video, especially for nuanced fine-grained motion understanding, still requires further investigation. To demonstrate the inadequacy of naively adapting prompting methodologies from static images to encapsulate the intrinsic temporal dynamics of events in video, we conducted direct transfer evaluations on recent motion-specific benchmarks [31, 12], and found that even background blur (blur reverse mask), the best-performing image-based visual prompt [41], paradoxically exhibited the poorest performance in fine-grained motion understanding. This approach tends to decrease the models ability in fine-grained motion understanding due to the loss of contextual information, as shown in Figure 3. Therefore, we designed novel video visual prompting method specifically for the unique requirements of the video motion understanding domain, which we call MotionSight. Specifically, we begin by decoupling the pertinent object motion and camera motion according to the problem. For object motion, we select action groups based on the MLLMs initial perception. Building upon this, we leverage an external knowledge module for object tracking to identify set of bounding boxes highly correlated with the queried motion. Subsequently, we smooth this series of boxes and apply spotlight-like visual prompt, which is then fed back into the MLLM to enhance its fine-grained object motion perception capabilities. For camera motion, which necessitates the MLLMs perception of subtle scene changesa capability where MLLMs often exhibit limitationswe designed method to synthetically introduce motion blur into video frames. Interestingly, our experiments reveal that this addition of motion blur significantly benefits camera motion determination. Finally, we obtain templated enhanced results through our carefully designed config, and MLLM inference yields the result. To further convert this capability into explicit, actionable data assets, we collected and annotated Θ(40K) video clips, Θ(87K) question-answer pairs. Through rigorous filtering mechanism that enhanced the quality of annotated data, we developed an SFT dataset and preference dataset for training strategies. This process distilled the fine-grained motion understanding capability of MLLMs and aligned it with human preference, constructed MotionVid QA, the first large-scale open-source dataset for fine-grained video motion understanding to date, encompassing diverse scenes and high-quality video footage. Figure 2: Overview of the interaction process. Left: Our MotionSight pipeline captions highquality data, transforming it into data assets. Right: This data undergoes rigorous filtering to align with human preferences, resulting in our high-quality dataset MotionVid QA. 2 Related Work MLLMs for video understanding. As MLLMs continue to advance, growing body of research focuses on applying them to video understanding [2, 11, 43, 38, 22, 46, 32, 47, 17, 35, 36, 45, 33, 8]. Video understanding models often use keyframes as samples, which are then encoded for LLMs. Several approaches develop specialized connectors (such as improved Q-Formers for video) [35, 36, 45], while models like QwenVL [2, 33] and InternVL [8] encode videos frame-by-frame through vision encoder before feeding them to the LLM. Although these methods excel at event-level video representation, they tend to struggle with fine-grained motion understanding due to limited perception of inter-frame dynamic differences, resulting in inaccurate information retrieval. To address this limitation, we propose training-free pipeline leveraging novel visual focusing and motion blur techniques to enhance the models understanding of fine-grained object motion and camera motion. Fine-grained motion understanding datasets. Early action recognition datasets [28, 15, 3] had limited fine-grained motion understanding due to simplistic categorical labels. Recent works use MLLMs for auto-annotation [6, 34, 39, 23], but granularity remains limited. Structured video captions [14, 10, 37] respond to the need for fine-grained semantics. However, deficiencies persist in motion semantics delineation due to the lack of well-designed approach for obtaining fine-grained semantic representations. Benchmarks like MotionBench [12] and FAVOR-Bench [31] have datasets with insufficient sample sizes, limiting scene diversity and semantic richness. To overcome these limitations, we propose MotionVid QA, the first large-scale dataset for fine-grained motion understanding, featuring extensive scene coverage and high video quality. 3 Figure 3: Comparison of our method with other existing methods. Directly applying image visual prompts like background blur can instead lead to misinterpretation due to the lack of motion context information in the background. By employing decoupled objectguided motion focusing and inter-frame information enhancement, our method addresses the challenge faced by previous methods. Figure 4: The detailed pipeline of MotionSight. Our method includes query-based motion decoupling, gating based on object motion and camera motion. Subsequently, it passes through three modules: Object Referring, Action Focusing, and Motion Blur. Then, we carefully designed template prompt for MLLMs to understand our enhanced input and make final decisions."
        },
        {
            "title": "3 MotionSight",
            "content": "3.1 Overview This section introduces MotionSight (Figure 4) with enhanced fine-grained motion perception. Our method decouples object and camera motion and discusses techniques to enhance MLLM input videos. Our approach is formalized as follows: Robj = MLLM(Φobj(Vs))), Rcam = MLLM(Φcam(Vs, V), [Robj]). (1) We sample the input video = {Ii}L j=1, with and denoting original video length and sampled frames length, respectively. Robj and Rcam are object and camera motion understanding, respectively. Φobj and Φcam are the corresponding visual prompting functions. [Robj] means that Robj is optional for camera motion understanding in QA tasks. i=1 (Ii R3HW ) to Vs = {Isj }T Based on question type, we employ specialized components for fine-grained motion perception: (i) Object Referring (Sec. 3.2) provides query-relevant object context; (ii) Action Focusing (Sec. 3.3) enhances object motion understanding by highlighting movements; and (iii) Motion Blur (Sec. 3.4) addresses camera motion via temporal aggregation. 3.2 Object Referring Initially, the MLLMs processes sampled frames Vs and the query to infer set of semantically relevant object categories = {c1, c2, ..., cn}. This inferred set guides our subsequent visual perception modules for object localization and trajectory estimation. Formally, the process of obtaining tracked object trajectories is defined by the composition: = Mtrack(Mdetect(Ist, C; θdet), {Isj }T j=t+1; θtrack), (2) where Mdetect(Ist, C; θdet) (GroundingDINO [21]) identifies bounding boxes for categories in key frame Ist. Mtrack(, {Isj }T j=t+1; θtrack) (SAM2 [24]) then propagates these detections across subsequent frames {Isj }T j=t+1 yielding trajectories O. While direct action inference can hallucinate, robust object identification, even with initial errors, is refinable by lower-confidence detections [7]. To address potential detection loss due to frequent object entry and exit (e.g., FAVOR-Bench [31]), we perform re-detection using Mdetect at fixed intervals on frames {n N, }, ensuring comprehensive object capture. 3.3 Action Focusing Given tracked objects = {(bst,i)Ti i=1 (bst,i: i-th objects bounding box at frame st; Ti: trajectory length), we use dynamic temporal aggregator to derive refined spatial regions = {bt}T t=1, which merge and stabilize bounding boxes against jittering. adaptively adjusts its temporal t=1}m 4 Table 1: The comparison of existing motion-specific datasets with ours. Our dataset significantly surpasses existing methodologies in both scale and annotation granularity. Furthermore, the quality of our dataset generally exceeds that of currently prevalent motion-specific datasets used for comparison. Dataset #Videos #Text Annotation Types Subject UCF101 [29] ActivityNet [3] Kinetics-700 [4] Charades [26] Charades-Ego [27] MotionBench-train [12] FavorBench-train [31] MotionVid QA (ours) - SFT - DPO 13K 20K 650K 9K 8K 5K 17K 40K 35K 5K N/A N/A N/A 27K 68K 5K 17K 87K 80K 7K Class Labels Class Labels Class Labels Captions Captions Captions Captions Human Human Human Human, Indoor objects Human, Indoor objects Diverse objects, Camera Diverse objects, Camera Mixed QAs QAs Chosen/Reject QAs Diverse objects, Decoupled camera - - Fine-grained motion understanding Enhance motion understanding Align with human preferences Usage Action recognition Action recognition Action recognition Video understanding Video understanding Motion understanding Motion understanding Scenario Human-centric Human-centric Human-centric Indoor Indoor Open domain Indoor Open domain - - aggregation window based on intra-trajectory positional variance V. Let = the union of bounding boxes in each frame: i=1(bs1:Ti ,i) denote t=1. = (X , V(X )) = {bt}T (3) Here, V() quantifies bounding box positional variance along trajectory. Specifically, with low positional variance, favors union of bounding boxes over longer temporal spans; with high variance, it focuses on localized regions in shorter temporal windows. To quantify positional variance, we measure Manhattan distance between pairs of bounding box centers as center(bst1 ,i) center(bst2 ,i)1. The object motion enhancement function Φobj then applies visual prompting techniques to the original frames using these dynamically aggregated object regions: Φobj(Vs) = FV (Vs, B), where FV represents our visual spotlight approach that darkens the background outside {bt}T t=1 while preserving the detected objects in their original positions, enhancing focus on the relevant moving elements. In this way, our visual prompt considers object focus and smooth transition. (4) 3.4 Motion Blur To overcome the inherent limitations of existing MLLMs in perceiving inter-frame changes, particularly subtle camera motions, we introduce dedicated Motion Blur Transformation TM as core component of our camera motion enhancement function Φcam. This function, Φcam(V, Vs) = V, operates on sampled timestamps {st}T t=1 to generate sequence of motion-enhanced frames = {I }T t=1, thereby amplifying temporal motion cues. For st given frame Ist, the enhanced frame st is derived through temporally weighted aggregation of its preceding frames from the original video. This process can be formally expressed as: t=1 using the entire video sequence = {It}T Φcam(V, Vs) = {TM B(Vs, N, t)}T t=1 , where TM B() = 1 (cid:88) k=0 wk(γ) Istk. (5) Here, is the temporal window size (zero-padding applied for 1). The temporal kernel we use follows geometric series being the decay factor (γ (0, 1) and (cid:80) wk(γ) = 1). This temporal aggregation within Φcam accentuates motion trajectories by inducing motion blur effects across V, enhancing the MLLMs capacity to perceive and interpret subtle camera movements."
        },
        {
            "title": "4 MotionVid",
            "content": "In this section, our work yields two key data resources: instruction/preference subsets from public data tailored for two-stage model refinement (SFT/DPO) towards high-quality motion understanding. 4.1 Preliminaries. Dataset collection and processing. MotionVid QA is curated from variety of sources to ensure multiple types of motion understanding tasks are covered, and richful scenarios ensure the diversity of the dataset, including ActivityNet [3], Kinetics-700 [4], Charades [26], Charades-Ego [27], Tarsier2Recap-585K [44], OpenVid-1M [23], and MotionBench-train [12]. To ensure the quality of the videos, data processing steps we outline in Figure 2 are applied to the original videos. 5 Figure 5: MotionVid QA: High-quality filtering. Construction of high-quality video dataset via filtration. Extensive data distribution. Diverse sources yield varied scenes, subjects, and camera perspectives. Human preference comparison. Our preference data annotation significantly surpasses baseline, particularly for camera motion. Figure 6: comparative visualization of MotionVid QA against existing data. Our proposed dataset incorporates high-fidelity QAs augmented with human signals. In contrast to antecedent methodologies reliant upon class labels or captions, our approach facilitates the provision of substantially richer and more diverse informational content. Notably, even in scenarios characterized by the absence of salient principal objects, our methodology consistently yields high-quality annotations pertaining to object and camera dynamics. SFT and DPO. SFT aims to produce specialized model πSFT capable of effectively capturing spatiotemporal dynamics and semantic motion patterns inherent in video data, thereby enhancing its performance on specific video understanding applications. DPO aims to simplify Reinforcement Learning from Human Feedback (RLHF) by utilizing the log-likelihood of the learning policy. Instead of learning an explicit reward model, it implicitly expresses the reward function through pair-wise preference data = {(xi, ychosen i=1 to optimize the policy model. Let πθ is the language model policy which always initialized to πSFT, and πref is also initialized from πSFT. The objective function is defined as: , yreject )}M LDPO(θ) = E(x,ychosen,yreject)D (cid:20) (cid:18) log σ β log πθ(ychosenx) πref(ychosenx) β log (cid:19)(cid:21) πθ(yrejectx) πref(yrejectx) (6) 6 Table 2: Quantitative results on MotionBench. We compared our MotionSight both proprietary MLLMs and open-source MLLMs on MotionBench, all of which have been trained on large-scale video data. The best results of open-source methods are marked in bold. Model # Frames Overall AVG. MR LM CM MO AO RC Gemini 2.0 Flash [9] Gemini 1.5 Pro [30] GLM-4V-Plus-0111 [48] GLM-4V-Plus [11] MiniCPM-V2.6 [43] [COLM24] PLLaVA-34B [38] Oryx-34B [22] [ICLR25] LLaVA-NeXT-Video-34B [46] CogVLM2-Video [11] Qwen2.5VL-7B [2] Qwen2.5VL-72B [2] InternVL3-8B [49] InternVL3-78B [49] TE Fusion [12] [CVPR25] Qwen2.5VL-7B + MotionSight InternVL3-78B + MotionSight 1fps 1fps 2fps 30 64 16 64 32 24 1fps 1fps 16 16 16 1fps Proprietary MLLMs 56.1 51 62.8 52.6 48 60.3 60.9 51 64.1 57.1 52 67.0 50.9 54 67. 74.1 67 73.5 37.6 40 46.7 35.0 22 42.8 Open-source MLLMs 54 52 52 49 48 41 53.0 58.3 58.1 61.5 58 55.6 63. 52 49 48 47 44 42 48.8 54.3 53.7 57.6 54 52.2 59.3 57 56 55 48 53 43 58.3 64.0 65.1 67.2 64 59.7 68.5 57 49 51 52 45 39 55.3 60.3 63.0 63.9 59 58.1 65. 54 45 47 44 36 38 34.0 48.6 47.8 55.8 51 48.3 58.7 69 72 66 65 66 64 71.5 73.2 74.1 78.1 69 73.6 78.6 40 39 38 42 39 37 39.5 46.8 39.7 44.9 41 40.1 47. 37 33 31 32 23 33 34.0 33.0 32.3 35.8 39 33.5 37.0 This serves as the optimization target for DPO. The goal of the DPO loss is to maximize the reward difference between preferred (ychosen) and non-preferred (yreject) samples. And the human preference comparison, as shown in Figure 5, will be the human preference signals. MotionVid QA: large-scale dataset for fine-grained motion understanding. For the prefiltered dataset, we selected Θ(40K) clips annotated them with MotionSight. Using VQAScore [19] and human thresholds for categorization, high-quality clips became preference dataset candidates, low-quality ones were eliminated, and the rest formed our instruction dataset. In the SFT phase, to enhance motion understanding capabilities via SFT, we used MotionSight annotations as text data. For the preference dataset, aiming to align fine-grained motion understanding with human preferences via DPO. With balance between efficiency and quality, we re-annotated this portion of the data using Tarsier2 [44] as the baseline. High-quality preferences data was then developed by incorporating human preference signals from multiple, guided, and well-educated individuals. Our curated dataset (Table 1, Figure 6) significantly advances fine-grained video motion understanding. Its key contributions are: (1) Pioneering scale and scope. The first large-scale, open-source dataset for this task, offering diverse scenes and high-quality footage. (2) Diversity and quality. Rigorous filtering enhances clarity (mitigating annotation hallucinations), text-video consistency and pronounced dynamics (cf. Figure 5). (3) Hierarchical data composition. Comprising SFT and preference subsets, it enables multi-faceted learning: general motion understanding (SFT) and refined, human-aligned fine-grained comprehension. Overall, this large-scale, high-quality, hierarchically structured resource will significantly support future model training and evaluation."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Benchmarks MotionBench [12] is large-scale, fine-grained motion-level benchmark encompassing wide range of video types with 5,385 videos and 8,052 QAs. Its question types include six fine-grained motion-oriented task categories: Motion Recognition (MR), Location-related Motion (LM), Action Order (AO), Repetition Count (RC), Motion-related Objects (MO), and Camera Motion (CM). We conducted testing using its publicly available dev set for evaluation. FAVOR-Bench [31] is fine-grained video action understanding benchmark comprising 1,776 videos with structured manual annotations of various motions. We tested our method using its publicly available close-ended evaluation, which encompasses six types of question-answer pairs: Action Sequence (AS), Holistic Action Classification (HAC), Single Action Detail (SAD), Multiple Action Details (MAD), Camera Motion (CM), and Non-Subject Motion (NSM). 7 Table 3: Quantitative results on FAVOR-Bench. We selected representative MLLMs as baselines for comparison. The best results of open-source methods are marked in bold. Model # Frames Overall AVG. AS HAC SAD MAD CM NSM GPT-4o [13] Gemini-1.5-Pro [30] Claude-3.7-Sonnet [1] Video-LLaVA-7B [18] [EMNLP24] LLaVA-NeXT-Video-7B [20] LLaVA-NeXT-Video-34B [20] Tarsier-7B [32] Tarsier-34B [32] Aria [16] LLaVA-Video-7B-Qwen2 [47] LLaVA-Video-72B-Qwen2 [47] VideoChat-Flash-Qwen2-7B [17] VideoLLaMA3-2B [45] VideoLLaMA3-7B [45] Qwen2.5VL-7B [2] Qwen2.5VL-72B [2] InternVL3-8B [49] InternVL3-78B [49] Qwen2.5VL-7B + MotionSight InternVL3-78B + MotionSight 1fps 1fps 1fps 8 8 8 8 8 8 64 64 1fps 1fps 1fps 1fps 1fps 16 16 1fps 16 Proprietary MLLMs 42.1 49.9 43.7 43.1 50.7 44.0 Open-source MLLMs 25.4 23.5 30.4 17.5 30.3 34.6 38.6 46.1 43.8 33.0 41.5 42.3 48.1 45.3 52.8 45.1 53.8 25.1 22.3 32.6 20.5 31.9 38.7 40.0 46.5 44.9 34.6 41.5 41.6 48.2 46.0 52.6 44.1 53.5 40.7 49.2 45.2 24.9 21.3 31.7 12.6 28.6 33.3 36.1 48.4 41.9 29.0 40.2 41.6 50.3 47.7 54. 44.0 56.2 45.1 53.7 43.0 21.5 22.5 32.0 21.2 35.0 41.1 41.3 47.5 48.4 36.6 44.1 46.7 47.0 48.0 58.1 49.5 58.9 42.8 48.8 41.8 25.5 26.1 32.3 17.9 26.9 30.1 41.3 45.3 42.8 34.9 42.4 43.5 48.1 45.3 53. 44.9 52.8 45.5 54.9 48.1 30.5 26.7 23.0 17.9 31.3 35.3 44.5 51.7 51.0 38.0 48.3 46.3 51.8 48.3 57.2 48.8 58.4 36.0 41.6 39.1 26.2 23.1 29.6 22.2 31.9 33.2 29.6 33.0 35.1 28.6 31.5 30.9 40.3 31.7 34. 38.1 37.1 48.4 56.3 46.9 21.9 14.1 46.9 31.3 37.5 59.4 46.9 53.1 50.0 40.6 42.2 40.6 51.6 54.7 57.8 39.1 57.8 Evaluation Metrics. We report accuracy for each problem type, with Overall representing accuracy across all problems. We also calculate AVG. as the average accuracy across question categories, giving equal weight to each motion problem type regardless of sample distribution. 5.2 Comparison with SOTA Method MotionBench. Table 2 presents the quantitative results on MotionBench. Our MotionSight consistently enhances the performance of base MLLMs. When using Qwen2.5VL as the backbone, our method achieves 3.4% improvement in category average, while camera motion improves by 14.3%. Furthermore, InternVL3-78B augmented with MotionSight demonstrates significant performance enhancement, achieving state-of-the-art results among open-source models like TE Fusion [12] and exhibits strong competitiveness against leading proprietary models like GLM-4V-Plus-0111 [48]. FAVOR-Bench. To further demonstrate our methods efficacy in fine-grained motion understanding, we include results on FAVOR-Bench [31] in Table 3, which mirror the positive impact of MotionSight. Qwen2.5VL-7B + MotionSight exhibits marked improvement of 3.0% improvement on category average and 2.5% improvement on overall metrics. Meanwhile, integrating MotionSight with InternVL3-78B yields state-of-the-art performance, with notable gains in categories such as AS, HAC, and MAD. Experiment results demonstrate the consistent effectiveness of our method. 5.3 Ablation Study Object motion understanding. We evaluate visual prompting strategies for action focus in videos to improve object motion understanding. As detailed in Table 4, our proposed visual spotlight technique yields the highest average object motion score (OM AVG.), outperforming alternatives like object cropping. Background blur, however, negatively impacted performance, contrasting its effectiveness in static image prompting [41]. We attribute this failure to blurred object boundaries, increasing the demand for robustness and misleading MLLMs. Other visual prompts, including object crop, object motion blur (applied solely to the object mask) and pose estimation (applied to the entire video), also provided marginal or negative impacts. These findings underscore the efficacy of visual spotlight in directing model attention to pertinent object movements. Camera motion understanding. For camera motion, our primary evaluation focused on applying motion blur to the entire video frame, also referred to as global motion blur. Table 4 shows that our Table 4: Ablation study on fine-grained motion understanding. We comparatively analyzed several visual prompt methods specialized for video motion. Green areas indicate best performance and red areas show lowest scores in each category. Method Qwen2.5VL-7B [2] + Visual Spotlight + Object Crop + Background Blur [41] + Object Motion Blur + Pose Estimation + Global Motion Blur Object Motion OM AVG. MR LM MO 51.7 53.0 52.5 49.3 50.2 50.6 50.8 58.3 59.7 59.0 56.2 60.2 56.4 57.2 55.3 58.1 55.9 53.9 52.5 54.6 57.0 71.5 73.6 71.3 65.4 68.8 70.1 69.1 AO 39.5 40.1 40.1 39.9 40.4 40.1 39. RC 34.0 33.5 36.5 31.3 29.3 32.0 31.5 Camera Motion CM AVG. 34.0 - - - - - 48.3 Figure 7: Qualitative examples of MotionSight. Our method, MotionSight, possesses finegrained inter-frame difference perception, enabling precise sensing of subtle motions. motion blur synthesis approach facilitates the models perception of subtle inter-frame differences, thereby yielding substantial improvement, significantly outperforming the baseline."
        },
        {
            "title": "6 Conclusions and Limitations",
            "content": "In this work, we address the challenge of fine-grained video motion understanding, task that has been relatively underexplored despite the advancements in MLLMs. We introduce MotionSight, novel zero-shot approach employing visual spotlight and motion blur to enhance MLLMs ability to perceive and understand subtle motion cues. Furthermore, we curate MotionVid QA, the first largescale open-source dataset designed for fine-grained video motion understanding. Our experiments demonstrate the effectiveness of MotionSight and the potential of MotionVid QA to facilitate future research in this domain. Limitations. While our work advances fine-grained video motion understanding, it has limitations: MotionSights performance is tied to the underlying MLLM, potentially inheriting its biases. Common to video analysis technology, our method and dataset also risk misuse (e.g., in surveillance). We advocate for ethical use, designing our work with transparency for scrutiny and misuse mitigation. 9 References [1] Anthropic. Claude 3.7. https://www.anthropic.com/claude/sonnet, February 2025. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. [4] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. [5] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 1947219495. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/ paper/2024/file/22a7476e4fd36818777c47e666f61a41-Paper-Datasets_and_ Benchmarks_Track.pdf. [6] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. [7] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou. Halc: Object hallucination reduction via adaptive focal-contrast decoding. arXiv preprint arXiv:2403.00425, 2024. [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [9] Google DeepMind. Gemini: Our most intelligent ai models, built for the agentic era. https: //deepmind.google/technologies/gemini/, 2025. [10] Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Zhenheng Yang, Chaoyou Fu, Xiang Li, Jian Yang, and Ying Tai. Instancecap: Improving text-to-video generation via instance-aware structured caption. arXiv preprint arXiv:2412.09283, 2024. [11] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. [12] Wenyi Hong, Yean Cheng, Zhuoyi Yang, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, and Jie Tang. Motionbench: Benchmarking and improving fine-grained video motion understanding for vision language models. arXiv preprint arXiv:2501.02955, 2025. [13] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [14] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. Advances in Neural Information Processing Systems, 37:4895548970, 2024. 10 [15] Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: large video database for human motion recognition. In 2011 International conference on computer vision, pages 25562563. IEEE, 2011. [16] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, et al. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. [17] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. [18] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [19] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. arXiv preprint arXiv:2404.01291, 2024. [20] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. [21] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [22] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: Ondemand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. [23] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. [24] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [25] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about red circle? visual prompt engineering for vlms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1198711997, 2023. [26] Gunnar Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 510526. Springer, 2016. [27] Gunnar Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charades-ego: large-scale dataset of paired third and first person videos. arXiv preprint arXiv:1804.09626, 2018. [28] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [29] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild, 2012. URL https://arxiv.org/abs/1212.0402. [30] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 11 [31] Chongjun Tu, Lin Zhang, Pengtao Chen, Peng Ye, Xianfang Zeng, Wei Cheng, Gang Yu, and Tao Chen. Favor-bench: comprehensive benchmark for fine-grained video motion understanding. arXiv preprint arXiv:2503.14935, 2025. [32] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. [33] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [34] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. [35] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024. [36] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [37] Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Shuicheng Yan, Hao Fei, et al. Any2caption: Interpreting any condition to caption for controllable video generation. arXiv preprint arXiv:2503.24379, 2025. [38] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [39] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: video is worth thousands of words. Advances in Neural Information Processing Systems, 37:5724057261, 2024. [40] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [41] Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. Fine-grained visual prompting. Advances in Neural Information Processing Systems, 36:2499325006, 2023. [42] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models, 2022. URL https: //arxiv.org/abs/2109.11797. [43] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [44] Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. Tarsier2: Advancing large vision-language models from detailed video description to comprehensive video understanding, 2025. URL https://arxiv.org/abs/2501.07888. [45] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. [46] Zhang, Li, Liu, Lee, Gui, Fu, Feng, Liu, and Li. Llava-next: strong zero-shot video understanding model. 2024. [47] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 12 [48] ZhipuAI. Glm-4v. https://bigmodel.cn/dev/howuse/glm-4v, 2025. [49] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [50] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479. 13 MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs"
        },
        {
            "title": "Supplementary Contents",
            "content": "In this supplementary material, we present comprehensive details and analyses across the following sections: Section presents the specific experimental configurations of our MotionSight, as well as the detailed prompt template used. Section outlines the methodological aspects and specific steps involved in filtering and curating video clip subsets for SFT and DPO, detailing the dataset construction process. Section details the guidelines for human annotators choosing the preferred textual description of videos fine-grained motion for DPO, using an interactive interface and defined criteria. Section focuses on MotionChat, our Qwen2.5VL-7B-based fine-tuned model. It details the training methodology and presents experimental evidence of our datasets crucial role in enhancing performance for motion-centric chat applications."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Implementation Details We performed all experiments of MotionSight on 8 NVIDIA 4090 GPUs with 48GB of memory each. For GroundingDINO, the box threshold and text threshold for post-processing grounded object detection are both set to 0.25. For SAM 2, the mask dictionary model uses an IOU threshold of 0.8 for updating masks. In motion blur part we use decay factor γ = 0.65 and temporal window = 7. For action focusing we use darken factor β = 0.9. A.2 Prompt Template Our prompt details show in S1. For the Object Referring stage, we use the prompt shown in the upper image to enable the MLLM to locate the most critical objects based on the video and question content. We utilize the concept of action groups , allowing the MLLM to identify relevant objectsand even their componentsin fine-grained manner. The code below defines key part of our final configuration template. Based on enhanced inputs such as Action focus or motion blur, we route the input into the MLLM using routing mechanism. In this template, description_for_video_type refers to descriptions tailored for different video types. video_descriptions = { 'original': 'Original video:n', 'spotlight': 'Spotlight video:n', 'motion_blur': 'Original video with motion blur to more clearly determine the type of motion (such as whether the camera is moving, as one frame combines information from multiple frames. If static objects in the background appear noticeably blurry, there is good chance that the camera is moving!):n' (cid:44) (cid:44) (cid:44) (cid:44) } Dataset Filter and Curation for MotionVid QA In this section, we focus on the methodological aspects and specific steps involved in filtering and curating the instruction and preference subsets for SFT and DPO, providing supplementary details to the dataset construction process of MotionVid QA described in Section 4. B.1 Initial Data Collection and Pre-processing MotionVid QA was aggregated from multiple sources (Section 4), resulting in an initial set of video clips, denoted as CR. These clips underwent an initial data processing pipeline, Pinitial. The pipeline Pinitial filters the raw clips CR such that only clips satisfying specific quality metrics are retained. Specifically, clip CR is included in the pre-filtered set CP if its optical flow score sf (c) is above threshold τf AND its clarity score sc(c) is above threshold τc. The set of pre-filtered clips is thus defined as: CP = {c CR sf (c) > τf sc(c) > τc} (S1) B.2 Detailed Filtering and Subset Creation for SFT and DPO The methodology for creating SFT and DPO subsets from the pre-filtered set CP is introduced in Section 4 of the main paper. Initial selection and annotation. subset of clips was chosen from CP for annotation with MotionSight. Let CA denote the set of successfully annotated clips. Annotation quality-based categorization. Each clip CA was evaluated using VQAScore [19]. To ensure the rationality of the chosen VQAScore thresholds for categorization (detailed below and in Table S1), we manually checked multiple samples at the boundaries of these thresholds. Based on this evaluation, the clips were categorized into three distinct groups and the text-video visualization results satisfying different threshold conditions are shown in Figure S2. 2 Figure S1: Our carefully designed prompt. High-Quality Clips (CH ): These clips were designated as high-quality and served as candidates for the DPO dataset. To ensure scenario diversity and account for varying annotation precision across different original data sources, clips were selected if their VQAScore [19] exceeded specific threshold τj,k defined for its original data source and motion aspect (e.g., {object, camera}). These VQAScore thresholds τj,k are detailed in Table S1. The set CH is formally defined as: CH = {c CA ExceedsVQAScoreThreshold(c)} (S2) where ExceedsVQAScoreThreshold(c) holds if the VQAScore of clip (from source j, for aspect k) is greater than the specific threshold τj,k for that source-aspect pair, as given in Table S1. Low-Quality Clips (CL): Clips that failed to meet the minimum quality criteria were eliminated. This includes clips whose VQAScore was below human-set minimum threshold τvL = 0.3. The set CL is defined as: CL = {c CA VQAScore(c) < 0.3} (S3) 3 Figure S2: The results corresponding to our three different thresholds are presented separately. Top: High consistency between text and video. The camera movement and changes in viewpoint are strictly described in chronological order, resulting in extremely high quality. Middle: Fairly good consistency between text and video. The actions of the main characters are described with reasonable accuracy, but some imprecise areas exist. Bottom: Relatively poor consistency between text and video, providing limited or erroneous information. Instruction Dataset Clips (CS): The remaining clips formed the SFT instruction dataset. This set is defined as: CS = CA (CH CL) (S4) SFT dataset construction and question types. The SFT dataset is constructed using the Instruction Dataset Clips (CS). For each clip in CS, we generate question-answer pair to fine-tune the models ability to understand and describe motion. To cover diverse aspects of motion understanding, we categorize our questions into three types: Object-centric, Camera-centric, and Mixed-focus. During the SFT data generation, one question is randomly selected from the pool of questions corresponding to the primary motion aspect (object, camera, or mixed) identified in the clips annotation. These SFT dialogues (question-answer pairs) are crucial as they also form the foundation for constructing the preference data for DPO. Object-centric Questions: These questions focus on the movement, actions, and interactions of objects within the video. Examples include: \"What objects are moving in this video?\" \"Can you describe the motion of objects in this video?\" \"What is happening to the objects in this scene?\" \"How are the objects moving in this video?\" \"Describe the movements of the main subjects in this clip.\" \"What actions are being performed by the objects in this video?\" \"How would you characterize the object motion in this scene?\" \"What kind of movement do you observe from the objects in this video?\" 4 Figure S3: Interactive annotation interface for DPO focused on fine-grained video motion understanding. This Python-based front-end allows annotators to choose between two textual descriptions (\"Option A\" and \"Option B\") for the same video clip, selecting the one that more accurately captures the nuanced motion in the video. The interface supports loading data in JSONL format and records annotator preferences, thereby providing data for the models preference learning. \"Describe the trajectory of the moving objects in this clip.\" \"How do the objects interact with each other in this video?\" Camera-centric Questions: These questions probe the cameras movement, techniques, and perspective. Examples include: \"How is the camera moving in this video?\" \"Describe the camera motion in this video.\" \"What camera techniques are used in this video?\" \"Is the camera stationary or moving in this clip?\" \"How does the camera angle change throughout this video?\" \"What kind of camera movements can you identify in this footage?\" \"How would you characterize the camera work in this video?\" \"Does the camera follow any specific subject in this video?\" \"What perspective does the camera provide in this scene?\" \"How does the camera movement contribute to the viewing experience?\" Mixed-focus Questions: These questions require comprehensive understanding of the interplay between object motion and camera work. Examples include: \"Describe the primary objects specific action, including its fine-grained motion. How does the cameras movement (e.g., tracking, zoom, pan) follow or frame this objects action, and what are the objects key visual attributes highlighted by this interplay?\" \"Considering the primary objects movement and its interaction with other elements, what is its implied goal? How does the cameras perspective (e.g., close-up, wide shot, pointof-view) and any dynamic changes in its movement contribute to or obscure this implied intention?\" 5 Table S1: VQAScore Thresholds (τj,k) for High-Quality Clip Selection (CH ), per Source and Motion Aspect. clip from source and aspect is included in CH if its VQAScore(c) > τj,k. Data Source (j) Object Motion (τj,object) Camera Motion (τj,camera) Kinetics-700 ActivityNet Charades Charades-Ego SSV2 OpenVid-1M 0.75 0.75 0.72 0.72 0.68 0.70 0.70 0.72 0.70 0.70 0.68 0.70 Table S2: Key criteria for human preference annotation in selecting textual descriptions of finegrained motion. Annotators chose the description that better satisfied these aspects. Criterion Guideline for Selection Key Questions 1. Accuracy 2. Granularity 3. Temporal Dynamics 4. Camera Movement Prefer more accurate identification & description of primary motion(s). - Core action correctly identified? - Agents/objects in motion correct? - Avoids misinterpreting actions? Prefer more fine-grained & detailed account of motion, capturing nuances. - Complex movements broken down? - Specific body/object details? - Overly general or specific? Prefer better capture of temporal aspects (sequence, duration, speed, rhythm). - Sub-actions order correct? - Pace/intensity conveyed? - Speed/tempo changes reflected? Prefer description that accurately identifies & describes significant camera movements (e.g., pan, tilt, zoom, tracking). - Camera movement (pan, tilt, zoom, dolly, static) correctly identified? - Effect of camera movement on scene understanding clear? - Distinguished from object motion? 5. Factual Correctness response Prefer factually grounded in visual evidence, no hallucinations. - Only visible elements/actions? - Contradicts visual information? - Infers unobservable intent? \"Analyze significant change in the primary objects motion or behavior. How does the cameras operation (e.g., sudden zoom, switch to slow motion, change in focus) coincide with and emphasize this specific change in the objects action?\" \"Discuss the overall pattern of the primary objects movement throughout key segment of the video. Correlate this with the dominant camera movement strategy used in that segment. How does this combined object-camera choreography affect the scenes narrative or the information conveyed about the objects activity?\" Preference dataset construction. The preference dataset, consists of preference pairs of the form (xi, ychosen ). These pairs were generated from the high-quality clips in CH . The process involved re-annotating these clips using Tarsier2 [44] and then incorporating human preference signals, as illustrated in Figure 5 of the main paper and more details in Section C. , yreject This rigorous, multi-stage curation methodology ensures the high quality of the MotionVid QA subsets, which are crucial for robust model training and evaluation in fine-grained motion understanding."
        },
        {
            "title": "C Guidelines for Human Preference Annotation",
            "content": "For DPO, annotators chose between two textual descriptions for video clip, selecting the one that better captured its fine-grained motion. To ensure fairness, the order in which these two descriptions were presented was randomized. The following guidelines ensured consistent, highquality annotations: To facilitate this process, we developed an interactive, python-based front-end for 6 Table S3: Results on FAVORBench and MotionBench of our MotionChat on MotionVid QA. w/ FT indicates with our fine-tuning, and w/o FT indicates without our fine-tuning. Model FAVORBench Metrics MotionBench Metrics Overall AVG AS HAC SAD MAD CM NSM Overall AVG MR LM CM MO w/ FT w/o FT 45.8 42.3 44.5 41.6 47.3 41.6 51.6 46.7 43.7 43.5 52.3 46. 30.1 30.9 42.2 40.6 54.8 53.0 50.6 48.8 61.0 58.3 58.8 55. 44.2 34.0 70.4 71.5 AO 40.3 39.5 RC 28.8 34. Figure S4: Quantitative results between baseline and our fine-tuned models trained using both SFT and DPO. For each case, the upper one is the Qwen2.5VL-7B baseline, and the lower one is our model after fine-tuning. Our fine-tuned model, trained on our dataset, demonstrates superior fine-grained motion perception capabilities and outperforms the baseline. user-friendly annotation, as shown in Figure S3. The relevant code can be found in the supplementary materials. Annotators selected the preferred response based on holistic evaluation of the criteria in Table S2, prioritizing superior understanding and articulation of fine-grained motion. Annotators were advised to review clips multiple times and compare descriptions against these criteria. When responses excelled in different areas, they selected the one most helpful for understanding fine-grained motion. Fine-tuning MotionChat on the Dataset For the finetuning experiments, we utilized 32 A100 GPUs with 80GB of memory each. The SFT process was conducted on the Qwen2.5-VL-7B-Instruct model, employing global batch size of 128. During SFT, the vision tower, LLM, and merger components were all trainable. During DPO training, we exclusively trained the LLM part. We conducted fine-tuning experiments to evaluate the effectiveness of our dataset, MotionVid QA. Specifically, we performed SFT on the publicly available Qwen2.5VL-7B model utilizing the instruc7 tion pairs from MotionVid QA. We designate the model fine-tuned with our complete approach as MotionChat. The quantitative results of these fine-tuning experiments are detailed in Table S3. On FAVORBench and MotionBench, our full MotionChat model achieves an overall improvement. These figures underscore the positive impact of both our dataset and the fine-tuning methodology towards fine-grained motion understanding. Qualitative evidence is presented in Figure S4. The figure illustrates sideby-side comparisons between the Qwen2.5VL-7B baseline and our further fine-tuned MotionChat via DPO. It is apparent that MotionChat exhibits enhanced fine-grained motion perception, more accurately interpreting complex motion narratives compared to the baseline model. While MotionChat itself is not positioned as major architectural innovation, its development serves as practical validation of MotionVid QAs utility. More importantly, we aim for MotionChat to be valuable asset for the research community. By providing readily usable model fine-tuned for motion understanding, we hope to facilitate further research and application development in areas requiring nuanced analysis of motion. MotionChat can serve as strong baseline or convenient tool for researchers, enabling them to more easily leverage the rich, motion-centric information curated within MotionVid QAand accelerate their own explorations in video understanding and motion-related tasks. We plan to release MotionChat to foster broader adoption and collaborative advancement within the field."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Nanjing University",
        "Nankai University"
    ]
}