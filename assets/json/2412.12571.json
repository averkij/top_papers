{
    "paper_title": "ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers",
    "authors": [
        "Lianghua Huang",
        "Wei Wang",
        "Zhi-Fan Wu",
        "Yupeng Shi",
        "Chen Liang",
        "Tong Shen",
        "Han Zhang",
        "Huanzhang Dou",
        "Yu Liu",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the inherent in-context generation capabilities of pretrained diffusion transformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks with minimal or no architectural modifications. These capabilities are unlocked by concatenating self-attention tokens across multiple input and target images, combined with grouped and masked generation pipelines. Building upon this foundation, we present ChatDiT, a zero-shot, general-purpose, and interactive visual generation framework that leverages pretrained diffusion transformers in their original form, requiring no additional tuning, adapters, or modifications. Users can interact with ChatDiT to create interleaved text-image articles, multi-page picture books, edit images, design IP derivatives, or develop character design settings, all through free-form natural language across one or more conversational rounds. At its core, ChatDiT employs a multi-agent system comprising three key components: an Instruction-Parsing agent that interprets user-uploaded images and instructions, a Strategy-Planning agent that devises single-step or multi-step generation actions, and an Execution agent that performs these actions using an in-context toolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench arXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with diverse instructions and varying numbers of input and target images. Despite its simplicity and training-free approach, ChatDiT surpasses all competitors, including those specifically designed and trained on extensive multi-task datasets. We further identify key limitations of pretrained DiTs in zero-shot adapting to tasks. We release all code, agents, results, and intermediate outputs to facilitate further research at https://github.com/ali-vilab/ChatDiT"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 1 7 5 2 1 . 2 1 4 2 : r CHATDIT: TRAINING-FREE BASELINE FOR TASK-AGNOSTIC FREE-FORM CHATTING WITH DIFFUSION TRANSFORMERS"
        },
        {
            "title": "TECHNICAL REPORT",
            "content": "Lianghua Huang* Wei Wang Zhi-Fan Wu Yupeng Shi Chen Liang Tong Shen Han Zhang Huanzhang Dou Yu Liu Jingren Zhou Tongyi Lab"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent research [Huang et al., 2024a,b] has highlighted the inherent in-context generation capabilities of pretrained diffusion transformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks with minimal or no architectural modifications. These capabilities are unlocked by concatenating self-attention tokens across multiple input and target images, combined with grouped and masked generation pipelines. Building upon this foundation, we present ChatDiT, zero-shot, general-purpose, and interactive visual generation framework that leverages pretrained diffusion transformers in their original form, requiring no additional tuning, adapters, or modifications. Users can interact with ChatDiT to create interleaved text-image articles, multi-page picture books, edit images, design IP derivatives, or develop character design settings, all through free-form natural language across one or more conversational rounds. At its core, ChatDiT employs multi-agent system comprising three key components: an Instruction-Parsing agent that interprets user-uploaded images and instructions, Strategy-Planning agent that devises single-step or multi-step generation actions, and an Execution agent that performs these actions using an in-context toolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench [Liang et al., 2024], comprising 100 real-world design tasks and 275 cases with diverse instructions and varying numbers of input and target images. Despite its simplicity and training-free approach, ChatDiT surpasses all competitors, including those specifically designed and trained on extensive multi-task datasets. While this work highlights the untapped potential of pretrained text-to-image models for zero-shot task generalization, we note that ChatDiTs Top-1 performance on IDEA-Bench achieves score of 23.19 out of 100, reflecting challenges in fully exploiting DiTs for general-purpose generation. We further identify key limitations of pretrained DiTs in zero-shot adapting to tasks. We release all code, agents, results, and intermediate outputs to facilitate further research at https://github.com/ali-vilab/ChatDiT. Keywords ChatDiT Zero-shot task generalization Diffusion transformers Image generation"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in text-to-image models have enabled the generation of high-quality images with remarkable fidelity to prompts [Ramesh et al., 2021, Esser et al., 2021, Ramesh et al., 2022, Rombach et al., 2022, Saharia et al., 2022a, Corresponding Author Emails: Lianghua Huang, Wei Wang, Zhi-Fan Wu, Tong Shen, Yu Liu, Jingren Zhou {xuangen.hlh, ww413411, wuzhifan.wzf, st456222, ly103369, jingren.zhou}@alibaba-inc.com, and Yupeng Shi (shiyupeng.syp@taobao.com). Chen Liang (liangchen2022@ia.ac.cn, Institute of Automation, Chinese Academy of Sciences), Han Zhang (hzhang9617@gmail.com, Shanghai Jiao Tong University) and Huanzhang Dou (hzdou@zju.edu.cn, Zhejiang University) contributed to this work during internships at Tongyi Lab. ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers Figure 1: Overview of the ChatDiT multi-agent framework. The framework consists of three core agents operating sequentially: the Instruction-Parsing Agent interprets user instructions and analyzes inputs, the Strategy-Planning Agent formulates in-context generation strategies, and the Execution Agent performs the planned actions using pretrained diffusion transformers. An optional Markdown Agent integrates the outputs into cohesive, illustrated articles. Sub-agents handle specialized tasks within each core agent, ensuring flexibility and precision in generation. Betker et al., 2023, Podell et al., 2023, Esser et al., 2024, Baldridge et al., 2024, Labs, 2024]. Additionally, variety of adapters have been developed to enhance the controllability of these models [Zhang et al., 2023, Ye et al., 2023, Huang et al., 2023, Ruiz et al., 2023, Wang et al., 2024a, Hertz et al., 2024]. However, real-world applications often involve complex requirements that surpass the limitations of existing adapters. For instance, generating picture book necessitates maintaining compositional consistency and intricate variations across multitude of elements. While recent efforts have sought to develop unified models capable of handling diverse tasks [Ge et al., 2023, Zhou et al., 2024a, Sheynin et al., 2024, Sun et al., 2024, Wang et al., 2024b], these approaches typically rely on large amounts of task-specific data and extensive multi-task training. Although such models exhibit zero-shot generalization capabilities, they tend to lack stability on unseen tasks, are challenging to scale, and fail to leverage abundant task-agnostic data effectively. Emerging work, such as Group Diffusion Transformers [Huang et al., 2024a], has proposed task-agnostic approach by utilizing group data for training. This method allows for the incorporation of diverse relational data sources, such as illustrated articles, video frames, and picture books, making the training data highly redundant. These models demonstrate the potential for zero-shot generalization across various tasks. Building on this, In-context LoRA [Huang et al., 2024b] simplifies the concept by highlighting the inherent in-context generation capabilities of text-to-image diffusion transformers. By fine-tuning these transformers with small dataset of 10100 image groups per task, In-context LoRA achieves impressive results across range of tasks. However, its reliance on per-task training limits its ability to generalize to unseen tasks. In this work, we aim to maximize the potential of the core observation underlying In-context LoRA [Huang et al., 2024b]: that diffusion transformers inherently possess in-context generation capabilities, and consequently, zero-shot task generalization potential. We propose training-free, zero-shot, interactive, and general-purpose image generation framework built directly upon diffusion transformers in their original form, without the need for fine-tuning, adapters, or structural modifications. We begin by introducing an in-context toolkit for diffusion transformers, enabling them to generate sets of images (instead of single outputs) conditioned on prompts and, optionally, reference set of images. The toolkit uses straightforward pipeline similar to that in In-context LoRA, where input and target images are concatenated into multi-panel layout, described by comprehensive prompt. The task then involves inpainting the target regions using visible input regions in training-free manner with blend diffusion [Avrahami et al., 2022]. This pipeline accepts prompts, zero to multiple reference images, and outputs one or more generated images. The core of our approach, ChatDiT, is multi-agent system comprising three main agents: 1. Instruction-Parsing Agent. This agent interprets user instructions and uploaded images to determine the number of desired output images and generate detailed descriptions for each input and target image. 2. Strategy-Planning Agent. Based on parsed instructions, this agent formulates step-by-step generation plan. Each step includes multi-panel prompt, selected reference image IDs (if applicable), and necessary parameters for image generation. 3. Execution Agent. Utilizing the in-context toolkit, this agent executes the planned steps, generating all target images through in-context operations. 2 ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers Figure 2: Selected single-round generation examples of ChatDiT on IDEA-Bench [Liang et al., 2024]. ChatDiT demonstrates versatility by handling diverse tasks, instructions, and input-output configurations in zero-shot manner through free-form natural language interactions. The user messages shown here are condensed summaries of the original detailed instructions from IDEA-Bench to conserve space. ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers Figure 3: Selected illustrated article generation examples of ChatDiT. ChatDiT is able to generate interleaved text-image articles based on users natural language instructions. It autonomously estimates the required number of images, plans and executes the generation process using in-context capabilities, and seamlessly integrates the outputs into coherent and visually engaging illustrated articles. 4 ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers Figure 4: Selected multi-round conversation examples of ChatDiT. By referencing images from the conversation history, ChatDiT is able to perform seamless multi-round generation and editing in response to free-form user instructions. This iterative process enables dynamic refinement and adaptation of outputs while maintaining contextual consistency across conversation turns. Key modifications specified in each instructional message are highlighted in yellow. ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers An optional Markdown Agent enables the generation of coherent, interleaved text-image articles, ensuring outputs are seamlessly formatted for readability. All agents are implemented using large language models (LLMs) and operate through JSON-based inputs and outputs (except for Markdown Agent outputs, which are text-based). The overall framework is illustrated in Figure 1. We evaluate ChatDiT on the IDEA-Bench [Liang et al., 2024], comprehensive benchmark consisting of 100 diverse design tasks and 275 test cases, covering wide range of instructions and input-output configurations. Example generation results are presented in Figure 2, with quantitative and qualitative comparisons against other approaches shown in Table 1 and Figure 5, respectively. Despite its simplicity and training-free nature, ChatDiT outperforms all competitors, including rephrasing-based text-to-image methods and specialized multi-task frameworks, demonstrating its zero-shot capabilities. We further highlight ChatDiTs versatility in Figure 3, which illustrates its ability to generate interleaved text-image articles, and in Figure 4, which showcases its multi-turn conversational outputs. While some imperfections remainsuch as difficulties in identity and detail preservation, and decline in instruction adherence when handling long contexts (i.e., an excessive number of inputs and/or outputs)ChatDiT establishes strong baseline. It also reveals the untapped in-context generation potential of pretrained diffusion models, offering valuable insights into how these models could be further enhanced for improved zero-shot generalization. Although ChatDiT achieves the best performance on IDEA-Bench, its score of 23.19 (out of 100) highlights the considerable gap that remains to achieve real-world, product-level general-purpose applications. This result underscores the challenges in fully exploiting the capabilities of diffusion transformers for highly complex tasks. We discuss the key limitations of ChatDiT in Section 4.6. To foster future research and innovation, we publicly release all code, agents, results, and intermediate outputs3."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Image Generation Text-to-image generation models have rapidly advanced in producing high-fidelity and stylistically diverse images from natural language prompts [Ramesh et al., 2021, 2022, Esser et al., 2021, Rombach et al., 2022, Saharia et al., 2022a, Betker et al., 2023, Podell et al., 2023, Chen et al., 2023, Esser et al., 2024, Baldridge et al., 2024, Labs, 2024]. Researchers have introduced various approaches to control specific attributes including identity preservation [Huang et al., 2023, Ye et al., 2023, Li et al., 2024, Wang et al., 2024a], color adaptation [Huang et al., 2023], style adaptation [Hertz et al., 2024, Huang et al., 2023], spatial composition [Zheng et al., 2023, Huang et al., 2023], pose guidance [Zhang et al., 2023], local editing [Meng et al., 2021, Lugmayr et al., 2022, Xie et al., 2022, Huang et al., 2023], object-level editing [Pan et al., 2023, Shi et al., 2023, Liu et al., 2024a], quality enhancement [Saharia et al., 2022b, Kawar et al., 2022, Xia et al., 2023, Li et al., 2023], and cross-image relationship modeling [Zhou et al., 2024b, Liu et al., 2024b, Yang et al., 2024]. While these methods address individual tasks, they rely on specialized training or adapters, limiting their applicability to broader, more complex tasks that involve multiple images and intricate relationships. 2.2 Unified Frameworks and Zero-Shot Generalization Several recent frameworks strive for generalization across wide range of generation tasks [Ge et al., 2023, Zhou et al., 2024a, Sheynin et al., 2024, Sun et al., 2024, Wang et al., 2024b, Huang et al., 2024a, Shi et al., 2024]. Emu Edit [Sheynin et al., 2024], Emu2 [Sun et al., 2024], Emu3 [Wang et al., 2024b], TransFusion [Zhou et al., 2024a], Show-o [Xie et al., 2024], OmniGen [Xiao et al., 2024], and other models demonstrate impressive versatility. Emu3, for example, extends text-to-image to video generation, while OmniGen aims at multi-modal tasks using large-scale training on curated datasets. Despite their breadth, these models typically rely on either explicit multi-task training or large-scale integration of diverse datasets. In contrast, recent studies [Huang et al., 2024b] indicate that standard text-to-image diffusion transformers already encode powerful in-context capabilities. In-context LoRA [Huang et al., 2024b], for instance, trains small LoRA adapters using modest number of image groups, revealing the models latent ability to handle multiple tasks without large-scale retraining. Our work takes step further by showing that even without such adaptation, pretrained diffusion transformers can exhibit remarkable zero-shot generalization. 3Project page: https://ali-vilab.github.io/ChatDiT-Page/ 6 ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers Table 1: Comparison of ChatDiT with other models across various tasks on IDEA-Bench [Liang et al., 2024]. Performance metrics are reported for different task types: T2I (Text-to-Image), I2I (Image-to-Image), Is2I (Image set to Image), T2Is (Text-to-Image set), and Is2Is (Image set to Image set). +GPT4o indicates that user instructions and uploaded images are reformulated into per-output-image prompts, enabling text-to-image models to generate results. The top two scores for each task are highlighted in red (best) and blue (second best). Task Type FLUX+GPT4o DALL-E3+GPT4o SD3+GPT4o Pixart+GPT4o InstructPix2Pix MagicBrush Anole Emu2 OmniGen ChatDiT T2I I2I Is2I T2Is Is2Is Avg. 46.06 12.13 4.89 20.15 29. 22.48 24.34 6.95 5.27 14.36 14.44 13.07 24.04 10.79 4.69 21.59 13.06 14.83 14.44 7.75 3.48 17.46 21. 12.90 0 17.58 0 0 0 3.52 0 19.07 0 0 0 3.81 0 0.64 0 1.74 0.48 17.98 7.05 8.98 0 0 6.80 21.41 8.17 2.77 0 0 6.47 50.91 21.58 2.36 27.77 13. 23.19 2.3 Multi-Agent Systems and Interactive Frameworks The rise of large language models (LLMs) [Radford et al., 2019, Brown, 2020, Touvron et al., 2023a,b, Dubey et al., 2024, Team et al., 2024] has inspired multi-agent architectures that leverage reasoning and planning for complex tasks [Durante et al., 2024, Wang et al., 2024c]. Agents can analyze inputs, plan strategies, and execute actions with tools or APIs. While multi-agent reasoning is commonly explored in the language domain, we integrate it into visual generation, using LLM-based agents to parse, plan, and execute multi-step workflows with diffusion transformers. This synergy between reasoning agents and latent diffusion models enables flexible, conversation-driven interface for complex image generation tasks."
        },
        {
            "title": "3 Method",
            "content": "3.1 Problem Formulation 3.1.1 Unified Group Generation Paradigm We adopt the image generation paradigm introduced in Group Diffusion Transformers [Huang et al., 2024a] and In-Context LoRA [Huang et al., 2024b], where image generation tasks are formulated as producing set of 1 target images, conditioned on another set of 0 reference images, alongside comprehensive prompt describing the combined (n + m) images. This unified formulation is highly versatile, accommodating wide range of design tasks, including picture book generation, storyboard creation, font design and transfer, identity-preserved generation, pose control, image editing, and IP derivation [Huang et al., 2024a]. In this framework, the relationships between reference images and target images are implicitly captured through group-wise consolidated prompts. By concatenating reference and target images into single multi-panel layout and pairing it with corresponding multi-panel prompts, we can seamlessly perform both reference-based and reference-free tasks. The flexibility of this approach lies in its ability to adapt to diverse task requirements simply by varying the number of panels and the configuration of input and output images. 3.1.2 Alignment with Human Intention While the group generation paradigm effectively unifies broad set of tasks, using multi-panel prompts and image concatenation as the primary interface can be cumbersome. Communicating design requirements through free-form natural language is far more intuitive, much like how consumer would convey ideas to an artist. Furthermore, when the context is long (i.e., involving many input and/or output images), conditioning or generating all these images simultaneously can significantly degrade performance, as text-to-image models struggle to map multiple panel descriptions accurately. To address this, we employ strategy combining parallel and iterative generation actions, preserving relationships between input and target images while maintaining fidelity to image-wise descriptions. The system translates user intent into format compatible with the in-context toolkit and plans generation strategies that handle large image sets effectively. This involves translating free-form natural language instructions and reference images into structured parameters, devising step-by-step strategies to ensure proper relationships between input and target images, and executing these steps to generate high-quality outputs using the in-context toolkit. 7 ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers Figure 5: Comparison of ChatDiT with existing approaches. This multi-agent system enables seamless, user-driven image generation framework that processes natural language instructions and outputs high-quality images, providing unified, training-free solution to diverse visual generation tasks. 3. In-Context Toolkit As demonstrated in previous works [Huang et al., 2024a,b], both reference-free and reference-based multi-image generation tasks can be reformulated as multi-panel image generation and inpainting tasks, which can be effectively handled by pure text-to-image models. In the case of inpainting, training-free approach is employed [Avrahami et al., 2022], where the visible regions of the target image are replaced by the corresponding reference image content, with different levels of Gaussian noise added at each denoising step. To ensure accurate image generation, the associated prompt must be comprehensive enough to describe the entire multi-panel content. To streamline these tasks, we developed an in-context toolkit that integrates essential functionalities such as panel merging and splitting, as well as prompt handling. This toolkit uses unified interface, simplifying user interaction and enabling seamless integration with the system. Specifically, the toolkit accepts multi-panel prompt and an image list as inputs and outputs corresponding image list, expressed as: output_images = pipe(prompt, input_images, num_outputs) This interface is designed for seamless compatibility with the Execution Agent, detailed in the following section. ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers 3.3 Multi-Agent System As illustrated in Figure 1, we have designed multi-agent system to interpret user intentions and generate outputs in free-form, task-agnostic manner. The system accepts natural language instructions, optionally accompanied by zero or more uploaded images, and produces one or more generated images. When required, the output can be formatted as an illustrated article. The system comprises three primary agents, each containing specialized sub-agents to handle specific responsibilities: Instruction-Parsing Agent: This agent interprets user instructions and processes the input images. It consists of three sub-agents: Counting Agent: Estimates the number of desired output images based on user instructions. Description Agent: Generates detailed descriptions for each uploaded input image to capture key attributes and context. Prompting Agent: Creates descriptions for the target images to guide the generation process. Strategy-Planning Agent: Based on the output from the Instruction-Parsing Agent, this agent formulates step-by-step generation strategy. It includes: Referencing Agent: Choose appropriate reference images for each output and organize references and outputs into groups. Panelizing Agent: Constructs in-context prompts for grouped references and outputs, preparing the input for the image generation pipeline. Execution Agent: This agent utilizes the in-context toolkit to execute the generation plan created by the Strategy-Planning Agent, producing the final output images. Additionally, Markdown Agent is optionally employed to format the generated images and accompanying descriptions into illustrated articles, such as storybooks or instructional content. Due to the limitations in long-context handling by DiTs, the Strategy-Planning Agent employs specific strategies to optimize the generation process: For text-to-images tasks, the number of panels is capped at 4 to ensure prompt adherence accuracy. If more than 4 outputs are required, subsequent images are generated iteratively, conditioned on the first 3 images. For images-to-images tasks, each output image is generated separately, referencing all input images to ensure consistency. For image-to-images tasks, generation is performed iteratively, conditioning each output on all input images and previously generated outputs. These strategies balance prompt adherence with the need to capture cross-panel relationships and maintain consistency across outputs. The multi-agent system leverages large language models (LLMs) for the Instruction-Parsing, Strategy-Planning, and Markdown Agents. The Execution Agent employs the in-context toolkit to handle the image generation tasks. JSONbased inputs and outputs are strictly enforced for LLM agents to ensure stability and consistency, except for the Markdown Agent, which outputs markdown-formatted text."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details We utilize the FLUX.1-dev text-to-image model [Labs, 2024] to build the in-context toolkit and the Execution Agent. The large language model (LLM) agentsInstruction-Parsing, Strategy-Planning, and Markdown agents are implemented using OpenAIs GPT-4o. For the inpainting task, we employ training-free approach [Avrahami et al., 2022], using the FluxInpaintPipeline to directly implement panel-wise inpainting for reference-based tasks, ensuring high-quality and contextually accurate image generation. 4.2 Evaluation Benchmark We evaluate the ChatDiT framework using the IDEA-Bench benchmark [Liang et al., 2024], which comprises 100 real-world design tasks, each with varied instructions and different input-output configurations. Spanning 275 cases, the ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers benchmark covers diverse range of tasks, including picture book creation, photo retouching, image editing, visual effect transfer, and pose transfer. ChatDiTs performance is compared against several general-purpose frameworks, including OmniGen [Xiao et al., 2024], Emu2 [Sun et al., 2024], Anole [Chern et al., 2024], InstructPix2Pix [Brooks et al., 2023] and MagicBrush [Zhang et al., 2024], along with text-to-image models [Labs, 2024, Esser et al., 2024, Chen et al., 2023] that incorporate language model rephrasing. These rephrasing-based models translate user-uploaded images and instructions into individual prompts for text-to-image generation. Although such models often fail to capture cross-image relationships, they serve as valuable baselines for comparison, as suggested by IDEA-Bench [Liang et al., 2024]. 4.3 Results on IDEA-Bench Table 1 presents the quantitative results, while Figure 2 provides example generation outputs, and Figure 5 visualizes comparisons between ChatDiT and other approaches across selected cases. In terms of overall performance, ChatDiT surpasses competing models, including those explicitly designed and trained on multi-task datasets [Xiao et al., 2024, Sun et al., 2024, Chern et al., 2024, Brooks et al., 2023, Zhang et al., 2024, Shi et al., 2024]. ChatDiT demonstrates strong performance in image-to-image and text-to-image tasks, showcasing its ability to generate high-quality outputs with strong contextual fidelity. However, challenges remain in tasks involving images-to-image and images-to-images scenarios, where extended context length and the complexity of managing multiple inputs and outputsoften with numerous elements or subjectsimpact consistency and overall performance. While ChatDiT exhibits significant capabilities, it still struggles with perfect identity and detail preservation, particularly in human portraits, animal representations, and fine product details. These limitations highlight areas for future enhancement, particularly in maintaining fine-grained visual consistency and accuracy. 4.4 Interleaved Text-Image Article Generation ChatDiT is able to generate interleaved text-image articles by interpreting user instructions alongside input and output image descriptions, converting them into markdown format using the Markdown agent. This process seamlessly integrates text and visuals to produce cohesive, engaging articles. Figure 3 highlights selection of curated examples. While the current implementation exhibits some imperfections, it demonstrates significant potential for creating interactive and dynamic interfaces. The ability to seamlessly blend text and images paves the way for further enhancements, such as more sophisticated formatting, improved narrative coherence, and expanded functionality in future iterations. 4.5 Multi-Round Conversation Figure 4 showcases examples of multi-round conversations using ChatDiT, where the system performs iterative generation and editing based on dynamic, free-form user instructions. By referencing previously generated images and maintaining contextual awareness across conversation turns, ChatDiT is able to refine outputs while preserving consistency and fidelity to user intent. Although ChatDiT demonstrates promising performance in many cases, challenges remain in preserving fine-grained details and maintaining consistent identity, particularly as the complexity of the conversation increases. Furthermore, cumulative errors can significantly affect performance as the conversation lengthens. Addressing these limitations represents an exciting opportunity for future enhancements. 4.6 Limitations of ChatDiT While ChatDiT demonstrates zero-shot generalization capabilities across variety of visual generation tasks, several limitations remain that highlight areas for further improvement. We summarize these limitations as follows: 1. Insufficient Reference Fidelity. ChatDiT struggles to accurately reference details from input images, particularly for maintaining identity and fine-grained details of characters, animals, products, or scenes. While the model can capture overall composition and themes, discrepancies often arise in style consistency, identity preservation, and other nuanced visual attributes. 2. Limited Long-Context Understanding. The models performance deteriorates significantly as the number of input or output images increases. When handling long-context scenarios, such as generating large image sets or processing many reference images, ChatDiTs semantic understanding and generation quality drop noticeably, leading to reduced coherence and visual fidelity. 10 ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers 3. Deficiencies in Expressing Narrative and Emotion. ChatDiT exhibits limited capability in generating content with strong narrative flow, emotional depth, or story-like qualities. This shortcoming can be attributed to the inherent challenges of text-to-image models in capturing and expressing emotions or complex storydriven scenes. Additionally, the model tends to simplify complex scenes, favoring the generation of visually straightforward outputs. 4. Weak High-Level In-Context Reasoning. ChatDiT has difficulty performing advanced in-context tasks. For example, when provided with group of input-output image pairs and new input, the model often fails to infer the desired action or generation task. This limitation highlights the models current inability to generalize higher-order relationships or abstract reasoning across in-context examples. 5. Limited Handling of Multi-Subject or Multi-Element Complexity. ChatDiT struggles to manage scenarios involving multiple subjects or elements, such as interactions between characters, crowded scenes, or objects with intricate relationships. In such cases, the generated outputs often lose compositional consistency, resulting in incoherent or incomplete representations. Addressing these limitations will require advancements in fine-grained reference alignment, long-context comprehension, narrative and emotional generation, and improved reasoning capabilities in in-context settings. These findings provide foundation for future research aimed at enhancing the general-purpose capabilities of diffusion transformers."
        },
        {
            "title": "5 Conclusion and Discussion",
            "content": "In this paper, we presented ChatDiT, novel zero-shot, general-purpose, and interactive visual generation framework based on pretrained diffusion transformers. Leveraging the inherent in-context generation capabilities of diffusion models, ChatDiT allows users to seamlessly create complex multi-image outputs, edit images, generate interleaved textimage articles, and design character settingsall with minimal user input and no additional fine-tuning or architectural modifications. By incorporating multi-agent system, we enable high degree of flexibility and customization, handling user instructions in natural language and transforming them into structured, step-by-step generation plans. Despite ChatDiTs zero-shot capabilities, several limitations remain. These include challenges with long-context handling, where performance drops as input-output complexity increases, and issues with fine-grained detail preservation, particularly in human faces, animals, and intricate designs. Additionally, ChatDiT struggles with high-level reasoning and generating narratives with emotional depth. Future improvements should focus on enhancing long-context understanding, fine-tuning for specific domains, and improving reasoning across complex scenarios. Addressing these will expand ChatDiTs potential for more nuanced and consistent visual generation across diverse tasks. 11 ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers"
        },
        {
            "title": "References",
            "content": "Lianghua Huang, Wei Wang, Zhi-Fan Wu, Huanzhang Dou, Yupeng Shi, Yutong Feng, Chen Liang, Yu Liu, and Jingren Zhou. Group diffusion transformers are unsupervised multitask learners. arXiv preprint arXiv:2410.15027, 2024a. Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024b. Chen Liang, Lianghua Huang, Jingwu Fang, Huanzhang Dou, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Junge Zhang, Zhao Xin, and Yu Liu. Idea-bench: How far are generative models from professional designing? arXiv preprint arXiv:2412.11767, 2024. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022a. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. Black Forest Labs. Flux: Inference repository. https://github.com/black-forest-labs/flux, 2024. Accessed: 2024-10-25. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024a. Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47754785, 2024. Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024a. 12 ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2249022499, 2023. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model, 2022. Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435, 2023. Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, and Shengfeng He. Drag your noise: Interactive point-based editing via diffusion semantic propagation, 2024a. Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image superresolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4):47134726, 2022b. Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In Advances in Neural Information Processing Systems, 2022. Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration, 2023. URL https://arxiv.org/abs/2303.09472. Xin Li, Yulin Ren, Xin Jin, Cuiling Lan, Xingrui Wang, Wenjun Zeng, Xinchao Wang, and Zhibo Chen. Diffusion models for image restoration and enhancementa comprehensive survey. arXiv preprint arXiv:2308.09388, 2023. Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. arXiv preprint arXiv:2405.01434, 2024b. Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm - open-ended visual storytelling via latent diffusion models. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 61906200, 2024b. Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024. Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. 13 ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, et al. Gemini: family of highly capable multimodal models, 2024. Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, et al. Agent ai: Surveying the horizons of multimodal interaction. arXiv preprint arXiv:2401.03568, 2024. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6): 186345, 2024c. Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36, 2024."
        }
    ],
    "affiliations": [
        "Alibaba Inc.",
        "Institute of Automation, Chinese Academy of Sciences",
        "Shanghai Jiao Tong University",
        "Taobao",
        "Tongyi Lab",
        "Zhejiang University"
    ]
}