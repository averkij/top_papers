{
    "paper_title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
    "authors": [
        "Tong Wei",
        "Yijun Yang",
        "Changhao Zhang",
        "Junliang Xing",
        "Yuanchun Shi",
        "Zongqing Lu",
        "Deheng Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR."
        },
        {
            "title": "Start",
            "content": "GTR-Turbo: Merged Checkpoint is Secretly Free Teacher for Agentic VLM Training 2025-12-16 1Tsinghua University Tong Wei1, Yijun Yang2 , Changhao Zhang1, Junliang Xing1, Yuanchun Shi1, Zongqing Lu3, Deheng Ye2 2Tencent AI Lab wt22@mails.tsinghua.edu.cn, yijun.steven.yang@gmail.com, zhangcha25@mails.tsinghua.edu.cn, jlxing@tsinghua.edu.cn, shiyc@tsinghua.edu.cn, zongqing.lu@pku.edu.cn, dericye@tencent.com 3Peking University 5 2 0 D 5 1 ] . [ 1 3 4 0 3 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon visionlanguage models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) (50) and On-Policy Distillation (24), but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as free teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the entropy collapse observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 1030% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR."
        },
        {
            "title": "Introduction",
            "content": "Vision-language models (VLMs) have evolved beyond simple static multi-modal question-answering systems, demonstrating the capability to perceive, reason, and act autonomously in interactive environments to achieve specific goals. Reinforcement learning with verifiable outcome rewards (RLVR) (37) enables such models to be fine-tuned directly through verifiable reward signals provided by the environment dynamics, effectively replacing learned reward models. This approach has shown remarkable success in domains such as mathematics and code generation (28; 27; 13; 42; 6). However, when applied to multi-turn agentic tasks, vanilla RL methods often struggle due to sparse rewards, long-horizon trajectories, and noisy environments. These challenges can lead to incomplete, inconsistent, and low-diversity responses and actions, ultimately degrading performance, referred to as thought collapse (50) or similar concepts (e.g., entropy collapse) in many recent literatures (40; 47; 5). To this end, methods such as Guided Thought Reinforcement (GTR) (50) and On-Policy Distillation (24) have introduced new paradigm for multi-turn agent training by distilling knowledge from larger and stronger teacher model that provides real-time guidance or correction. They effectively regulate the agents reasoning process and significantly improve the following action quality. However, in order to obtain the most appealing performance, such paradigm usually relies on expensive, often privileged models as the teacher, leading to severe constraints on scalability, including high computational cost, longer training time, and the potential inaccessibility of cutting-edge models. In this paper, we propose GTR-Turbo, highly efficient solution to the above challenge. We highlight that merging historical checkpoints generated throughout the RL training secretly creates capable teacher for guidance, completely free of additional training or external model dependency (see Figure 3 for visualized explanation). This design not only preserves the automated reward mechanism, flexibility, and final performance of the original GTR method, but also significantly accelerates training, reduces computational overhead, and thus achieves superior scalability. Specifically, after each PPO update (36), we save the model weights and include them in our checkpoint buffer. By employing the TIES merging technique (56), we effectively avoid parameter interference among these models. The resulting merged model aggregates previous experiences and consistently outperforms the current training agent, sufficiently serving as teacher (see Figure 2 for proof-of-concept result). The guidance information can be utilized either through SFT-based online imitation learning, similar to the original GTR framework, or via logit Corresponding Author. Project Lead. distillation with KL regularization, which replaces the autoregressive generation with single forward pass to further improve training efficiency and encourage exploration. As illustrated in Figure 3, GTR-Turbo is flexible, scalable, and self-evolving agent training method, which does not need external supervision signals from the API model or human annotation but enables reliable and controllable decision-making in complex and challenging visual agentic environments. Empirically, we post-train the Qwen2.5-VL-7B model (2) using GTR-Turbo. In the complex Points24 card game task (66), the resulting agent achieves state-of-the-art (SOTA) performance, surpassing both GTR and other agent models while requiring only 50% training time, absolutely zero API calling, and 40% of the compute cost, demonstrating remarkable efficiency improvement. In the widely adopted and more challenging ALFWorld visual environment (39), where the observation only consists of images, an episode can span over 50 steps, and rewards are extremely sparse, typically provided at the end of tasks, GTR-turbo still achieves rapid and stable improvements on task success rate, outperforming many baselines with similar model size. Furthermore, we conduct comprehensive ablation studies to evaluate different design choices and configurations."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Agent Training for LLMs and VLMs Training agents that utilize general-purpose foundation models to solve specific decision-making problems has long been central research topic. Early work primarily focuses on training-free prompting techniques or additional adapter modules to fixed base models (49; 61; 62; 48; 45; 16; 30; 1; 38; 52). For multimodal tasks built upon VLM backbones, common approaches involve translating observations into textual descriptions or aligning their embeddings with LLMs (1; 7; 11; 17; 26; 41; 59; 3; 69). However, such limited model adjustments struggle to cope with dynamic and complex environments, restricting the agents robustness and adaptability. More recent studies have focused on using RL to obtain improved agent policies through interaction with the environment. Beyond traditional PPO (36), variants such as GRPO (37) and DAPO (64) enhance training stability and sample efficiency, together with long Chain-of-Thought reasoning and inference-time scaling, showing strong performance in single-turn tasks such as math and code. For multi-turn agentic tasks, concurrently with our work, variety of large-scale RL training systems have emerged (9; 47; 67; 20). In the context of general visual reasoning, RL4VLM (66) introduced foundational framework that directly applies PPO to VLM post-training, providing reference point for many subsequent studies (50; 46). 2.2 Process Guidance Providing Dense Rewards Sparse reward has always been one of the core challenges in reinforcement learning. Prior research on deep-thinking LLMs has affirmed the critical role of process supervision in enhancing the logical consistency of reasoning (37). One approach trains Process Reward Model (PRM) to assess the reasoning process (23; 43), but requires costly human annotations to obtain high-quality data. Another solution focuses on credit assignment (44; 4; 65; 8), which decomposes the final reward into finer-grained signals to better attribute contributions across intermediate reasoning steps. Additionally, many studies have sought to enhance process guidance by leveraging large models, such as LLM-as-a-judge mechanisms (68; 10; 54; 60), automated label generation (50), or the use of world models to provide future information (46). 2.3 Model Merging Techniques Merging the weights of multiple models is well-established technique in the machine learning community for enhancing model capability (57). Studies have demonstrated that merging models trained on different downstream tasks can produce unified and more versatile model (18; 58; 63); combining models under varying hyperparameter settings can further boost performance (51); and merging historical checkpoints can help escape local optima while mitigating catastrophic forgetting (15; 21). Recently, model merging techniques have also found rapid adoption in large language models, proving effective in both pre-training (34; 22) and post-training stages (18; 63; 21). Beyond simple averaging, studies have explored many additional model merging techniques. Fisher Merging (25) estimates the importance of each parameter using the Fisher Information Matrix; Task Arithmetic (18) constructs task vectors and performs arithmetic operations; TIES-merging (56) introduces trimming and sign elections to mitigate parameter interference; and DARE (63) incorporates random dropouts to enhance the effectiveness of multi-task model integration."
        },
        {
            "title": "3 The GTR-Turbo Framework",
            "content": "3.1 Revisiting Guided Thought Reinforcement Guided Thought Reinforcement (GTR) (50) is an automated reinforcement learning framework designed for training VLM agents in multi-turn decision-making tasks. As shown in Figure 1, it leverages VLM-as-acorrector mechanism, where an external VLM model acts as corrector to evaluate and refine the agents reasoning at each RL step. By jointly performing SFT on reasoning tokens and PPO updates on action tokens simultaneously, GTR effectively solves thought collapse caused by the absence of verifiable thinking rewards. GTR also incorporates Dataset Aggregation (DAgger) (33) to mitigate the distribution shift issue that arises during the dynamic RL training. Formally, we denote the agents observation as o, thought output as th, and action as a, given agent model πθ and corrector model πcorr. represents the PPO data buffer and denotes the thought data buffer. If we term [l] as the l-th token and [< l] as the first tokens, then the objective of GTR can be represented as: Figure 1: Illustration of the original GTR framework (50). It uses multi-modal API model as the corrector, such as GPT or Gemini, to evaluate and refine the agents reasoning content (i.e., thought tht) at each RL step, which is costly, time-consuming, and potentially inaccessible, constraining its own scalability. min θ (o,a)B LPPO(o, a) + LSFT(o, πcorr(o, th)), (o,th)D where LPPO(o, a) = min (cid:18) πθ(ao) πθold (ao) Aπθ (o, a), clip (cid:18) πθ(ao) πθold (ao) , 1 c, 1 + (cid:19) Aπθ (o, a) (cid:19) , LSFT(o, th) = log πθ(th[l]o, th[<l]). (1) (2) Corrector Model Performance Token Usage (Cost) Time GPT-4o Qwen2.5-VL-72B Qwen2.5-VL-7B Although GTR has achieved remarkable progress across multiple tasks, it comes with significant prerequisites and costs. First, GTR requires larger and more powerful external model, ensuring the correctness to serve as reliable teacher. However, such models for downstream domains are sometimes inaccessible in practice, and their capability directly impacts the quality of training. Moreover, when using closed-source models such as GPT or Gemini as teachers, the need for step-level online API calls severely slows down training and incurs substantial expenses. As shown in Table 1, using GPT-4o (even light-weight model in the GPT family) as the teacher requires approximately four days and costs around 150 USD to post-train LLaVA-1.6-7B with GTR for 15,000 steps. Employing smaller teacher models can reduce the overhead but degrade the final performance, even failing to provide meaningful thought guidance. Table 1: Training time and token usage of the GTR framework. Experiments to train the LLaVA-v1.6mistral-7B model for 15,000 steps on the Points24 task, using different models as the corrector. * - The corrector model fails to provide valid thought guidance. 33.5M ($146.56) 33.8M ($18.59) 31.2M ($4.29) 17.5% 6.5% 0%* 86h 110h 56h In this paper, we introduce an elegant solution to the above limitation: merging the historical checkpoints generated during the RL training constructs teacher model for free, as shown in Figure 3, thereby eliminating the dependence on expensive external models. Empirical results demonstrate that this approach can achieve comparable and even superior performance while dramatically reducing both training time and token cost. 3.2 Merged Checkpoints as the Teacher As an efficient model adaptation method, model merging has been widely adopted in the post-training stage. It includes merging heterogeneous models trained on different downstream tasks, enabling continual learning and capability expansion (18; 58; 63), or merging homogeneous models trained on the same task, achieving stronger overall performance (51; 15; 21). In GTR-Turbo, we design buffer that consists of historical model checkpoints along the agents RL training progress. The merged model in the k-th update epoch is formulated by Equation 3: (k) merged = π k1 i=1 wiπ (i) θ . 3 (3) The merged teacher does not need additional training and, by optimizing over smoother loss surface while effectively preserving past experiences, leads to better model. To validate this statement, we use checkpoint trajectory produced by training Qwen2.5-VL-7B on Points24 with GTR. At each update, we evaluate the current model as well as the merged model obtained from all preceding checkpoints. As shown in Figure 2, the merged model is more stable and has better performance, providing capable teacher. Figure 2: The performance comparison of the merged checkpoint and the current checkpoint on Points24. We adopt the Qwen2.5-VL-7B as the base model and highlight that model merging leads to stronger and more stable agent π (k) merged (red line) that can serve as teacher to guide the following RL for training π . (k) θ Merging Method Directly merging all parameters of checkpoints can introduce harmful interference, where changes in redundant parameters affect the models performance after merging. To avoid this issue, we adopt the Trim, Elect Sign, and Merge (TIES) method (56), which consists of three steps, (1) Trimming: redundant parameter changes are removed by retaining only those with magnitudes in the top-k%; (2) Sign election: for each parameter, we compute the total magnitude of its positive and negative values across all models and apply majority vote to determine the elected sign vector; (3) Selective averaging: only parameters whose signs match the elected sign are included in the merging computation. This procedure mitigates the influence of minor perturbations, ensuring tractable merging process. Weight Adjustment Variants Adjusting weights for every checkpoint results in different variants of merging methods. We study two commonly used strategies in this work: Simple Moving Average (SMA) and Exponential Moving Average (EMA). SMA treats all checkpoints equally and computes the arithmetic mean of them. EMA prioritizes more recent checkpoints by applying sequence of decayed weights with smoothing factor α: (k) merged = π 1 1 (k) merged = α π π k1 i=1 (k1) θ π (i) θ , + (1 α) π (k1) merged. (SMA) (EMA) (4) 3.3 Thought Guidance via Supervised Fine-tuning After merging checkpoints, we can replace the corrector model (red dashed box in Figure 1) in the original GTR framework with the new merged teacher model. Since the teacher and agent are homologous, they take the same input. Similar to GTR, GTR-Turbo (SFT) implements the guidance by minimizing the supervised fine-tuning loss between thought tokens generated by two models, as demonstrated in Figure 3 (a). During each step of RL, after the agent generates its thought and action based on the given observation, the same information is provided to the teacher to produce reference thought, which is then stored in the database. In subsequent PPO updates, we sample thought data pairs to compute the SFT loss, which is added to the original PPO loss for back propagation. Similar to GTR, we also incorporate format rewards and DAgger (33) techniques to further stabilize training. If ˆth represents the thought output of the teacher model πmerged(o), the optimization target can be written as: min θ (o,a)B LPPO(o, a) + LSFT(o, ˆth). (o, ˆth)D (5) 3.4 Soft Logit Distillation via Minimizing Reverse KL Divergence As discussed in GTR, when the models ability is limited and its baseline task success rate is low, using naive numerical scores for process guidance proves ineffective. This is because scalar rewards fail to convey the precise informational granularity required for RL, often leading to passive exploration or reward hacking. GTR therefore 4 Figure 3: Overview of the GTR-Turbo framework. Beyond the GTR training of VLM agents (Figure 1), GTR-Turbo stores historical checkpoints and merges them into teacher model (blue region), and then incorporates the PPO update (orange region) with thought guidance by minimizing either SFT loss (green region) or KL divergence (purple region), enabling flexiable, scalable, and self-guided agentic RL training. adopts SFT-based online imitation thought guidance, which successfully injects the knowledge and reasoning pattern of an external model into the RL training process, achieving rapid improvement. However, we observe that once the model acquires certain level of capability, the situation changes. In this case, the stabilization effect of thought guidance becomes more critical than its role in knowledge injection, making it feasible to adopt more relaxed constraint. To this end, we compute the negative KL divergence between the agent and the teacher as thought reward, encouraging the agent to align its token-level output distribution with that of the teacher. Using KL divergence offers non-trivial advantages. First, since it is grounded in the models logit outputs, it is almost unhackable. smaller KL value directly indicates closer alignment between the agent and teacher outputs, reaching zero when they are identical. Second, KL divergence captures the probability information over all candidate tokens, whereas an SFT label presents as one-hot supervision for the target token. Finally, calculating KL divergence requires only single forward inference, making it highly efficient. This KL variant also removes the need for an additional thought dataset in GTR, saving memory consumption. Previous research (12; 53; 24) has proven the advantages of using reverse KL for knowledge distillation. As shown in Figure 3 (b) and Equation 6, given the thought output, we compute the reverse KL between the agent and the teacher, average over all tokens, and take its negative value as an auxiliary reward for PPO updates, which is required approach for multi-step RL optimization. Since this sentence-level KL estimation may yield negative values and produce misleading reward signals, we clip the negative parts to ensure the rewards validity (see Section 4.3 for detailed analysis). max θ (o,(th,a))B (cid:2)min (cid:0)rA, clip (r, 1 c, 1 + c) A(cid:1)(cid:3) , (6) in which, = πθ(ao) πθold (ao) RevKL(πθ, πmerged; th) = l"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup , = Aπθ (o, a) RevKL(πθ, πmerged; th), (cid:104) log πθ(th[l]th[<l]) log πmerged(th[l]th[<l]) (cid:105) . Environments Our experiments were conducted on two widely used and challenging visual agentic benchmarks: (1) Points24 (66) and (2) ALFWorld (39). In Points24, the model must first perform fine-grained poker card recognition based on purely visual observation, followed by language and mathematical reasoning. At each step, the agent decides which number or operator to append next, ultimately forming formula equal to 24. Episodes sometimes involve more than 10 steps and require domain-specific skills such as arithmetic reasoning, making it highly complex task. ALFWorld is multimodal embodied simulator featuring diverse household tasks. The agent needs to navigate in unfamiliar environments, locate and interact with novel objects to reach certain goals, which poses substantial challenges in visual perception, long-horizon planning, and common-sense reasoning. The length of ALFWorld tasks can exceed 50 steps, and each step involves more than 20 possible actions, creating an exploration space that surpasses large portion of existing VLM benchmarks, including many GUI device control environments (31; 32; 55). Both tasks only use sparse rewards. Although intermediate action legality checking provides small 5 Model SR(%) ER CNN+RL* GPT-4o GPT-4o + Tool Qwen2.5-VL-72B Qwen2.5-VL-32B Qwen2.5-VL-7B-sft RL4VLM GTR GTR-Turbo (SFT) GTR-Turbo (KL) 0 2.5 13.5 5.6 0 22.0 3.5 44.5 48.0 53.5 -1.12 -6.35 -3.59 -5.69 -7.25 -3.2 -13.3 0.53 1.32 2.39 Figure 4: Training curves on the Points24 game environment. While GTR benefits from external knowledge in the early stage, our GTR-Turbo framework is also able to maintain rational reasoning process and ultimately achieves the best overall performance. All curves are smoothed for better readability. All experiments employ the early-truncation strategy introduced by GTR for fair comparison. Table 2: Evaluation result of different models on the Points24 task. GTR-Turbo significantly outperforms other RL training methods and commercial models in both success rate (SR) and episode return (ER). * - Reported in previous work. step-wise rewards between -1 and +1, final rewards come only upon task completion (+10 for Points24 and +50 for ALFWorld). This makes them significantly more difficult than tasks with process rewards or guidance. Baselines We select other competitive multi-turn VLM agent RL training frameworks as our baselines. RL4VLM (66) directly applies PPO optimization on raw environment rewards. GTR (50) introduces thought guidance via an external GPT-4o corrector model for online imitation learning, enabling rapid improvement and representing the SOTA results. In addition, we include several privileged API models for comprehensive comparison. Training Details Following previous work, we adopt widely used base model, Qwen2.5-VL-7B-Instruct. We start from an SFT-initialized model that has necessary domain knowledge, aligning with prior studies. The full configuration details are provided in the appendix. To better evaluate the long-term training stability, we trained the agent for 30,000 steps for Points24 and 20,000 steps for ALFWorld, which are 2x and 4x longer than previously reported results, respectively. We used 2 NVIDIA GPUs, one deploying the merged checkpoint teacher and the other training the LoRA (14) finetuned agent via PPO. 4.2 Effectiveness of the GTR-Turbo Framework Points24 Figure 4 demonstrates the training curves of all finetuning methods. RL4VLM suffers from thought collapse, where the outputs become repetitive, incoherent, and templated. Consequently, both task success rate and episode return rapidly decline until the agent never succeeds. By introducing an external GPT-4o model with tool use, GTR enables the agent to quickly distill knowledge from the corrector model, leading to improvement in the early stage. However, as training progresses, the fixed external model cannot obtain or accumulate additional knowledge, consequently limiting further learning. In contrast, our GTR-Turbo approach, in the absence of any external knowledge, also achieves stable and consistent improvement purely through sparse environmental feedback. Ultimately, the GTR-Turbo (SFT) reaches performance comparable to GTR, while the GTR-Turbo (KL) version further surpasses all baselines. In Table 2, we present the final evaluation results of different RL methods, open source models, and priviledged API models on the Points24 task. GTR-Turbo achieves the best performance. The finetuned smaller model can easily outperform general-purpose models that are over 10 times larger in tasks that require specialized domain knowledge. GTR-Turbo thus offers promising approach for post-training VLM agents. ALFWorld As illustrated in Figure 5 and Table 3, the baseline RL4VLM still exhibits model collapse, which undermines its training effectiveness. In such complex tasks that require long-horizon reasoning, advanced models demonstrate substantially superior capabilities and thus can provide richer and more accurate guidance for RLtrained models, enabling rapid early increase. closer analysis shows that external knowledge markedly reduces the need for exploration, allowing the agent to learn correct trajectories directly. However, GTR-Turbo has no access to such extensive expert knowledge and must instead rely solely on its own exploratary experience collection. Even under this unfair comparison setting, GTR-Turbo (KL) attains performance on par with GTR while offering better efficiency and generalizability. These findings highlight the strong potential of GTR-Turbo for training agents in more challenging tasks, in which no mature experts currently exist. Training Time and Cost Table 4 provides an intuitive comparison of the computational overhead across training methods in both environments. The cost estimates include only the additional expenses needed to implement each 6 Model Average Pick Clean Heat Cool Look Pick2 CNN+RL* LLaMA-Adapter* GPT-4o Qwen-2.5-VL-72B Qwen-2.5-VL-32B Qwen-2.5-VL-7B-sft RL4VLM GTR GTR-Turbo (SFT) GTR-Turbo (KL) 0 0.13 0.42 0.32 0.21 0.08 0.08 0.16 0.12 0.15 0 0.17 0.53 0.38 0.44 0.26 0.40 0.44 0.36 0.40 0 0.10 0.67 0.14 0.25 0.07 0 0.14 0 0.09 0 0.27 0.22 0.50 0 0.09 0 0 0.13 0 0.22 0 0.27 0.2 0 0 0.14 0 0.14 0 0 0.25 0.63 0.16 0.05 0 0.05 0.125 0.08 0 0 0.47 0.21 0.2 0 0 0 0 0.07 Figure 5: Comparison of training curves in the ALFWorld environment. Without relying on any powerful external models, GTR-Turbo achieves comparable performance purely through its own exploration, experience, and thought guidance. Table 3: Comparison of success rates across different models in the ALFWorld environment. We present the peak performance in the training curve for RL methods. GTR-Turbo achieves the same task success rate compared to GTR with significantly less training time and lower computational cost, maintaining excellent performance under its model scale. * - Reported in previous work. training method and exclude the base cost of finetuning the VLM agent itself. For GTR, we calculate the cost based on the token number through API calls. For GTR-Turbo, since it requires an additional GPU to deploy the teacher model, we estimate its cost using the hourly deployment cost of single GPU multiplied by the total training time. It is also important to note that both OpenAI API pricing and GPU costs may fluctuate lot due to market conditions. Moreover, commercial model deployments often have proprietary cost-reduction techniques and economies of scale, which may also affect the precision of our cost estimates. The original RL4VLM is regarded as the baseline and does not produce additional overhead; however, its training performance is sub-optimal. GTR, on the other hand, leverages large external models such as GPT-4o to guide RL training, but the associated API costs are difficult to control. Additionally, this approach leads to network latency and data security issues. As result, GTR requires much longer training time and also considerable expense, both of which limit its real-world applicability. Our proposed GTR-Turbo is an elegant solution to this dilemma. By replacing costly external model calls with internal generation, the SFT-guided GTR-Turbo already achieves noticeable reductions in overall time and cost. The KL-guided variant further brings training time close to RL4VLM, which is roughly half of GTR. It also reduces monetary cost to as low as 40% of GTR. Moreover, in scenarios where cutting-edge models are inaccessible or data security is crucial, the fully self-contained and locally deployable GTR-Turbo shows unparalleled advantage. Env Method SR Time Cost Estimation Points24 ALFWorld RL4VLM GTR (GPT-4o) GTR-Turbo (SFT) GTR-Turbo (KL) RL4VLM GTR (GPT-4o) GTR-Turbo (SFT) GTR-Turbo (KL) 4% 86h 41% 191h 48% 168h 54% 89h 8% 70h 16% 164h 12% 118h 15% 78h $0 $307.78 / 70.35M tokens $216.72* $114.81* $0 $145.76 / 30.94M tokens $152.22* $100.62* Table 4: Computation Time and Cost Comparison. GTR-Turbo has comparable or even superior performance to GTR with significantly shorter training time and lower cost. Reported costs account only for additional overhead (excluding the base cost of agent training) and may fluctuate with market conditions. SR - task success rate, * - Estimation based on the deployment cost of an additional GPU. 4.3 Ablation Studies and Discussions Effectiveness of TIES Merging We conduct an experiment on Points24 between TIES merging and the traditional linear averaging method (18) to demonstrate the effectiveness of this technique. Results in Figure 6 show that TIES can indeed boost performance by mitigating the interference of redundant parameters and sign disagreement, allowing the merged model to better preserve and integrate learned capabilities. Meanwhile, linear merging also yields reasonable training gains, confirming the validity of the merged checkpoint as teacher. Range of Guidance In Figure 7, we try guiding the agents full response, including both the thought and action. Consistent with observations from GTR, this approach is less effective. Combined with earlier results showing the 7 Figure 6: Performance comparison with and without TIES merging. The superior performance of TIES demonstrates its robustness in the merging process, effectively enhancing the quality of the teacher model and improving the overall training gains. Figure 7: Comparing different ranges of guidance. Guiding full responses, including both the thoughts and actions simultaneously, is less effective, primarily because it limits the models exploration, process that is crucial for self-evolution in GTR-Turbo. advantage of KL-based guidance over SFT, we argue that for self-contained system like GTR-Turbo, efficient exploration of the environment is critical, as it directly determines the diversity of environmental feedback and experience accumulation. Guiding actions or imposing stronger SFT constraints may improve the agents ability to imitate the teacher, but at the cost of restricting the agents exploratory freedom, thus limiting overall ability to adapt to the environment. Different KL Estimation Methods Considering the exponentially large output space of VLMs, KL divergence computation cannot be done analytically from logits. Instead, it needs Monte-Carlo sampling. Consequently, the choice of the KL estimation method is critical, especially in GTR-Turbo. This is because directly calculating the log probability difference can produce negative estimates, which is problematic as we use the negative value of the sentence-level KL as auxiliary rewards. As demonstrated by the blue curve in Figure 8, such KL estimation can grow increasingly negative, pushing the agent further away from the teacher rather than encouraging alignment. Several approaches can resolve this issue. The simplest and intuitive method is to clip the negative part or take its absolute value. Additionally, Schulman (35) proposed the K3 estimator, which guarantees non-negativity, is unbiased, and exhibits lower variance. We also try the forward KL calculation. All estimators are presented in Equation 7: K1 = log πθ log πmerged; KLclip = clip(K1, 0, +); KLabs = K1; K3 = K1 + eK1 1; KLforward = clip(K1, 0, +). (7) Results in Figure 8 demonstrate that all approaches with non-negative output lead to model improvements, the clipping method achieved the best results, and the differences in mean and peak values among other estimates are minor. As observed from the KL estimation curves, this is likely because clipping controls the scale of KL values, providing finer-grained updates and better stability when both the teacher and student are dynamically changing. The forward KL can also achieve high peak performance, but it still underperforms reverse KL, consistent with findings from prior studies (19; 12). This is because reverse KL exhibits mode-seeking characteristic, allowing the agent to capture specific peak in the teachers behavior rather than span over the broader distribution, making it more targeted and effective for guidance. Different Weights Assignment Methods As described in Section 3.2, there are different strategies for assigning weights during merging. In Figure 9, we explore various weight assignment methods and parameter choices. Experiments mentioned before use the simplest arithmetic mean (SMA), which already produces satisfactory performance. The exponential moving average (EMA) strategy, controlled by parameter α, assigns decaying weights exponentially, giving greater influence to more recent models (see Equation 4). Results show that balanced α = 0.5 performs the best among all candidates, with peak performance comparable to SMA. Values of α that are too high or too low both degrade the models capability. An excessively high α causes the influence of historical checkpoints to diminish rapidly, weakening the benefits of model merging in terms of optimization smoothing and past knowledge integration. very low α, while theoretically closer to SMA and shown to perform similarly in related research on pre-training (22), quickly fails in our experiments. Although lower α results in set of weights closer to average after convergence, the online recursive computation (Eqn. 4) introduces substantial bias when is small, leading to unbalanced merging where the latest models have 8 Figure 8: Comparison among different KL estimation methods. All methods with non-negative output can achieve increased performance. The clipping method presents the best result, since it controls the magnitude of the KL value, leading to finer-grained updates and improved stability. The slightly lower result of forward KL proves the mode-seeking advantage of reverse KL. Figure 9: Comparing different weights assignment methods. Simple SMA already yields strong performance. balanced choice of α is critical for realizing the benefit of EMA. minimal impact, thus degrading in the early stage. This issue is less pronounced in pretraining, where the number of checkpoints is much larger. Overall, choosing balanced α is critical to the effectiveness of the EMA strategy."
        },
        {
            "title": "5 Conclusions and Limitations",
            "content": "Sparse rewards remain core challenge in multi-turn agent reinforcement learning. Previous process-guided approaches, such as GTR, depend on costly and possibly inaccessible external API models, which greatly limit their scalability and applicability. In this work, we introduce the GTR-Turbo framework, which leverages the merged checkpoint as free teacher and provides thought guidance through either supervised fine-tuning or KL-regularized methods. This simple yet powerful enhancement enables true self-evolution, substantially reducing training time and cost while achieving comparable or even superior performance to GTR. By better unleashing the models decision-making and reflective capabilities, GTR-Turbo offers more practical and efficient paradigm for complex multi-turn visual agentic tasks. It is worth noting that GTR-Turbo is self-contained training framework relying heavily on exploration and environmental feedback. Therefore, the base model needs certain level of capability; otherwise, the lack of positive rewards may result in the passive exploration issue observed in GTR. For models with limited initial success rate, traditional approaches that inject external knowledge remain necessary to achieve rapid performance improvement. Moreover, due to resource constraints, our experiments are primarily conducted on 7B models. Further research can investigate the performance of GTR-Turbo across different scales."
        },
        {
            "title": "References",
            "content": "[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 9 [4] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [5] Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. [6] Google Deepmind. Gemini 2.5 pro, 2025. URL https://deepmind.google/models/gemini/ pro/. [7] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. International conference on machine learning, 2023. [8] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. [9] Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025. [10] Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, et al. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. arXiv preprint arXiv:2406.14024, 2024. [11] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. In 2024 IEEE international conference on robotics and automation, pp. 1246212469. IEEE, 2024. [12] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [15] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John Hopcroft, and Kilian Weinberger. Snapshot ensembles: Train 1, get for free. In International Conference on Learning Representations, 2017. [16] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pp. 91189147. PMLR, 2022. [17] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. [18] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations, 2023. [19] Eric Jang. beginners guide to variational methods: Mean-field approximation, 2016. URL https: //blog.evjang.com/2016/08/variational-bayes.html. [20] Pengxiang Li, Zechen Hu, Zirui Shang, Jingrong Wu, Yang Liu, Hui Liu, Zhi Gao, Chenrui Shi, Bofei Zhang, Zihao Zhang, et al. Efficient multi-turn rl for gui agents via decoupled training and adaptive data curation. arXiv preprint arXiv:2509.23866, 2025. [21] Yuetai Li, Zhangchen Xu, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Xiang Yue, and Radha Poovendran. Temporal sampling for forgotten reasoning in llms. arXiv preprint arXiv:2505.20196, 2025. [22] Yunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen, Minrui Wang, Shiyi Zhan, et al. Model merging in pre-training of large language models. arXiv preprint arXiv:2505.12082, 2025. 10 [23] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The twelfth international conference on learning representations, 2023. [24] Kevin Lu and Thinking Machines Lab. On-policy distillation. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.20251026. https://thinkingmachines.ai/blog/on-policy-distillation. [25] Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems, 35:1770317716, 2022. [26] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in neural information processing systems, 36:2508125094, 2023. [27] OpenAI. Introducing gpt-5, 2025. URL https://openai.com/index/introducing-gpt-5/. [28] OpenAI. Introducing openai o3 and o4-mini, 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/. [29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [30] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pp. 122, 2023. [31] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36: 5970859728, 2023. [32] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. [33] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. [34] Sunny Sanyal, Atula Tejaswi Neerkaje, Jean Kaddour, Abhishek Kumar, et al. Early weight averaging meets high learning rates for llm pre-training. In First Conference on Language Modeling, 2024. [35] John Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-approx. html. [36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [38] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in neural information processing systems, 36:86348652, 2023. [39] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cˆote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. [40] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759, 2024. [41] Theodore Sumers, Kenneth Marino, Arun Ahuja, Rob Fergus, and Ishita Dasgupta. Distilling internet-scale vision-language models into embodied agents. arXiv preprint arXiv:2301.12507, 2023. [42] Qwen Team. Qwen3: Think deeper, act faster, 2025. URL https://qwen.ai/blog?id= 1e3fa5c2d4662af2855586055ad037ed9e555125. 11 [43] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [44] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*: Improving multi-step reasoning for llms with deliberative planning. arXiv preprint arXiv:2406.14283, 2024. [45] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. [46] Kangrui Wang, Pingyue Zhang, Zihan Wang, Yaning Gao, Linjie Li, Qineng Wang, Hanyang Chen, Chi Wan, Yiping Lu, Zhengyuan Yang, et al. Vagen: Reinforcing world model reasoning for multi-turn vlm agents. arXiv preprint arXiv:2510.16907, 2025. [47] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. [48] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023. [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [50] Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu, and Deheng Ye. Gtr: Guided thought reinforcement prevents thought collapse in rl-based vlm agent training. arXiv preprint arXiv:2503.08525, 2025. [51] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pp. 2396523998. PMLR, 2022. [52] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. [53] Taiqiang Wu, Chaofan Tao, Jiahao Wang, Runming Yang, Zhe Zhao, and Ngai Wong. Rethinking kullbackleibler divergence in knowledge distillation for large language models. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 57375755, 2025. [54] Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy. arXiv preprint arXiv:2404.05692, 2024. [55] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. [56] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36:70937115, 2023. [57] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. CoRR, 2024. [58] Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. Adamerging: Adaptive model merging for multi-task learning. In The Twelfth International Conference on Learning Representations, 2024. [59] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. In European conference on computer vision, pp. 2038. Springer, 2024. [60] Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, and Yuhui Shi. Embodied multi-modal agent trained by an llm from parallel textworld. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2627526285, 2024. [61] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [62] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International conference on learning representations, 2023. [63] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024. [64] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. CoRR, 2025. [65] Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. [66] Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems, 37:110935110971, 2025. [67] Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, et al. Agentrl: Scaling agentic reinforcement learning with multi-turn, multi-task framework. arXiv preprint arXiv:2510.04206, 2025. [68] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [69] Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, and Chengqi Zhang. Wall-e: World alignment by rule learning improves world model-based llm agents. arXiv preprint arXiv:2410.07484, 2024. 6: 7: 8: 9: 10: 11: 12: 13: 5: 6: 7: 8: 9: 10: 11: Checkpoint buffer Thought dataset On-policy RL data buffer Eqn. 3 Reference thought Eqn. 2 Eqn. 5 Checkpoint buffer On-policy RL data buffer Eqn. Eqn."
        },
        {
            "title": "A Pseudocodes",
            "content": "We present the GTR-Turbo pseudocodes, both for the SFT and KL thought guidance variants. Algorithm 1 Training Procedure of GTR-Turbo (SFT) 1: Input: Environment env, agent model πθ0, Replay buffer size B, update epoch 2: [πθ0 3: 4: for = 0 to 1 do 5: ] Obtain π ot = env.reset() while < do (k) merged by merging all checkpoints in Generate (tht, at) using πθk given ot (k) Generate ( ˆtht, ˆat) using π merged given ot rt, ot+1 = env.step(at) (ot, at, rt, ot+1) (ot, ˆtht) Sample mini-batch from B, from Compute LPPO with Compute LSFT with θk+1 = arg minθ(LPPO + LSFT) πθk+1 14: 15: 16: 17: 18: 19: Output: πθK Algorithm 2 Training Procedure of GTR-Turbo (KL) 1: Input: Environment env, agent model πθ0, Replay buffer size B, update epoch 2: [πθ0 ] 3: for = 0 to 1 do 4: Obtain π ot = env.reset() while < do (k) merged by merging all checkpoints in (cid:16) Generate (tht, at) using πθk given ot (cid:17) (k) Calculate RevKL merged; tht rt, ot+1 = env.step(at) (cid:16) ot, at, rt β RevKL πθk , π (cid:16) πθk , π (cid:17) (k) merged; tht , ot+1 (cid:17) Sample mini-batch from Compute LPPO with θk+1 = arg minθ LPPO πθk+1 12: 13: 14: 15: 16: Output: πθK In GTR-Turbo (KL), β controls the contribution of the reserve KL term within the reward. Throughout this paper, we use the default setting of β = 1."
        },
        {
            "title": "B Additional Details on Training",
            "content": "B.1 Training Setting Drawing inspiration from the common practice in RL post-training frameworks, (29; 66; 50), we perform one epoch of supervised fine-tuning on the base Qwen2.5-VL model (2) before RL training, so that the agent possesses basic instruction-following capability. The datasets are sourced from the RL4VLM paper (66), with labels for the Points24 provided by task solver and labels for the ALFWorld environment generated by GPT-4V. 1 B.2 Hyperparameters We provide the hyperparameters used for GTR-Turbo training in Table 5, which are primarily derived from previous work (66; 50). We employ LoRA (14) to fine-tune the entire VLM model. Hyperparameter Learning rate Initial learning rate Final learning rate Maximum learning rate step Discount factor γ GAE λ PPO entropy coefficient PPO value loss coefficient PPO clip parameter PPO epoch Gradient accumulation steps LoRA LoRA α LoRA dropout KL loss coefficient β General Setup - Training (for KL guidance) General Setup - Models Value CosineAnnealingLR 1e 5 1e 9 25 0.9 0.95 0.01 0.5 0.1 4 128 128 256 0.05 Generation max text length Generation temperature Generation repetition penalty Model Merging Method TIES Density Teacher Generation base temperature Teacher Generation max temperature Teacher Generation temperature retry coefficient (for SFT guidance) (for SFT guidance) (for SFT guidance) Environmental steps Thought probability coefficient Environmental steps Thought probability coefficient For Points24 task For ALFWorld task 256 0.2 1.2 TIES 0.8 0.2 0.9 1.1 30000 0.5 20000 0.2 Table 5: Hyperparameters of GTR-Turbo"
        },
        {
            "title": "C Additional Details on Environments",
            "content": "We provide detailed introduction to the experimental environments used in this study. C.1 Points24 State and action space. At each observation ot in the Points24 task, the agent observes an image showing four poker cards and text-based representation of the current formula. The goal is to form formula equal to 24 using the numbers represented by the four cards and basic operators. Cards J, Q, are all treated as number 10. The action space includes {1, 2, . . ., 10, +, -, *, /, (, ), =}, and each card can only be used once. Selecting number not present in the image or one that has already been used is considered an illegal action. If the action is legal, the corresponding number or operator is appended to the current formula, forming the next observation ot+1; if the action is illegal, the state remains unchanged ot+1 = ot. The environment does not guarantee that the four cards in the image have feasible solution equal to 24. Reward function. At each step, the agent receives reward = 1 for outputting an illegal action and reward = 0 for legal action. The episode terminates when the agent outputs = as an action or the step count exceeds = 20. At termination, if the formula evaluates to 24, the agent receives an outcome reward = 10; otherwise, it receives = 1. 2 Figure 10: The Points24 task. C.2 ALFWorld State and action space. In the ALFWorld environment in our experiments, the agent receives an RGB observation image and history of past actions at each observation ot. The action space includes all possible interactions in the current scenario, typically categorized as: (1) go to {recep}, (2) take {obj} from {recep}, (3) put {obj} in/on {recep}, (4) open {recep}, (5) close {recep}, (6) toggle {obj} {recep}, (7) clean {obj} with {recep}, (8) heat {obj} with {recep}, (9) cool {obj} with {recep}, where {obj} and {recep} denote objects and receptacles. After an admissible action is taken, ALFWorld renders the updated scene from the agents view as the next observation ot+1. ot+1 = ot if the action is illegal. Notably, the ALFWorld environment provides both an image and text description of the observation scene at each step. As noticed in GTR, the VLM agent may heavily rely on the textual description rather than visual observation, contradicting the purpose of visual agentic tasks. GTR therefore modified the state by removing the text description, which we adopt in GTR-Turbo. We also align with GTR by including the action history in the input prompt to better simulate the real-world scenario. These adjustments increase the difficulty of the task, emphasizing the agents comprehensive visual recognition and long-horizon decision-making capabilities. Reward function. The reward of ALFWorld consists of two components. Each observation has set of admissible actions Aadm(s), and illegal actions are penalized. Additionally, each task in ALFWorld has both the final goal gtask and sub-goals gsub, and achieving these goals also provides rewards. Formally, the reward function can be written as: r(st, at, st+1gtask) = 50 1(st+1 = gtask) + 1(st+1 = gsub) 1(at / Aadm(s)). (8) Figure 11: The ALFWorld task."
        },
        {
            "title": "D Additional Experiment Results",
            "content": "We also evaluate the efficacy of GTR-Turbo using the newly released Qwen3-VL-8B-Instruct model on the ALFWorld environment. The experiment shows that GTR-Turbo remains compatible with the latest model family, and the stronger base capability of Qwen3-VL leads to improved performance, even surpassing the success rate of Qwen2.5-VL-32B, model that is four times larger in scale. Moreover, we observe that in general knowledge reasoning tasks like ALFWorld, Qwen3-VL is capable of performing RL directly without any SFT initialization. This suggests that as foundation models continue to evolve, GTR-Turbo may become even simpler to use and more broadly applicable in the future. Figure 12: Result of Qwen3-VL-8B on ALFWorld."
        }
    ],
    "affiliations": [
        "Peking University",
        "Tencent AI Lab",
        "Tsinghua University"
    ]
}