{
    "paper_title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
    "authors": [
        "Xin Huang",
        "Tengfei Wang",
        "Ziwei Liu",
        "Qing Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Material Anything, a fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as a dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing a progressive material generation strategy guided by these confidence masks, along with a UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across a wide range of object categories and lighting conditions."
        },
        {
            "title": "Start",
            "content": "Material Anything: Generating Materials for Any 3D Object via Diffusion Xin Huang1*, Tengfei Wang2, Ziwei Liu3, Qing Wang1 1Northwestern Polytechnical University, 2Shanghai AI Lab, 3S-Lab, Nanyang Technological University 4 2 0 2 2 2 ] . [ 1 8 3 1 5 1 . 1 1 4 2 : r Figure 1. Material Anything: feed-forward PBR material generation model applicable to diverse range of 3D meshes across varying texture and lighting conditions, including texture-less, albedo-only, generated, and scanned objects."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We present Material Anything, fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages pre-trained image diffusion model, enhanced with triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing progressive material generation strategy guided by these confidence masks, along with UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across wide range of object categories and lighting conditions. * Work was done during an internship at Shanghai AI Lab. Corresponding authors. 1 Physically Based Rendering (PBR) involves complex interactions between geometry, materials, and illumination. High-quality physical materials ensure that 3D objects appear consistent and realistic under various lighting conditions, crucial for applications such as video games, virtual reality, and film production. Given the meshes in Fig. 1, skilled artist can create realistic textures and materials using software like Blender [5] and Substance 3D Painter [2]. However, this creation process is tedious and time-consuming, requiring expertise in graphic design. Despite recent advances in 3D texture painting [6, 24, 29, 47], they often fail to accurately model materials that disentangle light and texture, resulting in baked-in shading effects like unwanted highlights and shadows. Several recent works have emerged to tackle the challenge of generating materials for 3D objects; however, they remain largely impractical due to their complexity and specificity. These approaches either require specific optimizations for each case [43, 52] or rely on multi-modal models like GPT4-V [1] to retrieve materials for different parts of an object [12, 51]. Consequently, such approaches face significant challenges: (1) Limited scalability. Each case requires specific parameter adjustments, hindering the end-to-end automation of the creation process. (2) Compromised robustness. Complex pipelines that involve multiple models (e.g., SAM [20] and GPT for segmentation and (3) Limited assignment) may lead to system instability. generalization. Existing methods are sensitive to lighting and struggle to handle broad spectrum of scenarios, including realistic lighting (e.g., real-world scans), unrealistic lighting (e.g., generated textures), and absence of lighting (e.g., albedo). To tackle these challenges, we propose Material Anything, fully automated, stable, and universal generative model for physical materials. Our method accepts any 3D mesh as input and generates high-quality material maps through two-stage pipeline: image-space material generation and UV-space material refinement. Given 3D object, the image-space material diffusion model aims to produce PBR materials for each view of it. Considering the limited availability of PBR data, we leverage pre-trained image diffusion model and adapt it to material estimation using novel triple-head architecture and rendering loss, which together help stabilize training and bridge the gap between natural images and material maps. Once trained, this model can automatically generate materials for the views rendered from general 3D objects without predefined categories or part-level masks. To enable the image-space material diffusion model to support 3D objects across various lighting scenarios, we introduce confidence mask to indicate illumination certainty and propose data augmentation strategy to simulate various lighting conditions during training. (1) For meshes with realistic lighting effects, the confidence mask is set to higher value, enabling the model to utilize illumination cues to predict the materials accurately. (2) For meshes with lighting-free textures, the confidence is set to low, allowing the model to generate materials based on prompts and global semantic cues. (3) For generated objects and textureless objects (we initially use texture generation method to create coarse textures), their textures may exhibit unrealistic lighting effects that deviate from physical laws, often resulting in exaggerated highlights and shadows. In such cases, the confidence mask is adaptively set to varying values, ensuring the model relies on local semantic to generate plausible materials, as the lighting cues are unreliable. While the image-space model can effectively generate materials for each single view, directly applying it to 3D object can lead to appearance inconsistency across views. To boost multi-view consistency, we introduce confidence-aware progressive material generation scheme that uses the confidence mask to prompt our diffusion model to produce materials consistent with previous views. After progressively generating the materials for all views, we project them into UV space for further refinement, achieving 3D-consistent and high-quality UV maps that are userfriendly and easy to edit. Together, these components enable Material Anything to achieve remarkable performance in material generation. To train the model, we build Material3D dataset, comprising over 80K objects with high-quality PBR materials and UV unwrapping. Extensive experiments demonstrate significant improvement of our method over current approaches. We summarize our contributions as follows: fully automated, stable, and universal model to generate physical materials for diverse 3D objects, achieving state-of-the-art performance. material diffusion model with illumination confidence to handle various lighting conditions with one model. progressive material generation scheme guided by confidence masks, along with UV-space material diffusion model, to generate consistent and UV-ready materials. 2. Related Work 3D Object Generation. Previous works [8, 16, 23, 28, 34, 35, 40, 41] rely on image diffusion models for 3D generation with score distillation sampling but suffer from long optimization times for each object. To mitigate this, recent works have explored feed-forward models. These approaches either apply 3D diffusion models [14, 19, 27, 39] or employ U-Net or transformer-based models [13, 15, 22, 36, 42, 53] to directly generate 3D representations. Despite achieving impressive results, these models are limited in their ability to generate realistic materials, often producing textures entangled with complex lighting information. This limitation hinders the adaptability of generated objects for downstream applications, where material properties are essential. Recent works, such as Clay [50], Meta 3DGen [4], and 3DTopia-XL [9], have proposed frameworks for generating 3D objects with PBR materials from prompts or images. However, these methods struggle to handle diverse 3D object inputs under different texture and illumination conditions for different applications. Texture Generation for 3D Object. Given textureless 3D model, TEXTure [29] introduces progressive approach that generates textures view-by-view using diffusion. Latent-NeRF [25] further improves efficiency in lower-dimensional latent space, enabling high-quality texture synthesis. Text2Tex [6] introduces an automatic view sequence generation scheme, optimizing the generation sequence to achieve more consistent textures. SyncMVD [24] generates textures from multiple views simultaneously, ensuring coherent alignment of textures. Paint3D [47] first creates an initial texture map, then refines it in the UV space, achieving highly detailed and spatially coherent textures. Despite these advancements, the textures generated by these methods are typically entangled with complex lighting and shadows, lacking realistic material modeling. Material Generation for 3D Object. Early works employ optimization-based methods for material generation. Fantasia3D [7] involves learnable materials when gener2 Figure 2. Overview of Material Anything. For texture-less objects, we first generate coarse textures using image diffusion models, similar to the texture generation method [6]. For objects with pre-existing textures, we directly process them. Next, material estimator progressively estimates materials for each view from rendered image, normal, and confidence mask. The confidence mask serves as additional guidance for illuminance uncertainty, addressing lighting variations in the input image and enhancing consistency across generated multi-view materials. These materials are then unwrapped into UV space and refined by material refiner. The final material maps are integrated with the mesh, enabling the object for downstream applications. ating 3D objects. NvDiffRec [26] jointly optimizes the topology, materials, and lighting conditions from multiview image observations. Matlaber [43] proposes latent BRDF auto-encoder to enable material-aware 3D generation. Paint-it [45] introduces re-parameterization of PBR texture maps, facilitating robust and efficient optimization. DreamMat [52] finetunes lighting-aware diffusion model for distilling PBR materials. These methods are time-intensive and often suffer unnatural color. Retrievalbased methods [12, 51] rely on large multi-modal models such as SAM [20] and GPT [1] for segmentation and material assignment, limiting their scalability. Recently, several works [37, 38, 48] attempt to generate materials for images with diffusion models, however, they are not applicable to 3D objects due to consistency issues. 3. Approach Material Anything, illustrated in Fig. 2, is unified framework for generating high-quality physical materials for 3D objects, accommodating various lighting and texture sceIt effectively handles (1) texture-less objects, (2) narios. albedo-only objects (without lighting effects), (3) scanned objects (realistic lighting), and (4) generated objects (unrealistic lighting). Unlike existing methods that treat these scenarios as separate tasks, our method unifies them under single framework. To this end, we reformulate 3D material generation as an image-based material estimation task, enabling the use of pre-trained image diffusion models and simplifying the overall process. Our framework centers on two core components. First, we employ diffusionbased material estimator equipped with confidence masks, which generates materials for each view of the input object (Sec. 3.1). Next, we introduce progressive material generation strategy that utilizes confidence masks to ensure consistency of generated materials across views, and further integrate UV-space diffusion model for material refinement. (Sec. 3.2). Finally, we provide the construction details of our Material3D dataset in Sec. 3.3. 3.1. Image-based Material Diffusion The material estimator aims to produce albedo (R3), roughness (R), metallic (R), and bump (R3) maps from an input image. Given the limited availability of PBR data, we opt to leverage the power of pre-trained image diffusion models [30]. However, these models are primarily designed for natural image generation, posing three challenges for PBR material generation: Channel Gap. Image diffusion models typically operate on three channels (RGB), while PBR materials require more than three channels (eight channels in our method). This discrepancy can lead to inaccurate material representations, as the model must adapt to producing more complex set of outputs. Domain Gap. PBR material maps are different from natural images. This significant difference leads to unstable training and suboptimal performance. Various Lighting. Finally, our material estimator must be robust across images with diverse lighting conditions, ensuring consistent performance. To address these challenges, we introduce several key components. Triple-Head Diffusion. To adapt the three-channel diffusion model to handle multiple material-specific channels, one solution is to train material VAE [37]. However, this approach may discard the pre-trained priors of diffusion models, and training customized material VAE on our limited PBR data is challenging. Inspired by previous work [50], we design triple-head U-Net architecture, illustrated in Fig. 3 (a). The U-Net architecture comprises three distinct branches for the initial convolutional layer 3 Figure 3. Architectural design of material estimator and refiner. Both employ triple-head U-Net, generating albedo, roughness-metallic, and bump maps via separate branches. and first DownBlock, followed by shared middle layers enabling concurrent denoising across material modalities. The final UpBlock and output convolutional layer are also separated into three branches. Each output head produces specific material map: an albedo map, combined roughnessmetallic map (R channel set to 1, for roughness, for metallic), and bump map. This triple-head structure, instead of combining all materials into one output, ensures that each material map is generated without mutual interference while maintaining consistency among them. Confidence-Adaptive Generation. To manage inputs with various lighting conditions, we categorize these conditions into two main groups: high confidence (e.g., scanned objects) and low confidence (e.g., no lighting and generated lighting). To guide the model, we introduce certainty mask that indicates illumination confidence. For inputs with realistic lighting, the confidence value is set to 1, encouraging the diffusion model to leverage lighting cues for material estimation. In contrast, for inputs lacking lighting or with generated lighting, the confidence is set to 0, directing the model to focus on material generation instead of estimation. Note that, for images with generated lighting, the mask can selectively assign values of 1 in known material regions and 0 elsewhere to enhance multi-view material consistency, as detailed in the progressive material generation (Sec. 3.2). The confidence mask enables the diffusion model to seamlessly transition between material estimation and generation, effectively managing both realistic and synthetic lighting scenarios. The learning objective is v-prediction [31]: Lv = Ez,c,y,v,t (cid:13) ˆVθ(zt; c, y) vt (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 , (1) where vt represents the v-prediction targets at time-step for the albedo, roughness-metallic, and bump maps, respec4 Figure 4. Progressive material generation process for textureless object. Project denotes projecting known regions for the latent initialization of the next view. SD denotes the pre-trained stable diffusion model [30] with depth ControlNet [49] tively. zt denotes the noise latent. denotes the conditioning inputs (input image, confidence mask, and normal map), while represents the text prompt. ˆVθ refers to our triple-head diffusion network with learnable parameters θ. Rendering Loss. Due to the significant domain gap, training the material diffusion model with only the v-prediction objective is unstable. To address this, we introduce rendering loss by decoding albedo, roughness-metallic, and bump map from latent space into the image for reconstruction. These components are rendered into an image ˆr under random lighting conditions using differentiable rendering [21]. We then compute the perceptual loss [18] against the ground truth rendering as follows: Lp = (cid:88) ϕl(ˆr) ϕl(r)2 2 , (2) where ϕl represents VGG network [33]. The rendering loss ensures that the estimated materials exhibit realistic behavior under diverse lighting conditions, improving the quality of generated materials. Additionally, an L2 loss is applied to each material component to further improve performance. 3.2. Materials Generation for 3D Object While we have successfully estimated materials for images, applying the material estimator to multiple views of 3D object would lead to noticeable inconsistencies. One solution involves training material estimator to simultaneously predict materials across multiple views, similar to multiview diffusion [32]. However, the increased number of views and channels poses challenge for generating highresolution materials. To adapt our 2D materials estimator for 3D objects, we propose progressive generation strategy that dynamically estimates materials across different viewpoints based on the aforementioned confidence mask. We further project the multi-view materials into UV space and apply refinement diffusion model, which completes occluded regions and refines materials, ensuring seamless and consistent materials maps. Progressive Material Generation with Confidence Guidance. Given 3D object O, we define set of camera views and progressively generate materials for each view. For meshes lacking textures, we first generate textured image vi using an image diffusion model conditioned on depth map, similar to Text2Tex [6]. For meshes with existing textures, we directly render the textured image vi from the input object. Figure 4 illustrates the progressive material generation for texture-less object. The next step is to estimate the material from the image vi. As we generate materials for each view independently, our goal is to maintain consistency across views. When generating new view, we aim for the materials to remain consistent with existing regions in previous views, rather than relying solely on the current view. To achieve this, First, we initialize the noise latent using materials from previously processed views {vjj < i}, with mask ˆm indicating known regions, ensuring these regions are preserved and consistent. The latent initialization is formulated as: ˆzt = ˆzt (1 ˆm) + zt ˆm, (3) where ˆzt represents the noise latent at time step for material maps, zt denotes the latent of known regions. Consistency is especially challenging in views with generated lighting due to exaggerated highlights and shadows. Therefore, for these views with generated lighting, we additionally utilize the confidence mask introduced in Sec. 3.1 to further enhance consistency between newly generated and known regions. Specifically, we dynamically adjust the mask m, setting it to 1 for known regions with higher confidence and to 0 for regions requiring new generation. This approach guides the estimator to produce materials that align seamlessly with known regions, as our training data is designed to simulate these unrealistic lighting situations. Next, we bake these material maps into the UV space according to the UV unwrapping of object O. After processing all generated views and materials, we obtain the coarse UV material maps uv for the object. UV Refinement Diffusion. Although coarse UV material maps are generated, issues such as seams (resulting from baking across different views) and texture holes (due to selfocclusion during rendering) remain. We thus refine material maps directly in UV space using diffusion model. Unlike Paint3D [47], which fine-tunes diffusion model solely on albedo maps, our task is more complex, as it involves refining albedo, roughness-metallic, and bump maps. We trained material refiner that takes the coarse material maps uv as input, completing holes and smoothing seams. Additionally, canonical coordinate map (CCM) is introduced to incorporate 3D adjacency information during the diffusion process, guiding the regions that require inpainting, as shown in Fig. 3 (b). By integrating these components, the refiner produces high-quality, consistent UV material maps. 3.3. Material3D Dataset To train Material Anything, we build dataset Material3D that consists of 80K high-quality 3D objects curated from Objaverse [11]. Details of dataset construction are provided in the supplementary. For each model, we rendered multiview material images (albedo, roughness, metallic, bump), and normal maps from 10 fixed camera positions. Additionally, UV material maps and the CCM were rendered to facilitate the training of the material refiner. To enable the model to handle diverse lighting scenarios, we incorporated various lighting conditions, including Point Lighting, Area Lighting, Environment Lighting, and Without Lighting, for rendering input images. Additionally, we designed strategy to simulate the imperfect and inconsistent lighting conditions common during inference. Simulating Inconsistent Lighting. We randomly select two images under different lighting conditions for camera view and stitch portions of each into composite during training. This enables single image to exhibit two distinct lighting types, simulating the inconsistency in multiview materials. Furthermore, we introduce degradations to one of the images, applying effects such as blurring and color shifts. confidence mask is used to delineate the regions that have undergone degradation. The final input to the material estimator comprises the stitched image, the confidence mask, and the normal map. To train the material refiner, we randomly mask regions of the UV material maps and use these masked material maps as input. The CCM, derived from the UV mapping of 3D point coordinates, is also included. These maps guide the areas requiring inpainting and facilitate the integration of 3D adjacency information during the diffusion process. Refer to supplementary material for more details on our dataset. 4. Experiments We compare our method with texture generation methods, Text2Tex [6], SyncMVD [24], and Paint3D [47]. Additionally, we assess our method alongside optimizationbased material generation approaches, NvDiffRec [26] and DreamMat [52], and retrieval-based method, Make-itReal [12]. Finally, we also include comparisons with the closed-source methods, Rodin Gen-1 [10] and Tripo3D [3]. 4.1. Qualitative Evaluation Comparisons with Texture Generation Methods. We compare Material Anything with various texture generation methods in Fig. 5. These methods employ similar strategy, painting the texture-less meshes with pre-trained image 5 Figure 5. Comparisons with texture generation methods. These methods directly paint texture-less objects using image diffusion models but fail to generate the corresponding material properties. Figure 6. Comparisons with optimization methods. NvDiffRec [26] estimates materials using the textured model by SyncMVD [24] as input. The materials include albedo (top left); roughness (top right); metallic (bottom left); bump (bottom right). diffusion model. However, the lighting information generated by the diffusion model often results in textures that exhibit significant artifacts, as they are influenced by complex lighting conditions from multiple generated images. In contrast, our method produces clearer textures that inherently incorporate material properties, thus enabling robust support for downstream applications. Comparisons with Optimization Methods. We compare our approach with optimization methods, as shown in Fig. 6. These methods require extensive optimization time for each object and have difficulty accurately identifying object materials. In contrast, our method effectively distinguishes materials, as demonstrated in the barrel example, where it accurately represents the metal bands and wooden planks. This capability underscores the superiority of our method in generating realistic and diverse material maps. Comparisons with Retrieval Methods. For input objects with existing textures, we compare ours with the retrieval method Make-it-Real, as shown in Fig. 7. Make-it-Real retrieves materials based on segmenting the initial texture, which presents several limitations. First, the segmentation process struggles with accurately capturing small regions, such as the peeling paint on the fire hydrant. Additionally, it encounters difficulty in removing shadows in the initial texture, as observed in the shadowed albedo of the sculpture example. In contrast, our method generates more accurate material, better preserving fine details and removing artifacts such as shadows. Comparisons with Tripo3D and Rodin Gen-1. We compare our method with two closed-source methods, Tripo3D 6 Table 1. Quantitative comparisons. FID and CLIP scores (similarity between rendered views and text prompts) are computed on 1,200 images from 20 textured objects. For comparison with Make-it-Real, the CLIP score is calculated between rendered images from generated textures and those in Objaverse. Method Type Input Mesh FID CLIP Score Text2Tex [6] SyncMVD [24] Paint3D [47] NvDiffRec [26] DreamMat [52] Ours Learning Learning Learning Optimization Optimization Learning Texture-less Texture-less Texture-less Texture-less Texture-less Texture-less Make-it-Real [12] Ours Retrieval Learning Textured Textured 116.41 118.46 153.20 103.81 113.34 100.63 104.38 101.19 30.33 30.66 28.40 30.14 30.64 31.06 88.62 89. Table 2. Ablation study for triple-head U-Net and rendering loss. RMSE is calculated for the materials across the views from 1,000 Objaverse objects. Materials W/O Triple-head W/O Rendering Loss Albedo Roughness Metallic Bump 0.0800 0.1196 0.1584 0. 0.1442 0.1943 0.2594 0.0716 Full 0.0604 0.0877 0.1193 0.0313 Table 3. Ablation study for confidence masks. Mean RMSE is calculated for materials from 1,000 Objaverse objects with different simulated lighting conditions, including light-less (albedoonly), realistic (scanned), and unrealistic light (generated). Light-less Realistic Unrealistic W/O Confidence Full 0.1521 0.1102 0.1074 0.0747 0.1111 0.0847 Mean 0.1235 0.0899 4.3. Ablation Study Effectiveness of Triple-Head U-Net. We evaluate the performance of our method using vanilla U-Net architecture that directly generates all materials as 12-channel latent, instead of triple-head U-Net. As shown in Tab.2, the performance degrades due to the coupling effect between materials when outputting single 12-channel latent. In Fig. 9, this coupling effect is evident, where the bumps are incorrectly colored due to interference from the albedo. In contrast, the triple-head U-Net effectively decouples the materials. Additionally, the shared backbone among the three heads ensures alignment across the different material maps. Effectiveness of Rendering Loss. In Tab. 2, we present the quantitative results of our method when trained without rendering loss. Notably, performance in this ablation is worse compared to the variant trained with rendering loss. As illustrated in Fig. 9, the version without rendering loss exhibits noticeable detail degradation, with visible artifacts across the material views. Rendering loss acts as an additional constraint in image space, ensuring consistency under varying lighting conditions, which enhances training stabilFigure 7. Comparisons with retrieval methods. The inputs are textured objects, including an albedo-only object and scanned object. The materials include albedo (top left); roughness (top right); metallic (bottom left); bump (bottom right). Figure 8. Comparisons with Rodin Gen-1 and Tripo3D. Rodin Gen-1 and Tripo3D are two closed-source methods. Our approach uses significantly less data, yet produces comparable results. and Rodin Gen-1, as shown in Fig. 8. We utilize textureless meshes generated by Tripo3D as input for our material generation. Additionally, we provide the front-view image by an image diffusion model to Rodin Gen-1, ensuring the generation of the same 3D objects. While both Tripo3D and Rodin Gen-1 utilize significantly larger-scale training datasets, our method produces comparable results. 4.2. Quantitative Evaluation The quantitative evaluation of our method is presented in Tab. 1. As shown, our method achieves lower FID score, indicating that our generated textures are closer in distribution to those in Objaverse. Furthermore, the higher CLIP score of our method demonstrates its capability to generate textures more accurately aligned with the prompts. 7 Figure 9. Effectiveness of triple-head U-Net and rendering loss. In both ablation experiments, the confidence mask is set to 1. Figure 11. Effectiveness of strategies for material consistency. Figure 10. Effectiveness of confidence mask for various lighting conditions. W/O confidence mask indicates results from the material estimator without the confidence mask as input. ity and aids in capturing finer material properties. The results highlight the critical role of rendering loss in enhancing the fidelity and stability of our material estimator. Effectiveness of Confidence Mask. As shown in Fig. 10, the material estimator without confidence masks struggles to generate high-quality materials under different lighting conditions. In contrast, when guided by the confidence the model adapts well to these input variations. mask, Table 3 presents quantitative results for the model without confidence masks on Objaverse objects across different lighting conditions, revealing significant drops in material accuracy. Furthermore, for objects with generated lighting, the progressive generation without the confidence mask also shows noticeable inconsistencies in the material maps, as shown in Fig. 11. Conversely, with the confidence mask employed, the model can distinguish between regions for estimation and those for generation. By guiding the training process to focus on relevant regions, our method achieves more consistent, artifact-free materials. These results demonstrate that the confidence mask improves material consistency and addresses variations in lighting. Effectiveness of Known Material Initialization. Figure 11 shows the results of our method without using known Figure 12. Effectiveness of the UV-space material refiner. The material refiner effectively fills in holes caused by occlusions. materials from other views for initialization. As shown, the predicted metallic properties display noticeable variations across different views. In contrast, by progressively generating materials based on known ones, our method produces more consistent materials across multiple views. Effectiveness of the UV-Space Material Refiner. In Fig. 12, we shown the results without UV refinement. As shown, several holes appear in the predicted materials due to self-occlusions, leading to incomplete material maps. After applying our material refiner, these holes are effectively filled, resulting in more seamless and complete material representation. Our material refiner can handle occlusions and enhance the overall material generation quality. 5. Conclusion We proposed Material Anything, unified framework to generate PBR materials for various 3D objects, including texture-less, albedo-only, generated, and scanned meshes. By leveraging well-designed material diffusion model, our approach can generate high-fidelity materials in feedforward manner. To unify various input objects under complex lighting conditions, we introduced mask to indicate confidence levels for different illuminations, which also enhances multi-view material consistency. Extensive experiments have demonstrated that our method can generate high-quality PBR materials for various objects, with clear improvement over existing methods."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 3 [2] Adobe. Substance 3d painter. https : / / www . adobe . com / my _ en / products / substance3d / apps/painter.html, 2024. 1 [3] Tripo AI. Tripo3d. https://www.tripo3d.ai/, 2024. [4] Raphael Bensadoun, Tom Monnier, Yanir Kleiman, Filippos Kokkinos, Yawar Siddiqui, Mahendra Kariya, Omri Harosh, Roman Shapovalov, Benjamin Graham, Emilien Garreau, et al. Meta 3d gen. arXiv preprint arXiv:2407.02599, 2024. 2 [5] Blender. Blender project. https://www.blender. org/features/, 2024. 1 [6] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner. Text2tex: Text-driven texture synthesis via diffusion models. In ICCV, pages 18558 18568, 2023. 1, 2, 3, 5, 7 [7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for arXiv preprint high-quality text-to-3d content creation. arXiv:2303.13873, 2023. 2 [8] Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu. Comboverse: Compositional 3d assets creation using spatially-aware diffusion guidance. arXiv preprint arXiv:2403.12409, 2024. [9] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling highquality 3d asset generation via primitive diffusion. arXiv preprint arXiv:2409.12957, 2024. 2 [10] Deemos. Rodin gen-1. https : / / hyperhuman . deemos.com/rodin, 2024. 5 [11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, pages 13142 13153, 2023. [12] Ye Fang, Zeyi Sun, Tong Wu, Jiaqi Wang, Ziwei Liu, Gordon Wetzstein, and Dahua Lin. Make-it-real: Unleashing large multimodal models ability for painting 3d objects with realistic materials. arXiv preprint arXiv:2404.16829, 2024. 1, 3, 5, 7 [13] Zexin He and Tengfei Wang. Openlrm: Open-source large reconstruction models, 2023. 2 [14] Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Shuai Yang, Tengfei Wang, Liang Pan, 3dtopia: Large text-to-3d generaDahua Lin, et al. arXiv preprint tion model with hybrid diffusion priors. arXiv:2403.02234, 2024. 2 [15] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to In The Twelfth International Conference on Learning 3d. Representations, 2024. 2 [16] Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying Feng, Yebin Liu, and Qing Wang. Humannorm: Learning normal diffusion model for high-quality and realistic 3d human generation. In CVPR, pages 45684577, 2024. [17] Huggingface. Diffusers. https : / / github . com / huggingface/diffusers, 2024. 2 [18] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694711. Springer, 2016. 4 [19] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. Shap-e: GeneratarXiv preprint [20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 2, 3 [21] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics, 39(6), 2020. 4 [22] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with In sparse-view generation and large reconstruction model. The Twelfth International Conference on Learning Representations, 2024. 2 [23] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, pages 300309, 2023. 2 [24] Yuxin Liu, Minshan Xie, Hanyuan Liu, and Tien-Tsin Wong. Text-guided texturing by synchronized multi-view diffusion. arXiv preprint arXiv:2311.12891, 2023. 1, 2, 5, 6, [25] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In CVPR, pages 1266312673, 2023. 2 [26] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Muller, and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In CVPR, pages 82808290, 2022. 3, 5, 6, 7 [27] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generat9 ing 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 2 aware 3d assets from few exemplars. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [28] Ben Poole, Ajay Jain, Jonathan Barron, and Ben MildenICLR, hall. Dreamfusion: Text-to-3d using 2d diffusion. 2023. 2 [29] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In ACM SIGGRAPH Conference Proceedings, 2023. 1, 2 [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 3, 4, 2 [31] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 4 [32] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 4 [33] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [34] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. ICLR, 2024. 2 [35] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from single image with diffusion prior. In ICCV, pages 2281922829, 2023. 2 [36] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. 2 [37] Shimon Vainer, Mark Boss, Mathias Parger, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Nicolas Perony, and Simon Donne. Collaborative control for geometryconditioned pbr image generation. In European Conference on Computer Vision, pages 127145. Springer, 2024. 3 [38] Giuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur. Controlmat: controlled generative approach to material capture. ACM TOG, 43(5):117, 2024. 3 [39] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: generative model for sculpting 3d digital avatars using diffusion. In CVPR, pages 45634573, 2023. 2 [40] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. NeurIPS, 2023. 2 [41] Zhenwei Wang, Tengfei Wang, Gerhard Hancke, Ziwei Liu, and Rynson WH Lau. Themestation: Generating theme10 [42] Zhenwei Wang, Tengfei Wang, Zexin He, Gerhard Hancke, Ziwei Liu, and Rynson WH Lau. Phidias: generative model for creating 3d content from text, image, and 3d conditions with reference-augmented diffusion. arXiv preprint arXiv:2409.11406, 2024. 2 [43] Xudong Xu, Zhaoyang Lyu, Xingang Pan, and Bo Dai. Matlaber: Material-aware text-to-3d via latent brdf auto-encoder. arXiv preprint arXiv:2308.09278, 2023. 1, 3 [44] Jonathan Young. Xatlas. https://github.com/ jpcy/xatlas, 2024. 2 [45] Kim Youwang, Tae-Hyun Oh, and Gerard Pons-Moll. Paintit: Text-to-texture synthesis via deep convolutional texture map optimization and physically-based rendering. In CVPR, pages 43474356, 2024. [46] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lightIn ACM ing control for diffusion-based image generation. SIGGRAPH 2024 Conference Papers, pages 112, 2024. 1 [47] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In CVPR, pages 42524262, 2024. 1, 2, 5, 7 [48] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. Rgb-x: Image decomposition and synthesis using material-and lighting-aware diffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3 [49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. 4 [50] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2, 3 [51] Shangzhan Zhang, Sida Peng, Tao Xu, Yuanbo Yang, Tianrun Chen, Nan Xue, Yujun Shen, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. Mapa: Text-driven photorealistic material painting for 3d shapes. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 1, [52] Yuqing Zhang, Yuan Liu, Zhiyu Xie, Lei Yang, Zhongyuan Liu, Mengzhou Yang, Runze Zhang, Qilong Kou, Cheng Lin, Wenping Wang, et al. Dreammat: High-quality pbr material generation with geometry-and light-aware diffusion models. ACM Transactions on Graphics (TOG), 43(4):118, 2024. 1, 3, 5, 7 [53] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane meets gaussian splatting: Fast and generalizable single-view In Proceedings of 3d reconstruction with transformers. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1032410335, 2024. 2 Material Anything: Generating Materials for Any 3D Object via Diffusion"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Material3D Dataset A.1. Data Construction To train Material Anything, we constructed dataset Material3D that consists of 80K high-quality 3D objects curated from the Objaverse dataset, focusing specifically on models with comprehensive material maps. We further filtered objects using Blender, ensuring the presence of essential material maps: base color, roughness, metallic, and bump. Only models containing all these material properties were retained. Each object was imported into Blender, where Blenders Smart UV Project tool was used to generate UV mappings. For objects with multiple parts, all components were merged into single mesh to ensure UV mapping could be projected onto unified 2D map. After this filtering and preparation process, we rendered multi-view material images (albedo, roughness, metallic, and bump) from 10 fixed camera positions that are consistent with the setup used in our material generation phase, which served as training data for the material estimator. In addition, we rendered images under varying lighting conditions and included normal maps as input for model training, providing diverse lighting and surface information. UV material maps and CCM were also rendered to facilitate the training of the material refiner. A.2. Lighting Conditions Inspired by the image relighting method [46], we incorporated multiple lighting categories for rendering input images, enabling the model to handle diverse lighting scenarios. 1. Point Lighting. Point light sources are uniformly sampled from hemisphere (with 0 θ 60) surrounding the object, with radius sampled in the range [4m, 5m]. The number of point lights is randomly sampled between [1, 3]. The sum power of all lights is uniformly chosen within [900W, 2400W]. To ensure the visibility of highlighted regions, the hemisphere is rotated according to the camera position, while the camera itself remained fixed at the top of the hemisphere. 2. Area Lighting. Similar to point lighting, area light sources are sampled from hemisphere (with 0 θ 60) with radius from 4m to 5m. The size of the area light ranges from 3m to 10m, and its power is uniformly selected within [1000W, 2000W]. Only one area light is utilized during rendering. 3. Environment Lighting. Environmental lighting broadly influences scene illumination beyond isolated light sources. To counter the white balance bias common Figure 13. The virtualization of our training data. We apply various degradations and simulate inconsistent lighting effects in the inputs to enhance the robustness of our method. in diffusion-generated images, we employ white environment lighting with strengths ranging from [0.5, 3], avoiding colored HDR environment maps. 4. Without lighting. To ensure the material estimator can accurately predict materials independent of lighting conFigure 14. The camera poses for progressive material generation and building training data. Figure 15. Material editing with prompts. Material Anything enables flexible editing and customization of materials for textureless 3D objects by simply adjusting the input prompt. ditions, we render views of objects using only albedo textures, which is the same as rendering multi-view albedo maps. For each camera position, we render 13 images (including albedo, roughness, metallic, bump, normal maps, and 8 RGB images under point, area, and environment lighting). For UV material map rendering, we utilize Blenders smart UV project to unwarp the mesh, producing five UV space maps (albedo, roughness, metallic, bump, and canonical coordinate maps). A.3. Simulating Inconsistent Lighting Effects To improve the robustness of the material estimator, we randomly select two images under different lighting conditions for camera view and stitch portions of each into composite during training. This enables single image to exhibit two distinct lighting types, simulating the inconsistency in multi-view materials. Furthermore, we introduce degradations to one of the images, applying effects such as blurring and color shifts. confidence mask is used to delineate the regions that have undergone degradation. The final input to the material estimator comprises the stitched image, the confidence mask, and the normal map, as shown in Fig. 13 (a). To train the material refiner, we randomly mask regions of the UV material maps and apply degradations such as blurring and color shifts. These masked material maps are taken as input, as shown in Fig. 13 (b). The CCM, derived from the UV mapping of 3D point coordinates, is also included. These maps guide the areas requiring inpainting and facilitate the integration of 3D adjacency information during the diffusion process. B. Implementation Details B.1. Training Details We implemented Material Anything using the Diffusers [17], with Stable Diffusion v2.1 [30] serving as the backbone diffusion model. The training process leverages the AdamW optimizer with learning rate of 5 105. Our material estimator was trained over 300K iterations on 8 NVIDIA A100 GPUs with batch size of 32, requiring approximately 5 days to complete. In parallel, the material refiner was trained for 150K iterations under the same GPU configuration and batch size, with training duration of about 2 days. Training data was rendered at resolution of 512 512 using Blenders Cycles path tracer, ensuring high-quality reference materials for robust learning. B.2. Material Generation Details During material generation, each input object is centered within normalized bounding box. To capture comprehensive material properties, 6 or 10 views are rendered, as illustrated in Fig. 14. The input image resolution for our material refiner is set to 768 768, while the resolution for UV material maps is 1024 1024. This setup ensures highfidelity material maps that are detailed and adaptable across different viewing angles. For the input objects without UV mappings, xatlas [44] is used to unwrap them. All results, including those from our method and the baselines, are generated on single NVIDIA A100 GPU. C. Applications Material Anything offers robust capabilities to edit and customize materials of texture-less 3D objects by simply adjusting the input prompt, enabling flexible and intuitive material manipulation. As illustrated in Fig. 15, we demonstrate that barrels material can be transformed into realistic textures like wood, gold, and stone, showcasing the versatility of our approach across various material types. This application allows users to dynamically adapt 3D models to specific aesthetic or functional requirements, enhancing asset adaptability for virtual environments, simulations, and design visualization. Furthermore, our method supports relighting, enabling objects to be viewed under different lighting conditions, 2 Figure 16. Relighting results by Material Anything under various HDR environment maps. The left column displays the input textureless meshes, while the top row presents the HDR environment maps used. Figure 17. Failure Cases by Material Anything. D. Limitations and Failure Cases Material Anything is designed to address the complex task of generating materials for diverse range of 3D objects. However, our approach has certain limitations. First, owing to the characteristics of the Objaverse, where many objects exhibit uniform roughness and metallic attributes with minimal surface details in bump maps, our method may produce materials with constrained surface details. This limitation is illustrated in the elephant example in Fig. 17, where the resulting bump maps lack details. Additionally, for objects with existing textures, our method struggles to remove prominent artifacts. For example, in the apple instance in Fig. 17, large white artifacts are misinterpreted as part of the texture, resulting in an inaccurate albedo. as shown in Fig. 16. Material Anything generates material properties for each object, ensuring physically consistent relighting and enhanced realism. This functionality allows for more accurate simulations in AR, VR, and digital content creation, where realistic lighting is essential for immersion. Collectively, these capabilities make the proposed method versatile and efficient solution for content creators and researchers aiming to produce high-quality, relightable 3D objects with customized materials. E. Additional Results We present additional qualitative results to illustrate the effectiveness of Material Anything. Video results are present in our supplementary video. In Fig. 18, we display results generated by our material estimator on the Objaverse dataset, compared with their GT materials. As shown, our method effectively generates materials closely aligned with the ground truth, capturing essential details and textures to enhance realism. In Fig. 19, we show additional results on texture-less inputs, demonstrating our methods capability to handle complex UV mappings. Despite the complexity of certain UV layouts, our method consistently generates high-quality material maps in UV space, preserving material fidelity across the entire surface. Finally, we present additional results on various input types, including generated models, albedo-only inputs, and scanned 3D objects. These examples, shown in Fig. 20, highlight our methods robustness across varied lighting conditions and input characteristics, demonstrating its versatility in producing realistic materials adaptable to diverse lighting environments. 4 Figure 18. Results by our material estimator on 2D renderings from Objaverse. 5 Figure 19. Additional results by Material Anything on texture-less 3D objects. The generated UV material maps are provided. Figure 20. Additional results by Material Anything on albedo-only, generated, scanned 3D objects."
        }
    ],
    "affiliations": [
        "Northwestern Polytechnical University",
        "S-Lab, Nanyang Technological University",
        "Shanghai AI Lab"
    ]
}