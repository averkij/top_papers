{
    "paper_title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
    "authors": [
        "Yuetong Liu",
        "Yunqiu Xu",
        "Yang Wei",
        "Xiuli Bi",
        "Bin Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Restoring nighttime images affected by multiple adverse weather conditions is a practical yet under-explored research problem, as multiple weather conditions often coexist in the real world alongside various lighting effects at night. This paper first explores the challenging multi-weather nighttime image restoration task, where various types of weather degradations are intertwined with flare effects. To support the research, we contribute the AllWeatherNight dataset, featuring large-scale high-quality nighttime images with diverse compositional degradations, synthesized using our introduced illumination-aware degradation generation. Moreover, we present ClearNight, a unified nighttime image restoration framework, which effectively removes complex degradations in one go. Specifically, ClearNight extracts Retinex-based dual priors and explicitly guides the network to focus on uneven illumination regions and intrinsic texture contents respectively, thereby enhancing restoration effectiveness in nighttime scenarios. In order to better represent the common and unique characters of multiple weather degradations, we introduce a weather-aware dynamic specific-commonality collaboration method, which identifies weather degradations and adaptively selects optimal candidate units associated with specific weather types. Our ClearNight achieves state-of-the-art performance on both synthetic and real-world images. Comprehensive ablation experiments validate the necessity of AllWeatherNight dataset as well as the effectiveness of ClearNight. Project page: https://henlyta.github.io/ClearNight/mainpage.html"
        },
        {
            "title": "Start",
            "content": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration Yuetong Liu1 Yunqiu Xu2* Yang Wei1 Xiuli Bi1 Bin Xiao1 1Chongqing University of Posts and Telecommunications 2 Zhejiang University d230201022@stu.cqupt.edu.cn imyunqiuxu@gmail.com {weiyang,bixl,xiaobin}@cqupt.edu.cn 5 2 0 M 2 2 ] . [ 1 9 7 4 6 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Restoring nighttime images affected by multiple adverse weather conditions is practical yet under-explored research problem, as multiple weather conditions often coexist in the real world alongside various lighting effects at night. This paper first explores the challenging multiweather nighttime image restoration task, where various types of weather degradations are intertwined with flare effects. To support the research, we contribute the AllWeatherNight dataset, featuring large-scale high-quality nighttime images with diverse compositional degradations, synthesized using our introduced illumination-aware degradation generation. Moreover, we present ClearNight, unified nighttime image restoration framework, which effectively removes complex degradations in one go. Specifically, ClearNight extracts Retinex-based dual priors and explicitly guides the network to focus on uneven illumination regions and intrinsic texture contents respectively, thereby enhancing restoration effectiveness in nighttime scenarios. In order to better represent the common and unique characters of multiple weather degradations, we introduce weather-aware dynamic specific-commonality collaboration method, which identifies weather degradations and adaptively selects optimal candidate units associated with specific weather types. Our ClearNight achieves state-ofthe-art performance on both synthetic and real-world images. Comprehensive ablation experiments validate the necessity of AllWeatherNight dataset as well as the effectiveness of ClearNight. Project page: https://henlyta. github.io/ClearNight/mainpage.html. 1. Introduction Image restoration under adverse weather conditions is vital preprocessing step in many computer vision applications, such as autonomous driving [17, 50, 51, 56] and video surveillance [14, 35, 44]. Although significant efforts have been made to tackle the issues posed by adverse weather images, prior works [6, 45, 53, 55, 58] often neglect the complexities of lighting conditions and flares, limiting their *Corresponding authors. (a) ClearNight is the first unified framework that effectively removes multiple weather degradations within nighttime images in one go (b) benchmark results across various types of weather degradations, where rain and snow scenes contain images affected by compound weather degradations, and ClearNight achieves the best average results over all images Figure 1. Comparison of our introduced ClearNight with state-ofthe-art adverse weather image restoration approaches. ClearNight exhibits robust generalization across diverse weather conditions and shows superior effectiveness in handling mixed degradations. effectiveness in nighttime scenarios. Unlike daytime scenes, adverse weather degradations and lighting are closely intertwined in real nighttime scenes, severely obscuring the background contents. Although several works [33, 60, 64] have explored nighttime adverse weather image restoration, they typically focus only on single type of weather condition, leading to poor performance in real-world images with complex scenarios (see Figure 1a). Since multiple severe weather conditions usually co-occur in the real-world nighttime scenes, restoring 1 Table 1. Comparison to related adverse weather datasets. H, RS, RD and indicate haze, rain streak, raindrop and snow, respectively. refers to the presence of flares and normal lighting image pairs in datasets. Syn. denotes synthesized images. Datasets RS RD P # data Syn./Real Day/Night Outdoor-Rain [30] RainDS [40] BID-II [13] WeatherStream [62] CDD-11 [12] 10.5K 5.8K 32.6K 202K 15K Syn. Syn.&Real Syn. Real Syn. Day Day Day Day Day Raindrop Clarity[21] Night&YellowHaze [31] NHC&NHM&NHR [64] GTA5[52] UNREAL-NH [33] GTAV-NightRain [59] GTAV-NightRain-H [60] RVSD [3] AllWeatherNight (ours) 10K 10.3K Syn.&Real 12.8K 5.5K 101.9K Real Syn. Syn.&Real Syn. 15.2K 43.8K 17K 0.8K Syn. Syn. Syn. Day&Night Night Night Night Night Night Night Night Syn.&Real Night Figure 2. Real-world and synthetic samples in AllWeatherNight dataset. The synthetic images effectively simulate real-world nighttime scenes with various realistic adverse weather and flare degradations. images affected by multiple degradations is practical yet remains under-explored. This motivates us to investigate unified framework for nighttime image restoration that can effectively handle multiple weather degradations in one go, while also mitigating the complex interactions between uneven lighting and adverse weather effects. However, to the best of our knowledge, there is no existing dataset that is applicable to multi-weather nighttime image restoration. As summarized in Table 1, all previous datasets [3, 12, 21, 33, 60], overlook nighttime scenarios with multiple adverse weather conditions under nonuniform illumination. To facilitate the research, we construct AllWeatherNight, dataset for restoring nighttime images affected by multiple adverse weather effects and flares. Constructing such dataset is challenging due to the scarcity of high-quality degraded-clean image pairs. We design an illumination-aware degradation generation method to emulate real-world scenarios, which synthesizes the combined effect of uneven lighting and diverse adverse weather. The images generated with our illumination-aware synthesis not only achieve higher human perceptual preferences but also lead to improved performance on real-world images when used for training. In total, we generate 9K highly realistic synthetic image pairs for model training and evaluation (see Figure 2). In addition, we meticulously collect 1K nighttime photos to further assess models effectiveness and generalization capabilities on real images. In nighttime scenes, the interplay between artificial lighting and multiple adverse weather effects complicates image restoration, as their intertwined visual manifestations make it challenging for existing unified models [6, 48, 65, 68] to effectively remove diverse degradations and lead to suboptimal performance. To address these issues, we present ClearNight, the first unified framework tailored for multi-weather nighttime image restoration, integrating the Retinex-based dual prior guidance as well as the weatheraware dynamic specificity and commonality collaboration. Specifically, we extract multi-scale illumination and reflectance priors based on Retinex theory [10, 27], for explicit disentanglement of lighting and texture information within degraded images. The illumination prior guides the model to focus on illumination-related regions, enabling more effective restoration under uneven lighting conditions. The reflectance prior enhances the feature representation of various texture information, mitigating weather-induced degradations and recovering clear background details. In addition, specificity-commonality dual-branched architecture is devised to better represent the complex degradations caused by multiple types of adverse weather conditions, where the specificity branch is dynamically constructed and synergizes with the commonality branch based on the different weather types present in the input images. The dynamic specificity branch contains diverse subnetworks, each formed by selecting different combinations of candidate units [19, 41, 66]. To further enhance the capabilities of handling multi-weather degradations, we try to associate these units with specific weather types. Concretely, an auxiliary weather instructor is introduced to identify weather degradations and implicitly guide the dynamic allocator. Thereby, the dynamic allocator becomes weatheraware and is encouraged to select consistent units for the same types of adverse weather conditions. Extensive experiments and analysis demonstrate the superiority of our ClearNight on both synthetic and real-world images. The main contributions are summarized as follows: This work pioneers multi-weather image restoration in nighttime conditions. We contribute new dataset featuring 10K realistic illumination-aware synthetic images with multi-degradation alongside real-world samples. We propose unified framework to recover multi-weather nighttime images, which integrates the Retinex-based dual prior guidance as well as the weather-aware dynamic 2 Figure 3. Visualization of four synthesized variants of adverse weather nighttime images, where Weather Only and Flare Only denote synthesis with illumination-aware weather degradation and flare, respectively. Ours involves synthesis with both types of degradations. specificity-commonality collaboration method, tailored to address challenging uneven lighting conditions and diverse weather degradations entangled in nighttime scenes. Our approach effectively removes various types of degradations and cleans nighttime images. It outperforms stateof-the-art methods on both synthetic and real-world adverse weather nighttime images by non-trivial margin. 2. Related Work Adverse Weather Nighttime Image Restoration. Early works [39, 63] typically relied on physical priors and statistical assumptions, limiting their effectiveness and robustness in handling complex real-world scenarios. To address the limitations, several data-driven methods have been developed, achieving impressive results in restoring nighttime images degraded by haze [9, 20, 31, 33, 52, 64], rain [60] and snow [3]. However, existing works focus on single-type degradations and overlook the fact that real-world nighttime scenes often involve multiple simultaneous degradations caused by adverse weather and uneven lighting. Multi-Weather Image Restoration. Multi-weather image restoration aims to restore various weather-degraded scenes using single model [1, 6, 8, 12, 24, 26, 53, 68]. Recently, numerous Transformer-based approaches are developed, which investigate weather type queries [23], adverse weather pixel categorization [46], texture-guided appearance flow [47], codebook prior fusion [55], and etc. Moreover, several recent works [4, 65, 69] leverage remarkable generative capabilities of diffusion models for restoration performance boosting. To the best of our knowledge, all the previous methods focus on daytime scenes and ignore the entanglement of multiple adverse weather conditions with non-uniform illumination in nighttime scenes. Adverse Weather Datasets. Most previous datasets [11, 13, 30, 40, 49, 54, 62] only focuses on adverse weather restoration in daytime scenarios. few recent studies explore this task in nighttime scenarios and build datasets for haze [31, 33, 64], rain [59, 60] and snow scenes [3]. However, each existing nighttime dataset only considers single type of weather condition, ignoring the challenge of mixed weather conditions commonly found in the real world. 3. AllWeatherNight Dataset For advancing the multi-weather nighttime image restoration, we construct AllWeatherNight, new dataset featuring nighttime images with multiple adverse weather and uneven lighting conditions. We believe that handling mixed weather conditions is more practical for real-world scenarios, since multiple weather phenomena often co-occur, each causing unique distortions. Furthermore, flares in realworld photos obscure important details and distort the background, especially when intertwined with weather degradations (see Figure 2). This interplay significantly complicates image restoration, challenging models to maintain fidelity and enhance visibility in nighttime scenes. 3.1. Dataset Curation Data Collection and Filtering. We repurpose images from large-scale driving video dataset [56], which offers geographic and environmental diversity, and from low-light object detection dataset [36] covering scenes from very dark to twilight, using them as uncorrupted ground truths. We empirically design several rules to first filter out irrelevant daytime images and unsuitable nighttime images. We use average brightness, average gradient and grayscale variance product metrics to identify and select nighttime images with adequate visibility, rich textures and high clarity. After initial data filtering, we manually select 1,000 diverse images from each dataset as ground-truths. We also crawl and select 1,000 real-world nighttime images with various adverse weather conditions from the Internet and existing datasets [11, 20, 29, 34, 49, 61] to evaluate the model performance on real-world data. Illumination-Aware Degradation Generation. We observe that uneven lighting conditions in real-world nighttime scenes often interact with adverse weather degradations, yet this phenomenon is largely overlooked by prior works [30, 60, 64]. To synthesize more realistic nighttime images with adverse weather conditions, we introduce an illumination-aware degradation generation approach. 1e (ωe Ge(Xflare)) , (1) Figure 5. Distribution of the introduced AllWeatherNight dataset. (a) human ranking on syn. images Figure 4. Comparison of 4 different synthesized variants of nighttime images, where and indicate synthesis with illuminationaware weather degradation and flare effect respectively. (b) NIQE results on real images Given clean nighttime image X, the illumination-aware weather-degraded image Xd is synthesized by Xd = Xflare + (cid:88) eE where = {H, RS, RD, S} is set of weather effects including haze, rain streak, raindrop and snow. 1e indicates the presence of the respective weather effect. We devise weight map ωe to more accurately simulate the appearance of weather degradations under uneven lighting conditions, where ωe=RD is set to the Retinex decomposed [10, 27] illumination map I, while ωe=RD is empirically set to 1, as the appearance of raindrops is primarily influenced by the local background, neglecting the impact of distant illumination. Ge() represents corresponding adverse weather generation functions. For haze and rain streak generation, we apply atmospheric scattering model and interpolated Gaussian noise following [30]. We utilize Bezier curves as in [42] for raindrop simulation, and snow degradation is added using snowflake masks sampled from prior datasets [5, 34]. On the other hand, nighttime images often suffer from light flares, which are further exacerbated and intertwined with adverse weather effects. Therefore, we also introduce more flare effects into the clear images to faithfully simulate real-world nighttime conditions: (2) Xflare = α + β (L KAPSF), where Xflare is the output flare-degraded image. α is predefined hyper-parameter for clean image preservation, and β is dynamic hyper-parameter for flare content blending, determined by the pixel ratio of the light source region. The light source map is extracted from the clean image with thresholding and alpha matting refinement [28]. KAPSF is 2D kernel derived from the atmospheric point spread function as introduced in [20], and is the convolution operator. 3.2. Dataset Analysis Impact of Illumination-Aware Synthesis. To demonstrate the effectiveness of the proposed illumination-aware degradation generation, we analyze four different synthetic image variants of nighttime scenes (showcased in Figure 3). Ten volunteers are first recruited to rank image quality 4 along three dimensions (i.e., authenticity, lighting complexity and task relevance). As illustrated in Figure 4a, the combination of illumination-aware weather and flare achieves the best human preference. We evaluate whether training with our illumination-aware synthesized data improves image restoration performance in real-world scenarios. Figure 4b demonstrates that four representative models [8, 65, 68] trained with our synthesized data exhibit clear and consistent improvements. The images synthesized using our illumination-aware generation method not only exhibit greater visual realism, but also encourage models to disentangle these complex degradations, thereby enhancing restoration capabilities in challenging nighttime scenarios. Dataset Statistics. Overall, we synthesize 8,000 nighttime images for model training, covering both multiand singledegradation scenarios with varying scales, directions, patterns and intensities. As summarized in Figure 5, the test dataset comprises two subsets (i.e., synthetic and real-world subsets), each containing 1,000 images. The synthetic subset evaluates models across seven scenes, comprising both compound and single degradations. The real-world subset is categorized into four types of adverse weather conditions, enabling model evaluation in natural scenarios. 4. ClearNight Multi-weather nighttime image restoration aims to recover clear nighttime images degraded by flare effects and multiple adverse weather conditions (e.g., haze, rain, snow and the compound degradations). This paper introduces ClearNight, unified nighttime image restoration framework that can remove multiple weather degradations in one go. As depicted in Figure 6, ClearNight integrates Retinex-based dual prior guidance and weather-aware dynamic specificity-commonality collaboration. We explicitly decouple the uneven lighting and texture information, and extract Retinex-based dual priors to guide the network in Figure 6. Overview of our ClearNight framework. ClearNight primarily comprises Retinex-based dual prior guidance as well as weatheraware dynamic specificity and commonality branches. The extracted Retinex-based dual priors explicitly guide the network to focus on uneven illumination regions and intrinsic texture contents respectively. The weather-aware dynamic specificity branch adaptively accommodates specific weather conditions and collaborates with the commonality branch to effectively address multiple weather degradations. recovering clear images with natural lighting and rich background details. Building on the Swin-Unet-like architecture [2, 43], we devise weather-aware dynamic specificity branch and establish its collaboration with commonality branch to exploit the unique and shared characters of diverse weather degradations, enhancing the capability to restore images affected by multiple adverse weather conditions. 4.1. Retinex-Based Dual Prior Guidance Decoupling illumination and texture information is critical for nighttime image restoration, as it enables effective handling of non-uniform lighting and adverse weather effects. Guided by this insight, we extract multi-scale Retinexbased dual priors to explicitly guide the restoration processes, as shown in Figure 6. Specifically, based on Retinex theory [10, 27], we decompose input degraded image Xd: Xd = Rd Id, (3) where Rd and Id are the reflectance and illumination components decomposed from the input degraded image. The illumination component highlights the non-uniform lighting conditions, while the reflectance component represents intrinsic texture information of the image content. The two decomposed components prtx {Id, Rd} are then fed into independent multi-scale prior extraction units (MPE) to extract Retinex-based dual priors at three scales: with pn {in, rn}, p1, p2, p3 = MPE(prtx) (4) where pn denotes the extracted illumination or reflectance prior at the n-th scale, with {1, 2, 3}. The dual priors are extracted using MPE with identical structures but without weight sharing. The detailed architecture of MPE is depicted in the top-middle of Figure 6. dilated convolution is employed to project prtx into three scales, then the three-scale features are interactively fused to produce the final multi-scale illumination/reflectance priors pn. These multi-scale priors are subsequently used to guide the network, ensuring the awareness of illumination regions and intrinsic patterns across different stages of the network. More specifically, the extracted illumination priors in are successively injected into the Transformer blocks (TFBs) of the first three stages, which guide the network to decouple illumination from textures and pay more attention to the uneven lighting regions in nighttime images, thereby facilitating the handling of lighting-influenced weather degradations. As the reflectance priors rn contain rich intrinsic textures that not only capture background details but also reveal degradation types, we incorporate them into each weather-aware dynamic selection block (WDS) in the specificity branch to enhance weather type discrimination and improve multi-weather restoration performance. 4.2. Weather-Aware Dynamic Specificity and Commonality Collaboration Dynamic Specificity and Commonality Synergy. As different weather degradations (e.g., snow and rain streaks) share certain commonalities while exhibiting unique characteristics, we introduce synergistic design of dynamic specificity and commonality branches to effectively model complex multi-weather degradations. The commonality branch is built upon [43] consisting of sequential Transformer blocks with skip connections, while the specificity branch incorporates multiple weather-aware dynamic selec5 Figure 7. Qualitative results on AllWeatherNight synthetic testing subset. Please zoom in for better view. tion blocks (WDS) as residuals to each stage of TFBs. As shown in the top-right of Figure 6, each WDS consists of an encoder-decoder structure coupled with dynamic selection module. The WDS block merges the interacted features of the two branches and the reflectance priors as input. Inspired by [16, 66], we adopt the encoder-decoder structure to construct compact feature space that benefits WDS in learning discriminative weather-specific representation. The dynamic selection module contains multiple candidate units Uk() and learns to dynamically construct distinct sub-networks by sparsely selecting candidates tailored to different inputs, where is the unit index. Besides the candidate units, the dynamic selection module includes gating layer W() that computes the importance weights assigned to each candidate unit, and router R() that estimates the probability of each candidate unit being selected. Given the input feature a, the output of the a-th dynamic selection module can be calculated by (cid:88) Ma= kT W(f a)k Uk(f a) with =TopK(R(f a)), (5) where denotes the set of selected top-K unit indices, and W(f a)k is the importance weight for the k-th unit. Dynamic Weather Degradation Modeling. The dynamic selection module adaptively selects candidate units based solely on the visual content of each input sample, which fails to fully capture the correlations and distinctive characteristics of different weather types, thereby limiting its effectiveness in handling challenging multi-weather scenarios. To address this, we introduce weather instructor to associate distinct weather types with designated candidate units, enabling more structured modeling and improving generalization across diverse weather combinations. Specifically, we reorganize the dynamic selection module into distinct parts: the candidate units, router and gating layer serve as dynamic allocators, while the shared linear layers preceding them functions as the knowledge-sharing unit. We further introduce classifier after the knowledgesharing unit as weather instructor to aggregate features corresponding to the same weather condition by learning set of prototypes (one for each single weather degradation): ya = WI(f a) and = KSU(f enc), (6) where KSU() and WI() denote the knowledge-sharing unit and weather instructor, respectively. enc indicates the output of the encoder, and ya is the weather type predicted by the a-th dynamic selection module. The weather instructor performs multi-label classification on each input, supervised by binary cross-entropy loss Lbce with respect to the corresponding weather tags. During training, these prototypes pull the features from the corresponding weather towards themselves. Under the guidance of the weather instructor, features of the images affected by the same weather degradations tend to share similar representations. Consequently, the dynamic allocator is implicitly guided to be weather-aware and consistently selects similar candidate units for the inputs affected by the same weather degradations, thereby improving the diversity and specificity of candidate units and restoration effectiveness. 4.3. Network Optimization Multiple losses are utilized to jointly optimize ClearNight, where L1 loss L1 and perceptual loss Lper [22] are used to ensure that restored results resemble ground-truths closely. To enhance the structure and details of outputs, depth loss Ldepth is exploited via minimizing the differences between ground-truths and predictions of pre-trained depth estimation model [32]. In addition, we use load balancing loss Llb [41] to balance the utilization of the candidate units. We jointly optimize the network using the total loss function: Ltotal = L1+λperLper+λbceLbce+λlbLlb+λdepthLdepth, (7) where λper, λbce, λlb and λdepth are the loss weights. 5. Experiments 5.1. Implementation Details The synthetic image size is 640 360. During training, the input image is randomly cropped to 256 256. We adopt DehazeFormer [43] as our baseline for its advanced normalization and spatial aggregation, allowing effective natural lighting restoration. We use Adam [25] as optimizer and the initial learning rate is set to 2 104 for 100 epochs. The learning rate is adjusted using the cosine annealing scheme [18]. The loss weight λper, λbce, λlb and λdepth are set to 0.1, 0.001, 0.01 and 0.02, respectively. α is set to 0.995. 6 Figure 8. Qualitative results on AllWeatherNight real-world testing subset. Please zoom in for better view. Table 2. Quantitative results on AllWeatherNight synthetic testing subset. The best and second-best results are highlighted in red and blue. Method Rain Scene Rain Streak PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM Snow Scene Snow Flare Haze Multi-Degradation Single-Degradation Raindrop TKL [6] FSDGN [57] WeatherDiff [69] WGWS [68] RLP [60] RAMiT [8] DiffUIR [65] DEA-Net [7] 29.0919 / 0.8769 26.2657 / 0.8456 31.6136 / 0.9401 30.4227 / 0.8844 32.5573 / 0.9561 31.0471 / 0.9247 36.7239 / 0.9741 29.9850 / 0.8730 28.0147 / 0.8474 34.1807 / 0.9378 31.3309 / 0.8851 33.9535 / 0.9638 31.9022 / 0.9288 38.7223 / 0.9798 29.7631 / 0.8936 27.1109 / 0.8537 30.5020 / 0.9256 33.3630 / 0.9352 35.9385 / 0.9726 33.6573 / 0.9488 40.2057 / 0.9826 30.5811 / 0.8961 27.9023 / 0.8658 31.6132 / 0.9181 32.8077 / 0.9221 34.3616 / 0.9554 32.1325 / 0.9196 32.1085 / 0.9017 21.2180 / 0.6641 19.3059 / 0.6124 18.6348 / 0.6379 31.1586 / 0.8867 32.4204 / 0.9322 31.5645 / 0.9101 34.7658 / 0.9451 30.4565 / 0.9106 29.1169 / 0.8889 36.4414 / 0.9738 31.9433 / 0.9204 33.8452 / 0.9632 32.8934 / 0.9491 43.0080 / 0.9934 27.6676 / 0.8040 25.8151 / 0.7892 28.4763 / 0.8341 30.2547 / 0.8624 29.6778 / 0.8740 26.9833 / 0.8055 31.2789 / 0.8816 31.4244 / 0.9202 29.2500 / 0.8956 35.8174 / 0.9612 32.7631 / 0.9285 34.8406 / 0.9704 33.5811 / 0.9493 38.6533 / 0.9807 Baseline 28.7976 / 0.8825 27.1337 / 0.8452 30.2905 / 0.9257 30.5615 / 0.8994 33.0758 / 0.9598 31.3318 / 0.9182 36.0821 / 0.9738 ClearNight (ours) 32.5937 / 0.9223 30.6464 / 0.9041 36.4655 / 0.9621 33.6238 / 0.9331 35.4282 / 0.9723 33.9747 / 0.9539 38.7707 / 0.9838 Table 3. Quantitative results on AllWeatherNight real-world testing subset, evaluated using the commonly used NIQE metric. Method Haze Rain Streak Raindrop Snow TKL [6] FSDGN [57] WeatherDiff [69] WGWS [68] RLP [60] RAMiT [8] DiffUIR [65] DEA-Net [7] Baseline ClearNight (ours) 4.1872 4.2780 4.1964 4.1879 4.9699 4.1655 4.1063 4. 4.2054 4.1623 3.7765 4.4694 3.7842 3.7732 4.1882 3.9298 3.8728 3.9826 3.9983 3.7653 3.9238 4.9149 3.9254 3.9635 5.6240 4.0808 4.0471 4.0889 4.0778 3.8882 3.2680 4.1528 3.3451 3.2769 4.7669 3.2497 3.3547 3. 3.4605 3.2191 5.2. Comparison with the State-of-the-Arts Results on Synthetic Data. We compare ClearNight with state-of-the-art adverse weather image restoration methods on AllWeatherNight, including multi-weather image restoration method [6, 8, 65, 68, 69] and task-specific approaches [7, 57, 60]. As shown in Figure 7, TKL [6], FSDGN [57] and DiffUIR [65] produce overly smooth results such as the clock and railing regions, while other approaches often lose structural details or leave residual artifacts. In contrast, ClearNight preserves rich background details and restores natural lighting, illustrating robust performance across diverse and complex nighttime scenarios. Table 2 reports quantitative results on multi-degradation and single-degradation scenes, evaluated using PSNR [15] and SSIM [67] for synthetic samples. Among these methods, FSDGN [57], RLP [60] and DEA-Net [7] are tailored for daytime/nighttime task-specific image restoration, yet they exhibit limited performance in other scenarios due to their inability to extract diverse weather features. In contrast, our method focuses on robustly addressing weather effects in nighttime scenes. As the training data lack dedicated flare samples, its performance in flare removal remains moderate. Nevertheless, ClearNight achieves the best results on the multi-degradation subset and delivers competitive performance on single-degradation samples. Results on Real-World Data. To further validate ClearNights performance, we conduct qualitative and quantitative experiments on real-world samples. As shown in Figure 8, ClearNight effectively removes most weather effects and mitigates flare impacts, even when degradation blends with background content. Our method can consistently restore more natural results compared to state-of-theart methods. Furthermore, we use NIQE [37] to assess the restored images, as shown in Table 3. The experimental results demonstrate that ClearNight can predict highly realistic images under various adverse weather conditions. 5.3. Ablation Studies Impact of Model Components. We evaluate the effectiveness of the illumination prior, reflectance prior, dynamic allocator and weather indicator on the Rain Scene testing subset. As shown in Table 4, dual priors significantly enhance the restoration performance over the baseline, while the dy7 Table 4. Ablation study of key component. IP and RP denote illumination prior and reflectance prior, respectively. DA and WI indicate dynamic allocator and weather indicator in WDS. Case case 1 case 2 case 3 case 4 case 5 case 6 ClearNight IP DA RP WI PSNR / SSIM 28.7976 / 0.8825 32.1304 / 0.9176 31.7075 / 0.9113 32.2393 / 0.9179 32.4430 / 0.9214 32.2528 / 0.9184 32.5937 / 0.9223 Table 5. Ablation on Retinex-based dual prior extraction. (b) impact of reflection prior (a) impact of illumination prior i1 i3 PSNR / SSIM 28.7976 / 0.8825 31.6843 / 0.9109 31.6714 / 0.9114 31.6429 / 0.9110 31.7217 / 0.9115 32.1304 / 0.9176 r1 r2 r3 PSNR / SSIM 31.7075 / 0.9113 31.9680 / 0.9198 31.9374 / 0.9201 31.8438 / 0.9172 32.0335 / 0.9203 32.0594 / 0. namic allocator and the weather indicator further optimize candidate unit selection to handle multi-weather scenes. Quantitative results demonstrate that the integration of all components achieves the best performance. Influence of Retinex-Based Dual Prior Guidance. To evaluate the impact of multi-scale prior, we conduct ablation studies on illumination and reflectance priors at various scales, using case 1 and case 3 as baselines, respectively. As shown in Table 5, incorporating multi-scale dual priors into the network enables it to better focus on lighting and texture features, significantly enhancing restoration performance. Effectiveness of WDS. We investigate the number of candidate units (B) and top-K selection parameter (K). As shown in Figure 9, increasing the number of units enhances selective capacity, with an optimal being critical for superior performance. Notably, performance improves with larger up to threshold, beyond which irrelevant information causes degradation. Figure 10 illustrates the relationships between various degradations and selected units, demonstrating the effectiveness of WDS in capturing the specificity of different degradations. The WDS differentiates weather features, selecting similar unit combinations for the same weather and different ones for distinct weather, enabling the network to efficiently eliminate diverse weather distortions. We visualize the feature of different weather types in Figure 11. Despite the visual similarity between raindrop and snow, ClearNight still differentiates them. Notably, the features of H+S lie between haze and snow features, indicating that our model learns the correlations among multiple weather effects in complex nighttime scenarios. Figure 9. Ablation study of the number of candidate units B, and selected top-K units. We set and to 25 and 10 by default. Figure 10. Correlation of selected units and diverse degradations. The WDS associates various distortions with candidate units. (a) four different single-degradations (b) singleand multi-degradations Figure 11. Feature distribution visualization of different weather using t-SNE. H, RS, RD and indicate haze, rain streak, raindrop and snow. The number after weather types is the index of WDS. 6. Conclusion This paper explores practical yet under-explored task, i.e., multi-weather nighttime image restoration. To facilitate this task, we introduce an illumination-aware degradation generation approach and construct new dataset featuring 10K high-quality nighttime images with various compositional adverse weather and lighting conditions. In addition, we propose ClearNight, unified framework, tailored for the new task, capable of removing multiple degradations in one go. ClearNight leverages Retinex-based dual priors to explicitly guide the network to focus on uneven illumination regions and intrinsic texture contents respectively. Moreover, ClearNight incorporates weather-aware dynamic specific-commonality collaboration to better capture the characteristics of various weather conditions, enabling effective multi-weather degradation removal. Comprehensive experiments and analysis on both synthetic and real-world images demonstrate the superiority of our ClearNight. 8 Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Implementation Details The model was trained with batch size of 1. To stabilize training, we adopt cosine annealing warm restarts strategy with restart period of 50 epochs. The proposed framework is trained and tested on an NVIDIA RTX A6000. The architecture of ClearNight, illustrated in Figure 2, applies multi-branch structure [38, 60, 68], integrating illumination and reflectance priors into the backbone. The backbone consists of commonality branch and weatheraware dynamic specificity branch. The commonality branch employs five Transformer blocks [43] with modified normalization layers, activation functions, and spatial feature aggregation to enhance global feature extraction for improved restoration performance. The weather-aware dynamic specificity branch includes five weather-aware dynamic selection blocks (WDS) tailored to adverse weather degradations. After each interaction between unique and common features, we use upsampling and downsampling operations to adjust feature map scales, doubling or halving the width and height as needed. B. Dataset License and Intended Use The generated images and labels in AllWeatherNight dataset are released under the BSD 3-Clause License, permissive open-source license that grants users the freedom to use, copy, modify, and distribute the dataset, whether in its original form or as part of derivative works. The groundtruths are released under the BSD 3-Clause License. The AllWeatherNight dataset is designed to advance research in multi-weather nighttime image restoration. It provides comprehensive resource for developing, training, and evaluating algorithms to restore nighttime images degraded by diverse adverse weather degradations and nonuniform flare effects. Furthermore, it serves as standardized benchmark for comparing methods in adverse weather image restoration field. C. Limitations and Future Works While our approach effectively handles nighttime illumination and various adverse weather degradations, its performance under extremely dynamic lighting conditions, such as rapidly changing artificial lighting, needs improvement. As shown in Figure 12, while most weather-induced degradations can be removed, rapidly changing flares still remain. Future work could focus on improving the robustness to such extreme scenarios. The original data for AllWeatherNight are sourced from BDD100K [56] and Exdark Figure 12. Adverse weather samples and their restored results under rapidly changing lighting conditions. [36], which exclusively focus on driving and object detection. Although our dataset incorporates various nighttime adverse weather degradations, the natural scenes in it are relatively homogeneous, limiting its environmental diversity. We consider expanding the AllWeatherNight to include wider range of real-world scenarios to further enhance the models generalization ability. D. Additional Analysis of AllWeatherNight Analysis of AllWeatherNights Authenticity. To validate the authenticity of the AllWeatherNight dataset, we conduct statistical analysis of feature distribution across synthetic adverse weather nighttime datasets The synthetic datasets inand real-world samples. cludes haze datasets (UNREAL-NH [33], GTA5 [52], NHC&NHR&NHM [64], YellowHaze&NightHaze [31]), rain datasets (GTAV-NightRain [59], GTAV-NightRainH [60]) and snow dataset (RVSD [3]). As shown in Figure 13, the t-SNE visualization reveals that AllWeatherNights adverse weather degradation distributions are more closely aligned with real-world nighttime images. From left to right, the subfigures depict the feature distributions for haze, rain, and snow, respectively. Notably, AllWeatherNights haze distribution closely matches that of real-world nighttime haze, while its rain and snow distributions exhibit the highest similarity to real-world samples. These findings demonstrate that AllWeatherNight outperforms existing synthetic datasets in approximating realworld nighttime degradation features, making it highly suitable for training models that generalize to diverse realworld scenarios. Consequently, AllWeatherNight provides robust foundation for enhancing model performance in multi-weather nighttime image restoration tasks. 9 (a) visualization of haze dataset features (b) visualization of rain dataset features Figure 13. The t-SNE map of various nighttime adverse weather datasets. The feature distribution of adverse weather degradations in our AllWeatherNight is closer to real-world nighttime scenes than existing nighttime synthetic datasets. (c) visualization of snow dataset features Figure 14. Visualization of 4 different synthesized variants of adverse weather nighttime images, where Weather only and Flare only indicate synthesis with illumination-aware weather degradation and flare degradation respectively. Ours includes both degradation synthesis. Supplemental Visualizations of Illumination-Aware Generation. Existing synthetic adverse weather nighttime datasets typically model single type of weather distortion and rarely account for background blurring induced by artificial lighting. In contrast, the proposed AllWeatherNight encompasses diverse range of degradation, including haze, rain streaks, raindrops, snow, flare, and their combinations. Moreover, we design an illumination-aware degradation generation method to simulate realistic adverse weather and flare effects in nighttime scenes. To verify 10 (a) without Lbce and Llb (b) with only Lbce (c) with only Llb (d) with both Lbce and Llb Figure 15. Correlation between degradation types and candidate units correlation under different loss supervision. Figure 16. Ablation study on λbce. the effectiveness of the generation method, we synthesized datasets under four settings, with supplemental visualizations provided in Figure 14. Each type of degradation in AllWeatherNight varies in scales, directions, densities, and styles, ensuring comprehensive coverage. The generated flare preserves background clarity in the original image to some extent, enabling the network to effectively restore natural lighting and fine details. E. Additional Analysis of WDS Analysis of the Loss Functions. Our method employs the loss functions Llb and Lbce to supervise the dynamic allocator and weather instructor, respectively, enabling effective auxiliary selection tasks. To evaluate their impact, we conduct experiments on AllWeatherNight, with results visualized in Figure 15. Evidently, Llb optimizes the allocation of candidate units, ensuring efficient utilization. Without Llb, as shown in Figure 15b, many units remain underutilized, degrading performance in complex multi-weather scenes. Similarly, applying Lbce enhances the correlation between diverse weather conditions and candidate units seFigure 17. Comparison of ClearNight with other methods on AllWeatherNight synthetic testing subset. The marker size reflects the number of model parameters. lection, improving the models ability to address multiple adverse weather effects. Without Lbce, the selected units for different weather distortions tend to converge, impairing multi-weather nighttime image restoration, as shown in Figure 15c. In contrast, the combined use of Lbce and Llb enables precise identification of different adverse weather degradations and balanced utilization of candidate units. As depicted in Figure 15d, the correlation visualization reveals that distinct units are selected for different adverse weather conditions, with similar sets of units consistently chosen for the same weather type, and all units are effectively utilized, maintaining balanced load distribution. Consequently, this dual-loss collaboration strategy significantly enhances the models performance in restoring complex nighttime scenarios with diverse adverse weather conditions and flare ef11 Figure 18. Qualitative results of rain and snow scenes in the AllWeatherNight. fects, achieving high-quality results. The weighting of the loss function critically influences the performance of the network during training, particularly for multi-weather nighttime scenes with flare effects. To determine an optimal weighting stratregy for Lbce, we conduct an ablation study on the Rain Scene in AllWeatherNight dataset, with results shown in Figure 16. The absence of Lbce significantly degrades models performance, as it fails to distinguish diverse adverse weather degradation features, impairing the guidance for unit selection. When 12 Figure 19. Qualitative results of single degradation in the AllWeatherNight, including haze, rain streak, raindrop, snow and flare. 13 Figure 20. Qualitative results on real-world samples. 14 the weight was set to 0.01, the overly strong constraint of the Lbce limits its restoration performance for different weather with similar patterns. Conversely, with an excessively low weight of 0.0001, various adverse weather features cannot be effectively distinguished, resulting in poor performance across multiple weather conditions. Weights of 0.001 achieve the best performance. Therefore, we set the λbce to 0.001 for our model. F. Analysis of Computational Complexity Figure 17 compares the parameters and inference time of our method with recent competitive approaches. While diffusion-based method such as WeatherDiff [69] and DiffUIR [65] achieve impressive performance, they suffer from higher inference time, which limits their practicality in realworld scenes. On the other hand, lightweight models such as RAMiT [8] and DEA-Net [7] demonstrate fast inference, but often compromise restoration quality. Our ClearNight achieves good trade-off between performance and efficiency. With only 2.84M parameters and an inference time of 0.32s, it is significantly faster than most diffusion-based models while offering much stronger restoration performance than lightweight models. In summary, ClearNight maintains relatively low computational cost, making it practical and effective solution for multi-weather nighttime image restoration. G. Additional Visualization Results Multi-Degradation. We have provided more visual results of our model on Rain Scene and Snow Scene in AllWeatherNight, as shown in Figure 18. The Rain and Snow Scene testing subsets contain degraded images with complex multi-weather degradations, effectively simulating real-world nighttime scenes. The visual results demonstrate that ClearNight excels in removing the influence of adverse weather while restoring rich background details. Especially when the background is similar to the degradation caused by the entanglement of adverse weather and flares, ClearNight accurately distinguishes these factors, producing high-quality images. Single-Degradation. Although multiple adverse weather conditions frequently occur in an intertwined manner, the restoration of simple weather nighttime scenes should not be ignored. We also evaluate the generalization capability of ClearNight on single-degradation conditions. As shown in Figure 19, we can see that our method is equally effective in single-degradation scenarios. Real-World. Nighttime real-world degraded images often contain complex weather interferences and diverse flares. As shown in Figure 20, we provide more comparison result of real-world scenes, e.g., haze (1st and 2nd rows), rain (3rd and 4th rows), raindrop (5th and 6th rows), snow images, (7th and 8th rows). The rain and snow scenes often exhibit subtle haze effects, as evidenced by real-world nighttime consistent with multi-degradation scenarios in our dataset. The restored results show that ClearNight effectively removes the interference of multiple adverse weather conditions while recovering natural nighttime lighting. Compared to existing adverse weather image restoration approaches, our method is superior in reconstructing scene content and lighting conditions."
        },
        {
            "title": "References",
            "content": "[1] Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, and Ran He. Multimodal prompt perceiver: Empower adaptiveness, generalizability and fidelity for all-in-one image restoration. In CVPR, 2024. 3 [2] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In ECCVW, 2022. 5 [3] Haoyu Chen, Jingjing Ren, Jinjin Gu, Hongtao Wu, Xuequan Lu, Haoming Cai, and Lei Zhu. Snow removal in video: new dataset and novel method. In ICCV, 2023. 2, 3, 9 [4] Sixiang Chen, Tian Ye, Kai Zhang, Zhaohu Xing, Yunlong Lin, and Lei Zhu. Teaching tailored to talent: Adverse weather restoration via prompt pool and depth-anything constraint. In ECCV, 2024. 3 [5] Wei-Ting Chen, Hao-Yu Fang, Cheng-Lin Hsieh, Cheng-Che Tsai, I-Hsiang Chen, Jian-Jiun Ding, and Sy-Yen Kuo. All snow removed: Single image desnowing algorithm using hierarchical dual-tree complex wavelet representation and contradict channel loss. In ICCV, 2021. 4 [6] Wei-Ting Chen, Zhi-Kai Huang, Cheng-Che Tsai, HaoHsiang Yang, Jian-Jiun Ding, and Sy-Yen Kuo. Learning multiple adverse weather removal via two-stage knowledge learning and multi-contrastive regularization: Toward unified model. In CVPR, 2022. 1, 2, 3, 7 [7] Zixuan Chen, Zewei He, and Zhe-Ming Lu. Dea-net: Single image dehazing based on detail-enhanced convolution and content-guided attention. IEEE TIP, 2024. 7, [8] Haram Choi, Cheolwoong Na, Jihyeon Oh, Seungjae Lee, Jinseop Kim, Subeen Choe, Jeongmin Lee, Taehoon Kim, and Jihoon Yang. Reciprocal attention mixing transformer for lightweight image restoration. In CVPRW, 2024. 3, 4, 7, 15 [9] Xiaofeng Cong, Jie Gui, Jing Zhang, Junming Hou, and Hao Shen. semi-supervised nighttime dehazing baseline with spatial-frequency aware and realistic brightness constraint. In CVPR, 2024. 3 [10] Herbert. Land Edwin. The retinex theory of color vision. Scientific American, 1977. 2, 4, 5 [11] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding, and John Paisley. Removing rain from single images via deep detail network. In CVPR, 2017. 3 [12] Yu Guo, Yuan Gao, Yuxu Lu, Huilin Zhu, Ryan Wen Liu, and Shengfeng He. Onerestore: universal restoration framework for composite degradation. In ECCV, 2024. 2, 15 [13] Junlin Han, Weihao Li, Pengfei Fang, Chunyi Sun, Jie Hong, Mohammad Ali Armin, Lars Petersson, and Hongdong Li. Blind image decomposition. In ECCV, 2022. 2, 3 [14] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal alignment networks for long-term video. In CVPR, 2022. 1 [15] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In ICPR, 2010. 7 [16] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. In Lora: Low-rank adaptation of large language models. ICLR, 2022. 6 [17] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In CVPR, 2023. [18] Loshchilov Ilya and Hutter Frank. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017. 6 [19] Heng Jia, Yunqiu Xu, Linchao Zhu, Guang Chen, Yufei Wang, and Yi Yang. Mos2: Mixture of scale and shift experts for text-only video captioning. In ACM MM, 2024. 2 [20] Yeying Jin, Beibei Lin, Wending Yan, Yuan Yuan, Wei Ye, and Robby T. Tan. Enhancing visibility in nighttime haze images using guided apsf and gradient adaptive convolution. In ACM MM, 2023. 3, 4 [21] Yeying Jin, Xin Li, Jiadong Wang, Yan Zhang, and Malu Zhang. Raindrop clarity: dual-focused dataset for day and night raindrop removal. In ECCV, 2024. 2 [22] Justin Johnson, Alexandre Alahi, and Fei-Fei Li. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. [23] Jeya Maria Jose Valanarasu, Rajeev Yasarla, and Vishal M. Patel. Transweather: Transformer-based restoration of imIn CVPR, ages degraded by adverse weather conditions. 2022. 3 [24] Youngrae Kim, Younggeol Cho, Thanh-Tung Nguyen, Seunghoon Hong, and Dongman Lee. Metaweather: Few-shot weather-degraded image restoration. In ECCV, 2024. 3 [25] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. 6 [26] Ashutosh Kulkarni, Shruti S. Phutke, and Subrahmanyam Murala. Unified transformer network for multi-weather image restoration. In ECCV, 2023. 3 [27] Edwin H. Land and John J. Mccann. Lightness and retinex theory. The Optical Society of America, 1971. 2, 4, 5 [28] Anat Levin, Dani Lischinski, and Yair Weiss. closed-form solution to natural image matting. IEEE TPAMI, 2008. 4 [29] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking singleimage dehazing and beyond. IEEE TIP, 2019. 3 [30] Ruoteng Li, Loong-Fah Cheong, and Robby T. Tan. Heavy rain image restoration: Integrating physics model and conditional adversarial learning. In CVPR, 2019. 2, 3, [31] Yinghong Liao, Zhuo Su, Xiangguo Liang, and Bin Qiu. Hdp-net: Haze density prediction network for nighttime dehazing. In Advances in Multimedia Information Processing PCM, 2018. 2, 3, 9 [32] Lina Liu, Xibin Song, Mengmeng Wang, Yong Liu, and Liangjun Zhang. Self-supervised monocular depth estimation for all day images using domain separation. In ICCV, 2021. 6 [33] Yun Liu, Zhongsheng Yan, Sixiang Chen, Tian Ye, Wenqi Ren, and Erkang Chen. Nighthazeformer: Single nighttime haze removal using prior query transformer. In ACM MM, 2023. 1, 2, 3, 9 [34] Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, and Jenq-Neng Hwang. Desnownet: Context-aware deep network for snow removal. IEEE TIP, 2018. 3, 4 [35] Zuhao Liu, Xiao-Ming Wu, Dian Zheng, Kun-Yu Lin, and Wei-Shi Zheng. Generating anomalies for video anomaly detection with prompt-based feature mapping. In CVPR, 2023. 1 [36] Yuen Peng Loh and Chee Seng Chan. Getting to know lowlight images with the exclusively dark dataset. CVIU, 2019. 3, [37] Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. Making completely blind image quality analyzer. IEEE SPL, 2013. 7 [38] Dongwon Park, Byung Hyun Lee, and Se Young Chun. Allin-one image restoration for unknown degradations using In adaptive discriminative filters for specific degradations. CVPR, 2023. 9 [39] Soo-Chang Pei and Tzu-Yen Lee. Nighttime haze removal using color transfer pre-processing and dark channel prior. In ICIP, 2012. 3 [40] Ruijie Quan, Xin Yu, Yuanzhi Liang, and Yi Yang. Removing raindrops and rain streaks in one go. In CVPR, 2021. 2, 3 [41] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017. 2, 6 [42] Vera Soboleva and Oleg Shipitko. Raindrops on windshield: Dataset and lightweight gradient-based detection algorithm. In SSCI, 2021. [43] Yuda Song, Zhuqing He, Hui Qian, and Xin Du. Vision IEEE TIP, 2023. transformers for single image dehazing. 5, 6, 9 [44] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In CVPR, 2018. 1 [45] Shangquan Sun, Wenqi Ren, Xinwei Gao, Rui Wang, and Xiaochun Cao. Restoring images in adverse weather conditions via histogram transformer. In ECCV, 2024. 1 [46] Shangquan Sun, Wenqi Ren, Xinwei Gao, Rui Wang, and Xiaochun Cao. Restoring images in adverse weather conditions via histogram transformer. In ECCV, 2024. 3 [47] Chao Wang, Zhedong Zheng, Ruijie Quan, Yifan Sun, and Yi Yang. Context-aware pretraining for efficient blind image decomposition. In CVPR, 2023. 3 [48] Chao Wang, Zhedong Zheng, Ruijie Quan, and Yi Yang. Depth-aware blind image decomposition for real-world adverse weather recovery. In ECCV, 2024. [49] Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang Zhang, and Rynson W.H. Lau. Spatial attentive single-image 16 [66] Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, and Chun Yuan. Convolution meets lora: Parameter efficient finetuning for segment anything model. In ICLR, 2024. 2, 6 [67] Wang Zhou, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 2004. 7 [68] Yurui Zhu, Tianyu Wang, Xueyang Fu, Xuanyu Yang, Xin Guo, Jifeng Dai, Yu Qiao, and Xiaowei Hu. Learning weather-general and weather-specific features for image In restoration under multiple adverse weather conditions. CVPR, 2023. 2, 3, 4, 7, 9 [69] Ozan Ozdenizci and Robert Legenstein. Restoring vision in adverse weather conditions with patch-based denoising diffusion models. IEEE TPAMI, 2023. 3, 7, 15 deraining with high quality real rain dataset. 2019. In CVPR, [50] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worlddrive world models for autonomous driving. In ECCV, 2024. 1 [51] Yunqiu Xu, Linchao Zhu, and Yi Yang. Mc-bench: benchmark for multi-context visual grounding in the era of mllms. arXiv preprint arXiv:2410.12332, 2024. 1 [52] Wending Yan, Robby T. Tan, and Dengxin Dai. Nighttime defogging using high-low frequency decomposition and grayscale-color networks. In ECCV, 2020. 2, 3, 9 [53] Hao Yang, Liyuan Pan, Yan Yang, and Wei Liang. Languagedriven all-in-one adverse weather removal. In CVPR, 2024. 1, 3 [54] Wenhan Yang, Robby T. Tan, Jiashi Feng, Zongming Guo, Shuicheng Yan, and Jiaying Liu. Joint rain detection and removal from single image with contextualized deep networks. IEEE TPAMI, 2020. [55] Tian Ye, Sixiang Chen, Jinbin Bai, Jun Shi, Chenghao Xue, Jingxia Jiang, Junjie Yin, Erkang Chen, and Yun Liu. Adverse weather removal with codebook priors. In ICCV, 2023. 1, 3 [56] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: diverse driving dataset for heterogeneous multitask learning. In CVPR, 2020. 1, 3, 9 [57] Hu Yu, Naishan Zheng, Man Zhou, Jie Huang, Zeyu Xiao, and Feng Zhao. Frequency and spatial dual guidance for image dehazing. In ECCV, 2022. 7 [58] Conghan Yue, Zhengwei Peng, Junlong Ma, Shiyan Du, Pengxu Wei, and Dongyu Zhang. Image restoration through generalized ornstein-uhlenbeck bridge. In ICML, 2024. 1 [59] Fan Zhang, Shaodi You, Yu Li, and Ying Fu. Gtav-nightrain: Photometric realistic large-scale dataset for night-time rain streak removal. arXiv:2210.04708, 2022. 2, 3, 9 [60] Fan Zhang, Shaodi You, Yu Li, and Ying Fu. Learning rain location prior for nighttime deraining. In ICCV, 2023. 1, 2, 3, 7, [61] He Zhang, Vishwanath Sindagi, and Vishal M. Patel. Image de-raining using conditional generative adversarial network. IEEE TCSVT, 2020. 3 [62] Howard Zhang, Yunhao Ba, Ethan Yang, Varan Mehra, Blake Gella, Akira Suzuki, Arnold Pfahnl, Chethan Chinder Chandrappa, Alex Wong, and Achuta Kadambi. Weatherstream: Light transport automation of single image deweathering. In CVPR, 2023. 2, 3 [63] Jing Zhang, Yang Cao, Shuai Fang, Yu Kang, and Chang Wen Chen. Fast haze removal for nighttime image using maximum reflectance prior. In CVPR, 2017. 3 [64] Jing Zhang, Yang Cao, Zheng-Jun Zha, and Dacheng Tao. In ACM Nighttime dehazing with synthetic benchmark. MM, 2020. 1, 2, 3, 9 [65] Dian Zheng, Xiao-Ming Wu, Shuzhou Yang, Jian Zhang, Selective hourglass Jian-Fang Hu, and Wei-Shi Zheng. mapping for universal image restoration based on diffusion model. In CVPR, 2024. 2, 3, 4, 7,"
        }
    ],
    "affiliations": [
        "Chongqing University of Posts and Telecommunications",
        "Zhejiang University"
    ]
}