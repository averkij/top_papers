{
    "paper_title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
    "authors": [
        "Chunyu Xie",
        "Bin Wang",
        "Fanjing Kong",
        "Jincheng Li",
        "Dawei Liang",
        "Ji Ao",
        "Dawei Leng",
        "Yuhui Yin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-grained vision-language understanding requires precise alignment between visual content and linguistic descriptions, a capability that remains limited in current models, particularly in non-English settings. While models like CLIP perform well on global alignment, they often struggle to capture fine-grained details in object attributes, spatial relations, and linguistic expressions, with limited support for bilingual comprehension. To address these challenges, we introduce FG-CLIP 2, a bilingual vision-language model designed to advance fine-grained alignment for both English and Chinese. Our approach leverages rich fine-grained supervision, including region-text matching and long-caption modeling, alongside multiple discriminative objectives. We further introduce the Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar captions. Trained on a carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual performance. To enable rigorous evaluation, we present a new benchmark for Chinese multimodal understanding, featuring long-caption retrieval and bounding box classification. Extensive experiments on 29 datasets across 8 tasks show that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results in both languages. We release the model, code, and benchmark to facilitate future research on bilingual fine-grained alignment."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 1 2 9 0 1 . 0 1 5 2 : r FG-CLIP 2: BILINGUAL FINE-GRAINED VISIONLANGUAGE ALIGNMENT MODEL Chunyu Xie* Bin Wang*, Fanjing Kong, Jincheng Li, Dawei Liang, Ji Ao, Dawei Leng, Yuhui Yin"
        },
        {
            "title": "360 AI Research",
            "content": "Code&Model&Dataset: https://360cvgroup.github.io/FG-CLIP"
        },
        {
            "title": "ABSTRACT",
            "content": "Fine-grained vision-language understanding requires precise alignment between visual content and linguistic descriptions, capability that remains limited in current models, particularly in non-English settings. While models like CLIP perform well on global alignment, they often struggle to capture fine-grained details in object attributes, spatial relations, and linguistic expressions, with limited support for bilingual comprehension. To address these challenges, we introduce FG-CLIP 2, bilingual vision-language model designed to advance fine-grained alignment for both English and Chinese. Our approach leverages rich fine-grained supervision, including region-text matching and long-caption modeling, alongside multiple discriminative objectives. We further introduce the Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar captions. Trained on carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual performance. To enable rigorous evaluation, we present new benchmark for Chinese multimodal understanding, featuring long-caption retrieval and bounding box classification. Extensive experiments on 29 datasets across 8 tasks show that FG-CLIP 2 outperforms existing methods, achieving stateof-the-art results in both languages. We release the model, code, and benchmark to facilitate future research on bilingual fine-grained alignment."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-language alignment models (Tschannen et al., 2025; Chuang et al., 2025) have undergone rapid evolution in recent years, driven by pioneering works such as CLIP (Radford et al., 2021), which introduced large-scale contrastive pre-training on image-text pairs and demonstrated remarkable success in learning joint multimodal representations. These models excel at global alignment tasks such as zero-shot image classification and image-text retrieval, forming the foundation for wide range of multimodal understanding systems (Zhu et al., 2025; Team et al., 2025a; Li et al., 2025a; Wu et al., 2025; Team et al., 2025b). Their ability to align visual and linguistic concepts without explicit supervision has enabled strong generalization to diverse scenarios, including visual question answering (Lu et al., 2025; Wang et al., 2025a), image captioning (Bai et al., 2025; Li et al., 2025b), and content-based retrieval (Zhang et al., 2024a; Wei et al., 2024). However, their performance often degrades on fine-grained understanding tasks that require discriminating between similar object attributes, spatial configurations, or semantic distinctions. Such tasks demand precise alignment at both visual and linguistic levels: visually, they involve recognizing objects, attributes, and their spatial arrangements; linguistically, they require distinguishing between semantically similar expressions. This performance gap stems from reliance on coarse-grained image-text pairs during training, which encourages thematic alignment while failing to capture the fine-grained correspondences essential for robust visual grounding or attribute recognition. *Equal contribution. E-mail: xiechunyu@360.cn, wangbin10@360.cn Corresponding Author. E-mail: lengdawei@360.cn Several recent works have sought to address these limitations. Approaches such as FineCLIP (Jing et al., 2024) and LongCLIP (Zhang et al., 2024a) improve fine-grained understanding by incorporating region-level signals or supporting longer textual inputs. FG-CLIP (Xie et al., 2025) significantly advances the state of fine-grained modeling through large-scale data curation and attribute-based hard negative sampling. On the language side, Chinese-CLIP (Yang et al., 2022) and R2D2 (Xie et al., 2023) have laid the groundwork for Chinese vision-language alignment, yet they primarily focus on short-caption retrieval and lack support for fine-grained or region-level tasks. The development of fine-grained capabilities has thus remained largely confined to English, whereas Chinese models operate at coarser semantic level. No existing framework unifies these directions, and there is notable absence of comprehensive benchmarks for evaluating fine-grained understanding in Chinese, which hinders systematic progress in bilingual multimodal research. To address these challenges, we propose FG-CLIP 2, unified framework for bilingual fine-grained vision-language alignment. Our training strategy employs two-stage paradigm to progressively refine model capabilities. In the first stage, we perform initial global alignment with both short and long textual descriptions to capture coarse and detailed semantic content at the early phase of training. In the second stage, we incorporate fine-grained learning objectives that improve regional alignment, discriminative capability, and cross-modal ranking performance. To further refine the models ability to distinguish similar region-level descriptions, we propose the Textual Intra-modal Contrastive (TIC) loss, which learns from filtered hard negatives among high-similarity text pairs. FG-CLIP 2 is trained on large-scale, high-quality bilingual datasets with careful curation, enabling strong performance in both English and Chinese across diverse fine-grained vision-language tasks. We further contribute new benchmark suite to advance evaluation in Chinese multimodal understanding, featuring challenging tasks such as long caption image-text retrieval and bounding box classification in Chinese that go beyond conventional short-text retrieval and assess fine-grained comprehension more rigorously. Extensive experiments show that FG-CLIP 2 outperforms existing models on 29 datasets across 8 vision-language tasks in both Chinese and English, demonstrating powerful bilingual fine-grained vision-language alignment capability. To support future research and real-world deployment, our model, code and benchmark will be made publicly available."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Vision-language alignment models trained on large-scale data, such as CLIP (Radford et al., 2021), EVA-CLIP (Sun et al., 2023), SigLIP (Zhai et al., 2023), MetaCLIP (Xu et al., 2024) and DFN (Fang et al., 2024) have demonstrated strong zero-shot capabilities but primarily focus on global semantic alignment and are often trained on English-only corpora. While these models serve as backbones for downstream tasks including multimodal reasoning (Bai et al., 2025), open-vocabulary detection (Fu et al., 2025), and segmentation (Cho et al., 2024; Wang et al., 2025b), their lack of fine-grained and multilingual understanding limits broader applicability. Recent efforts aim to improve localization capability and dense feature alignment through region-level supervision. Methods like AlphaCLIP (Sun et al., 2024), FineCLIP (Jing et al., 2024), and FGCLIP (Xie et al., 2025) leverage bounding boxes or masked regions to enhance local correspondence, while CLOC (Chen et al., 2024a), TIPS (Maninis et al., 2025), and SigLIP 2 (Tschannen et al., 2025) introduce architectural or training enhancements for richer feature generation. On the multilingual front, Chinese-CLIP (Yang et al., 2022) and R2D2 (Xie et al., 2023) target Chinese understanding, and MetaCLIP 2 (Chuang et al., 2025) scales multilingual data collection for broader language coverage. However, these works often treat fine-grained and multilingual understanding separately, and none explicitly optimize both in unified framework. This gap inspires our work."
        },
        {
            "title": "3 APPROACH",
            "content": "Figure 1 illustrates FG-CLIP 2, vision-language model supporting Chinese and English. Our approach follows two-stage hierarchical learning framework: the first stage establishes strong semantic alignment by training on large-scale image-text pairs, each associated with both short caption and long caption; the second stage extends this learning by incorporating region-level alignment and fine-grained contrastive signals, enabling the model to preserve holistic scene understanding while enhancing its ability to discriminate fine-grained visual-language correspondences in both languages. Figure 1: Overview of the FG-CLIP 2. 3.1 ARCHITECTURE We build upon the SigLIP 2 (Tschannen et al., 2025) dual-encoder framework, introducing key adaptations for fine-grained understanding and bilingual alignment. For the text encoder, we extend the maximum input length from 64 to 196 tokens to accommodate longer descriptions. We employ the multilingual Gemma tokenizer (Team et al., 2024) with 256K vocabulary. On the vision side, we adopt data-adaptive resolution strategy: the target resolution is selected from {128, 256, 576, 784, 1024} based on the maximum image size per mini-batch, avoiding the stochastic sampling of SigLIP 2 and ensuring consistent training and inference behavior with minimal upscaling or downscaling. Vision and text features are aggregated using masked attention pooling (MAP) head (Zhai et al., 2022). We implement three model variants (Base, Large, and So400M (Alabdulmohsin et al., 2023)) to evaluate performance across different scales. 3.2 TRAINING OBJECTIVES Our training proceeds in two stages. In Stage I, we optimize only the global alignment objective. In Stage II, we jointly optimize five complementary objectives: = λ1LGlobal + λ2LFGV + λ3LFGT + λ4LCMR + λ5LTIC, with fixed weights λ1 = 1.0, λ2 = 0.1, λ3 = 0.5, λ4 = 0.4, λ5 = 0.1, chosen to ensure stable and effective multi-objective optimization. The following paragraphs detail each component. (1) Global Alignment Learning. We adopt the sigmoid loss from SigLIP (Zhai et al., 2023), which treats image-text matching as binary classification. For each image-text pair, similarity scores are computed across all pairs in the batch, and logistic regression is applied to distinguish positive from negative pairs. To enrich textual supervision, we include both original short captions and long captions generated by Large Multimodal Models (LMMs). This dual-caption strategy provides complementary signals: concise labels for global semantics and detailed descriptions for richer context. Fine-Grained Visual Learning. To enable sub-image alignment, we generate dense image embeddings by introducing an additional self-attention module, circumventing the information bottleneck caused by CLS-only pooling in SigLIP 2. For each annotated region, we extract patch-level embeddings and apply RoIAlign (He et al., 2017) to obtain region-specific features. Corresponding text embeddings are derived from phrase-level descriptions aligned to each bounding box. The regional contrastive loss encourages alignment between matched region-text pairs, promoting fine-grained cross-modal understanding. Fine-Grained Textual Learning. Following FG-CLIP (Xie et al., 2025), we leverage hard negatives from the FineHARD dataset. Each positive region-text pair is paired with 10 semantically similar negatives, constructed by perturbing key attributes (e.g., color, count, action) while preserving syntactic structure. The model is trained with binary classification loss over the 1 positive and 10 negatives, encouraging the model to assign higher scores to the matched pair. This objective enhances the models ability to discriminate subtle textual differences. Cross-modal Rank Loss with Global Threshold Synchronization. We adopt the Cross-modal Rank (CMR) loss (Zhang et al., 2024b) to enforce margin between positive and hard negative pairs, thereby strengthening the models discrimination of semantic boundaries. For positive pair (I, ) and hard negative Tk, the loss is defined as: LCMR = max (0, S(I, Tk) S(I, ) + τk) , (2) where S(, ) denotes cosine similarity. At training step t, the margin τk is synchronized globally across all GPUs via all-reduce, where Bglobal denotes the union of all local batches across GPUs: τ = 1 Bglobal (cid:88) (cid:0)St1(I, ) St1(I, Tk)(cid:1) , (3) (I,T )Bglobal with St1(, ) denoting similarities computed at the previous step. This design ensures stable and consistent thresholding in distributed training. Textual Intra-modal Contrastive Loss. While cross-modal alignment ensures image-text correspondence, the text encoder itself often lacks sufficient discriminative pressure to separate semantically similar but distinct region descriptions, which is critical for fine-grained visual grounding. To address this, we propose the Textual Intra-modal Contrastive (TIC) loss, which operates purely within the text modality to sharpen the text encoders representation space. Given batch of region texts, we compute pairwise similarities and filter out pairs with sim > 0.95 to avoid over-penalization. The top-10 most similar texts per sample are selected as hard negatives. The TIC loss is then defined as: LTIC = (cid:88) i= log (cid:80) TmTi 1 exp(S(Ti, Tm)) , (4) where Ti denotes the set of filtered hard negatives for text Ti. This encourages the text encoder to assign lower similarity to hard negative pairs, thereby improving its ability to separate semantically close but distinct region descriptions. 3.3 TRAINING DATA In the first stage, we train on image-text pairs from diverse sources, with particular emphasis on enhancing semantic depth and linguistic coherence. For English, we adopt an enhanced version of the LAION-2B dataset (Xie et al., 2025), where we augment the original short captions with detailed long captions generated by LMMs. The original captions are often fragmented and contain keyword stacking or irrelevant noise, making them insufficient for training models to understand rich, compositional language. We retain original captions to preserve diversity of natural language expressions, while simultaneously training on their enhanced long-caption counterparts. This dualcaption strategy enables the model to learn from both concise, real-world descriptions and semantically dense, contextually coherent narratives. For Chinese, we combine three datasets: Wukong (100 million pairs) (Gu et al., 2022), Zero (250 million pairs) (Xie et al., 2023), and large-scale in-house dataset (500 million pairs). In the second stage, we extend training with fine-grained region-text pairs to further improve spatial grounding. For English, we use the FineHARD dataset (Xie et al., 2025), which includes 12 million images, 40 million bounding boxes with fine-grained region descriptions, and 10 million hard negative samples. For Chinese, we use an in-house dataset containing 12 million images. An overview of our training datasets is provided in Table in Appendix. 3.4 BILINGUAL EVALUATION PROTOCOLS Existing multimodal benchmarks in English are diverse and well-established, covering broad spectrum of vision-language tasks such as fine-grained understanding (e.g., FG-OVD (Bianchi et al., Table 1: Performance comparison on fine-grained understanding and bounding box classification tasks using Top-1 accuracy. Method Backbone Fine-Grained Understanding BBox Classification Hard Medium Easy Trivial COCO80 LVIS1203 BoxClass-CN566 CLIP EVA-CLIP Long-CLIP FineCLIP SigLIP 2 FG-CLIP FG-CLIP 2 CLIP EVA-CLIP Long-CLIP FineCLIP SigLIP 2 FG-CLIP FG-CLIP ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/16 ViT-L/14 ViT-L/16 Meta CLIP 2 SigLIP 2 FG-CLIP 2 ViT-H/14 ViT-So/16 ViT-So/16 12.0 14.0 9.2 26.8 24.9 46.1 52.3 15.4 18.3 9.6 22.8 24.1 48.4 55. 16.5 26.0 54.0 23.1 30.1 18.4 49.8 46.5 66.6 76.3 25.3 38.4 19.7 46.0 47.1 69.5 77.5 36.6 48.7 77.4 22.2 29.4 16.2 50.4 48.7 68.7 80.3 25.7 35.2 16.0 46.0 47.4 71.2 83. 34.7 49.9 79.8 58.5 58.3 51.8 71.9 85.0 83.4 92.0 38.8 62.7 39.8 73.6 84.1 89.7 92.5 79.6 87.4 93.5 44.2 30.6 36.7 48.4 53.4 52.3 74.9 33.8 32.1 35.6 54.5 54.7 63.2 74. 52.0 62.0 77.4 20.9 14.4 18.2 23.3 20.6 28.6 47.3 9.3 18.3 10.4 22.5 25.9 38.3 41.9 24.4 31.4 43.9 - - - - 57.9 - 60.7 - - - - 56.6 - 68. 55.2 63.6 66.5 2024)), open-vocabulary object detection (e.g., LVIS (Gupta et al., 2019), COCO (Lin et al., 2014)), and image-text retrieval (e.g., Flickr30K (Young et al., 2014), DCIUrbanek et al. (2024)). These resources enable comprehensive evaluation of model capabilities across different granularities and semantic complexities. In contrast, Chinese multimodal datasets remain limited in scope and diversity, with most focusing on short-caption retrieval tasks such as COCO-CN (Li et al., 2019) and Flickr30kCNA (Xie et al., 2023). Such benchmarks are insufficient for evaluating fine-grained cross-modal alignment, particularly at the region level or with long, descriptive textual inputs. To address this gap, we introduce suite of Chinese evaluation benchmarks tailored for fine-grained vision-language tasks. We first construct three long-caption image-text retrieval datasets: LIT-CN, DCI-CN, and DOCCI-CN. These datasets support the evaluation of cross-modal alignment with rich and descriptive textual inputs. We then present BoxClass-CN, region-based classification dataset designed to assess region-level vision-language alignment in Chinese. LIT-CN integrates diverse sources: 15,000 images from AI-Challenger Caption (Wu et al., 2019), 3,230 from MUGE (Lin et al., 2021), and 20,000 from curated web images. All images are uniformly re-captioned using Qwen2.5-VL-32B-Instruct-AWQ (Bai et al., 2025), prompted to generate rich, context-aware descriptions with an average length of 131 tokens. Images below 256256 resolution are filtered, resulting in 33,010 high-quality image-text pairs. DCI-CN is derived from the Densely Captioned Images (DCI) dataset (Urbanek et al., 2024), with English captions translated into Chinese using the same LMM. The translations are validated by native speakers to ensure linguistic fluency and alignment with the original semantics. Similarly, DOCCI-CN is constructed from the DOCCI (Onoe et al., 2024) dataset, following an identical translation and validation pipeline. BoxClass-CN is region classification dataset that evaluates the alignment between image regions and their corresponding Chinese textual descriptions. It complements existing Chinese benchmarks by providing region-level supervision and serves as an evaluation suite for assessing models fine-grained understanding of visual content. We construct this dataset through scalable automated pipeline based on the LAION-2B corpus (Schuhmann et al., 2022). We first sample 200,000 images and generate detailed captions using LMM (Hong et al., 2024). These captions are parsed to extract referring expressions, which are then localized using pretrained object detector (Cheng et al., 2024) to produce bounding box proposals. Non-maximum suppression removes overlapping boxes, and only those with region-text similarity above 0.15 (computed by FG-CLIP (Xie et al., 2025)) are retained. Detected categories undergo semantic clustering and merging, resulting in 566 semantically refined categories. These categories are translated into Chinese, and the final dataset consists of 24,221 images and 66,258 high-quality region-text pairs. Together, these datasets provide more rigorous and comprehensive assessment for bilingual visionlanguage alignment models, supporting deeper evaluation of fine-grained understanding capability."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "The first stage is conducted on 160ASCEND 910B NPUs, and the second stage uses 16NVIDIA H800 GPUs. We use three vision encoder configurations: ViT-B/16, ViT-L/16, and ViT-So/16, initialized with SigLIP 2 (Tschannen et al., 2025) pre-trained weights. We employ the AdamW optimizer with learning rate of 1 106 and weight decay coefficient of 0.001. The momentum parameters β1 and β2 are set to 0.9 and 0.98, respectively. learning rate warmup strategy is applied during the first 300 iterations for stability. To accelerate training, we employ Zero-2 (Rajbhandari et al., 2020), CUDA TF32 precision, FlashAttention (Dao, 2023), and BFloat16 mixed-precision training. Batch sizes are set based on model size and training stage. In the first stage, the global batch sizes are 61,440 for ViT-B, 30,720 for ViT-L, and 18,432 for ViT-So. In the second stage, they are reduced to 4,096, 3,072, and 2,560, respectively. All models are trained for one epoch per stage. 4.2 LOCALIZATION TASKS 4.2.1 FINE-GRAINED UNDERSTANDING We evaluate open-source image-text alignment models on FG-OVD (Bianchi et al., 2024), finegrained benchmark emphasizing grounding in specific local image regions. Each region is paired with one positive description and ten synthetically perturbed negatives, forming challenging discrimination task. The benchmark comprises four subsets: trivial, easy, medium, and hard, arranged by increasing linguistic subtlety between correct and distractor texts, requiring finer-grained reasoning for accurate matching. Following FineCLIP (Jing et al., 2024), we extract dense visual features and use ROIAlign with provided bounding boxes to obtain region-specific representations. Similarity scores are computed between region features and textual descriptions, with Top-1 accuracy used as the evaluation metric. Results on the left side of Table 1 show that FG-CLIP 2 achieves significant gains over prior models. This demonstrates its superior capability in distinguishing subtle visual-linguistic correspondences, key requirement for fine-grained understanding. 4.2.2 BOUNDING BOX CLASSIFICATION We evaluate zero-shot bounding box classification on COCO-val2017 (Lin et al., 2014), LVIS (Gupta et al., 2019), and our proposed BoxClass-CN dataset, following the protocol of (Xie et al., 2025). While the former focus on English category recognition within localized regions, BoxClass-CN targets Chinese, enabling bilingual assessment of fine-grained vision-language alignment. As shown in the right part of Table 1, FG-CLIP 2 achieves state-of-the-art performance in both languages and significantly outperforms all compared open-source models. These results demonstrate its robust ability to align local visual content with semantic concepts across linguistic boundaries. 4.2.3 OPEN-VOCABULARY OBJECT DETECTION To assess the impact of vision-language alignment models on open-vocabulary detection (OVD), we adopt training-free evaluation protocol that avoids biases from detector fine-tuning. Unlike prior approaches (Wu et al., 2024) that require small-scale detector training on fixed categories, our method directly leverages the zero-shot generalization of alignment models to calibrate confidence scores and category predictions in the final OVD output. This allows for cleaner analysis of the contribution of image-text alignment to OVD performance. We adopt fusion strategy that leverages the visionlanguage alignment model to refine the class predictions and confidence scores of pre-trained detector. Specifically, similarity scores from the alignment model are combined with the detectors original confidences via geometric averaging, enabling more semantically accurate and calibrated open-vocabulary detection. We use LLMDet (Fu et al., 2025) as the base detector and evaluate on the challenging LVIS dataset (Gupta et al., 2019), which contains 1,203 categories. Categories are split into frequent, common, and rare based on occurrence frequency, and AP is reported per group. As shown in Table 2, FG-CLIP 2 combined with LLMDet achieves the best performance among Table 2: Open-vocabulary object detection results on LVISminival and LVIS. AP is reported across all categories and frequency splits. Method YOLO-World-L (Cheng et al., 2024) OWL-ST (Ye et al., 2023) DetCLIP v3 (Yao et al., 2024) T-Rex2 (Jiang et al., 2024) OV-DINO (Wang et al., 2024) LLMDet GLIP (Li et al., 2022) Grounding-DINO (Liu et al., 2023b) OWL-ST (Ye et al., 2023) MM-GDINO (Zhao et al., 2024) LLMDet Backbone YOLOv8-L ViT-B/16 Swin-T Swin-T Swin-T Swin-T Swin-L Swin-L ViT-L/14 Swin-L Swin-L LLMDet + FG-CLIP LLMDet + SigLIP 2 LLMDet + FG-CLIP 2 LLMDet + FG-CLIP LLMDet + SigLIP 2 LLMDet + FG-CLIP 2 LLMDet + SigLIP 2 LLMDet + Meta CLIP 2 LLMDet + FG-CLIP Swin-T + ViT-B/16 Swin-T + ViT-B/16 Swin-T + ViT-B/16 Swin-T + ViT-L/14 Swin-T + ViT-L/16 Swin-T + ViT-L/16 Swin-T + ViT-So/16 Swin-T + ViT-H/14 Swin-T + ViT-So/16 LVISminival APc APr 24.4 38.3 45.1 37.4 34.5 37.3 28.2 22.2 41.5 28.1 45. 40.6 42.4 47.5 41.9 46.9 48.6 48.4 50.5 50.8 34.0 47.7 39.7 39.5 39.5 34.3 30.7 31.8 46.1 47.7 45.6 50. 49.3 48.9 51.8 49.0 51.1 52.3 AP 35.5 34.4 47.0 42.8 40.1 44.7 37.3 33.9 40.9 36.8 51.1 48.0 47.9 51. 50.5 49.9 52.6 50.2 52.2 53.1 LVIS APf 38.8 46.7 46.5 41.5 50.7 41.5 38.8 42.8 56. 51.4 51.0 53.2 53.1 51.3 54.0 51.7 53.5 54.2 AP 28.7 28.6 38.9 34.8 32.9 34.9 26.9 35.2 29.1 42. 41.0 41.8 44.0 43.1 43.6 45.5 44.3 44.5 45.9 APr 22.9 30.3 37.2 29.0 29.1 26.0 17.1 36.2 19.7 31. 35.1 36.1 37.4 37.0 40.4 41.0 41.1 41.7 42.1 APc 24.9 37.5 31.5 30.4 30.1 23.3 25.6 38. 38.9 39.9 42.8 41.3 42.0 44.2 42.7 42.9 44.6 APf 35.4 41.2 41.2 37.4 44.3 36.4 37.2 50. 45.8 45.2 48.2 47.7 45.9 49.0 46.4 47.6 49.0 Table 3: Comparisons on English image-level tasks, including long/short caption image-text retrieval, and zero-shot image classification. Method Backbone ShareGPT4V TI IT DCI MSCOCO Flickr30k IT TI IT TI IT TI IN-1K IN-v2 Top-1 Top-1 CLIP EVA-CLIP Long-CLIP FineCLIP FG-CLIP SigLIP 2 FG-CLIP CLIP EVA-CLIP Long-CLIP FineCLIP FG-CLIP SigLIP 2 FG-CLIP 2 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/16 ViT-L/16 SigLIP 2 Meta CLIP 2 FG-CLIP 2 ViT-So/16 ViT-H/14 ViT-So/16 78.2 90.5 94.7 70.6 96.7 66.0 95. 86.5 91.5 95.8 73.4 97.4 84.4 96.9 78.6 93.9 97.5 79.6 85.5 93.4 73.3 94.9 67.9 95.4 83.6 89.4 95.6 82.7 96.8 83.0 96.6 79.5 89.2 96.7 45.5 41.9 51.7 35.5 61.8 32.3 64. 37.2 47.2 44.2 40.1 66.7 46.7 70.0 46.0 53.0 70.6 43.0 41.2 57.3 34.4 60.6 34.2 64.9 36.4 47.8 52.5 46.2 66.1 47.1 71.6 47.1 50.2 72.1 51.8 58.7 57.6 54.5 64.1 71.2 72. 58.0 64.2 62.8 - 68.9 71.6 75.1 71.3 66.8 74.6 32.7 41.6 40.4 40.2 45.4 55.2 54.5 37.1 47.9 46.3 - 50.9 53.4 58.6 56.0 47.7 56.7 82.2 85.7 85.9 82.5 90.7 92.6 94. 87.4 89.2 90.0 - 93.7 93.1 96.6 94.1 91.9 95.9 62.1 71.2 70.7 67.9 76.4 78.0 81.9 67.3 77.9 76.2 - 81.5 81.4 84.8 82.5 77.0 85.0 68.4 74.7 66.8 55.7 69.0 81.2 79. 76.6 80.4 73.5 60.8 76.1 82.8 83.1 84.3 81.7 84.1 61.9 67.0 61.2 48.8 61.8 74.5 72.2 70.9 73.8 67.9 53.4 69.0 75.2 77.4 79.1 75.7 77.8 open-source methods, demonstrating strong practical utility and validating its superior generalization in detection scenarios. 4.3 IMAGE-LEVEL TASKS 4.3.1 LONG/SHORT CAPTION IMAGE-TEXT RETRIEVAL To comprehensively evaluate image-text alignment under varying linguistic complexity, we conduct experiments on both short and long caption retrieval tasks. Short-text retrieval (Flickr30k (Young et al., 2014), MSCOCO (Lin et al., 2014)) assesses basic semantic matching, while long-text retrieval requires fine-grained understanding of detailed descriptions and complex visual scenes. For long-text retrieval in English, we use the 1K subset of ShareGPT4V (Chen et al., 2024b) and the full test set of DCI (Urbanek et al., 2024) following the protocol of Long-CLIP (Zhang et al., 2024a). For Chinese, Table 4: Performance on Chinese image-text retrieval benchmarks, covering both long-text and short-text settings. Results are reported in terms of Recall@1 (%). Method Backbone LIT-CN DCI-CN IT TI IT TI DOCCI-CN TI IT Flickr-CNA TI IT COCO-CN TI IT R2D2 Chinese-CLIP SigLIP 2 FG-CLIP 2 R2D2 Chinese-CLIP SigLIP 2 FG-CLIP SigLIP 2 Meta CLIP 2 FG-CLIP 2 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-L/14 ViT-L/14 ViT-L/16 ViT-L/16 ViT-So/16 ViT-H/14 ViT-So/16 35.7 45.7 4.5 82.4 48.3 48.6 16.0 86. 16.3 77.2 87.6 27.4 35.6 3.2 81.1 33.3 38.9 13.6 85.9 11.2 67.6 86.3 25.9 30.1 5.0 53.9 35.6 31.4 13.9 60. 13.4 53.8 62.7 27.3 27.9 3.9 55.7 34.2 32.7 13.4 62.2 12.0 52.1 65.1 36.1 44.6 7.6 71.2 49.5 49.7 25.1 77. 25.0 73.8 79.7 36.9 43.1 5.7 75.4 46.3 50.8 24.2 81.9 21.3 77.2 84.0 69.7 75.8 71.7 85.4 78.8 82.9 76.6 90. 78.4 89.3 91.5 51.1 62.4 49.1 69.9 60.0 69.6 51.2 75.0 51.7 72.2 77.2 60.1 68.8 68.5 77.2 69.6 74.3 71.6 82. 72.0 80.1 83.2 45.5 54.9 46.2 62.9 52.7 59.9 51.3 66.5 50.7 63.1 68.1 Table 5: Performance on dense prediction tasks. Backbone A-847 PC-459 APC-59 VOC-20 VOC-21 Model CLIP CLIPSelf FineCLIP FG-CLIP SigLIP 2 FG-CLIP 2 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-L/14 CLIP ViT-L/14 CLIPSelf ViT-L/14 FineCLIP ViT-L/14 FG-CLIP ViT-L/16 SigLIP 2 FG-CLIP 2 ViT-L/16 FG-CLIP 2 ViT-So/16 8.4 10.1 12.2 12.3 10.4 16. 10.8 13.6 14.1 14.6 14.3 18.8 20.0 16.6 - - 19.1 17.0 24.0 20.4 - - 23.3 24.1 26.6 27.5 27.2 29.7 32.4 33.4 28.5 38.5 31.5 34.9 36.1 36.9 38.8 41.2 42.2 57.5 55.3 56.0 58.2 55.4 61. 62.0 59.1 59.9 61.4 62.4 62.4 63.3 93.7 - - 95.3 94.4 97.1 96.6 - - 97.4 97.0 97.6 97.8 78.3 - - 77.9 75.8 81.1 81.8 - - 81.8 82.3 81.8 83.2 we evaluate on LIT-CN, DCI-CN, and DOCCI-CN for long-text, and Flickr-CNA (Xie et al., 2023) and COCO-CN (Li et al., 2019) for short-text. These datasets cover diverse content and caption styles, providing robust evaluation of multilingual performance. As shown in Table 3 and Table 4, FG-CLIP 2 achieves consistent improvements across all settings, with particularly strong gains on long-text retrieval, highlighting its superior capability in fine-grained vision-language alignment. Notably, FG-CLIP 2 outperforms Meta CLIP 2 (Chuang et al., 2025), the current multilingual SOTA, on both language settings, despite using smaller ViT-L/16 backbone with 1.0 billion parameters compared to Meta CLIP 2s 1.8 billion parameter ViT-H/14. This highlights the effectiveness of our training paradigm in achieving stronger performance with reduced model scale. 4.3.2 ZERO-SHOT IMAGE CLASSIFICATION We evaluate zero-shot image classification on ImageNet-1K (Deng et al., 2009) and ImageNetv2 (Recht et al., 2019) using standard prompts (Radford et al., 2021). As shown in Table 3, FG-CLIP 2 achieves competitive performance compared to SigLIP 2 (Tschannen et al., 2025), and outperforms EVA-CLIP (Sun et al., 2023), Long-CLIP (Zhang et al., 2024a), and FineCLIP (Jing et al., 2024). This confirms that the improvements in fine-grained alignment do not come at the cost of standard recognition accuracy, demonstrating well-balanced representation capability. 4.4 DENSE PREDICTION TASKS We evaluate dense prediction through open-vocabulary segmentation, task that requires models to segment object categories beyond fixed training set. We adopt Cat-Seg (Cho et al., 2024) as the base framework, trained on COCO-Stuff-164k (172 categories), and evaluate on datasets with diverse category schemas: ADE20k (847 or 150 categories, denoted A-847/A-150), Pascal Context (PC-459/PC-59), and Pascal VOC (VOC-20/VOC-21). As shown in Table 5, FG-CLIP 2 achieves Table 6: Comparisons on large multimodal model benchmarks. Method GQA MMMU TextVQA LLaVA-1.5 + CLIP LLaVA-1.5 + SigLIP 2 LLaVA-1.5 + Meta CLIP 2 LLaVA-1.5 + FG-CLIP 2 61.9 62.0 62.4 64.0 35.7 37.0 37.0 38.1 58.2 56.6 59.4 62.0 RefCOCO val 76.2 79.5 76.7 84.9 testA testB 67.9 83.4 73.9 84.4 69.8 82.8 79.5 89.8 MMBench-EN MMBench-CN dev 65.1 66.2 66.5 67.6 test 58.4 57.2 60.3 61.4 dev 58.2 58.0 59.3 60.5 test 66.5 64.8 67.0 66. Table 7: Ablation study results for FG-CLIP 2. Method FG-CLIP 2 w/o LCMR w/o LTIC w/o LCMR, LTIC Flickr30k T2I I2T ImageNet-V2 TopCOCO80 FG-OVD Top-1 Top-5 Hard Medium Easy Trivial 94. 93.3 93.7 93.5 81.9 81.9 81.8 81.6 72.2 72.1 72.1 72.0 74. 74.0 70.1 62.7 95.7 94.9 94.8 93.4 52.3 50.9 51.6 51.7 76. 75.1 75.1 76.0 80.3 77.8 79.1 77.6 92.0 93.5 92.1 90.7 the best performance across models of various scales, delivering consistent improvements over the baseline. This demonstrates its strong capability in enabling pixel-level generalization, crucial for downstream dense understanding tasks. 4.5 LARGE MULTIMODAL MODEL TASKS We empoly FG-CLIP 2 as vision encoder in large multimodal models (LMMs) to assess its compatibility and utility in advanced multimodal reasoning. We integrate FG-CLIP 2 into standard LLaVA-style LMM architecture, following the pre-training and supervised fine-tuning protocol of LLaVA-1.5 (Liu et al., 2023a). Results in Table 6 show that LMMs equipped with FG-CLIP 2 outperform those using other open-source encoders, including SigLIP 2 and Meta CLIP 2. This indicates that the fine-grained and bilingual capabilities of FG-CLIP 2 effectively transfer to higherlevel multimodal tasks, making it strong candidate for integration into next-generation LMMs. 4.6 ABLATION STUDY As our training framework leverages global-level, region-level, and hard-negative data through complementary learning mechanisms, we adopt baseline model that incorporates global alignment alongside fine-grained visual and textual learning. This allows us to specifically evaluate the impact of the Cross-modal Rank Loss with Global Threshold Synchronization (LCMR) and the Textual Intra-modal Contrastive Loss (LTIC). As shown in Table 7, removing LTIC leads to 4.8-point drop in COCO Top-1 accuracy (to 70.1%) and decrease in FG-OVD Hard performance from 52.3% to 51.6%, confirming its critical role in distinguishing semantically similar texts. Removing LCMR also degrades performance, with FG-OVD Hard falling to 50.9%, indicating its importance in cross-modal alignment. When both losses are removed, COCO Top-1 drops sharply to 62.7%, demonstrating their complementary benefits. The full model achieves consistent gains across all tasks, validating the effectiveness of our proposed training objectives."
        },
        {
            "title": "5 CONCLUSION AND LIMITATIONS",
            "content": "In this work, we present FG-CLIP 2, bilingual vision-language model that advances fine-grained understanding for both English and Chinese. Our two-stage training paradigm progressively refines alignment by leveraging both short and long captions, region-text supervision, and multiple discriminative objectives, including the proposed Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar descriptions. Trained on large-scale, high-quality English and Chinese datasets, FG-CLIP 2 achieves superior performance across 29 datasets and 8 tasks, demonstrating strong bilingual generalization. To advance evaluation in non-English settings, we introduce new benchmark for Chinese multimodal understanding with challenging tasks such as long caption image-text retrieval and bounding box classification. We release the model, code, and benchmark to support future research in bilingual fine-grained vision-language understanding. In future work, we will focus on extending the model to handle longer textual inputs and explicitly model relational structures among objects, enabling richer fine-grained multimodal understanding."
        },
        {
            "title": "REFERENCES",
            "content": "Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. Getting vit in shape: Scaling laws for compute-optimal model design. Advances in Neural Information Processing Systems, 36:1640616425, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Lorenzo Bianchi, Fabio Carrara, Nicola Messina, Claudio Gennaro, and Fabrizio Falchi. The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding. In CVPR, pp. 2252022529, 2024. Hong-You Chen, Zhengfeng Lai, Haotian Zhang, Xinze Wang, Marcin Eichner, Keen You, Meng Cao, Bowen Zhang, Yinfei Yang, and Zhe Gan. Contrastive localized language-image pre-training. arXiv preprint arXiv:2410.02746, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, pp. 370387, 2024b. Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In CVPR, pp. 1690116911, 2024. Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim. Cat-seg: Cost aggregation for open-vocabulary semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 41134123, 2024. Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, et al. Metaclip 2: worldwide scaling recipe. arXiv preprint arXiv:2507.22062, 2025. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pp. 248255, 2009. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. In ICLR, 2024. Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, and Wei-Shi Zheng. Llmdet: Learning strong open-vocabulary object detectors under the supervision of large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1498714997, 2025. Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems, 35:2641826431, 2022. Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In CVPR, pp. 53565364, 2019. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, pp. 29612969, 2017. Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, and Lei Zhang. T-rex2: Towards generic object detection via text-visual prompt synergy. In European Conference on Computer Vision, pp. 3857. Springer, 2024. Dong Jing, Xiaolong He, Yutian Luo, Nanyi Fei, Guoxing Yang, Wei Wei, Huiwen Zhao, and Zhiwu Lu. Fineclip: Self-distilled region-based clip for better fine-grained understanding. In NeurIPS, 2024. Jiaqi Li, Junshu Tang, Zhiyong Xu, Longhuang Wu, Yuan Zhou, Shuai Shao, Tianbao Yu, Zhiguo Cao, and Qinglin Lu. Hunyuan-gamecraft: High-dynamic interactive game video generation with hybrid history condition, 2025a. URL https://arxiv.org/abs/2506.17201. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In CVPR, pp. 1096510975, 2022. Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jieping Xu. Cococn for cross-lingual image tagging, captioning, and retrieval. IEEE Transactions on Multimedia, 21(9):23472360, 2019. Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, et al. Eagle 2: Building post-training data strategies from scratch for frontier vision-language models. arXiv preprint arXiv:2501.14818, 2025b. Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, et al. M6: chinese multimodal pretrainer. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 32513261, 2021. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pp. 740755, 2014. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023a. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023b. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, Yuxuan Han, Haijun Li, Wanying Chen, Junke Tang, Chengkun Hou, Zhixing Du, , et al. Ovis2.5 technical report. arXiv:2508.11737, 2025. Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Ghosh, Arjun Karpur, Koert Chen, Ye Xia, Bingyi Cao, Daniel Salz, Guangxing Han, Jan Dlabal, et al. Tips: Text-image pretraining with spatial awareness. In ICLR, 2025. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, et al. Docci: Descriptions of connected and contrasting images. In ECCV, pp. 291309, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763, 2021. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, pp. 53895400, 2019. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-clip: clip model focusing on wherever you want. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1301913029, 2024. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025a. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, et al. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025b. URL https://arxiv.org/abs/2507.01006. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana RomeroSoriano. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In CVPR, pp. 2670026709, 2024. Bin Wang, Chunyu Xie, Dawei Leng, and Yuhui Yin. Iaa: Inner-adaptor architecture empowers frozen large language model with multimodal capabilities. In AAAI, volume 39, pp. 2103521043, 2025a. Hao Wang, Pengzhen Ren, Zequn Jie, Xiao Dong, Chengjian Feng, Yinlong Qian, Lin Ma, Dongmei Jiang, Yaowei Wang, Xiangyuan Lan, et al. Ov-dino: Unified open-vocabulary detection with language-aware selective fusion. arXiv preprint arXiv:2407.07844, 2024. Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, and Xiaodan Liang. X-sam: From segment anything to any segmentation. arXiv preprint arXiv:2508.04655, 2025b. Dafeng Wei, Tian Gao, Zhengyu Jia, Changwei Cai, Chengkai Hou, Peng Jia, Fu Liu, Kun Zhan, Jingchen Fan, Yixing Zhao, et al. Bev-clip: Multi-modal bev retrieval methodology for complex scene in autonomous driving. arXiv preprint arXiv:2401.01065, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. URL https://arxiv.org/abs/2508. 02324. Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang, Wenjia Wang, Shipei Zhou, Guosen Lin, Yanwei Fu, et al. Ai challenger: large-scale dataset for going deeper in image understanding. In IEEE International Conference on Multimedia and Expo, 2019. Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, and Chen Change Loy. CLIPSelf: Vision transformer distills itself for open-vocabulary dense prediction. In ICLR, 2024. URL https://openreview.net/forum?id=DjzvJCRsVf. Chunyu Xie, Heng Cai, Jincheng Li, Fanjing Kong, Xiaoyu Wu, Jianfei Song, Henrique Morimitsu, Lin Yao, Dexin Wang, Xiangzheng Zhang, et al. Ccmb: large-scale chinese cross-modal benchmark. In ACM MM, pp. 42194227, 2023. Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei Leng, and Yuhui Yin. Fg-clip: Fine-grained visual and textual alignment. In ICML, 2025. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In ICLR, 2024. An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. Chinese clip: Contrastive vision-language pretraining in chinese. arXiv preprint arXiv:2211.01335, 2022. Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, and Dan Xu. Detclipv3: Towards versatile generative open-vocabulary object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2739127401, 2024. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023. arXiv preprint arXiv:2304.14178. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1210412113, 2022. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In ECCV, pp. 310325, 2024a. Le Zhang, Rabiul Awal, and Aishwarya Agrawal. Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic compositional understanding. In CVPR, pp. 13774 13784, 2024b. Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, and Haian Huang. An open and comprehensive pipeline for unified object grounding and detection. arXiv preprint arXiv:2401.02361, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A TRAINING DATA DETAILS",
            "content": "Table summarizes the datasets used in our two-stage training pipeline. Table A: Overview of the training datasets used in two stages. Data Type Dataset Language Size Image-text LAION-2B-enhanced Wukong Zero In-house Data Region-text FineHARD In-house Data English Chinese Chinese Chinese English Chinese 1.6B 100M 250M 500M 12M 12M OPEN-VOCABULARY OBJECT DETECTION EXPERIMENTAL DETAILS In our experiments, we adopt LLMDet as the base open-vocabulary object detection model. Its output consists of bounding boxes, each associated with predicted category and confidence score. To improve category accuracy, we recalibrate these predictions using vision-language alignment models, such as FG-CLIP 2, without modifying LLMDets parameters. For each detected bounding box, we extract its visual feature by applying RoI-Align on the dense ViT feature map produced by FG-CLIP 2. We also encode all candidate category names into text embeddings using the same model. We then compute the cosine similarity between the regions visual feature and each categorys text embedding, followed by Softmax normalization to obtain categorywise alignment similarity distribution. To produce the final prediction, we combine LLMDets original confidence score with FG-CLIP 2s normalized similarity score through weighted multiplicative fusion. The resulting fused score reflects both the detectors localization confidence and the alignment models semantic relevance. We then select the category with the highest fused score as the final predicted class, and assign the corresponding fused value as the final confidence output. This approach leverages FG-CLIP 2s fine-grained understanding to recalibrate LLMDets predictions across the entire category space. It ensures that categories with strong semantic alignment but initially low detection scores can still be correctly selected if their fused confidence is highest. This global recalibration significantly improves performance on novel categories, while maintaining compatibility with the original detectors structure. Table B: Detailed parameters for training with Cat-Seg. Parameter name Value Parameter name Text Guidance Proj Dim Appearance Guidance Proj Dim Decoder Dims Decoder Guidance Dims Decoder Guidance Proj Dims Num Layers Num Heads Hidden Dims Pooling Sizes Feature Resolution Window Sizes Attention Type 128 128 [64, 32] [256, 128] [32, 16] 2 4 128 [2, 2] [24, 24] 12 Linear Min Size Train Min Size Train Sampling Min Size Test Size Divisibility Format Value 384 Choice 640 384 RGB Dataset Mapper Name Mask Former Semantic Images Per Batch LR Scheduler Name Base Learning Rate Max Iterations Backbone Multiplier CLIP Multiplier 8 Warmup Cosine LR 0.0002 80,000 0.0 0."
        },
        {
            "title": "C DENSE PREDICTION TASKS EXPERIMENTAL DETAILS",
            "content": "We adopt Cat-Seg (Cho et al., 2024) as the base model for open-vocabulary segmentation, which supports plug-and-play integration of various image-text models. unified training configuration is used across different ViT backbones, with detailed hyperparameters provided in Table B."
        },
        {
            "title": "VISUAL FEATURES",
            "content": "We present the alignment capability of FG-CLIP 2 between dense visual features and text in both Chinese and English contexts. The results are shown in Figure A, where warmer colors indicate higher similarity between image regions and the matched text. Compared to the previous version, FG-CLIP 2 supports denser visual feature output and achieves strong bilingual semantic alignment and fine-grained perception capabilities. EXAMPLES OF CHINESE LONG CAPTION IMAGE-TEXT PAIRS In Table C, we provide examples of long caption image-text pairs from the LIT-CN dataset, covering diverse scene categories such as indoor, outdoor, animals, products, and buildings. These captions not only describe fine-grained subject attributes (e.g., appearance, posture, spatial layout), but also detail the surrounding context, reflecting the datasets semantic richness and descriptive complexity. BOXCLASS-CN CATEGORY SCHEMA AND EXAMPLE EXPLANATION Table D.1 and D.2 provide the complete list of categories in the BoxClass-CN dataset, separated by commas. We further present some examples from BoxClass-CN in Figure B. Figure A: Visualization of FG-CLIP 2s dense feature maps and semantic alignment capability in bilingual scenarios. Figure B: Examples from BoxClass-CN. Table C: Examples from LIT-CN. Table D.1: All categories in BoxClass-CN, grouped and displayed by ID (1-300). id:1-100 id:101-200 id:201-300 Table D.2: All categories in BoxClass-CN, grouped and displayed by ID (301-566). id:301id:401-500 id:501-"
        }
    ],
    "affiliations": [
        "360.cn"
    ]
}