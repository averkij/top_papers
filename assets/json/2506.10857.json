{
    "paper_title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "authors": [
        "Jiashuo Yu",
        "Yue Wu",
        "Meng Chu",
        "Zhifei Ren",
        "Zizheng Huang",
        "Pei Chu",
        "Ruijie Zhang",
        "Yinan He",
        "Qirui Li",
        "Songze Li",
        "Zhenxiang Li",
        "Zhongying Tu",
        "Conghui He",
        "Yu Qiao",
        "Yali Wang",
        "Yi Wang",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 1,010 long videos (with an average duration of 1.6 hours), along with 9,468 human-labeled multi-step question-answering pairs and 30,292 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning."
        },
        {
            "title": "Start",
            "content": "VRBench: Benchmark for Multi-Step Reasoning in Long Narrative Videos Jiashuo Yu1 Yue Wu1 Meng Chu1 Ruijie Zhang1 Yinan He1 Qirui Li1 Zhifei Ren1 Songze Li1 Zizheng Huang2,1 Zhenxiang Li1 Pei Chu1 Zhongying Tu1 Conghui He1 Yu Qiao1 Yali Wang3, 1(cid:66) Yi Wang1(cid:66) Limin Wang2, 1(cid:66) 5 2 0 2 2 1 ] . [ 1 7 5 8 0 1 . 6 0 5 2 : r 1Shanghai Artificial Intelligence Laboratory 2Nanjing University 3Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences VRBench.github.io Figure 1. Overview of VRBench. We present VRBench, long narrative video benchmark for multi-step reasoning. VRBench includes 1,010 manual-filtered narrative videos, covering 8 languages and 7 video categories that are suitable for reasoning about temporal relations. We also provide high-quality stepwise annotations for reasoning, which are labeled and reviewed by human experts. Each video incorporates 8-10 complex question-answer pairs, multi-step reasoning chain, and fine-grained timestamps. To fully evaluate the capability of models in multi-step reasoning, we propose multi-phase evaluation pipeline that assesses model results both from the processand outcome-level. Our VRBench is the first video reasoning benchmark that both supports multi-step annotation and evaluation."
        },
        {
            "title": "Abstract",
            "content": "We present VRBench, the first long narrative video benchmark crafted for evaluating large models multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 1,010 long videos (with an average duration of 1.6 hours), along with 9,468 human-labeled multi-step question-answering pairs and 30,292 reasoning steps with timestamps. These videos are curated via multistage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, equal contributions. (cid:66)corresponding authors. spanning seven types (e.g., event attribution, implicit inference). VRBench designs multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on VRBench, we undertake thorough analysis and provide valuable insights that advance the field of multi-step reasoning. 1. Introduction The rapid evolution of vision language models (VLMs) has heightened the need for benchmarks that rigorously eval1 uate complex reasoning capabilities. While existing standards like GSM8K [8] focus on domain-specific knowledge in mathematics and science, they neglect critical reasoning dimension: contextual analysis in visual narrative content. Real-world applications increasingly demand temporal reasoning across interconnected elements: tracking character dynamics in films, interpreting gameplay strategies, or understanding cause-and-effect chains in documentaries. This capability gap persists because current video benchmarks [16, 53, 67, 74] primarily assess single-step perception rather than sustained reasoning processes. We find three fundamental limitations in current evaluation paradigms: (1) an overemphasis on domain expertise rather than plot-driven reasoning, (2) the lack of temporally-grounded reasoning chains in annotations, and (3) Outcome-focused metrics that ignore procedural validity. To address these challenges, we present VRBench (as in Figure 1), the first benchmark specifically designed for multi-step reasoning in long-form narrative videos. Its construction involves the collection of narrative videos, human-in-the-loop reasoning process method for annotation, and multi-phase evaluation pipeline considering both procedure and outcome. In data construction, VRBench aggregates 1,010 meticulously selected videos (1.6h average duration) across seven travelogues), narrative categories (e.g., movies, sports, sourced through multi-stage filtering process. Our curation pipeline combines automated retrieval with expert validation (inter-rater reliability ρ=0.82), prioritizing plot coherence over domain-specific knowledge. This contrasts with existing benchmarks like MMVU [89] that emphasize disciplinary competence. To conduct multi-step reasoning chain annotation, we develop human-AI collaborative approach, generating 8-10 QA pairs per video with explicit temporal grounding. Each question requires no less than 2 reasoning steps annotated with precise timestamps (Figure 3), validated through iterative expert review (95% interannotator agreement). The taxonomy spans seven reasoning types from event prediction to implicit inference, ensuring comprehensive coverage of narrative analysis skills. When evaluating models on VRBench, we propose multi-phase evaluation pipeline that combines outcome verification with process analysis. Beyond conventional multiple-choice accuracy that evaluates the outcome-level performance, we further introduce the process-level metric LLM-guided scoring to evaluate the overall reasoning process quality. This dual approach reveals critical insights for instance, while GPT-4o achieves 83.25% outcome accuracy, its process rating scores drop to 58.1%, exposing reasoning fragility. Evaluations of 28 state-ofthe-art large models reveal proprietary VLMs with longcontext support outperform text-only LLMs by 12.2% absolute, emphasizing the importance of dense visual groundFigure 2. An example of annotation in VRBench. For each question, VRBench provides the question-answer pair, multi-step reasoning chain, question type, and the start-to-end timestamps of the entire question as well as each reasoning step. ing. System-2 optimization strategies yield disproportionate gains in process metrics, and despite parameter parity, open-source VLMs lag behind proprietary counterparts by 8.99%, suggesting architectural limitations beyond scale. Our ablation studies show strong alignment between human and LLM evaluations and quantify the impact of testtime scalingdoubling context windows improves QwQ32Bs [62] accuracy by 12.43%. VRBench sets new standard for evaluating narrative reasoning, offering insights distinct from knowledgecentric benchmarks. By separating domain expertise from contextual analysis, our work aids in developing models capable of sustained, human-like understanding in real-world video content. The entire suite, including annotation protocols and evaluation tools, has been fully open-sourced to advance reasoning research. 2. Related Works Reasoning Large Models. As the capabilities of LLMs continue to expand, frontier models have demonstrated significant potential in addressing high-order reasoning tasks. OpenAI pioneered this advancement with the release of o1 [48], an LLM capable of sophisticated reasoning tasks. Subsequently, series of proprietary [12, 49, 59] and opensource [3, 18, 62] LLMs designed for advanced reasoning have emerged. Furthermore, researchers have shifted their focus towards developing VLMs possessing similar higher-order reasoning capabilities. Previously, some VLMs [2, 7, 30, 32, 37, 68, 70, 85, 88] to certain extent exhibited good reasoning proficiency due to their proficiency in tackling long-sequence context. More recently, series of VLMs [14, 63, 66, 78] trained on data incorporating higher-order Chains-of-Thought in RL-based algorithms [33, 52, 55] have been developed, enabling them"
        },
        {
            "title": "Dataset",
            "content": "#Size #QA pairs #Dur.(s)"
        },
        {
            "title": "Anno Step Eval Step Eval Target Anno Type Clue Multilingual",
            "content": "MMLU [21] MMLU-Pro [69] LiveCodeBench [26] SciEval [57] GSM8k [8] C-Eval [23] ScienceQA [54] VisScience [27] MMMU [82] MMMU-Pro [83] MathVista [42] MathVision [64] CharXiv [71] OlympicArena [25] MVBench [31] EgoSchema [43] LongVideoBench [74] LVBench [67] CGBench [4] MLVU [90] VideoMME [16] Video-MMMU [22] MMWorld [20] MMVU [89] VRBench (Ours) 14,079 12,032 511 15,901 8,792 12,342 10,332 3,000 11,500 3,460 6,141 3,040 2,323 7, 3,641 5,063 3,763 103 1,219 1,730 900 300 1,910 1,529 1,010 14,079 12,032 511 15,901 8,792 12,342 21,208 3,000 11,500 3,460 6,141 3,040 11,615 11,163 4,000 5,063 6,678 1,549 12,129 3,102 2,700 900 6,627 3,000 / / / / / / / / / / / / / / MCQ MCQ MCQ MCQ+Open MCQ+Open MCQ MCQ MCQ+Open MCQ+Open MCQ MCQ+Open MCQ+Open Open MCQ+Open Multi-Disc Multi-Disc Code Science Math Multi-Disc Science Science Multi-Disc Multi-Disc Math Math Multi-Disc Multi-Disc Open-Domain 16.0 Egocentric 180.0 Open-Domain 473.0 4,101 Open-Domain 1624.4 MCQ+Open Open-Domain"
        },
        {
            "title": "MCQ\nMCQ\nMCQ\nMCQ",
            "content": "930 1017.9 506.2 108.0 51.4 MCQ MCQ MCQ MCQ MCQ+Open Narrative Open-Domain Multi-Disc Multi-Disc Multi-Disc 9,468 5,796.0 MCQ+Open"
        },
        {
            "title": "Multi",
            "content": "LLM, VLM A+M M A+M A+M M A+M M A+M Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Table 1. Comparison between VRBench and existing benchmarks. #Size is the number of text/images/videos, #Dur. means the average video duration, and indicate automatic and manual annotation type, respectively, and multi-disc denotes multi-disciplinary data source. Clue means clue-grounded annotation. Multilingual requires the number of data source languages to be greater than 2. to solve multidisciplinary, knowledge-intensive problems through combination of fast and slow thinking processes. uation. Table 1 further presents the detailed statistics of our dataset and distinguishes the difference between VRBench and existing text, image, and video reasoning benchmarks. Reasoning Benchmarks. With the rapid advancement of LLMs and VLMs in reasoning capability, numerous reasoning benchmarks have been developed in text and image-text modalities to facilitate comprehensive evaluation. For the textual domain, several knowledge-driven benchmarks [5, 15, 21, 23, 57, 69] have been derived from exam and textbook data sources, and comprise expert-level questions that require multi-step reasoning. Meanwhile, some multimodal benchmarks also emerge to introduce multi-discipline tasks based on charts [44, 71, 75], plots [45], exam [10, 25, 36, 82, 86], or expert-level questions printed on the static imIt is noted that even in the image ages [27, 35, 54, 83]. and text domains, benchmarks with process-level annotations [8, 84] are relatively scarce. For video understanding, evaluation also gradually shifts from short video clip perception [6, 9, 28, 29, 31, 39, 46, 50, 77] to long-form long video understanding [13, 16, 40, 43, 53, 56, 58, 67, 74, 87] and single-step reasoning. [4, 16, 19, 34, 73, 81]. More recently, some advances [13, 20, 89] propose several new video reasoning tasks in multi-disciplinary scenarios like healthcare, engineering, and science. Different from previous works, VRBench is the first narrative video-based benchmark that is purely used for multi-step reasoning eval3. Benchmark We present VRBench, comprehensive multi-step reasoning benchmark consisting of collection of long, multilingual, and narrative videos with corresponding questionanswer pairs. Figure 1 gives the overall pipeline of the benchmark construction, and we illustrate it as follows. 3.1. Video Curation We collect long narrative videos from YouTube, yielding an initial pool of over 10,000 public videos. Considering plausible reasoning in steps requires videos with rich and coherent plots, we source long-form footage using cherrypicked tag set and comprehensive criterion for narration. Plot-related Tags for Queries. We employ manually curated set of 7 semantic tags (e.g., Film & Animation, Sport, Travel & Events) to retrieve videos with strong narrative potential. This tag set is developed through iterative validation by domain experts, explicitly excluding nonnarrative categories such as news broadcasts and lecture recordings. Our analysis revealed that these excluded categories exhibit limited visual-semantic dynamics (e.g., static 3 Figure 3. Statistics of VRBench. We provide the detailed distribution of videos and annotations of VRBench, including video languages and durations, steps, types, and temporal duration of questions, as well as the token numbers of answers and the reasoning process. camera angles in talk formats) and minimal event progression, rendering them ineffective for benchmarking temporal reasoning capabilities. fications while maintaining multimodal reasoning fidelity. To further improve annotation quality, we also implement comprehensive quality assurance. Criterion for Narration. Our sourcing standards include video duration, its language source, and user ratings. The duration requirement (a minimum of 20 minutes and an average of 1.61 hours) ensures adequate temporal context for constructing reasoning chains. Our language diversity strategy intentionally excludes English and Chinese to counterbalance existing dataset biases, aligning with established findings that linguistic variety improves benchmark generalizability. Upon the aforementioned two quantitative prerequisites, we organize panel of 14 multilingual domain experts to evaluate candidate videos using standardized 10-point scale based on plot coherence and richness. The scoring is guided by several instructions. Videos scoring below 7 are systematically excluded, resulting in final curated set of 1,010 high-quality narratives. Detailed annotation protocols are documented in the supplementary materials. 3.2. Stepwise Reasoning Annotations We label videos with reasoning steps via two-stage human-in-the-loop framework. It first generates pseudo candidate QA pairs through automated pipelines, then employs expert-guided rewriting to curate high-quality annotations, ensuring rigorous adherence to benchmark speciAutomatic Pipelines. We first employ AutoShot [91] to cut videos into several segments, and then use VideoChat2 [31] to caption them. For auditory content, we adopt whisper-large-v3 [51] to obtain speech transcripts, and DeepL [11] to translate them into English. The video captions and translated subtitles are then put into GPT4o [47] to generate 6 pseudo QA pairs with multi-step reasoning process. To ensure the quality of reasoning, we specify 7 multi-step reasoning types for narrative videos as: Event prediction: Forecast subsequent events in the video timeline. Hypothetical reasoning: Deduce plausible scenario outcomes from stated premises. Event attribution: Determine causal origins or underlying motivations of video events. Implicit inference: Extract unstated temporal, emotional, or relational context from visual cues. Logical linkage: Establish event-mediated connections between visual/narrative elements. Information synopsis: Condense critical information across multimodal inputs. Counting problems: Quantify state changes through arithmetic/combinatorial analysis. 4 Expert-guided Rewriting. We recruit and train 67 graduate students to generate 8-10 high-quality QA pairs per video, providing raw video footage, translated subtitles, and GPT-generated pre-annotations that offer contextual hints without meeting final benchmark standards. Each QA pair must satisfy four rigorous criteria: (1) Non-synopsis questions require 2 timestamped reasoning steps (start/end times documented); (2) Temporal distribution constraints (4 questions from 0-15min, 3 from 15-40min, 1 from 40-120min); (3) Coverage of 5 predefined reasoning taxonomies (from 7 categories) ensuring diversity; (4) Mandatory multimodal grounding where solutions demand both visual analysis (excluding subtitle-only answers) and explicit reasoning (beyond basic perception). Comprehensive Quality Assurance. To ensure annotations meet our stringent multimodal standards, we implement rigorous verification protocol: 10 trained reviewers validate each annotation, with non-compliant entries returned to annotators for iterative revisions until fully compliant. Annotators and reviewers are exclusively recruited from top-tier universities to ensure academic rigor. We further enforce quality through dual safeguards: systematic 5% random sampling audit across both annotation and review stages, coupled with full documentation of protocols, annotation guidelines, and quality assessment criteria in supplementary materials. 3.3. Multi-Phase Evaluation Pipeline We benchmark VLMs and LLMs through comprehensive multi-phase evaluation pipeline, which compares predictions against ground-truth annotations both at the processlevel and outcome level. For the outcome-level evaluation stage, we adopt multiple-choice question (MCQ) format, where false options are generated by DeepSeek-V3 [38] using carefully-designed prompts based on human-annotated answers. For the process-level stage, we propose the openended rating to fully evaluate the models multi-step reasoning capability. Specifically, we adopt an LLM to evaluate the overall quality of the whole reasoning process through logical coherence (40%), similarity to four 0-10 scores: ground truth (40%, excluded for event prediction and hypothetical reasoning tasks), factual accuracy (10%), and clarity (10%). DeepSeek-V3 [38] serves as the judge due to its optimal balance between cost and human alignment (Section 4). For the event prediction and hypothetical reasoning tasks, we argue that there is no ground-truth reasoning steps since there might be multiple possible predictions, hence, we remove the similarity score and compute the final score with an 8:1:1 weight. All metrics are normalized to 0-100 scales, and we report the rankings by computing average scores across all metrics. Figure 4. Human preference alignment results. For each plot, we show the win ratios of three different tested LLMs evaluated by human experts and VRBench. We then fit them with straight line and quantify the correlation by calculating the Spearman correlation coefficient. LLM Evaluation Support. For text-only LLMs, we convert videos to text inputs using Qwen2.5-72B-Instruct [80] to synthesize video captions and subtitles into fine-grained summaries. The process involves: 1) Dividing inputs into 5-minute chunks for duplicate removal and abstraction; 2) Merging chunk-level abstracts into coherent summaries with detailed audio-visual descriptions. These summaries enable LLM evaluations while addressing context window limits for VLMs. 4. Experiments We test number of both open-source and proprietary VLMs (e.g. Qwen2-VL [65], InternVL2.5 [7], InternVideo2.5 [70], GPT-4o [47], Gemini-2.0-Pro [60], etc.) and LLMs (DeepSeek-R1 [18], QwQ [62], OpenAI o1 [48], Claude-3.7-Sonnet [59], etc.) with two-stage evaluation on VRBench. The configuration of each evaluated model is detailed in the supplementary material. Evaluation Protocols. Our protocol involves two phases. Given question, 1) models produce multi-step reasoning using Chain-of-Thought [72] prompts. Then 2) models choose final answers from multiple-choice options. For answer extraction, final answers are parsed from response endings to mitigate verbose option analysis, following [31]. While reasoning process evaluations use judge LLMs with structured prompts and example-guided rating extraction. All evaluation prompts and implementation details are included in the supplementary materials. 4.1. Main Results Table 2 shows the CoT evaluation results of LLMs and VLMs on VRBench. Among LLMs, we found the proprietary model Gemini-2.0-Flash-Thinking [12] achieves the optimal performance while other reasoning models such as OpenAI o1-preview [48] and DeepSeek-R1 [18] also"
        },
        {
            "title": "LLMs",
            "content": "Proprietary Models GPT-4o [47] o1-preview [48] Gemini-2.0-Flash-Thinking [60] Claude-3.7-Sonnet [59] Open-Source Models DeepSeek-V3 [38] DeepSeek-R1 [18] Qwen2.5-7B-Instruct [80] Qwen2.5-72B-Instruct [80] QwQ-32B-preview [61] QwQ-32B [62] InternLM3-8B-Instruct [3] Llama3.3-70B-Instruct [41]"
        },
        {
            "title": "VLMs",
            "content": "Proprietary Models GPT-4o [47] Gemini-2.0-Pro [60] Claude-3.7-Sonnet [59] Open-Source Models Qwen2-VL-7B [65] Qwen2.5-VL-7B [2] Qwen2.5-VL-72B [2] DeepSeek-VL2 [76] InternVL2.5-8B [7] InternVL2.5-78B [7] Phi-3.5-Vision [1] Aria [30] H2OVL Mississippi-2B [17] VideoChat-Flash-7B [32] InternVideo2.5 [70] LongVA-7B [88] LongVA-7B-DPO [88]"
        },
        {
            "title": "Results by Taxonomy",
            "content": "MCQ-O OE-P EA CP HR II IS EP LL 59.34 63.64 63.79 61.50 60.40 63.75 50.70 56.18 34.87 55.96 48.79 62.16 70.68 76.61 70.17 60.75 63.06 69.23 24.57 56.11 66.10 48.02 60.27 32.61 57.30 56.89 56.57 58. 64.10 70.02 69.38 67.39 65.14 70.26 50.40 58.44 18.41 56.14 44.97 69.68 83.25 85.32 82.10 84.61 82.63 82.48 19.65 80.61 84.82 52.04 84.07 23.43 84.81 85.35 80.74 80.93 54.57 57.25 58.20 55.60 55.65 57.24 50.99 53.92 51.33 55.77 52.61 54. 58.10 67.90 58.23 36.88 43.49 55.98 29.49 31.61 47.37 44.00 36.47 41.79 29.79 28.46 32.40 36.65 54.25 58.67 63.56 59.23 55.86 59.00 47.60 53.39 35.50 54.14 45.65 62.20 68.63 73.11 65.13 59.52 60.94 65.81 20.04 58.79 66.45 43.47 59.84 31.63 60.88 60.54 56.63 57. 34.00 34.01 35.78 36.92 37.35 39.20 35.41 36.61 38.50 36.93 35.65 37.54 38.52 65.23 34.98 28.88 34.14 41.61 17.69 31.40 34.48 30.23 32.88 29.72 33.45 33.16 28.06 27.42 67.92 73.40 72.24 66.51 68.72 74.04 59.58 64.87 44.64 62.89 57/12 71. 78.68 83.02 74.61 69.14 69.69 74.54 23.43 62.92 74.62 50.27 70.55 38.83 63.67 64.50 63.33 65.79 63.07 68.71 71.89 68.33 65.40 67.10 56.14 61.13 42.03 61.19 54.75 66.98 72.45 77.74 73.14 66.66 67.30 69.03 26.00 63.04 70.87 50.18 65.25 38.40 63.36 63.83 63.33 65. 74.83 72.43 75.60 69.81 70.89 70.05 63.17 69.71 57.38 74.96 60.58 78.04 80.01 89.14 77.38 85.29 83.93 90.04 30.02 85.99 87.54 71.42 86.21 60.05 79.36 84.59 76.93 79.37 65.64 67.06 66.66 62.58 65.95 70.44 56.99 60.49 41.42 62.19 53.82 67. 74.01 79.94 73.34 67.34 67.23 71.89 23.87 62.32 71.43 56.16 67.59 39.64 63.87 62.42 62.63 64.67 70.09 72.69 69.99 69.03 69.46 70.73 60.58 66.93 42.08 63.54 59.38 70.39 75.19 77.90 72.29 71.74 71.26 77.88 25.07 68.19 74.68 46.99 70.39 41.12 64.97 64.58 66.80 68. Table 2. Evaluation results of LLMs and VLMs on VRBench across three evaluation metrics (outcome-level MCQ, process-level MCQ, process-level open-ended evaluation) and seven QA taxonomies (event attribution, counting problems, hypothetical reasoning, implicit inferences, information synopsis, event prediction, logical linkage). bold values indicate the best results among all models, and Underlined values are best results of LLMs. demonstrate strong results. It is noted that the opensource model DeepSeek-R1 [18] achieves the best outcomelevel MCQ accuracy of 70.26%. For the VLMs, Gemini2.0-Pro [60] achieves 76.61% overall accuracy, surpassing all components by at least 5.93%. GPT-4o [47] and Claude-3.7-Sonnet [59] also show competitive results, which demonstrate 70.68% and 70.17% overall performance, respectively. For open-source models, InternVL2.578B [7], emerges as the best non-proprietary VLMs with 66.10% overall accuracy. We then delve into the specific results of each metric and taxonomy. For results in each evaluation stage, we find that most of LLMs and VLMs are capable of achieving high MCQ accuracy, yet compared to LLMs, VLMs struggle to demonstrate their reasoning steps and exhibit lower reasoning ratings. Through results across taxonomy, we found 6 that most of the large models are not proficient in counting problems since they require fine-grained visual perception, in contrast with other tasks. Since the perception of LLMs fully depends on the video summary and some VLMs only support small number of frame inputs, they are unable to correctly perceive the original frames containing the target elements, resulting in the MCQ accuracy close to random guessing and low reasoning process ratings. 4.2. Ablations and Analysis We then further discuss the observations from the evaluation results and highlight some insights on obtaining higher multi-step reasoning performance. Does the evaluation of VRBench align with Human Preference? In contrast to MCQs that compute accuracy via deterministic method, the correctness of process-level openended evaluation highly depends on the judgment of LLMs performance, which may include hallucinations that could influence the evaluation reliability. To this end, we select subset of 30 videos with 300 questions to perform processlevel human evaluation, aiming to quantify the correlation between human and LLM assessment results. Specifically, the human annotators are asked to give process-level open-ended ratings following the same requirement for the judging LLM, i.e., four separated ratings are needed for each question to evaluate the logical coherence, similarity to ground-truth, factual accuracy, and expression clarity. We then compute the win ratio of pairwise comparison for each tested model following [24], where model scores 1 if its rating is higher than its current opponent, 0 if lower, and 0.5 for tie. Finally, we calculate the average win ratio of each model and the Spearman correlation coefficients (ρ) of the win ratios evaluated by LLMs and human annotators. We first investigate the human preference alignment of several LLMs. We select GPT-4o [47], DeepSeek-V3 [38], Qwen2.5-7B [80], and Qwen2.5-72B [80] as evaluation models and give them the same rating prompt for fair comparison. As shown in Figure 4, Qwen2.5 shows low human preference alignment compared with DeepSeekV3 and GPT-4o, which both have correlation coefficients greater than 0.8. Though the human alignment of GPT4o is slightly better than DeepSeek-V3, we observe the evaluation cost of GPT-4o is approximately 10 times that of DeepSeek, which imposes significant burden on the benchmark users. Hence, we adopt DeepSeek-V3 as the judging model for its optimal trade-off between performance and cost. We also probe the evaluation correctness of each openended sub-metric. Following the same procedure, we compute the win ratio and correlation coefficient of DeepSeekV3 for each sub-metric, and the results are shown in Figure 4 (b-d). Results show the coefficients in all metrics are higher than 0.8, indicating strong positive monotonic relationship between LLM and human evaluation results. Another alternative is to instruct the LLM to compute the final score based on the given weights of each sub-metric, which is denoted as DeepSeek w/o sep in Figure 4. Results show that adding this additional computing process diminishes the robustness of evaluation LLM, thus in practice, we ask the model to output all sub-metric rating separately and automatically compute the final score. The role of System-2 thinking. o1-like LLMs, also denoted as system-2 models, have emerged with superior reasoning abilities in multi-disciplinary scenarios such as science and math. We further explore these models performance on the narrative video benchmark. We both assess the role of open-source and proprietary o1-models, as well as the gap compared with their previous system-1 version. For proprietaries, we compare OpenAI o1-preview [48] with GPT-4o [47], and results show that the system-2 version o1 outperforms GPT-4o with 4.30% overall accuracy. For open-source model DeepSeek-V3 [38] and R1 [11], we observe that DeepSeek-R1 achieves higher scores both on It multiple-choice questions and open-ended evaluations. is noted that compared with Qwen2.5-72B-Instruct [79], QwQ-32B [62] gets higher reasoning ratings yet lower outcome-level MCQ results. This shows that though some system-2 models are more proficient in multi-step thinking, their ability to converge lengthy thinking processes into correct answers still needs improvement. Recently, some o1-like training strategies have also been utilized by some VLMs, such as LongVA-7B-DPO [88], model that utilizes direct preference optimization [52] for long video understanding. It achieves 61.44% overall performance and surpasses its vanilla version, suggesting that training VLMs with different optimization methods is feasible and developing system-2 VLM models is crucial for tackling multi-step reasoning questions in narrative videos. Impact of Model Size. It is commonly believed that models with large scales are often accompanied by better perception and reasoning capabilities. To verify such claims, we conduct experiments on the models with different scales, such as Qwen2.5-7B [80] and Qwen2.5-72B, Qwen2.5VL [2] with 7B and 72B parameters, and InternVL2.58B [7] and InternVL2.5-78B. As shown in Table 2, models with more parameters obtain higher overall accuracy (50.70% vs 56.18% for Qwen2.5, 63.06% vs 69.23% for Qwen2.5-VL, and 56.11% vs 66.10% for InternVL2.5), and the performance gap on process-level metrics is more apparent. This indicates that large-scale models are more likely to achieve advanced reasoning capabilities. However, we also noticed that models specifically trained on the reasoning corpus like QwQ-32B [62], can achieve comparable performance compared with its 72B Qwen2.5 counterpart (55.96% vs. 56.18% in overall accuracy) with smaller 7 parameter amount. This suggests that training small-size models with various preference optimization strategies and reasoning-centric corpora might improve its overall reasoning capability. Long Context Helps. We observe substantial performance gap between models with and without long context support. As shown in Table 2, models with long frame input tend to achieve higher overall accuracy. For example, Gemini-2.0-Pro [60] is capable of inference with 0.5 fps and supports large number frame inputs when answering questions of videos over 1 hour, and it achieves the optimal performance compared with all other fixed-length models. For the open-source models, Qwen2.5-VL-7B [2] and VideoChat-Flash [32] with 512 frame input achieve 63.06% and 57.30% overall accuracy, which is 6.95% and 1.19% higher than the 8B InternVL-2.5 [7] model with 64 frames input. Since the average length of VRBench is 1.61 hours, the necessity of long context support is becoming more pronounced, so that models can perceive more plot elements that help the analysis of long video content. LLMs vs. VLMs. Since tested LLMs are only provided with the automatically-generated video summary, yet VLMs are capable of directly perceiving the raw visual content, it is unfair to directly compare the numerical results of LLMs and VLMs. However, there are still some noteworthy phenomena that can be discussed. First, VLMs with coarsegrained visual input (e.g., 4 frames for H2OVL Mississippi2B [17] and DeepSeek-VL2 [76]) perform significantly worse than several system-2 LLMs like DeepSeek-R1 [18] and Claude-3.7-Sonnet [59]. This indicates that randomly selecting small number of frames is not enough to tackle narrative-based reasoning tasks that require long-range perception. Furthermore, VLMs with fine-grained visual perception, such as Gemini-2.0-Pro with 0.5 fps perform better than top-tier LLMs on both process-level and outcomelevel metrics, showing the inevitability of detailed visual content input for tackling questions in VRBench. 4.3. Test-Time Scaling Exploration We here probe models performance when scaling test-time compute cost. Since we tend to prompt the models with Chain-of-Thought template when tackling complex reasoning tasks, it is observed that system-2 models are capable of dramatically expanding the inference budget to improve their performance. To this end, we set series of token limitations on these models and require the models to think with different instructions. Specifically, we select QwQ32B [62] as the experimental LLM and Qwen2-VL-7B [79] as the testing VLM on subset with 300 videos and 2,403 questions. For each model, several parallel experiments are conducted with increasing maximum token limits from 256 to 2048. We instruct the models to output the reasoning process as much as possible for large token limitations, and 8 Figure 5. Test-Time Scaling Results. We report the average accuracy of outcome-level MCQ and process-level open-ended ratings. let them think concisely when the token limit is low. Since we aim to investigate the quality of models initial reasoning process and their capability to generate the right answers through thinking, we report the overall scores that both evaluate the outcome accuracy and process quailty. Results are shown in Figure 5, where QwQ shows remarkable rating boost from 48.91% to 61.34% when setting large token limit. On the contrary, the small-sized system-1 model Qwen2-VL-7B performs even worse when using large token number limitation and instructing the model to output more thinking process. The model tends to output more ambiguous outputs that lead to the wrong answer. This observation suggests that models with large parameter scales and system-2 capabilities benefit from the test-time scaling strategy. It also provides the insight that developing more test-time scaling approaches that guide the model to generate long reasoning traces could be feasible and promising way to unlock more potential capabilities of system-2 models, thereby benefiting complex tasks that require multi-step reasoning. 5. Conclusion This paper introduces VRBench, comprehensive longnarrative video benchmark for evaluating multi-step reasoning. Through manually filtered narrative videos, highquality stepwise annotations, and multi-phase evaluation pipeline, VRBench distinguishes itself from other existing reasoning benchmarks and shows the robust capability to evaluate LLMs and VLMs both from the process and outcome perspectives. By evaluating and analyzing 28 frontier large models, we thoroughly demonstrate the detailed performance of current reasoning models across various reasoning questions and provide valuable insights towards constructing more advanced multi-step reasoning models."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 6, 19 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 6, 7, 8, 19 [3] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei arXiv preprint Chu, et al. arXiv:2403.17297, 2024. 2, 6 Internlm2 technical report. [4] Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cgbench: Clue-grounded question answering benchmark for long video understanding. arXiv preprint arXiv:2412.12075, 2024. 3 [5] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: theorem-driven question answering dataset. arXiv preprint arXiv:2305.12524, 2023. [6] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question In European Conference on Computer Vision, answering. pages 179195. Springer, 2024. 3 [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2, 5, 6, 7, 8, 19 [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 2, 3 [9] Daniel Cores, Michael Dorkenwald, Manuel Mucientes, Tvbench: RearXiv preprint Cees GM Snoek, and Yuki Asano. designing video-language evaluation. arXiv:2410.07752, 2024. 3 [10] Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Ivan Koychev, and Preslav Dimitar Iliyanov Dimitrov, Nakov. Exams-v: multi-discipline multilingual multimodal exam benchmark for evaluating vision language models. arXiv preprint arXiv:2403.10378, 2024. [11] DeepL. Deepl translate: The worlds most accurate translator. https://www.deepl.com/en/translator, 2025. 4, 7 [12] Google Deepmind. Gemini 2.0 flash thinking. https: / / deepmind . google / technologies / gemini / flash-thinking/, 2025. Accessed: 2025-01-21. 2, 5 long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124, 2025. 3 [14] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. arXiv preprint arXiv:2501.03230, 2024. 2 [15] Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. Sciknoweval: Evaluating multilevel scientific knowledge of large language models. arXiv preprint arXiv:2406.09098, 2024. 3 [16] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 3 [17] Shaikat Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, and Sri Satish Ambati. H2ovl-mississippi vision language models technical report. arXiv preprint arXiv:2410.13611, 2024. 6, 8, [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 5, 6, 8 [19] Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, and Si Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame selection. arXiv preprint arXiv:2411.14794, 2024. 3 [20] Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, et al. Mmworld: Towards multi-discipline arXiv multi-faceted world model evaluation in videos. preprint arXiv:2406.08407, 2024. 3 [21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. MeaarXiv suring massive multitask language understanding. preprint arXiv:2009.03300, 2020. 3 [22] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. 3 [23] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36:6299163010, 2023. 3 [24] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [13] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: [25] Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, 9 Ethan Chern, Yixin Ye, et al. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. Advances in Neural Information Processing Systems, 37: 1920919253, 2025. 3 [26] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. 3 [27] Zhihuan Jiang, Zhen Yang, Jinhao Chen, Zhengxiao Du, Weihan Wang, Bin Xu, Yuxiao Dong, and Jie Tang. Visscience: An extensive benchmark for evaluating k12 educational multi-modal scientific reasoning. arXiv preprint arXiv:2409.13730, 2024. [28] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696, 2018. 3 [29] Bozheng Li, Yongliang Wu, Yi Lu, Jiashuo Yu, Licheng Tang, Jiawang Cao, Wenqing Zhu, Yuyang Sun, Jay Wu, and Wenbo Zhu. Veu-bench: Towards comprehensive understanding of video editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 13671 13680, 2025. 3 [30] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-ofexperts model. arXiv preprint arXiv:2410.05993, 2024. 2, 6, 19 [31] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 3, 4, 5 [32] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. 2, 6, 8, 19 [33] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal arXiv preprint perception via reinforcement fine-tuning. arXiv:2504.06958, 2025. [34] Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. Videovista: versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303, 2024. 3 [35] Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, et al. Mmsci: multimodal multi-discipline dataset for phd-level scientific comprehension. In AI for Accelerated Materials Design-Vienna 2024, 2024. 3 [36] Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, and Xiangliang Zhang. Scemqa: scientific college entrance level multimodal question answering benchmark. arXiv preprint arXiv:2402.05138, 2024. 3 [37] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2668926699, 2024. 2 [38] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 5, 6, [39] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 3 [40] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Et bench: Towards open-ended event-level video-language understanding. arXiv preprint arXiv:2409.18111, 2024. 3 [41] Llama-3.3. Llama-3.3-70b-instruct. https : / / huggingface . co / meta - llama / Llama - 3 . 3 - 70B-Instruct, 2025. 6 [42] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [43] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 3 [44] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 3 [45] Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 15271536, 2020. 3 [46] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. 3 [47] OpenAI. Hello gpt4-o. https : / / openai . com / index/hello-gpt-4o/, 2024. Accessed: 2024-05-13. 4, 5, 6, 7, 16, 17, 19 [48] OpenAI. Introducing openai o1. https://openai. com/o1/, 2024. 2, 5, 6, 7 [49] OpenAI. Openai o3-mini. https://openai.com/ index/openai-o3-mini/, 2025. 2 [50] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. 3 10 [51] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In Internarecognition via large-scale weak supervision. tional conference on machine learning, pages 2849228518. PMLR, 2023. [52] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 2, 7 [53] Ruchit Rawal, Khalid Saifullah, Miquel Farre, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813, 2024. 2, 3 [54] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289301, 2022. 3 [55] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2 [56] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 3 [57] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. Scieval: multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1905319061, 2024. 3 [58] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through questionIn Proceedings of the IEEE conference on answering. computer vision and pattern recognition, pages 46314640, 2016. [59] Anthropic. Claude Team. Claude 3.7 sonnet. https:// www.anthropic.com/claude/sonnet, 2025. 2, 5, 6, 8, 19 [60] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 5, 6, 8, 19 [61] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/ qwq-32b-preview/, 2024. Accessed: 2024-11-28. 6 [62] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning. https://qwenlm.github.io/blog/ qwq-32b/, 2025. Accessed: 2025-3-6. 2, 5, 6, 7, 8 [63] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamavo1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. 2 [64] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. 3 [65] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 5, 6, 19 [66] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. 2 [67] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 2, [68] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for mulIn European Conference on timodal video understanding. Computer Vision, pages 396416. Springer, 2024. 2 [69] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 3 [70] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 2, 5, 6, 19 [71] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2025. 3 [72] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 5 [73] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. 3 [74] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2025. 2, 3 [75] Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, and Yuyu Luo. Chartinsights: Evaluating multimodal large language models for low-level chart question answering. arXiv preprint arXiv:2405.07001, 2024. 11 [88] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 2, 6, 7, 19 [89] Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, et al. Mmvu: Measuring expertlevel multi-discipline video understanding. arXiv preprint arXiv:2501.12380, 2025. 2, 3 [90] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 3 [91] Wentao Zhu, Yufang Huang, Xiufeng Xie, Wenxian Liu, Jincan Deng, Debing Zhang, Zhangyang Wang, and Ji Liu. Autoshot: short video dataset and state-of-the-art shot boundary detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2238 2247, 2023. 4 [76] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 6, 8, [77] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 3 [78] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason stepby-step. arXiv preprint arXiv:2411.10440, 2024. 2 [79] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report, 2024. URL https://arxiv. org/abs/2407.10671, 2024. 7, 8 [80] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 5, 6, 7, 17 [81] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: video is worth thousands of words. Advances in Neural Information Processing Systems, 37:5724057261, 2025. 3 [82] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 3 [83] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multiarXiv discipline multimodal understanding benchmark. preprint arXiv:2409.02813, 2024. [84] Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. Mr-gsm8k: meta-reasoning benchmark for large language model evaluation. arXiv preprint arXiv:2312.17080, 2023. 3 [85] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 2 [86] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, et al. Cmmmu: chinese massive multiarXiv discipline multimodal understanding benchmark. preprint arXiv:2401.11944, 2024. 3 [87] Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, and Yu Qiao. Movqa: benchmark of versatile question-answering for long-form arXiv preprint arXiv:2312.04817, movie understanding. 2023. 3 12 VRBench: Benchmark for Multi-Step Reasoning in Long Narrative Videos"
        },
        {
            "title": "Supplementary Material",
            "content": "F. Question Exemplar Due to page limitation, we only provide one qualitative demo of our annotation QA pairs in the main manuscript, hence we complement more examples regarding all question types in this section in Figure A6-A12. Figure A6. Event attribution: Determine causal origins or underlying motivations of video events. G. Manual Filtering Details In this part, we provide additional details about the video manual-filtering progress, including the complete annotation guideline and the interface screenshot. G.1. Annotation Guideline G.1.1. Confidentiality Statement The content of this document involves trade secrets or other secrets, and is only for internal members and authorized units or individuals to review, please receive this document to keep it in safe place, without the consent of any third party shall not be disclosed to this document. G.1.2. Task Overview Given video with corresponding translated subtitles, the annotators need to: Quickly browse long videos and subtitles and determine their video category. Depending on the video category, rate the long video from 1 to 10 according to the category-specific rating guideline. Give an overall description of the video and the reason for the rating. Figure A7. Event prediction: Forecast subsequent events in the video timeline. Figure A8. Implicit inference: Extract unstated temporal, emotional, or relational context from visual cues. Overall Rating Rules. The overall range of scoring is from 1-10, with higher scores representing more suitable for constructing multi-step reasoning questions. score of 7-10 means this video is very suitable for constructing multi-step reasoning pairs, score of 4-6 represents it is moderately suitable, and score of 1-3 represents complete lack of Figure A9. Information synopsis: Condense critical information across multimodal inputs. Figure A11. Logical linkage: Establish event-mediated connections between visual/narrative elements. Figure A10. Hypothetical reasoning: Deduce plausible scenario outcomes from stated premises. suitability. Score distribution requirements. There should not be situation where one/some of the categories are all above or below 7 points. Separate score judgments based on the current video category according to the category-specific rating criteria. ASR Transcripts Quality Check. Please note that annotators must use the video content in conjunction with the ASR transcript to determine whether reasoning is appropriate or not. If the video contents are static (e.g., speech, PPT presentation, two people sitting and chatting) and there is lot of reasonable content in the speech, it should be labeled as unsuitable for multi-step reasoning annotation. Figure A12. Counting problems: Quantify state changes through arithmetic/combinatorial analysis. Figure A13. Interface of manual video filtering. G.1.3. Rating Standard Movies and Animation. Plot coherence: if it is movie/TV series/animation with coherent and complete plot, it is suitable for labeling, if it is an episode/spliced fragmented video, it is not suit14 scripts, but the overall content must be understandable. Gaming Videos Game Type: Games with rich mechanics are suitable for constructing reasoning problems, such as MoBA games, RPG games, etc. Puzzle games are not suitable for constructing reasoning tasks. Content coherence: full game match is good for constructing reasoning questions while the highlight collection is not suitable. First/Third View. For role-playing games, first view videos accompanied by an anchors explanation are suitable for constructing deduction missions, but for multiplayer online games, the third-person view is more suitable for constructing qa questions, and the first-person view is not suitable. Game length: Full-length games are better suited for constructing reasoning questions, long videos spliced together from multiple short-length games are not suitable, such as Fortnite/PUBG anchor recordings with no obvious contextual connection. Transcripts correctness: the translated subtitle should be clear and readable, there may be some inaccurate transcripts, but the overall content must be understandable. Science and technology/military videos Plot coherence: Must be complete scientific/military video, try to choose scientific and military explanatory videos, not news and reports. Content richness: In conjunction with subtitle content, there needs to be clear reasoning element, in addition to some relevance at long time granularity for constructing long time questions Transcripts correctness: the translated subtitle should be clear and readable, there may be some inaccurate transcripts, but the overall content must be understandable. Other videos For other categories of video that may appear, such as news and interviews, the rating should not be higher than 3 in principle. In practice, the annotator may also make subjective judgments on videos in some rarely occurring category, and may give high scores to videos with high reasoning content as appropriate, but the score should not exceed 8. G.1.4. Quality Assessment The quality assessors will conduct 10:1 sampling ratio to check whether the video scoring meets the QA standards and the overall score distribution requirements. G.2. Annotation Interface The annotation interface is illustrated in Figure A13 H. Human Annotation Details In this section, we complement more human annotation details, including the complete annotation guideline and the Figure A14. Interface of manual video labeling. able. Content richness: if its long documentary/landscape movie/music MV, its not suitable for multi-step reasoning. If it contains rich plot, it is suitable for labeling Transcripts correctness: the translated subtitle should be clear and readable, there may be some inaccurate transcripts, but the overall content must be understandable."
        },
        {
            "title": "Sports videos",
            "content": "Sports category: If it is video with small visual change content such as golf, racing, riding, etc., it is not suitable for multi-step reasoning annotation. If it is video containing rich technical and tactical content and formation change, such as basketball, soccer, volleyball, etc., it is suitable for constructing reasoning tasks. Episode coherence: Suitable for labeling if it is coherent and complete sports video, not suitable if it is collection/spliced fragmented video. Transcripts correctness: The translated subtitle should be clear and readable, there may be some inaccurate transcripts, but the overall content must be understandable. In addition, the subtitles need to contain some explanation of the sports content to facilitate the understanding of the sports content and the construction of QA questions First/third person: Videos in first person is much less likely to construct multi-step reasoning problems, while third/rebroadcast view allows to gather details of the overall lineup and makes it easier to construct reasoning problems Travel & Vlogs Episode Coherence: must be complete travel video that can be spread out over multiple days, but not compilation of multiple vlogs of multiple people in the same location, nor compilation of vlogs of one person in multiple locations Content richness: Travel content needs to have clear plot twist, in addition to some relevance in terms of long time granularity to construct long time questions. If it is running travel record or contains only record of scenery, it is not suitable for multi-step reasoning annotations. Transcripts correctness: the translated subtitle should be clear and readable, there may be some inaccurate tran15 interface screenshot. H.1. Annotation Guideline H.1.1. Confidentiality Statement The content of this document involves trade secrets or other secrets, and is only for internal members and authorized units or individuals to review, please receive this document to keep it in safe place, without the consent of any third party shall not be disclosed to this document. H.1.2. Task Overview Given video with subtitles and 6 pseudo pre-annotations, generate 8-10 question-answer pairs that require multi-step reasoning based on the video content and provide the detailed reasoning process with both question and stepwise timestamps. H.1.3. Annotation Standard Please follow the process below for labeling: Preliminaries. We provide 6 pseudo pre-annotations generated by GPT-4o [47]. Please evaluate their authenticity and generate new questions based on these pre-annotations. Annotation Requirements. The questions posed must be reasoning-based, not perception-based. Answers should rely on logical reasoning and require an understanding of the overall plot development, rather than simple scene/object comprehension. The answers to the question are expected to be relatively fixed. Except for information synopsis type, answers require multi-step reasoning. The number of reasoning steps is equal to the number of reasoning processes - 1 (For example, if the reasoning process only contains the vanilla question and answers, the reasoning step number is counted as 1.). No more than 2 single-step reasoning questions are allowed per video. Questions and answers must be derived solely from the video content or combination of video and subtitles, not from subtitles alone or common life knowledge. Different question types: Each videos 8-10 QA pairs must include 5 out of the 7 provided question types. No more than 1 event summary-type question is allowed. Different time granularities: Design questions that can be answered based on diverse video lengths, divided into [015 minutes, 15-40 minutes, 40 minutes-2 hours]. For the 0-15 minutes questions, reasoning questions can be single-step, as multi-step questions may not be feasible. No more than 4 questions should be from the 0-15 minute interval. Videos shorter than 20 minutes can be marked as unanswerable and will be approved without review. Videos shorter than 50 minutes do not require questions from the 40 minutes-2 hours; only questions from the 0-15 minutes and 15-40 minutes intervals are needed. Multi-Step Reasoning Expressions. We provide the logical expressions of each multi-step question type to facilitate the understanding of multi-step reasoning. 1. Event Prediction: Predict the next event based on event that has already occurred in the video. Multi-step process 1: D. Multi-step process 2: if is attributed by B. 2. Hypothetical Reasoning: Given hypothetical premise A, infer the corresponding development. Multi-step process 1: D. Multi-step process 2: if is attributed by B. 3. Event Attribution: Analyze the cause or purpose of event in the video. Multi-step process: A. 4. Implicit Reasoning: Infer the feelings/emotions of specific character D, relationships between characters, or the situation of event development at the current point in time. Multi-step process: A. 5. Logical Links: Analyze the correlation between two elements and in the video and explain their logical relationship. Multi-step process: B. 6. Information synopsis: Pose summary question based on the video content and attempt to answer it (note: the question should not simply summarize the entire video but should be synopsis question based on the video, requiring only single-step reasoning). 7. Counting Problems: Infer the transformation of element under multiple conditions, possibly involving arithmetic or counting components such as numbers, dates, or specific points in time. Multi-step process: D, where represents any logical/mathematical operation. Annotation Format. The annotation format should be as follows: Timestamp: [xx:xx:xx->xx:xx:xx] Question: xxxx Answer: xxxx Reasoning Process: 1. [xx:xx:xx->xx:xx:xx] 2. [xx:xx:xx->xx:xx:xx] 3. [xx:xx:xx->xx:xx:xx] Reasoning Type: xxxx H.1.4. Quality Assessment We adopt full-scale quality assessment strategy, and the unqualified annotations should be modified until they meet the qualified criteria. We detail the quality assessment process in Section I. 16 Figure A15. Interface of video-level evaluation. Figure A16. Interface of question-level evaluation. H.2. Annotation Interface The annotation interface is illustrated in Figure H.3. Details of Video Summary Generation The process of generating video summary involves two main steps. First, for each video segment, we merge visual descriptions with subtitles in chronological order while eliminating duplicate subtitles. If the subtitle content will be removed if it is overlapped with the former subtitle. This pattern continues until the final scene, The second step entails generating video-level summary based on segment-level summaries. We set 10minute temporal interval to generate multiple segment-level summaries by integrating multimodal descriptions in each scene. During the integration, the information from previous segments, including both visual descriptions and subtitles, serves as the narrative background for each subsequent segment. The first segment-level summary becomes the narrative background for generating the second summary, which does not reiterate events from the first segment. This process continues iteratively until the final segmentlevel summary, resulting in comprehensive summary of the video across all clips. We illustrate the prompt of generating the first video summary in Figure A17, and the prompt of generating the following video summary in Figure A18. We adopt Qwen2.5-72B-Instruct [80] to generate video summary. Figure A17. Prompts of generating the first video summary. Figure A18. Prompts of generating other video summaries. H.4. Prompts GPT pseudo annotations We provide the prompts for generating GPT pseudo annotations in Figure A19. During the generation process, we put the prompt with the video summary into GPT-4o [47]. 17 2. Give four-choice question-level rating of unqualified/normal/good/excellent for each QA pair and provide the rationale. Validation Requirements. The quality assessors need to follow the following criteria to determine whether the given set of annotations is qualified or not. The quality assessors need to: Determine whether the 8-10 QA pairs of the entire video meet the video-level QA standards. If not, directly choose to fail under the first question and provide the reason for failure without proceeding to the second step of QA. Determine whether each question under the entire video meets the following question-level quality control standards. If all meet, it will be judged as correct labeling; if one or more do not meet, it will be judged as unqualified labeling, and the reasons for failure must be provided in the prescribed format. If the annotation is judged to be correct, proceed to question-level validation. Otherwise, check the radio box: failed, and in the following text box, provide the specific number of non-compliance with the quality control It is necessary to write out all standards and reasons. non-compliant QC standard entries to facilitate subsequent changes by the labeler. Video-Level Quality Control Standard. a. Time Granularity Audit: Each video question time span must contain [0-15min, 15-40min, 40min-2h] the three time granularities of the question, and 0-15min questions should not exceed 4. Videos shorter than 20 minutes can be marked as unanswerable and will be reviewed without moderation. Videos shorter than 50 minutes do not need to mark the 40min-2h dimension problem; the review only needs to determine whether the other two time dimension conditions are met. b. Number/Type of Questions Reviewed: Each video must contain 8-10 QA pairs, including at least 5 of the 7 types of tasks. There can only be one task type for information synopsis. c. Single-Step Reasoning Review: No more than 2 single-step reasoning questions per video. I.2. Configurations of Evaluated Models We provide the detailed configuration of evaluated VLM, including the model version and input visual frames, which is illustrated in Figure A3. Problem-Level Quality Control Standard. a. Question Type Review: The type of question must be reasoning question, not perception question. Perceptual Questions: Questions that can be derived directly from the picture. Figure A19. Prompts of generating GPT pre-annotation. I. Human Validation Details I.1. Validation Guideline I.1.1. Confidentiality Statement The content of this document involves trade secrets or other secrets, and is only for internal members and authorized units or individuals to review, please receive this document to keep it in safe place, without the consent of any third party shall not be disclosed to this document. I.1.2. Task Overview Given video, translated subtitles, 6 pre-generated pseudo annotations, and 8-10 manually generated multi-step reasoning annotations specific to the video content, the quality assessor needs: 1. Give unqualified/qualified binary video-level rating for the question distributions and provide reasons for the unqualified annotations."
        },
        {
            "title": "Version",
            "content": "Support Video?"
        },
        {
            "title": "Input\nFrames",
            "content": "# multi-step reasoning Pipeline"
        },
        {
            "title": "Proprietary Models",
            "content": "OpenAI Google GPT-4o [47] Gemini 2.0 Pro [60] Anthropic Claude-3.7-Sonnet [59] Open-source Multimodal Foundation Models Qwen2-VL-7B [65] Qwen2.5-VL-7B [2] Qwen2.5-VL-72B [2] InternVL2.5-78B [7] InternVL2.5-8B [7] InternVideo2.5-8B [70] VideoChat-Flash [32] Phi-3.5-Vision [1] Aria [30] Alibaba Shanghai AI Lab OpenGVLab Microsoft Rhymes H2O 2024-8 2025-2 2025-2 2024-8 2025-1 2025-1 2024-11 2024-11 2025-2 20252024-7 gpt-4o-2024-08-06 Gemini 2.0 Pro Claude-3.7-Sonnet-No-extended-thinking Qwen2-VL-7B-Instruct Qwen2.4-VL-7B-Instruct Qwen2.5-VL-72B-Instruct InternVL2.5-78B InternVL2.5-8B InternVideo2.5-Chat-8B VideoChat-Flash-7B@224 Phi-3.5-vision-instruct 2024-11 Aria-Chat H2OVL Mississippi-2B [17] 2024h2ovl-mississippi-2b DeepSeek DeepSeek-VL2 [76] 2024-12 deepseek-vl2 lmms-lab LongVA-7B [88] LongVA-7B-DPO 2024-6 2024-6 longVA-7B LongVA-7B-DPO [88] 64 0.5fps 20 128 128 64 64 64 512 8 8 4 128 128 API API API HF HF HF HF HF HF HF HF Table A3. Details of evaluated multimodal foundation models. The Input Frames column represents the default number of input frames, chosen from 4, 8, 16, 32, 64, 128, 256, 512, based on the maximum value that does not exceed the models context window and the constraints of GPU memory. HF means Hugging Face. Reasoning Problems: Problems that require reasonFor reasoning step timestamp markers, time reduning in conjunction with perceptual results. dancy must not exceed 1 minute. b. Question Modal Audit: The answer to question must be inferred from the video content or video + subtitle content, not from the subtitles alone. c. Annotation Format Review: The annotation must contain the following five items: the corresponding start time of the question, the question, the answer, the multistep reasoning, and the timestamp of each step (for question types such as hypothetical reasoning and event prediction, the timestamp of the step can be omitted if these virtual events do not occur in the video), the question type. Answers must conform to the given template. d. Question Correctness Audit: According to the labeled video start time, the answer to the question can be obtained through the video content, and the reasons and problem categories given by the quality inspector are reasonable. e. Reasoning Step Audit: Except for the 0-15min time granularity and information synopsis task, all questions must be multi-step reasoning problems. If you find that the contents of the reasoning steps are too similar to other steps, you can merge the reasoning steps, and the final reasoning step numbers will be calculated according to the merged reasoning steps. The number of reasoning steps is equal to the number of reasoning process bars - 1. f. Timestamp Redundancy Audit: For issue timestamp markers, time redundancy should not exceed 5 minutes. Example: If for manually generated question, the latest time node to start watching is t1, and the earliest time node that can end watching is t2, then the questions timestamp is labeled as [t1-t2]. If the manually generated questions timestamp is [t3-t4], the question is labeled as failing when t3 + 5 < t1 or t4 5 > t2. Step 2: Labeling Quality Scores. Quality Control Standards: a. Reasoning Step QC: 5 or more steps of reasoning meet the excellent rating, 3-4 steps of reasoning are labeled as good, and 1-2 steps of reasoning are labeled as normal. (Note: Questions that do not meet the QC criteria for the reasoning step review should be directly labeled as failing and will not participate in scoring.) b. Question Difficulty QC: Subjective judgment of the difficulty of questions posed by annotators. Questions that are easy to answer qualify for normal grade, those that are more difficult to answer qualify for good grade, and those that are very difficult qualify for an excellent grade. c. Reasoning Process Uniqueness Judgment: If the reasoning process conforms to uniqueness, i.e., the answer given by the user cannot be reached by other reasoning steps, then it conforms to the excellent grade. normal grade is met if it is clear that some other reasonable reasoning process could have led to the answer given by the user; good grade is met if it is not clear whether 19 the answer is unique or not. d. Reasoning Process Detail QC: If each reasoning step is concise, complete sentence, it meets the excellent rating. If the reasoning is redundant and complete in one or more sentences per step, or if the reasoning is concise but less than complete in one sentence per step, it meets the good rating. If each step of reasoning is neither concise nor complete, it meets the normal grade. Following the above four quality control standards for grade determination, the final score is calculated: 1 point for excellent, 0 points for good, -1 point for normal. If the total score is greater than 0 points, it will be labeled as excellent; if the total score is equal to 0 points, it will be labeled as good; and if the total score is less than 0 points, it will be labeled as normal. I.2.1. Quality Assessment We randomly sample human validation results with portion of 5%-10% for quality assessment following these criteria: 1. Assessing correctness to determine compliance with quality control standards. 2. Assessing quality scores for compliance with validation standards. 3. Whether each manual annotation is scored, with or without reasons for vacant scoring. I.3. Validation Interface The video-level annotation interface is illustrated in Figure A15, and we show the question-level annotation interface in Figure A16. J. Evaluation Setup J.1. Prompts for Model Inference We here provide prompts for model inference in Figure A20 for LLM and VLM. J.2. Prompts for Open-ended Evaluation Prompts for open-ended evaluation are illustrated in Figure A21. K. Ethical Discussion K.1. Data Safety Discussion VRBench is constructed from large number of long narrative videos that may include harmful visual content and speech transcripts. To avoid potential ethical issues, we employ multimodal unsafe detection strategy. First, we extract frames at 1 fps and use an image safety detection API to filter videos that include pornography, horror, politics, violence, and vulgarity content. Then we use multimodal language safety detection API to filter all vanilla speech (a) System prompt for LLM (b) System prompt for LLM (c) User prompt template Figure A20. Prompt for LLM Inference transcripts and video captions. We use the same API tags as image unsafe detection to filter textual data. During manual filtering, we also explicitly request the human annotators to discard videos deemed to contain terror, pornographic, or political content for data safety assurance. K.2. Dataset Bias VRBench includes videos in 8 languages that exclude English and Chinese, the two most widely used languages around the world. Therefore, VRBench may contain potential cultural bias for countries and regions where these languages are used. During data curation, we strive to ensure that all races, skin colors, and genders are included in the data collection scope, which demonstrates that we have not exhibited but actively avoided any subjective bias regarding these characteristics. 20 Figure A21. System prompt for question having unique answer K.3. Potential Social Impact We argue that VRBench does not contain elements that could pose potential societal negative impact. We have implemented the aforementioned measures in Section K.1 to prevent the inclusion of harmful information, and we will make every effort to revise the dataset if any content is found to produce unforeseen negative impacts. On the contrary, VRBench might draw the AI researchs focus more to the area of non-Chinese and non-English communities, which we believe is currently being overlooked by published datasets. We consider this to be potential socially beneficial aspect. K.4. LLM Usage Claim We use LLMs for proofreading the writing of the paper. We have verified that all content reflects the authors original intent and does not contain factual errors or hallucinated information from LLMs."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences"
    ]
}