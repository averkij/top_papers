{
    "paper_title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability",
    "authors": [
        "Matteo Cargnelutti",
        "Catherine Brobston",
        "John Hess",
        "Jack Cushman",
        "Kristi Mukk",
        "Aristana Scourtas",
        "Kyle Courtney",
        "Greg Leppert",
        "Amanda Watson",
        "Martha Whitehead",
        "Jonathan Zittrain"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use."
        },
        {
            "title": "Start",
            "content": "Technical report preprint. Institutional Books 1.0: 242B token dataset from Harvard Librarys collections, refined for accuracy and usability Matteo Cargnelutti ᵃ, Catherine Brobston ᵃ, John Hess ᵇ, Jack Cushman ᵇ, Kristi Mukk ᵇ, Aristana Scourtas ᵇ, Kyle Courtney ᶜ, Greg Leppert ᵃ, Amanda Watson ᵈ, Martha Whitehead ᶜ, Jonathan Zittrain ᵉ ᵃ Institutional Data Initiative, Harvard Law School Library ᵇ Library Innovation Lab, Harvard Law School Library ᶜ Harvard Library ᵈ Harvard Law School Library ᵉ Harvard Law School, Harvard School of Engineering and Applied Sciences, Harvard Kennedy School"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, large collection of public domain books originally digitized through Harvard Librarys participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Librarys collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this projects goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use. Corresponding author: gleppert@law.harvard.edu"
        },
        {
            "title": "1 Introduction................................................................................................................................................. 2 \n2 Contributions...............................................................................................................................................3 \n3 Retrieval of the source materials.................................................................................................................4 \n3.1 Context...............................................................................................................................................4 \n3.2 Process............................................................................................................................................... 4 \n3.3 Results............................................................................................................................................... 4 \n4 Analysis and post-processing...................................................................................................................... 4 \n4.1 Goals and scope................................................................................................................................. 4 \n4.2 Available text..................................................................................................................................... 5 \n4.3 Temporal coverage.............................................................................................................................7 \n4.4 Language coverage............................................................................................................................ 8 \n4.5 Topic classification.......................................................................................................................... 12 \n4.6 Collection-level deduplication.........................................................................................................15 \n4.7 OCR Artifact Analysis.....................................................................................................................16 \n4.8 Text analysis.................................................................................................................................... 18 \n4.9 OCR text post-processing................................................................................................................ 20 \n5 Rights determination................................................................................................................................. 25 \n5.1 Methodology....................................................................................................................................25 \n5.2 Results............................................................................................................................................. 25 \n5.3 Rights determination statement....................................................................................................... 26 \n6 Discussion and future directions............................................................................................................... 26 \nAcknowledgements...................................................................................................................................... 27 \nDisclaimers...................................................................................................................................................28 \nHarmful Language and Content in this Dataset.....................................................................................28 \nHarmful Language in Bibliographic Description.................................................................................. 28 \nReference list................................................................................................................................................29 \nAppendices................................................................................................................................................... 34 \nAppendix A: Dataset fields....................................................................................................................34 \nAppendix B: Temporal coverage breakdown........................................................................................ 38 \nAppendix C: Language coverage breakdown........................................................................................40 \nAppendix D: Source to LCC topic classification mapping................................................................... 43 \nAppendix E: Sample of the collection’s pre-existing topic/subject classification.................................45 \nAppendix F: Sample of the collection’s pre-existing genre/form classification....................................46 \nAppendix G: Composition of the topic classification training sets....................................................... 47 \nAppendix H: Topic classification model training report....................................................................... 48 \nAppendix I: OCR lines classification - Training dataset generation prompt.........................................49 \nAppendix J: Hathitrust Rights Determination Breakdown....................................................................50",
            "content": ""
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have direct impact on their quality (Kaplan et al., 2020; Muennighoff et al., 2023; Gunasekar et al., 2023). The rapid development and adoption of LLMs has brought into focus the scarcity of publicly available, high-quality data at the scale necessary for effective model training. In addition, body of recent work has described data practices that often ignore data provenance and licensing terms (Longpre et al., 2024; Baack et al., 2025); modify fundamental properties of the information, introducing data drift; rely on decontextualized data that affects training outputs (Albalak et al., 2024; Longpre et al., 2024b; Welbl et al., 2021); and often exclude any filtering options for particular use cases due to limited or nonexistent description (Dodge et al., 2021). Furthermore, the vast majority of training data that shapes the outputs and limitations of large language models is from the English speaking web (Liu et al., 2024c; Dodge et al., 2021; Gao et al., 2021; Penedo et al., 2024), creating very limited context available to these models. We have been encouraged by several recent examples of datasets published with the goal of making advances against one or more of these challenges. FineWeb, built on top of the web-based data of Common Crawl, focused on careful deduplication and description to facilitate filtering and improve training results (Penedo et al., 2024). Before that, The Pile prioritized data from diverse sources to make training gains (Gao et al., 2021). In their experiments, Gunasekar et al (2023) created high quality, textbook-like training-set which reduced the amount of data needed to train their model. As well, Common Corpus (Langlais et al., 2025) recognizes the impact that high-quality library collections can have on model training. The Institutional Data Initiative, established at the Harvard Law School Library, seeks to build on this work and establish new paradigm of data publication and use. We believe collections from libraries and other knowledge institutions are well positioned to improve the training data ecosystem by diversifying its sources, improving documentation, strengthening provenance chains, and increasing accountability to original source material. All of this is made possible by centuries of careful stewardship by libraries of their collections (Padilla et al., 2023). By collaborating with knowledge institutions to publish their collections as well-documented datasets while working across the institutional and AI communities to co-develop data practices, we aim to create healthier and more efficient foundation for model development across the commercial, academic, and public spheres. To catalyze this process, this technical report introduces the Institutional Books corpus, beginning with nearly one million books digitized from Harvard Librarys collections. In our processing and documentation of this dataset, we have taken rigorous approach to information stewardship. This includes an expansive view of provenance that broadly addresses the steps followed to create this dataset; decisions about its structure and analysis meant to aid users in searching and filtering this data for their particular use case; and special care to account for the legibility and quality of the underlying data. The output of this work is roughly 250B token public domain dataset of digitized books and other bound material. We have refined the dataset through collection-level deduplication (see Section 4.6), OCR artifact and text analysis (see Section 4.7 and Section 4.8), and OCR text post-processing (see Section 2 4.9). While this collection spans multiple centuries, 60% of its contents were published between 1820 and 1920 (see Section 4.3). In addition, volume-level analysis shows the inclusion of texts from over 250 languages, while more nuanced analysis shows the presence of 379 languages when accounting for the occurrence of multiple languages within given volume (see Section 4.4). Our experimental application of high-level topics classification shows thorough coverage of literature, law, philosophy, and science, along with 16 other topic areas (see Section 4.5). Finally, we have taken special care with respect to copyrighted materials, and will initially release only those volumes that we believe to be in the public domain, as denoted in HathiTrusts robust rights database1 (see Section 5). This accounts for 983,004 volumes. With the preliminary publication of this dataset, we further seek to establish community-led process to grow, improve, and use institutional data in ways that strengthen the knowledge ecosystem and assert the importance of ongoing stewardship of training data from the originating knowledge institutions themselves. To this end, we are experimenting to find the best way to release this data in manner that facilitates collaboration. We encourage input on this process to guide the full publication of this and future dataset dataset releases, beginning with the following decisions: At preliminary launch, we have published the metadata, including experimental metadata, in full for anyone to access and use. At preliminary launch, we have published the dataset including OCR-extracted text under noncommercial license, and with click-through that requires users to accept this license, additional terms of use, and to share basic contact information with us so that we can engage the community in its early use. At preliminary launch, we have chosen to postpone the release of the raw scan images, though we will share them liberally with researchers and libraries who wish to review them. While we know AI developers and researchers are eager for more raw materials, we believe this minor friction can help build the relationships and norms necessary to grow collaborative community."
        },
        {
            "title": "2 Contributions",
            "content": "With this technical report, we introduce this initial set of contributions: 1. detailed breakdown of the analysis and processing work we have conducted on Harvard Librarys Google Books collection, spanning 1,075,899 volumes, in order to make it easier for humans and machines alike to navigate and use. 2. public dataset containing the 983,004 volumes in the collection for which there is no known copyright, accounting for 91.37% of the entire collection. For each volume, this dataset features: the original OCR-extracted text, post-processed version of the OCR text (when suitable), as well as bibliographic, source, and generated metadata. Available at https://huggingface.co/datasets/instdin/institutional-books-1.0 3. The Python pipeline we created to analyze and process this collection. Available at https://github.com/instdin/institutional-books-1-pipeline 4. The text classification model we trained and used as part of our topic classification experiment. Available at https://huggingface.co/instdin/institutional-books-topic-classifier-bert 1 https://www.hathitrust.org/the-collection/preservation/rights-database/"
        },
        {
            "title": "3.1 Context",
            "content": "The collection consists of over one million books digitized through collaboration between Harvard Library and Google Books, which began in 20062. The first step of our process was to work with Harvard Library and Google to retrieve digitized materials for the entire collection from GRINthe Google Return Interface hosted by Google to enable Google Books participating institutions to retrieve the digitized copies of their collections. This initial step required us to write custom retrieval pipeline, which we intend to release as open-source software following further refinement."
        },
        {
            "title": "3.2 Process",
            "content": "We used the GRIN API to list and retrieve data for all available volumes from Harvards collection. Each volume is identified by barcode, which was originally assigned by Harvard Library. For each barcode, we sought to retrieve an (encrypted) .tar.gz file, containing scan images, OCR data, as well as bibliographic and processing-related metadata. The majority of these volumes werent readily available for download and instead required us to request their conversion into downloadable format which, in addition to other queuing and rate-limiting constraints, considerably added to the time it took to retrieve the collection (at least 15 days). Once retrieved, each {barcode}.tar.gz file was processed as follows: The full archive was decrypted and sent as is to dedicated raw bucket. The OCR-extracted text, bibliographic metadata, and processing-related metadata were extracted from the archive and sent to separate primary bucket as JSONL and CSV files."
        },
        {
            "title": "3.3 Results",
            "content": "Through this process, we were able to collect full archive for 1,004,977 out of the 1,075,899 volumes listed by Googles API. At the time of writing this paper, we do not have full clarity on the reasons why 70,922 of these volumes could not be retrieved. We hypothesize that at least portion of these volumes were not scanned, but rather that their metadata had been uploaded in anticipation of scanning that had not occurred. This is question we intend to revisit in future iterations."
        },
        {
            "title": "4.1 Goals and scope",
            "content": "We see Institutional Books 1.0 as the beginning of collaborative and iterative research process, and the work we undertook leading to its release focused on facilitating that process. To that end, the analysis and post-processing described in this section was designed with two goals in mind: To get better understanding of the collection and share the insights we generated along the way. We hope these will help users make better informed decisions about their use of the resulting dataset. To complement or improve the collections data, whenever relevant and possible, in order to make the resulting dataset easier to filter, read, and use. 2 https://library.hds.harvard.edu/collections/digital/harvard-google-project 4 This work included some methods we approached as experimental as we did not always have an external basis for comparison to ground them. When leveraging those processes, we established clear benchmarks and, whenever possible, manually reviewed samples of the outputs to further confirm we were meeting those benchmarks. Only after satisfying these requirements did we decide to include these outputs in the final dataset. These outputs are labeled in the datasets headings using the prefix _gen. It is important to note that when working with historical materials, which comprise almost all of this collection, users are working with content that is reflective of its time and, therefore, sometimes problematic. Special care should be taken to ensure the use of research and training methods that account for the presence of offensive or harmful language, including racism, sexism, colonial attitudes, and other forms of discrimination. See our disclaimer at the end of this paper and in the dataset card for more details. Unless specified otherwise, the analysis and post-processing steps described in this section were run on the entire collection of 1,075,899 volumes, of which the public domain volumes we are releasing are subset. We chose to run our analysis on the full collection, including records for which we had metadata but no contents, to provide complete picture to the Harvard community, which will have access to the full collection; because we believe the public domain subset is largely reflective of the whole; and because, as additional materials are scanned or enter the public domain, we hope to establish process to expand the dataset at regular cadence. Whenever possible, we focused on form of frugal computing (Vanderbauwhede, 2023), both to be mindful of the resources we used and to improve the reproducibility of our experiments. The data that was both collected and generated for each volume as part of these experiments is being made available in the dataset released alongside this technical report (See appendix A)."
        },
        {
            "title": "4.2.1 Methodology",
            "content": "In order to better understand how much OCR-extracted text was readily available in the collection, we gathered the following metrics: Total number of volumes without scanned pages. We used the page count metric retrieved from GRIN, after confirming that it matched what could be observed in the OCR-extracted text export (or lack thereof). Total number of volumes without OCR-extracted text. We considered every volume with less than 100 o200k_base (OpenAI, 2022) tokens of OCR-extracted text to be textless. This choice was made in order to account for edge cases we observed in the review of raw scans from the collection. For example, some volumes may have few scanned pages, but no meaningful text (example in Figure 1). Figure 1: Screenshot. Example of volume from the collection with scanned pages, but no actual text. As seen from HathiTrusts viewer for record hvd.32044036307312. Per-volume token counts. We collected token counts for all available volumes using 5 different LLM tokenizers, as way to measure the scale of the collections OCR-extracted text. Namely, we used OpenAIs tiktoken3 to run the GPT-4 (OpenAI, 2023) and GPT-4o (OpenAI, 2024) tokenizers, and HuggingFaces tokenizers library4 to collect token counts for meta-llama/Llama-3.1-70B-Instruct (Dubey et al., 2024), microsoft/phi-4 (Abdin et al., 2024), and mistralai/Mixtral-8x22B-Instruct-v0.15. We selected this set of target models based on their relative similarity in nature (instruct-tuned text generation models) but marked differences in number of parameters and language support."
        },
        {
            "title": "4.2.2 Results",
            "content": "Using the metrics described in the previous subsection, we have identified that, for the 1,075,899 volumes listed in the collection: 394,338,216 pages were scanned, OCR-extracted, and retrievable, with an average of 367 pages per volume. 71,015 volumes (6.60% of the collection) do not have scanned pages. This means that, in addition to the 70,922 volumes that could not be retrieved, 93 could be retrieved but did not have any associated scanned images. 71,335 volumes (6.63% of the collection) have either no OCR-extracted text at all (71,094), or less than 100 o200k_base tokens. By nature, token counts vary widely on tokenizer-by-tokenizer basis. For the entirety of the OCR-extracted text of this collection, token counts range from 248B to 311B  (Table 1)  . 3 https://github.com/openai/tiktoken 4 https://github.com/huggingface/tokenizers 5 https://mistral.ai/news/mixtral-8x22b 6 Table 1: Collection-level OCR-extracted text token counts by target LLM Target LLM openai/gpt-4o openai/gpt-4 Total tokens Per-volume average 248,299,000,580 230,783 275,637,216, 256,192 mistralai/Mixtral-8x22B-Instruct-v0.1 311,589,475,275 289,608 microsoft/phi-4 275,637,216,999 256, meta-llama/Llama-3.1-70B-Instruct 267,899,787,754 249,001 Looking at the distribution of token and page counts across the collection (Figures 2 and 3) revealed that the majority of the digitized volumes are at least 100 pages long, and the majority of OCR-extracted texts are at least 100,000 o200k_base tokens long. Beyond giving us insight on the availability of text at collection level, these results suggest that this collection is likely good fit for training and evaluating models on tasks involving long context comprehension and generation, problem domain text-generation models tend to struggle with (Li et al., 2024; Liu et al., 2024; Liu et al., 2024b). Figure 2: Bar chart. Volume-level token count distribution across the collection (o200k_base). Notably, over 629,000 volumes contain over 100,000 tokens. Figure 3: Bar chart. Volume-level page count distribution across the collection. Over 830K volumes in the collection contain 100 pages or more."
        },
        {
            "title": "4.3.1 Methodology",
            "content": "To get coarse sense of the collections temporal coverage, we collected and analyzed the date and date type fields from each volumes bibliographic metadata extracted from GRIN. These data points were parsed from MARC 21 data field6, representing bibliographic information originally provided to Google by Harvard Library. For each record, we attempted to use Date 1 when possible, Date 2 as fallback, and to exclude any likely invalid date. 6 https://www.loc.gov/marc/bibliographic/bd008a.html 7 For the purpose of this analysis we considered as invalid dates that: Contained non-numeric character. For example, is generally used to denote that at least part of date is unknown (e.g: 18uu). Were empty or marked as 9999. Had date type indicating that the volume is continuing resource such as periodical. In that case, the date fields tended to represent the publications lifespan as opposed to specific volumes publication date. Had No attempt to code in their date type."
        },
        {
            "title": "4.3.2 Results",
            "content": "Out of 1,075,899 records, we identified that: 729,604 entries (67.81% of the collection) had valid date according to the criteria outlined in the previous subsection. The vast majority of these dates were from the 19th and 20th centuries. 650,979 of these dates (60.55% of the collection) ranged from 1820 to 1920, with clear spike between 1880 and 1910 (393,878 volumes, or 36.61% of the collection). Figure 4: Bar chart. Number of volumes by century, inferred from bibliographic metadata (MARC 21 date fields, filtered). Breakdown available in Appendix C. Figure 5: Line chart. Number of volumes by century, inferred from bibliographic metadata (MARC 21 date fields, filtered). Breakdown available in Appendix C. These results suggest high concentration of volumes from the mid-to-late 19th as well as the early 20th centuries (Figures 4 and 5). It is worth noting that 32.19% of the collection could not be dated in the context of this experiment, and that the nature of these dates varies7. These results therefore give us some indication, but not complete picture, of the temporal coverage and diversity of the collection. detailed breakdown is available in Appendix B. detailed breakdown of this analysis is available in appendix B. 7 https://www.loc.gov/marc/bibliographic/bd008a.html"
        },
        {
            "title": "4.4.1 Methodology",
            "content": "The multilingual capabilities of LLMs are direct result of the language diversity of the datasets used to train them and of how well these linguistic resources are used (Gordon, Duh and Kaplan, 2021; Chang et al., 2024; Sina and Agrawal, 2024). As such, having detailed understanding of datasets language coverage may prove critical as it can help facilitate decisions that result in better-performing models. To that end, we combined and compared two different language analysis methods, the results of which we incorporated in the datasets released as part of this project. First, we parsed and analyzed the volume-level language information available in the collections bibliographic metadata for each volume. This datapoint, originally represented as single ISO 639-2B language code8, was parsed from MARC 21 data field representing bibliographic information originally provided by Harvard Library. We then converted this language code to ISO 639-3 using the iso639-lang Python package9. While collecting this initial data point proved helpful, using single language code to describe the contents of given volume can be limiting. As way to get secondary signal on the main language of each volume and also to get sense of the language distribution within each volume, we ran text-detection algorithm on the OCR-extracted text of each volume. We chose to use Python port of the franc library10 11 on chunks of up to 768 characters, split using Langchains RecursiveCharacterTextSplitter12 . While the trigram-based detection technique used by franc to detect languages is simple, we found it to be well-suited to the needs of this collection: First and foremost because of its extensive language coverage. Indeed, the Python port of franc we used comes with support for 414 different languages. We also hypothesized that the nature of the text we needed to analyze would benefit from conversion to trigrams in that context. Text that has been OCR-extracted from books often contains non-semantic line breaks and hyphenations, which we hypothesized that conversion to trigram could partially mitigate. Finally, after initial rounds of testing and manual validation, this method appeared to be an acceptable compromise between accuracy, coverage and computational cost when accounting for the scale and scope of the collection at hand (400M pages). As part of this analysis, we also collected an o200k_base token count for each 768-character chunk we processed in order to get language-specific token count for each volume. We also excluded from our statistics any volume-level results under 1,000 o200k_base tokens, which manual review revealed to be likely noise. 8 https://www.loc.gov/standards/iso639-2/php/code_list.php 9 https://github.com/LBeaudoux/iso639 10 https://github.com/wooorm/franc 11 https://github.com/cyb3rk0tik/pyfranc 12 https://python.langchain.com/api_reference/text_splitters/index.html"
        },
        {
            "title": "4.4.2 Results",
            "content": "Out of 1,075,899 records, we identified: 241 unique volume-level languages, according to the collections bibliographic metadata. 72,673 volumes (6.75% of the collection) did not have volume-level language metadata. 254 unique volume-level languages, according to the detection we performed. 71,656 volumes (6.66% of the collection) could not be analyzed using text-level language detection. This is 321 items more than the total volumes identified as missing OCR-extracted text, due to variety of reasons including edge cases in the selected library and insufficient text to make determination. 379 unique text-level languages, according to our detection. At volume-level, the results of our detection only marginally differed from the assessment found in the bibliographic records originally provided by Harvard Library (Figure 6 and Appendix C). This analysis confirmed that volumes primarily written in English, other West European languages, and classical languages such as Latin make up the vast majority of this collection. English alone represents 47% of it. Figure 6: Bar chart. Comparison of volume counts for the 10 most represented languages in the collection, according to metadata. Blue bars represent total volumes for given language based on volume-level language metadata, black bars total volumes for that same language based on our detection. The text-level metrics we collected, although coarse, have the potential to help users make better-informed decisions regarding data use. For example, by mapping out the language distribution within each volume of the collection, we were able to identify books that appear to be side-by-side translations (example in Figure 7). These texts can be particularly relevant in the context of training models for bitext mining (Resnik, 1999) and other translations tasks. 10 Figure 7: Screenshot. Example of volume from the collection for which text-level language detection proved useful, as it appears to be translation which includes the original text. This volume was initially labeled as Latin; we found that it likely contains 61% FRA tokens and 38% LAT tokens. As seen from HathiTrusts viewer for record hvd.wl117z. Furthermore, this text-level detection allowed us to get sense of how much text is available in any given language, irrespective of the number of volumes using it as their main language (Table 2 and Appendix C). Through this lens, we observed that there are: 17 languages for which we detected more than 1B o200k_base tokens. 45 languages for which we detected more than 100M o200k_base tokens. 105 languages for which we detected more than 10M o200k_base tokens. And 230 languages for which we detected more than 1M o200k_base tokens. Table 2: Total detected o200k_base tokens by language code. Top 10. Language code Total detected tokens % of total detected tokens eng deu fra ita lat spa rus ell nld heb 105,918,942,360 41,803,724,013 33,852,308,477 9,763,407,270 7,718,749,717 5,424,427, 4,956,088,535 3,498,189,810 3,006,044,500 2,376,672,753 43.83% 17.30% 14.01% 4.04% 3.19% 2.24% 2.05% 1.45% 1.24% 0.98% 11 Finally, running detection at text-level let us attempt to map-out detected languages for the 1,291 volumes originally tagged as mul (multiple languages) and identify volume-level language for the 3,611 volumes originally tagged as und (undetermined). The results of this analysis confirm that this collection focuses mainly on Western European languages while offering varying levels of coverage for wide variety of languages. few million tokens can make an important difference when training LLMs in low resource languages (Gessler and Zeldes, 2022) and these results suggest that this collection may offer meaningful support for that use case. It is worth noting that not only can detection never be fully accurate, accurate language classification sometimes requires additional context. For example, the difference between ancient and modern Greek (grc and ell) can be difficult to detect without bibliographic context. For that reason, we encourage users to compare the original language classification with the results of our detection when making decisions about their use of the collection. Per-volume data is available in the dataset we released alongside this manuscript."
        },
        {
            "title": "4.5.1 Methodology",
            "content": "Our goal with this experiment was to identify clear topical tranches within the collection. Curating and using topical datasets has proven to be an effective strategy to improve the performance of models in specialized domains and tasks, and this process generally starts with adequate classification (Parmar et al., 2024; Penedo et al., 2024). To that end, we first collected and analyzed all of the topic/subject and form/genre information available in the collections bibliographic metadata. We quickly identified that the majority of the records in the collection did not have consistent topic classification but that some of this data could be used to train classifier. We therefore filtered and used all of the topic/subject metadata we could directly map to high-level topics in order to generate classification training set (see Appendix D). Specifically, we used HuggingFaces autotrain-advanced (Thakur, 2024) to fine-tune google-bert/bert-base-multilingual-uncased (Devlin et al., 2019) as text-classifier in order to assign one of 20 main classes13 from the first level of Library of Congress Classification Outline (LCC)14 to each volume in the collection, using only available bibliographic metadata and the results of our language detection experiment as input signal, as illustrated in Figure 8. 13 The first level of the Library of Congress Classification Outline contains 21 items, but 2 of them are identical at that level (E -- HISTORY OF THE AMERICAS and -- HISTORY OF THE AMERICAS). 14 https://www.loc.gov/catdir/cpso/lcco/ 12 Title: treatise on analytical geometry of tree dimensions, containing the theory of curve surfaces and of curves of double curvature. Author: Hymers, J. Year: 1848 Language: English Figure 8: Example. Example of bibliographic data presented to our topic classification model during training and inference in order to assign high-level topic classification to individual volumes. We chose not to provide existing topic/subject and genre/form metadata as part of this prompt but to include general note15 when available. While this input format constitutes relatively weak signal and the model we chose to train is small (168M parameters), we hypothesized that small transformer could likely perform this task with fairly high level of accuracy given that we had access to extensive training data for task focused mainly on weighted word associations. We therefore chose this setup as way to start small, improve reproducibility, and limit computational costs. In the process of assembling our training dataset, we set aside 5,000 records for validation and isolated an additional 1,000 records for benchmarking purposes to measure the accuracy of our model. Finally, we hypothesized that the first level of the LCC would likely map well to the collection at hand. The LCC is classification system that was first designed in the 19th century and is best suited for academic collections (Lund and Agbaji, 2018)a description that largely matches this collection. Early experiments we conducted using the first layers of the Dewey Decimal Classification system, the Thema Category Scheme16, and Wikipedias topic classification system17 revealed that the models we initially tested (text-generation models used as classifiers) performed better when using the LCC against that collection which encouraged us to continue in that direction."
        },
        {
            "title": "4.5.2 Results",
            "content": "Out of 1,075,899 records, we first identified that: Only 466,356 volumes (43.35% of the collection) had any topic/subject classification metadata. Only 106,350 volumes (9.88% of the collection) had any form/genre classification metadata. An analysis of the most represented values in both pre-existing classifications revealed that this data could not be used as-is to infer, at high-level, what given volume is about (see appendices and F). Using the filtering mechanism described in Section 4.5.1, we identified 86,830 unique topic/subject values that we could map to the first level of the LCC, which were therefore used to build training dataset (see Appendix for detailed breakdown of the training set). 15 https://www.loc.gov/marc/bibliographic/bd500.html 16 https://www.editeur.org/151/thema 17 https://en.wikipedia.org/wiki/Category:Main_topic_classifications 13 After training google-bert/bert-base-multilingual-uncased on these examples (see training report in Appendix H), we tested it for accuracy against the classification data we set aside for benchmarking purposes (1,000 rows), on which it achieved an overall accuracy of 97.8%  (Table 3)  . Table 3: Benchmarking results for our topic-classification model fine-tuned from bert-base-multilingual-uncased Total rows Matches Mismatches Average confidence (all) Standard deviation Average confidence (mismatches) Standard deviation Average confidence (matches) Standard deviation 1000 22 0.991 0.056 0.825 0.240 0. 0.036 As part of that process, we also fine-tuned and tested google-bert/bert-base-multilingual-cased and FacebookAI/xlm-roberta-large (Conneau et al., 2020), which respectively achieved 97.1% and 95.1% accuracy against our benchmark and helped confirm our initial choice. Finally, we ran our fine-tuned classification model against the entire collection in order to assign high-level topic to each volume, resulting in the collection-level classification illustrated in Table 4. Table 4: Results of the topic classification experiment. Topic classification Total volumes % of collection LANGUAGE AND LITERATURE LAW 255,665 139,212 PHILOSOPHY. PSYCHOLOGY. RELIGION 124,617 SCIENCE SOCIAL SCIENCES AGRICULTURE AUXILIARY SCIENCES OF HISTORY MEDICINE HISTORY OF THE AMERICAS POLITICAL SCIENCE GEOGRAPHY. ANTHROPOLOGY. RECREATION EDUCATION FINE ARTS TECHNOLOGY 120,181 54,865 39,770 36,811 34,571 29, 29,279 25,386 24,602 23,945 18,217 23.76% 12.94% 11.58% 11.17% 5.10% 3.70% 3.42% 3.21% 2.73% 2.72% 2.36% 2.29% 2.23% 1.69% 14 MUSIC AND BOOKS ON MUSIC 14,150 1.32% WORLD HISTORY AND HISTORY OF EUROPE, ASIA, AFRICA, AUSTRALIA, NEW ZEALAND, ETC. MILITARY SCIENCE GENERAL WORKS BIBLIOGRAPHY. LIBRARY SCIENCE. INFORMATION RESOURCES (GENERAL) NAVAL SCIENCE 7,839 7, 7,225 5,880 5,482 0.73% 0.69% 0.67% 0.55% 0.51% Out of 1,075,899 records, 1,004,511 items (93.36% of the collection) could be labeled. The rest of the collection did not have sufficient metadata available, and was therefore set aside. While not clearly indicative of the accuracy of the classification performed by the model, we collected and analyzed the confidence score of each prediction, which we averaged for each category as way to control for the models consistency across the classification system (Figure 9). We observed spread ranging from 0.82 (GENERAL WORKS) to 0.95 (MEDICINE), which does not appear to directly correlate to the distribution of topics within the training set. Figure 9: Bar chart. Average confidence score of our fine-tuned topic classification model, by topic. Error bars were omitted for clarity: standard deviation across data points ranges from 0.14 to 0.20, with an average of 0.19. The results of this experiment suggest an important concentration of volumes on the topics of LANGUAGE AND LITERATURE, LAW, PHILOSOPHY. PSYCHOLOGY. RELIGION and SCIENCE. While this ML-assisted topic classification can be helpful, we encourage users to consider the following when using it to filter the collection: Benchmarking results and confidence scores are not indicative of the exactitude of the classification performed by the model. Instead, the benchmarking results reflect how well the model learned to 15 reproduce classification patterns from the existing classification data. The confidence scores reflect how confident the model was in its predictions. The results of this topic classification experiment are intended to provide general overview of topical coverage at collection level, and have not been reviewed by librarians at volume level."
        },
        {
            "title": "4.6.1 Methodology",
            "content": "While research shows that deduplicating LLM pre-training data improves model performance (Lee et al., 2022; Abbas et al., 2023), there is no one-size-fits-all when it comes to deduplication strategies. Instead, they should be tailored to the nature of the underlying data and target model behaviors (Albalak et al., 2024). As suchand because we anticipate this collection to have variety of use caseswe have chosen to focus our efforts on non-destructive, collection-level deduplication. Our goal with this experiment was therefore to identify near-duplicate OCR-extracted texts. For that purpose, and because this deduplication process was based on text similarity, we considered different editions of the same book to be duplicates if their text was nearly identical. This necessitated finding technique flexible enough to account for variations that result from digitizing issues, but focused enough to avoid considering two separate versions of given book with clear additions as near-duplicates. We first generated locality-sensitive hashes for every single OCR-extracted text in the collection. Specifically, we used Python implementation18 of the Simhash algorithm (Charikar, 2002) and grouped together volumes with identical hashes. Overlap in these hashes suggested the presence of near duplicate OCR-extracted texts. While often used for web content (Manku, Jain and Das Sarma, 2007), research suggests that Simhash can effectively be used on the OCR-extracted text of books (Vladimir et al., 2015). Through trial and error, we identified that using 7-character-long shingles yielded the overall lowest level of false positives for that collection. We then used series of heuristics to eliminate as many false positives as possible. In that context, we considered as false positives volumes in series of detected near-duplicates that: Had different volume-level detected language. Had 15% or more difference in continuous character count. In that context, the continuous character count of volume is the total number of characters in its OCR-extracted text, excluding whitespaces, line-breaks, and hyphenations. Removing these characters from the count helps to account for identical texts with slightly different layouts (e.g: the exact same text in two different print formats). Throughout the design and implementation of this experiment, we performed accuracy control by manually reviewing series of 100 randomly selected groups of near-duplicates. The last manual validation we performed yielded 97% accuracy rate. 18 https://github.com/1e0ng/simhash"
        },
        {
            "title": "4.6.2 Results",
            "content": "Out of 1,075,899 records: We generated 1,004,681 hashes. This is more than our total of volumes with OCR-extracted text as we chose to generate simhashes for the volumes with less than 100 o200k_base tokens of text. We identified group of 73,797 texts with at least 1 near-duplicate. Out of that group, we found 32,431 unique texts. The above figures therefore suggest that 41,366 volumes (3.84% of the collection) are potential near-duplicates that should be set aside in certain contexts. While we carefully designed and tested this deduplication pipeline, we chose not to use it to exclude volumes from the dataset released alongside this technical report. Instead, we elected to list the barcodes of likely near-duplicates as part of the dataset itself so users can make their own assessments in accordance with their goals."
        },
        {
            "title": "4.7.1 Methodology",
            "content": "The performance of LLMs trained on OCR-extracted texts can suffer from the presence of artifacts resulting from, for example, the misinterpretation of characters (Todorov and Colavizza, 2022). Being able to measure the prevalence of these artifacts in corpus can help inform assessments of its underlying quality for training purposes. To perform this analysis we collected and compared two different OCR quality metrics: primary volume-level OCR quality score, provided by GRIN. secondary volume-level OCR quality score, which we computed by running full OCR-extracted texts against PleiAIs OCRoscope19. This Python software library uses CLD220 in order to assess whether text chunks are likely to be valid text before returning an overall score (Langlais et al., 2025). The underlying technology used by this library is limiting factor; CLD2s language support is limited to 80 languages which potentially reduces the effectiveness of OCRoscope on texts in languages that are not supported."
        },
        {
            "title": "4.7.2 Results",
            "content": "Out of 1,075,899 records: 70,255 did not have Google-provided OCR score. This is 667 less than the total of volumes with no archives. 71,260 could not be assessed using OCROscope. This is 338 more than the total of volumes with no archives. The average Google-provided OCR score is 88.38 with standard deviation of 12.31. The average OCRoscope OCR score is 88.16 with standard deviation of 16.16. 19 https://github.com/Pleias/OCRoscope 20 https://github.com/CLD2Owners/cld2 17 These collection-level averages are comparable, although higher standard deviation of OCRoscope scores suggest more variability. Plotting per-decade averages gives additional insight as to how these scores compare (Figure 10). Figure 10: Line chart. Comparison of Google Books and OCRoscope OCR quality score averages over time. Averages aggregated by decade. This plot focuses on the 1750 to 1990 period, which is the lengthiest continuous period of time with over 1,000 volumes per decade. The curves indicate 10 point difference between the two metrics for both the 18th and 20th century, which we were not able to fully interpret. Error bars were omitted for clarity. Standard deviation for Google Books-provided scores ranged from 8.6 to 15.1 across data points, with an average of 11.35. Standard deviation for OCROscope-computed scores ranged from 8.5 to 23.14, with an average of 15.9. These scores indicate that the collection mostly contains OCR-extracted text with limited amounts of OCR artifacts, with clear outliers. We were able to confirm this trend through the manual inspection of random samples. We acknowledge that these scores are difficult to further evaluate for accuracy without referring to the original scans, which we intend to release at later date. Beyond collection-level analysis, we posit that combining these scores can be helpful when used at volume-level when trying to, for example, make decisions about further analysis or post-processing."
        },
        {
            "title": "4.8.1 Methodology",
            "content": "Filtering and selecting texts suitable for the purpose of machine learning (ML) experiment requires the use of metrics which can inform users about the underlying nature and quality of these texts. To that end, we chose to analyze the OCR-extracted text of each volume in order to collect and share the following high-level metrics: 18 Total and unique counts at word, bigram, trigram and sentence level. Text segmentation was performed using the Polyglot Python library21. Type-token ratios at word, bigram, trigram and sentence level. Research suggests that the complexity of the texts used to train model can affect its performance (Agrawal and Singh, 2023). Average sentence length (in characters). This metric may be used as secondary text complexity or text quality metric. An approximate tokenizability score. This score, ranging from 0.0 to 100.0, indicates how efficiently o200k_base can encode this text. Specifically, it measures how close to 1.25 tokens per word the text is, rough approximation of the tokenizers average compression level Character count and continuous character count. For high-level filtering purposes. These metrics were selected for their simplicity and genericity in order to help provide insight on most of the texts available in the collection, regardless of their language or nature."
        },
        {
            "title": "4.8.2 Results",
            "content": "Combining these metrics can be helpful in filtering the collection when selecting texts in the context of an ML/NLP experiment. Furthermore, we hypothesize that some of these metrics can be used to detect the presence of scanning or OCR issues that would otherwise be difficult to identify. For example, low tokenizability score for text in language that is well supported by o200k_base may suggest that the OCR-extracted text is partially unusable in its current form. Indeed, filtering records primarily written in English with tokenizability score under 30.0 surfaces volumes containing mainly tables, graphs, and music sheets, which all proved challenging to capture and transcribe in the current state of this collections OCR (example in Figures 11 and 12). Using this metric in combination with the volume-level average sentence length may further help identify such cases. Figure 11: Screenshot. Example of document from the collection consisting mainly of tables and graphs. The vast majority of the pages in the volume are in the format illustrated here, which the tokenizability score reflects. As seen from HathiTrusts viewer for record hvd.32044088771472. 21 https://github.com/aboSamoor/polyglot 19 Figure 12: Screenshot. Top, middle and end section of the resulting OCR-extracted text for the document presented in Figure 11. This text would likely need further processing (or re-OCR) before it could be used. Conversely, the very nature of some of these metrics, and in particular those concerned with lexical complexity, can be misleading when used across contexts and languages (Oh and Pellegrino, 2022). As such we have chosen not to use them to evaluate the underlying quality of the texts at collection level. Instead, we elected to: Use them as basis of comparison for measuring the impact of post-processing (Section 4.9) and Provide them as is as part of the dataset we released."
        },
        {
            "title": "4.9.1 Methodology",
            "content": "While the word-level quality of the OCR-extracted text appears to be generally satisfactory, we observed clear limitations. Most of these limitations are inherent to the semantic and positional decontextualization that comes with exporting OCR data as plain text. As illustrated in figures 11 and 12, words extracted from complex layouts such as tables, maps, illustrations, etc. are difficult to represent as plain text in coherent way, even when transcribed accurately. Simpler prose can also present significant challenges; in attempting to faithfully retranscribe the contents of scanned page, OCR pipelines generally include line breaks and hyphenations that mirror the source content layout. This can limit the direct usability of the text in ML/NLP contexts. More problematically, we observed that page numbers, running heads and headings were often commingled with the rest of the text. As first step toward improving the usability of OCR-extracted text, we developed post-processing pipeline focused on addressing this category of issues. More specifically, we: Trained static embedding model to detect the type of each OCR-extracted text line (types are outlined in Table 5). Used that coarse classification in addition to heuristics to reassemble the OCR-extracted text. The goal of this process was to offer an alternative text output that improves machine usability for ML/NLP use cases alongside overall readability. We provide this output alongside the original OCR-extracted text exported from Google Books. Line-level type detection We hypothesized that, since the OCR-extracted text of this collection is often segmented into short lines that directly map to single type, it would be possible to get signal on the nature of each line by: Using text generation model to generate line type detection training dataset. Fine-tuning text similarity model as classifier to perform that detection at scale. We also posited that, since this signal would be used in combination with text-based heuristics, it did not need to be strong for the prediction to be useful. Finally, we chose to limit this post-processing experiment to the 5 most common languages in the corpus (English, German, French, Italian, and Spanish accounting for 81% of the collection), and to only process the OCR extracted text from books with no known copyright (Section 5). We first used microsoft/phi-4 (14b) (Abdin et al., 2024) to generate the training dataset for this experiment. Using the prompt described in Appendix I, we labeled individual lines from sample pages by presenting them to an 8-bit quantized version of Phi-4. For each item to annotate, we provided the model with the current, previous, and next line, as well as positional information. We used Ollama22 to perform inference on single A6000 GPU. The model's temperature was set to 0.0. 235,168 OCR lines from 5,000 randomly sampled pages were annotated and 10% of these samples were set aside for benchmarking purposes. Table 5: Generated OCR line type detection dataset, train split Line type Samples % of total samples PARAGRAPH_CHUNK 94,847 LOOSE_SENTENCE_OR_LIST_ITEM 56,522 NOISE_OR_BROKEN_TEXT HEADING_OR_TITLE PAGE_NUMBER SEPARATOR PARAGRAPH_END RUNNING_HEAD UNKNOWN 27,629 19,048 7,344 3, 974 664 6 45.01% 26.82% 13.11% 9.04% 3.49% 1.75% 0.46% 0.32% 0.00% The resulting training dataset was, predictably, somewhat imbalanced. While some of this imbalance is due to the nature of the documents we processed (e.g: there is likely only one running head per page), the rest is likely the result of mistakes made by the model in its classification. However, we collected enough samples for the categories we assessed to be the most critical to our core goals: assembling sentences and paragraphs, separating headings, and reducing noise  (Table 5)  . 22 https://github.com/ollama/ollama 21 We then used Model2Vec23 to distill sentence-transformers/LaBSE (Feng et al., 2022; Reimers and Gurevych, 2019; Reimers and Gurevych, 2020) as static embedding model which we fine-tuned as classifier. Distillation and fine-tuning (3 epochs) were performed on single Apple M4 MAX SoC, taking approximately one minute each. During both fine-tuning and inference, the model was provided with very little context about the line it needed to label, limited to positional information within the page and within the volume, as illustrated in Figure 13. <<12-45,5-456>> Hello world <<{PAGE NUMBER},{TOTAL PAGES}-{LINE NUMBER}-{TOTAL-LINES}>> TEXT Figure 13: Example. Input format for our OCR line type detection model. Each text chunk is prefixed with positional information. While static embedding models lack an attention mechanism (MinishLab, 2024) that would allow them to make informed use of that positional information, we reasoned that, with sufficient volume, the model could still learn patterns from this semi-structured data. The resulting model yielded 71% accuracy rate against our benchmarking data, which matched our previously described target for coarse signal. Inference and post-processing We then used the static embedding model we trained to help guide the post-processing of the OCR-extracted text. For each volume matching our criteria, our pipeline: Used the model we trained to get signal on the type of each OCR line. Used the resulting signal as well as positional, lexical, and punctuational information to make decision as to how the OCR line should be rendered. For example, lines detected as HEADING_OR_TITLE were wrapped in double line breaks, unless they were part of series, and PAGE_NUMBER lines were not considered as such if they were in the middle of page. Whenever possible, page numbers and running heads were removed. The details of this processing step can be found on our GitHub repository24. Inference and post-processing was run on single Apple M4 MAX SoC and took approximately 5 days to complete. Inference-related statistics were collected in the process. Finally, in order to measure the effects of these transformations on the resulting texts, we chose to perform the text analysis described in Section 4.8 on the post-processed OCR-extracted text."
        },
        {
            "title": "4.9.2 Results",
            "content": "859,999 volumes were processed by our pipeline for total of 335 million pages. In the process, the static embedding model we trained performed detections on 18,405,607,403 OCR lines. Breaking these detections down by type  (Table 6)  confirms some of the patterns we observed during training and testing. Indeed, the model appears to detect PARAGRAPH_CHUNK, LOOSE_SENTENCE_OR_LIST_ITEM, NOISE_OR_BROKEN_TEXT and HEADING_OR_TITLE in proportions that match our overall expectations for set of 335 million pages of text extracted mainly from books. Conversely, the model appears to largely underperform at detecting PAGE_NUMBER and RUNNING_HEAD lines, with little over 10.5 million detections 23 https://github.com/MinishLab/model2vec 24 https://github.com/instdin/institutional-books-1-pipeline 22 for both types combined. More surprising is the total of lines detected as UNKNOWN, because of how underrepresented this type was in the models training set. Overall, these numbers matched our expectations for coarse signal, and we were able to use these OCR line type detections as signal in our post-processing pipeline in addition to positional, lexical, and punctuational information. Table 6: Summary of the line-type detection process. Out of 859,999 volumes. Detected line type Number of lines % of total detections PARAGRAPH_CHUNK 9,276,300,158 LOOSE_SENTENCE_OR_LIST_ITEM 4,570,972,627 NOISE_OR_BROKEN_TEXT 2,397,121, HEADING_OR_TITLE UNKNOWN SEPARATOR PAGE_NUMBER RUNNING_HEAD PARAGRAPH_END 1,371,820,357 740,229,355 37,576,126 7,022,568 4,564,506 50.40% 24.83% 13.02% 7.45% 4.02% 0.20% 0.04% 0.02% 0.00% Analyzing the resulting post-processed text and comparing it to the original using the text analysis methods described in Section 4.8 revealed two distinct patterns (Figures 14, 15, 16 and 17). The first is that the average o200k_base tokenizability score (see Section 4.8.1) is consistently higher for the post-processed texts. Performing this analysis by grouping texts by language shows an average increase of 4.6 points, while doing the same comparison by grouping texts by likely decade of publication shows an average increase of 6.1 points. The second pattern is that the average detected sentence length (in characters) of the post-processed text is generally lower than that of the original OCR-extracted text. Performing this analysis by grouping texts by language shows that French is an outlier in that regard (+31 characters) while the 4 other languages we processed (English, German, Italian and Spanish) showed an average reduction of 46 characters. 23 Figure 14: Bar plot. Average o200k_base tokenizability score for OCR-extracted texts and their post-processed counterparts, grouped by language. Error bars were omitted for clarity. Standard deviation across data points for the source texts ranged from 3.6 to 7.3 with an average of 4.75, and ranged from 2.8 to 4.8 with an average of 3.9 for the post-processed texts. Figure 15: Line chart. Average o200k_base tokenizability score for OCR-extracted texts and their post-processed counterparts, grouped by likely decade of publication. Decades included: 1750 to 1990. Error bars were omitted for clarity. Standard deviation across data points for the source texts ranged from 5.6 to 10.3 with an average of 8.9, and ranged from 5.3 to 10.9 with an average of 8.3 for the post-processed texts. Figure 16: Bar plot. Average sentence length for OCR-extracted texts and their post-processed counterparts, grouped by language. Standard deviation across data points ranged from 77 to 159 with an average of 110 for the source texts, and ranged from 53 to 87 with an average of 68 for the post-processed texts. Figure 17: Line chart. Average sentence length for OCR-extracted texts and their post-processed counterparts, grouped by likely decade of publication. Decades included: 1750 to 1990. Standard deviation across data points for the source texts is highly variable, ranging from 71 to 1310 with an average of 158. It ranged from 45 to 207 with an average of 76 for the post-processed texts. We hypothesize that these metrics, while varying in accuracy, indicate that post-processing these texts made them easier to work with in certain ML/NLP contexts. Because very little text was removed in the process (-0.97% characters based on compared continuous character counts), the notable change in average sentence length and o200k_base tokenizability scores suggest possible reduction in signals that could confuse sentence segmenter or make pre-trained BPE tokenizer (Sennrich, Haddow and Birch, 2016; 24 Gage, 1994) less efficient. In particular, the hyphenations removed in the process of reassembling paragraphs, as well as the partial removal of page numbers and running heads, may have helped in that regard. The very high variations in detected average sentence lengths in the original OCR-extracted text compared to its post-processed counterpart (figure 17) might be an additional clue pointing in that direction. For example, the text analysis data of the source texts shows that 3,001 volumes, primarily written in English, have an average detected sentence length of at least 500 characters. That figure decreased to 480 for the post-processed text. Further analysis is needed to confirm this trend and we encourage users to compare all available metrics, including their own, when making decisions about their use of this collection. We chose to provide both the original and post-processed OCR-extracted texts as part of the dataset released alongside this technical report, as well as the text analysis metrics for both versions, so that: This post-processing, which is minimally but partially destructive, doesnt replace the original, full OCR-extracted text. Users can make decision on which source to use based on their needs and own evaluation."
        },
        {
            "title": "5.1 Methodology",
            "content": "In order to determine the copyright status of the volumes present in the collection, we used the API provided by the HathiTrust Digital Library25 to match records for individual volumes and retrieve their current rights determination status, as inferred through their right clearances processes. Because HathiTrust preserves copy of Harvard Librarys Google Books collection and uses the collections original barcodes as part of their htid identifier, matching these records proved as trivial as prefixing each barcode with collection code (hvd)."
        },
        {
            "title": "5.2 Results",
            "content": "Out of 1,075,899 records, we were able to retrieve rights determination data for 1,004,497 volumes (93.36% of the collection) from HathiTrusts API. 983,510 of these volumes (91,41% of the collection) had one of the following status: pd, pdus or cc-zero (Table 7, full breakdown in Appendix J). Table 7: HathiTrust rights status distribution across the collection. HathiTrust status Total volumes % of collection PD, PDUS, CC-ZERO 983,510 91.41% Unknown status Known copyright Other statuses 75, 16,902 134 7.00% 1.57% 0.01% The information retrieved through this process was used to help determine what part of the collection has no known copyright, and could therefore be included in the dataset released along this technical report. After filtering out volumes with no text (Section 4.2), 983,004 volumes were included in the dataset, for total of 242B o200k_base tokens. 25 https://www.hathitrust.org/member-libraries/resources-for-librarians/data-resources/bibliographic-api/"
        },
        {
            "title": "5.3 Rights determination statement",
            "content": "We respect the intellectual property rights of authors, publishers, and other rights holders. While we have taken deliberate steps to include only those volumes for which there is no known copyright restriction, specifically those identified by the HathiTrust Digital Library with status of public domain, public domain in the United States, or CC-Zero, copyright determinations are complex and context-dependent, and occasionally subject to error. While this is relatively low risk, some volumes in this dataset may be in the public domain in the United States but still subject to copyright or other rights protections in other jurisdictions. Additionally, the absence of an explicit copyright claim or rights status does not guarantee that work is in the public domain, either in the U.S. or abroad. Information about the copyright status of individual volumes is provided on good-faith basis and reflects available data at the time of determination, but we cannot guarantee its completeness or accuracy. Users of this dataset will be solely responsible for making independent legal assessments about how and where they use the materials. Some uses of materials may also be restricted by trademark, privacy, publicity rights, or other such rights or restrictions. It is the user's sole responsibility to consider the possibility that such rights or restrictions may be involved and to secure any needed permissions. If any rights holder believes that work included in this release is misidentified or improperly included, we welcome contact and will promptly review any concerns. Our goal is to provide broad public access while maintaining respect for intellectual property rights and ensuring responsible data stewardship."
        },
        {
            "title": "6 Discussion and future directions",
            "content": "In building, documenting, and analyzing dataset at this scale with the primary goal of gaining insight and improving usability, we made several decisions that shaped our work. First, we wanted to release the dataset to the community in less time than it would take to come close to complete analysis. We instead approached our version of the dataset as launch pad for future iteration and use that could seed an ongoing collaborative publishing process. Second, we observed that we did not require access to extensive compute power to achieve this level of data refinement, so long as we were willing to creatively approach the data. Even as our compute capacity may expand, our mission requires us to heavily weight reproducibility and sustainability in our work. This will keep us grounded in set of practices sometimes referred to as frugal computing (Vanderbauwhede, 2023), even as we will experiment with higher resource methods to better understand advances in the field. While we believe this is worthwhile trade-off, it is an important element for understanding the processing choices we made. Third, we are making conscious choice to prioritize accountability to and public roadmap for the underlying source material in our analysis and outputs, as we believe this will yield the most productive data across commercial, academic, and public uses. This led to decision to, for example, preserve potential duplicates within the collection without the capacity to manually confirm every result. Looking beyond the publication of this dataset, we will expand our work across several key areas, only one of which is engineering-centric: data processing, collaboration across library and AI spheres, and the establishment of structure for this work at scale. 26 First, we plan to conduct additional analysis of this dataset to expand its utility and refine processes for future work. This includes an export of the existing OCR as fully-structured text, building on the OCR post-processing work we outlined here, to make the dataset more usable for humans and machines alike. We think this is useful for both machine comprehension of individual documents, and also for traditional library use cases including more specific search and more legible OCR-extracted text for individual document review. In collaboration with libraries and the AI community we are also interested in exploring the potential for finer application of topic classification for this dataset and other large-scale datasets we release, building on the work we began here. We further plan to broaden our exploration of useful and responsible metadata to provide richer understanding of both individual and collective texts, beginning with our upcoming work on newspaper dataset with Boston Public Library. Finally, we see the potential to extract, describe, and release dataset of images found within the raw page scans to support multimodal model training. Second, we are in the process of establishing diverse collaborations to expand this dataset, improve our understanding of its contents, and sharpen our processing decisions in the future. Harvard Library is one of many Google Books partners, and we are in conversation with others who want to increase access to their scanned works. We hope the work initiated in the making of this dataset will be the beginning of process that makes millions more books accessible to the public for variety of uses. We also plan to partner with fellow AI research labs to evaluate how the dataset impacts model outputs. We will work with researchers in digital humanities to understand how this and other datasets from knowledge institutions can best support their work, both by making more cultural heritage material available for variety of uses and also by providing high quality texts at scale to use as part of their own ML/NLP research. We will further explore ways to expand our use of librarian-generated metadata through the use of HathiTrust data and those from other organizations that make catalog records available at scale. Finally, we envision this collaborative publishing process growing into an organic institutional commons that is cultivated by the community, incorporating improvements from the AI and research communities back into source datasets for collective benefit. Governed through the efforts of cross-disciplinary maintainers who guide standards and practices, such commons would balance the need for large scale training data with firm commitment to data integrity and stewardship by collecting institutions."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors of this technical report would like to thank: Harvard Library for allowing us the opportunity to work with this unique collection. The team at the Library Innovation Lab at Harvard Law School Library, which initially incubated and significantly contributed to the success of this project. In particular, we would like to thank Ben Steinberg for providing the technical infrastructure that made most of these experiments possible. HuggingFace, and in particular Daniel Van Strien, Yacine Jernite and Clémentine Fourrier, who provided resources (compute and storage credits), time and expertise which helped with the preparation and release of this dataset. HathiTrust, and in particular Mike Furlough, Jennifer Vinopal, Kristina Hall, Janet Swatcheno, and Aaron Elkiss, for their guidance. The team and leadership at Google Books without whom this work would not be possible, and who assisted us in retrieving volumes and metadata and cleared the way for their release. 27 This work was supported by unrestricted gifts from Microsoft and OpenAI."
        },
        {
            "title": "Harmful Language and Content in this Dataset",
            "content": "This dataset is collection of historical works that reflect the language, culture, and perspectives of their time. Users should be aware that some materials may contain language or portrayals that are outdated, offensive, or harmful today, such as racism, sexism, colonial attitudes, and other forms of discrimination. Some content may include inaccurate information, providing insight into historical contexts that existed at the time of writing. The text is maintained in its original form to retain contextual understanding and facilitate research efforts, but we encourage critical awareness and cultural sensitivity for the creators and/or subjects of the collection. These materials are offered as part of historical perspective, but should not be considered stand-alone research collection constructed to give balanced perspective on any topic."
        },
        {
            "title": "Harmful Language in Bibliographic Description",
            "content": "Metadata for this collection may contain language that is overtly or implicitly harmful, outdated, or biased, or may by omission fail to represent important perspectives. Metadata may contain language created decades ago. It is common practice within the field of library science to reuse descriptions provided from the creator of the materials. While in some instances this allows communities and individuals to represent their materials in their own words, unexamined use of this practice may mean that racist or other offensive terminologies appear in our description. We also use national standardized terms in our work that can be outdated and harmful. Note that terminology in historical materials and in library descriptions does not always match the language we currently understand to be preferred by members of the communities depicted. Furthermore, we acknowledge that the act of collecting materials is not always neutral, and the work of describing and classifying library materials is influenced by inherent personal, institutional, and societal biases. Outdated or offensive terminologies may be present in metadata such as subject headings, and harmful language or bias may be introduced by catalogers supplying titles and descriptions. In other cases, books themselves present racist, offensive or otherwise harmful viewpoints in titles or descriptions that are routinely transcribed by catalogers. Note: Some language in this statement was adopted from Harvard Librarys statement on Harmful Language in Library collections26. 26 https://library.harvard.edu/harmful-language-library-collections"
        },
        {
            "title": "Reference list",
            "content": "Abbas, A., Tirumala, K., Simig, D., Ganguli, S. and Morcos, A.S. (2023). SemDeDup: Data-efficient learning at web-scale through semantic deduplication. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2303.09540. Abdin, M., Aneja, J., Behl, H., Bubeck, S., Eldan, R., Gunasekar, S., Harrison, M., Hewett, R.J., Javaheripi, M., Kauffmann, P., Lee, J.R., Lee, Y.T., Li, Y., Liu, W., Mendes, Nguyen, A., Price, E., Rosa, de, Saarikivi, O. and Salim, A. (2024). Phi-4 Technical Report. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2412.08905. Agrawal, A. and Singh, S. (2023). Corpus complexity matters in pretraining language models. In: S. Moosavi, I. Gurevych, Y. Hou, G. Kim, Y.J. Kim, T. Schuster and A. Agrawal, eds., Proceedings of the Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP). [online] Association for Computational Linguistics, pp.257263. doi:https://doi.org/10.18653/v1/2023.sustainlp-1.20. Albalak, A., Elazar, Y., Xie, S.M., Longpre, S., Lambert, N., Wang, X., Muennighoff, N., Hou, B., Pan, L., Jeong, H., Raffel, C., Chang, S., Hashimoto, T. and Wang, W.Y. (2024). Survey on Data Selection for Language Models. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2402.16827. Baack, S., Biderman, S., Odrozek, K., Skowron, A. and Wolf, T. (2025). Towards Best Practices for Open Datasets for LLM Training. arXiv Preprint. [online] doi:https://doi.org/10.48550/arXiv.2501.08365. Chang, T.A., Arnett, C., Tu, Z. and Bergen, B. (2024). When is multilinguality curse? Language modeling for 250 highand low-resource languages. In: Y. Al-Onaizan, M. Bansal and Y.-N. Chen, eds., Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. [online] Association for Computational Linguistics, pp.40744096. doi:https://doi.org/10.18653/v1/2024.emnlp-main.236. Charikar, M.S. (2002). Similarity estimation techniques from rounding algorithms. In: Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of Computing. [online] Association for Computing Machinery, pp.380388. doi:https://doi.org/10.1145/509907.509965. Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L. and Stoyanov, V. (2020). Unsupervised cross-lingual representation learning at scale. In: D. Jurafsky, J. Chai, N. Schluter and J. Tetreault, eds., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. [online] Association for Computational Linguistics, pp.84408451. doi:https://doi.org/10.18653/v1/2020.acl-main.747. Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In: J. Burstein, C. Doran and T. Solorio, eds., Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). [online] Association for Computational Linguistics, pp.41714186. doi:https://doi.org/10.18653/v1/N19-1423. Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M. and Gardner, M. (2021). Documenting large webtext corpora: case study on the colossal clean crawled corpus. In: M.-F. 29 Moens, X. Huang, L. Specia and S.W. Yih, eds., Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. [online] Association for Computational Linguistics, pp.12861305. doi:https://doi.org/10.18653/v1/2021.emnlp-main.98. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A. and Rodriguez, A. (2024). The Llama 3 Herd of Models. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2407.21783. Feng, F., Yang, Y., Cer, D., Arivazhagan, N. and Wang, W. (2022). Language-agnostic BERT sentence embedding. In: S. Muresan, P. Nakov and A. Villavicencio, eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). [online] Association for Computational Linguistics, pp.878891. doi:https://doi.org/10.18653/v1/2022.acl-long.62. Gage, P. (1994). new algorithm for data compression. Users J., 12, pp.2338. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Anish Thite, Noa Nabeshima, Presser, S. and Leahy, C. (2021). The Pile: An 800GB Dataset of Diverse Text for Language Modeling. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2101.00027. Gessler, L. and Zeldes, A. (2022). MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2212.12510. Gordon, M.A., Duh, K. and Kaplan, J. (2021). Data and parameter scaling laws for neural machine translation. In: M.-F. Moens, X. Huang, L. Specia and S.W. Yih, eds., Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. [online] Association for Computational Linguistics, pp.59155922. doi:https://doi.org/10.18653/v1/2021.emnlp-main.478. Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C.C.T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl, H.S., Wang, X., Bubeck, S., Eldan, R., Kalai, A.T., Lee, Y.T. and Li, Y. (2023). Textbooks Are All You Need. arXiv Preprint. [online] doi:https://doi.org/10.48550/arXiv.2306.11644. Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J. and Amodei, D. (2020). Scaling laws for neural language models. arXiv Preprint. [online] doi:https://doi.org/10.48550/arXiv.2001.08361. Langlais, P.-C., Hinostroza, C.R., Nee, M., Arnett, C., Chizhov, P., Jones, E.K., Girard, I., Mach, D., Stasenko, A. and Yamshchikov, I.P. (2025). Common corpus: The largest collection of ethical data for LLM pre-training. arXiv Preprint. [online] doi:https://doi.org/10.48550/arXiv.2506.01732. Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C. and Carlini, N. (2022). Deduplicating training data makes language models better. In: S. Muresan, P. Nakov and A. Villavicencio, eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). [online] Association for Computational Linguistics, pp.84248445. doi:https://doi.org/10.18653/v1/2022.acl-long.577. 30 Li, T., Zhang, G., Do, Q.D., Yue, X. and Chen, W. (2024). Long-context LLMs Struggle with Long In-context Learning. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2404.02060. Liu, N.F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F. and Liang, P. (2024a). Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, [online] 12, pp.157173. doi:https://doi.org/10.1162/tacl_a_00638. Liu, X., Dong, P., Hu, X. and Chu, X. (2024b). LongGenBench: Long-context generation benchmark. In: Y. Al-Onaizan, M. Bansal and Y.-N. Chen, eds., Findings of the Association for Computational Linguistics: EMNLP 2024. [online] Association for Computational Linguistics, pp.865883. doi:https://doi.org/10.18653/v1/2024.findings-emnlp.48. Liu, Y., Cao, J., Liu, C., Ding, K. and Jin, L. (2024c). Datasets for Large Language Models: Comprehensive Survey. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2402.18041. Longpre, S., Mahari, R., Chen, A., Obeng-Marnu, N., Sileo, D., Brannon, W., Muennighoff, N., Khazam, N., Kabbara, J., Perisetla, K., Wu, X., Shippole, E., Bollacker, K., Wu, T., Villa, L., Pentland, S. and Hooker, S. (2024a). large-scale audit of dataset licensing and attribution in AI. Nature Machine Intelligence, [online] 6(8), pp.975987. doi:https://doi.org/10.1038/s42256-024-00878-8. Longpre, S., Yauney, G., Reif, E., Lee, K., Roberts, A., Zoph, B., Zhou, D., Wei, J., Robinson, K., Mimno, D. and Ippolito, D. (2024b). Pretrainers Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity. In: Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). [online] Association for Computational Linguistics. doi:https://doi.org/10.18653/v1/2024.naacl-long.179. Lund, B.D. and Agbaji, D.A. (2018). What Scheme Do We Prefer? An Examination of Preference Between Library of Congress and Dewey Decimal Classification Among U.S.-Based Academic Library Employees. KNOWLEDGE ORGANIZATION, [online] 45(5), pp.380392. doi:https://doi.org/10.5771/0943-7444-2018-5-380. MinishLab (2024). Model2Vec Introduction blogpost. Model2Vec: Distill Small Fast Model from any Sentence Transformer. Available at: https://minishlab.github.io/hf_blogpost/. Muennighoff, N., Rush, A., Barak, B., Scao, L., Tazi, N., Piktus, A., Pyysalo, S., Wolf, T. and Raffel, C.A. (2023). Scaling Data-Constrained Language Models. In: Advances in Neural Information Processing Systems 36 (NeurIPS 2023). [online] pp.5035850376. Available at: https://proceedings.neurips.cc/paper_files/paper/2023/hash/9d89448b63ce1e2e8dc7af72c984c196-Abstra ct-Conference.html. Oh, Y.M. and Pellegrino, F. (2022). Towards robust complexity indices in linguistic typology corpus-based assessment. Studies in Language. International Journal sponsored by the Foundation Foundations of Language, [online] 47(4). doi:https://doi.org/10.1075/sl.22034.oh. OpenAI (2023). GPT-4 Technical Report. arXiv Preprint. [online] doi:https://doi.org/10.48550/arXiv.2303.08774. 31 OpenAI (2024). GPT-4o system card. arXiv Preprint. [online] doi:https://doi.org/10.48550/arXiv.2410.21276. Padilla, T., Kettler, S., Varner, S. and Shorish, Y. (2023). Vancouver statement on collections as data. [online] doi:https://doi.org/10.5281/zenodo.8342171. Parmar, J., Prabhumoye, S., Jennings, J., Liu, B., Jhunjhunwala, A., Wang, Z., Patwary, M., Shoeybi, M. and Catanzaro, B. (2024). Data, data everywhere: guide for pretraining dataset construction. In: Y. Al-Onaizan, M. Bansal and Y.-N. Chen, eds., Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. [online] Association for Computational Linguistics, pp.1067110695. doi:https://doi.org/10.18653/v1/2024.emnlp-main.596. Penedo, G., Kydlíček, H., Ben allal, L., Lozhkov, A., Mitchell, M., Raffel, C., Werra, V. and Wolf, T. (2024). The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2406.17557. Reimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-Networks. In: K. Inui, J. Jiang, V. Ng and X. Wan, eds., Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). [online] Association for Computational Linguistics, pp.39823992. doi:https://doi.org/10.18653/v1/D19-1410. Reimers, N. and Gurevych, I. (2020). Making monolingual sentence embeddings multilingual using knowledge distillation. In: B. Webber, T. Cohn, Y. He and Y. Liu, eds., Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). [online] Association for Computational Linguistics, pp.45124525. doi:https://doi.org/10.18653/v1/2020.emnlp-main.365. Resnik, P. (1999). Mining the web for bilingual text. In: Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics. [online] Association for Computational Linguistics, pp.527534. doi:https://doi.org/10.3115/1034678.1034757. Sennrich, R., Haddow, B. and Birch, A. (2016). Neural machine translation of rare words with subword units. In: K. Erk and N.A. Smith, eds., Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). [online] Association for Computational Linguistics, pp.17151725. doi:https://doi.org/10.18653/v1/P16-1162. Sina, B.N. and Agrawal, A. (2024). What drives performance in multilingual language models? In: Y. Scherrer, T. Jauhiainen, N. Ljubešić, M. Zampieri, P. Nakov and J. Tiedemann, eds., Proceedings of the Eleventh Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial 2024). [online] Association for Computational Linguistics, pp.1627. doi:https://doi.org/10.18653/v1/2024.vardial-1.2. Manku, G.S., Jain, A. and Das Sarma, A. (2007). Detecting near-duplicates for web crawling. In: Proceedings of the 16th international conference on World Wide Web. [online] Association for Computing Machinery, pp.141150. doi:https://doi.org/10.1145/1242572.1242592. Thakur, A. (2024). AutoTrain: No-code training for state-of-the-art models. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2410.15735. 32 Todorov, K. and Colavizza, G. (2022). An Assessment of the Impact of OCR Noise on Language Models. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2202.00470. Vanderbauwhede, W. (2023). Frugal Computing -- On the need for low-carbon and sustainable computing and the path towards zero-carbon computing. arXiv Preprint. [online] doi:https://doi.org/10.48550/arxiv.2303.06642. Vladimir, K., Silic, M., Romic, N., Delac, G. and Srbljic, S. (2015). preliminary study on similarity-preserving digital book identifiers. In: K. Zervanou, van Erp and B. Alex, eds., Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH). [online] Association for Computational Linguistics, pp.7883. doi:https://doi.org/10.18653/v1/W15-3712. Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L.A., Anderson, K., Kohli, P., Coppin, B. and Huang, P.-S. (2021). Challenges in detoxifying language models. In: M.-F. Moens, X. Huang, L. Specia and S.W. Yih, eds., Findings of the Association for Computational Linguistics: EMNLP 2021. [online] Association for Computational Linguistics, pp.24472469. doi:https://doi.org/10.18653/v1/2021.findings-emnlp.210."
        },
        {
            "title": "Appendices",
            "content": "Appendix A: Dataset fields Suffix _src _gen _ext Table App. A1: Field suffixes glossary. Description \"From source\". This field's data comes from information we gathered from the collection itself. \"Generated\". This field's data was generated as part of our analysis / post-processing. \"External\". This field's data was pulled from an external source via records matching mechanism. Table App. A2: Row-level fields list. Field name barcode_src title_src Type String String author_src String date1_src String date2_src String date_types_src String Description Section The volume's barcode. Serves as primary key/identifier. Merge of all the title-related bibliographic metadata available for this volume. Merge of all the author name-related bibliographic metadata available for this volume. First available date for that volume. Described in date_types_src. May contain placeholder characters. See MARC 21 specification for details. Second available date for that volume. Describes the nature of date1_src and date2_src. See MARC 21 specification for details. page_count_src Int Page count for that volume. token_count_o200k_base_gen Int language_src String Total tokens for that volume's OCR-extracted text, as measured with o200k_base. ISO 639-3 code for the main language of this book, as expressed in the collection's bibliographic metadata. Converted from original 3 3 3 4.3 4.3 4. 4.2 4.2 4.4 34 language_gen String language_distribution_gen Dict topic_or_subject_src String topic_or_subject_gen String topic_or_subject_score_gen Float genre_or_form_src String general_note_src String ISO 639-2B for convenience. ISO 693-3 code for the main language of this book, as detected by our text-level language analysis of the OCR-extracted text. Distribution of the languages detected by our text-level language analysis. Only languages for which more than 1000 o200k_base tokens were detected in total were kept. Topic or subject information, as expressed in the collection's bibliographic metadata. Only available for (approximately) half of the collection. High-level \"topic\" assigned to this volume by our topic classification model. Inferred from existing metadata. One of the Library of Congress' Classification Outline first-level items. Confidence score returned by our topic classification model for this specific prediction. Genre or form information, as expressed in the collection's bibliographic metadata. Only available for (approximately) 10% of the collection. 4.4 4.4 4.5 4.5 4. 4.5 Additional notes about this specific volume in the collection's bibliographic metadata. 3 ocr_score_src Int (0-100) Primary OCR quality score, as 4. expressed in the collection's metadata. ocr_score_gen Int (0-100) Secondary OCR quality score, 4.7 likely_duplicates_barcodes_gen List text_analysis_gen Dict generated by using pleias/OCRoscope on the collection's OCR-extracted text. List of barcodes for which the OCR-extracted text is highly-similar to this volume's. High-level text analysis of the OCR-extracted text, both original and post-processed. 4.6 4. 35 identifiers_src Dict hathitrust_data_ext Dict List of bibliographic identifiers, as expressed in the collection's metadata. Rights determination data pulled from the Hathitrust API for this volume. 3 5 text_by_page_src List[String] Original OCR-extracted text for this 4. text_by_page_gen volume. List[String] Post-processed OCR-extracted text for this volume. Available for books in the following languages: eng, deu, fra, ita, spa (850K books). 4.9 Table App. A3: Fields nested under language_distribution_gen Field name Type Description languages List[String] List of ISO 693-3 codes. proportion List[Float] Sorted by prevalence. List of percentages. Sorted by prevalence. Table App. A4: Shape of the dictionaries nested under both fields of text_analysis_gen. Field name Type Description tokenizability_score Float (0.0-100.0) Measure of how close to 1.25 o200k_base char_count word_count word_count_unique Int Int Int token per word this text is. Total characters. Total detected words (language-aware tokenization). Total unique detected words. word_type_token_ratio Float (0.0-100.0) Lexical diversity at word level. bigram_count bigram_count_unique Int Int Total bigrams. Total unique bigrams. bigram_type_token_ratio Float (0.0-100.0) Lexical diversity at bigram level. trigram_count trigram_count_unique Int Int Total bigrams. Total unique bigrams. trigram_type_token_ratio Float (0.0-100.0) Lexical diversity at trigram level. sentence_count sentence_count_unique Int Int Total detected sentences. Total unique detected sentences. Note: Fields are identical for text_by_page_src and text_by_page_gen. 36 Table App. A4: Fields nested under identifiers_src Field name Type Description lccn List[String] List of Library of Congress Control Numbers, if available. isbn List[String] List of International Standard Book Numbers, if available. ocolc List[String] List of OCLC Control Numbers, if available. Table App. A5: Fields nested under hathitrust_data_ext Field name Type Description url String Permalink to that volume on Hathitrust. rights_code String Hathitrust's rights determination code. reason_code String last_check String Hathitrust's rights determination reason code. Date at which that information was pulled from the Hathitrust API. 37 Appendix B: Temporal coverage breakdown Table App.B1: Volumes by century (from: filtered metadata) Century Total volumes % of collection 1200 1300 1400 1500 1600 1800 1900 2000 1 1 129 751 13,232 470,468 243,977 1, 0.00% 0.00% 0.00% 0.01% 1.05% 1.23% 43.73% 22.68% 0.10% Table App.B2: Volumes by decade (from: filtered metadata). Decade Total volumes % of collection 1250 1320 1420 1470 1510 1520 1530 1540 1550 1570 1580 1590 1600 1610 1630 1640 1650 1660 1 1 1 4 1 2 6 7 13 17 22 52 51 53 40 57 97 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.01% 0.01% 0.01% 38 1680 1690 1700 1710 1720 1740 1750 1760 1770 1780 1800 1810 1820 1830 1840 1860 1870 1880 1890 1900 1920 1930 1940 1950 1960 1980 1990 2000 2010 88 148 348 312 524 633 1,215 1,240 1,761 2,947 3,424 8, 9,190 24,633 28,817 35,690 48,188 53, 65,779 88,436 107,367 131,338 66,737 22, 4,960 4,349 3,274 2,735 3,057 2, 2,493 895 148 0.01% 0.01% 0.01% 0.03% 0.03% 0.05% 0.06% 0.08% 0.11% 0.12% 0.16% 0.27% 0.32% 0.78% 0.85% 2.29% 2.68% 3.32% 4.48% 5.02% 6.11% 8.22% 9.98% 12.21% 6.20% 2.10% 0.46% 0.40% 0.30% 0.25% 0.28% 0.23% 0.23% 0.08% 0.01% Not listed: Decades with 0 volumes. 39 Appendix C: Language coverage breakdown Table App.C1: Volumes by languages, based on bibliographic metadata. Top 30. Language Total volumes % of collection eng deu fra ita spa lat rus nld por heb swe zho dan hun pol und ara ces ell nor isl jpn grc cym fin mul ota hye hrv srp 506,900 157,745 134,854 46, 29,368 21,102 15,359 12,928 7,294 6, 6,262 6,086 5,572 4,994 3,758 3, 3,337 3,099 2,715 2,360 2,340 1, 1,779 1,678 1,454 1,291 1,150 872 848 47.11% 14.66% 12.53% 4.36% 2.73% 1.96% 1.43% 1.20% 0.68% 0.62% 0.58% 0.57% 0.52% 0.46% 0.35% 0.34% 0.31% 0.29% 0.25% 0.22% 0.22% 0.17% 0.17% 0.16% 0.14% 0.12% 0.11% 0.08% 0.08% 0.08% 40 Table App.C2: Volumes by languages, based on detection. Top 30. Language Total volumes % of collection eng deu fra ita spa lat rus nld dan por swe ell heb cmn hun pol arb ces isl cym jpn oci prs fin san hye ydd bul srp ukr 500,900 159,496 137,620 46,878 29,369 21, 15,368 12,866 8,014 7,346 6,668 6, 6,482 6,479 4,976 3,794 3,506 3, 2,475 1,753 1,562 1,432 1,201 1, 1,046 981 860 825 749 46.56% 14.82% 12.79% 4.36% 2.73% 2.03% 1.43% 1.20% 0.74% 0.68% 0.62% 0.61% 0.60% 0.60% 0.46% 0.35% 0.33% 0.29% 0.23% 0.16% 0.15% 0.13% 0.11% 0.11% 0.10% 0.09% 0.08% 0.08% 0.07% 0.05% 41 Table App.C3: Total token counts (o200k_base) by detected language. Top 30. Volume-level token counts < 1000 for given language were discarded. Language Total detected tokens % of total detected tokens eng deu fra ita lat spa rus ell nld heb dan cmn sco por swe hun pol ces arb jpn bul oci ina cym pcm mxi und hye 105,918,942,360 41,803,724,013 33,852,308, 9,763,407,270 7,718,749,717 5,424,427,269 4,956,088,535 3,498,189,810 3,006,044, 2,376,672,753 2,042,386,683 1,986,174,170 1,817,812,828 1,412,080,527 1,407,369, 1,342,999,109 1,024,108,501 969,876,169 804,498,637 770,432,004 768,212, 703,373,679 651,697,782 607,416,029 538,714,730 537,472,573 440,463, 329,697,846 43.83% 17.30% 14.01% 4.04% 3.19% 2.24% 2.05% 1.45% 1.24% 0.98% 0.85% 0.82% 0.75% 0.58% 0.58% 0.56% 0.42% 0.40% 0.33% 0.32% 0.32% 0.29% 0.27% 0.25% 0.22% 0.22% 0.18% 0.14% Appendix D: Source to LCC topic classification mapping This mapping was used as part of our topic classification experiment in order to generate training dataset. We used it to match individual items from the existing topic classification (source) with first-level items from the Library of Congress Classification outline (target).Two typographical errors have been corrected in this version: single instance of Swedigh instead of Swedish, and another single instance of literarure instead of literature. Table App. D: Topic classification mapping. Target: LCC 1st level Source: Existing classification GENERAL WORKS PHILOSOPHY. PSYCHOLOGY. RELIGION Encyclopedias and dictionaries, Newspapers, Periodicals Philosophy, Theology, Logic, Psychology, Aesthetics, Ethics, Mythology, Rationalism, Judaism, Islam, Theosophy, Buddhism, Christianity AUXILIARY SCIENCES OF HISTORY Archaeology, Numismatics, Heraldry, Genealogy, WORLD HISTORY AND HISTORY OF EUROPE, ASIA, AFRICA, AUSTRALIA, NEW ZEALAND, ETC. HISTORY OF THE AMERICAS Biography World history Indians of South America, Indians of North America GEOGRAPHY. ANTHROPOLOGY. RECREATION Geography, Cartography, Anthropology, Folklore, Manners and customs, Oceanography, Atlases, Mathematical geography SOCIAL SCIENCES POLITICAL SCIENCE LAW Social sciences, Statistics, Commerce, Finance, Sociology, Socialism, Communism, Anarchism, Criminology Political science, Democracy, Local government, Municipal government, International relations, Representative government and representation Law, Civil law, Criminal law, Constitutional law, Commercial law, Maritime law, Administrative law, Military law, Mining law, Corporation law, Educational law and legislation, Labor laws and legislation, Railroad law, Fishery law and legislation, Banking law, Marriage law, Liquor laws, Insurance law, Customary law, Patent laws and legislation, Building laws, Press law, Emigration and immigration law EDUCATION Education, Textbooks MUSIC AND BOOKS ON MUSIC Music, Piano music, Music theory, Musical notation, Orchestral music 43 FINE ARTS LANGUAGE AND LITERATURE SCIENCE MEDICINE AGRICULTURE TECHNOLOGY MILITARY SCIENCE NAVAL SCIENCE BIBLIOGRAPHY. LIBRARY SCIENCE. INFORMATION RESOURCES (GENERAL) Architecture, Sculpture, Drawing, Painting, Decorative arts Philology, Classical philology, Oriental philology, Romance philology, Russian philology, Greek philology, Language and languages, English language, French language, German language, Latin language, Greek language, Hebrew language, Spanish language, Italian language, Arabic language, Sanskrit language, Chinese language, Indo-European languages, Russian language, Dutch language, Portuguese language, Swedish language, Irish language, Japanese language, Syriac language, Romance languages, Old Norse language, Literature, English literature, French literature, Italian literature, American literature, Russian literature, Spanish literature, Greek literature, Chinese literature, Latin literature, Polish literature, Comparative literature, Children's literature Science, Mathematics, Astronomy, Physics, Chemistry, Geology, Natural history, Biology, Botany, Zoology, Physiology, Human anatomy Medicine, Pathology, General Surgery, Ophthalmology, Gynecology, Obstetrics, Pediatrics, Dentistry, Dermatology, Therapeutics, Pharmacology, Pharmacy, Homeopathy Agriculture, Horticulture, Forests and forestry, Hunting Technology, Engineering, Civil engineering, Electrical engineering, Electric engineering, Mechanical engineering, Mining engineering, Hydraulic engineering, Steam engineering, Home economics Artillery, Military engineering, Infantry drill and tactics Naval art and science, Naval architecture, Shipbuilding, Marine engineering Library science, Bibliography, Paleography 44 Appendix E: Sample of the collections pre-existing topic/subject classification. Table App. E: 20 most common values in the collections pre-existing topic/subject classification. Topic classification Total books % of the collection Law reports, digests, etc 13, Law Science Education Botany Agriculture Medicine Theology Natural history English language Legislation World War, 1914-1918 Jews Plants Railroads Geography Geology Church history History French language 7,481 6,729 3,492 3,267 3, 2,936 2,745 2,526 2,357 2,238 2, 2,107 1,831 1,799 1,789 1,666 1, 1,598 1,595 1.29% 0.70% 0.63% 0.32% 0.30% 0.28% 0.27% 0.26% 0.23% 0.22% 0.21% 0.21% 0.20% 0.17% 0.17% 0.17% 0.15% 0.15% 0.15% 0.15% 45 Appendix F: Sample of the collections pre-existing genre/form classification. Table App. F: 20 most common values in the collections pre-existing genre/form classification. Genre/Form History Periodicals Criticism, interpretation, etc Computer network resources, Electronic journals Fiction History, Sources Dictionaries Biography Early works Bibliography Electronic journals, Periodicals, Computer network resources Biographies Sermons Poetry Commentaries Guidebooks Drama Fiction, History Court decisions and opinions Catalogs Fiction, Juvenile works Total books % of the collection 19,632 12,093 3,650 3,113 3, 2,500 2,140 2,101 1,807 1,588 1, 1,196 1,137 1,085 946 877 795 699 661 638 1.82% 1.12% 0.34% 0.29% 0.29% 0.23% 0.20% 0.20% 0.17% 0.15% 0.13% 0.11% 0.11% 0.10% 0.09% 0.08% 0.08% 0.07% 0.06% 0.06% 0.06% 46 Appendix G: Composition of the topic classification training sets After initially testing series of rebalancing mechanisms and observing worse performance, we chose not to rebalance these training sets. Table App. G: Composition of the topic classification models training sets. Target topic AGRICULTURE AUXILIARY SCIENCES OF HISTORY 1,831 Train rows Test rows Benchmarking rows BIBLIOGRAPHY. LIBRARY SCIENCE. INFORMATION RESOURCES (GENERAL) EDUCATION FINE ARTS GENERAL WORKS GEOGRAPHY. ANTHROPOLOGY. RECREATION HISTORY OF THE AMERICAS LANGUAGE AND LITERATURE LAW MEDICINE MILITARY SCIENCE MUSIC AND BOOKS ON MUSIC NAVAL SCIENCE PHILOSOPHY. PSYCHOLOGY. RELIGION POLITICAL SCIENCE SCIENCE SOCIAL SCIENCES TECHNOLOGY 3,799 416 3,296 1, 879 2,614 1,030 15,676 11,560 3, 110 1,389 221 7,666 1,395 116 35 177 75 57 57 1012 678 260 7 10 488 83 62 24 35 18 9 26 13 155 57 1 21 5 22 19,139 1148 228 3,165 250 55 33 12 WORLD HISTORY AND HISTORY OF EUROPE, ASIA, AFRICA, AUSTRALIA, NEW ZEALAND, ETC. 30 8 47 Appendix H: Topic classification model training report We fine-tuned google-bert/bert-base-multilingual-uncased for text classification using HuggingFace autotrain-advanced. The fine-tuning process, spanning 3 epochs, took less than 19 minutes to complete on 2 A6000 GPUs. Table App. H: Topic classification model fine-tuning validation metrics. loss f1_macro f1_micro 0.157407745718956 0.9613886456444749 0. f1_weighted 0.9693030681223207 precision_macro 0.9679892485977634 precision_micro 0. precision_weighted 0.9695713537396466 recall_macro 0.9560667596679707 recall_micro recall_weighted accuracy 0.9694 0.9694 0.9694 Figure App. H: Screenshot. Topic classification model fine-tuning learning curve, as seen in TensorBoards UI. Appendix I: OCR lines classification - Training dataset generation prompt The initial version of this prompt included two typographical errors which have since been corrected (one missing closing > for <context>, as well analyse instead of analyze). You are text classifier, helping with the post-processing of OCR text extracted from books. You will be given one or multiple text chunks to analyze as well as some contextual information. In this experiment, 1 chunk = 1 line extracted from plain text OCR export. ## Information will be structured as follows: - `<context>`: Information about the text excerpt, such as: the page number of the book this excerpt is from, the position of this chunk on the page, and the book's main language. - `<current>` The text chunk to analyze. - `<previous>` The text chunk that precedes the one to analyze, if any. - `<next>` The text chunk that follows the one to analyze, if any. ## Your role is: - To determine the TYPE of the text chunk in `<current>`. You should use all of the information available to help make that determination, not just the text in `<current>`. Carefully analyze all of the information you are given. - To return that TYPE, and nothing else. Your response MUST be one of the TYPES listed, it cannot be anything else. ## Possible values for TYPE: - UNKNOWN - NOISE_OR_BROKEN_TEXT - PAGE_NUMBER - RUNNING_HEAD - HEADING_OR_TITLE - PARAGRAPH_CHUNK - PARAGRAPH_END - LOOSE_SENTENCE_OR_LIST_ITEM - SEPARATOR Carefully analyze the information you are given to accurately determine the type of the text in `<current>`. Return TYPE and nothing else. <context>Page 12 of 320, Line 4 of 128</context> <previous>Lorem ipsum </previous> <current>dolor sit</current> <next>amet.</next> Figure App. I: Prompt used to generate the OCR chunk classification dataset, alongside an example of contextualized data chunk sent to the model alongside the prompt presented above. 49 Appendix J: Hathitrust Rights Determination Breakdown For the 1,004,497 for which rights determination data could be found on Hathitrust. Table App. J: Collection-level rights determination breakdown, HathiTrust data. HathiTrust status Total volumes % of collection pd pdus ic und cc-zero cc-by-4.0 cc-by-nc-nd-4. icus cc-by-nd-4.0 cc-by-nc-sa-4.0 cc-by-nc-4.0 786,964 196, 16,899 3,951 375 109 16 3 3 3 73.14% 18.23% 1.57% 0.37% 0.03% 0.01% 0.00% 0.00% 0.00% 0.00% 0.00%"
        }
    ],
    "affiliations": [
        "Harvard Law School Library",
        "Harvard Law School, Harvard School of Engineering and Applied Sciences, Harvard Kennedy School",
        "Harvard Library",
        "Institutional Data Initiative, Harvard Law School Library",
        "Library Innovation Lab, Harvard Law School Library"
    ]
}