{
    "paper_title": "Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control",
    "authors": [
        "Jannis Becktepe",
        "Aleksandra Franz",
        "Nils Thuerey",
        "Sebastian Peitz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym."
        },
        {
            "title": "Start",
            "content": "Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Jannis Becktepe 1 2 Aleksandra Franz 3 Nils Thuerey 3 Sebastian Peitz 1 2 6 2 0 2 1 2 ] . [ 1 5 1 0 5 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes scalable foundation for future research in learning-based flow control, and is available at https://github. com/safe-autonomous-systems/ fluidgym. 1. Introduction Active flow control (AFC) plays central role in wide range of real-world systems, such as aerodynamics (Batikh et al., 2017), energy harvesting (Barthelmie et al., 2009), nuclear fusion (Pironti & Walker, 2005), and reduction of turbulence (Jimenez, 2013). Europe, for instance, could save 106 tonnes of CO2 per year by reducing more than 20 drag on cars using AFC (Brunton & Noack, 2015). 1TU Dortmund University, Dortmund, Germany 2Lamarr Institute for Machine Learning and Artificial Intelligence, Dortmund, Germany 3Technical University Munich, Munich, Germany. Correspondence to: Jannis Becktepe <jannis.becktepe@tudortmund.de>. Preprint. January 22, 2026. 1 Figure 1. The four uncontrolled environment classes in FluidGym. However, manually designing control strategies is challenging due to the high dimensionality and inherent nonlinearities of such systems (Duriez et al., 2017). Recently, reinforcement learning (RL) has demonstrated strong potential for advancing AFC in complex systems, e.g., stabilizing the plasma in Tokamak reactor (Degrave et al., 2022). Despite its success, research in RL for flow control remains fragmented, and establishing clear state of the art is difficult for several reasons. Experimental setups vary widely across studies in terms of actuators, sensor placements, and physical parameter settings as well as RL algorithms and hyperparameters (Viquerat et al., 2022; Moslem et al., 2025). This results in inconsistent problem formulations that hinder direct comparisons. Moreover, insufficiently rigorous evaluation and the use of few random seeds increase statistical variance (Henderson et al., 2018; Agarwal et al., 2021). Existing benchmarks (see Table 1) have seen limited adoption for two main reasons. First, most rely on external computational fluid dynamics (CFD) solvers that must be installed, configured, and coupled to Python RL code through additional interfaces, which demands CFD expertise and creates brittle software stacks. Second, differentiability is either absent or limited to small subset of scenarios, which prevents end-to-end use of differentiable predictive control (DPC, Drgoˇna et al. (2022)) and recent differentiable RL methods that can accelerate training and outperform classical RL (Xing et al., 2025; Lagemann et al., 2025b). Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control To address these limitations, we introduce FluidGym, the first standalone, fully differentiable RL benchmark for AFC in incompressible flows. Building entirely on PyTorch (Ansel et al., 2024), FluidGym requires no external solver dependencies and seamlessly integrates with common RL interfaces such as Gymnasium (Towers et al., 2024) or PettingZoo (Terry et al., 2021) and algorithm frameworks like Stable-Baselines3 (Raffin et al., 2021) or TorchRL (Bou et al., 2023). As all simulations and control interfaces live in one Python package, users can install FluidGym via pip and immediately run experiments with standard RL libraries, without compiling or coupling external CFD codes. Being inherently end-to-end differentiable, FluidGym enables researchers to use gradient-based control methods alongside classical RL without any further modifications. Our benchmark provides diverse environments with consistent task definitions, supports single-agent (SARL) and multiagent (MARL) settings, spans three difficulty levels in 2D and 3D, and enables transfer-learning studies. In summary, our main contributions are (1) the first standalone, fully differentiable, plug-and-play benchmark for RL in AFC, implemented in single PyTorch codebase without external solver dependencies; (2) collection of standardized environment configurations spanning diverse 3D and MARL control tasks (see Figure 1); (3) an extensive experimental study covering all FluidGym environments and difficulty levels, including transfer-learning evaluations, amounting to over 16 GPU hours, all publicly available. 2. Background and Related Work RL for AFC Fluid flows are governed by the NavierStokes equations, set of nonlinear partial differential equations (PDEs) exhibiting highly complex behavior over wide range of scales both in space and time. Due to their inherent complexity, analytical solutions are infeasible without substantial simplifications. Computational fluid dynamics (CFD) has become standard approach to approximate solutions using spatial and temporal discretization (Ferziger et al., 2020). Such simulations, however, are computationally expensive and typically require specialized solvers such as OpenFOAM (Weller et al., 1998), FEniCS (Alnæs et al., 2015), or FLEXI (Krais et al., 2021). In many applications, the goal is not only to simulate the flow but to manipulate it. Active flow control (AFC) uses actuation to influence fluid motion, e.g., to reduce aerodynamic drag (Nair et al., 2019). Classical AFC approaches have demonstrated notable successes, ranging from the re-laminarization of turbulent channel flows using adjointbased model-predictive control (MPC) (Bewley et al., 2001) to control of the separation bubble behind bluff body using evolutionary optimization strategies (Gautier et al., 2015). However, these methods often rely on simplified models or require full-state information and expensive online optimization, which limits their scalability to complex, nonlinear, or high-dimensional flow configurations. Reinforcement learning (see Appendix for an introduction to the basics and notation) has therefore emerged as compelling alternative and has been explored across variety of AFC problems, including drag reduction in bluff-body wakes (Rabault et al., 2019; Tokarev et al., 2020), turbulent channel-flow control (Guastoni et al., 2023), and heat-transfer enhancement (Beintema et al., 2020; Vignon et al., 2023). Several works have also studied multi-agent reinforcement learning (MARL, (Albrecht et al., 2024)) for wall turbulence modeling (Bae & Koumoutsakos, 2022), heat transfer enhancement (Beintema et al., 2020; Vasanth et al., 2024; Vignon et al., 2023; Markmann et al., 2025), and proposed convolutional RL for distributed control (Peitz et al., 2024). To avoid the high computational cost of CFD simulations, RL has also been used together with surrogate models (Werner & Peitz, 2024; Zolman et al., 2025). However, the research area faces challenges similar to those observed more broadly in machine learning for PDEs (McGreivy & Hakim, 2024). Evaluation practices vary widely. Many works compare learned policies only to uncontrolled baselines (Tokarev et al., 2020; Ren et al., 2021; Wang et al., 2022b; Vignon et al., 2023; Vasanth et al., 2024; Ren et al., 2024; Zhao et al., 2024; Suarez et al., 2025; Montal`a et al., 2025). RL episodes often start from the same initial state (Rabault et al., 2019; Vignon et al., 2023; Ren et al., 2024; Sonoda et al., 2023; Garcia et al., 2025), even though this choice can substantially affect the performance of policies (Guastoni et al., 2023). In several cases, test episodes reuse the same initial conditions used for training (Vignon et al., 2023; Vasanth et al., 2024), making generalization hard to assess. Reproducibility and statistical robustness is also limited: some works report single run without seeds (Guastoni et al., 2023; Sonoda et al., 2023), despite the fact that this is known source of variance in RL (Henderson et al., 2018; Agarwal et al., 2021). Finally, although the soft actor critic (SAC, Haarnoja et al. (2018)) algorithm often outperforms proximal policy optimization (PPO, Schulman et al. (2017)) on nonlinear continuous-control tasks (Abuduweili & Liu, 2023), more than 75% of AFC studies rely on PPO as surveyed by Moslem et al. (2025). RL for AFC Benchmarks Benchmark design is key to addressing the evaluation and reproducibility challenges outlined above. Several RL benchmarks for AFC exist, and their characteristics are stated in Table 1. However, existing efforts cover only parts of the AFC landscape and leave important gaps in accessibility, differentiability, RL methodologies, and dimensionality. 2 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Table 1. Overview of existing RL for AFC benchmarks in terms of external solver dependence, differentiability of all environments, multi-agent RL support, and 3D capabilities. BENCHMARK NO EXTERNAL SOLVER FULLY DIFFERENTIABLE MARL 3D DRLINFLUIDS (WANG ET AL., 2022A) DRLFOAM (WEINER & GEISE, 2022) DRLFLUENT (MAO ET AL., 2023) GYM-PRECICE (SHAMS & ELSHEIKH, 2023) HYDROGYM (LAGEMANN ET AL., 2025B) FLUIDGYM (OURS) General PDE control benchmarks, such as those proposed by Bhan et al. (2024); Zhang et al. (2024); Mouchamps et al. (2025), focus on low-dimensional or non-fluid systems and do not address the complexities of high-dimensional fluid flows. Several frameworks have attempted to bridge the gap between CFD solvers and RL algorithms (Pawar & Maulik, 2021; Kurz et al., 2022; Xiao et al., 2025). However, they introduce additional software layers for the coupling rather than standardized benchmark environments. DRLinFluids (Wang et al., 2022a) and drlFoam (Weiner & Geise, 2022) interface with OpenFOAM but are limited to 2D cases (e.g., flow past cylinder or fluidic pinball), while DRLFluent (Mao et al., 2023) couples RL with the commercial solver Fluent (ANSYS Inc., 2026), again focusing on 2D cylinder flows. Gym-preCICE (Shams & Elsheikh, 2023) uses the preCICE coupling library (Chourdakis et al., 2022) and includes 2D flow past cylinder. HydroGym (Lagemann et al., 2025b;a) provides collection of 2D and 3D flow scenarios, with individual environments depending on different solver backends: FEniCS for 2D simulations, and m-AIA (Institute of Aerodynamics, 2024) for 3D simulations. Only the two environments based on JAX (Bradbury et al., 2018) are differentiable. Limitations of Existing Benchmarks Existing AFC benchmarks share several limitations (see Table 1): (i) they typically depend on external CFD solvers (e.g., OpenFOAM, Fluent, FEniCS, m-AIA), which require complex and often brittle software pipelines and indirect coupling layers that hinder integration with Python RL libraries and complicate long-term maintenance; (ii) lack of differentiability, despite its potential for accelerating RL training (Xu et al., 2022; Xing et al., 2025; Lagemann et al., 2025b) and in DPC (Drgoˇna et al., 2022), (iii) limited support for multi-agent RL, despite its natural alignment with spatially distributed actuation; and (iv) predominantly 2D environments, which fail to capture essential 3D flow physics. To our knowledge, no existing benchmark simultaneously provides standalone implementation, uniform differentiability across all tasks, native multi-agent support, and high-fidelity 3D environments. 3. FluidGym: Overview Motivated by the limitations of existing work on RL for AFC and related benchmarks, FluidGym is designed around the following desiderata: (i) standardized, standalone, and easy-to-use RLCFD interface that runs entirely in Python without external CFD software, (ii) an end-to-end differentiable framework suitable for various control methodologies, (iii) inherent support of multi-agent control, and (iv) high- -fidelity 3D tasks. In the following, we outline the core design principles underlying our benchmark and describe how FluidGym fulfills these desiderata. 3.1. Architecture and Interaction Interface Figure 2 summarizes the architecture of FluidGym, which unifies CFD simulation and control under single, RLcentric interface. To meet desiderata (i) and (ii), FluidGym integrates the GPU-accelerated PICT solver (Franz et al. (2026); see Appendix B) with modular PyTorch (Ansel et al., 2024) interaction layer. Because the design runs entirely in PyTorch, environment stepping and backprop use the same autograd mechanisms as standard deep networks. Consequently, no external CFD software or coupling code is required, and environments are compatible with common RL libraries through lightweight API. The FluidEnv abstraction encapsulates all CFD computations and exposes standardized observation, action, and reward interfaces for both differentiable and classical RL methods. Finally, FluidGym scales to large experimental workloads via parallel execution of environments across multiple GPUs. Addressing desideratum (iii), the FluidEnv is implemented from the ground up with both single-agent and multi-agent RL in mind. Its interface provides standardized observation, action, and reward specifications for centralized or decentralized control. All environments are modular, enabling new tasks to be defined by specifying domain configuration and control logic. This design makes FluidGym an extensible platform for future research on RL for AFC. Finally, addressing desideratum (iv), our environments built on top of FluidEnv focus on state-of-the-art, high-fidelity 3D flow simulations (see Section 3.2). 3 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 2. Overview of FluidGym using the 2D RayleighBenard Convection (RBC) environment. The framework provides three modes of interaction: single-agent RL (SARL), multi-agent RL (MARL), and gradient-based methods. The action space consists of 12 heater actuators along the lower boundary. In SARL, single agent outputs the full action vector, whereas in MARL, each agent controls one actuator via local action. Local actions are internally aggregated and mapped to boundary actuation values via the transformation function Γ. Gray dots indicate virtual sensor locations: in SARL, the agent receives all measurements, while in MARL, each agent observes only the local subset around its assigned actuator (denoted by the window framed in purple). Modes of Interaction FluidGym supports three modes of interaction through its environment interfaces, which expose the FluidEnv via common RL environment interfaces, including Gymnasium (Towers et al., 2024), PettingZoo (Terry et al., 2021), Stable-Baselines3 (SB3, Raffin et al. (2021)), and TorchRL (Bou et al., 2023). First, in the single-agent RL (SARL) setting, single RL agent applies global action at at each control step and the environment returns global observation ot+1 and scalar reward rt+1. Secondly, in the multi-agent RL (MARL) configuration, multiple agents act simultaneously at different spatial locations in the domain. Each agent selects local action ai and receives local observation t . Reward functions in these settings are typically constructed of weighted sum of local and global properties of the domain. This interaction mode enables decentralized cooperation control strategies, where equivariance to translations allows us to deploy the same agent in all locations (Vasanth et al., 2024; Peitz et al., 2024). Lastly, in addition to standard RL, FluidGym supports gradient-based control methods by providing end-toend differentiability of the step() function with respect to the reward. This allows gradients to be backpropagated through FluidGym to the policy parameters. and individual reward lower boundary of the domain. In the SARL scenario, single agent outputs the complete action vector, assigning temperature intensities to all actuators. In contrast, in the MARL configuration, each agent controls one individual actor via its local action. Internally, the environment interface aggregates local actions into global action vector. The resulting action vector is then transformed into physically meaningful boundary condition values via the control mapping function Γ, in this case normalization and spatial smoothing. Observations are constructed from virtual sensor measurements indicated by the gray dots in the figure. Training and Evaluation Protocol Many prior works lack standardized training and evaluation procedures for RL in AFC, with studies differing widely in how many and which initial conditions they use. FluidGym addresses this by providing unified protocol based on three predefined splits (train, val, and test) each containing ten randomly generated initial domains. On first use, initial domains are automatically downloaded and cached locally. Each env.reset() applies random perturbations and random rollout steps; with consistent RNG seeding, this creates standardized and reproducible train/val/test protocol. Example: 2D RayleighBenard Convection Possible interaction modes are visualized in Figure 2 using the 2D Rayleigh-Benard Convection (RBC) environment as an example. Here, the action space consists of 12 scalar control inputs corresponding to heater elements placed along the 3.2. Benchmark Environments FluidGym provides diverse set of environments, each introducing distinct challenges for learning well-performing RL policies. Formal SARL and MARL environment defini4 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Table 2. Overview of the FluidGym environments, listing control objectives, observation and action dimensions, SARL/MARL support, and mean per-step runtime across all difficulty levels on single NVIDIA A100 GPU. SARL is omitted for environments with very large action spaces, where centralized control becomes impractical. For more details, see Table 4 in Appendix and Table 7 in Appendix E. ID PREFIX OBJECTIVE #SENSORS #ACTORS SARL MARL CY D RRO T2D CY D RJE T2D CY D RJE T3D RBC2D RBC2D-W RBC3D RBC3D-W AI I L2D AI I L3D TCFSM L3D-B TCFSM L3D-B O TCFLA E3D-B TCFLA E3D-B O DRAG REDUCTION HEAT TRANSFER ENHANCEMENT AERODYNAMIC EFFICIENCY ENHANCEMENT DRAG REDUCTION 302 302 4832 768 1 536 221 184 884 418 2508 1 024 512 4 096 2 048 1 1 8 12 24 64 256 3 12 1 024 512 4 096 2 RUNTIME [SEC/STEP] 1.95 2.01 9.52 1.92 1.99 1.17 1.71 28.76 52.89 0.33 0.29 0.56 0. tions are stated in Appendix A. Each environment is offered in three difficulty levels to introduce increasing levels of turbulence and flow complexity. An overview of the environments is shown in Figure 1 and summarized in Table 2. In the following, we outline four key flow scenarios, building the foundation of the 13 FluidGym environments. Flow Past Cylinder The von Karman vortex street is canonical setup in which flow separation behind cylinder induces periodic vortex shedding and fluctuating forces on the cylinder (Schafer et al., 1996). This configuration has consistently served as benchmark for AFC using RL to reduce the drag acting on the cylinder (Koizumi et al., 2018; Rabault et al., 2019; Xu et al., 2020; Tang et al., 2020; Ren et al., 2021; Han et al., 2022; Suarez et al., 2025). The system is parametrized via the Reynolds number Re = ν with mean incoming velocity , cylinder diameter D, and kinematic viscosity ν. The objective is to reduce the drag coefficient CD while keeping the lift CL small, using the CLTact , with lift reward rt = CD,ref CDTact regularization weight Tact referring to averag0 and ing over the actuation interval and reference uncontrolled drag coefficient CD,ref . We note that normalization with uncontrolled reference metrics is not essential in principle, but is used consistently across the benchmark. Actuation uses either (i) opposing synthetic jets on the top and bottom surfaces of the cylinder, or (ii) cylinder rotation. Difficulty levels, defined via Re, span different flow regimes in 2D/3D. Rayleigh-Benard Convection The Rayleigh-Benard Convection (RBC, Benard (1900); Rayleigh (1916)) models buoyancy-driven flow between heated bottom plate and cooled top plate. This leads to convective fluid motion and the formation of thermal plumes with complex, potentially chaotic patterns (Pandey et al., 2018). The system is defined by two dimensionless parameters, the Prandtl number Pr and the Rayleigh number Ra. Pr is material property of the fluid, while Ra controls the intensity of buoyancydriven convection. Our setup follows Vignon et al. (2023), extended to 3D as in Vasanth et al. (2024), with the domain height reduced from 2 to 1 to match the standard dimensionless configuration (Pandey et al., 2018). The task aims to reduce convective heat transfer by minimizing the instantaneous Nusselt number Nuinstant = RaPr , where uy denotes the vertical fluid velocity, the temperature field, and volume average (Pandey et al., 2018), resulting Nuinstant. Control is applied in the reward rt = Nuref via bottom-boundary heaters whose temperatures are normalized, clipped, and spatially smoothed. The environment difficulty is varied by adjusting the Rayleigh number Ra, with higher values in both 2D and 3D resulting in more turbulent convection. An additional wide-domain variant with aspect ratio 2π introduces richer spatial patterns. uyT Flow Past an Airfoil The flow around an airfoil is fundamental configuration in aerodynamics and common benchmark for AFC (Wang et al., 2022b; Garcia et al., 2025; Liu et al., 2025; Montal`a et al., 2025). Variations in Reynolds number and angle of attack influence flow separation and vortex dynamics. Control aims to improve aerodynamic efficiency by increasing the lift to drag ratio, i.e., CL,ref /CD,ref . Actuation is prort = vided by zero net-mass-flux synthetic jet actuators mounted on the airfoil surface. Task difficulty is set by the Reynolds number, with higher values producing sharper separation and stronger turbulence. In this work, we only consider the easy 3D difficulty level due to the high computational cost. CDTact CLTact / Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control computational constraints. Additionally, to study the utility of differentiable benchmarks, we evaluate differentiable model predictive controller (D-MPC; see Appendix D), demonstrated on the CylinderJet2D environment. For each run, we collect ten evaluation episodes on the test set. We report mean reward per step rather than cumulative return to avoid confounding effects from episode length. Since episode lengths are constant within each environment, this choice does not affect relative or normalized metrics. 4.2. Overall Benchmark Performance Before presenting quantitative results, we first show exemplary final flow fields from controlled test set rollouts to illustrate the resulting flow states. Figure 3 displays four 3D environments with their uncontrolled and controlled cases at the end of test episodes, including transferred policies. Then, to assess the overall performance of RL algorithms on FluidGym, we consider their respective performance profiles following Agarwal et al. (2021), which depict the tail distribution of normalized rewards aggregated across all environments and random seeds. Figure 4 (left) shows the profiles of PPO, SAC, and their respective multi-agent variants. Notably, the performance profiles of PPO and SAC vary substantially. We attribute this to slower overall learning and convergence behavior (see Appendix for detailed results). For the multi-agent variants, we observe similar performance profiles for both algorithms. MA-SAC exhibits marginally higher scores overall, though the differences partially lie within the associated confidence intervals. Inspecting performance across environment categories and difficulty levels (Figure 4, right) shows consistent pattern: SAC achieves the highest normalized test set relative improvement over the baseflow across all levels, while MAPPO performs slightly better on the TCF environments. Overall, two trends emerge: (i) SAC reliably outperforms PPO across all difficulty levels, while the multi-agent variants are more comparable, likely because PPO benefits from increased sample counts, which reduces SACs usual sample-efficiency advantage; and (ii) environments with similar flow structures (e.g., cylinder and airfoil) yield similar learning dynamics and performance, despite differing reward definitions. These observations highlight the importance of algorithmic robustness and sample efficiency when scaling RL to turbulent AFC tasks. 4.3. Results for Individual Environments Next, we discuss an individual test set episode using the final policies for CylinderJet2D-easy-v0. Figure 5 shows the temporal evolution of applied actions at and the resulting drag coefficients CD. Both RL policies rapidly attenuate oscillations and reduce drag relative to the unconFigure 3. Final 3D flow fields at the end of test episodes for uncontrolled and controlled cases across four FluidGym environments using PPO, SAC, or multi-agent variants. Transfer cases use policies trained on corresponding 2D or smaller domains. turbulent Turbulent Channel Flow The channel flow (TCF, the flow between two parallel, infinitely large plates) is classic experiment for studying wall-bounded turbulence. Most AFC strategies aim to reduce the wall shear stress by imposing wall normal velocities (blowing or suction) via spatially distributed actuators at the walls (Bewley et al., 2001; Stroh et al., 2015; Guastoni et al., 2023; Sonoda et al., 2023; Zhao et al., 2025). The objective is captured through reward based on the instantaneous reduction of shear stress τwall relative to the uncontrolled reference τwall,ref , i.e., rt = 1 τwall/τwall,ref . FluidGym provides both small and large channel variant, enabling evaluation under different spatial scales. Additionally, FluidGym provides pre-computed opposition control baseline for this environment consistent with previous work (Guastoni et al., 2023). 4. Experiments In the following, we present comprehensive evaluation of FluidGym. All experimental results and trained models are publicly available at https://huggingface. co/datasets/safe-autonomous-systems/ fluidgym-experiments. 4.1. Experimental Setup In our experiments, we evaluate Proximal Policy Optimization (PPO, Schulman et al. (2017)) and Soft ActorCritic (SAC, Haarnoja et al. (2018)) using their StableBaselines3 (SB3, Raffin et al. (2021)) implementations, denoted as MA-PPO and MA-SAC in the MARL setting. To enable the first large-scale evaluation of these algorithms on AFC, we use default SB3 hyperparameters (see Appendix D). We conduct all experiments using five random seeds, with the exception of the 3D Airfoil and Cylinder environments, which are evaluated on three seeds due to 6 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 4. Left: Performance profiles as proposed by Agarwal et al. (2021) summarizing scores over all FluidGym environments. Error bars indicate pointwise 95% confidence intervals based on 2 stratified bootstrap replications across random seeds. Right: Interquartile mean (IQM) scores over environment classes (middle) and difficulty levels (right). For all panels, scores are computed as minmax normalized relative improvements over the baseflow, with normalization performed independently for each environmentdifficulty pair. Figure 5. Time evolution of the control action at and drag coefficient CD for the uncontrolled baseflow, the final PPO and SAC policies, and the differentiable model predictive control (DMPC, see Algorithm 1 in Appendix D) controller evaluated on the CylinderJet2D-easy-v0 test environment. Figure 6. Time evolution of the Nusselt number Nuinstant for the baseflow and MA-PPO policy (left) and bottom-plate actuation at = 175 (right) on the RBC3D-easy-v0 test environment. trolled baseflow, with SAC achieving the lowest final CD corresponding to drag reduction of approximately 8%, and PPO in agreement with findings by Rabault et al. (2019). In addition to the classical RL policies, we evaluate D-MPC, which selects actions exclusively by ascending the reward gradient through the differentiable simulation. Its observed drag reduction indicates that reward gradients provide effective control signals for AFC and underscores the value of FluidGym as the first fully differentiable AFC benchmark. Beyond single-agent cylinder control, FluidGym also enables studying multi-agent AFC tasks. Figure 6 shows test episode on RBC3D-easy-v0 using MA-PPO, where agents coordinate bottom-wall heating to form two stable convection rolls. Notably, when investigating the actuation, we observe emerging coordinated behavior between the individual agents, leading to two separate convection rolls. These spatial heating patterns are consistent with the findings of Vasanth et al. (2024) and suggest that RL can learn spatially invariant control policy forming globally coordinated behavior. This highlights the potential of MARL for AFC, key capability of FluidGym. 4.4. Policy Transfer Across Environment Variations We further evaluate policy transfer in FluidGym, considering (i) dimensionality transfer for the cylinder flow and (ii) domain-size transfer for the TCF. Transfer across Dimensionalities We investigate how policies trained in 2D transfer to their 3D counterparts using the cylinder environment. Figure 7 shows the mean test set drag reduction for three approaches: 3D SARL and MARL trained in 3D, and transferred 2D 3D policy applied to the eight actuators in 3D individually. On the easy task, the transferred policy outperforms the 3D-trained baselines. On medium difficulty, it is on par, slightly below PPO and MA-SAC. On the hard task, it again achieves the highest drag reduction. These findings indicate that direct transfer from 2D to 3D can be robust despite the added complexity. 7 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control of the results is still limited when it comes to comparisons between algorithms. Second, FluidGym currently requires CUDA-enabled GPU for fast simulation, as the underlying solver depends on custom CUDA kernels. Although installation is simplified through pre-built wheels, CPU-only execution is not yet supported. Third, despite full differentiability of FluidGym, we focus on model-free RL and only demonstrate D-MPC leveraging reward gradients as proof of concept. Systematic comparisons with other differentiable control approaches are not included. Finally, baseline algorithms are evaluated using standard hyperparameters from off-the-shelf libraries, which promotes comparability but may not reflect each algorithms optimal performance. Overall, these limitations stem from computational and practical considerations rather than inherent constraints of FluidGym. Several directions offer potential for extending FluidGym and broadening its utility and scope. First, increasing the number of random seeds used during training and evaluation will improve the statistical robustness of the reported baseline results. Additionally, evaluating gradient-based methods, e.g., DPC (Drgoˇna et al., 2022) and differentiable RL (Xu et al., 2022; Xing et al., 2025), where the latter combines gradient-based control with classical RL, is natural next step. Expanding the set of environments to cover additional geometries and physical regimes would provide more comprehensive assessment of control strategies across diverse flow configurations. Beyond incompressible NavierStokes, we also plan to extend FluidGym to magnetohydrodynamic (MHD) flows, enabling the study of control in electrically conducting fluids (e.g., in fusionrelevant settings). Finally, we intend to add progressively more challenging environments as control methods advance to keep the benchmark aligned with the state of the art. 6. Conclusion In this work, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for reinforcement learning in active flow control. By combining GPU-accelerated CFD solver with standardized RL interface, FluidGym removes the dependency on external CFD code and provides unified, accessible, and reproducible platform that bridges RL research and fluid dynamics. Our benchmark suite provides diverse 2D and 3D environments with consistent observation, actuation, and reward definitions, unified evaluation protocols, and support for singleand multi-agent RL as well as gradient-based methods. PPO and SAC baselines align with prior findings and show FluidGyms suitability for RL and gradient-based control, with D-MPC demonstrating the effectiveness of leveraging reward gradients for AFC. By releasing all environments and trained models, we aim to lower the barrier to entry for researchers and foster reproducibility and comparability. Figure 7. CylinderJet3D: Drag reduction across difficulty levels for PPO and SAC comparing SARL 3D, MARL 3D, and transferred SARL 2D MARL 3D with 95% confidence intervals. Figure 8. TCF: Mean test-episode drag reduction with 95% confidence intervals for opposition control (Opp. Control) as well as policies trained on the small (S) and large (L) channel, respectively. Transfer across Domain Sizes Finally, we study whether policies trained in smaller TCF domains transfer to larger ones. This setting is motivated by two factors: (i) lower simulation cost in smaller domains, and (ii) MARL may yield control policies that are translation-equivariant and thus insensitive to the absolute domain size. Figure 8 shows mean test-episode drag reduction in the large domain for policies trained either on the small channel (S) or directly on the large channel (L), together with an opposition control baseline. Notably, policies trained in the small domain perform comparably to opposition control and substantially outperform those trained directly in the large domain. This suggests that MARL can learn spatially transferable control strategies that generalize across domain scales. 5. Limitations and Future Work While FluidGym provides unified and extensible platform for studying RL for AFC, limitations remain. First, the current evaluation is based on limited number of random seeds due to the substantial computational cost associated with CFD simulations. As result, the statistical robustness 8 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control"
        },
        {
            "title": "Acknowledgements",
            "content": "JB and SP acknowledge funding from the European Research Council (ERC Starting Grant KoOpeRaDE) under the European Unions Horizon 2020 research and innovation programme (Grant agreement No. 101161457). The computations were performed on the compute cluster of the Lamarr Institute for Machine Learning and Artificial Intelligence, as well as on the high-performance computer Noctua 2 at the NHR Center Paderborn Center for Parallel Computing (PC2), both of which are funded by the Federal Ministry of Research, Technology and Space and by the state of Northrhine-Westfalia."
        },
        {
            "title": "Impact Statement",
            "content": "In this work, we introduce benchmark suite for reinforcement learning in active flow control with the goal of improving algorithms and policies for controlling fluid systems. Potential positive societal impacts include more energyefficient transport and industrial processes, emission reduction, energy harvesting, and improved study of fluid flows. At the same time, deploying learning-based controllers in safety-critical settings without rigorous validation could pose risks. The environments in FluidGym are idealized and do not capture the full complexity, including uncertainties and constraints of real systems. Training reinforcement learning algorithms on high-fidelity simulations can also be computationally expensive and energy-consuming, which motivates future work on more sample-efficient algorithms. Overall, this benchmark is research tool to advance control methods for fluid systems, and we do not foresee direct societal harms associated with its use."
        },
        {
            "title": "References",
            "content": "Abuduweili, A. and Liu, C. An optical control environment for benchmarking reinforcement learning algorithms. Transactions on Machine Learning Research, 2023. Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 2021. Albrecht, S. V., Christianos, F., and Schafer, L. Multiagent reinforcement learning: Foundations and modern approaches. MIT Press, 2024. Alnæs, M., Blechta, J., Hake, J., Johansson, A., Kehlet, B., Logg, A., Richardson, C., Ring, J., Rognes, M. E., and Wells, G. N. The FEniCS project version 1.5. Archive of Numerical Software, 2015. Publisher: University Library Heidelberg Version Number: 1.0.0. Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., and Chintala, S. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS 24). ACM, 2024. ANSYS Inc. ANSYS Fluent, 2026."
        },
        {
            "title": "URL",
            "content": "https://www.ansys.com/products/ fluids/ansys-fluent. Bae, H. J. and Koumoutsakos, P. Scientific multi-agent reinforcement learning for wall-models of turbulent flows. Nature Communications, 13, 2022. Barthelmie, R. J., Hansen, K., Frandsen, S. T., Rathmann, O., Schepers, J. G., Schlez, W., Phillips, J., Rados, K., Zervos, A., Politis, E. S., and Chaviaropoulos, P. K. Modelling and measuring flow and wind turbine wakes in large wind farms offshore. Wind Energy, 12, 2009. Batikh, A., Baldas, L., and Colin, S. Application of active flow control in aircrafts State of the art. In Proceedings of the International Workshop on Aircraft System Technologies, Hamburg, Germany, 2017. Beintema, G., Corbetta, A., Biferale, L., and Toschi, F. Controlling RayleighBenard convection via reinforcement learning. Journal of Turbulence, 21, 2020. Bewley, T. R., Moin, P., and Temam, R. DNS-based predictive control of turbulence: an optimal benchmark for feedback algorithms. Journal of Fluid Mechanics, 447, 2001. Bhan, L., Bian, Y., Krstic, M., and Shi, Y. PDE control gym: benchmark for data-driven boundary control of partial differential equations. In 6th Annual Learning for Dynamics & Control Conference, 15-17 July 2024, University of Oxford, Oxford, UK, volume 242. PMLR, 2024. Bou, A., Bettini, M., Dittert, S., Kumar, V., Sodhani, S., Yang, X., Fabritiis, G. D., and Moens, V. TorchRL: data-driven decision-making library for PyTorch. arXiv preprint arXiv:2306.00577, 2023. 9 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax. Brunton, S. L. and Noack, B. R. Closed-loop turbulence control: Progress and challenges. Applied Mechanics Reviews, 67, 2015. Benard, H. Les tourbillons cellulaires dans une nappe liquide. Revue Generale des Sciences Pures et Appliquees, 11, 1900. Chourdakis, G., Davis, K., Rodenberg, B., Schulte, M., Simonis, F., Uekermann, B., Abrams, G., Bungartz, H.-J., Cheung Yau, L., Desai, I., Eder, K., Hertrich, R., Lindner, F., Rusch, A., Sashko, D., Schneider, D., Totounferoush, A., Volland, D., Vollmer, P., and Koseomur, O. Z. preCICE v2: sustainable and user-friendly coupling library. Open Research Europe, 2, 2022. Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., De Las Casas, D., Donner, C., Fritz, L., Galperti, C., Huber, A., Keeling, J., Tsimpoukelli, M., Kay, J., Merle, A., Moret, J.-M., Noury, S., Pesamosca, F., Pfau, D., Sauter, O., Sommariva, C., Coda, S., Duval, B., Fasoli, A., Kohli, P., Kavukcuoglu, K., Hassabis, D., and Riedmiller, M. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602, 2022. Drgoˇna, J., Kiˇs, K., Tuor, A., Vrabie, D., and Klauˇco, M. Differentiable predictive control: Deep learning alternative to explicit model predictive control for unknown nonlinear systems. Journal of Process Control, 116, 2022. Duriez, T., Brunton, S. L., and Noack, B. R. Machine Learning Control Taming Nonlinear Dynamics and Turbulence, volume 116 of Fluid Mechanics and Its Applications. Springer International Publishing, Cham, 2017. Ferziger, J. H., Peric, M., and Street, R. L. Computational methods for fluid dynamics. Springer International Publishing, 2020. Franz, A., Wei, H., Guastoni, L., and Thuerey, N. PICTA differentiable, GPU-accelerated multi-block PISO solver for simulation-coupled learning tasks in fluid dynamics. Journal of Computational Physics, 544, 2026. Garcia, X., Miro, A., Suarez, P., Alcantara Avila, F., Rabault, J., Font, B., Lehmkuhl, O., and Vinuesa, R. Deepreinforcement-learning-based separation control in twodimensional airfoil. arXiv preprint arXiv:2502.16993, 2025. Gautier, N., Aider, J.-L., Duriez, T., Noack, B. R., Segond, M., and Abel, M. Closed-loop separation control using machine learning. Journal of Fluid Mechanics, 770, 2015. Guastoni, L., Rabault, J., Schlatter, P., Azizpour, H., and Vinuesa, R. Deep reinforcement learning for turbulent drag reduction in channel flows. The European Physical Journal E, 46, 2023. ISSN 1292-895X. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft Actor-Critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research. PMLR, 2018. Han, B.-Z., Huang, W.-X., and Xu, C.-X. Deep reinforcement learning for active control of flow over circular cylinder with rotational oscillations. International Journal of Heat and Fluid Flow, 96, 2022. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. Deep reinforcement learning that matters. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, 2018. Institute of Aerodynamics. m-AIA, 2024. Issa, R. Solution of the implicitly discretised fluid flow equations by operator-splitting. Journal of Computational Physics, 62, 1986. Jimenez, J. Near-wall turbulence. Physics of Fluids, 25, 2013. Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101, 1998. Kajishima, T. and Taira, K. Computational Fluid Dynamics. Springer International Publishing, 2017. URL http://link.springer.com/10.1007/ 978-3-319-45304-0. Koizumi, H., Tsutsumi, S., and Shima, E. Feedback control of Karman vortex shedding from cylinder using deep reinforcement learning. In 2018 Flow Control Conference, 2018. Krais, N., Beck, A., Bolemann, T., Frank, H., Flad, D., Gassner, G., Hindenlang, F., Hoffmann, M., Kuhn, T., Sonntag, M., and Munz, C.-D. FLEXI: high order discontinuous Galerkin framework for hyperbolicparabolic 10 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control conservation laws. Computers & Mathematics with Applications, 81, 2021. Kurz, M., Offenhauser, P., Viola, D., Resch, M., and Beck, A. Relexi scalable open source reinforcement learning framework for high-performance computing. Software Impacts, 14, 2022. Lagemann, C., Mokbel, S., Gondrum, M., Ruttgers, M., Callaham, J., Paehler, L., Ahnert, S., Zolman, N., Lagemann, K., Adams, N., Meinke, M., Schroder, W., Loiseau, J.-C., Lagemann, E., and Brunton, S. L. HydroGym: reinforcement learning platform for fuid dynamics. arXiv preprint arXiv:2512.17534, December 2025a. Lagemann, C., Paehler, L., Callaham, J., Mokbel, S., Ahnert, S., Lagemann, K., Lagemann, E., Adams, N., and Brunton, S. Hydrogym: Reinforcement Learning Platform for Fluid Dynamics. In Proceedings of the 7th Annual Learning for Dynamics & Control Conference. PMLR, 2025b. Liu, Q., Corona, L. J. T., Shu, F., and Gross, A. Reinforcement learning-based closed-loop airfoil flow control. arXiv preprint arXiv:2505.04818, 2025. Maliska, C. R. Fundamentals of Computational Fluid Dynamics: The Finite Volume Method, volume 135 of Fluid Mechanics and Its Applications. Springer International Publishing, 2023. Mao, Y., Zhong, S., and Yin, H. DRLFluent: distributed co-simulation framework coupling deep reinforcement learning with Ansys-Fluent on high-performance computing systems. Journal of Computational Science, 74, 2023. ISSN 1877-7503. Markmann, T., Straat, M., Peitz, S., and Hammer, B. Control of Rayleigh-Benard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime. arXiv preprint arXiv:2504.12000, 2025. McGreivy, N. and Hakim, A. Weak baselines and reporting biases lead to overoptimism in machine learning for fluidrelated partial differential equations. Nature Machine Intelligence, 6, 2024. Montal`a, R., Font, B., Suarez, P., Rabault, J., Lehmkuhl, O., Vinuesa, R., and Rodriguez, I. Deep reinforcement learning for active flow control around three-dimensional arXiv preprint flow separated wing at Re = 1,000. arXiv:2509.10195, 2025. Moslem, F., Jebelli, M., Masdari, M., Askari, R., and Ebrahimi, A. Deep reinforcement learning for active flow control in bluff bodies: state-of-the-art review. Ocean Engineering, 327, 2025. Mouchamps, A., Malherbe, A., Bolland, A., and Ernst, D. Gym-TORAX: Open-source software for integrating RL with plasma control simulators. arXiv preprint arXiv:2510.11283, 2025. Nair, A. G., Yeh, C.-A., Kaiser, E., Noack, B. R., Brunton, S. L., and Taira, K. Cluster-based feedback control of turbulent post-stall separated flows. Journal of Fluid Mechanics, 875, 2019. Navier, C.-L. Memoire sur les lois du mouvement des fluides. Memoire de lAcademie des Sciences de lInstitut des Sciences, Paris, 1827. Pandey, A., Scheel, J. D., and Schumacher, J. Turbulent superstructures in Rayleigh-Benard convection. Nature Communications, 9, 2018. Pawar, S. and Maulik, R. Distributed deep reinforcement learning for simulation control. Machine Learning: Science and Technology, 2, 2021. Peitz, S., Stenner, J., Chidananda, V., Wallscheid, O., Brunton, S. L., and Taira, K. Distributed control of partial differential equations using convolutional reinforcement learning. Physica D: Nonlinear Phenomena, 461, 2024. Pironti, A. and Walker, M. Fusion, tokamaks, and plasma control: an introduction and tutorial. IEEE Control Systems Magazine, 25, 2005. Rabault, J., Kuchta, M., Jensen, A., Reglade, U., and Cerardi, N. Artificial neural networks trained through deep reinforcement learning discover control strategies for active flow control. Journal of Fluid Mechanics, 865, 2019. Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N. Stable-Baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22, 2021. Rayleigh, L. LIX. On convection currents in horizontal layer of fluid, when the higher temperature is on the under side. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 32, 1916. Ren, F., Rabault, J., and Tang, H. Applying deep reinforcement learning to active flow control in weakly turbulent conditions. Physics of Fluids, 33, 2021. Ren, F., Zhang, F., Zhu, Y., Wang, Z., and Zhao, F. Enhancing heat transfer from circular cylinder undergoing vortex induced vibration based on reinforcement learning. Applied Thermal Engineering, 236, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 11 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Schafer, M., Turek, S., Durst, F., Krause, E., and Rannacher, R. Benchmark computations of laminar flow around In Flow Simulation with High-Performance cylinder. Computers II, volume 48. 1996. Shams, M. and Elsheikh, A. H. Gym-preCICE: Reinforcement learning environments for active flow control. SoftwareX, 23, 2023. Sonoda, T., Liu, Z., Itoh, T., and Hasegawa, Y. Reinforcement learning of control strategies for reducing skin friction drag in fully developed turbulent channel flow. Journal of Fluid Mechanics, 960, 2023. Stokes, G. G. On the theories of the internal friction of fluids in motion, and of the equilibrium and motion of elastic solids. Transactions of the Cambridge Philosophical Society, 8:287341, 1845. Stroh, A., Frohnapfel, B., Schlatter, P., and Hasegawa, Y. comparison of opposition control in turbulent boundary layer and turbulent channel flow. Physics of Fluids, 27, 2015. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An introduction. The MIT Press, Cambridge, MA, 1998. Suarez, P., AlcantaraAvila, F., Rabault, J., Miro, A., Font, B., Lehmkuhl, O., and Vinuesa, R. Flow control of three-dimensional cylinders transitioning to turbulence via multi-agent reinforcement learning. Communications Engineering, 4, 2025. Tang, H., Rabault, J., Kuhnle, A., Wang, Y., and Wang, T. Robust active flow control over range of Reynolds numbers using an artificial neural network trained through deep reinforcement learning. Physics of Fluids, 32, 2020. Terry, J., Black, B., Grammel, N., Jayakumar, M., Hari, A., Sullivan, R., Santos, L. S., Dieffendahl, C., Horsch, C., Perez-Vicente, R., et al. PettingZoo: Gym for multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021. Tokarev, M., Palkin, E., and Mullyadzhanov, R. Deep reinforcement learning control of cylinder flow using rotary oscillations at low Reynolds number. Energies, 13, 2020. Vignon, C., Rabault, J., Vasanth, J., AlcantaraAvila, F., Mortensen, M., and Vinuesa, R. Effective control of two-dimensional RayleighBenard convection: Invariant multi-agent reinforcement learning is all you need. Physics of Fluids, 35(6), 2023. Viquerat, J., Meliga, P., Larcher, A., and Hachem, E. review on deep reinforcement learning for fluid mechanics: An update. Physics of Fluids, 34, 2022. Wang, Q., Yan, L., Hu, G., Li, C., Xiao, Y., Xiong, H., Rabault, J., and Noack, B. R. DRLinFluids: An opensource Python platform of coupling deep reinforcement learning and OpenFOAM. Physics of Fluids, 34, August 2022a. Wang, Y.-Z., Mei, Y.-F., Aubry, N., Chen, Z., Wu, P., and Wu, W.-T. Deep reinforcement learning based synthetic jet control on disturbed flow over airfoil. Physics of Fluids, March 2022b. Weiner, A. and Geise, J. drlFoam: Deep reinforcement learning with OpenFOAM, 2022. URL https://github. com/OFDataCommittee/drlfoam. Weller, H. G., Tabor, G., Jasak, H., and Fureby, C. tensorial approach to computational continuum mechanics using object-oriented techniques. Computers in Physics, 12, 1998. Werner, S. and Peitz, S. Numerical evidence for sample efficiency of model-based over model-free reinforcement learning control of partial differential equations. In European Control Conference (ECC). IEEE, 2024. Williamson, C. H. K. Vortex Dynamics in the Cylinder Wake. Annual Review of Fluid Mechanics, 28, 1996. Xiao, M., Wang, Y., Rodach, F., Font, B., Kurz, M., Suarez, P., Zhou, D., AlcantaraAvila, F., Zhu, T., Liu, J., Montal`a, R., Chen, J., Rabault, J., Lehmkuhl, O., Beck, A., Larsson, J., Vinuesa, R., and Pirozzoli, S. SmartFlow: CFD-solver-agnostic deep reinforcement learning framework for computational fluid dynamics on HPC platforms. arXiv preprint arXiv:2508.00645, 2025. Towers, M., Kwiatkowski, A., Terry, J., Balis, J. U., De Cola, G., Deleu, T., Goulao, M., Kallinteris, A., Krimmel, M., KG, A., et al. Gymnasium: standard interface for reinforcement learning environments. arXiv preprint arXiv:2407.17032, 2024. Xing, E., Luk, V., and Oh, J. Stabilizing reinforcement learning in differentiable multiphysics simulation. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025, 2025. Vasanth, J., Rabault, J., AlcantaraAvila, F., Mortensen, M., and Vinuesa, R. Multi-agent Reinforcement Learning for the Control of Three-Dimensional RayleighBenard Convection. Flow, Turbulence and Combustion, 2024. Xu, H., Zhang, W., Deng, J., and Rabault, J. Active flow control with rotating cylinders by an artificial neural network trained by deep reinforcement learning. Journal of Hydrodynamics, 32, 2020. 12 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Xu, J., Makoviychuk, V., Narang, Y., Ramos, F., Matusik, W., Garg, A., and Macklin, M. Accelerated policy learning with parallel differentiable simulation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. Zhang, X., Mao, W., Mowlavi, S., Benosman, M., and Basar, T. ControlGym: Large-scale control environments for benchmarking reinforcement learning algorithms. In 6th Annual Learning for Dynamics & Control Conference, 1517 July 2024, University of Oxford, Oxford, UK, volume 242. PMLR, 2024. Zhao, F., Zhou, Y., Ren, F., Tang, H., and Wang, Z. Mitigating the lift of circular cylinder in wake flow using deep reinforcement learning guided self-rotation. Ocean Engineering, 306, 2024. Zhao, Z., Li, Z., Hassibi, K., Azizzadenesheli, K., Yan, J., Bae, H. J., Zhou, D., and Anandkumar, A. Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows. arXiv preprint arXiv:2510.03360, 2025. Zolman, N., Lagemann, C., Fasel, U., Kutz, J. N., and Brunton, S. L. SINDy-RL for interpretable and efficient model-based reinforcement learning. Nature Communications, 16, 2025. 13 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control A. Reinforcement Learning for Active Flow Control Based on the definition by Sutton & Barto (1998), reinforcement learning (RL) agent interacts with Markov decision R, and process (MDP) = ( transition function : . We note that we focus on deterministic MDPs here and do consider the discount factor γ as RL hyperparameter and not as part of the MDP. , R, ) with finite set of states (cid:55) , reward function : , finite set of actions (cid:55) A , At each time step t, the agent selects an action at = π(st) based on its policy π. In practice, π is often described by neural network with parameters θ and therefore denoted as πθ. Then, the environment returns the next state st+1 and reward rt computed by R(st, at). (cid:55) When RL is applied to active flow control (AFC), the information based on which the agent selects its action is typically not the full state st but set of sensor observations ot. This can be formalized as partially observable Markov decision process (POMDP, Kaelbling et al. (1998)), which extends the MDP tuple by finite set of observations Ω and observation Ω. Again, we only consider POMDPs here. This leads to the following MDP definition for function : single-agent RL (SARL) used in this paper: Based on the definition of partially observable stochastic game (POSG, Albrecht et al. (2024)), we can extend our SARL definition to multiple agents. However, in the following, we again consider deterministic scenarios. In this setting, we individual agents. While the sets of states, actions, and observations are shared between agents, each consider agent has an individual observation function Oi and reward function Ri. This leaves us with the following MDP: MMARL = (I, MSARL = ( , Ri, T, Ω, Oi). , R, T, Ω, O). S , , B. The PICT Solver In the following, we describe the core numerical details of the PICT solver (Franz et al., 2026) and provide numerical evidence to validate the underlying simulation of our benchmark. B.1. The PISO Algorithm The Pressure Implicit with Splitting of Operators (PISO) algorithm introduced by Issa (1986) is common method for the simulation of incompressible flows, which are governed by the Navier-Stokes equations (Navier, 1827; Stokes, 1845), consisting of the momentum equation and the continuity equation t + (uu) ν 2u = + with time t, velocity u, pressure p, viscosity ν, and external source term S. = 0, (1) (2) The PISO algorithm consists of two main procedures: (i) predictor step, which advances the simulation and produces predicted velocity u, and (ii) typically two predictor steps computing the pressure, which is then used to make the predicted velocity divergence free. In PICT, the PISO algorithm is discretized using the finite volume method (FVM, Kajishima & Taira (2017); Maliska (2023))on collocated grid. For the time advancement, the implicit Euler scheme is used. For buoyancy-driven convection, we employ the Boussinesq approximation. B.2. Gradient Computation Simulation gradients in PICT are obtained via combination of the Discretize-then-Optimize (DtO) and Optimize-thenDiscretize (OtD) paradigms, with DtO applied to the global algorithmic structure and OtD to the inner linear system solves. B.3. Validation First and foremost, the PICT solver was numerically validated by Franz et al. (2026). Additionally, we provide numerical evidence for the correctness of the environments in FluidGym. 14 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Flow Past Cylinder For the cylinder, the temporal mean of the uncontrolled drag coefficient of 3.328 closely aligns with the value of approximately 3.205 reported by Rabault et al. (2019), resulting in relative deviation of 3.84%. We partially attribute this to the difference between non-reflecting advective outflow boundary in PICT and the free-stress boundary condition implemented by Rabault et al. (2019). Nevertheless, as described in Section 4.3, the resulting drag reductions achieved by the RL policies match both quantitatively and qualitatively. Table 3. RBC grid refinement study. Reported Nusselt numbers correspond to the temporal mean of Nuinstant over 10 uncontrolled episodes. RESOLUTION Nuinstant #CELLS 96 144 192 4.896 4.755 4.786 5 856 13 248 23 242 Rayleigh-Benard Convection Prior work has largely relied on numerical setups that differ from the standard nondimensional formulation (Pandey et al., 2018), partially yielding inconsistent Nusselt numbers (Vignon et al., 2023; Markmann et al., 2025). To validate our environment, we perform grid refinement study  (Table 3)  . The grid with resolution 96 shows relative deviation of 2.298%, and demonstrates learning behavior consistent with previous studies (Vignon et al., 2023). Flow Past an Airfoil For the airfoil, we obtain mean drag coefficient of 0.278 and mean lift coefficient of 0.993. These values compare well with those reported by Wang et al. (2022b), who obtained an average drag of 0.324 and an average lift of 1.003. We note that our computational domain is longer (6 chord lengths versus 3.5), which accounts for part of the discrepancy. Nevertheless, we observe consistent quantitative and qualitative behavior across all flow states. Turbulent Channel Flow For the channel configuration, we adopt the same numerical setup previously validated by Franz et al. (2026), including the wall-stress computation used in the forcing term. Therefore, additional baseline validation is not required. Our opposition-control case yields 20% drag reduction, and the RL-controlled case reaches 30%, both of which are in close agreement with prior work (Guastoni et al., 2023). 15 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control C. Environments Table 4. Difficulty levels and corresponding physical parameters for all FluidGym environments. Cylinder and airfoil tasks are parameterized by the Reynolds number Re, RBC by the Rayleigh number Ra, and turbulent channel flow (TCF) by the friction Reynolds number Reτ . ID PREFIX DIFFICULTY PARAMETER VALUE DOMAIN SIZE (L H[D]) CY D RRO T2D CY D RJE T2D CY D RJE T3D RBC2D RBC2D-W RBC3D RBC3D-W AI I L2D AI I L3D TCFSM L3D-B TCFSM L3D-B O TCFLA E3D-B TCFLA E3D-B O EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD Re Re Re Re Re Re Re Re Re Ra Ra Ra Ra Ra Ra Ra Ra Ra Ra Ra Ra Re Re Re Re Re Re Reτ Reτ Reτ Reτ Reτ Reτ Reτ Reτ Reτ Reτ Reτ Reτ 100 250 500 100 250 500 100 250 500 8 104 4 105 8 105 8 104 4 105 8 6 103 8 103 1 104 6 103 8 103 1 104 1 103 3 103 5 103 1 103 3 103 5 103 180 330 550 180 330 180 330 550 180 330 550 22 4.1 22 4.1 22 4.1 22 4.1 22 4.1 22 4.1 22 4.1 4 22 4.1 4 22 4.1 4 π 1 π 1 π 2π 1 2π 1 2π 1 π 1 π π 1 π π 1 π 2π 1 2π 2π 1 2π 2π 1 2π 6 1.4 6 1.4 6 1.4 6 1.4 1.4 6 1.4 1.4 6 1.4 1.4 π 2 π/2 π 2 π/2 π 2 π/ π 2 π/2 π 2 π/2 π 2 π/2 2π 2 π 2π 2 π 2π 2 π 2π 2 π 2π 2 π 2π 2 π Initial domains are publicly available in our HuggingFace dataset at https://huggingface.co/datasets/ safe-autonomous-systems/fluidgym-data. All environments provide unified action space of [ 1, 1] and scale the actions internally. summary of all environments is stated in Table 4. We note that the medium and hard cases for the 3D Airfoil environment are not considered in this work due to computational limitations. Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control C.1. Flow Past Cylinder (a) 2D cylinder configuration. (b) 3D cylinder configuration. Figure 9. Overview of the 2D and 3D cylinder environments used in our benchmark. Jets are shown in orange and feature parabolic profile with total deflection angle of 10. Sensor locations are indicated in pink (dots in 2D, planes in 3D, with sensors placed analogously within each plane). In 3D, the domain is extended along the spanwise direction, yielding eight individual jet pairs. Reward Function The objective is to reduce the drag coefficient CD of the cylinder. Thus, the reward at step is defined CLTact , where the lift penalty ω is set to 1.0 as proposed by Ren et al. (2021) and the as rt = CD,ref reference value corresponds to the uncontrolled baseline. TAact corresponds to the temporal average over an actuation period, i.e., the simulation steps where the agents actions are kept fixed. Following Rabault et al. (2019), the respective drag and lift coefficients are computed as CDTact ω CD = FD 2 1 2 ρU and CL = FL 2 1 2 ρU with the density ρ = 1 and forces acting on the cylinder FD = (cid:90) (σ n) ex dS and FL = (cid:90) (σ n) ey dS. (3) (4) (5) Here, σ is the Cauchy stress tensor, the unit normal vector at the cylinder surface pointing into the fluid, and ex = (1, 0, 0) and ey = (0, 1, 0) the normal vectors along the and directions, respectively. In the MARL case, individual β) rglobal agent rewards are computed as ri . Local rewards are computed over the cylinder segment controlled by agent i, whereas the global reward is computed for the full cylinder. The local reward weight β defines the impact of the local rewards and is set to 0.8 following Suarez et al. (2025). = β ri,local + ( Actuation Our 2D setups are based on jet actuators with parabolic profile (Rabault et al., 2019) and cylinder rotation (Tokarev et al., 2020) with maximum absolute value of for the jet and rotation velocity, respectively. We further extend the jet actuation setup to 3D following setup similar to previous work (Suarez et al., 2025). Additionally, as cs1), where cs denotes the proposed by Rabault et al. (2019), the action is smoothed over time using cs = cs1 + α(at applied control value at simulation sub-step given the current action at at episode step and previous control step cs1. Observations Observations consist of vertical and horizontal velocity components at the sensor locations indicated in Figure 9. In 3D, the observations also include the spanwise velocity component. To enable transfer from 2D to 3D, the number of sensor planes as well as the included velocity components can be set to match the 2D observations. Difficulty Levels Difficulty is defined via the Reynolds number (Re) and we use easy at Re = 100, medium at Re = 250, and hard at Re = 500. Higher Reynolds numbers increase turbulence intensity and flow unsteadiness, which Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control makes control more challenging. The medium and hard settings introduce three-dimensional flow interactions (Williamson, 1996). C.2. Rayleigh-Benard Convection (a) 2D RBC configuration. Dashed lines indicate heater segments used for actuation. (b) 3D RBC configuration. Actuation is applied via discretized heater patches along the bottom boundary. Each agent receives temperature and velocity observations within local window of size 3 surrounding its actuator. Figure 10. Overview of the 2D and 3D RayleighBenard convection (RBC) environments used in our benchmark. Control is provided through thermal actuation applied at the bottom boundary, while the top boundary is held at fixed lower temperature. The environments support both centralized and decentralized control depending on the number and placement of actuators. For 3D, we omit centralized control in this work due to the large number of actuators. Reward Function The objective is to reduce convective heat transfer. We use the instantaneous dimensionless Nusselt number Nuinstant = RaPr denotes spatial averaging (Pandey et al., 2018) as performance measure, where Nuinstant, where the reference Nusselt number corresponds to the over the domain. The reward is defined as rt = Nuref uncontrolled case. uyT Actuation The control is implemented via localized heaters at the bottom boundary. Before being applied to the domain, the heater temperatures are normalized and clipped to ensure mean of the default bottom temperature and maximum heater limit. Additionally, spatial smoothing is applied to avoid hard transitions in temperature between neighboring heaters. Observations Observations include all velocity components and the temperature at the sensor locations shown in Figure 10. Difficulty Levels We vary the Rayleigh number (Ra) to adjust the turbulence intensity. In 2D: easy at Ra = 8 104 (Vignon et al., 2023), medium at Ra = 4 105, and hard at Ra = 8 2024), medium at Ra = 8 increasingly chaotic convection patterns. 103 (Vasanth et al., 103, and hard at Ra = 104. Higher Rayleigh numbers lead to stronger plume interactions and 105. In 3D: easy at Ra = 6 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control C.3. Flow Past Airfoil Figure 11. Schematic visualization of the 2D airfoil control environment. stationary NACA 0012 airfoil is immersed in uniform inflow at an angle of attack of 20. Actuation is provided through surface-mounted blowing and suction jets distributed along the airfoil surface (highlighted in orange), and sensors are placed at the pink marker locations. The corresponding 3D configuration follows the same setup but extends the domain spanwise with depth of = 1.4. In 3D, the actuation is discretized into four spanwise jet segments, yielding 12 individual actuators. In the MARL setting, each agent controls group of three adjacent jets (one spanwise segment), enabling decentralized control. Reward Function The objective is to improve aerodynamic efficiency by increasing lift relative to drag. The reward at timestep is defined as rt = CLTact CDTact CL,ref CD,ref , (6) where CL and CD denote lift and drag coefficients, respectively, and averaging is performed over the actuation interval Tact. The reference value corresponds to the uncontrolled baseline. Actuation Actuation is implemented using surface-mounted synthetic jet actuators placed on top of the airfoil (Garcia et al., 2025). zero-net mass flux is enforced. As in previous environments, the raw RL control signal is temporally filtered using exponential smoothing to ensure physically consistent actuation. Observations Observations follow the definition for the cylinder flow with sensor locations as shown in Figure 11. As the 3D case is extended similarly to the cylinder, we only visualize the 2D case. Difficulty Levels Difficulty is determined by the Reynolds number (Re), where higher Reynolds numbers lead to more abrupt separation and larger turbulence, which increases the challenge of effective flow control. We define easy at 105. Re = 103, medium at Re = 3 103, and hard at Re = 3 19 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control C.4. Turbulent Channel Flow Figure 12. Schematic visualization of the large TCF environment. The configuration consists of rectangular channel with constantheight cross section, where actuation is applied through spanwise-oriented blowing and suction jets (indicated by the orange plane) along the bottom wall. Sensor measurements are sampled at distance of y+ = 15 from the wall at locations directly above the actuator (shown by the pink plane). smaller channel variant shares the same height but has half the streamwise length and spanwise depth. In the bottom-actuation variant, only the sensor observations from the bottom wall are provided to the control policy. The actuator visualizations are not drawn to scale and do not represent the actual number of control units; they are shown purely for illustration. In the small channel, 32 32 actuators are placed per wall, whereas in the large channel 64 64 actuators are used. Reward Function The reward is defined based on the reduction of instantaneous wall shear stress rt = 1 where τwall,ref is the reference value of the uncontrolled flow. The wall shear stress is computed as τwall/τwall,ref , τwall = ν ux (cid:12) (cid:12) (cid:12) (cid:12)y=0 . (7) For environments with single-wall actuation, only the bottom wall is considered; for dual-wall actuation, the stress is averaged across both walls. Actuation The control is applied via wall-normal blowing and suction at the boundary using multiple spatially distributed actuators, where zero net-mass-flux is enforced. Two configurations are provided: one with actuation at the bottom wall only, and one with actuation at both walls. As in previous environments, we apply exponential smoothing to the action signal to avoid abrupt control variations. Observations Observations include the velocity fluctuations, i.e., the difference from the volume mean velocity, right over the corresponding actuator at wall distance y+ = 15. Difficulty Levels Difficulty is defined using the friction Reynolds number (Reτ ). We use easy at Reτ = 180, medium at Reτ = 330, and hard at Reτ = 550. Opposition Control Baseline For the TCF, common baseline (Guastoni et al., 2023; Sonoda et al., 2023) is opposition control, which sets the wall normal velocity to the negative vertical velocity, i.e., observation. D. Experimental Setup D.1. Hardware and Software Configuration General Experimental Setup Unless stated otherwise, all experiments were conducted using the following shared hardware and software configuration: Python: 3.10 PyTorch: 2.9.1 20 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control CUDA: 12.8 System Memory: 32 GB RAM CPU: 32 cores of an AMD EPYC 7742 (64-core processor) GPU: 1 NVIDIA A100 (40 GB or 80 GB) CylinderJet3D-hard-v0 Environment Experiments for the CylinderJet3D-hard-v0 environment and SARL were conducted on compute nodes with the following differing hardware configuration: CPU: 8 cores of an AMD EPYC 7763 (Milan architecture) GPU: 2 NVIDIA A100 (40 GB) D.2. Algorithm Hyperparameter Configurations The hyperparameters used in our experiments are stated in Tables 5 and 6 for PPO and SAC, respectively. We note that for all SAC experiments on TCF environments, we set the number of gradient steps per update to 1 to avoid excessive gradient updates due to the large number of pseudo multi-agent environments. Table 5. PPO hyperparameters used in all experiments. HYPERPARAMETER POLICY NETWORK LEARNING RATE STEPS PER ROLLOUT (n steps) BATCH SIZE UPDATE EPOCHS (n epochs) DISCOUNT FACTOR (γ) GAE λ CLIP RANGE ADVANTAGE NORMALIZATION ENTROPY COEFFICIENT (c ent) VALUE FUNCTION COEFFICIENT (c vf ) MAX GRADIENT NORM DEVICE VALUE ML PPO Y 3 104 2048 64 10 0.99 0.95 0.2 TRUE 0.01 0.5 0.5 CPU Table 6. SAC hyperparameters used in all experiments except TCF environments. For TCF, we set the number of gradient steps per update to 1. HYPERPARAMETER VALUE POLICY NETWORK LEARNING RATE DISCOUNT FACTOR (γ) SOFT UPDATE COEFFICIENT (τ ) REPLAY BUFFER SIZE BATCH SIZE LEARNING STARTS TRAINING FREQUENCY GRADIENT STEPS PER UPDATE ENTROPY COEFFICIENT (α) TARGET ENTROPY TARGET UPDATE INTERVAL DEVICE ML PPO Y 3 104 0.99 0.005 106 256 100 1 1 (EQUAL TO I E Q) O O 1 CUDA 21 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control D.3. Differentiable Model Predictive Control To isolate the value of reward gradients in our fully differentiable AFC benchmark, we evaluate differentiable model predictive control (D-MPC) baseline that relies solely on gradient information. D-MPC is inspired by differentiable predictive control (DPC, Drgoˇna et al. (2022)) and, at each control step, optimizes sequence of future actions via gradient ascent through the differentiable flow simulator in order to maximize predicted rewards, without using policy network, value function, or model-free exploration. Only the first action of the optimized sequence is executed on the environment, and the horizon is shifted forward, yielding standard receding-horizon control loop. This gradient-only optimization procedure is summarized in Algorithm 1. In our experiments, we set = 20, = 10, α = 0.1, and γ = 0.999 and evaluate ten seeds with each one test set episode for all three 2D Cylinder environments. Algorithm 1 D-MPC: Optimize Action Sequence Input: differentiable env env, start state s0, horizon H, iterations , learning rate α, discount γ, previous actions aprev (optional) Output: optimized action sequence a0:H1 if aprev 0:H1 else 0:H1 is not provided then Initialize a0:H1 Initialize a0:H2 from aprev Initialize aH2 0 1:H1 end if for = 1 to do Rest env Set env to state s0 Detach gradients in env Initialize return for = 0 to 0, discount factor 1 1 do Clamp at to action bounds: at Step env: (st+1, rt, terminated, truncated) rt if terminated or truncated then + γ clip(at, amin, amax) env.step(at) break end if end for Compute loss Backpropagate gradients of Update a0:H1 with gradient descent/Adam using step size α Clamp a0:H1 to action bounds w.r.t. a0:H1 end for return a0:H 22 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control E. Additional Results E.1. Runtime Benchmarks Table 7. Experiment details and GPU runtimes. Total GPU hours are computed as #steps #seeds #seconds #algorithms, where environments not included in this study are set to zero. ENVIRONMENT DIFFICULTY #STEPS #SEEDS #ALGORITHMS SECONDS PER STEP GPU HOURS CYLINDERROT2D CYLINDERROT2D CYLINDERROT2D CYLINDERJET2D CYLINDERJET2D CYLINDERJET2D CYLINDERJET3D CYLINDERJET3D CYLINDERJET3D RBC2D RBC2D RBC2D RBC2D-WIDE RBC2D-WIDE RBC2D-WIDE RBC3D RBC3D RBC3D RBC3D-WIDE RBC3D-WIDE RBC3D-WIDE AIRFOIL2D AIRFOIL2D AIRFOIL2D AIRFOIL3D AIRFOIL3D AIRFOIL3D TCFSMALL3D-BOTH TCFSMALL3D-BOTH TCFSMALL3D-BOTH TCFSMALL3D-BOTTOM TCFSMALL3D-BOTTOM TCFSMALL3D-BOTTOM TCFLARGE3D-BOTH TCFLARGE3D-BOTH TCFLARGE3D-BOTH TCFLARGE3D-BOTTOM TCFLARGE3D-BOTTOM TCFLARGE3D-BOTTOM TOTAL EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD EASY MEDIUM HARD 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 50000 20000 20000 20000 20000 20000 20000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 5 5 5 5 5 5 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 5 5 5 5 5 5 5 5 5 5 5 5 1.241 2.059 2.561 1.259 2.209 2. 4.209 7.684 16.679 1.265 2.232 2.260 1.314 2.292 2.349 1.168 1.157 1.199 1.675 1.689 1.754 18.851 30.145 37. 34.526 60.244 63.913 0.481 0.250 0.248 0.427 0.218 0.220 0.846 0.417 0.417 0.759 0.387 0.408 2 2 2 2 2 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 4 4 4 2 2 2 2 2 2 2 2 2 2 2 172.400 285.960 355.700 174.890 306.740 354.430 701.580 1280.710 2779.910 351.260 620.000 627.730 0.000 0.000 0.000 162.250 160.730 166. 0.000 0.000 0.000 1047.290 1674.740 2071.010 2301.760 0.000 0.000 133.680 69.380 68.940 0.000 0.000 0.000 235.060 115.920 115. 0.000 0.000 0.000 16 334.420 Table 7 states individual environment wall-clock times per step as well as the number of steps, seeds, and total GPU hours of the experiments presented in this paper. Results were obtained by running 80 (8 for medium and hard 3D airfoil cases) RL steps with random actions and averaging the results. Experiments were conducted on single NVIDIA A100 GPU. 23 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control E.2. Quantitative Training Results Figure 13. Mean training reward for CylinderJet2D. Error bars indicate 95% confidence intervals. Figure 14. Mean training reward for CylinderRot2D. Error bars indicate 95% confidence intervals. Figure 15. Mean training reward for CylinderJet3D. Error bars indicate 95% confidence intervals. 24 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 16. Mean training reward for RBC2D. Error bars indicate 95% confidence intervals. Figure 17. Mean training reward for RBC3D. Error bars indicate 95% confidence intervals. Figure 18. Mean training reward for Airfoil2D. Error bars indicate 95% confidence intervals. 25 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 19. Mean training reward for Airfoil3D. Error bars indicate 95% confidence intervals. Figure 20. Mean training reward for TCFSmall3D-both. Error bars indicate 95% confidence intervals. Figure 21. Mean training reward for TCFLarge3D-both. Error bars indicate 95% confidence intervals. 26 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Table 8. Cylinder test set metrics. All values report the interquartile mean (IQM) over test episodes and random seeds. Drag reduction is measured relative to the mean drag over 10 uncontrolled training episodes of the baseflow. Best result per environment is highlighted in bold. ENVIRONMENT ALGORITHM REWARD CD CL DRAG REDUCTION (%) CYLINDERROT2D-EASY-V0 CYLINDERROT2D-EASY-V0 CYLINDERROT2D-EASY-V0 CYLINDERROT2D-MEDIUM-V0 CYLINDERROT2D-MEDIUM-V0 CYLINDERROT2D-MEDIUM-V0 CYLINDERROT2D-HARD-V0 CYLINDERROT2D-HARD-V0 CYLINDERROT2D-HARD-V0 CYLINDERJET2D-EASY-V0 CYLINDERJET2D-EASY-V0 CYLINDERJET2D-EASY-V CYLINDERJET2D-MEDIUM-V0 CYLINDERJET2D-MEDIUM-V0 CYLINDERJET2D-MEDIUM-V0 CYLINDERJET2D-HARD-V0 CYLINDERJET2D-HARD-V0 CYLINDERJET2D-HARD-V0 CYLINDERJET3D-EASY-V0 CYLINDERJET3D-EASY-V0 CYLINDERJET3D-EASY-V0 CYLINDERJET3D-EASY-V0 CYLINDERJET3D-EASY-V0 BASEFLOW PPO SAC BASEFLOW PPO SAC BASEFLOW PPO SAC BASEFLOW PPO SAC BASEFLOW PPO SAC BASEFLOW PPO SAC BASEFLOW PPO SAC MA-PPO MA-SAC CYLINDERJET3D-MEDIUM-V0 CYLINDERJET3D-MEDIUM-V0 CYLINDERJET3D-MEDIUM-V0 CYLINDERJET3D-MEDIUM-V0 MA-PPO CYLINDERJET3D-MEDIUM-V0 MA-SAC BASEFLOW PPO SAC CYLINDERJET3D-HARD-V0 CYLINDERJET3D-HARD-V0 CYLINDERJET3D-HARD-V0 CYLINDERJET3D-HARD-V0 CYLINDERJET3D-HARD-V0 BASEFLOW PPO SAC MA-PPO MA-SAC - 0.002 0.037 3.328 0.042 0.035 3.191 0.016 3.179 - 0.309 0.344 - 0.162 0. 3.152 0.037 2.489 0.060 2.475 0.093 3.619 0.057 2.962 0.028 2.440 0.180 - 0.052 0.051 3.328 0.042 0.065 3.141 0.032 3.105 - 0.274 0.426 - 1.173 1. - 0.217 0.040 0.178 0.041 - 0.187 0.027 0.280 0.034 - 0.184 0.646 0.222 0.133 0.037 3.152 2.487 0.066 2.484 0.004 3.619 2.158 2.011 0.057 0.038 0. 3.305 0.028 3.216 0.009 0.039 3.224 0.030 3.193 0.095 3.103 2.984 0.008 2.764 0.205 0.024 2.791 0.067 2.955 0.045 2.718 2.571 0.018 2.564 0.086 0.190 2.692 2.565 0.040 2.509 0.030 - 4.125 4.477 - 21.033 21.496 - 18.162 32. - 5.638 6.697 - 21.110 21.216 - 40.385 44.426 - 2.719 2.471 3.398 6.118 - 7.395 6.486 0.972 8.934 - 0.286 4.696 0.249 2. 27 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Table 9. RBC test set metrics. All values report the interquartile mean (IQM) over test episodes and random seeds. Heat transfer improvement is measured relative to the mean instant Nusselt number Nuinstant over 10 uncontrolled training episodes of the baseflow. Best result per environment is highlighted in bold. ENVIRONMENT ALGORITHM REWARD Nuinstant HEAT TRANSFER IMPROVEMENT (%) RBC2D-EASY-V0 RBC2D-EASY-V0 RBC2D-EASY-V0 RBC2D-EASY-V0 RBC2D-EASY-V BASEFLOW PPO SAC MA-PPO MA-SAC RBC2D-MEDIUM-V0 RBC2D-MEDIUM-V0 RBC2D-MEDIUM-V0 RBC2D-MEDIUM-V0 MA-PPO RBC2D-MEDIUM-V0 MA-SAC BASEFLOW PPO SAC RBC2D-HARD-V0 RBC2D-HARD-V0 RBC2D-HARD-V0 RBC2D-HARD-V0 RBC2D-HARD-V0 RBC3D-EASY-V0 RBC3D-EASY-V0 RBC3D-EASY-V0 BASEFLOW PPO SAC MA-PPO MA-SAC BASEFLOW MA-PPO MA-SAC RBC3D-MEDIUM-V0 RBC3D-MEDIUM-V0 MA-PPO RBC3D-MEDIUM-V0 MA-SAC BASEFLOW RBC3D-HARD-V0 RBC3D-HARD-V0 RBC3D-HARD-V0 BASEFLOW MA-PPO MA-SAC - 0.888 0.779 1.024 0. - 0.138 0.790 0.056 0.018 - 0.304 0.525 0.484 0.715 - 0.367 0.400 - 0.340 0.384 - 0.341 0.323 4.841 4.008 4.117 3.872 4. 6.856 6.291 5.639 6.373 6.447 7.854 7.547 6.717 7.726 7.958 2.182 1.815 1.782 2.444 2.105 2.061 2.684 2.343 2.361 - 17.200 14.952 20.015 12. - 8.238 17.746 7.041 5.960 - 3.911 14.467 1.622 1.327 - 16.815 18.333 - 13.893 15.692 - 12.713 12.050 Table 10. Airfoil test set metrics. All values report the interquartile mean (IQM) over test episodes and random seeds. Aerodynamic efficiency improvement is measured relative to the mean aerodynamic efficiency over 10 uncontrolled training episodes of the baseflow. Best result per environment is highlighted in bold. ENVIRONMENT ALGORITHM REWARD AERODYNAMIC EFFICIENCY IMPROVEMENT (%) AIRFOIL2D-EASY-V0 AIRFOIL2D-EASY-V0 AIRFOIL2D-EASY-V0 AIRFOIL2D-MEDIUM-V0 AIRFOIL2D-MEDIUM-V0 AIRFOIL2D-MEDIUM-V0 AIRFOIL2D-HARD-V0 AIRFOIL2D-HARD-V0 AIRFOIL2D-HARD-V AIRFOIL3D-EASY-V0 AIRFOIL3D-EASY-V0 AIRFOIL3D-EASY-V0 AIRFOIL3D-EASY-V0 AIRFOIL3D-EASY-V0 BASEFLOW PPO SAC BASEFLOW PPO SAC BASEFLOW PPO SAC BASEFLOW PPO SAC MA-PPO MA-SAC - 1.422 1. - 3.134 3.666 - 1.338 2.636 - 0.105 1.462 0.084 1.584 28 2.887 4.309 4.592 3.572 6.706 7. 6.063 7.401 8.699 2.838 2.733 4.300 2.922 4.422 - 49.265 59.072 - 87.747 102.633 - 22.065 43.470 - 3.691 51.513 2.951 55. Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Table 11. TCF test set metrics. All values report the interquartile mean (IQM) over test episodes and random seeds. Drag reduction is measured relative to the mean wall stress τwall over 10 uncontrolled training episodes of the baseflow. Best result per environment is highlighted in bold. ENVIRONMENT ALGORITHM REWARD τwall DRAG REDUCTION (%) TCFSMALL3D-BOTH-EASY-V0 TCFSMALL3D-BOTH-EASY-V0 TCFSMALL3D-BOTH-EASY-V BASEFLOW MA-PPO MA-SAC TCFSMALL3D-BOTH-MEDIUM-V0 TCFSMALL3D-BOTH-MEDIUM-V0 MA-PPO TCFSMALL3D-BOTH-MEDIUM-V0 MA-SAC BASEFLOW TCFSMALL3D-BOTH-HARD-V0 TCFSMALL3D-BOTH-HARD-V0 TCFSMALL3D-BOTH-HARD-V0 TCFLARGE3D-BOTH-EASY-V0 TCFLARGE3D-BOTH-EASY-V0 TCFLARGE3D-BOTH-EASY-V0 BASEFLOW MA-PPO MA-SAC BASEFLOW MA-PPO MA-SAC TCFLARGE3D-BOTH-MEDIUM-V0 TCFLARGE3D-BOTH-MEDIUM-V0 MA-PPO TCFLARGE3D-BOTH-MEDIUM-V0 MA-SAC BASEFLOW TCFLARGE3D-BOTH-HARD-V0 TCFLARGE3D-BOTH-HARD-V0 TCFLARGE3D-BOTH-HARD-V0 BASEFLOW MA-PPO MA-SAC - 0.207 0. - 0.193 0.173 - 0.120 0.089 - 0.129 0.045 - 0.019 0.094 - 0.001 0.059 0.002 0.001 0. 0.002 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.002 0.001 0.001 0.001 0.001 - 20.689 17. - 19.281 17.290 - 11.999 8.945 - 12.885 4.514 - 1.903 9.415 - 0.113 5.877 E.3. Quantitative Test Results Figure 22. Mean test reward for CylinderJet2D. Figure 23. Mean test reward for CylinderRot2D. 29 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 24. Mean test reward for CylinderJet3D. Figure 25. Mean test reward for RBC2D. Figure 26. Mean test reward for RBC3D. 30 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 27. Mean test reward for Airfoil2D. Figure 28. Mean test reward for Airfoil3D. Figure 29. Mean test reward for TCFSmall3D-both. 31 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 30. Mean test reward for TCFLarge3D-both. 32 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control E.4. Qualitative Test Results In the following, we present qualitative visualizations of uncontrolled and final controlled flow fields for all environments and algorithms for seed 0. Figure 31. Qualitative test results for CylinderJet2D. 33 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 32. Qualitative test results for CylinderRot2D. Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 35 Figure 33. Qualitative test results for CylinderJet3D. Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 36 Figure 34. Qualitative test results for RBC2D. Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 35. Qualitative test results for RBC3D. 37 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 36. Qualitative test results for Airfoil2D. Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 39 Figure 37. Qualitative test results for Airfoil3D. Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 38. Qualitative test results for TCFSmall3D-both. Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Figure 39. Qualitative test results for TCFLarge3D-both."
        }
    ],
    "affiliations": [
        "Lamarr Institute for Machine Learning and Artificial Intelligence, Dortmund, Germany",
        "TU Dortmund University, Dortmund, Germany",
        "Technical University Munich, Munich, Germany"
    ]
}