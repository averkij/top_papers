{
    "paper_title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
    "authors": [
        "Mohamed Elfeki",
        "Rui Liu",
        "Chad Voegele"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases. We introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches. When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount."
        },
        {
            "title": "Start",
            "content": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs Mohamed Elfeki * 1 Rui Liu * 1 Chad Voegele *"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 5 2 0 2 7 2 ] . [ 1 3 7 2 6 1 . 1 0 5 2 : r The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoderdecoders one-time input processing and efficient separation of understanding and generation phases. We introduce novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches. When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount. *Equal contribution. 1Microsoft. Code and models will be released at: microsoft/encoder-decoder-slm. 1 The introduction of the Transformer (Vaswani, 2017) as an encoder-decoder architecture revolutionized sequence modeling and proven remarkably robust over time. While encoder-decoder transformers demonstrated strong performance, their inherent information bottleneck between encoder and decoder components ultimately constrained further scaling to hundreds of billions of parameters, as demonstrated in Raffel et al. (2020); Chowdhery et al. (2023). This limitation led the field toward decoder-only architectures, following Suttons bitter lesson (Sutton, 2019) that general methods leveraging computation often outperform specialized architectures. Models like GPT (Radford, 2018) and LLaMA (Touvron et al., 2023) have demonstrated impressive scaling properties (Kaplan et al., 2020), achieving state-of-the-art performance through increased parameter counts by eliminating this bottleneck and enabling more flexible parameter utilization at scale. Despite this trend, encoder-decoder architectures offer specific advantages through their separation of comprehension and generation stages. The encoder constructs fixed representation of the input sequence, while the decoder performs targeted attention over encoded information during generation. This separation creates two key benefits: efficient handling of divergent input-output distributions (as in summarization and translation) and elimination of key-value (KV) cache requirements for input sequences, making them particularly efficient for tasks involving large inputs like code or long document reasoning/QA. Recent advances in positional encoding, particularly Rotary Positional Embeddings (RoPE), further enhances encoderdecoder architectures in two ways: by enabling precise encoding of relative token distances for improved reasoning over long contexts, and by facilitating efficient parameter utilization through asymmetric scaling. This dual benefit allows the architecture to both handle complex positional reasoning tasks like QA while enabling flexible optimization of encoder and decoder sizes for different tasks. Through pipelined inference, 2N-parameter encoder-decoder model can match the computational efficiency of an N-parameter decoder-only model (Wang et al., 2022). While the architectures inherent information bottleneck Return of the Encoder: Maximizing Parameter Efficiency for SLMs Figure 1. Architectural Efficiency in SLMs. Left: Comparison of architectures where encoder-decoder creates fixed input representation with KV cache only for output, while decoder-only requires growing KV caches for both input and output. Top right: Inference time scaling with input length, showing encoder-decoders efficient fixed-representation approach versus decoder-onlys steeper computational growth. Bottom right: Performance across tasks showing encoder-decoders advantages at fixed compute budget, further enhanced by KD. may constrain scaling to extremely large sizes (Raffel et al., 2020), this same constraint provides valuable inductive biases at more practical scalesas demonstrated by Chia et al. (2023); Raffel et al. (2020), where T5 models at just 20B parameters consistently outperform much larger architectures like the 62B parameter PaLM-1 (Chowdhery et al., 2023) on specific downstream tasks requiring precise positional understanding and efficient parameter utilization. Contributions systematic analysis demonstrating that encoder-decoder architectures achieve superior performance (+2-4% improvement) and efficiency (47% lower latency) at small scales (1B parameters) across GPU, CPU, and NPU platforms. novel knowledge distillation framework enabling small encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural efficiency advantages. An extended architecture incorporating modern advances (RoPE, ViT) that maintains efficiency benefits while generalizing to vision-language tasks, showing consistent improvements in multimodal reasoning. 2. Related Work Language model architectures have evolved through the interplay of theoretical advances and practical constraints, particularly in resource-constrained deployments of smallscale language models (SLMs). 2.1. Evolution of Transformer Architectures The Transformer architecture (Vaswani, 2017) revolutionized sequence modeling by replacing recurrent networks with attention mechanisms. Its core design demonstrated remarkable stability, requiring only minimal refinements such as pre-layer normalization (Xiong et al., 2020) and improved positional embeddings (Su et al., 2024). significant empirical study by (Wang et al., 2022) later validated the advantages of non-causal visibility and masked language modeling, showing encoder-decoder setups consistently outperforming decoder-only architectures. (Liu et al., 2024b) demonstrated that architectural choices become crucial at smaller scales. In particular, their findings revealed that deeper, thinner models and grouped-query attention (GQA) mechanisms are particularly effective for sub-billion parameter models. Building on these insights, our architecture incorporates GQA and emphasizes depth over width. 2.2. Efficient Model Design Recent approaches to efficient modeling have evolved along several paths. Decoder-only solutions like SmolLM (Allal et al., 2024) and MobileLM (Liu et al., 2024b) focus on higher quality data and architectural optimizations respectively, while Llama 3.2 (Dubey et al., 2024) explores pruning and knowledge distillation. While large models rely heavily on prompt engineering for task adaptation, our work suggests that finetuning smaller, more efficient models may offer more practical and scalable approach for deploy2 Return of the Encoder: Maximizing Parameter Efficiency for SLMs ment scenarios. This hypothesis is particularly relevant for resource-constrained environments where the computational overhead of prompt engineering large models may outweigh their performance benefits. Our study isolates architectural choice as the key variable by maintaining consistent training data, parameter counts, and optimization strategies across variants. This controlled comparison reveals fundamental differences in training objectives: decoder-only models employ uniform autoregressive objectives, while encoder-decoder architectures like T5 (Raffel et al., 2020) leverage specialized objectives optimized for their architecture. Studies have shown this specialization to be better (Wang et al., 2022; Tay et al., 2022b), with modern approaches often combining strategies (Tay et al., 2022a; Anil et al., 2023) to balance efficiency and flexibility. Paralleling our work, (Warner et al., 2024) recently showed significant improvements in encoder-only architectures through modernization of both data and architecture. Our work extends this modernization to encoder-decoder architectures while demonstrating their superiority for SLMs. 2.3. Architectural Trade-offs While decoder-only models excel in zero-shot scenarios (Wang et al., 2022), they struggle with long-sequence generation (Fu et al., 2023) and resource utilization. Encoderdecoder architectures, in contrast, offer significant advantages through their architectural design. The one-time input processing with fixed latent representations enables substantial inference optimization, while the natural separation of phases (i.e., encoder for understanding and decoder for generation) allows for efficient parameter distribution across the model. This architecture also enables task-specific optimization through independent sizing of encoder and decoder components, providing flexible asymmetric scaling capabilities. Furthermore, the bidirectional attention mechanism delivers superior performance at smaller scales (Wang et al., 2022), opening opportunities for advanced optimizations such as pooling and linear attention techniques (Tay et al., 2021). While these benefits traditionally came with engineering challenges in handling variable-length inputs, our work shows that modern approaches like RoPE (Su et al., 2024) can effectively mitigate these limitations while preserving the core advantages of the encoder-decoder. 2.4. Knowledge Distillation Advances Knowledge distillation has emerged as crucial bridge between efficiency and performance (Hinton, 2015; Dubey et al., 2024; Sreenivas et al., 2024), with notable successes like Llama 3.3s 70B student matching its 405B teacher. Unlike previous approaches maintaining decoder-only architecture for both student and teacher, we introduce novel crossarchitecture distillation, enabling small encoder-decoder models to benefit from large-scale decoder-only training while preserving their efficiency advantages. 3. Parameter Efficient SLMs Building on insights from decoder-only and encoderdecoder architectures, we present parameter-efficient framework that combines the architectural advantages of encoder-decoders with novel knowledge distillation approach, enabling smaller models to benefit from larger decoder-only teachers while maintaining their inherent efficiency benefits. 3.1. Architectural Design Our encoder-decoder architecture addresses two fundamental challenges in language modeling: efficient handling of variable-length sequences and optimal parameter allocation. While decoder-only models process concatenated input-output sequences uniformly, our approach enables specialized processing and flexible resource distribution based on the distinct roles of understanding (encoder) and generation (decoder). Core Architecture The design builds on the encoderdecoder foundation from (Raffel et al., 2020), with several key modifications for improved efficiency. Following (Wang et al., 2022), we maintain consistent training FLOPs across all architectural variants while incorporating modern components: pre-layer normalization (Xiong et al., 2020), Rotary Positional Embeddings (RoPE) (Su et al., 2024), and Grouped-Query Attention (GQA) (Ainslie et al., 2023). GQA proves particularly valuable in our resourceconstrained setting, aligning with findings from (Liu et al., 2024b) on its effectiveness for small-scale deployments. Sequence Length Management critical piece in our design lies in the handling of variable-length sequences. Traditional encoder-decoder architectures suffer from two key inefficiencies: separate padding requirements for encoder and decoder components, and complex cross-attention management. We address these challenges through an integrated approach to sequence processing. At the core of our solution is the integration of RoPE with neural tangent kernel (NTK) scaling (Chowdhery et al., 2023), enabling flexible sequence length handling without rigid padding constraints. The encoder produces token-wise representations with fixed dimensionality (while the sequence length varies with input), enabling efficient memory utilization during generation. Furthermore, cross-attention computations leverage these pre-computed encoder representations, eliminating the need for repeated input processing. This approach maintains the architectural benefits of separate encoding and decoding while mitigating traditional efficiency bottlenecks. 3 Return of the Encoder: Maximizing Parameter Efficiency for SLMs To validate these efficiency claims, we measured memory utilization and computational overhead when processing sequences of length 4096 with batch size 32 across both architectures. The encoder-decoder used 11-16% less memory during inference due to its fixed-size representations, and required only 78% of the FLOPs compared to the decoderonly model for generating 256 tokens, with the gap widening further for longer generation lengths due to the one-time input processing advantage. Parameter Allocation For our base 330M parameter models, we explore three principal encoder-decoder configurations  (Table 1)  . The first uses 1/3-2/3 split with 12 encoder and 32 decoder layers, the second employs balanced 1/2-1/2 split with 22 layers each, and the third implements 2/3-1/3 split with 32 encoder and 12 decoder layers. For comparison, our decoder-only baseline uses 48 layers to maintain parameter parity parity - the encoderdecoder variants use fewer total layers (44) but achieve parameter matching through additional cross-attention components in their decoder layers. Causal masking is applied throughout decoder-only variants but only in the decoder for encoder-decoder models. We find the 2/3-1/3 configuration consistently outperforms other splits, which we attribute to the inherent asymmetry in sequence-to-sequence tasks: the encoder must capture complex bidirectional relationships and long-range dependencies in the input, while the decoders task of conditional generation can leverage the rich encoded representations and requires fewer layers for effective autoregressive modeling. This aligns with findings from attention pattern analysis showing that early layers predominantly focus on building robust contextual representations, while later layers specialize in task-specific generation (Raffel et al., 2020). Scaling Analysis We extend these configurations to 500M and 1B parameters, focusing on the best-performing 2/3-1/3 split for evaluation on SQuAD and XSum (Figure 2). While we limit our experiments to 1B scale, understanding the transition point where encoder-decoder advantages diminish could provide valuable insights into fundamental architectural trade-offs. We encourage the research community to investigate these limits at larger scales. Our architectures handling of variable-length sequences becomes increasingly advantageous at scale. As sequence lengths grow, the encoder-decoder architecture provides significant efficiency gains through two mechanisms: (1) one-time input processing with storage of only final layer representations, versus decoder-only models need to maintain KV cache states across all layers, and (2) more efficient memory utilization during generation since cross-attention operates on pre-computed encoder states. These benefits are particularly pronounced in tasks involving long documents 4 or multi-step reasoning. The combination of efficient sequence handling and flexible parameter allocation enables our architecture to maintain strong performance while reducing computational overhead, especially for longer sequences where traditional architectures struggle with memory and computation constraints. 3.2. Knowledge Distillation Framework key contribution of our work is novel knowledge distillation framework that enables encoder-decoder models to learn from larger decoder-only architectures, despite their fundamentally different input-output schemasdecoderonly models use unified attention on concatenated sequences, while encoder-decoder models maintain separate processing stages. Building upon (Agarwal et al., 2024), we introduce novel sequence alignment strategies specifically designed for distilling encoder-decoder models from decoder-only teachers. For an input sequence of length x, we first generate output sequence using the student model. We structure the inputs distinctly: the teacher receives concatenated sequence [P AD]ne [P AD]nd (where [P AD] denotes padding tokens, ne and nd are encoder and decoder padding lengths), while the student model processes [P AD]ne in the encoder and [BOS] [P AD]nd in the decoder (where [BOS] denotes the beginning-of-sequence token). This arrangement ensures proper alignment through careful offset management, with teacher logits starting at position (x + ne 1) and student logits coming directly from the decoder output. The complete distillation process follows Algorithm 1, employing temperature parameter τ and combining reverse KL-divergence with cross-entropy loss, where the mixing ratio is tuned per dataset. The temperature parameter serves dual purposes: softening probability distributions for improved knowledge transfer and scaling gradients through the τ 2 term. Further ablation studies (see Appendix) demonstrate the effectiveness of this approach compared to alternative distillation methods. 3.3. Training and Evaluation Methodology Training Process Training is done in two stages. We first conduct pretraining using span corruption (Raffel et al., 2020) with 15% noise ratio and span length = 3 on decontaminated 100B token dataset from FineWeb-Edu (Penedo et al., 2024). For downstream tasks, we implement two training strategies: standard sequence-to-sequence learning with cross-entropy loss and knowledge distillation from Phi-3.5Mini (3.3B parameters) (Abdin et al., 2024) that is fine-tuned on the downstream task. Return of the Encoder: Maximizing Parameter Efficiency for SLMs Algorithm 1 Knowledge Distillation: Decoder-only to Encoder-decoder Require: Teacher model , Student model S, Temperature τ , Loss ratio α 1: Input: Training batch 2: Generate(S, x) {Generate with student} 3: ne encoder seq len {Encoder padding} 4: nd decoder seq len {Decoder padding} 5: // Prepare sequences 6: xt [P AD]ne [P AD]nd {Teacher input} 7: xs [P AD]ne {Student encoder input} 8: ys [BOS] [P AD]nd {Student decoder input} 9: // Forward passes 10: lt (xt) {Teacher logits} 11: ls S(xs, ys) {Student logits} 12: // Align and scale logits 13: lt lt[x + ne 1 : + ne + y] 14: ls ls[: y] 15: pt softmax(lt/τ ) 16: ps softmax(ls/τ ) 17: // Compute mixed loss and optimize 18: Lkd α τ 2 KL(ptps) + (1 α) CE(y, ys) 19: Optimize(S, Lkd) Training Efficiency We train all the models using 16 A100 GPUs. The encoder-decoder (2/3-1/3) model completed pretraining in 250 hours compared to 350 hours for the decoder-only varianta mere 71% training time while achieving superior final performance. While decoderonly models require quadratic attention computations over concatenated input-output sequences, encoder-decoder models process input sequences once, creating reusable fixed representations. This advantage amplifies during training where multiple output tokens are generated and evaluated for each input sequence. Our encoder-decoder model uses only 42% of the FLOPs at 1024-token inputs compared to its decoder-only counterpart. The efficiency gap widens with scaleat 4096 tokens, computational requirements differ by 3.2x, demonstrating how one-time input processing becomes increasingly valuable with longer sequences. Implementation Details We utilize mixed precision training (BF16) with the Muon optimizer (Jordan et al., 2024), employing learning rate of 3e-4 with 2000 warmup steps and cosine decay schedule. Our training setup processes batches of 32 samples per GPU using gradient accumulation, with maximum sequence lengths of 1024 and 256 tokens for input and output, respectively. Evaluation Framework Our evaluation framework targets tasks that stress specific computational aspects: reasoning through SQuAD-v2.0 ((Rajpurkar, 2016)), divergent length distributions via summarization (XSUM) ((Narayan Model Phi-3.3B SQuAD (RL/RG) 0.85/0.96 IELTS (RL/RG) 0.31/0.70 CodeX (RL/RG) 0.93/0. XSum (RL/RG) 0.36/0.28 Average (RL/RG) 0.61/0.67 Decoder-only Seq2Seq KD 0.55/0.90 0.57/0.90 0.30/0.29 0.31/0. 0.91/0.61 0.93/0.63 0.19/0.14 0.24/0.19 0.49/0.49 0.51/0.53 1/3-2/3 1/2-1/2 2/3-1/3 1/3-2/3 1/2-1/2 2/3-1/3 0.61/0.91 0.64/0.92 0.67/0. 0.62/0.93 0.60/0.91 0.69/0.94 Encoder-Decoder (Seq2Seq) 0.93/0.65 0.31/0.29 0.93/0.66 0.31/0.28 0.31/0.26 0.93/0.66 Encoder-Decoder (KD) 0.93/0.70 0.32/0.40 0.93/0.73 0.32/0.45 0.93/0.74 0.32/0.46 0.23/0.15 0.24/0.17 0.25/0.19 0.26/0.19 0.27/0.20 0.27/0.20 0.52/0.50 0.53/0.51 0.54/0. 0.53/0.56 0.53/0.57 0.55/0.59 Table 1. Downstream Task Performance of 330M Model Variants with Varying Encoder-Decoder Allocations and Post-Training. et al., 2018)), structural transformation through code translation with CodeXGLUE ((Lu et al., 2021)), and creative generation via IELTS creative writing ((Chillies, 2024)). We employ two complementary metrics: Rouge-L(RL) for lexical similarity assessment and Ragas-GPT (RG) (Gradients, 2024) for GPT-4 based evaluation (LLM-as-a-judge) of context precision/recall, response relevancy, and output faithfulness. Pretraining Results We conducted pretraining on 100B tokens randomly sampled from FineWeb-Edu (Penedo et al., 2024), ensuring decontamination from evaluation sets following (Lambert et al., 2024). After one epoch, all model variants (both encoder-decoder and decoder-only) achieved comparable performance on preplexity (2.81 0.02), and on closed-form knowledge and common-sense evaluations: MMLU (24.82 0.44), Arc-easy (24.92 0.51), and Arcchallenge (22.950.36), showing no statistically significant differences. Downstream Performance The empirical results presented in Table 1 strongly validate our architectural thesis, particularly at the 330M parameter scale. The 2/31/3 encoder-decoder configuration with knowledge distillation demonstrates superior performance across all tasks: SQuAD 2.0 (0.69/0.94), IELTS (0.32/0.46), CodeXGLUE (0.93/0.74), and XSum (0.27/0.20). These results outperform both Seq2Seq and KD variants of the decoder-only baseline in terms of lexical overlap (RL) and reasoning capabilities (RG). These findings align with observations from (Chung et al., 2024; Wang et al., 2022) that encoder-decoder advantages over decoder-only primarily emerge during posttraining rather than pretraining. The effectiveness of the 2/3-1/3 split, which dedicates more capacity to input processing through the encoder, supports our hypothesis regarding the importance of comprehensive input analysis in smaller models. The consistent improvements from Seq2Seq to KD variants (+2 - 8 Ragas points across tasks) demonstrate that encoder-decoder architectures effectively leverage knowledge from larger models while maintaining their architectural advantages. This is 5 Return of the Encoder: Maximizing Parameter Efficiency for SLMs Platform Model GPU CPU NPU Dec-only Enc-dec (2/3-1/3) Dec-only Enc-dec (2/3-1/3) Dec-only Enc-dec (2/3-1/3) First Throughput (ms) 149 86 2242 1591 358 189 (tok/s) 9.7 37.4 4.0 15.3 26.5 123.8 Table 2. Hardware Cross-Platform Efficiency Analysis (330M). Our analysis focuses on two critical metrics for inference: first token latency for response time assessment, and subsequent token speed for throughput evaluation. We report the average values over 100 examples using an input context length 512 and max output token length 128. The results in Table 2 demonstrate the clear efficiency advantage of encoder-decoder architectures over decoder-only models across all tested hardware platforms. Encoderdecoder models consistently achieve lower first-token latency (42% reduction on GPU, 29% on CPU, 47% on NPU) and significantly higher decoding speed (3.9 on GPU, 3.8 on CPU, 4.7 on NPU). This superiority stems from the encoder-decoders one-time input processing, which eliminates redundant computation present in decoder-only models, making it more efficient choice for resourceconstrained deployments. 4. Extending to Vision-Language Tasks Having demonstrated our architectures efficiency for text tasks, we extend these benefits to vision-language tasks where modality separation proves even more crucial for computational efficiency. 4.1. Vision-Language Architecture Our vision-language model (Figure 3) maintains the core benefits of the text encoder-decoder architecture and incorporating visual processing capabilities. We utilize CLIPs ViT-L-336px (Radford et al., 2021) as our vision encoder, choosing the highest resolution variant to maximize visual understanding. Following (Liu et al., 2023; Li et al., 2024), we employ 2-layer MLP projection layer to align the vision encoders output with the text embedding space. Figure 2. Performance across various model scales across top two architectures (2/3-1/3 enc-dec vs dec-only). particularly evident in tasks involving divergent input-output distributions (XSum: 0.27/0.20 vs 0.24/0.19) and comprehensive reasoning (SQuAD: 0.69/0.94 vs 0.57/0.90). Additionally, on standard SQuAD 2.0 F1/EM metrics, the decoder-only KD model measures 70/66 while the encoderdecoder (2/3-1/3) KD reaches 78/74. Another important remark is that while decoder-only (Seq2Seq) initially shows stronger performance in creative writing tasks (IELTS) compared to encoder-decoder (Seq2Seq) variants due to its enhanced generative capabilities and zero-shot generalization, the encoder-decoder architecture with knowledge distillation ultimately achieves superior performance (0.32/0.46 vs 0.31/0.40) even in this generative domain. Scaling Analysis As illustrated in Figure 2, the architectural advantages persist across different model scales. The performance gap between architectures remains substantial, ranging from +7% at 330M to +6% at 1B parameters. The knowledge distillation boost shows its strongest effect for encoder-decoder models at the 330M scale (+7%), where architectural inductive biases most significantly influence the learning process. Notably, decoder-only models with KD struggle to match the performance of base encoder-decoder models at 330M and fall further behind at larger scalesat 1B parameters, decoder-only with KD underperforms even the base encoder-decoder without KD (0.37 vs. 0.39). 3.4. Hardware Efficiency Analysis To demonstrate practical deployment capabilities, we conduct comprehensive efficiency analysis across three representative platforms: an NVIDIA RTX A6000-48GB GPU for high-performance computing, X1E80100-Snapdragon Qualcomm Oryon CPU for consumer-grade client-side deployment, and Qualcomm SnapDragon Hexagon neural processing unit (NPU) for mobile/edge applications. To maintain consistency in model execution environments, we compiled both 330M architectures into the ONNX format and executed them across CPU, GPU, and NPU platforms. To handle high-resolution images efficiently while preserving detail, we follow (Li et al., 2025; Chen et al., 2024) in implementing high-resolution image processing pipeline. Our approach first partitions input images into sub-images to enable detailed visual analysis.To provide holistic view with global visual context, the original image is also resized to low-resolution thumbnail image that matches the input resolution of the vision encoder (Liu et al., 2024a). These sub-images are then processed independently through the vision encoder, after which the encoded vision tokens are 6 Return of the Encoder: Maximizing Parameter Efficiency for SLMs Figure 3. Vision Language Encoder-Decoder Architecture. concatenated before being fed to the text encoder-decoder. This architecture maintains our principle of separation between encoding and generation, with the complete model comprising three stages: vision encoding, text encoding, and decoding. For comparison, we also implement decoderonly variant by removing the text encoder component, allowing direct evaluation of architectural choices in the multimodal setting. 4.2. Efficient Training Strategy Our process begins with feature alignment, where we train the projection layer on 600K image-caption pairs for one epoch, establishing the foundation for vision-language integration. We then enhance OCR capabilities using 200K image-OCR examples, which is critical for tasks requiring text extraction from images. This is followed by instruction following training on 700K examples to develop general visual reasoning capabilities. Finally, we fine-tune on specific downstream tasks such as VQAv2 (Goyal et al., 2017) and TextVQA (Singh et al., 2019). Our training leverages diverse datasets spanning multiple domains: feature alignment via LLaVA pretraining dataset (Liu et al., 2023), OCR understanding via UReader (Ye et al., 2023) and SynDoG (Kim et al., 2022), diagram understanding through AI2D (Kembhavi et al., 2016), chart comprehension via ChartQA (Masry et al., 2022), document analysis using DocVQA (Mathew et al., 2021), and general visual reasoning through GQA (Hudson & Manning, 2019) and VG (Krishna et al., 2017). Vision Token Compression Processing high-resolution images presents computational challenges, as they can generate between 5k-10k vision tokens. To address this, we implement novel efficient token compression strategy. Our primary approach is variance-based selection, computationally efficient heuristic that reduces vision tokens by 67% (from 3,000 to 1,000) by identifying and filtering lowinformation background regions. We also explored learned token weighting through trainable selection layer, though our experiments suggest this approach requires additional 7 Figure 4. Performance comparison across vision-language tasks. Despite equal parameter constraints (800M), our encoder-decoder architecture consistently outperforms the decoder-only baseline. optimization through auxiliary losses. Recent developments offer promising directions for further efficiency improvements. These include n-token concatenation as demonstrated in Leopard (Jia et al., 2024), dynamic resolution selection through FlexAttention (Li et al., 2025), and semantic patch retention (Chen et al., 2024). These methods complement our architecture and present opportunities for future optimization. 4.3. Experimental Evaluation Our evaluation framework for vision-language models maintains the same principles of efficiency and effectiveness established in our text experiments. For comprehensive assessment, we compare three architectural configurations: an 1/2-1/2 encoder-decoder model with 800M parameters (22 encoder, 22 decoder layers) and parameter-matched decoder-only baseline (48 layers). Our evaluation framework for vision-language models maintains the same principles of efficiency and effectiveness established in our text experiments. Building on insights from work like Hinton (2015) and Agarwal et al. (2024), we leverage knowledge distillation to maximize model capabilities within our computational budget. Our evaluation compares two configurations: an encoder-decoder model with 800M parameters (22 encoder, 22 decoder layers), similarly-distilled decoder-only baseline of 800M parameters (48 layers). Both are distilled from task-finetuned Phi3-Vision (4.1B parameters) (Abdin et al., 2024) as largerscale reference point (teacher). We use knowledge distillation and the same training pipelines consistently across the encoder-decoder and decoder-only variants to ensure fair comparison that isolates architectural differences while maintaining optimal performance for each configuration. The evaluation spans several key tasks strategically chosen to stress different aspects of multimodal understanding: Return of the Encoder: Maximizing Parameter Efficiency for SLMs VQAv2 (Goyal et al., 2017) for complex visual reasoning, TextVQA (Singh et al., 2019) for cross-modal comprehension, and ChartQA (Masry et al., 2022) for structured visual analysis. These tasks specifically probe the architectures abilities to handle divergent input-output distributions and complex cross-modal reasoning. The empirical results in Figure 4 strongly validate our architectural thesis, with the encoder-decoder model consistently outperforming its decoder-only counterpart across all vision-language tasks. Most notably, on tasks requiring complex visual reasoning and cross-modal integration (VQAv2: +11.21%, TextVQA: +8.17%), the encoder-decoder architecture demonstrates superior capability in handling divergent input-output distributions. The performance gap is particularly pronounced in structured visual analysis (ChartQA: +7.28%), where the architectures ability to maintain separate encoding and generation stages proves advantageous. While still showing headroom compared to the larger Phi-3V model, our 800M parameter encoder-decoder achieves these gains while maintaining significant parameter efficiency, reinforcing our core argument about the importance of architectural choices in resource-constrained deployments. Further ablations (see Appendix) demonstrate the effectiveness of this approach. 5. Conclusion and discussion Rather than focusing on state-of-the-art comparisons with orthogonal SLMs of different training configurations like Mehta et al. (2024) and Allal et al. (2024), our work focuses on the fundamental architectural advantages of encoderdecoder designs in resource-constrained deployments. Recent trends have favored massive decoder-only models following Sutton (2019)s bitter lesson, but our systematic investigation reveals that architectural choices become increasingly crucial as parameter budgets decrease. Intriguingly, we find that the encoder-decoder information bottleneck - often cited as limitation for scaling to hundreds of billions of parameters - becomes valuable inductive bias at smaller scales, effectively constraining the model to learn more efficient representations and processing patterns. The encoder-decoder architecture delivers four key benefits in small-scale deployments: Optimized performance for asymmetric tasks and divergent input-output distributions (e.g., summarization, long-context QA) Flexible parameter allocation between components for task-specific optimization (i.e., asymmetric scaling) Training & Inference efficiency through one-time input processing and fixed memory footprint Successful integration of modern advances like ViT (Alexey, 2020) and RoPE (Su et al., 2024), extending benefits to multimodal tasks Further, our knowledge distillation framework effectively bridges large-scale training benefits with efficient deployment, enabling smaller encoder-decoder models to leverage capabilities from larger decoder-only teachers while preserving their architectural advantages. Encoder-Decoder for Reasoning Models The emergence of advanced reasoning models like OpenAIs o1 and o3(OpenAI et al., 2024), along with successful distillations like o-mini series, DeepSeek-R1, and Kimi-k1.5(Team, 2025), demonstrates the effectiveness of knowledge distillation. For smaller-scale deployments, encoder-decoder architectures offer key advantages while maintaining ability to learn from larger teachers: Efficient Long-form Generation: Modern applications increasingly demand lengthy generations, making per-token generation cost crucial. Encoder-decoders one-time input processing eliminates the need for repeated computation of cached input representations Memory Optimization: Fixed memory footprint after initial encoding enables more efficient resource utilization compared to decoder-only models expanding KV cache Natural Handling of I/O Asymmetry: The architectures separation of understanding and generation phases aligns perfectly with modern tasks requiring extensive outputs from concise inputs (e.g., short prompts generating extensive code or detailed CoTs) Future Research Directions While our work shows clear advantages at smaller scales, previous research (Raffel et al., 2020; Fu et al., 2023) identifies the encoder-decoder information bottleneck as key challenge. This points to two critical areas requiring further investigation. First, we need to determine the precise scale at which this bottleneck becomes prohibitive and decoder-only architectures become more advantageous. Second, we should explore novel mechanisms for information flow between encoders and decoders, such as residual connections, to overcome scaling limitations. Additionally, developing specialized knowledge distillation techniques could allow us to combine the benefits of scalable decoder-only training with the efficient inference offered by encoder-decoder architectures. Our findings provide concrete evidence that thoughtful architecture design can outperform scaled-down versions of larger models in resource-constrained deployments (1B parameters). Encoder-decoder architectures with knowledge distillation demonstrate consistent improvements across tasks while reducing latency by 47% and achieving 4.7x higher throughput. These results suggest that architectural deliberation, rather than parameter abundance, may be the key to efficient SLMs for edge and small-scale deployment. Return of the Encoder: Maximizing Parameter Efficiency for SLMs"
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Joshua Elsdon for his crucial contribution in deploying and testing our models on NPU platforms, which enabled our cross-hardware analysis. We also thank Xiaoyan Hu and Justin Wagle for their valuable insights and constructive discussions that helped shape this work."
        },
        {
            "title": "Supplementary Details",
            "content": "The following supplementary section provide detailed ablation studies and experimental analyses that further validate these architectural advantages. We begin by examining different knowledge distillation configurations, followed by exploring various approaches to vision token compression - both of which were crucial design decisions that enabled our architecture to achieve its efficiency gains while preserving model capability. 5.1. Knowledge Distillation Ablation Studies KD Loss Function Analysis We conducted extensive experiments to analyze different knowledge distillation approaches on our encoder-decoder architecture, particularly focusing on the 2/3-1/3 split configuration with the SQuAD dataset. Figure 5 presents the results across different KD variants and model scales. The study explores several key dimensions: Loss Mixing Parameter (α): We varied α from 0.0 to 1.0, where α = 0 corresponds to pure sequence-tosequence learning and α = 1.0 represents pure KD loss. Teacher-Student Generation: We compared forward teacher (using teacher generations), reverse teacher (using student generations), and forward student (using student generations for KD). Model Scaling: We evaluated the same KD method across different model sizes (330M, 500M, and compared against Phi3.5). Key findings from this ablation study include: 1. For SQuAD specifically, sequence-to-sequence learning (α = 0.0) shows strong performance, with results generally improving as α approaches 0. This trend was unique to SQuAD and not observed across other datasets. 2. Different KD loss functions (forward vs. reverse) show comparable performance, suggesting that for this specific task and architecture, the choice of KD method is less critical than other hyperparameters. Figure 5. Knowledge Distillation ablation study on SQuAD showing Rouge-L scores across different KD configurations. First six columns compare KD methods (forward/reverse) and loss mixing ratios (α) on 330M models, where α = 0.0 represents pure sequence-to-sequence learning. Last three columns demonstrate performance scaling from 330M to Phi3.5. Error bars show standard deviation across 3 runs. 3. Using student generations for KD (on-policy distillation) demonstrates competitive performance while offering practical advantages: faster training times and elimination of teacher generation caching requirements. 4. The scaling behavior across model sizes (330M 500M Phi3.5) follows expected scaling laws, providing validation for our architectural choices at different scales. These findings align with recent work on generalized knowledge distillation (Agarwal et al., 2024), particularly regarding the effectiveness of on-policy distillation. However, our results suggest that the optimal KD strategy may be taskdependent, especially for specialized tasks like question answering. 5.2. Vision Token Compression Analysis For our vision-language integration, we conducted detailed ablation studies on different token compression and crossattention strategies, as shown in Table 3. Our experiments focused on the TextVQA dataset, which requires both strong visual understanding and text comprehension. We evaluated several architectural variants: Baseline: Standard sequence-to-sequence architecture without specialized vision components Cross-attention Placement: We tested vision-cross attention in both decoder and encoder components Token Selection Strategies: We compared two approaches for managing vision tokens: 9 Return of the Encoder: Maximizing Parameter Efficiency for SLMs Model Baseline (seq2seq) Vision-cross attn on decoder Vision-cross attn on encoder Variance-based selection Learned token weighting RougeLSum 0.5506 0.5546 0.5544 0.5383 0.4787 Table 3. Ablation Study Across Vision-Language Model Variants on TextVQA Variance-based selection: computationally efficient approach that reduces vision tokens by 67% (from 3,000 to 1,000) Learned token weighting: trainable selection mechanism using auxiliary losses Key findings from this study include: 1. Both encoder and decoder cross-attention placements show similar improvements over the baseline (+0.004 Rouge-L), suggesting that the presence of vision-text interaction is more important than its specific location. 2. Variance-based selection, while showing small performance decrease (-0.012 Rouge-L), offers favorable efficiency-performance trade-off given its significant reduction in computational requirements. 3. Learned token weighting, despite its theoretical appeal, shows degraded performance (-0.072 Rouge-L). This may be due to optimization challenges or the complexity of training the selection mechanism jointly with the main task. These results informed our final architecture choices, favoring the simpler variance-based selection approach for its balance of efficiency and performance. The comparable performance of encoder and decoder cross-attention also supports our architectural principle of maintaining clean separation between modality processing stages."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. Alexey, D. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929, 2020. Allal, L. B., Lozhkov, A., and Bakouch, E. Smollm - blazingly fast and remarkably powerful. https:// huggingface.co/blog/smollm, 2024. Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, arXiv preprint Z., et al. Palm 2 technical report. arXiv:2305.10403, 2023. Chen, K., Thapa, R., Chalamala, R., Athiwaratkun, B., Song, S. L., and Zou, J. Dragonfly: Multi-resolution zoom supercharges large visual-language model. arXiv preprint arXiv:2406.00977, 2024. Chia, Y. K., Hong, P., Bing, L., and Poria, S. Instructeval: Towards holistic evaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757, 2023. Ielts writing dataset, Chillies. tion huggingface.co/datasets/chillies/ IELTS-writing-task-2-evaluation. cessed: 2024-12-03. task URL 2024. 2 evaluahttps:// AcChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fu, Z., Lam, W., Yu, Q., So, A. M.-C., Hu, S., Liu, Z., and Collier, N. Decoder-only or encoder-decoder? interpreting language model as regularized encoder-decoder. arXiv preprint arXiv:2304.04052, 2023. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Garea, S. R., Geist, M., and Bachem, O. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations, 2024. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. 10 Return of the Encoder: Maximizing Parameter Efficiency for SLMs Gradients, E. Ragas Documentation, 2024. URL https: //docs.ragas.io/en/stable/. Accessed: 202412-07. Hinton, G. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Hudson, D. A. and Manning, C. D. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6700 6709, 2019. Jia, M., Yu, W., Ma, K., Fang, T., Zhang, Z., Ouyang, S., Zhang, H., Jiang, M., and Yu, D. Leopard: vision language model for text-rich multi-image tasks. arXiv preprint arXiv:2410.01744, 2024. Jordan, K., Jin, Y., Boza, V., Jiacheng, Y., Cecista, F., Newhouse, L., and Bernstein, J. Muon: An optimizer for hidden layers in neural networks, 2024. URL https: //kellerjordan.github.io/posts/muon/. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., and Farhadi, A. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 235251. Springer, 2016. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., and Park, S. Ocr-free document understanding transformer. In European Conference on Computer Vision, pp. 498517. Springer, 2022. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., et al. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Li, J., Chen, D., Cai, T., Chen, P., Hong, Y., Chen, Z., Shen, Y., and Gan, C. Flexattention for efficient high-resolution vision-language models. Computer Vision, pp. 286302. Springer, 2025."
        },
        {
            "title": "In European Conference on",
            "content": "Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2023. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines In Proceedings of the with visual instruction tuning. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024a. Liu, Z., Zhao, C., Iandola, F., Lai, C., Tian, Y., Fedorov, I., Xiong, Y., Chang, E., Shi, Y., Krishnamoorthi, R., et al. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905, 2024b. Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C. B., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., and Liu, S. Codexglue: machine learning benchmark dataset for code understanding and generation. CoRR, abs/2102.04664, 2021. Masry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: In Proceedings dataset for vqa on document images. of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Mehta, S., Sekhavat, M. H., Cao, Q., Horton, M., Jin, Y., Sun, C., Mirzadeh, S. I., Najibi, M., Belenko, D., Zatloukal, P., et al. Openelm: An efficient language model family with open training and inference framework. In Workshop on Efficient Systems for Foundation Models II@ ICML2024, 2024. Narayan, S., Cohen, S. B., and Lapata, M. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018. OpenAI, :, Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., Iftimie, A., Karpenko, A., Passos, A. T., Neitz, A., Prokofiev, A., Wei, A., Tam, A., Bennett, A., Kumar, A., Saraiva, A., Vallone, A., Duberstein, A., Kondrich, A., Mishchenko, A., Applebaum, A., Jiang, A., Nair, A., Zoph, B., Ghorbani, B., Rossen, B., Sokolowsky, B., Barak, B., McGrew, B., Minaiev, B., Hao, B., Baker, B., Houghton, B., McKinzie, B., Eastman, B., Lugaresi, 11 Return of the Encoder: Maximizing Parameter Efficiency for SLMs C., Bassin, C., Hudson, C., Li, C. M., de Bourcy, C., Voss, C., Shen, C., Zhang, C., Koch, C., Orsinger, C., Hesse, C., Fischer, C., Chan, C., Roberts, D., Kappler, D., Levy, D., Selsam, D., Dohan, D., Farhi, D., Mely, D., Robinson, D., Tsipras, D., Li, D., Oprica, D., Freeman, E., Zhang, E., Wong, E., Proehl, E., Cheung, E., Mitchell, E., Wallace, E., Ritter, E., Mays, E., Wang, F., Such, F. P., Raso, F., Leoni, F., Tsimpourlas, F., Song, F., von Lohmann, F., Sulit, F., Salmon, G., Parascandolo, G., Chabot, G., Zhao, G., Brockman, G., Leclerc, G., Salman, H., Bao, H., Sheng, H., Andrin, H., Bagherinezhad, H., Ren, H., Lightman, H., Chung, H. W., Kivlichan, I., OConnell, I., Osband, I., Gilaberte, I. C., Akkaya, I., Kostrikov, I., Sutskever, I., Kofman, I., Pachocki, J., Lennon, J., Wei, J., Harb, J., Twore, J., Feng, J., Yu, J., Weng, J., Tang, J., Yu, J., Candela, J. Q., Palermo, J., Parish, J., Heidecke, J., Hallman, J., Rizzo, J., Gordon, J., Uesato, J., Ward, J., Huizinga, J., Wang, J., Chen, K., Xiao, K., Singhal, K., Nguyen, K., Cobbe, K., Shi, K., Wood, K., Rimbach, K., Gu-Lemberg, K., Liu, K., Lu, K., Stone, K., Yu, K., Ahmad, L., Yang, L., Liu, L., Maksin, L., Ho, L., Fedus, L., Weng, L., Li, L., McCallum, L., Held, L., Kuhn, L., Kondraciuk, L., Kaiser, L., Metz, L., Boyd, M., Trebacz, M., Joglekar, M., Chen, M., Tintor, M., Meyer, M., Jones, M., Kaufer, M., Schwarzer, M., Shah, M., Yatbaz, M., Guan, M. Y., Xu, M., Yan, M., Glaese, M., Chen, M., Lampe, M., Malek, M., Wang, M., Fradin, M., McClay, M., Pavlov, M., Wang, M., Wang, M., Murati, M., Bavarian, M., Rohaninejad, M., McAleese, N., Chowdhury, N., Chowdhury, N., Ryder, N., Tezak, N., Brown, N., Nachum, O., Boiko, O., Murk, O., Watkins, O., Chao, P., Ashbourne, P., Izmailov, P., Zhokhov, P., Dias, R., Arora, R., Lin, R., Lopes, R. G., Gaon, R., Miyara, R., Leike, R., Hwang, R., Garg, R., Brown, R., James, R., Shu, R., Cheu, R., Greene, R., Jain, S., Altman, S., Toizer, S., Toyer, S., Miserendino, S., Agarwal, S., Hernandez, S., Baker, S., McKinney, S., Yan, S., Zhao, S., Hu, S., Santurkar, S., Chaudhuri, S. R., Zhang, S., Fu, S., Papay, S., Lin, S., Balaji, S., Sanjeev, S., Sidor, S., Broda, T., Clark, A., Wang, T., Gordon, T., Sanders, T., Patwardhan, T., Sottiaux, T., Degry, T., Dimson, T., Zheng, T., Garipov, T., Stasi, T., Bansal, T., Creech, T., Peterson, T., Eloundou, T., Qi, V., Kosaraju, V., Monaco, V., Pong, V., Fomenko, V., Zheng, W., Zhou, W., McCabe, W., Zaremba, W., Dubois, Y., Lu, Y., Chen, Y., Cha, Y., Bai, Y., He, Y., Zhang, Y., Wang, Y., Shao, Z., and Li, Z. Openai o1 system card, 2024. Penedo, G., Kydlıˇcek, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L. V., and Wolf, T. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/ 2406.17557. erative pre-training. Advances in Neural Information Processing Systems (2017), 2018. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Rajpurkar, P. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Sreenivas, S. T., Muralidharan, S., Joshi, R., Chochowski, M., Patwary, M., Shoeybi, M., Catanzaro, B., Kautz, J., and Molchanov, P. Llm pruning and distillation in practice: The minitron approach. arXiv preprint arXiv:2408.11796, 2024. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Sutton, R. S. The bitter //www.incompleteideas.net/IncIdeas/ BitterLesson.html, 2019. Accessed: 2023-11-07. lesson. http: Tay, Y., Tran, V. Q., Ruder, S., Gupta, J., Chung, H. W., Bahri, D., Qin, Z., Baumgartner, S., Yu, C., and Metzler, D. Charformer: Fast character transformers via gradient-based subword tokenization. arXiv preprint arXiv:2106.12672, 2021. Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J., Wang, X., Chung, H. W., Shakeri, S., Bahri, D., Schuster, T., et al. Ul2: Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022a. Tay, Y., Wei, J., Chung, H. W., Tran, V. Q., So, D. R., Shakeri, S., Garcia, X., Zheng, H. S., Rao, J., Chowdhery, A., et al. Transcending scaling laws with 0.1% extra compute. arXiv preprint arXiv:2210.11399, 2022b. Team, K. Kimi k1.5: Scaling reinforcement learning with Radford, A. Improving language understanding by genllms. Github, 2025. 12 Return of the Encoder: Maximizing Parameter Efficiency for SLMs Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Wang, T., Roberts, A., Hesslow, D., Le Scao, T., Chung, H. W., Beltagy, I., Launay, J., and Raffel, C. What language model architecture and pretraining objective works best for zero-shot generalization? In International Conference on Machine Learning, pp. 2296422984. PMLR, 2022. Warner, B., Chaffin, A., Clavie, B., Weller, O., Hallstrom, O., Taghadouini, S., Gallagher, A., Biswas, R., Ladhak, F., Aarsen, T., Cooper, N., Adams, G., Howard, J., and Poli, I. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference, 2024. Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 10524 10533. PMLR, 2020. Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Xu, G., Li, C., Tian, J., Qian, Q., Zhang, J., et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023."
        }
    ],
    "affiliations": [
        "Microsoft"
    ]
}