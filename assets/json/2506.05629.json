{
    "paper_title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
    "authors": [
        "Ananth Muppidi",
        "Abhilash Nandy",
        "Sambaran Bandyopadhyay"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The performance of large language models in domain-specific tasks necessitates fine-tuning, which is computationally expensive and technically challenging. This paper focuses on parameter-efficient fine-tuning using soft prompting, a promising approach that adapts pre-trained models to downstream tasks by learning a small set of parameters. We propose a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that generates soft prompts based on the input tokens and attends different tokens with varying importance. Our method is simple and efficient, keeping the number of trainable parameters small. We show the merits of the proposed approach compared to state-of-the-art techniques on various tasks and show the improved zero shot domain transfer capability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 9 2 6 5 0 . 6 0 5 2 : r Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs Ananth Muppidi* IIIT Hyderabad India ananth.muppidi21@gmail.com Abhilash Nandy* IIT Kharagpur India nandyabhilash@gmail.com Sambaran Bandyopadhyay Adobe Research India samb.bandyo@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "The performance of large language models in domain-specific tasks necessitates finetuning, which is computationally expensive and technically challenging. This paper focuses on parameter-efficient fine-tuning using soft prompting, promising approach that adapts pre-trained models to downstream tasks by learning small set of parameters. We propose novel Input Dependent Soft Prompting technique with self-Attention Mechanism (IDSPAM) that generates soft prompts based on the input tokens and attends different tokens with varying importance. Our method is simple and efficient, keeping the number of trainable parameters small. We show the merits of the proposed approach compared to state-of-theart techniques on various tasks and show the improved zero shot domain transfer capability."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have made significant advancements in natural language processing tasks, such as generation, translation and summarization (Yeo et al., 2023; Zhang et al., 2023a). Despite their success, LLMs performance in domainspecific tasks is limited, and fine-tuning on taskoriented datasets is crucial. As models from BERT (Devlin et al., 2019) to GPT-3 (Brown et al., 2020) have millions to billions of parameters, fine-tuning becomes computationally expensive and challenging. Therefore, parameter efficient fine-tuning (Han et al., 2024) research aims to adapt pre-trained models to downstream tasks by fixing most parameters and only learning small subset. Soft prompting is promising direction for finetuning large models. Without changing the core architecture of an LLM, soft prompt methods generally introduce small trainable vector (known as soft prompt) at the beginning of one or more *Equal contribution. Work done during the internship at Adobe Research. 1 transformer layers inputs within the LLM. During fine tuning, only the soft prompt is trained to adapt to the downstream task keeping the parameters of the base LLM frozen. Lester et al. (2021) propose Prompt Tuning by prepending the trainable soft prompt vector before the embeddings of the text input, just after the embedding layer of the base LLM. On similar lines, Li and Liang (2021) introduce Prefix Tuning by prepending soft prompt at every transformer layer and Liu et al. (2021) come up with P-tuning by interleaving learnable prompts with input embeddings. Contrary to text prompt engineering (Wei et al., 2022) or optimizing discrete token representations via in-context learning (Dai et al., 2023), Petrov et al. (2023) suggest that the continuous embedding space of soft prompts inherently possesses greater amount of information. Recent literature introduces several variants of soft prompt techniques such as removing the reparameterization module (Liu et al., 2022b), hierarchical structured pruning (Ma et al., 2022), introducing an adaptive gate mechanism to control the prefix importance in each transformer layer (Zhang et al., 2023b), diving the soft prompt into query, key and value prompts (Wang et al., 2023), learning multiple short soft prompts and gating mechanism to route an input to specific soft prompt (Choi et al., 2023), and decomposing the soft prompt into low rank matrices (Shi and Lipani, 2024). Many of these methods keep the soft prompt independent of the actual input given to the LLM. However, this limits the soft prompt to adjust based on the actual input during the inference time. It is unlikely that unified prompt would lead to performance improvement across different input instances. It also makes the training difficult by increasing the convergence time. To address this, few recent approaches leverage input dependent soft prompts. But they need to concatenate the soft prompts either at every transformer layer of the base LLM (Wu et al., 2022) or all the layers after an Figure 1: ID-SPAM Framework. Given an LM, the generated soft-prompt can be prepended to any transformer layers inputs (the figure can be best seen in color) intermediate layer (Liu et al., 2022a), or transform the soft prompt by using cross-attention with the input tokens without explicitly generating from them (Jin et al., 2023). These input dependent prompting techniques still have multiple limitations: (i) Many of them employ relatively complicated architecture by concatenating soft prompts in multiple internal transformer layers of the LLM; (ii) Since, task may contain diverse samples with different types of words, it is important to attend different words of the input with different weights while generating the soft prompt; And (iii) Number of trainable parameters often increases significantly. To address the above research gaps, we introduce an input dependent soft prompt technique where the soft prompt is generated by trainable network that attends different tokens of the input with different importance by employing self-attention mechanism. We prepend the soft prompt with the input to single transformer layer of the base LLM, keeping the number of trainable parameters small and training smooth. Following are the contributions made in this work: (i) We propose ID-SPAM, novel (Input Dependent Soft Prompting technique with self-Attention Mechanism); Our method is simple and efficient to train. (ii) We show the merit of the proposed approach on six tasks from the GLUE benchmark (Wang et al., 2018); And (iii) Due to the use of trainable attention on the input tokens, our approach is more efficient in zero-shot domain transfer as shown in the experiment."
        },
        {
            "title": "2 Proposed Solution",
            "content": "In this section, we introduce our proposed method ID-SPAM (see its framework in Figure 1). Given Task having training data represented as Dtrain = {(xi, yi)}K i=1. Following Lester et al. (2021), we represent the input as xi = E([SEP]S1[SEP]S2[EOS]) for task with pair of sentences S1, S2 as the input or xi = E([SEP]S1[EOS]) for for task with single sentence S1 as the input, where E() is the token embedding for the input sentence(s). We introduce learnable soft prompt such that the prompt not only varies with the task at hand, but is also generated based on the input in such way that it primarily attends to those input tokens that are essential for the given task. To make the learning efficient, we freeze the parameters of the original LM . Our proposed soft prompt for the task can be defined as ST Rnt, where is the number of tokens in the prompt representation and is the hidden dimension of the LM under consideration. ST is obtained by first applying learnable attention layer (Vaswani et al., 2017) over the input embeddings E() and averaging the outputs, providing context-rich representation. The 1 dimensional vector so obtained is passed through downward projection MLP Layer having learnable weights Wdown Rnc and bias bdown Rc, followed by ReLU Activation Layer (Nair and Hinton, 2010), and then an upward projection MLP 2 Layer having learnable weights Wup Rcn.t and bias bdown Rn.t, where < n. The output so obtained is re-sized to get the learnable, inputdependent soft prompt ST Rnt, which is either prepended to the token embeddings or to the input of any intermediate transformer layer of the LM M. We will show some analysis on the choice of intermediate layer in the experiments. Mathematically, = mean softmax (cid:32) (EWQ) (EWK ) dk (cid:33) (EWV ) ST = resize(σ(Wupσ(Wdown(A)))) (1) (2) WQ, WK, and WV are the query, key, and value parameter matrices respectively, and 1 is scaldk ing factor, as used in Vaswani et al. (2017). σ is non-linear activation which we used ReLU here."
        },
        {
            "title": "3 Experimental Evaluation",
            "content": "Here, we describe our experimental setup, evaluate ID-SPAM framework on GLUE and SuperGLUE benchmarks, and zero-shot domain transfer between tasks against several baselines, followed by detailed analysis. 3.1 Experimental Setup We compare ID-SPAM with the following baselines - (1) Transformer fine-tuning: Here, all parameters of LM are learned (2) Parameter-Efficient Soft Prompt-based Methods - (a) Prompt Tuning: We use standard prompt tuning (Lester et al., 2021), which learns soft prompts through backpropagation to condition frozen language models (b) P-tuning: P-tuning (Liu for specific tasks. et al., 2022b) is variant of Deep Prompt Tuning (Li and Liang, 2021; Qin and Eisner, 2021) adapted for NLU (c) Sparse Mixture of Prompts (SMoP): SMoP (Choi et al., 2023) leverages multiple short soft prompts with gating mechanism to train multiple prompts tailored in addressing different data subsets (d) Late Prompt Tuning (LPT): LPT (Liu et al., 2022a) injects late prompt into an intermediate layer of the LM, rather than into the input layer or across all layers. (e) Decomposed Prompt Tuning (DEPT): DEPT (Shi and Lipani, 2024) employs decomposition strategy for the soft prompt, breaking it down into pair of low-rank matrices. These components are then optimized independently, each with its own specific learning rate. (3) Parameter Efficient Fine-tuning using LowRank Adaptation (LoRA): LoRA (Hu et al., 2022) addresses challenge of fine-tuning large language models by freezing pre-trained models weights and introducing trainable low-rank matrices into each layer. Note that it does not use soft prompt. For all methods, we train upto 30 epochs (Section of Appendix shows convergence after 30 epochs) using Standard Cross-Entropy Loss and Adam Optimizer (Loshchilov and Hutter, 2018), and number of soft-prompt tokens = 10. We perform hyperparameter tuning for ID-SPAM, as described in Section of Appendix. We use NVIDIA A100 GPU with VRAM of 80 GB for all experiments. 3.2 Evaluation on GLUE Benchmark We evaluate ID-SPAM and baselines on the following 6 Natural Language Understanding (NLU) Tasks from GLUE Benchmark (Wang et al., 2018) - SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005), and QQP (Quora, 2017). These tasks cover various aspects of natural language understanding and inference, providing comprehensive assessment of our approachs performance across different language processing challenges. All datasets were obtained from the Hugging Face library (Wolf et al., 2020; Lhoest et al., 2021). Further dataset statistics are shared in Table 1. We report accuracy for {SST, MNLI, QNLI, RTE} and average of accuracy and macro F1Score for {MRPC, QQP} using RoBERTa-BASE, RoBERTa-LARGE backbones (Liu et al., 2019) in Table 2. that We infer ID-SPAM outperforms all Parameter-Efficient Soft Prompt-based baselines on 4 out of 6 GLUE tasks and w.r.t average task performance, and is close second for 2 tasks, when using both RoBERTa-BASE and RoBERTaLARGE backbones. This could be attributed to the attention layer followed by 2-layer MLP in IDSPAM, which efficiently generates context-rich soft prompt. Also, ID-SPAM is shown to be more or similarly efficient compared to well-performing LPT baseline in Section of Appendix. Section of Appendix shows - ID-SPAM performs better than Soft Prompt baselines - (1) on 2/4 and 3/4 SuperGLUE (Wang et al., 2019) tasks using RoBERTA-BASE and RoBERTA-LARGE backbones respectively, while giving best average score; (2) when using autoregressive GPT-2 backbone on 3/6 and 2/4 GLUE and SuperGLUE tasks Category Single-sentence Sentence-pair Datasets SST-2 MNLI MRPC QNLI QQP RTE Train 67349 392702 3668 104743 363846 2490 Dev 872 19647 408 5463 40430 277 Labels 2 3 2 2 2 Type sentiment NLI paraphrase NLI paraphrase NLI Labels positive, negative entailment, neutral, contradiction equivalent, not equivalent entailment, not entailment equivalent, not equivalent entailment, not entailment Table 1: Statistics of the datasets used in our experiments. MNLI QNLI SST-2 MRPC RTE QQP Mean 3.3 Evaluation on SuperGLUE Benchmark Method GLUE (RoBERTa-BASE Backbone) Fine-tuning LoRA Prompt Tuning P-Tuning SMoP LPT DEPT ID-SPAM (ours) Method Fine-tuning LoRA Prompt Tuning P-Tuning SMoP LPT DEPT ID-SPAM (ours) 87.42.4 88.70.4 78.32.1 822.2 80.71.0 81.70.6 81.50.3 83.10.8 87.61.7 89.11.1 83.41.1 86.40.7 86.71.1 84.21.1 83.31.2 87.40.8 91.31.0 84.22.1 81.41.1 82.50.3 82.91.4 83.21.1 87.91.2 86.40.4 92.30.6 90.40.3 89.31.4 88.10.5 89.80.3 91.81.3 90.21.2 92.71.2 92.70.7 79.30.5 74.40.7 81.91.7 78.12.1 84.30.2 75.70.6 82.80. 82.51.3 77.61.1 57.90.5 67.40.9 71.71.8 73.60.7 71.21.0 79.20.4 GLUE (RoBERTa-LARGE Backbone) 94.72.3 87.90.3 88.20.2 88.71.2 88.42.2 86.10.5 88.81.3 91.10.4 95.41.3 95.10.2 92.60.5 95.80.8 95.81.4 93.41.4 91.21.8 94.61.2 92.11.2 86.50.9 73.91.4 76.31.1 79.60.8 87.30.2 77.70.3 86.10.3 88.40.3 78.70.1 60.80.6 62.60.5 76.31.4 74.20.7 73.20.8 81.10. 90.90.8 81.80.2 77.81.6 84.20.1 83.70.9 84.10.5 79.20.3 84.60.5 90.70.2 88.40.3 81.20.6 85.21.3 86.70.3 85.31.3 82.20.7 88.40.5 89.5 83.7 76.5 81 81.2 83.1 81.0 84.8 91.48 87.6 80.0 82.5 85.6 85.1 82.7 88.1 Table 2: Test results on GLUE benchmark. We use RoBERTa-BASE, RoBERTa-LARGE Backbones for all methods. We report the score, along with stddev for 3 runs (in the subscript) for all tasks. The best performing Soft Prompt-based methods results are in bold respectively, while giving better average score; (3) on average when using GPT-2 Large Backbone. Comparison with LoRA: ID-SPAM gives better average score compared to LoRA. Specifically, IDSPAM outperforms LoRA in 5/6 and 3/6 tasks when using RoBERTa-BASE and RoBERTa-LARGE backbones respectively. Also, ID-SPAM is shown to be more efficient than LoRA based on the number of trainable parameters and training and inference times in Section of Appendix. Ablation Analysis: We compare the results of IDSPAM with just using mean-pooling directly using the RoBERTa-LARGE backbone on 3 GLUE Datasets in Table 3. ID-SPAM outperforms meanpooling on all 3 tasks, giving an average improvement of 5.82%, thus highlighting the importance of the self-attention layer in ID-SPAM. Method Mean-pooling ID-SPAM MRPC RTE QQP 84.2 75.2 88.4 81.1 82.3 86.1 Table 3: Ablation Analysis on ID-SPAM We compare ID-SPAM with several Soft PromptBased Baselines on 4 SuperGLUE Datasets using RoBERTA-BASE and RoBERTA-LARGE backbones in Tables and 5 respectively. We observe that ID-SPAM outperforms the baselines on 2/4 and 3/4 tasks using RoBERTA-BASE and RoBERTALARGE backbones respectively, while also giving the best average score. Prompt Tuning P-Tuning SMoP LPT DEPT ID-SPAM CB COPA MultiRC BoolQ Mean 64.8 67.2 75.9 65.6 67.9 76.3 68.6 67.2 79.9 69.2 70.8 80.6 67.5 67.1 78.6 71.1 72.9 83.9 63.6 63.7 69.7 66.3 71.4 69.9 52.5 54.7 57.7 59.2 52.9 57.8 Table 4: Test results on 4 SuperGLUE Datasets using RoBERTa-BASE Backbone. The best performing method is bold for each task. Prompt Tuning P-Tuning SMoP LPT DEPT ID-SPAM CB COPA MultiRC BoolQ Mean 65.4 67.2 78 65.8 68.1 76 70.4 69.6 81.9 70.2 71.0 82 69.0 79 68.2 72.0 73.0 63.3 64.0 71.1 68.0 71.0 70.0 53 55 59 60 54 60 Table 5: Test results on 4 SuperGLUE Datasets using RoBERTa-LARGE Backbone. The best performing method is bold for each task. 3.4 Zero-Shot Task, Domain Transfer Table 6 shows Zero-Shot Transfer using RoBERTaLARGE backbone, where model is trained on training set of dataset, evaluated on another dataset. We use (QQP, MRPC) and (SST-2, IMDB)1 pairs for transfer across tasks and domains respectively similar to Lester et al. (2021). Table 6 shows ID-SPAM performs better than Soft Promptbased baselines, showing ID-SPAM is generaliz1Task for SST-2 and IMDB is binary classification. SSTcontains phrases, while IMDB contains full movie reviews 4 able across datasets. ID-SPAM even outperforms Fine-tuning in 3/4 pairs. Also, even though IDSPAM has much less number of parameters compared to LoRA (see Section of Appendix), IDSPAM gives better/comparable performance. In addition, we show that ID-SPAM performs better/comparable to well-performing LPT baseline in Few-Shot Task Transfer in Section of Appendix. Tuning Method Fine-tuning LoRA Prompt Tuning P-Tuning SMoP LPT DEPT ID-SPAM (ours) QQP MRPC MRPC QQP SST-2 IMDB IMDB SST-2 64.00.7 71.10.1 54.10.3 57.61.2 67.90.4 66.70.4 63.31.8 70.91.2 68.31.3 66.10.4 54.60.2 52.71.1 64.10.6 64.50.3 58.80.5 69.20.4 87.10.0 90.30.2 68.71.1 66.50.0 84.50.5 67.10.8 69.80.1 89.10. 88.80.4 87.61.1 63.53.8 66.81.3 83.31.0 71.11.6 69.30.9 86.00.8 Table 6: Mean, stddev of zero-shot task, domain transfer for different methods. Score is average of Accuracy and macro F1-Score. The best performing Soft Promptbased methods results are in bold. 3.5 Method Analysis Figure 2: Effect of Variation in layer index (m) corresponding to which soft prompt is prepended on performance (m = 0 refers to input embeddings). Metrics are average of acc. and F1 for MRPC and acc. for RTE. We analyze the effect of varying layer index where soft prompt is prepended (m in Figure 1) on performance of LPT and ID-SPAM on 2 GLUE datasets using RoBERTa-LARGE backbone in Figure 2. We infer that ID-SPAM and LPT perform better when soft prompt is prepended to inputs in middle layers of LM. Also, ID-SPAM significantly outperforms LPT corresponding to almost every layer index for RTE Dataset. Also, ID-SPAM performs better for earlier layers, as soft prompt is generated by using single attention layer over input embeddings. Hence, prepending this prompt to an earlier layers outputs performs better than later layers outputs, as later layers outputs are obtained after input embeddings are passed through several attention layers, reducing compatibility with the soft prompt. Also, if we prepend soft prompt to later layers, it passes through small number of layers of LLM, thus showing reduced performance."
        },
        {
            "title": "4 Discussions and Conclusion",
            "content": "In this paper, we propose ID-SPAM which significantly improves parameter-efficient fine-tuning and zero-shot task and domain transfer performance on various NLU tasks compared to several SOTA parameter-efficient baselines. Notably, further analysis shows that ID-SPAM performs reasonably well when the generated soft prompt is prepended at any layers inputs. Hence, ID-SPAM is an efficient, input-dependent soft prompt generation framework that could generalize well across several NLP tasks."
        },
        {
            "title": "5 Limitations",
            "content": "We have shown that our proposed approach IDSPAM improves the performance of two backbone LLMs (RoBERTa-BASE and RoBERTa-LARGE) on multiple NLP tasks. Our framework is generic and can be used with any open source LLMs as backbone. However, we could not use more recent very large scale pre-trained LLMs (like Llama-3.170B and Mixtral 8x22B) with tens of billions of parameters as backbone LMs in our experiments due to limited computational resources. We are interested to see the performance gain when we use our approach with those large scale state-of-the-art LLMs in some future work. In the current work, we do not have an automated way to choose the layer of the LM where we input the soft prompt. The layer number is kept as hyperparameter in the current work and its effect is shown in Section 3.5. In future, we want to automatically identify the optimal transformer layer, as proposed by Zhu and Tan (2023)."
        },
        {
            "title": "References",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Joon-Young Choi, Junho Kim, Jun-Hyung Park, WingLam Mok, and SangKeun Lee. 2023. Smop: Towards efficient and effective prompt tuning with sparse In The 2023 Conference on mixture-of-prompts. Empirical Methods in Natural Language Processing. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, volume 1. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, pages 40054019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171 4186. William Dolan and Chris Brockett. 2005. Automatically constructing corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP). Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. 2024. Parameter-efficient fine-tuning for large models: comprehensive survey. arXiv preprint arXiv:2403.14608. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Feihu Jin, Jinliang Lu, Jiajun Zhang, and Chengqing Zong. 2023. Instance-aware prompt learning for language understanding and generation. ACM Transactions on Asian and Low-Resource Language Information Processing, 22(7):118. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 30453059. Quentin Lhoest, Albert Villanova del Moral, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Timo Müller, Isabella Géron, Simon Brandeis, Sylvain Gugger, Théo Matussière, Abhishek Thakur, Philipp Schmid, Yacine Jernite, Jeff Boudier, Francesco Calefato, Clara Ma, Clement Delangue, Thibault Goehringer, Victor Sanh, Canwen Xu, Alexander M. Rush, and Thomas Wolf. 2021. Datasets: community library for natural language processing. Preprint, arXiv:2109.02846. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582 4597. Xiangyang Liu, Tianxiang Sun, Xuan-Jing Huang, and Xipeng Qiu. 2022a. Late prompt tuning: late prompt could be better than many prompts. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 13251338. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022b. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 6168. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. arXiv:2103.10385. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: robustly optimized bert pretraining approach. Preprint, arXiv:1907.11692. Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations. Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang, Wei Wu, Xiaojun Quan, and Dawei Song. 2022. Xprompt: Exploring the extreme of prompt tuning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1103311047. Vinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML10, page 807814, Madison, WI, USA. Omnipress. Aleksandar Petrov, Philip Torr, and Adel Bibi. 2023. When do prompting and prefix-tuning work? theory of capabilities and limitations. In The Twelfth International Conference on Learning Representations. Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 52035212, Online. Association for Computational Linguistics. Quora. 2017. Quora question pairs. Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 11121122. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Zhuofeng Wu, Sinong Wang, Jiatao Gu, Rui Hou, Yuxiao Dong, VG Vinod Vydiswaran, and Hao Ma. 2022. Idpg: An instance-dependent prompt generation method. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 55075521. Yee Hui Yeo, Jamil Samaan, Wee Han Ng, PengSheng Ting, Hirsh Trivedi, Aarshi Vipani, Walid Ayoub, Ju Dong Yang, Omer Liran, Brennan Spiegel, et al. 2023. Assessing the performance of chatgpt in answering questions regarding cirrhosis and hepatocellular carcinoma. medRxiv, pages 202302. Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023a. Summit: Iterative text summarization via chatgpt. arXiv preprint arXiv:2305.14835. Zhen-Ru Zhang, Chuanqi Tan, Haiyang Xu, Chengyu Wang, Jun Huang, and Songfang Huang. 2023b. Towards adaptive prefix tuning for parameter-efficient language model fine-tuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 12391248. Wei Zhu and Ming Tan. 2023. Spt: Learning to selectively insert prompts for better prompt tuning. In The 2023 Conference on Empirical Methods in Natural Language Processing. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392. Zhengxiang Shi and Aldo Lipani. 2024. DePT: Decomposed prompt tuning for parameter-efficient finetuning. In The Twelfth International Conference on Learning Representations. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 16311642. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353355, Brussels, Belgium. Association for Computational Linguistics. Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang, Xiaojun Quan, Zenglin Xu, et al. 2023. Aprompt: Attention prompt tuning for efficient adaptation of pre-trained language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 91479160. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American"
        },
        {
            "title": "A Experiment Settings",
            "content": "For our experiments, we use roberta-base and roberta-large implementations from HuggingFace. For all baselines, the number of appended prompt tokens (for Prompt Tuning, Ptuning, Late Prompt Tuning) are set to 10 tokens. For DEPT, we set the rank to 45. For P-Tuning, we set the encoder reparameterization type to MLP. For ID-SPAM, appended prompt tokens are set to 10 tokens. The search space for hyperparameters for tuning are shown in Table 7. For all experiments, standard CrossEntropyLoss was used. For all experiments, we train using warm-up rate of 0.06, and AdamW optimizer with ϵ of 1 106, β1 of 0.9, β2 of 0.98. In Figure 2, we can see that layers 11-13 show optimal performance for both ID-SPAM and LPT. LPT (Liu et al., 2022a) shows that the 13th layer is optimal. This makes our method ID-SPAM comparable to LPT taking the layer number into account. Also, following the trend from other prior art on soft prompts (Lester et al., 2021; Liu et al., 2022a; Li and Liang, 2021; Choi et al., 2023), we used the best hyperparameter set for each of the baselines. Our experimental approach is also logical and consistent as the experimental settings (choice of backbone LMs, datasets) are same for baselines and our method ID-SPAM. Hyperparameter Epochs Batch Size Learning Rates Dropout Rate Weight Decay Layer (RoBERTa-Large) Layer (RoBERTa-Base) Values {1, 5, 10, 20, 30} {16, 32, 64} {1e-3, 5e-4, 1e-4, 5e-3, 1e-5} {0.1, 0.2, 0.3} {0, 0.01, 0.1} {1,2,3...23} {1,2,3...11} Table 7: Hyperparameters used for tuning ID-SPAM. Evaluation using GPT-2 and GPT-"
        },
        {
            "title": "Large Backbones",
            "content": "Using GPT-2 Backbone. We carry out experiments with decoder-only GPT-2 backbone on 6 GLUE Datasets - Table 8 shows that when using GPT-2 as backbone, ID-SPAM outperforms LPT on 3/6 tasks and gives an average performance improvement of 2.3%. Next, we carry out experiments with decoderonly GPT-2 backbone on 4 SuperGLUE Datasets MNLI QNLI SST-2 RTE QQP MRPC AVG 77.3 62.8 69.5 LPT ID-SPAM 78.3 79.1 71.6 80.3 82.9 81.9 79.5 79.4 77. 90.1 85.1 Table 8: Test results on 6 GLUE Datasets using GPT-2 Backbone. The best performing PEFT method is bold for each task. Table 9 shows that compared to Soft Prompt-Based baselines, ID-SPAM gives the best average score, and performs the best on 2 tasks, while performing the second best on one of them. Prompt Tuning P-Tuning SMoP LPT DEPT ID-SPAM CB COPA MultiRC BoolQ Mean 63.6 61.7 71.7 65 63.2 73.3 70.1 68.4 81.4 72.4 72.1 82.1 73.5 76.1 68 73.8 88.1 71.7 64.1 65.7 69.4 74.1 67.2 72. 57 57.7 61.2 61.3 55.1 63.1 Table 9: Test results on 4 SuperGLUE Datasets using GPT-2 Backbone. The best performing method is bold for each task. Using GPT-2 Large Backbone. We compare the performance of ID-SPAM with LoRA and LPT using large generative model GPT-2 Large (around 0.8 Billion Parameters) as the backbone on 2 GLUE Datasets - RTE and MRPC, as shown in Table 10. Method RTE MRPC Average 74.0 LoRA LPT 69.9 ID-SPAM 73. 80.0 82.9 81.1 77.0 76.4 77.4 Table 10: Test results on 2 GLUE Datasets using GPT-2 Large Backbone. ID-SPAM gives an average improvement of 0.5% and 1.3% compared to LoRA and LPT respectively across the 2 GLUE Datasets, showing that IDSPAM is competitive even for large, generative backbone LM. Few-Shot Task Transfer Eval (Few-shot, 100 samples) Tuning Train MRPC QQP MRPC QQP MRPC QQP QQP QQP QQP MRPC MRPC MRPC Fine-Tuning LPT ID-SPAM Fine-Tuning LPT ID-SPAM Score 81.7 74.4 73.1 79.7 69.4 72.5 Table 11: Few-shot task transfer for different methods using the RoBERTa-LARGE Backbone. ID-SPAM and LPT (a well-performing baseline in Table 2) using the RoBERTa-LARGE Backbone are fine-tuned on the first dataset, and then further fine-tuned on 100 training samples from the second. This model is then evaluated on the second dataset. From Table 11, we can see that ID-SPAM performs better than LPT on QQP->MRPC, while the performance is comparable for MRPC->QQP. Dataset Method BoolQ BoolQ BoolQ WiC WiC WiC LPT LoRA ID-SPAM LPT LoRA ID-SPAM Training Time per sample (in secs) 0.669 0.715 0.651 0.082 0.113 0. Inference Time per sample (in secs) 0.236 0.313 0.251 0.041 0.067 0.035 Table 14: Training and inference times of ID-SPAM and well-performing baselines LPT and LoRA for 2 SuperGLUE Datasets. Comparison of ID-SPAM with baselines w.r.t model size and training and inference times LPT Model 2,162,688 RoBERTa-BASE RoBERTa-LARGE 2,883,584 LoRA 3,495,312 7,931, ID-SPAM 2,064,384 3,538,944 Table 12: number of trainable parameters of ID-SPAM and well-performing baselines LPT and LoRA (see Table 2). Table 12 shows that the number of trainable parameters in ID-SPAM is lesser than that of LoRA for both backbones, and is lesser than that of LPT using RoBERTa-BASE backbone, while they are comparable in case of RoBERTa-LARGE backbone. Backbone GPT2 GPT2-medium GPT2-large GPT2-xl Gemma-2B (Team et al., 2024) FLAN-T5-xl (Chung et al., 2024) No. of Parameters in Backbone LM 126.8 361.1 785.8 1577.3 2525.8 2823. ID-SPAM LoRA 2.1 3.5 5.1 8.3 13.4 13.4 2.4 (1.1x) 6.3 (1.8x) 11.8 (2.3x) 19.7 (2.4x) 19.6 (1.5x) 35.5 (2.6x) Table 13: Number of trainable parameters (in millions) of ID-SPAM compared to LoRA for several LM backbones of different sizes. The decrease in the number of trainable parameters of ID-SPAM compared to LoRA is written within brackets. Table 14 shows that ID-SPAM requires less time for training as well as for inference, in comparison to LoRA on both BoolQ (a yes/no QA dataset) and WiC (dataset for binary classification) Datasets (2 datasets from SuperGLUE). Also, ID-SPAM takes lesser time to train on BoolQ than LPT, while the times are comparable on WiC. In case of inference, ID-SPAM takes lesser time than LPT for WiC, while taking slightly more time than LPT for BoolQ. Hence, ID-SPAM has comparable training and inference times w.r.t LPT, while giving better performance on GLUE datasets (see Table 2). MNLI QNLI SST-2 RTE QQP MRPC Fine Tuning LPT ID-SPAM 2887s 2013s 1902s 270s 157s 166s 224s 161s 171s 247s 168s 168s 1854s 1157s 1004s 87s 59s 51s Table 15: Total training time cost before convergence (in seconds) of ID-SPAM compared to baselines Table 15 shows the training convergence times (in seconds - lower the better) for LPT and our proposed ID-SPAM (By convergence, we mean the epoch where the validation error is the least) using RoBERTa-LARGE Backbone. We can see that IDSPAM gives better/similar convergence time compared to LPT on 4 out of 6 GLUE Tasks. Also, LPT takes an average convergence of time of 619 s, while ID-SPAM takes 577 s, giving an improvement of 7.3% in average convergence time."
        },
        {
            "title": "E Convergence of the LoRA Baseline",
            "content": "Table 13 shows that as the size of the backbone LM increases, efficiency in the number of trainable parameters of ID-SPAM compared to LoRA tends to increase. Hence, ID-SPAM is suitable even for massive LMs. The training loss is tabulated every 5 epochs in Table 16 when training LoRA with the RoBERTaBASE backbone on the MRPC and RTE Datasets from the GLUE Benchmark. 9 Epoch MRPC RTE 0.4 0.21 0.14 0.12 0.07 0.05 0.06 0.02 0.04 0.02 0.02 0.0001 5 10 15 20 25 30 Table 16: Training Loss across epochs when training LoRA with the RoBERTa-BASE backbone We can see that the training loss continuously decreases with increasing epochs on both the MRPC and RTE Datasets. Also, the losses are considerably lowered after 30 epochs as can be seen in the table, thus showing convergence."
        }
    ],
    "affiliations": [
        "Adobe Research, India",
        "IIIT Hyderabad, India",
        "IIT Kharagpur, India"
    ]
}