{
    "paper_title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs",
    "authors": [
        "Jaemin Kim",
        "Hangeol Chang",
        "Hyunmin Hwang",
        "Choonghan Kim",
        "Jong Chul Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable general capabilities, but enhancing skills such as reasoning often demands substantial computational resources and may compromise their generalization. While Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious alternative, they typically requires retraining for each LLM backbone due to architectural dependencies. To address these challenges, here we propose Universal Reasoner (UniR) - a single, lightweight, composable, and plug-and-play reasoning module that can be used with any frozen LLM to endow it with specialized reasoning capabilities. Specifically, UniR decomposes the reward into a standalone reasoning module that is trained independently using predefined rewards, effectively translating trajectory-level signals into token-level guidance. Once trained, UniR can be combined with any frozen LLM at inference time by simply adding its output logits to those of the LLM backbone. This additive structure naturally enables modular composition: multiple UniR modules trained for different tasks can be jointly applied by summing their logits, enabling complex reasoning via composition. Experimental results on mathematical reasoning and machine translation tasks show that UniR significantly outperforms existing baseline fine-tuning methods using the Llama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong generalization: reasoning modules trained on smaller models effectively guide much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust solution for enhancing reasoning in LLMs without compromising their core capabilities. Code is open-sourced at https://github.com/hangeol/UniR"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 2 5 7 0 9 1 . 5 0 5 2 : r Universal Reasoner: Single, Composable Plug-and-Play Reasoner for Frozen LLMs Jaemin Kim* Hangeol Chang* Hyunmin Hwang* Choonghan Kim Jong Chul Ye Graduate School of AI *Equal contribution Korea Advanced Institute of Science and Technology (KAIST) {kjm981995, hangeol, hyunmin_hwang, choonghankim, jong.ye}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable general capabilities, but enhancing skills such as reasoning often demands substantial computational resources and may compromise their generalization. While ParameterEfficient Fine-Tuning (PEFT) methods offer more resource-conscious alternative, they typically requires retraining for each LLM backbone due to architectural dependencies. To address these challenges, here we propose Universal Reasoner (UniR) - single, lightweight, composable, and plug-and-play reasoning module that can be used with any frozen LLM to endow it with specialized reasoning capabilities. Specifically, UniR decomposes the reward into standalone reasoning module that is trained independently using predefined rewards, effectively translating trajectory-level signals into token-level guidance. Once trained, UniR can be combined with any frozen LLM at inference time by simply adding its output logits to those of the LLM backbone. This additive structure naturally enables modular composition: multiple UniR modules trained for different tasks can be jointly applied by summing their logits, enabling complex reasoning via composition. Experimental results on mathematical reasoning and machine translation tasks show that UniR significantly outperforms existing baseline fine-tuning methods using the Llama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong generalization: reasoning modules trained on smaller models effectively guide much larger LLMs. This makes UniR cost-efficient, adaptable, and robust solution for enhancing reasoning in LLMs without compromising their core capabilities. Code is open-sourced at https://github.com/hangeol/UniR"
        },
        {
            "title": "Introduction",
            "content": "Recent Large Language Models (LLMs), such as OpenAI-o1 [28], DeepSeek-R1 [8], Kimi-1.5 [37], BOLT [29], and Open-Reasoner-Zero [19], have demonstrated remarkable capabilities in complex reasoning tasks. These models leverage Chain-of-Thought (CoT) prompting [39] to elicit stepby-step reasoning processes from LLMs, thereby significantly enhancing their ability to tackle intricate problems. key driver of this progress has been Reinforcement Fine-Tuning (RFT), which aligns model outputs with desired criteria, often human preferences, through reinforcement learning (RL) algorithms such as Proximal Policy Optimization (PPO) [33], Direct Preference Optimization (DPO) [31], Group Relative Policy Optimization (GRPO) [34], etc. Unfortunately, adapting these powerful LLMs to specialize in specific domains or further enhance their reasoning abilities through RFT presents significant hurdles. The process is often prohibitively Preprint. Under review. expensive due to the substantial computational and memory resources required for fine-tuning largescale models. While Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA [18] have been proposed to mitigate these costs, they introduce their own limitations: (1) Model Dependency: LoRA modules strongly depend on specific model architectures, limiting their transferability across models of different sizes (e.g., from 3B to 14B). (2) Poor Linearity Support: Combining multiple domainspecific LoRA adapters through linear operations lacks robust theoretical support and frequently yields unpredictable outcomes. To address these challenges, we introduce UniR (Universal Reasoner), novel method that decouples the reasoning capability into specialized reasoning module, independently trained and designed to enhance capabilities of backbone LLM. This lightweight reasoning module can be seamlessly integrated into an architecture-agnostic backbone model without requiring explicit information about the models internal structure. This approach not only fosters modularity, preserving the core capabilities of the base model, but also enables effective weak-to-strong generalization where compact reasoning module can enhance significantly larger models. Moreover, this modularity opens avenues for combining distinct reasoning modules across multiple domains. The core innovation of UniR lies in its novel decoupling of reward model training from full policy updates. Instead of relying on preference datasets, UniR leverages explicit, predefined rewards that are often available for tasks with verifiable solutions, such as mathematical problem-solving or machine translation. We propose to model these typically trajectory-level rewards, or rewards over full response, as sum of log-probabilities derived from our decomposed reasoning module. This formulation effectively translates global reward signal into sequence of implicit token-level guidance signals that steer the reasoning process. The reasoning module is then trained to maximize this reward using policy gradient algorithm, ensuring that only the smaller reasoning module is updated while the backbone policy remains unchanged. During inference, reasoning module provides guidance to main backbone model by combining their outputs, typically by adding their respective logits before sampling. Furthermore, because the guidance occurs at the logit level, multiple reasoning modules trained for distinct tasks can be composed through simple logit additionenabling composite probabilistic reasoning without additional training or architectural modification. Our contributions are summarized as follows: Computational Efficient Training. We introduce method to train decomposed reasoning module by directly optimizing for predefined rewards, circumventing the need for expensive preference dataset creation. Furthermore, our algorithm backpropagates only through smaller reasoning model while keeping the large backbone model frozen, reducing computational and memory overhead. Transferability Across Model Sizes. Although our approach trains only small models, the learned reasoning module can be effectively transferred to larger models at inference time. For example, 0.5B reasoning module trained alongside 3B backbone can be used in conjunction with 14B model, providing token-level guidance that improves the larger models performance without requiring any fine-tuning of the 14B model itself. Composibility. Our method allows optimal composition of multiple small reasoning modules specialized for specific domains or tasks. These specialized models can then be selectively deployed based on the domain context, enabling modular and efficient inference pipelines where domain expertise is encapsulated in lightweight, task-specific policies. Superior Performance on Reasoning and Translation Tasks. Our experiments on challenging mathematics reasoning and translation benchmarks show that UniR substantially outperforms conventional fine-tuning methods, showcasing its efficacy in enhancing LLM capabilities."
        },
        {
            "title": "2 Related Work",
            "content": "Enhancing LLM Reasoning. Enhancing the reasoning capabilities of LLMs is key research objective to develop toward AI agent [10], where complex reasoning and understanding are needed. One prominent strategy involves Supervised Fine-Tuning (SFT) on specialized, often small, datasets curated to foster reasoning skills; these datasets might be distilled from larger models (e.g., BESPOKESTRATOS-7B [5], OPENTHINKER-7B [1] using outputs from models like DeepSeek-R1 [8]) or 2 Figure 1: UniR Framework Overview. Our approach trains lightweight, transferable reasoning module (πr) using predefined rewards to guide frozen backbone model (πb), offering (1) transferability across different backbone models or tasks; and (2) composability by combining multiple specialized reasoning modules through reward optimization. meticulously selected from high-quality examples (e.g., S1 [27], LIMO [44]). Recently, RL-based Group Relative Policy Optimization (GRPO) and related methods [34, 45, 24] have shown promise in enhancing LLM reasoning capabilities using rule-based rewards without requiring separate value function. Similarly, our work also aims to improve the reasoning ability of LLMs using the specialized reasoning module through evaluation on tasks that require complex reasoning. Efficient Methods for Domain-Specific Tasks. To mitigate the substantial computational costs associated with both SFT and RL fine-tuning, Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA [18] are commonly employed. However, LoRA modules are inherently tied to the architecture of the model they were trained on, limiting their transferability across LLMs of different sizes, including the number of layers or size of hidden dimensions. In contrast, UniR trains compact, self-contained reasoning module that interacts with large, frozen backbone model at the logit level. This not only makes the training process highly efficient but also results in reasoning module that is more flexible and architecturally unconstrained for transfer and guidance. Guided LLM generation. Guiding the generation of frozen LLMs at inference or decoding time offers flexible way to improve outputs without full retraining [21]. Established methods like Bestof-N (BoN) sampling [12] or search algorithms such as Monte Carlo Tree Search (MCTS) [23] can enhance generation quality but often incur substantial computational costs due to multiple forward passes. More recent techniques focus on finer-grained, token-level guidance [40, 43]. Especially, GenARM [40] proposes token-level reward model, trained on human preference data, to provide token-level scores that guide frozen LLM at test-time. In addition, several works [26, 36, 16] train value function by temporal difference learning to steer generation towards desired attributes. UniR, while also employing guiding component, is distinguished by the nature and training of its guidance component. UniR does not involve learning separate value function; instead, it directly trains the reasoning module using policy gradient algorithm. Furthermore, UniR trains on dedicated training dataset using predefined rewards, rather than real-time adaptation at inference time."
        },
        {
            "title": "3 Problem Definition",
            "content": "A significant trend in enhancing LLM capabilities, particularly in reasoning, involves guiding their behavior using predefined rewards rather than relying solely on human preference data. This approach leverages objective metrics like task accuracy or other rule-based criteria, as demonstrated in works like DeepSeek-R1 [8] and methodologies employing GRPO [34]. Following this direction, our research concentrates on tasks where performance can be directly assessed through such predefined rewards. We denote the predefined reward as r(x, y) for given prompt from dataset and generated output sequence y. Standard Reinforcement Learning (RL) fine-tuning aims to find an (yx), that maximizes the expected reward while remaining close to backbone optimal policy, πθ policy (πb(yx)), typically pre-trained base LLM. This objective is formulated as: max πθ ExD,yπθ(yx)[r(x, y)] βDKL[πθ(yx)πb(yx)], (1) 3 where DKL[] represents the Kullback-Leibler divergence, and β > 0 controls the strength of the (yx) for this objective can be regularization towards the backbone policy πb. The optimal policy πθ expressed in closed form as [48, 31]: πθ (yx) ="
        },
        {
            "title": "1\nZ(x)",
            "content": "πb(yx) exp (cid:19) r(x, y) , (cid:18) 1 β (2) πb(yx) exp( 1 where Z(x) = (cid:80) β r(x, y)) is partition function. key challenge in directly applying Eq. (2) for autoregressive generation is that the trajectory-level reward r(x, y) can be obtained only after the full sequence is complete. However, it does not provide token-specific feedback during the generation process. For fine-grained guidance of an LLMs reasoning steps, pertoken signals are more desirable. Theoretically, the optimal token-level decision rule for the objective in Eq. (1) can be expressed using the optimal soft Q-function, Q(st, at) where st = (x, y<t) is the current state and at = yt is the next token. This Q(st, at) represents the maximum expected future return following the optimal policy πθ . The optimal token-level policy is then given by [15, 48]: log πθ (ytx, y<t) = log πb(ytx, y<t) + 1 β Q(x, y<t, yt) log (x, y<t), (3) where (x, y<t) is the per-step partition function [38, 15]. However, guiding with is impractical itself. This challenge motivates the need since it requires knowledge of the optimal policy πθ to bridge observable trajectory-level rule-based rewards with practical formulation that enables effective token-level guidance."
        },
        {
            "title": "4 UniR: Universal Reasoner",
            "content": "To circumvent the direct estimation of Q, our core idea is to learn separate, typically smaller, reasoning module, denoted as πr(yx) with parameters ϕ. This policy is specifically trained to capture the desired reasoning or task-specific behaviors that lead to high rewards. We then use this learned reasoning module to guide larger LLM. 4.1 Modeling Trajectory Rewards via the Reasoning Module To connect trajectory-level reward r(x, y) to our reasoning module πr and enable its use in token-bytoken guidance mechanism, we posit that the reward can be represented by the sum of log-probabilities of the reasoning module πr: 1 β r(x, y) = (cid:88) t=1 log πr(ytx, y<t; ϕ). (4) Here, β is the same temperature parameter from Eq. (1). This formulation implies that sequences assigned higher sum of token-level log-probabilities by πr should correspond to sequences achieving higher rewards r(x, y). The feasibility of representing reward function via the log-likelihood of policy is supported by theoretical work in [40], which claims that all reward equivalence classes can be represented using the parameterization log πr(y x) for some probability distribution πr(y x). Since log πr(yx) = (cid:80)y t=1 log πr(ytx, y<t) by the chain rule, this suggests that sufficiently expressive reasoning module πr can indeed model any trajectory-level reward function r(x, y). By substituting the definition from Eq. (4) into the optimal policy form of Eq. (2), we derive the structure of our guided policy for autoregressive generation of the next token yt. This translates to combining their next-token predictions: log πθ(ytx, y<t) = log πb(ytx, y<t) + log πr(ytx, y<t) log (x, y<t), (5) where (x, y<t) is the per-step partition function. Accordingly, the logits from the frozen backbone πb and the learned module πr can be separable to produce the logits for the guided policy πθ. This additive and separable guidance mechanism extends to scenarios involving multiple reward criteria. Instead of training single monolithic model to jointly optimize for all objectives, UniR allows }, each specialized for for the training of several distinct reasoning modules {πr different reward function ri(x, y). At inference, these specialized modules can be combined by taking 2, , πr 1, πr 4 weighted sum of their logits and adding them to the backbone models logits: log πθ(ytx, y1:t1) log πb(ytx, y1:t1) + (cid:88) i=1 αi log πr i(ytx, y1:t1), (6) i. This approach offers significant where αi 0 controls the influence of each reasoning policy πr flexibility, as the weights αi can be adjusted at inference time to explore various trade-offs between objectives without any retraining. Such strategy is analogous to solving weighted KL-regularized optimization problem, aligning with principles in multi-objective policy optimization [2, 9]. 4.2 Training πr with GRPO The parameters ϕ of the reasoning module πr are trained to maximize the expected predefined reward r(x, y), not the sum of its own log-probabilities defined in Eq. (4). We employ Group Relative Policy Optimization (GRPO) [34] for this purpose. For each input D, we generate group of candidate responses {y1, y2, , yG} by sampling from πθ(yix; ϕold), where ϕold denotes the parameters of the reasoning module from the previous iteration. For each generated sequence yi, we compute its external reward r(x, yi) (denoted ri for brevity) and then calculate the normalized advantage Ai: Ai = (ri mean({r1, r2, , rG})) /std({r1, r2, , rG}). In GRPO, the objective function for updating the parameters ϕ of the reasoning module is then: LGRPO(ϕ) = ExD, yiπθ(yix;ϕold) (cid:34) (cid:40) (cid:88) i=1 1 (cid:33) (cid:32) min πθ(yix) πθ(yix; ϕold) Ai, (cid:19) clip (cid:18) πθ(yix) πθ(yix; ϕold) , 1 ϵ, 1 + ϵ Ai βKLDKL[πθ(yix; ϕ)πθ(yix; ϕ0)] (7) (8) (cid:41)(cid:35) . Here, ϵ is the clipping hyperparameter from PPO-clip, ϕ0 is initial value of ϕ, and βKL controls the KL-regularization term. The learning direction is determined by the ratio πθ(yix,ϕ) πθ(yix;ϕold) , which simplifies to πr(yix,ϕ) πr(yix;ϕold) as the terms from the πb cancel out, thereby exclusively influencing πr. Furthermore, the gradient ϕLGRPO only affects the parameters of πr since πb is frozen. KL Divergence Considerations. The original GRPO formulation [34] includes KL divergence term to regularize the optimized policy πθ towards the initial policy. However, in our UniR framework, the reasoning module πr is often initialized from general pre-trained small model, which may not possess strong task-specific reasoning capabilities. Regularizing πθ towards πb which is heavily influenced by the initial, potentially πr old can impede the learning of high-reward reasoning paths. Therefore, we omit such an explicit KL regularization by βKL = 0, allowing πr more freedom to explore and learn effective reasoning strategies driven by the predefined rewards. This choice aligns with findings in related works such as DAPO [45], which also found benefits in adjusting or omitting certain regularization components. 4.3 Interpreting πr with We now provide theoretical grounding to understand what the learned reasoning module πr captures at each token-level decision point. This provides theoretical justification for interpreting log πr as meaningful guidance signal that drives the policy towards optimality for the trajectory-level objective. Theorem 1. (Informal) If the reasoning module πr satisfies Eq. (4), such that its summed per-token log-probabilities accurately approximate the scaled reward for trajectories that are consistent with the optimal policy πθ , then log πr(ytx, y<t) = 1 β Q(x, y<t, yt). (9) for all states st = Proof. We assume guided policy πθ has converged to the optimal policy πθ (x, y<t) and actions at = yt by optimizing the GRPO objective to maximize the reward r(x, y) since the KL-regularized objective typically admits unique optimum. Given that the trajectory-level policies are identical, their per-token conditional distributions must also match for any st. Comparing Eq. (5) with the standard form of the optimal policy expressed in terms of the soft Q-function (Eq. (3)) shows that log πr(yt x, y<t) corresponds to the scaled optimal soft Q-value. 5 Method Trained Model In-distribution Out-of-distribution GSM8K MATH-500 AIME24 Minerva OlympiadBench Mathstral-7B-v0.1* Internlm2-math-plus-7B* NuminaMath-7B-CoT* DeepSeekMath-7B-RL* 7 t 3 - 2 . 3 l 3 - 5 . 2 Q Baseline Baseline + 1B GRPO LoRA GRPO Full Ours Baseline Baseline + 0.5B GRPO LoRA GRPO Full Ours 7B 7B 7B 7B - - 3B 3B 1B - - 3B 3B 0.5B 84.9 84.0 75.4 88. 66.1 64.7 67.9 77.3 78.3 75.5 72.1 78.2 82.8 82.2 56.6 54.4 55.2 52.4 38.0 35.0 41.8 42.4 49.2 46.8 41.8 64.2 66.0 64.8 - - - - 0.0 10.0 13.3 3.3 16.7 6.7 6.7 13.3 16.7 10.0 16.2 17.3 19.1 20.6 14.8 14.7 16.9 16.2 18.4 23.5 16.2 28.3 28.3 26.8 21.5 18.8 19.9 19. 11.4 11.4 14.2 13.5 17.3 25.5 22.1 30.1 30.1 27.0 Avg. - - - - 26.1 27.2 30.8 30.6 36.0 35.6 31.8 42.8 44.8 42. Table 1: Zero-shot pass@1 performance on mathematic benchmarks. The out-of-distribution results are obtained using models trained on the MATH training dataset. Models marked with * are taken from the report [42]. Method Trained Model EN DE DE EN BLEU CometKiwi XComet Avg. BLEU CometKiwi XComet Avg. Google translator GPT4.1-nano Qwen2.5-3B-Instruct Baseline 3 - 2 . 3 l Baseline + 1B GRPO LoRA GRPO Full Ours - - - - - 3B 3B 1B 32.39 31.89 23.28 25.68 24.62 25.68 26.89 27.94 80.20 83.90 78.25 79.87 79.15 80.07 81.10 81.71 92.00 96.00 91.26 92.34 91.83 92.92 93.66 93. 68.20 70.60 64.26 65.96 65.20 66.22 67.22 67.87 32.95 37.14 32.75 32.46 32.51 33.36 37.68 37.88 81.00 83.70 82.66 82.26 81.89 82.06 82.72 82. 92.90 97.00 95.70 95.28 94.88 95.34 96.02 96.02 68.95 72.61 70.37 70.00 69.76 70.25 72.14 72.27 Table 2: Pass@1 performance on machine translation benchmarks for both English-to-German (ENDE) and German-to-English (DEEN) directions. GPT-4.1-nano and Qwen2.5-3B-Instruct were evaluated with the same system and user prompts as used for our method."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Settings We evaluate UniR on two tasks: mathematical problem-solving and machine translation. Mathematical problem-solving requires explicit, step-by-step reasoning, while machine translation involves implicit reasoning based on domain-specific knowledge and semantic understanding. These tasks provide complementary benchmarks for assessing our decomposed reasoning module. Models and Baselines. Our experiments utilize models from Qwen2.5 [41] and Llama3.2 [13]. We train the reasoning module πr alongside 3B-scale backbone πb: (1) πb as Qwen2.5-3B with πr as Qwen2.5-0.5B, and (2) πb as Llama3.2-3B with πr as Llama3.2-1B. For the transferability experiment, we use Qwen2.5-14B and Llama3.1-8B as backbone models. All models are Instruct variants unless stated otherwise. As for baseline training, we apply GRPO to fine-tune the entire backbone model (e.g., the 3B-scale πb model) directly on each task dataset using the same predefined rewards. This baseline is implemented through either full model tuning or LoRA-based fine-tuning. Dataset and Reward Formulation. For mathematical reasoning tasks, we use the Math-12k [22, 24] and GSM8k [7] datasets. The reward r(x, y) is determined by the rule-based correctness of the final answer, following the methodology in Dr.GRPO [24]. For machine translation, we use the IWSLT 2017 dataset [6] for English-to-German and German-to-English translation, employing BLEU [30] and CometKiwi 1 [32] as reward during training. Following MT-R1-Zero [11], we combine lexical and semantic metrics for balanced training. Evaluation Metrics. We evaluate mathematical reasoning using pass@1 accuracy on five benchmarks: GSM8k [7], MATH-500 [22], AIME24 2, Minerva [20], and OlympiadBench [17]. GSM8k uses its own training split; the others use Math-12K. For the translation task, we report BLEU, 1https://huggingface.co/Unbabel/wmt22-CometKiwi-da 2https://huggingface.co/datasets/AI-MO/aimo-validation-aime 6 Figure 2: Effectiveness of Reasoning Policy Transfer. Results demonstrate that trained reasoning module can improve performance when integrated with larger backbone models across diverse mathematical reasoning tasks. CometKiwi, and XComet-XL 3 [14] scores. Further details on training hyperparameters, optimization settings, evaluation details and prompt templates are provided in Appendix A. 5.2 Enhancing Reasoning Ability As shown in Table 1 , our framework demonstrates substantial improvements over GRPO with LoRA across both in-distribution and out-of-distribution benchmarks. UniR achieves an average score of 36.0, surpassing GRPO Full and GRPO LoRA with the Llama3.2 model. When using the Qwen2.5, UniR does not surpass that of GRPO method. However, it is noteworthy that UniRs performance gain over the naive logit summation approach (Baseline + 0.5B) was more substantial (+10.4) than the improvement GRPO achieved over the original baseline (+9.2). Furthermore, in machine translation, as shown in Table 2, UniR consistently outperforms GRPO with LoRA in both translation directions (ENDE and DEEN), while achieving performance comparable to full GRPO. This indicates that UniR effectively generalizes beyond mathematical reasoning, extending its efficacy to implicit reasoning tasks such as translation. To further verify the importance of reward-driven alignment for the reasoning module, we compare it with naive ensemble approach, directly combining the logits of the frozen backbone model with those of an intact reasoning module without any reward optimization (e.g., Baseline + 1B). This strategy results in degraded performance compared to even baselines. These findings highlight that merely combining outputs from different models does not lead to enhanced reasoning ability. Instead, our reward-driven policy alignment substantially improves reasoning performance, demonstrating the effectiveness of the proposed UniR framework. 5.3 Additional Experiments Transferability to Other Frozen Backbone LLMs. Figure 2 illustrates the transferability of our reasoning module πr, which was originally trained on Math-12k and GSM8k datasets with 3B-scale backbone. We evaluate its effectiveness when deployed to guide larger frozen backbone models with 8B and 14B parameters. In most benchmarks, the transferred πr improves average reasoning performance (35.5 and 52.8) compared to the respective backbone baselines (33.7 and 51.1), even without additional fine-tuning. This suggests that πr is not narrowly specialized to its original backbone but instead functions as generalizable reasoning augmentation module. These findings highlight the potential of UniR to serve as scalable and resource-efficient framework, enabling lightweight modules to enhance the reasoning capabilities of stronger models at inference time. Combining Specialized Reasoning Modules. To investigate the advantages of integrating multiple specialized reasoning capabilities enabled by UniR, we design composite task: translating German math problems from the MGSM dataset [35] into English. We utilize two distinct reasoning modules: πMath , module specialized for mathematical problem-solving on the GSM8k dataset, and πTrans , module trained for German-to-English translation. These modules are combined with frozen base LLM (πb) through weighted summation of their logits: log πb + α log πMath + (1 α) log πTrans , (10) 3https://huggingface.co/Unbabel/XCOMET-XL Prompt: Tanya is teaching at school. She earns $15 for every hour and an additional $5 per day if she teaches more than 3 classes. On Monday she teaches 4 classes for 5 hours, and on Wednesday 2 classes for 2 hours. How much did Tanya earn for these two days of teaching? Backbone Model πb: <think> First, lets calculate the earnings for Monday. Tanya teaches 4 classes for 5 hours each. Her base pay for teaching is $15 per hour. So, her earnings from teaching alone on Monday would be 4 classes * 5 hours/class * $15/hour. Additionally, ... </think> <answer> For Monday, Tanyas earnings from teaching are 4 * 5 * 15 = $300. Since she teaches more than 3 classes, she earns an additional $5, so her total for Monday is $300 + $5 = $305.For Wednesday, Tanyas earnings from teaching are 2 * 2 * 15 = $60. ... The answer is $365.</answer> Reasoning Module πr: <think> <answer> <mark> <ul> <li>On Monday, she earns $15/hour * 4 classes * 5 hours = $300</li> <li> On Monday, she earns $15/hour * 2 classes * 2 hours = $60</li> <li> On Wednesday, she earns $15/hour * 2 classes * 2 hours = $60</li> <li> On Wednesday, she earns $15/hour * 2 classes * 2 hours = $60</li> <li>On Monday, she earns $15/hour * 4 classes * 5 hours = $300</li> <li> On Monday, she earns $15/hour * 2 classes * 2 hours = $60 </li> <li> On Wednesday, she earns $15/hour * 2 classes * 2 hours = $60</li> <li>On Wednesday, she earns $15/hour * 2 classes * 2 hours = $60</li> ... Ours πb + πr: <think> First, lets calculate the earnings for Monday. Tanya teaches 4 classes for 5 hours each, so she earns $15 per hour for 5 hours, which is $15 * 5 = $75. Additionally, since she teaches more than 3 classes, she earns an extra $5 per day, which is $5. So, her total earnings for Monday are $75 + $5 = $80. Next, ... Now, we add the earnings from both days to get the total earnings for the two days: $80 (Monday) + $30 (Wednesday) = $110.</think> <answer> $110 </answer> Figure 3: Reasoning Performance of the πr Module. (Left) The backbone model πb and (Middle) the standalone reasoning module πr produce incorrect, repetitive and logically flawed reasoning. (Right) When combined, it generates coherent reasoning and arrive at the correct solution, showing the effectiveness of the modular guidance. where α [0, 1] is coefficient that balances the influence between both modules. We employ GPT-4.1-nano to evaluate translation quality and the accuracy of the generated output. Detailed configurations for this experiment are provided in Appendix A. As depicted in Figure 4, we observe that increasing the weight α generally leads to improved accuracy in solving the mathematical problems, while higher weight for the translation module tends to improve translation quality. This highlights crucial capability of UniR: the ability to combine and balance multiple specialized reasoning modules. This modularity allows for precise control over the policy models behavior, leading to improved performance on multi-faceted tasks. We illustrate the example of the responses in Figure 14 in Appendix B. Figure 4: Performance on German-to-English Math problem-solving task. The numbers in the figure indicate the value of α. Analysis of πr Reasoning Performance. The reasoning capabilities of the πr module and backbone models in isolation reveal both their individual limitations and the significant synergy achieved through their combination. As shown in Figure 3 for mathematical reasoning tasks, the standalone πr module fails to generate coherent and relevant reasoning. Likewise, the large backbone model alone struggles to reach correct conclusions on complex tasks. However, when the reasoning module is paired with stronger backbone, it provides valuable guidance to the backbones chain-of-thought process, resulting in significantly improved reasoning performance. Additional illustrative examples are presented in Appendix B. 5.4 Computational Efficiency in Training Due to its structural decomposability, our framework substantially reduces VRAM usage by avoiding gradient updates to the large backbone model. While LoRA also requires minimal memory for gradients and optimizer states by restricting updates to small subset of parameters, it still incurs substantial memory usage for intermediate activations because it has to save the full computation graph for back-propagation [46]. In contrast, our approach updates only the lightweight reasoning module, drastically minimizing the memory required for these activations. This leads to significantly lower VRAM usage, particularly in scenarios with large batch sizes or long sequence lengths where activation memory typically dominates. We conduct an experiment examining peak memory usage based on the number of tokens processed in one step. The input length was fixed to 250 tokens (plus up to 5 output tokens), and batch size was progressively increased under an 80GB VRAM limit. As shown in Figure 5, our method supports batch sizes up to 128, whereas LoRA and full fine-tuning are limited to 64 and 32, respectively. Although sequence length was fixed in this specific experiment, activation memory usage generally 8 Figure 5: VRAM usage versus batch size under an 80GB constraint. Our method scales to batch size 128, while full fine-tuning and LoRA are limited, demonstrating memory efficiency for large batch. Figure 6: Training Dynamics. We visualize the mean (Left) and the standard deviation (Right) of the reward on the GSM8k (Top) and Math-12k (Bottom) during training. grows with both batch size and sequence length. As result, our method provides greater flexibility in scaling the number of tokens processed per step under memory constraints. Furthermore, UniR remains fully compatible with wide range of efficiency techniques, including quantization, caching, and adapter-based methods like LoRA applied to the reasoning module itself. 5.5 Reward Stabilization and Training Dynamics. notable empirical finding during training is that UniR not only achieves higher rewards compared to GRPO but also exhibits smaller standard deviation in the rewards, as shown in Figure 6. Since the advantage calculation in GRPO normalizes the unbiased advantage ri mean({r1, r2, , rG}) by their standard deviation (Eq. (7)), lower variance amplifies the magnitude of the effective advantage Ai. This means even small deviations from the mean reward translate into strong learning signals. This dynamic relates to the Question-level difficulty bias identified by [24], where inputs with highly consistent reward induce strong learning directions and thus lead to unstable policy updates. Indeed, GRPO often suffers from training instabilities when encountering policy collapse that can result in overfitting on too easy or too hard samples. UniR, on the other hand, ensures structural stability by maintaining fixed backbone model πb. This design choice allows UniR to produce consistent responses, which typically exhibit low standard deviations in their rewards. Due to the inherent robustness of architecture, resistance of UniR to severe policy collapse allow it to mitigate amplified signals mode effectively. Instead of leading to instability, these strong learning signals can be constructively used by UniR, as opposed to the unstable state of GRPO."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present Universal Reasoner, novel plug-and-play reasoning module that directly optimizes predefined rewards by focusing solely on compact reasoning module while keeping the larger backbone model frozen, leading to efficient training comparable to other parameter-efficient methods. Our approach demonstrates remarkable transferability; the learned policies can effectively guide models of various sizes at inference without requiring further fine-tuning of the larger models. Furthermore, UniR facilitates the convenient combination of modules specialized for different tasks. This suggests the potential of our framework to freely integrate multiple modules, each trained on domain-specific datasets, to achieve synergistic improvements across diverse range of tasks. Limitations. The performance of our framework is inherently bounded by the capabilities of the selected frozen backbone model πb, as πr primarily guides its existing representations. In addition, our approach employs an additional reasoning module introducing computational overhead at inference time. The extent to which compact reasoning module πr can capture and generalize complex, multi-faceted reasoning across diverse scenarios remains an area for ongoing investigation, and its effectiveness can depend on the careful design of πrs architecture and training regimen."
        },
        {
            "title": "References",
            "content": "[1] Open thoughts. https://open-thoughts.ai, 2025. Accessed: 2025-05-05. [2] Abbas Abdolmaleki, Sandy Huang, Leonard Hasenclever, Michael Neunert, Francis Song, Martina Zambelli, Murilo Martins, Nicolas Heess, Raia Hadsell, and Martin Riedmiller. distributional view on multi-objective policy optimization. In International conference on machine learning, pages 1122. PMLR, 2020. [3] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Bespoke of bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation, 2025. Accessed: 2025-05-5. effectiveness https://www.bespokelabs.ai/blog/ Labs. reasoning Bespoke-stratos: unreasonable distillation."
        },
        {
            "title": "The",
            "content": "[6] Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuitho Sudoh, Koichiro Yoshino, and Christian Federmann. Overview of the iwslt 2017 evaluation campaign. In Proceedings of the 14th International Workshop on Spoken Language Translation, pages 214, 2017. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [8] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [9] Jasper Dekoninck, Marc Fischer, Luca Beurer-Kellner, and Martin Vechev. Controlled text generation via language model arithmetic. arXiv preprint arXiv:2311.14479, 2023. [10] Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, et al. Agent ai: Surveying the horizons of multimodal interaction. arXiv preprint arXiv:2401.03568, 2024. [11] Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, and Zuozhu Liu. Mt-r1-zero: Advancing llm-based machine translation via r1-zero-like reinforcement learning. arXiv preprint arXiv:2504.10160, 2025. [12] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [14] Nuno Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André FT Martins. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995, 2024. [15] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning In International conference on machine learning, pages with deep energy-based policies. 13521361. PMLR, 2017. [16] Seungwook Han, Idan Shenfeld, Akash Srivastava, Yoon Kim, and Pulkit Agrawal. Value augmented sampling for language model alignment and personalization. arXiv preprint arXiv:2405.06639, 2024. [17] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 10 [19] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. [20] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. [21] Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. Rain: Your language models can align themselves without finetuning. arXiv preprint arXiv:2309.07124, 2023. [22] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [23] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Making ppo even better: Value-guided monte-carlo tree search decoding. Openreview https://openreview.net/forum?id=QaODpeRaOK, 2023. [24] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [25] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [26] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023. [27] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [28] OpenAI. reason with learning-to-reason-with-llms/, September 2024. Accessed: 2025-05-05. Learning llms. https://openai.com/index/ to [29] Bo Pang, Hanze Dong, Jiacheng Xu, Silvio Savarese, Yingbo Zhou, and Caiming Xiong. Bolt: Bootstrap long chain-of-thought in language models without distillation. arXiv preprint arXiv:2502.03860, 2025. [30] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [32] Ricardo Rei, Marcos Treviso, Nuno Guerreiro, Chrysoula Zerva, Ana Farinha, Christine Maroti, José GC De Souza, Taisiya Glushkova, Duarte Alves, Alon Lavie, et al. Cometkiwi: Ist-unbabel 2022 submission for the quality estimation shared task. arXiv preprint arXiv:2209.06243, 2022. [33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [35] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners, 2022. [36] Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language learning. arXiv preprint arXiv:2206.11871, 2022. 11 [37] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [38] Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, and Tommaso Biancalani. Inference-time alignment in diffusion models with reward-guided generation: Tutorial and review. arXiv preprint arXiv:2501.09685, 2025. [39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [40] Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, and Sumitra Ganesh. Genarm: Reward guided generation with autoregressive reward model for test-time alignment. arXiv preprint arXiv:2410.08193, 2024. [41] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [42] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [43] Shentao Yang, Shujian Zhang, Congying Xia, Yihao Feng, Caiming Xiong, and Mingyuan Zhou. Preference-grounded token-level guidance for language model fine-tuning. Advances in Neural Information Processing Systems, 36:2446624496, 2023. [44] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. [45] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [46] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303, 2023. [47] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [48] Brian Ziebart, Andrew Maas, Andrew Bagnell, Anind Dey, et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 14331438. Chicago, IL, USA, 2008."
        },
        {
            "title": "A Experimental Details",
            "content": "A.1 Prompt template For all models in the LLaMA family, we modified the default chat template by removing the knowledge cutoff and the generation time, as these fields vary depending on the time of execution and thus hinder reproducibility, and are also irrelevant to the task. For models in the Qwen family, however, we retained the default chat template as provided. A.1.1 Math Reasoning Task. The prompt template for mathematical problems follows the style of DeepSeek-R1 [8, 24]. Our system prompts were designed to encourage the model to articulate its reasoning process before providing final answer, illustrated in Figure 7, 8. Specifically, for the Math-12k dataset using Qwen model in Figure 8, we instructed the model to output its answer within boxed LaTeX environment. This format was adopted to ensure that various valid answer formats were correctly recognized and evaluated. A.1.2 Translation Task. For translation, inspired by the findings of MT-R1-Zero [11], which show that training with and without explicit reasoning yields comparable performance, we use direct translation prompts without explicit reasoning instructions-detailed in Figure 9. A.1.3 Compositional Task. We instructed the model to solve math problems written in German and to perform reasoning and solving in English. The reasoning process was placed within <think> tags, and the final answer was enclosed within <answer> tags, as illustrated in Figure 10. Using the prompt shown in Figure 11, we evaluated the responses with GPT-4.1-nano-2025-04-14 on two dimensions: translation quality and math accuracy. Translation quality measures how well the response was translated into English, while math accuracy assesses accuracy using ground truth for given problem. While rule-based evaluation is often used for math accuracy due to the structured format of answers, we chose to use an LLM-based evaluator. This is because the math module is explicitly trained to follow specific output format, whereas the translation module is not. Therefore, using rule-based approach would unfairly favor the math side. To ensure fair comparison between translation and problem-solving accuracy, we opt for GPT-4.1-nano as an impartial judge capable of understanding semantic correctness beyond strict formatting. The prompt for GSM8K System Prompt: conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively. Your response should be in the following format: <think> Your reasoning here </think> <answer> answer here </answer>. The reasoning process Note that respond by English, NOT use other languages. User Prompt: {question} Figure 7: For GSM8K, the prompt specifies reasoning-then-answer format, where the model is instructed to articulate its thought process within <think> tags, followed by the final answer enclosed in <answer> tags. question denotes the question of the benchmark. 13 The prompt for Math-12K - Llama System Prompt: conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively. Your response should be in the following format: <think> Your reasoning here </think> <answer> answer here </answer>. User Prompt: {question} - Qwen System Prompt: conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively. Your response should be in the following format: <think> Your reasoning here </think> <answer> boxed{{your answer here }} </answer>. User Prompt: {question} Figure 8: In the Math-12K prompt format used with the LLaMA and Qwen models, answers follow reasoning-then-answer structure using <think> and <answer> tags. With LLaMA, we observed frequent failures to generate valid boxed{} expressions, so we chose to omit this syntax. In contrast, when using Qwen, we explicitly enclose the final answer within boxed{}, prompting the model to express its answer in boxed format. The prompt for Translation - Translation DE to EN System Prompt: You are helpful assistant that translates German to English. User Prompt: Translate the following German sentence into English. Respond ONLY with the translation. German: {source} - Translation EN to DE System Prompt: You are helpful assistant that translates English to German. User Prompt: Translate the following English sentence into German. Respond ONLY with the translation. English: {source} Figure 9: Prompt used for Translation English-to-German and German-to-English. source indicates the source sentences to be translated. 14 The prompt for German math questions into English System Prompt: You are helpful assistant that translates German math questions into English and solves them. The user asks math question in German. You first think about the reasoning process and answer. Both should be in English. Your response should be enclosed within <think> </think> and <answer> </answer> tags, respectively. Your response should be in the following format: <think> Your reasoning here </think> <answer> answer here </answer>. User Prompt: German Question: {question} Figure 10: System prompt used for English to German Math task: specifies the reasoning-then-answer format with <think> and <answer> tags. question indicates the math question in German. The prompt for scoring German math questions into English - Evaluating Translation Quality System Prompt: You are linguistic evaluator. Your job is to assess how well the models response is in English and whether it avoids using German. Score only the language usage, not the math correctness. User Prompt: Here is models response to German math question. Please evaluate how well it used English instead of German. Assign score between 1 and 5: - 1: Mostly or entirely in German - 3: Mixed, partly German, partly English - 5: Fully in English, fluent and natural Response: {response} End your response with only the score after ####. Example: #### 4 - Evaluating Accuracy System Prompt: You are correctness evaluator. You will assess the factual accuracy of math answer. You will return: AnswerCorrect: <True or False> User Prompt: Here is models response to German math question. Ground Truth Answer: {gt_answer} {response} Evaluate: - AnswerCorrect: <True or False> Figure 11: Prompt used for scoring English-to-German Math task: Translation quality and Math Accuracy. response and gt_answer represent the model output and the ground truth of each math question, respectively. A.2 Hyperparameter Setup The detailed hyperparameter settings are provided in Table 3. For the fair comparison, we used the same hyperparameters across all methods, including baseline model, GRPO, GRPO with LoRA, and simple logit-sum. Specifically, we set the KL divergence coefficient β = 0.04 for GRPO and GRPO with LoRA, following the default value in the Hugging Face TRL library 4. We observed that the original GRPO framework tends to suffer from unstable generation without KL divergence. Therefore, we adopted β = 0.04 for GRPO-based baselines to ensure stable training. Among the reward configurations, rule_based_accuracy assigns reward of 1 if the numeric value between the <answer> and </answer> tags matches the ground truth and 0 otherwise. The boxed_reward 4https://github.com/huggingface/trl Parameter GSM8K Math-12K-Qwen Math-12K-Llama Translation Training Configuration gradient_checkpointing gradient_checkpointing_kwargs learning_rate lr_scheduler_type lr_scheduler_kwargs warmup_ratio max_steps effective_batch_size max_prompt_length max_completion_length num_generations temperature Evaluation Settings max_prompt_length max_completion_length do_sample Reward Configuration reward_funcs reward_weights true use_reentrant: false 1.0e-06 cosine with min_lr min_lr_rate: 0.1 0.1 1000 64 512 1024 8 0.7 true use_reentrant: false 1.0e-06 cosine with min_lr min_lr_rate: 0.1 0.1 1000 32 512 2048 8 0.7 true use_reentrant: false 1.0e-06 cosine with min_lr min_lr_rate: 0.1 0.1 1000 32 512 2048 8 0.7 true use_reentrant: false 1.0e-06 cosine with min_lr min_lr_rate: 0.1 0.1 10000 32 512 1024 8 0.7 512 1024 False 512 2048 False 512 2048 False 512 250 False rule_based_accuracy 1.0 boxed_reward 1.0 tag_based_reward 1. BLEU, cometkiwi 1.0, 1.0 Table 3: Hyperparameter Setups. evaluates the value enclosed within the boxed{} format, assigning reward of 1 if it matches the correct answer and 0 otherwise. Similarly the tag_based_reward evaluates the value enclosed within the answer tag. For the implementation of boxed_reward and tag_based_reward we adopted the official code from the DR.GRPO repository [24]5. For BLEU, we used the default implementation of BLEU scoring provided by SacreBLEU6. A.3 Evaluation Detail For evaluation, BLEU and accuracy were computed using the same reward logic applied during training for each dataset. Translation baselines included outputs from Google Translator, implemented via the GoogleTranslator interface of the deep-translator library7. GPT-4.1-nano-2025-04-14 was also used for comparison, with decoding performed at sampling temperature of 0.7. All other model generations were conducted deterministically using do_sample=False."
        },
        {
            "title": "B Qualitative Examples",
            "content": "B.1 Example of Math Task Based on Table 1, fine-tuning with GRPO demonstrates improved performance over the base model; however, it also exhibits tendency to generate longer outputs, which is alongside the results observed in [45, 37, 3]. In contrast, our proposed method not only surpasses the performance of both the base model and the GRPO-tuned model but also achieves this with lower number of tokens to reach the final answer. Figure 12 contains detailed examples that clearly show that our model is able to reach the correct answer in fewer steps. B.2 Example of Translation Task As shown in Figure 13, our method generates more fluent and faithful translations in both directions. Compared to the base model and GRPO, our DEEN translation correctly uses the natural phrase \"What about...\", aligning with the tone of the original. On the ENDE side, only our method preserves the correct compound adjective \"künstlich intelligenten\", which is grammatically accurate and semantically precise. In contrast, baseline and GRPO variants either use awkward phrasing (\"artificial intelligent system\") or deviate from standard German compound structures. This demonstrates the effectiveness of our method in producing higher-quality, context-aware translations. 5https://github.com/sail-sg/understand-r1-zero 6https://github.com/mjpost/sacrebleu 7https://github.com/nidhaloff/deep-translator Prompt: Hannahs city is having big display of fireworks for the 4th of July. Theyre going to set off 15 boxes of 20 fireworks each. Hannahs house is at the right angle to see 40% of the citys fireworks. Hannah will also set off 3 boxes of 5 fireworks each in her backyard. How many fireworks will Hannah see in total? Base Model: <think> First, calculate the total number of fireworks in the display. There are 15 boxes of 20 fireworks each, so the total number of fireworks is 15 * 20. Then, calculate the number of fireworks Hannah can see in the display, which is 40% of the total number of fireworks. Next, calculate the number of fireworks in the 3 boxes she sets off in her backyard, which is 3 * 5. Finally, add the number of fireworks she can see in the display to the number of fireworks she sets off in her backyard to get the total number of fireworks Hannah sees. </think> <answer> The total number of fireworks in the display is 15 * 20 = 300. Hannah can see 40% of these, which is 0.40 * 300 = 120 fireworks. The number of fireworks in the 3 boxes she sets off in her backyard is 3 * 5 = 15. Therefore, the total number of fireworks Hannah sees is 120 + 15 = 135. </answer> GRPO: <think> First, lets calculate the total number of fireworks in the display. There are 15 boxes of 20 fireworks each, so the total number of fireworks is 15 * 20. Then, we need to find out how many of these fireworks Hannah can see, which is 40% of the total. After that, well add the 3 boxes of 5 fireworks each that Hannah sets off in her backyard. The total number of fireworks Hannah sees will be the sum of the fireworks she can see in the display and the fireworks she sets off in her backyard. </think> <answer> The total number of fireworks in the display is 15 * 20 = 300. Hannah can see 40% of these, which is 0.40 * 300 = 120 fireworks. She also sets off 3 boxes of 5 fireworks each, which is 3 * 5 = 15 fireworks. Therefore, the total number of fireworks Hannah sees is 120 + 15 = 135. </answer> Ours: <think> First, lets calculate the total number of fireworks in the display. There are 15 boxes of 20 fireworks each, so the total number of fireworks is 15 * 20 = 300. Hannahs house is at the right angle to see 40% of the citys fireworks, so the number of fireworks she will see is 300 * 0.4 = 120. In her backyard, she will set off 3 boxes of 5 fireworks each, which totals 3 * 5 = 15 fireworks. Therefore, the total number of fireworks Hannah will see is 120 + 15 = 135. </think> <answer> 135 </answer> Figure 12: Chain-of-thought comparison on GSM8k example: while the Base and GRPO models both arrive at the correct answer with lengthy explanations, our approach produces the same result with much more concise reasoning process. Source Prompt: EN : What about the artificially intelligent system in the movie \"Her,\" Samantha? DE : Was ist mit dem künstlichen intelligenten System im Film \"Her\", Samantha? Base Model: DE EN: Whats with the artificial intelligent system in the movie \"Her\", Samantha? EN DE: Was ist mit dem künstlichen Intelligenzsystem im Film \"Her\", Samantha? GRPO: DE EN: Whats with the artificial intelligence system in the movie \"Her\"? EN DE: Was ist mit dem künstlichen Intelligenzsystem in dem Film \"Her\", Samantha? Ours: DE EN: What about the artificial intelligence system in the movie \"Her,\" Samantha? EN DE: Was ist mit dem künstlich intelligenten System im Film \"Her\", Samantha? Figure 13: Example of bidirectional translation (German (DE) and English (EN)). Our approach produces more fluent and grammatically correct outputs in both directions, preserving natural phrasing and accurate compound structures. B.3 Example of Combining Specialized Modules Figure 14 presents an example of the response to German math-solving in English task. We compare the results from the UniR framework using the math-specific reasoning module (πM ath ), the DEEN translation-specific reasoning module (πT rans ), and their combination. This example 17 Prompt: Billy schält und schneidet Kartoffeln als Vorbereitung für die Zubereitung des Kartoffelsalats für seine große Grillparty anlässlich eines Familientreffens. Es ist ein großes Ereignis, also muss er 60 Kartoffeln schaffen. Die Kartoffeln haben ungefähr die gleiche Größe, also braucht es ungefähr gleich viel Zeit, um jede Einzelne zu schälen und zu schneiden. Billy braucht ungefähr eineinhalb Minuten, um eine Kartoffel zu schälen, aber nur ungefähr 5 Sekunden, um sie klein zu schneiden. Wie lange braucht Billy, um die Kartoffelvorbereitung abzuschließen? : Base Model + πM ath <think> Zunächst müssen wir die Gesamtzeit berechnen, die Billy benötigt, um die Kartoffeln zu schälen und zu schneiden. Da es 60 Kartoffeln gibt und jede Kartoffel ungefähr die gleiche Größe hat, können wir die Zeit pro Kartoffel berechnen. Die Zeit, die Billy benötigt, um eine Kartoffel zu schälen, beträgt ungefähr 1,5 Minuten, also 1,5 * 60 = 90 Sekunden. Die Zeit, die Billy benötigt, um eine Kartoffel zu schneiden, beträgt ungefähr 5 Sekunden. Um die Gesamtzeit zu berechnen, müssen wir die Zeit pro Kartoffel für die Schälen und Schneiden addieren. Die Gesamtzeit beträgt also 90 + 5 = 95 Sekunden. Da es 60 Kartoffeln gibt, müssen wir die Gesamtzeit mit 60 multiplizieren, um die Gesamtzeit für die Vorbereitung aller Kartoffeln zu erhalten. Die Gesamtzeit beträgt also 60 * 95 = 5700 Sekunden. Um die Zeit in Minuten umzurechnen, können wir die Sekunden durch 60 teilen. Die Gesamtzeit beträgt also 5700 / 60 = 95 Minuten. </think> <answer> 95 Minuten </answer> </answer> : Base Model + πT rans <think> To solve this problem, we need to calculate the total time it takes Billy to prepare the carrots for his party. Since he needs to prepare 60 carrots, and the time it takes to prepare one carrot is 1.5 minutes, we can multiply the number of carrots by the time it takes to prepare one carrot. However, we also need to consider the time it takes to cut the carrots into smaller pieces, which is 5 seconds per carrot. So, we need to add the time it takes to cut the carrots to the total time. First, lets calculate the time it takes to prepare one carrot: 1.5 minutes = 90 seconds. Now, lets calculate the time it takes to cut one carrot: 5 seconds. Since Billy needs to cut 60 carrots, we can multiply the time it takes to cut one carrot by 60: 5 seconds * 60 = 300 seconds. Now, lets add the time it takes to prepare one carrot to the time it takes to cut one carrot: 90 seconds + 300 seconds = 390 seconds. Finally, lets convert the total time from seconds to minutes: 390 seconds / 60 = 6.5 minutes. So, Billy needs 6.5 minutes to prepare the carrots for his party. </think> <answer> Billy needs 6.5 minutes to prepare the carrots for his party. : + 0.5πT rans Base Model + 0.5πM ath <think> First, lets calculate the total time it takes to peel and chop one potato. It takes Billy 1.5 minutes to peel potato, and 5 seconds to chop it. To convert the time to seconds, we multiply 1.5 minutes by 60 seconds per minute, which equals 90 seconds. Now, we add the time it takes to peel and chop one potato, which is 90 seconds + 5 seconds = 95 seconds. Since Billy needs to peel and chop 60 potatoes, we multiply the time it takes to peel and chop one potato by 60, which equals 95 seconds * 60 = 5700 seconds. To convert this to minutes, we divide by 60, which equals 5700 seconds / 60 = 95 minutes. Therefore, Billy needs 95 minutes to peel and chop all 60 potatoes. </think> <answer> 95 minutes </answer> Figure 14: We illustrate outputs for German math problem solved in English. (Top) Guidance with only the math module solves correctly but responds in German; (Middle) while using only translation module ensures English output but fails at math. (Bottom) Their combination achieves both target language and correct solution. demonstrates that using only the math module fails to generate response in English, while using only the translation module results in an incorrect solution. In contrast, incorporating both modules achieves both high translation quality and mathematical accuracy. B.4 Example of πr Reasoning Performance Figure 15 illustrates specific instance where the backbone model and the reasoning module fail to adequately comprehend the problems context. In contrast, our combined approach demonstrates correct and coherent reasoning process for the same problem, highlighting the synergistic benefit of integrating the specialized reasoning module with the backbone model for improved contextual understanding and accurate inference. 18 Prompt: Russell works at pet store and is distributing straw among the rodents. The rats are kept in 3 cages in equal groups and each rat is given 6 pieces of straw. There are 10 cages of hamsters that are kept alone and each hamster is given 5 pieces of straw. There is also pen of rabbits where 20 pieces of straw are distributed among the rabbits. No straw is used anywhere else in the store. If 160 pieces of straw have been distributed among the small rodents, how many rats are in each cage? Backbone Model: <think> Lets denote the number of rats as R. Each rat gets 6 pieces of straw, so the total number of pieces of straw given to rats is 6R. There are 10 cages of hamsters, and each hamster gets 5 pieces of straw. Lets denote the number of hamsters as H. So, the total number of pieces of straw given to hamsters is 5H. The rabbits get 20 pieces of straw. The total number of pieces of straw distributed is 160. Therefore, we can write the equation: 6R + 5H + 20 = 160. </think> <answer> 160 pieces of straw are distributed among the small rodents. From the equation 6R + 5H + 20 = 160, we can simplify it to 6R + 5H = 140. We need to find the number of rats in each cage, which is R. However, we dont have the number of hamsters H. Without knowing H, we cannot determine directly. But we can infer that the number of rats in each cage is whole number and the total number of pieces of straw distributed to rats is 6R. Given the total number of pieces of straw distributed to hamsters and rabbits, we can deduce that the number of rats must be factor of 140 that fits the context of the problem. Lets assume the simplest scenario where the number of hamsters is minimized. 140, which simplifies to = 140 / 6 = 23.33. Since the number of rats must be whole number, this scenario is not possible. If we assume = 0 (no hamsters), then 6R = If we assume = 1 (one hamster), then 6R + 5 = 140, which simplifies to 6R = 135, and = 135 / 6 = 22.5. Again, this is not whole number. If we assume = 2 (two hamsters), then 6R + 10 = 140, which simplifies to 6R = 130, and = 130 / 6 = 21.67. If we assume = 3 (three hamsters), then 6R + 15 = 140, which simplifies to 6R = 125, and = 125 / 6 = 20.83. If we assume = 4 (four hamsters), then 6R + 20 = 140, which simplifies to 6R = 120, and = 120 / 6 = 20. This is whole number, so the number of rats in each cage is 20. </answer> Reasoning Module: <think> <answer> <think> Lets denote the number of rats in each cage as r. The total number of rats is 3r. The total number of hamsters is 3r (since each cage has 3 hamsters, and there are 3 cages, so 3r hamsters). The total number of rabbits is 20 (since each cage has 20 pieces of straw, and there are 3 cages, so 20 3 = 60 pieces of straw are distributed among the rabbits). The total number of pieces of straw is 160. </think> </answer> </think> Ours: <think> Lets denote the number of rats in each cage as r. The total number of rats is 3r. Each rat gets 6 pieces of straw, so the total number of pieces of straw given to the rats is 6 3r = 18r. The hamsters are kept in 10 cages, and each hamster gets 5 pieces of straw. So, the total number of pieces of straw given to the hamsters is 5 10 = 50. The rabbits get 20 pieces of straw. The total number of pieces of straw distributed is 160. Therefore, the equation for the total straw is: 18r + 50 + 20 = 160 18r + 70 = 160 18r = 90 = 5 So, the number of rats in each cage is = 5. </think> <answer> 5 </answer> Figure 15: The Backbone Model and Reasoning Module both demonstrate flawed reasoning processessuch as making incorrect assumptions or applying invalid constraintsultimately leading to confusion or incorrect deductions. In contrast, ours performs accurate symbolic reasoning and produces the correct answer with clear, concise explanation, demonstrating the effectiveness of our approach in structured reasoning tasks. 19 Figure 16: Illustrative examples of responses from the baseline VLM and our UniR-extended framework on tasks from the (Left) Geometry3k and (Right) MathVerse benchmarks."
        },
        {
            "title": "C Additional Experiments",
            "content": "C.1 Extension of UniR to Vision Language Models (VLMs) Dataset and Models: To investigate whether our UniR framework can enhance reasoning in multimodal contexts, we extended it by employing Vision Language Model (VLM) as the backbone. Specifically, we utilized Qwen2.5-VL-3B-Instruct [4] as the frozen backbone model πb. This VLM was guided by 0.5B Qwen-based reasoning module πr, which was previously trained on the textbased GSM8k dataset. For evaluation, we used the MathVerse [47] and Geometry3k [25] benchmarks, which feature mathematical problems accompanied by visual diagrams, thus requiring both visual understanding and mathematical reasoning. Experimental Setup: To assess the effectiveness of the reasoning module on processing combined visual and textual information, particularly for the MathVerse dataset, we focused on its \"Vision Intensive\" questions that include images. Although the reasoning module does not process visual tokens, we provided the same multimodal input to the overall UniR system. Evaluation protocols and system prompts followed those used in the GSM8k experiments. Method MathVerse Geometry3k Qwen2.5-VL-3B-Instruct Ours (+ πr) 8.25 11.29 23.96 25.12 Table 4: Performance comparison on visually-related mathematical benchmarks. UniR enhances the reasoning capabilities of frozen VLM backbone using text-only math reasoning module. Results: As shown in Table 4, UniR significantly improves performance over the baseline VLM on both benchmarks. These enhancements are particularly noteworthy because the reasoning module πr was trained exclusively on text-based mathematical problems and had no direct access to visual information during its training. The results suggest that UniR allows the reasoning module to effectively guide the VLM, leading to better overall problem-solving capabilities in these multimodal mathematical tasks. This outcome highlights the ability of our framework to flexibly adapt to multimodal LLM backbones, allowing the framework to seamlessly extend its reach into multimodal reasoning domains. An example illustrating the qualitative difference in responses is depicted in Figure 16. 20 C. Impact of Reasoning Model Size Figure 17: Transferability of the 0.5B πr and 1.5B πr reasoning modules when combined with 14B frozen backbone model. The 1.5B πr module demonstrated superior performance. Method 3 - 5 . 2 Q Baseline Baseline + 0.5B Baseline + 1.5B GRPO LoRA GRPO Full Ours(3b+0.5b) Ours(3b+1.5b) Trained Model - - - 3B 3B 0.5B 1.5B In-distribution Out-of-distribution GSM8K MATH-500 AIME24 Minerva OlympiadBench 75.5 72.1 76.0 78.2 82.8 82.2 84.8 46.8 41.8 50.2 64.2 66.0 64.8 68. 6.7 6.7 6.7 13.3 16.7 10.0 10.0 23.5 16.2 23.1 28.3 28.3 26.8 28.7 25.5 22.1 24.7 30.1 30.1 27.0 27.6 Avg. 35.6 31.8 36.1 42.8 44.8 42.2 43.9 Table 5: Zero-shot Pass@1 performance on mathematical benchmarks comparing reasoning modules of different sizes (0.5B vs. 1.5B) trained with 3B backbone. We conducted an ablation study to investigate the impact of reasoning module size, training Qwen2.5-1.5B-Instruct reasoning module πr with 3B backbone. As shown in Table 5, this larger module achieved improved performance compared to the 0.5B πr. Notably, it even outperformed model fine-tuned using LoRA, suggesting that larger reasoning model can learn more comprehensive information for reasoning tasks. Figure 17 further illustrates the transferability of the 1.5B reasoning module πr to 14B frozen backbone model πb. The transferred 1.5B πr led to an improved average performance (53.6) compared to the 0.5B πr (52.8). Nevertheless, it is important to emphasize that our main contribution lies in demonstrating the ability to achieve strong results with comparatively small language model during training, prioritizing efficiency. C.3 Analysis of Reasoning Module during Training We evaluated the performance of trained πr reasoning modules, as depicted in Figure 18. This figure tracks the GSM8k dataset accuracy throughout the training process of reasoning modules designed for two different backbone architectures. The results indicate that the reasoning modules, when used in isolation, exhibit suboptimal performance. However, when these same reasoning modules are combined with their respective backbone models within our proposed framework, the resulting performance surpasses that of the backbone model alone. This highlights the efficacy of our learned reasoning modules in providing valuable guidance when integrated with larger language model, leading to enhanced reasoning capabilities in the combined system. 21 Figure 18: Performance comparison on the GSM8K dataset between standalone reasoning modules (πr, green) and our UniR framework (red), evaluated against their respective backbone models (orange). (Left) πr module using LLaMA-3.2-1B with LLaMA-3.2-3B backbone. (Right) πr module using Qwen2.5-0.5B with Qwen2.5-3B backbone."
        }
    ],
    "affiliations": [
        "Korea Advanced Institute of Science and Technology (KAIST)"
    ]
}