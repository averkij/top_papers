{
    "paper_title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "authors": [
        "Zijian Wu",
        "Lingkai Kong",
        "Wenwei Zhang",
        "Songyang Gao",
        "Yuzhe Gu",
        "Zhongrui Cai",
        "Tianyou Ma",
        "Yuhong Liu",
        "Zhi Wang",
        "Runyuan Ma",
        "Guangyu Wang",
        "Wei Li",
        "Conghui He",
        "Dahua Lin",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales."
        },
        {
            "title": "Start",
            "content": "2025-12-12 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Zijian Wu1,2*, Lingkai Kong1,3*, Wenwei Zhang1*, Songyang Gao1, Yuzhe Gu1, Zhongrui Cai1, Tianyou Ma1, Yuhong Liu1, Zhi Wang1, Runyuan Ma1, Guangyu Wang1, Wei Li1, Conghui He1, Dahua Lin1,2 and Kai Chen1 1Shanghai AI Laboratory, 2MMLab, The Chinese University of Hong Kong, 3Shanghai Jiao Tong University 5 2 0 2 1 ] . [ 1 6 5 7 0 1 . 2 1 5 2 : r Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current processbased verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPVs superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales. 1. Introduction Large language models (LLMs) have achieved remarkable performance on challenging reasoning tasks [3, 18, 19, 24]. This advancement is largely attributed to the growing use of verifiable oversight. Verifiers are crucial components that not only assign rewards in Reinforcement Learning with Verifiable Rewards (RLVR) [10] but also select optimal responses in test-time scaling [29] and benchmark the capabilities of LLMs [7, 8]. As LLMs generate increasingly long and intricate chains of thought (CoTs), the fidelity of these verifiers becomes critical factor that determines the capability and reliability of LLMs. Existing verifiers fall into two categories, each with its own limitations. Outcome-based verifiers (OVs) assess only the final answer against the ground truth, and overlook the reliability of intermediate steps in long CoTs. In contrast, process-based verifiers (PVs) [12] examine the entire CoT step-by-step to locate errors. However, they struggle with complex reasoning structures in the input CoT and incur prohibitive costs in both automated verification and expert annotations. Previous works [14, 22, 30] resort to coarse heuristics for training and fail to provide accurate correctness verdicts or error locations. This landscape highlights the need for more accurate and efficient paradigm for long CoT verification. To this end, we propose the Outcome-based Process Verifier (OPV), process verifier that operates on summarized outcomes from long CoTs  (Fig. 1)  . Analogously to summarization, our approach first preserves Corresponding authors: Wenwei Zhang (zhangwenwei@pjlab.org.cn), Kai Chen (chenkai@pjlab.org.cn) * Equal Contributions OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Figure 1: Comparison of different verification paradigms. The policy model happens to guess the correct answer, but fails to solidly prove it, resulting in flawed reasoning. Left: The Outcome-based Verifier ignores underlying reasoning failures. Middle: The Process-based Verifier examines the complex thinking process step-by-step, struggling to identify the tricky logic dependencies. Right: The Outcome-based Process Verifier efficiently detects potential process errors from the summarized rationale. only the key steps that contribute to the final answer and discards redundant components (e.g., trial-and-error attempts, recalculations, self-overturned assumptions) to form concise solution path. Then it performs step-by-step verification on the summarized outcome and presents correctness verdict and, if incorrect, the error location. Compared with OVs, OPV provides more fine-grained supervision, which is useful for the policy model. Moreover, its summarization process significantly reduces the complex, redundant reasoning structures in the input CoT, making OPV more efficient and less susceptible to interference from redundancy than vanilla PV. The simplified CoT also facilitates human annotation, which allows for collecting large-scale, fine-grained expert annotations for training. To streamline massive expert annotation, we adopt an iterative human-in-the-loop framework driven by active learning  (Fig. 2)  . In each round, the current OPV evaluates each summarized solution multiple times and selects the most uncertain cases for annotation. Expert annotators then provide natural language explanations, correctness verdicts, and error localizations. The newly annotated data are incorporated to retrain the OPV using combination of off-line rejection fine-tuning and on-line reinforcement learning. This strategy effectively strengthens the verifier by focusing on its weaknesses under limited annotation budgets. After several iterations, we curated 40k annotated solutions across diverse domains of problems spanning from K-12 to undergraduate levels, including high-quality held-out evaluation set of 2.2k sample answers, namely the OPV-Bench. Empowered by active learning with fine-grained supervision, OPV demonstrates strong performance and broad applicability. Despite its compact size, it achieves performance comparable to much larger open-source models across multiple public and internal benchmarks, including our held-out OPV-Bench. We further validate OPVs versatility in assisting reasoning models across multiple Figure 2: Our iterative framework yields enlarged judgment dataset and improved verifier performance. 2 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification stages, from training to inference. On AM-DeepSeek-R1-0528-Distilled [9], widely used synthetic dataset verified solely by final answers, OPV identifies false positives at an estimated rate of 7.0%, closely aligning with expert assessments. In collaboration with various policy models, OPV consistently enhances their test-time performance, with the improvement margin growing as the compute budget scales. For instance, it boosts the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025. 2. Method We define the Outcome-based Process Verifier (OPV), novel framework that bridges outcome and process verification through faithful, verifiable proxy for long CoT verification ( 2.1). To build competent OPV model, our approach leverages an iterative active learning framework to select high-quality data for expert annotations ( 2.2) and combination of off-line and on-line learning approaches to update the verifier ( 2.3). We also illustrate the statistics of the annotated data ( 2.4). 2.1. Task Formulation Recent reasoning LLMs generate long chains of thought (CoTs) to solve mathematical problems. These CoTs consist of numerous sequential steps with complex inter-step dependencies, making verification particularly challenging. Two dominant verification paradigms have emerged. Outcome-based Verifier (OV) checks only the final answer against the ground truth. Process-based Verifier (PV) sequentially verifies each step throughout the whole CoT. Both approaches have limitations. OV suffers from false positives accepting correct answers derived from flawed reasoning and cannot pinpoint errors in incorrect solutions. PV, with its fine-grained nature, struggles with intricate dependencies of long CoTs. Moreover, verifying lengthy CoTs is computationally expensive for verifier models and labor-intensive for human annotators. To bridge this gap, we propose the Outcome-based Process Verifier (OPV), hybrid paradigm that balances faithfulness and efficiency. OPV first summarizes verbose and meandering CoT trajectories into concise, linear solution paths, retaining only the key steps that contribute to the final result while pruning redundant explorations. This summary serves as faithful proxy of the underlying reasoning rationale, enabling both efficient verification and large-scale human annotation. The verifier then performs step-by-step validation on this summary to identify the first erroneous step. Given CoT generated for problem ğ‘ƒ , summarizer is first applied to produce structured, ğ‘›-step solution ğ’® = {ğ‘ 0, . . . , ğ‘ ğ‘›1}. Subsequently, our OPV, denoted as ğœ‹, takes the problem and the structured solution as input and predicts the index Ë†â„“ of the first incorrect step, together with natural language explanation Ë†â„° ( Ë†â„°, Ë†â„“) ğœ‹( ğ‘ƒ, ğ’®), Ë†â„“ {1, 0, . . . , ğ‘› 1}. (1) Here, Ë†â„“ = 1 indicates fully correct solution. We focus on identifying the first error, as subsequent steps though potentially valid in isolation are built upon faulty premises and thus lack mathematical soundness. 2.2. Active Learning Framework Finding potential errors within an answer, even after summarization, is challenging task. Therefore, it is essential to maximize the utilization of human annotation. To achieve this, we have constructed an iterative human-in-the-loop active learning framework, as shown in Fig. 3. We start with our base verifier ğœ‹0. In each round, we first use our best OPV model to identify the most uncertain cases for annotation. After human annotation, we then use combination of off-line expert iteration and on-line reinforcement learning to maximally utilize the information obtained from annotation. Data Preparation. Initially, we constructed large data pool of high-quality problems and to-be-verified solutions sampled from top-tier models, represented by the unlabeled data pool ğ’ŸU = {(ğ‘ƒğ‘–, ğ’®ğ‘–)}. All sampled solutions are pre-summarized by DeepSeek-V3 [4] to preserve core rationale. Our set of annotation labels ğ’Ÿâ„’0 3 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Figure 3: Overview of our iterative active learning framework. Step 1: For each sampled solution from the data pool, the verifier generates ğ‘ candidate judgment trajectories and constructs training set selected by uncertainty scores. Step 2: Human experts annotate the most uncertain solutions with ground truth error positions and explanations, expanding the dataset for OPV refinement. Step 3: We then sample high-quality trajectories from the annotated data. The current OPV is then updated using these trajectories with off-line or on-line algorithms. is empty at annotation round 0. We also established the annotation protocol for our annotators. The complete details are available in Appendix A.1 Selection Strategy. Since verifiers exhibit uneven competency across different types of mathematical reasoning errors, random sampling wastes resources on cases already handled well. We therefore select the most uncertain cases at each round ğ‘¡. For each unlabeled pair (ğ‘ƒğ‘–, ğ’®ğ‘–) ğ’Ÿğ’° , the current OPV ğœ‹ğ‘¡ performs ğ‘ independent verifications to obtain set of predicted indices ( Ë†â„° (ğ‘—) , Ë†â„“(ğ‘—) are the explanation and index predicted by the ğ‘—-th roll-out. ğ‘– ), where Ë†â„° (ğ‘—) and Ë†â„“(ğ‘—) ğ‘– ğ‘– To quantify the models uncertainty, we compute consistency score based on the frequency of the most common prediction: Consistency(ğ’®ğ‘–) = 1 ğ‘ max â„“ ğ‘ ğ‘—=1 [Ë†â„“(ğ‘—) ] ğ‘– = â„“ . (2) lower consistency score indicates higher uncertainty due to disagreement among the verifiers predictions. We then identify the most uncertain cases based on the consistency score: ğ’¬ğ‘¡ = {(ğ‘ƒğ‘–, ğ’®ğ‘–) ğ’Ÿğ’° Consistency(ğ’®ğ‘–) < ğœğ‘¡} , where ğœğ‘¡ (0, 1) is dynamically adjustable threshold controlling annotation cost. To mitigate possible overconfidence, we also sample small proportion of high-consistency data. (3) Human Annotation. We then send the selected data ğ’¬ğ‘¡ for expert annotation. Expert annotation yields new labeled data entries with ground truth error positions â„“* ğ‘– to enlarge the annotation set and reasons â„° * ğ‘– We then train on the new dataset ğ’Ÿâ„’ğ‘¡+1 to obtain stronger OPV model ğœ‹ğ‘¡+1. ğ’Ÿâ„’ğ‘¡+1 ğ’Ÿâ„’ğ‘¡ {(ğ‘ƒğ‘–, ğ’®ğ‘–, â„° * ğ‘– , â„“* ğ‘– )}(ğ‘ƒğ‘–,ğ’®ğ‘–)ğ’¬ğ‘¡ . (4) 4 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification 2.3. OPV Update by Reinforcement Learning Human annotation only provides the index of the error with concise explanation, not thorough reasoning trajectory that checks each step in detail. Therefore, we use combination of on-line and off-line approaches to refine the OPV model after each round of annotation. The updated model is then used again to select data for the next round of annotation. Expert Iteration. For each annotated data entry (ğ‘ƒ, ğ’®, â„° *, â„“*), we sample multiple verification attempts from the current OPV. We also sample from other powerful models such as those in the R1 and Qwen families to further boost performance. We retain only those generated verifications that are most consistent with the annotation (i.e., where the predicted index Ë†â„“ matches the ground truth â„“*). These valid verification trajectories are then added to the global verification dataset to update the OPV model. Following expert iteration [1], we iterate this process to maximize performance gains. On-line Reinforcement Learning. We also use on-line reinforcement learning approaches to stimulate the verification ability. To ensure stable on-line RL training, we filter the dataset by excluding: (1) highly ambiguous cases that challenge even experts, avoiding annotation noise; and (2) trivial cases with obvious errors, preventing bias toward oversimplified patterns. Then, given model-predicted index Ë†â„“ and ground-truth index â„“*, we define the exponential-decay reward as follows: { ğ‘…(Ë†â„“, â„“*) = if sgn(Ë†â„“ + 1) = sgn(â„“* + 1), 1 ğœ†^â„“â„“* otherwise. , where ğœ† (0, 1) controls the penalty for localization errors. The reward is strongly negative only when misclassifying correctness (correct as incorrect or vice versa). Otherwise, it remains positive with exponential decay based on distance error. This design addresses the sparse reward problem for challenging samples requiring precise error localization. We then adopt the DAPO algorithm [26] to obtain our final OPV model. 2.4. Final Dataset Statistics Our framework improves the OPV models performance while simultaneously establishing massive, highquality dataset. After completing all rounds of the \"annotate-then-train\" process, we progressively scale up the dataset to over 40k expert annotations and over 80k high-quality judge trajectories. The final fully annotated set spans multiple difficulty levels and knowledge domains. Particularly, we meticulously curate high-quality samples for evaluation and construct held-out evaluation set of 2.2k samples, namely the OPV-Bench, to effectively estimate the verification ability of the trained OPV model. The detailed breakdown of all annotated data and the OPV-Bench is available at Appendix A.2. 3. Experiment 3.1. Experiment Setup Implementation. In our framework, we use the R1-Distill-Qwen-32B [3] model to fine-tune the OPV. Further implementation details can be found in Appendix B. Evaluation. We evaluate model performance on OPV-Bench and ProcessBench [31] using three distinct correctness criteria with varying levels of stringency. The precise criterion requires exact identification of the erroneous step for judgment to be considered correct. The approximate criterion adopts more error-tolerant approach, accepting predictions as correct when the identified step is adjacent to the actual error position. Finally, the rough criterion considers any error detection for an incorrect answer as correct judgment. Under each criterion, we compute accuracy separately for erroneous and correct samples, and calculate the harmonic mean of precision and recall of correct samples to obtain the F1 score. We then compare the performance of OPV with various state-of-the-art open-source models, including the widely-used Deepseek-R1-0528, Qwen3-Max-Preview, and gpt-oss-120b. We apply the same prompt engineering approach to repurpose these models as critic models. The prompt template is provided in Appendix C. To underscore the superior quality of our expert annotations compared to heuristic labeling, we also evaluate 5 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Table 1: Evaluation results on ProcessBench and OPV-Bench. We report accuracy and F1 scores for three evaluation standards: precisely/absolutely identifying erroneous steps, approximately identifying erroneous steps (within 1 steps), and roughly identifying whether the whole solution contains errors. All results are reported under maj@8 voting."
        },
        {
            "title": "Model",
            "content": "Precise/Abs. Accuracy Precise/Abs. F"
        },
        {
            "title": "Approximate\nAccuracy",
            "content": "Approximate F"
        },
        {
            "title": "Rough\nAccuracy",
            "content": "Rough F1 ProcessBench (With Standard Answers) Qwen3-Max-Preview DeepSeek-V3-0324 DeepSeek-R1-0528 gpt-oss-120b (high) Qwen2.5-Math-PRM-72B R1-Distill-Qwen-32B OPV-32B 83.2 74.5 82.2 83.3 77.2 64.7 80.1 78.9 71.7 77.7 78.8 74.0 60.8 76.2 89.4 83.7 88.7 89.2 84.9 73.6 87. ProcessBench (Without Standard Answers) Qwen3-Max-Preview DeepSeek-V3-0324 DeepSeek-R1-0528 gpt-oss-120b (high) Qwen2.5-Math-PRM-72B R1-Distill-Qwen-32B OPV-32B Qwen3-32B QwQ-32B DeepSeek-V3-0324 DeepSeek-R1-0528 Qwen3-Max-Preview gpt-oss-120b (high) Qwen2.5-Math-PRM-72B R1-Distill-Qwen-32B OPV-Stage1 OPV-Stage2 OPV-32B Qwen3-32B QwQ-32B DeepSeek-V3-0324 DeepSeek-R1-0528 Qwen3-Max-Preview gpt-oss-120b (high) Qwen2.5-Math-PRM-72B R1-Distill-Qwen-32B OPV-Stage1 OPV-Stage2 OPV-32B 84.4 72.1 83.2 84.7 76.7 75.8 80.9 68.0 65.2 67.9 67.0 66.4 61.0 55.4 70.5 68.8 71.6 78. 79.9 69.8 79.3 80.3 73.2 73.6 76.8 89.8 81.0 89.3 90.4 83.9 83.6 88.1 OPV-Bench(With Standard Answers) 73.4 71.6 71.7 71.2 71.2 66.6 66.5 76.3 75.7 75.9 79.1 72.4 70.3 72.9 72.3 71.7 69.8 58.4 74.9 72.6 76.2 83.3 OPV-Bench(Without Standard Answers) 61.2 58.2 60.8 56.9 61.0 57.9 55.1 61.7 60.9 64.4 71.9 68.7 66.7 67.8 64.7 67.3 64.1 66.0 71.1 70.3 70.3 74.7 67.0 64.2 66.0 63.2 67.0 66.8 58.2 65.0 64.8 70.1 78.2 85.5 79.9 84.6 85.1 81.2 67.5 83.8 85.9 77.2 85.8 86.6 79.8 80.5 84.1 76.2 74.8 75.0 74.6 74.6 72.1 70.0 79.1 78.0 79.0 82. 72.1 70.1 70.8 68.2 70.8 69.3 67.6 72.9 72.4 73.8 79.1 95.4 94.4 95.4 95.7 93.7 87.4 96.2 95.2 91.5 96.0 96.2 91.2 93.3 95.8 77.8 75.6 78.6 78.3 78.0 77.7 66.0 78.7 76.1 81.1 87.2 74.2 70.9 72.8 71.7 75.2 75.6 63.4 69.4 69.8 77.6 83.1 93.2 92.0 93.1 93.5 91.1 81.3 94. 92.9 88.4 94.1 94.2 87.8 91.1 93.8 79.9 78.2 79.1 79.0 79.0 77.8 72.3 81.7 80.2 82.6 86.2 76.7 74.2 75.2 73.6 76.3 75.4 70.4 75.5 75.4 79.0 83.1 Qwen2.5-Math-PRM-72B, discriminative process reward model trained on labels that integrate Monte Carlo estimation with LLM-as-a-judge. To better demonstrate the effectiveness of our training framework, we also include several intermediate models (OPV-Stage1 and OPV-Stage2) for comparison. These models have undergone different rounds of annotate-then-train iterations and are trained on different amounts of annotated data, as shown in Fig. 2. The final OPV model is obtained after the final stage. 3.2. Benchmark Evaluation Our experiments on two benchmarks reveal distinct challenges in evaluating reasoning verifiers. Tab. 1 presents results on both ProcessBench and OPV-Bench, highlighting their different characteristics and difficulty 6 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification levels. ProcessBench, which samples answers from LLMs without thinking capabilities, exhibits performance saturation. As shown in the \"Rough F1\" column, most long reasoning models successfully detect the existence of errors in the reasoning process, achieving F1 scores above 90%. This saturation indicates that error patterns in ProcessBench are more readily identifiable and less representative of sophisticated reasoning failures. The limitation arises because models without thinking mechanisms typically produce more elementary errors, whereas newer thinking LLMs generate more nuanced and subtle logical flaws. Moreover, ProcessBench exclusively comprises problems with explicit, verifiable outcomes, which are more straightforward compared to proof-based problems that demand complex multi-step reasoning. In contrast, OPV-Bench presents significantly more complex challenges to verifiers. The test set encompasses wider spectrum of problems and requires more advanced skills to identify exact errors. Our iterative training paradigm demonstrates its effectiveness by boosting 32B models performance above much larger models. Notably, most open-source reasoning models struggle with identifying error positions (see Appendix for detailed breakdown). While these models achieve high recall, their poor precision indicates an inability to effectively detect errors in solutionsa limitation possibly inherited from training solely on verifiable outcomes. 4. Application We further explore various applications of OPV in this section, demonstrating how it facilitates both the training and inference phases of LLM development. 4.1. Examining AM-DeepSeek-R1-0528-Distilled using OPV primary application of OPV is providing fine-grained supervision for robust training. Outcome-verified synthetic datasets often contain false positives. By using OPV for process verification, we can identify and remove these instances, yielding higher-quality datasets for supervised fine-tuning. To validate this approach, we evaluated AM-DeepSeek-R1-0528-Distilled using OPV. Each data entry was verified 8 times, with entries flagged as problematic if OPV reported errors 6 times. We conducted human evaluation on 50 randomly sampled problems to check the reliability of OPV under this setting. Out of 674k math-related data entries checked, 53.7k were flagged as problematic by OPV. The distribution of OPV votes and human evaluation results are available at Fig. 4 and Tab. 2. Human evaluation results show that OPV demonstrates high reliability in verification, with 88% of the judgments being valid. Therefore, it is estimated that more than 53.7 674.0 88% = 7.0% data entries checked contain process errors. For cost efficiency, we used vanilla summaries rather than re-summarizing the thinking content. However, some answers might be incorrectly flagged due to inappropriate summarization. We note that 2 of the 50 solutions checked (marked as \"Poor Summary\") were actually correct but identified as incorrect by OPV because their original summaries introduced logical gaps. This again highlights the importance of re-summarization for precise verification. 4.2. Scaling of collaborative reasoning Beyond evaluating our OPV on static benchmarks, we study whether it can improve test-time performance in collaboration with policy models. In collaborative setting, policy model first samples ğ‘ complete solutions for given problem. The verifier then checks each solution ğ‘€ times to estimate its correctness. Finally, we select the answer by aggregating verification verdicts across all solutions. We conduct experiments on AIME2025 and consider both moderate-sized distilled models and top-tier models as policies. We use OPV as the verifier and define the verification pass rate as the proportion of runs in which the verifier deems the solution correct. We set ğ‘ = 8 and ğ‘€ = 16. When multiple answers tie with the same frequency, we report the average accuracy across the tied answers. Notably, we directly employ the summary part of the original CoT here to reduce compute budget and avoid introducing extra noise. We evaluate the following collaborative strategies and compare them against verifier-free majority voting: Majority Voting. Among the ğ‘ sampled solutions, choose the most frequent answer. Best-of-ğ‘ . Rank solutions by their verification pass rate and output the answer from the top-ranked solution. 7 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification"
        },
        {
            "title": "Count",
            "content": "% Logical Process Errors Non-Math Problem Hallucination"
        },
        {
            "title": "Total",
            "content": "24 1 19 10 9 6 2 4 48.0 2.0 38.0 20.0 18.0 12.0 4.0 8.0 100.0 Figure 4: The distribution of OPVs votes on AMDeepSeek-R1-0528-Distilled. 0 means OPV does not report any error among the 8 votes. Table 2: Error Analysis by Human Experts Table 3: Performance of different policies and collaborative reasoning strategies on AIME2025. Pass@1: single-sample accuracy averaged over all samples. Pass@8: oracle success with up to eight samples (correct if any of the eight is correct), serving as an upper bound for 8-sample strategies."
        },
        {
            "title": "Policy",
            "content": "Pass@1 Majority Voting@8 Best-of-8 Verifier Voting@8 Pass@8 DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Llama-70B DeepSeek-R1-0528 gpt-oss-120b QwQ-32B Qwen3-235B-A22B-Thinking-2507 39.1 55.9 44.0 87.1 92.3 70.0 95.4 49.2 63.8 50.0 88.3 93.3 78.3 96.7 56.3 66.6 56.1 87.5 95.1 76.2 97. 55.4 68.0 57.8 90.8 96.7 80.0 98.3 66.7 76.7 70.0 96.7 96.7 83.3 100.0 Verifier Voting. Use the verification pass rate as the weight of each solution and select the answer with the highest verifier-weighted frequency. As shown in Tab. 3, OPV consistently boosts performance across all policies in the collaborative setting. For distilled models, verifier voting yields substantial gains of 6.1% on average over majority voting. Even for top-tier models where majority voting already achieves high accuracy, voting with OPV still provides stable improvements of 2.3% on average. Notably, verifier voting matches Pass@8 (the 8-sample oracle) for gpt-oss-120b. Moreover, verifier voting outperforms Best-of-8 on most policies, indicating that aggregating verification pass rates across multiple candidates is generally more robust than selecting single best solution. We further evaluate how collaborative reasoning scales with the policy sampling size ğ‘ and the verifier sampling size ğ‘€ . The evaluation focuses on DeepSeek-R1-Distill-Qwen-32B as the policy, scaling and from 1 to 64, respectively. Initial sampling uses ğ‘ = 64 and ğ‘€ = 64. For each (ğ‘, ğ‘€ ) configuration, random subsets are chosen to evaluate different strategies performance, with 64 repetitions to determine average scores. Figure 5: Performance of collaborative reasoning scaling along policy sampling size ğ‘ and verifier sampling size ğ‘€ . Colored bands denote the area covered by accuracy curves as ğ‘€ increases. 8 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Fig. 5 demonstrates that accuracy improves with larger across all strategies, with OPV-enabled Bestof-N and Verifier-Voting consistently outperforming Majority-Voting. Verifier-Voting achieves the highest performance, reaching 73.3% at ğ‘ = 64 and ğ‘€ = 64 6.7 point improvement over Majority-Voting. Best-ofN gradually converges to Majority-Voting as increases, indicating that integrating OPVs verification with voting mechanisms yields superior overall performance. These results confirm that OPV collaborates effectively with policy models, with gains increasing proportionally to larger computational budgets. Besides, although the policy models native summary part is more concise than the summarized rationale used during training, OPV still proves effective, which can be attributed to the annotation protocols tolerance for minor logical gaps. 5. Related Work LLM Reasoning. Reasoning is regarded as core capability toward artificial general intelligence. LLMs predominantly perform reasoning in chain-of-thought (CoT) manner [23]. Frontier LLMs [3, 19] further extend the length and complexity of their CoTs to solve challenging math problems. Rejection Fine-Tuning (RFT) [27] and Reinforcement Learning with Verifiable Rewards (RLVR) [10] rely on verifiers to curate highquality training data. Recent works like OREAL [15] underscore the importance of well-designed verifiers in Reinforcement Learning (RL). Motivated by these observations, we develop verifier that generates verification trajectories in CoT manner and framework that iteratively updates it using RFT and RL. Outcome-based verification vs. process-based verification. Outcome-based verifiers (OVs) assess solutions solely by whether the final answer matches the ground truth. Rule-based verifiers, like the Math-Verify library from HuggingFace1, and LLM-as-a-judge approaches, like CompassVerifier [13], make simple answer checks scalable. However, OVs overlook the reliability of intermediate steps. In contrast, process-based verifiers (PVs) meticulously verify the reasoning process step-by-step. Empirical evidence shows that PVs can outperform OVs on difficult mathematical benchmarks [12]. Yet the prohibitive cost of fine-grained process annotation poses practical bottleneck. Prior work has therefore resorted to coarse heuristics for training. Monte Carlo methods have been used to assess and assign credit to intermediate steps [14, 22], which can introduce simulation bias and label noise. Another line of work [5, 20, 25, 30] uses stronger teacher models as judges to provide step-level labels, though the resulting verifiers are ultimately bounded by the teachers capacity. With the rapid progress of LLM reasoning, the possibility of using powerful LLMs to generate verification trajectories in CoT manner has been explored [16, 21, 28]. Our OPV follows this generative paradigm but, crucially, engages human experts to provide finer-grained supervision than heuristics, aiming to overcome the inherent limitations of purely model-driven verification. Similarly, ProcessBench [31] also engages human experts for process annotations, while we extend to more demanding setting with more challenging queries and longer CoTs from frontier LLMs. By summarizing long CoTs into concise rationales and then verifying them, we enable larger-scale expert supervision to power our OPV. 6. Conclusion We introduced the Outcome-based Process Verifier (OPV), which bridges outcome and process verification by operating on summarized solutions from long CoTs. Through an iterative active learning framework with expert annotations, OPV progressively improves its verification capabilities while minimizing annotation costs. Our approach achieves state-of-the-art results across multiple benchmarks, outperforming much larger models including DeepSeek-R1, despite its compact size. OPV demonstrates broad applicability throughout the reasoning pipeline: it identifies false positives in outcome-verified synthetic data, and yields consistent gains when collaborating with policy models at inference time. The accompanying OPV-Bench dataset of 2.2k expert-annotated solutions provides valuable resource for future research. By enabling efficient and accurate process verification at scale, OPV addresses critical bottleneck in developing reliable reasoning systems. As LLMs tackle increasingly complex problems with longer reasoning chains, the principle of verifying summarized rationales offers scalable path toward more trustworthy AI-generated reasoning. 1https://github.com/huggingface/Math-Verify 9 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification"
        },
        {
            "title": "References",
            "content": "[1] Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search, 2017. 2.3 [2] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems. arXiv e-prints, page arXiv:2110.14168, October 2021. A.1 [3] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 1, 3.1, 5, [4] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. DeepSeek-V3 Technical Report. arXiv e-prints, page arXiv:2412.19437, December 2024. 2.2 [5] Keyu Duan, Zichen Liu, Xin Mao, Tianyu Pang, Changyu Chen, Qiguang Chen, Michael Qizhe Shieh, and Longxu Dou. Efficient Process Reward Model Training via Active Learning. arXiv e-prints, page arXiv:2504.10559, April 2025. 5 [6] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. A.1 [7] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems. arXiv e-prints, page arXiv:2402.14008, February 2024. 1, A.1 [8] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. arXiv e-prints, page arXiv:2103.03874, March 2021. 1, A. [9] Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, and Xiangang Li. AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale. arXiv e-prints, page arXiv:2505.08311, May 2025. 1 [10] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing Frontiers in Open Language Model Post-Training. arXiv e-prints, page arXiv:2411.15124, November 2024. 1, 5 [11] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. A.1 [12] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step. arXiv e-prints, page arXiv:2305.20050, May 2023. 1, 5 [13] Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, and Kai Chen. CompassVerifier: Unified and Robust Verifier for LLMs Evaluation and Outcome Reward. arXiv e-prints, page arXiv:2508.03686, August 2025. 5 [14] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve Mathematical Reasoning in Language Models by Automated Process Supervision. arXiv e-prints, page arXiv:2406.06592, June 2024. 1, [15] Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, and Kai Chen. Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning. arXiv e-prints, page arXiv:2502.06781, February 2025. 5 [16] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, JanPhilipp FrÃ¤nken, Chelsea Finn, and Alon Albalak. Generative Reward Models. arXiv e-prints, page arXiv:2410.12832, October 2024. 5 [17] Sadegh Mahdavi, Muchen Li, Kaiwen Liu, Christos Thrampoulidis, Leonid Sigal, and Renjie Liao. Leveraging online olympiad-level math problems for llms training and contamination-resistant evaluation. arXiv preprint arXiv:2501.14275, 2025. A.1 11 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification [18] OpenAI, :, Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul K. Arora, Yu Bai, Bowen Baker, Haiming Bao, Boaz Barak, Ally Bennett, Tyler Bertao, Nivedita Brett, Eugene Brevdo, Greg Brockman, Sebastien Bubeck, Che Chang, Kai Chen, Mark Chen, Enoch Cheung, Aidan Clark, Dan Cook, Marat Dukhan, Casey Dvorak, Kevin Fives, Vlad Fomenko, Timur Garipov, Kristian Georgiev, Mia Glaese, Tarun Gogineni, Adam Goucher, Lukas Gross, Katia Gil Guzman, John Hallman, Jackie Hehir, Johannes Heidecke, Alec Helyar, Haitang Hu, Romain Huet, Jacob Huh, Saachi Jain, Zach Johnson, Chris Koch, Irina Kofman, Dominik Kundel, Jason Kwon, Volodymyr Kyrylov, Elaine Ya Le, Guillaume Leclerc, James Park Lennon, Scott Lessans, Mario Lezcano-Casado, Yuanzhi Li, Zhuohan Li, Ji Lin, Jordan Liss, Lily, Liu, Jiancheng Liu, Kevin Lu, Chris Lu, Zoran Martinovic, Lindsay McCallum, Josh McGrath, Scott McKinney, Aidan McLaughlin, Song Mei, Steve Mostovoy, Tong Mu, Gideon Myles, Alexander Neitz, Alex Nichol, Jakub Pachocki, Alex Paino, Dana Palmie, Ashley Pantuliano, Giambattista Parascandolo, Jongsoo Park, Leher Pathak, Carolina Paz, Ludovic Peran, Dmitry Pimenov, Michelle Pokrass, Elizabeth Proehl, Huida Qiu, Gaby Raila, Filippo Raso, Hongyu Ren, Kimmy Richardson, David Robinson, Bob Rotsted, Hadi Salman, Suvansh Sanjeev, Max Schwarzer, D. Sculley, Harshit Sikchi, Kendal Simon, Karan Singhal, Yang Song, Dane Stuckey, Zhiqing Sun, Philippe Tillet, Sam Toizer, Foivos Tsimpourlas, Nikhil Vyas, Eric Wallace, Xin Wang, Miles Wang, Olivia Watkins, Kevin Weil, Amy Wendling, Kevin Whinnery, Cedric Whitney, Hannah Wong, Lin Yang, Yu Yang, Michihiro Yasunaga, Kristen Ying, Wojciech Zaremba, Wenting Zhan, Cyril Zhang, Brian Zhang, Eddie Zhang, and Shengjia Zhao. gpt-oss-120b & gpt-oss-20b model card, 2025. [19] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin QuiÃ±onero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, and Reimar Leike. OpenAI o1 System Card. arXiv e-prints, page arXiv:2412.16720, December 2024. 1, 5 [20] Shuaijie She, Junxiao Liu, Yifeng Liu, Jiajun Chen, Xin Huang, and Shujian Huang. R-PRM: ReasoningDriven Process Reward Modeling. arXiv e-prints, page arXiv:2503.21295, March 2025. 5 [21] Wenlei Shi and Xing Jin. Heimdall: test-time scaling on the generative verification. arXiv e-prints, page arXiv:2504.10337, April 2025. 5 [22] Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. 12 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. arXiv e-prints, page arXiv:2312.08935, December 2023. 1, 5 [23] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv e-prints, page arXiv:2201.11903, January 2022. 5 [24] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. 1 [25] Zhaohui Yang, Chenghua He, Xiaowen Shi, Linjing Li, Qiyue Yin, Shihong Deng, and Daxin Jiang. Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning. arXiv e-prints, page arXiv:2505.14391, May 2025. [26] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. 2.3 [27] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. arXiv e-prints, page arXiv:2308.01825, August 2023. 5 [28] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative Verifiers: Reward Modeling as Next-Token Prediction. arXiv e-prints, page arXiv:2408.15240, August 2024. 5 [29] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, Irwin King, Xue Liu, and Chen Ma. Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well? arXiv e-prints, page arXiv:2503.24235, March 2025. 1 [30] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The Lessons of Developing Process Reward Models in Mathematical Reasoning. arXiv e-prints, page arXiv:2501.07301, January 2025. 1, 5 [31] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. ProcessBench: Identifying Process Errors in Mathematical Reasoning. arXiv e-prints, page arXiv:2412.06559, December 2024. 3.1, 13 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification The Use of Large Language Models (LLMs) We used LLMs solely for language polishing. The scientific ideas, methodology, analyses, and conclusions were entirely developed by the authors, while the LLMs assisted only in improving clarity and readability of the text. A. Annotation Details A.1. Data and Annotation Preliminaries Problem Curation. We curated math problems from widely-used benchmarks, published problem sets and open contests to ensure broad spectrum of challenges in terms of difficulty and knowledge domains. The problems span K-12 education, high-school competitions, and undergraduate-level mathematics. To construct our dataset, we aggregated our initial data pool from three primary sources: (1) Challenging Benchmarks (1k samples): High-difficulty problems from Putnam and USAMO; (2) Math Forums (10k samples): Community-curated advanced problems from AoPS2; and (3) Open-Source Datasets (1M samples): Largescale collections including NuminaMath [11] and AoPS-Instruct [17]. We explicitly excluded multiple-choice, fillin-the-blank, and true/false questions, as these formats allow for correct answers via \"lucky guessing\" or shortcuts. Consequently, we retained only open-ended calculation and proof-based problems that necessitate rigorous, step-by-step reasoning. Furthermore, to prevent data leakage, we implemented strict decontamination pipeline against major public benchmarks (GSM8K [2], MATH [8], OlympiadBench [7], Omni-MATH [6], AIME 2024, and AIME 2025). This process utilized two-stage mechanism: first, Exact Matching via normalized string comparison to remove duplicates; and second, Semantic Matching using LLM-based embedding similarity to identify and discard queries semantically similar to the evaluation sets. CoT Generation and Summarization. We sample 8 to 12 unique CoTs per problem from state-of-theart models (R1 and Qwen families) to capture diverse reasoning paths. Initial attempts to use the default summaries following the </think> tag proved inadequate, as they omitted crucial intermediate steps and resisted improvement through prompt engineering. Therefore, we employ Deepseek-V3 to re-summarize the reasoning content within <think>...</think> tags, preserving all calculations, enumerations, and case analyses while segmenting steps uniformly with \"---\" delimiters. This procedure yields our initial unlabeled data pool, ğ’Ÿğ’° = {(ğ‘ƒ1, ğ’®1), . . . , (ğ‘ƒğ‘š, ğ’®ğ‘š)}, which serves as the starting point for active learning. Active Learning Configuration To maximize the utility of the annotation budget, we employed mixed sampling strategy. In each iteration loop, the data selected for expert annotation consists of two parts: (1) Uncertainty Sampling (80%): The majority of the budget is allocated to samples with the lowest consistency scores. We utilized dynamic consistency threshold ğœğ‘¡, set to ğœ1 = 0.25 for the first stage and ğœ2 = 0.5 for the second stage, to capture increasingly subtle errors as the model improves; (2) High-Confidence Sampling (20%): To prevent the model from becoming overconfident or forgetting established knowledge, the remaining 20% of samples are randomly selected from the high-consistency pool. This ensures the verifier receives feedback on cases it considers \"correct,\" allowing us to correct confident errors (false negatives) during the Expert Iteration phase. Annotation Protocol. We established precise protocol to guide expert annotation. For each sample (ğ‘ƒğ‘–, ğ’®ğ‘–), annotators provide brief explanation Ë†â„°ğ‘– and identify the index Ë†â„“ğ‘– of the first erroneous step. Reference solutions were provided to facilitate this process. To address ambiguity, we instructed annotators to identify \"flawed but tolerable\" stepssteps that are imperfect but could be easily corrected within 2-3 sentences and precede the first definitive error. Such steps were not classified as erroneous. To ensure data quality, three experts independently evaluate each solution. Annotations are valid only when: (1) all experts agree the solution is correct, or (2) at least two experts identify an error within two-step window. This window accounts for errors that span multiple steps and resist single-step attribution. As shown in Tab. 4, annotators typically achieve stronger consensus. This protocol ensures only high-confidence labels are added to our dataset during active learning. 14 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Figure 6: The topic distribution by chart of categories (inner) and domains (outer). Figure 7: Distribution of error positions in OPVBench. Table 4: Statistics of OPV-Bench. % ğ‘› steps denotes the proportion of samples whose solutions have ğ‘› step. % 3/3 and 2/3 agreement denotes the proportion of samples for which each type of annotator agreement is achieved. K"
        },
        {
            "title": "Undergraduate",
            "content": "error correct error correct error correct # Samples 82 119 398 # Average Steps % 4 steps % 8 steps % 12 steps # Average Error Position 7. 6.74 6.70 93.9% 97.5% 100.0% 40.2% 30.5% 30.3% 3.5% 5.0% 6.1% 3.25 / 2.97 / 7.11 100.0% 36.1% 2.5% 600 600 6.73 99.0% 29.5% 1.3% 2.49 6.71 97.3% 29.3% 3.2% / % 3/3 agreement % 2/3 agreement 76.8% 100.0% 85.4% 14.6% 0.0% 23.2% 100.0% 0.0% 100.0% 100.0% 0.0% 0.0% A.2. Dataset Statistics The resulting OPV-Bench comprises three subsets totaling 2,202 test cases. Tab. 4 presents detailed statistics, and Fig. 7 shows the error position distribution within erroneous samples. Since all solutions are re-summarized by Deepseek-V3 before evaluation, average step counts remain consistent across problems within each subset. However, incorrect solutions contain slightly more steps than correct ones. Most errors occur in the initial reasoning steps. High school competition problems, which require less formula application but more exploration and analysis, show delayed error peak compared to other problems. B. Verifier Training Details In our experimental framework, we employ the pre-trained R1-distilled-32B [3] model as the base architecture for fine-tuning the verifier. The verifier is trained with the following hyperparameters: 1 epoch, learning rate of 8e-5, sequence length of 32k, and weight decay of 10. We utilize the DAPO training algorithm with specific configurations for the final on-policy RL stage. For the generation settings of the RL stage, we configure global batch size of 256 and prompt repetition factor of 8 to generate multiple samples per prompt. The optimization utilizes global batch size of 128. Training spans 80 total steps without warmup. 2https://artofproblemsolving.com/community 15 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification C. Prompting Details The re-summarization prompt and verification prompt are shown in Fig. 8 and Fig.9. As student proficient in mathematics, you are presented with problem and standard solution. Your task is to transform the given standard solution into clear, well-structured solution that precisely articulates each reasoning step based on the material provided. Your final solution should be comprehensive, methodical, and leave no logical gaps. It should be in Chinese and should be split into steps divided by the \"\" delimiter. Remember to use the same notation, symbols and language as the original solution. If the standard solution is very brief, you may present it in just 2-3 steps. Otherwise, aim for 5-15 steps. 1. Solution Analysis: Carefully review the provided standard solution to understand the complete solution path. 2. Structured Step-by-Step Solution: Provide thorough solution where each step follows logically from the previous one. Clearly explain every mathematical operation, theorem application, and reasoning transition mentioned in the standard solution. 3. Strict Adherence to Original Content: Your steps must strictly follow the information provided in the standard solution. Do not add new methods, concepts, or information not found in the original solution. Your task is to reorganize and clarify the existing content, not to enhance or expand it. 4. Mathematical Rigor: When presenting formulas, equations, or mathematical statements, ensure they are expressed with proper notation and justified appropriately. Define all variables and symbols when they first appear. 5. Format Requirements: Present your solution as sequence of paragraphs, each thoroughly describing distinct step in the solution process. Separate paragraphs with \"\" as delimiters. Do not use numbered or bulleted lists. Each paragraph should represent meaningful unit of the solution with comparable depth and detail. If the problem contains multiple sub-problems, maintain this paragraph structure throughout, treating the entire solution as continuous sequence of steps. Conclude with final paragraph summarizing the answer if appropriate. sub-problem should have at least 2 paragraphs splitted. Do not use any lists or indexes nested inside paragraph separated by \"\". Since the solution is separated by the \"\" delimiters that implicitly defines the index of the steps, you should not introduce any other explicit indexes, neither in numbers, English or Chinese. Every \"\" delimiter should occupy whole line. Mathematical expressions should be clearly formatted, with proper attention to notation, subscripts, superscripts, and mathematical symbols. Use standard mathematical notation conventions. PROBLEM STATEMENT: {problem_statement} STANDARD SOLUTION: {standard_solution} DETAILED SOLUTION WITH STEP-BY-STEP REASONING AND PROPER FORMAT: Figure 8: Prompt Template for Summarization. 16 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification You are mathematics and educational expert tasked with evaluating the correctness of students answer. The students solution is broken down into steps, and your goal is to identify the index of the first incorrect step. The index starts at zero for the first step. If all steps are correct, you should output -1. Instructions: - You will receive question along with the students answer, divided into steps. Each step is presented in separate paragraph. - You are encouraged to express your internal reasoning, but your final response must always include an integer within box{STEP}. For example, if step 2 is incorrect, respond with box{STEP2}. If all steps are correct, respond with box{STEP-1}. Also, after you reported the incorrect step index, you should also briefly report the reason for this incorrectness. - Some steps may initially appear incorrect but are later corrected in subsequent steps. If reflection or revision is both accurate and reasonable, the step should be considered correct. If there are multiple reflections, consider only the final one. - The students answer may employ multiple approaches to solve the problem. Within single response, some approaches may be logically sound while others may not be. If the final conclusions are correct and at least one approach is logically sound, the entire solution should be considered correct. - In cases where the problem is ambiguous, consider all possible interpretations and determine if the students response aligns with any of them. - Evaluate the entire solution, as some intermediate steps might seem incorrect initially but are rectified later, such as dismissing an extraneous root. Ensure you consider the entire context and, if necessary, review the steps more than once. - The errors to identify can be very subtle, sometimes hiding in the inexplicit applications of theorems or conditions. So you should actively checking every small logical inferences at small granularity carefully, either in natural language or in formulas. - To help you identify the possible errors, every first time you checking step, you should repeat it in case you missed subtle information. Then you should check its validity by examining its logical inferences within the step/sentences/subsentences one by one. When proof is required, meticulously verify the soundness of each logical step. Incomplete inductive reasoning is unacceptable. If you suspect step is flawed, consider constructing counter-examples to test its validity. - Every step should have solid logical basis. Guessing without proof is not allowed. - For you convenience, you may be provided with reference solution. The reference solution might be gaped or just hint. It rarely is wrong but do not miss the possibility. However, some problems might ask just for one valid answer while multiple possible answers exist. In such case you should not judge the students answer is wrong because it has different solution. Reference answers should be viewed as guides rather than absolute standards. Students may use alternative methods, notations, or approaches that are equally valid. **Question**: {question} **Answer to Verify**: {answer_to_verify} Figure 9: Prompt Template for Process Verification. 17 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification D. Additional Experiment Results Table 5: Detailed evaluation results on GSM8k partition of ProcessBench. Model Absolute Approximate Rough Acc Pre Rec F1 Acc Pre Rec F1 Acc Pre Rec 88.5 DeepSeek-V3-0324 92.0 DeepSeek-R1-0528 91.3 Qwen-Max-Preview gpt-oss-120b(high) 91.5 Qwen2.5-Math-PRM-72B 89.0 87.8 DeepSeek-R1-Distill-32B 91.0 OPV 84.3 DeepSeek-V3-0324 91.5 DeepSeek-R1-0528 91.8 Qwen-Max-Preview gpt-oss-120b(high) 92.3 Qwen2.5-Math-PRM-72B 87.3 89.3 DeepSeek-R1-Distill-32B 89.8 OPV 81.0 86.4 85.0 85.3 82.0 81.6 84.6 76.2 86.6 86.4 87.5 79.8 82.3 83.9 With Standard Answers 90.1 93.2 91.0 90.1 90.5 89.0 90.1 89.3 92.3 91.6 91.9 89.7 88.4 91. 94.5 96.0 95.0 94.5 94.5 92.5 94.5 99.5 99.0 99.5 99.5 99.0 96.4 99.5 Without Standard Answers 97.9 97.4 98.4 97.9 98.5 99.0 97.4 85.7 91.7 92.0 92.4 88.2 89.9 90.2 81.8 92.6 91.3 93.6 86.8 86.4 90.8 88.5 95.0 94.8 95.8 92.0 92.0 94. 99.5 99.0 99.5 99.5 99.0 96.4 99.5 97.9 97.4 98.4 97.9 98.5 99.0 97.4 94.6 96.0 95.0 94.6 94.6 92.5 94.6 89.2 94.9 94.8 95.7 92.2 92.3 94.0 97.8 98.5 98.5 98.3 97.8 96.0 98.5 91.3 97.3 97.0 98.0 94.5 94.3 97. 96.0 97.9 97.5 97.0 96.5 95.4 97.5 85.9 96.9 95.5 97.9 90.9 90.1 96.9 99.5 99.0 99.5 99.5 99.0 96.4 99.5 97.9 97.4 98.4 97.9 98.5 99.0 97.4 97.7 98.5 98.5 98.2 97.7 95.9 98.5 91.5 97.2 96.9 97.9 94.5 94.3 97. Table 6: Detailed evaluation results on MATH partition of ProcessBench."
        },
        {
            "title": "Rec",
            "content": "F"
        },
        {
            "title": "Rec",
            "content": "F"
        },
        {
            "title": "Rec",
            "content": "F"
        },
        {
            "title": "79.8\nDeepSeek-V3-0324\n87.7\nDeepSeek-R1-0528\n89.5\nQwen-Max-Preview\ngpt-oss-120b(high)\n90.1\nQwen2.5-Math-PRM-72B 81.7\n83.7\nDeepSeek-R1-Distill-32B\n87.5\nOPV",
            "content": "68.6 79.8 81.8 82.6 72.6 67.5 77.4 68.0 79.5 83.1 84.3 71.0 72.1 78.6 With Standard Answers 78.7 88.5 90.3 91.2 82.3 77.8 88.7 79.2 84.9 86.5 87.1 82.3 77.0 85.0 87.1 91.4 92.7 93.2 89.7 85.4 92.8 93.6 90.6 91.9 92.1 95.1 89.7 94. Without Standard Answers 95.1 93.8 93.1 92.9 92.9 97.8 95.1 79.3 86.1 87.8 88.4 80.5 83.0 86.1 77.0 88.0 90.0 92.4 79.7 82.5 88.5 86.5 92.3 93.0 94.0 87.5 90.7 93.0 93.6 90.6 91.9 92.1 95.1 89.7 94.3 95.1 93.8 93.1 92.9 92.9 97.8 95. 85.5 89.5 91.1 91.7 88.2 83.3 91.4 85.1 90.8 91.5 92.6 85.8 89.5 91.7 95.1 95.6 95.7 96.3 95.6 92.2 96.8 93.3 96.5 96.2 96.9 92.8 95.1 96.7 94.3 98.4 97.4 98.7 94.2 91.0 97.7 89.1 97.4 97.4 99.5 89.8 90.8 96. 93.6 90.6 91.9 92.1 95.1 89.7 94.3 95.1 93.8 93.1 92.9 92.9 97.8 95.1 93.9 94.4 94.6 95.3 94.6 90.3 96.0 92.0 95.6 95.2 96.1 91.3 94.2 95.9 18 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Table 7: Detailed evaluation results on OlympiadBench partition of ProcessBench. Model Absolute Approximate Rough Acc Pre Rec F1 Acc Pre Rec F1 Acc Pre Rec F1 73.4 DeepSeek-V3-0324 81.8 DeepSeek-R1-0528 80.7 Qwen-Max-Preview 81.4 gpt-oss-120b(high) Qwen2.5-Math-PRM-72B 75.6 65.8 DeepSeek-R1-Distill-32B 78.1 OPV 69.9 DeepSeek-V3-0324 81.9 DeepSeek-R1-0528 82.1 Qwen-Max-Preview gpt-oss-120b(high) 82.6 Qwen2.5-Math-PRM-72B 74.2 76.4 DeepSeek-R1-Distill-32B 79.3 OPV 56.7 68.1 66.4 68.1 59.2 49.8 62.4 53.4 67.2 68.9 69.6 57.9 59.3 64.3 With Standard Answers 67.6 81.7 78.6 79.9 69.2 60.2 73.2 70.0 76.6 75.4 75.7 71.7 61.4 73.6 82.2 89.1 87.6 87.7 83.1 75.2 85. 91.4 87.5 87.2 85.1 90.8 80.1 89.6 Without Standard Answers 90.5 91.4 86.3 86.3 88.4 97.3 87.8 67.2 77.4 76.6 77.1 70.0 73.7 74.2 63.2 79.3 81.2 81.4 66.7 69.0 75.6 78.8 89.0 88.6 88.7 81.1 84.2 86.2 91.4 87.5 87.2 85.1 90.8 80.1 89. 90.5 91.4 86.3 86.3 88.4 97.3 87.8 77.7 84.5 82.7 82.4 78.5 68.7 80.6 74.4 84.9 83.7 83.8 76.1 80.7 81.3 93.8 95.2 94.6 94.6 92.7 88.1 95.5 90.7 95.7 94.6 95.0 89.0 94.4 94.9 90.6 98.3 96.7 99.0 88.2 84.1 97. 83.5 95.9 97.6 99.0 80.9 87.7 97.0 91.4 87.5 87.2 85.1 90.8 80.1 89.6 90.5 91.4 86.3 86.3 88.4 97.3 87.8 91.0 92.6 91.7 91.5 89.4 82.0 93.2 86.9 93.6 91.6 92.2 84.5 92.2 92.2 Table 8: Detailed evaluation results on Omni-MATH partition of ProcessBench."
        },
        {
            "title": "Rec",
            "content": "F"
        },
        {
            "title": "Rec",
            "content": "F"
        },
        {
            "title": "Rec",
            "content": "F"
        },
        {
            "title": "61.5\nDeepSeek-V3-0324\n76.7\nDeepSeek-R1-0528\n78.6\nQwen-Max-Preview\ngpt-oss-120b(high)\n78.0\nQwen2.5-Math-PRM-72B 69.9\n69.0\nDeepSeek-R1-Distill-32B\n72.5\nOPV",
            "content": "39.5 47.7 51.9 51.3 42.2 34.6 44.8 37.4 51.0 53.7 52.8 43.8 43.6 46.1 With Standard Answers 52.1 60.6 65.7 66.3 52.6 45.2 58.8 54.8 60.4 64.2 63.5 57.7 49.0 58.6 77.6 82.8 85.6 85.8 78.1 71.6 82.0 89.2 82.2 84.2 83.4 91.3 83.8 84. Without Standard Answers 88.4 88.0 80.9 83.0 88.4 97.1 83.0 52.5 64.5 64.6 64.5 58.6 60.2 59.3 48.4 62.7 66.8 66.7 54.9 53.4 60.2 74.5 84.5 85.7 85.9 79.7 78.9 82.7 89.2 82.2 84.2 83.4 91.3 83.8 84.6 88.4 88.0 80.9 83.0 88.4 97.1 83. 65.7 69.7 73.8 73.9 66.8 58.7 69.4 62.6 73.2 73.2 73.9 67.7 68.9 69.8 92.8 94.2 94.7 95.4 91.0 88.3 95.4 90.7 95.2 94.2 95.4 90.4 92.9 95.2 82.4 93.0 93.1 97.1 76.1 72.1 95.8 76.6 91.8 94.2 97.6 75.8 78.5 96. 89.2 82.2 84.2 83.4 91.3 83.8 84.6 88.4 88.0 80.9 83.0 88.4 97.1 83.0 85.7 87.2 88.5 89.7 83.0 77.5 89.9 82.1 89.8 87.1 89.7 81.6 86.8 89.3 19 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Table 9: Detailed evaluation results on K-12 partition of OPV-Bench. Model Absolute Approximate Rough Acc Pre Rec F1 Acc Pre Rec F1 Acc Pre Rec F1 75.7 DeepSeek-V3-0324 86.6 DeepSeek-R1-0528 87.6 Qwen-Max-Preview gpt-oss-120b(high) 82.1 Qwen2.5-Math-PRM-72B 60.4 75.1 DeepSeek-R1-Distill-32B 76.6 OPV-Stage1 86.6 OPV-Stage2 87.6 OPV-Stage3-w/o RL 86.1 OPV 70.3 DeepSeek-V3-0324 83.1 DeepSeek-R1-0528 84.6 Qwen-Max-Preview gpt-oss-120b(high) 79.0 Qwen2.5-Math-PRM-72B 61.9 73.6 DeepSeek-R1-Distill-32B 72.6 OPV-Stage1 83.1 OPV-Stage2 84.6 OPV-Stage3-w/o RL 87.6 OPV 80.3 85.4 87.9 79.9 61.4 74.5 72.2 82.4 88.5 89.6 71.7 80.1 83.3 77.7 62.9 70.6 69.3 80.1 84.4 86.7 With Standard Answers 83.2 86.1 89.3 82.8 62.1 76.6 74.1 83.6 90.0 91.2 79.3 89.2 89.7 86.1 73.0 80.8 83.3 89.7 89.6 88.0 77.7 87.1 88.6 84.6 61.4 77.1 78.6 87.6 88.6 87. 78.3 93.3 91.6 93.3 90.0 88.2 98.3 98.3 90.8 86.6 Without Standard Answers 82.5 95.0 92.4 90.8 87.5 95.0 96.6 95.0 90.8 93.3 76.7 86.9 87.7 83.7 73.2 81.0 80.7 86.9 87.5 89.9 73.3 80.1 85.3 80.0 63.3 71.5 69.7 80.7 85.0 87.4 71.8 83.1 86.1 81.0 62.4 74.6 73.1 83.6 85.1 88.1 78.3 93.3 91.6 93.3 90.0 88.2 98.3 98.3 90.8 86. 82.5 95.0 92.4 90.8 87.5 95.0 96.6 95.0 90.8 93.3 80.7 89.5 90.5 87.8 73.5 82.0 84.5 90.4 90.4 88.8 77.7 86.9 88.7 85.0 73.4 81.6 81.0 87.3 87.8 90.2 81.2 88.6 90.0 87.6 68.8 80.6 81.1 90.5 90.0 88.1 76.2 85.1 87.6 85.0 64.4 77.1 74.6 87.1 87.1 90.5 88.7 88.1 91.6 86.7 67.9 80.8 76.5 87.3 92.3 92. 78.6 82.5 87.3 85.0 64.8 73.9 71.0 85.0 87.8 91.0 78.3 93.3 91.6 93.3 90.0 88.2 98.3 98.3 90.8 86.6 82.5 95.0 92.4 90.8 87.5 95.0 96.6 95.0 90.8 93.3 83.2 90.6 91.6 89.9 77.4 84.3 86.0 92.5 91.5 89.6 80.5 88.3 89.8 87.8 74.5 83.1 81.9 89.7 89.3 92.1 Table 10: Detailed evaluation results on High School Competition partition of OPV-Bench."
        },
        {
            "title": "Rec",
            "content": "F"
        },
        {
            "title": "Rec",
            "content": "F"
        },
        {
            "title": "Rec",
            "content": "F"
        },
        {
            "title": "66.5\nDeepSeek-V3-0324\n67.0\nDeepSeek-R1-0528\n69.4\nQwen-Max-Preview\n67.1\ngpt-oss-120b(high)\nQwen2.5-Math-PRM-72B 49.5\n64.6\nDeepSeek-R1-Distill-32B\n65.8\nOPV-Stage1\n69.8\nOPV-Stage2\n76.4\nOPV-Stage3-w/o RL\n78.3\nOPV",
            "content": "64.6 69.4 67.5 64.2 47.0 66.0 64.5 69.0 78.6 81.7 58.6 61.4 63.8 62.7 47.0 59.0 59.9 63.6 72.1 72.6 With Standard Answers 71.6 76.4 74.0 75.1 47.9 71.6 68.5 74.4 88.0 90.0 74.6 79.7 77.4 73.5 63.4 78.6 77.6 79.6 80.4 83.5 78.4 82.3 79.4 78.6 51.3 79.1 76.3 80.8 85.5 87.9 88.1 93.5 90.8 86.1 97.6 97.0 97.5 94.0 82.3 85. Without Standard Answers 90.7 92.8 90.3 85.3 97.3 97.5 96.3 93.3 86.3 91.0 71.2 73.9 74.8 72.3 63.4 73.5 73.9 75.6 78.6 80.8 64.6 68.1 72.2 73.1 47.8 61.7 63.9 71.3 80.1 81.9 73.1 74.5 77.6 76.9 51.1 68.4 70.8 77.8 82.4 85.4 88.1 93.5 90.8 86.1 97.6 97.0 97.5 94.0 82.3 85.3 90.7 92.8 90.3 85.3 97.3 97.5 96.3 93.3 86.3 91. 79.0 84.1 81.6 80.2 64.3 82.4 80.5 83.1 85.1 87.6 75.4 78.5 80.2 78.8 64.1 75.6 76.8 80.8 83.1 86.2 85.3 88.9 87.3 86.3 56.6 83.1 80.1 87.0 90.3 91.3 81.2 83.3 86.4 84.9 54.4 73.1 76.1 85.4 89.0 90.8 81.5 85.7 84.9 86.5 50.9 76.0 72.5 82.5 97.9 96.9 74.0 78.0 83.8 84.7 49.6 65.7 68.7 80.7 91.3 90. 88.1 93.5 90.8 86.1 97.6 97.0 97.5 94.0 82.3 85.3 90.7 92.8 90.3 85.3 97.3 97.5 96.3 93.3 86.3 91.0 84.7 89.4 87.7 86.3 66.9 85.3 83.1 87.9 89.5 90.7 81.5 84.8 87.0 85.0 65.7 78.5 80.2 86.5 88.8 90.8 20 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Table 11: Detailed evaluation results on Undergraduate partition of OPV-Bench. Model Absolute Approximate Rough Acc Pre Rec F1 Acc Pre Rec F1 Acc Pre Rec F1 62.2 DeepSeek-V3-0324 58.3 DeepSeek-R1-0528 58.7 Qwen-Max-Preview gpt-oss-120b(high) 52.8 Qwen2.5-Math-PRM-72B 56.2 67.9 DeepSeek-R1-Distill-32B 65.8 OPV-Stage1 66.5 OPV-Stage2 72.2 OPV-Stage3-w/o RL 75.1 OPV 53.3 DeepSeek-V3-0324 46.4 DeepSeek-R1-0528 51.9 Qwen-Max-Preview gpt-oss-120b(high) 48.9 Qwen2.5-Math-PRM-72B 55.2 58.0 DeepSeek-R1-Distill-32B 56.0 OPV-Stage1 58.2 OPV-Stage2 61.4 OPV-Stage3-w/o RL 65.4 OPV 59.1 57.0 56.9 52.4 53.5 62.3 60.3 62.7 74.2 76.6 52.2 47.5 51.6 49.3 52.9 55.1 53.8 56.4 60.8 63.7 With Standard Answers 63.3 62.5 62.1 61.1 54.3 65.7 63.2 67.8 81.0 84.7 67.5 62.4 64.1 58.9 68.5 74.1 73.3 71.1 71.3 74.5 66.6 63.6 64.2 62.0 57.6 71.7 69.4 71.4 76.2 79. 78.8 68.9 73.3 67.4 95.2 91.5 93.3 82.2 68.6 72.5 Without Standard Answers 79.2 64.3 68.3 63.0 94.0 88.9 86.3 73.7 65.1 72.5 62.9 54.7 58.8 55.3 67.7 68.0 66.3 63.9 62.9 67.8 55.5 52.5 56.0 57.7 54.1 57.4 56.4 61.1 68.4 71.8 57.9 52.9 57.1 58.2 57.1 61.3 59.7 63.2 67.4 71.9 78.8 68.9 73.3 67.4 95.2 91.5 93.3 82.2 68.6 72. 79.2 64.3 68.3 63.0 94.0 88.9 86.3 73.7 65.1 72.5 70.2 65.5 67.3 64.1 69.2 76.5 75.4 74.3 74.3 78.1 65.3 57.8 61.5 60.2 68.7 69.8 68.2 66.8 66.7 72.2 71.5 69.9 70.0 70.7 62.0 75.4 72.7 75.7 81.0 84.3 63.9 62.3 65.8 68.2 60.4 65.7 64.9 71.2 73.1 77.0 68.7 70.5 68.9 72.5 57.2 69.4 66.2 72.9 91.5 95. 60.6 62.0 65.3 70.5 56.2 60.9 60.6 70.3 77.6 79.8 78.8 68.9 73.3 67.4 95.2 91.5 93.3 82.2 68.6 72.5 79.2 64.3 68.3 63.0 94.0 88.9 86.3 73.7 65.1 72.5 73.4 69.7 71.1 69.8 71.5 78.9 77.5 77.2 78.4 82.3 68.7 63.1 66.7 66.5 70.4 72.3 71.2 71.9 70.8 76.0 E. Case Studies Here we list interesting cases containing process errors that were found by OPV across different datasets. 21 OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification Question: Let ğ‘, ğ‘, ğ‘ > 0 such that ğ‘ + ğ‘ ğ‘. ğ‘ƒ = (ğ‘4 + ğ‘4 + ğ‘4) ( 1 ğ‘4 + 1 ğ‘4 + 1 ğ‘4 ) ."
        },
        {
            "title": "Find the minimum value of",
            "content": "Model Answer from AM-DeepSeek-R1-0528-Distilled: Given positive real numbers ğ‘, ğ‘, and ğ‘ with the constraint ğ‘ + ğ‘ ğ‘, the goal is to minimize the expression ğ‘ƒ = (ğ‘4 + ğ‘4 + ğ‘4) ( 1 ğ‘4 + 1 ğ‘4 + 1 ğ‘ ) . The expression ğ‘ƒ is symmetric in ğ‘ and ğ‘, and the constraint is also symmetric in ğ‘ and ğ‘. Therefore, the minimum likely occurs when ğ‘ = ğ‘. Assuming ğ‘ = ğ‘, the constraint becomes 2ğ‘ ğ‘. Let ğ‘¡ = ğ‘/ğ‘, so ğ‘¡ 2. Then, ğ‘ƒ = (ğ‘4 + ğ‘4 + (ğ‘¡ğ‘)4) ( 1 ğ‘4 + 1 ğ‘4 + 1 (ğ‘¡ğ‘)4 ) = (2ğ‘4 + ğ‘¡4ğ‘4) ( 2 ğ‘4 + ) . 1 ğ‘¡4ğ‘4 The ğ‘4 terms cancel, giving ğ‘ƒ = (2 + ğ‘¡4) ( 2 + ) 1 ğ‘¡4 = 4 + 2ğ‘¡4 + 2 ğ‘¡4 + 1 = 5 + 2ğ‘¡4 + 2 ğ‘¡4 . Substituting ğ‘¢ = ğ‘¡4 (so ğ‘¢ 16),ğ‘ƒ = 5 + 2ğ‘¢ + 2 ğ‘¢2 , . The derivative is ğ‘ƒ (ğ‘¢) = 2 2 ğ‘¢ which is positive for ğ‘¢ > 1. Since ğ‘¢ 16 > 1, ğ‘ƒ (ğ‘¢) is increasing for ğ‘¢ 16, and the minimum occurs at ğ‘¢ = 16 (i.e., ğ‘¡ = 2). At this point, ğ‘ƒ = 5 + 2(16) + 2 16 = 5 + 32 + 0.125 = 37.125 = 297 8 . This value is achieved when ğ‘ = ğ‘ and ğ‘ = 2ğ‘, satisfying ğ‘ + ğ‘ = ğ‘ (equality in the constraint). To confirm this is the global minimum, consider cases where ğ‘ = ğ‘ or ğ‘ + ğ‘ < ğ‘. When ğ‘ + ğ‘ < ğ‘, setting ğ‘ = ğ‘ = 1 and ğ‘ > 2 gives ğ‘ƒ > 297/8. For ğ‘ = ğ‘ with ğ‘ + ğ‘ = ğ‘, define ğ‘Ÿ = ğ‘/ğ‘ > 0, so ğ‘ƒ = [1 + ğ‘Ÿ4 + (1 + ğ‘Ÿ)4] [ 1 + 1 ğ‘Ÿ4 + 1 (1 + ğ‘Ÿ) ] . At ğ‘Ÿ = 1, ğ‘ƒ = 297/8. The derivative at ğ‘Ÿ = 1 is zero, and ğ‘ƒ (ğ‘Ÿ) > 297/8 for ğ‘Ÿ = 1. As ğ‘Ÿ 0+ or ğ‘Ÿ , ğ‘ƒ . Thus, under ğ‘ + ğ‘ = ğ‘, the minimum is 297/8. When ğ‘ + ğ‘ < ğ‘, ğ‘ƒ > 297/8, so the minimum is achieved when equality holds in the constraint. Thus, the minimum value of ğ‘ƒ is 297 8 . Judgments of OPV: The first error occurs in Step 1 where the student assumes ğ‘ = ğ‘ based solely on symmetry without rigorous justification. While the problem is symmetric in ğ‘ and ğ‘, the minimum might not necessarily occur at ğ‘ = ğ‘ without proof. Although the student later verifies this assumption through additional analysis, the initial step lacks formal argument (e.g., using inequalities, convexity, or perturbation methods) to confirm that deviations from ğ‘ = ğ‘ would increase ğ‘ƒ . This makes Step 1 logically incomplete and thus incorrect. Figure 10: case study sampled from AM-DeepSeek-R1-0528-Distilled dataset where the reasoning process is defective despite the final answer being correct. The rigor of the inequality is not proven in the models proposed answer, which is identified by OPV. In the generations, we use ref color to present the defective content."
        }
    ],
    "affiliations": [
        "MMLab, The Chinese University of Hong Kong",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University"
    ]
}