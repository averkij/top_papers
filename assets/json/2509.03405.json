{
    "paper_title": "LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations",
    "authors": [
        "Daniela Gottesman",
        "Alon Gilae-Dotan",
        "Ido Cohen",
        "Yoav Gur-Arieh",
        "Marius Mosbach",
        "Ori Yoran",
        "Mor Geva"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, a suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide a controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 5 0 4 3 0 . 9 0 5 2 : r LMEnt: Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations Daniela Gottesmanτ Alon Gilae-Dotanτ Ido Cohenτ Yoav Gur-Ariehτ Marius Mosbachω Ori Yoranτ Mor Gevaτ τ Tel Aviv University ω Mila Quebec AI Institute & McGill University gottesman3@mail.tau.ac.il"
        },
        {
            "title": "Abstract",
            "content": "Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics. huggingface.co/LMEnt github.com/LMEnt"
        },
        {
            "title": "Introduction",
            "content": "(LMs) capture substantial Language models amounts of knowledge and beliefs about the world from their training data (Petroni et al., 2019; Roberts et al., 2020; AlKhamissi et al., 2022; Andreas, 2022). fundamental, yet underexplored, question is how such knowledge representations are formed and shaped during pretraining. Namely, what is the interplay between data composition, training dynamics, and the internal knowledge mechanisms in LMs. An understanding of these processes could provide better control over the models knowledge, potentially improving its factuality and reasoning. key prerequisite for studying the interplay between data and knowledge representations is the ability to locate exactly where specific knowledge appears in the pretraining data. However, such metadata is rarely available for pretraining corpora, and existing tools for locating it rely on post-hoc string-based search (Elazar et al., 2024; Liu et al., 2024a, 2025), which lack robustness against variability in phrasing of semantically equivalent information. For example, consider retrieving information about the American football team the Buffalo Bills  (Fig. 2)  . Stringbased approaches are less likely to retrieve relevant documents where the team is referred to as Buffalo, The Bills, or even the team. Further, when expanding queries with name aliases, existing tools often suffer from noisy retrieval, as we observe in 5.2. This noise is due in part to alias ambiguity, failing to differentiate between the team Buffalo and the city Buffalo, NY. In this work, we tackle this gap by introducing LMEnt (Language Models with annotated Entity mentions), suite of pretrained models with full transparency about the locations in training where specific entity was mentioned. The focus on entity mentions stems from mounting evidence that entity names are central to the construction of knowledge representations in LMs (rather than individual tokens; Li et al., 2021; Meng et al., 2022; Geva et al., 2023; Gottesman and Geva, 2024). LMEnt has three components: (1) 7.3M Figure 1: The LMEnt suite is composed of three components (left) fine-grained entity mentions for every document in the pretraining corpus, (middle) an index that retrieves by the entity QID and outperforms string-based retrieval methods, (right) 12 models trained on 1, 2, 4, and 6 epochs where each step can be mapped to the entities it mentions, and each entity can be traced to the steps that introduce it. fine-grained entity annotations across all documents in pretraining corpus built on English Wikipedia (Fig. 1, left, 2), (2) an Elasticsearch index with 10.5M chunks which enables retrieval of all the chunks that mention specific entity by their unique Wikidata identifier (Fig. 1, center, 3), and (3) collection of 12 LMs with 170M, 600M, and 1B parameters, pretrained on the annotated data with 110 intermediate checkpoints per epoch (Fig. 1, right, 4). To build LMEnt, we annotate each document in the English Wikipedia with fine-grained entity mentions extracted from three complementary sources: Wikipedia hyperlinks, entity-linking and coreference resolution  (Fig. 2)  . We then train LMEnt models on this annotated corpus, which has 0.03%4.7% of the tokens typically used to train LMs of similar sizes. Experiments show that LMEnt is an effective testbed for studying knowledge acquisition in LMs (5). On factual question answering (Mallen et al., 2023), LMEnt models reach comparable performance (7.4% on all entities, 66% on popular entities) to Pythia-1.4B (Biderman et al., 2023) (8.7%, 67%) and OLMo-1B (Groeneveld et al., 2024a) (10.4%, 66%), with noticeably lower performance than OLMo-2-1B (OLMo et al., 2025) (13.9%, 74%) and SmolLM-1.7B (Allal et al., 2025) (14.9%, 73%) mostly due to recall failures on rare facts. Moreover, LMEnt entitybased retrieval shows clear benefits over stringbased search methods similar to WIMBD (Elazar et al., 2024) and Infinigram (Liu et al., 2024a); LMEnt returns more relevant document chunks for as many as 80% of entities, while maintaining > 97% precision as the limit on the number of chunks retrieved increases. Conversely, string-based tools exhibit sharp precision drops, to as low as 27%. Last, we demonstrate the utility of LMEnt by analyzing knowledge acquisition across checkpoints (6). We find that while learning correlates with fact frequency, the rates of both learning and forgetting increase with frequency phenomenon not yet fully understood. Overall, LMEnt provides novel capabilities for precise tracking of information during pretraining and lays the groundwork for studying the interplay between pretraining data and knowledge representations in LMs. We release LMEnt to the community as an open-source suite on HuggingFace, including the pretraining data, fine-grained entity mentions, pretrained models, and intermediate checkpoints."
        },
        {
            "title": "2 Labeling the English Wikipedia with",
            "content": "Fine-Grained Entity Mentions To support analyzing knowledge evolution over training, we select corpus rich in factual content and annotate its documents with fine-grained entity mentions (Fig. 1, left). These annotations enable tracking exactly which entities model observes at each training step when training on this data. Next, we explain our choice of Wikipedia for pretraining data, and the annotation process.1 1The annotation pipeline used 8 H100 GPUs (80GB VRAM each). Running coreference resolution and entity linking took 5 days will full GPU utilization. Figure 2: The document for the entity Josh Allen is split into two chunks, which are processed independently during pretraining. In the first chunk, the Buffalo Bills (Q221626) is mentioned explicitlyidentified through hyperlinks and entity linkingand implicitly, through coreference resolution. Although both Q221626 (Buffalo Bills) and Q40435 (the city of Buffalo) share the surface form Buffalo, LMEnt disambiguates them as Mention (2) is linked to Q221626 with 1.0 confidence by coreference resolution, while Mention (4) is linked to Q40435 with 0.98 confidence using entity linking, and the two mentions are placed in separate coreference clusters. Mention (3) the team is linked to Q221626 since it is in the same coreference cluster as Buffalo Bills. Pretraining data Wikipedia is natural choice for the source of pretraining data because it is knowledge base that is structured around entities, such as people, locations, and world events. Specifically, related entity pages are connected through hyperlinks, which enables us to easily disambiguate entities and map them to their unique identifiers, called QIDs. Moreover, it provides snapshot of world knowledge at given time, minimizing contradicting information. In addition, Wikipedia is common resource for knowledgefocused benchmarks (e.g., Petroni et al., 2021; Sciavolino et al., 2021; Lewis et al., 2021; Mallen et al., 2023), making it well-suited for tracking how changes in pretraining affect learning. Annotation objectives Our goal is to be able to query which entities model saw at any given training step. To achieve this, we design an entitymention annotation process that satisfies the following criteria (see Fig. 2 for illustration): 1. Disambiguation between entities with similar names and aliases, e.g., Buffalo for the Buffalo Bills team and Buffalo for the city of Buffalo, by mapping mentions to Wikidata QIDs (Vrandeˇcic and Krötzsch, 2014). 3. Providing coverage of all mentions across the document, as LMs train on each chunk in isolation and may lack the full document context. We meet these conditions by leveraging three complementary sources for annotating entity mentions. The first by extracting Wikipedia hyperlinks and applying entity linking; and the second through coreference resolution. To satisfy the third condition, we structure the entity mentions so that each mentions character span maps to its entity QID. We detail each of these steps next."
        },
        {
            "title": "2.1 Entity Mention Sources",
            "content": "Hyperlinks To extract hyperlinks, we parse the raw XML Wikipedia dump and extract the url attributes from href tags. Each url links to the Wikipedia article of the corresponding entity. We perform entity disambiguation by mapping the url to its QID using Wikidata. Entity linking Since hyperlinks are manually added by human editors, they are the most reliable source, but they offer poor coverage within document.2 To identify additional mentions, we use ReFinED (Ayoola et al., 2022), state-of-theart modular entity linking system that performs 2. Capturing entities indirectly through descriptive phrases and pronouns, e.g., the team, their and the hospital. referenced 2Wikipedia style linking guidelines only the first mention of an entity in document: https://en.wikipedia.org/wiki/Wikipedia: Manual_of_Style/Linking. recommend mention detection, fine-grained typing, and entity disambiguation. ReFinED supports zero-shot entity linking by encoding the mention context and candidate entity descriptions using RoBERTa (Liu et al., 2019), and selecting the entity whose description embedding best aligns with the mention representation. By replacing the underlying base of entity descriptions from our Wikipedia dump, we can identify mentions of new entities and map them to their QIDs without retraining ReFinED. Coreference resolution The above two sources are used to extract explicit entity mentions, however, entities may still be referred to implicitly, e.g., the team in Fig. 2. To fill in this gap, we use the Maverick coreference resolution model (Martinelli et al., 2024), which achieves state-ofthe-art performance on WikiCoref (Ghaddar and Langlais, 2016). To run Maverick at scale, we reduce its memory footprint by replacing the model backbone, allowing us to parallelize inference on documents (see Appendix B.1 for details). Maverick outputs clusters of mentions that refer to the same entity. However, these mentions must still be mapped to their QID, which we discuss below."
        },
        {
            "title": "2.2 Scoring Entity Mentions in a Document",
            "content": "For every document in the corpus, each of the sources yields set of entity mentions defined by character span = (cstart, cend) (the same mention can be extracted from one or more sources). Here, our goal is to map all mentions in document to their QIDs, providing the flexibility to chunk the document arbitrarily while being able to identify which chunks contain given entity. To create mention-QID mappings, we define three scoring procedures based on the different sources that indicate how confidently mention can be linked to its corresponding entity  (Fig. 2)  : Hyperlinks (H) identify the first occurrence of direct entity mention like Buffalo Bills. This mention links to the Wikipedia article about the football team and is mapped to QID Q221626 using Wikidata. Since hyperlinks are the most reliable source, we define H(m, Q221626) = 1 if there is hyperlink in the span of directing to the Wikipedia page of the entity with QID Q221626, and H(m, Q221626) = 0 otherwise. falo. The ReFinED model already performs entity disambiguation and provides score reflecting its confidence that mention links to QID, e.g., (m, Q40435) = 0.98. Therefore, we simply use this score such that EL(m, Q40435) = (m, Q40435). Coref (C) connects indirect mentions, like pronouns, aliases, and generic descriptors, to their entity QIDs. Suppose we are trying to resolve the QID of the team. We run Maverick on the document, and it identifies that the team is related to the cluster of mentions containing Buffalo Bills, the Buffalo, the Bills, and their. How can we leverage the cluster to map the team to its QID? We compute distribution of scores over all QIDs already mapped to some menIn this case, Q221626 tion in the cluster. is the only QID supported by this cluster so C(the team, Q221626) = 1.0. Since this score is computed using the entire cluster, the score is the same across all mentions in the cluster. Occasionally, coreference mention encapsulates multiple entities, as in John R. Oishei Childrens Hospital Buffalo, which leads to score distribution over several entities. In this case of mapping ambiguity, we rely on textual similarity of mentions in the cluster to promote one entity over another. B.2 provides the formal definition of this score, and detailed explanation of how we resolve mapping ambiguity. The final entity mention structure maps each mention to list of candidate entity QIDs with up to three scores per candidate. Keeping three scores per mention affords us flexibility to filter chunk retrieval based on source and confidence thresholds (3.2). This list structure also accommodates cases where span encompasses multiple entities or belongs to multiple coreference clusters. Also, by mapping character spans to QIDs, we can chunk the document arbitrarily and identify which entities are referenced by each chunk. In B.3, we include qualitative analysis of error patterns over random sample of 112 mentions to justify the design of the scores."
        },
        {
            "title": "Pretraining Data",
            "content": "Entity linking (EL) are used to increase coverage of direct mentions like, e.g., BufNow that the documents are annotated with entity mentions (Fig. 1, left), we need to process them into chunks of information for pretraining. During this processing, we propagate the relevant annotations, allowing us to infer the entities mentioned at the particular training step when chunk is introduced (Fig. 1, right). In 3.1, we describe how documents are processed into chunks, and in 3.2, we discuss our index for efficient retrieval of all chunks (and consequently, training steps) mentioning an entity (Fig. 1, center)."
        },
        {
            "title": "3.1 Data Processing for Training",
            "content": "Tokenization Each document is tokenized using the dolma2-tokenizer (Soldaini et al., 2024). This results in dataset with total of 3.6 billion unique tokens, which is less than 0.09% of the tokens used to train OLMo-2 (OLMo et al., 2025). Chunking chunk is sequence of tokens that the model independently processes per step. Unlike existing chunking strategies, like concat-andchunk used in Liu et al. (2019); Ott et al. (2019); Touvron et al. (2023a,b), we ensure that each chunk only contains content from single document using the Variable Sequence Length Curriculum (Pouransari et al., 2024). This prevents the model from learning spurious correlations between unrelated documents. Chunking curricula split documents at some sequence length, which can result in entity mentions being split across chunk boundaries. To prevent splitting an entity mention, we modify the chunking logic to terminate just before mention and pad the remaining tokens to the desired length. These design choices aim to reduce artifacts when training for knowledge recall. The entity mentions for chunk are defined as those fully contained within its boundaries. Practically, we extend the OLMo-Core framework (OLMo et al., 2025) to retrieve chunks with their entity mentions and adjust mention span indices to be relative to chunk boundaries."
        },
        {
            "title": "3.2 Chunk Retrieval",
            "content": "To support retrieval of chunks, we build an Elasticsearch index (Banon, 2010) of all chunks in the corpus. Each entry in the index includes the chunk ID, the chunk text, and associated entity mentions. The chunk ID is traceable to the specific training step at which it was introduced, allowing us to retrieve all chunks seen by an intermediate model checkpoint. We can also match an entity mention on its QID, source, and three scores, allowing us to retrieve all chunks that mention certain entities, and the flexibility to tune thresholds for the scores. Retrieval algorithms typically apply an ordering scheme to the items they return (Robertson et al., 1995). To accommodate this, we assign score to each chunk based on the entity mentions that match the retrieval query. For every matching mention, we compute weighted average of its three source scores defined in 2.2. The score used to rank the chunk is the highest of these aggregated scores among its matching mentions."
        },
        {
            "title": "4 The LMEnt Suite",
            "content": "With the procedures for annotating entity mentions and processing pretraining data established, we now provide an overview of the resulting pretraining data index and LMEnt models. Pretraining data index Our pretraining data includes 3.6B tokens over 10.5M chunks, and features more than 7M different entities. The 400M mentions extracted are composed of 115M mentions from hyperlinks, 203M from entity linking, and 310M from coreference resolution. Additional statistics are provided in A.1, Tab. 2. Pretrained models We introduce collection of models trained on the annotated dataset, which serve as testbed for analyzing knowledge evolution over training. We train models of three different sizes, 170M, 600M, and 1B parameters, based on the OLMo-2 architecture (OLMo et al., 2025). Notably, the 170M model has the computeoptimal size for 3.6B tokens (Hoffmann et al., 2022). Each model is trained for 1, 2, 4, and 6 epochs (3.6B, 7.2B, 14.4B, 21.6B tokens, respectively), resulting in four variants per size. Each epoch initializes different batching and ordering of the chunks. We also release intermediate checkpoints every 1,000 training steps, yielding 110 checkpoints per epoch and proportionally more for longer training schedules. Additional details on model training are in B.4."
        },
        {
            "title": "5 Experiments",
            "content": "In the previous sections, we described the construction and composition of the LMEnt suite. Here, we establish the credibility of the framework by showing that LMEnt models match the knowledge recall performance of existing open-source models (5.1), and that entity-based chunk reFigure 3: Accuracy on popular PopQA entities as function of compute budget. LMEnt models achieve comparable performance to other models with better compute efficiency. trieval using LMEnt mentions outperforms stringbased methods (5.2)."
        },
        {
            "title": "5.1 Model Performance",
            "content": "Experimental setup To evaluate the LMEnt models, we use benchmarks that test for knowledge in Wikipedia. We choose two widely adopted benchmarks: PopQA (Mallen et al., 2023) and PAQ (Lewis et al., 2021). PopQA contains questions about 11K entities with varying popularities, and PAQ has 65M questions generated from Wikipedia passages. To keep computational costs manageable we subsample PAQ and only consider samples containing the same subject entities found in PopQA. This results in an evaluation set of approximately 70K questions.3 Results on PAQ are found in A.2. In addition to knowledgecentric tasks, we report results on commonsense reasoning, multiple choice, and reading comprehension tasks in A.4. Since LMEnt models are not instruction-tuned, we convert all samples into cloze-style prompts, such that the answer is the next expected phrase. For example, the original QA-style query what tv network does funny or die presents air on translates to Funny or Die Presents airs on the TV network. Full details of this conversion process are in B.6. Baselines We evaluate the performance of LMEnt against leading open-source models of comparable sizes, including Pythia (160M, 410M, 1B, 1.4B) (Biderman et al., 2023), OLMo (1B, 120K-84B) (Groeneveld et al., 2024a), OLMo-21B (OLMo et al., 2025), and SmolLM2 (135M, 360M, 1.7B) (Allal et al., 2025). We include 3Evaluating 1B parameter model on all PAQ questions using one H-100 80GB GPU would take 200 days. Figure 4: Mean accuracy on PopQA questions binned according to the number of chunks that the subject and answer entities co-occur in. Increasing model size helps learning associations between entities that appear more frequently in the same chunk. OLMo-1-20K-84Ban intermediate checkpoint of OLMo-1B trained for 20K steps on 84B unique tokensbecause it is the most comparable baseline to LMEnt models in terms of training token count.4 We also include two intermediate LMEnt baselines: LMEnt-1B-0E (randomly initialized) and LMEnt-1B-0.1E (trained for 10K steps). These serve to illustrate how more training data improves knowledge recall capabilities. Knowledge of LMEnt models is on par with models trained on up to 3K times more data Fig. 3 shows the performance of LMEnt models and the baselines as function of compute budget, measured in FLOPs. LMEnt achieves comparable performance on popular entities in PopQA to both the Pythia and OLMo-1B models, despite being trained with two orders-of-magnitude less compute. We also present results for all popularity levels in the A.2 which show similar trends. Though, since PopQA is dominated by tail entities, overall accuracy is lower across all models. Notably, the LMEnt 600M model surpasses the OLMo-1B intermediate checkpoint and the Pythia-1B checkpoint, which reaffirms that pretraining on Wikipedia enables models to acquire substantial factual knowledge (Devlin et al., 2019) and achieve better compute-performance tradeoffs. This suggests that LMEnt models provide useful testbed for studying knowledge in LMs, with implications for models trained on substantially larger amounts of data. Measuring the effect of model scale on knowledge encoding Since all LMEnt models are pre4Notably, OLMo-1-20K-84B is trained on 4 times more data than our largest LMEnt-1B model, and it may not observed all the facts we evaluate on."
        },
        {
            "title": "Match Examples",
            "content": "QID + Scores Buffalo, New York; Buffalo; the Queen City; this city; the Citys; the City of Buffalo; the city Buffalo; town on the banks of Lake Erie; his hometown"
        },
        {
            "title": "Canonical",
            "content": "Buffalo, New York"
        },
        {
            "title": "Extended",
            "content": "Buffalo, New York; Buffalo, NY; Buffalo, N. Y.; City of Light; The Queen City; The Nickel City"
        },
        {
            "title": "Canonical",
            "content": "buffalo, new york"
        },
        {
            "title": "Extended",
            "content": "buffalo, new york; buffalo, ny; buffalo, n. y.; city of light; the queen city; the nickel city M - S - Table 1: Example matched mentions for the entity Buffalo, New York (Q40435) across different methods. For LMEnt, mention is considered match if one of its candidate entities has the QID Q221626 and satisfies at least one of the score thresholds. The Canonical method matches only the entitys canonical name, whereas the Extended method matches any of the entitys aliases. trained on the same data, we can isolate the effect of model size on knowledge acquisition (Roberts et al., 2025). Fig. 4 shows that increasing model size improves learning popular of facts (subject and answer co-occur in 100 chunks), while having little effect on tail facts (1100 chunks). We leave exploring how scaling model size further can enhance knowledge learning for future work."
        },
        {
            "title": "5.2 Chunk Retrieval Performance",
            "content": "A key use-case of the LMEnt entity annotations is retrieving all the training data chunks that mention specific entity. Here, we compare the quality of entity-based retrieval using LMEnt mentions to existing string-based retrieval methods. Experimental setup We evaluate on test set of 1K entities, chosen via stratified sampling by their number of hyperlinks. We use the LMEnt index to retrieve their corresponding chunks, matching both on the entitys QID and on at least one of the score thresholds: = 1, EL 0.6, 0.6. These thresholds were empirically determined using dev set of 60 entities, described in A.5. We then compare the set of retrieved chunks to those retrieved by popular string-based methods. To evaluate retrieval precision, we use Gemini 2.5 Flash (Comanici et al., 2025) as judge (Gu et al., 2024; Li et al., 2024) which predicts Yes or No based on whether the mention directly relates to the target entity. The prompt used and the statistical test justifying our use in an LMFigure 5: Pairwise wins rates for LMEnt with multiple string-based methods and ablated LMEnt variations. LMEnt outperforms string-based methods by 66.7% 80.4%. Ablations (bottom three rows) show that hyperlinks and entity linking are the most crucial components of LMEnt. judge are provided in B.5. Since some entities are mentioned very frequently  (Fig. 6)  , we randomly sample 100 chunks from the set retrieved per method and entity, and measure the absolute number of chunks judged as Yes for each entity. Baselines We compare to string-based baselines that retrieve chunks using either casesensitive (CS-SS) or case-insensitive (CI-SS) exact matches of entity names within the chunk text. The Canonical variant matches only on the entitys canonical name, and the Extended variant additionally matches against the entitys Wikidata aliases which is common strategy used in prior work to improve recall (e.g., Cohen et al., 2024; Yang et al., 2024). The CS-SS Canonical baseline approximates the Infinigram (Liu et al., 2024a) tool which matches based on case-sensitive n-grams, and CI-SS Canonical baseline approximates WIMBD (Elazar et al., 2024) which retrieves on exact case-insensitive string matches. Although CS-SS Canonical relies on exact string matches rather than n-gram matches, we show in 5.2 that shorter references (e.g., the ngram Buffalo) introduce noise as they retrieve chunks referencing the city, animal, and football team together. Tab. 1 summarizes the retrieval strategies used by LMEnt and the baselines. For clarity, we also include example mentions that triggered the retrieval of chunk for the entity Buffalo, NY (Q40435). Entity-based retrieval largely outperforms the baselines Fig. 5 presents pairwise win rates between retrieval methods, where win is defined as one method retrieving more Yes-judged chunks than another for given entity. LMEnt conFigure 6: Distribution of the number of chunks retrieved by LMEnt, and both Canonical and Expanded variants of CI-SS and CS-SS. Retrieval was performed for 1K PopQA entities, selected via stratified sampling based on hyperlink counts. LMEnt retrieves more chunks than the Canonical variants for torso and tail entities, which together account for 99.7% of all entities in Wikipedia. The higher number of chunks returned for head entities by Expanded variants is likely bloated by noisy retrieval  (Fig. 7)  . sistently outperforms all other methods, retrieving more correct mentions for 66.3%80.4% of the entities. Surprisingly, the Expanded variants of both CI-SS and CS-SS perform worse than their canonical counterparts by 40%, suggesting that additional noise introduced by alias expansion outweighs any gains in recall (A.6, Fig. 13). Additionally, we observe that LMEnt is superior to string-based methods across all entity frequency levelsand often by substantial number of chunks (A.7, Fig. 14). Ablating entity mention sources reduces retrieval quality We also ablate components of LMEnt and examine the impact on win rate  (Fig. 5)  . Hyperlinks are the most valuable component as relying solely on them (LMEnt -EL -C) wins on 33.3% of entities. In contrast, using only entity linking (LMEnt -H -C) lowers wins to 29.3%. This indicates that while hyperlinks play crucial role, entity linking alone remains reasonably effectivehighlighting the potential of extending LMEnt to pretraining corpora beyond Wikipedia, where hyperlinks are unavailable. Removing coreference resolution has little effect on LMEnts performance, because judging such chunks in isolation is difficult. For example, in the chunk referring to Donald Trump solely as his Mexico City Policy, it is difficult to identify that his implicitly refers to Trump without context from earlier in the document which mentions him explicitly. Figure 7: Precision at various retrieval depths (k), the top-k chunks retrieved per entity. As the depth increases, LMEnt maintains above 97% precision, while CS-SS Canonical and CI-SS Expanded consistently decrease to 84% and 27%. LMEnt provides better coverage for rare entities Fig. 6 shows the distribution over the total number of retrieved chunks per entity. We observe that LMEnt offers greater coverage for tail and torso entities compared to Canonical stringbased variants. This is especially significant since tail and torso entities together make up 99.7% of the total entities in the corpus (A.1, Tab. 2). While it appears that Expanded string-based variants retrieve more chunks, this is likely noise due to alias ambiguity. There is also noticeable difference in variancespecifically string-based methods show egregious retrieval failures, sometimes retrieving less than 10 documents for head entities and no documents for torso entities. LMEnt maintains high precision as the number of retrieved chunks increases We also analyze the precision of retrieved chunks at varying retrieval depths, k. For each method, we consider only the top-k retrieved chunks per entity, and randomly sample 100 chunks if 100 for feasibility. An LLM then judges the mention, and precision is computed as the fraction of \"Yes\" responses out of k. As shown in Fig. 7, LMEnt maintains high precision 97% as increases, while precision substantially declines for all other methods, reaching 27% and 84% for CI-SS and CS-SS, respectively. This indicates that as more chunks are retrieved, string-based methods introduce noise while LMEnt maintains high quality. Entity co-occurence better aligns with model performance than popularity The number of Wikipedia pageviews for subject entity, denoted as Subj Pop, has been the standard way to proxy real-world popularity and is known to correlate with models ability to recall facts about it Figure 8: Accuracy for LMEnt-1B-6E model on PopQA. Subj+Answer Chunks counts chunks that mention both the subject and answer entities of question. Answer Chunks and Subject Chunks count chunks that mention the answer and subject entities individually. Subj Pop and Answer Pop are pageview popularities from Mallen et al. (2023). Subj+Answer Chunks correlates best with model behavior. In Fig. 8, we compare (Mallen et al., 2023). model performance against several potential indicators. In PopQA, each question includes both subject entity and an answer entity. Leveraging the LMEnt index, which can retrieve chunks containing many co-occurring entities, we introduce new indicator: Subject-Answer Chunks. This is defined as the number of chunks in which the subject and answer entities co-occur. Intuitively, this co-occurence shows the strongest correlation with model performance. Interestingly, this trend is also consistent for models trained on pretraining data that extend beyond Wikipedia (A.3, Fig. 12)."
        },
        {
            "title": "6 Knowledge Acquisition in Pretraining",
            "content": "We demonstrate the utility of LMEnt for knowledge analysis in LMs, focusing on the question of when during training do models learn knowledge best and how this relates to fact frequency. Experimental setup To study this, we use the facts in PopQA and evaluate the LMEnt-1B model every 20K steps. Since entity mentions do not specify the relations linking entities in chunk, we define fact by (Subject, Answer) entity pair. fact is learned if the model correctly answers at least one question that links the subject to the answer. Between training steps (X, + 20K), we consider all the facts seen in the training interval and measure (a) fact frequency: the number of chunks in this interval where the subject and answer co-occurred (Subject+Answer Chunks), (b) % of learned facts: facts that were not learned at Figure 9: % of facts learned (top) and forgotten (bottom) by fact frequency between intermediate checkpoints of LMEnt-1B-6E. While fact frequency correlates with gains in knowledge, it is unclear why rates of both learning and forgetting increase with frequency. step and then learned at step + 20K, and (c) % of forgotten facts: facts that were learned at step and then not learned, i.e. forgotten, at step + 20K. Findings Fig. 9 shows that in the first 20K steps of training, the model acquires substantial amount of knowledge, and continues to learn and forget facts as training proceeds. Interestingly, the rates of both learning and forgetting increase with fact frequency, leading to higher net gains in knowledge (A.8, Fig. 15). The model seems to learn facts across all frequencies in intermediate steps. Moreover, knowledge acquisition is not correlated with learning rate, which decreases after 1K warmup steps, as acquisition is highest in the last training steps. Overall, this suggests that while knowledge acquisition is correlated with fact frequency, the interplay between internal mechanisms and training dynamics is yet to be fully understood."
        },
        {
            "title": "7 Related Work",
            "content": "Factual knowledge in language models Focusing on injecting factual knowledge into language models, several previous works combined learning from explicit knowledge graphs and textual data (Ahn et al., 2016; Yang et al., 2017; Logan et al., 2019; Peters et al., 2019; Xiong et al., 2020; Zouhar et al., 2021; Zhao et al., 2025, inter alia). Similar to us, Logan et al. (2019) released dataset, Linked WikiText-2, linking 2M words from Wikipedia to the Wikidata knowledge graph. While their focus is on improving language modeling performance by accessing the knowledge graph, our focus is on creating resource that allows to study the acquisition of knowledge purely from language modeling. Another body of related work focuses on evaluating how much factual knowledge LMs possess and how they acquire it during training (Petroni et al., 2019; Elazar et al., 2021; Jiang et al., 2020; Liu et al., 2021; Li et al., 2022; Chang et al., 2024; Allen-Zhu and Li, 2024; Kim et al., 2025, inter alia). Our work is particularly relevant to work on knowledge acquisition and will facilitate future work (see 8.1) on the training dynamics of knowledge acquisition. Annotating and searching training data Existing tools for studying the effect of training data on model behavior and representations use efficient string-based retrieval that relies on exact matches (Elazar et al., 2024; Liu et al., 2025) or n-gram matches (Liu et al., 2024a). LMEnt uses fine-grained entity annotations for retrieval, allowing more precise analysis of the role of pretraining data for factuality in language models (5.2). Open-source LM toolkits Similar to LMEnt, Pythia (Biderman et al., 2023) OLMo (Groeneveld et al., 2024a; OLMo et al., 2025), LLM360 (Liu et al., 2024b), SmolLM (Allal et al., 2025; Bakouch et al., 2025), and NVIDIA (Kuchaiev et al., 2019; NVIDIA, 2025) released suite of models as well as their training data and training framework to facilitate the study of scaling laws and training dynamics in relation to the pre-training data. LMEnt differs from these previous works in focusing particularly on providing toolkit to study questions related to (factual) knowledge in language models."
        },
        {
            "title": "8 Conclusion and Discussion",
            "content": "We introduce LMEnt, suite of 12 LMs, matching pretraining data annotated with entity mentions, and an entity-based retrieval index that facilitates studying the connection between knowledge acquisition and pretraining data. We show that LMEnt models are capable of knowledge recall tasks, and that our entity-based retrieval outperforms string-based methods on 66.3%80.4% of entities. Overall, LMEnt serves as flexible and extensible testbed for investigating broad set of questions regarding knowledge representations in LMs. In the following sections, we list future applications of LMEnt and limitations."
        },
        {
            "title": "8.1 Future Applications of LMEnt\nKnowledge plasticity and editing LMEnt can\nbe used to investigate the plasticity of knowledge\nin language models. That is, identifying steps dur-\ning pretraining when models are more receptive\nto acquiring new knowledge—following previous\nworks that show that pretrained models struggle\nto learn new facts (Lyle et al., 2022), or that ex-\ncessive pretraining can make models resistant to\nfine-tuning (Springer et al., 2025). Since LMEnt\npinpoints where entities appear during pretraining,\nit can be used to examine a model’s ability to in-\nternalize new facts during training.",
            "content": "Improving factuality of LMs LMEnt provides entity annotations for each chunk in the pretraining corpus. possible extension is using the annotated corpus to improve the models factuality by experimenting with different methods for data ordering, or using the annotations to edit the pretraining data, for example by replacing an implicit mention (pronouns, descriptors) with its explicit entity name. Effect of other data sources In this work, we train models on relatively small and knowledgerich corpus, which results in models that are capable in knowledge tasks, yet perform poorly on out-of-distribution tasks, such as commonsense reasoning (A.4). Modern LMs are trained on much larger corpora, often surpassing 10T tokens (Allal et al., 2025), derived from sources that are knowledge-poor, e.g. coding (Rozière et al., 2024; Hui et al., 2024), synthetic stories (Eldan and Li, 2023), and formal languages (Hu et al., 2025). One can easily extend the LMEnt pretraining dataset with such sources to analyze if the addition of these tokens improves factuality. Our approach can also be applied to other knowledgerich sources; while most sources are not equipped with hyperlinks, even relying just on entity linking is still effective (LMEnt -H -C, Fig. 5). interpretability The Mechanistic enhanced visibility that LMEnt provides into the training process facilitates controlled, yet natural, setup for studying the formation of latent knowledge representations and circuits in LMs."
        },
        {
            "title": "8.2 Limitations",
            "content": "Pretraining data, model size and architecture LMEnt presents relatively small pretraining corpus and collection of compute-efficient models. Such experimental settings have been successful in recent years, enabling academic labs to develop procedures that were later scaled effectively, e.g., (Rafailov et al., 2023). As mentioned in 8.1, an interesting future direction is to scale LMEnt annotations to other knowledge-rich or knowledgepoor sourcesthereby increasing the size of the pretraining corpus and facilitating explorations into how different types of data affect knowledge acquisition. Also, LMEnt can be easily extended to architectures beyond dense transformers like mixture-of-experts architectures (Dai et al., 2024; Muennighoff et al., 2025; Cai et al., 2025), which is already supported by the OLMo framework (Groeneveld et al., 2024b; Muennighoff et al., 2025). Beyond pretraining Since models see the vast majority of tokens during pretraining, this stage is critical for understanding how they acquire and represent knowledge. While our study focuses on pretraining, it is typically followed by midand post-training phases. In these stages, models are presented with additional high-quality next-token prediction data and fine-tuning examples (OLMo et al., 2025; Kumar et al., 2025). Future work could extend LMEnt by adding annotations to the data used for midand post-training, and further train LMEnt models on this data. Dense Retrievers In this work, we do not compare LMEnt retrieval against dense retrievers such as DPR (Karpukhin et al., 2020) and ColBERT (Khattab and Zaharia, 2020), since sparse retrieval methods like WIMBD (Elazar et al., 2024) and Infinigram (Liu et al., 2024a) are the predominant approaches for retrieving relevant information in pretraining corpora."
        },
        {
            "title": "Acknowledgments",
            "content": "We are grateful to Maor Ivgi, Alon Mendelson, Yanai Elazar, and Ohav Barbi for their valuable discussions and feedback. We also thank Noam Steinmetz, Clara Suslik, and Asaf Avrahamy for their participation in the LM-judge evaluation. This work was supported in part by AMDs AI & HPC Fund, the Mila P2v5 grant, the MilaSamsung grant, the Google PhD Fellowship program, the Alon scholarship, and the Israel Science Foundation grant 1083/24."
        },
        {
            "title": "References",
            "content": "Sungjin Ahn, Heeyoul Choi, Tanel Pärnamaa et al. 2016. Neural Knowledge Language Model. Badr AlKhamissi, Millicent Li, Asli Celikyilmaz et al. 2022. review on language models as knowledge bases. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch et al. 2025. Smollm2: When smol goes big data-centric training of small language model. Zeyuan Allen-Zhu and Yuanzhi Li. 2024. Physics of language models: part 3.1, knowledge storage and extraction. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. Jacob Andreas. 2022. Language models as In Findings of the Association agent models. for Computational Linguistics: EMNLP 2022, pages 57695779, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Tom Ayoola, Shubhi Tyagi, Joseph Fisher et al. 2022. ReFinED: An efficient zero-shot-capable approach to end-to-end entity linking. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, pages 209220, Hybrid: Seattle, Washington + Online. Association for Computational Linguistics. Elie Bakouch, Loubna Ben Allal, Anton Lozhkov smol, multilinSmolLM3: https:// et al. 2025. gual, huggingface.co/blog/smollm3. long-context reasoner. Shay Banon. 2010. Elasticsearch: distributed, RESTful search and analytics engine. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony et al. 2023. Pythia: suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 23972430. PMLR. Weilin Cai, Juyong Jiang, Fan Wang et al. 2025. survey on mixture of experts in large language models. IEEE Transactions on Knowledge and Data Engineering, page 120. Nitay Calderon, Roi Reichart and Rotem Dror. 2025. The alternative annotator test for LLMas-a-judge: How to statistically justify replacing human annotators with LLMs. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1605116081, Vienna, Austria. Association for Computational Linguistics. Hoyeon Chang, Jinho Park, Seonghyeon Ye et al. 2024. How do large language models acquire factual knowledge during pretraining? In Advances in Neural Information Processing Systems, volume 37, pages 6062660668. Curran Associates, Inc. Roi Cohen, Eden Biran, Ori Yoran et al. 2024. Evaluating the ripple effects of knowledge editTransactions of ing in language models. the Association for Computational Linguistics, 12:283298. Gheorghe Comanici, Eric Bieber, Mike Schaekermann et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Damai Dai, Chengqi Deng, Chenggang Zhao et al. 2024. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12801297, Bangkok, Thailand. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee et al. 2019. BERT: Pre-training of deep bidirectional transformers for language understandIn Proceedings of the 2019 Conference ing. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Yanai Elazar, Akshita Bhagia, Ian Helgi Magnusson et al. 2024. Whats in my big data? In The Twelfth International Conference on Learning Representations. Yanai Elazar, Nora Kassner, Shauli Ravfogel et al. 2021. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:10121031. Ronen Eldan and Yuanzhi Li. 2023. Tinystories: How small can language models be and arXiv preprint still speak coherent english? arXiv:2305.07759. Mor Geva, Jasmijn Bastings, Katja Filippova et al. 2023. Dissecting recall of factual associations In Proin auto-regressive language models. ceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1221612235, Singapore. Association for Computational Linguistics. Abbas Ghaddar and Phillippe Langlais. 2016. WikiCoref: An English coreference-annotated corpus of Wikipedia articles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pages 136142, Portorož, Slovenia. European Language Resources Association (ELRA). Daniela Gottesman and Mor Geva. 2024. Estimating knowledge in large language models withIn Proceedings out generating single token. of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 3994 4019, Miami, Florida, USA. Association for Computational Linguistics. Dirk Groeneveld, Iz Beltagy, Evan Walsh et al. 2024a. OLMo: Accelerating the science of In Proceedings of the 62nd language models. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1578915809, Bangkok, Thailand. Association for Computational Linguistics. Dirk Groeneveld, Iz Beltagy, Evan Walsh et al. 2024b. OLMo: Accelerating the science of In Proceedings of the 62nd language models. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1578915809, Bangkok, Thailand. Association for Computational Linguistics. Jiawei Gu, Xuhui Jiang, Zhichao Shi et al. 2024. arXiv preprint survey on llm-as-a-judge. arXiv:2411.15594. Pengcheng He, Xiaodong Liu, Jianfeng Gao {DEBERTA}: {DECODING}- et al. 2021. {enhanced} {bert} {with} {disentangled} {atIn International Conference on tention}. Learning Representations. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch et al. 2022. Training compute-optimal In Proceedings of the large language models. 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Michael Y. Hu, Jackson Petty, Chuan Shi et al. 2025. Between circuits and Chomsky: Prepretraining on formal languages imparts linguistic biases. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 96919709, Vienna, Austria. Association for Computational Linguistics. Binyuan Hui, Jian Yang, Zeyu Cui et al. 2024. Qwen2.5-coder technical report. Zhengbao Jiang, Frank F. Xu, Jun Araki et al. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423438. Vladimir Karpukhin, Barlas Oguz, Sewon Min et al. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics. Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via conIn Protextualized late interaction over bert. ceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 20, page 3948, New York, NY, USA. Association for Computing Machinery. Jiyeon Kim, Hyunji Lee, Hyowon Cho et al. 2025. Knowledge entropy decay during language model pretraining hinders new knowlIn The Thirteenth Internaedge acquisition. tional Conference on Learning Representations. Knowledgator. 2025. Flashdeberta. https: //github.com/Knowledgator/ FlashDeBERTa. Accessed: 2025-08-01. Oleksii Kuchaiev, Jason Li, Daniil Shleifer et al. 2019. Nemo: toolkit for building ai applications using neural modules. arXiv preprint arXiv:1909.09577. Komal Kumar, Tajamul Ashraf, Omkar Thawakar et al. 2025. Llm post-training: deep dive into reasoning large language models. Patrick Lewis, Yuxiang Wu, Linqing Liu et al. 2021. PAQ: 65 million probably-asked questions and what you can do with them. Transactions of the Association for Computational Linguistics, 9:10981115. Belinda Z. Li, Maxwell Nye and Jacob Andreas. Implicit representations of meaning in 2021. In Proceedings of neural language models. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 18131827, Online. Association for Computational Linguistics. Li, Jiang, Huang et al. 2024. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arxiv. Shaobo Li, Xiaoguang Li, Lifeng Shang et al. 2022. How pre-trained language models capture factual knowledge? causal-inspired analIn Findings of the Association for Comysis. putational Linguistics: ACL 2022, pages 1720 1732, Dublin, Ireland. Association for Computational Linguistics. Jiacheng Liu, Taylor Blanton, Yanai Elazar et al. 2025. OLMoTrace: Tracing language model outputs back to trillions of training tokens. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 178 188, Vienna, Austria. Association for Computational Linguistics. Jiacheng Liu, Sewon Min, Luke Zettlemoyer et al. 2024a. Infini-gram: Scaling unbounded n-gram In First language models to trillion tokens. Conference on Language Modeling. Yinhan Liu, Myle Ott, Naman Goyal et al. 2019. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Zeyu Liu, Yizhong Wang, Jungo Kasai et al. 2021. Probing across time: What does RoBERTa In Findings of the Associknow and when? ation for Computational Linguistics: EMNLP 2021, pages 820842, Punta Cana, Dominican Republic. Association for Computational Linguistics. Zhengzhong Liu, Aurick Qiao, Willie Neiswanger et al. 2024b. LLM360: Towards fully transparent open-source LLMs. In First Conference on Language Modeling. Robert Logan, Nelson F. Liu, Matthew E. Peters et al. 2019. Baracks wife hillary: Using knowledge graphs for fact-aware language modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 59625971, Florence, Italy. Association for Computational Linguistics. Clare Lyle, Mark Rowland and Will Dabney. 2022. Understanding and preventing capacity loss in In International Conreinforcement learning. ference on Learning Representations. Alex Mallen, Akari Asai, Victor Zhong et al. 2023. When not to trust language models: Investigating effectiveness of parametric and In Proceedings of non-parametric memories. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, Toronto, Canada. Association for Computational Linguistics. Giuliano Martinelli, Edoardo Barba and Roberto Navigli. 2024. Maverick: Efficient and accurate coreference resolution defying recent trends. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1338013394, Bangkok, Thailand. Association for Computational Linguistics. Kevin Meng, David Bau, Alex Andonian et al. 2022. Locating and editing factual associations In Advances in Neural Information in GPT. Processing Systems. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld et al. 2025. OLMoe: Open mixture-ofexperts language models. In The Thirteenth International Conference on Learning Representations. NVIDIA. 2025. Nvidia nemotron nano 2: An accurate and efficient hybrid mamba-transformer reasoning model. Team OLMo, et al. 2025. abs/2501.00656. Pete Walsh, Luca Soldaini CoRR, 2 olmo 2 furious. Myle Ott, Sergey Edunov, Alexei Baevski et al. 2019. fairseq: fast, extensible toolkit for seIn Proceedings of NAACLquence modeling. HLT 2019: Demonstrations. Matthew E. Peters, Mark Neumann, Robert Logan et al. 2019. Knowledge enhanced contexIn Proceedings of tual word representations. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4354, Hong Kong, China. Association for Computational Linguistics. Fabio Petroni, Aleksandra Piktus, Angela Fan et al. 2021. KILT: benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 25232544, Online. Association for Computational Linguistics. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel et al. 2019. Language models as knowledge In Proceedings of the 2019 Conferbases? ence on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 24632473, Hong Kong, China. Association for Computational Linguistics. Hadi Pouransari, Chun-Liang Li, Jen-Hao Rick Chang et al. 2024. Dataset decomposition: Faster LLM training with variable sequence length curriculum. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. foundation language models. arXiv:2302.13971. arXiv preprint Hugo Touvron, Louis Martin, Kevin Stone Llama 2: Open foundation arXiv preprint et al. 2023b. and fine-tuned chat models. arXiv:2307.09288. Denny Vrandeˇcic and Markus Krötzsch. 2014. Wikidata: free collaborative knowledgebase. Commun. ACM, 57(10):7885. Wenhan Xiong, Jingfei Du, William Yang Pretrained encyclopedia: Wang et al. 2020. Weakly supervised knowledge-pretrained lanIn International Conference on guage model. Learning Representations. Sohee Yang, Elena Gribovskaya, Nora Kassner et al. 2024. Do large language models latently perform multi-hop reasoning? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1021010229, Bangkok, Thailand. Association for Computational Linguistics. Zichao Yang, Phil Blunsom, Chris Dyer et al. In 2017. Reference-aware language models. Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 18501859, Copenhagen, Denmark. Association for Computational Linguistics. Linxi Zhao, Sofian Zalouk, Christian K. Belardi et al. 2025. Pre-training large memory language models with internal and external knowledge. Vilém Zouhar, Marius Mosbach, Debanjali Biswas et al. 2021. Artefact retrieval: Overview of NLP models with knowledge base access. In Workshop on Commonsense Reasoning and Knowledge Bases. Rafael Rafailov, Archit Sharma, Eric Mitchell et al. 2023. Direct preference optimization: Your language model is secretly reward In Thirty-seventh Conference on Neumodel. ral Information Processing Systems. Adam Roberts, Colin Raffel and Noam Shazeer. 2020. How much knowledge can you pack In into the parameters of language model? Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 54185426, Online. Nicholas Roberts, Niladri S. Chatterji, Sharan Narang et al. 2025. Compute optimal scaling of skills: Knowledge vs reasoning. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1329513316, Vienna, Austria. Association for Computational Linguistics. Stephen Robertson, Steve Walker, Susan Jones et al. 1995. et almbox. 1995. okapi at trec-3. Nist Special Publication Sp, 109:109. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle et al. 2024. Code llama: Open foundation models for code. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee et al. 2021. Simple entity-centric questions challenge dense retrievers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6138 6148, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Luca Soldaini, Rodney Kinney, Akshita Bhagia et al. 2024. Dolma: an open corpus of three trillion tokens for language model pretraining In Proceedings of the 62nd Annual research. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1572515788, Bangkok, Thailand. Association for Computational Linguistics. Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen et al. 2025. Overtrained language models are harder to fine-tune. In Cant Believe Its Not Better: Challenges in Applied Deep Learning. Hugo Touvron, Thibaut Lavril, Gautier Izacard Llama: Open and efficient et al. 2023a. Appendix: Supplementary Results A.1 Pretraining Data Statistics Tab. 2 summarize statistics over the LMEnt pretraining corpus. These results support the overview provided in 4. 3.6B # tokens 10.5M # chunks 7.3M # entities 109K # batches (steps) 400M # total mentions # hyperlink mentions (H) 115M # entities = 1 993K # entities 1 < 10 2.2M # entities 10 < 100 967K # entities 100 < 1K 141K # entities > 1K 13K 203M # entity linking mentions # of coreference mentions 151M # of coreference cluster mentions 310M Table 2: Statistics of the LMEnt pretraining corpus. A.2 PopQA and PAQ Performance In this section we present additional results for PopQA and PAQ accuracies as functions of compute budget. Fig. 10 shows the results for all entities and moderately frequently co-occuring subject and answer entities in PopQA, and Fig. 11 shows the results for all entities and frequently co-occuring subject and answer entities in PAQ. These results support 5.1. Figure 10: Accuracy on PopQA as function of compute budget: (left) all entities and (right) questions for which the subject and answer entities appear together in 100-1K chunks. A.3 Popularity and Fact Frequency Indicators for Model Behavior on Additional Models In this section, we present additional results for PopQA accuracies sliced by popularity and fact frequency indicators. Fig. 10 shows the results for OLMo-2-1B (OLMo et al., 2025), Pythia-1.4B (Biderman et al., 2023), and SmolLM-2-1.7B (Allal et al., 2025). These results support 5.2. A.4 Results on Additional Tasks We report results on non-knowledge tasks in Tab. 3, which complement 5.1. A.5 Empirical Experiment for Choosing LMEnt Score Thresholds In this section, we described the empirical experiment that determined the score thresholds for LMEnt retrieval (5.2). To determine thresholds for the hyperlink score H, entity-linking score EL, and coreference scores and CC, we sample 60 entities from PopQA based on hyperlinks counts. For each Figure 11: Accuracy on PAQ as function of compute budget: (left) all entities and (right) questions for which the subject and answer entities appear together in 1K+ chunks. Figure 12: Accuracy of OLMo-2-1B (OLMo et al., 2025), Pythia-1.4B (Biderman et al., 2023), and SmolLM-21.7B (Allal et al., 2025) on PopQA, sliced by various indicators of popularity and fact frequency on PopQA. entity, we retrieve their scores using fixed hyperlink threshold of = 1, while varying EL, C, and CC across the set {0.4, 0.5, 0.6, 0.7, 0.8}, and evaluate precision over chunks retrieved at different depths k. If > 100, we randomly sample 100 of the returned chunks and submit them to Gemini 2.5 Flash Preview 6-17 using the prompt shown in Fig. 17 to compute precision at k. Results are reported in Tab. 4. A.6 Win Rates In Fig. 13, we extended the experiments in (5.2, Fig. 5) to compare win rates between the best string based method, CS-SS Canonical, and all other string-based variants (left), and well as CI-SS Canonical versus CI-SS Expanded (right). These results complement Fig. 5. Figure 13: Pairwise win rates between CS-SS Canonical and other string-based methods (left). Pairwise win rates between CI-SS Canonical and CI-SS Expanded (right). Table 3: Performance of LMEnt models and baseline models on commonsense reasoning, multiple choice, and reading comprehension tasks. Model truthfulqa winogrande naturalqs_open arc_challenge openbookqa hellaswag jeopardy arc_easy triviaqa gsm8k squad boolq copa coqa drop piqa sciq LMEnt-170M-1E LMEnt-170M-2E LMEnt-170M-4E LMEnt-170M-6E LMEnt-600M-1E LMEnt-600M-2E LMEnt-600M-4E LMEnt-600M-6E LMEnt-1B-1E LMEnt-1B-2E LMEnt-1B-4E LMEnt-1B-6E OLMo-1B OLMo-1B-20K-84B OLMo-2-1B Pythia-1.4B Pythia-1B Pythia-410M Pythia-160M SmolLM2-1.7B SmolLM2-360M SmolLM2-135M 0.217 0.247 0.254 0.271 0.271 0.301 0.254 0.274 0.254 0.244 0.278 0.264 0.338 0.288 0.435 0.348 0.331 0.301 0.311 0.468 0.448 0. 0.370 0.368 0.282 0.389 0.409 0.405 0.389 0.409 0.421 0.377 0.391 0.446 0.565 0.458 0.635 0.539 0.525 0.467 0.423 0.704 0.649 0. 0.574 0.620 0.367 0.400 0.604 0.620 0.565 0.625 0.603 0.626 0.545 0.611 0.617 0.640 0.646 0.582 0.623 0.588 0.498 0.733 0.620 0. 0.570 0.610 0.510 0.640 0.650 0.630 0.610 0.630 0.650 0.630 0.670 0.680 0.770 0.760 0.800 0.760 0.740 0.700 0.640 0.840 0.760 0. 0.072 0.081 0.000 0.082 0.085 0.113 0.150 0.167 0.109 0.132 0.160 0.157 0.574 0.269 0.694 0.566 0.519 0.428 0.218 0.720 0.596 0. 0.022 0.024 0.000 0.034 0.012 0.046 0.055 0.075 0.063 0.068 0.067 0.070 0.218 0.079 0.353 0.206 0.185 0.166 0.136 0.259 0.190 0. 0.020 0.025 0.000 0.025 0.025 0.055 0.015 0.050 0.035 0.045 0.010 0.035 0.030 0.000 0.395 0.020 0.025 0.040 0.020 0.315 0.045 0. 0.271 0.274 0.247 0.280 0.285 0.296 0.306 0.318 0.295 0.309 0.322 0.328 0.627 0.433 0.688 0.540 0.478 0.393 0.300 0.695 0.551 0. 0.006 0.006 0.000 0.006 0.008 0.009 0.034 0.023 0.009 0.020 0.021 0.043 0.342 0.059 0.641 0.273 0.195 0.069 0.019 0.706 0.516 0. 0.002 0.008 0.000 0.007 0.001 0.020 0.010 0.029 0.019 0.018 0.024 0.031 0.123 0.051 0.191 0.098 0.077 0.058 0.031 0.194 0.107 0. 0.304 0.320 0.264 0.356 0.340 0.350 0.338 0.374 0.358 0.360 0.374 0.390 0.434 0.406 0.474 0.434 0.378 0.360 0.324 0.496 0.444 0. 0.541 0.560 0.487 0.541 0.555 0.554 0.559 0.569 0.557 0.557 0.557 0.555 0.729 0.676 0.746 0.707 0.683 0.663 0.606 0.760 0.710 0. 0.633 0.697 0.237 0.711 0.722 0.744 0.735 0.762 0.714 0.765 0.763 0.770 0.879 0.790 0.953 0.874 0.890 0.833 0.746 0.940 0.905 0. 0.038 0.043 0.000 0.056 0.044 0.085 0.092 0.119 0.084 0.089 0.092 0.107 0.624 0.251 0.818 0.565 0.534 0.438 0.176 0.771 0.656 0. 0.000 0.002 0.000 0.004 0.001 0.005 0.016 0.045 0.006 0.047 0.008 0.025 0.324 0.085 0.512 0.250 0.188 0.104 0.042 0.545 0.305 0. 0.471 0.471 0.488 0.471 0.472 0.468 0.470 0.444 0.469 0.448 0.455 0.439 0.329 0.394 0.369 0.386 0.389 0.410 0.442 0.367 0.334 0. 0.493 0.505 0.488 0.501 0.490 0.516 0.515 0.510 0.506 0.523 0.517 0.529 0.594 0.531 0.654 0.566 0.533 0.532 0.502 0.671 0.591 0. Table 4: Empirical evaluation of LMEnt retrieval precision across multiple thresholds and retrieval depths on the development set of 60 PopQA entities."
        },
        {
            "title": "Score Thresholds",
            "content": "1 5 10 100 1K 10K 100K H=1, EL = = CC = 0.4 H=1, EL = = CC = 0.5 H=1, EL = = CC = 0.6 H=1, EL = = CC = 0.7 H=1, EL = = CC = 0. 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.96 0.96 0.96 0.96 0.96 0.96 0.96 0.96 0.98 0.98 0.98 0.98 0.98 0.99 0.99 0.99 0.99 0.97 0.98 0.99 0.98 0.98 0. 0.94 0.91 0.94 0.91 0.96 A.7 LMEnt wins across all entity popularity levels, often by large margins Fig. 14 shows Yes chunk count differences between methods and the cumulative percentage of entities where one method outperforms the other, across entities of different frequencies. For tail entities (right), LMEnt outperforms by few chunks, but this is meaningful given the scarcity of mentions. For torso entities middle), LMEnt wins by substantial margin ( 20 additional correct chunks) in 40% of cases. For head entities (left), the margin is typically smaller (125 chunks), suggesting that string-based search performs more competitively in these cases though, LMEnt still outperforms it on 60% of entities. Figure 14: Comparison between LMEnt retrieval and the best performing string-based approach (CS-SS Canonical). Results are grouped by popularity; tail entities (right, under 10 hyperlinks), torso entities (center, between 10 and 1K chunks) center, and head entities (left, at least 1K chunks). LMEnt outperforms string-based retrieval on all popularity levels, and often wins by large amount of better chunks for popular entities. A.8 Knowledge Acquisition during Pretraining Fig. 15 shows the net gains in facts learned between LMEnt-1B-6E checkpoints across frequency bins of subject and answer entity co-occurence. The split of % of facts learned and forgotten is in 6, Fig. 9. Figure 15: Percentage of net gains in facts learned between intermediate checkpoints of LMEnt-1B-6E. We analyze these results in detail in 6. Appendix: Additional Implementation Details B.1 Running Maverick At Scale The backbone of Maverick (Martinelli et al., 2024) is DeBERTa (He et al., 2021) which has quadratic attention complexity with respect to sequence length. Therefore, running Maverick on long documents is impractical, e.g. 15K token long documents take 40-50GB VRAM. Therefore, we reduce the memory footprint, and accelerate and parallelize inference across multiple documents by replacing the DeBERTa backbone with FlashAttention-based implementation (Knowledgator, 2025). Then, we apply Maverick to non-overlapping windows of 50K tokens, which may result in multiple distinct coreference clusters for the same entity. In these cases, we rely on the coverage of hyperlinks and the entity linker (which runs on the entire document) to map each cluster to the target entitys QID. B.2 Coreference scoring In this section we detail how we compute the score derived from coreference resolution, and how we resolve ambiguity when multiple entities are associated with single coreference cluster (2.2). Coref (C) is designed to associate shortened references, not previously found by hyperlinks or entity linking, with their canonical names, e.g. the hospital with John R. Oishei Childrens Hospital. The coreference cluster of the hospital contains the hospital and John R. Oishei Childrens Hospital in Buffalo. There are two entities related to this cluster, with QIDs: Q93565992 and Q40435. We can reduce ambiguity by leveraging the shared substring of (H,h)ospital to promote the hospital being mapped to Q93565992 more strongly. We represent this promotion by score denoted by C, which we formally define below. Let the coreference cluster associated with be C, and define the sets of hyperlink and entity linking mentions that overlap with as CH and CEL, respectively. To calculate C(m, e), for each mention CH CEL, we compute the longest common substring (LCS) with m. Let sim(m, m) be the harmonic mean of the LCS overlap ratios between mention and m, defined as: sim(m, m) = (cid:40) 0.0 2ab a+b if = 0 or = 0 otherwise where = LCS(m, m) , = LCS(m, m) (1) We weight either the hyperlink or entity linking score of for by its textual similarity to m: support(m, m, e) = sim(m, m) (cid:40) H(m, e) EL(m, e) if CH if CEL (2) Finally, the score for mention and entity is given by the maximum support score over all other mentions C: C(m, e) = max mCHCEL support(m, m, e) (3) Coref-Cluster (CC) connects generic indirect mentions (that dont share textual similarity with the entity name) to their canonical entity names, e.g. their with Buffalo Bills. In this case, we compute distribution of scores over all the entities linked to some mention in the cluster. The score CC(e) is defined per entity per cluster, and is shared across all mentions C. For each mention and entity e, we look at the existing source scores and compute msupport score. We dont aggregate the existing source scores (e.g. compute an average), because we found that this disproportionately rewarded single mentions identified by multiple sources and outweighed multiple mentions found by single sources, leading to incorrect cluster to entity associations. The Csupport for entity in cluster is simply the sum over all msupport scores. msupport(m, e) = H(m, e) EL(m, e) C(m, e) if CH if CEL otherwise Csupport(e) = (cid:88) mC msupport(m, e) (4) To compute distribution of scores over all entities mentioned in the cluster, we compute softmax over CC(C, e) scores for the set of entities supported by at least one mention in C, denoted by EC. CC(C, e) = exp (cid:0)Csupport(e)(cid:1) (cid:80) eEC exp (cid:0)Csupport(e)(cid:1) (5) B.3 Error Analysis of LMEnt Annotations This section describes qualitative error analysis in which we sampled 112 mentions retrieved using LMEnt, and manually analyzed the annotation errors observed and their frequency. We saw three errors total (2.7%) that are displayed in Fig. 16. The errors were results of entity linking failures and one coreference resolution failure. Figure 16: Error analysis of LMEnt entity mentions. B.4 Pretrained Models Details of the LMEnt models are described here and support the overview in 4. The 170M, 600M, and 1B models use (layers, hidden dimension) configurations of (10, 768), (16, 1344), and (16, 2048), respectively. hyperparameter search was conducted over the following ranges: global batch size {16,384, 32,768, 65,536, 131,072, 262,144}, peak learning rate {3 104, 6 104, 8 104, 1.2 103, 3103, 5103}, and weight decay {0.005, 0.05, 0.1}. Hyperparameters were selected based on the minimal final training perplexity. All LMEnt models were trained using the AdamW optimizer with global batch size of 32,768, rank batch size of 8,192, peak learning rate of 5 103, weight decay of 0.05, and 1,000 warmup steps. B.5 LM Judge for Chunk Precision To automatically evaluate the retrieval quality of both LMEnt (our entity-based method) and string-based baselines (see 5.2), we use Gemini 2.5 Flash Preview 6-17. For each of the 1K entities in the test set, we apply LMEnt, its ablations, and string-based methods CS-SS-(Canonical, Expanded) and CI-SS-(Canonical, Expanded), to retrieve sets of relevant chunks. For retrieved set containing more than 100 chunks, we randomly sample 100 chunks from it for evaluation. To determine whether chunk mentions the entity, we prompt Gemini using the instruction shown in Fig. 17. To provide context, we include the entitys description from ReFinED (Ayoola et al., 2022), which consists of the Wikipedia article title and the first sentence in the article. For each chunk, we identify matching mentions of the entity to create short context window for Gemini. For LMEnt, mentions are selected based on QID and score thresholds. For string-based methods, we use the highlight block in the Elasticsearch (Banon, 2010) query to extract character spans that match the entity name. Around each mention, we select 130-character window to give Gemini some context of the mention in the chunk text. We evaluate up to three mentions per chunk. If Gemini returns Yes for any, the chunk is marked as mentioning the entity. If all are judged No, the chunk is considered not to mention it. For LMEnt, mentions are prioritized by weighted sum of their scores, and for string-based methods, mentions are evaluated in the order they appear in the document. Justifying use of LLM-as-a-Judge To support our use of an LLM-as-a-Judge for evaluating modelgenerated responses, we use the alternative annotator test introduced by Calderon et al. (2025). This test determines whether the LLMs performance is comparable or better than that of randomly chosen human annotator. In line with their methodology, we employed three human annotators (graduate students) and used dataset of 100 entity mentions randomly sampled from Yes and No judged chunks. For each mention, the annotators received the same inputs as the LLM judge: the entity name, entity description, and 130-character context window around the mention, and were asked to evaluate whether the chunk named the entity. Instructions are found in Fig. 18. Following Calderon et al. (2025), we set ϵ = 0.1, which yielded winning rate of ω = 1.00 with p-values of 0.001, 0.001, and 8.28e5 for all three annotators, indicating that the LLM can be relied on. B.6 Converting Queries to Cloze-Style Prompts Since LMEnt models are not instruction-tuned, we evaluate them on PAQ by converting each question into cloze-style prompt, where the expected answer is the next predicted phrase. To perform this transformation, we use Gemini 2.5 Flash Preview 6-17 with the prompt shown in Fig. 19. Figure 17: Prompt given to Gemini to automatically judge whether chunk mentions an entity directly. Figure 18: Instructions given to annotators for evaluating LLM-as-a-judge. Figure 19: Prompt given to Gemini 2.5 Flash Preview 6-17 to convert PAQ questions to cloze-style prompts."
        }
    ],
    "affiliations": [
        "McGill University",
        "Mila Quebec AI Institute",
        "Tel Aviv University"
    ]
}