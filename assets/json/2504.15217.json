{
    "paper_title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
    "authors": [
        "Yatong Bai",
        "Jonah Casebeer",
        "Somayeh Sojoudi",
        "Nicholas J. Bryan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a versatile framework for fine-tuning media generation models towards a desired outcome. Compared with traditional reinforcement learning with human feedback (RLHF) or pairwise preference approaches such as direct preference optimization (DPO), DRAGON is more flexible. It can optimize reward functions that evaluate either individual examples or distributions of them, making it compatible with a broad spectrum of instance-wise, instance-to-distribution, and distribution-to-distribution rewards. Leveraging this versatility, we construct novel reward functions by selecting an encoder and a set of reference examples to create an exemplar distribution. When cross-modality encoders such as CLAP are used, the reference examples may be of a different modality (e.g., text versus audio). Then, DRAGON gathers online and on-policy generations, scores them to construct a positive demonstration set and a negative set, and leverages the contrast between the two sets to maximize the reward. For evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20 different reward functions, including a custom music aesthetics model, CLAP score, Vendi diversity, and Frechet audio distance (FAD). We further compare instance-wise (per-song) and full-dataset FAD settings while ablating multiple FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an 81.45% average win rate. Moreover, reward functions based on exemplar sets indeed enhance generations and are comparable to model-based rewards. With an appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality win rate without training on human preference annotations. As such, DRAGON exhibits a new approach to designing and optimizing reward functions for improving human-perceived quality. Sound examples at https://ml-dragon.github.io/web."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 7 1 2 5 1 . 4 0 5 2 : r DRAGON: Distributional Rewards Optimize Diffusion Generative Models Yatong Bai University of California, Berkeley"
        },
        {
            "title": "Jonah Casebeer\nAdobe Research",
            "content": "Somayeh Sojoudi University of California, Berkeley Nicholas J. Bryan Adobe Research yatong_bai@berkeley.edu jonah.casebeer@ieee.org sojoudi@berkeley.edu njb@ieee.org"
        },
        {
            "title": "Abstract",
            "content": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), versatile framework for fine-tuning media generation models towards desired outcome. Compared with traditional reinforcement learning with human feedback (RLHF) or pairwise preference approaches such as direct preference optimization (DPO), DRAGON is more flexible. It can optimize reward functions that evaluate either individual examples or distributions of them, making it compatible with broad spectrum of instance-wise, instance-to-distribution, and distribution-to-distribution rewards. Leveraging this versatility, we construct novel reward functions by selecting an encoder and set of reference examples to create an exemplar distribution. When cross-modality encoders such as CLAP are used, the reference examples may be of different modality (e.g., text versus audio). Then, DRAGON gathers online and on-policy generations, scores them to construct positive demonstration set and negative set, and leverages the contrast between the two sets to maximize the reward. For evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20 different reward functions, including custom music aesthetics model, CLAP score, Vendi diversity, and Fréchet audio distance (FAD). We further compare instance-wise (per-song) and full-dataset FAD settings while ablating multiple FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an 81.45% average win rate. Moreover, reward functions based on exemplar sets indeed enhance generations and are comparable to model-based rewards. With an appropriate exemplar set, DRAGON achieves 60.95% human-voted music quality win rate without training on human preference annotations. As such, DRAGON exhibits new approach to designing and optimizing reward functions for improving human-perceived quality. Example generations can be found at https://ml-dragon.github.io/web."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in diffusion models have transformed content generation across media domains, establishing new standards for generating high-quality images, video, and audio (Rombach et al., 2022; Ho et al., 2022; Liu et al., 2023; Ghosal et al., 2023). While these models achieve impressive results through sophisticated training schemes, their optimization process typically focuses on metrics that may not align with downstream objectives or human preferences. This misalignment creates fundamental challenge: how can we effectively steer these models toward desired output distributions or optimize them for specific performance metrics? Work done as an intern at Adobe Research. 1 prominent approach to address this challenge has been fine-tuning using instance-level feedback. Methods such as reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Williams, 1992; Schulman et al., 2017) and related methods like Direct Preference Optimization (DPO) (Rafailov et al., 2023) and Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024) leverage pre-trained reward models or large-scale, pairwise preference data collected offline to guide the optimization process. While effective, these approaches face several key challenges. First, media like audio, music, and video are multi-modal and highly perceptual, making it not only hard and expensive to create reliable preference pairs (e.g. music (Cideron et al., 2024), in-the-wild audio (Liao et al., 2024)), but also challenging to adopt criteria-based reward signals, as recently popularized in language model training (Guo et al., 2025). Second, preference-based training methods are constrained by the implicit reward functions hidden in their training data, making it difficult to adapt these approaches to new objectives or target distributions without collecting new preference data. Third, these approaches are not fully connected to the generative model evaluation metrics that often measure distributional properties like Fréchet embedding distance, diversity, and coverage. To address these limitations, we introduce Distributional RewArds for Generative OptimizatioN (DRAGON), versatile framework for fine-tuning generative models towards desired outcome or target distribution. DRAGON offers an alternative to existing reinforcement learning (RL) methods or pair-wise preference approaches for optimizing broad spectrum of rewards, including instance-wise, instance-to-distribution, and distribution-to-distribution signals. As shown in Figure 1a, the key components of DRAGON are: 1) pre-trained embedding extractor and set of (possibly cross-modal) reference examples, 2) reward-based scoring mechanism that creates positive and negative sets of on-policy online generations, and 3) an optimization process that leverages the contrast between the positive and negative sets. To our knowledge, DRAGON is the first practical algorithm that reliably optimizes our entire taxonomy of reward functions for generative models. We believe the ability to handle distribution-to-distribution rewards is particularly valuable since we can directly optimize generation quality metrics. Such reward functions are ubiquitous in standard generative model evaluation metrics (Fréchet embedding distance (Heusel et al., 2017), KullbackLeibler (KL) divergence (Kullback & Leibler, 1951), Inception score (Salimans et al., 2016)). Moreover, leveraging DRAGONs unique versatility, we can construct new reward functions by simply collecting set of ground-truth examples without human preference, drastically reducing the effort to construct reward signals. We demonstrate DRAGONs effectiveness through comprehensive experiments with text-to-music diffusion transformers. Our evaluation incorporates multiple common music generation metrics: audio-text alignment (CLAP score) (Elizalde et al., 2023; Wu* et al., 2023), Fréchet audio distance (FAD) (Kilgour et al., 2019) evaluated across diverse reference sets and embedding models, Vendi score (Friedman & Dieng, 2023) for reference-free output diversity, and custom-built human preference model for reference-free aesthetics scoring, totaling 20 reward functions. As shown in Figure 1b, DRAGON consistently improves over baselines across these diverse reward functions, achieving an average of 81.45% target reward win rate while generalizing the improvement across evaluation metrics. Via human listening test, we show that DRAGON can achieve 61.2% win rate in human-perceived music quality without training on human preference data. The contributions of our work can be summarized as follows: We propose DRAGON, versatile online, on-policy reward optimization framework for content generation models. DRAGON can optimize non-differentiable reward functions that evaluate either individual generations or distribution of them. We propose new approach to construct reward functions by simply selecting an embedding extractor and set of examples to represent an exemplar distribution. We propose human aesthetics preference model for AI-generated music and find that DRAGON can leverage it to improve human-perceived music quality with small set of (1,676) human-rated clips. We analyze the relationship between FAD and human preference, and show that DRAGON can improve human-perceived music quality without human rating data by optimizing per-song or full-dataset FAD. We show that by leveraging cross-modal embedding spaces between text and music, DRAGON can improve music generation quality with text-only music descriptions without audio data. While our experimentation focuses on music diffusion models, our framework is not specific to modality or modeling technique and can be applied to image or video generation as well as auto-regressive models. 2 (a) Overall diagram of DRAGON, versatile on-policy learning framework for media generation models that can optimize various types of reward functions. (b) DRAGON significantly improves full suite of rewards. Each vertex of the plot considers reward metric and reports the win rate of the DRAGON model optimized for the metric."
        },
        {
            "title": "2 Background",
            "content": "2.1 Music Generation Music generation has seen significant advances via auto-regressive token-based models (Vaswani et al., 2017; Zeghidour et al., 2021; Agostinelli et al., 2023; Borsos et al., 2023; Copet et al., 2023) and (latent) diffusion or flow-matching models (Novack et al., 2024a; Huang et al., 2023; Evans et al., 2024b; Wu et al., 2024; Evans et al., 2024a). Among auto-regressive approaches, MusicLM (Agostinelli et al., 2023) and MusicGen (Copet et al., 2023) are notable, with the former extended with RL via MusicRL (Cideron et al., 2024). Latent diffusion models like Stable Audio (Evans et al., 2024b;a) additionally show high-quality results and potential for ultra-fast generation (Bai et al., 2024; Novack et al., 2024b; 2025). However, reward optimization for diffusion models has proven more challenging than for auto-regressive ones (Wallace et al., 2024). 2.2 Diffusion Models Diffusion models have emerged as powerful paradigm able to generate high-quality and diverse samples (Sohl-Dickstein et al., 2015; Ho et al., 2020) via iterative de-noising. Such models commonly consist of de-noising network fθ that inputs noisy input xt, diffusion time step t, and condition (e.g. text), and can either be discrete-time (Ho et al., 2020) or continuous-time score models (Song et al., 2021b; Karras et al., 2022). During training, given demonstrative example x0, we add Gaussian noise to form the noisy example xt, and train fθ to undo the noising. At inference time, generation begins with pure noise which is repeatedly de-noised. That is, fθ is applied repeatedly to gradually de-noise, forming trajectory along the time step and noise level. Latent diffusion models (Rombach et al., 2022), where the above diffusion process takes place in latent embedding space, have been leveraged across nearly all perceptual generation domains, including creating images (Rombach et al., 2022; Karras et al., 2022), video (Ho et al., 2022), speech (Popov et al., 2021; Eskimez et al., 2024), in-the-wild audio (Liu et al., 2023; Bai et al., 2024), and music (Huang et al., 2023; Forsgren & Martiros, 2022). Recent advances have shown that transformer-based architectures are advantageous, leading to diffusion transformers (DiT) (Peebles & Xie, 2023). 2.3 Reward Optimization for Diffusion Models Diffusion models present unique challenges for reward-based optimization due to their iterative de-noising process. Existing work tackled this by formulating the diffusion process as Markov Decision Process, with the reward signal assigned either exclusively to the final time step (Fan et al., 2024) or to all time steps (Black et al., 2024). More recent works explored the alternative of implicit reward optimization via learning from contrastive demonstrations. Diffusion-DPO (Wallace et al., 2024) and MaPO (Hong et al., 2024) optimize rewards defined with explicit binary preferences between paired samples (e.g., media contents 3 with the same captions). Diffusion-KTO (Li et al., 2024) offers flexibility by being compatible with unpaired collections of preferred and non-preferred samples. Concurrent to our work, TangoFlux (Hung et al., 2024) represents an application of CLAP score reward in diffusion modeling, proposing semi-on-policy approach for in-the-wild audio generation."
        },
        {
            "title": "2.4 Human Feedback Datasets and Aesthetics Models",
            "content": "Human-feedback datasets and aesthetics models play crucial role in guiding the optimization of generative models towards human preferences and reward models. Datasets such as SAC (Pressman et al., 2022), AVA (Murray et al., 2012), LAION-Aesthetics V2 (Schuhmann, 2022), Pick-a-Pic (Kirstain et al., 2023), and RichHF-18K (Liang et al., 2024) have been instrumental in improving the quality and alignment of generated images. These datasets are then used to train aesthetics models, which often map pre-extracted embeddings to preference score. For audio/music generation, such an approach has been investigated with MusicRL (music) and BATON (in-the-wild audio), but is still rare and limited. In Section 4.1 and Appendix B, we describe the construction of our own aesthetics dataset and reward model for music generation."
        },
        {
            "title": "3 Distributional Reward Optimization For Diffusion Models",
            "content": "We propose DRAGON, reward optimization framework for optimizing diffusion models with wide variety of reward signals as shown in Figure 1a. We consider distributional reward function rdist : that assigns reward value to distribution of generations, where is the set of all such distributions. We allow rdist to be non-differentiable (e.g., human preference). The outputs of our generative model fθ form distribution Dθ. When fθ is conditional generator, Dθ depends on the distribution of conditioning C, although we omit for notation simplicity. Our goal is to fine-tune pre-trained model via maxθ rdist(Dθ). (1) The formulation (1) includes the widely studied instance-level reward optimization tasks, such as RLHF. Specifically, we recover instance-level optimization when the distributional reward rdist(Dθ) is the expectation over an instance-level reward EXDθ [rinstance(X)]. However, our framework extends beyond this special case, which is significant because traditional RL methods such as policy gradients are limited to instance-level rewards, as they cannot distinguish between high and low reward generations without more granular feedback. To tackle such challenges and optimize rdist(Dθ), we construct positive demonstrative distribution D+ such that rdist(D+) > rdist(Dθ) and negative demonstrative distribution D. We then optimize model parameters θ to make Dθ imitate D+ and repel D. In the following sections, we address how to: 1) construct D+ and D, and 2) optimize θ to make Dθ imitate D+ and repel D. Please also find pseudocode in Appendix F. 3.1 On-Policy Construction of D+ and Existing work often assumes demonstrations D+ and to be provided in advance (offline and off-policy), with D+ known to achieve higher reward than (Rafailov et al., 2023; Ethayarajh et al., 2024; Majumder et al., 2024). For instance-level rewards, this can be achieved by splitting dataset into two halves at reward threshold, common RLHF approach. The limitations of off-policy learning necessitate shift toward on-policy learning for several key reasons. First, for non-instance-level rewards such as FAD, the split becomes less straightforward. Second, large-scale offline data, required for effective off-policy learning, may be unavailable in practice. Third, on-policy optimization disentangles reward from dataset, providing more flexibility in reward choice. Finally, on-policy learning has demonstrated superior effectiveness and robustness because it enables real-time feedback and reduces data-policy mismatch. In the context of generative modeling, Tajwar et al. (2024) showed on-policy data helps language models learn from negative examples, and Hung et al. (2024) showed that even partially on-policy data collection pipeline improves diffusion models. Hence, we focus on an online and on-policy approach, although offline data can be optionally incorporated into DRAGON with minimal algorithmic changes. To construct on-policy distributions D+ and D, we sample from Dθ (which updates throughout training) online and query the distributional reward rdist on the fly. Specifically, before each training step, we collect 4 Algorithm 1 Greedy algorithm for constructing D+ and to optimize distributional reward rdist. 1: Query fθ twice to get generations D1 = {x11, . . . , x1n} and D2 = {x21, . . . , x2n}.1 2: (D(0) 3: for = 0, 1, . . . , do 4: ) (D1, D2) if rdist(D1) > rdist(D2) else (D2, D1). + , D(0) and D(i) to form (i) ) > rdist(D + (i) + ) else (D (i) and (i) + , . (i) ). Swap the ith generation pair in D(i) + (D(i+1) ) (D(i) ) if rdist(D(i) 5: + + 6: end for 7: The final (D+, D) result is (D(n+1) , D(i+1) + , D(i) , D(n+1) + ). + , D(0) with one in D(0) two batches of observations from Dθ by running full model inference with fθ and denote them as D1 and D2. While the notations D1, D2, D+, and technically represent distributions of generations, for simplicity, we also use them to denote the sampled demonstration sets. For the instance-level reward special case, where rdist = EXDθ [rinstance(X)], the contrastive demonstration sets (D+, D) can be constructed by taking the better/worse halves of the union D1 D2. This split can be determined by protocols such as element-level pair-wise comparison (if D1 and D2 consist of paired examples) or comparison with the batch median reward. Optimizing general rewards like rdist that evaluate distributions of generations is more delicate as we need to disentangle each elements contribution. To this end, we propose Algorithm 1, greedy algorithm. We initialize the positive/negative demonstration sets (D(0) ) with the higher/lower reward batch between D1 and D2. Next, we iteratively improve D+ through swapping procedure. First, we tentatively swap (0) generation in D(0) improves the reward + over D(0) ). Otherwise, reject the swap and set + + , D(0) + , D(1) (D(1) ) for = 0, 1, . . .. When stopping conditions are met, we take the latest (D(i) ) as the final (D+, D) pair. Each step of Algorithm 1 is guaranteed to improve or maintain D+s reward. We maintain equal sizes for D1, D2, D+, and and define the stopping condition as one complete pass over D1 and D2. When the loss function (to be discussed in Section 3.2) requires paired demonstrations, we use the same conditioning but different random seeds to generate D1 and D2. Subsequent swaps are also performed with same-conditioning pairs, ensuring that elements in D+ and are paired to provide direct contrast. For multi-GPU training parallelization, we broadcast all generations to each GPU and then only swap the indices originally present on each GPU. As result, the initial sets (D(0) ) are identical across GPUs, but subsequent (D(i) ) may differ from different swapping indices. Even still, the copy of D+ per GPU is guaranteed to be as good as both D1 and D2. The pseudocode in Appendix includes this parallelization. to form updated sets and (0) + , D(1) ). Repeating these steps, we obtain (D(i) + , D(i) , then accept the swap and set (D(1) ) to (D(0) (0) + ) to (D . Then, if + , D(0) + , D(i) + , D(i) (0) + , (0) + 3.2 Learning From D+ And We now optimize the generator parameters θ to make Dθ attract D+ and repel D, mathematically formulating this goal as minimizing the KL divergence KL(D+Dθ) and maximizing KL(DDθ). Intuitively, this means encouraging Dθ to cover as much of D+ and as little of as possible. Note that KL(D+Dθ) = arg max log πθ(x0) dD+(x0) = arg max Ex0D+ [log πθ(x0)] , (2) θ θ arg min θ θ KL(DDθ) is equivalent to finding arg min where πθ() denotes the likelihood for the model to generate given example, and the integral is over the support of D+. Similarly, finding arg max [log πθ(x0)]. Hence, our goal is equivalent to maximizing the log-likelihood for the model to generate examples in D+ and minimizing that of D. For diffusion models, πθ() is implicit, making optimization more challenging compared to auto-regressive models. Despite this challenge, DRAGON can steer the likelihoods by leveraging specific loss functions, which include but are not limited to Diffusion-DPO loss (Wallace et al., 2024) and Diffusion-KTO loss (Li et al., 2024). Diffusion-DPO requires paired contrastive demonstrations, whereas Diffusion-KTO is more flexible and accepts unpaired ones. While Diffusion-DPO and Diffusion-KTO previously specialized in offline learning from large-scale human preference, they become components of an Ex0D+ θ 5 on-policy framework that optimizes arbitrary rewards when integrated into DRAGON. We provide mathematical details about these two loss functions in Appendix A.1 and empirically compare them in Section 5.2, where we also ablate between paired and unpaired demonstrations. In Appendix A.2, we discuss potential extensions beyond binary D+ and D, compatibilities with alternative loss functions like GRPO (Shao et al., 2024) and DPOK (Fan et al., 2024), and relationships to reward-weighted regression. In total, DRAGON follows the illustration in Figure 1a and offers multiple advantages over existing reward optimization methods. First, DRAGON extends to distributional rewards like rdist that are hard/unstable to differentiate (e.g. FAD with large network backbones) via learning from contrastive demonstrations. Second, DRAGON allows for cross-modal supervision by learning from distributions rather than exact point-wise matches. This flexibility allows us to use cross-modal exemplar embeddings to construct rewards, even when the reference and generation modalities have substantial structural differences. In our experiments, we show that DRAGON can leverage text embeddings to improve music generation using only textual descriptions."
        },
        {
            "title": "4 Reward Functions",
            "content": "4.1 Instance-Wise Reward Human Preference Dataset and Aesthetics Score Predictor One of the most popular reward optimization tasks for content generation is aligning with human feedback, where human preference is the reward. To this end, human ratings of AI-generated examples provide relevant in-distribution guidance, and are thus more effective than ratings of human-created contents. Although opensource human preference datasets of AI image generations exist (Kirstain et al., 2023; Schuhmann, 2022; Schuhmann & contributors, 2022; Murray et al., 2012; Pressman et al., 2022), such resources are extremely rare for music. To demonstrate DRAGONs ability to align music generations to human preferences, we collect human rating dataset of AI-generated music and build custom music aesthetics predictor model. This predictor serves dual purposes: reward model for DRAGON to optimize, and an evaluation metric for evaluating DRAGON models trained for other reward functions. Our human preference dataset, which we call Dynamo Music Aesthetics (DMA), consists of 800 prompts, 1,676 music pieces with various durations (total 15.97 hours), and 2,301 ratings from 63 raters on scale of 1-5. The 1-5 rating scale makes our human feedback more fine-grained than binary pairwise comparison datasets such as (Kirstain et al., 2023). Details about this dataset and its collection are reported in Appendix B.1. Our aesthetics predictor consists of pre-trained CLAP audio encoder and kernel regression prediction head, which we train on the DMA dataset. The textual prompt is not shown to the predictor. To determine model implementation details (e.g., music pre-processing, label normalization, CLAP embedding hop length) to optimize model performance on unseen data, we use train/validation dataset split to perform an ablation study, which is presented in Appendix B.2. With model generalization verified, we remove the train/validation split and use all data to train the final predictor. subjective test verified that generations with high predicted aesthetics scores indeed sound better than those with low scores, demonstrating more authentic instruments and better musicality. When the aesthetics score assigned by the predictor model is used as the reward function, DRAGON requires no additional music data. 4.2 Instance-to-Instance Reward CLAP Score We use CLAP score (Wu* et al., 2023), popular music evaluation metric (Cideron et al., 2024; Bai et al., 2024; Hung et al., 2024), to demonstrate DRAGONs capability to optimize instance-to-instance rewards. CLAP score is defined as the cosine similarity (clipped to be non-negative) between the CLAP embedding of single generated audio instance and single reference embedding. Leveraging CLAPs unified crossmodal audio-text embedding space, we use the CLAP text embedding of the matching textual prompt as the reference for each audio generation. Intuitively, higher CLAP score means higher quality and semantic similarity. When maximizing CLAP score, DRAGON only requires set of prompts and does not need any human-created music. When optimizing aesthetics score or CLAP score, both of which assign reward values to individual generations, D+ and are constructed via pair-wise comparison. We find that DRAGON can improve overall generation quality by optimizing CLAP score."
        },
        {
            "title": "4.3 Distribution-to-Distribution Reward – Full-Dataset FAD",
            "content": "We use the Fréchet audio distance (FAD) to demonstrate how we can accommodate reward signals that compare distributions or sets of generation outputs (audio) to corresponding target distributions (audio or text). FAD (lower is better) is one of the most commonly used metrics for evaluating music generation models (Kilgour et al., 2019). Intuitively, FAD represents the difference between generated music distribution and reference distribution (often human-created music) in an embedding space. Suppose that µθ and µref Rd are respectively the means of the embeddings associated with generated and reference examples. Similarly, let Σθ, Σref Sd + denote the covariance matrices of the two distributions. FAD is computed as FAD(cid:0)µθ, Σθ, µref, Σref (cid:1) := (cid:13) (cid:13)µθ µref (cid:13) 2 (cid:13) + Trace (cid:16) Σθ + Σref + (cid:0)Σ 1 2 θ ΣrefΣ 1 2 θ 2 (cid:17) (cid:1) 1 . (3) To minimize dataset FAD to match reference distribution, we start by approximating the true generation distribution Dθ with all generations in training batch across all GPUs. Since full-dataset FAD assigns reward value to set of generations and does not rate each individual, D+ and must be determined via Algorithm 1. I.e., the positive demonstrative set D+ is constructed with Algorithm 1 to have minimal dataset FAD. When multi-modal embedding spaces are used, the reference distribution can be in any supported modality. For example, when CLAP is used as the encoder, the reference can be either audio or text. When an audio embedding distribution is used as reference, DRAGON only requires the distributions mean and covariance. When the reference is text embedding distribution, no audio data is needed for supervision. 4.4 Instance-to-Distribution Reward Per-Song FAD In addition to using FAD for distribution-to-distribution rewards, we also use FAD in an instance-todistribution setting. While FAD is typically used to compare two distributions, it can also compare single generation instance to distribution by bootstrapping the instance. For music, we can split generated waveform into shorter chunks and encode each chunk, forming per-song embedding distribution (Gui et al., 2024). We can then use (3) to compute the FAD between this single generation and the reference statistics. The reference statistics need not be per song and are computed using the entire reference dataset, and can again be in non-audio modalities if supported by the embedding space. Since per-song FAD is assigned to each example, DRAGON constructs the demonstration sets (D+, D) via element-wise comparison (same as aesthetics and CLAP score optimization). In the literature, per-song FAD has been used to predict audio quality and identify dataset outliers (Gui et al., 2024). We show that with DRAGON, music generation models can improve generation quality by directly minimizing per-song FAD. 4.5 Reference-Free Distributional Reward Embedding Diversity (Vendi Score) The Vendi score, introduced in (Friedman & Dieng, 2023), is diversity metric, for which larger value means more diverse. Intuitively, Vendi score of means that the diversity of set of embeddings is similar to that of completely dissimilar vectors. To compute the Vendi score of given embeddings with dimension represented as matrix Rnd, we first assemble an positive semi-definite kernel matrix K. We use linear kernel = ˆX ˆX , where ˆX is obtained by normalizing so that each embedding has an ℓ2 norm of 1. Next, we compute the eigenvalues of K, denoted as λ1, . . . , λn. The Vendi score is then the eigenvalues exponentiated entropy: During training, Vendi score is computed over generations in each training batch. We demonstrate directly improving Vendi with Algorithm 1, result only possible because DRAGON operates on distributions. Vendi(X) := exp (cid:0) Pn i=1 λi log λi (cid:1). (4)"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Models, Datasets, Training Settings, and Evaluation Metrics Baseline model and pre-training. We use the base diffusion model from Presto (Novack et al., 2025) to generate 32-second single-channel (mono) 44.1kHz audio. It includes latent-space score-prediction de7 noising module (Rombach et al., 2022; Karras et al., 2022; Song et al., 2021b) based on DiT-XL (Peebles & Xie, 2023) that takes in the noise level and text embedding as conditioning signals, convolutional variational autoencoder (VAE) that converts audio to and from the diffusion latent space (Kumar et al., 2023), and FLAN-T5-based text encoder (Chung et al., 2024a). The baseline model is pre-trained with diffusion loss to convergence on 3600-hour instrumental music dataset with musical-metadata-grounded synthetic captions, which we call the Adobe Licensed Instrumental Music dataset (ALIM). Our inference uses 40 diffusion steps with the second-order DPM sampler (Lu et al., 2022) with CFG++ (w = 0.8) enabled in selected time steps (Chung et al., 2024b). Please see Appendix for details. Training and evaluation prompts. We use ALIM training prompts (same setting as in pre-training) for DRAGON fine-tuning. Our evaluation uses combination of the captions in an independent ALIM test split (800 pieces), the captions in non-vocal Song Describer subset (Manco et al., 2023) (585 pieces, abbreviated as SDNV), and the real-world user prompts in the DMA dataset (800 pieces). Unless specified otherwise, all evaluation metrics are computed with generations from these 2,185 prompts (one generation per prompt). Evaluation initial noise. Diffusion models iteratively de-noise from random initializations, and hence their generations are highly dependent on the initial noise. For deterministic and fair comparison, we hash each test prompt into random seed, and sample the initial noise from this seed. As result, the initial noises are different across prompts but identical for all models. Evaluation metrics. Our evaluation metrics include the predicted aesthetics score, CLAP score, persong FAD, full-dataset FAD, and Vendi diversity score. Since these metrics vary in numerical range and directionality, we report the win rate over the baseline model to ensure comparability. For dataset FAD, we sample 40-example generation subsets with replacement (the subset indices are the same for all models) 1000 times, compute the dataset FAD for each subset, and report the win rate among these 1000 results. Correlation measures. In addition to evaluating and comparing DRAGON models, we offer correlation analyses between various reward signals. We quantify correlation with overall Pearson correlation (PLCC) and per-prompt Spearmans rank correlation (SRCC) in Appendix B.2. FAD encoders. The per-song and dataset FAD use ALIM and SDNV as reference statistics. DRAGON training uses ALIMs training split statistics to compute FAD, while evaluation uses the test split. We consider CLAP and the diffusion VAE as FAD encoders, with the former known to provide high-quality semantic embeddings (Gui et al., 2024). To account for any VAE reconstruction inaccuracies, we decode the generated VAE embeddings to audio and back. Compared with CLAP, the VAE encoder produces considerably more embeddings per song (1,836 versus 9) but in much lower dimension (32 versus 512). CLAP embedding setting. We use the LAION-CLAP checkpoint that specializes in music (Wu* et al., 2023; Chen et al., 2022). Since CLAP takes in 10-second 48kHz audio whereas our generations are 32-second 44.1kHz, pre/post-processing is required. We focus on two settings: MA and FADTK. Our MA setting is optimized for our music aesthetics predictor with ablation studies in Appendix B.2 while our FADTK setting follows (Gui et al., 2024). More details and comparison between the two settings are in Appendix C.2. Open-source comparison. We compare DRAGON with open-source models such as Stable Audio (Evans et al., 2024a) and MusicGen (Copet et al., 2023) in Appendix D. 5.2 Optimizing Instance-Level Rewards Predicted Aesthetics Score and CLAP Score We first use DRAGON to optimize instance-wise (reference-free) and instance-to-instance (reference-based) rewards using our aesthetics model and CLAP score, respectively. As shown in Figure 2, when optimizing the aesthetics score, DRAGON consistently achieves at least 80% reward win rate over the baseline, validating that our aesthetics model encodes learnable information, and DRAGON has strong optimization capability. When optimizing CLAP score, DRAGON achieves 60.1% CLAP score win rate and 68.7% aesthetics score win rate. This result shows CLAP score can act as surrogate when human ratings are unavailable, but directly optimizing human ratings when available is more effective. Such an observation aligns with our statistical analysis on CLAP score. Over the DMA dataset, CLAP score and human-provided aesthetics have 0.194 overall PLCC and 0.135 per-prompt SRCC, indicating positive but weak correlation. 8 DPO versus KTO; paired versus unpaired. Using the aesthetics score as the reward function, we perform ablation studies on different loss functions described in Section 3.2. Specifically, we focus on DPO loss with paired demonstrations, KTO loss with paired demonstrations, and KTO loss As with unpaired demonstrations. shown in Figure 2, DPO-Paired slightly outperforms KTO-Paired. DPO sees more stable training, whereas KTO improves the model faster. While KTO is more flexible by allowing for unpaired demonstrations, KTO-Paired outperforms KTO-Unpaired. Hence, direct pair-wise contrasting signals are advantageous, and we should prefer paired demonstrations when available. Figure 3: DRAGON with different demonstration diffusion steps and inference steps. Figure 2: DPO versus KTO loss function; paired versus unpaired demonstrations. Ablation on diffusion steps. Reducing the number of diffusion steps significantly accelerates trainingtime online generation at the cost of demonstration quality, but how does this affect the model fine-tuned with DRAGON? As shown in Figure 3, reducing the training-time diffusion steps from 40 to 10 (but still generating test examples with 40 steps) only induces tiny change in generation quality. If we also decrease the number of test-time inference steps to 10, then the model trained with 10-step demonstrations can even outperform the one trained with 40 steps. In all inference settings, both DRAGON models outperform the baseline, confirming the generalization of model improvement across diffusion step settings. Moreover, the DRAGON models have overall flatter quality-vs-steps curves, with their 10-step generations outperforming the baseline models 40-step generations. Hence, to achieve similar generation quality, DRAGON can significantly reduce inference-time computation, manifesting some properties of distilled diffusion models such as consistency models (Song et al., 2023; Bai et al., 2024; Novack et al., 2025). All subsequent experiments use the Diffusion-KTO loss function with paired 40-step demonstrations. Appendix B.3 presents additional analyses on aesthetics optimization. 5.3 Optimizing Instance-to-Distribution Reward Per-Song FAD We fine-tune our music generator to minimize per-song FAD. In Appendix C.1, we perform ablation studies to demonstrate the statistical correlation between per-song FAD and human aesthetic perception, providing theoretical foundation for improving human-perceived music quality via optimizing per-song FAD. Table 1a presents the model performance when optimizing per-song FAD with different reference statistics. We first consider using audio embeddings of human-created music as the FAD reference statistics. As shown in Table 1a, for CLAP and diffusion VAE encoders alike, when the reference is ALIM ground-truth embeddings, minimizing per-song FAD enhances the target reward as well as the aesthetics score. The VAE embeddings are particularly powerful optimizing the per-song VAE-FAD (ALIM) not only achieves 93.9% win rate in this metric, but also generalizes to multiple other metrics. Over all models, the improvement in the per-song FAD to ALIM statistics is highly correlated with the SDNV-FAD improvement. However, using SDNV statistics as the DRAGON optimization target is less effective, likely due to SDNVs less consistent quality and smaller size, as well as the mismatch between SDNV FAD reference and ALIM training prompts. We thus highlight the importance of using high-quality reference music and avoiding dataset mismatches. Overall, these results show DRAGON can enhance music generation without human feedback. Next, we leverage the cross-modality nature of the CLAP embedding space and use text embeddings as the FAD reference statistics for generated audio. Notably, by minimizing the per-song FAD to ALIM captions CLAP embeddings, DRAGON achieves all-around improvements across all metrics in Table 1a. Surprisingly, the cross-metric generalization even outperforms optimizing FAD with audio reference. While it seems counter-intuitive that text can be more helpful than music, this result is explainable. As shown in Appendix C.1, compared to per-song FAD to audio reference, the audio-to-text FAD with ALIM captions is 9 Table 1: DRAGONs win rates across reward functions. The reward win rate and reward before/after columns evaluate the reward function to optimize, which is different for each model. Aesthetics score, CLAP score, and FAD are reported for all models. FAD evaluation considers the diffusion VAE encoder and the CLAP audio encoder, using audio embeddings from the ALIM and SDNV datasets as the reference statistics. (a) Win rates of DRAGON models that optimize instance-wise or instance-to-distribution reward functions. Individual Metric Win Rates Reward Optimized Aesthetics CLAP-Score Per-Song VAE-FAD ALIM-Audio Per-Song VAE-FAD SDNV-Audio Per-Song CLAP-FAD ALIM-Audio Per-Song CLAP-FAD SDNV-Audio Per-Song CLAP-FAD ALIM-Text Per-Song CLAP-FAD SDNV-Text Per-Song CLAP-FAD Human-Text Per-Song CLAP-FAD Mixtral-Text Reward Reward Win Rate Before/After Aesthetics CLAP VAE Encoder Score CLAP Encoder Score ALIM SDNV ALIM SDNV Per-Song FAD 85.2% 60.1% 93.9% 66.4% 73.6% 56.3% 83.5% 70.1% 83.7% 70.1% .187/.638 .300/.317 30.8/16.3 31.1/28.4 .947/.867 .990/.973 1.58/1.48 1.56/1.53 1.60/1.54 1.53/1. 85.2% 68.7% 78.3% 51.3% 61.5% 49.1% 78.3% 49.7% 52.9% 65.5% 52.2% 60.1% 50.9% 49.0% 51.6% 46.0% 65.4% 55.7% 60.7% 52.2% 54.9% 64.9% 93.9% 63.5% 49.5% 35.7% 64.0% 57.6% 38.0% 52.8% 55.3% 61.4% 94.0% 66.4% 49.6% 33.1% 65.7% 57.1% 41.0% 52.2% 58.9% 65.2% 83.9% 48.3% 73.6% 54.0% 70.4% 63.8% 64.3% 60.3% 55.1% 54.2% 81.0% 42.8% 70.7% 56.3% 65.9% 58.2% 63.3% 60.5% (b) Win rates of DRAGON models that optimize reward functions like rdist that evaluate distributions. Individual Metric Win Rates Reward Optimized Reward Reward Win Rate Before/After Aesthetics CLAP VAE Encoder Score CLAP Encoder Score ALIM SDNV ALIM SDNV Dataset FAD Dataset VAE-FAD ALIM-Audio Dataset VAE-FAD SDNV-Audio Dataset CLAP-FAD ALIM-Audio Dataset CLAP-FAD SDNV-Audio Dataset CLAP-FAD ALIM-Text Dataset CLAP-FAD SDNV-Text Dataset CLAP-FAD Human-Text Dataset CLAP-FAD Mixtral-Text 70.5% 59.4% 73.6% 83.2% 85.4% 81.6% 98.4% 99.8% 8.26/7.58 8.30/8.05 .214/.207 .260/.251 .983/.967 .799/. .837/.813 .832/.786 51.4% 42.8% 58.3% 47.7% 68.8% 41.4% 57.4% 64.6% 49.7% 47.8% 45.7% 48.8% 59.5% 52.4% 55.8% 61.1% 70.5% 61.9% 61.5% 0.0% 88.2% 1.2% 26.5% 8.3% 58.7% 59.4% 50.2% 0.0% 84.7% 1.9% 38.9% 14.1% 1.0% 0.0% 73.5% 42.4% 2.1% 6.2% 26.2% 1.3% 1.5% 0.2% 29.9% 83.2% 0.9% 5.8% 14.1% 0.0% more strongly correlated with human preference. DRAGON can similarly optimize the per-song text-FAD to SDNV captions. Similar to the audio-reference case, when steering toward SDNV captions, while the reward win rate is high and the improvement generalizes to per-song audio-FAD, other metrics benefit less. Optimizing per-song text-FAD is similar to optimizing CLAP score in that both achieve aesthetic improvements with ALIM captions only, without any ground-truth music. In nearly all metrics, optimizing per-song text-FAD outperforms optimizing CLAP score. Hence, our novel instance-to-distribution paradigm is more In conclusion, DRAGON can align music effective than the traditional instance-to-instance method. generation with human preference using high-quality captions without human-created music. Given DRAGONs capability of learning from text-only data, we experiment whether ungrounded music descriptions not associated with any music collection can also improve music generation. We specifically consider two sets of prompts: 800 human-created prompts in the DMA dataset, and 870 prompts generated by the Mixtral-8x7B large language model (LLM) (Jiang et al., 2024). To ensure LLM-generated prompt quality, we gather 10,506 initial prompts, and then use greedy pruning algorithm similar to Algorithm 1 to minimize the dataset text-to-text CLAP-FAD with ALIM captions. As shown in Table 1a, when using human-created prompts, which are noisy and not based on actual music, as the exemplar set, DRAGON extracts learnable information via per-song FAD optimization, achieving an 83.7% reward win rate while improving CLAP score and per-song CLAP-audio-FAD. Similarly, the LLM-created prompts also encode learnable information and generalize even better to other metrics. Hence, we conclude that DRAGON can learn from ungrounded text-only music descriptions. 10 On average, DRAGON achieves an 81.4% win rate across all per-song FAD runs. Many runs (especially those using ALIM as the reference) improve the predicted aesthetics score, reaching an average aesthetics win rate of 59.9% without any human rating."
        },
        {
            "title": "5.4 Optimizing Distribution-to-Distribution\nReward – Full-Dataset FAD",
            "content": "Next, we leverage DRAGONs unique capability to learn from reward functions that evaluate distributions and minimize full-dataset FAD. We consider the same reference statistics as in the per-song FAD experiments in Section 5.3, and present the results in Table 1b. Optimizing dataset FAD is particularly challenging task due to reward ambiguity and strong learning starting point performance. Specifically, from reward like rdist that evaluates distribution is harder and noisier than learning from an instancelevel rinstance due to the ambiguity of each instances contribution to the reward. Meanwhile, because diffusion training implicitly matches the generated distribution to the true data, the baseline model already starts at decent FAD. Figure 4: Vendi score of models optimized for each reward type. Point height represents Vendi score and point size represents aesthetics win rate. Each per-song/dataset FAD point train with different reference statistic. Bar height averages point height. Despite these challenges, all but one dataset FAD optimization run improve the reward function they optimize as shown in Table 1b. As with per-song FAD optimization, using the ALIM dataset as the reference statistics achieves multi-metric improvements, generalizing across FAD to different references and improving aesthetics score. This enhancement is observed for all three encoder-modality combinations (VAE, CLAPaudio, CLAP-text), with the CLAP text embeddings attaining the best performance. Moreover, via dataset FAD optimization, ungrounded text embeddings (Human-Text and Mixtral-Text) can enhance music generation, especially in terms of aesthetics score and CLAP score. On average, dataset FAD optimization achieves reward win rate of 81.5%, matching the average improvement for per-song FAD optimization. We thus conclude that DRAGON can enhance diffusion models by directly minimizing dataset FAD. Compared to per-song FAD results, we observe several trends with dataset FAD optimization. First, VAE results are comparatively weaker and CLAP results are stronger. This makes sense because VAE embeddings are lower-dimensional, summarizing the entire generated distribution Dθ with µθ R32 and Σθ S32 , + totaling 1056 numbers. Hence, information is lost and optimization signals are weak. Second, cross-reference generalization is weaker. Improving per-song FAD to one reference statistic often means better per-song FAD to other references, correlation not observed with dataset FAD. Recall that the references are always fulldataset even when the generated statistics are per-song. Hence, per-song FAD improvement may partially come from per-song-versus-full-dataset gap shrinkage, which generalizes across reference statistics. This gap does not exist for dataset FAD, and hence imitating distribution may imply moving away from others. Third, optimizing dataset FAD to SDNV often hurts other metrics. We believe this is because all training prompts are from ALIM, and hence using SDNV reference statistics induces confusion. Specifically, if we run DRAGON with ALIM prompts and ALIM reference statistics, then when tested on SDNV prompts, the FAD with respect to SDNV statistics improves (see Figure 10). However, if DRAGON pairs ALIM prompts with SDNV statistics, then we do not observe this improvement. In summary, with dataset FAD optimization, it is important to select suitable encoder and reference statistic that matches the prompt distribution. Additionally, optimizing dataset FAD preserves more generation diversity than optimizing instance-level rewards like aesthetics, CLAP score, and per-song FAD. Figure 4 shows that optimizing per-song FAD worsens Vendi diversity score by an average of 17.7%, whereas optimizing dataset FAD only loses 3.2%. The next section will show that DRAGON can also explicitly optimize Vendi, balancing aesthetics and diversity."
        },
        {
            "title": "5.5 Optimizing Reference-Free Distributional Reward – Vendi Diversity Score",
            "content": "To demonstrate DRAGONs capability to improve generation diversity, we select the CLAP embedding Vendi score as the reward function. During training, we compute the Vendi score across all demonstrations in each training batch. During evaluation, we compute Vendi score with all 2,185 test generations. As shown in Figure 4, when explicitly optimizing Vendi, DRAGON significantly increases the score, achieving 40.84% relative improvement. However, since Vendi does not provide any music quality information, optimizing Vendi alone distorts the generations and hurts their aesthetics. To this end, we co-optimize Vendi score and aesthetics score by randomly selecting one of the two rewards at each training iteration with equal probability. The result is model that simultaneously improves Vendi and aesthetics, producing diverse high-quality music. In summary, we find DRAGON can promote generation diversity."
        },
        {
            "title": "5.6 Human Listening Test",
            "content": "We perform listening test for subjective evaluation using two DRAGON models from Table 1 one optimizing aesthetics score and the other optimizing per-song VAE-FAD to ALIM audio. We instruct 21 raters to compare the overall quality of blinded music pairs. Each rater is given 40 independently selected random SDNV prompts, along with the corresponding generation pairs. Out of each pair, one piece is from our baseline model and the other is from DRAGON model (20 pairs for each DRAGON model). Clips are loudness-normalized to 23dB LUFS, and presented in random order. To accelerate the test, clips are randomly cropped into 5-second snippets (same start/end timestamp per pair). Across all raters, both DRAGON models outperform the baseline, with DRAGON-aesthetics achieving 60.2% human-labeled win rate and DRAGON-VAE-FAD managing 61.0%. Despite DRAGON-aesthetics receiving higher machine-predicted aesthetics win rate (85.2%) than DRAGON-VAE-FAD (78.3%), its human-perceived quality is slightly worse. This is likely because DRAGON-aesthetics incurs some overfitting by directly maximizing the predicted aesthetics. In contrast, DRAGON-VAE-FAD learns in an instanceto-distribution approach, reducing overfitting by not relying on the DMA preference dataset. In summary, DRAGON improves human-perceived music quality with sparse human feedback (via our aesthetics model) and even with no human feedback (via per-song FAD). In Appendix B.4, we use statistical hypothesis testing to confirm our improvement and derive 95% confidence win rate lower bound."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented DRAGON, versatile reward optimization framework for content creation models that optimizes various generation quality metrics, along with novel reward design paradigm based on exemplar sets. DRAGON gathers online and on-policy generations, uses the reward signal to construct positive set and negative set of demonstrations, and leverages their contrast to improve the model. In addition to traditional reward functions that assign score to each individual generation, DRAGON can directly optimize metrics that assign single value to distribution of generations. Leveraging such flexibility, we constructed reward functions that match generations to reference exemplar set in instance-to-instance, instance-to-distribution, and distribution-to-distribution formats. We evaluated DRAGON by fine-tuning text-to-music diffusion models with 20 reward functions, including custom music aesthetics model trained on human preference, CLAP score, per-song FAD, full-dataset FAD, and Vendi diversity. Additionally, we provided ablation studies and analyzed the correlation between FAD and human preference. When optimizing the aesthetics score, DRAGONs win rate reaches up to 88.9%. When optimizing per-song/dataset FAD, DRAGON achieves an 81.4%/81.5% average win rate. By optimizing Vendi score, DRAGON improves generation diversity. Through listening tests, we show DRAGON improves human-perceived music quality at 60.9% win rate with sparse or no human preference annotations, without additional high-quality music. In total, DRAGON exhibits new approach to reward function optimization and offers promising alternative for human-preference fine-tuning that lessens human data acquisition needs."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Ge Zhu, Zhepei Wang, Juan-Pablo Caceres, Ding Li, and anonymous raters who participated in constructing the DMA human preference dataset and evaluating the DRAGON models."
        },
        {
            "title": "References",
            "content": "Andrea Agostinelli, Timo Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. MusicLM: generating music from text. arXiv preprint arXiv:2301.11325, 2023. Yatong Bai, Trung Dang, Dung Tran, Kazuhito Koishida, and Somayeh Sojoudi. ConsistencyTTA: accelerating diffusion-based text-to-audio generation with consistency distillation. In Interspeech, 2024. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In International Conference on Learning Representations (ICLR), 2024. Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. AudioLM: language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP), 2023. Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. HTS-AT: hierarchical token-semantic audio transformer for sound classification and detection. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Neural Information Processing Systems (NeurIPS), 30, 2017. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research (JMLR), 2024a. Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. CFG++: manifoldconstrained classifier free guidance for diffusion models. arXiv preprint arXiv:2406.08070, 2024b. Geoffrey Cideron, Sertan Girgin, Mauro Verzetti, Damien Vincent, Matej Kastelic, Zalán Borsos, Brian McWilliams, Victor Ungureanu, Olivier Bachem, Olivier Pietquin, Matthieu Geist, Leonard Hussenot, Neil Zeghidour, and Andrea Agostinelli. MusicRL: Aligning music generation to human preferences. In International Conference on Machine Learning (ICML), 2024. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez. Simple and controllable music generation. Neural Information Processing Systems (NeurIPS), 2023. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. International Conference on Learning Representations (ICLR), 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. CLAP: learning audio concepts from natural language supervision. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023. Sefik Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Hemin Yang, Zirun Zhu, Min Tang, Xu Tan, et al. E2 TTS: Embarrassingly easy fully non-autoregressive zero-shot tts. arXiv preprint arXiv:2406.18009, 2024. 13 Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. KTO: Model alignment as prospect theoretic optimization. In International Conference on Machine Learning (ICML), 2024. Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. arXiv preprint arXiv:2407.14358, 2024a. Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Long-form music generation with latent diffusion. In International Society on Music Information Retrieval (ISMIR), 2024b. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. Neural Information Processing Systems (NeurIPS), 2024. Seth Forsgren and Hayk Martiros. Riffusion - stable diffusion for real-time music generation. URL https://riffusion.com, 2022. Dan Friedman and Adji Bousso Dieng. The Vendi score: diversity evaluation metric for machine learning. Transactions on Machine Learning Research (TMLR), 2023. Alan Gelfand, Susan Hills, Amy Racine-Poon, and Adrian FM Smith. Illustration of bayesian inference in normal data models using gibbs sampling. Journal of the American Statistical Association, 1990. Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction guided latent diffusion model. In ACM International Conference on Multimedia, 2023. Azalea Gui, Hannes Gamper, Sebastian Braun, and Dimitra Emmanouilidou. Adapting Fréchet audio distance for generative music evaluation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, incentivizing reasoning capability in llms via reinforcement Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: learning. arXiv preprint arXiv:2501.12948, 2025. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs In Neural Information trained by two time-scale update rule converge to local nash equilibrium. Processing Systems (NeurIPS), 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Neural Information Processing Systems (NeurIPS), 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Neural Information Processing Systems (NeurIPS), 2022. Jiwoo Hong, Sayak Paul, Noah Lee, Kashif Rasul, James Thorne, and Jongheon Jeong. Margin-aware preference optimization for aligning diffusion models without reference. arXiv preprint arXiv:2406.06424, 2024. Qingqing Huang, Daniel Park, Tao Wang, Timo Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2Music: Text-conditioned music generation with diffusion models. arXiv preprint arXiv:2302.03917, 2023. Chia-Yu Hung, Navonil Majumder, Zhifeng Kong, Ambuj Mehrish, Rafael Valle, Bryan Catanzaro, and Soujanya Poria. TangoFlux: super fast and faithful text to audio generation with flow matching and clap-ranked preference optimization. arXiv preprint arXiv:2412.21037, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 14 Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Neural Information Processing Systems (NeurIPS), 2022. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fréchet audio distance: reference-free metric for evaluating music enhancement algorithms. In Interspeech, 2019. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-A-Pic: an open dataset of user preferences for text-to-image generation. Neural Information Processing Systems (NeurIPS), 2023. Solomon Kullback and Richard Leibler. On information and sufficiency. The annals of mathematical statistics, 1951. Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved RVQGAN. In Neural Information Processing Systems (NeurIPS), 2023. Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility. In Neural Information Processing Systems (NeurIPS), 2024. Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi PontTuset, Sarah Young, Feng Yang, et al. Rich human feedback for text-to-image generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Huan Liao, Haonan Han, Kai Yang, Tianjiao Du, Rui Yang, Zunnan Xu, Qinmei Xu, Jingquan Liu, Jiasheng Lu, and Xiu Li. BATON: aligning text-to-audio model with human preference feedback. arXiv preprint arXiv:2402.00744, 2024. Dennis Lindley and Adrian FM Smith. Bayes estimates for the linear model. Journal of the Royal Statistical Society Series B: Statistical Methodology, 1972. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. In International Conference on Machine Learning (ICML), 2023. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 364, 2019. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver: fast ODE solver for diffusion probabilistic model sampling in around 10 steps. Neural Information Processing Systems (NeurIPS), 2022. Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization. In ACM International Conference on Multimedia, 2024. Ilaria Manco, Benno Weck, Seungheon Doh, Minz Won, Yixiao Zhang, Dmitry Bogdanov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, et al. The Song Describer dataset: corpus of audio captions for music-and-language evaluation. In NeurIPS Workshop on Machine Learning for Audio, 2023. Naila Murray, Luca Marchesotti, and Florent Perronnin. AVA: large-scale database for aesthetic visual analysis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2012. Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. DITTO: Diffusion In International Conference on Machine Learning inference-time T-optimization for music generation. (ICML), 2024a. Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. DITTO-2: Distilled In International Society of Information diffusion inference-time T-optimization for music generation. Retrieval (ISMIR), 2024b. Zachary Novack, Ge Zhu, Jonah Casebeer, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. In International Conference on Presto! Distilling steps and layers for accelerating music generation. Learning Representations (ICLR), 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision (ICCV), 2023. Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In International Conference on Machine Learning (ICML), 2007. Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS: diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning (ICML), 2021. John David Pressman, Katherine Crowson, and Simulacra Captions Contributors. Simulacra aesthetic captions. Technical Report Version 1.0, Stability AI, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Neural Information Processing Systems (NeurIPS), 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Neural information processing systems (NeurIPS), 2016. Christoph Schuhmann. Explaining the aesthetic predictor: blog by LAION. https://laion.ai/blog/ laion-aesthetics, 2022. Accessed: 2024-12-25. Christoph Schuhmann and contributors. Improved aesthetic predictor. https://github.com/ christophschuhmann/improved-aesthetic-predictor, 2022. Accessed: 2024-12-25. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepSeekMath: pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations (ICLR), 2021a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations (ICLR), 2021b. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning (ICML), 2023. Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of LLMs should leverage suboptimal, on-policy data. In International Conference on Machine Learning (ICML), 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing Systems (NeurIPS), 2017. 16 Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992. Shih-Lun Wu, Chris Donahue, Shinji Watanabe, and Nicholas J. Bryan. Music ControlNet: multiple timevarying controls for music generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP), 2024. Yusong Wu*, Ke Chen*, Tianyu Zhang*, Yuchen Hui*, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Largescale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP), 2021."
        },
        {
            "title": "A Loss Functions for Learning from Demonstrations",
            "content": "A.1 Diffusion-DPO and Diffusion-KTO Loss Performing reward optimization for diffusion models can be more challenging than auto-regressive ones, because the likelihood πθ is implicit and not directly available. To this end, we leverage the Gaussian assumption of diffusion models and the structure of the Gaussian density function (a squared, scaled, and exponentiated ℓ2 norm) to approximate the log-likelihood log πθ with an ℓ2 distance term. Specifically, consider the forward diffusion process, where we inject Gaussian noise into demonstration x0 to form noisy example xt. That is, we randomly select diffusion time step from fixed distribution supported on [0, tmax]. Then, we sample xt from Gaussian distribution q(xtx0) = (xt; αtx0, σ2 I), where αt [0, 1] and σt represent the noise scheduling (Ho et al., 2020; Song et al., 2021a; Karras et al., 2022). Next, we query our model fθ to denoise from xt and obtain the result fθ(xt, t), where we omit the conditional information (prompt) in the notation for simplicity. Wallace et al. (2024) showed log πθ(x0) can be maximized via minimizing the surrogate objective function EtT ,xtq(xtx0) x0 fθ(xt, t)2 2 . (5) Similarly, log πθ(x0) can be minimized by maximizing this surrogate objective. Practical algorithms that optimize the above quantity often incorporate weighting and regularization terms to stabilize training (Yang et al., 2024; Wallace et al., 2024; Li et al., 2024; Hong et al., 2024). Among these algorithms, Diffusion-DPO and Diffusion-KTO are two of the most popular examples. Diffusion-DPO, which requires paired demonstrations, solves max θ (x+0,x0)(D+,D), tT x+tq(x+tx+0), xtq(xtx0) (cid:16) β(t) (cid:0)Aθ(x+0, x+t, t) Aθ(x0, xt, t)(cid:1)(cid:17)i , σ where Aθ(x0, xt, t) := (cid:13) 2 (cid:13) (cid:13)x0 fref(xt, t)(cid:13) 2 (cid:13) (cid:13)x0 fθ(xt, t)(cid:13) 2 2. (cid:13) (6) (7) Here, Aθ(x0, xt, t) represents how much fθs de-noising result is closer to the noiseless demonstration than frefs result, where fref is reference model used for regularization. In practice, the pre-trained base model before DRAGON fine-tuning is used as fref. Intuitively, (6) pulls the de-noising results from fθ towards corresponding demonstrations in D+ and pushes them away from examples in D. When unpaired data is more accessible, Diffusion-KTO can be used. Diffusion-KTO is qualitatively similar to Diffusion-DPO, but allows for decoupling positive and negative demonstrations. Specifically, it optimizes the following objective: max θ DrBernoulli(D+,D) tT , x0Dr, xtq(xtx0) (cid:16) σ β(t) sgn(Dr = D+) (cid:0)Aθ(x0, xt, t) Aθ (cid:1)(cid:17)i , (8) where the distribution Dr is randomly chosen between D+ and D, the binary variable sgn(Dr = D+) is 1 when Dr is D+ and 1 when Dr is D, the term Aθ(x0, xt, t) is defined in (7), and Aθ is regularization term. Specifically, Aθ is obtained by sampling an independent batch of x(1) 0 Dr, t(1), . . . , t(m) , and x(1) ) and computing the average Aθ() value via the formula q(xtx(1) ), . . . , q(xtx(m) 0 , . . . , x(m) , . . . , x(m) 0 Aθ := max (cid:18) 0, 1 X i=1 Aθ (cid:0)x(i) 0 , x(i) , t(i)(cid:1) (cid:19) . In practice, due to the independence between examples in training batch, each training batch itself can be used as surrogate to compute Aθ without additional explicit queries to the data loader. In Section 5.2, we present ablation studies between Diffusion-KTO and Diffusion-DPO and between paired and unpaired demonstrations. 18 A.2 Other Loss Functions and Beyond Binary D+ and Using DRAGON, we can incorporate other loss functions that learn from binary demonstrations without modification. Beyond Diffusion-DPO (Wallace et al., 2024) and Diffusion-KTO (Li et al., 2024), such loss functions include MaPO (Hong et al., 2024) and D3PO (Yang et al., 2024). DRAGON can also scale beyond binary demonstrations. For reward functions like rinstance that evaluate individual generations, this extension is straightforward. Loss functions for binary demonstrations generally decide the direction of optimization via the positive/negative nature of demonstration. That is, they explicitly or implicitly involve the sgn(Dr = D+) term introduced in the Diffusion-KTO loss (8). Since D+ and are formed via reward-based splitting operation, if we focus on particular example x0 sampled from the demonstrative distribution Dr, then sgn(Dr = D+) is equivalent to sgn(rinstance(x0) > rthreshold), where rthreshold is the splitting threshold. We can extend beyond the binary preference assumption by replacing the discontinuous sign function with continuous function, such as sigmoid or identity. One simple continuous-reward loss function is thus min θ EtT , x0Dθ, xtq(xtx0) β(t) (cid:0)rinstance(x0) rthreshold (cid:1) (cid:13) (cid:13)x0 fθ(xt, t)(cid:13) 2 (cid:13) , (9) which is equivalent to the DPOK objective with the KL-D setting (Fan et al., 2024), making it an example of reward-weighted regression algorithms (Peters & Schaal, 2007). That is, DPOK can be regarded as special case of DRAGON under the instance-level reward scenario with non-binary preferences. One can similarly continuize the Diffusion-KTO loss as max θ EtT , x0Dθ, xtq(xtx0) (cid:16) σ β(t) σ(cid:0)rinstance(x0) rthreshold (cid:1) (cid:0)Aθ(x0, xt, t) Aθ (cid:1)(cid:17)i . (10) Alternatively, one can consider other traditional RL loss functions for diffusion models like DDPO (Black et al., 2024) or derive variants of GRPO (Shao et al., 2024) specialized for diffusion models, For reward functions like rdist that evaluate distributions, we can similarly replace sgn(Dr = D+) with some continuous transformation of rdist(Dr). For example, the Diffusion-KTO loss can be continuized as max θ DrBernoulli(D+,D) tT , x0Dr, xtq(xtx0) (cid:16) σ β(t) σ(cid:0)rdist(Dr) rdist(D+)+rdist(D) 2 (cid:1) (cid:0)Aθ(x0, xt, t) Aθ (cid:1)(cid:17)i . (11) It is also possible to scale the number of demonstrative distributions beyond two by modifying Algorithm 1."
        },
        {
            "title": "B Details and Ablations for Human Aesthetics Preference Alignment",
            "content": "B.1 The DMA Music Preference Dataset The collection pipeline of our DMA preference dataset is two-phase process. In Phase 1, users interact with collection of music generation models via an interface. After receiving the user prompt, the interface generates piece, which the user rates on scale of 1-5. In Phase 2, we reuse the user prompts from Phase 1 and generate additional music pieces. We provide four examples per prompt, which the user again rates on scale of 1-5, providing direct contrastive signal. To enhance data diversity, Phase 2 also randomly mixes in some LLM-created prompts and ALIM training set captions. During DRAGON fine-tuning, generation quality is expected to improve. To help our aesthetics model generalize and mitigate the likelihood for DRAGON to quickly become out of distribution, we additionally mix in some high-quality human-created music. Specifically, for ALIM prompts used in Phase 2, we randomly mix in ground-truth ALIM music. Our final dataset consists of 800 prompts, 1,676 music pieces with various durations totaling 15.97 hours, and 2,301 ratings from 63 raters (multiple raters can rate the same generation). The proportion of each prompt source is shown in Table 2. Due to the small dataset size, we do not explicitly disentangle different aspects of music quality and text correspondence, and instead ask for single overall opinion rating. Despite our dataset being orders of magnitude smaller than comparable modern image aesthetics datasets (see Table 3 for detailed comparisons), we will show that by leveraging DRAGONs versatile and on-policy training pipeline, we can improve human-perceived generation quality with high data efficiency. 19 Table 2: The DMA datasets data sources, occurrences, and mean ratings of each source. Collection Phase Prompt Source Music Source Occurrences Mean Rating Phase-1 Phase-2 Phase-2 Phase-2 Phase-2 Total User prompts Generated Generated User prompts (reused) Training dataset captions Generated LLM-generated prompts Generated Training dataset captions Human-created 634 487 361 196 119 1,676 2.992 2.875 3.277 2.546 3.966 2.919 Table 3: Comparison of aesthetic datasets across different modalities, sources, and rating scales. Dataset Modality Size Content Source Rating Source Rating Scale SAC (Pressman et al., 2022) AVA (Murray et al., 2012) LAION-Aes V2 (Schuhmann, 2022) Pick-a-Pic (Kirstain et al., 2023) RichHF-18K (Liang et al., 2024) BATON (Liao et al., 2024) Audio-Alpaca (Majumder et al., 2024) MusicRL (Cideron et al., 2024) Ours Image Image Image Image Image In-the-wild Audio In-the-wild Audio Music Music >238,000 >250,000 1.2 Billion >1 Million 18,000 2,763 15,000 285,000 1,676 AI-generated Human-created Human-created AI-generated AI-generated AI-generated AI-generated AI-generated Mostly AI-generated Human-rated Human-rated Model-predicted Human-rated Human-rated Human-rated Model-predicted Human-rated Human-rated 1-10 score 1-10 score 1-10 score Paired binary 1-5 multi-facet Paired binary Paired binary Paired binary 1-5 score B.2 Aesthetics Predictor Details and Ablations Our aesthetics predictor consists of pre-trained CLAP audio encoder and kernel regression layer as the prediction head. The textual prompt is not shown to the predictor. To determine model implementation details (e.g., music pre-processing, label normalization, CLAP embedding hop length) to optimize model performance on unseen data, we use train/validation dataset split to perform an ablation study. With model generalization verified, we remove the train/validation split and use all data to train the final predictor. Finally, we perform subjective test, verifying that generations with high predicted aesthetics scores indeed sound better than those with low scores, demonstrating more authentic instruments and better musicality. To verify the performance of the aesthetics predictor and perform ablation studies, we split the DMA dataset into train/validation subsets by an 85/15 ratio. Specifically, we use overall PLCC and per-prompt SRCC to quantify the agreement between predicted aesthetics and human ratings on the validation split. PLCC is number between 1 and 1 that represents the normalized covariance (a larger value means more positive correlation and zero implies no correlation). SRCC is defined as the PLCC over rankings, and per-prompt means computing the SRCC among all generations from each prompt and averaging across all prompts. Per-prompt SRCC more accurately analyzes reward signals agreement when comparing generations from the same prompt. Since we learn from contrastive demonstrations D+ and evaluated with reward signals (often paired by prompt), per-prompt SRCC is particularly meaningful, as higher per-prompt SRCC makes the aesthetics predictor more reliable as reward function. Ablation 1: Embedding averaging. As mentioned in Section 5.1, there are numerous ways to pre-process our 32-second 44.1kHz generations into the 10-second 48kHz format required by CLAP. We start with the original CLAP inference setting introduced in (Wu* et al., 2023) and explore several modifications: 1. Average then rate (original CLAP inference). For each music waveform, we take three random 10second chunks an additional down-sampled chunk that captures global information, encode each chunk, average the four embeddings, and compute the aesthetics score with the average embedding. This setup is shared between training and inference for the aesthetics model. 2. Rate then average. During inference, we uniformly gather and encode four (partially overlapping) 10-second chunks, compute an aesthetics score from each embedding, and average the four scores. During training, we similarly extract up to four CLAP embeddings from each training music piece and train the aesthetics prediction head with the combined embedding dataset. 20 Figure 5: Ablation study on aesthetics model settings. Higher correlation with human ratings means better aesthetics model performance. Figure 6: Histograms of human-rated and predicted aesthetics score over the DMA dataset after global label normalization. 3. Setting 2 with down-sampled chunk. On top of Setting 2, add the down-sampled global chunk from Setting 1 during inference (overall five embeddings). The model itself is the same as Setting 2. 4. Setting 3 with 8+1 inference chunks. Same as Setting 3, but increase the number of ordinary (non-down-sampled) chunks from four to eight (total nine chunks) during inference. The model itself is the same as Settings 2 and 3. 5. Setting 3 with 16+1 inference chunks. Same as Setting 3, but increase the number of ordinary inference chunks to 16 (total 17 embeddings). The model itself is the same as Settings 2, 3, and 4. 6. Setting 4, but train with 8 embeddings. Setting 4 uses four embeddings per music piece to train the aesthetics prediction head but use 8+1 embeddings during inference. Setting 6 increases the number of training embeddings to eight per piece to match the inference setting (thereby also increasing the size of the kernel matrix in the prediction head). As shown in Figure 5, all settings obtain similar overall PLCC, with Setting 4 achieving the best per-prompt SRCC. Recall that the overall PLCC is computed across all music from all captions, entangling semantics and aesthetics, whereas per-prompt SRCC removes the influence of the semantic context by unifying the prompt. The low-SRCC settings tend to pay more attention to the semantics information dictated by the prompt and infer the aesthetics score via semantics-aesthetics correlation. In contrast, high-SRCC settings directly learn aesthetics information, making their outputs more closely aligned with human preference when ranking generations from the same prompt (which is the aesthetics models task in DRAGON). In conclusion, while the regression quality is similar among all settings, different training/inference settings make the models learn different information. Hence, when training regressive aesthetics models, we need to look beyond typical full-dataset metrics and evaluate in settings that mirror given use case. In our case, Setting 4 is the most reliable online music evaluation protocol. Figure 5 also shows that the rate-then-average paradigm produces noticeably better results than CLAPs original average-then-rate approach proposed in (Wu* et al., 2023). This makes sense because when averaged across all pieces in the DMA dataset, the within-piece standard deviation among Setting 4s nine raw aesthetics predictions is 0.238. For reference, the dataset-wide aesthetics prediction standard deviation is 0.755, meaning that the within-piece variance is non-trivial. That is, different chunks of the same music piece can vary noticeably in predicted aesthetics score, and the rate-then-average approach mitigates this variance via ensembling, resulting in better performance. Since Setting 4 achieves the best performance, we perform further ablation studies based on this setting. Ablation 2: Peak normalization. While Setting 4 approximates human preference, it may have difficulty generalizing on all generated musical styles. That is, it may be possible for the music generator to trick the aesthetics model into assigning higher rewards by simple operations that do not improve human-perceived aesthetics. While it can be hard to completely generalize, we make efforts to block straightforward ones, specifically focusing on loudness sensitivity. We consider peak normalization, i.e., normalizing the waveforms to be within specific range and focus on two normalization settings for the CLAP encoder: 21 4b. Setting 4 with [0.5, 0.5] peak normalization. Use the same training and inference setting as Setting 4, except we apply peak normalization to each 10-second chunk before encoding them, so that all waveforms are in [0.5, 0.5]. 4c. Setting 4 with [1, 1] peak normalization. Same as Setting 4b, except the peak normalization scales each 10-second chunk to [1, 1] instead of [0.5, 0.5]. As shown in Figure 5, Setting 4b ([0.5, 0.5]) achieves much higher per-prompt SRCC than Setting 4c ([1, 1]). While Setting 4bs per-prompt SRCC is slightly lower than Setting 4, we believe the magnitudeinvariance guarantee outweighs the minor performance drop, and thus keep the peak normalization. Note that the diffusion VAE encoder, which doubles as mapping to the latent diffusion space and an FAD reference embedding extractor, does not use peak normalization to preserve information for audio reconstruction. Ablation 3: Label normalization. We perform label normalization to transform our 1-5 raw human ratings into well-conditioned zero-mean distribution and study two settings: 1. Settings 4 and 4b Global label normalization. The conventional normalization approach, which shifts the ratings by their population mean and scales them by their population standard deviation. The underlying assumption is that all labels are independent and form standard Gaussian distribution. 2. Settings 4-G and 4b-G Per-rater normalization with Gibbs sampler. Drawing inspiration from Hierarchical Bayesian Model (Lindley & Smith, 1972), we assume that the rater harshness, quantified by raters average rating, forms standard Gaussian distribution. The ratings from each rater then form another Gaussian distribution centered around this average. To effectively estimate the average harshness and the rating variance of each rater, we utilize the Gibbs sampler (Gelfand et al., 1990). Via rater harshness modeling, we can perform more fine-grained per-rater normalization. As shown in Figure 5, per-rater label normalization does not always outperform conventional global normalization, with 4b-G better than 4b but Setting 4 superior to Setting 4-G. While per-rater normalization is more sophisticated, time-invariant Gaussian distribution may not accurately model each raters ratings due to small sample size and potential preference change over time. As result, global normalization, with the less precise overall Gaussian modeling, remains competitive. Our final selection is Setting 4b, which reliably generalizes to unseen data while being simple. On the validation split of the DMA dataset, Setting 4b achieves an overall PLCC of 0.576 and per-prompt SRCC of 0.484. For DRAGON training and evaluation, we remove the train/validation split and use Setting 4b to train new model with all available data. Figure 6 presents this new models prediction distribution over the DMA dataset. Comparison with other aesthetics models. With Setting 4b, the performance of our preference model is on par with existing preference models of other modalities trained with much larger data sizes. For example, Liang et al. (2024) trained sophisticated multi-branch image rating model that simultaneously predicts aesthetics score and several other metrics. With similar 1-5 scoring scale, this model is one of the most comparable works to ours. Trained on about 16,000 rated images, the aesthetics prediction in (Liang et al., 2024) achieves PLCC of 0.605. Despite our DMA preference dataset containing one order of magnitude fewer ratings, our model achieves similar validation set PLCC of 0.576. B.3 Additional Aesthetics Score Optimization (RLHF) Analyses As shown in Figure 7, when using DRAGON to optimize aesthetics score, the middle-difficulty prompts, i.e., prompts on which the baseline model achieves near-average aesthetics score, see the most significant improvement. This makes sense, because highly rated music pieces are comparatively rare in the DMA dataset. As result, as the aesthetics score improves throughout DRAGON training, the aesthetics predictor gradually becomes less reliable, and further improvements become more challenging. To understand this, notice in Figure 7d that after DRAGON training, the model generations aesthetics scores form cluster whose most mass falls in the proximity of 0.9. As shown in Figure 6, the aesthetics model rarely makes predictions greater than 1 on its training dataset, and scores greater than 1.4 are almost never assigned. That is, DRAGON approaches the upper limit of the aesthetics models useful output range, where the 22 (a) Aesthetics score before vs after DRAGON. (b) Aesthetics improvement vs baseline score. (c) Aesthetics improvement vs DRAGON score. (d) Aesthetics score histogram. Figure 7: When optimizing aesthetics score, DRAGON improves low to medium-quality examples the most. Table 4: Statistical analyses on human evaluation results. Model Measured WR 95% Conf WR Lower Bound p-Value P(WR > 50%) DRAGON-Aesthetics DRAGON-VAE-PSFAD-ALIM 60.24% 60.95% 56.15% 56.87% 1.58 105 4.15 106 99.9987% 99.9997% aesthetics score becomes less reliable. Hence, if the aesthetics model can be improved in future work, we are confident that DRAGON optimization can further enhance music generation. From human preference alignment perspective, earlier image-domain approaches leveraged sparse human feedback (Pressman et al., 2022) or feedback on non-AI-generated examples that induce distribution shift (Murray et al., 2012).2 These weaker human preference signals are then augmented by training aesthetics predictors (Schuhmann & contributors, 2022) to create larger-scale synthetic preference datasets (Schuhmann, 2022). In some sense, DRAGON aesthetics score optimization follows similar paradigm, but the creation of the synthetic dataset is now part of the learning algorithm, and the demonstrations are onpolicy. Hence, DRAGON sees less distribution shift and better utilizes sparse human preference. While more recent preference alignment methods leverage larger-scale human preference annotations for AI generation (Kirstain et al., 2023; Cideron et al., 2024), such data are highly expensive to collect, especially for the music modality where large-scale open-source preference datasets do not yet exist. We show that DRAGON addresses this challenge by learning from novel reward functions based on exemplar sets, achieving comparable results to directly learning from explicit human aesthetics ratings. B.4 Statistical Analyses on Human Listening Test Results Following the collection of the DMA dataset, our listening test collects opinions about overall music quality, without disentangling different aspects of quality and prompt adherence. Based on the binary ratings from our listening test, we perform statistical analyses to obtain the following information: Whether we can reject the null hypothesis of DRAGON does not improve upon the baseline. 95% confidence lower bound for DRAGONs win rate. The posterior probability for DRAGON to outperform the baseline model (i.e. > 50% win rate). Since we collect binary human preferences, we model the observed win rate as binomial distribution Binomial(n, w), where = 2120 = 420 is the total number of preference annotations, is DRAGONs underlying true win rate which we aim to estimate, and the random variable is the number of positive ratings out of the data points. To evaluate the null hypothesis about DRAGONs performance, we perform binomial test using our observed number of wins kD. We present the resulting p-values in Table 4. Intuitively, our results mean that if the true win rate were to be no greater than 50%, then the probabilities of obtaining our measured win rate would be 1.58 105 and 4.15 106 for the two DRAGON models. 2While SAC now has over 238,000 images, it only had 4,000-5,000 when used to train the LAION aesthetics predictor. 23 Figure 8: Correlation between per-song FAD with various reference statistics and aesthetics score. All numbers are negative because smaller is better for FAD whereas larger is better for aesthetics. Since these are extremely small numbers, we can reject our null hypothesis with high confidence, concluding that DRAGON indeed outperforms the baseline. To compute one-sided 95% confidence lower bound, we solve for the largest value such that the probability of observing our positive count PKBinomial(n,w)(K kD w), is no greater than 0.05. We consider the ClopperPearson confidence interval. Plugging in the binomial distribution mass function, we get i= (cid:0)n (cid:0)K kD PKBinomial(n,w) (cid:1)wi(1 w)ni 0.05. (cid:12) (cid:12) w(cid:1) = 1 PkD1 The closed-form solution is wlower-bound = Beta1(α, kD, kD + 1), where Beta1 is the inverse cumulative distribution function of the beta distribution. Plugging in our observed kD values for the two DRAGON in this test, we obtain the 95% confidence lower bound for shown in Table 4. Finally, we use Bayesian framework to analyze P(w > 0.5 kD), the posterior probability for DRAGONs true win rate to be greater than 50%. Using non-informative uniform prior Uniform(0, 1), we have the prior density function p(w) 1. Via Bayes Theorem, we have p(wkD) p(kDw) p(w). Substituting p(kDw) with the binomial mass function, we get p(wkD) pkD(1 p)nkD , meaning that the posterior distribution is Beta(kD + 1)(n kD + 1). We can then use the cumulative distribution function of the beta distribution to compute P(w > 0.5 kD). Table 4 shows that both DRAGON models have near-one posterior probability for > 0.5, and thus we say with high confidence that DRAGON outperforms its baseline. Note that the above analysis framework implicitly assumes that all binary preferences are independent. However, this is not strictly satisfied because the ratings are grouped by raters. To model the individual preference of each rater, we consider the alternative approach of treating each raters observed win rate as binomial variable. We then fit generalized linear model (GLM) with binomial family via logistic regression. Via the GLM approach, the 95% confidence lower bound for is 56.25% and 56.97% for the two DRAGON models, and the p-values are 1.55 105 and 4.25 106. These numbers are extremely close to the simple binomial modeling results shown in Table 4, meaning that while the independence assumption is not strictly satisfied, the effect of this inaccuracy is minuscule for our analyses."
        },
        {
            "title": "C FAD Details and Ablations",
            "content": "C.1 Per-Song FAD Correlation Analysis Per-song FAD is convenient statistical instance-to-distribution music quality metric. Since it is relatively new, its behavior has been less understood than more traditional metrics like human preference and dataset FAD. To motivate using per-song FAD as reward function, we analyze its correlation with 1) human-labeled aesthetics score, 2) model-predicted aesthetics score, and 3) full-dataset FAD. Correlation between aesthetics scores and per-song FAD. Similarly to how we evaluated the music aesthetics model, we use overall PLCC and the per-prompt SRCC to evaluate the agreement between persong FAD and aesthetics score over the DMA dataset. We include both predicted aesthetics scores and raw human ratings in this analysis. Intuitively, overall PLCC measures the across-dataset correlation, whereas 24 per-prompt SRCC reflects the degree to which ranking persong FAD agrees with ranking aesthetics score, which is particularly meaningful for DRAGON training. Table 5: Relationship between optimizing per-song and full-dataset FAD. The reference statistics come from the CLAP audio embeddings in the ALIM dataset. Generation Distribution Per-Song Dataset FAD () FAD () As shown in Figure 8, the per-song FAD that uses ALIM audio as the reference statistics is noticeably correlated with human preference. Hence, it is valid music quality measure, and we can expect optimizing it to enhance the model. Note that persong FADs correlation with predicted aesthetics score is systematically more prominent than per-song FADs correlation with human-provided aesthetics annotations, especially when per-song FAD uses the same CLAP embeddings in the aesthetics model. This observation suggests that the aesthetics model predictions are less noisy than human-predicted ones, but may bring bias. Surprisingly, using ALIMs textual music descriptions CLAP embeddings as the reference produces stronger correlations than using ALIM audio embeddings as the reference, implying that high-quality audio captions can offer rich and powerful insights into music quality. One explanation for text embeddings being more powerful than audio ones is that CLAPs text encoder, RoBERTa (Liu et al., 2019), was pretrained with datasets that covered much wider topics than CLAPs audio encoder HTS-AT (Chen et al., 2022). Per-song text-FADs high statistical correlation to human preference explains the high performance of the DRAGON model that optimizes this quantity (as shown in Table 1a), which enhances content creation without expensive human ratings based on text-only data. Dref DDRAGON-Aes D+persong D+dataset .106 .116 .092 .074 .947 .920 .858 . Connection between per-song and full-dataset FAD. To understand the connection between per-song and full-dataset FAD, we analyze whether constructing D+ that optimizes one quantity also improves the other. We take the test generations from the baseline model and the DRAGON model that optimizes music aesthetics with the DPO loss, denoted as Dref and DDRAGON-Aes respectively, resulting in 2185 generation pairs. From each pair, we select the sample with lower per-song FAD, and collect the 2185 chosen generations as D+persong. Meanwhile, we use Algorithm 1 to produce D+dataset, which also selects one piece from each of the 2185 pairs, but instead directly minimizes the full-dataset FAD. Table 5 presents the per-song and full-dataset FAD of Dref, DDRAGON-Aes, D+persong, and D+dataset. We observe that the dataset FAD of D+persong is lower than that of both Dref and DDRAGON-Aes, but higher than D+dataset. Conversely, the average per-song FAD of D+dataset is lower than both Dref and DDRAGON-Aes, but higher than D+persong. We can thus conclude that per-song FAD is correlated with full-dataset FAD, but if our goal is to minimize one of these two metrics, then directly optimizing that metric is more powerful than using the other as surrogate. This observation highlights the importance of DRAGONs unique capability to directly optimize reward functions that evaluate distributions, such as full-dataset FAD. C.2 MA Versus FADTK CLAP Embeddings for FAD Calculation As discussed in Section 5.1, we consider two settings to compute CLAP embeddings for FAD calculation (for both per-song and full-dataset variants): MA and FADTK. The MA setting was determined via the ablation studies in Appendix B.2. It up-samples the 32-second 44.1kHz generations to 48kHz and uniformly splits them into eight partially overlapping 10-second audio chunks. The MA setting additionally down-samples the entire 32-second waveform to match the CLAP input sequence length and uses it to represent global information. Before encoding each chunk, we perform peak normalization to range of [.5, .5], preventing reward hacking by merely manipulating the output magnitude. Eventually, the MA setting extracts nine 512-dimensional CLAP embeddings. The FADTK setting uses the same model checkpoint and up-sampling setting as MA. However, peak normalization is instead performed on the entire 32-second piece, which is subsequently divided into 10-second chunks with 1-second hop length. As result, FADTK produces more embeddings (25 instead of 9) per generation. The down-sampled chunk in the MA setting is not included. For aesthetics score and CLAP score calculations, we obtain score for each of the nine MA CLAP embeddings and average them. FAD (per-song and full-dataset) to text reference embeddings is also based on the MA embeddings due to their cross-modality nature shared with CLAP score. Meanwhile, since FAD (a) Between FADTK and MA embeddings of SDNV music. (b) Between different datasets and modalities. Figure 9: FAD heatmap between different CLAP reference statistics. to audio references is not cross-modal, we use the FADTK setting, which is reliable and standard in the literature. To compute Vendi score for batch of music, we average the nine MA CLAP embeddings for each piece, and compute the Vendi score following (4) with the per-song mean embeddings in this batch. To summarize, the differences between MA and FADTK are threefold: Number of chunks/hop size. The FADTK setting uses one-second hop size (resulting in 25 chunks for our 32-second generations) whereas the MA setting uniformly samples 8 chunks. Peak normalization. The MA setting performs peak normalization for each chunk, whereas FADTK performs peak normalization before splitting into chunks. Down-sampled global chunk. Inspired by the original CLAP inference setting, the MA setting includes down-sampled chunk to encode global information. The FADTK setting does not use this. In this section, we ablate the influence of these three differences by encoding human-created music with various encoding settings and computing the FAD between the embedding distributions resulting from different settings. These FAD numbers are listed in Figure 9a. On the open-source SDNV dataset, we gradually modify the MA setting, removing the three main differences one by one to get closer to the FADTK setting, and examine how the FAD between the modified setting and the FADTK setting changes. The FAD between the unmodified MA setting and FADTK is almost as large as the FAD between two different audio datasets (SDNV-versus-ALIM) at 0.0688. When we remove the down-sampled block from the MA setting, the FAD decreases to 0.0439 but is still quite large. If we further remove peak normalization from both settings (now the only difference is hop size), then the FAD becomes only 0.0146. Comparing these numbers, we conclude that all three differences between FADTK and MA contribute to their discrepancies, with peak normalization asserting the strongest influence and hop size mattering the least. In Figure 9b, we compare the SDNV and ALIM datasets CLAP embedding distributions, considering FADTK and MA music embeddings as well as caption text embeddings. In both FADTK and MA settings, the FAD between SDNV and ALIM audio embeddings is about 0.1. The text embedding FAD between the two datasets is much larger at 0.48. In terms of audio-text correspondence, SDNV music descriptions are statistically closer to music pieces in the CLAP embedding space, with an FAD of around 0.65. In comparison, ALIM captions see large FAD of nearly 1 from the corresponding audio data. For both ALIM and SDNV, MA audio embeddings are closer to the text embeddings. This observation supports using the MA setting when computing the FAD between generated audio and ground-truth text. 26 Figure 10: Comparing DRAGON and its pre-trained baseline model with open-source music generators. Next, we analyze how the differences between FADTK and MA affect their role as per-song FAD reference statistics for evaluating generated music. As shown in Figure 8, when ALIM or SDNV audio embeddings are used as reference (the setting to encode generated music matches with the reference setting for consistency), the per-song FAD computed with the MA setting is more strongly correlated with human preference. In contrast, the FADTK setting sees much lower PLCC and near-zero per-prompt SRCC with human ratings, suggesting that per-song FADTK makes decisions almost entirely based on semantic information, rather than aesthetics. If we remove peak normalization from FADTK, then we recover some correlation with human feedback, implying that per-song FADTK also attends to loudness. Gui et al. (2024) proposed to use per-song FADTK to predict music quality and identify outliers, and our result shows that such capabilities may be result of the correlation between aesthetics, loudness, and semantics. This makes sense, because Gui et al. (2024) showed that the quality ratings given by per-song FADTK (based on audio) and large language models (GPT-4, based on captions) strongly agree with each other. In summary, even with the same encoder, the embedding statistics can significantly vary depending on the specific encode settings. When computing FAD, it is important to select suitable setting and be consistent. Comparison with Open-Source Models This section compares DRAGON with open-source models, specifically considering the following: MusicGen (Copet et al., 2023), non-diffusion auto-regressive music generation model that predicts discrete audio tokens (Défossez et al., 2024) in sequence, coming in small, medium, and large variants. Stable Audio Open (Evans et al., 2024a), state-of-the-art audio diffusion model that generates variable durations of up to 45 seconds, following similar design to our base model. We use the non-vocal subset of Song Describer (SDNV) as the evaluation benchmark, and compare in aesthetics score, CLAP score (with text), per-song FAD, and dataset FAD metrics. Based on the results shown in Figure 10, we make the following observations: DRAGON and its pre-training baseline achieve higher aesthetics scores than Stable Audio and AudioGen. Since open-source models are out-of-distribution for our aesthetics model, there may be bias. That said, among open-source models, the aesthetics model generally assigns higher scores to models known to be more capable. In terms of CLAP score, our baseline model is between Stable Audio and AudioGen. When we use DRAGON to optimize CLAP score, while the training prompts are from ALIM, the improvement generalizes to SDNV. The per-song FAD of our baseline model is in the best cohort. By explicitly optimizing per-song FAD via DRAGON, we achieve the state-of-the-art among the tested models. Again, while the training prompts and the FAD reference audio are all from ALIM, the improvement generalizes to SDNV. Using DRAGON to optimize CLAP score or per-song FAD also improves full-dataset FAD, with the DRAGON model that optimizes per-song FAD achieving one of the best dataset FAD numbers. Surprisingly, smaller MusicGen models achieve lower dataset FAD than larger ones in the family. 27 In summary, our baseline model is at least on par with state-of-the-art open-source models, and DRAGON can steer music generations to improve various performance metrics."
        },
        {
            "title": "E Model Details and Hyperparameters",
            "content": "Convolutional VAE. We build on the Descript Audio Codec (DAC, or Improved RVQGAN) (Kumar et al., 2023) architecture and training scheme by using KL-bottleneck with dimension of 32 and an effective hop of 768 samples, resulting in an approximately 57Hz VAE. We train to convergence using the recommended mel-reconstruction loss and the least-squares GAN formulation with ℓ1 feature matching on multi-period and multi-band discriminators. Diffusion Transformer (DiT). Following the base model in (Novack et al., 2025), our model backbone builds upon DiT-XL (Peebles & Xie, 2023), with modifications aimed at optimizing computational efficiency. Specifically, it uses streamlined transformer block design, consisting of single attention layer followed by single feed-forward layer, similar to Llama (Dubey et al., 2024). The diffusion hyperparameter design follows EDM (Karras et al., 2022), with σdata = 0.5, Pmean = 0.4, Pstd = 1.0, σmax = 80, and σmin = 0.002. Also following EDM, we apply logarithmic transformation to the noise levels, followed by sinusoidal embeddings. These processed noise-level embeddings are then combined and integrated into the DiT block through an adaptive layer normalization block. For text conditioning, we concatenate the T5-embedded text tokens with audio tokens at each attention layer. As result, the audio token query attends to concatenated sequence of audio and text keys, enabling the model to jointly extract relevant information from both modalities. Pre-training of the baseline diffusion model lasted five days across 32 Nvidia A100 GPUs with total batch size of 256 and learning rate of 104 with Adam. DRAGON training settings and hyperparameters. All DRAGON fine-tuning is performed on top of the baseline model introduced above on four or eight A100 GPUs with total batch size of 80, with the baseline model used as the reference model fref required by the loss functions, as defined in the Aθ term in (7). During DRAGON fine-tuning, the textual conditions are from the pre-training dataset, and no ground-truth audio is used unless required by the reward function. We use Adam with fixed learning rate of 3 106 and gradient clip of 45 (determined via gradient logging). For the DPO loss (6) and the KTO loss (8), we select β = 5000 following (Wallace et al., 2024) and do not update fref. With paired DPO or KTO, the batch of 80 demonstrations is generated from 40 prompts in pairs, each pair consisting of one positive and one negative demonstration. With unpaired KTO, the 80 demonstrations are from 80 distinct prompts, and the positive/negative label is assigned by comparing each demonstration with the batch mean. All training runs (pre-training and DRAGON) use 10% condition dropout to enhance classifier-free guidance (CFG). The online audio demonstrations in DRAGON are generated with the default inference setting (40 diffusion steps with the second-order DPM sampler with CFG++ enabled in selected time steps). We use fθ to produce the demonstrations with probability 0.9 and use fref with probability 0.1. Intuitively, mixing in fref generations anchors the generation quality to the reference model and provides additional regularization."
        },
        {
            "title": "F Training Loop Pseudocode Walkthrough",
            "content": "We provide an algorithm walkthrough with pseudocode for the most relevant, novel aspects of our DRAGON fine-tuning framework. For simplicity, we consider paired demonstrations. We begin by defining our main training loop that inputs prompts (prompts), the model being fine-tuned (f_theta), reference model (f_ref), the reward function (reward_function), and an optional exemplar set (expemlar_set). 1 def train_step ( prompts , f_theta , f_ref , reward_function , exemplar_set ) : 2 3 4 6 7 8 9 # Generate on - policy diffusion latents for two batches of demonstrations with torch . no_grad : embd_1 , embd_2 = un _o _p li cy _ in er nc ( prompts ) # Decode demonstration latents to audio waveforms to query reward function audio_1 , audio_2 = vae_decode ( embd_1 , embd_2 ) 28 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 # Use reward function to construct D_ + and D_ - if ta ce _ vel _ wa rd : D_pos , D_neg = compare_r_ins tance ( audio_1 , audio_2 , reward_function , exemplar_set ) else : D_pos , D_neg = compare_r_dist ( audio_1 , audio_2 , reward_function , exemplar_set ) # Randomly sample noise levels . # DPO uses the same timesteps for each pair ; KTO samples . . . sigmas = s _ t u n _ _ d ( embds ) # Get x_t from x_0 ( the demonstrations ) by adding noise ( forward process ) embds_all = torch . stack ([ embd_1 , embd_2 ]) x_noisy = embds_all + sigmas * torch . randn_like ( embds ) # Compute de - noised values f_theta ( x_t , ) x_theta = f_theta . denoise ( x_noisy , sigmas , prompts ) # Use the reference model to get f_ref ( x_t , ) with torch . no_grad () : x_ref = f_ref . denoise ( x_noisy , sigmas , prompts ) # Calculate the KTO / DPO training loss return diffu si on_kto _l oss ( D_pos , D_neg , x_ref , x_theta ) Given the main train loop, we further add helper functions for comparing instance-level rewards like rinstance as well as distribution-level rewards like rdist. For reward functions that evaluate individual generations, we input two demonstration sets (audio_1 and audio_2), the reward function, and an optional exemplar set: 1 def co mpare_r_ insta nc ( audio_1 , audio_2 , reward_function , exemplar_set ) : 2 3 4 6 7 8 9 10 # Compute reward values rewards_1 = reward_function ( audio_1 , exemplar_set ) rewards_2 = reward_function ( audio_2 , exemplar_set ) # Element - wise swap for minimization D_pos = torch . where ( rewards_1 > rewards_2 , audio_1 , audio_2 ) D_neg = torch . where ( rewards_1 < rewards_2 , audio_1 , audio_2 ) return D_pos , D_neg For reward functions that evaluate distributions, we show piece of psuedocode with GPU parallelization logic. The function compare_r_dist handles the parallelization and calls optimize_r_dist which implements Algorithm 1. 1 def compare_r_dist ( 2 3 ) : 4 5 6 7 8 10 11 audio_1 , audio_2 , reward_function , exemplar_set , parallelize = True batch_size _p er _gpu = audio_1 . shape [0] # Gather all embeddings from all GPUs audio_1 , audio_2 = all_gather ( audio_1 ) , all_gather ( audio_2 ) audio_1 , audio_2 = audio_1 . flatten (0 , 1) , audio_2 . flatten (0 , 1) # audio_1 and audio_2 now have shape (# gpus * batch_size_per_gpu , wav_length ) # Get the indices of the current GPU embeddings 29 12 13 14 15 17 18 19 20 21 23 24 25 26 27 curr_gpu_idx = self . trainer . global_rank idx_curr_gpu = np . arange ( at ch _size_ pe r_gp ) + curr_gpu_idx * batch_s iz e_ pe r_ gp # GPU 0 should have 0 , ... , batch_size_per_gpu -1 # GPU 1 should have batch_size_per_gpu , ... , 2* batch_size_per_gpu -1 # etc . # If using parallelized optimization , only swap the indices of the current GPU # Otherwise , swap all indices ( more accurate but slower ) idx_to_swap = idx_curr_gpu if parallelize else np . arange ( audio_1 . shape [0]) # Optimize the dataset FAD D_pos , D_neg = optimize_r_dist ( audio_1 , audio_2 , reward_function , exemplar_set , idx_to_swap ) return D_pos , D_neg For Algorithm 1 (optimize_r_dist), we show an example implementation that optimizes full-dataset FAD as follows. Other rewards like Vendi can be handled similarly. 1 def optimize_r_dist ( audio_1 , audio_2 , fad_encoder , exemplar_set , idx_to_swap ) : 2 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 30 31 32 33 34 36 37 38 # Convert everything to an embedding space ref_embds = fad_encoder . encode ( exemplar_set ) embd_1 = fad_encoder . encode ( audio_1 ) embd_2 = fad_encoder . encode ( audio_2 ) # Get mean and covariance of reference embeddings ref_stats = get_mean_and_cov ( ref_embds ) # fad_from_embd computes mean and covariance of the generated embedding set # and computes FAD relative to the reference following Eq .(3) fad_1 = fad_from_embd ( embd_1 , ref_stats ) fad_2 = fad_from_embd ( embd_2 , ref_stats ) # Initialize positive and negative sets if fad_score_1 < fad_score_2 : # If D_1 is better , we initialize D_ + with D_1 and D_ - with D_2 D_pos , fad_pos = embds_1 , fad_score_1 D_neg , fad_neg = embds_2 , fad_score_2 else : # If D_2 is better , we initialize D_ + with D_2 and D_ - with D_1 D_pos , fad_pos = embds_2 , fad_score_2 D_neg , fad_neg = embds_1 , fad_score_1 # Iterative swapping procedure for idx in idx_to_swap : D_pos [ idx ] , D_neg [ idx ] = D_neg [ idx ] , D_pos [ idx ] # Calculate FAD for the pos / neg sets with the swapped pair new_fad_pos = fad_from_embd ( D_pos , ref_stats ) if new_fad_pos < fad_pos : fad_pos = new_fad_pos # If dataset FAD improved , accept the swap else : # If dataset FAD did not improve , revert the swap D_pos [ idx ] , D_neg [ idx ] = D_neg [ idx ] , D_pos [ idx ] return D_pos , D_neg 30 Table 6: All DRAGON models instance-level reward win rate. DRAGON Model Aesthetics CLAP CLAP-Audio Per-Song FAD CLAP-Text VAE-Audio Score Score ALIM SDNV ALIM SDNV Slackbot Mixtral ALIM SDNV Reference (40 inference steps) Reference (10 inference steps) KTO Aesthetics DPO Aesthetics (40/40 train/inference steps) DPO Aesthetics (40/10 train/inference steps) DPO Aesthetics (10/40 train/inference steps) DPO Aesthetics (10/10 train/inference steps) KTO-Unpaired Aesthetics KTO CLAP Score KTO Per-Song FAD VAE-ALIM-Audio KTO Per-Song FAD VAE-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Audio KTO Per-Song FAD CLAP-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Text KTO Per-Song FAD CLAP-SDNV-Text KTO Per-Song FAD CLAP-Slackbot-Text KTO Per-Song FAD CLAP-Mixtral-Text KTO Dataset FAD VAE-ALIM-Audio KTO Dataset FAD VAE-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Audio KTO Dataset FAD CLAP-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Text KTO Dataset FAD CLAP-SDNV-Text KTO Dataset FAD CLAP-Slackbot-Text KTO Dataset FAD CLAP-Mixtral-Text KTO Vendi Score KTO Aesthetics + Vendi Score 50.0% 13.9% 85.2% 88.9% 53.5% 88.2% 63.9% 80.0% 68.7% 78.3% 51.3% 59.1% 44.0% 78.3% 49.7% 52.9% 65.5% 51.4% 42.8% 58.3% 47.7% 68.8% 41.4% 57.4% 64.6% 21.1% 52.1%"
        },
        {
            "title": "G Example Spectrograms",
            "content": "50.0% 50.0% 50.0% 50.0% 50.0% 23.8% 48.4% 53.0% 43.9% 48.3% 52.2% 46.4% 38.9% 58.9% 55.1% 57.1% 59.8% 61.9% 66.8% 68.2% 34.2% 68.1% 77.2% 68.4% 75.7% 55.1% 55.8% 54.1% 64.1% 62.4% 35.7% 75.0% 79.6% 75.7% 78.2% 57.1% 48.2% 41.1% 60.8% 55.0% 60.1% 56.3% 46.5% 65.2% 54.2% 50.9% 84.0% 76.9% 83.9% 81.0% 49.0% 45.8% 39.5% 48.3% 42.8% 52.4% 80.5% 79.3% 78.3% 77.4% 41.1% 56.8% 61.3% 55.5% 58.0% 65.4% 58.8% 54.2% 70.4% 65.9% 55.7% 61.5% 54.6% 63.8% 58.2% 60.7% 59.8% 60.9% 64.3% 63.3% 52.2% 46.7% 48.3% 60.3% 60.5% 49.7% 42.2% 43.0% 50.4% 51.5% 47.8% 38.9% 41.9% 38.7% 40.9% 45.7% 60.6% 59.2% 57.3% 55.3% 48.8% 65.2% 71.4% 61.4% 65.4% 59.5% 33.0% 34.2% 42.7% 42.4% 52.4% 41.8% 41.8% 48.0% 46.9% 55.8% 42.0% 40.3% 39.9% 37.5% 61.1% 37.6% 33.3% 50.1% 43.3% 14.1% 2.7% 5.9% 43.4% 16.1% 14.7% 24.9% 36.8% 2.8% 6.3% 50.0% 43.0% 50.2% 61.8% 63.6% 69.2% 69.9% 69.4% 81.8% 75.9% 54.1% 70.8% 38.6% 83.5% 56.3% 76.0% 67.0% 60.0% 47.2% 49.1% 49.9% 57.2% 61.0% 50.9% 74.0% 5.5% 33.0% 50.0% 47.8% 54.3% 62.6% 68.4% 54.8% 66.5% 71.9% 50.0% 50.0% 50.5% 51.7% 54.9% 55.3% 51.2% 52.8% 56.6% 57.9% 46.2% 47.4% 50.5% 51.8% 58.7% 60.1% 75.7% 64.9% 61.4% 70.1% 59.2% 74.4% 54.0% 82.8% 70.1% 79.8% 74.8% 59.3% 52.7% 54.5% 57.4% 57.4% 63.7% 54.7% 69.1% 9.3% 30.5% 93.9% 94.0% 63.5% 66.4% 38.9% 39.6% 43.7% 43.6% 64.0% 65.7% 57.6% 57.1% 38.0% 41.0% 52.8% 52.2% 53.5% 52.7% 51.1% 50.8% 40.9% 40.1% 30.5% 31.4% 53.5% 53.2% 36.8% 38.2% 45.2% 47.5% 51.8% 54.5% 6.4% 6.6% 18.5% 19.8% We show the spectrograms of example model generations in Figures 11, 12, and 13. The DRAGON model that optimizes per-song VAE-FAD with ALIM reference statistics improves the balance over the frequency ranges. The effect of optimizing aesthetics score is less visible from the spectrograms, but our listening tests find reduced artifacts and improved overall music quality."
        },
        {
            "title": "H Full Result Tables",
            "content": "We use five tables to list the reward metrics achieved by all DRAGON models discussed in the paper. First, we present the win rates and the average values of instance-level rewards (aesthetics score, CLAP score, and per-song FAD) in Tables 6 and 7. Next, we present the win rates and the average values of distributionlevel rewards like rdist (full-dataset FAD and Vendi), evaluated in bootstrapped setting, in Tables 8 and 9. As mentioned in Section 5.1, this means sampling 40-example generation subsets from our 2185-prompt evaluation set with replacement 1000 times, computing the reward metric for each subset, and reporting the average and win rate among these 1000 numbers. Finally, we present the distribution-level rewards evaluated over the full 2185-instance evaluation set without bootstrapping in Table 10. Vendi is computed over per-song average MA embeddings in the full-dataset setting as in the main paper body, but is computed over all MA embeddings without averaging in the bootstrapped setting. 31 Figure 11: Spectrograms of example generations (part 1). 32 Figure 12: Spectrograms of example generations (part 2). Figure 13: Spectrograms of example generations (part 3). 34 Table 7: All DRAGON models average instance-level reward. DRAGON Model Aesthetics CLAP CLAP-Audio Per-Song FAD CLAP-Text VAE-Audio Score Score ALIM SDNV ALIM SDNV Slackbot Mixtral ALIM SDNV Reference (40 inference steps) Reference (10 inference steps) KTO Aesthetics DPO Aesthetics (40/40 train/inference steps) DPO Aesthetics (40/10 train/inference steps) DPO Aesthetics (10/40 train/inference steps) DPO Aesthetics (10/10 train/inference steps) KTO-Unpaired Aesthetics KTO CLAP Score KTO Per-Song FAD VAE-ALIM-Audio KTO Per-Song FAD VAE-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Audio KTO Per-Song FAD CLAP-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Text KTO Per-Song FAD CLAP-SDNV-Text KTO Per-Song FAD CLAP-Slackbot-Text KTO Per-Song FAD CLAP-Mixtral-Text KTO Dataset FAD VAE-ALIM-Audio KTO Dataset FAD VAE-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Audio KTO Dataset FAD CLAP-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Text KTO Dataset FAD CLAP-SDNV-Text KTO Dataset FAD CLAP-Slackbot-Text KTO Dataset FAD CLAP-Mixtral-Text KTO Vendi Score KTO Aesthetics + Vendi Score 0.187 -0.484 0.619 0.638 0.232 0.596 0.373 0.596 0. 0.562 0.102 0.291 0.120 0.541 0.151 0.221 0.370 0.199 0.107 0.250 0.175 0.399 0.091 0.258 0.368 -0.328 0.222 0.300 0.239 0.304 0.312 0.265 0.307 0.270 0.312 0.947 0. 0.962 0.922 0.889 0.932 0.875 0.949 0.990 0.986 1.018 0.965 0.918 0.977 0.915 1.004 1.576 1.597 1.572 1.545 1.529 1.532 1.510 1.521 1.561 1. 1.554 1.540 1.522 1.552 1.528 1.522 0.317 0.929 0.997 1.487 1. 0.303 0.278 0.304 0.286 0.325 0.306 0.317 0.302 0.300 0.296 0.295 0.299 0.313 0.304 0.307 0.320 0.142 0.285 0.847 0.990 0.867 0.935 0.921 0.921 0.929 0.959 0.963 0.976 0.928 0.921 0.985 0.965 0.964 0.981 1.302 1. 0.920 1.040 0.923 0.973 0.977 0.980 0.969 0.998 1.003 1.010 0.977 0.957 1.024 1.005 1.008 1.030 1.323 1.095 1.497 1.621 1.522 1.598 1.482 1.568 1.509 1.534 1.555 1.590 1.582 1.574 1.560 1.552 1.575 1.513 1.864 1. 1.521 1.572 1.515 1.555 1.499 1.532 1.508 1.515 1.548 1.560 1.558 1.550 1.549 1.543 1.553 1.527 1.753 1.588 1.596 1.591 1.589 1.578 1.554 1.584 1.558 1.563 1. 1.559 1.610 1.550 1.600 1.536 1.568 1.536 1.551 1.584 1.600 1.590 1.587 1.574 1.577 1.588 1.557 1.794 1.627 1.534 1.551 1.529 1.508 1.491 1.509 1.486 1.484 1. 1.473 1.568 1.480 1.542 1.457 1.511 1.469 1.489 1.517 1.539 1.535 1.529 1.517 1.515 1.527 1.483 1.800 1.585 30.84 30.77 29.88 30.13 28.66 30.71 30.00 29.95 31.12 30. 30.12 30.14 28.54 30.83 29.91 30.03 27.51 28.55 16.31 28.99 34.55 32.98 27.30 29.30 34.81 30.54 31.15 32.17 32.91 36.38 30.96 33.37 32.71 32.09 80.57 41. 16.89 28.38 34.52 33.31 27.23 29.83 34.08 30.92 31.70 32.46 33.40 36.20 31.32 33.24 32.58 31.79 79.64 41.08 Table 8: All DRAGON models bootstrapped distribution-level reward win rate. Full-Dataset FAD CLAP-Audio CLAP-Text ALIM SDNV ALIM SDNV Slackbot Mixtral ALIM SDNV VAE-Audio Diversity Vendi DRAGON Model Reference (40 inference steps) Reference (10 inference steps) 50.0% 50.0% 50.0% 50.0% 0.0% 0.0% 0.0% 0.0% KTO Aesthetics DPO Aesthetics (40/40 train/inference steps) DPO Aesthetics (40/10 train/inference steps) DPO Aesthetics (10/40 train/inference steps) DPO Aesthetics (10/10 train/inference steps) KTO-Unpaired Aesthetics 0.9% 4.3% 7.1% 0.0% 1.1% 42.4% 44.8% 37.3% 0.0% 0.0% 0.6% 0.0% 0.1% 42.1% 33.8% 85.0% 7.7% 0.8% 0.3% 0.0% 89.5% 28.5% 0.3% 2.6% KTO CLAP Score KTO Per-Song FAD VAE-ALIM-Audio KTO Per-Song FAD VAE-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Audio KTO Per-Song FAD CLAP-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Text KTO Per-Song FAD CLAP-SDNV-Text KTO Per-Song FAD CLAP-Slackbot-Text KTO Per-Song FAD CLAP-Mixtral-Text KTO Dataset FAD VAE-ALIM-Audio KTO Dataset FAD VAE-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Audio KTO Dataset FAD CLAP-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Text KTO Dataset FAD CLAP-SDNV-Text KTO Dataset FAD CLAP-Slackbot-Text KTO Dataset FAD CLAP-Mixtral-Text 31.7% 0.2% 100.0% 35.1% 45.8% 0.6% 38.1% 0.0% 0.7% 2.9% 0.1% 7.7% 50.3% 15.5% 46.0% 6.9% 7.0% 0.0% 16.8% 7.5% 99.8% 44.5% 2.0% 16.0% 16.3% 67.0% 8.1% 43.9% 98.4% 74.2% 6.7% 1.9% 55.6% 40.4% 0.0% 0.1% 1.5% 0.2% 78.9% 38.3% 1.0% 0.0% 16.9% 63.5% 73.6% 29.9% 13.8% 29.8% 42.4% 83.2% 13.2% 15.0% 85.4% 75.2% 2.1% 6.2% 92.7% 81.6% 26.2% 14.1% 86.0% 98.8% 0.0% 100.0% 89.2% 1.3% 0.9% 5.8% KTO Vendi Score KTO Aesthetics + Vendi Score 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 2.9% 35 50.0% 0.0% 2.7% 1.6% 0.0% 1.0% 0.0% 17.3% 45.3% 0.0% 9.6% 7.0% 0.7% 45.5% 53.1% 89.6% 40.5% 33.9% 50.2% 39.4% 15.3% 97.8% 85.0% 98.4% 95.3% 0.0% 2.8% 50.0% 0.0% 2.8% 10.3% 0.0% 18.4% 0.1% 81.5% 50.0% 50.0% 0.0% 0.0% 20.1% 17.4% 13.3% 17.1% 0.2% 0.2% 18.4% 22.3% 0.0% 0.1% 16.5% 16.8% 96.3% 85.2% 55.3% 1.8% 3.0% 21.7% 0.2% 93.4% 42.7% 95.6% 50.4% 62.8% 38.9% 24.0% 11.9% 88.2% 86.3% 96.5% 99.8% 0.0% 0.1% 86.9% 85.1% 13.6% 4.4% 0.0% 0.0% 60.9% 55.8% 88.5% 92.0% 29.9% 17.6% 0.0% 0.1% 83.5% 79.8% 70.5% 58.7% 61.9% 59.4% 61.5% 50.2% 0.0% 0.0% 88.2% 84.7% 1.2% 1.9% 26.5% 38.9% 14.1% 8.3% 0.0% 0.0% 0.0% 0.0% Score 50.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 40.5% 0.0% 6.8% 0.0% 0.4% 0.0% 0.0% 7.3% 71.8% 19.8% 6.2% 58.3% 36.6% 93.2% 7.6% 99.8% 83.1% Vendi Score 12.705 10.401 10.968 10.601 8.736 10.731 8.710 10.664 10. 8.287 12.567 10.348 12.028 10.048 11.428 11.083 10.737 11.944 13.042 12.325 11.939 12.824 12.549 13.436 11.842 16.068 13.292 Vendi Score 12.230 8. 10.782 9.648 7.059 10.068 7.164 10.140 Table 9: All DRAGON models average bootstrapped distribution-level reward. DRAGON Model Full-Dataset FAD CLAP-Audio CLAP-Text ALIM SDNV ALIM SDNV Slackbot Mixtral ALIM SDNV VAE-Audio Diversity Reference (40 inference steps) Reference (10 inference steps) KTO Aesthetics DPO Aesthetics (40/40 train/inference steps) DPO Aesthetics (40/10 train/inference steps) DPO Aesthetics (10/40 train/inference steps) DPO Aesthetics (10/10 train/inference steps) KTO-Unpaired Aesthetics 0.214 0.344 0.243 0.216 0.295 0.217 0.262 0.242 0.260 0.374 0.311 0.262 0.321 0.265 0.297 0.303 0.983 1. 1.004 0.987 1.029 0.969 1.006 0.963 0.799 0.876 0.831 0.826 0.876 0.832 0.876 0.806 KTO CLAP Score 0.222 0. 0.926 0.804 KTO Per-Song FAD VAE-ALIM-Audio KTO Per-Song FAD VAE-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Audio KTO Per-Song FAD CLAP-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Text KTO Per-Song FAD CLAP-SDNV-Text KTO Per-Song FAD CLAP-Slackbot-Text KTO Per-Song FAD CLAP-Mixtral-Text KTO Dataset FAD VAE-ALIM-Audio KTO Dataset FAD VAE-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Audio KTO Dataset FAD CLAP-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Text KTO Dataset FAD CLAP-SDNV-Text KTO Dataset FAD CLAP-Slackbot-Text KTO Dataset FAD CLAP-Mixtral-Text KTO Vendi Score KTO Aesthetics + Vendi Score 0.219 0.272 0.214 0.229 0.227 0.216 0.243 0. 0.239 0.257 0.207 0.216 0.243 0.234 0.223 0.245 0.668 0.340 0.306 0.334 0.274 0.270 0.290 0.279 0.281 0.315 0.283 0.294 0.265 0.251 0.292 0.278 0.272 0.304 0.692 0.370 0.984 1.039 0.984 1.020 0.937 0.997 0.950 0. 0.973 0.996 0.995 0.999 0.967 0.963 0.968 0.925 1.298 1.066 0.872 0.827 0.818 0.812 0.801 0.794 0.791 0.802 0.802 0.794 0.803 0.810 0.791 0.788 0.775 0.782 1.014 0.831 0.837 0. 0.863 0.864 0.906 0.865 0.904 0.850 0.838 0.908 0.863 0.857 0.860 0.838 0.836 0.821 0.840 0.841 0.836 0.839 0.849 0.813 0.825 0.813 0.813 1.038 0.868 0.832 0. 0.859 0.848 0.899 0.843 0.888 0.819 0.806 0.867 0.882 0.843 0.861 0.812 0.834 0.808 0.832 0.828 0.835 0.839 0.847 0.816 0.818 0.811 0.786 1.122 0.886 8.261 13. 9.135 9.616 12.604 9.245 13.657 9.703 8.297 12.502 9.182 9.288 11.844 9.022 12.798 9.560 6.955 8.135 7.027 10.722 14.034 7.927 6.879 8.958 12.928 7. 7.579 7.909 7.923 14.529 6.980 11.382 9.014 10.404 41.102 15.192 7.305 9.748 13.456 8.132 6.822 9.381 11.916 7.440 8.137 8.053 8.273 13.712 7.353 10.849 8.639 9.863 39.209 14.801 Table 10: All DRAGON models full-dataset distribution-level reward. DRAGON Model Full-Dataset FAD CLAP-Audio CLAP-Text ALIM SDNV ALIM SDNV Slackbot Mixtral ALIM SDNV VAE-Audio Diversity Reference (40 inference steps) Reference (10 inference steps) KTO Aesthetics DPO Aesthetics (40/40 train/inference steps) DPO Aesthetics (40/10 train/inference steps) DPO Aesthetics (10/40 train/inference steps) DPO Aesthetics (10/10 train/inference steps) KTO-Unpaired Aesthetics 0.107 0. 0.129 0.118 0.221 0.110 0.184 0.134 0.155 0.297 0.198 0.164 0.247 0.161 0.219 0.194 0.901 1.002 0.922 0.912 0.966 0.891 0.941 0.884 0.684 0. 0.716 0.721 0.789 0.723 0.786 0.695 KTO CLAP Score 0.125 0.213 0.852 0. KTO Per-Song FAD VAE-ALIM-Audio KTO Per-Song FAD VAE-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Audio KTO Per-Song FAD CLAP-SDNV-Audio KTO Per-Song FAD CLAP-ALIM-Text KTO Per-Song FAD CLAP-SDNV-Text KTO Per-Song FAD CLAP-Slackbot-Text KTO Per-Song FAD CLAP-Mixtral-Text KTO Dataset FAD VAE-ALIM-Audio KTO Dataset FAD VAE-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Audio KTO Dataset FAD CLAP-SDNV-Audio KTO Dataset FAD CLAP-ALIM-Text KTO Dataset FAD CLAP-SDNV-Text KTO Dataset FAD CLAP-Slackbot-Text KTO Dataset FAD CLAP-Mixtral-Text KTO Vendi Score KTO Aesthetics + Vendi Score 0.135 0.164 0.128 0.130 0.124 0.115 0.145 0.166 0.128 0.154 0.108 0.119 0.134 0.123 0.114 0.130 0.546 0. 0.223 0.227 0.190 0.172 0.187 0.179 0.184 0.213 0.173 0.192 0.168 0.154 0.184 0.168 0.165 0.191 0.571 0.254 0.781 0.712 0.721 0.704 0.694 0.686 0.685 0.693 0.686 0.681 0.696 0.703 0.676 0.671 0.659 0.664 0.879 0. 0.919 0.959 0.915 0.943 0.861 0.920 0.875 0.903 0.890 0.914 0.918 0.922 0.884 0.879 0.885 0.842 1.200 0.977 36 0.716 0.804 0.740 0.753 0.813 0.749 0.809 0. 0.729 0.811 0.742 0.754 0.746 0.725 0.722 0.709 0.725 0.718 0.716 0.725 0.735 0.691 0.701 0.690 0.688 0.895 0.738 0.734 0.845 0.761 0.758 0.825 0.749 0.811 0. 0.718 0.789 0.785 0.760 0.769 0.721 0.742 0.718 0.739 0.729 0.738 0.747 0.756 0.717 0.718 0.711 0.686 1.007 0.780 7.470 12.898 8.399 8.965 12.255 8.528 13.267 8. 7.503 12.025 8.443 8.635 11.475 8.291 12.383 8.813 6.218 7.403 9.510 6.764 10.321 13.418 7.033 6.236 8.295 12.202 6. 6.664 6.971 7.053 13.905 6.143 10.661 8.193 9.564 39.049 14.290 7.030 9.328 12.831 7.237 6.172 8.720 11.187 6.727 7.221 7.111 7.407 13.065 6.513 10.119 7.810 9.014 37.143 13.902 7.061 12.601 8.867 11.200 9.269 10.718 10.085 10. 11.665 12.548 11.387 10.943 12.476 12.331 13.044 11.669 17.225 13."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of California, Berkeley"
    ]
}