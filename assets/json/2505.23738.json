{
    "paper_title": "How Animals Dance (When You're Not Looking)",
    "authors": [
        "Xiaojuan Wang",
        "Aleksander Holynski",
        "Brian Curless",
        "Ira Kemelmacher",
        "Steve Seitz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a keyframe-based framework for generating music-synchronized, choreography aware animal dance videos. Starting from a few keyframes representing distinct animal poses -- generated via text-to-image prompting or GPT-4o -- we formulate dance synthesis as a graph optimization problem: find the optimal keyframe structure that satisfies a specified choreography pattern of beats, which can be automatically estimated from a reference dance video. We also introduce an approach for mirrored pose image generation, essential for capturing symmetry in dance. In-between frames are synthesized using an video diffusion model. With as few as six input keyframes, our method can produce up to 30 second dance videos across a wide range of animals and music tracks."
        },
        {
            "title": "Start",
            "content": "How Animals Dance (When Youre Not Looking) XIAOJUAN WANG, University of Washington, USA ALEKSANDER HOLYNSKI, UC Berkeley, USA BRIAN CURLESS, University of Washington, USA IRA KEMELMACHER-SHLIZERMAN, University of Washington, USA STEVEN M. SEITZ, University of Washington, USA 5 2 0 2 9 2 ] . [ 1 8 3 7 3 2 . 5 0 5 2 : r Fig. 1. Starting from small set of generated keyframes, e.g., marmot in various poses, our method generates an animal dance video that follows specified choreography pattern, extracted from reference dance video. We present keyframe-based framework for generating music-synchronized, choreography aware animal dance videos. Starting from few keyframes representing distinct animal posesgenerated via text-to-image prompting or GPT-4o we formulate dance synthesis as graph optimization problem: find the optimal keyframe structure that satisfies specified choreography pattern of beats, which can be automatically estimated from reference dance video. We also introduce an approach for mirrored pose image generation, essential for capturing symmetry in dance. In-between frames are synthesized using an video diffusion model. With as few as six input keyframes, our method can produce up to 30 second dance videos across wide range of animals and music tracks. See our project page for more results: how-animals-dance.github.io. CCS Concepts: Computing methodologies Computer vision. Additional Key Words and Phrases: dance video generation, diffusion models, vision for graphics Authors addresses: Xiaojuan Wang, xiaojwan@cs.washington.edu, University of Washington, USA; Aleksander Holynski, UC Berkeley, USA, holynski@google.com; Brian Curless, University of Washington, USA, curless@cs.washington.edu; Ira KemelmacherShlizerman, University of Washington, USA, kemelmi@cs.washington.edu; Steven M. Seitz, University of Washington, USA, seitz@cs.washington.edu."
        },
        {
            "title": "INTRODUCTION",
            "content": "Everything in the universe has rhythm; everything dances. Maya Angelou Humans dance spontaneously to musicjust picture toddler cheerfully bouncing to the beats at birthday party. Animals can dance too; Snowball the cockatoo  (Fig. 2)  can perform up to 14 distinct dance movements in response to different musical cues [Keehn et al. 2019]. In fact, our animal friends are probably dancing all the time when were not looking. In this paper, we capture this hidden world of animal dance, and expose it for the first time to the human population  (Fig. 1)  . While we happen to be particularly obsessed with dancing animals, this paper introduces new framework for generating musicsynchronized, highly structured, 30 second+ long dance videos. Such capabilities are challenging for current state of the art generative models, most of which are limited to short clips of few seconds, do not produced synchronized audio and video, and lack intuitive controls for long range motion. Beyond text prompting, most controls for video generation are fine-grained and operate on single frame at time, e.g., body pose, camera pose, motion brushes, etc. 2 Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher-Shlizerman, and Steven M. Seitz Fig. 2. Snowball the cockatoo: real bird that can dance. In contrast, we introduce choreography patterns as new control for video generation. Specifically, we allow the user to specify structured sequence of dance moves, or beats, e.g., A-B-A-B-C-D-A, where each letter corresponds to particular move, and constrain the motion in the video to follow that choreography. Furthermore, we show how these choreography patterns can be automatically estimated from existing (human) dance videos. well-formed dance follows basic choreographic rules [Chen et al. 2021], which structure the movements to align with the rhythmic flow of the accompanying music, and often involve recurring patterns such as mirroring and repetition to help reinforce the musical structure [Kim et al. 2006, 2003]. We leverage this inherent structure of dance to make the generation task more tractable. As input, we use collection of initially generated keyframes, each representing distinct pose. We then formulate the dance synthesis as graph optimization problem, i.e., find the optimized walk path through the keyframes to satisfy specified choreography pattern of beats. Each selected keyframe in the path is aligned to musical beat. The final dance video is produced by synthesizing in-between frames between the keyframes using generative video inbetweening model [Wang et al. 2024a,b]  (Fig. 3)  . Beyond 1) introducing new type of generative video control (choreography patterns), and 2) practical system for generating music-synchronized dance videos, this paper makes the following technical contributions. First, we introduce technique for inferring choreography patterns from human dance videos, such as those found on Youtube and TikTok. Second, we formulate the satisfaction of these constraints as graph optimization problem and solve it. Finally, we demonstrate an approach for pose-mirroring in the image domain, while retaining asymmetries in foreground and background features. We demonstrate the effectiveness of our method by generating dance videos up to 30 seconds long across approximately 25 animal instances across 10 classesincluding marmots, sea otters, hedgehogs, and catspaired with various songs. These videos represent the first-ever recorded demonstrations of these animals performing such complex musical dance routines and will be studied by generations of zoologists."
        },
        {
            "title": "2 PRIOR WORK",
            "content": "Music-driven generative dance synthesis. Earlier learning-based approaches developed neural networks that synthesize human dance Fig. 3. System overview. Given few initially generated keyframes as input, we generate mirrored counterparts, extract choreography pattern from dance video, and optimize the keyframe structure accordingly. The final dance is synthesized by generating in-between frames with video diffusion model and warped to the musical beats. Our method is highlighted in gray. motion directly from music input [Huang et al. 2020; Lee et al. 2019, 2018; Li et al. 2021; Sun et al. 2020; Zhang et al. 2022]. Recent advances have shifted toward diffusion-based methods [Le et al. 2023; Qi et al. 2023; Tseng et al. 2023], which also focus primarily on generating skeletal motions from music. More recently, some works have begun exploring direct dance video generation using video diffusion models [Ruan et al. 2023; Sun et al. 2020]. However, directly enforcing choreography structure within these frameworks remains challenging. In addition, these learning-based approaches typically require training dataan issue in our case, as dance videos featuring animals are extremely scarce. Graph-based human motion synthesis. In contrast to learningbased approaches, graph-based frameworks [Chen et al. 2021; Kim et al. 2006, 2003; ManfrÃ¨ et al. 2016; Ofli et al. 2008] synthesize new motions from an existing dance motion segments database, and cast the dance synthesis as graph optimization problem: finding an optimal path in the constructed motion graph that aligns with the input music. For example, Kim et al. [Kim et al. 2003] introduced rhythmic and beat-based constraints to guide the path search, while more recent work ChoreoMaster [Chen et al. 2021] incorporated richer choreography rules, requiring not only structural alignment with music but also stylistic compatibility. Our approach follows this paradigm but differs in key ways. First, instead of relying on motion capture database, we take small set of keyframes of an animal or subject as input. We augment this set by generating mirrored pose images, creating complete keyframe set for dance synthesis. The graph is constructed over these keyframes, and video diffusion model is applied to generate realistic in-between frames along the optimized walk path, producing the final dance video. Second, while basic choreography rules can be inferred from the musical structure as done in [Chen et al. 2021], different performers may interpret the same piece differently. As such, we propose way to extract choreography patterns directly from reference dance video and use it as the control. Anthropomorphic character animation. Given reference image, character image animation, generates videos following target human skeletal pose sequence. While existing methods [Hu 2024; Hu et al. 2025] are primarily designed for human figures, recent work [Tan et al. 2024] extends to anthropomorphic characters by learning generalized motion representation. However, it remains favor human-like anatomy, generating animals with features like elongated limbs. They also struggle to generate high-fidelity videos from single image when handling long and diverse sequences, such as 30-second dance. In contrast, our method does not rely on per-frame human skeleton pose as guidance. Instead, we use small set of keyframes generated from text prompt as pose alternative, and synthesize intermediate frames using generative inbetweening model. Since our approach is keyframe-based and choreographically structured, it is not constrained by video length and can scale to longer dance sequences."
        },
        {
            "title": "3 APPROACH\nWe begin by generating a small set of keyframes {ğ¼ğ‘˜ }ğ¾ âˆ’1\nğ‘˜=0 , each de-\npicting the subject, e.g., a marmot, in different poses while maintain-\ning a consistent background and static camera view (see Section 4.1\nfor details). Our goal is to synthesize a dance video of the input\nanimal from the provided keyframes, synchronized to the beats\nand following the choreography pattern extracted from a reference\ndance video.",
            "content": "We first introduce how we extract the choreography pattern directly from human dance video in Section 3.1. Since motion mirroring is an essential component of dance, we then present our approach for generating mirrored pose counterpart for each keyframe to augment the keyframe set in Section 3.2. Finally, in Section 3.3, we present how to synthesize the full dance video using the complete set of keyframes, including mirrored ones, to follow the choreography pattern."
        },
        {
            "title": "3.1 Choreography Pattern Labeling",
            "content": "Choreography are closely tied to the rhythmic structure of music. In music theory, beat is the basic temporal unit, while bar (or measure) groups fixed number of beats. The meter defines how beats are grouped and emphasized within the bar, and is indicated by time signature, e.g., 2/4, 3/4, 4/4, where the upper number specifies beats per bar, and the lower number denotes the note value that receives one beat. In this work, we focus on music with 4/4 time signatureeach bar contains four quarter note beatsthe most common structure in popular music. Problem definition. Given 4/4 music track with synchronized dance video, we begin by detecting the beat times = {ğ‘¡0, ğ‘¡1, ..., ğ‘¡ğ‘ 1}, ğ‘ assuming total of ğ‘ beats, corresponding to 4 bars. Based on the beat times, we construct sequence of motion segments = {ğ‘ 0, ğ‘ 1, ..., ğ‘  ğ‘ 2 1} where each segment ğ‘ ğ‘– spans from beat ğ‘¡2ğ‘– to beat ğ‘¡2ğ‘–+1. Each bar thus yields two motion segments: one from the first to the second beat, and another from the third to the fourth beat, aligning with the 4/4 music structure where major movements typically How Animals Dance (When Youre Not Looking) 3 begin on accented beats and end on weak ones, whereas transitions occur across weak-to-accented intervals. The choreo pattern labeling task outputs sequence of labels = {ğ‘™0, ğ‘™1, ..., ğ‘™ ğ‘ 2 1}, e.g., A-A-B-C-D-D, where each ğ‘™ğ‘– corresponds to motion segment ğ‘ ğ‘– . Distinct motions receive different labels, identical motions share the same one, and mirrored motions are indicated by prime-labeled counterparts (e.g., and A). Motion segments quantization. We formulate motion segment sequence labeling as quantization problem: clustering similar motion segments and assigning each cluster ID as its label. Each segment ğ‘ ğ‘– of length ğ‘‡ğ‘– is represented by the SMPL-X [Pavlakos et al. 2019] pose sequence recovered from the video: ğ‘ ğ‘– = {(ğœƒğ‘¡ğ‘– ğ‘‡ğ‘– R3 ( ğ½ +1), ğœğ‘¡ğ‘– R3)} ğ‘¡ğ‘– =0, where ğœƒğ‘¡ğ‘– contains per-joint axis-angle rotation for ğ½ = 21 body joints in addition to joint for global rotation (the 0-th joint), and ğœğ‘¡ğ‘– denotes the global translation in 3D space. For clustering, we focus solely on posesignoring global translationsto capture distinctive motion patterns. The distance between two SMPL-X poses is defined as the average geodesic distance across joints: ğ‘‘ğœƒ (ğœƒ1, ğœƒ2) = 1 ğ½ ğ½ ğ‘—=0 log(ğ‘…(ğœƒ ğ‘— )ğ‘‡ ğ‘…(ğœƒ ğ‘— 2 ))ğ¹ (1) where ğ‘…(ğœƒ ğ‘— ) converts the axis-angle representation of joint ğ‘— into rotation matrix. To account for slight temporal offsets between beats, we compute the distance between two motion segments using dynamic time warping (DTW), with ğ‘‘ğœƒ as the local cost metric between poses. The clustering function is then defined as: C(S; DTWğ‘‘ğœƒ , ğœ–ğœƒ ) {C1, C2, ..., Cğ¶ } (2) ğ‘=1 Cğ‘ = , with Cğ‘– (cid:209) Cğ‘— = , for ğ‘– ğ‘—. Segments within where (cid:208)ğ¶ the same cluster satisfy: DTWğ‘‘ğœƒ (ğ‘ ğ‘–, ğ‘  ğ‘— ) < ğœ–ğœƒ , ğ‘ ğ‘–, ğ‘  ğ‘— Cğ‘ . Mirrored motion segments detection. After the quantization stage, we identify mirrored motion segments in two steps. Mirrored pose clusters. mirrored joint rotation is defined by reflecting the axis-angle vector across the sagittal (YZ) plane: (ğœƒ ğ‘— ) = (ğœ”ğ‘¥ , ğœ”ğ‘¦, ğœ”ğ‘§), where ğœƒ ğ‘— = (ğœ”ğ‘¥ , ğœ”ğ‘¦, ğœ”ğ‘§). Then mirrored pose ğœƒ is obtained by applying this reflection to each joint after left-right joint swapping: ğœƒ ğ‘— = (ğœƒ ğœ‹ ( ğ‘— ) ) for ğ‘— = 0, . . . , ğ½ (3) here ğœ‹ ( ğ‘—) denotes the left-right joint permutation (e.g. swapping left/right arms, legs, and shoulders), and for central joints (e.g., spine, neck, head), ğœ‹ ( ğ‘—) = ğ‘—, so only the reflection is applied. Two clusters Cğ‘ and Cğ‘ are considered mirrored if there exists at least one pair of segments ğ‘ ğ‘– Cğ‘, ğ‘  ğ‘— Cğ‘ , such that the mirğ‘‡ğ‘– rored version of ğ‘ ğ‘– , denoted by ğ‘  ğ‘– = {ğœƒ ğ‘¡ğ‘– =0, is similar to ğ‘  ğ‘— unğ‘¡ğ‘– } der dynamic time warping: DTWğ‘‘ğœƒ (ğ‘  ğ‘– , ğ‘  ğ‘— ) < ğœ–ğœƒ . The resulting set of mirrored cluster pairs is: = {(Cğ‘, Cğ‘ ) ğ‘ ğ‘– Cğ‘, ğ‘  ğ‘— Cğ‘, DTWğ‘‘ğœƒ (ğ‘  ğ‘– , ğ‘  ğ‘— ) < ğœ–ğœƒ }. Mirrored motion directions within cluster. For clusters without mirrored counterpart, we further check whether they can be internally partitioned into two directionally mirrored groups. We first extract the overall motion direction (cid:174)ğ‘‘ğ‘ ğ‘– of each motion segment Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher-Shlizerman, and Steven M. Seitz ğ‘¡ğ‘– ğœ 0 ğ‘¡ğ‘– ğœ 0 ğ‘¡ğ‘– )/ğœğ‘‡ğ‘– ğ‘ ğ‘– using its global translation: (cid:174)ğ‘‘ğ‘ ğ‘– = (ğœğ‘‡ğ‘– ğ‘¡ğ‘– , where ğ‘¡ğ‘– and ğœğ‘‡ğ‘– ğœ 0 ğ‘¡ğ‘– denote the segments start and end positions, respectively. To identify mirrored directions, each motion direction (cid:174)ğ‘‘ğ‘ ğ‘– is reflected across the YZ plane as (cid:174)ğ‘‘ ğ‘ ğ‘– = diag([1, 1, 1]) (cid:174)ğ‘‘ğ‘ ğ‘– . We then perform bipartite matching to find mirror pairs (ğ‘ ğ‘–, ğ‘  ğ‘— ) that satisfy (cid:174)ğ‘‘ ğ‘ ğ‘– (cid:174)ğ‘‘ğ‘  ğ‘— < ğœ–ğ‘‘ . If valid pairs are found, we assign all matched segments into two directionally consistent groups based on their directional similarity. The original cluster Cğ‘– is then partitioned ğ‘– , C1 into two mirrored subgroups (C0 ğ‘– ), which are then added to the mirrored cluster set M. Finally, we assign unique label to each cluster. For each mirrored pair (Cğ‘, Cğ‘ ) M, we assign base label ğ‘™ğ‘ (e.g. A) to segments in Cğ‘, and its mirrored label ğ‘™ ğ‘ (e.g. A) to segments in Cğ‘ . Clusters without mirrored counterpart are assigned distinct label without prime."
        },
        {
            "title": "3.2 Mirrored Pose Image Generation",
            "content": "Fig. 4. Mirrored pose generation. We fine-tune text-to-image model with ControlNet using the canny edges extracted from each keyframe as conditioning. During inference, mirrored pose images are generated by flipping only the subject edges and using an inpainted background edge composed from the keyframe set. ğ¾ 1 , . . . , ğ¼ To augment the keyframe set with mirrored counterparts, we generate visually consistent keyframe pairs for each input pose. This process  (Fig. 4)  involves fine-tuning text-to-image model with ControlNet, generating mirrored edge maps, and re-generating the original keyframes for consistency. In the end, we get complete set of consistent keyframes = {ğ¼0, . . . , ğ¼ğ¾ 1, ğ¼ }, where 0 ğ¼ ğ‘˜ is the mirrored version of ğ¼ğ‘˜ . Fine-tuning. We fine-tune pretrained text-to-image model on the input keyframes set, overfitting it to capture the appearance of the specific input subject instance and the background. To provide structural guidance, we incorporate ControlNet [Zhang et al. 2023] using the canny edge maps extracted from the input images as conditional input. We use the prompt format: photo of [V] [subject class name] dancing [background description]., where [V] is unique token for identifying the subject instance rather than class. The placeholders [subject class name] and [background description] are replaced with the actual class name of the subject and background description. For example, photo of [V] marmot dancing with alpine landscape as background.\" Mirrored edge generation. To generate mirrored pose images, We first extract the subject mask using SAM [Kirillov et al. 2023]. We then construct unified background canny edge map by inpainting and stitching the background edges from all input keyframes. For each keyframe, we extract the subjects edge map and horizontally flip it to create mirrored subject edge map. This flipped edge map is then composited with the shared background edge map to generate full mirrored edge map. which is used as input to the fine-tuned model to generate the corresponding mirrored image. Keyframes re-generation. The original keyframes may contain slight inconsistencies in the background due to generation instability. Additionally, the fine-tuned model may introduce subtle color shifts during inference. To ensure visual consistency among the augmented keyframes set, we regenerate the original keyframes using the same model and shared background edge map (see Fig. 5 for an example). (a) raw keyframes (b) re-generated keyframes Fig. 5. Improving visual consistency by re-generating keyframes with shared background edges. Implementation details. We use FLUX.1-dev and Xlabs-AI/fluxcontrolnet-canny as the pretrained text-to-image and controlnet model. We fine-tune them jointly with LoRA rank of 16 for 500 epochs, Training on 6 keyframes takes around 90 minutes on single A100 GPU. Canny edges are extracted using threshold values of (50, 100)."
        },
        {
            "title": "3.3 Choreography Pattern Driven Dance Synthesis\nGiven the augmented keyframe set I = {ğ¼0, . . . , ğ¼ğ¾ âˆ’1, ğ¼ â€²\n},\nğ¾ âˆ’1\n0\nwhere ğ¼ â€²\nğ‘˜ denotes the mirrored counterpart of keyframe ğ¼ğ‘˜ , and the\nchoreography pattern label sequence L = {ğ‘™0, ğ‘™1, ..., ğ‘™ ğ‘\n2 âˆ’1}, the goal\nis to find an optimal walk path P = {ğ¼ğ‘0\n, . . . , ğ¼ğ‘ğ‘ âˆ’1 } through\nthe keyframe set, and the ğ‘–-th keyframe ğ¼ğ‘ğ‘– in the path corresponds\nto the ğ‘–-th beat. We then apply video diffusion model to generate\nin-between frames, finally producing the final dance video.",
            "content": ", . . . , ğ¼ , ğ¼ğ‘1 Since each label ğ‘™ğ‘– corresponds to motion segment between keyframe pairs (ğ¼ğ‘2ğ‘– , ğ¼ğ‘2ğ‘–+1 ), we cast path planning as graph optimization, where each node represents candidate keyframe pair. The choreography label sequence specifies assignment constraints: same labels map to the same pair, distinct labels to distinct pairs, and mirrored labels to mirrored pairs. The object is to assign each label ğ‘™ğ‘– to node such that these constraints are met while minimizing the total transition cost along the path. How Animals Dance (When Youre Not Looking) mirrored: ğ‘™ğ‘– ğ‘™ ğ‘— , (ğ¼ğ‘, ğ¼ğ‘ ) (cid:48) (ğ¼ğ‘, ğ¼ğ‘‘ ) (9) where ğœ™ (ğ‘™ğ‘– ) = (ğ¼ğ‘, ğ¼ğ‘ ), ğœ™ (ğ‘™ ğ‘— ) = (ğ¼ğ‘, ğ¼ğ‘‘ ). (2) Consecutive, distinct, and non-mirrored labels must not be assigned to nodes that share keyframe, to prevent unnecessary single keyframe repetition. ğ‘™ğ‘– ğ‘™ğ‘–+1, ğ‘™ where ğœ™ (ğ‘™ğ‘– ) = (ğ¼ğ‘, ğ¼ğ‘ ), ğœ™ (ğ‘™ğ‘–+1) = (ğ¼ğ‘, ğ¼ğ‘‘ ). ğ‘– ğ‘™ğ‘–+1 (ğ¼ğ‘ ğ¼ğ‘ ğ¼ğ‘ ğ¼ğ‘‘ ) (10)"
        },
        {
            "title": "3.4 Warp to Music",
            "content": ", ğ¼ğ‘1 We generate the final dance video by applying video diffusion model to synthesize in-between frames along the optimized keyframe walk path = {ğ¼ğ‘0 , . . . , ğ¼ğ‘ğ‘ 1 }, where each keyframe ğ¼ğ‘ğ‘– corresponds to beat position ğ‘–. Note that since there are motion repetition in the choreography, we only have to synthesize videos between unique keyframe pairs. In practice, we use Framer [Wang et al. 2024a], which generates 14 in-between frames, and we assume fps of 25. To synchronize with the music, we warp the video timeline such that the timing of every keyframe in align with the corresponding beat time in the audio. Following the visual rhythm strategy from [Davis and Agrawala 2018], we accelerate the warping rate into beat points and decelerate before and after to preserve beat saliency while ensuring temporal smoothness."
        },
        {
            "title": "4 EXPERIMENTS\n4.1 Keyframes Generation",
            "content": "We generate initial keyframes set by prompting text-to-image model, such as FLUX to generate an image grid with consistent keyframes using prompt template like 3x2 grid of frames, showing 6 consecutive frames from video clip of [DESCRIPTION PROMPT]\". For example, the description prompt might be marmot dancing wildly in the wild alpine landscape, striking variety of fun and energetic poses.\". The same prompt format can also be used with GPT-4os image generation modules, which supports even finer specifications. For instance, one can specify: Generate grid with 2 rows and 3 columns. Depict quokka with distinct poses with wild background. The postures should feel natural for the quokkas anatomy....\". In all of our results, we use total of 6 input keyframes."
        },
        {
            "title": "4.2 Choreoraphy Pattern Labeling",
            "content": "We collect total of 20 dance video clips featuring various 4/4 music tracks from Youtube and Tiktok, ranging from 12s to 25s in length. To create ground truth, we manually annotate the choreography pattern label sequence. We then evaluate our method in Section 3.1 by comparing our extracted label sequences against the ground truth ones. Beat times are detected using Librosa, and SMPL-X pose sequences are recovered from the videos using GVHMR [Shen et al. 2024]. We set the threshold values as follows: ğœ–ğœƒ = 0.21, ğœ–ğœƒ = 0.25 and ğœ–ğ‘‘ = 0.1. Specifically, we evaluate two aspects: (1) Clustering accuracy, where each unique labelincluding mirrored variantsis treated as distinct cluster. We assess the clustering results using standard metrics: Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI); (2) Mirror detection accuracy, where (a) ğ¹ (ğ¼ğ‘¢, ğ¼ğ‘£ ) = 9.03 (b) ğ¹ (ğ¼ğ‘¢, ğ¼ğ‘£ ) = 72.52 Fig. 6. Keyframe pairs with excessively small or large motion are excluded from the graph nodes. Keyframe graph construction. We construct the keyframe graph ğº = (ğ‘‰ , ğ¸) as directed graph, where each node (ğ¼ğ‘¢, ğ¼ğ‘£) ğ‘‰ , with ğ¼ğ‘¢ ğ¼ğ‘£, represents an ordered pair of keyframes from the augmented keyframe set I. Note (ğ¼ğ‘¢, ğ¼ğ‘£) (ğ¼ğ‘£, ğ¼ğ‘¢ ). Each node corresponds to potential dance segment from ğ¼ğ‘¢ to ğ¼ğ‘£, and each edge (ğ¼ğ‘¢, ğ¼ğ‘£) (ğ¼ğ‘¤, ğ¼ğ‘¥ ) ğ¸, with ğ¼ğ‘£ ğ¼ğ‘¤, represents valid transition between segments. To ensure both expressive motion and synthesis feasibility, we filter nodes based on the average per-pixel flow magnitude ğ¹ (ğ¼ğ‘¢, ğ¼ğ‘£), computed over the foreground region of the subject. Nodes with flow that is too small (insufficient motion) or too large (challenging for video synthesis) are discarded, as shown in Fig. 6 . Only node pairs with acceptable motion range are retained: ğ‘‰ = (cid:8)(ğ¼ğ‘¢, ğ¼ğ‘£) ğ‘€low < ğ¹ (ğ¼ğ‘¢, ğ¼ğ‘£) < ğ‘€high (cid:9) (4) To make the synthesized dance smooth and fluid, we define the edge cost between two nodes as the flow magnitude between the end keyframe of the first node and start one of the next node: ((ğ¼ğ‘¢, ğ¼ğ‘£) (ğ¼ğ‘¤, ğ¼ğ‘¥ )) = ğ¹ (ğ¼ğ‘£, ğ¼ğ‘¤). We prune high-cost transitions by including only edges with flow below threshold: ğ¸ = (cid:8)((ğ¼ğ‘¢, ğ¼ğ‘£) (ğ¼ğ‘¤, ğ¼ğ‘¥ )) (cid:12) (cid:12) ğ¹ (ğ¼ğ‘£, ğ¼ğ‘¤) < ğ‘€high (cid:9) (5) We also define mirroring function ğœ‡ : ğ‘‰ ğ‘‰ over graph node such that two nodes are mirrored if and only their respective keyframes are mirrored: ğœ‡ ((ğ¼ğ‘¢, ğ¼ğ‘£)) = (ğ¼ Graph optimization. We define node assignment function:ğœ™ : ğ‘‰ , which maps each choreography label ğ‘™ğ‘– to graph node (ğ¼ğ‘¢, ğ¼ğ‘£) ğ‘‰ , forming walk path through the keyframe graph. The goal is to find the optimal assignment ğœ™ that minimizes the total transition cost across the sequence: ğ‘¢, ğ¼ ğ‘£). ğœ™ = argmin ğ‘ 2 2 ğ‘–=0 (ğœ™ (ğ‘™ğ‘– ), ğœ™ (ğ‘™ğ‘–+1)) subject to the following constraints: ğ‘™ğ‘– = ğ‘™ ğ‘— ğœ™ (ğ‘™ğ‘– ) = ğœ™ (ğ‘™ ğ‘— ) ğ‘™ ğ‘– = ğ‘™ ğ‘— ğœ™ (ğ‘™ğ‘– ) = ğœ‡ (ğœ™ (ğ‘™ ğ‘— )) (6) (7) (8) To ensure visual variety and avoid redundancy, we introduce two additional constraints: (1) Different labels cannot map to partially mirrored node pairsdefined as nodes sharing one keyframe (in any order), with the other keyframes 6 Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher-Shlizerman, and Steven M. Seitz Clustering ARI NMI 0.98 0.94 Mirroring Precision Recall 0.93 Table 1. Evaluation on choreography pattern labeling. 0.91 F1 0. Fig. 7. User study results. Green: Beat accuracy; Red: Dance perception; Purple: Anatomy plausibility; Brown: Visual coherence. we compute precision and recall based on the correctly identified mirrored motion segments pairs. We report the results in Table 1. Our training-free extraction method achieves overall strong quantization accuracy, effectively differentiating different motion patterns. For mirroring, our prediction occasionally misses mirrored pairs, typically in cases where the poses are symmetrical but exhibit subtle mirroring in head or body orientation. Since our method also can output representative motions for each choreography label, users can more easily visualize the structure and manually correct annotation errors if needed."
        },
        {
            "title": "4.3 Dance Results",
            "content": "We generate collection of input keyframe grids featuring approximately 25 animal instances. The animal classes include marmot, capybara, hedgehog, meerkat, penguin, sea otter, cat, raccoon, quokka, beaver and others. We also incorporate anthropomorphic characters such as Elmo. For the video results, we generate dance sequences for these instances using six popular song clips, ranging from 16 to 30 seconds in length, with choreography patterns extracted from the corresponding YouTube videos. While we showcase our method using these examples, it can adapt to any choreography patterns paired with music. Selected results are shown in Fig. 9 and Fig. 12, and we highly recommend viewing the supplementary video for the full experience. User study. Since there are no real animal dance videos or existing methods for direct comparison, we conducted user study to evaluate our approach. We used 40 generated dance videos across 6 different songs, and invited 31 participants to take part. Each participant was presented random set of 8 dance videos and asked to rate the following aspects on scale from 0 to 5: (1) Beat accuracyDoes the movement of the animal appear synchronized with the music beat? (2)Dance perceptionHow confident are you that this is dance rather than random motion? (3) Anatomical plausibilityDoes the animals appearance align with its anatomical structure? (4) Visual coherence Does the video looks visually coherent? The responses are shown in Fig. 7. Our generated dance videos obtained average score of 4.37, 4.35, 4.11, and 3.70 for these factors, respectively. Fig. 8. Keyframe pose grid mimicking. Top row: keyframe pose grid template showing six distinct poses of capybara. Middle and bottom rows: meerkat and hedgehog mimicking the capybaras poses, while preserving their own anatomical structure and personality, generated using GPT-4o. allowed motion range in the graph, and define custom constraints during graph optimization. Pose-grid template control. Given keyframe pose grid as template, we prompt GPT-4o to generate new grid in which another animal mimics\" each pose from the original, though the poses are not expected to be exact same since different animals have different anatomical structures. This template may come from previously generated grid (see Fig. 8 for an example) or be extracted from human dance video by identifying distinct poses. This provides way to guide or customize the input poses, which allows to generate dance videos where different animals dance alike (see supplementary video for examples). Motion range control. The threshold parameters ğ‘€low and ğ‘€high control the range of allowed motion magnitudes between keyframes. typical setting is ğ‘€low 12. and ğ‘€high 60.. Within this range, lowering ğ‘€low and increasing ğ‘€high introduces more candidate nodes and transitions, potentially resulting in richer and more expressive dances. User custom constraints control. During graph optimization, users can specify hard constraints on node assignmentsfor instance, enforcing preferred keyframe pair(s) for specific label(s). Additionally, for some dance, mirrored poses happen at the start and end of choreography label. When such mirrored pose pairs are detected for specific label, we can enforce corresponding node assignments during optimization. For example, such labels have to be assigned to nodes (ğ¼ğ‘¢, ğ¼ğ‘£) where ğ¼ğ‘£ = ğ¼ ğ‘¢ , This allows closer alignment with the reference choreography."
        },
        {
            "title": "4.4 Dance Controllability",
            "content": "Given the same choreography pattern for the dance, the user can use pose-grid template to guide the input keyframe poses, control the In the keyframe graph, we use average per-pixel flow magnitude between two keyframes as proxy for motion strength. However, this measure becomes unreliable in cases where the two keyframes represent mirrored side views, where the flow magnitude fails to D How Animals Dance (When Youre Not Looking) 7 C D Fig. 9. Selected keyframes from our generated dance video set to the Bumblebee song. The choreography pattern follows reference dance extracted from Youtube video [GIq7ZgmxE2w], from 15.5s to 45.5s: A-B-A-B-C-D-C-D-A-B-A-B-C-D-C-D-E-F-G-G-G-G-H-H-H-H-G-G-G-G-H-H. reflect the true motion complexity between poses. For example, in Fig. 10, the average flow magnitude from the left one to the right one is ğ¹ (ğ¼ğ‘¢, ğ¼ğ‘£) = 36.81, which appears moderate due to incorrect correspondences between opposite sides. In reality, this motion is difficult for the video model to synthesize. Fig. 10. Failures. While average flow magnitude from the left one to the right one ğ¹ (ğ¼ğ‘¢, ğ¼ğ‘£ ) = 36.81 is not large, the motion is hard to synthesize for video model."
        },
        {
            "title": "6 DISCUSSIONS\n6.1 Comparison with Anthropomorphic Animation",
            "content": "Another potential relevant line of work for generating animal dance videos is character animation, which animates an input image using pose sequence, typically extracted from human motion videos. However, transferring human poses to animals remains an open challenge due to the need to solve complex correspondence problems across different body morphologies and modalities, let alone dealing with long and diverse pose sequences as in dance. As an example, we experimented with generating such videos using Animate-X [Tan et al. 2024]: we extract pose sequences from real dance video and use GPT-4o to generate an input image of marmot that mimics the human pose from the first frame, providing better start point for animation. As shown in Figure 11, the results looks blurry and the animated marmot taking on human-like body shapes. Fig. 11. Example results using character animation method. Top row: pose sequence extracted from real dance video (Youtube shorts, [jkRIIH42Vo8]). Bottom row: animation results from applying the pose sequence to the leftmost image using Animate-X [Tan et al. 2024]."
        },
        {
            "title": "6.2 Limitations",
            "content": "We use an offline video diffusion model to generate short motion segments between keyframes, for example, 0.5s for 120 BPM song. While the results are often visually coherent, , the motion can sometimes appear unrealistic; animals may appear to slide or morph between poses rather than moving in physically plausible way. This reflects the current limitations of video diffusion models in producing naturalistic motion for articulated subjects. However, with continued advances in large-scale video diffusion models, we are optimistic that these issues can be addressed in the future."
        },
        {
            "title": "6.3 Future Work",
            "content": "To generate more advanced and musically aligned dances, two directions can be explored: (1) dance motion realism: the motions generated by the video diffusion model may not always reflect plausible or expressive dance motion. Incorporating priors that favor 8 Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher-Shlizerman, and Steven M. Seitz A E E (a) Selected keyframe walk path set to Uptown Funk song. Choreography pattern extracted from Youtube video [U9Zj1BaH01c], from 16.0s to 36.0s: A-A-A-AA-A-A-A-B-B-C-D-E-E-E-E. A E (b) Selected keyframes from our generated dance video set to the APT. song. The choreography pattern follows reference dance extracted from Youtube video [DJz1zlm73HI], from 11.0s to 37.0s: A-A-A-A-A-A-B-C-A-A-A-A-A-A-B-C-D-D-D-D-E-E-E-E-D-D-D-D-E-E-E-E . D (c) Selected keyframes from our generated dance video set to the Cant Stop Feeling song. The choreography pattern follows reference dance extracted from Youtube video [xyMBnn3dzdU], from 59.0s to 79.0s: A-A-B-C-D-D-D-D-E-E-E-E-F-F-F-F-A-A-B-C-D-D-D-D-E-E-E-E-F-F-F-F. Fig. 12. Dance results. Keyframe pairs are labeled by the choreography pattern label. The sequence should be read from left to right across the top row, followed by left to right across the bottom row. See supplementary for the full dance video with music. How Animals Dance (When Youre Not Looking) 9 Conference on Computer Vision and Pattern Recognition. 1021910228. Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. 2024. World-Grounded Human Motion Recovery via GravityView Coordinates. In SIGGRAPH Asia Conference Proceedings. Guofei Sun, Yongkang Wong, Zhiyong Cheng, Mohan Kankanhalli, Weidong Geng, and Xiangdong Li. 2020. Deepdance: music-to-dance motion choreography with adversarial learning. IEEE Transactions on Multimedia 23 (2020), 497509. Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. 2024. Animate-x: Universal character image animation with enhanced motion representation. arXiv preprint arXiv:2410.10306 (2024). Jonathan Tseng, Rodrigo Castellon, and Karen Liu. 2023. Edge: Editable dance generation from music. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 448458. Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, and Chunhua Shen. 2024a. Framer: Interactive frame interpolation. arXiv preprint arXiv:2410.18978 (2024). Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, and Steven Seitz. 2024b. Generative inbetweening: Adapting image-tovideo models for keyframe interpolation. arXiv preprint arXiv:2408.15239 (2024). Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. Mingao Zhang, Changhong Liu, Yong Chen, Zhenchun Lei, and Mingwen Wang. 2022. Music-to-dance generation with multiple conformer. In Proceedings of the 2022 International Conference on Multimedia Retrieval. 3438. natural, dance-like movement could improve alignment with musical context. (2) style compatibility: although our method follows the choreography pattern, it does not consider musical style. Modeling genre-specific movement characteristics could enhance the stylistic coherence of generated dances."
        },
        {
            "title": "6.4 Conclusion",
            "content": "We present novel keyframe-based paradigm for generating musicsynchronized, choreography-aware animal dance videos. Our work opens up exciting opportunities for creative and fun applications of dancing animals in entertainment and social media. Acknowledgments. We thank Tong He and Baback Elmieh for helpful discussions and feedback. This work was supported by the UW Reality Lab and Google."
        },
        {
            "title": "REFERENCES",
            "content": "Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-Chen Guo, Weidong Zhang, and Shi-Min Hu. 2021. Choreomaster: choreography-oriented music-driven dance synthesis. ACM Transactions on Graphics (TOG) 40, 4 (2021), 113. Abe Davis and Maneesh Agrawala. 2018. Visual rhythm and beat. ACM Transactions on Graphics (TOG) 37, 4 (2018), 111. Li Hu. 2024. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 81538163. Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. 2025. Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance. arXiv preprint arXiv:2502.06145 (2025). Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, and Daxin Jiang. 2020. Dance revolution: Long-term dance generation with music via curriculum learning. In International conference on learning representations. Joanne Jao Keehn, John Iversen, Irena Schulz, and Aniruddh Patel. 2019. Spontaneity and diversity of movement to music are not uniquely human. Current Biology 29, 13 (2019), R621R622. Jae Woo Kim, Hesham Fouad, and James Hahn. 2006. Making Them Dance.. In AAAI Fall Symposium: Aurally Informed Performance, Vol. 2. 2. Tae-hoon Kim, Sang Il Park, and Sung Yong Shin. 2003. Rhythmic-motion synthesis based on motion-beat analysis. ACM Transactions on Graphics (TOG) 22, 3 (2003), 392401. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr DollÃ¡r, and Ross Girshick. 2023. Segment Anything. arXiv:2304.02643 (2023). Nhat Le, Tuong Do, Khoa Do, Hien Nguyen, Erman Tjiputra, Quang Tran, and Anh Nguyen. 2023. Controllable group choreography using contrastive diffusion. ACM Transactions on Graphics (TOG) 42, 6 (2023), 114. Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, MingHsuan Yang, and Jan Kautz. 2019. Dancing to music. Advances in neural information processing systems 32 (2019). Juheon Lee, Seohyun Kim, and Kyogu Lee. 2018. Listen to dance: Music-driven choreography generation using autoregressive encoder-decoder network. arXiv preprint arXiv:1811.00818 (2018). Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. 2021. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF international conference on computer vision. 1340113412. Adriano ManfrÃ¨, Ignazio Infantino, Filippo Vella, and Salvatore Gaglio. 2016. An automatic system for humanoid dance creation. Biologically Inspired Cognitive Architectures 15 (2016), 19. Ferda Ofli, Yasemin Demir, YÃ¼cel Yemez, Engin Erzin, Murat Tekalp, Koray BalcÄ±, Ä°dil KÄ±zoÄŸlu, Lale Akarun, Cristian Canton-Ferrer, JoÃ«lle Tilmanne, et al. 2008. An audio-driven dancing avatar. Journal on Multimodal User Interfaces 2 (2008), 93103. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body from Single Image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 1097510985. Qiaosong Qi, Le Zhuo, Aixi Zhang, Yue Liao, Fei Fang, Si Liu, and Shuicheng Yan. 2023. Diffdance: Cascaded human motion diffusion model for dance generation. In Proceedings of the 31st ACM International Conference on Multimedia. 13741382. Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. 2023. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF"
        }
    ],
    "affiliations": [
        "UC Berkeley, USA",
        "University of Washington, USA"
    ]
}