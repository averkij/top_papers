{
    "paper_title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
    "authors": [
        "Yuyang Li",
        "Yinghan Chen",
        "Zihang Zhao",
        "Puhao Li",
        "Tengyu Liu",
        "Siyuan Huang",
        "Yixin Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation."
        },
        {
            "title": "Start",
            "content": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation"
        },
        {
            "title": "Yuyang Li",
            "content": ", Yinghan Chen , Zihang Zhao , Puhao Li , Tengyu Liu , Siyuan Huang , Yixin Zhu https://tacthru.yuyang.li 1 5 2 0 2 0 ] . [ 1 1 5 8 9 0 . 2 1 5 2 : r Fig. 1: Learning multimodal robot manipulation with simultaneous tactile-visual perception. TacThru enables clear visual perception and robust marker tracking via transparent elastomer and keyline markers (left), providing rich multimodal signals for learning manipulation policies. TacThru-UMI demonstrates efficacy across fine-grained and contact-rich tasks requiring precise multimodal coordination (right). AbstractRobotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-Through-Skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning This work is supported in part by the National Science and Technology Innovation 2030 Major Program (2025ZD0219400), the National Natural Science Foundation of China (62376009), the State Key Lab of General AI at Peking University, the PKU-BingJi Joint Laboratory for Artificial Intelligence, and the National Comprehensive Experimental Base for Governance of Intelligent Society, Wuhan East Lake High-Tech Development Zone. We thank Lei Yan (LeapZenith AI Research), Shengyu Guo (PKU), Yu Liu (THU), and Leiyao Cui (PKU) for their assistance. Yuyang Li and Yinghan Chen contributed equally. Corresponding and emails: yixin.zhu@pku.edu.cn, huangsiyuan@ucla.edu, liutengyu@bigai.ai. Yuyang Li, Yinghan Chen, Zihang Zhao, and Yixin Zhu are with the Institute for Artificial Intelligence, Peking University. Yixin Zhu is also with the School of Psychological and Cognitive Sciences, Peking University. Yuyang Li, Yinghan Chen, Zihang Zhao, and Yixin Zhu are also with the Beijing Key Lab of Behavior and Mental Health, Peking University. Yuyang Li, Tengyu Liu, and Siyuan Huang are with the Beijing Institute for General Artificial Intelligence. All authors are with the State Key Lab for General Artificial Intelligence. Yixin Zhu is also with the Embodied Intelligence Lab, PKU-Wuhan Institute for Artificial Intelligence. Yinghan Chen is also with the Department of Computer Science and Technology, University of Cambridge. system integrates these signals through Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation. Index TermsManipulation, tactile sensing, proximity sensing, imitation learning I. INTRODUCTION ANIPULATION requires comprehensive environmental perception that spans the pre-contact to post-contact phases [1, 2]. Current sensing modalities each address different aspects but have complementary limitations. Vision provides rich global context, but frequently fails during manipulation due to occlusions from the robots own end-effector or objects, precisely when precise control is most critical [3]. Visionbased Tactile Sensors (VBTSs) [47] excel in providing highfidelity contact information [3, 4, 8, 9], but offer no information during the crucial pre-contact approach phase and provide only local, sometimes sparse signals [10]. Dedicated proximity sensors partially bridge this pre-contact gap [1113], but lack the precision and rich information content of vision or tactile modalities, limiting their utility for fine-grained manipulation. See-Through-Skin (STS) sensors have emerged as promising solution integrating tactile and visual sensing [10, 14 27]. Adapting VBTS designs, they replace opaque reflective coatings with semi-opaque or transparent alternatives, allowing embedded cameras to see through the skin for both tactile 2 signals (e.g., contact, force distributions) and visual signals (e.g., object appearance, proximity). Recent work has demonstrated their effectiveness in object perception [22, 25, 26], slip detection [15], grasping [17, 22, 28], in-hand manipulation [10], and articulated object manipulation [18, 24]. Despite this progress, three fundamental limitations prevent the broader adoption of STS sensors. Most current designs require the switch between tactile and visual modalities through illumination control [23, 24] or movable components [29, 30], preventing simultaneous multimodal perception and requiring additional control logic. Tactile markers essential for shear force measurement become difficult to track against noisy, unpredictable external backgrounds where contrast diminishes, limiting applications in open environments [14]. Finally, existing STS applications rely primarily on hand-crafted controllers, with integration into modern data-driven manipulation pipelines that remain largely unexplored. We introduce TacThru, an STS sensor that addresses these limitations, featuring specialized design choices: (i) fully transparent elastomer that enables clear visual perception, (ii) persistent illumination that eliminates mode switching, (iii) novel keyline markers that maintain visibility against any background, and (iv) an efficient tracking algorithm processing marker deviations at 6.08 ms per frame. This design enables truly simultaneous tactile-visual perception while remaining compatible with the standard VBTS fabrication pipeline for seamless integration into existing systems. We further develop TacThru-UMI, integrating TacThru into an imitation learning framework based on Universal Manipulation Interface (UMI) [31]. With both visual and tactile modalities provided, Transformer-based Diffusion Policy learns to properly attend to these rich signals for manipulation control. We evaluate the system on five diverse real-world tasks, including pick-and-place, sorting, and insertion  (Fig. 1)  , where TacThru provides persistent environment, object, and contact perception. Our policies achieve an average success rate of 85.5%, representing 54.3% and 29.0% relative improvements over baselines of vision only (55.4%) and alternating tactile-visual (66.3%), respectively. Our contributions include: (i) TacThru, novel STS sensor that enables efficient, robust, simultaneous tactilevisual perception; (ii) TacThru-UMI, an imitation learning system with design compatible with UMI for data collection, processing, and policy deployment; and (iii) comprehensive experimental validation demonstrating how TacThrus simultaneous multimodal perception enables superior fine-grained and contact-rich manipulation. STS Sensors: II. RELATED WORK Conventional VBTSs like GelSight [4] and 9D-Tact [5] provide high-resolution tactile sensing for contact measurement [4, 5, 7, 32], state estimation [8], and manipulation [3, 3335], but require physical contact, limiting pre-contact perception. STS sensors address this limitation by integrating tactile and visual perception [14, 1825], typically replacing opaque coatings with semi-transparent alternatives [18, 23, 24]. Existing implementations include alternating internal illumination for modal switching [18, 19, 22, 23], mechanically movable components [27, 29], and switchabletransparency films [20, 21]. Advanced designs incorporate fluorescent markers with UV lighting [19, 25], stereo depth perception [23, 28], and lens arrays [25]. In contrast, our design achieves truly simultaneous tactile-visual perception through fully transparent elastomer and persistent illumination, eliminating the need for modal switching. Further designs include novel keyline markers with an efficient marker tracking algorithm for robust tactile perception. Multimodal Perception for Manipulation: The combination of tactile and visual modalities addresses the inherent limitations of individual sensing modes: tactile feedback provides precise contact information but remains local and sparse [10], while vision offers wider spatial context but fails during occlusion or contact. This multimodal approach improves object pose estimation [22, 26, 28], slip detection [15], material recognition [13], in-hand manipulation [10], and visual servoing [18, 22, 24, 27], allowing unified sensor solutions for object localization, approach, and interaction. However, previous research focused predominantly on hand-crafted taskspecific controllers rather than leveraging modern generalpurpose learning frameworks [31, 36]. Learning with Multimodal Sensing: Modern learningbased policies encode multimodal sensory streams into unified representations or tokens. Visual data processing employs Convolutional Neuron Network (CNN) or Transformer encoders [3741], while tactile integration varies by siglow-dimensional vectors of piezoelecnal characteristics: tric arrays utilize Multi-Layer Perceptrons (MLPs) [42 44], while high-resolution VBTS images leverage specialized encoders or foundation models like T3 [45, 46]. To reduce learning complexity, many approaches use processed tactile signalscontact depth, marker displacementsrather than raw images [4749]. Despite these advances, the integration of STS sensors into learning frameworks remains largely unexplored [24]. We address this gap by developing TacThru-UMI, which integrates TacThru into an imitation learning framework and demonstrates that simultaneous multimodal perception enables superior performance in fine-grained and contact-rich manipulation. III. THE TA CTH SENSOR The TacThru sensor achieves simultaneous tactile-visual perception through three design principles: fully transparent elastomer for clear visual access, persistent illumination to eliminate modal switching, and robust keyline markers for reliable tactile tracking. TacThru maintains compatibility with the standard VBTS fabrication pipeline, differing primarily in elastomer material to allow easy adoption. A. Transparent Elastomer with Persistent Illumination Existing STS designs preserve traditional VBTS depth estimation by semi-transparent coatings [20, 21, 23], switchable illumination [23], or movable components [29, 30], but compromise visual clarity and require complex switching mechanisms. We adopted fully transparent elastomer with LI et al.: TACTHRU 3 ˆ 40 mm elastomer. Design constraints include: camera focus distance that enables both marker detection and visual perception, marker size that balances detectability with minimal visual occlusion, and spacing that exceeds the maximum marker deviation to prevent tracking ambiguity. C. Robust and Efficient Marker Tracking Although keyline markers solve the fundamental detectability problem, environmental noise and large contact deformations still cause tracking failures. We employ Kalman filtering for robust marker tracking, modeling each markers position xt R2 at time using its known initial position x0 from fabrication. The state transition and measurement follow: xt Atxt1 ` wt, zt Htxt ` vt (1) I2q are process and I2q and vt p0, σ2 where wt p0, σ2 measurement noise. We adopt the random walk model with At I2 and direct position observation with Ht I2. The filter maintains posterior estimates ˆxt and covariances Pt : following standard prediction and update steps [51]. The final deviations of the markers are computed as xt : ˆxt ˆx0. pxt ˆxtqpxt ˆxtqT Fig. 2: Fabrication of the TacThru sensor and integration into the TacThru-UMI system. (a) The keyline marker elastomer is fabricated by sequentially spraying inner (black) and outer (white) markers on transparent elastomer using laser-cut masks. (b) The TacThru sensor features an extended linkage that serves as gripper fingers. (c) The TacThru-UMI platform includes robot endeffector (left) and data collector (right) that share identical body and finger designs, with the fingers actuated by an Inspire LAS30-021D servo electric cylinder with maximum opening width of 72 mm. trading traditional depth perpersistent LED illumination, ception for continuous visual access. This design recognizes that geometric contact information, while valuable, is often less critical than multimodal coordination for manipulation tasks [18, 24, 50]. Nevertheless, contact detection remains available through two mechanisms: light reflection changes at contact interfaces [18, 22, 50] and marker divergence from elastomer deformation [24]. B. Keyline Markers Elastomer transparency creates two marker detection challenges: (i) degraded detectability: conventional solid markers become invisible against matching backgrounds; (ii) noisy detections: environmental objects with blob-like appearances generate false detections. To address the detectability issue, we introduce keyline markers: two concentric circles with contrasting colors, ensuring the inner edge remains visible as detectable keyline regardless of background. As shown in Fig. 2ab, the markers are fabricated by sequentially painting inner circles (black, rin 0.6 mm) and outer circles (white, rout 1.0 mm) using laser-cut masks. We deploy Nm 64 markers with 3.5 mm spacing on our 40 mm Measurement acquisition proceeds through grayscale conversion, intensity thresholding (pixels ă τ set to black) to reveal keylines while removing environmental edges, and blob detection resulting in candidates Zt : tˆztu BlobDetpItq. Since Zt also contains environmental false detections, we filter through distance-based data association, matching each marker to its nearest detection: zt arg min zPZt }z ˆxt1}. (2) D. Evaluation on Marker Tracking We evaluate the proposed marker tracking algorithm by comparing keyline and solid marker designs. Fig. 3a shows two TacThru sensors mounted on the UMI data collector [31], one with keyline markers and one with solid black markers. We collect 8 trajectories (1628 frames per sensor), grasping plastic bottle with complex black-and-white text that challenges marker detection. The TacThru sensor positions are swapped for half of the trajectories to ensure fair exposure to environmental conditions on both sides, and the bottle is rotated 90 between collections, creating marker overlays with various areas around the bottle. Fig. 3b illustrates the detection challenge: solid markers become invisible against black backgrounds, while keyline markers remain detectable, although environmental noise requires filtering. We further quantitatively compare three tracking approaches: Solid: Solid markers with standard blob detection. Keyline: Keyline markers with standard blob detection, matching detected blobs to nearest initial positions. Keyline, filtered: Keyline markers with Kalman filtering (Sec. III-C), using σw 0.1, σv 0.025. Fig. 3c presents quantitative results that include detected markers per frame and processing time. Solid markers suffer Fig. 3: Keyline marker design and filtering enable robust tracking. (a) Evaluation setup compares two sensor types (keyline vs. solid markers) during bottle grasping tasks. (b) The TacThru sensor view comparison shows that keyline markers (left) remain distinct against complex backgrounds, while solid markers (right) become invisible. (c) Quantitative results demonstrate our filtered keyline method achieves stable tracking of all 64 markers while keeping efficiency (6.08 ms processing time per frame), while solid markers suffer missed detections and unfiltered keyline detection produces false positives (count > 64). from severe missed detections, resulting in incomplete tactile information. Unfiltered keyline detection reports marker counts exceeding the actual Nm 64 due to environmental false detections (orange dashed box). Our filtering strategy achieves the design goal: persistent tracking of all markers with minimal computational overhead, eliminating false detections, while maintaining 6.08 ms processing suitable for highfrequency perception (e.g., 120 Hz) and real-time operation. IV. LEARNING MANIPULATION WITH TA CTH U-UMI Although previous studies show invaluable applications of STS sensors with task-specific controllers [10, 1420, 22, 23, 2527], we apply TacThru to robotic manipulation based on imitation-learning, using its simultaneous tactile-visual perception for fine-grained and contact-rich tasks. We develop the TacThru-UMI imitation learning framework, extending the UMI [31] and the Diffusion Policy [36] with multimodal tactile-visual observations. Evaluations across multiple tasks highlight the sensors multimodal perception capabilities, providing close-up visual observation of environments and objects along with tactile feedback, enabling more precise manipulation than traditional single-modality approaches. A. System Setup Data collector: The TacThru-UMI data collector adapts the UMI [31] design, replacing standard fingers with STS sensors on extended linkages (Fig. 2c). The TacThru sensors are connected to computer via USB to stream realtime images during demonstrations and policy inference. Fig. 4: Diffusion policy architecture for TacThru-UMI. Multimodal observationswrist-camera RGB images, sensor RGB images, detected marker deviations, and proprioceptionare encoded into tokens and augmented with positional and modality-specific embeddings. These tokens condition Transformer-based diffusion policy that denoises Gaussian noise into action chunks for robot execution. The example shows how the policy leverages the TacThrus closeup view to align the cap and mount during the InsertCap task. Robot end-effector: Although the UMI end-effector design allows for inference with various parallel grippers such as the Panda Hand, we design low-cost gripper that directly mirrors the data collectors design to minimize embodiment gaps (Fig. 2c). Both platforms share identical body and finger des, with fingers actuated by an Inspire LAS30-021D servoelectric cylinder ( $280). B. Data Collection and Processing Our pipeline extends UMI [31] with tactile modalities. To improve robustness during contact-rich manipulation, we replaced the original SLAM-based pose tracking with an HTC Vive Tracker, which maintains stable tracking even when SLAM fails due to visual occlusion. All data streamswrist camera, tactile sensors, and proprioceptionare synchronized to wrist-camera timestamps and stored in Zarr format for efficient training access. C. Policy Learning and Inference We employ Diffusion Policy [36] with the Transformer architecture [52] to learn mappings from multimodal observations to robot actions, allowing dynamic attention across simultaneously provided visual, tactile, and proprioceptive signals. itnobs itnobs sut1 wut , sensor frames It At timestep t, the observations include wrist-camera frames It : tI : tI , marker deviations xt : txi,j, 1, . . . , Nmut1 , and propriitnobs oception st : tsiut1 containing the pose of the endeffector and the width of in relative coordinates [31]. Visual observations are encoded using DINOv2 [53]: ViT-Base for wrist cameras and ViT-Small for TacThru frames (both 14 ˆ 14 patch size). Despite the domain shift from markers and elastomer, DINOv2 effectively the gripper itnobs LI et al.: TACTHRU handles tactile sensor imagery. Marker deviations and proprioception use dedicated MLPs. Each modality receives learnable embeddings (zw, zs, zx, zp) for transformer distinguishability: DINOwpIq ` zwI It ( DINOspIq ` zsI It MLPxpxq ` zxx xt MLPppsq ` zps st zw zs zx zp (3) (5) (4) (6) ( ( ( , , , . Concatenated tokens with positional embeddings condition the diffusion policy πθ, which denoises Gaussian samples into action chunks [54] taiut`Ta1 πθpazw, zs, zx, zpq. Each action ai includes relative end-effector pose and gripper width targets. During execution, the first La actions (La ď Ta) are sent to the robot controller for Cartesian space servoing. it V. EXPERIMENTS We evaluated TacThru-UMI across five manipulation tasks spanning pick-and-place, sorting, and insertion scenarios. These tasks systematically assess different sensing modalities: tactile information (contact events and shear forces), visual perception (object and environment observation), and their simultaneous operation. Our key finding is that TacThrus multimodal feedback enriches perception of object appearance, state, position, and the environment throughout the entire manipulation process even before contact or when handling extremely thin and soft objects. This allows policies to leverage detailed environmental cues for enhanced fine-grained and contact-rich manipulation while maintaining inference efficiency. A. Task Settings Fig. 5 illustrates the experimental setups. Each column shows task with the object to manipulate (first row), wristcamera view during demonstration (second row), and the corresponding TacThru and VBTS images (third row). PickBottle: bottle and bowl are placed randomly. The robot must grasp the bottle and place it in the bowl. This basic pick-and-place task validates TacThru-UMIs effectiveness of imitation learning and real-world inference. PullTissue: tissue pack is placed randomly, and the robot must grasp and fully extract single tissue. Standard tactile sensors struggle to detect contact with thin, soft paper, making the visual modality of STS crucial for this task. SortBolt: One of three M1225 bolts is placed between fingertips. The robot must grasp the bolt and place it in the corresponding bowl. Bolts vary in head shape (button head for A, socket head for and C) and color (black for and C, silver for B). The global wrist-camera view cannot distinguish these small bolts, while traditional tactile sensing cannot separate geometrically identical but differently colored bolts. TacThru allows for distinguishing both color and shape through STS perception. HangScissors: The scissors are placed between the fingertips; the robot must grasp and hang them on hook. This task, borrowed from Liu et al. [42] to benchmark VBTSbased UMI systems, requires tactile feedback to distinguish successful hanging (triggering gripper release) from missed attempts (triggering retry). InsertCap: bottle cap is placed on bottle. The robot must grasp the cap and insert it onto white mount, which requires millimeter-level precision alignment. Although insertion tasks are typically based on tactile signals [48, 49], TacThru enables direct visual servoing for the alignment of the cap. When the view is occluded (e.g., due to biased grasping), tactile signals from marker displacement provide robust fallback guidance. B. Experimental Setup Setup and policy variants: For fair comparison, we equip the gripper with TacThru on one finger and GelSighttype sensor on the other, ensuring identical training trajectories across modality variants. We collect 62147 demonstrations per task  (Fig. 6)  and train four policy variants that naturally form ablations and comparisons: TT-M: Full TacThru with image frames (It s) and marker deviations (xt). TT: TacThru image frames only (It s); markers are visible in the frames but not explicitly extracted and provided (ablation w/o marker deviations). GS-M: GelSight sensor frames (rectified by the idle image for revealing only intensity changes) and marker deviations (tactile baseline). Wrist: Wrist camera only (It All policies include wrist camera (It (st) as the base input. w) (vision-only baseline). w) and proprioception 1, nobs Training and evaluation: We use observation horizons 1, nobs nobs 2, predict Ta 16 action steps, and execute 6-step window (steps 3-8). Each policy is trained for 150 epochs using the AdamW optimizer [55] with learning rate of 3 ˆ 104 and one-cycle scheduling. We evaluated the checkpoints after 20 runs per task (24 for SortBolt) with randomized initialization that matched the data-collection distribution. C. Results Basic manipulation: All variants achieve near-perfect success in PickBottle (Fig. 7a), with success rates exceeding 95%. This validates that TacThru-UMIs architecture effectively learns manipulation policies across different sensing modalities. The consistent performance demonstrates that our diffusion policy framework successfully integrates diverse input types without degrading basic manipulation capabilities. This establishes foundation for evaluating more challenging scenarios in which specific sensing modalities become critical. PullTissue reveals the fundamental limitations of conventional tactile sensing. Traditional contact-based sensors require sufficient normal and shear forces to generate detectable signals, but tissues exert minimal pressure and deform rather than resist. TacThru overcomes this through direct visual observation of the interfinger workspace, providing continuous feedback on tissue position and deformation. When tissue slippage (Fig. 7b) occursoften due to insufficient grip force on the delicate Thin-and-soft object perception: 6 Fig. 5: Task demonstrations across five manipulation scenarios. (a) PickBottle: basic pick-and-place, (b) PullTissue: thin-and-soft object manipulation, (c) SortBolt: visual discrimination, (d) HangScissors: tactile discrimination, (e) InsertCap: multimodal fusion. Top: Initial object configurations. Middle: Wrist-camera view progression during demonstration. Bottom: Corresponding TacThru (top) and GelSight sensor (bottom) observations, illustrating distinct sensing modalities and information content. materialTacThru immediately detects the displacement and triggers corrective re-grasping actions. The wrist camera fails due to insufficient resolution at its typical mounting distance (15 cm), whereas the GS-M shows near-zero success because soft tissues cannot generate the contact patterns needed for reliable tactile inference. Visual discrimination: SortBolt (Fig. 7c) demonstrates TacThrus superior visual discrimination capabilities. The small M1225 bolts (12mm head diameter) present significant challenges: wrist cameras cannot resolve geometric details at manipulation distances, while identical bolt shapes make purely tactile discrimination impossible. TacThrus close-proximity view (2-3mm from objects) captures fine geometric features and subtle color differences that remain invisible to distant cameras. Fig. 8 provides quantitative evidence through DINOv2 embedding analysis: TacThru produces clearly separated feature clusters with inter-cluster distances exceeding 0.8, while GelSight embeddings for bolts and overlap with similarity scores above 0.9. This embedding separation directly correlates with the 85% vs. 45% success rates observed between TT-M and GS-M policies. Fig. 6: Quantitative results across manipulation tasks and sensing modalities. Success rates for four policy variants: TT-M (TacThru with markers), TT (TacThru only), GS-M (GelSight with markers), and Wrist (vision-only). Each task evaluates specific sensing capabilities: basic manipulation (PickBottle), thin-and-soft object manipulation (PullTissue), visual discrimination (SortBolt), tactile discrimination (HangScissors), and multimodal fusion (InsertCap). Error bars show standard deviation across evaluation runs. The rightmost column presents overall performance averages. Tactile discrimination: HangScissors (Fig. 7d) exemplifies scenarios where visual observation alone cannot determine task completion. The 2D wrist camera cannot reliably detect whether scissor handles have successfully engaged the hook, due to depth perception and occlusion. Physical contact patterns captured through marker displacement provide unambiguous confirmation: successful engagement creates characteristic force patterns as the scissors settle into position, while failed attempts show continued downward forces. Both TT-M and GS-M leverage these tactile signals effectively, achieving success rates of 80%+ compared to 35% for visiononly baselines. The explicit marker-based feedback enables precise timing of the gripper releasea critical decision point that determines task success. Multimodal fusion: InsertCap (Fig. 7e-f) presents the unique advantage of TacThru: simultaneous access to visual and tactile information enables the selection of adaptive strategies. When the cap-mount interface remains visible, the policy employs vision-based servoing, directly aligning visual features for precise insertion (Fig. 7e). However, when grasping occludes the view or lighting conditions degrade visual signals, the policy seamlessly returns to tactile-based insertion, using marker displacement patterns to detect contact and guide alignment (Fig. 7f). This adaptive behavior emerges naturally from the training process without explicit strategy programming, demonstrating the policys ability to weight modalities based on their reliability in different contexts. The 90% success rate reflects this robust dual-strategy approach, significantly outperforming single-modality baselines that lack this adaptive capability. D. Discussions Adaptive multimodal strategies: Our experimental results reveal three key insights on tactilevisual perception for learning multimodal robot manipulation. Policies trained with TacThru naturally learn to weight sensing modalities based on context reliability, as demonstrated in InsertCap, where the same policy employs vision-based alignment when visible and tactile-based insertion when occluded. This adaptive behavior emerges without explicit programming, suggesting fundamental advantages of simultaneous over sequential sensing approaches."
        },
        {
            "title": "Overcoming",
            "content": "conventional tactile limitations:"
        },
        {
            "title": "TacThru uniquely handles",
            "content": "scenarios where traditional LI et al.: TACTHRU 7 Fig. 7: Qualitative policy rollouts demonstrating manipulation execution. Each column (a-f) shows the temporal progression of task execution from top to bottom, with three synchronized views: third-person perspective (left), wrist camera (center), and TacThru close-up (right) with tracked marker deviations overlaid (4 magnified for visibility). Colored annotations highlight key manipulation phases and sensing feedback. See supplementary video for additional rollouts across all policy variants. forces for conventional contact-based sensors fail. Thin objects such as tissue generate tactile detection, but insufficient remain clearly observable through direct optical monitoring. This expands the range of manipulable objects beyond current tactile sensing capabilities. Practical deployment viability: Despite significant domain differencestransparent elastomer, marker overlays, contact deformationsstandard pre-trained visual encoders prove sufficient for robust policy learning. This finding substantially reduces implementation barriers and suggests that TacThru can be integrated into existing vision-based manipulation pipelines with minimal modification. VI. CONCLUSIONS We introduce TacThru, an STS tactile sensor that provides simultaneous tactile and visual perception through transparent elastomer, persistent illumination, and keyline marker tracking. Integrated within our TacThru-UMI imitation learning platform, it demonstrates superior performance across various manipulation tasks, addressing the fundamental limitations of existing tactile sensing while maintaining compatibility with standard vision pipelines. The accessible design of TacThru and TacThru-UMI platform position this work as practical enhancement for the manipulation research community. Future directions include large-scale data collection combined with synthetic tactile simulation [35, 48, 49] to support pre-training of specialized encoders, and exploration of complex dexterous tasks that fully leverage TacThrus simultaneous sensing capabilities. REFERENCES [1] A. Billard and D. Kragic, Trends and challenges in robot manipulation, Science, vol. 364, no. 6446, p. eaat8414, 2019. 8 Fig. 8: Bolt sorting performance analysis across sensing modalities. (a) Confusion matrices showing placement accuracy for each policy variant. Rows indicate ground-truth bolt type, columns show predicted placement (bowls A-C or Out for misses). TacThrubased policies (TT-M, TT) successfully distinguish all bolt types, while GS-M confuses geometrically identical bolts and C. (b) tSNE visualization of DINOv2 CLS token embeddings from sensor images. TacThru produces clearly separated clusters for all bolts, while GelSight embeddings for bolts and overlap, explaining the observed classification failures. [2] Y. Zhu, T. Gao, L. Fan, S. Huang, M. Edmonds, H. Liu, F. Gao, C. Zhang, S. Qi, Y. N. Wu, et al., Dark, beyond deep: paradigm shift to cognitive ai with humanlike common sense, Engineering, vol. 6, no. 3, pp. 310345, 2020. [3] Z. Zhao, Y. Li, W. Li, Z. Qi, L. Ruan, Y. Zhu, and K. Althoefer, TacMan: Tactile-informed prior-free manipulation of articulated objects, T-RO, vol. 41, pp. 538557, 2025. [4] W. Yuan, S. Dong, and E. H. Adelson, Gelsight: High-resolution robot tactile sensors for estimating geometry and force, Sensors, vol. 17, no. 12, p. 2762, 2017. [5] C. Lin, H. Zhang, J. Xu, L. Wu, and H. Xu, 9dtact: compact vision-based tactile sensor for accurate 3d shape reconstruction and generalizable 6d force estimation, RA-L, vol. 9, no. 2, pp. 923930, 2023. [6] B. Ward-Cherrier, N. Pestell, L. Cramphorn, B. Winstone, M. E. Giannaccini, J. Rossiter, and N. F. Lepora, The tactip family: Soft optical tactile sensors with 3d-printed biomimetic morphologies, Soft Robotics, vol. 5, no. 2, pp. 216227, 2018. [7] W. Li, Z. Zhao, L. Cui, W. Zhang, H. Liu, L.-A. Li, and Y. Zhu, Minitac: An ultra-compact 8 mm vision-based tactile sensor for enhanced palpation in robot-assisted minimally invasive surgery, RA-L, vol. 9, no. 12, pp. 1117011177, 2024. [8] Z. Zhao, W. Li, Y. Li, T. Liu, B. Li, M. Wang, K. Du, H. Liu, Y. Zhu, Q. Wang, et al., Embedding high-resolution touch across robotic hands enables adaptive human-like grasping, Nature Machine Intelligence, pp. 112, 2025. [9] Z. Zhao, Z. Qi, Y. Li, L. Cui, Z. Han, L. Ruan, and Y. Zhu, Tacmanturbo: Proactive tactile control for robust and efficient articulated object manipulation, arXiv preprint arXiv:2508.02204, 2025. [10] P. Lancaster, P. Gyawali, C. Mavrogiannis, S. S. Srinivasa, and J. R. Smith, Optical proximity sensing for pose estimation during in-hand manipulation, in IROS, 2022. [11] L.-T. Jiang and J. R. Smith, Seashell effect pretouch sensing for robotic grasping., in ICRA, 2012. [12] S. E. Navarro, S. Mühlbacher-Karrer, H. Alagi, H. Zangl, K. Koyama, B. Hein, C. Duriez, and J. R. Smith, Proximity perception in humancentered robotics: survey on sensing systems and applications, T-RO, vol. 38, no. 3, pp. 15991620, 2021. [13] C. Fang, D. Wang, D. Song, and J. Zou, Toward fingertip non-contact material recognition and near-distance ranging for robotic grasping, in ICRA, 2019. [14] A. Yamaguchi and C. G. Atkeson, Combining finger vision and optical tactile sensing: Reducing and handling errors while cutting vegetables, in International Conference on Humanoid Robots (Humanoids), 2016. [15] A. Yamaguchi and C. G. Atkeson, Implementing tactile behaviors using fingervision, in International Conference on Humanoid Robotics (Humanoids), 2017. [16] R. Patel, R. Cox, and N. Correll, Integrated proximity, contact and force sensing using elastomer-embedded commodity proximity sensors, Autonomous Robots, vol. 42, no. 7, pp. 14431458, 2018. [17] P. E. Lancaster, J. R. Smith, and S. S. Srinivasa, Improved proximity, contact, and force sensing via optimization of elastomer-air interface geometry, in ICRA, 2019. [18] F. R. Hogan, J.-F. Tremblay, B. H. Baghi, M. Jenkin, K. Siddiqi, and G. Dudek, Finger-sts: Combined proximity and tactile sensing for robotic manipulation, RA-L, vol. 7, no. 4, pp. 1086510872, 2022. [19] Q. Wang, Y. Du, and M. Y. Wang, Spectac: visual-tactile dualmodality sensor using uv illumination, in ICRA, 2022. [20] Q. K. Luu, D. Q. Nguyen, N. H. Nguyen, and V. A. Ho, Soft robotic link with controllable transparency for vision-based tactile and proximity sensing, in International Conference on Soft Robotics (RoboSoft), 2023. [21] Q. K. Luu, D. Q. Nguyen, N. H. Nguyen, N. P. Dam, and V. A. Ho, Vision-based proximity and tactile sensing for robot arms: Design, perception, and control, T-RO, vol. 41, pp. 50005019, 2025. [22] S. Athar, G. Patel, Z. Xu, Q. Qiu, and Y. She, Vistac: Toward unified multimodal sensing finger for robotic manipulation, IEEE Sensors Journal, vol. 23, no. 20, pp. 2544025450, 2023. [23] E. Roberge, G. Fornes, and J.-P. Roberge, Stereotac: novel visuotactile sensor that combines tactile sensing with 3d vision, RA-L, vol. 8, no. 10, pp. 62916298, 2023. [24] T. Ablett, O. Limoyo, A. Sigal, A. Jilani, J. Kelly, K. Siddiqi, F. Hogan, and G. Dudek, Multimodal and force-matched imitation learning with see-through visuotactile sensor, T-RO, vol. 41, pp. 946959, 2025. [25] L. Luo, B. Zhang, Z. Peng, Y. K. Cheung, G. Zhang, Z. Li, M. Y. Wang, and H. Yu, Compdvision: Combining near-field 3d visual and tactile sensing using compact compound-eye imaging system, in IROS, 2024. [26] W. Fan, H. Li, W. Si, S. Luo, N. Lepora, and D. Zhang, Vitactip: Design and verification of novel biomimetic physical vision-tactile fusion sensor, in ICRA, 2024. [27] Y. Dong, J. Ren, Z. Liu, Z. Peng, Z. Yuan, N. Zhang, and G. Gu, Lookto-touch: vision-enhanced proximity and tactile sensor for distance and geometry perception in robotic manipulation, in IROS Workshop, 2025. [28] K. Shimonomura, H. Nakashima, and K. Nozu, Robotic grasp control with high-resolution combined tactile and proximity sensing, in ICRA, 2016. [29] J. Xu, L. Wu, C. Lin, D. Zhao, and H. Xu, Dtactive: vision-based tactile sensor with active surface, arXiv preprint arXiv:2410.08337, 2024. [30] D. Yueshi, J. Ren, Z. Liu, Z. Peng, Z. Yuan, N. Zhang, and G. Gu, Look-to-touch: vision-enhanced proximity and tactile sensor for distance and geometry perception in robotic manipulation, in IROS Workshop, 2025. [31] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song, Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots, in RSS, 2024. [32] S. Suresh, H. Qi, T. Wu, T. Fan, L. Pineda, M. Lambeta, J. Malik, M. Kalakrishnan, R. Calandra, M. Kaess, et al., Neuralfeels with neural fields: Visuotactile perception for in-hand manipulation, Science Robotics, vol. 9, no. 96, p. eadl0628, 2024. [33] R. Li, R. Platt, W. Yuan, A. Ten Pas, N. Roscup, M. A. Srinivasan, and E. Adelson, Localization and manipulation of small parts using gelsight tactile sensing, in IROS, 2014. [34] J. Lloyd and N. F. Lepora, Pose-and-shear-based tactile servoing, IJRR, vol. 43, no. 7, pp. 10241055, 2024. [35] Y. Li, W. Du, C. Yu, P. Li, Z. Zhao, T. Liu, C. Jiang, Y. Zhu, and S. Huang, Taccel: Scaling up vision-based tactile robotics via highperformance gpu simulation, in NeurIPS, 2025. [36] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song, Diffusion policy: Visuomotor policy learning via action diffusion, IJRR, vol. 44, no. 10-11, pp. 16841704, 2025. [37] S. Levine, C. Finn, T. Darrell, and P. Abbeel, End-to-end training of deep visuomotor policies, JMLR, vol. 17, no. 39, pp. 140, 2016. [38] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al., Scalable deep reinforcement learning for vision-based robotic manipulation, in CoRL, 2018. [39] Y. Wu, W. Yan, T. Kurutach, L. Pinto, and P. Abbeel, Learning to manipulate deformable objects without demonstrations, in RSS, 2020. [40] M. Shridhar, L. Manuelli, and D. Fox, Perceiver-actor: multi-task transformer for robotic manipulation, in CoRL, 2023. [41] A. Goyal, J. Xu, Y. Guo, V. Blukis, Y.-W. Chao, and D. Fox, Rvt: Robotic view transformer for 3d object manipulation, in CoRL, 2023. [42] F. Liu, C. Li, Y. Qin, A. Shaw, J. Xu, P. Abbeel, and R. Chen, Vitamin: Learning contact-rich tasks through robot-free visuo-tactile manipulation interface, arXiv preprint arXiv:2504.06156, 2025. [43] L. Heng, H. Geng, K. Zhang, P. Abbeel, and J. Malik, Vitacformer: Learning cross-modal representation for visuo-tactile dexterous manipulation, arXiv preprint arXiv:2506.15953, 2025. [44] Z. Zhao, S. Haldar, J. Cui, L. Pinto, and R. Bhirangi, Touch begins where vision ends: Generalizable policies for contact-rich manipulation, in RSS Workshop, 2025. [45] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transferable visual models from natural language supervision, in ICML, 2021. [46] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks, RA-L, vol. 7, no. 3, pp. 73277334, 2022. LI et al.: TACTHRU 9 [47] N. Funk, C. Chen, T. Schneider, G. Chalvatzaki, R. Calandra, and J. Peters, On the importance of tactile sensing for imitation learning: case study on robotic match lighting, arXiv preprint arXiv:2504.13618, 2025. [48] W. Chen, J. Xu, F. Xiang, X. Yuan, H. Su, and R. Chen, Generalpurpose sim2real protocol for learning contact-rich manipulation with marker-based visuotactile sensors, T-RO, vol. 40, pp. 15091526, 2024. [49] J. Xu, S. Kim, T. Chen, A. R. Garcia, P. Agrawal, W. Matusik, and S. Sueda, Efficient tactile simulation with differentiability for robotic manipulation, in CoRL, 2023. [50] S. Zhang, Y. Sun, J. Shan, Z. Chen, F. Sun, Y. Yang, and B. Fang, Tirgel: visuo-tactile sensor with total internal reflection mechanism for external observation and contact detection, RA-L, vol. 8, no. 10, pp. 63076314, 2023. [51] R. E. Kalman, new approach to linear filtering and prediction problems, Journal of Basic Engineering, vol. 82, no. 1, pp. 3545, 1960. [52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, in NeurIPS, 2017. [53] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al., Dinov2: Learning robust visual features without supervision, Transactions on Machine Learning Research (TMLR), 2023. [54] T. Zhao, V. Kumar, S. Levine, and C. Finn, Learning fine-grained bimanual manipulation with low-cost hardware, in RSS, 2023. [55] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in ICLR, 2019."
        }
    ],
    "affiliations": [
        "National Comprehensive Experimental Base for Governance of Intelligent Society, Wuhan East Lake High-Tech Development Zone",
        "PKU-BingJi Joint Laboratory for Artificial Intelligence",
        "Peking University",
        "State Key Lab of General AI at Peking University"
    ]
}