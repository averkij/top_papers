{
    "paper_title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "authors": [
        "Yu Xu",
        "Hongbin Yan",
        "Juan Cao",
        "Yiji Cheng",
        "Tiankai Hang",
        "Runze He",
        "Zijin Yin",
        "Shiyi Zhang",
        "Yuxin Zhang",
        "Jintao Li",
        "Chunyu Wang",
        "Qinglin Lu",
        "Tong-Yee Lee",
        "Fan Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations."
        },
        {
            "title": "Start",
            "content": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts Yu Xu1,2 Hongbin Yan1 Runze He2 Zijin Yin2 Chunyu Wang2 Qinglin Lu2 Juan Cao1 Yiji Cheng2 Shiyi Zhang2 Yuxin Zhang1 Tiankai Hang2 Jintao Li1 Tong-Yee Lee Fan Tang1 6 2 0 2 2 1 ] . [ 1 1 8 8 8 0 . 1 0 6 2 : r 1University of Chinese Academy of Sciences 2Tencent Hunyuan 3National Cheng-Kung University https://yuci-gpt.github.io/TAG-MoE/ Figure 1. We present TAG-MoE, by injecting high-level task semantic intent into the local routing decisions of the MoE gating network, we enabling the diffusion transformer model to handle diverse generative tasks."
        },
        {
            "title": "Abstract",
            "content": "Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-ofExperts (MoE) paradigm is promising solution, its gating networks remain task-agnostic, operating based on local Work done during internship at Tencent Hunyuan. Project leader. Corresponding author. tfan.108@gmail.com features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose novel framework to inject semantic intent into MoE routing. We introduce Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the tasks high-level semantics. This regularization evolves the gating network from task-agnostic executor to dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, 1 and our analysis shows that experts naturally develop clear and semantically correlated specializations. 1. Introduction The field of visual synthesis is rapidly converging toward unified image generation and editing models [7, 15, 16], frameworks designed to consolidate disparate image manipulation tasksfrom subject customization and style transfer to high-fidelity inpainting and instruction-based editinginto single, robust system with the help of largescale, dense Diffusion Transformers (DiT). While promising efficiency, this unification is critically bottlenecked by severe task interference. The shared parameter space must simultaneously execute inherently contradictory objectives: local editing demands precise content preservation, while subject-driven generation requires expressive diversity and novel synthesis. This fundamental conflict forces the network toward mediocre compromise solution, preventing the necessary representational specialization and ultimately degrading performance across the spectrum of user intents. To overcome the scalability [11] and capacity [45] limits of dense DiT, the sparse Mixture-of-Experts (MoE) paradigm is adopted to dramatically expand model capacity with manageable inference costs of large-scale generative models. However, these efforts mainly focus on single, general-purpose image generation tasks, and have not (and do not need to) account for the complex task diversity within the unified generation framework. Applying standard MoE to the heterogeneous unified domain introduces critical architectural failure: the task-agnostic nature of conventional gating networks. Standard routers rely solely on local token features, remaining entirely oblivious to the high-level, global task intent (e.g., identity preservation or style modification). This profound information gap between the local gate and the global objective leads to spontaneous, inefficient expert specialization, fundamentally failing to structurally disentangle multi-task interference. How to inject the high-level, global task semantics into the local MoE routing mechanism to enable task-aware specialization remains an open challenge. In this study, we propose TAG-MoE, task-aware gating network for unified image generation and editing. First, to provide structured unified task representation, we introduce hierarchical task semantic annotation scheme, by decomposing specific generative task into multi-faceted descriptor, capturing the operational scope (e.g., local/- global editing), the semantic type (e.g., attribute/action editing), and essential preservation constraints (e.g., identity/style preservation). Such structured representations provides the necessary rich supervisory signal previously missing. Furthermore, we propose novel training framework founded on the principle that semantically similar generation tasks evokes similar expert usage patterns. To enforce this, we design an innovative predictive alignment regularization to correlate the high-level task semantic intent with the underlying routing decisions. Such regularization serves as bridge to compel the models internal routing strategy to become predictive of the tasks macro-semantics, injecting global semantic intent into the local routing mechanism, leading the gating network to evolve from taskagnostic executor into an aware, intelligent dispatch center. Experiments on unified image generation benchmarks ICE-Bench, image editing benchmark EmuEdit and GEdit, subject-driven generation benchmark DreamBench++ and OmniContext indicate that our method achieves the best overall performance. Our primary contributions are summarized as follows: 1. We propose novel task-aware sparse MoE framework and successfully apply it to Diffusion Transformer-based unified image generation and editing tasks. 2. We introduce hierarchical task semantic annotation scheme and corresponding predictive alignment regularization that, together, effectively resolve the taskagnostic of the MoE gate by aligning its routing strategy with the tasks semantic intent. 3. By successfully mitigating task interference, our model achieves SOTA overall performance against open-source baselines across five comprehensive benchmarks. 2. Related Work 2.1. Unified Image Generation and Editing Recent efforts in unified image generation aim to build single models capable of handling broad range of image manipulation tasks, moving beyond specialized, task-specific approaches [3840]. Early methods treat the problem as sequence-to-sequence task, concatenating text, source, and target image tokens for large transformers [12, 14, 37]. Subsequent works refine input representations and architectures to improve multimodal conditioning. Methods such as UniReal [5] and RealGeneral [18] introduces trainable index, subject, and condition embeddings to enhance alignment, while Flux-Kontext [16] employes 3D rotary positional encodings to distinguish source from target images. Architectural innovations include dual-branch models that decouple subject and background processing [17], channelwise concatenation to preserve contextual signals [20], and the integration of auxiliary MLLMs or transformers for improved scene understanding [10, 29, 35], albeit with increased complexity and compute. Despite these advances, current unified models overlook central challenge: the inherent conflict between the objectives of different image-to-image tasks. Editing tasks (e.g., style transfer, object removal) require precise regional preservation while modifying others, whereas customization tasks (e.g., subject-driven generation) demand strong identity consistency across new contexts. Without explicitly modeling these distinctand often competingrequirements, existing approaches struggle to adaptively serve the full spectrum of user intents, limiting their practical robustness and generalization. 2.2. Image Generation with Mixture of Experts The MoE paradigm increases model capacity by routing inputs to specialized sub-networks, or experts, avoiding proportional rise in per-sample computation. Its success in large language models has motivated adoption in visual generation: pioneering works such as DiTMoE [11], and scaled variants like HunyuanImage-3.0 [3] and Dense2MoE [45], show that sparse expert architectures can enhance the expressiveness of diffusion transformers. Extending MoE to image editing, ICEdit [43] integrates LoRA-based MoE modules into attention blocks. However, purely data-driven routing is fundamentally limited: taskagnostic routers cannot resolve conflicts between heterogeneous tasks (e.g., editing vs. customization), and the restricted capacity of LoRA experts hampers learning multitask behaviors. Our approach overcomes these limitations by introducing task-aware expert routing. We condition the gating mechanism on learnable embeddings corresponding to specific task categories, enabling dynamic selection of the most relevant experts. This mitigates inter-task conflicts, promotes effective specialization, and achieves superior performance across diverse image-to-image tasks while maintaining the efficiency of the MoE framework. 3. Method Our unified framework  (Fig. 2)  employs Multimodal Diffusion Transformer (MM-DiT) with MoE layers for efficient, dynamic task handling (3.1). We introduce hierarchical task semantic annotation (3.2) and novel semanticaligned router (3.3). This router guides the MoEs specialization by aligning its routing decisions with these explicit task semantics in an interpretable manner . 3.1. MoE-based Multimodal Diffusion Transformer Building upon an MM-DiT architecture, our approach processes diverse inputs within unified token sequence framework. To interpret user instructions, we employ powerful pre-trained Multimodal Large Language Model (MLLM) to encode the input text ctext into sequence of text embeddings C. Separately, pre-trained VAE encoder maps both the conditional image Ic and the target image I0 into latent representations, zc and z0. During training, Gaussian noise is sampled and added to the z0 to produce noisy version zt. Both zc and zt are then patchified into sequences of visual tokens. Finally, the complete input to our MM-DiT is single sequence formed by concatenating the text embeddings C, the image tokens from zc, the image tokens from the noisy target latent zt, and timestep embedding [24]. We replace the feed-forward networks (FFNs) of the image stream in diffusion transformer blocks with MoE layers. This leverages sparse activation to significantly increase model capacity at fixed activation parameter, enabling superior performance over dense models with comparable budget. We only implement MoE layers in the later transformer blocks as high-level semantic synthesis in these deeper layers benefits most from the increased capacity [9, 26]. The MoE layer consists of set of expert networks and gating network G. The gating network maps each input token to probability distribution over the experts, thereby determining their top selections {E1, . . . , EN }. The output is weighted sum of the activated experts outputs: MoE(x) = (cid:88) EiT (x) G(x)i Ei(x). (1) This MoE-enhanced architecture is trained end-to-end using Flow Matching objective. 3.2. Hierarchical Task Semantic Annotation To train unified model that supports broad range of generation and editing tasks, structured representation of task semantics is essential. single coarse label (e.g., edit) cannot capture user intent. For example, change the background to beach and make the person smile are both edits but require fundamentally different behaviors and preservation constraints. To address this, we introduce threetier annotation scheme that provides each training instance (source image, instruction, target image) with rich semantic descriptor: Scope - the tasks operational nature and spatial extent (e.g., global editing, local editing, content customization). Type the semantic category of the manipulation (e.g., object editing, style transfer, attribute editing). Preservation the invariants that must remain unchanged (e.g., identity, background, structure preservation). An automated pipeline utilizing Qwen-VL [1] is established to analyze training triplets. It involves providing definitions of three-tier system and instructing Qwen-VL to output atomic tags. The rule set is continuously refined to maintain consistency and semantic quality. For instance, the task Make the person in the photo wear sunglasses would be annotated with tags such as Scope: local editing; Type: object editing; Preservation: identity preservation, background preservation, style preservation. This rich set of atomic tags forms the basis for our semantic representation. Inference Stage. This hierarchical annotation scheme is exclusively used for training. During the inference stage, these ground-truth tags are no longer required. Instead, as 3 Figure 2. Pipeline of our method. TAG-MoE consists of: (1) MM-DiT with MoE layers; (2) Hierarchical Task Semantic Annotation that labels training data with atomic task descriptors; (3) novel Semantic-Aligned Router explicitly aligns MoE routing behavior with task semantics through Predictive Alignment Regularization. lightweight pre-processing step, we pass the users raw instruction ctext and the source image Ic to VLM (e.g., Qwen-VL [1]). The VLM performs instruction rewriting, analyzing the image and text to generate more detailed, descriptive prompt. This enriched prompt is then encoded as the text embedding and fed into the MM-DiT. 3.3. Semantic-Aligned Gating Network We design novel semantic-aligned gating network to force the models internal routing strategy (encoded as routing signature g) to predict the tasks macroscopic semantics (encoded as semantic embedding s). This predictive alignment serves as bridge, connecting local routing decisions with global task intent. Our mechanism comprises three key components: (1) construction of the global semantic embedding s; (2) construction of the aggregated routing signature g; and (3) the predictive alignment loss Lalign. 3.3.1. Global Semantic Embedding Based on the hierarchical task semantic annotation described in 3.2, we first define global vocabulary containing all atomic tags (e.g., local editing, identity preservation). We instantiate learnable tag embedding matrix Wtag RKD for this vocabulary, where is the models hidden dimension. For given training sample, its associated tags form set Tp (e.g., Tp = {local editing, face preservation}). To convert this variable-sized set Tp into fixed-dimension vector s, we first retrieve the corresponding embedding vector et = Wtag[index(t)] for each tag Tp, and then aggregate them via element-wise summation. This constructs the global semantic embedding s, which represents the macro-level semantic ground truth: (cid:88) = Wtag[index(t)]. (2) tTp This vector RD is permutation-invariant, meaning the It order of tags does not affect the final representation. serves as the structured supervisory signal for our subsequent alignment loss. 3.3.2. Aggregated Routing Signature Correspondingly, we require vector to represent the internal routing strategy the model actually employs for the current sample. The gating network (see 3.1) generates routing scores Sl,t RN for each token in each of the MoE layers, where is the number of experts. To obtain single vector representing the expert usage pattern for the entire sample, we design an aggregated routing signature g. First, we average the routing scores across all MoE layers to get per-token average score St = 1 l=1 Sl,t. Next, we apply mean pooling over the sequence (token) dimension to get the final signature (cid:80)L 4 RN : ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 St ="
        },
        {
            "title": "1\nT · L",
            "content": "T (cid:88) (cid:88) t=1 l=1 Sl,t. (3) This vector encodes which experts are activated on average to process the sample, capturing its de facto internal routing policy. 3.3.3. Predictive Alignment Regularization We now have two vectors: RD, representing what the task should be, and RN , representing what the model actually do. To align them, we introduce lightweight prediction head Hpred (a two-layer MLP), to project the aggregated routing signature from the expert space RN into the semantic space RD, yielding predicted semantic embedding ˆs = Hpred(g). We force the routing strategy to predict the task semantics by minimizing the cosine similarity loss between ˆs and s. This is our Predictive Alignment Loss Lalign: Our proprietary in-house dataset is meticulously constructed using multi-stage pipeline to cover wide spectrum of specialized tasks. First, we source pristine images from large-scale public datasets. Next, we employ large language models (e.g., GPT-4o [22]) to generate vast array of diverse editing and generation instructions for these images. To obtain high-quality target images, we utilize combination of specialist and generalist models: for instance, specialist models like ControlNet [42] are used for Control generation tasks, while powerful generalist models (e.g., Flux-Kontext [16], Qwen-Edit [34], and SeedEdit [32]) are employed for broad range of edits. Following the methodology of UniReal [5], we also process video frames to create dynamic editing datasets (e.g., for pose/view changes). Finally, to enhance robustness and quality, we systematically augment the data by constructing corresponding inverse tasks and instructions (e.g., pairing object addition with object removal), which significantly improves generative fidelity. 4. Experiments Lalign = 1 sim(ˆs, s) = 1 ˆs ˆss . (4) 4.1. Implenentation Details Minimizing Lalign trains the parameters of Hpred and, more importantly, backpropagates the gradient through to the gating networks of all MoE layers. This compels to evolve from task-agnostic executor into semantic-aware scheduler: it must learn to route tokens intelligently, such that the resulting aggregate signature contains sufficient information to predict the global task s. 3.3.4. Overall Training Objective Our proposed Lalign is an auxiliary loss that complements the models primary objective. The final overall loss Ltotal is weighted sum of the main generation loss (e.g., Lf low), the standard MoE load balancing loss Llbl, and our semantic alignment loss Lalign: Ltotal = Lf low + λlblLlbl + λalignLalign, (5) where λlbl and λalign are hyperparameters that balance the contribution of each loss term. 3.4. Dataset Construction Our model is trained on large-scale, diverse dataset comprising both publicly available and proprietary in-house data, totaling over 11 million samples. This hybrid approach ensures broad coverage across the unified task space. The public portion (2.2M samples) is compiled from established benchmarks, including InstructP2P [2], UltraEdit [44], and OmniEdit [33] for universal instructive editing, supplemented by VTON-HD [6] for virtual try-on tasks and Ominicontrol [30] for subject driven generation. Our model is based on Qwen-Image T2I model [34], we integrate the MoE layers by replacing the standard FFNs of the image stream in the final 10 layers of our diffusion transformer. Each MoE layer consists of four experts, where each expert possesses an architecture identical to the original FFN it replaces. The gating network is implemented as two-layer MLP, and we employ top-1 routing strategy. 4.2. Experiments Settings Baselines. We compare our method against three categories of SOTA baselines. (1) Unified generation and editing methods for diverse image-to-image tasks, including ACE++ [20], Flux.1 Kontext [16], BAGEL [10], OmniGen2 [35], Qwen-Edit [35] and DreamOmni2 [36]. We also include comparisons against product-level, closed-source models (e.g. GPT-4o [22] and Gemini-2.5-flash (aka. Nanobanana) [13], to contextualize our performance. However, our primary quantitative evaluation and main claims are benchmarked against open-source baselines. (2) Specialized zero-shot instruction-based editing methods, including InstructPix2Pix [2], EmuEdit [27], MagicBrush [41], UltraEdit [44], ICEdit [23], and Step1X-Edit [19]. (3) Specialized zero-shot subject-driven generation methods, including DreamO [21], OminiControl [30] and UNO [? ]. Evaluation benchmarks. To comprehensively assess our model in the unified image generation and editing setting, we adopt ICE-Bench [23] as our primary benchmark, as it is specifically designed for unified models and spans both diverse editing tasks and subject-driven generation. For 5 more fine-grained evaluation, we further include specialized benchmarks: EmuEdit-Bench [27] and GEdit-Bench [19] for detailed editing analysis, and DreamBench++ [25] together with OmniContext [35] to evaluate subject-driven generation performance. Metrics. We employ comprehensive set of metrics to evaluate both visual quality and task correctness. Aesthetic quality is assessed using SigLip-based predictor. Consistency with the source image is measured via CLIP-src (for editing) and CLIP-ref (for subject-driven generation), while text alignment is captured by CLIP-cap. For editing evaluation, we further use Qwen2-VL-72B [31] to determine whether the instruction is correctly executed based on the source image, instruction, and output image, yielding the vllmqa score. For subject-driven tasks, we assess three key preservation dimensions: facial identity (Face-ref, using the buffalo model from InsightFace App [8]), subject similarity (DINO-ref, via DINO [4]), and style fidelity (Style-ref, via CSD [28]). All metrics not originally within the [-1, 1] range are normalized. For every metric reported, higher values indicate better performance. In the tables, the best results are highlighted in bold, and the second-best results are underlined. 4.3. Quantitative Comparison Unified generation evaluation. We report the main results on ICE-Bench in Tab. 1. Our method achieves the highest scores among all open-source baselines across three key metrics: aesthetic quality, CLIP-cap, and vllmqa. Notably, our CLIP-cap score not only surpasses all opensource competitors but also exceeds closed-source, productlevel models such as GPT-4o and Gemini-2.5-flash, indicating stronger alignment with user instructions across diverse generation and editing tasks. Although some baselines exhibit high source fidelity (e.g., DreamOmni2 on CLIP-src), our model attains more favorable overall balance by excelling in instruction adherence and semantic alignment. We further present per-category breakdown over 26 task types on ICE-Bench, visualized in the radar charts in Fig. 4. Our model achieves state-of-the-art performance in the vast majority of categories, demonstrating robust and well-balanced capability. DreamOmni2s high referencegeneration scores largely stem from copy-paste behavior on source subjects, which artificially inflates similarity metrics. Image editing evaluation We further evaluate our model against specialized zero-shot editing baselines on EmuEditbench [27] and GEdit-bench [19], with results shown in Tab. 2. (Note: Since EmuEdit is not open-source and only provides pre-generated outputs on its own benchmark, its performance on GEdit-bench is unavailable.) Although our Method Aes. CLIP-src CLIP-cap CLIP-ref vllmqa 0.637 0.263 5.219 ACE++ Kontext 0.629 0.274 5.165 0.699 0.276 4.757 BAGEL OmniGen2 0.787 0.279 5.238 0.774 0.279 5.358 Qwen-Edit 0.664 0.268 5.188 DreamOmni2 0.852 0.282 5.399 Ours 0.889 0.278 GPT-4o 5.801 0.847 0.281 Gemini-2.5-flash 5.571 0.713 0.728 0.687 0.728 0.671 0.739 0.732 0.693 0. 0.851 0.863 0.863 0.855 0.840 0.866 0.857 0.823 0.879 Table 1. Comparison results for unified tasks on ICE-Bench [23] test sets. Open-source models are in the first block and closesource produce-level models are in the second block. model does not achieve top-1 performance on every metric, it clearly leads on the most important indicator vllmqa achieving the highest scores on both benchmarks. This is particularly noteworthy because, unlike static CLIP similarity, vllmqa uses powerful VLLM to evaluate the correctness of the executed instruction, offering more intelligent and reliable measure of editing success. Our strong results on this metric underscore the models advanced instructionfollowing capability. Method InsP2P EmuEdit MagicBrush UltraEdit ICEdit Step1X-Edit ACE++ Kontext BAGEL OmniGen2 Qwen-Edit DreamOmni2 Ours EmuEdit-bench GEdit-bench CLIP-src CLIP-cap vllmqa CLIP-src CLIP-cap vllmqa 0.3191 0.8589 - 0.8854 0.3783 0.8552 0.4605 0.8625 0.4145 0.8912 0.8158 0.8845 0.0559 0.8367 0.9091 0.7303 0.7961 0.8565 0.6546 0.8932 0.875 0.8832 0.6349 0.9035 0.8854 0.9054 0.3192 - 0.3146 0.3323 0.3283 0.346 0.2518 0.3419 0.3470 0.3373 0.3522 0.3401 0.3485 0.2919 0.3098 0.2951 0.3075 0.3026 0.3119 0.2385 0.3093 0.3129 0.3087 0.3159 0.3096 0.3152 0.2507 0.6253 0.4573 0.3609 0.3609 0.7893 0.0606 0.741 0.7989 0.5978 0.9174 0.6997 0.9284 0.8604 - 0.8068 0.8459 0.9007 0.8967 0.8160 0.9190 0.8727 0.8940 0.9104 0.9229 0. Table 2. Comparison of instruction-based editing methods on EmuEdit-bench and GEdit-bench with multiple metrics. Method DreamO Ominicontrol UNO ACE++ Kontext BAGEL OmniGen2 Qwen-Edit DreamOmni2 Ours DreamBench++ OmniContext CLIP-cap CLIP-ref DINO-ref Face-ref Style-ref CLIP-cap CLIP-ref DINO-ref Face-ref Style-ref 0.2899 0.296 0.2832 0.2791 0.2829 0.3036 0.298 0.3009 0.2731 0.3011 0.7792 0.7642 0.776 0.7759 0.819 0.7338 0.7712 0.7595 0.8062 0.7906 0.7518 0.6991 0.7429 0.732 0.7919 0.6998 0.752 0.7187 0.8008 0.7613 0.335 0.0579 0.2572 0.1636 0.3429 0.0487 0.1213 0.2188 0.2344 0.3678 0.5355 0.3876 0.4328 0.5306 0.5655 0.5065 0.5167 0.5095 0.5364 0.5679 0.2986 0.3067 0.2962 0.2832 0.2962 0.2914 0.3056 0.3152 0.2848 0. 0.7302 0.7009 0.7106 0.7183 0.765 0.7188 0.7544 0.7115 0.7733 0.7297 0.7075 0.6126 0.6961 0.6932 0.7494 0.7094 0.7289 0.6797 0.7611 0.7628 0.4522 - 0.3665 0.1789 0.5596 0.1264 0.3919 0.3019 0.5111 0.5607 - - - - - - - - - - Table 3. Comparison of subject-driven generation methods on DreamBench++ and OmniContext with multiple metrics. Figure 3. Qualitative comparison on diverse tasks. Our model successfully resolves complex task conflicts where baselines fail. Figure 4. Comprehensive scores on different image editing and generation tasks. Subject driven evaluation. We evaluate our models finegrained preservation ability against specialized subjectdriven generation methods on DreamBench++ and OmniContext, with results shown in Tab. 3. We focus on metrics that measure subject, identity, and style fidelity (noting that OmniContext does not include style-related tasks). The results indicate strong preservation performance: our model achieves SOTA Face-ref scores on both benchmarks and the highest Style-ref score on DreamBench++. In addition, we obtain the top DINO-ref score on OmniContext and remain highly competitive on DreamBench++. These findings demonstrate that our unified model can match or surpass specialized models, effectively mitigating the typical tension between subject fidelity and generative diversity. 4.4. Qualitative Comparison Qualitative comparison with unified baselines. As demonstrated in the preceding qualitative comparison  (Fig. 3)  , our method consistently surpasses SOTA baselines Figure 5. Compare with specialized image editing models and subject-driven generation models. in complex tasks characterized by interfering intents. These unified models typically fail to resolve inherent task conflicts, resulting in critical failures such as copy-paste artifacts in subject-driven generation, stylistic dissonance during inpainting, or incomplete execution in compositional editing. Our approach successfully navigates these challenges by utilizing the Predictive Alignment Regularization. This mechanism effectively decouples and routes conflicting sub-tasks (e.g. local semantic edits versus global style preservation) to specialized experts, thereby mitigating the core task interference that plagues unified models. 7 Qualitative comparison with specialized baselines. We further present comprehensive comparison against specialized image editing methods (InstructPix2Pix [2], MagicBrush [41], UltraEdit [44], ICEdit [43]) and subjectdriven models (DreamO [21], OmniControl [30], UNO [? ]) in Fig. 5. For image editing, specialized baselines struggle with significant structural or geometric changes. As shown in the silver car case, they fail to execute the complex motion of turning around, resulting in minor texture changes; similarly, they fail to synthesize the side view of the complex shelf structure. In contrast, our method accurately handles these 3D-aware edits, benefiting from the structural diversity and geometric awareness implicitly learned from subject-driven data. Conversely, in subjectdriven tasks, specialized models often compromise identity or instruction following. For the human subject, baselines either lose facial identity/clothing details (OmniControl) or fail to render the office context (UNO). For the toy subject requiring handstand, baselines generate incorrect upright poses. Our method, however, maintains robust identity while adhering to complex motion instructions. This enhanced fidelity is attributed to the high consistency derived from editing alignment data during unified training. Overall, our model effectively handles both task types by leveraging semantic-aligned routing. This mechanism assigns conflicting objectives to specialized experts, enabling cross-task benefits: generative diversity from subject data improves editing geometry, while fidelity constraints from editing data enhance identity preservation in generation. 4.5. Ablation Study Effectiveness of the MoE architecture. We compare our sparse MoE architecture to dense baseline of an equivalent activated parameter count. This dense model shows severe performance drop on ICE-Bench metrics (Tab.4) and slower convergence (Fig. 6 left). This validates that the sparse architecture is fundamentally more effective at mitigating the severe task interference inherent in the unified task space than computationally-equivalent dense model. this loss, Effect of predictive alignment regularization. We ablate the semantic-alignment loss by removing Lalign. Without the MoE gating network performs taskagnostic expert selection, receiving no semantic guidance from our hierarchical tags. As shown in Tab. 4, this variant exhibits substantial degradation across all major metrics. This finding is key: sparse MoE architecture alone is not sufficient. Lalign is what enables semantically guided routing, which is essential for mitigating task interference. Notably, the MoE w/o Lalign variant still surpasses the dense baseline, benefiting from the larger effective capacity of the sparse MoE structure, which allows exploration of richer solution space under the same computational budget. Figure 6. Left: Training loss curves of the dense and MoE architecture. Right: Token strategy in different generation tasks. xxxMethod Dense MoE w/o Lalign MoE w/ Lalign DINO-ref Face-ref Style-ref CLIP-src CLIP-cap vllmqa 0.637 0.677 0.847 0.3544 0.3779 0.4642 0.7196 0.7355 0. 0.5177 0.5251 0.5679 0.263 0.274 0.281 0.851 0.863 0.879 Table 4. Ablation study on dense model and predictive alignment regularization. Analysis of expert specialization. To provide direct evidence of our methods success, we visualize the inferencetime routing decisions and analyze the internal expert activation patterns. Our analysis is two-step process. First, we compute an Expert Utilization Rate for each MoE layer (shown as the heatmap in the middle of Fig. 6), which represents the percentage of total image tokens routed to each expert. utilization of 0% (blue) or 100% (red) indicates no specialization. We focus our analysis on layers exhibiting differentiated routing, where utilization is mixed (near white), as this is where functional specialization occurs. Second, for these active layers, we visualize the pertoken routing scores for each expert, reshaping them to the images spatial dimensions. In these token heatmaps, high score (blue) indicates that the corresponding image tokens are strongly routed to that specific expert. The results reveal clear, spatially-aware, and task-specific specialization. For Change Material and Change Color, the model activates distinct combinations of experts. Critically, the token heatmaps for these active experts show that computation is spatially concentrated on the backpacks pixels, precisely the region relevant to the edit. The non-relevant background tokens are correctly routed to other experts (or have nearzero activation for these experts). This analysis provides strong evidence that our model has learned sophisticated specialization that is both task-specific (using unique expert combinations for different tasks) and spatially-aware (experts learn to process semantically relevant image regions). This confirms our method successfully resolves task conflicts by dispatching them to distinct, specialized computational pathways. 4.6. User study We conducted user study with 65 participants on 50 cases from ICE-Bench [23]. Participants were asked to select the single best result according to three criteria: (1) Reference 8 Alignment (consistency with the source image), (2) Prompt Alignment (faithfulness to the textual instruction), and (3) Overall Preference (overall visual quality). In total, 350 sets were evaluated, and the aggregated results are shown in Fig. 7. The results reveal clear and consistent preference for our method, which achieved the highest selection rate across all three evaluation criteria. Figure 7. User study on reference alignment, prompt alignment and overall perference. 5. Limitations and Future Work key limitation is our frameworks lack of unified input understanding. Our model relies on pre-processed instructions (the intent) and cannot jointly reason over this intent and the visual content of the source image. This separation restricts tasks requiring integrated semantic and perceptual understanding. For instance, our model fails at contentbased reasoning (e.g., solving math problem in an image) because it understands the editing intent (e.g., scope, type) but not the contextual information in the pixels themselves. promising future direction is an end-to-end system incorporating multimodal reasoning engine to unify perceptual understanding (content), intent comprehension (command), and conceptual generation (reasoning). 6. Conclusion In this paper, we propose TAG-MoE, task-aware MoE framework for unified image generation and editing. We identify the task-agnostic routing as the core bottleneck for applying MoE to diverse, conflicting tasks. To address this, we introduce Hierarchical Task Semantic Annotation scheme and Predictive Alignment regularization to effectively injects global task intent into the local routing decisions, forcing the model to develop meaningful expert specialization. Our experiments demonstrate that TAGMoE significantly mitigates task interference, outperforming dense models and task-agnostic MoE baselines in both quantitative metrics and qualitative fidelity."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 4 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 5, 8 [3] Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. 3 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 6 [5] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1250112511, 2025. 2, 5 [6] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul try-on via Viton-hd: High-resolution virtual Choo. In Proceedings of the misalignment-aware normalization. IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021. [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 2 [8] deepinsight. Insightface. https : / / github . com / deepinsight/insightface, 2021. Accessed: 202511-04. 6 [9] DeepSeek-AI. Deepseek-v3 technical report, 2024. 3 [10] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, 5 [11] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformers to 16 billion parameters. arXiv preprint arXiv:2407.11633, 2024. 2, 3 [12] Tsu-Jui Fu, Yusu Qian, Chen Chen, Wenze Hu, Zhe Gan, and Yinfei Yang. Univg: generalist diffusion model for unified image generation and editing. arXiv preprint arXiv:2503.12652, 2025. [13] Google. Nano banana. Technical report, Google, 2025. 5 [14] Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chen-Wei Xie, Yu Liu, and Jingren Zhou. Ace: All-round creator and editor following instructions via diffusion transformer. In The Thirteenth International Conference on Learning Representations, 2025. 2 [15] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2 9 [16] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 2, 5 [17] Yaowei Li, Lingen Li, Zhaoyang Zhang, Xiaoyu Li, Guangzhi Wang, Hongxiang Li, Xiaodong Cun, Ying Shan, and Yuexian Zou. Blobctrl: unified and flexible framework for element-level image generation and editing. arXiv preprint arXiv:2503.13434, 2025. 2 [18] Yijing Lin, Mengqi Huang, Shuhan Zhuang, and Zhendong Mao. Realgeneral: Unifying visual generation via temporal in-context learning with video models. arXiv preprint arXiv:2503.10406, 2025. [19] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 5, 6 [20] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instructionbased image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 2, 5 [21] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. 5, 8 [22] OpenAI. Gpt-4o, 2025. 5 [23] Yulin Pan, Xiangteng He, Chaojie Mao, Zhen Han, Zeyinzi Jiang, Jingfeng Zhang, and Yu Liu. Ice-bench: unified and comprehensive benchmark for image creating and editing. arXiv preprint arXiv:2503.14482, 2025. 5, 6, 8 [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 3 [25] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. In The Thirteenth International Conference on Learning Representations, 2025. 6 [26] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power nextgeneration ai scale. In International conference on machine learning, pages 1833218346. PMLR, 2022. [27] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871 8879, 2024. 5, 6 [28] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. 6 [29] Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, et al. Query-kontext: An unified multimodal arXiv preprint model for image generation and editing. arXiv:2509.26641, 2025. 2 [30] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1494014950, 2025. 5, 8 [31] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 [32] Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025. [33] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024. 5 [34] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 5 [35] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 2, 5, 6 [36] Bin Xia, Bohao Peng, Yuechen Zhang, Junjia Huang, Jiyang Liu, Jingyao Li, Haoru Tan, Sitong Wu, Chengyao Wang, Yitong Wang, et al. Dreamomni2: Multimodal arXiv preprint instruction-based editing and generation. arXiv:2510.06679, 2025. 5 [37] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 2 [38] Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Xiaoyu Kong, Jintao Li, Oliver Deussen, and Tong-Yee Lee. Headrouter: training-free image editing framework for mmdits by adaptively routing attention heads. arXiv preprint arXiv:2411.15034, 2024. 2 [39] Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Oliver Deussen, Weiming Dong, Jintao Li, and Tong-Yee Lee. B4m: Breaking low-rank adapter for making content-style customization. ACM Transactions on Graphics, 44(2):117, 2025. [40] Yu Xu, Fan Tang, You Wu, Lin Gao, Oliver Deussen, Hongbin Yan, Jintao Li, Juan Cao, and Tong-Yee Lee. In-context brush: Zero-shot customized subject insertion with contextaware latent space manipulation. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers, pages 112, 2025. 2 [41] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 5, 8 [42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 5 [43] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 3, 8 [44] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 5, 8 [45] Youwei Zheng, Yuxi Ren, Xin Xia, Xuefeng Xiao, and Xiaohua Xie. Dense2moe: Restructuring diffusion transformer to moe for efficient text-to-image generation. arXiv preprint arXiv:2510.09094, 2025. 2,"
        }
    ],
    "affiliations": [
        "National Cheng-Kung University",
        "Tencent Hunyuan",
        "University of Chinese Academy of Sciences"
    ]
}