{
    "paper_title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
    "authors": [
        "Chaoyou Fu",
        "Haojia Lin",
        "Xiong Wang",
        "Yi-Fan Zhang",
        "Yunhang Shen",
        "Xiaoyu Liu",
        "Yangze Li",
        "Zuwei Long",
        "Heting Gao",
        "Ke Li",
        "Xiawu Zheng",
        "Rongrong Ji",
        "Xing Sun",
        "Caifeng Shan",
        "Ran He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 7 5 9 1 0 . 1 0 5 2 : r VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction Chaoyou Fu1,, Haojia Lin3, Xiong Wang2, Yi-Fan Zhang4, Yunhang Shen2 Xiaoyu Liu1, Yangze Li2, Zuwei Long2, Heting Gao2, Ke Li2 Xiawu Zheng3, Rongrong Ji3, Xing Sun2,, Caifeng Shan1, Ran He4 1NJU, 2Tencent Youtu Lab, 3XMU, 4CASIA Project Leader Corresponding Author Demo Video: Click YouTube Link Source Code: https://github.com/VITA-MLLM/VITA"
        },
        {
            "title": "Abstract",
            "content": "Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains significant challenge due to the fundamental modality differences. In this paper, we propose carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. The training and inference codes have been released at https://github.com/VITA-MLLM/VITA (1K+ Stars by now)."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in MLLMs [13, 31, 67, 10, 49, 61, 42, 17] have led to significant progress, particularly in the integration of visual and textual modalities. The introduction of visual information into LLMs has notably enhanced model capabilities across range of multimodal tasks. However, with the growing appeal of human-computer interaction, the role of the speech modality has become increasingly prominent, especially in the multimodal dialogue system. In such system, speech not only serves as key medium for information transmission but also greatly improves the naturalness and convenience of interactions. Consequently, integrating both visual and speech modalities to achieve high-performance multimodal interactions has emerged as critical research focus. The integration of vision and speech in MLLMs is not straightforward due to their inherently differences [40]. For example, visual data, such as images, convey spatial information, while speech data convey dynamic changes in time series. These fundamental differences pose challenges for simultaneous optimization of both modalities, often leading to conflicts during training. For instance, the inclusion of speech data may degrade performance on vision tasks, and vice versa. In addition, traditional speech-to-speech systems rely on separate modules for Automatic Speech Email: {bradyfu24,winfred.sun}@gmail.com Figure 1: VITA-1.5 enables near real-time vision and speech interaction via an end-to-end framework. It allows you to turn on the camera and have fluent speech conversation. Please see our demo video at this YouTube link. Recognition (ASR) and Text-to-Speech, which can increase latency and reduce coherence, limiting their practicality in real-time applications [44, 16, 63]. In this paper, we introduce VITA-1.5, multimodal LLM that integrates vision, language, and speech through carefully designed three-stage training methodology. The training strategy progressively incorporates vision and speech data, relieving modality conflicts while maintaining strong multimodal performance. In the first stage, we focus on vision-language by training visual adapters and finetuning the model with descriptive caption and visual QA data. This step establishes the models foundational visual capabilities, enabling robust image and video understanding. The second stage introduces audio input processing by training an audio encoder using speech-transcription paired data, followed by fine-tuning with speech QA data. This stage equips the model with the ability to understand and respond to audio inputs effectively. Finally, in the third stage, we train an audio decoder to enable end-to-end speech output, eliminating the need for external TTS modules. This allows VITA-1.5 to generate fluent speech replies, enhancing the naturalness and interactivity of multimodal dialogue systems. We have conducted extensive evaluations on various benchmarks related to image, video, and speech understanding, comparing the results with both open-source and proprietary models. VITA-1.5 demonstrates comparable perception and reasoning capabilities comparable to leading image/video based MLLMs, and shows significant improvements in the speech capability."
        },
        {
            "title": "2 Related Work",
            "content": "Recently, thanks to the rapid development of language models such as GPTs [41, 3], LLaMA [52, 53], Alpaca [48], Vicuna [12], and Mistral [24], researchers have successfully extended text comprehension to multimodal understanding/reasoning through techniques like multimodal alignment and instruction tuning. For example, models such as LLaVA [31], Qwen-VL [2], Cambrian-1 [51], Mini-Gemini [28], MiniCPM-V 2.5 [23], DeepSeek-VL [36], and SliME [66] have made significant advances in image perception and reasoning, while models like LongVA [65] and Video-LLaVA [29] have showcased the latest progress in video understanding. These models are increasingly capable of handling diverse data types, driving the continuous improvement of multimodal perception and understanding capabilities. However, compared to proprietary models that support multiple modalities, including audio, image, and text (e.g., GPT-4o [42] and Gemini-Pro 1.5 [50]), most open-source models have primarily 2 focused on image and text modalities [61]. Moreover, few open-source models have involved multimodal interaction capabilities, which is relatively unexplored area. While works like VITA1.0 [16] have made initial attempts to introduce speech for human-computer interaction, introducing additional speech data poses challenges to the models original multimodal abilities. Furthermore, speech generation typically relies on existing TTS systems, which often results in high latency, thus impacting user experience. In this paper, we present VITA-1.5 that leverages refined training strategies, excelling in perceiving data across four modalities (video, image, text, and audio), while also realizing near real-time vision and speech interaction."
        },
        {
            "title": "3 VITA-1.5",
            "content": "3.1 Model Architecture The overall architecture of VITA-1.5 is depicted in Fig. 2. The input side is the same as that of the VITA-1.0 version [16], that is, adopting the configuration of Multimodal Encoder-Adaptor-LLM. It combines the Vision/Audio Transformer and the Multi-Layer Connector with an LLM for joint training, aiming to enhance the unified understanding of vision, language, and audio. With respect to the output side, VITA-1.5 has its own end-to-end speech module, instead of using the external TTS model like the original VITA-1.0 version. 3.1.1 Visional Modality Visual Encoder. VITA-1.5 adopts InternViT-300M1 as the visual encoder, with an input image size of 448448 pixels, generating 256 visual tokens per image. For highresolution images, VITA-1.5 employs dynamic patching [10] strategy to capture local details, improving the accuracy of image understanding. Video Processing. Videos are treated as special type of multiple-image input. If the video length is shorter than 4 seconds, 4 frames are uniformly sampled; for videos between 4 and 16 seconds, one frame per second is sampled; for videos longer than 16 seconds, 16 frames are uniformly sampled. No dynamic patching is applied to video frames to avoid excessive visual tokens that could hinder processing efficiency. Figure 2: Overall Architecture of VITA-1.5. The input side consists of vision and audio encoders, along with their adapters connected to LLM. The output side has an end-to-end speech generation module, rather than directly using an external TTS model as the initial VITA-1.0 version [16]. Vision Adapter. two-layer MLP is used to map the visual features to visual tokens suitable for the subsequent understanding of LLM. 3.1.2 Audio Modality Speech Encoder. Similar to [56], our audio encoding module consists of multiple downsampling convolutional layers (4x downsampling) and 24 Transformer blocks (with hidden size of 1024). The downsampling layers help reduce the frame rate of the audio features, improving the processing speed of LLM. The audio encoder has about 350M parameters and an output frame rate of 12.5Hz. Mel-filter bank features are used as the input of the audio encoder, with window size of 25ms and shift of 10ms [56]. Speech Adapter. It consists of multiple convolutional layers with 2x downsampling. Speech Decoder. TiCodec [45] is used as our codec model, customizing single codebook with size of 1024. This single-codebook design simplifies the decoding process during the inference phase. The codec model is responsible for encoding continuous speech signals into discrete speech tokens 1https://huggingface.co/OpenGVLab/InternViT-300M-448px 3 with the frequency of 40Hz, and at the same time has the ability to decode them back into speech signals with the sample rate of 24,000Hz. The current LLM can only output text tokens, and the speech generation capability requires the LLM to be able to output speech tokens. To this end, we add two speech decoders after the text tokens following [56]: 1) Non-Autoregressive (NAR) Speech Decoder, which processes text tokens globally and models semantic features, with the aim of generating an initial distribution of speech tokens; 2) Autoregressive (AR) Speech Decoder generates higher quality speech tokens step by step, based on the speech information produced by the NAR decoder. The final sequence of speech tokens is then decoded into continuous speech signal flow (waveform) using the speech decoder of the Codec model. We adopt 4 LLaMA decoder layers for both NAR and AR speech decoders, where the hidden size is 896 and the parameter size is about 120M. 3.2 Training Data As shown in Table 1, the training data of multimodal instruction tuning encompass wide range of categories, such as caption data and QA data, both Chinese and English. During different training phases, subsets of the overall dataset are selectively sampled to serve different objectives. Specifically, the datasets are categorized as follows: Image Captioning Data. Datasets such as ShareGPT4V [9], ALLaVA-Caption [6], SharedGPT4o-Image2, and synthetic data are used to train the model to generate descriptive languages for images. Image QA Data. Datasets like LLaVA-150K3, LLaVA-Mixture-sample [31], LVISInstruct [55], ScienceQA [38], ChatQA [35], and subsets sampled from LLaVA-OV [26], such as general image QA and mathematical reasoning datasets, are utilized to train the model in answering image-based questions and performing visual reasoning tasks. OCR & Diagram Data. This category supports the model in understanding OCR and diagram content, using datasets such as Anyword-3M [54], ICDAR2019-LSVT4, UReader [58], SynDOG5, ICDAR2019-LSVT-QA6, and corresponding data sampled from LLaVA-OV. Video Data. Datasets like ShareGemini [47] and synthetic data are used to train the model to handle video inputs and perform tasks such as captioning and video-based QA. Pure Text Data. This category enhances the models capability to understand and generate languages, facilitating text-based QA tasks. In addition to the image and video data listed in Table 1, 110,000 hours of internal speech-transcription paired ASR data, covering both Chinese and English, are incorporated to train the audio encoder and align the audio encoder with the LLM. Furthermore, 3,000 hours of text-speech paired data generated by TTS system are used to train the speech decoder. 3.3 Three Stage Training Strategies In order to ensure that VITA-1.5 performs well in tasks involving vision, language, and audio, we have to face key challenge, i.e., training conflicts between different modalities. For example, adding the speech data could negatively impact the understanding of the vision data, as the features of speech differ significantly from those of vision, causing interference during the learning process. To address this challenge, we devise three-stage training strategy as shown in Fig. 3. The core idea is to gradually introduce different modalities into the model, allowing it to increase the power of new modality while maintaining the power of the existing modalities. 3.3.1 Stage 1: Vision-Language Training Stage 1.1 Vision Alignment. In this stage, our goal is to bridge the gap between vision and language. The features of the former are extracted from the pre-trained vision encoder InternViT-300M, and the 2https://sharegpt4o.github.io/ 3https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K 4http://icdar2019.org/ 5naver-clova-ix/synthdog-en 6http://icdar2019.org/ Table 1: Training data of multimodal instruction tuning. The images of the synthetic data come from open-source datasets like Wukong [19], LAION [46], and CC12M [5]. Data Scenario QA Type Dataset Name Description ShareGPT4V ALLaVA-Caption ShareGTP4o-Image Synthetic Data Questions (K) 99.50 697.40 55.50 593.70 Language Eng Eng Eng CN General Image QA Description OCR & Diagram LLaVA-150K LLaVA-Mixture-sample LVIS-Instruct ScienceQA ChatQA LLaVA-OV General LLaVA-OV Math Reasoning Synthetic Data Anyword-3M ICDAR2019-LSVT UReader SynDOG-EN SynDOG-CN QA ICDAR2019-LSVT-QA LLaVA-OV Doc Chart Screen LLaVA-OV General OCR General Video Description Pure Text QA QA ShareGemini Synthetic Data Synthetic Data Synthetic Data Total 218.36 1872.10 939.36 12.72 7.39 1754.65 1140.92 212.68 1709.30 366.30 100.00 100.00 101.90 630.08 4431.50 404.20 205.70 569.40 CN Eng Eng Eng Eng Eng Eng CN CN CN Eng Eng CN CN Eng Eng CN CN & Eng 4336.30 CN & Eng 1574.20 22133. CN & Eng CN & Eng latter is introduced through the LLM. We use 20% of the descriptive caption data from Table 1 for training, where only the visual adapter is trainable, while the other modules are frozen. This approach allows the LLM to initially align the visual modality. Stage 1.2 Vision Understanding. In this stage, our goal is to teach the LLM to transcribe image content. Toward this end, we use all the descriptive caption data from Table 1. During this process, the encoder and adapter of the visual module, as well as the LLM, are trainable. The focus is to enable the model to establish strong connection between vision and language by learning from descriptive texts about images, allowing it to understand image content via generating natural language descriptions. Stage 1.3 Vision SFT. Following Stage 1.2, the model has acquired basic understanding of images and videos. However, the instruction following ability is still limited, and it is difficult to cope with the visual QA task. To achieve this, we use all the QA data from Table 1 while retaining 20% of the descriptive caption data to increase the diversity of the dataset and the complexity of the tasks. During training, the encoder and adapter of the visual module, as well as the LLM, are trainable. The key objective of this stage is to enable the model not only to understand visual content but also to answer questions following instructions. 3.3.2 Stage 2: Audio Input Tuning Stage 2.1 Audio Alignment. After completing the training of Stage 1, the model has developed strong foundation in image and video understanding. In this stage, our goal is to reduce the discrepancy between audio and language based on Stage 1, enabling the LLM to understand audio inputs. The training data consists of 11,000 hours of speech-transcription pairs. We follow two-step approach: (a) Speech Encoder Training: We adopt training framework used in common speech recognition systems, using Connectionist Temporal Classification (CTC) loss function [18] to train the speech encoder. The aim is for the encoder to predict the transcription text from the speech input. This step ensures that the audio encoder can extract speech features and map them to the text representation space. (b) Speech Adapter Training: After training the speech encoder, we integrate it 5 Figure 3: Training Pipeline of VITA-1.5. The training process is divided into three stages to incrementally incorporate vision and audio into the LLM while relieving modality conflicts. Stage focuses on Vision-Language Training, including vision alignment (Stage 1.1, using 20% caption data from Table 1), vision understanding (Stage 1.2, using 100% caption data), and instruction tuning for visual QA (Stage 1.3, using 20% caption data and 100% QA data). Stage 2 introduces Audio Input Tuning, with audio alignment (Stage 2.1, utilizing 11,000 hours of speech-transcription pairs) and instruction tuning for speech QA (Stage 2.2, sampling 4% caption data and 20% QA data). Finally, Stage 3 focuses on Audio Output Tuning, including the training of the codec model (Stage 3.1, using 3,000 hours of text-speech data) and speech decoder training (Stage 3.2). The percentages shown in the image correspond to the data sampling ratios specified in Table 1. with the LLM, using an audio adapter to introduce audio features into the input layer of the LLM. The training objective at this stage is to enable the LLM to output the transcription text of the speech data. Besides, in step (b), we introduce special trainable input tokens to guide the speech understanding process. These tokens provide additional contextual information that guides the LLM used for the QA task to perform the ASR task. Stage 2.2 Audio SFT. The focus of this stage is to introduce the QA functionality with speech questions and text answers. To achieve this, we sample 4% of the caption data and 20% of the QA data from Table 1. In terms of data processing, approximately half of the text-based questions are randomly replaced with their corresponding speech versions, generated using TTS system. In this stage, both the visual encoder and adapter, the audio encoder and adapter, as well as the LLM are trainable, aiming to improve the models adaptability with multimodal inputs. In addition, we add classification head to the LLMs output. This head is used to distinguish whether the input comes from speech or text. As result, the model can more accurately interpret speech inputs and process different modalities efficiently and flexibly. 6 Table 2: Evaluation on Image Understanding Benchmarks. VITA-1.5 shows performance comparable to the leading open-source models and advanced closed-source counterparts. MMB refers to MMBench, MMS to MMStar, Hal to HallusionBench, MathV to MathVista, and OCR to OCRBench. Note that after the training of Stages 2 (Audio Input Tuning) and 3 (Audio Output Tuning), VITA-1.5 retains almost its original visual-language capabilities in Stage 1 (Vision-Language Training). Method VILA-1.5 LLaVA-Next CogVLM2 InternLM-Xcomposer2 Cambrian InternVL-Chat-1.5 Ovis1.5 InternVL2 MiniCPM-V 2.6 LLM Vicuna-v1.5-13B Yi-34b Llama3-8B-Instruct InternLM2-7B Nous-Hermes-2-Yi-34B InternLM2-20B Gemma2-9B-It InternLM2.5-7b Qwen2-7B MMB MMS MMMU MathV Hal AI2D OCR MMVet MME Avg 52.1 68.5 58.3 77.8 58.8 70.7 61.2 77.6 61.4 77.8 65.1 79.7 66.9 77.3 67.3 79.4 68.5 78. 1718.2 2006.5 1869.5 2220.4 2049.9 2189.6 2125.2 2215.1 2268.7 460.0 574.0 757.0 532.0 591.0 720.0 752.0 794.0 852.0 39.3 34.8 41.3 41.0 41.6 47.4 48.2 45.0 48.1 45.0 50.7 57.8 46.7 53.2 55.4 53.8 54.3 60.0 42.5 40.4 38.6 59.5 50.3 54.7 65.6 58.3 60.6 69.9 78.9 73.4 81.2 79.5 80.6 84.5 83.6 82. GPT-4V GPT-4o mini Gemini 1.5 Pro GPT-4o Claude3.5 Sonnet - - - - - VITA-1.0 VITA-1.5 (Stage 1) VITA-1.5-Audio (Stage 3) Mixtral-8x7B Qwen2-7B Qwen2-7B 65.5 76.0 73.9 82.8 78.5 71.8 77.1 76. 3.3.3 Stage 3: Audio Output Tuning 48.2 52.4 57.7 56.5 61.6 44.9 66.2 66.2 39.3 46.1 45.6 51.7 49.9 39.7 44.1 44.9 71.4 77.8 79.1 77.4 80. 73.1 80.3 79.3 678.0 785.0 754.0 663.0 788.0 678.0 752.0 732.0 49.0 66.9 64.0 66.5 66.0 41.6 51.1 49.6 1790.3 2003.4 2110.6 2328.7 1920. 2097.0 2311.0 2352.0 58.5 66.3 67.2 69.3 69.3 57.8 67.1 66.8 41.1 44.2 48.8 51.6 42.6 50.5 41.4 56.2 50.4 54.2 46.8 57.1 49.7 58.1 51.2 61.5 57.5 49.8 Proprietary 59.3 50.4 60.0 54.8 60.6 59.1 62.8 61.6 65.9 62.2 Ours 46.4 59.1 59. 47.3 53.1 52.1 In the first two stages of training, the VITA-1.5 model has effectively developed its multimodal understanding capabilities. However, crucial capacity, i.e., speech output, remains absent, which is essential for its role as an interactive assistant. To introduce speech output functionality without compromising the models fundamental abilities, we draw on the strategy [56], using 3,000 hours of text-speech data and employing two-step training approach (see Fig. 3). Stage 3.1 Codec Training. The goal of this step is to train codec model with single codebook using speech data. The encoder of the codec model has the ability to map speech to discrete tokens, while the decoder can map the discrete tokens back to speech stream. During the inference phase of VITA-1.5, only the decoder is used. Stage 3.2 NAR + AR Decoder Training. The training of this stage uses text-speech paired data, where the text is fed into the tokenizer and the embedding later of the LLM to obtain its embedding vectors, and the speech is fed into the encoder of the codec model to obtain its speech tokens. The text embedding vectors are sent to the NAR speech decoder to get global semantic features, and then the features are sent to the AR speech decoder, which predicts the corresponding speech tokens. Note that the LLM is frozen during this stage, thus the multimodal performance is not affected."
        },
        {
            "title": "4 Evaluation",
            "content": "4.1 Vision-Language Evaluation Baselines. We compare series of open-source MLLMs, including VILA-1.5 [30], LLaVA-Next [25], CogVLM2 [22], InternLM-XComposer2.5 [64], Cambrian-1 [51], MiniCPM-V-2.6 [23], Ovis1.5 [39], InternVL-Chat-1.5, InternVL-2 [11], LLaVA-OV [26], and Video-LLaVA [29], SliME [66], and LongVA [65], as well as 5 closed-source MLLMs, including GPT-4V7, GPT-4o8, GPT-4o-mini, Gemini 1.5 Pro [50], and Claude 3.5 Sonnet9. Evaluation Benchmarks. To assess the image perception and understanding capabilities of VITA1.5, we utilize several evaluation benchmarks, including MME [14], MMBench [32], MMStar [8], MMMU [60], MathVista [37], HallusionBench [20], AI2D [21], OCRBench [34], and MMVet [59]. These benchmarks cover wide range of aspects, including general multimodal capabilities (e.g., MME, MMBench, and MMMU), mathematical reasoning (MathVista), hallucination detection (HallusionBench), chart (AI2D) and OCR (OCRBench) understanding, providing comprehensive 7https://openai.com/index/gpt-4v-system-card/ 8https://openai.com/index/hello-gpt-4o/ 9https://www.anthropic.com/news/claude-3-5-sonnet 7 Table 3: Evaluation on Video Understanding Benchmarks. Although VITA-1.5 still lags behind models like GPT-4o and Gemini-1.5-Pro, it performs comparably to many open-source models. Note that after the training of Stages 2 (Audio Input Tuning) and 3 (Audio Output Tuning), VITA-1.5 retains almost its original visual-language capabilities in Stage 1 (Vision-Language Training). Method Video-LLaVA SliME LongVA VILA-1.5 InternLM-XComposer-2.5 LLaVA-OneVision InternVL-2 MiniCPM-V-2.6 LLM Vicuna-v1.5-13B Llama3-8B-Instruct Qwen2-7B Llama3-8B-Instruct InternLM2-7B Qwen2-7B InternLM2.5-7b Qwen2-7B GPT-4o-mini Gemini-1.5-Pro GPT-4o - - - VITA-1.0 VITA-1.5 (Stage 1) VITA-1.5 (Stage 3) Mixtral-8x7B Qwen2-7B Qwen2-7B Video-MME w/o sub Video-MME w/ sub MVBench TempCompass 39.9 45.3 52.6 - - 58.2 - 60.9 Proprietary 64.8 75.0 71.9 Ours 55.8 56.8 56. 41.6 47.2 54.3 - - 61.5 - 63.7 68.9 81.3 77.2 59.2 59.5 58.7 - - - - 56.7 - - - - - - 56.8 55. 49.8 - 57.0 58.8 62.1 64.2 66.0 66.3 67.1 73.8 62.3 65.5 66.7 Table 4: Evaluation on ASR Benchmarks. VITA-1.5 has demonstrated strong performance in both Mandarin and English ASR tasks. It outperforms specialized speech models, achieving better results in both languages. Model Wav2vec2-base Mini-Omini2 Freeze-Omini CN (CER) Eng (WER) aishell-1 - - 2.8 test net - - 12.6 test meeting - - 14.2 dev clean 6.0 4.8 4. dev other 13.4 9.8 10.2 test clean - 4.7 4.1 test other - 9.4 10.5 VITA-1.0 VITA-1.5 - 2.2 12.2 8. Ours 16.5 10.0 7.6 3.3 16.6 7.2 8.1 3.4 18.4 7. evaluation results. For video understanding, we use representative evaluation benchmarks including Video-MME [15], MVBench [27], and TempCompass [33]. Vision-Language Capabilities. Table 2 presents comparison of VITA-1.5s image understanding performance. After the training of the three stages, VITA-1.5 performs comparably to the most advanced open-source models and even surpasses some closed-source models like GPT-4V and GPT-4o-mini. This result highlights the robust capabilities of VITA-1.5 in image-language tasks. As shown in Table 3, VITA-1.5 shows comparable performance to the top open-source models in the evaluation of video understanding. The notable gap compared to proprietary models suggests that VITA-1.5 still has significant room for improvement and potential for further enhancement in video understanding. Please note that after the training of Stages 2 (Audio Input Tuning) and 3 (Audio Output Tuning), VITA-1.5 retains almost its original visual-language capabilities in Stage 1 (Vision-Language Training). 4.2 Speech Evaluation Baselines. The following three baseline models are used for comparison: Wav2vec2-base [1], Mini-Omini2 [57], Freeze-Omini [56], and VITA-1.0 [16]. Evaluation Benchmarks. The Mandarin Evaluation Sets consists of three datasets: aishell-1 [4], test net [7], and test meeting [62]. These datasets are used to evaluate the models performance on Mandarin speech. The evaluation metric is the Character Error Rate (CER). The English Evaluation Sets include four datasets: dev-clean, dev-other, test-clean, and test-other [43], which are used to evaluate the models performance on English speech. The evaluation metric is Word Error Rate (WER). ASR Performance. The evaluation results in Table. 4 indicate that VITA-1.5 achieves leading accuracy in both Mandarin and English ASR tasks. This demonstrates that VITA-1.5 has successfully integrated advanced speech capability to support multimodal interaction."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we has presented VITA-1.5, multimodal LLM designed to integrate vision and speech through carefully crafted three stage training strategy. By relieving the inherent conflicts between modalities, VITA-1.5 achieves robust capabilities in both vision and speech understanding, enabling efficient speech-to-speech interactions without relying on separate ASR or TTS modules. Extensive evaluations demonstrate that VITA-1.5 performs competitively across multimodal benchmarks. We hope that VITA-1.5 can take over the banner of VITA-1.0 and continue to promote the progress of open-source models in the field of real-time multimodal interaction."
        },
        {
            "title": "References",
            "content": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. NeurIPS, 2020. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. [4] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-source mandarin speech corpus and speech recognition baseline. In O-COCOSDA. IEEE, 2017. [5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. [6] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4vsynthesized data for lite vision-language model, 2024. [7] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [9] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. [10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. [12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. 9 [13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. NeurIPS, 2024. [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [15] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [16] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. [17] Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, et al. Mme-survey: comprehensive survey on evaluation of multimodal llms. arXiv preprint arXiv:2411.15296, 2024. [18] Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In ICML, 2006. [19] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. NeurIPS, 2022. [20] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In CVPR, 2024. [21] Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 2021. [22] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. [23] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. [24] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [25] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024. [26] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [27] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. 10 [28] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [29] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [30] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. [32] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. [33] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, arXiv preprint and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv:2403.00476, 2024. [34] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. [35] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. arXiv preprint arXiv:2401.10225, 2024. [36] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng towards real-world vision-language Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: understanding. arXiv preprint arXiv:2403.05525, 2024. [37] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [38] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. NeurIPS, 2022. [39] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. [40] Dan Oneat, and Horia Cucu. Improving multimodal speech recognition by data augmentation and speech representations. In CVPR, 2022. [41] OpenAI. Gpt-4 technical report. 2023. [42] OpenAI. Hello gpt-4o. 2023. [43] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In ICASSP, 2015. [44] Madhusudhana Reddy, Vaishnavi, and Pavan Kumar. Speech-to-text and text-to-speech recognition using deep learning. In ICECAA. IEEE, 2023. [45] Yong Ren, Tao Wang, Jiangyan Yi, Le Xu, Jianhua Tao, Chu Yuan Zhang, and Junzuo Zhou. Fewer-token neural speech codec with time-invariant codes. In ICASSP. IEEE, 2024. 11 [46] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. [47] Share. Sharegemini: Scaling up video caption data for multimodal large language models, June 2024. https://github.com/Share14/ShareGemini. [48] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. [49] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [50] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [51] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [54] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054, 2023. [55] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. [56] Xiong Wang, Yangze Li, Chaoyou Fu, Lei Xie, Ke Li, Xing Sun, and Long Ma. Freeze-omni: smart and low latency speech-to-speech dialogue model with frozen llm. arXiv preprint arXiv:2411.00774, 2024. [57] Zhifei Xie and Changqiao Wu. Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities. arXiv preprint arXiv:2410.11190, 2024. [58] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023. [59] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [60] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [61] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. [62] Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, et al. Wenetspeech: 10000+ hours multi-domain mandarin corpus for speech recognition. In ICASSP, 2022. [63] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. [64] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. [65] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [66] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. [67] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024."
        }
    ],
    "affiliations": [
        "CASIA",
        "NJU",
        "Tencent Youtu Lab",
        "XMU"
    ]
}