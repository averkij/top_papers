{
    "paper_title": "Training-Free Reasoning and Reflection in MLLMs",
    "authors": [
        "Hongchen Wei",
        "Zhenzhong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have showcased impressive reasoning capabilities via reinforcement learning. However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by the prohibitive costs of retraining and the scarcity of high-quality, verifiable multimodal reasoning datasets. This paper introduces FRANK Model, a training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning and reflection abilities, without any gradient updates or extra supervision. Our key insight is to decouple perception and reasoning across MLLM decoder layers. Specifically, we observe that compared to the deeper decoder layers, the shallow decoder layers allocate more attention to visual tokens, while the deeper decoder layers concentrate on textual semantics. This observation motivates a hierarchical weight merging approach that combines a visual-pretrained MLLM with a reasoning-specialized LLM. To this end, we propose a layer-wise, Taylor-derived closed-form fusion mechanism that integrates reasoning capacity into deep decoder layers while preserving visual grounding in shallow decoder layers. Extensive experiments on challenging multimodal reasoning benchmarks demonstrate the effectiveness of our approach. On the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2, outperforming the strongest baseline InternVL2.5-38B by +5.3, and even surpasses the proprietary GPT-4o model. Our project homepage is at: http://iip.whu.edu.cn/frank/index.html"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 1 5 1 6 1 . 5 0 5 2 : r Training-Free Reasoning and Reflection in MLLMs Hongchen Wei1 and Zhenzhong Chen*1 1School of Remote Sensing and Information Engineering, Wuhan University Abstract Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have showcased impressive reasoning capabilities via reinforcement learning. However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by the prohibitive costs of retraining and the scarcity of high-quality, verifiable multimodal reasoning datasets. This paper introduces FRANK Model, training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning and reflection abilities, without any gradient updates or extra supervision. Our key insight is to decouple perception and reasoning across MLLM decoder layers. Specifically, we observe that compared to the deeper decoder layers, the shallow decoder layers allocate more attention to visual tokens, while the deeper decoder layers concentrate on textual semantics. This observation motivates hierarchical weight merging approach that combines visual-pretrained MLLM with reasoning-specialized LLM. To this end, we propose layer-wise, Taylor-derived closed-form fusion mechanism that integrates reasoning capacity into deep decoder layers while preserving visual grounding in shallow decoder layers. Extensive experiments on challenging multimodal reasoning benchmarks demonstrate the effectiveness of our approach. On the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2, outperforming the strongest baseline InternVL2.5-38B by +5.3, and even surpasses the proprietary GPT-4o model. Our project homepage is at: http://iip.whu.edu.cn/frank/index.html"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Recent reasoning-focused large language models (LLMs) [1, 2, 3, 4] such as DeepSeek-R1 [1] and OpenAI-o1 [5] have demonstrated strong performance in tasks requiring complex logic, including math reasoning, symbolic manipulation, and program synthesis. These models leverage mechanisms like reinforcement learning to perform multi-step problem-solving and iterative self-correction, often surpassing even human experts. In real-world scenarios, numerous tasks demand sophisticated multimodal reasoning capabilities. For instance, solving visual mathematics problems, interpreting diagrams, and understanding code snippets embedded within images require the integration of visual perception with logical reasoning. Inspired by the successes of reasoning-augmented LLMs, researchers have begun exploring methods to endow multimodal large language models (MLLMs) [6, 7, 8, 9] with similar reasoning abilities. prevalent approach [10, 11, 12] involves adapting reinforcement learning techniques, such as GRPO [1], to the multimodal context. However, this strategy encounters significant challenges. Firstly, the reinforcement learning training of large-scale MLLMs demands substantial computational resources, making it resource-intensive. Secondly, there is notable scarcity of high-quality, verifiable multimodal reasoning datasets, which are essential for effective training. This paucity of suitable data severely impedes the development and scalability of reasoningcapable MLLMs. In this paper, we introduce FRANK, training-free and R1like MLLM that is designed to endow existing MLLMs with advanced reasoning and reflection capabilities without any additional training or supervision. Figure 1 visualizes this pipeline. Our method is built upon two key insights: Homologous Model Merging: We conceptualize MLLMs as base LLMs fine-tuned on visual-text data, while reasoningCorresponding author: Zhenzhong Chen, E-mail:zzchen@ieee.org specialized LLMs represent the same base LLM fine-tuned on reasoning tasks. According to the task arithmetic hypothesis [13, 14, 15], the difference in weights between fine-tuned model and its base model encapsulates the task-specific adaptations. By merging task vectors from models fine-tuned on different tasks, we can integrate multiple capabilities into single model without additional training. Layer-wise Functional Specialization in MLLMs: Drawing inspiration from the hierarchical processing observed in the human brain [16, 17], where sensory inputs are initially processed in primary sensory areas and progressively integrated into higher-order cognitive functions in association cortices, we observe similar pattern in MLLMs. As shown in Figure 2, compared to the deeper decoder layers, the shallow decoder layers allocate more attention to visual tokens, facilitating perceptual grounding, while the deeper decoder layers concentrate on textual semantics. Based on these two key insights, we design hierarchical weight merging strategy to effectively integrate vision-text pretrained MLLM with reasoning-specialized LLM. Specifically, we develop layer-wise, Taylor-derived closed-form fusion mechanism that enables fine-grained control over the contribution of each model at different depths of the decoder. This mechanism builds on the task vector formulation and refines it with layer-wise optimization strategy: For each decoder block, we derive closed-form solution for the optimal task vector fusion weights by minimizing the Taylor-approximated task loss difference. This allows precise control over how visual and reasoning adaptations are combined at each layer. Furthermore, guided by the empirical prior that shallow decoder layers in MLLMs attend more to visual inputs while deeper layers focus on symbolic reasoning, we incorporate layer-dependent fusion weights to align with the distinct functional roles across the model hierarchy. This design enables FRANK to embed reasoning capabilities into deeper layers, responsible for abstraction and reasoning, while preserving the visual grounding in shallower layers, which Figure 1: Non-reasoning MLLMs lack reasoning and reflection abilities, while reasoning LLMs are unable to perceive visual information. We propose training-free, closed-form layerwise fusion method that combines visual perception and language reasoning strengths, substantially enhancing overall reasoning capability in multimodal settings. are more sensitive to perceptual signals. We validate the effectiveness of FRANK through comprehensive evaluations on challenging multimodal reasoning benchmarks. Notably, our model FRANK-38B achieves an accuracy of 69.2 on the MMMU benchmark, outperforming the strongest baseline InternVL2.5-38B by +5.3, and even surpasses the proprietary GPT-4o model. These results underscore FRANKs ability to enhance reasoning capabilities in MLLMs without additional training or supervision. the model to generate structured outlines before deriving final answers. In concurrent work, methods like MM-Eureka [26], Vision-R1 [10], and LMM-R1 [11] extend reinforcementlearning-based reasoning (e.g., GRPO) to the multimodal setting, using multimodal datasets to fine-tune MLLMs. However, all of these approaches require costly secondary training and depend on the availability of large, high-quality multimodal reasoning corpora, which remain extremely scarce, thus limiting their scalability and generalization. In summary, our contributions are threefold: The layer-wise functional specialization in MLLMs, where shallow decoder layers focus on visual perception and deeper layers on textual reasoning, is identified and leveraged. novel, training-free hierarchical weight fusion mechanism is proposed, integrating reasoning capabilities into existing MLLMs by merging task vectors at each layer, guided by Taylor-derived closed-form solution. It is demonstrated that FRANK effectively enhances the reasoning and reflection abilities of MLLMs, achieving superior performance on multimodal reasoning tasks without the need for additional training or supervision."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Multimodal Large Language Models Multimodal large language models (MLLMs) have recently advanced the integration of visual and linguistic understanding through large-scale vision-language pretraining and instruction tuning. Early works [18, 19, 20, 21] enable zeroor few-shot visual question answering, image captioning, and multimodal instruction following by augmenting an LLM with pretrained visual encoder. While these models excel at aligning visual features with text, their performance on tasks requiring multi-step logical reasoning or self-reflection remains limited. Building on chain-of-thought (CoT) [22] prompting in pure-text LLMs, several recent works [6, 7, 8, 9, 10, 11, 12] attempt to endow MLLMs with explicit reasoning capabilities. LLaVAReasoner [23] shows that naively appending CoT prompts often yields marginal gains or can even degrade MLLM accuracy. To better guide intermediate reasoning, CCoT [24] and TextCoT [25] introduce plan-based CoT prompting, prompting 2.2 Model Merging Model merging [27, 13, 14, 28, 29, 30] has emerged as an efficient alternative to multi-task fine-tuning, enabling the fusion of multiple task-specific models into single model without accessing original training data. Early work on weight averaging [13] showed that simple parameter interpolation can improve robustness across tasks. Task Arithmetic [14] generalized this idea by introducing task vectors that represent the differences between fine-tuned and base model weights, and linearly combining them to incorporate new capabilities. To address interference between tasks, TIES-Merging [28] trims low-magnitude updates and aligns parameter signs before merging, while DARE [31] further sparsifies and rescales delta parameters to reduce redundancy. MetaGPT [29] formalizes model merging as multi-task learning problem, achieving balanced task performance by computing the norms of different model parameters. Although these methods achieve strong results for text-only LLMs, they do not exploit the unique structure of multimodal models. In concurrent work, VLM-Merging [32] employs fixed fusion weights (with an MLLM weight of 0.9 and an LLM weight of 0.1) to merge MLLM and LLM, enabling the MLLM to acquire reasoning capabilities. However, such manually designed fusion weights are often suboptimal and struggle to transfer the LLMs reflective abilities effectively. In this work, we extend the model merging paradigm to MLLMs by incorporating multimodal priors, specifically, layer-wise perception-to-cognition specialization, into the fusion process, thereby enabling training-free integration of visual perception and reasoning capabilities."
        },
        {
            "title": "3 Approach",
            "content": "In this section, we introduce FRANK, training-free and R1-like MLLM that endows off-the-shelf MLLMs with advanced reasoning and reflection by merging them with reasoning-specialized 2 LLMs, entirely without gradient updates or extra supervision. At its core, FRANK relies on two principles: (1) homologous model merging, which treats both models as task-fine-tuned variants of the same base LLM and fuses them via task-vector arithmetic; and (2) hierarchical layer-wise fusion, which exploits the observation that shallow decoder layers chiefly process visual inputs while deep layers focus on text, allowing us to inject reasoning only where it is most effective. 3.1 Preliminary: Task Arithmetic for Homologous Model Merging We build on the paradigm of task arithmetic [14], which provides simple yet effective mechanism to merge multiple fine-tuned models, so-called homologous models, that share the same base architecture. Let ( ; θ0) denote pre-trained base model, and let θt = arg min θ Lt (cid:0) ( ; θ), Dt (cid:1) (1) be the parameters obtained by fine-tuning on task with loss Lt over dataset Dt. We define the task vector for task as τt = θt θ0 . Under the homologous assumption that all θt lie in the same parameter space as θ0, we can form merged model by linear combination of these task vectors: θ = θ0 + (cid:88) t{V,R} λt τt , (2) where {λt} are non-negative fusion weights controlling each tasks contribution. Early approaches choose all λt heuristically (e.g., constant 0.3) or via grid search on held-out data, but these methods either under-utilize model capacity or incur prohibitive search costs as grows. 3.2 FRANK: Training-free and R1-like MLLM Building on the task-arithmetic preliminary, FRANK introduces two key innovations: Layer-wise Fusion, which respects the functional specialization of each decoder layer (for clarity, we define decoders block as layer); Modality Priors, which steer shallow layers toward visual grounding and deep layers toward symbolic reasoning. We next detail (i) how we decompose the MLLM decoder into per-layer task vectors; (ii) the Taylor-based derivation of closed-form fusion weights at each layer; and (iii) the incorporation of layer-dependent modality priors. 3.2.1 Layer-Wise Fusion Setup for MLLMs FRANK bridges these two paradigms by viewing the vision-finetuned decoder and reasoning-specialized decoder as homologous variants of the same base model checkpoint. Leveraging their shared architecture, we can merge their strengths without retraining. The decoder consists of stacked transformer layers, indexed by = 1, . . . , L. Figure 2 shows that, compared to the deeper decoder layers, the shallow decoder layers allocate more attention to visual tokens, facilitating perceptual grounding, while the deeper decoder layers concentrate on textual semantics. To preserve this functional hierarchy and prevent interference between vision and reasoning, FRANK performs , θ(l) layer-wise weight fusion: each decoder layer is merged independently, preserving its specialized role. Concretely, let θ(l) 0 , θ(l) denote the parameters of layer for the pre-trained base model, the vision-fine-tuned MLLM, and the reasoning-fine-tuned LLM, respectively. We define the layerwise task vectors: τ(l) (3) These differences capture how each task (vision vs. reasoning) shifts the model weights at each depth. θ(l) 0 , θ(l) 0 . = θ(l) = θ(l) τ(l) To measure the impact of fusing these shifts, we introduce two metrics. First, the Layer-Wise Task Loss Difference (LTLD) compares the fused layers loss against each branchs own finetuned layer: Definition 1 (Layer-Wise Task Loss Difference, LTLD). Let L(l) (θ, x) be the loss of branch {V, R} at layer on input x. For fusion weights (λV , λR), define the fused parameter θ(l) = θ(l) 0 + λV τ(l) + λR τ(l) , (4) where, θ(l) represents the parameters of the fusion model at layer l, while τ(l) and τ(l) denote the task vectors at layer of the nonreasoning MLLM and reasoning-specialized LLM, respectively. Then, we define the layer-wise task loss difference (LTLD) as LTLD(l) (λV , λR) = L(l) (θ(l) , x) L(l) (θ(l) , x), (5) quantifies the degradation when using the fused weights in place of branch bs own. Definition 2 (Layer-Wise Average Loss Difference, LALD). The LALD averages LTLD across both branches, (cid:0)LTLD(l) LALD(l)(λV , λR) = 1 2 and serves as our per-layer fusion objective. By minimizing LALD independently at each layer, we derive fusion weights that optimally balance visual grounding and reasoning without cross-layer interference. + LTLD(l) (6) (cid:1), We emphasize that LTLD is theoretical sensitivity measure, not an operation we perform in implementation. Specifically, LTLD represents the second-order Taylor approximation of the loss increase induced by small perturbations in the parameters of layer l, holding all other layers fixed. Under the Neural Tangent Kernel (NTK) [33] linearization and task-vector orthogonality assumptions, this localized view yields tractable quadratic form in the norms τ(l) 2, from which we derive closedform fusion weights (see Section 3.2.2). Importantly, we do not compute per-layer losses on the fly; LTLD merely guides our analytical derivation. In subsequent sections, we will show how to approximate LTLD via second-order Taylor expansion, invoke NTK linearization and vector orthogonality, and obtain data-agnostic closed-form for (λ(l) 2 and τ(l) , λ(l) ). 3.2.2 Taylor-Based Approximation and Closed-Form Fusion Weights To obtain efficient, data-agnostic fusion weights at each decoder layer, we approximate the layer-wise average loss difference (LALD) using second-order Taylor expansion. Intuitively, 3 Figure 2: Layer-wise visual attention of NVIL-15B. Each curve shows the average attention from text token to all visual tokens across layers. Shallow layers assign significantly higher attention to visual tokens, while attention in deeper layers approaches zero and rapidly descends indicating shift from perception to language reasoning. This supports our use of an exponential decay prior to the fusion process. (NVIL-15B) Cosine similarity between task vectors of Figure 3: vision-finetuned reasoning-finetuned and (DeepSeekDistil-Qwen2.5-14B) models at each decoder block. The task vector at each block is computed by flattening the weight deltas with respect to the base model. The similarity remains close to 0 across all layers, indicating strong nearorthogonality. this expansion captures how small perturbationarising from merging vision and reasoning task vectorsaffects the loss. Under two standard assumptions: NTK Linearization. In the infinite-width regime, neural networks evolve under training according to fixed NTK [33], which implies that small weight perturbations produce locally linear changes in the model output [33, 29]. The appendix A.5 provides additional details. Previous study [29] has empirically validated this NTK-linear behavior for LLMs. They evaluated LLaMA-2-7b-chat-hf [34] on the AGIEval benchmark [35], sampling three random prompts and measuring model outputs for interpolation coefficients α {0, 0.1, . . . , 1.0}. The output trajectories scale almost perfectly linearly with α, confirming that LLM operates in the NTK regime during fine-tuning, which is specifically suitable for the LLMs arithmetic scenario. Task-Vector Orthogonality. Although both vision and reasoning fine-tuning update the same decoder weights, their resulting task vectors often lie in nearly orthogonal subspaces. As shown in Figure 3, we verify this by computing the cosine similarity between τ(l) at each layer, which remains close to zero across all layers. The appendix A.6 provides additional details. and τ(l) Under the NTK linearization and task-vector orthogonality assumptions, we can now quantify how merging vision and reasoning updates perturbs the layer loss. In the next step, we expand each tasks loss around its fine-tuned parameters to derive closed-form bound on the loss increase. Let θ(l) be the fine-tuned weights for task {V, R}. Define the fused weights 0 be the initialization at layer l, and θ(l) = θ(l) 0 + τ(l) θ(l) = θ(l) 0 + (cid:88) t{V,R} τ(l) λ(l) and the fusion residual h(l) = θ(l) θ(l) (cid:88) = k(cid:44)t{V,R} τ(l) λ(l) (1 λ(l) ) τ(l) (7) . (8) Around θ(l) , second-order Taylor expansion of the layer loss L(l) gives (θ(l) L(l) ) L(l) (θ(l) ) + L(l) (θ(l) )h(l) (cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125) 2 h(l) + 1 2L(l) (θ(l) ) h(l) , (9) where the first-order term vanishes under near-convergence, as guaranteed by the NTK theory for wide networks. This NTK regime further implies two key properties for the second-order term: 1) The Hessian is dominated by the Jacobian Gram matrix due to the approximate linearity of the network in parameter space, and 2) Its eigenvalue distribution becomes approximately isotropic. Concretely, the Hessian admits the following approximation: 0 )(cid:17) 2L(l) , (10) where, δ(l) is data-dependent constant, dl represents the paramt eter dimension of the l-th layer, Idl denotes the dl-dimensional identity matrix, which is used to simplify the structure of the Hessian. Consequently, 0 ) (l)(xt; θ(l) (l)(xt; θ(l) ) δ(l) (θ(l) = 1 dl δ(l) Idl , tr (cid:16) t ) δ(l) = L(l) (θ(l) LTLD(l) ) L(l) (θ(l) 2. Summing the above bound for {V, R} and using τ(l) yields layer-wise bound on the average loss increase: (cid:104) )2τ(l) (1λ(l) LALD(l) 1 2 )2τ(l) 2 h(l) 2+ (λ(l) δ(l) (cid:88) (cid:88) , τ(l) 0 2(cid:105) τ(l) (11) 2. t{V,R} k(cid:44)t{V,R} (12) To derive the optimal fusion weights, we minimize the LALD bound under the constraint λV + λR = 1. Setting the derivative to zero and solving yields the closed-form solution: τ(l) 2 2 + τ(l) {V, R}. τ(l) λ(l) (13) = , This compact expression emphasizes FRANKs key advantage: fusion weights depend solely on the observed parameter shifts of each task, eliminating the need for held-out data, grid search, or additional training. For full statements of the Taylor expansion lemma, formal properties, and proofs leading to this closed form, please refer to Appendix A.7 and A.8. Table 1: Comparison of FRANK variants (8B, 15B, 38B) and state-of-the-art baselines across five multimodal reasoning benchmarks: MMMU val, MMMU-Pro standard (10 opts), MathVista testmini, MathVision testmini, and WeMath testmini. * indicates the baseline model. Methods Size MMMU MMMU-Pro MathVista MathVision WeMath a i M r 7B LLaVA-1.5 [36] [2023.10.05] 7B LLaVA-NeXT [37] [2024.01.30] LLaVA-LLaMA3 [37] [2024.05.10] 8B VILA1.5-LLaMA3 [38] [2024.05.16] 8B Idefics3-LLaMA3* [39] [2024.08.07] 8B FRANK (Ours) 8B LLaVA-1.5 [36] [2023.10.05] ShareGPT4V [40] [2023.11.21] LLaVA-NeXT [37] [2024.01.30] VILA-1.5 [38] [2024.05.16] NVILA* [41] [2024.12.05] FRANK (Ours) 13B 13B 13B 13B 15B 15B 34B LLaVA-NeXT [37] [2024.01.30] VILA-1.5 [38] [2024.05.16] 40B LLaVA-OneVision [42] [2024.08.06] 72B 72B Qwen2-VL [43] [2024.09.18] - GPT-4o [44] [2024.05.13] InternVL2.5* [45] [2024.12.06] FRANK (Ours) 38B 38B 35.7 35.3 39.2 38.6 43.9 48. 37.0 36.6 36.2 37.9 53.2 61.3 48.1 51.9 56.8 64.5 69.1 63.9 69.2 19.7 19.4 - - 32.6 34. - - 19.8 - 36.2 49.4 30.3 35.9 38.0 49.2 54.0 48.0 56.8 25.6 24.9 40.0 36.7 58.4 50. 27.7 29.3 - 42.7 67.6 55.4 46.5 49.5 67.5 70.5 63.8 71.9 73.1 10.2 10.0 - - 20.1 27. 13.1 13.9 - 15.2 23.2 37.2 - - 25.3 26.6 29.9 32.2 39.7 7.0 3.3 - - 12.3 11. 7.4 - - 11.4 31.1 32.3 - - 32.0 36.0 - 38.3 47.0 3.2.3 Incorporating Modality Priors While the closed-form fusion weights from Section 3.2.2 balance visual and reasoning shifts purely by their magnitudes, MLLM decoders exhibit clear functional hierarchy: Shallow layers (small l) predominantly attend to visual tokens, anchoring the model in perceptual representations. Deep layers (large l) focus on textual tokens, supporting abstraction and symbolic reasoning. To encode this prior knowledge, we introduce layer-dependent modality priors w(l) > 0 and reformulate the per-layer fusion objective: , w(l) min λV ,λR LTLD(l) w(l) (λV , λR) + w(l) LTLD(l) (λV , λR). (14) The modality prior-guided closed-form is λ(l) = w(l) τ(l) 2 2 + w(l) τ(l) w(l) τ(l) 2 , {V, R}. (15) Attention-Guided Decay Priors During our experiments, we observed that compared to deeper decoder layers, shallower decoder layers in the MLLM allocate more attention to visual tokens, thereby facilitating visual perceptual grounding, while deeper layers focus primarily on textual semantics, as illustrated in Figure 2. To introduce reasoning and reflection capabilities while preserving visual perception, and to maintain methodological simplicity without introducing additional supervision, we derive modality priors from the models layer-wise attention patterns and fit them with an exponential decay function (prioritizing simplicity, though more complex quadratic functions could alternatively be used for fitting). This approach naturally captures the non-uniform transition from visual grounding to symbolic reasoning observed in practice. Specifically, we first collect the visual attention weights al from each decoder layer in the MLLM. Then, we posit al ˆα l, and obtain ˆα, log via linear regression on (cid:8)(l, log al)(cid:9)L l=1. Finally, we set w(l) = exp( ˆα l) j=1 exp( ˆα j) (cid:80)L , w(l) = 1 w(l) . (16) This attention-guided exponential schedule requires no labels and ensures the modality priors faithfully mirror the models intrinsic shift from visual grounding to reasoning across the decoder hierarchy. Please refer to Appendix A.9 for details. To conclude, FRANK delivers training-free, interpretable, and efficient framework for the per-layer fusion of visual grounding and logical reasoning in MLLMs, requiring only task-vector norms and simple priors. Its closed-form weights avoid any extra labeling data or optimization, making FRANK practical for scaling multimodal intelligence."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Datasets and Model Variants We evaluate FRANK on five widely-used multimodal reasoning benchmarks: MMMU val [46], MMMU-Pro standard (10 opts) [47], MathVista testmini [48], MathVision testmini [49], and WeMath testmini [50]. These datasets encompass broad spectrum of task formats (e.g., diagram interpretation, symbolic math problems) and difficulty levels. To study the effects of model scale and architecture, we instantiate three FRANK variants by applying our layer-wise fusion to pairs of non-reasoning MLLMs and reasoning-specialized LLMs. Specifically, FRANK-8B merges Idefics3-8B [39] 5 Table 2: Perceptual performance of FRANK-15B on MME compared to vision-only NVIL-15B. w/o MP denotes the ablation without Modality Prior. Sub-task abbreviations: Comm. = Commonsense Reasoning, Num. = Numerical Calculation, Text. = Text Translation, Code. = Code Reasoning. MME Existence Count Position Color OCR Poseter Celebrity NVIL-15B (upper bound) FRANK-15B w/o MP FRANK-15B 100.0 96.7 96. 80.0 78.3 80.0 86.7 68.3 60.0 91.7 90.0 91.7 90.0 75.0 85.0 94.2 91.2 91.8 83.2 76.2 77. MME Scene Landmark Artwork Comm. Num. Text. Code. NVIL-15B (upper bound) FRANK-15B w/o MP FRANK-15B 83.5 80.0 83.0 90.5 83.7 84.3 82.3 76.5 74.3 82.9 84.3 85.0 72.5 75.0 72.5 67.5 57.5 60. 85.0 82.5 85.0 (non-reasoning MLLM) with DeepSeekDistil-LLaMA3-8B [1] (reasoning LLM), FRANK-15B merges NVIL-15B [41] with DeepSeekDistil-Qwen2.5-14B [1], and FRANK-38B merges InternVL2.5-38B [45] with QwQ-32B [51]. These variants test our fusion across LLaMAand Qwen-based architectures and varying capacities. These three FRANK variants enable analysis of our approach across small, medium, and large model scales. We apply FRANKs closed-form fusion weights layer-by-layer without any additional fine-tuning. The attention-guided exponential decay prior is fit on 1000 randomly sampled validation examples from MSCOCO dataset [52] (note: only the image is needed, no annotations) using least-squares regression to obtain decay parameter α and normalization constant C. 4.2 Quantitative Evaluation Table 1 presents the performance of FRANK variants and stateof-the-art baselines on five multimodal reasoning benchmarks. Below we analyze results by dataset, highlighting both absolute gains and relative improvements to demonstrate the efficacy and scaling behavior of our layer-wise fusion. MMMU/MMMU-Pro. On MMMU (college-level image-text questions), FRANK-8B achieves 48.3, 4.4-point gain over its vision branch. This improvement indicates that even at 8B parameters, our fusion effectively combines visual grounding and reasoning. On the more stringent MMMU-Pro, FRANK-8B reaches 34.7. Increasing model capacity yields further gains: FRANK-15B attains 61.3 on MMMU and 49.4 on MMMU-Pro, demonstrating that additional parameters enable richer reasoning adaptations. FRANK-38B further improves to 69.2 (MMMU) and 56.8 (MMMU-Pro), outperforming InternVL2.5-38B by 5.3 and 8.8 points, respectively, and confirming strong scaling trend. Math Benchmarks (MathVista, MathVision, WeMath). We evaluate FRANK variants on three math-focused datasets  (Table 1)  . FRANK-8B and FRANK-15B underperform on MathVista (50.7 and 55.4, -7.7 and -12.2 vs. Idefics3-LLaMA3 and NVIL-15B). In contrast, FRANK-38B achieves 73.1 on MathVista (+1.2 vs. InternVL2.5), 39.7 on MathVision (+7.5), and 47.0 on WeMath (+8.7), demonstrating that larger capacity better absorbs fusion weights, mitigating model merging interference, while still enhancing deeper-layer symbolic reasoning. Scale and Architecture Analysis. Aggregating results across all three benchmarks reveals clear scaling law: FRANK variants continue to reap sustained reasoning benefits and exhibit greater robustness as model size increases. Both Qwen-based Table 3: Ablation study results of different model mergeing methods on the MMMU. Method MMMU Acc. NVIL-15B (baseline) VLM-Merging Task Arithmetic MetaGPT FRANK-15B w/o Modality Prior FRANK-15B 53.2 53.6 56.1 57.9 58.4 61.3 Table 4: The number of reflection tokens in MMMU responses across models. Reflection NVIL-15B FRANK-15B Wait Hmm Mistake Alternatively Check 0 0 0 0 0 20125 1733 1663 7712 1586 FRANK-15B and LLaMA-based FRANK-8B showed consistent improvements in reasoning capabilities, indicating that our layer-wise fusion is independent of the backbone architecture. Moreover, FRANK-38Bs strong performance on the most challenging benchmarks underscores that larger models can more fully exploit the injected reasoning capabilities, all achieved without any additional gradient-based training. 4.3 Visual Perception Evaluation We assess visual perception capability on the MME benchmark [53]  (Table 2)  . NVIL-15B, being non-reasoning multimodal model, achieves the strong perceptual performance across all 14 sub-tasks, representing the upper bound on visual grounding accuracy. As expected, fusing in decoder layers incurs some degradation; however, by incorporating our attention-guided Decay Prior, FRANK-15B only experiences very slight drops relative to NVIL-15B, substantially outperforming the no-prior ablation. By contrast, the ablated FRANK-15B w/o MP falls further behind, indicating that the Modality Prior is crucial for preserving visual perception when fusing the model. Moreover, in sub-tasks requiring commonsense reasoning (Comm.), FRANK-15B even slightly surpasses NVIL-15B (85.0 vs 82.9), suggesting that our fusion maintains and can enhance performance in scenarios where light reasoning complements visual 6 chain-of-thought with reflection tokens, arriving at the correct solution. As shown in Figures 5, 6, and 7, across all three model scales, the FRANK variants consistently produce step-by-step reasoning chains and explicit reflection checks. This enables them to correct intermediate arithmetic or logical reasoning and arrive at the correct results. In contrast, the non-reasoning baselines directly output an incorrect answer without any justification."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we presented FRANK, training-free and R1like MLLM that endows off-the-shelf MLLMs with advanced reasoning and self-reflection capabilities. By decomposing the decoder into per-layer task vectors and deriving closed-form fusion weights under NTK linearization and task-vector orthogonality, FRANK seamlessly integrates visual grounding from vision-fine-tuned MLLMs with logical reasoning from reasoning-specialized LLMs, guided by attention-driven modality priors. Extensive experiments across five multimodal reasoning benchmarks demonstrate that FRANK consistently outperforms state-of-the-art baselines at 8B, 15B, and 38B scales, with particularly strong gains on reasoning tasks. Future work includes extending FRANK to support more diverse modalities (e.g., audio, video), exploring dynamic fusion strategies for real-time tasks, and investigating theoretical guarantees under broader neural architectures. We believe FRANK offers practical and interpretable path toward scalable multimodal intelligence without the overhead of task-specific retraining."
        },
        {
            "title": "References",
            "content": "[1] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [2] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi K1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [3] Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. 2 R: Teaching llms to self-verify and self-correct via reinforcement learning. arXiv preprint arXiv:2502.12853, 2025. [4] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In ICLR, 2024. [5] OpenAI. OpenAI o1. https://openai.com/o1/, 2024. [6] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. LLaVA-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. Figure 4: Average output length of the FRANK on the MMMU benchmark, stratified by task difficulty. understanding. 4.4 Ablation Study Table 3 evaluates each fusion component on MMMU using the same NVIL-15B and DeepSeekDistil-Qwen2.5-14B backbones without fine-tuning. The results confirm that model merging injects reasoning into pretrained MLLM: Traditional fusion baselines (VLM-Merging [32], Task Arithmetic [14], and MetaGPT [29]) achieve 53.6%, 56.1%, and 57.9%, respectively. Task Arithmetic follows fixed fusion weight λV of 0.3, shown in prior work [14, 29] to perform robustly across diverse tasks, which nonetheless yields only modest gains (3.6-4.7 points), indicating limited integration of reasoning capabilities. Layer-only fusion (FRANK-15B w/o Modality Prior) leverages closed-form weights for per-layer merging and improves to 58.4%, outperforming all traditional methods by 1.7-3.6 points. Full FRANK-15B adds the attention-guided exponential decay modality prior and attains 61.3%, further 2.9-point gain. This confirms that the learned prior successfully balances visual grounding and symbolic reasoning across layers. 4.5 Reflection Frequency and Output Length Analysis We quantify FRANKs self-correction capability by analyzing reflection tokens (Wait, Hmm, Mistake, Alternatively, Check) in MMMU val set responses (N=900). As Table 4 shows: 1) NVIL-15B (non-reasoning) produces zero reflection tokens. 2) FRANK-15B generates multiple reflection cycles per example This demonstrates that our fusion method intrinsically enables iterative self-correction during reasoning. Figure 4 reveals two scaling trends on MMMU: 1) Output length increases with task difficulty (EasyHard). 2) Larger models produce longer responses at each level. This demonstrates our methods inference scaling capability, which automatically expands reasoning for harder problems while benefiting from increased model capacity. 4.6 Case Study: Exemplary Model Outputs To illustrate the qualitative improvements brought by our FRANK fusion, we present three representative examplesone for each model size (8B, 15B, 38B)comparing nonreasoning baseline against the corresponding FRANK variant. In each case, the baseline model provides direct (and incorrect) answer, whereas the FRANK model produces detailed 7 Figure 5: Output examples from FRANK-8B and the non-reasoning baseline model Idecifics3-8B. Here, <think> and </think> denote R1-like reasoning processes, while blue text indicates reflection tokens. Figure 6: Output examples from FRANK-15B and the non-reasoning baseline model NVIL-15B. 8 Figure 7: Output examples from FRANK-38B and the non-reasoning baseline model InternVL2.5-38B. 9 [7] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-V: Exploring long-chain visual reasoning with multimodal large language models. In CVPR, 2025. [8] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [9] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. VisualRFT: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [10] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. VisionR1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [11] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. LMM-R1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [12] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-VL: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [13] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In ICML, volume 162, pages 2396523998, 2022. [14] Gabriel Ilharco, Marco Túlio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In ICLR, 2023. [15] Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with arithmetic operation. In NeurIPS, New Orleans, LA, USA, 2023. [16] Scott Brincat, Markus Siegel, Constantin von Nicolai, and Earl Miller. Gradual progression from sensory to task-related processing in cerebral cortex. Proceedings of the National Academy of Sciences, 115(30):E7202E7211, 2018. [17] Haruka Kawasaki, Satoshi Nishida, and Ichiro Kobayashi. Hierarchical processing of visual and language information in the brain. In AACL-IJCNLP, pages 405410, 2022. [18] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. In NeurIPS, 2023. [19] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, New Orleans, LA, USA, 2023. [21] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In ICLR, Vienna, Austria, 2024. [22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, New Orleans, LA, USA, 2022. [23] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. [24] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-of-thought prompting In CVPR, pages 14420 for large multimodal models. 14431, Seattle, WA, USA, 2024. [25] Bozhi Luan, Hao Feng, Hong Chen, Yonghui Wang, Wengang Zhou, and Houqiang Li. TextCoT: Zoom in for enhanced multimodal text-rich image understanding. arXiv preprint arXiv:2404.09797, 2024. [26] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. MM-Eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [27] Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. In NeurIPS, New Orleans, LA, USA, 2022. [28] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A. Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. In NeurIPS, New Orleans, LA, USA, 2023. [29] Yuyan Zhou, Liang Song, Bingning Wang, and Weipeng Chen. Metagpt: Merging large language models using model exclusive task arithmetic. In EMNLP, pages 1711 1724, Miami, FL, USA, 2024. [30] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging weights of language models. In ICLR, Kigali, Rwanda, 2023. [31] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In ICML, Vienna, Austria, 2024. [32] Shiqi Chen, Jinghan Zhang, Tongyao Zhu, Wei Liu, Siyang Gao, Miao Xiong, Manling Li, and Junxian He. Bring reason to vision: Understanding perception and reasoning through model merging. arXiv preprint arXiv:2505.05464, 2025. [33] Arthur Jacot, Clément Hongler, and Franck Gabriel. Neural Tangent Kernel: Convergence and generalization in 10 Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert In CVPR, pages 95569567, Seattle, WA, USA, AGI. 2024. [47] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. MMMU-Pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. [48] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, Vienna, Austria, 2024. [49] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with mathvision dataset. In NeurIPS, Vancouver, BC, Canada, 2024. [50] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [51] Qwen. Qwq. https://qwenlm.github.io/blog/qwq-32bpreview/, 2024. [52] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, volume 8693, pages 740755, Zurich, Switzerland, 2014. [53] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large language models. In CAICE, pages 405409, 2024. neural networks. In NeurIPS, pages 85808589, Montreal, Canada, 2018. [34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [35] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: human-centric benchmark for evaluating foundation models. In NAACL, pages 2299 2314, Mexico City, Mexico, 2024. [36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2628626296, Seattle, WA, USA, 2024. [37] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext. https://llava-vl.github.io/blog/2024-01-30-llava-next/, 2024. [38] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. VILA: on pre-training for visual language models. In CVPR, pages 2667926689, Seattle, WA, USA, 2024. [39] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding visionlanguage models: insights and future directions. In Workshop on RBFM, 2024. [40] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, volume 15075, pages 370387, Milan, Italy, 2024. [41] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. [42] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy visual task transfer. TMLR, 2024. [43] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [44] OpenAI. GPT-4o. https://openai.com/index/hello-gpt-4o/, 2024. [45] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [46] Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Implementation Details As shown in Table 5, we summarize the precise fusion configurations used in our FRANK variant. For each model size, we list the vision-finetuned model, the reasoning-finetuned model, and the underlying base model that we linearly combine via our layer-wise Taylor-derived weights. All our experiments were conducted on NVIDIA GPUs. In each case, the Non-Reasoning MLLM provides the task vector encoding of the vision-adapted decoder updates, the Reasoning LLM supplies the task vector for pure language reasoning, and the Base Model is the pre-trained backbone into which these vectors are fused according to our attention-guided exponential decay schedule. A.2 Ablation of the Multimodal Prior To validate the contribution of our attention-driven multimodal prior (MP), we perform an ablation study on FRANK-15B across five standard benchmarks. Table 6 compares the full FRANK15B against variant with the MP component removed (w/o MP). On MMMU, accuracy drops from 61.3% to 58.4% when MP is disabled, 2.9-point decrease that underscores the priors role in balancing visual and textual signals. The impact is even more pronounced on MathVision, where performance falls from 37.2% to 28.7%, reflecting 8.5-point degradation in complex visual-mathematical reasoning. We observe consistent drops on the remaining benchmarks (MMMU-Pro, MathVista, and WeMath), indicating that MP systematically guides layer-wise fusion towards more effective cross-modal integration. These results confirm that our multimodal prior is essential for robust reasoning: by weighting visual and language contributions according to per-layer attention, it ensures that FRANK-15B can harness both modalities optimally across diverse tasks. A.3 Ablation of the Attention-Guided Exponential Decay Prior To further assess the effectiveness of our attention-guided exponential decay strategy, we compare against two manually chosen exponential decay factors (0.3 and 0.5) and simple layer-indexed linear decay. As shown in Table 7, both fixed-rate exponentials (59.1% and 56.3%) and the linear schedule (59.6%) improve over the NVIL-15B baseline (53.2%), but remain significantly below our full FRANK-15B (61.3%). This clear margin demonstrates that leveraging per-layer visual attention to drive the decay adapts the modality weighting more precisely than uniform schedules and achieves the best trade-off between visual perception and language reasoning. Layer-Wise Attention Patterns Figures 8 and 10 plot the average per-layer attention weights that each models decoder assigns to visual tokens when processing vision inputs. In both the 8B and 38B variants, we observe clear decay: the first few layers exhibit high visual attention, and then steadily decline toward deeper layers. This consistent decay profile confirms our multimodal prior hypothesis across both scales, justifying the use of an exponential decay schedule guided by these attention statistics. Task-Vector Cosine Similarity Figures 9 and 11 report the cosine similarity between the task vectors extracted from vision-finetuned models (Idefics3-8B / InternVL2.5-38B) and those from reasoning-finetuned counterparts (DeepSeekDistilLLaMA3-8B / QwQ-32B) at each decoder block. In both cases, the similarity values remain close to zero across almost all layers, confirming that the vision and reasoning task vectors occupy nearly orthogonal subspaces. This orthogonality underpins our linear fusion derivation via Taylor expansion, ensuring that the combined update remains meaningful superposition of the two modalities without destructive interference. A.5 Proof of NTK Linearization (Property) Under the Neural Tangent Kernel (NTK) regime for wide transformers, the network behavior near initialization θ0 admits: (x; θ0 + θ) = (x; θ0) + θ (x; θ0)θ + O(θ2/ width). (17) For task-specific loss L(l) , the Hessian exhibits two key propt erties: Jacobian dominance: Residual terms vanish due to 2 (l) O(1/ width). Isotropy: Gradient directions become nearly orthogonal in high dimensions. This leads to the layerwise Hessian approximation: (l) (l)(cid:105) (cid:104) (θ) Ex where the curvature scalar: (θ(l) 0 ) (cid:104) (l)(x; θ(l) 0 )2(cid:105) 2L(l) 2L(l) δ(l) Idl , (18) (19) δ(l) Ex Tr (cid:16) , (cid:17) = 1 dl = 1 dl captures the average gradient magnitude at initialization. A.6 Proof of Task-Vector Orthogonality (Property) Figure 2 shows that, vision-fine-tuning and reasoning-finetuning update disjoint aspects of the LLMs representations. Concretely, let τ(l) = θ(l) θ(l) 0 , τ(l) = θ(l) θ(l) 0 . We observe that )τ(l) where ε is small constant, indicating near-orthogonality. = O(ε), (τ(l) (20) (21) A.4 Validation of Layer-Wise Multimodal Prior and Task-Vector Orthogonality A.7 Proof of Taylor Expansion of Layer Task Loss Difference (Lemma) To verify the two key assumptions underpinning our fusion strategynamely that (1) shallow decoder layers focus predominantly on visual perception while deeper layers prioritize language reasoning (the multimodal prior), and (2) task vectors from vision-finetuned and reasoning-finetuned models are mutually orthogonalwe analyze both Idefics3-8B and InternVL2.538B. To quantify the impact of parameter fusion on task performance, we analyze the Layer-wise Task Loss Difference (LTLD) through rigorous Taylor expansion approach. The fused parameters combine both task updates through convex combination: τ(l) λ(l) , where λ(l) [0, 1], (22) (cid:88) = θ(l) 0 θ(l) + 12 t{V,R} Table 5: Detailed fusion components for FRANK configurations FRANK Variant Non-Reasoning MLLM Reasoning LLM Base Model FRANK-8B FRANK-15B FRANK-38B Idefics3-8B NVIL-15B InternVL2.5-38B DeepSeekDistil-LLaMA3-8B DeepSeekDistil-Qwen2.5-14B QwQ-32B LLaMA3.1-8B Qwen2.5-14B Qwen2.5-32B Table 6: Ablation study results of FRANK-15B across five multimodal reasoning benchmarks: MMMU val, MMMU-Pro standard (10 opts), MathVista testmini, MathVision testmini, and WeMath testmini. * indicates the baseline model. Methods MMMU MMMU-Pro MathVista MathVision WeMath NVILA* FRANK-15B w/o MP FRANK-15B 53.2 58.4 61.3 36.2 41.8 49.4 67.6 52.5 55. 23.2 28.7 37.2 31.1 31.4 32.3 Table 7: Ablation study results of attention-guided exponential decay prior on the MMMU. Method MMMU Acc. NVIL-15B (baseline) 0.3 0.5 linear FRANK-15B 53.2 59.1 56.3 59.6 61.3 A.8 Derivation of Closed-Form Fusion Weights To enable efficient model merging while preserving task performance, we derive theoretically-grounded fusion weights through layer-wise NTK and Task-Vector Orthogonality analysis. The key insight is that LLMs exhibit approximately quadratic loss landscapes under NTK, permitting closed-form solutions. Under the NTK linearization regime for LLMs, we first analyze the layer-wise behavior. For any layer with initialization θ(l) 0 and perturbation τ(l) = θ(l) θ(l) 0 , the parameter admits the first-order approximation: 0 denotes the pre-trained initialization at layer l, θ(l) = denotes the fine-tuned parameters for task {V, R}, where, θ(l) θ(l) + τ(l) 0 denotes the task-vector update from θ(l) τ(l) Fusion Residual Vector: Measures deviation from optimal task parameters: 0 to θ(l) . h(l) = θ(l) θ(l) (cid:88) = k(cid:44)t{V,R} τ(l) λ(l) (1 λ(l) )τ(l) Interpolation Path: Defines linear trajectory in parameter space from the fine-tuned parameters θ(l) to the fused parameters θ(l) , enabling exact Taylor expansion along the fusion direction: + βh(l) t (β) = θ(l) γ(l) β [0, 1]. (24) , Taylor Expansion Analysis: Applying second-order expansion along γ(l) : (θ(l) L(l) , xt) , xt) = L(l) (θ(l) = L(l) (θ(l) + 1 2 h(l) + h(l) , xt) + L(l) (cid:32)(cid:90) (θ(l) 2L(l) (γ(l) 0 , xt)h(l) (cid:33) (β))dβ (25) h(l) , xt) 0, we obtain: Under fine-tuning convergence L(l) , xt) L(l) (cid:32)(cid:90) 1 LTLD(l) (θ(l) = L(l) (θ(l) (θ(l) , xt) = 1 h(l) 0 2L(l) (γ(l) (cid:33) (β))dβ (26) h(l) . This derivation establishes the theoretical foundation for our layer-wise fusion analysis, connecting parameter perturbations to task performance through differentiable geometry. 13 (l)(x; θ(l)) (l)(x; θ(l) 0 ) + θ(l) (l)(x; θ(l) 0 )τ(l) (cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125) Linear term +O(τ(l)2) (27) This linearity emerges in wide transformers where the networks output changes linearly with parameter perturbations. For the quadratic loss L(l) compute the Hessian: 2 (l)(xt; θ(l)) yt2, we (θ(l), xt) = . (23) 2L(l) = (l)(xt; θ(l)) (l)(xt; θ(l)) + ( (l)(xt; θ(l)) yt)2 (l)(xt; θ(l)) (cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125) Vanishes under NTK regime (28) Under the NTK regime, the second term becomes negligible due to two intrinsic properties of wide neural networks: 1) the output residual (l) yt vanishes with near-optimal fine-tuning (guaranteed by NTKs convex-like optimization landscape), and 2) the Hessian 2 (l) shrinks as O(1/ width) (a direct consequence of NTKs linearization effect). Under NTK conditions: 2L(l) (θ(l)) (l)(xt; θ(l) 0 ) (l)(xt; θ(l) 0 ) (29) Taking the isotropic approximation for layers: 2L(l) δ(l) Idl , δ(l) = 1 dl (cid:16) tr (l)(xt; θ(l) 0 ) (l)(xt; θ(l) 0 )(cid:17) (30) where dl is the parameter dimension at layer l. This follows from the observation that gradient directions become nearly orthogonal in high dimensions. Substituting the Hessian approximation into the Taylor remainder: 2L(l) (γ(l) (β)) dβ δ(l) I, (31) (cid:90) 1 0 Figure 8: Layer-wise visual attention of Idefics3-8B. Figure 10: Layer-wise visual attention of InternVL2.5-38B. Cosine similarity between task vectors of Figure 9: vision-finetuned reasoning-finetuned and (DeepSeekDistil-LLaMA3-8B) models at each decoder block. (Idefics3-8B) Figure 11: Cosine similarity between task vectors of visionfinetuned (InternVL2.5-38B) and reasoning-finetuned (QwQ32B) models at each decoder block. collapses the quadratic form to scalar multiple of h(l) 2, yielding tractable bound and closed-form solution for fusion weights. The second-order Taylor remainder is bounded by the extremal eigenvalues of the integral Hessian. Substituting our isotropic approximation: h(l) = δ(l) δ(l) Idl h(l) 2 2LTLD(l) h(l) (cid:17) (cid:16) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) = δ(l) (cid:88) k(cid:44)t{V,R} λ(l) τ(l) (1 λ(l) )τ(l) 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) (32) where we used the identity + b2 = a2 + b2 + 2ab. The cross-term vanishes due to task vector orthogonality. This demonstrates that the layer-wise task loss difference is governed by the product of the initialization curvature δ(l) and the squared fusion residual norm h(l) 2, justifying our layer-wise analysis. Next, we show the optimal layer-wise fusion weights. The layerwise analysis enables independent optimization at each layer. The LALD decomposes into layer-specific terms: LALD(l) (cid:88) t{V,R} δ(l) τ(l) 2 (1 λ(l) )2τ(l) 2 + (cid:88) k(cid:44)t{V,R} (λ(l) )2τ(l) (33) This additive structure permits layer-wise optimization by solving (where is the total number of layers) independent problems: {λ(l) }t{V,R} = arg min J(l), = 1, ..., (34) 14 Focusing on single layer (omitting superscript (l) for clarity), we reformulate the objective: (cid:88) = t{V,R} δt 2 = 1 2 (cid:88) t{V,R} τt2(cid:104) (1 λt)2τt2 + (cid:88) kτk2(cid:105) λ2 δtτt4(1 λt)2 + 1 2 k(cid:44)t{V,R} (cid:88) (cid:88) t{V,R} k(cid:44)t{V,R} δtλ2 kτt2τk2 Under task orthogonality τ vanish, simplifying the gradient to: (35) τR = 0, the cross-derivative terms = δtτt4(1 λt) + λtτt2 (cid:88) δkτk2 λt 0 = δtτt4 + λt k(cid:44)t{V,R} δtτt4 + τt2 (cid:88) δkτk2 k(cid:44)t{V,R} (36) Solving the linear system yields: λt = δtτt4 δtτt4 + τt2 (cid:80) k(cid:44)t δkτk2 = (cid:80) δtτt2 k{V,R} δkτk2 (37) Under the uniform curvature assumption δk δ0 (arising from NTKs layer-wise gradient statistics in wide networks, where Tr( (l) (l)) becomes task-invariant as dl ), the solu1 dl tion simplifies to: λ(l) = τ(l) 2 k{V,R} τ(l) 2 (cid:80) λ(l) τ(l) 2 (38) This closed-form solution adaptively suppresses interfering task vectors while preserving target task information at each layer. A.9 Derivation of Attention-Guided Exponential Decay Priors In Section 3.2.3 we introduce an attention-guided exponential schedule for modality priors. Here, we provide step-by-step derivation. Modeling the decay. We observe the layer-wise visualattention ratio (cid:88) al = 1 Attnlvis(x, h) Attnlvis(x, h) + Attnltext(x, h) = 1, . . . , L, x, (39) where indexes inputs, indexes attention heads, and is normalization factor. We posit an exponential decay model: al eα l, (40) where > 0 and α > 0 are unknown constants. Logarithmic linearization. Taking natural logarithm on both sides of Eq. ( 40), we obtain ln al ln α l. (41) Define yl = ln al, xl = l, b0 = ln C, b1 = α. Then Eq. ( 41) becomes linear regression problem: yl b0 + b1 xl. (42) Least-squares estimation. We collect the dataset {(xl, yl)}L l=1 and solve the normal equations for the least-squares fit: b1 = (cid:80)L l=1(xl x)(yl y) (cid:80)L l=1(xl x) , where = 1 (cid:80) b0 = b1 x, (cid:80) xl and = 1 yl. We then recover ˆα = b1, ˆC = b0 . (43) (44) Constructing modality priors. Finally, we define the normalized exponential priors w(l) = ˆα j=1 ˆα (cid:80)L , w(l) = 1 w(l) . (45) These priors smoothly interpolate from strong visual emphasis in early layers to strong reasoning emphasis in late layers, and can be replaced by alternative data-driven schedules if desired."
        }
    ],
    "affiliations": [
        "School of Remote Sensing and Information Engineering, Wuhan University"
    ]
}