{
    "paper_title": "3D CoCa: Contrastive Learners are 3D Captioners",
    "authors": [
        "Ting Huang",
        "Zeyu Zhang",
        "Yemin Wang",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D captioning, which aims to describe the content of 3D scenes in natural language, remains highly challenging due to the inherent sparsity of point clouds and weak cross-modal alignment in existing methods. To address these challenges, we propose 3D CoCa, a novel unified framework that seamlessly combines contrastive vision-language learning with 3D caption generation in a single architecture. Our approach leverages a frozen CLIP vision-language backbone to provide rich semantic priors, a spatially-aware 3D scene encoder to capture geometric context, and a multi-modal decoder to generate descriptive captions. Unlike prior two-stage methods that rely on explicit object proposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a shared feature space, eliminating the need for external detectors or handcrafted proposals. This joint training paradigm yields stronger spatial reasoning and richer semantic grounding by aligning 3D and textual representations. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that 3D CoCa significantly outperforms current state-of-the-arts by 10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at https://github.com/AIGeeksGroup/3DCoCa."
        },
        {
            "title": "Start",
            "content": "3D CoCa: Contrastive Learners are 3D Captioners Ting Huang1 Zeyu Zhang2 Yemin Wang3 Hao Tang4 1Shanghai University of Engineering Science 2The Australian National University 3Xiamen University 4Peking University Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. 5 2 0 A 3 1 ] . [ 1 8 1 5 9 0 . 4 0 5 2 : r Abstract 3D captioning, which aims to describe the content of 3D scenes in natural language, remains highly challenging due to the inherent sparsity of point clouds and weak cross-modal alignment in existing methods. To address these challenges, we propose 3D CoCa, novel unified framework that seamlessly combines contrastive visionlanguage learning with 3D caption generation in single architecture. Our approach leverages frozen CLIP vision-language backbone to provide rich semantic priors, spatially-aware 3D scene encoder to capture geometric context, and multi-modal decoder to generate descriptive captions. Unlike prior two-stage methods that rely on explicit object proposals, 3D CoCa jointly optimizes contrastive and captioning objectives in shared feature space, eliminating the need for external detectors or handcrafted proposals. This joint training paradigm yields stronger spatial reasoning and richer semantic grounding by aligning 3D and textual representations. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that 3D CoCa significantly outperforms current stateof-the-arts by 10.2% and 5.76% in CIDEr@0.5IoU, respectively. Code will be available at https://github.com/AIGeeksGroup/3DCoCa. CCS Concepts Computing methodologies Scene understanding; Keywords 3D Captioning, Contrastive Learning, Multimodal Vision-Language Model"
        },
        {
            "title": "1 Introduction\nIn recent years, 3D learning research has been increasing, driven by\nvarious practical applications such as robotics, autonomous driv-\ning, and augmented reality [14, 15, 24, 39]. Within this burgeoning\nfield, the intersection of computer vision (CV) and natural language\nprocessing (NLP) has prompted researchers to strive to bridge the\ngap between visual perception and language expression, thus pro-\nmoting the rise of cross-modal tasks such as visual captioning. The\nemergence of large-scale vision-language models has brought un-\nprecedented breakthroughs in the generation of captions for 2D\nimages. With the development of 3D vision-language datasets, 3D\ncaptions have also shown promising prospects. 3D captioning ex-\ntends 2D image captioning and aims to accurately perceive the\n3D structure of objects and generate reasonable descriptions by\nleveraging a comprehensive set of attribute details and contextual\ninteraction information between objects and their surroundings.\nHowever, due to the sparsity of point clouds and the cluttered dis-\ntribution of objects, describing objects within a 3D scene remains a\nparticularly challenging endeavor.",
            "content": "Figure 1: Conceptual homepage figure for 3D CoCa, highlighting its architecture (left) and performance (right). Left: The 3D CoCa model unifies contrastive learning and multimodal captioning in one framework. Right:Radar chart comparison of 3D CoCa and previous methods Scan2Cap [6], 3DJCG [3], 3D-VLP [43], Vote2Cap-DETR [12], Vote2Cap-DETR++ [13] on the ScanRefer [8] benchmark. Early approaches to 3D dense captioning adopted two-stage detect-then-describe paradigm, where object proposals were first detected from point clouds and then described individually. For example, Scan2Cap [6] is the first attempt to integrate 3D object detection and caption generation into 3D scenes in cascade manner. [21] introduced novel 3D language pre-training approach that uses context-aware alignment and mutual masking to learn generic representations for 3D dense captioning tasks. Although effective, two-stage pipeline can suffer from significant performance degradation. First, the detection stage usually produces redundant bounding boxes, and thus careful tuning using the Non-Maximum Suppression (NMS) [29] operation is required, which introduces additional hyperparameters and increases computational overhead. Second, the cascade design of the detect-then-describe process makes caption generation highly dependent on the quality of the detection stage. In this context, the exploration of one-stage endto-end 3D dense captioning models has attracted widespread attention. Vote2Cap-DETR [12] and its advanced version Vote2CapDETR++ [13] are notable examples, using the Transformer framework to simultaneously locate and describe objects during inference in single forward pass, improving both efficiency and performance. Other recent approaches, such as BiCA [23] introduced Bi-directional Contextual Attention mechanism to disentangle object localization from contextual feature aggregation in 3D scenes and See-It-All (SIA) model [22] adopted late aggregation strategy to capture both local object details and global contextual information with novel aggregator. Moreover, TOD3Cap [20] employed Birds Eye View (BEV) representation for the generation of object proposals and integrated the Q-Former Relation with the LLaMAAdapter to generate descriptive sentences, particularly for outdoor environments. Despite progress, 3D captioning remains very challenging, especially in modeling spatial relations and aligning 3D visual data with textual semantics. Describing complex spatial arrangements requires the model to understand 3D geometry and relative object positions, which is non-trivial to encode and reason about. Bridging the gap between the 3D modality and language is also difficult. Existing methods treat vision and language as separate stages with weak cross-modal interaction. This leads to suboptimal alignment between visual and textual representations. These challenges point to the need for unified framework that can enhance spatial reasoning and cross-modal alignment using strong visual-linguistic priors. Foundation models in visionlanguage research CoCa [40] have shown that contrastive pretraining on large image-text corpora yields representations with rich semantics and excellent alignment between modalities. Inspired by this, we hypothesize that bringing such powerful priors into 3D captioning will significantly improve performance and generalization. This insight motivates us to design 3D captioning approach that jointly learns spatially-grounded captions and visual-text alignments within single end-to-end model, leveraging knowledge from large-scale vision-language training. In this paper, we introduce 3D CoCa (Contrastive Captioner for 3D), as illustrated in Figure 1, novel approach that integrates contrastive learning and caption generation into unified model for 3D scenes. The core idea is to train 3D scene encoder and text encoder together with shared contrastive learning objective, while simultaneously training multi-modal decoder to generate captions. By coupling these tasks, 3D CoCa learns joint feature space where 3D representations and captions are deeply aligned. The model leverages rich semantic knowledge from large-scale pretraining: we build on vision-language backbone initialized with learned visual and linguistic features, injecting strong priors about objects and language into the 3D domain. This allows the model to recognize wide range of concepts in the scene and associate them with the correct words. Furthermore, 3D CoCa is designed to be spatially aware the 3D scene encoder preserves geometric structure, and the decoders attention mechanism can attend to specific regions when wording the description. As result, the generated captions capture not only object attributes, but also their precise spatial context, directly addressing the core difficulty of 3D captioning. In essence, our approach marries powerful contrastive learner with captioning model, demonstrating that contrastive learners are effective 3D captioners. In summary, the main contributions of this work include: We propose 3D CoCa, the first end-to-end framework to unify contrastive vision-language learning with 3D captioning. This design eliminates the need for external 3D object detectors by jointly learning to localize and describe from point clouds. We demonstrate how to leverage strong visual-linguistic priors from large-scale image-text pretraining within 3D captioner. By integrating contrastive alignment objective, our model attains improved semantic understanding and cross-modal alignment, enabling richer and more accurate captions for complex 3D scenes. Extensive evaluations on benchmark datasets show that 3D CoCa achieves state-of-the-art captioning performance on Nr3D [1] (52.84% C@0.5) and Scanrefer [8] (77.13% C@0.5)."
        },
        {
            "title": "2 Related Works",
            "content": "3D Dense Captioning. 3D dense captioning involves localizing objects in 3D scene and describing them in natural language. Early work like Scan2Cap [6] pioneered this task by leveraging point cloud data with spatial reasoning, marking departure from conventional 3D detection pipelines focused only on classification and bounding boxes [4, 5, 44, 45]. Subsequent methods were built on this foundation with improved relational modeling. For example, the Multi-Order Relation Extraction (MORE) framework [19] introduced higher-order relationship reasoning, showing that richer spatial context leads to more informative and accurate captions. The introduction of Transformer architectures further accelerated progress in 3D captioning. SpaCap3D [36] employed Transformerbased encoderdecoder with spatially guided encoder to capture geometric context and an object-centric decoder for attributerich descriptions. ğœ’-Trans2Cap [42] extended this idea by distilling knowledge from 2D vision-language models into 3D captioner, effectively transferring semantic understanding from images to point clouds. Recent works strive for unified architectures that handle multiple tasks: 3DJCG [3] uses shared Transformers to jointly optimize 3D captioning and visual grounding, and UniT3D [7] demonstrates that pre-training Transformer on large-scale point cloudtext pairs can yield state-of-the-art results across diverse 3D scene understanding benchmarks. Despite these advances, most approaches still follow two-stage detect-then-describe paradigm [3, 6, 36, 42], where an object detector provides regions that are then described. This separation can cause error propagation and misalignment between the vision and language components. To overcome this limitation, end-toend paradigms have been explored. Vote2Cap-DETR [12] and its improved variant Vote2Cap-DETR++ [13] reformulate dense captioning as direct set-prediction task, similar to DETR in 2D vision. They jointly localize and caption objects in one stage, eliminating dependence on pre-trained detectors. Through Transformer encoderdecoder with learnable queries and iterative refinement, these one-stage models achieve competitive performance while simplifying the pipeline. 3D Pre-training and Vision-Language Foundations. Another line of work has focused on pre-training 3D representations to provide stronger foundations for downstream tasks. Unsupervised 3D representation learning techniques can be categorized into global contrastive methods [28, 35] that learn holistic point cloud embeddings, local contrastive methods [37, 38] that distinguish fine-grained geometric structures or multi-view correspondences, and masked point modeling approaches [30, 41] that adapt masked autoencoding to 3D data. These approaches learn powerful geometric features; however, they operate purely on 3D geometry and lack grounding in natural language semantics. To bridge this gap, researchers have explored 3D vision-language pre-training. For example, 3D-VLP [43] uses contrastive learning to Figure 2: Illustration of multi-modal Transformer architecture for 3D vision-language understanding. The input point cloud and textual description are processed by CLIP Vision and Text Encoders, respectively. Cross-attention mechanisms fuse these features within Multi-Modal Decoder, enabling the generation of descriptive captions. The model training is guided by contrastive and captioning losses, promoting effective alignment between visual and textual modalities. align point cloud segments with text descriptions, yielding representations that improve 3D dense captioning and visual grounding performance by injecting semantic knowledge. Similarly, UniT3D [7] showed that training on large-scale point cloudcaption pairs endows unified model with strong multi-task 3D understanding capabilities. Such findings underscore the value of learning joint 3Dlanguage representations as foundation for captioning. Multimodal Large Language Models for 3D Scenes. Recently, the success of large language models in vision-language tasks has sparked interest in extending them to 3D scene understanding. representative example is LL3DA [11], Large Language 3D Assistant that combines 3D visual inputs with an LLM, allowing the model to follow natural-language instructions and generate responses about 3D scene. This enables interactive tasks such as 3D captioning, visual grounding, and question answering by leveraging the reasoning ability of LLMs. Similarly, Chat-3D [17] aligns point cloud features directly with pretrained language models embedding space, demonstrating impressive conversational capabilities to describe 3D environments. Such systems illustrate the promise of MLLMs for 3D grounding, dense captioning, and dialogue-based interaction. However, these LLM-driven frameworks typically rely on an external language model and complex alignment procedures, treating captioning as just one of many tasks rather than dedicated end-to-end objective. Consequently, fine-grained spatial details can be difficult to handle without additional tricks. In contrast, 3D CoCa takes different route: it directly integrates multimodal pre-training into unified captioning architecture. By jointly training 3D scene encoder and text decoder with contrastive vision-language objective, 3D CoCa harnesses rich semantic priors from foundation models while remaining end-to-end trainable for the captioning task. This design eliminates the need for separate detection modules or post-hoc LLM integration; to our knowledge, 3D CoCa is the first to unify contrastive vision-language pre-training with 3D dense captioning in single model, marking novel paradigm in 3D captioning within the evolving MLLM-centered landscape."
        },
        {
            "title": "3 The Proposed Method\n3.1 Overview\nIn this section, we present the proposed 3D CoCa, a framework that\nbridges the gap between 3D point cloud representation learning\nand natural language understanding for captioning. Our approach\nbuilds on principles of contrastive alignment and multi-modal cap-\ntioning, inspired by the successes of CLIP-style image-text mod-\nels [33] and the Contrastive Captioner (CoCa) paradigm [40]. As\nillustrated in Figure 2, 3D CoCa consists of four key components: a\n3D Scene Encoder, a Text Encoder, a Contrastive Learning module,\nand a Multi-Modal Fusion Decoder.",
            "content": "Unlike traditional methods that focus on either 2D images or purely 3D data, 3D CoCa leverages knowledge distilled from largescale 2D-text pre-training and adapts it to the complexities of point cloud data. Most of CLIPs pre-trained weights are frozen in our framework to preserve the robust visual and linguistic representations, introducing only minimal additional parameters for 3D processing. The following subsections describe each component in detail. We conclude this section with the joint training objectives that bind these components into unified model for generating captions from 3D scenes. 3.2 3D Scene Encoder The role of the 3D scene encoder is to transform an unstructured point cloud into set of latent tokens that capture the scenes geometric and semantic content. We build our scene encoder based on the EPCL architecture [18], design that integrates point-based processing with frozen 2D CLIP visual backbone. The encoder comprises three parts: (i) point cloud tokenizer that groups points into patch tokens, (ii) set of learnable task tokens that inject 3Dcaptioning context, and (iii) frozen CLIP vision transformer that encodes the combined token sequence. Figure 2 (top-left) depicts how raw point clouds are converted into tokens and fed into the encoder. 3.2.1 Point cloud tokenizer. Given an input point cloud ğ‘ƒ Rğ‘ (3+ğ¹ ) (with ğ‘ points, each described by 3D coordinates (ğ‘¥, ğ‘¦, ğ‘§) and ğ¹ additional features such as color, normal, height or multiview feature), we first convert it into discrete token sequence. We sample ğ‘€ representative points as patch centers using farthest point sampling (FPS) to ensure even coverage of the scene. FPS reduces redundancy in dense regions while preserving structure in sparse areas. Next, for each sampled center, we group its ğ¾ nearest neighbor points to form local patch. This yields ğ‘€ patches ğ‘ƒ1, ğ‘ƒ2, . . . , ğ‘ƒğ‘€ , each containing ğ¾ points that are spatially proximate. We then pass each patch through small point-wise network (a series of Multi-Layer Perceptrons, MLPs) to encode local geometry and appearance features. This produces set of ğ‘€ point tokens (one per patch), each ğ·ğ‘ -dimensional embedding: ğ¸ğ‘ (ğ‘ƒ) = [eğ‘1, eğ‘2, . . . , eğ‘ğ‘€ ] Rğ‘€ ğ·ğ‘ , where eğ‘ğ‘– is the embedding of the ğ‘–-th patch. By treating each local patch as token, the continuous 3D data is converted into structured sequence of vectors. This tokenization balances fine local detail (within each patch of ğ¾ points) and global coverage (through the ğ‘€ sampled patches) of the scene. (1) 3.2.2 Task token mechanism. While the above point tokens capture visual elements of the scene, the model still needs guidance that the task is 3D captioning (describing the scene in words). To provide this context, we introduce small set of learnable task tokens. Each task token is an embedding vector (implemented as part of the model parameters) that is prepended to the sequence of point tokens. Following the prompt tuning approach in [26], we initialize these task token embeddings with distinct fixed values (e.g. enumerated numbers) and allow them to be learned. The task tokens act as high-level prompt or query that informs the model about the captioning task. By attending over the entire point cloud, these tokens learn to pull out global semantic information (e.g. the overall scene context or salient objects) that is useful for generating descriptive text. In essence, the task tokens provide shared contextual bias for the 3D scene, helping the encoder emphasize elements relevant to language description. Frozen CLIP vision encoder. After obtaining the ğ‘€ point to3.2.3 kens and ğ‘šğ‘¡ task tokens, we concatenate them into single sequence: [eğ‘1, . . . , eğ‘ğ‘€ ; t1, . . . , tğ‘šğ‘¡ ], (2) where tğ‘— denotes the ğ‘—-th task token embedding. This combined sequence of length ğ‘€ + ğ‘šğ‘¡ is then fed into the CLIP visual Transformer encoder [33]. We adopt the CLIP image encoder architecture and keep its weights frozen to leverage the rich visual features it learned from massive image-text data. Freezing the CLIP vision backbone preserves its robust representation power and stabilizes training we avoid updating large number of parameters, thus preventing catastrophic forgetting of prior knowledge. It also improves efficiency: with most parameters fixed, memory usage, and training time are significantly reduced. The CLIP vision encoder processes the token sequence and outputs sequence of latent features in high-dimensional space. This output encodes both the 3D geometry and the task context. From these outputs, we can derive global scene representation that will be used for downstream alignment with text. In practice, we obtain the global 3D scene feature ğ‘“ğ‘’ğ‘›ğ‘ from the CLIP encoders output. This feature ğ‘“ğ‘’ğ‘›ğ‘ Rğ· with ğ· the encoder output dimension is compact, semantically rich summary of the entire 3D scene conditioned on the captioning task. It encapsulates the visual content in form suitable for aligning with language and will serve as the 3D scene embedding for the contrastive learning module."
        },
        {
            "title": "3.3 Text Encoder\nWhile the 3D scene encoder encodes visual information from point\nclouds, the text encoder processes natural language descriptions\ninto a compatible embedding space. We use the text encoder branch\nof CLIP [33] to obtain language features. This text encoder is a\nTransformer-based model that we also keep frozen, so as to exploit\nthe linguistic knowledge gained from large-scale pre-training. By\nusing a fixed pre-trained text encoder, we ensure that our captions\nare encoded in the same semantic space as the CLIP representations,\nwhich facilitates alignment with the 3D scene features.",
            "content": "3.3.1 Text tokenizer. Given an input sentence ğ‘‡ , we first tokenize it into sequence of ğ¿ tokens. Each token ğ‘¤ğ‘– is mapped to an embedding vector in Rğ·ğ‘¡ using learned embedding table. This produces sequence of text token embeddings: (3) ğ¸ğ‘¡ (ğ‘‡ ) = [eğ‘¡1, eğ‘¡2, . . . , eğ‘¡ğ¿ , ] Rğ¿ğ·ğ‘¡ , where eğ‘¡ğ‘– corresponds to the ğ‘–-th token in the sentence. We prepend special beginning-of-sequence token to this sequence, which will be used to aggregate the sentence-level information. We also add positional encodings to each token embedding ğ¸ğ‘¡ (ğ‘‡ ) to preserve the order of words, which is crucial to capture the syntactic structure and meaning of the caption. We employ subword tokenizer to handle out-of-vocabulary words by breaking them into known subunits, ensuring that any arbitrary caption can be represented by the token sequence. Frozen CLIP text encoder. The sequence of text embeddings 3.3.2 ğ¸ğ‘¡ (ğ‘‡ ) is then passed through the CLIP text Transformer encoder, which has ğ‘ğ‘¡ğ‘’ layers of multi-head self-attention and feed-forward networks. We denote the hidden states at layer ğ‘™ as ğ»ğ‘™ with ğ» 0 = ğ¸ğ‘¡ (ğ‘‡ ) being the input. The Transformer applies its layers successively: batch. For the ğ‘–-th scene and ğ‘—-th text in the batch, the similarity is defined as: ğ»ğ‘™ = TransformerBlockğ‘™ (ğ»ğ‘™ 1), ğ‘™ [1, . . . , ğ‘ ğ‘¡ğ‘’], comprising self-attention, layer normalization, and MLP sublayers in each block. We keep all weights of this text encoder frozen during training to preserve the rich language understanding it acquired through pre-training on image-text pairs. Freezing also mitigates overfitting, given that 3D captioning datasets are relatively small compared to general text corpora. (4) From the final layer of the text Transformer, we extract the output corresponding to the special [CLS] token, which we treat as the global text representation for the caption. Denote this vector ğ‘’ğ‘›ğ‘ Rğ·ğ‘¡ . This vector encodes the semantic content of the as ğ‘“ ğ‘¡ entire description ğ‘‡ in single feature. It will be used in our contrastive learning module to align with the 3D scene feature ğ‘“ğ‘’ğ‘›ğ‘ from the scene encoder. By using CLIPs text encoder and keeping it fixed, we ensure ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘ lies in language embedding space that is directly comparable to CLIP visual features, aiding the multimodal alignment."
        },
        {
            "title": "3.4 Contrastive Learning Paradigm\nTo bridge the heterogeneous modalities of 3D point clouds and text,\nwe adopt a contrastive learning strategy for feature alignment. The\ncore idea is to project both the 3D scene feature ğ‘“ğ‘’ğ‘›ğ‘ and the text\nfeature ğ‘“ ğ‘¡\nğ‘’ğ‘›ğ‘ into a shared latent space where corresponding 3D-\nscenes and captions are pulled closer together, while non-matching\npairs are pushed farther apart. This follows in the same spirit as\nthe CLIP multimodal training objective, encouraging the model to\nlearn cross-modal associations. We describe the feature projection\nand normalization, followed by the contrastive loss formulation.",
            "content": "Feature alignment. Before computing the similarity between 3.4.1 ğ‘“ğ‘’ğ‘›ğ‘ and ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘ , we transform them into common embedding space using learnable projection heads. In particular, we apply two small MLPs to map the features to shared dimension. Specifically, we use two-layer MLP to project the features: ğ‘“ğ‘’ğ‘›ğ‘ = MLPğ‘£ (ğ‘“ğ‘’ğ‘›ğ‘ ) , ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘ = MLPğ‘¡ (cid:0)ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘ (cid:1) , (5) where MLPğ‘£ and MLPğ‘¡ are two-layer perceptrons for the 3D scene feature and text feature respectively. Each MLP consists of linear layer, ReLU activation, and second linear layer. These learned projections ensure that the 3D and text embeddings are not only of the same dimension but also tuned for maximal alignment. After projection, we L2-normalize each feature vector to unit length: Ë†ğ‘“ğ‘’ğ‘›ğ‘ = ğ‘“ğ‘’ğ‘›ğ‘ (cid:13) ğ‘“ğ‘’ğ‘›ğ‘ (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13) , Ë†ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘ = ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘ (cid:13) ğ‘“ ğ‘¡ (cid:13) ğ‘’ğ‘›ğ‘ (cid:13)2 (cid:13) (cid:13) (cid:13) . (6) This normalization enables direct comparison via cosine similarity during loss computation. sim (cid:16) Ë†ğ‘“ğ‘’ğ‘›ğ‘,ğ‘–, Ë†ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘,ğ‘— (cid:17) = Ë†ğ‘“ğ‘’ğ‘›ğ‘,ğ‘– Ë†ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘,ğ‘— (cid:13) (cid:13) Ë†ğ‘“ ğ‘¡ Ë†ğ‘“ğ‘’ğ‘›ğ‘,ğ‘– (cid:13) (cid:13) ğ‘’ğ‘›ğ‘,ğ‘— (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) , (cid:13) (cid:13) (cid:13) (7) which is simply the dot product of the two unit-normalized feature vectors. The contrastive learning objective then maximizes the similarity of each scene with its matched caption (where ğ‘– = ğ‘—) while minimizing its similarity with unmatched captions (ğ‘– ğ‘—). Specifically, for each scene ğ‘–, we define the contrastive loss using softmax over the ğ‘ captions: LCon = 1 ğ‘ ğ‘ ğ‘–=1 log exp (cid:16)sim(cid:0) Ë†ğ‘“ğ‘’ğ‘›ğ‘,ğ‘–, Ë†ğ‘“ ğ‘¡ ğ‘—=1 exp (cid:16)sim(cid:0) Ë†ğ‘“ğ‘’ğ‘›ğ‘,ğ‘–, Ë†ğ‘“ ğ‘¡ (cid:205)ğ‘ ğ‘’ğ‘›ğ‘,ğ‘– (cid:17) (cid:1)/ğœ (cid:1)/ğœ ğ‘’ğ‘›ğ‘,ğ‘— , (cid:17) (8) where ğœ is learnable temperature parameter that scales the logits before softmax. This InfoNCE loss encourages sim(ğ‘“ğ‘’ğ‘›ğ‘,ğ‘–, ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘,ğ‘– ) to be larger than sim(ğ‘“ğ‘’ğ‘›ğ‘,ğ‘–, ğ‘“ ğ‘¡ ğ‘’ğ‘›ğ‘,ğ‘— ) for any ğ‘— ğ‘–, thereby aligning the ğ‘–-th 3D scene only with its correct description. In summary, the contrastive loss LCon provides strong supervisory signal that couples the 3D scene features and text features, driving the model to produce joint embedding space where cross-modal correspondences are captured."
        },
        {
            "title": "3.5 Multi-Modal Fusion Decoder\nThe final component of 3D CoCa is the multi-modal fusion decoder,\nwhich generates natural language descriptions for the input 3D\nscene. This decoder takes the aligned 3D-text representations and\nfuses them to produce fluent, contextually grounded sentences.\nWe design the decoder as an autoregressive Transformer that uses\ncross-attention to incorporate visual context at each step of gen-\neration. In essence, the decoder serves as a conditional language\nmodel: it outputs a caption word-by-word, while attending to the\n3D scene features to ensure the caption accurately describes the\nscene. By leveraging the aligned features from the contrastive stage,\nthe decoder can inject detailed 3D scene information into the gen-\neration process, producing descriptions that are both coherent and\nfaithful to the visual input.",
            "content": "The decoder operates in an autoregressive manner. It begins with special start-of-sequence token and generates the caption one token at time. At each time step ğ‘¡, the decoder has access to all previously generated words ğ‘¦<ğ‘¡ as context, and predicts the next word ğ‘¦ğ‘¡ . This causal self-attention mechanism within the decoder allows it to capture intra-sentence dependencies, ensuring that the resulting sentence is grammatically correct and contextually consistent. In parallel, at every decoding step, the decoder is conditioned on the 3D scene representation, so that what it writes is grounded in the scene content. We achieve this through cross-attention mechanism. 3.4.2 Contrastive loss function. With the features projected and normalized, we employ contrastive loss to train the model to align the correct 3D-text pairs. We follow the InfoNCE loss formulation popularized by CLIP. Consider training batch of ğ‘ pairs of 3D scenes and their corresponding captions. We first compute the pairwise cosine similarities between all scenecaption pairs in the 3.5.1 Cross-Attention mechanism. To integrate visual information from the 3D scene into the captioning process, the decoder incorporates cross-modal attention layers. In each decoder layer, cross-attention layer allows the decoder to attend to the encoded 3D scene tokens (the output of the 3D scene encoder from Section 3.2). Formally, let ğ‘„text be the query matrix containing the decoders current hidden states (for each position in the sequence at that layer), and let ğ¾task and ğ‘‰scene be the key and value matrices derived from the set of 3D scene token embeddings. The cross-attention is computed as: Attention (ğ‘„text, ğ¾task, ğ‘‰scene) = softmax (cid:33) (cid:32) ğ‘„textğ¾ ğ‘‘ğ‘˜ task ğ‘‰scene, (9) where ğ‘‘ğ‘˜ is the dimensionality of the keys. This operation produces an attention output for the decoder at each position, which is essentially weighted sum of the 3D scene value vectors ğ‘‰scene, with weights determined by the compatibility of queries ğ‘„text with keys ğ¾task. In this way, the decoder can retrieve the relevant visual information needed to accurately describe that object. The crossattention mechanism ensures that the caption not only reflects the overall context of the scene, but also captures important local details by looking at the appropriate regions in the 3D data. The cross-attention layers are interleaved with the self-attention layers in the decoder, allowing for continuous exchange of information between the textual and visual modalities. This iterative process of fusing self-attention and cross-attention enables the model to build refined understanding of the scene context while preserving the grammatical and sequential coherence of the generated text. 3.5.2 Training objectives and joint optimization. Training the multimodal decoder is accomplished with combination of captioning loss and the previously introduced contrastive loss. We jointly optimize these objectives so that the model learns to generate accurate captions and maintain cross-modal alignment at the same time. The contrastive loss LCon (Eq. (8)) applied to the encoder outputs encourages the 3D and text features to stay aligned, which provides good initialization and constraint for the decoders cross-attention. Meanwhile, the decoder itself is primarily supervised by captioning loss that measures how well its generated text matches the reference description. For the caption generation task, we use the standard crossentropy loss between the predicted caption and the ground-truth caption. Given generated caption Ë†ğ‘Œ = ( Ë†ğ‘¦1, Ë†ğ‘¦2, , Ë†ğ‘¦ğ¿) and the corresponding ground truth ğ‘Œ = (ğ‘¦1, , ğ‘¦ğ¿), the captioning loss is defined as: LCap = ğ¿ ğ‘¡ =1 log ğ‘ƒ ( Ë†ğ‘¦ğ‘¡ = ğ‘¦ğ‘¡ Ë†ğ‘¦<ğ‘¡ , ğ‘“ğ‘’ğ‘›ğ‘ ) , (10) where ğ‘“ğ‘’ğ‘›ğ‘ is the conditioning global 3D feature, ensuring that the generated sentence is tightly linked to the visual content of the scene. This captioning loss is jointly optimized with the contrastive loss described in the previous section 3.4. The total loss function is expressed as: (11) LTotal = LCon + ğœ† LCap, where ğœ† is scalar hyperparameter that balances the two terms. By tuning ğœ†, we regulate the trade-off between enforcing multimodal feature alignment and producing accurate natural-language output. In our experiments, we set ğœ† to give roughly the same importance to both objectives. This joint optimization scheme causes the two parts of the model to reinforce each other. The contrastive alignment ensures that the visual encoder produces features that are readily attended to by the text decoder. Conversely, the act of captioning provides feedback that can refine the shared embedding space - the decoder will only succeed if the visual features ğ‘“ğ‘’ğ‘›ğ‘ encode the information needed for generation, which in turn pressures the encoder to capture fine-grained, caption-relevant details. Overall, the combined loss drives the model to generate captions that are not only linguistically fluent and descriptive, but also correspond closely to the 3D scene content. Jointly training 3D CoCa in this manner leads to improved integration of visual context into the language output, and tighter cross-modal correspondence between the 3D scenes and their generated captions. Algorithm 1: 3D CoCa Algorithm Require: Point cloud data ğ‘ƒ, Text input ğ‘‡ Ensure: Generated caption Ë†ğ¶ 1: Point Cloud & Text Input Processing: 2: Eğ‘ Point cloud tokenizer(ğ‘ƒ) {Tokenize input point cloud into sequence} 3: Eğ‘¡ Text tokenizer(ğ‘‡ ) {Tokenize input text into sequence} 4: Feature Encoding via Frozen CLIP Encoders: 5: fğ‘’ğ‘›ğ‘ CLIPvisual (Eğ‘ ) {Frozen CLIP visual encoder} 6: fğ‘¡ ğ‘’ğ‘›ğ‘ CLIPtext (Eğ‘¡ ) {Frozen CLIP text encoder} 7: Feature Alignment & Contrastive Learning: 8: (Ë† 9: LCon InfoNCE(cid:0)Ë† fğ‘’ğ‘›ğ‘, Ë† ğ‘’ğ‘›ğ‘ ) Feature alignment & Normalize(cid:0)fğ‘’ğ‘›ğ‘, fğ‘¡ fğ‘¡ (cid:1) {Contrastive loss for matching fğ‘’ğ‘›ğ‘, Ë† fğ‘¡ ğ‘’ğ‘›ğ‘ vs. non-matching pairs} ğ‘’ğ‘›ğ‘ (cid:1) 10: Update alignment layers using LCon 11: Multi-modal Decoding & Caption Generation: 12: Ë†ğ¶ TransformerDecoder(fğ‘’ğ‘›ğ‘ ) {Cross-attention over fğ‘’ğ‘›ğ‘ , autoregressive generation} 13: Joint Optimization Objective: 14: LCap CrossEntropy(cid:0) Ë†ğ¶, ğ¶ğ‘”ğ‘¡ (cid:1) {Caption generation loss} 15: LTotal LCap + ğœ† LCon"
        },
        {
            "title": "4 Experiments\n4.1 Datasets and Evaluation Metrics\n4.1.1 Datasets. We analyze the performance of 3D captioning us-\ning two benchmark datasets: ScanRefer [8] and Nr3D [1]. These\ndatasets provide extensive descriptions of 3D scenes and objects gen-\nerated by humans. ScanRefer contains 36,665 descriptions covering\n7,875 objects in 562 scenes, while Nr3D contains 32,919 descriptions\nof 4,664 objects in 511 scenes. The training data for both datasets\ncome from the ScanNet [16] database, which contains 1,201 3D\nscenes. For evaluation, we use 9,508 descriptions of 2,068 objects in\n141 scenes from ScanRefer and 8,584 descriptions of 1,214 objects\nin 130 scenes from Nr3D, all of which are from the 312 3D scenes\nin the ScanNet validation set.",
            "content": "4.1.2 Evaluation metrics. We use four metrics to evaluate model performance: CIDEr [34] measures human-like consensus via TFIDF weighted n-gram similarity, BLEU-4 [31] evaluates the accuracy of n-gram overlap between generated and reference captions, METEOR [2] evaluates semantic alignment by considering synonyms and paraphrases, and ROUGE-L [25] evaluates structural similarity based on the longest common subsequence, denoted as C, B-4, M, Table 1: Comparison of various methods on the ScanRefer dataset [8]. We evaluate the performance of each method, with and without additional 2D input, at IoU thresholds of 0.25 and 0.5. Metrics include CIDEr (C) [34], BLEU-4 (B-4) [31], METEOR (M) [2], and ROUGE-L (R) [25]. Our proposed 3D CoCa achieves state-of-the-art results across all settings. Method Scan2Cap [6] MORE [19] SpaCap3d [36] 3DJCG [3] D3Net [9] 3D-VLP [43] Vote2Cap-DETR [12] Unit3D [7] Vote2Cap-DETR++ [13] 3D CoCa (Ours) w/o additional 2D input w/ additional 2D input IoU = 0.25 B-4 26.14 34.25 26.36 35.41 26.16 35.30 27.45 39.67 - - 27.65 39.84 28.25 39.34 - - 28.70 41.37 53.73 58.89 58.06 60.86 - 64.09 71.45 - 76.36 54.95 55.41 55.03 59.02 - 58.78 59.33 - 60.00 IoU = 0.50 B-4 21.44 22.36 21.65 23.01 22.84 25.38 24.28 31.53 - - 24.53 31.87 26.22 34.46 - - 26.89 37.05 35.20 38.98 42.76 47.68 - 50.02 61.81 - 67. 43.57 44.33 45.66 51.80 - 51.17 54.40 - 55.64 IoU = 0.25 B-4 26.29 34.18 26.75 36.25 26.71 36.46 27.66 40.17 - - 28.14 41.03 28.06 39.17 - - 28.53 40.99 56.82 62.91 63.30 64.70 - 70.73 72.79 - 77.03 55.27 56.33 55.71 59.23 - 59.72 59.23 - 59.59 IoU = 0.50 B-4 21.97 23.32 21.66 22.93 22.33 25.26 24.22 31.03 24.35 30.29 24.83 32.31 25.28 32.42 21.91 27.22 26.04 34.73 39.08 40.94 44.02 49.48 46.07 54.94 59.32 46.69 64. 44.78 44.42 45.36 50.80 51.67 51.51 52.53 45.98 53.67 85.42 45.56 30.95 61.98 77. 41.23 28.52 57.40 86.12 44.79 30. 61.45 74.52 38.42 28.03 55.23 and R, respectively. Following previous studies [3, 6, 12, 19, 36], Non-Maximum Suppression (NMS) is initially applied to filter out duplicate object predictions in the proposals. In order to accurately evaluate the models caption generation capabilities, we adopt the ğ‘š@ğ‘˜ğ¼ğ‘‚ğ‘ˆ metric and set the IoU thresholds to 0.25 and 0.5 in our experiments, following [6]: ğ‘š@ğ‘˜ğ¼ğ‘‚ğ‘ˆ = 1 ğ‘ ğ‘ ğ‘–=1 ğ‘š ( Ë†ğ‘ğ‘–, ğ¶ğ‘– ) (cid:110) ğ¼ğ‘œğ‘ˆ (cid:16) Ë†ğ‘ğ‘–, ğ‘ğ‘– (cid:17) (cid:111) , ğ‘˜ (12) where ğ‘ represents the total number of annotated objects in the evaluation dataset, Ë†ğ‘ğ‘– is the generated caption, ğ¶ğ‘– is the groundtruth caption, and ğ‘š can be any natural language generation metric, such as CIDEr [34], METEOR [2], BLEU-4 [31], and ROUGE-L [25]."
        },
        {
            "title": "4.2 Implementation Details\nWe provide implementation details of different baselines. â€œw/o ad-\nditional 2Dâ€ means that the input P âˆˆ R40,000Ã—10 contains the\nabsolute positions of 40, 000 points representing the 3D scene, as\nwell as color, normal, and height. â€œadditional 2Dâ€ means that we\nreplace the color information with 128-dimensional multiview fea-\ntures extracted from 2D images by ENet [10] following [6]. The 3D\nscene encoder backbone is based on EPCL [18], integrated with the\nfrozen CLIP visual encoder [33], and the text embedding is obtained\nby the frozen CLIP text encoder.",
            "content": "We train the models for 1,080 epochs using the standard crossentropy loss and contrastive loss on ScanRefer [8] and Nr3D [1], using the AdamW optimizer [27] with learning rate of 0.1, batch size of 4, and cosine annealing learning rate scheduler. All experiments mentioned above are conducted on single RTX4090 GPU."
        },
        {
            "title": "4.3 Comparative Study\nIn this section, we compare the performance with existing works on\nmetrics C, M, B-4, R as abbreviations for CIDEr [34], METEOR [2],\nBLEU-4 [31], Rouge-L [25] under IoU thresholds of 0.25, 0.5 for\nScanRefer (Table 1) and 0.5 for Nr3D (Table 2). In both tables, \"-\"\nindicates that neither the original paper nor any follow-up works\nprovide such results.",
            "content": "Scanrefer. The description in ScanRefer includes the attributes 4.3.1 of the object and its spatial relationship with surrounding objects. Table 2: Comparison on Nr3D [1] at IoU=0.5. Our model outperforms existing methods, demonstrating higher CIDEr (C) [34], BLEU-4 (B-4) [31], METEOR (M) [2], and ROUGE-L (R) [25] scores. Method Scan2Cap [6] SpaCap3d [36] D3Net [9] 3DJCG [3] Vote2Cap-DETR [12] Vote2Cap-DETR++ [13] C@0.5 B-4@0.5 M@0.5 R@0.5 49.06 27.47 50.50 33.71 53.38 33.85 52.99 38.06 54.43 43.84 55.22 47.08 17.24 19.92 20.70 22.82 26.68 27.70 21.80 22.61 23.13 23.77 25.41 25. 3D CoCa (Ours) 52.84 29.29 25.55 56.43 As shown in Table 1, our method outperforms the existing methods in all data settings and IoU thresholds. 4.3.2 Nr3D. The Nr3D dataset evaluates the models ability to interpret human-spoken, free-form object descriptions. As shown in Table 2, our approach achieves significant performance improvements over existing models in generating diverse descriptions."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Table 3: The impact of Contrastive Learning Loss weight ğœ† on the model description performance. Four evaluation indicators, CIDEr(C) [34], BLEU-4(B-4) [31], METEOR(M) [2], and ROUGE-L(R) [25] are listed. ğœ† (Contrastive Weight) C@0.5 B-4@0.5 M@0.5 R@0.5 58.76 59.60 60.40 61.98 59.30 27.45 28.10 28.75 30.95 28.00 40.98 41.80 42.55 45.56 41.50 74.12 77.30 79.55 85.42 76.89 0 0.1 0.5 1.0 2. 4.4.1 Contrastive learning loss impact analysis. We first investigate the impact of using contrastive learning loss and the sensitivity to different weight coefficients(ğœ†). By controlling the contrastive loss weight coefficient ğœ† = {0, 0.1, 0.5, 1.0, 2.0}, the performance of the model was compared without contrastive learning and with different strength contrastive learning strategies. As shown in Table 3, it can be seen that when contrastive loss is not used, the model performs the worst in all indicators; the performance is significantly improved after moderate introduction of contrastive learning. For Figure 3: visual comparison on the ScanRefer [8] dataset showcasing indoor scenes described by Vote2Cap-DETR++ [13], our method (Ours), and the ground truth (GT), highlighting differences in descriptive accuracy and style. example, when ğœ† increases from 0 to 0.5, CIDEr increases from 74.12% to 79.55%, and the best performance is achieved when ğœ†=1. However, after increasing the weight to 2.0, the indicator dropped slightly, which is still better than in the case without contrast loss. The above results show that an appropriate amount of contrast learning objectives can improve the models ability to align and capture the semantics of 3D scenes, thereby improving the description quality. Table 4: Comparison of the impact of different 3D point cloud encoder architectures on description performance. EPCL is the encoder proposed in this paper, and PointNet++ is the traditional point cloud encoder. Encoder Architecture C@0.5 B-4@0.5 M@0.5 R@0.5 56.30 PointNet++ (Baseline) 61.98 EPCL (Proposed) 38.95 45. 26.80 30.95 72.48 85.42 4.4.2 Point cloud encoder structure analysis. We compared the performance difference between the proposed EPCL point cloud encoder fused with CLIP features and the traditional PointNet++ [32] point cloud encoder under the same settings. From Table 4, it can be seen that when using our EPCL-based encoder, the model performance is significantly better than that of PointNet++, for example, CIDEr exceeds PointNet++ by 12.94%. The comprehensive improvement of various indicators shows that the EPCL framework combined with the pre-trained CLIP visual features effectively enhances the semantic expression and spatial modeling capabilities of point clouds and can capture richer scene information, thereby generating more accurate and detailed descriptions. Table 5: The impact of different caption generation decoders on model performance. Comparison of the description indicators of the original GPT-2 generator and the CoCa-style multimodal decoder in this paper under the same visual features. Caption Decoder GPT-2 Captioner (Baseline) C@0.5 B-4@0.5 M@0.5 R@0.5 59.50 76.20 41.00 27.80 CoCa Transformer (Proposed) 85.42 45. 30.95 61.98 4.4.3 Decoder architecture comparison. Finally, we analyze the impact of the caption generation decoder structure on performance while keeping the output features of the visual encoder unchanged. We replace the CoCa-style multimodal Transformer decoder with the traditional GPT-2 text generation model. As shown in Table 5, it can be seen that the model description quality is significantly reduced when using the GPT-2 captioner. This demonstrates that the CoCa-style Transformer decoder in our approach can more effectively incorporate contrastively learned aligned visual features into the language generation process, resulting in descriptions that are more semantically rich and more closely related to the scene."
        },
        {
            "title": "5 Conclusion\nIn this work, we propose 3D CoCa, a unified contrastive-captioning\nframework for 3D vision-language tasks. By jointly learning con-\ntrastive 3D-text representations and caption generation within a\nsingle model, 3D CoCa eliminates the need for any explicit 3D\nobject detectors or proposal stages. This unified approach enables\ndirect 3D-to-text alignment in a shared feature space, leading to\nimproved spatial reasoning and more precise semantic grounding\ncompared to previous methods. Experiments on two widely used\ndatasets validate that our proposed 3D CoCa model significantly\noutperforms existing methods across standard captioning metrics\nand proves the benefits of our contrastive learning strategy.",
            "content": "References [1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. 2020. ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes. 16th European Conference on Computer Vision (ECCV) (2020). [2] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss (Eds.). Association for Computational Linguistics, Ann Arbor, Michigan, 6572. https://aclanthology.org/W05-0909/ [3] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 2022. 3DJCG: Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1644316452. doi:10.1109/CVPR52688.2022.01597 [4] Guohui Cai, Ying Cai, Zeyu Zhang, Yuanzhouhan Cao, Lin Wu, Daji Ergu, Zhinbin Liao, and Yang Zhao. 2024. Medical ai for early detection of lung cancer: survey. arXiv preprint arXiv:2410.14769 (2024). [5] Guohui Cai, Ruicheng Zhang, Hongyang He, Zeyu Zhang, Daji Ergu, Yuanzhouhan Cao, Jinman Zhao, Binbin Hu, Zhinbin Liao, Yang Zhao, et al. 2024. Msdet: Receptive field enhanced multiscale detection for tiny pulmonary nodule. arXiv preprint arXiv:2409.14028 (2024). [6] DaveZhenyu Chen, Ali Gholami, Matthias Niesner, and AngelX. Chang. 2021. Scan2Cap: Context-aware Dense Captioning in RGB-D Scans. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/ cvpr46437.2021. [7] DaveZhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias NieÃŸner, and AngelX. Chang. 2023. UniT3D: Unified Transformer for 3D Dense Captioning and Visual Grounding. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). 1806318073. doi:10.1109/iccv51070.2023.01660 [8] Dave Zhenyu Chen, Angel Chang, and Matthias NieÃŸner. 2020. ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language. 16th European Conference on Computer Vision (ECCV) (2020). [9] Dave Zhenyu Chen, Qirui Wu, Matthias NieÃŸner, and Angel Chang. 2021. D3Net: Speaker-Listener Architecture for Semi-supervised Dense Captioning and Visual Grounding in RGB-D Scans. arXiv preprint arXiv:2112.01551 (2021). [10] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z. Chen, and Jian Wu. 2020. Hierarchical Graph Network for 3D Object Detection on Point Clouds. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 389398. doi:10.1109/CVPR42600.2020.00047 [11] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. 2024. LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2641826428. doi:10.1109/cvpr52733.2024.02496 [12] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, and Tao Chen. 2023. End-to-End 3D Dense Captioning with Vote2Cap-DETR. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1112411133. doi:10.1109/cvpr52729.2023.01070 [13] Sijin Chen, Hongyuan Zhu, Mingsheng Li, Xin Chen, Peng Guo, Yinjie Lei, Gang Yu, Taihao Li, and Tao Chen. 2024. Vote2Cap-DETR++: Decoupling Localization and Describing for End-to-End 3D Dense Captioning. IEEE Transactions on Pattern Analysis and Machine Intelligence 46, 11 (Nov 2024), 73317347. doi:10. 1109/tpami.2024. [14] Xin Chen, Anqi Pang, Yang Wei, Wang Peihao, Lan Xu, and Jingyi Yu. 2021. TightCap: 3D Human Shape Capture with Clothing Tightness Field. ACM Transactions on Graphics (Presented at ACM SIGGRAPH) (2021). [15] Xin Chen, Anqi Pang, Wei Yang, Yuexin Ma, Lan Xu, and Jingyi Yu. 2021. SportsCap: Monocular 3D Human Motion Capture and Fine-Grained Understanding in Challenging Sports Videos. International Journal of Computer Vision (Oct 2021), 28462864. doi:10.1007/s11263-021-01486-4 [16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias NieÃŸner. 2017. ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE. [17] Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, et al. 2024. Chat-scene: Bridging 3d scene and large language models with object identifiers. Proceedings of the Advances in Neural Information Processing Systems, Vancouver, BC, Canada (2024). [18] Xiaoshui Huang, Zhou Huang, Sheng Li, Wentao Qu, Tong He, Yuenan Hou, Yifan Zuo, and Wanli Ouyang. 2024. Frozen CLIP Transformer Is an Efficient Point Cloud Encoder. Proceedings of the AAAI Conference on Artificial Intelligence 38, 3 (Mar. 2024), 23822390. doi:10.1609/aaai.v38i3.28013 [19] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. 2022. MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. In In Proceedings of the European conference on computer vision. 528545. doi:10.1007/978-3-031-19833-5_ [20] Bu Jin, Yupeng Zheng, Pengfei Li, Weize Li, Yuhang Zheng, Sujie Hu, Xinyu Liu, Jinwei Zhu, Zhijie Yan, Haiyang Sun, Kun Zhan, Peng Jia, Xiaoxiao Long, Yilun Chen, and Hao Zhao. 2025. TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes. In In Proceedings of the European conference on computer vision. 367384. doi:10.1007/978-3-031-72649-1_21 [21] Zhao Jin, Munawar Hayat, Yuwei Yang, Yulan Guo, and Yinjie Lei. 2023. Context-aware Alignment and Mutual Masking for 3D-Language Pre-training. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1098410994. doi:10.1109/cvpr52729.2023.01057 [22] Minjung Kim, Hyung Lim, SeungHwan Kim, Soonyoung Lee, Bumsoo Kim, and Gunhee Kim. 2024. See It All: Contextualized Late Aggregation for 3D Dense Captioning. In Findings of the Association for Computational Linguistics ACL 2024. 33953405. doi:10.18653/v1/2024.findings-acl.202 [23] Minjung Kim, HyungSuk Lim, Soonyoung Lee, Bumsoo Kim, and Gunhee Kim. 2025. Bi-directional Contextual Attention for 3D Dense Captioning. In In Proceedings of the European conference on computer vision. 385401. doi:10.1007/9783-031-72649-1_22 [24] Yongbin Liao, Hongyuan Zhu, Yanggang Zhang, Chuangguan Ye, Tao Chen, and Jianchao Fan. 2021. Point Cloud Instance Segmentation with Semi-supervised Bounding-Box Mining. Cornell University - arXiv,Cornell University - arXiv (Nov 2021). [25] Chin-Yew Lin. 2004. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 7481. https://aclanthology.org/W04-1013/ [26] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 6168. doi:10.18653/v1/2022.acl-short.8 [27] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In International Conference on Learning Representations. https://openreview.net/ forum?id=Bkg6RiCqY7 [28] Guofeng Mei, Xiaoshui Huang, Juan Liu, Jian Zhang, and Qiang Wu. 2022. Unsupervised Point Cloud Pre-Training Via Contrasting and Clustering. In 2022 IEEE International Conference on Image Processing (ICIP). doi:10.1109/icip46576.2022. 9897388 [29] A. Neubeck and L. Van Gool. 2006. Efficient Non-Maximum Suppression. In 18th International Conference on Pattern Recognition (ICPR06), Vol. 3. 850855. doi:10.1109/ICPR.2006.479 [30] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. 2022. Masked autoencoders for point cloud self-supervised learning. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part II. Springer, 604621. [31] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (Philadelphia, Pennsylvania) (ACL 02). Association for Computational Linguistics, USA, 311318. doi:10.3115/1073083. [32] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. 2017. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems 30 (2017). [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020 [cs.CV] https://arxiv.org/ abs/2103.00020 [34] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 45664575. doi:10.1109/CVPR.2015. 7299087 [35] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matt J. Kusner. 2021. Unsupervised Point Cloud Pre-training via Occlusion Completion. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV). doi:10.1109/iccv48922.2021. 00964 [36] Heng Wang, Chaoyi Zhang, Jianhui Yu, and Weidong Cai. 2022. Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence. 13931400. doi:10.24963/ijcai.2022/194 [37] Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, and Jiwen Lu. 2023. Take-APhoto: 3D-to-2D Generative Pre-training of Point Cloud Models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). 56175627. doi:10.1109/ ICCV51070.2023. [38] Saining Xie, Jiatao Gu, Demi Guo, Charles R. Qi, Leonidas Guibas, and Or Litany. 2020. PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding. In Computer Vision ECCV 2020, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International Publishing, Cham, 574591. [39] Fukun Yin, Zilong Huang, Tao Chen, Guozhong Luo, Gang Yu, and Bin Fu. 2023. DCNet: Large-scale Point Cloud Semantic Segmentation with Discriminative and Efficient Feature Aggregation. IEEE Transactions on Circuits and Systems for Video Technology (Jan 2023), 11. doi:10.1109/tcsvt.2023.3239541 [40] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022. CoCa: Contrastive Captioners are Image-Text Foundation Models. arXiv:2205.01917 [cs.CV] https://arxiv.org/abs/2205.01917 [41] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. 2022. Point-BERT: Pre-Training 3D Point Cloud Transformers with Masked Point Modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). [42] Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin Li, Shuguang Cui, and Zhen Li. 2022. -Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 85538563. doi:10.1109/cvpr52688.2022. [43] Taolin Zhang, Sunan He, Tao Dai, Zhi Wang, Bin Chen, and Shu-Tao Xia. 2024. Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding. Proceedings of the AAAI Conference on Artificial Intelligence 38, 7 (Mar 2024), 72967304. doi:10.1609/aaai.v38i7.28559 [44] Zeyu Zhang, Nengmin Yi, Shengbo Tan, Ying Cai, Yi Yang, Lei Xu, Qingtai Li, Zhang Yi, Daji Ergu, and Yang Zhao. 2024. Meddet: Generative adversarial distillation for efficient cervical disc herniation detection. In 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE, 40244027. [45] Rui Zhao, Zeyu Zhang, Yi Xu, Yi Yao, Yan Huang, Wenxin Zhang, Zirui Song, Xiuying Chen, and Yang Zhao. 2025. Peddet: Adaptive spectral optimization for multimodal pedestrian detection. arXiv preprint arXiv:2502.14063 (2025)."
        }
    ],
    "affiliations": [
        "Peking University",
        "Shanghai University of Engineering Science",
        "The Australian National University",
        "Xiamen University"
    ]
}