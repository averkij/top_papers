{
    "paper_title": "Style-Friendly SNR Sampler for Style-Driven Generation",
    "authors": [
        "Jooyoung Choi",
        "Chaehun Shin",
        "Yeongtak Oh",
        "Heeseung Kim",
        "Sungroh Yoon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new \"style templates\", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation."
        },
        {
            "title": "Start",
            "content": "Style-Friendly SNR Sampler for Style-Driven Generation Jooyoung Choi1, Chaehun Shin1, Yeongtak Oh1 Heeseung Kim1 Sungroh Yoon1,2, 1Data Science and AI Laboratory, ECE, Seoul National University 2AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University {jy choi, chaehuny, dualism9306, gmltmd789, sryoon}@snu.ac.kr 4 2 0 2 2 ] . [ 1 3 9 7 4 1 . 1 1 4 2 : r Figure 1. Our method learns style templates from reference images, capturing elements including color schemes, layouts, illumination, and brushstrokes. For each image, we fine-tune diffusion models using the reference image in the red insert box to generate the output. We show fluffy baby sloth with knitted hat trying to figure out laptop in different meme templates: meme with the words you just activated my trap card (top left), multi-panel comic layout (bottom left), and two-panel meme (top middle). On the right side, we display typographies in the styles of wooden sculpture and minimal line drawing. We also present singing kangaroo drinking beer in various artistic stylesflat illustration, crayon drawing, watercolor, and line drawing."
        },
        {
            "title": "Abstract",
            "content": "Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new style templates, enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation. 1. Introduction Recently, large-scale text-to-image diffusion models [1, 8, 29, 37, 38, 42, 47] have achieved remarkable progress in visual content creation. In particular, open-weights like Stable Diffusion series [8, 42] and FLUX [29] have been among the most notable for their photorealistic image quality and language understanding capabilities. Behind this strong performance lies the advancement of the diffusion framework that encompasses the principles of score-based models [55] and flow matching [32, 33], diffusion formulations [17, 32, 33, 55], loss weighting [4, 24], noise level scheduling [18, 22], and architectural improvements [8, 23, 36]. These advancements have predominantly focused on generating high-quality images with respect to object-centric benchmarks [11, 21] and metrics [15, 28]. Motivated by the success of text-to-image models, there is growing need for style-driven generation, where the generated samples capture styles desired by individual users or artists. Here, style encompasses various elements such as color schemes, layouts, illumination, and brushwork [7, 10, 31, 51]all contributing to the unique nuances of an image. However, relying solely on text prompts has its limitations in capturing these complex and personal artistic styles, especially those not present in the pre-training data. To enable style-driven generation, researchers have turned to fine-tuning [45, 51] text-to-image models [3, 47] using reference images of the desired styles, often employing lightweight adapters such as LoRA [19], as this remains the most effective method for accurately capturing and reproducing new artistic styles. However, they blindly apply the same objective functions and noise level distributions used for pre-trainingoriginally optimized for object-centric benchmarks [11, 21]without considering the unique characteristics inherent to style images, which are distinct from object-centric images. Consequently, even with numerous style-driven generation studies [14, 30, 44, 51, 61], we observe prevalent failure cases when fine-tuning diffusion models on style references. Addressing these failures sometimes requires two-stage training with heavy human feedback [51]. In this paper, we address these limitations by introducing the Style-friendly SNR sampler, method that significantly enhances the models ability to capture new artistic styles during fine-tuning. Our approach is motivated by two key observations 1) diffusion models struggle to learn new styles, and 2) styles emerge at higher noise levels. Building upon these observations, we propose adjusting the noise level sampling in the diffusion models objective function during fine-tuning. By directly sampling signal-to-noise ratio (SNR) using our Style-friendly SNR sampler, we bias the distribution towards higher noise levels where style features emerge, such as color schemes, layouts, and illuminationkey elements for style representation. Unlike previous methods [8, 30], our approach intensifies the focus on capturing these stylistic aspects. Our Style-friendly SNR sampler enables state-of-theart models such as FLUX-dev [29] and Stable Diffusion 3.5 [8, 57] to effectively learn style templates with unprecedented accuracy. Our method leads to more faithful styledriven generation, capturing the uniqueness of reference style images. Moreover, we unveil the key components that explain why previous works excel at learning object-centric concepts but struggle with styles, providing deeper insights into the diffusion process for style-driven generation. Ultimately, our approach facilitates the creation of style templates from reference images, which can be easily shared and utilized to practitioners for content creation, expanding the capabilities of text-to-image diffusion models. 2. Training Diffusion Models 2.1. Diffusion Process Various diffusion models [17, 32, 33, 50, 55] are based on the forward process that progressively degrades data x0 into pure noise x1 as time progresses from 0 to 1, following the unified formulation below: xt = αtx0 + σtϵ, (1) Correspondence to: Sungroh Yoon (sryoon@snu.ac.kr) Both authors contributed equally to this work where αt and σt are predefined noise schedules, and ϵ (0, I) represents standard Gaussian noise. Recent state2 From this perspective, we can observe the inductive bias imposed on the diffusion model through p(λ). Prior works [4, 6] discuss that the model learns low-frequency information at large noise levels (small λ) and learns high-frequency details at small noise levels (large λ). Consequently, diffusion models generate images in coarse-to-fine manner [6, 41]. 2.3. SNR Samplers Previous diffusion models have predominantly been designed based on the timestep rather than the log-SNR λt, with typically sampled from uniform distribution [17, 36, 42] during training. Thus, the importance of each noise level in diffusion loss is determined solely by the function λt. However, EDM [22] argues that this approach entangles the design space of diffusion models and proposes sampling noise variance σt from log-normal distribution. (cid:17) (cid:16) 1t SD3 [8] extends this approach to the training of textto-image models by sampling logit function of timestep log from normal distribution. As mentioned earlier, SD3 follows the rectified flow formulation and has the logSNR as λt = 2 log (cid:0) 1t (cid:1). In addition, they propose shifting timestep to tnew by for high resolution training: tnew = kt 1 + (k 1)t , (5) which is equivalent to shifting λt by 2 log as follows: (cid:18) 1 tnew tnew = λt 2 log k. λtnew = 2 log (cid:19) (6) 3. Method 3.1. Observations Diffusion Models Struggle to Capture Styles. We begin by examining the fine-tuning capability of recent state-of-the-art model. Many current text-to-image models concentrate on object-centric image quality and demonstrate their performance. Among the latest diffusion models, FLUX [29], the representative text-to-image model, excels in object-driven image generation through fine-tuning, enabling high-quality generation of fine-tuned objects across various scenarios. In Fig. 2a, the dog-themed backpack integrates well into the nighttime surfing scene, matching the appropriate evening lighting. The dog below is dressed in new clothes, and the typography is also well-generated. However, FLUX struggles to capture styles in terms of color schemes, illumination, and brushwork when fine-tuning with the SNR sampler used for pre-training [8]. In Fig. 2b, the glowing example only causes the sloths fur to glow but does not reflect the dark background and blue lighting of the reference. Similarly, for the Van Gogh oil painting reference, while it captures the blue color tone, it fails to reproduce Van Goghs distinctive brushstrokes. Figure 2. Fine-tuning capability. While FLUX succeeds in learning objects (a), it struggles to capture styles (b). We enable FLUX to learn styles (c). References are shown in the red insert box. of-the-art diffusion models, such as Stable Diffusion 3 (SD3) [8] and FLUX [29], utilize the noise schedule from rectified flow [32, 33] with αt = 1 and σt = t. This choice is effective due to straight diffusion trajectories. While the diffusion process is commonly parameterized by the timestep t, Kingma et al. [24, 25] characterize the noise level using the log signal-to-noise ratio (log-SNR): λt = log (cid:19) . (cid:18) α2 σ2 (2) In the case of rectified flow, λt = 2 log (cid:0) 1t (cid:1). In our work, we demonstrate that adjusting the timestep distribution based on λt facilitates learning styles more effectively than using t. 2.2. Unified Loss Function Kingma et al. [24, 25] showed that the objectives of various diffusion models [17, 22, 32, 33, 42, 54, 55] share the same form. The general loss function is expressed as: LDM (x0) = Eλp(λ) (cid:20) fθ(xλ, λ) (xλ, λ)2 (cid:21) , (3) where (xλ, λ) is the target function. In the case of rectified flow, the model fθ predicts the velocity (xλ, λ) = ϵ x0. Considering the role of p(λ), it serves as weighting factor in the loss function that determines the importance assigned to different noise levels during training: (cid:90) λmax LDM (x0) = p(λ)fθ(xλ, λ) (xλ, λ)2dλ (4) λmin = EλU (λmin,λmax) (cid:20) p(λ)fθ(xλ, λ) (xλ, λ)2 (cid:21) . Figure 3. Prompt switching during generation. λt indicates log-SNR. The style prompts are minimalist flat round logo, sticker, detailed pen and ink drawing, and abstract rainbow colored flowing smoke wave. Styles emerge in the initial 10% of denoising steps; therefore, (c) and (d) fail to capture target styles. Here, we use FLUX, with the guidance scale 7 across the whole denoising process. Styles Emerge at Higher Noise Levels. To investigate the failure of fine-tuning styles, we perform further analysis related to styles. We investigate the noise level at which the style features emerge when generating images with text-toimage diffusion models. We incorporate style descriptions, such as in flat cartoon illustration only at specific intervals during the denoising process. As illustrated in Fig. 3, we implement the textconditioned generation process using text yw/o style to exclude style descriptions during the early denoising steps, and then switch to text yw/ style to include style descriptions in the remaining steps. When style descriptions are used throughout all denoising steps, the generated images align well with desired styles (Fig. 3a). Interestingly, omitting style descriptions during just the initial 10% of the denoising steps results in images where styles are not adequately reflected (Fig. 3c). These images closely resemble those generated without any style descriptions at allyielding photorealistic samples (Fig. 3d). The case that uses yw/o style in the later denoising steps is provided in Fig. S8. This observation indicates that the stylistic features are determined during the early stage of the denoising process, corresponding to an interval where the log-SNR λt is small (i.e., higher noise levels). The subsequent denoising steps primarily render content and fine details that are independent of styles. Thus, missing the style prompt in the initial steps significantly hinders the models ability to generate images in desired styles, emphasizing the importance of early-stage conditioning for style representation. 3.2. Style-Friendly SNR Sampler Our observations indicate the key motivation for style learning. While styles emerge in the early steps of the denoising process, the current fine-tuning process utilizes an SNR Figure 4. Probability distribution of Log-SNR. Colored region indicates style-emerging noise levels discussed in Sec. 3.1. sampler from pre-training which prioritizes object-centric generation [11, 21] as shown in the green line of Fig. 4. This SNR sampler places greater emphasis on intermediate steps to capture fine details of objects better, yet it does not sufficiently focus on the noise levels where styles emerge. As result, despite excelling in object-driven generation, the current fine-tuning struggles to fully capture and represent target styles in style-driven generation. Building upon this motivation, we propose to fine-tune diffusion models by biasing the noise level distribution towards higher noise levels (lower log-SNR λt values) where stylistic features emerge. Specifically, we sample log-SNR from normal distribution: λt (µlow, σ2), (7) with lowered mean µlow, thereby focusing the training on the critical noise levels essential for style learning. We set 4 Figure 5. Effect of varying µ. Diffusion models start to capture the reference glowing style when µ is lower than 4. The target prompt is Christmas tree in glowing style. (a) Varying µ. (b) Varying σ. (c) Varying LoRA Rank Figure 6. SNR sampler analysis. DINO similarities of varying SNR sampler parameters with FLUX and SD3.5-8B. Dotted lines in (c) indicate results of SD3 sampler [8]. Unless specified, we use µ = 6, σ = 2, and rank 32. CLIP scores are shown in Fig. S7. the standard deviation σ = 2 to maintain balance between concentrating on the critical noise levels and providing sufficient variation in noise levels for effective learning. While timestep shifting in Eq. (5) weakly biases the noise level distribution, Style-friendly SNR sampler allows for more aggressive emphasis on desired noise levels. Setting the mean to µ = 6 concentrates timesteps between 0.8 and 1.0, where style features emerge (Sec. 3.1). This enables us to support the diffusion model in fine-tuning with strong focus on styles across various style templates. 3.3. Trainable Parameters of MM-DiT We fine-tune both FLUX-dev [29] and SD3.5 [8, 57] by training LoRA [19] adapters on specific layers to capture new styles. Currently, no studies have fine-tuned the MultiModal Diffusion Transformer (MM-DiT) [8], the core architecture for both FLUX-dev and SD3.5. MM-DiT comprises dual-stream transformer blocks with separate parameters for text tokens and image tokens, which interact through joint attention mechanisms. To effectively learn the stylistic features encompassing both visual and linguistic characteristics, we train LoRA on the attention layers of both modalities. Additionally, FLUX includes singlestream blocks that handle both modalities simultaneously with attention mechanisms and projection layers that skip this attention, to which we also apply LoRA. This targeted fine-tuning achieves high style-alignment without training the entire network, providing parameter-efficient method for fine-tuning MM-DiT. 4. Experiments We evaluate our method by fine-tuning FLUX-dev [29] and SD3.5-8B [8, 57] on 18 reference styles from the StyleDrop [51]. For each reference style, we generate two images for each of the 23 evaluation prompts collected from [51], resulting in total of 828 images evaluated per experiment. For quantitative evaluation, we assess style alignment using DINO [2] ViT-S/16 and CLIP [39] ViT-B/32 image similarity (CLIP-I), and we measure alignment to the target prompts using CLIP text-image similarity (CLIP-T). All models are fine-tuned using the Adam optimizer [26] for 300 steps at learning rate of 104. We perform LoRA [19] fine-tuning on the frozen pre-trained models, using rank of 32 in all experiments except for the rank ablation studies. During inference, we use 28 denoising steps and set the guidance [16, 34] scale to 7.0. 4.1. Analysis of Style-Friendly SNR Sampler In Fig. 5 and Fig. 6, we conduct experiments to analyze the impact of varying the parameters of our Style-friendly SNR samplerspecifically, the mean (µ) and the standard deviation (σ) of the log-SNR sampling distribution, as well as the LoRA rank. We refer to the noise level distribution used for the SD3 pre-training as the SD3 sampler [8]. Effect of Varying µ. The mean µ is the most critical design choice influencing style learning. We experiment with µ values ranging from 0 to 8 for both FLUX-dev and SD3.5-8B. As shown in Fig. 5, increasing µ leads the models to increasingly fail to learn reference styles. In Fig. 6a, the DINO similarity decreases with increasing µ, indicating inferior style alignment. Conversely, in Fig. 5, when µ is set to 6 or lower (more negative), the models begin to capture and reflect the reference styles effectively. In Fig. S5, we also show the effect of µ on object references. Effect of Varying σ. We also investigate the effect of varying the standard deviation σ of the log-SNR sampling distribution in Fig. 6b. When σ is less than 2, the model learns from narrower interval, which can reduce style alignment due to less diversity in noise levels. This suggests that while σ influences the breadth of noise levels, maintaining moderate σ (e.g., σ = 2) provides good balance between focusing on critical noise levels and learning on sufficiently diverse noise levels for style learning. Effect of Varying Rank. In Fig. 6c, we examine the impact of model capacity by varying the LoRA rank. Notably, with low µ = 6, rank of 4 achieves higher DINO similarity compared to the SD3 sampler at rank 32 (dotted lines). This demonstrates focusing on higher noise levels (lower λt) has more pronounced effect on style learning than model capacity alone. 4.2. Qualitative Results We compare our Style-friendly SNR sampler against previous methods, including the SD3 sampler [8], direct consistency optimization (DCO) [30], and IP-Adapter[60, 61], which use FLUX-dev as the backbone model; RBModulation [44], which uses Stable Cascade [37]; and Style-Aligned [14], which uses SDXL [38]. In Fig. 7, our Style-friendly SNR sampler accurately captures the styles of reference images, reflecting stylistic features including color schemes, layouts, illumination, and brushstrokes. In contrast, fine-tuning FLUX-dev with the standard SD3 sampler often fails to capture key stylistic components, such as layouts (columns 1, 2) and color schemes (columns 3-7).1 Fine-tuning FLUX-dev with DCO struggles to learn the reference styles due to strong regularization that prevents significant deviation from the pre-trained model. IPAdapter with FLUX-dev and RB-Modulation rely on embeddings of CLIP [39] and CSD [52], which may not capture fine stylistic details, leading to less accurate style 1Note that some open-source implementations (e.g., Hugging Face Diffusers [58] 0.31) may omit timestep shifting during fine-tuning. Readers should be aware of this potential omission when reproducing results."
        },
        {
            "title": "Model",
            "content": "win tie lose Style-Aligned [14] SDXL RB-Mod [44] IP-Adapter [61] DCO [30] SD3 sampler [8] 61.0 % 7.1% 31.9% 55.6 % 12.6% 31.8% Cascade FLUX-dev 59.2 % 8.0% 32.8% FLUX-dev 56.0 % 10.2% 33.8% FLUX-dev 56.0 % 9.2% 34.8%"
        },
        {
            "title": "Model",
            "content": "win tie lose Style-Aligned [14] SDXL RB-Mod [44] IP-Adapter [61] DCO [30] SD3 sampler [8] 60.7% 7.5% 31.8% 54.3% 6.3% 39.4% Cascade FLUX-dev 56.0% 4.6% 39.4% FLUX-dev 53.2% 10.0% 36.8% FLUX-dev 56.5% 14.0% 29.5% Table 1. Human evaluation. User preference results comparing style and text alignments between our method and the baselines. reproduction. Style-Aligned shares self-attention features within the diffusion model, which can cause artifacts such as destroyed structure (columns 1, 2) or duplicated limbs (columns 4-7) when attention features conflict. We further conduct user study to quantify human preferences using Amazon Mechanical Turk. Following previous work [51], we compare our method to each method with two separate questionnaires. According to the reference style image and target text prompt, users are asked to select which of the two generated images is more similar to the style in the reference image and represents the target text prompt. We obtain 450 answers from 150 participants for each comparison, and the results are presented in Tab. 1. Our method outperforms prior arts in both aspects (p < 0.05 in the Wilcoxon signed-rank test), consistent with the qualitative results and demonstrates the superiority of our method in learning stylistic elements. More details on our user study are provided in Appendix C.3. 4.3. Quantitative Results In Tab. 2, we evaluate our method and prior works using DINO [2] and CLIP image similarities (CLIP-I) to assess style alignment, and CLIP text-image similarity (CLIP-T) for text alignment. Our method achieves the highest DINO and CLIP-I scores across all backbones, demonstrating its superior ability to capture styles from the reference images. While our CLIP-T score is slightly lower compared to some methods, we already showed superior text alignment in human evaluation (Tab. 1). This implies that methods that fail to capture styles often generate the most common interpretation of the prompt, leading to higher CLIP-T scores due to bias towards typical representations. Overall, our 6 Figure 7. Qualitative comparison. All samples are generated with the same seed. Please zoom in. 7 Method Model Style-Aligned [14] SDXL RB-Mod [44] DCO [30] SD3 sampler [8] Style-friendly IP-Adapter [61] DCO [30] SD3 sampler [8] Style-friendly Cascade SD3.5 SD3.5 SD3.5 FLUX-dev FLUX-dev FLUX-dev FLUX-dev Metrics DINO CLIP-I CLIP-T 0.675 0.410 0.647 0.317 0.661 0.399 0.670 0.424 0.698 0.489 0.656 0.361 0.643 0.373 0.645 0.373 0.686 0. 0.340 0.363 0.355 0.350 0.349 0.354 0.353 0.350 0.344 Table 2. Quantitative comparison. Style alignment (DINO and CLIP-I) and text alignment (CLIP-T) with 18 styles from [51]. Our style-friendly exhibits superior style-alignment scores. Figure 8. Multi-panel and typography. First row demonstrates generating multiple coherent images as single image. Second row shows customized typography with unique style. quantitative results confirm that our method accurately reflects both styles and texts. 4.4. Applications Multi-Panel. Dreambooth [45] shows multi-panel comic by generating each panel with model fine-tuned on cartoon character. However, in the first row of Fig. 8, we treat these multiple panels as single image during fine-tuning. When new subject is specified with target prompt, we generate the subject across multiple comic-style panels simultaneously. This approach allows for the generation of coherent multi-panel comics using single reference. Typography. Our method can also be extended to typography, including meme generation as shown in Fig. 1. Leveraging the spelling capabilities of recent models [29, 57], we generated customized typography with unique styles, and the results are showcased in the second row of Fig. 8. This versatility enables users to effortlessly generate wide range of customized textual elements. 8 5. Related Works 5.1. Diffusion Models Diffusion models generate data from noise, encapsulating approaches based on denoising score matching [22, 54, 55], maximum likelihood training [25], and rectified flow [32, 33]. One of the critical factors influencing the performance of diffusion models is the sampling distribution over noise levels during training, known as the importance sampling of noise levels. Studies focusing on noise schedule adjustment [18, 35] and weight adjustment [4, 22, 24] have succeeded in training high-quality diffusion models by carefully weighting different noise levels. Their effectiveness has been shown with object-centric metrics and benchmarks [11, 15, 21, 28]. 5.2. Style-Driven Generation With advancements in text-to-image models, practitioners have increasingly sought to generate images featuring personal styles [30, 44, 51, 61]. Fine-tuning methods [30, 45, 51] have been particularly prominent in this area. StyleDrop [51], study closely related to our work, utilizes masked generative model [3] and involves human data selection through multi-stage training. Some works focus on learning multiple concepts simultaneously [20, 27] or merging several fine-tuned models [30, 49], while others analyze the diffusion models U-Net [43] layers to identify those most effective for learning styles [9]. However, fine-tuning requires hyperparameter searches with each new large-scale model release [8, 29, 38], and is often applied without deep understanding of the diffusion objectives. As an alternative approach for style-driven generation, zero-shot approaches have been proposed [14, 44, 59, 61], but these methods still fall short in style alignment compared to fine-tuning and are often limited to specific domains [13, 46]. Due to these limitations, we focus on finetuning approaches in our work, aiming to provide insights into the behavior of diffusion objectives to make fine-tuning more accessible and effective. 6. Conclusion In this paper, we observed that stylistic features in diffusion models emerge predominantly at higher noise levels. To address the limitations of previous fine-tuning approaches in capturing new artistic styles, we proposed the Style-friendly SNR sampler, which biases the SNR distribution towards higher noise levels. We showed style-driven generation that reflects reference styles in terms of color schemes, layouts, illumination, and brushstrokes. We hope this work will serve as stepping stone toward using diffusion models as digital art previewers. Acknowledgement This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) [No. 2022R1A3B1077720], Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) [NO.RS-2021-II211343, Artificial Intelligence Graduate School Program (Seoul National University)], and the BK21 FOUR program of the Education and Research Program for Future ICT Pioneers, Seoul National University in 2024."
        },
        {
            "title": "References",
            "content": "[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 2 [2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 5, 6 [3] Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Patrick Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked In International Conference on generative transformers. Machine Learning, pages 40554075. PMLR, 2023. 2, 8 [4] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception priIn Proceedings of oritized training of diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1147211481, 2022. 2, 3, 8 [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in neural information processing systems, pages 87808794, 2021. 21 [6] Sander Dieleman. Diffusion is spectral autoregression, 2024. [7] Alexei Efros and William Freeman. Image quilting for texture synthesis and transfer. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 571576. 2023. 2 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 5, 6, 8, 12, 19, 20, 21, 22 [9] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Implicit style-content separation using b-lora. Cohen-Or. arXiv preprint arXiv:2403.14572, 2024. 8 [10] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24142423, 2016. 2 [11] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 2, 4, [12] Nicholas Guttenberg. Diffusion with offset noise. https: //www.crosslabs.org/blog/diffusion-withoffset-noise, 2023. 12, 21 [13] Zecheng He, Bo Sun, Felix Juefei-Xu, Haoyu Ma, Ankit Ramchandani, Vincent Cheung, Siddharth Shah, Anmol Kalia, Harihar Subramanyam, Alireza Zareian, et al. Imagine yourself: Tuning-free personalized image generation. arXiv preprint arXiv:2409.13346, 2024. 8 [14] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47754785, 2024. 2, 6, 8, 12, 21 [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 2, 8 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [18] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution imIn International Conference on Machine Learning, ages. pages 1321313232. PMLR, 2023. 2, [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2, 5, 21, 23 [20] Hexiang Hu, Kelvin CK Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Instruct-imagen: Image genGong, William Cohen, et al. In Proceedings of eration with multi-modal instruction. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47544763, 2024. 8 [21] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint arXiv: 2307.06350, 2023. 2, 4, 8 [22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 2, 3, 8 [23] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the In Proceedings of training dynamics of diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. 2 [24] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 9 [25] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. 3, 8 [26] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5 [27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of textto-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 8 [28] Tuomas Kynkaanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in frechet inception distance. In The Eleventh International Conference on Learning Representations, 2023. 2, 8 [29] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX.1-dev, 2024. 2, 3, 5, 8, 12 Flux.1-dev. [30] Kyungmin Lee, Sangkyung Kwak, Kihyuk Sohn, and Jinwoo Shin. Direct consistency optimization for compositional textto-image personalization. arXiv preprint arXiv:2402.12004, 2024. 2, 6, 8, 12, 21 [31] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature In Advances in neural information processing transforms. systems, pages 386396, 2017. 2 [32] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 2, 3, [33] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. 2, 3, 8 [34] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 5 [35] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. 8 [36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2, 3 [37] Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher J. Pal, and Marc Aubreville. Wuerstchen: An efficient architecture for large-scale text-to-image diffusion models, 2023. 2, 6, 21 [38] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 2, 6, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5, 6, 21 [40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 21 [41] Severi Rissanen, Markus Heinonen, and Arno Solin. GenIn The erative modelling with inverse heat dissipation. Eleventh International Conference on Learning Representations, 2023. 3 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 8 [44] Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Rb-modulation: Training-free personalization of diffusion models using stochastic optimal control. arXiv preprint arXiv:2405.17401, 2024. 2, 6, 8, 12, [45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2, 8, 22 [46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65276536, 2024. 8 [47] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [48] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2025. 23 [49] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. In European Conference on Computer Vision, pages 422438. Springer, 2025. 8 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, [50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using 10 In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 2 [51] Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel Ruiz, Dilip Krishnan, Huiwen Chang, Yuanzhen Li, Irfan Essa, Michael Rubinstein, et al. Styledrop: Text-to-image synthesis of any style. Advances in Neural Information Processing Systems, 36, 2024. 2, 5, 6, 8, 22, 25 [52] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. 6 [53] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 21 [54] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 3, [55] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. 2, 3, 8 [56] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, pages 3221132252. PMLR, 2023. 23 https : stable-diffusion-3.5-large. / / huggingface . co / stabilityai / stable - diffusion-3.5-large, 2024. 2, 5, 8, 12 [57] stabilityai. [58] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. 6 [59] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Instantstyle: Free lunch towards Qin, and Anthony Chen. style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. 8 [60] XLabs-AI. flux-ip-adapter. https://huggingface. co/XLabs-AI/flux-ip-adapter, 2024. 6 [61] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 6, 8, 12, 21, 23 11 methods, namely the SD3 sampler [8] and DCO [30]. As shown in Fig. S6, the results are consistent with the qualitative comparisons using FLUX-dev presented in Fig. 7. A.2. Quantitative Results CLIP Scores. In Fig. 6, we presented analyses of the mean µ, standard deviation σ, and LoRA rank using the DINO similarity score. In Fig. S7a, we provide the corresponding CLIP image similarity (CLIP-I) scores to further validate our findings. The CLIP-I scores exhibit similar trend to the DINO scores, where decreasing µ enhances style alignment. Varying σ affects the CLIP-I scores consistently with the DINO results. Our Style-friendly SNR sampler with µ = 6 and rank of 4 still outperforms the SD3 sampler with rank of 32 (dotted lines). Effectiveness Compared to Increasing Model Capacity. To demonstrate that our method is more effective than increasing model capacity, we conduct an additional experiment where we fine-tune the model using the SD3 sampler with higher LoRA rank of 128. As shown in Tab. S1, our Style-friendly SNR sampler with rank of 32 achieves higher DINO and CLIP-I scores compared to the SD3 sampler with rank of 128. This indicates that focusing on the critical noise levels where styles emerge has more significant impact than increasing the number of trainable parameters. Therefore, our approach enhances style alignment without necessitating larger model capacities. Trainable Parameters. To validate the importance of fine-tuning both transformer blocks of MM-DiT [8], we conduct an ablation study on SD3.5-8B, comparing the results of training LoRA adapters on only the imagetransformer blocks versus training on both the image and text-transformer blocks. As shown in Tab. S2, fine-tuning both the image and text-transformer blocks leads to higher DINO and CLIP-I scores compared to fine-tuning only the image-transformer blocks, while the CLIP-T scores are identical. This indicates that including the text-transformer blocks in the fine-tuning process enhances the models ability to learn stylistic features without compromising text alignment. These results suggest that to effectively capture new styles, it is beneficial to fine-tune both the visual and linguistic components of MM-DiT."
        },
        {
            "title": "Appendix",
            "content": "A. Additional Results A.1. Qualitative Results Additional Samples. We present additional samples using the FLUX-dev [29] to demonstrate the versatility of our method. Fig. S1 shows that even when fine-tuned on square reference images, our model can generate images with different aspect ratios while maintaining the reference style. For each prompt, we show results from two different random seeds to illustrate diversity across various aspect ratios. Fig. S2 provides additional typography samples in different aspect ratios, exhibiting our capability to produce stylized textual content. Additional Comparison. We further demonstrate the effectiveness of Style-friendly SNR sampler in learning complex style templates, such as multi-panel images. As shown in Fig. S3, our method captures the given multipanel style, generating images that closely resemble the reference. In contrast, previous fine-tuning approaches, SD3 sampler [8] and DCO [30], fail to learn the multi-panel concept, producing images without the panel structure. The offset noise [12] method attempts to reflect the style but still generates images with single panel or fewer panels than the reference. Zero-shot approaches including IPAdapter [61], RB-Modulation [44], and Style-Aligned [14] also attempt to generate multi-panel images but often produce outputs with structures different from the reference, as shown in Fig. S4. This highlights the capability of our method to handle challenging styles that other approaches struggle with. Effect of Varying µ on Object References. We examine how the parameter µ in our Style-friendly SNR sampler impacts object reference fine-tuning in Fig. S5. While FLUX fine-tunes object references well using the SD3 sampler, setting µ = 0 leads to failures in color binding and structure. For example, generating clock results in missing color and numeral; sunflowers around backpack and vase cause the objects to turn yellow; and purple rug makes the object also appear purple. In the flat cartoon devil example, the model fails to capture the references unique short limbs and structure. These findings indicate that larger µ hinders the learning of stylistic elements including color schemes inherent to objects. This suggests that the success of FLUX in object fine-tuning arises from its noise level distribution being adjusted toward object-centric generation. SD3.5 Samples. We extend our qualitative comparison by evaluating our Style-friendly SNR sampler using the SD3.58B model [57], comparing it against previous fine-tuning 12 Figure S1. Additional samples. Each row shows images generated with the same random seed at resolution of 1216832, using the prompts cute city made of sushi in {style prompt} style and mischievous ferret with playful grin squeezes itself into large glass jar, in {style prompt} style. 13 Figure S2. Typography. The first column shows reference images. The second and third columns display samples generated at resolution of 8321216, and the fourth column presents samples at 7041408 resolution. The prompts used are the words that says {letters} are written in English, in {style prompt} style, where {letters} represents the words synthesized in the samples. 14 Figure S3. Additional qualitative comparison. Our Style-friendly approach successfully captures complex multi-panel styles, generating images that closely resemble the reference. The prompts used are fluffy baby sloth with knitted hat trying to figure out laptop, close up in {style prompt} style, banana in {style prompt} style, Christmas tree in {style prompt} style, and bench in {style prompt} style. 15 Figure S4. Additional qualitative comparison. Our method effectively captures the multi-panel style, whereas zero-shot methods generate images with different structures or introduce artifacts. 16 Figure S5. Varying µ on object references. The object names are written at the top of the reference images. Setting µ = 0 (high log-SNR value) leads to failures in color binding and structure when fine-tuning on object references, whereas using the SD3 sampler allows FLUX to fine-tune object references effectively. This unveils why recent diffusion models perform well on object fine-tuning, as their noise level distributions are adjusted toward object-centric generation. 17 Figure S6. Comparison of fine-tuning the SD3.5-8B. The results with SD3.5-8B are consistent with the qualitative comparison based on FLUX-dev presented in Fig. 7. 18 Method Model SD3 Sampler [8] FLUX-dev FLUX-dev FLUX-dev w/ rank 128 Style-friendly Metrics DINO CLIP-I CLIP-T 0.645 0.373 0.668 0.426 0.686 0. 0.350 0.345 0.344 Table S1. Comparison to increasing LoRA rank."
        },
        {
            "title": "Method",
            "content": "DINO CLIP-I CLIP-T Style-friendly w/o Text attn 0.489 0.462 0.698 0.693 0.349 0.349 Table S2. Ablation study on trainable parameters. A.3. Additional Observations Styles Emerge at Higher Noise Levels. We examine the case where style descriptions are given only during the initial denoising steps. Specifically, we condition the diffusion model with the prompt yw/ style for the first 10% of the denoising process and then switch to the non-style prompt yw/o style for the remaining steps. As shown in Fig. S8, the generated images still reflect the intended styles despite the absence of style descriptions during most of the denoising. This confirms that stylistic features emerge primarily at higher noise levels, corresponding to the early denoising steps. The images are similar to those generated when style descriptions are used throughout all steps, indicating that early conditioning suffices for style representation. Probability Distribution of Timestep. We present the probability density plot of timesteps corresponding to sampled log-SNR values. By negatively shifting the log-SNR distribution, the resulting timestep distribution becomes aggressively skewed toward = 1, as shown in Fig. S9. The colored region indicates the timestep interval where we observe styles emerging. This shift increases the probability density in the timesteps critical for style representation, focusing on these timesteps during fine-tuning for style-driven generation. 19 (a) Varying µ. (b) Varying σ. (c) Varying LoRA Rank. Figure S7. SNR sampler analysis. CLIP-I similarities with FLUX and SD3.5-8B. Dotted lines in (c) indicate the results of SD3 sampler [8]. Figure S8. Prompt switching during generation. The generated images still reflect the intended styles even without style descriptions in most of the denoising process, indicating that stylistic features emerge mainly at the early denoising steps. Figure S9. Probability distribution of timestep t. Colored region indicates style-emerging noise levels. The timestep ranges between 0 and 1, where = 1 corresponds to pure noise. As increases, the log-SNR λt decreases. 20 B. Baselines In this section, we review the baseline methods used for comparison and highlight how our approach differs from them in addressing style-driven generation. The baseline methods include the fine-tuning-based methodsSD3 sampler [8], Direct Consistency Optimization (DCO) [30], and our methodas well as the zero-shot methods IPAdapter [61], RB-Modulation [44], and Style-Aligned [14]. All fine-tuning-based methods are trained using the same reference images and training prompts to ensure fair comparison. Notably, our Style-friendly SNR sampler disables timestep shifting [8] during fine-tuning. For the fine-tuning process, we utilize LoRA [19] with rank of 32 (except the LoRA rank analysis) and learning rate of 104. The models are trained with batch size of 1, employing gradient accumulation over 4 steps. All experiments are implemented using the Hugging Face Diffusers library version 0.31.0 and conducted on single NVIDIA A40 GPU. The FLUX-dev model is trained with gradient checkpointing to save memory consumption during training. B.1. Direct Consistency Optimization Direct Consistency Optimization (DCO) [30] is finetuning method inspired by direct preference optimization [40] commonly used in large language models (LLMs). Instead of directly minimizing the diffusion loss, DCO aims to ensure that the diffusion loss of the fine-tuned model is lower than that of the pre-trained model on the reference data. The objective function is defined as: LDCO(x0) = Et,ϵ (cid:20) logσ(βT w(t) (cid:21) fθ(xt, t) (xt, t)2 fϕ(xt, t) (xt, t)2) . (8) In this objective, the parameter βT controls the strength of the preference towards the fine-tuned model over the pretrained model. DCO increases the relative likelihood of the fine-tuned model over the pre-trained model, penalizing less when the fine-tuned models loss is smaller. This helps preserve the text-to-image alignment of the pre-trained model. However, DCO requires computations involving both the fine-tuned and pre-trained models, making it computationally more intensive than directly fine-tuning using the standard diffusion loss. In our experiments, we observed that using large value of βT = 1000 resulted in slower convergence and suboptimal performance. Therefore, we set βT = 1 to achieve better results. B.2. IP-Adapter IP-Adapter [61] is designed to enable text-to-image models to generate identity-preserving images by training compact adapter that encodes CLIP image embeddings [39]. This adapter introduces the CLIP image embedding as an additional input by concatenating its output with the text embeddings. The parameter-efficient nature of IP-Adapter allows for easy training and deployment across various textto-image models. However, notable limitation is its restricted style alignment due to the expressive constraints of CLIP embeddings, which may result in generated images that do not fully capture detailed stylistic characteristics. B.3. RB-Modulation RB-Modulation [44] is zero-shot approach using Stable Cascade [37], model accepting both CLIP image embeddings and text embeddings as inputs. During the denoising process, RB-Modulation employs gradient guidance of CSD, model fine-tuned from CLIP to measure style similarity, resembling classifier guidance [5]. At each denoising step, CSD computes the similarity between the approximated x0 and the reference image, guiding the generation process to enhance this similarity. RB-Modulation also aggregates multiple attention features. However, this approach relies on models that accept CLIP image embeddings, limiting model selection. Additionally, using gradient guidance of CSD increases inference costs, making the generation process more computationally intensive. B.4. Style-Aligned Style-Aligned [14] generates consistent sets of images with the same style by ensuring that features of each image attend to those of reference image through shared key and value features in self-attention layers of image tokens. It first maps the reference image to noise using DDIM inversion [53] and shares self-attention features during denoising. The fidelity to the reference style can be controlled by amplifying the self-attention logits in the diffusion model. However, Style-Aligned is not directly applicable to MMDiT [8] architecture that lacks image-only self-attention layers. Moreover, artificially amplifying self-attention logits can lead to artifacts and lower-quality images due to conflicting attention features. B.5. Offset Noise Offset noise [12] is method proposed to fine-tune diffusion models for generating monochromatic images. During the diffusion process, constant offset noiseidentical across all pixel positionsis added to the standard Gaussian noise, scaled by small factor (e.g., 0.1). This introduces an explicit bias toward monotonic noise patterns, encouraging the model to learn and reproduce solid colors. While offset noise aids in learning monotonous patterns, it can hinder the models capacity to learn more complex styles. Here, we additionally experiment with incorporating offset noise into our training process in Tab. S3. Offset noise 21 Method Model w/ offset 0.1 Style-friendly SD3 Sampler [8] SD3.5 SD3.5 SD3.5 SD3.5 w/ offset 0.01 w/ offset 0.1 Style-friendly SD3 Sampler [8] FLUX-dev FLUX-dev FLUX-dev FLUX-dev w/ offset 0.01 Metrics DINO CLIP-I CLIP-T 0.670 0.424 0.678 0.452 0.698 0.489 0.697 0.476 0.645 0.373 0.679 0.451 0.686 0.461 0.704 0. 0.350 0.353 0.349 0.350 0.350 0.349 0.344 0.341 Table S3. Incorporating offset noise. Offset noise improves SD3 sampler but still does not reach the performance of our Stylefriendly SNR sampler; combining our Style-friendly approach with Offset Noise at smaller scale (0.01) slightly enhances the style alignment of FLUX-dev. with scale of 0.1 improves the SD3 samplers results in DINO and CLIP-I scores, as many reference styles from the StyleDrop paper [51] have monochromatic backgrounds, favoring this trick. However, it still does not reach the performance of our Style-friendly SNR sampler. Moreover, when we combine our Style-friendly approach with smaller scale of offset noise (0.01), we observe slight improvement in the style alignment of FLUX-dev. This quantitative evaluation is based on the monochromatic backgrounds prevalent in the StyleDrop [51] references. Our qualitative comparisons in Fig. S3 show that offset noise struggles with complex references, failing to capture intricate stylistic details. This indicates that while offset noise can help with simple, uniform styles, it is vulnerable to complex styles. C. Experimental Details C.1. Style Prompts We conduct all quantitative evaluations using the 18 reference styles shown in the appendix of the StyleDrop paper [51]. The style prompts for these 18 styles can also be found in the StyleDrop appendix. For qualitative evaluations, we use additional challenging style references and we display the corresponding style prompt for each image in Fig. S10. The first and second references are produced with open-source meme generator source1 and source2 respectively. The third reference is cropped from the appendix of Dreambooth [45] paper. C.2. Evaluation Prompts We present 23 evaluation prompts collected from StyleDrop paper [51] used for our quantitative and qualitative comparisons: An Opera house in Sydney in {style prompt} style fluffy baby sloth with knitted hat trying to figure out laptop, close up in {style prompt} style Figure S10. Style prompts used for experiments. Golden Gate bridge in {style prompt} style The letter in {style prompt} style man riding snowboard in {style prompt} style panda eating bamboo in {style prompt} style friendly robot in {style prompt} style baby penguin in {style prompt} style moose in {style prompt} style towel in {style prompt} style An espresso machine in {style prompt} style An avocado in {style prompt} style crown in {style prompt} style banana in {style prompt} style bench in {style prompt} style boat in {style prompt} style butterfly in {style prompt} style An F1 race car in {style prompt} style Christmas tree in {style prompt} style cow in {style prompt} style hat in {style prompt} style piano in {style prompt} style wood cabin in {style prompt} style C.3. User Study In this section, we provide detailed information about the setup of our user study. Our user study aims to measure human preferences in two key objectives of style-driven image generation: style alignment and text alignment. To assess these preferences, we conduct pairwise comparisons be22 that offer faster inference speeds, such as Consistency Models [56] or Adversarial Diffusion Distillation models [48]. These developments could reduce both training and inference times, making style-driven generation more accessible and efficient. E. Broader Impact Our Style-friendly SNR sampler makes diffusion models successful in fine-tuning various style references. This advancement allows diffusion models to function effectively as digital art previewers, benefiting artists and non-expert users by simplifying the creative process. However, we note that it is important to be careful of copyright when using reference images for fine-tuning. Practitioners should ensure they have permissions to use reference images. tween our method and each baseline for each objective. Participants are shown the reference image, target text prompt, and two generated images (one from each method) and are asked to choose the image that better satisfies the objective. We collect three responses from each of the 150 participants, resulting in total of 450 responses for each comparison. The full instructions used in our questionnaires are as follows. For style alignment objective, Given reference image and two machine-generated images, select which machine-generated output better matches the style of the reference image for each pair. Please focus only on the style including color schemes, layouts, illumination, and brushstrokes. If its difficult to determine preference, please select Cannot Determine / Both Equally. For text alignment objective, Given reference image and two machine-generated images, select which machine-generated output better matches the target text for each pair. Please focus only on the text, without regard for the reference image. If its difficult to determine preference, please select Cannot Determine / Both Equally. C.4. Implementation To ensure reproducibility, we provide pseudo-code implementations of Style-friendly SNR samplers in Fig. S11 and the addition of LoRA [19] parameters to MM-DiT for training in Fig. S12. D. Limitations and Discussions Style Prompt Design. As shown in Fig. S13, using different style prompt during fine-tuning can lead to emphasizing different stylistic features, such as child-like elements or background architectures (second row) instead of watercolor painting elements (first row), which may not align with the users focus. Users should be mindful that variations in the style prompt can lead to different results. Nevertheless, our approach demonstrates effective style learning for style prompts given by the users. Computational Cost. While fine-tuning diffusion models remains the most promising approach for achieving style alignment, it involves significant computational costs. Finetuning for new style typically requires around 300 finetuning steps, and due to the iterative nature of diffusion models, generating single image during inference can take several seconds. We anticipate that future work will explore applying our Style-friendly SNR sampler during the training of zero-shot models [61] or integrating it with models 23 2 3 4 5 6 8 9 # Inputs: mu, std, B, latent # sample log-SNR logsnr = torch.normal(mean=mu, std=std, size=(B,)) # compute timestep = torch.nn.functional.sigmoid(-logsnr / 2).view(B, 1, 1, 1) # sample noise noise = torch.randn_like(latent) # diffuse latent noisy_latent = (1.0 - t) * latent + * noise # Generate Gaussian noise Figure S11. PyTorch implementation of Style-friendly SNR sampler. 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 # Inputs: model_name, rank # Configure LoRA for the specified model if model_name == \"FLUX\": target_modules = [ \"to_k\", \"to_q\", \"to_v\", \"to_out.0\", \"add_k_proj\", \"add_q_proj\", \"add_v_proj\", \"proj_mlp\", \"proj_out\" ] elif model_name == \"SD3\": target_modules = [ \"to_k\", \"to_q\", \"to_v\", \"to_out.0\", \"add_k_proj\", \"add_q_proj\", \"add_v_proj\", \"to_add_out\" ] else: raise ValueError(f\"Unsupported model: {model_name}\") # LoRA configuration transformer_lora_config = LoraConfig( r=rank, lora_alpha=rank, init_lora_weights=\"gaussian\", target_modules=target_modules, ) # Add adapter to the transformer transformer.add_adapter(transformer_lora_config) Figure S12. PyTorch implementation of LoRA integration. 24 Figure S13. Effect of Style Prompt Design. The first row shows images generated using style prompts from the StyleDrop paper [51] during both fine-tuning and generation. The second row shows images generated using different style prompt during both fine-tuning and generation. Each column is generated using the same random seed. This demonstrates how varying the style prompt can lead to different stylistic elements being emphasized in the generated images."
        }
    ],
    "affiliations": [
        "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
        "Data Science and AI Laboratory, ECE, Seoul National University"
    ]
}