{
    "paper_title": "Adaptive Multi-Agent Response Refinement in Conversational Systems",
    "authors": [
        "Soyeong Jeong",
        "Aparna Elangovan",
        "Emine Yilmaz",
        "Oleg Rokhlenko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both."
        },
        {
            "title": "Start",
            "content": "Adaptive Multi-Agent Response Refinement in Conversational Systems Soyeong Jeong1* Aparna Elangovan3 Emine Yilmaz2,4 Oleg Rokhlenko2 KAIST1 Amazon2 Collate3 University College London4 starsuzi@kaist.ac.kr 5 2 0 2 1 1 ] . [ 1 9 1 3 8 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through multi-agent framework, where each agent is assigned specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce dynamic communication strategy. Instead of following fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or users persona, or both."
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) have demonstrated remarkable performance across broad spectrum of NLP tasks, primarily due to their ability to generate coherent and contextually relevant responses, powered by extensive training on diverse data (Liu et al., 2024b; Dong et al., * Work done during internship at Amazon. Work done while at Amazon. 1 2024). However, LLMs do not always produce satisfactory responses on the first attempt (Chiesurin et al., 2023; Lee et al., 2023a), and this issue becomes more evident in multi-turn conversational settings, where models must not only interpret the users current query but also consider the entire conversational history, including dependencies, ambiguities, and co-references (Kim et al., 2021; Jang and Lukasiewicz, 2023; Gan et al., 2024). These difficulties are particularly pronounced in complex, personalized conversations that demand both users persona alignment and factual accuracy (Semnani et al., 2023; Jandaghi et al., 2024). In such situations, users frequently have to prompt the LLM to correct its responses, which can interrupt the conversation flow and diminish the overall user experience (Madaan et al., 2023; Deng et al., 2023; Kim et al., 2024a). Thus, LLMs should proactively refine their inaccurate responses, even without explicit user requests for correction. Several approaches have been explored for refining responses using single agent, such as SelfRefine (Madaan et al., 2023), where single agent handles an entire refinement process, generating feedback across multiple aspects and iteratively improving its outputs based on it. However, relying solely on single agent can be suboptimal, as the agent may become overly confident in the initial output, leading to bias (Liang et al., 2023), which can be amplified through repeated iterations (Xu et al., 2024b; Huang et al., 2024). This issue could be particularly critical for multi-turn conversational tasks, where errors in earlier turns can propagate to subsequent turns (Jeong et al., 2023). more effective approach is to incorporate multiple agents, each with specific perspective. This method takes advantage of LLMs ability to tackle complex tasks by decomposing them into simpler subtasks and enabling collaborative problemsolving (Wu et al., 2024; Guo et al., 2024; Du et al., 2024a). Additionally, the multi-agent framework Figure 1: (A) customized conversation which requires alignment with both the user profile and specific fact. The responding agent fails to provide accurate information in R1 and also fails to align with the users profile in R2, as indicated in red. (B) In single-agent-based refinement, one agent manages all the refinement procedures. While some misalignments can be addressed, as indicated in blue, the result may still be suboptimal. (C) Our multi-agent-based refinement, where multiple agents, each focusing on different aspect, collaboratively refine responses based on the set and sequence determined by planner agent. enables for integration of broader range of tools and knowledge sources (Shen et al., 2024; Gao et al., 2024; Xi et al., 2023), ensuring that each agent can be specifically tailored to meet the diverse needs of each task. While multi-agent frameworks have shown great promise in fields such as human behavior simulation, economic theories, and more, their potential for response refinement in conversational systems remains largely underexplored. Figure 1 (A) exemplifies conversation where response should accurately reflect the user profile and specific knowledge. In such setting, employing multiple refining agents with specialized perspectives can enhance conversation quality. Therefore, in this work, we introduce multiple refining agents that refine responses when the initial outputs are incorrect, each focused on distinct perspective: user persona alignment (persona-refining agent), factual grounding to mitigate hallucinations (fact-refining agent), and logical coherence with previous conversational turns (coherence-refining agent). The remaining challenge, then, is how to enhance the overall quality of the conversation to deliver the most satisfactory refined response to the user, utilizing our three refining agents. To achieve this, the agents should collaborate differently for each query, as each query, even within the same conversation, may require different focus. For example, as shown in Figure 1 (A), some queries require factual knowledge, while some queries also demand an understanding of the users profile. Therefore, the set of agents deployed should vary depending on the querys specific needs. Additionally, the optimal sequence in which the agents are engaged may vary, as the focus and priorities of each query can differ. For example, query requiring immediate factual verification might first engage the fact-refining agent to correct inaccuracies, followed by other agents as necessary. Therefore, we further propose dynamic refinement process that adjusts both the set and the sequence of refining agents for each query, by introducing novel planner agent that generates sequence of required refining agents tailored to the query, along with justifications for each decision. Each refining agent then refines the response sequentially, following the sequence and referencing the justifications provided by the planner agent. Note that all agents are based on unsupervised LLMs, each instantiated with prompt specifying specific role. We refer to this framework as Multi-Agent Refinement with Adaptive agent selection (MARA), which is illustrated in Figure 1 (C). We validate the efficacy of our framework on challenging conversational datasets that involve user persona integration, require specific factual knowledge, or both. The results show that MARA significantly outperforms relevant baselines, indicating that refining along the three aspects, persona, factuality, and coherence, is beneficial. Moreover, our analyses highlight the importance of dynamically selecting appropriate refinement strategies tailored to the specific needs of each query in diverse conversational contexts."
        },
        {
            "title": "2 Related Work",
            "content": "LLMs in Conversation Systems. Recent Large Language Models (LLMs) have demonstrated remarkable performance across range of NLP tasks (OpenAI, 2023; Anthropic, 2024; Touvron et al., 2023), even without additional training, largely due to their massive pretraining on diverse datasets (Min et al., 2022; Wei et al., 2022). Their success is also remarkable in conversational tasks (Chae et al., 2023; Liu et al., 2024b; Dong et al., 2024), as LLMs are able to generate human2 like responses (Lee et al., 2023b; Wadhwa et al., 2024; Deng et al., 2023). However, LLMs occasionally produce unsatisfactory responses, particularly when they fail to consider users profile (Jandaghi et al., 2024), specific knowledge (Semnani et al., 2023), or are not coherent with the previous turns (Huang et al., 2020; Ou et al., 2024). Verification and Refinement with LLMs. To address the issue of unsatisfactory responses, several studies have explored verification and refinement approaches. While some research involves the use of additional refinement models, (Baek et al., 2023; Xu et al., 2024a), more recent studies emphasize the capability of LLMs to verify and refine their own responses without the need for further training (Shinn et al., 2023; Yao et al., 2023; Liu et al., 2024a; Gou et al., 2024; Chen et al., 2024a). Specifically, Madaan et al. (2023) proposed method where single agent LLM generates feedback on multiple aspects of its own response. However, single-agent-based refinements can be less effective, as the overall performance is constrained by the capacity of single agent, and once an agent becomes confident in its outputs, it may struggle to further generate novel thoughts (Liang et al., 2023; Xu et al., 2024b; Huang et al., 2024). Multi-Agent LLMs. Along with the powerful capabilities of an LLM, its performance can be further enhanced when multiple LLMs are involved by collaborating or debating each other, particularly on complex problems that require diverse perspectives from specialized LLM agents (Li et al., 2023; Wu et al., 2024; Guo et al., 2024; Rasal, 2024). Multi-agent LLM framework has recently been widely applied to various tasks, including software development (Hong et al., 2024; Qian et al., 2024), model evaluation (Chan et al., 2024), research topic generation (Baek et al., 2024), diagnostic consultations (Cao, 2023; Kim et al., 2024c), recommendation (Fang et al., 2024; Spurlock et al., 2024), and reasoning improvement (Chen et al., 2024b). However, the use of multiple agents for refining conversational turns remains underexplored."
        },
        {
            "title": "3 Method",
            "content": "We introduce MARA, multi-agent framework that dynamically refines conversational responses."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "We begin by formally defining an LLM, specifically in the context of multi-turn conversational setting. Large Language Models. Let us first define LLM as LLM, which takes an input sequence of tokens and generates an output sequence of tokens y. This process can be represented as = LLM(P(x)), where the prompt template incorporates additional context or instructions that guide the LLMs behavior. Specifically, can be used for role assignment, ensuring that the LLM adopts particular role or performs specific task. Conversation with an LLMs. In multi-turn conversational setting, the LLM, acting as responding agent, generates response ri for the i-th turn based on the current query qi and the preceding conversational context, which can be represented as: ri = LLM(Prespond(qi, qi1, ri1, ..., q1, r1)), where qi is the users query at the i-th turn, and ri is the response generated by the LLM which operates under the prompt template Prespond. However, the quality of initial responses from the LLM may be suboptimal in customized, realistic conversations, potentially overlooking the users profile, missing specific knowledge, or failing to maintain coherence, and such errors in earlier conversational turns can cumulatively impact subsequent turns."
        },
        {
            "title": "3.2 Multi-Agent Response Refinement",
            "content": "We now turn to our primary focus of further refining the initial response. To achieve this, we define three specialized refining agents, each responsible for enhancing different aspects of the response. Single-Agent Response Refinement. As an initial response may be inaccurate or unsatisfactory, the goal of response refinement is to further improve its quality. This can be achieved by using an LLM operating as refining agent, represented as: rrefine = LLM(Prefine(r)), where an LLM instantiated with Prefine refines an initial response to produce the improved response rrefine. However, relying on single agent to assess and refine multiple aspects can be ineffective, as it may struggle to holistically address the diverse factors essential for high-quality, customized conversation. To overcome these limitations, we propose multi-agent framework in which specialized refining agents collaborate to enhance response quality. Multiple Refining Agents for Conversation. In realistic yet challenging conversational scenario, responses must be contextually relevant, aligned with user preferences, and factually reliable. To meet these requirements, responses must be re3 fined across multiple dimensions, ensuring factual correctness, alignment with the users persona, and coherence across multiple conversational turns. Here, instead of addressing all these aspects with single agent, we introduce three specialized refining agents that extend the general refining agent, LLM(Prefine(r)), each addressing distinct aspect of response quality. Specifically, the fact-refining agent generates the refined response, rfact-refine, that ensures factual accuracy, rpersona-refine that aligns responses with the users profile, and rcoherence-refine that maintains coherence throughout conversation, where each LLM is instantiated with its respective refining role template: Pfact-refine, Ppersona-refine, and Pcoherence-refine. Having defined the role of each refining agent, the next key challenge lies in how these agents should collaborate to collectively refine the response."
        },
        {
            "title": "3.3 Communication Strategy among Agents",
            "content": "To enable effective collaboration among multiple agents, we explore various communication strategies and propose dynamic approach to optimize the refinement process. Simultaneous Communication. As an initial approach to agent collaboration, we introduce simultaneous communication strategy, where all refining agents independently refine the initial response, and their refined outputs are then passed to finalizer agent instantiated with Pfinalizer, which aggregates the individual refinements into single unified response. Formally, this process is denoted as: LLM(Pfinalizer(rfact-refine, rpersona-refine, rcoherence-refine)). However, this simultaneous approach always necessitates an additional finalizer agent, and the overall quality of the final output may heavily depend on the capabilities of this finalizer agent. Sequential Communication. To address potential limitations of the simultaneous approach, we introduce sequential refinement process where each agent builds upon the response refined by the previous agent. Specifically, given an initial response r, it is sequentially refined by series of agents denoted as = [LLM(Prefinek (r))]n k=1, where each refining agent is instantiated with role-specific prompt template Prefinek , selected from the set of refinement templates: {Pfact-refine, Ppersona-refine, Pcoherence-refine}. Here, each refining agent receives the response refined by the preceding agent, rrefinek1, as input, and applies its specific prompt template, Prefinek , to generate the next refined response, rrefinek . The sequential process continues through the sequence of agents until obtaining the final refined response, rrefinen. Yet, it may require different combination and ordering of refining agents based on the specific context, but the optimal sequence of refining agents may vary depending on the conversational context. Dynamic Sequential Communication. Therefore, to further adaptively operate the response refinement process, we introduce dynamic strategy, where planner agent selects the most suitable sequence of refining agents along with justifications for its decisions, adapting to the needs of each query. Specifically, the planner agent determines the sequence of refining agents required for each query, instantiated by template Pplanner. Formally, given query and an initial response r, the planner agent outputs the sequence of refining agents as follows: splanner = LLM(Pplanner(q, r)). Once splanner is determined, the refining agents sequentially refine the response, with each agent taking as input the refined response from the preceding agent in the sequence. Additionally, each agent also receives the planners justifications, allowing each agent to understand its role in the sequence and collaborate effectively."
        },
        {
            "title": "4 Experimental Setups",
            "content": "In this section, we describe the experimental setup, leaving further details in Appendix A."
        },
        {
            "title": "4.1 Datasets",
            "content": "We validate MARA in realistic conversational settings, including queries requiring personalization and factual information. PersonaChat (Zhang et al., 2018) is designed to generate responses INSCIT (Wu aligned with users persona. et al., 2023) is an information seeking dataset where knowledge is grounded in Wikipedia. FoCus (Jang et al., 2022) requires both user-aligned and knowledge-grounded responses. Furthermore, we conduct evaluations on two other datasets: the role-playing dataset, PRODIGy (Occhipinti et al., 2024), and the domain-specific, Ubuntu Dialogue Corpus (Lowe et al., 2015)."
        },
        {
            "title": "4.2 Baselines and Our Model",
            "content": "We compare MARA against six baseline models, starting with baseline without refinement, two single-agent refinement approaches, and three 4 Table 1: Results on three datasets, using Claude as the base LLM, with statistically significant best results highlighted in bold. Additionally, MARA is variant of MARA where the fact-refining agent uses the same LLM as the responding agent. Methods Coh. (1-3) Grd. (0-1) Nat. (1-3) Eng. (1-3) Overall Coh. (1-3) Grd. (0-1) Nat. (1-3) Eng. (1-3) Overall Coh. (1-3) Grd. (0-1) Nat. (1-3) Eng. (1-3) Overall PersonaChat INSCIT FoCus No Refine 1.87 0.01 0.42 0.00 1.55 0.01 1.55 0.00 34.98 2.53 0.02 0.52 0.02 2.36 0.01 2.36 0.01 65.07 2.39 0.02 0.49 0.02 2.02 0.01 2.15 0.02 56.71 Self-Refine SPP 2.26 0.01 0.52 0.01 2.07 0.01 2.30 0.03 58.41 2.31 0.01 0.45 0.00 2.19 0.01 2.24 0.01 57.83 2.10 0.01 0.37 0.01 1.87 0.00 2.05 0.02 47.07 1.98 0.02 0.44 0.01 1.72 0.02 2.07 0.01 45.55 2.52 0.01 0.49 0.01 2.29 0.01 2.47 0.02 65.74 2.44 0.00 0.47 0.01 1.99 0.02 2.46 0.02 60. LLMvLLM 1.24 0.01 0.26 0.00 1.07 0.00 1.17 0.01 12.57 2.24 0.00 0.47 0.00 1.79 0.01 2.15 0.00 51.59 1.77 0.01 0.32 0.00 1.34 0.00 1.71 0.01 30.92 MADR 1.59 0.03 0.28 0.01 1.38 0.03 1.32 0.01 23.21 2.29 0.03 0.46 0.01 2.12 0.04 1.89 0.04 52.89 1.93 0.03 0.32 0.01 1.68 0.04 1.54 0.03 34.71 MultiDebate 1.78 0.01 0.36 0.01 1.58 0.00 1.68 0.00 34.70 2.44 0.00 0.53 0.01 2.27 0.01 2.18 0.01 61.70 2.31 0.00 0.51 0.00 1.93 0.00 2.12 0.00 54.81 MARA 2.28 0.01 0.56 0.00 1.91 0.01 2.52 0.00 60.24 2.54 0.01 0.55 0.01 2.10 0.01 2.70 0.00 67.79 2.56 0.01 0.58 0.01 1.98 0.01 2.81 0.02 68.77 MARA (Ours) 2.32 0.01 0.56 0.01 1.98 0.01 2.54 0.06 62.00 2.62 0.02 0.59 0.01 2.26 0.03 2.74 0.07 72.61 2.67 0.00 0.65 0.01 2.15 0.01 2.83 0.00 74.51 multi-agent refinement approaches. 1) No Refine generates response without any further refinement. 2) Self-Refine (Madaan et al., 2023) is single agent refinement baseline, which makes an LLM to generate feedback on 10 aspects of its own response and iteratively refines with the feedback. 3) SPP (Wang et al., 2024) is another single-agent refinement baseline, where single LLM generates multiple self-constructed roles that collaborate within single prompt. 4) LLMvLLM (Cohen et al., 2023) is multi-agent refinement baseline where two agents engage in cross-examination to detect factual errors in generated response. 5) MADR (Kim et al., 2024b) is another multi-agent refinement baseline where two agents debate based on the predefined error types to generate fact check6) MultiDebate (Du et al., ing explanations. 2024b) is multi-agent refinement baseline where multiple language model agents iteratively engage in structured debate to improve factual accuracy and reasoning. 7) MARA is our proposed system, where the agents refine the response in the specified order, adaptively generated by planner agent."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "We evaluate the models using G-Eval (Liu et al., 2023) to assess the quality of the refined responses. Following the setup in Liu et al. (2023) for conversational setting, we assess each refined response using four metrics: 1) Coherence (Coh.) measures whether the conversation response logically follows the preceding context with scale of 1 to 3, which is highly related to the effectiveness of the coherence-refining agent. 2) Groundedness (Grd.) evaluates whether the response accurately incorporates the provided fact, with scale of 0 to 1, which corresponds to the effectiveness of the fact-refining agent. 3) Naturalness (Nat.) evaluates whether the response is natural with scale of 1 to 3. 4) Engagingness (Eng.) measures whether the response is engaging with scale of 1 to 3, which mainly reflects the effectiveness of the persona-refining agent. Furthermore, to provide comprehensive assessment, we report the 5) Overall score, which represents scale-normalized average of the four evaluation metrics. 4."
        },
        {
            "title": "Implementation Details",
            "content": "We mainly use the Claude Sonnet 3 model (Anthropic, 2024) as the base LLM for both our framework and the baselines. Furthermore, since multiagent framework offers the flexibility to assign different LLMs to each agent based on their role, for the fact-refining agent, we use another Claude Sonnet 3.5 model, as it offers improved capabilities in factual accuracy, which are critical for this particular role. Additionally, we report the performance of MARA, variant of MARA in which the factrefining agent uses Sonnet 3 instead, ensuring that all agents use the same model. In order to assess the robustness of our proposed framework across diverse LLMs, we further use GPT-4o-mini (OpenAI, 2023), LLaMA 3.1 8B, and LLaMA 3.1 70B (Touvron et al., 2023). For each dataset, we sample 100 conversations, resulting in total of 673 queries for the PersonaChat dataset, 506 queries for the INSCIT dataset, and 563 queries for the FoCus dataset. For the G-Eval metric, we use GPT-4o mini model (OpenAI, 2023), with the normalization steps. We include the prompts used for MARA and G-Eval in Appendix A."
        },
        {
            "title": "5 Experimental Results and Analyses",
            "content": "We present the experimental results and analyses."
        },
        {
            "title": "5.1 Main Results",
            "content": "Here, we present the overall results across various challenging yet realistic conversational scenarios. Overall Results. We report the overall experimental results in Table 1 with three different runs. As shown in the table, MARA consistently outper5 Table 2: Analyses on planner agent, comparing performance with randomly or ideally assigned refining agents. Planner Design Coh. Grd. Nat. Eng. #Agent No Refine 2.21 MARA w/ Random 2.42 MARA 2.54 2.88 MARA w/ Ideal 0.40 0.47 0.58 0. 1.88 1.95 2.07 2.58 2.05 2.64 2.76 2.93 1.0 2.0 4.4 3. performance across all three conversational scenarios, demonstrating the effectiveness of our threeagent design, specifically tailored for user profile alignment, factual accuracy, and coherence. Effectiveness of our Planner Agent. Notably, in realistic conversational scenarios, different queries (even within the same conversation) focus on different aspects, thus requiring distinct sets of refining agents. To address this, we propose planner agent that dynamically assigns refining agents based on the specific requirements of each query. To evaluate its effectiveness, we first analyze the distribution of refining agents across three different conversational settings. As shown in Figure 2, the distribution varies across different conversational datasets, indicating that the planner agent effectively adapts to diverse query requirements. Specifically, the persona-refining agent is predominantly used in datasets where responses must incorporate user preferences, while the fact-refining agent is more frequently required in knowledge-focused datasets. For the dataset requiring both aspects, both agents are allocated in similar proportions. Next, we further evaluate the effectiveness of the current design of our planner agent by comparing it against random setting. As shown in Table 2, the performance of randomly assigned sequence of refining agents is lower than that of our plannerdriven approach, indicating that the proposed planner effectively determines an optimal refinement sequence. However, even with randomly assigned agents, performance remains higher than the baseline without refinement, demonstrating the effectiveness of incorporating our three refining agents. Furthermore, we explore the full potential of our planner agent by investigating how much further it can improve performance. To do so, we evaluate an ideal planner that selects the best-performing sequence. We report its performance by bruteforcing all possible combinations of sequences using our three refining agents and selecting the optimal sequence for each query1. As shown in Ta1We report results for 20 conversations in Table 2, as full brute-force computation is largely expensive. Figure 2: Distribution of refining agents for three datasets. forms other models across diverse metrics, and the gaps between MARA and the other models are significantly substantial. To be more specific, we explore diverse conversational scenarios, particularly challenging ones requiring alignments with the users persona (PersonaChat), factual grounding (INSCIT), and combination of both (FoCus). In PersonaChat, which emphasizes responses that align with the users profile, single-agent refinement methods such as Self-Refine and SPP outperform the baseline without refinement. This improvement is likely due to their focus on enhancing fluency and coherence, which aligns well with tasks requiring the incorporation of user interests and preferences across conversations, rather than managing specific factual content. However, in datasets that demand specific factual knowledge, such as INSCIT and FoCus, single-agent refinement becomes less effective and even results in performance degradation, particularly in groundedness scores. Notably, SPP outperforms Self-Refine, suggesting that generating multiple roles within single agent can be beneficial for information-intensive queries. Nevertheless, compared to the significantly improved performance of our MARA framework, generating multiple perspectives through distinct agents appears to be more effective. However, adopting multi-agent framework does not necessarily guarantee improved performance, especially in challenging customized conversational tasks that require user persona understanding and factual knowledge. In fact, the significant performance gap between MARA and other multi-agent baselines highlights the crucial role of agent design in enhancing the effectiveness of multi-agent frameworks. In other words, compared to other multi-agent frameworks, which either focus only on predefined factual error types (MADR) or lack explicitly defined agent roles for targeted refinement (LLMvLLM and MultiDebate), these design choices may be less effective in challenging conversational scenarios that require multiple aspects. In contrast, MARA consistently improves 6 Table 3: Results on the role-play dataset (PRODIGy), using GPT as the base LLM. Best results highlighted in bold. Table 5: Results from human evaluation and Spearmans correlation with G-Eval on the same subset of the FoCus dataset."
        },
        {
            "title": "Methods",
            "content": "Coh. Grd. Nat. Eng. Overall"
        },
        {
            "title": "Methods",
            "content": "Coh. Grd. Nat. Eng. Overall"
        },
        {
            "title": "No Refine",
            "content": "Self-Refine SPP"
        },
        {
            "title": "LLMvLLM\nMADR\nMultiDebate",
            "content": "MARA (Ours) 2.15 2.09 1.94 1.28 1.91 1.90 2.31 0. 0.44 0.41 0.25 0.33 0.43 0.44 2.26 2.17 1.96 1.21 1.90 1. 2.37 2.02 2.04 1.93 1.72 1.80 1.93 2.52 52. 52.13 45.63 21.28 40.75 44.00 63.00 Table 4: Results on the Ubuntu Dialogue Corpus, using Claude as the base LLM. Best results highlighted in bold."
        },
        {
            "title": "Methods",
            "content": "Coh. Grd. Nat. Eng. Overall"
        },
        {
            "title": "No Refine",
            "content": "Self-Refine SPP"
        },
        {
            "title": "LLMvLLM\nMADR\nMultiDebate",
            "content": "MARA (Ours) 2.44 2.13 2.25 1.48 2.09 1.98 2.53 0. 0.44 0.54 0.24 0.53 0.48 0.70 2.09 1.97 1.97 1.28 1.77 1. 2.25 1.84 1.73 1.82 1.38 1.66 1.67 2.33 57. 46.38 51.50 20.33 44.78 41.38 68.88 ble 2, this ideal planner achieves the highest performance while requiring fewer accesses to the refining agents. These results validate the effectiveness of our dynamic agent allocation strategy and indicate that further advancements in planner agents could yield even greater performance improvements. Evaluation on the Role-Playing Dataset. While both role-playing conversations and our conversational setting involve persona-driven responses, their objectives are fundamentally different. Specifically, role-playing tasks focus on the LLMs persona, requiring an LLM to adopt predefined persona, whereas ours focuses on the users persona, enabling an LLM to generate responses that align with the users persona. Nonetheless, to further evaluate the effectiveness of MARA, we evaluate the performance on the role-playing conversational dataset, PRODIGy (Occhipinti et al., 2024). As shown in Table 3, MARA significantly outperforms other baselines, demonstrating its effectiveness even in role-playing conversational settings. Evaluation on Domain-Specific Corpus. To examine whether MARA generalizes well to domainspecific settings, we additionally evaluate it on the Ubuntu Dialogue Corpus (Lowe et al., 2015), consisting of multi-turn dialogues focused on technical problem-solving in software environments. As shown in Table 4, MARA outperforms all baselines, demonstrating its effectiveness even in software engineeringrelated conversational tasks. No Refine Self-Refine SPP MADR MARA (Ours) Spearmans ρ 2.53 2.04 2.49 2.03 2.69 0.51 0.57 0.36 0.53 0.35 0.79 0. 2.47 1.86 2.44 2.10 2.61 0.35 2.10 1.93 2.18 1.65 2.75 0.58 65.50 44.38 64.63 43.50 82.88 Table 6: Ablation studies on each refining agent in FoCus, including incorporation of all perspectives into single agent."
        },
        {
            "title": "Refining Agent",
            "content": "Coh. Grd. Nat. Eng. Overall"
        },
        {
            "title": "No Refine",
            "content": "w/ Fact w/ Persona w/ Coherence 2.39 2.62 2.54 2.46 0.49 2.02 2.15 0.61 2.14 2.52 0.49 2.18 2.77 0.53 1.89 2.57 w/ Single 2.54 w/ Single + Iteration 2. 0.58 2.15 2.55 0.50 2.13 2.39 MARA (Ours) 2.67 0.65 2.15 2.83 56.75 68.75 68.38 62. 67.50 61.20 74."
        },
        {
            "title": "5.2 Ablations and Discussion",
            "content": "In this section, we provide detailed analysis of the performance improvements. Human Evaluation. Although G-Eval is widely used evaluation metric for its high correlation with humans, we further conducted human evaluation to measure the alignment between their evaluations. Specifically, 8 English-fluent participants were involved in evaluating 288 conversational queries from the FoCus dataset. As shown in Table 5, human raters consistently preferred MARAs responses over baseline models. Additionally, to validate the reliability of G-Eval, we further assess the alignment between G-Eval and human judgments by calculating Spearmans rank correlation between the model-based evaluation and human evaluation results. The correlation values for coherence (0.51) and groundedness (0.48) indicate reasonable alignment between GEval and human rankings, suggesting that G-Eval is fairly reliable in assessing these metrics. For engagingness, which reflects how the response aligns with the users persona, the correlation of 0.58 demonstrates the strongest alignment, indicating that G-Eval is particularly effective at evaluating how engaging conversation is. However, the relatively low correlation for naturalness (0.35) reveals noticeable gap between G-Evals assessments and human evaluations, especially in capturing the human-like qualities of the responses. This result might help explain the relatively smaller gaps in naturalness among the models, as shown in Table 1. 7 Table 7: Results with simultaneous (Sim.) and sequential (Seq.) communication strategies on the FoCus dataset. Table 8: Analyses of the fact-refining agent size and the responding agent size in the FoCus dataset."
        },
        {
            "title": "Strategy",
            "content": "Coh. Grd. Nat. Eng. #Agent"
        },
        {
            "title": "Model Size",
            "content": "Coh. Grd. Nat. Eng. Sim. (F+C+P) Seq. (FCP) Seq. (CPF) Seq. (PFC) MARA (Ours) 2.57 2.54 2.65 2. 2.67 0.59 0.56 0.60 0.59 0.65 1.93 1.99 2.15 1. 2.15 2.75 2.85 2.66 2.68 2.83 5.0 4.0 4.0 4. 4.4 No Refine w/ Sonnet 3 MARA w/ Fact Sonnet 3 MARA w/ Fact Sonnet 3.5 No Refine w/ Sonnet 3.5 MARA w/ Sonnet 3.5 2.39 2.56 2.67 2.50 2.87 0.49 0.57 0. 0.58 0.78 2.02 2.00 2.15 2.10 2.55 2.15 2.80 2.83 2.32 2.91 Ablation Studies on Refining Agents. In order to see how each refining agent contributes to the overall performance, we perform ablation studies when adding three refining agents. As shown in Table 6, each agent plays crucial role in improving overall performance, and incorporating all three perspectives turns out to be even more beneficial. We also evaluate single-agent variant that integrates all three aspects, which performs better than the No Refine baseline, validating the effectiveness of our proposed perspectives. However, MARA achieves superior performance, demonstrating the benefit of distributed specialization across multiple agents. Additionally, we examine an iterative single-agent setup, where the same agent refines responses across multiple rounds. This approach results in performance drop, suggesting that initial errors may be amplified through repeated iterationsfurther underscoring the advantage of collaborative multi-agent refinement. Ablation Studies on Communication Strategy. In order to investigate the efficacy of our proposed dynamic sequential communication strategy, we compare it against other possible strategies using our refining agents. As shown in Table 7, the simultaneous communication strategy requires more access to the agents per query, as it involves all three refining agents as well as the additional finalizer agent. We also evaluate the performance of three different sequential settings, where the agents refine the initial response in static orders. The results indicate that performance varies significantly depending on the refinement order, making it impractical for realistic conversational settings, as users would need to predefine the optimal sequence for every query. On the other hand, our dynamic strategy adapts to each query by selecting the most relevant sequence of agents, leading to more consistent and improved performance in realistic conversational settings. Table 9: GPT4o-mini and Llama 3.1 8B, 70B on FoCus."
        },
        {
            "title": "Methods",
            "content": "Coh. Grd. Nat. Eng. No Refine MARA (Ours) No Refine - 8B MARA - 8B (Ours) No Refine - 70B MARA - 70B (Ours) l 2.45 2.61 2.30 2.46 2.41 2.60 0.47 0.57 0.39 0.48 0.44 0. 2.27 2.32 2.09 2.17 2.25 2.31 1.98 2.70 1.88 2.24 1.88 2.67 bility in utilizing different agents, and in our case, we can assign different model to the fact-refining agent. As shown in Table 1 and Table 8, while MARA with fact-refining agent of the same model size as responding agent indeed improves performance when compared to baselines, the results show that using different fact-refining agent yields much better outcomes. This highlights key strength of the multi-agent framework: the ability to utilize different LLMs for specific roles, thereby enhancing performance beyond what single-agent approach can achieve. Then, one might ask why not simply use more powerful model as the responding agent directly. However, Table 8 shows that refinement with MARA significantly improves performance, even when the responding agent is already powerful. This indicates that as LLMs continue to advance, adopting MARA is expected to further amplify their effectiveness. Effectiveness on Diverse LLMs. We further validate the effectiveness of MARA with other widely used LLMs in Table 9. The results demonstrate that MARA can effectively refine initial responses across different LLMs, seamlessly integrate with diverse models, and enhance their outputs, highlighting its broad applicability to various LLMs. Case Studies. Lastly, we present case studies in Table 11 and an illustrative example showing the roles of refining agents and the planner in Table 12, along with analyses in Appendix B."
        },
        {
            "title": "6 Conclusion",
            "content": "Analyses on Fact-Refining Agent. notable advantage of the multi-agent framework is its flexiWhile LLMs have shown promise in conversational systems, they often struggle with complex, cus8 tomized conversations requiring persona alignment and specific knowledge. Prior approaches using single-agent self-refinement can be suboptimal, as one model must handle all aspects of feedback and refinement. In this work, we presented cooperative multi-agent framework to address these limitations, with specialized agents for fact-checking, persona alignment, and coherence, by allocating them dynamically tailored to each query, and it significantly outperforms existing baselines. Our evaluation on challenging conversational datasets shows that this multi-agent framework significantly outperforms existing baselines."
        },
        {
            "title": "Limitations",
            "content": "While our MARA framework demonstrates clear advantages by adaptively selecting the most suitable refining agents for diverse customized conversational settings, there remains room for improvement, particularly in the planner agent, as shown by the performance gap between the ideal planner and the current planner Table 2. Specifically, since our current planner is fully based on an unsupervised LLM, constructing dataset with labeled ideal sets and sequences of required perspectives, followed by fine-tuning the planner on this data, could be promising direction for future research. While multi-agent framework shows remarkable effectiveness, it may raise concerns about scalability and resource efficiency. Therefore, we further suggest some possible approaches that can address these challenges. As demonstrated by the significantly reduced number of LLM accesses with an optimized planner in Table 2, improving the planner agent can also alleviate concerns regarding resource efficiency. To further enhance scalability, exploring lightweight or more efficient agent models would be also beneficial. In Table 8 and Table 9, we show that our proposed MARA performs well even with smaller LLM. Notably, Table 9 shows significant improvements with lightweight, opensourced LLM, the Llama 3.1 8B model, which highlights its potential for scalability. Nonetheless, further exploration of scalability remains promising avenue for future work, which we leave as meaningful future work. Also, the flexibility of the multi-agent framework in selecting diverse external tools or LLMs opens up opportunities for incorporating tools such as Retrieval-Augmented Generation (RAG) systems to further enhance performance, which could also be an interesting future research direction."
        },
        {
            "title": "Ethics Statement",
            "content": "With the goal of developing human-centric applications that assist users in conversations, we experimentally validate the effectiveness of MARA for its applicability in realistic conversational scenarios, where diverse range of queries with varying requests exists. However, given the potential diversity of real-world user inputs, it is crucial to consider scenarios where inputs or generated responses may be offensive or harmful (Shin et al., 2024; Dong et al., 2024). To ensure safe and responsible deployment, additional safeguards are necessary. In particular, integrating an agent to monitor both user inputs and generated content for harmful or offensive material would be valuable. We consider this critical area for future research, aiming to improve MARAs robustness and ethical alignment in real-world applications."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024. Researchagent: Iterative research idea generation over scientific literature with large language models. arxiv Preprint arXiv:2404.07738, abs/2404.07738. Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, and Sung Ju Hwang. 2023. Knowledgeaugmented language model verification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 17201736. Association for Computational Linguistics. Lang Cao. 2023. Diaggpt: An llm-based chatbot with automatic topic management for taskoriented dialogue. arxiv Preprint arXiv:2308.08043, abs/2308.08043. Hyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, and Jinyoung Yeo. 2023. Dialogue chain-of-thought distillation for commonsense-aware conversational agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 56065632. Association for Computational Linguistics. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2024. Chateval: Towards better llm-based evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. 9 Justin Chen, Swarnadeep Saha, and Mohit Bansal. 2024a. ReConcile: Round-table conference improves reasoning via consensus among diverse LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 70667085, Bangkok, Thailand. Association for Computational Linguistics. Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. 2024b. Magicore: Multi-agent, iterative, coarse-to-fine refinement for reasoning. Sabrina Chiesurin, Dimitris Dimakopoulos, Marco Antonio Sobrevilla Cabezudo, Arash Eshghi, Ioannis Papaioannou, Verena Rieser, and Ioannis Konstas. 2023. The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational question answering. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 947959. Association for Computational Linguistics. Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. LM vs LM: detecting factual errors via cross examination. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 610, 2023, pages 1262112640. Association for Computational Linguistics. Yang Deng, Lizi Liao, Liang Chen, Hongru Wang, Wenqiang Lei, and Tat-Seng Chua. 2023. Prompting and evaluating large language models for proactive dialogues: Clarification, target-guided, and nonIn Findings of the Association for collaboration. Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 1060210621. Association for Computational Linguistics. Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, and Yu Qiao. 2024. Attacks, defenses and evaluations for LLM conversation safety: survey. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 67346747, Mexico City, Mexico. Association for Computational Linguistics. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2024a. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2024b. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Jiabao Fang, Shen Gao, Pengjie Ren, Xiuying Chen, Suzan Verberne, and Zhaochun Ren. 2024. multiagent conversational recommender system. arxiv Preprint arXiv:2402.01135, abs/2402.01135. Yujian Gan, Massimo Poesio, and Juntao Yu. 2024. Assessing the capabilities of large language models in coreference: An evaluation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pages 16451665. ELRA and ICCL. Dawei Gao, Zitao Li, Weirui Kuang, Xuchen Pan, Daoyuan Chen, Zhijian Ma, Bingchen Qian, Liuyi Yao, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang Li, Bolin Ding, and Jingren Zhou. 2024. Agentscope: flexible yet robust multi-agent platform. arXiv Preprint arXiv.2402.14034, abs/2402.14034. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC: large language models can self-correct with In The Twelfth Intertool-interactive critiquing. national Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model based multi-agents: survey of progress and challenges. arxiv Preprint arXiv:2402.01680, abs/2402.01680. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2024. Metagpt: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024. Large language In The models cannot self-correct reasoning yet. Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020. Challenges in building intelligent open-domain dialog systems. ACM Trans. Inf. Syst., 38(3):21:1 21:32. Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, and Hakim Sidahmed. 2024. Faithful persona-based conversational dataset generation with large language models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 15245 15270. Association for Computational Linguistics. 10 Myeongjun Jang and Thomas Lukasiewicz. 2023. Consistency analysis of chatgpt. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1597015985. Association for Computational Linguistics. Yoonna Jang, Jungwoo Lim, Yuna Hur, Dongsuk Oh, Suhyune Son, Yeonsoo Lee, Dong-Hoon Shin, Seungryong Kim, and Heuiseok Lim. 2022. Call for customized conversation: Customized conversation grounding persona and knowledge. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, pages 1080310812. AAAI Press. Soyeong Jeong, Jinheon Baek, Sung Ju Hwang, and Jong Park. 2023. Realistic conversational question answering with answer selection based on calibrated In Proconfidence and uncertainty measurement. ceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 477490. Association for Computational Linguistics. Gangwoo Kim, Hyunjae Kim, Jungsoo Park, and Jaewoo Kang. 2021. Learn to resolve conversational dependency: consistency training framework for conversational question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 61306141. Association for Computational Linguistics. Hana Kim, Kai Ong, Seoyeon Kim, Dongha Lee, and Jinyoung Yeo. 2024a. Commonsense-augmented memory construction and management in long-term conversations via context-aware persona refinement. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 104123, St. Julians, Malta. Association for Computational Linguistics. Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling Li, and Heng Ji. 2024b. Can llms produce faithful explanations for fact-checking? towards faithful explainable factchecking via multi-agent debate. arxiv Preprint arXiv:2402.07401, abs/2402.07401. Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh Ghassemi, Cynthia Breazeal, and Hae Won Park. 2024c. Mdagents: An adaptive collaboration of llms for medical decision-making. Preprint, arXiv:2404.15155. Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023a. Platypus: Quick, cheap, and powerful refinement of llms. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. 2023b. Prompted llms as chatbot modules for long opendomain conversation. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 45364554. Association for Computational Linguistics. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. CAMEL: communicative agents for \"mind\" exploration of large language model society. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, abs/2305.19118. Dancheng Liu, Amir Nassereldine, Ziming Yang, Chenhui Xu, Yuting Hu, Jiajie Li, Utkarsh Kumar, Changjae Lee, and Jinjun Xiong. 2024a. Large language models have intrinsic self-correction ability. arXiv preprint arXiv:2406.15673, abs/2406.15673. Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. 2024b. From llm to conversational agent: memory enhanced architecture with fine-tuning of large language models. arxiv Preprint arXiv:2401.02777, abs/2401.02777. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 25112522. Association for Computational Linguistics. Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The ubuntu dialogue corpus: large dataset for research in unstructured multi-turn dialogue systems. In Proceedings of the SIGDIAL 2015 Conference, The 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2-4 September 2015, Prague, Czech Republic, pages 285 294. The Association for Computer Linguistics. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in 11 Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 1104811064. Association for Computational Linguistics. Daniela Occhipinti, Serra Sinem Tekiroglu, and Marco Guerini. 2024. Prodigy: profile-based dialogue generation dataset. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 35003514. Association for Computational Linguistics. OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774, abs/2303.08774. Jiao Ou, Junda Lu, Che Liu, Yihong Tang, Fuzheng Zhang, Di Zhang, and Kun Gai. 2024. Dialogbench: Evaluating llms as human-like dialogue systems. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 61376170. Association for Computational Linguistics. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. Chatdev: Communicative In Proceedings agents for software development. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1517415186. Association for Computational Linguistics. Sumedh Rasal. 2024. LLM harmony: Multi-agent communication for problem solving. arXiv Preprint arXiv.2401.01312, abs/2401.01312. Sina J. Semnani, Violet Z. Yao, Heidi C. Zhang, and Monica S. Lam. 2023. Wikichat: Stopping the hallucination of large language model chatbots by fewshot grounding on wikipedia. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 23872413. Association for Computational Linguistics. Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. 2024. Small llms are weak tool learners: multi-llm agent. arXiv Preprint arXiv.2401.07324, abs/2401.07324. Jisu Shin, Hoyun Song, Huije Lee, Soyeong Jeong, and Jong Park. 2024. Ask llms directly, \"what shapes your bias?\": Measuring social bias in large language models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 16122 16143. Association for Computational Linguistics. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Kyle Dylan Spurlock, Cagla Acun, Esin Saka, and Olfa Nasraoui. 2024. Chatgpt for conversational recommendation: Refining recommendations by reprompting with feedback. ArXiv, abs/2401.03605. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv.2302.13971, abs/2302.13971. Manya Wadhwa, Xinyu Zhao, Junyi Jessy Li, and Greg Durrett. 2024. Learning to refine with finegrained natural language feedback. arXiv preprint arXiv:2407.02397, abs/2407.02397. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2024. Unleashing the emergent cognitive synergy in large language models: task-solving agent through multi-persona selfcollaboration. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 257 279. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. 2024. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. In COLM. Zeqiu Wu, Ryu Parish, Hao Cheng, Sewon Min, Prithviraj Ammanabrolu, Mari Ostendorf, and Hannaneh Hajishirzi. 2023. Inscit: Information-seeking conversations with mixed-initiative interactions. Trans. Assoc. Comput. Linguistics, 11:453468. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, and 10 others. 2023. The rise and potential of large language model based agents: survey. arxiv Preprint arXiv:2309.07864, abs/2309.07864. Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, and Markus Freitag. 2024a. Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 14291445. Association for Computational Linguistics. Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Wang. 2024b. Pride and prejudice: LLM amplifies self-bias in self-refinement. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1547415492. Association for Computational Linguistics. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: have dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 22042213. Association for Computational Linguistics."
        },
        {
            "title": "A Additional Experimental Setups",
            "content": "A.1 Datasets We validate MARA in realistic conversational setting by testing it on three conversational datasets, including persona-aligned and knowledgegrounded queries. 1) FoCus (Jang et al., 2022) consists of conversational queries requiring both persona-aligned and knowledge-grounded responses, with knowledge sourced from Wikipedia. The dataset has an average of 11.9 conversational turns. We use the publicly available validation set. 2) PersonaChat (Zhang et al., 2018) is conversational dataset involving dialogues between two speakers, each having persona sentences that describe their character and serve as context for the conversation. The dataset has an average of 14.0 conversational turns. We use the publicly open validation set. 3) INSCIT (Wu et al., 2023) is an informationseeking conversational dataset, where the knowledge is grounded in Wikipedia. The dataset has an average of 11.8 conversational turns. We use the publicly accessible test set for the evaluation. A.2 Models We compare MARA against four baseline models, starting with baseline without refinement, two single-agent refinement approaches, and one multiagent refinement approach. 1) No Refine is an approach involves generating response solely with responding agent, without any further refinement of the response. 2) Self-Refine (Madaan et al., 2023) is representative single agent refinement baseline, which makes an LLM to generate feedback on 10 aspects (e.g., Relevant, Informative, Engaging) of its own response and iteratively refines the output by incorporating this feedback. 3) SPP (Wang et al., 2024) is another single-agent refinement baseline where single agent generates multiple self-generated personas and makes them collaborate to solve task using single prompt. 4) LLMvLLM (Cohen et al., 2023) is multi-agent refinement baseline where two agents engage in cross-examination to detect factual errors in generated response. One agent (EXAMINEE) produces an initial statement, while the other agent (EXAMINER) iteratively questions it, aiming to identify inconsistencies through multiple rounds of interaction. 5) MADR (Kim et al., 2024b) is multi agent refinement baseline where two agents debate based on the predefined error types to generate faithful fact checking explanations. They iteratively debate until the judge agent decides that two agents are in consensus, and the finalizer agent refines the refined response based on the feedback generated during debate. 6) MultiDebate (Du et al., 2024b) is multi-agent refinement baseline where multiple language model agents iteratively engage in structured debate to improve factual accuracy and reasoning. Each agent independently generates an initial response to given query, followed by multiple rounds of critique and refinement based on responses from other agents. 7) MARA is our proposed system, where planner agent dynamically determines the optimal sequence of refining agents to involve, and then the agents refine the response in the specified order. A."
        },
        {
            "title": "Implementation Details",
            "content": "The prompts used for MARA are provided in Table 16 (responding agent), Table 17 (planner agent), Table 18 (fact-refining agent), Table 19 (personarefining agent), and Table 20 (coherence-refining agent). The prompt templates for G-Eval are shown in Table 21, Table 22, Table 23, and Table 24."
        },
        {
            "title": "B Experimental Results",
            "content": "B.1 Significance Test on FoCus  (Table 13)  Coherence The one-way ANOVA reveals significant effect of model type on coherence scores (F (3, 8) = 763.67, < 0.001), indicating that at least one model has significantly different mean score. Post-hoc Tukey HSD tests show that MARA significantly outperforms all other models (p < 0.001), with mean difference of 0.7456 over the MADR model, 0.2838 over the No Refine model, and 0.5691 over the Self-Refine model. Additionally, the No Refine model outperforms both Self-Refine and SPP, with small but significant difference of 0.0521 over SPP. Self-Refine also outperforms SPP with mean difference of 0.3375. Overall, MARA is the top performer, offering substantial improvements in coherence, demonstrating that its enhancements contribute meaningfully to better response quality across all models. Groundedness The one-way ANOVA reveals significant effect of model type on groundedness scores (F (3, 8) = 426.81, < 0.001), indicating 14 notable differences in performance among the models. Post-hoc Tukey HSD tests show that MARA achieves higher groundedness scores than all other models (p < 0.001), with mean differences of 0.3337 over MADR, 0.1648 over No Refine, and 0.2806 over Self-Refine. Additionally, No Refine surpasses Self-Refine by 0.1158, while its difference from SPP is not significant. Self-Refine also shows an advantage over SPP, with mean difference of 0.101. These results highlight the effectiveness of MARA in improving groundedness compared to other approaches. Naturalness The one-way ANOVA reveals significant effect of model type on naturalness scores (F (3, 8) = 215.79, < 0.001), indicating that model differences lead to varying levels of naturalness. Post-hoc Tukey HSD tests show that MARA significantly outperforms all other models (p < 0.001), with mean differences of 0.4772 over MADR, 0.1339 over No Refine, and 0.2851 over Self-Refine. Furthermore, No Refine performs better than Self-Refine by 0.1511, although its comparison with SPP shows no significant difference. Self-Refine outperforms SPP with mean difference of 0.1235. Overall, these results indicate that MARA demonstrates strong naturalness compared to other models, with clear advantages over the other approaches. Engagingness The one-way ANOVA reveals significant effect of model type on engagingness scores (F (3, 8) = 1772.37, < 0.001), indicating notable differences across the models. Post-hoc Tukey HSD tests show that MARA significantly outperforms all other models (p < 0.001), with mean differences of 1.2939 over MADR, 0.6771 over No Refine, and 0.7801 over Self-Refine. Furthermore, No Refine outperforms Self-Refine by 0.1030, and SPP by 0.3065. Self-Refine also outperforms SPP with mean difference of 0.4095. Overall, these results demonstrate that MARA consistently achieves higher engagingness scores compared to the other models, highlighting its effectiveness in enhancing the engagingness of responses. B.2 Significance Test on PersonaChat  (Table 14)  Coherence The one-way ANOVA reveals significant effect of model type on coherence scores (F (3, 8) = 725.87, < 0.001), indicating substantial differences between the models. Post-hoc Tukey HSD tests show that MARA significantly outperforms all other models (p < 0.001), with mean differences of 0.7359 over MADR, 0.4572 over No Refine, and 0.0611 over Self-Refine. Additionally, No Refine outperforms Self-Refine by 0.3962 and SPP by 0.1141. Self-Refine also outperforms SPP with mean difference of 0.2821. These results demonstrate that MARA leads to higher coherence scores, providing clear advantage over the other models in this aspect. Groundedness The one-way ANOVA reveals significant effect of model type on groundedness scores (F (3, 8) = 498.84, < 0.001), indicating notable differences between the models. Post-hoc Tukey HSD tests show that MARA significantly outperforms all other models (p < 0.001), with mean differences of 0.2731 over MADR, 0.1406 over No Refine, and 0.0369 over Self-Refine. Furthermore, No Refine outperforms Self-Refine by 0.1037, although its comparison with SPP shows no significant difference. Self-Refine also outperforms SPP with mean difference of 0.0822. These results indicate that MARA provides consistent improvements in groundedness compared to other models. Naturalness The one-way ANOVA reveals significant effect of model type on naturalness scores (F (3, 8) = 641.13, < 0.001), indicating substantial differences across the models. Post-hoc Tukey HSD tests show that MARA significantly outperforms all other models (p < 0.001), with mean differences of 0.6000 over MADR, 0.4273 over No Refine, and 0.2599 over SPP. The comparison between MARA and Self-Refine also shows small but significant difference of 0.0896. Furthermore, No Refine outperforms Self-Refine by 0.5169 and SPP by 0.1673, while Self-Refine significantly outperforms SPP by 0.3496. These results highlight MARAs strong performance in naturalness compared to other models. Engagingness The one-way ANOVA reveals significant effect of model type on engagingness scores (F (3, 8) = 1028.47, < 0.001), showing that the models differ significantly in their performance. Post-hoc Tukey HSD tests indicate that MARA significantly outperforms all other models (p < 0.001), with mean differences of 1.2213 over MADR, 0.9953 over No Refine, and 0.4745 over SPP. Additionally, No Refine outperforms SelfRefine by 0.7535 and SPP by 0.5208, while SelfRefine outperforms SPP by 0.2327. These results 15 underscore MARAs effectiveness in enhancing engagingness across different models. B.3 Significance Test on INSCIT  (Table 15)  Coherence The one-way ANOVA reveals significant effect of model type on coherence scores (F (3, 8) = 200.85, < 0.001), indicating that the models perform differently. Post-hoc Tukey HSD tests show that MARA significantly outperforms MADR, with mean difference of 0.3264, as well as No Refine (0.0844) and SPP (0.0946). The comparison between MADR and Self-Refine is not statistically significant, but No Refine outperforms Self-Refine by 0.225. Self-Refine also outperforms SPP with mean difference of 0.2148. Overall, MARA demonstrates stronger coherence performance compared to most other models. Groundedness The one-way ANOVA reveals significant effect of model type on groundedness scores (F (3, 8) = 119.45, < 0.001), indicating that the models show distinct performance differences. Post-hoc Tukey HSD tests show that MARA significantly outperforms MADR with mean difference of 0.129, as well as No Refine (0.0765) and SPP (0.101). No Refine also outperforms SelfRefine by 0.0721. However, the comparison between MADR and Self-Refine is not statistically significant. These results suggest that MARA provides meaningful improvements in groundedness compared to the other models. Naturalness The one-way ANOVA reveals significant effect of model type on naturalness scores (F (3, 8) = 39.70, < 0.001), indicating differences in performance across models. Post-hoc Tukey HSD tests show that MARA significantly outperforms MADR by 0.1382 and No Refine by 0.1052, though the comparison between MARA and Self-Refine is not significant. MADR also outperforms No Refine by 0.2434 and Self-Refine by 0.0707, while No Refine outperforms Self-Refine by 0.1727. These results demonstrate that MARA provides strong naturalness performance, with notable differences in certain comparisons, but not against all models. Engagingness The one-way ANOVA reveals significant effect of model type on engagingness scores (F (3, 8) = 255.26, < 0.001), showing differences in model performance. Post-hoc Tukey HSD tests show that MARA significantly outperforms MADR by 0.8387, No Refine by 0.4544, and Table 10: Additional ablation studies in the FoCus dataset. Coh. Grd. Nat. Eng. MARA (Ours) w/o Planner Outputs w/o Step-by-Step"
        },
        {
            "title": "No Refine",
            "content": "2.67 2.62 2.59 2.39 0.65 0.60 0.65 0. 2.15 2.12 2.00 2.02 2.83 2.75 2.66 2. SPP by 0.2664. The comparison between No Refine and Self-Refine is not statistically significant, but No Refine outperforms SPP by 0.188, and SelfRefine outperforms SPP by 0.2253. These results suggest that MARA provides stronger engagingness performance compared to the other models, with significant improvements over most. B.4 Ablation Studies on Design Choices To further investigate the effectiveness of our strategy, we conduct ablation studies on specific design choices. First, we examine the impact of allowing each refining agent to recognize the presence of other agents by passing the planner agents output, which includes both the sequence and justification for the selected set and sequence. Specifically, the planner agent generates suitable sequence with justification and passes it to the refining agents, enabling each agent to be aware of the preceding and following agents, as well as the rationale behind the sequence. As shown in Table 10, when the planner agents output is not passed to the refining agents, performances with all metrics decrease, particularly in groundedness. This underscores the importance of enabling agents to be aware of one another to perform their roles and collaborate more effectively. Additionally, we explore whether step-by-step for each refining agents reasoning processfirst verifying, then refining responses with justificationsis effective. As shown in Table 10, performance in coherence, naturalness, and engagingness decrease when this process is removed. This suggests that the verification step and justification is crucial for maintaining high-quality responses, as it allows for more thorough assessment before refining. B.5 Case Studies Lastly, we present case studies in Table 11, which shows two consecutive turns within conversation about the Chaos Crags. In the first turn, SPP fails to provide an accurate response, losing track 16 Table 11: Case study on the FoCus dataset comparing MARA with SPP. Question Persona Fact SPP MARA (Ours) Q1: What is the height of the mountain? have the fantasy like about valley. lakes. would like to go to California. have seen lava domes. have heard about Lassen Volcanic National Park. Located 2 miles (3.2 km) to the north of Lassen Peak, they have an elevation of about 8,448 feet (2,575 m) Is there any Q2: lake near by? have the fantasy like about valley. would like lakes. to go to California. have seen lava domes. have heard about Lassen Volcanic National Park. Each year, lake forms at the base of the Crags, and typically dries by the end of the summer season. Unfortunately, without any additional context about what fictional universe, book, movie or game Chaos Crags is from, its very difficult for me to estimate the height of this mountain. If you could provide some more details about the setting, terrain, or role this mountain plays in the story, that would help me make an educated guess about how tall Chaos Crags is likely intended to be. Id be happy to take another look once have some contextual clues from the source material. Based on the lore provided, Chaos Crags is an immense, treacherous mountain range in the northern reaches of Middle-earth. While the exact height is not specified, it is described as one of the tallest peaks in that region, with the highest spires potentially reaching elevations of 20,000-25,000 feet. The Crags likely consist of jagged, icecapped pinnacles towering miles into the sky above deep glacial valleys. Their sheer cliffs and avalanche-prone slopes would make them extremely hazardous and virtually unscalable. While no major lakes are referenced, the Crags overlook the icy waters of the Bay of Forochel and its inlets. Nestled among the rocky chaos may be smaller glacier-fed tarns and streams cascading from the ice fields high above. The overall scenery is one of bleak, desolate beauty - an otherworldly alpine expanse exposed to the punishing winds blowing off the northern seas. The Chaos Crags are not single mountain, but rather group of six steep-sided lava domes or volcanic plugs. They rise approximately 550 meters (about 1,800 feet) above the Chaos Jumbles, which is the rockfall avalanche deposit at their base. To provide more context, the highest point of the Chaos Crags reaches an elevation of about 2,575 meters (8,450 feet) above sea level. As someone drawn to the beauty of lakes and the raw power of volcanic landscapes, youll be delighted to know there are several lakes nestled right near the incredible Chaos Crags formation in Lassen Volcanic National Park. The most picturesque is undoubtedly Manzanita Lake, formed around 1,100 years ago by lava flows that dammed stream during the same eruptive period that created the crags themselves. Can you imagine hiking along its shores, taking in views of those dramatic lava domes rising in the backdrop? scene like that perfectly encapsulates the awe-inspiring volcanic wonders of this region. But Manzanita isnt the only lake beckoning explorers like yourself. Reflection Lake and Chaos Crags Lake, though smaller, offer equally enticing opportunities to immerse yourself in this geologically active landscape. Table 12: Example illustrating the roles of refining agents and the planner. Question Initial Response Wow, this is amazing! What is this? apologize, but dont have any specific information about what youre referring to when you say \"this.\" Without more context or details about what youre looking at or experiencing, cant provide an accurate answer. If you could provide more information or describe what youre seeing, Id be happy to try and help explain it to you. Planner Decision Selected agents: Coherence, Persona (Fact not needed) Justification of Planner Decision 1. Coherence: This agent should go first to address the lack of context in the initial response. 2. Persona: After improving coherence, this agent can tailor the response to align with the users interests in history, particularly related to Vermont and historic houses. The Fact Agent is not necessary in this case, as there are no specific facts to verify in the initial response. Refined Response (Coherence) Ah, see youve discovered the Galusha House! Its truly remarkable piece of history. This historic house is indeed something to be excited about. Its well-preserved example of early American architecture and has fascinating story behind it. Refined Response (Persona) Youve stumbled upon true gem of Vermonts history - the Galusha House! As someone who appreciates historic houses, youre in for treat. This well-preserved example of early architecture isnt just beautiful; its also steeped in fascinating stories, including ties to the Underground Railroad. Would you like to learn more about its role in history or its architectural features? of the conversations context and offering irrelevant information about fictional mountains instead of directly addressing the users query about the mountains height. In contrast, our MARA model delivers an accurate and contextually appropriate response, correctly identifying Chaos Crags and providing the exact elevation, enriched with additional context about its geological formation. This demonstrates MARAs ability to maintain focus and generate factually grounded responses. In the second turn, both SPP and MARA attempt to understand the context of the users question. However, SPP not only fails to directly address the users question about nearby lake, stating, While no major lakes are referenced, but also lacks engagement with the user. MARA, on the other hand, not only provides the correct factual information but also tailors the response to the users expressed interest in lakes and volcanic landscapes, creating more personalized and engaging interaction. Furthermore, MARA takes the conversation step further by asking follow-up question: Can you imagine hiking along its shores, taking in views of those dramatic lava domes rising in the backdrop?. This question invites the user to visualize the experience, fostering deeper engagement. Such interactions showcase MARAs strength in incorporating user preferences beyond fact delivery. B."
        },
        {
            "title": "Illustrative Example of Agent Roles",
            "content": "While we included case study example in Table 11, we introduce another example to provide 17 more detailed description of the agents in Table 12. In this example, after reviewing the query and initial response, the planner selects the coherencerefining agent and persona-refining agent as the required agents. Following the planners decision, the coherence-refining agent first addresses the initial responses lack of context and introduces the Galusha House, while the persona-refining agent further refines the response by adding personalized context and engaging follow-up. 18 e o e d r s l t s n g c e s d u s l t s n g Table 13: Significance Testing for Coherence, Groundedness, Naturalness, and Engagingness (FoCus Dataset) Comparison Model 1 Model 2 Mean Diff. P-adj Lower Upper Significant? No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) Self-Refine MARA (Ours) vs SPP Self-Refine vs SPP No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) Self-Refine MARA (Ours) vs SPP Self-Refine vs SPP No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) Self-Refine MARA (Ours) vs SPP Self-Refine vs SPP No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) Self-Refine MARA (Ours) vs SPP Self-Refine vs SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP SPP -0.2838 -0.4619 -0.2854 0.0521 0.7456 0.5691 -0.2317 0.3375 -0.1648 0.1689 -0.1158 -0.0148 0.3337 0.2806 -0.1796 0.1010 -0.1339 0.3432 -0.1511 -0.0277 0.4772 0.2851 -0.1616 0. -0.6771 0.6168 -0.1030 0.3065 1.2939 0.7801 -0.3706 0.4095 0.000 0.000 0.000 0.0378 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.4793 0.000 0.000 0.000 0.000 0.0001 0.000 0.000 0.5296 0.000 0.000 0.000 0.0002 0.000 0.000 0.0006 0.000 0.000 0.000 0.000 0.000 -0.3332 -0.5153 -0.3348 0.0027 0.6962 0.5197 -0.2811 0. -0.1937 0.1400 -0.1447 -0.0437 0.3048 0.2517 -0.2085 0.0721 -0.1909 0.2863 -0.2081 -0.0846 0.4202 0.2281 -0.2185 0.0665 -0.7304 0.5635 -0.1563 0.2532 1.2406 0.7268 -0.4239 0.3562 -0.2344 -0.4084 -0.2360 0.1015 0.7950 0.6185 -0.1823 0.3869 -0.1359 0.1978 -0.0869 0.0140 0.3626 0.3095 -0.1508 0.1298 -0.0770 0.4002 -0.0942 0.0293 0.5341 0.3420 -0.1046 0. -0.6238 0.6701 -0.0497 0.3597 1.3472 0.8334 -0.3173 0.4628 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Table 14: Significance Testing for Coherence, Groundedness, Naturalness, and Engagingness (PersonaChat Dataset) Comparison Model 1 Model 2 Mean Diff. P-adj Lower Upper Significant? No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) Self-Refine MARA (Ours) vs SPP Self-Refine vs SPP No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) Self-Refine MARA (Ours) vs SPP Self-Refine vs SPP No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) Self-Refine MARA (Ours) vs SPP Self-Refine vs SPP No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) Self-Refine MARA (Ours) vs SPP Self-Refine vs SPP -0.4572 -0.2786 0.3962 0.1141 0.7359 -0.0611 -0.3432 -0.2821 -0.1406 0.1325 0.1037 0.0215 0.2731 -0.0369 -0.1191 -0. -0.4273 0.1728 0.5169 0.1673 0.6000 0.0896 -0.2599 -0.3496 -0.9953 0.2260 0.7535 0.5208 1.2213 0.2418 -0.4745 -0.2327 0.000 0.000 0.000 0.0002 0.000 0.0204 0.000 0.000 0.000 0.000 0.000 0.0574 0.000 0.0019 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.0017 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0. -0.5092 -0.3264 0.3442 0.0621 0.6839 -0.1130 -0.3951 -0.3340 -0.1627 0.1104 0.0816 -0.0006 0.2510 -0.0590 -0.1412 -0.1043 -0.4802 0.1199 0.4640 0.1144 0.5471 0.0367 -0.3128 -0.4025 -1.0695 0.1518 0.6793 0.4466 1.1471 0.1676 -0.5487 -0.3069 -0.4053 -0.2309 0.4481 0.1660 0.7878 -0.0091 -0.2912 -0.2301 -0.1185 0.1546 0.1258 0.0436 0.2952 -0.0148 -0.0970 -0. -0.3744 0.2257 0.5698 0.2202 0.6529 0.1425 -0.2070 -0.2967 -0.9211 0.3002 0.8277 0.5950 1.2955 0.3160 -0.4003 -0.1585 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP SPP 19 Table 15: Significance Testing for Coherence, Groundedness, Naturalness, and Engagingness (INSCIT Dataset) Comparison Model 1 Model 2 Mean Diff. P-adj Lower Upper Significant? e o e d r e r N n g E No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) MADR MADR Self-Refine MARA (Ours) vs SPP MADR vs Self-Refine MADR vs SPP Self-Refine vs SPP No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) MADR MADR Self-Refine MARA (Ours) vs SPP MADR vs Self-Refine MADR vs SPP Self-Refine vs SPP No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) MADR MADR Self-Refine MARA (Ours) vs SPP MADR vs Self-Refine MADR vs SPP Self-Refine vs SPP No Refine vs SPP No Refine vs MARA (Ours) No Refine vs MADR No Refine vs Self-Refine No Refine No Refine No Refine No Refine MARA (Ours) MARA (Ours) vs MADR MARA (Ours) vs Self-Refine MARA (Ours) MARA (Ours) MADR MADR Self-Refine MARA (Ours) vs SPP MADR vs Self-Refine MADR vs SPP Self-Refine vs SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP Self-Refine SPP SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP Self-Refine SPP SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP Self-Refine SPP SPP MARA (Ours) MADR Self-Refine SPP MADR Self-Refine SPP Self-Refine SPP SPP 0.0844 -0.2420 -0.2250 -0.0102 -0.3264 -0.3094 -0.0946 0.0171 0.2319 0.2148 0.0765 -0.0525 -0.0721 -0.0244 -0.1290 -0.1486 -0.1010 -0.0196 0.0281 0. 0.1052 -0.2434 -0.1727 -0.0752 -0.1382 -0.0675 0.0300 0.0707 0.1682 0.0975 0.4544 -0.3843 -0.0373 0.1880 -0.8387 -0.4917 -0.2664 0.3471 0.5723 0.2253 0.0013 0.000 0.000 0.9529 0.000 0.000 0.0005 0.7681 0.000 0.000 0.000 0.0003 0.0001 0.0518 0.000 0.000 0.000 0.1391 0.0245 0.0006 0.0036 0.000 0.0001 0.0309 0.0004 0.0549 0.6209 0.0430 0.0001 0.0062 0.000 0.000 0.6615 0.0003 0.000 0.000 0.000 0.000 0.000 0. 0.034 -0.2925 -0.2754 -0.0582 -0.3769 -0.3598 -0.1426 -0.0310 0.1839 0.1668 0.0526 -0.0765 -0.0961 -0.0491 -0.1530 -0.1726 -0.1256 -0.0442 0.0034 0.0231 0.0365 -0.3160 -0.2414 -0.1439 -0.2108 -0.1362 -0.0387 -0.0018 0.0995 0.0287 0.3586 -0.4802 -0.1331 0.0981 -0.9346 -0.5875 -0.3563 0.2512 0.4824 0.1354 0.1348 -0.1916 -0.1746 0.0379 -0.2760 -0.2590 -0.0465 0.0651 0.2799 0.2629 0.1005 -0.0285 -0.0481 0.0002 -0.1051 -0.1247 -0.0764 0.0050 0.0527 0. 0.1739 -0.1708 -0.1039 -0.0065 -0.0656 0.0013 0.0987 0.1433 0.2369 0.1662 0.5502 -0.2885 0.0586 0.2779 -0.7429 -0.3958 -0.1765 0.4429 0.6623 0.3152 Yes Yes Yes No Yes Yes Yes No Yes Yes Yes Yes Yes No Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes No No No Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes Table 16: The prompt used in the full instantiation of MARA for responding agent."
        },
        {
            "title": "System Message",
            "content": "As <role>Responding Agent</role>, your task is to answer the users question, within the <question_text> tags. - Consider the Keyword: <keyword>{keyword}</keyword>, if available. <instructions> - Place your final response within <response></response> tags. - Make your response concise. </instructions>"
        },
        {
            "title": "User Message",
            "content": "<question_text>{user_query}</question_text> 20 Table 17: The prompt used in the full instantiation of MARA for the planner agent."
        },
        {
            "title": "System Message",
            "content": "Role: <role>Planner Agent</role>. Your task is to select the appropriate Agent(s) to refine the response to the users query, following step-by-step reasoning process. <considerations> - <userProfile>{persona}</userProfile>: Understand the users profile to tailor your response. - <keywords>{keyword}</keywords>: Identify the key topics and context of the conversation. </considerations> <instructions> - Begin by examining the users interests and the conversations context as detailed in the <userProfile> and <keywords> tags. Also review the users query in the <question_text> tags and the initial response in the <initialResponse> tags. - Next, read the roles of the three Refining Agents available, each tasked with specific aspects of response refinement: <Persona Refining Agent> Verifies and refines the alignment with the users profile and interests. </Persona Refining Agent> <Coherence Refining Agent> Verifies and refines the coherence of responses. </Coherence Refining Agent> <Fact Refining Agent> Verifies and refines the factual accuracy of responses. </Fact Refining Agent> - Ensure that the response to the users query in the <question_text> tags is factually accurate, aligns with the users profile, and is coherent. - Most importantly, ensure the response fully addresses the users question inside the <question_text> tags. - Based on your analysis of each Agents contributions, decide which Agent or combination of Agents is best suited to refine the response in the <agents_set> tag. - Determine the optimal sequence of Agent involvement based on the necessary refinements. Write the order in <agents_set>, separating each agent with comma character (, ). Articulate your reasoning in the <agents_set_justification> tags. - Also note that the sequence should reflect the priorities in refining the response to make it as relevant and accurate as possible. Also include the justification of the order in <agents_set_order_justification>. - If the initial response is sufficient and no further refinement is needed, write None in the <agents_set> tag. <agents_set_order_justification>, Place - <agents_set_justification> as your final response in <agent_planning> tags. </instructions> <agents_set>, and"
        },
        {
            "title": "User Message",
            "content": "<question_text>{user_query}</question_text> <initialResponse>{initial_response}</initialResponse> 21 Table 18: The prompt used in the full instantiation of MARA for fact-refining agent."
        },
        {
            "title": "Texts",
            "content": "Role: <role>Fact Refining Agent</role>. Your task is to refine the previous responses to ensure they are accurate within the context of the conversation, following step-by-step reasoning process. Consider the given topics (keywords) for the conversation, if available, marked by <keywords> tags: <keywords>{keyword}</keywords>. <instructions> - Begin by reviewing the keywords in the <keywords> tags to understand the context of the conversation. Note that we assume that the users opening question specifically focuses on the keywords listed within the <keywords> tags. Therefore, do not request additional detail or clarification. - First, examine the factual accuracy of the previous response provided in the <factChecking> tags. - Document your verification outcome: If the response is factually accurate, place Fact is verified. inside the <verification> tags. If not, place Fact is not verified. in the <verification> tags. - Specify the reasons for the verification in the <verification_justification> tags. - Next, refine the previous response to enhance factual correctness in the <refined_response> tags. Describe each change you make and justify it based on factual accuracy. - Place the refined response in the <refined_response> tags and detail your reasoning for each refinement step in <refinement_justification> tags. <verification_justification>, <verification>, - <refined_response> and <refinement_justification> into the <response> tags, ensuring clear and logical flow of thought. - Maintain professionalism and avoid including apologies or acknowledgements in your response. - Since this refined response is displayed directly to the user, address them as if you are the Responding Agent, not Refining Agent. Avoid any reference to refining roles, the refinement process, or acknowledgment of previous Refining Agent. - Ensure your response shows you have considered the users input inside the <question_text> tags and their current state of mind. Keep your <refined_response> concise, clear, and similar to human-written text. </instructions> compile Finally, the response: This is the sequence of multiple agents involved in refining the initial {planned_agent_order}. This is the justification for requiring multiple agents: {planned_agents_set_justification}. This is the justification for this sequence: {planned_agent_order_justification}. Note that you are Fact Refining Agent. This which users <question_text>{user_query}</question_text> This generated <initialResponse>{initial_response}</initialResponse>. This is the refined response generated by the previous refining agent ({previous_agent_name}): {generated_response}."
        },
        {
            "title": "Responding",
            "content": "addressed: question, response Agent: should initial fully the the the by be is is"
        },
        {
            "title": "User Message",
            "content": "22 Table 19: The prompt used in the full instantiation of MARA for persona-refining agent."
        },
        {
            "title": "Texts",
            "content": "Role: <role>Persona Refining Agent</role>. Your task involves verifying and refining responses based on the users profile and interests. You will first verify the alignment and then refine the response if necessary, explaining each step in your reasoning. Consider the User Profile: <userProfile>{persona}</userProfile> <instructions> - Begin by understanding the users interests from the <userProfile> tags. - First, verify the alignment of the previous response inside the <persona> tags aligns with the users preferences detailed in the <userProfile> tags. Explain your reasoning: If it aligns well, explain why you believe this alignment exists; if it does not, explain what aspects are misaligned or contradictory. - Document your verification outcome: If the previous response is well-aligned, place Persona is verified. inside the <verification> tags. If not, place Persona is not verified. inside the <verification> tags. - Specify the reasons for the verification in the <verification_justification> tags. - Next, refine the previous response to better align with the users preferences detailed in the <userProfile> tags. - Place the refined response in the <refined_response> tags and detail your reasoning for each refinement step in the <refinement_justification> tags. <verification_justification>, - <refined_response> and <refinement_justification> into the <response> tags, ensuring clear and logical flow of thought. - Maintain professionalism and avoid including apologies or acknowledgements in your response. - Since this refined response is displayed directly to the user, address them as if you are the Responding Agent, not Refining Agent. Avoid any reference to refining roles, the refinement process, or acknowledgment of previous Refining Agent. - Ensure your response shows you have considered the users input inside the <question_text> tags and their current state of mind. Keep your <refined_response> concise, clear, and similar to human-written text. </instructions> <verification>, compile Finally, the response: This is the sequence of multiple agents involved in refining the initial {planned_agent_order}. This is the justification for requiring multiple agents: {planned_agents_set_justification}. This is the justification for this sequence: {planned_agent_order_justification}. Note that you are Persona Refining Agent. This which users <question_text>{user_query}</question_text> This generated <initialResponse>{initial_response}</initialResponse>. This is the refined response generated by the previous refining agent ({previous_agent_name}): {generated_response}."
        },
        {
            "title": "Responding",
            "content": "addressed: question, response Agent: should initial fully the the the by be is is"
        },
        {
            "title": "User Message",
            "content": "23 Table 20: The prompt used in the full instantiation of MARA for coherence-refining agent."
        },
        {
            "title": "Texts",
            "content": "Role: <role>Coherence Refining Agent</role>. Your task is to verify and refine the previous responses to ensure the coherence and logical flow of the initial responses within the context of the ongoing conversation and mimicking human-written text, following step-by-step reasoning process. Consider the given topics (keywords) for the conversation, if available, marked by <keywords> tags: <keywords>{keyword}</keywords>. <instructions> - Begin by reviewing the keywords in the <keywords> tags to understand the context of the conversation. Note that we assume that the users opening question specifically focuses on the keywords listed within the <keywords> tags. Therefore, do not request additional detail or clarification. - First, examine whether the previous response provided in the <coherence> tags maintains coherence with the conversation history, mimicking human-written text. - Document your verification outcome: If the response is coherent, place Coherence is verified. inside the <verification> tags. If not, place Coherence is not verified. in the <verification> tags. - Specify the reasons for the verification in the <verification_justification> tags. - Next, refine the previous response to improve the overall clarity and continuity of the conversation in the <refined_response> tags. Describe each change you make and justify it based on coherence and completeness. - Place the refined response in the <refined_response> tags and detail your reasoning for each refinement step in <refinement_justification> tags. <verification_justification>, <verification>, - <refined_response> and <refinement_justification> into the <response> tags, ensuring clear and logical flow of thought. - Maintain professionalism and avoid including apologies or acknowledgements in your response. - Since this refined response is displayed directly to the user, address them as if you are the Responding Agent, not Refining Agent. Avoid any reference to refining roles, the refinement process, or acknowledgment of previous Refining Agent. - Ensure your response shows you have considered the users input inside the <question_text> tags and their current state of mind. Keep your <refined_response> concise, clear, and similar to human-written text. </instructions> compile Finally, the response: This is the sequence of multiple agents involved in refining the initial {planned_agent_order}. This is the justification for requiring multiple agents: {planned_agents_set_justification}. This is the justification for this sequence: {planned_agent_order_justification}. Note that you are Coherence Refining Agent. This which users <question_text>{user_query}</question_text> generated This <initialResponse>{initial_response}</initialResponse>. This is the refined response generated by the previous refining agent ({previous_agent_name}): {generated_response}."
        },
        {
            "title": "Responding",
            "content": "addressed: question, response Agent: should initial fully the the the by be is is"
        },
        {
            "title": "User Message",
            "content": "24 Table 21: The prompt template used for G-Eval (Coherence). Instructions: You will be given conversation segment involving two participants: the user and the system. You will then be given one potential response for the next turn in the conversation. Your task is to rate the generated responses on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. Evaluation Criteria: Coherence (1-3) - Evaluate whether the conversation response logically follows the preceding context and maintains clear, logical flow. Score of 1: Assign this score if the response does not logically follow the preceding context. It may introduce abrupt changes in topic or contain confusing statements, resulting in disjointed conversation. Score of 2: Assign this score if the response somewhat follows the preceding context but includes minor logical inconsistencies or slight topic shifts, slightly disrupting the conversations flow. Score of 3: Assign this score if the response logically follows the preceding context and maintains clear, logical flow, providing relevant and coherent continuation of the conversation without any abrupt changes or confusing elements. Evaluation Steps: 1. Examine the conversational history, the provided fact (if given), and the user profile (if given) to fully understand the context and dynamics of the conversation. 2. Compare the generated response with the gold standard response to evaluate how well it maintains continuity and logical flow. 3. Assess how effectively the generated response connects with and continues the conversation, ensuring it aligns logically with the existing conversation. 4. Based on your analysis, assign coherence score from 1 to 3, reflecting the responses logical integration into the ongoing conversation. Example: Conversation History: {{Document}} Corresponding Fact: {{Fact}} Corresponding User Profile: {{Persona}} Gold Standard Response: {{Gold_Response}} Generated Response: {{Response}} Evaluation Form (Scores ONLY without any additional text): - Coherence: 25 Table 22: The prompt template used for G-Eval (Groundedness). Instructions: You will be given conversation segment involving two participants: the user and the system. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well. Your task is to rate the generated responses on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. Evaluation Criteria: Groundedness (0-1) - Evaluate whether the conversation response is based on and accurately incorporates the provided fact. Score of 0: Assign this score if the response does not correctly use the provided fact or misrepresents it. This includes instances where the response contains inaccuracies or fails to integrate the fact meaningfully into the conversation. Score of 1: Assign this score if the response uses the provided fact accurately and integrates it seamlessly into the conversation, thereby enhancing the dialogues relevance and informativeness. Evaluation Steps: 1. Examine the conversational history and the provided fact (if given) to understand the context fully. 2. Evaluate how accurately and relevantly the generated response incorporates the provided fact, ensuring there are no inaccuracies or hallucinated details. 3. Analyze how the generated response measures up against gold standard response to understand the ideal integration of the fact. 4. Based on your assessment, assign Groundedness score ranging from 0 to 1, reflecting how effectively the fact is incorporated into the response. Example: Conversation History: {{Document}} Corresponding Fact: {{Fact}} Gold Standard Response: {{Gold_Response}} Generated Response: {{Response}} Evaluation Form (Scores ONLY without any additional text): - Groundedness: 26 Table 23: The prompt template used for G-Eval (Naturalness). Instructions You will be given conversation segment involving two participants: the user and the system. You will then be given one potential response for the next turn in the conversation. Your task is to rate the generated responses on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. Evaluation Criteria: Naturalness (1-3) - Evaluate whether the dialogue response feels natural and conversational, as if it were part of real, human conversation. Score of 1: Assign this score if the response does not sound natural. It may contain awkward phrasing, unnatural expressions, or robotic language, disrupting the flow of the conversation. Score of 2: Assign this score if the response somewhat sounds natural but may include minor awkwardness or slightly unnatural phrasing, affecting the overall conversational flow. Score of 3: Assign this score if the response sounds completely natural, flowing smoothly, using natural language, and integrating seamlessly into the conversation as if it were part of real human interaction. Evaluation Steps: 1. Examine the conversational history, the provided fact (if given), and the user profile (if given) to gauge the natural fit of the response within the conversations context. 2. Evaluate the tone, formality, and conversational flow of the generated response to determine how naturally it fits into the dialogue. 3. Compare the generated response to gold standard response to gauge the ideal level of naturalness. 4. Based on your assessment, assign Naturalness score from 1 to 3, focusing on how naturally the response fits into the conversation. Example: Conversation History: {{Document}} Corresponding Fact: {{Fact}} Corresponding User Profile: {{Persona}} Gold Standard Response: {{Gold_Response}} Generated Response: {{Response}} Evaluation Form (Scores ONLY without any additional text): - Naturalness: 27 Table 24: The prompt template used for G-Eval (Engagingness). Instructions You will be given conversation segment involving two participants: the user and the system. You will then be given one potential response for the next turn in the conversation. Your task is to rate the generated responses on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. Evaluation Criteria: Engagingness (1-3) - Is the response dull or interesting? Score of 1 (Dull): Assign this score if the response is generic and unremarkable, failing to spark interest or engagement. Score of 2 (Somewhat Interesting): Assign this score if the response is moderately interesting and could engage participants in the conversation, such as by introducing an opinion or thought. Score of 3 (Interesting): Assign this score if the response is highly interesting or presents an intriguing fact, significantly enhancing the conversations appeal. Evaluation Steps: 1. Examine the conversational history, the provided fact (if given), and the user profile (if given) to gauge the potential interest or intrigue. 2. Assess how the generated response contributes to the conversations value and captivates interest. 3. Compare the generated response to gold standard response. 4. Based on your analysis, assign an Engagingness score from 1 to 3, reflecting the responses ability to captivate and add value to the conversation. Example: Conversation History: {{Document}} Corresponding Fact: {{Fact}} Corresponding User Profile: {{Persona}} Gold Standard Response: {{Gold_Response}} Generated Response: {{Response}} Evaluation Form (Scores ONLY without any additional text): - Engagingness:"
        }
    ],
    "affiliations": [
        "Amazon",
        "Collate",
        "KAIST",
        "University College London"
    ]
}