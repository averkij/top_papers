{
    "paper_title": "Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning",
    "authors": [
        "Yijun Yang",
        "Zhao-Yang Wang",
        "Qiuping Liu",
        "Shuwen Sun",
        "Kang Wang",
        "Rama Chellappa",
        "Zongwei Zhou",
        "Alan Yuille",
        "Lei Zhu",
        "Yu-Dong Zhang",
        "Jieneng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Providing effective treatment and making informed clinical decisions are essential goals of modern medicine and clinical care. We are interested in simulating disease dynamics for clinical decision-making, leveraging recent advances in large generative models. To this end, we introduce the Medical World Model (MeWM), the first world model in medicine that visually predicts future disease states based on clinical decisions. MeWM comprises (i) vision-language models to serve as policy models, and (ii) tumor generative models as dynamics models. The policy model generates action plans, such as clinical treatments, while the dynamics model simulates tumor progression or regression under given treatment conditions. Building on this, we propose the inverse dynamics model that applies survival analysis to the simulated post-treatment tumor, enabling the evaluation of treatment efficacy and the selection of the optimal clinical action plan. As a result, the proposed MeWM simulates disease dynamics by synthesizing post-treatment tumors, with state-of-the-art specificity in Turing tests evaluated by radiologists. Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs in optimizing individualized treatment protocols across all metrics. Notably, MeWM improves clinical decision-making for interventional physicians, boosting F1-score in selecting the optimal TACE protocol by 13%, paving the way for future integration of medical world models as the second readers."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 7 2 3 2 0 . 6 0 5 2 : r Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning Yijun Yang1,*, Zhao-Yang Wang2, Qiuping Liu3, Shuwen Sun3, Kang Wang4, Rama Chellappa2, Zongwei Zhou2, Alan Yuille2, Lei Zhu1,5,, Yu-Dong Zhang3, Jieneng Chen2, 1The Hong Kong University of Science and Technology (Guangzhou) 2Johns Hopkins University 3The First Affiliated Hospital of Nanjing Medical University 4University of California, San Francisco 5The Hong Kong University of Science and Technology Project page: https://yijun-yang.github.io/MeWM"
        },
        {
            "title": "Abstract",
            "content": "Providing effective treatment and making informed clinical decisions are essential goals of modern medicine and clinical care. We are interested in simulating disease dynamics for clinical decision-making, leveraging recent advances in large generative models. To this end, we introduce the Medical World Model (MeWM), the first world model in medicine that visually predicts future disease states based on clinical decisions. MeWM comprises (i) vision-language models to serve as policy models, and (ii) tumor generative models as dynamics models. The policy model generates action plans, such as clinical treatments, while the dynamics model simulates tumor progression or regression under given treatment conditions. Building on this, we propose the inverse dynamics model that applies survival analysis to the simulated post-treatment tumor, enabling the evaluation of treatment efficacy and the selection of the optimal clinical action plan. As result, the proposed MeWM simulates disease dynamics by synthesizing post-treatment tumors, with state-of-the-art specificity in Turing tests evaluated by radiologists. Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs in optimizing individualized treatment protocols across all metrics. Notably, MeWM improves clinical decision-making for interventional physicians, boosting F1-score in selecting the optimal TACE protocol by 13%, paving the way for future integration of medical world models as the second readers. 1. Introduction Clinical decision-making is at the heart of patient care, driving outcomes and shaping the trajectory of healthcare interventions. Physicians constantly weigh the multimodal fac- *Work done while visiting at JHU. Corresponding authors. Figure 1. Formulation of Medical World Model. It integrates imaging observations with perception modules to form an initial state, which is then processed by progression generative model to predict future states of disease under different treatment conditions. Recovery-conditioned policies guide treatment decisions, creating feedback loop for optimizing clinical interventions. tors including medical images and patient history to determine the best course of action for each patient. Artificial intelligence (AI) models are increasingly assuming crucial role in this process by analyzing the complex multimodal data, revealing patterns that may be difficult to detect with human observation alone, and suggesting tailored treatment strategies based on predictive analytics. Foundation models [9] such as large language models (LLMs) [37, 64] present new frontier in medical AI research and development. However, recent studies [29] show that LLMs, even those specifically tailored for medicine [16, 61, 62], diagnose significantly worse than clinicians and make less informed treatment decisions. This is due to several challenges. First, the complexity of diseases themselves, such as tumors that evolve under the influence of diverse biological and chemotherapy factors, calls for models that can adapt and account for disease progression. Second, clinical decision-making necessitates not only accurate predictions but also visually trackable insights that physicians can trust. Recent breakthroughs in world models (WMs) [4, 10, 43, 51, 69] provide promising avenue for overcoming these obstacles. By generating predictive distribution of how the world states evolve, WMs mirror the way human planners imagine future scenarios and then make informed decisions via inverse dyanmics [4, 21]. Although they remain largely underexplored in the medical domain, world models hold significant potential for generating clinically realistic images and simulating disease progression, which in turn can facilitate more effective and visually trackable treatment planning. Figure 1 illustrates our formulation of introducing WMs into generalized medical scenarios and how WMs integrate these capabilities to support clinical decision-making. In this work, we introduce Medical World Model (MeWM) to address these challenges and push the boundaries of AI-driven clinical decision support. MeWM comprises three primary components: (1) Policy Model powered by vision-language architectures, which generates the potential action combos from patients current state and specific clinical scenario; (2) Dynamics Model that forwards and simulates tumor dynamics, predicting how tumors could progress or regress under different treatment conditions by generative modeling; (3) an Inverse Dynamics Model that performs survival risk analysis on the simulated post-treatment tumor, and quantitatively evaluates treatment efficacy. Beyond forward simulation, this system heuristically explores the optimal plan with the assistance of segmentation model. By uniting these elements, MeWM delivers holistic framework for decision-making: it can synthesize realistic post-treatment tumors that pass Turing tests against radiologists, and it outperforms specialized GPT-like models on Transarterial Chemoembolization (TACE) Protocol Exploration (over 10% in F1-score). Overall, our contributions are threefold. We propose the medical world models, where we develop multimodal policy model that leverages visionlanguage capabilities to propose tailored set of treatment action combos, and we design generative dynamics model that accurately captures potential evolution of tumors, enabling forward-looking simulations for different interventions. We integrate an inverse dynamics model that translates these action-conditioned simulation into survival analysis metrics, thereby offering transparent and evidencebased tool for choosing the optimal treatment protocol. We demonstrate substantial leap in AI-driven decision support for interventional medicine, improving the F1score in selecting the optimal treatment protocol by 13% and offering compelling glimpse into the future of precision healthcare. 2. Related Work 2.1. Generative World Modeling World models [26, 43] aim to simulate dynamic environments by predicting future states and rewards based on current observations and actions. Originally developed for constrained settings like Atari games [27], their ability to model state transitions has been extended to real-world scenarios through joint learning of policies and world models, improving sample efficiency in simulated robotics [59], real-world robots [70] and autonomous driving [33, 68]. While early world models focused on simple state transitions, modern approaches integrate structured action-object relationships [66] and multi-modal conditioning [7, 24]. For instance, Du et al. [20] present long-horizon video plans by synergising vision-language models and text-to-video models. Luo et al. [52] propose to ground video models to continuous action by leveraging video-guided goal-conditioned exploration to learn goal-conditioned policy. In embodied decision-making, Lu et al. [51] enables agents to imaginatively explore the world with high generation quality and exploration consistency using video generative models. However, there is still no work investigating the applicability of world modeling in medical image analysis and clinical decision-making. 2.2. Tumor Synthesis Tumor synthesis has emerged as an attractive research topic across various medical imaging modalities, such as CT [14, 53, 74], MRI [6, 35, 71], and endoscopic videos [15, 45]. There are also many works on synthesizing non-cancerous lesions including chest CT synthesis [8, 53, 74, 75], and diabetic lesion synthesis in retinal images [18, 78]. Recent studies focus on improving the realism of synthetic tumors in the liver, kidney and pancreas [14, 34, 42] by leveraging the large generative models like diffusion models [32, 58, 63]. While these methods are conditioned only on shape masks, Li et al. [47] propose text-driven tumor synthesis by descriptive reports and conditional diffusion models. However, most of these works implemented tumor synthesis as data augmentation to improve tumor detection tasks. They overlook its potential to empower clinical In this work, we decision-making in treatment planning. delve into the relatively unexplored field of tumor dynamics simulation by generating post-treatment tumors using pretreatment scans and treatment actions. 2.3. Prognosis and Clinical Decision-making Post-treatment prognosis in medical imaging is essential for evaluating therapy effectiveness, predicting disease recurrence, and guiding further clinical decisions. CT is widely used to assess structural and functional changes in tumors following interventions such as surgery, chemotherapy, raFigure 2. Overview of TACE Protocol Exploration by Medical World Model. (1) GPTs (Policy Model): construct the TACE action combos by the observation of pre-treatment CT, integrating clinical guidelines and policies. (2) Tumor Generative Model (Dynamics Model): simulates post-treatment tumor based on different TACE intervention protocols, predicting treatment outcomes. (3) Survival Analysis Model (Heuristic Function): assesses risk scores from both simulated post-treatment CT and pre-treatment CT to determine the most effective TACE protocol. Note that the 3D tumor masks (colored in red) can be extracted using well-trained segmentation network (as Assistant Model). The framework enables visually trackable protocol optimization by iterating between clinical policy guidance, generative modeling, and survival analysis. diation therapy, transarterial chemoembolization (TACE), and immunotherapy [28, 36, 44, 48, 72, 73]. Lee et al. [44] employ CNN-based model to predict the post-treatment survival of patients with hepatocellular carcinoma (HCC) using CT images and clinical information. In addition, LLMs are increasingly being explored to assist in clinical decision-making [11, 29, 40, 46]. However, little attention has been given to applying LLMs for post-treatment prognosis. They did not leverage the feedback from survival analysis to achieve prompt intervention as well. 3. Medical World Models Overall Framework. As shown in Fig. 2, our MeWM takes visual observation of pre-treatment CT x0, language treatment goal to simulate the future state and explore the best treatment protocol. Policy model ( 3.1) acquires the descriptive observation based on the visual state, and constructs set of treatment protocols by the language goal and clinical guidelines. To perform the exploration, given the pre-treatment CT and an action combo, the dynamics model ( 3.2) predicts the concrete resulting state, i.e., generating post-treatment CT. Finally, inverse dynamics model ( 3.3) driven by Heuristic Function predicts the risk score from pre-treatment CT and simulated post-treatment CT with tumor masks from Assistant Model, to effectively prune branches in search and heuristically determine the optimal solution. 3.1. Policy Model Vision-language models [13, 77] have emerged as powerful source of prior knowledge about the clinical world, providing rich information about how to complete promising treatment from large-scale internet data and clinical guidelines. Based on TACE clinical guidelines, we set up the exploration configurations, including all potential chemotherapy drugs (e.g., Raltitrexed, Cisplatin) and embolism materials (e.g., Lipiodol, Gelatin Sponge). The two parts constitute the action base, which provides possible TACE protocols for Generative Dynamics Models as conditions. Then, we adopt pre-trained large multimodal model (LMM), e.g., GPT-4o, to serve as policies. Given highlevel goal (e.g., What TACE treatment protocols are recommended for patient diagnosed with hepatocellular carcinoma (HCC) given the pre-treatment CT?), the policy model πVLM(x0, g) extracts the visual observation and tumor context from the given pre-treatment CT x0 to prompt the proper Transarterial Chemoembolization (TACE) actions. To constrain the excessively large tree search in the action base, we further prompt the Large Language Reasoning Model, i.e., Deepseek-R1 [25], to refine the drug set and embolism set by the clinical policies, whose final cardinalities are and E, respectively. 3.2. Dynamics Model Radiotherapy Report Extraction and Generation. While most existing studies focus on human-authored radiology reports, we aim to address radiotherapy reports to extract Figure 3. Dynamics Model based on Tumor Generative Model. The training framework consists of three parts: (a) Radiotherapy Report Extraction and Generation: GPT-4o and Deepseek-R1 extract key treatment details from radiotherapy reports and generate corresponding TACE surgical actions. (b) Post-Treatment Tumor Generation: An Action-driven 3D Diffusion Model is conditioned by fused action embeddings and attenuated CT features to generate post-treatment tumors that simulate treatment outcomes. (c) Combo Contrastive Learning (CCL): The model learns from treatment variations by pushing apart dissimilar combos and pulling together similar ones, improving its ability to generate realistic and action-aware post-treatment tumor appearances. more comprehensive information on treatment protocols. However, raw radiotherapy reports pose significant challenges due to noise and fragmented information, which hinder controlled tumor synthesis. To mitigate these issues, we propose two-stage text preprocessing framework consisting of data cleaning and augmentation. In the first stage, we perform keyword extraction by aggregating the outputs of both GPT-4o and DeepSeek-R1, focusing on key entities such as drugs, embolic agents, and their corresponding dosages. In the second stage, we leverage the same tools for text generation, constructing structured core action description based on the extracted keywords. This approach enhances the consistency and informativeness of processed reports, facilitating downstream tasks in tumor synthesis and treatment analysis. Post-Treatment Tumor Generation. We adopt Latent Diffusion Models (LDMs) [58] for latent feature extraction from 3D Pre-treatment CT volumes and integrate textual action embedding for controlled tumor synthesis. Each 3D Post-treatment CT volume x1 RHW is encoded into lower dimensional latent representation z1 = E(x1) using 3D VQGAN autoencoder [22]. In the latent space, following the spirit of DiffTumor [14], we define diffusion process that progressively adds noise to the latent representation z1 over discrete time steps = 1, ..., . Given 0 x0) where pair of tumor-present pre-treatment CT volume x0 and the mask of its tumor region m0, we condition the denoising model on the masked pre-treatment latent representation 0 = E(m 0 is the attenuated mask from m0 by our proposed Text-driven Morpho-Gaussian Attenuation. Specifically, to mimic the effects of TACE treatment, the process begins with occlusion assessment on radiotherapy reports. The textual descriptions (e.g., occluded, reduced, disappear) are extracted and analyzed to determine the attenuation level {1, 2, 3, 4}, where higher value corresponds to better tumor curative effects. Then, morphological erosion and dilation with the adaptive kernel by are applied to m0, simulating occlusion-induced tumor structural dynamics. Simultaneously, adaptive Gaussian blurring is employed to exhibit the characteristics of heterogeneous intensity changes due to lipiodol deposition, necrotic transformation, and reduced perfusion. The final attenuated mask 0 is computed by the three steps, ensuring smooth transition between tumor and organ tissues. Note that this attenuation is only used during training. Also, we condition the denoising model on the generated textual action. Given the action combo = {a1, ..., aH }, each sub-action, respectively, undergoes encoding through CLIP [57] text encoder ϕ() followed by linear projection σ1(), enabling dimension reduction to latent clinical concept space. To enhance the semantics of action conditions in different drug and embolism keywords, we introduce learnable concept embeddings c, which extract keyword representations from the given action combo. This explicit pharmaceutical grounding enables precise modeling of therapeutic components while maintaining robustness to context variations. The final action condition τ (a) is the fusion of holistic text embeddings and concept embeddings by fully connected layers σ2, : τ (a) = σ2([[σ1(ϕ(a1)), ..., σ1(ϕ(aH ))], c]), (1) where [] denote concatenation operation. The training objective of diffusion model is as follows: Ez1,ϵN (0,1),t (cid:20)(cid:13) (cid:13) (cid:13)ϵ ϵθ (cid:16) zt, 0, m0, τ (a), (cid:17)(cid:13) 2 (cid:13) (cid:13) 2 (cid:21) , (2) where ϵθ(, t) is 3D U-Net with interleaved self-attention layers and convolutional layers [14, 32, 56] that predict the noise given the input variable and conditions. Combo Contrastive Learning. We adopt contrastive learning strategy that aligns action combos with tumor evolution to enhance the realism and discrimination of posttreatment tumor synthesis. Given pre-treatment anchor pair (x0, m0), along with an action combo a, the goal is to generate post-treatment CT ˆx by generative model fDM(). = positive For fDM(x+ 0 , a+), are defined as synthetic post-treatment CT from another pair but its action combo contains the same drug/embolism keywords. Negative samples, ˆx = fDM(x0, m0, a), in contrast, are generated using the same pair but action combos with diverse keywords, leading to distinct tumor evolution patterns. This contrastive loss is incorporated into the tumor generative model: each 0 , m+ samples, anchor, ˆx+ (cid:20) log exp (sim(ˆx, ˆx+)/δ) (cid:80) exp (sim(ˆx, ˆx)/δ) exp (sim(ˆx, ˆx+)/δ) (cid:21) , (3) where sim() denotes cosine similarity, δ is the temperature scaling factor. This contrastive learning strategy ensures that tumors simulated from similar treatment protocols exhibit consistent attenuation effects, while those derived from distinct protocols remain differentiable. 3.3. Inverse Dynamics Model Inverse Dynamics Model, which empowers our full framework, aims to infer the most effective treatment strategy by analyzing relationships between pre-treatment conditions, intervention actions, and expected post-treatment outcomes. We unfold its essence in three aspects: (1) Assitant Model; (2) Heuristic Function and (3) TACE Protocol Exploration. Assistant Model. To better discriminate the tumor in synthesized post-treatment CT ˆx, we introduce tumor segmentation model as Assistant Model Hseg(). Posttreatment tumors are characterized by heterogeneous highintensity regions due to calcification/lipiodol deposition, irregular shapes reflecting necrotic tissue changes, and reduced or absent contrast enhancement in viable tumor areas, in contrast to traditional pre-treatment CT tumors. Thus, we adapt pre-trained nnUNet-based [38] model to this posttreatment context by finetuning it on our ground truth pairs of post-treatment CT and mask. Given the well-trained Assistant Model, the simulated post-treatment CT ˆx from Tumor Generative Model is processed for the segmentation of liver and tumor. The post-treatment CT with the predicted mask ˆm is subsequently utilized for survival analysis. Heuristic Function. We use survival analysis model to implement heuristic function Hsurv(x0, m0, ˆx, m0, g), which quantifies the efficacy under the specific TACE action combo by the output risk score. Inspired by DeepSurv [39], we utilize 3D convolution-based model structure, the 3D ResNet (MC3) [65], as the feature extractor of survival analysis model. Given the pre-treatment pair (x0, m0) and simulated post-treatment pair (ˆx, ˆm), we extract their concatenated CT and mask, respectively, and bidirectionally align the semantics of preand post-treatment by Cross-Attention Transformer [41]. After that, we adopt an attention-based aggregator to fuse preand post-features, followed by fc layers to determine the risk score. The action combo with lower risk score should bring greater efficacy for the patient. Note that, for training, we leverage multi-task learning strategy, i.e., CoxPH [39] and OS regression, to improve the generalization of survival analysis. TACE Protocol Exploration. Given combination of the proposed models above, we are able to predict TACE protocol from any Pre-treatment CT by language treatment goal g. To reason the optimal action combo, we propose to search for list of actions to reach g, corresponding to finding treatment plan consisting of both drug and embolism components, which optimizes: ˆx 1:H = arg min ˆx1:H πVLM,fDM Hsurv(x0, m0, ˆx, Hseg(ˆx), g), (4) where = Hd + He. With this objective in mind, we exhibit tree-search exploration procedure. Our exploration algorithm initializes set of parallel protocol beams. We sample the potential action space composed of drugs and embolisms and clinical rules using πVLM. The clinical rules are introduced to prune unreasonable branches, e.g., concomitant use of multiple platinum-based agents is contraindicated due to the risk of cumulative toxicity and myelosuppression. We sequentially explore the two parts to ensure that TACE protocol contains both drugs and embolism. For each current action combo, we synthesize post-treatment tumors from fDM(x0, m0, a) to obtain more reliable simulation. We Algorithm 1 TACE Protocol Exploration with MeWM 1: Input: Pre-treatment CT x0, Pre-treatment tumor mask m0, Language treatment goal 2: Functions: VLM Policy Model πVLM, Dynamics Model fDM, Heuristic Function Hsurv, Assistant Model Hseg 3: Hyperparameters: Drug Actions factor D, Embolism Actions factor E, Tumor Generative factor , Protocol Beams B, Drug horizon Hd, Embolism horizon He # Initialize Different TACE Protocol Beams # Generate Different Drug, Embolism Actions, Clinical Rules tumors [fDM(x, drugi) for in (1 . . . ) for in (1 . . . D) if rule] # Generate tumors from and plans[b] under rule plans[b].append(argmin(tumors, Hsurv, Hseg)) # Add Tumor with Lowest Risk to Plan end for max idx, min idx argmax(plans, Hsurv, Hseg), argmin(plans, Hsurv, Hseg) plans[max idx] plans[min idx] # Periodically Replace the Plan with High Risk 4: plans [[x0] {1 . . . B}] 5: drug1:D, embo1:E, rule πVLM(x0, g) 6: for = 1 . . . Hd do for = 1 . . . do 7: 8: 9: 10: 11: 12: 13: end for 14: for = 1 . . . He do for = 1 . . . do 15: 16: 17: 18: 19: 20: 21: end for 22: plan argmin(plans, Hsurv, Hseg) tumors [fDM(x, emboi) for in (1 . . . ) for in (1 . . . E) if rule] # Generate tumors from and plans[b] under rule plans[b].append(argmin(tumors, Hsurv, Hseg)) # Add Tumor with Lowest Risk to Plan end for max idx, min idx argmax(plans, Hsurv, Hseg), argmin(plans, Hsurv, Hseg) plans[max idx] plans[min idx] # Periodically Replace the Plan with High Risk then use our heuristic function Hsurv(x0, m0, ˆx, ˆm, g) with the assistance of Hseg(ˆx) to select the generated tumor with the best average survival score of replicas among or actions. After every step of extending all beams, we discard the beam with the worst survival score and replace its action combo with the best beam. To prevent cumulative toxicity and organ dysfunction, we prohibit over-exploration by drug horizon Hd and embolism horizon He. Our final action combo is taken from the beam with the best survival score and adopted as TACE protocol for the patient. The pseudocode of our method is also provided in Algorithm 1. 4. Experiments HCC-TACE In-house Dataset. We collect large repository of 338 longitudinal paired preand post-treatment CT scans with well-annotated liver/tumor masks and clinical records, such as TACE radiotherapy reports as gold action and Overall Survival (OS) time. We split the training set (validation set included) and testing sets in the 9:1 ratio. HCC-TACE-Seg Public Dataset [55]. For external validation, we use patients from HCC-TACE-Seg public dataset referring to single-institution collection with confirmed HCC treated at The University of Texas MD Anderson Cancer Center. We conduct data curation and preprocessing to collect 78 cases containing pre-treatment CT, posttreatment CT, TACE Gold Action, and OS time. We use 80% cases to fine-tune and validate MeWM and leave 20% cases for the exploration evaluation. # Return Plan with Lowest Risk 4.1. Evaluation on Generation Quality Visual Turing Test (Human Evaluation). We conduct an action-driven Visual Turing Test on 240 CT scans of post-treatment tumors, where 120 scans contain real posttreatment tumors, and 120 scans contain synthetic posttreatment tumors generated by different tumor synthesis models. Three radiologists (R1-R3) participated in this study, independently evaluating five groups of 48 CT scans each and classifying them as either real or synthetic. It is important to note that the radiologists evaluations are based on whether the synthetic tumor closely resembles post-treatment tumor, which typically contains mixture of lipiodol deposition, necrotic, and viable tumor regions, distinguishing it from ordinary pre-treatment tumors. The test results are summarized in Table 1. The sensitivity scores of all radiologists remain high (above 91%), demonstrating their ability to correctly identify real post-treatment tumors. However, specificity scores vary among the methods, indicating different levels of realism in the synthetic tumors. Notably, our method MeWM achieves the lowest specificity scores (79.17% for R1, 70.83% for R2, and 75.00% for R3), suggesting that large proportion of synthetic tumors generated by our approach are mistaken as real. This indicates superior realism compared to other methods such as SynTumor [34], Pixel2Cancer [42], DiffTumor [14], and TextoMorph [47]. Figure 4 illustrates examples from the test, where real tumor is compared with synthetic tumors that Table 1. Action-driven Visual Turing Test involves three radiologists (R1-R3) each evaluating five groups of 48 CT scans each, with 24 real post-treatment tumors and 24 synthetic post-treatment tumors from tumor generative model, respectively. They were tasked with categorizing each CT scan as either real or synthetic. higher sensitivity score indicates better discriminative ability of radiologists, while lower specificity score indicates higher number of synthetic tumors being identified as real. We also provide perceptual evaluation using FID and LPIPS compared to corresponding real post-treatment scans. Lower FID and LPIPS indicate better simulation results. Methods SynTumor [34] Pixel2Cancer [42] DiffTumor [14] TextoMorph [47] MeWM (Ours) sensitivity 100.0 95.83 100.0 100.0 100. R1 specificity 95.83 100.0 91.67 91.67 79.17 accuracy 97.92 97.92 95.83 95.83 89.58 sensitivity 87.50 91.67 95.83 91.67 91.67 R2 specificity 95.83 95.83 87.50 83.33 70.83 accuracy 91.67 93.75 91.67 87.50 81.25 sensitivity 100.0 100.0 100.0 95.83 91. R3 specificity 95.83 100.0 87.50 87.50 75.00 accuracy 97.92 100.0 93.75 91.67 83.33 Perceptual metrics FID 3.33 3.34 1.40 1.03 0.71 LPIPS 0.6832 0.6831 0.7660 0.9111 0.6120 Figure 4. Examples of Visual Turing Test. We present one real tumor alongside examples of synthetic tumors that were correctly and incorrectly identified. red dot indicates the radiologist classified the post-treatment tumor as synthetic, while green dot signifies it was identified as real. radiologists correctly or incorrectly classified. This highlights synthetic tumors closely resemble real post-treatment tumors. Perceptual Evaluation. We perform perceptual evaluation using FID and LPIPS scores, where lower values indicate better simulation quality. Our method achieves the best FID (0.71) and LPIPS (0.6120), demonstrating the highest fidelity in synthetic tumor generation. These results confirm that MeWM effectively synthesizes realistic post-treatment tumors, making it more challenging for radiologists to distinguish between real and synthetic cases. 4.2. Survival Analysis In Figure 5, we evaluate survival risk regression between the popular Cox Proportional Hazards model [23] and our heuristic function model on the HCC-TACE-Seg dataset. The true risk distribution (left) is estimated using the Nelson-Aalen estimator [17]. The Cox model fails to accurately distinguish between highand low-risk samples from low-dimensional deep features, resulting in an overly smoothed risk distribution. In contrast, our model produces risk map that better aligns with the true distribution, effectively capturing variations in risk levels. Error analysis shows higher Mean Square Error (MSE), i.e., 0.3550 for Cox and lower MSE, 0.2142 for our model, indicating superior accuracy. Figure 6 further presents Kaplan-Meier survival curves comparing risk stratification performance beFigure 5. Performance of heuristic function on survival analysis. The first three heatmaps show the true risk distribution, Cox model predictions, and our heuristic function predictions. The last two depict prediction errors, with lower MSE (0.2142) for our model compared to the Cox model (0.3550), demonstrating improved accuracy in capturing localized risk patterns. Figure 6. Kaplan-Meier Survival Curves: Radiomics-based Cox Model [67] vs. our deep model. The left shows the survival curves predicted by the Cox model based on Radiomics features. The right presents the survival curves from our model based on deep features, which achieves significantly higher c-index of 0.752 and log-rank p-value of 6.74e 5, demonstrating stronger ability to distinguish between highand low-risk groups. Shaded areas represent confidence intervals. tween the Radiomics-based Cox model and our deep model. These results demonstrate that our heuristic function better estimates survival risks, reduces prediction errors, and captures complex patterns beyond the capabilities of the Cox model and radiomics features. Table 2. TACE Protocol Exploration Evaluation on HCC-TACE in-house dataset and public dataset. F1-score, Jaccard index, Precision, and Recall are computed between the predicted action combo and gold action. MeWM significantly advances multimodal GPTs in exploring optimal individualized treatment protocol across all metrics, even comparable to interventional physicians. Methods Physician w/ Pre-CT Physician w/ MeWM Qwen2.5-VL [3] GPT-4o [37] Claude-3.7-sonnet [2] CT2Rep [30] MedGPT [19, 54] HuatuoGPT-Vision [76] MeWM(Ours) F1-score 48.81 61.51 (+13%) 37.09 41.97 40.93 27.75 37.51 40.13 52.38 In-house dataset Jaccard 38.44 49.89 24.49 27.81 28.55 17.21 25.57 29.08 38.59 Precision Recall 54.67 65.44 41.83 52.78 37.78 25.83 45.21 42.28 46.17 46.67 60.89 34.44 35.93 45.83 30.83 32.78 40.11 63. Public dataset F1-score 71.43 80.00 (+9%) 47.14 44.29 44.76 43.61 47.14 52.62 64.08 Jaccard 63.10 73.81 34.40 32.74 33.81 28.57 40.48 42.26 48.45 Precision Recall 78.57 85.71 42.86 38.10 35.71 37.50 45.24 51.19 58.93 66.67 76.16 53.57 57.14 64.29 53.57 50.00 54.76 72.62 4.3. Results on TACE Protocol Exploration Evaluation Strategy. For treatment planning evaluation, we utilize four metrics in Table 2: (1) F1-score: harmonizes Precision and Recall, balancing redundancy and omissions; (2) Jaccard Index: measures prediction overlap with gold actions, emphasizing category-level alignment; (3) Precision: reflects recommendation purity, penalizing incorrect or redundant drugs/embolisms; (4) Recall: captures therapeutic coverage, highlighting critical omissions. Partial Observation Misleads GPTs. For Multimodal Large Language Models (e.g., GPT-4o, MedGPT, HuatuoGPT-Vision), they are prompted with pre-treatment CT slices and allowed to predict the action combo from the given action set. These inferior results (over -10% in F1score) to MeWM demonstrate that it tends to make deficient planning relying solely on vision-language models and their commonsense reasoning. This also validates the necessity of simulation from pre-treatment to post-treatment. MeWM as Clinical Decision-support Tool. MeWM demonstrates significant potential in augmenting the capabilities of radiologists and physicians, underscoring its clinical relevance in optimizing TACE planning. Reliance solely on pre-treatment CT often results in partial observation and suboptimal targeting due to heterogeneous pathological conditions. By incorporating MeWMs recommended protocol, clinical decision-making is markedly enhanced, yielding performance improvements of 12.70, 11.45, 14.22, and 10.77 in F1-score, Jaccard, Precision, and Recall on our dataset. MeWM facilitates accurate tumor localization and enables predictive assessment of postembolization outcomes, thereby reducing procedural uncertainty. Moreover, its synthetic post-treatment CT projections help anticipate embolization efficacy, optimize TACE distribution, and mitigate non-target embolization risks, contributing to enhanced therapeutic precision and individualized strategies. MeWM serves as critical decisionsupport tool in interventional oncology, bridging anatomiFigure 7. Example of MeWM intervention in clinical applications. The radiologist initially proposes TACE protocol with Raltitrexed, Lobaplatin, Idarubicin, and embolization using Lipiodol and Gelatin Sponge. MeWM simulates protocol with Raltitrexed, Lobaplatin, and Lipiodol. After intervention, the optimized protocol removes Idarubicin but restores Gelatin Sponge, aligning with gold action. Table 3. Ablation studies of TACE protocol exploration on both datasets. AM denotes Assistant Model, while CCL denotes Combo Contrastive Learning. Two components significantly contribute to better exploring the optimal treatment. Metrics F1-score Jaccard Precision Recall In-house dataset Public dataset w/o AM 49.13 (-3.9) 35.40 (-3.2) 56.39 (-6.7) 45.22 (-1.0) w/o CCL 50.97 (-1.4) 36.76 (-1.8) 60.57 (-2.5) 45.36 (-0.8) w/o AM 60.03 (-4.1) 45.36 (-3.1) 75.00 (+2.4) 52.38 (-5.6) w/o CCL 62.90 (-1.2) 46.97 (-1.5) 70.10 (-2.5) 56.72 (-2.2) cal imaging with functional assessment for meaningful clinical outcomes. As shown in Figure 7, interventional physicians refine TACE protocols by MeWM intervention, aligning treatment with expert practices. Ablation Study. As shown in Table 3, we ablate the effectiveness of Assitant Model and Combo Contrastive Learning (CCL) in TACE Protocol Exploration. The results demonstrate that both the Assistant Model and Combo Contrastive Learning (CCL) contribute significantly to its performance. Removing the assistant model, which provides the location information of tumors for heuristic function, leads to critical drop in F1-score (52.3849.13) on our dataset, as well as on public dataset. Similarly, omitting CCL reduces F1-score (e.g., 52.3850.97), indicating that CCL enhances the models discrimination on action units. Overall, MeWM achieves the best results across all metrics, even outperforming radiologists in some areas, validating its effectiveness. 5. Conclusion We present Medical World Model, which marks step toward AI-driven precision medicine by simulating disease evolution and optimizing clinical strategies. By bridging generative modeling with medical decision-making, it enables deeper understanding of treatment outcomes and refines intervention planning. The advancements of MeWM in tumor synthesis and survival analysis set the stage for future AI systems that seamlessly integrate with clinical workflows, driving the next generation of longitudinal datadriven healthcare."
        },
        {
            "title": "References",
            "content": "[1] deedsbcv. 2, 5 [2] AI Anthropic. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 2024. 8 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 8 [4] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In CVPR, 2025. 2 [5] Patrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene Vorontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin, Colin Jacobs, Gabriel Efrain Humpire Mamani, Gabriel Chartrand, et al. The liver tumor segmentation benchmark (lits). Medical image analysis, 84:102680, 2023. [6] Benjamin Billot, Douglas Greve, Oula Puonti, Axel Thielscher, Koen Van Leemput, Bruce Fischl, Adrian Dalca, Juan Eugenio Iglesias, et al. Synthseg: Segmentation of brain mri scans of any contrast and resolution without retraining. Medical image analysis, 86:102789, 2023. 2 [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [8] Christian Bluethgen, Pierre Chambon, Jean-Benoit Delbrouck, Rogier van der Sluijs, Małgorzata Połacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis Langlotz, and Akshay Chaudhari. visionlanguage foundation model for the generation of realistic chest x-ray images. Nature Biomedical Engineering, pages 113, 2024. 2 [9] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. 1 [10] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 2 [11] Felix Busch, Philipp Prucker, Alexander Komenda, Sebastian Ziegelmayer, Marcus Makowski, Keno Bressem, and Lisa Adams. Multilingual feasibility of gpt-4o for automated voice-to-text ct and mri report transcription. European Journal of Radiology, 182:111827, 2025. [12] Jorge Cardoso, Wenqi Li, Richard Brown, Nic Ma, Eric Kerfoot, Yiheng Wang, Benjamin Murrey, Andriy Myronenko, Can Zhao, Dong Yang, et al. Monai: An open-source framework for deep learning in healthcare. arXiv preprint arXiv:2211.02701, 2022. 5 [13] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024. 3 [14] Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, and Zongwei Zhou. Towards generalizable tumor synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1114711158, 2024. 2, 4, 5, 6, 7 [15] Tong Chen, Shuya Yang, Junyi Wang, Long Bai, Hongliang Ren, and Luping Zhou. Surgsora: Decoupled rgbd-flow diffusion model for controllable surgical video generation. arXiv preprint arXiv:2412.14018, 2024. 2 [16] Zeming Chen, Alejandro Hernandez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Kopf, Amirkeivan Scaling medical Mohtashami, et al. arXiv preprint pretraining for large language models. arXiv:2311.16079, 2023. 1 Meditron-70b: [17] Enrico Colosimo, Fla vio Ferreira, Maristela Oliveira, and Cleide Sousa. Empirical comparisons between kaplanmeier and nelson-aalen survival function estimators. Journal of Statistical Computation and Simulation, 72(4):299308, 2002. [18] Pedro Costa, Adrian Galdran, Maria Ines Meyer, Meindert Niemeijer, Michael Abr`amoff, Ana Maria Mendonca, and Aurelio Campilho. End-to-end adversarial retinal image synthesis. IEEE transactions on medical imaging, 37(3):781 791, 2017. 2 [19] Michael Moor. Medgpt, 2025. Accessed: March 7, 2025. 8 [20] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua Tenenbaum, et al. Video language planning. arXiv preprint arXiv:2310.10625, 2023. 2 [21] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua Tenenbaum, et al. Video language planning. In ICML, 2024. [22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 4 [23] John Fox and Sanford Weisberg. Cox proportional-hazards regression for survival data. An and S-PLUS companion to applied regression, 2002, 2002. 7 [24] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 2 [25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3, 5 [26] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. 2 [27] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. [28] Amr Hagag, Ahmed Gomaa, Dominik Kornek, Andreas Maier, Rainer Fietkau, Christoph Bert, Yixing Huang, and Florian Putz. Deep learning for cancer prognosis predicIn Intion using portrait photos by stylegan embedding. ternational Conference on Medical Image Computing and Computer-Assisted Intervention, pages 198208. Springer, 2024. 3 [29] Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, et al. Evaluation and mitigation of the limitations of large language models in clinical decision-making. Nature medicine, 30(9):26132622, 2024. 1, 3 [30] Ibrahim Ethem Hamamci, Sezgin Er, and Bjoern Menze. Ct2rep: Automated radiology report generation for 3d medical imaging. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 476 486. Springer, 2024. 8 [31] Jan Hinrichs, Hoen-Oh Shin, Daniel Kaercher, Davut Hasdemir, Tim Murray, Till Kaireit, Carolin Lutat, Arndt Vogel, Bernhard Meyer, Frank Wacker, et al. Parametric response mapping of contrast-enhanced biphasic ct for evaluating tumour viability of hepatocellular carcinoma after tace. European radiology, 26:34473455, 2016. 5 [32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 2, 5 [33] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. [34] Qixin Hu, Yixiong Chen, Junfei Xiao, Shuwen Sun, Jieneng Chen, Alan Yuille, and Zongwei Zhou. Label-free liver tumor segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74227432, 2023. 2, 6, 7 [35] Pu Huang, Dengwang Li, Zhicheng Jiao, Dongming Wei, Bing Cao, Zhanhao Mo, Qian Wang, Han Zhang, and Dinggang Shen. Common feature learning for brain tumor mri synthesis by context-aware generative adversarial network. Medical Image Analysis, 79:102472, 2022. 2 [36] Xiaoyu Huang, Yong Huang, Ping Li, and Kai Xu. Ct-based deep learning predicts prognosis in esophageal squamous cell cancer patients receiving immunotherapy combined with chemotherapy. Academic Radiology, 2025. 3 [37] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 8 [38] Fabian Isensee, Paul Jaeger, Simon AA Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203211, 2021. 5, [39] Jared Katzman, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval Kluger. Deepsurv: personalized treatment recommender system using cox proportional hazards deep neural network. BMC medical research methodology, 18:112, 2018. 5 [40] Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh Ghassemi, Cynthia Breazeal, Hae Park, et al. Mdagents: An adaptive collaboration of llms for medical decisionmaking. Advances in Neural Information Processing Systems, 37:7941079452, 2024. 3 [41] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 5 [42] Yuxiang Lai, Xiaoxi Chen, Angtian Wang, Alan Yuille, and Zongwei Zhou. From pixel to cancer: Cellular automata in computed tomography. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 3646. Springer, 2024. 2, 6, 7 [43] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. 2 [44] Kyung Hwa Lee, Jungwook Lee, Gwang Hyeon Choi, Jihye Yun, Jiseon Kang, Jonggi Choi, Kang Mo Kim, and Namkug Kim. Deep learning-based prediction of posttreatment survival in hepatocellular carcinoma patients using pre-treatment ct images and clinical data. Journal of Imaging Informatics in Medicine, pages 112, 2024. 3 [45] Chenxin Li, Hengyu Liu, Yifan Liu, Brandon Feng, Wuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, and Yixuan Yuan. Endora: Video generation models as endoscopy simulators. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 230240. Springer, 2024. [46] Jia Li, Zichun Zhou, Han Lyu, and Zhenchang Wang. Large language models-powered clinical decision support: enhancing or replacing human expertise?, 2025. 3 [47] Xinran Li, Yi Shuai, Chen Liu, Qi Chen, Qilong Wu, Pengfei Guo, Dong Yang, Can Zhao, Pedro RAS Bassi, Daguang arXiv preprint Xu, et al. Text-driven tumor synthesis. arXiv:2412.18589, 2024. 2, 6, 7, 5 [48] Junhao Liang, Weisheng Zhang, Jianghui Yang, Meilong Wu, Qionghai Dai, Hongfang Yin, Ying Xiao, and Lingjie Kong. Deep learning supported discovery of biomarkers for clinical prognosis of liver cancer. Nature Machine Intelligence, 5(4):408420, 2023. 3 [49] Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for In Proceedings organ segmentation and tumor detection. of the IEEE/CVF International Conference on Computer Vision, pages 2115221164, 2023. 5 [50] Jie Liu, Yixiao Zhang, Kang Wang, Mehmet Can Yavuz, Xiaoxi Chen, Yixuan Yuan, Haoliang Li, Yang Yang, Alan Yuille, Yucheng Tang, et al. Universal and extensible language-vision models for organ segmentation and tumor detection from abdominal computed tomography. Medical image analysis, 97:103226, 2024. 5 [51] Taiming Lu, Tianmin Shu, Alan Yuille, Daniel Khashabi, and Jieneng Chen. Generative world explorer. arXiv preprint arXiv:2411.11844, 2024. [52] Yunhao Luo and Yilun Du. Grounding video models to actions through goal conditioned exploration. arXiv preprint arXiv:2411.07223, 2024. 2 [53] Fei Lyu, Mang Ye, Jonathan Frederik Carlsen, Kenny Erleben, Sune Darkner, and Pong Yuen. Pseudo-label guided image synthesis for semi-supervised covid-19 pneumonia infection segmentation. IEEE Transactions on Medical Imaging, 42(3):797809, 2022. 2 [54] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan Krumholz, Jure Leskovec, Eric Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259265, 2023. 8 [55] Ali Morshid, Khaled Elsayes, Ahmed Khalaf, Mohab Elmohr, Justin Yu, Ahmed Kaseb, Manal Hassan, Armeen Mahvash, Zhihui Wang, John Hazle, et al. machine learning model to predict hepatocellular carcinoma response to transcatheter arterial chemoembolization. Radiology: Artificial Intelligence, 1(5):e180021, 2019. 6, 2 [56] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. Conference on Machine Learning, pages 81628171. PMLR, 2021. 5 [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4 [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, [59] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. Masked In Conference on Robot world models for visual control. Learning, pages 13321344. PMLR, 2023. 2 [60] Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang Ji, et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. Advances in neural information processing systems, 34:21362147, 2021. 6 [61] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large Nature, language models encode clinical knowledge. 620(7972):172180, 2023. 1 [62] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, et al. Toward expert-level medical question answering with large language models. Nature Medicine, pages 18, 2025. 1 [63] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. [64] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 [65] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. closer look at spatiotemporal In Proceedings of the convolutions for action recognition. IEEE conference on Computer Vision and Pattern Recognition, pages 64506459, 2018. 5 [66] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 15261535, 2018. 2 [67] Joost van Griethuysen et. al. Radiomics-based cox model, 2025. Accessed: March 7, 2025. 7 [68] Wang, Zhu, Huang, Chen, and Drivedreamer Lu. Towards real-world-driven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023. [69] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worldIn European drive world models for autonomous driving. Conference on Computer Vision, pages 5572, 2024. 2 [70] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for In Conference on robot learning, physical robot learning. pages 22262240. PMLR, 2023. 2 [71] Zhaohu Xing, Sicheng Yang, Sixiang Chen, Tian Ye, Yijun Yang, Jing Qin, and Lei Zhu. Cross-conditioned diffuIn Insion model for medical image to image translation. ternational Conference on Medical Image Computing and Computer-Assisted Intervention, pages 201211. Springer, 2024. 2 [72] Jiawen Yao, Yu Shi, Kai Cao, Le Lu, Jianping Lu, Qike Song, Gang Jin, Jing Xiao, Yang Hou, and Ling Zhang. Deepprognosis: Preoperative prediction of pancreatic cancer survival and surgical margin via comprehensive understanding of dynamic contrast-enhanced ct imaging and tumor-vascular contact parsing. Medical image analysis, 73:102150, 2021. 3 [73] Jiawen Yao, Yu Shi, Le Lu, Jing Xiao, and Ling Zhang. Deepprognosis: Preoperative prediction of pancreatic cancer survival and surgical margin via contrast-enhanced ct imagIn International Conference on Medical Image Coming. puting and Computer-Assisted Intervention, pages 272282. Springer, 2020. 3 [74] Qingsong Yao, Li Xiao, Peihang Liu, and Kevin Zhou. Label-free segmentation of covid-19 lesions in lung ct. IEEE transactions on medical imaging, 40(10):28082819, 2021. [75] Wenfang Yao, Chen Liu, Kejing Yin, William Cheung, and Jing Qin. Addressing asynchronicity in clinical multimodal fusion via individualized chest x-ray generation. Advances in Neural Information Processing Systems, 37:2900129028, 2025. 2 [76] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards arXiv preprint taming language model to be doctor. arXiv:2305.15075, 2023. 8 [77] Yanxin Zheng, Wensheng Gan, Zefeng Chen, Zhenlian Qi, Qian Liang, and Philip Yu. Large language models for medicine: survey. International Journal of Machine Learning and Cybernetics, pages 126, 2024. 3 [78] Yi Zhou, Xiaodong He, Shanshan Cui, Fan Zhu, Li Liu, and Ling Shao. High-resolution diabetic retinopathy imIn Inage synthesis manipulated by grading and lesions. ternational conference on medical image computing and computer-assisted intervention, pages 505513. Springer, 2019."
        },
        {
            "title": "Table of Contents",
            "content": "A. Data Preprocessing A.1. HCC-TACE-Seg dataset preprocessing . . A.2. HCC-TACE dataset preprocessing . . B. Implementation Details . . B.1. Policy Model . . . B.2. Dynamics Model B.3. Assistant Model . . . B.4. Heuristic Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Comparison against Multi-Modal GPTs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 5 5 5 5 5 5 6 A. Data Preprocessing A.1. HCC-TACE-Seg dataset preprocessing The HCC-TACE-Seg dataset [55] refers to single-institution collection of patients with confirmed hepatocellular carcinoma (HCC) who were treated at The University of Texas MD Anderson Cancer Center. Data preprocessing for HCC-TACE-Seg involves resampling the provided CT images to standardized spatial resolution while preserving the integrity of the original data structure. Specifically, images and masks are resampled to target spacing of 0.8mm 0.8mm 3.0mm to standardize voxel dimensions across different cases. Longitudinal Registration: Accurate image registration is essential to ensure that tumor boundaries are clearly defined across both the liver and HCC regions in different imaging modalities, such as arterial phase (AP) and portal venous phase (PVP) scans. The longitudinal registration process involves aligning the post-AP image to the pre-AP image, and the postPVP to the pre-PVP image, addressing any misalignments between scans. Both linear and non-linear registration methods are employed through the open-sourced registration framework deedsBCV [1] for optimal alignment. Liver and HCC Cancer Segmentation: We utilize nnUNet-based [38] mode trained on the public LiTS dataset [5] for liver and HCC cancer segmentation. For postprocessing, we adopt connected component analysis to extract the liver and HCC regions precisely. This approach ensures that the tumor and liver boundaries are defined clearly, which is crucial for downstream analysis. An example of preand post-treatment CT images, along with liver and tumor segmentation generated from the HCC-TACE-Seg dataset, is shown in Figure 8. We also conduct Component Size Filtering strategy. component size filtering step is applied, with minimum threshold of 300 voxels, ensuring the accurate identification of tumor and liver regions. This step helps to remove noise or irrelevant small regions, improving the precision of segmentation results. Only the image data paired with the following meta-information are selected for further analysis: Chemotherapy: Information about whether the patient underwent chemotherapy treatment, including details about the type of chemotherapy regimen. Overall survival: Overall survival time in months. Survival status: 0 indicates that the patient is alive or lost to follow-up, while 1 indicates death. Details are in Table 4. Figure 8. Example of HCC-TACE-Seg dataset. The first row shows HCC Pre-CT images, and the second row shows HCC Post-CT images. The red mask represents the liver, while the green mask represents the HCC tumor."
        },
        {
            "title": "Chemotherapy",
            "content": "Overall Survival (months)"
        },
        {
            "title": "Survival Status",
            "content": "HCC 009 Cisplatin; Doxorubicin; Mitomycin; Lipiodol HCC 011 Cisplatin; Doxorubicin; Mitomycin; Lipiodol HCC 025 Cisplatin; Doxorubicin; Mitomycin; Lipiodol HCC 034 HCC 042 HCC 051 HCC 058 Doxorubicin; Lipiodol; LC beads Cisplatin; Mitomycin; Lipiodol Cisplatin; Mitomycin; Lipiodol Cisplatin; Mitomycin; Lipiodol HCC 067 Cisplatin; Doxorubicin; Mitomycin; Lipiodol HCC 079 HCC Doxorubicin; LC beads; Lipiodol Doxorubicin; LC beads; Lipiodol 4.7 19.3 30.0 18. 34.1 12.9 87.0 90.9 42.5 25. 1.0 1.0 1.0 1.0 1.0 1. 0.0 0.0 1.0 0.0 Table 4. An example of HCC-TACE-Seg dataset metadata, including chemotherapy, overall survival, and survival status. For survival status, value of 0 indicates that the patient is alive or lost to follow-up, while value of 1 indicates death. Figure 9. Example of HCC-TACE dataset. The first row shows HCC Pre-CT images, and the second row shows HCC Post-CT images. The red mask represents the liver, while the green mask represents the HCC tumor. In post-treatment CT imaging of HCC, particularly after Transarterial Chemoembolization (TACE), the viable tumor region and its enhancement intensity decrease due to Lipiodol accumulation and treatment-induced necrosis. Lipiodol appears hyperdense (bright) on post-treatment CT, indicating areas that have been successfully embolized."
        },
        {
            "title": "Processed Chemotherapy",
            "content": "OS (months)"
        },
        {
            "title": "Survival Status",
            "content": "HCC 08116730 HCC 01061677 HCC 01192613 HCC 01204059 HCC 01532843 HCC Raltitrexed 4 mg was infused through the catheter; 5 ml ultra-liquid Lipiodol and 5 mg Epirubicin were mixed to create an emulsion for embolization; the emulsion was slowly injected under fluoroscopic guidance; an appropriate amount of Gelatin Sponge particles was used to embolize the tumor-feeding branches of the S8 segment of the right hepatic artery; Lipiodol deposition in the tumor was satisfactory; tumor-feeding arteries were occluded on the final angiography. THP 10 mg was infused through the catheter; 10 mg THP and 10 ml ultra-liquid Lipiodol were mixed to create an emulsion for embolization; 12 ml of the emulsion was slowly injected under fluoroscopic guidance; Lipiodol deposition in the tumor and satellite lesions was satisfactory; tumor-feeding arteries were occluded on the final angiography. THP 40 mg and 30 ml ultra-liquid Lipiodol, along with small amount of contrast agent, were mixed to create an emulsion for embolization; 30 ml of the emulsion was slowly injected under fluoroscopic guidance; small amount of Gelatin Sponge particles was used for embolization; Lipiodol deposition in the tumor was satisfactory; no tumor staining was observed on the final angiography. Cisplatin 40 mg was infused through the catheter; 10 ml ultra-liquid Lipiodol was slowly injected under fluoroscopic guidance for embolization of the right hepatic artery tumor-feeding branches; 3 ml ultra-liquid Lipiodol was injected for protective embolization of the segment II branch of the left hepatic artery; Lipiodol deposition in the tumor was acceptable; tumor staining mostly disappeared on the final angiography. Oxaliplatin 100 mg and Epirubicin 30 mg were infused through the catheter; 10 mg Epirubicin and 10 ml ultraliquid Lipiodol were mixed to create an emulsion for embolization; 10 ml of the emulsion was slowly injected under fluoroscopic guidance; an appropriate amount of Gelatin Sponge particles was used to embolize the tumorfeeding branches of the right hepatic artery; Lipiodol deposition in the tumor was satisfactory; tumor staining disappeared on the final angiography. Epirubicin 40 mg and Oxaliplatin 100 mg were infused through the catheter; 10 ml ultra-liquid Lipiodol was slowly injected under fluoroscopic guidance for embolization; Lipiodol deposition in the tumor was satisfactory; tumor-feeding arteries were mostly occluded on the final angiography. 1.4 0.0 75.7 1.0 84.6 1. 17.1 0.0 29.3 1.0 21.6 1. Table 5. An example of HCC-TACE dataset metadata, including chemotherapy, overall survival, and survival status. For survival status, value of 0 indicates that the patient is alive or lost to follow-up, while value of 1 indicates death. A.2. HCC-TACE dataset preprocessing The HCC-TACE dataset is large-scale, self-collected repository containing 338 longitudinal pairs of preand post-treatment CT scans, along with well-annotated liver and tumor masks, as well as clinical records. These records include TACE radiotherapy reports (considered the gold action) and Overall Survival (OS) time. Details are presented in Table 5. The dataset is split into training (including validation) and testing sets in 9:1 ratio. All images and masks are resampled to target spacing of 0.8mm 0.8mm 3.0mm to standardize voxel dimensions across different cases. Longitudinal Registration: We also employ deedsBCV [1] to align the post-AP image with the pre-AP image, and the post-PVP image with the pre-PVP image, addressing any misalignments between the scans. Liver and HCC Cancer Annotation: In this dataset, all liver and tumor masks for each CT scan are carefully annotated by radiologists. For postprocessing, we also apply connected component analysis to accurately extract the liver and HCC regions. An example of preand post-treatment CT images, along with liver and tumor segmentation generated from the HCC-TACE dataset, is shown in Figure 9. B. Implementation Details B.1. Policy Model We adopt GPT-4o to obtain the initial observation from the given pre-treatment CT scans and collect the individualized potential drugs and embolism during TACE treatment. An example is presented in Figure 10. Then, we refine the action set using DeepSeek-R1 [25], which reasons the clinical conflicts in the current action set and summarizes better action set using clinical guidelines for individuals (e.g., Multiple platinum-based drugs cannot be used simultaneously). B.2. Dynamics Model In this study, we implement Dynamics Model by training the corresponding Diffusion Model [47] specifically from pretreatment liver tumors to post-treatment liver tumors. The CT scans are oriented according to specific axcodes and resampled to achieve isotropic spacing of 1.0 1.0 1.0 mm3. 96 96 96 patches are randomly cropped around either foreground voxels based on set ratio. Their intensities are truncated to the range [175, 600] to maintain the discrimination of lipiodol/necrosis/viable areas [31], then linearly normalized to [-1, 1]. We utilize the Adam optimizer with hyperparameters β1 = 0.9 and β2 = 0.999, learning rate of 0.0001, and batch size of 10 per GPU. The training is conducted on A6000 GPUs for 2 days, over total of 2,000 iterations. B.3. Assistant Model We employ nnUNet-based [38] segmentation model for the segmentation of liver and tumor in post-treatment CT. As suggested by Chen et al. [14], we generate realistic tumor-like shapes using ellipsoids, and combine these generated tumor masks with the healthy CT volumes to create range of realistic liver tumors. We pre-train the model on the generated and real tumors for robust generalization. Then, we finetune it on post-treatment CT scans as well as liver and tumor masks. The implementation is in Python, leveraging MONAI*. The CT scans are oriented according to specific axcodes and resampled to achieve isotropic spacing of 1.0 1.0 1.0 mm3. Their intensities are truncated to the range [175, 600] to maintain the discrimination of lipiodol/necrosis/viable areas, then linearly normalized to [-1, 1]. During training, 96 96 96 patches are randomly cropped around either foreground or background voxels based on set ratio. Each patch is subjected to 90 rotation with probability 0.1 and an intensity shift of 0.1 with probability 0.2. To avoid confusing the left and right organs, mirroring augmentation is not used. The model is initialized with pre-trained liver tumor weights from DiffTumor [14], then fine-tuned on our dataset for 2,000 epochs. We set the base learning rate to 0.0002 and use batch size of 8, along with linear warmup and cosine annealing schedule. Training spans 2 days on eight A6000 GPUs. Additional details on the tumor synthesis process during Segmentation Model training can be found in DiffTumor [14]. For inference, sliding window strategy with 0.75 overlap is used. Tumor predictions that fall outside their corresponding organs are removed by post-processing with organ pseudo-labels obtained from previous research. B.4. Heuristic Function We implement CNN-based survival analysis model as Heuristic Function. The framework adopts 3D ResNet (MC3) as the backbone and Two-way Transformer as the interaction module of pre-treatment and post-treatment CT features. multi- *Cardoso et al. [12]: https://monai.io/ Liu et al. [49, 50]: https://github.com/ljwztc/CLIP-Driven-Universal-Model It is implemented instance aggregator [60] with consecutive fully connected layers is utilized for survival risk scoring. on PyTorch using 8 NVIDIA RTX A6000 GPUs. Their intensities are truncated to the range [175, 600] to maintain the discrimination of lipiodol/necrosis/viable areas, then linearly normalized to [-1, 1]. During training, 96 96 96 patches are randomly cropped around either foreground or background voxels based on set ratio. Each patch is subjected to 90 rotation with probability 0.1 and an intensity shift of 0.1 with probability 0.2. To avoid confusing the left and right organs, mirroring augmentation is not used. We utilize the Adam optimizer with hyperparameters β1 = 0.9 and β2 = 0.999, learning rate of 0.00002, and batch size of 5 per GPU. For the inference of each case, we predict the survival risk scores of 5 patches around the foreground and average them to obtain the final score. C. Comparison against Multi-Modal GPTs We carefully design prompt templates for multi-modal GPTs to generate TACE treatment protocol. The first template (Figure 11) is for our dataset with larger action space, defining the task description, predefined set of chemotherapy drugs and embolization materials, and an example of input-output in JSON format. The second template (Figure 12) is specifically designed for the HCC-TACE-Seg dataset, featuring different selection of chemotherapy drugs and embolization materials. Both prompts instruct GPTs to analyze CT images and generate an appropriate TACE treatment plan, submitting results in JSON format with predefined keywords. Figure 10. Policy model prompt template for our dataset. Figure 11. VLM prompt template for our dataset. Figure 12. VLM prompt template for HCC-TACE-Seg."
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "The First Affiliated Hospital of Nanjing Medical University",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "University of California, San Francisco"
    ]
}