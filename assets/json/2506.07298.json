{
    "paper_title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context",
    "authors": [
        "Yijia Dai",
        "Zhaolin Gao",
        "Yahya Satter",
        "Sarah Dean",
        "Jennifer J. Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)$\\unicode{x2013}$their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequences$\\unicode{x2013}$an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 8 9 2 7 0 . 6 0 5 2 : r Pre-trained Large Language Models Learn Hidden Markov Models In-context Yijia Dai Cornell University yijia@cs.cornell.edu Zhaolin Gao Cornell University zg292@cornell.edu Yahya Satter Cornell University ysattar@cornell.edu Sarah Dean Cornell University sdean@cornell.edu Jennifer J. Sun Cornell University jjs533@cornell.edu"
        },
        {
            "title": "Abstract",
            "content": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)their ability to infer patterns from examples within prompt. On diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequencesan advance that deepens our understanding of in-context learning in LLMs and establishes its potential as powerful tool for uncovering hidden structure in complex scientific data. Our code is available at https://github.com/DaiYijia02/icl-hmm."
        },
        {
            "title": "Introduction",
            "content": "Many natural and artificial systems, from animal decision-making to ecological processes to climate patterns, generate observations governed by underlying, unobservable states that follow Markovian dynamics [3, 17, 34, 48, 57]. Hidden Markov Models (HMMs) [39] provide powerful framework for studying such phenomena. However, accurately modeling these systems presents significant challenges. Parameter estimation and model fitting require complex algorithms like Baum-Welch [5], and Gibbs Sampling [16]. These methods are often computationally intensive and can be algorithmically unstable, demanding extensive domain expertise [8, 9]. For scientists across disciplines, these accessibility and computational bottlenecks limit the practical applications of HMM modeling tools. Recently, large language models (LLMs) [1, 18] have reshaped the landscape of AI. Trained on vast amounts of sequential text data, they have achieved unprecedented performance across natural language processing tasks and exhibit remarkable in-context learning (ICL) capabilitiesthe ability to learn patterns and perform new tasks directly from examples provided in the input context, without explicit parameter updates [7]. While prior theoretical and empirical works [13, 33, 40] have demonstrated LLMs capabilities as Bayesian learners and their ability to model fully observed Markov processes, their capacity to implicitly learn Hidden Markov Modelswith latent states, complex transition dependencies, and observation emissionsremains largely unexplored. Understanding this Preprint. Under review. Figure 1: Overview of our study. We start by studying whether ICL using pre-trained LLMs can converge to theoretical optimum on HMM sequences (Q1, Section 2), then study how HMMs properties affect the convergence rate/gap with theoretical conjectures (Q2, Section 3), and finally we demonstrate how these findings translate to insights on real-world datasets for studying behaviors in science (Q3, Section 4). capacity could illuminate the mechanisms underlying in-context learning HMMs, and reveal new ways to leverage LLMs for analyzing complex sequential phenomena in scientific contexts. In this paper, we present comprehensive study on the ability of pre-trained LLMs to learn HMMs through in-context learning (Figure 1), revealing their surprisingly strong performance and offering actionable insights for real-world scientific experiments. key finding is that pre-trained LLMs demonstrate remarkable capacity to learn HMMs nearly optimally, achieving performance that approaches optimal Bayesian inference and often surpasses traditional statistical methods. These results not only advance our understanding of the emergent capabilities of in-context learning, but also introduce novel and practical framework for using LLMs as powerful, efficient statistical tools in complex scientific data analysis. Our study makes three key contributions: 1. We conduct systematic, controlled experiments on synthetic HMMs and empirically show that pretrained LLMs outperform traditional statistical methods such as BaumWelch. Moreover, their prediction accuracy consistently converges to the theoretical optimumas given by the Viterbi algorithm with ground-truth model parametersacross wide range of HMM configurations (Section 2). 2. We identify and characterize empirical scaling trends showing that LLM performance improves with longer context windows, and that these trends are shaped by fundamental HMM properties such as mixing rate and entropy. We further provide theoretical conjectures to explain these phenomena, drawing connections toand highlighting distinctions fromclassical HMM learning paradigms, including spectral methods. These findings offer important insights into the learnability of stochastic systems through in-context learning (Section 3). 3. We translate our findings into practical guidelines for scientists, demonstrating how LLM incontext learning can serve as diagnostic tool for assessing data complexity and uncovering underlying structure. When applied to real-world animal decision-making tasks, LLM ICL performs competitively with domain-specific models developed by human experts (Section 4)."
        },
        {
            "title": "2 Synthetic Experiments and ICL Convergence",
            "content": "We investigate the in-context learning capabilities of pre-trained LLMs on sequences generated by synthetic HMMs. We first review key HMM properties (Section 2.1) and outline our experimental setup (Section 2.2). We then empirically demonstrate that the prediction accuracy of pre-trained LLMs consistently converges to the theoretical optimum (Section 2.3). 2 Figure 2: Properties of HMMs. 2.1 HMM Background Hidden Markov model: HMMs impose set of probabilistic assumptions on how sequences of data are generated. The elements of the sequence are called observations, denoted at each step by Ot. The observations depend on hidden state denoted by Xt, which evolves according to Markov chain. HMM is characterized by the Markov chains initial state distribution and its state transitions, along with the emission probabilities of an observation given the hidden state. The key assumptions are that the state transition depends only on the previous state (Markov property), the observation depends only on the current hidden state (output independence), and both transition and emission probabilities are time-invariant (stationarity). We focus on the setting with finitely many states and observations. Without loss of generality, states take values in = {1, 2, . . . , } while observations take values in = {1, 2, . . . , L}. The initial state distribution is denoted as π RM with πj the probability of starting in state j, the state transitions are describe by the matrix RM with elements aij the probability of transitioning to state from state i, and the emission matrix RM contains bjl the probability of observing when in hidden state j. The triple λ = (π, A, B) completely parameterizes finite-alphabet HMM. Stationary distributions: Under certain conditions (see Appendix A), Markov chains are guaranteed to converge to unique stationary distributions, which are given by the µ RM satisfying µ = µA [14]. The stationary distribution of the hidden state characterizes the long term behavior of the HMM, and therefore plays an important role in both predicting future observations and learning HMM parameters. The rate of convergence is characterized by the mixing rate which for finite-alphabet HMMs is equal to λ2, the second-largest eigenvalue of A. From any initial distribution, the hidden state distribution approaches the stationary distribution geometrically with multiplier λ2. smaller mixing rate indicates faster convergence to the stationary distribution. Entropy: HMMs can describe processes which vary from deterministic to purely random, depending on how transition and emission probabilities are defined. Entropy is measure of the randomness or unpredictability of random variable. By considering the average entropy over the stochastic processes of hidden state and observation, we can quantify the entropy of particular HMM by H(A) = (cid:80) j,l µjbjl log bjl. We additionally define normalized entropies H(A) = H(A)/ log and H(B, µ) = H(B, µ)/ log as metrics for visualization. smaller entropy indicates more predictable process. See Appendix for further explanation. i,j µiaij log aij and H(B, µ) = (cid:80) 2.2 Experimental Setup Experiment setting: Our experiment follows three-step protocol: First, we specify the HMM parameters λ = (π, A, B) according to our control variables (described below). Second, we generate observation sequences {o1, o2, . . .} from this parameterized model. Third, we evaluate the ability of candidate models to predict the next observation ot+1 given preceding observations o1:t. We systematically vary five control parameters and consider 234 total HMM settings: (1) state and observation space dimensions, with M, {2, 4, 8, 16, 32, 64}; (2) mixing rate of the hidden Markov chain, with λ2 {0.5, 0.75, 0.95, 0.99}, where λ2 is the second-largest eigenvalue of A; (3) skewness of the stationary distribution µ (uniform or non-uniform); (4) entropy of the transition and emission matrices and B, ranging from deterministic (zero entropy) to maximum entropy (random); and (5) initial state distribution π (uniform or deterministic). While generating π and is straight-forward, for matrix A, we define constrained optimization problem and solve using first order optimization. See Appendix for additional details. For each parameter configuration, we sample 4,096 state-observation sequence pairs, each of length 2,048. We assess model performance across context lengths ranging from 4 to 2,048 observations, specifically {4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048}. For each HMM setting, we report performance metrics 3 Method Input Variable Description O1:t, λ Viterbi [51] (Ot+1Otk:t) Otk:t, λ Baum-Welch [5] O1:t, LSTM (RNN) n-gram O1:t O1:t Finds most likely hidden state sequence Direct conditional probability EM algorithm for parameter estimation Neural network with memory cells Predicts based on preceding (n 1) tokens Table 1: Methods for HMM prediction task. Figure 3: (Left) We define as when LLM converges (see Appendix for computation metric), and ε as the final accuracy gap at sequence length 2048. (Middle) Examples when LLM accuracy converges to Viterbi. Each curve represents different HMM parameter setting. LLM ICL shows consistent convergence behavior. (Right) Examples of convergence in Hellinger distance (distance between two probability distributions). LLM ICL is not just guessing the most probable output, but converging distributionally. averaged over the 4,096 samples. Our candidate models are open-source pre-trained LLMs (Qwen and Llama family). Note that we are not training the LLMs, only evaluating their capability for in-context learning. In the following sections, we evaluate LLMs performance by comparison to several other approaches  (Table 1)  . 2.3 ICL Converges to Theoretical Optimum We define convergence as achieving prediction accuracy comparable to the Viterbi algorithm. The Viterbi algorithm, given ground-truth HMM parameters λ, computes the most likely hidden state sequence x1:t from observations o1:t (see Appendix for details). Since Viterbi has access to the true model parameters, its performance represents the theoretical optimum. Remarkably, ICL with pre-trained LLM achieves this near-optimal prediction accuracy across diverse HMM parameter configurations in our experiments. Figure 3 illustrates examples and conditions under which this convergence occurs. Convergence occurs reliably when HMM entropy is low and mixing is fast. For challenging conditions where LLM convergence fails or proceeds exceptionally slowlylike the red areas shown in the left-hand side of Figure 4Viterbi algorithm also exhibits diminished prediction accuracy and requires substantially longer context windows to achieve reliable performance. This degraded performance reflects the fundamental limits of stochastic system learnability due to random dynamics and long-range dependencies, affecting even the optimal inference methods. See Appendix for detailed examples."
        },
        {
            "title": "Impact of HMM Properties on Convergence",
            "content": "Having established that ICL with pre-trained LLM converges to the theoretically optimal predictions, we now provide an in depth characterization of their performance across variable settings. We summarize the scaling trends in terms of key HMM properties, compare these patterns against other popular methods for HMM prediction, and conclude by providing theoretical conjectures. 4 Figure 4: (Left) Convergence gap ε increases with higher mixing rate (slower mixing) and higher entropy. This plot is showing results averaged across all HMM configurations we tested. (Right) Slower mixing (λ2 = 0.5, 0.75) shows delayed convergence compared to (Middle) fast mixing (λ2 = 0.95, 0.99) at similar entropy levels. Figure 5: HMM parameters = 8, = 8, H(A) = 1.5, H(B) = 1. (Left) The gap between (Ot+1Ot) and Viterbi is small when mixing is fast. (Middle) Accuracy comparison with baselines. (Right) Hellinger distance measures distance between two probability distributions. 3.1 In-context Scaling Trends Neural scaling laws describe empirical power-law relationships that characterize how neural network performance improves with increases in key resources such as model size, dataset size, and compute [24]. For in-context learning [29], it describes trends between prediction accuracy and context window length. We further show how these scaling trends depend on the underlying stochastic process characteristics: entropy, mixing rate, and state-space dimensionality. Context window length: Given an observation sequence sampled from an HMM, LLM performance generally improves monotonically with increasing sequence length before plateauing. Representative examples are shown in Figure 3. Fluctuations may occur when the entropy of the sequence is high; additional examples are provided in Appendix D. Entropy of transitions and emissions: Entropy determines the predictability of the observation sequence. The entropies of both the transition matrix and the emission matrix are positively correlated with the number of steps required for LLM convergence, as shown in the middle and right plots of Figure 4. However, this relationship is not strictly monotonic in practice. Mixing rate: We control the mixing rate of synthetic HMMs using λ2, the second-largest eigenvalue of A. Lower values of λ2 indicate faster mixing. As illustrated in Figure 4 (middle vs. right), for the same entropy level, convergence occurs significantly later when mixing is slowindicating that slower mixing delays LLM learning. Number of hidden states and observations: The dimensionality of the state and observation spaces affects the maximum possible entropy. While larger state spaces intuitively allow for higher entropy, our experiments show that when entropy is held constant, varying the number of states does not impact LLM convergence rates. Detailed examples and discussion are provided in Appendix D. 3.2 Comparison to Baselines We compare the in-context learning performance of pre-trained LLMs against several established methods commonly used for HMM prediction tasks, as summarized in Table 1. The conditional predictor (Ot+1 Otk:t) leverages the true HMM parameters, like the Viterbi algorithm, but truncates the observation history to simulate the impact of limited memory on otherwise optimal inference. For learning-based baselines, we include the classical BaumWelch (BW) expectationmaximization algorithm, which remains the statistical state of the art for HMM parameter estimation [56]. We also evaluate an LSTM baseline, reflecting the widespread use of RNNs in applied domains such as neuroscience [54]. Finally, we include an n-gram model to bridge our HMM results with recent findings on ICL performance in Markovian settings [40]. Full algorithm descriptions and implementation details are provided in Appendix C. Across range of HMM configurations, we find that in-context learning with pre-trained LLMs consistently outperforms empirical learning baselines. representative example is shown in Figure 5. Among the n-gram models with {1, . . . , 4}, the bigram model performs best, aligning with its role as the maximum likelihood estimator for first-order Markov chains. However, because HMM observations are not Markovian when H(B) > 0, the bigram model is inherently suboptimal. Trigram models converge more slowly than bigram due to increased data sparsity and resulting estimation bias. The BaumWelch (BW) algorithm, while given the correct HMM structure and leveraging expectationmaximization, suffers from nonconvex optimization. Its global convergence is not guaranteed. Even when averaging across multiple random seeds and 4096 samples per setting, BW converges slowly and often unreliably. LSTM baselines, despite their flexibility, require significant computational resources and exhibit unstable accuracy across varying context lengths. Using the Hellinger distance metric, we find that LSTMs converge more slowly to the true output distribution compared to ICL. Notably, LLMs via ICL demonstrate clearly superior behavior across all baselinesachieving faster, more stable convergence to the ground-truth distribution and highlighting their surprising efficiency in modeling HMMs. On the other hand, the conditional predictor (Ot+1 Otk:t) can approach Viterbi-level performance, particularly when the mixing rate is low. This observation suggests that approximate prediction using truncated observation history can be nearly as effective as statistically optimal inference in fast-mixing regimes, motivating the conjecture discussed in Section 3.3. 3.3 Possible Theoretical Explanation In this section, we attempt to explain the ICL behavior of pre-trained LLMs for HMM sequence prediction by comparing it with the spectral learning algorithm [21]. This is motivated by empirical evidences [31, 42] showing the convergence of spectral learning prediction to theoretical optimum (in limited settings). Furthermore, for partially observed linear dynamical systems, Li et al. [27] observes that transformers can learn statistically optimal predictions in-context when trained on many similar tasks in the meta-learning setting. These findings suggest that ICL by LLMs may exhibit similar performance characteristics to spectral learning algorithms. key idea in spectral learning literature is to compute the probability of observation sequences in terms of observation operators [21]: For any O, define Ao := Adiag([B]1,o, . . . , [B]M,o), where [B]i,j denotes the ij-th element of B. Then for any > 0, 1A π P(Ot+1 = ot+1 (cid:12) (cid:12) O1:t = o1, . . . , ot) = (1) ot+1 1A ot ot o1 o1 π , where 1 is all one vector of appropriate dimension. In this formulation, the conditional probability is estimated by first learning the spectral parameters (see supplementary) using training samples (in-context observations). Then, one can predict the next observation directly using these parameters along-with hidden state belief updates, without explicitly learning the matrices A, and B. This is therefore an example of improper learning, which has been extensively studied in related areas like linear dynamical systems [45]. The spectral learning algorithm is theoretically well understood. The following theorem is obtained by extending the results by Hsu et al. [21] to single trajectory spectral learning. Theorem 1 (Informal) Fix ϵ, δ > 0. Let Σ2 denote the pairwise probability matrix of observations such that [Σ2]ij = P(Ot+1 = i, Ot = j). Suppose π > 0 element-wise, and A, are rank . Suppose , and let σM () denote the -th largest singular value. Suppose the observation operator Ao > 0 element-wise for all O, and 1 1 λ2(A) (cid:18) 2L ϵ4σM (B)2σM (Σ2)4 + ϵ2σM (B)2σM (Σ2) (cid:19) log (cid:19) , (cid:18) 1 δ (2) Then, with probability at least 1 δ, the next observation prediction ˆP( (cid:12) (cid:12) O1:t) using spectral learning algorithm (detailed in Appendix F) satisfies the following upper bound in Hellinger distance, 2 (cid:16) P(Ot+1 (cid:12) (cid:12) O1:t = o1, . . . , ot), ˆP(Ot+1 (cid:12) (cid:12) O1:t = o1, . . . , ot) (cid:17) ϵ (3) Theorem 1 indicates that the scaling trends we observed in Section 3.1 are similar to those of spectral learning based predictions. Specifically, like our observations in Section 3.1, the prediction accuracy improves with more samples (i.e., larger t). The mixing rate, captured by 1λ2(A) , affects ICL and spectral learning similarly. This occurs because spectral parameter estimation from single trajectory degrades with 1λ2(A) the faster the HMM mixes, the smaller the estimation error. Finally, the effect of entropy is captured by the observability conditions in Theorem 1. Estimation error is maximized when the HMM is unobservable, which corresponds to maximum entropy rate. The relationship between entropy and HMM observability has been well studied in literature [28, 32]. 1 1 One of the practical limitations of spectral learning algorithm is the requirement of rank conditions of and B. Furthermore, the spectral learning algorithm is sensitive to the conditioning1 of the observed sequence, making its numerical performance robust only in limited settings. ICL by pretrained LLMs seems to handle such issues more gracefully, pointing to an intriguing gap in our statistical understanding for learning HMMs."
        },
        {
            "title": "4 Guidelines for Practitioners: How to (creatively) use LLMs for your data?",
            "content": "LLMs capacity in deciphering complex sequential patterns in language can be repurposed: as demonstrated in our synthetic experiments (Sections 2 and 3), pre-trained LLMs can effectively model HMM-generated sequences through ICL, achieving theoretically optimal prediction accuracy under favorable conditions. This section first translates these findings into practical guidelines for scientists, then demonstrates our observations on real-world data in animal behavior (Section 4.1). Guideline 1: LLM in-context learning as diagnostic tool for data structure and learnability. Our synthetic experiments (Sections 2.3 and 3.1) reveal that key HMM propertiesnotably entropy and mixing ratestrongly influence LLM ICL convergence behavior. Practitioners can leverage this relationship as diagnostic tool for their own sequential data. If you observe that an LLMs ICL prediction accuracy on your data sequence steadily improves and saturates with increased context length (like Figure 3), this strongly indicates learnable, non-random underlying structure. Our findings show that LLMs achieve near-optimal prediction accuracy on HMMs with clear, learnable patterns (Section 2.3). The characteristics of this convergence provide further insight: faster convergence and higher final accuracy in LLM ICL experiments are consistently associated with HMMs having lower entropy (less randomness) and faster mixing rates, as shown in Section 3.1 and Figure 4. Conversely, if LLM ICL on your data converges slowly, requires exceptionally long contexts, or plateaus at low accuracy, this suggests the underlying process has high entropy or slow mixing dynamicscharacteristics that inherently limit predictability and affect even optimal methods like Viterbi (Section 2.3). While calculating intrinsic HMM parameters from real-world data is challenging, you can qualitatively assess your datas learnability by comparing its ICL convergence profile to our synthetic HMM experiments (Figures 3 and 4). Guideline 2: LLMs are data efficient in giving accurate next observation prediction in-context. Pre-trained LLMs offer remarkably data-efficient and accessible approach for next-observation prediction through ICL, particularly valuable when rapid insights are needed or when data for training bespoke models is scarce. Our analyses (Section 3.2) show that LLM ICL achieves strong predictive performance with fewer domain-specific assumptions than Baum-Welch (which faces non-convexity 1This issue should not be insurmountable, similar to how (appropriately tuned) regularization can overcome poor conditioning in ridge regression [37]. However, we are unaware of prior work which provides solution. Figure 6: IBL dataset mice decision-making task. (Left) GLM-HMM model developed by neuroscientists. (Middle) cartoon illustration of the task. mouse observes visual stimulus presented on one side of screen, with one of six possible intensity levels. It then chooses side, receiving water reward if the choice matches the stimulus location. (Right) LLM ICL performance curve averaged across all animals, with 1-σ error bar. Its prediction accuracy steadily increase with longer context window, exceeding the domain-specific model performance. issues) and fewer training resources than specialized sequence models like LSTMs/RNNs (which require substantial data and careful tuning). LLMs therefore deliver immediate predictive capabilities with stable performance on limited data. key practical advantage of LLM ICL is accessibility: while traditional methods require substantial computational expertise, applying pre-trained LLMs simply involves formatting data as text prompts, dramatically lowering barriers to sequence analysis. We are not positioning LLM ICL as universal replacement for meticulously tuned, domain-specific models. Rather, its strength lies in providing strong, often surprisingly near-optimal predictions (Section 2.3) without any task-specific parameter updates or fine-tuning. Our key observation is that general-purpose LLMs can effectively model HMM-generated sequences and real-world scientific data tasks for which they were not explicitly pre-trained on. This highlights vast untapped potential and suggests that future LLM ICL development could yield transformative scientific tools. 4.1 Real World Examples We extend our synthetic HMM findings to real-world biological decision processes, focusing on two extensively studied behavioral neuroscience datasets. Understanding how animals make decisions and learn efficiently remains fundamental challenge, with researchers investing tremendous effort in high-precision modeling to capture underlying cognitive mechanisms. These datasets serve as ideal testbeds given the neuroscience communitys modeling efforts and the inherent connections between agentic decision-making and HMMs (Figure 6). We represent animal decisions as discrete token sequences and compare LLM ICL performance against established domain-specific models in predicting future actions. Decision-making Mice Dataset: This dataset, developed by the International Brain Laboratory (IBL) [25], has gained significant traction for studying mouse behavior within the neuroscience community. popular study [3] characterizes mice choice behavior as an interplay among multiple interleaved strategies governed by hidden states in HMM. Their GLM-HMM model (Figure 6 left) achieved an average prediction accuracy of 82.2%, outperforming standard approaches like the classic lapse model by 2.8%. For scientists investigating animal decision-making, these performance improvements are significant for advancing model fidelity and experimental interpretation. We compare GLM-HMM to in-context LLMs on data from 7 mice, following the descriptions in Ashwood et al. [3]. The experimental data consists of three components: stimulus, choice, and reward. For each trial, the mouse perceives visual stimulus presented to their left or right, makes choice by turning steering wheel, and receives water drop as reward when correct (Figure 6 middle). Each mouse is described by one sequence, composed of trials. The trials are ordered sequentially as they occurred during experiments. Remarkably, when provided with context of more than 1000 trials, LLM ICL consistently achieved higher prediction accuracy (average of 86.2%) than the expert-developed GLM-HMM (Figure 6 right). More importantly, the convergence trend of LLM ICL mirrors the in-context scaling we observed in synthetic experiments, particularly when entropy is relatively low. This observation suggests that 8 Figure 7: Rat reward-learning task. (Left) Analog agent learning to HMMs. (Middle) cartoon illustration of the more challenging task. No stimulus is presented on either side; instead, the reward probabilities for left and right choices evolve independently via random walks. As the optimal choice changes over time, the rat must learn and adapt its decisions based solely on the history of past rewards. (Right) LLM ICL performance curve averaged across all animals, with 1-σ error bar. Its performance curve improves only marginally with increasing context length. mouse decision-making processes contain learnable structures that LLMs, even without task-specific training, can effectively identify and leverage for prediction. Reward-learning Rats Dataset: The dataset from Miller et al. [36] allows us to explore LLM ICL capabilities on more complex learning behaviors. This task presents significantly greater challenge than the IBL dataset for two primary reasons: first, animals receive no explicit stimuli to guide their choices towards potential rewards; secondly, the dataset captures the entire dynamic learning process itself, rather than behavior after learning. Consequently, the underlying behavioral dynamics are expected to be more complex and less stationary. To benchmark LLM ICL in this scenario, we compare its performance against state-of-the-art model from recent work [10], which employed code generation and evolutionary search to discover interpretable symbolic programs well-fitted to this dataset. It is important to note that this state-of-the-art model results from an extensive, computationally intensive evolutionary search, setting very high performance bar. As shown in Figure 7, the prediction accuracy of LLM ICL on this dataset improves only marginally with increasing context length. This limited improvement parallels the ICL behavior we observed for synthetic HMMs characterized by high entropy and slow mixing rates. LLM ICL exhibits substantial performance gap when compared to the specialized model. This outcome is consistent with our hypothesis that the underlying dynamics of this naturalistic learning process are complex, potentially pushing the limits of what current off-the-shelf LLMs can capture through ICL alone."
        },
        {
            "title": "5 Related Works",
            "content": "LLMs and In-Context Learning. The surprising ability of LLMs to perform ICL [7] has led to significant interest in understanding its underlying mechanisms [11, 22, 53]. Several works [20, 52, 55] interpret ICL as implicit Bayesian inference, suggesting that LLMs naturally perform posterior updates through attention mechanisms. Theoretical works analyzing transformers ability to model Markovian data [6, 13, 33, 40, 41] show that they can efficiently learn fully observed Markov chains. However, the transition from observable Markov sequences to latent-variable models like HMMs remains underexplored. Recent studies also evaluate LLMs predictive performance on structured tasks, including dynamical systems [29], density estimation [30], and time series forecasting [19, 49]. While these works explore LLMs empirical capabilities, they do not systematically analyze how intrinsic properties of underlying stochastic processessuch as mixing time and entropyaffect ICL performance. Our work fills this gap by providing controlled study on synthetic HMMs and offering theoretical conjectures for the observed scaling trends in ICL performance. Spectral learning (SL) HMMs. SL algorithms have emerged as compelling tools for learning HMMs from observations, using method-of-moments to learn the spectral parameters. Several works [2, 21, 42] construct matrices with observations, perform singular value decomposition and projection to obtain the beliefs of the HMM operators. Recent work [31] improves the practicality of SL by projecting the probability beliefs onto simplex after every belief update. Despite these, SL algorithms have practical limitations and make assumptions on how observations carry information about the 9 HMM dynamics. ICL seems to handle such issues by learning better observation operators without requiring the limiting assumptions of SL algorithms. Neuroscience and Animal Behavior. Many neuroscience studies model animal behavior as HMMs [47, 50]. common modern approach involves training data-specific RNNs and finding attractor dynamics [4, 23, 54], which can be highly data-inefficient. Large generative models have accelerated neuroscience discoveries, from data processing [44, 46] to model discovery [10]. In our work, we present novel approach, leveraging the frontier of AI to help scientists understand their data, focusing on discrete behaviors [36, 43]."
        },
        {
            "title": "6 Discussion & Takeaways",
            "content": "LLMs are surprisingly effective HMM learners through in-context learning: We observe that LLM prediction accuracy often converges towards theoretical optimum achieved by the Viterbi algorithm, which knows true model parameters. Contrasting with iterative and computationally intensive traditional HMM estimation algorithms or neural architectures (e.g., LSTMs), LLM ICL offers simplicity as tool. As demonstrated by the competitive performance to domain-specific models on real-world animal decision-making tasks, LLM ICL offers new avenue for rapid data exploration. LLMs can serve as zero-shot statistical tool, enabling scientists to diagnose data complexity and generate future predictions without the overhead of extensive model developmentaddressing common bottleneck in many scientific workflows. Existing gaps: While we observe promising trends, our experiments also point to existing gaps in the broad application of LLM ICL. primary bottleneck is the reliance on discrete tokenization, which poses challenges for modeling continuous, real-valued, or high-dimensional observationssuch as neural recordingswithin the ICL framework. Although our experiments successfully employed tokenization strategies for discrete sequences (see Appendix E.2 for ablations), adapting LLMs to handle continuous state-space models or direct real-valued inputs remains an open question. Moreover, despite achieving high predictive accuracy, the inherently black-box nature of LLMs limits interpretability. While our findings demonstrate that LLMs can effectively model HMM dynamics, extracting explicit and interpretable parameterssuch as transition or emission probabilitiesfrom the models internal representations is nontrivial. Yet, such interpretability is often central to the goals of scientists and practitioners seeking to understand underlying system dynamics. We hope this work lays the groundwork for future research into extending ICL to continuous domains and developing tools for extracting interpretable structure from LLMs. Call to action: Realizing the full potential of LLMs and HMMs to advance our understanding of complex systems demands multidisciplinary effort. There is growing need for next-generation foundation models specifically designed to meet the challenges of scientific dataranging from structured sequences to high-dimensional, continuous signals. Moving beyond adaptations of NLPfocused models, such advancements are critical not only for enabling more effective scientific analysis, but also for deepening our understanding of in-context learning and the structure embedded within human language corpora. Ultimately, this progress will be essential to unlocking the transformative potential of LLMs in scientific discovery across broad range of disciplines."
        },
        {
            "title": "Acknowledgements",
            "content": "YD thanks Kristin Branson and Kimberly Stachenfeld for insightful technical discussions, and Owen Oertell for their support. ZG is supported by LinkedIn through the LinkedInCornell Grant. This work was partly funded by NSF CCF 2312774, NSF OAC-2311521, gift to the LinkedIn-Cornell Bowers CIS Strategic Partnership, and an AI2050 Early Career Fellowship program at Schmidt Science."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Animashree Anandkumar, Daniel Hsu, and Sham Kakade. method of moments for mixture models and hidden markov models. In Conference on learning theory, pages 331. JMLR Workshop and Conference Proceedings, 2012. [3] Zoe Ashwood, Nicholas Roy, Iris Stone, International Brain Laboratory, Anne Urai, Anne Churchland, Alexandre Pouget, and Jonathan Pillow. Mice alternate between discrete strategies during perceptual decision-making. Nature Neuroscience, 25(2):201212, 2022. [4] Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin Chadwick, Thomas Degris, Joseph Modayil, et al. Vectorbased navigation using grid-like representations in artificial agents. Nature, 557(7705):429433, 2018. [5] Leonard Baum, Ted Petrie, George Soules, and Norman Weiss. maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. The annals of mathematical statistics, 41(1):164171, 1970. [6] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of transformer: memory viewpoint. Advances in Neural Information Processing Systems, 36: 15601588, 2023. [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [8] Olivier Cappé, Eric Moulines, and Tobias Rydén. Inference in Hidden Markov Models. Springer Series in Statistics. Springer, New York, NY, 1st edition, 2005. ISBN 978-0-387-40264-2. doi: 10.1007/0-387-28982-8. [9] George Casella and Edward I. George. Explaining the gibbs sampler, 1992. [10] Pablo Samuel Castro, Nenad Tomasev, Ankit Anand, Navodita Sharma, Rishika Mohanta, Aparna Dev, Kuba Perlin, Siddhant Jain, Kyle Levin, Noémi Élteto, Will Dabney, Alexander Novikov, Glenn Turner, Maria Eckstein, Nathaniel Daw, Kevin Miller, and Kimberly Stachenfeld. Discovering symbolic cognitive models from human and animal behavior. bioRxiv, 2025. doi: 10.1101/2025.02.05.636732. URL https://www.biorxiv. org/content/early/2025/02/06/2025.02.05.636732. [11] Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers, 2022. URL https://arxiv.org/abs/2205. 05055. [12] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954. [13] Benjamin L. Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: In-context learning markov chains, 2024. URL https: //arxiv.org/abs/2402.11004. [14] Y. Ephraim and N. Merhav. Hidden markov processes. IEEE Transactions on Information Theory, 48(6):15181569, 2002. doi: 10.1109/TIT.2002.1003838. [15] Robert Gallager. Discrete stochastic processes. Journal of the Operational Research Society, 48(1):103103, 1997. [16] Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Transactions on pattern analysis and machine intelligence, (6): 721741, 1984. 11 [17] Richard Glennie, Timo Adam, Vianey Leos-Barajas, Théo Michelot, Theoni Photopoulou, and Brett McClintock. Hidden markov models: Pitfalls and opportunities in ecology. Methods in Ecology and Evolution, 14(1):4356, 2023. [18] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [19] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters, 2024. URL https://arxiv.org/abs/2310.07820. [20] Ritwik Gupta, Rodolfo Corona, Jiaxin Ge, Eric Wang, Dan Klein, Trevor Darrell, and David Chan. Enough coin flips can make llms act bayesian. arXiv preprint arXiv:2503.04722, 2025. [21] Daniel Hsu, Sham Kakade, and Tong Zhang. spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences, 78(5):14601480, 2012. [22] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Do llms dream of elephants (when told not to)? latent concept association and associative memory in transformers, 2024. URL https://arxiv.org/abs/2406.18400. [23] Michael I. Jordan. Attractor dynamics and parallelism in connectionist sequential machine, page 112127. IEEE Press, 1990. ISBN 0818620153. [24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. [25] The International Brain Laboratory, Valeria Aguillon-Rodriguez, Dora Angelaki, Hannah Bayer, Niccolo Bonacchi, Matteo Carandini, Fanny Cazettes, Gaelle Chapuis, Anne Churchland, Yang Dan, Eric Dewitt, Mayo Faulkner, Hamish Forrest, Laura Haetzel, Michael Häusser, Sonja Hofer, Fei Hu, Anup Khanal, Christopher Krasniak, Ines Laranjeira, Zachary Mainen, Guido Meijer, Nathaniel Miska, Thomas Mrsic-Flogel, Masayoshi Murakami, Jean-Paul Noel, Alejandro Pan-Vazquez, Cyrille Rossant, Joshua Sanders, Karolina Socha, Rebecca Terry, Anne Urai, Hernando Vergara, Miles Wells, Christian Wilson, Ilana Witten, Lauren Wool, and Anthony Zador. Standardized and reproducible measurement of decision-making in mice. eLife, 10:e63711, may 2021. ISSN 2050-084X. doi: 10.7554/eLife.63711. URL https://doi.org/10.7554/eLife.63711. [26] David Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017. [27] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International conference on machine learning, pages 1956519594. PMLR, 2023. [28] Andrew Liu and Robert Bitmead. Observability and reconstructibility of hidden markov models: Implications for control and network congestion control. In 49th IEEE Conference on Decision and Control (CDC), pages 918923. IEEE, 2010. [29] Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, and Christopher J. Earls. Llms learn governing principles of dynamical systems, revealing an in-context neural scaling law, 2024. URL https://arxiv.org/abs/2402.00795. [30] Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, and Christopher J. Earls. Density estimation with llms: geometric investigation of in-context learning trajectories, 2025. URL https: //arxiv.org/abs/2410.05218. [31] Xiaoyuan Ma and Jordan Rodu. Bridging the usability gap: Theoretical and methodological advances for spectral learning of hidden markov models. arXiv preprint arXiv:2302.07437, 2023. 12 [32] John Mahoney, Christopher Ellison, Ryan James, and James Crutchfield. How hidden are hidden processes? primer on crypticity and entropy convergence. Chaos: An Interdisciplinary Journal of Nonlinear Science, 21(3), 2011. [33] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with markov: framework for principled analysis of transformers via markov chains, 2024. URL https://arxiv.org/abs/2402.04161. [34] Brett McClintock, Roland Langrock, Olivier Gimenez, Emmanuelle Cam, David Borchers, Richard Glennie, and Toby Patterson. Uncovering ecological state dynamics with hidden markov models. Ecology letters, 23(12):18781903, 2020. [35] Florence Merlevède, Magda Peligrad, and Emmanuel Rio. Bernstein inequality and moderate deviations under strong mixing conditions. In High dimensional probability V: the Luminy volume, volume 5, pages 273293. Institute of Mathematical Statistics, 2009. [36] Kevin J. Miller, Matthew M. Botvinick, and Carlos D. Brody. From predictive models to cognitive models: Separable behavioral processes underlying reward learning in the rat. bioRxiv, 2021. doi: 10.1101/461129. URL https://www.biorxiv.org/content/early/2021/02/ 19/461129. [37] Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can mitigate double descent. arXiv preprint arXiv:2003.01897, 2020. [38] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. [39] Lawrence Rabiner. tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257286, 1989. [40] Nived Rajaraman, Marco Bondaschi, Kannan Ramchandran, Michael Gastpar, and Ashok Vardhan Makkuva. Transformers on markov data: Constant depth suffices, 2024. URL https: //arxiv.org/abs/2407.17686. [41] Nived Rajaraman, Jiantao Jiao, and Kannan Ramchandran. An analysis of tokenization: Transformers under markov data. Advances in Neural Information Processing Systems, 37:62503 62556, 2024. [42] Jordan Rodu. Spectral estimation of hidden Markov models. University of Pennsylvania, 2014. [43] Matthew Rosenberg, Tony Zhang, Pietro Perona, and Markus Meister. Mice in labyrinth show rapid learning, sudden insight, and efficient exploration. eLife, 10:e66175, jul 2021. ISSN 2050-084X. doi: 10.7554/eLife.66175. URL https://doi.org/10.7554/eLife.66175. [44] Cristina Segalin, Jalani Williams, Tomomi Karigo, May Hui, Moriel Zelikowsky, Jennifer Sun, Pietro Perona, David Anderson, and Ann Kennedy. The mouse action recognition system (mars) software pipeline for automated analysis of social behaviors in mice. eLife, 10:e63720, nov 2021. ISSN 2050-084X. doi: 10.7554/eLife.63720. URL https://doi.org/10.7554/ eLife.63720. [45] Max Simchowitz, Karan Singh, and Elad Hazan. Improper learning for non-stochastic control. In Conference on Learning Theory, pages 33203436. PMLR, 2020. [46] Jennifer J. Sun, Ann Kennedy, Eric Zhan, David J. Anderson, Yisong Yue, and Pietro Perona. Task programming: Learning data efficient behavior representations, 2021. URL https: //arxiv.org/abs/2011.13917. [47] Weinan Sun, Johan Winnubst, Maanasa Natrajan, Chongxi Lai, Koichiro Kajikawa, Michalis Michaelos, Rachel Gattoni, James E. Fitzgerald, and Nelson Spruston. Learning produces hippocampal cognitive map in the form of an orthogonalized state machine. bioRxiv, 2023. doi: 10.1101/2023.08.03.551900. URL https://www.biorxiv.org/content/early/2023/ 08/07/2023.08.03.551900. 13 [48] Atika Syeda, Lin Zhong, Renee Tung, Will Long, Marius Pachitariu, and Carsen Stringer. Facemap: framework for modeling neural activity based on orofacial tracking. Nature neuroscience, 27(1):187195, 2024. [49] Mingtian Tan, Mike Merrill, Vinayak Gupta, Tim Althoff, and Tom Hartvigsen. Are language models actually useful for time series forecasting? Advances in Neural Information Processing Systems, 37:6016260191, 2024. [50] Diego Vidaurre, Laurence Hunt, Andrew J. Quinn, Benjamin A.E. Hunt, Matthew J. Brookes, Anna C. Nobre, and Mark W. Woolrich. Spontaneous cortical activity transiently organises into frequency specific phase-coupling networks. bioRxiv, 2017. doi: 10.1101/150607. URL https://www.biorxiv.org/content/early/2017/10/20/150607. [51] A. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, 13(2):260269, 1967. doi: 10.1109/TIT. 1967.1054010. [52] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning, 2024. URL https://arxiv.org/abs/2301.11916. [53] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. Larger language models do in-context learning differently, 2023. URL https://arxiv.org/abs/2303.03846. [54] Thomas J. Wills, Colin Lever, Francesca Cacucci, Neil Burgess, and John OKeefe. Attractor dynamics in the hippocampal representation of the local environment. Science, 308:873 876, 2005. URL https://api.semanticscholar.org/CorpusID:13909368. [55] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of incontext learning as implicit bayesian inference, 2022. URL https://arxiv.org/abs/2111. 02080. [56] Fanny Yang, Sivaraman Balakrishnan, and Martin J. Wainwright. Statistical and computational guarantees for the baum-welch algorithm, 2015. URL https://arxiv.org/abs/1512. 08269. [57] Walter Zucchini and Peter Guttorp. hidden markov model for space-time precipitation. Water Resources Research, 27(8):19171923, 1991."
        },
        {
            "title": "Table of Contents",
            "content": "Appendix A: Additional Background on HMMs Appendix B: Additional Details of Experimental Setup Appendix C: Details of Benchmark Models Appendix D: Additional Synthetic Experiment Results Appendix E: Ablations on LLMs Appendix F: Spectral Learning HMMs for Prediction Task Appendix G: Additional Real World Experiments"
        },
        {
            "title": "A Additional Background on HMMs",
            "content": "In this section, we define in detail the HMM settings we are interested in, including the conditions for Markov chains to converge to unique stationary distributions. Recall that HMM is characterized by the Markov chains initial state distribution and its state transitions, along with the emission probabilities of an observation given the hidden state. With finitely many states and observations, without loss of generality, states take values in = {1, 2, . . . , } while observations take values in = {1, 2, . . . , L}. The initial state distribution is denoted as π RM with πj the probability of starting in state j, the state transitions are describe by the matrix RM with elements aij the probability of transitioning to state from state i, and the emission matrix RM contains bjl the probability of observing when in hidden state j. The triple λ = (π, A, B) completely parameterizes finite-alphabet HMM. Let {X1, X2, ...} denote discrete-time Markov chain taking values in with transition matrix A. Let p(n) ij = P(Xt+n = jXt = i) denote the n-step transition probability between states i, . State is said to be accessible from state if there exists an integer 1 such that p(n) ij > 0. subset is called irreducible if every pair of states i, is mutually accessible. The period of state is defined as c(i) = gcd{n 1 : p(n) ii > 0}, the greatest common divisor of all possible return times. State is aperiodic if c(i) = 1. Markov chain is termed geometrically ergodic if it is irreducible and aperiodic, which guarantees convergence to unique stationary distribution µ RM satisfying µ = µA. The mixing rate ρ [0, 1) is such that for all states i, , there exists constant 0 for which p(n) ij µj Cρn for all 1. For finite-alphabet HMM, ρ equals λ2, the second-largest eigenvalue of A. We run experiments on few non-ergodic cases, while the majority of HMMs are with ergodic state transitions to avoid dependence on the initial state. The entropy H(X) of discrete random variable is defined as H(X) = (cid:80) xX p(x) log p(x). fundamental property of entropy is that conditioning reduces uncertainty: for any two random variables and , we have H(XY ) H(X), with equality holding if and only if and are statistically independent [12]. By applying the chain rule of entropy, the joint entropy of stochastic process can be expressed as H(X1, X2, . . . , Xn) = (cid:80)n i=1 H(XiXi1, . . . , X1). For Markov chain with stationary distribution µ, the entropy rate is defined as H(X ) = i,j µiaij log aij, which depends solely on the transition limn matrix A. We additionally define the entropy of the emission matrix as (cid:80) j,l µjbjl log bjl, which quantifies the average uncertainty in observations given the underlying states. Although the entropy rate of the observation process in HMM has no known closed-form expression, it can be bounded as H(OnOn1, Xn1, . . . , O1, X1) H(O) H(OnOn1, . . . , O1). As defines transitions from Xt to Xt+1, and determines sampling Ot from Xt, the entropies of and combined help us to control the entropy lower bound of the sampled HMM sequence. H(X1, X2, . . . , Xn) = (cid:80)"
        },
        {
            "title": "B Additional Details of Experimental Setup",
            "content": "Construct with specific mixing rate, entropy, and steady state distribution. For an ergodic Markov chain that converges to unique stationary distribution, the stochastic matrix can be decomposed into eigenvalues and eigenvectors with the ordering shown in Figure 8, where 1 RM is vector of ones, λ2 is the second-largest eigenvalue of A, and µ is the stationary distribution [15]. We leverage this decomposition to construct with predefined λ2 and µ. To determine the remaining eigenvalues and eigenvectors, we formulate an optimization problem based on the following requirements: (1) all entries of are non-negative; (2) each row of sums to 1; (3) U1U = 1; and (4) all remaining eigenvalues have magnitudes not exceeding λ2. The optimization problem has the following form, where we translate the constraints above into penality terms. min λ3:M ,V2 (cid:88) i,j=1 max{aij, 0} + (cid:16)(cid:16)(cid:80)M i=1 aij (cid:17) (cid:17)2 1 + (cid:88) j=1 (cid:88) i,j=1 (VU I)2 ij + (cid:88) i=3 max{λi λ2, 0} s.t. = Vdiag(1, λ2, λ3, ..., λM )U, = [1 V2] , = (cid:21) (cid:20) µ 2 This is nonconvex problem, which we solve using first order methods with pytorch. We randomly initialize the free variables λ3, ..., λM and V2 and then run 5000 iterations of Adam with step size 0.01 and default values for other parameters. After the optimizer terminates, we reject instances which do not satisfy the constraints exactly. By initializing with multiple random seeds, we generate matrices spanning the desired entropy spectrum. Figure 8: The singular value decomposition of ergodic unichain Markov matrix A. The darker shaded region is pre-defined for our controlled experiments. The lighter shaded region is randomly initialized and calculated using neural network. Steady state distribution. We construct steady state distributions with varying skewness using the Beta distribution with α = 1 and different values of β. When α = 1 and β = 1, the resulting steady state distribution is uniform. As β increases, the distribution becomes increasingly skewed toward smaller state indices. Unless otherwise specified (Appendix D.3), we use uniform steady state distribution as the default configuration. Entropy for visualizations. The entropy definitions H(A) and H(B, µ) we introduced in Section 2.1 are used for constructing HMM parameters and sampling trajectories. For graphing Figure 4 (Left), we define normalized entropy considering both matrices: H(A, B, µ) = H(A) + H(B, µ) log + log . We define H(A) = H(A)/ log and H(B, µ) = H(B, µ)/ log for Figure 4 (Middle) and (Right). is when LLM converges to Viterbi. The concept of convergence, though intuitive to human eyes, requires specific numerical definition for plots like Figure 4. We define convergence as the point where two conditions are simultaneously satisfied: (i) the accuracy difference between Viterbi and LLM is within 0.025, and (ii) LLM achieves at least 95% of Viterbis accuracy. We use both constant and relative thresholds to ensure strict convergence definition that accounts for different baseline performance levels across experimental conditions. Hellinger Distance. For two discrete probability distributions P, RL, the Hellinger distance is defined as DHellinger(P, Q) = 1 2 (cid:88) (cid:112) ( Pi (cid:112)Qi)2. i="
        },
        {
            "title": "C Details of Benchmark Models",
            "content": "In this section, we provide descriptions and pseudocode for the benchmark models we use  (Table 1)  . The executable code for all methods are included in supplemental materials. Algorithm 1: Viterbi Algorithm Input: States = {1, 2, . . . , }, initial distribution µ, transition matrix A, emission matrix B, and observation sequence {o1, . . . , oT }. Output: Most likely state sequence path = {x1, . . . , xT } Initialization: P[0][s] µ[s] B[s][o1] for all ; Forward recursion: for = 1 to 1 do for do P[t][s] max rX {P[t 1][r] A[r][s] B[s][ot]}; Q[t][s] arg max rX {P[t 1][r] A[r][s] B[s][ot]}; end end Backtracking: path[T 1] arg max sX P[T 1][s]; path[t] Q[t + 1][path[t + 1]] for = 2, . . . , 0; return path Viterbi algorithm. The Viterbi algorithm is dynamic programming technique for efficiently finding the most likely sequence of hidden states in Markov model, given sequence of observations. It iteratively computes the highest probability path to each state at time by considering all possible predecessor states at time 1, their transition probabilities, and the emission probabilities of the current observation. Rather than exhaustively evaluating all possible state sequences, Viterbi maintains only the most promising paths at each time step, storing both their probabilities and the penultimate states that maximize these probabilities. After computing probabilities for all time steps, the algorithm traces backward from the most probable final state to reconstruct the optimal state sequence. We use the most probable final state and ground-truth and to calculate the prediction distribution of the next observation. Algorithm 2: Compute (Ot+1Otk:t) Input: States = {1, 2, . . . , }, initial distribution µ, transition matrix A, emission matrix B, and observation sequence {otk, . . . , ot}. Output: Probability of next observation (ot+1otk:t) Forward pass over observation window: αtk[s] B[s][otk] µ[s] for all ; αi[s] B[s][oi] (cid:80) A[r][s] αi1[r] for = + 1, . . . , t, ; rX Normalize to get posterior: (sotk:t) αt[s] Prediction step: (sotk:t) (cid:80) Marginalize over states: (ot+1otk:t) (cid:80) return (ot+1otk:t) (cid:80) rX A[r][s] (rotk:t) for all ; sX B[s][ot+1] (sotk:t); X αt[s] for all ; Optimal inference with truncated memory (Ot+1Otk:t). The forward-based prediction algorithm computes the probability of the next observation in hidden Markov model by using three-step approach. First, it calculates the posterior distribution over current hidden states via the forward algorithm, recursively processing the observation window while accounting for transitions and emissions. Second, it projects this belief state forward by applying the transition matrix to compute the distribution over next possible states. Finally, it determines (ot+1otk:t) by marginalizing over all possible next states, weighting each by its emission probability. Baum-Welch algorithm. The Baum-Welch algorithm is an expectation-maximization method for estimating hidden Markov model parameters. It iteratively alternates between computing state posteriors γt(s) and transition posteriors ξt(s, r) via forward-backward recursion (E-step), and updating parameters to maximize likelihood (M-step): setting the initial distribution to γ1, transition 17 Algorithm 3: Baum-Welch Algorithm Input: States = {1, 2, . . . , }, observations = {1, 2, . . . , L}, observation sequence {o1, . . . , oT }, initial parameters µ(0), A(0), B(0), and threshold ϵ. Output: Refined parameters µ, A, Initialize: µ µ(0), A(0), B(0), Lprev ; repeat A[r][s] αt1[r] for = 2, . . . , , ; Lprev L; E-Step: Forward pass: α1[s] B[s][o1] µ[s] for all ; αt[s] B[s][ot] (cid:80) (cid:80) αT [s]; Backward pass: βT [s] 1 for all ; βt[s] (cid:80) Expected counts: γt[s] αt[s]βt[s] ξt[s][r] αt[s]A[s][r]B[r][ot+1]βt+1[r] M-Step: µ[s] γ1[s] for all ; t=1 ξt[s][r] A[s][r] t=1 γt[s] t=1 γt[s]1(ot=v) t=1 γt[s] for all , Y; for all s, ; B[s][v] (cid:80)T 1 (cid:80)T 1 (cid:80)T r A[s][r] B[r][ot+1] βt+1[r] for = 1, . . . , 1, ; for = 1, . . . , , ; for = 1, . . . , 1, s, ; (cid:80)T until Lprev < ϵ; return µ, A, probabilities to normalized expected transitions, and emission probabilities to normalized observation counts per state. This process continues until the log-likelihood converges, yielding locally optimal parameters that maximize the probability of generating the observed sequence. We use the learned parameters to predict next observation similar to the Viterbi algorithm. Algorithm 4: n-gram Based Next-Observation Prediction Input: Observation sequence = {o1, . . . , oT }, context length 1, smoothing parameter δ Output: n-gram model for predicting (otot(n1):t1) Count extraction: countsn, countsn1 empty associative arrays; for = 1 to 1 do context [ot(n1), . . . , ot1]; Increment countsn[context ot] and countsn1[context]; end Model construction with smoothing: number of unique symbols in O; for each observed context in countsn1 do for each unique observation in do model[c, o] countsn[co]+δ δ+countsn1[c] ; end end Back-off for unseen contexts: Punif (o) 1 for all o; (cid:26)model[[ot(n1), . . . , ot1], ot], (otot(n1):t1) return model Punif (ot), if context observed otherwise ; n-gram. n-gram models provide an elegant, computationally efficient framework for next-observation prediction in Markov chain processes by directly estimating conditional probabilities from observed sequences. These models embody the Markov assumption that (Ot+1O1:t) (Ot+1Otn+2:t), making them particularly effective for stochastic processes where future states depend only on limited history of previous states. For first-order Markov chains, bigram models (n = 2) precisely capture the underlying transition dynamics, while higher-order dependencies can be modeled by increasing n. 18 Algorithm 5: LSTM for Single Sequence Prediction Input: Observation sequence = {o1, . . . , oT }, vocabulary size , embedding dimension d, hidden dimension h, layers L, learning rate α, epochs Output: Trained LSTM model for (ot+1o1:t) Architecture: Initialize embedding layer: Embedding : Rd; Initialize LSTM layers: LSTM : Rd Rh with layers; Initialize output projection: Linear : Rh RV ; Initialize optimizer with learning rate α; Training: for epoch = 1 to do for = 1 to 1 do ; O1:t Ot+1 Update model to maximize (yx) via gradient descent; // Use all previous observations as context // Next observation as target ; end end Inference: For prefix O1:t, compute (ot+1O1:t) = softmax(model(O1:t)); return model RNN LSTM. LSTM networks are specialized recurrent neural architectures designed to model sequential data through memory cells regulated by input, forget, and output gates. These gates control information flow, allowing LSTMs to selectively retain relevant historical patterns while discarding irrelevant information. LSTMs excel at next-observation prediction tasks by capturing both short-term correlations and long-term dependencies in the observation history. The network processes window of prior observations sequentially, updating its hidden state to encode temporal patterns, then projects this state through softmax layer to generate probability distribution over possible next observationsmaking LSTMs particularly effective for forecasting future values in time series where the prediction depends on complex patterns spanning multiple time scales."
        },
        {
            "title": "D Additional Synthetic Experiment Results",
            "content": "This section presents additional results from synthetic experiments. All methods are evaluated using the average performance over 4,096 sequences, with the exception of LSTM, which is evaluated on 16 sequences due to its high computational cost. Consequently, the LSTM results exhibit higher variance. Nonetheless, in metrics such as Hellinger distancewhich account for the full output distribution rather than relying solely on the argmax for accuracyLSTM underperforms compared to the LLM most of the time. D.1 Varying Entropy of In this section, we present detailed results on varying the entropy of matrix over 4/8/16 states and emissions, reporting accuracies and Hellinger distances. Figure 9: Accuracies of six methods across different entropy, entropy, number of states, and number of emissions with λ2 = 0.75 and uniform steady state distribution. 20 Figure 10: Hellinger distances of six methods across different entropy, entropy, number of states, and number of emissions with λ2 = 0.75 and uniform steady state distribution. 21 D.2 Varying Mixing Rate of In this section, we present detailed results on varying the mixing rate (λ2) of matrix over 4/8/16 states and emissions, reporting accuracies and Hellinger distances. Figure 11: Accuracies of six methods across different mixing rates (λ2), entropy, number of states, and number of emissions with uniform steady state distribution and (1, 2, 3) entropy for (4, 8, 16) states respectively. 22 Figure 12: Hellinger distances of six methods across different mixing rates (λ2), entropy, number of states, and number of emissions with uniform steady state distribution and (1, 2, 3) entropy for (4, 8, 16) states respectively. 23 D.3 Varying Steady State Distribution of In this section, we present detailed results on varying the steady state distributions of matrix over 4/8/16 states and emissions, reporting accuracies and Hellinger distances. We construct steady states with different skewness using Beta distribution with α = 1. Notably, with α = 1 and β = 1, the steady state distribution is uniform. As we increase β, the distribution becomes more skewed. We test with β = 1, 2, 3, representing uniform, skewed, and very skewed respectively. Figure 13: Accuracies of six methods across different steady state distributions, entropy, number of states, and number of emissions with (0, 0.5, 2) entropy for (4, 8, 16) states respectively and (0.99, 0.95, 0.75) λ2 for (4, 8, 16) states respectively. 24 Figure 14: Hellinger distances of six methods across different steady state distributions, entropy, number of states, and number of emissions with (0, 0.5, 2) entropy for (4, 8, 16) states respectively and (0.99, 0.95, 0.75) λ2 for (4, 8, 16) states respectively. 25 D.4 Discussions When LLMs fail to converge. While LLMs converge to Viterbi performance efficiently under most HMM parameter settings (scaling trends summarized in Section 3.1), we identify two conditions where convergence fails or proceeds exceptionally slowly. First, when entropy of or is approaches its maximum (log or log respectively), the prediction accuracy gap ε at context length 2048 remains substantial. For instance, in the last row of Figure 9 with = 16 and the entropy of is 3.5 (near the maximum of log 16 = 4), the LLM (Qwen2.5-7B) exhibits gradual convergence with persistent gap. Second, when mixing is slow (λ2 approaches 1), such as in the third-to-last row of Figure 11 with = 16 and λ2 = 0.95, performance gap persists even at maximum context length. Importantly, the Viterbi algorithm also struggles under these challenging conditions. Under high entropy, as shown in the last row of Figure 9, Viterbi accuracy barely exceeds random prediction (0.25/0.125/0.0625 for = 4/8/16). Under slow mixing, such as in the fourth row of Figure 11 with = 8 and λ2 = 0.99, Viterbi algorithm requires context length 512 to achieve peak performance. These results demonstrate that LLM performance degradation under high entropy and slow mixing conditions reflects fundamental limits of stochastic system learnabilityarising from random dynamics and long-range dependenciesthat affect even optimal inference methods. Monotonicity of LLM performance with respect to context length. We observe that LLM performance almost always improves monotonically with longer context lengtha property notably absent in other learning baselines. Even excluding LSTM from this comparison (due to high variance from averaging over fewer sequences, as discussed in the first paragraph in Appendix D), both BaumWelch and bigram models lack monotonic convergence behavior. For Baum-Welch, the accuracy graphs (Figures 9, 11, and 13) reveal multiple cases where performance dips and recovers, or deteriorates as context length increases. The Hellinger distance graphs (Figures 10, 12, and 14) provide clearer evidence that both BW and bigram exhibit non-monotonic learning patterns. In most cases, LLM Hellinger distance decreases monotonically, while BW and bigram display erratic behavior: sometimes experiencing early-context bumps, other times starting very close to the ground truth emission distribution (occasionally even closer than the oracle Viterbi by empirical chance) before gradually converging to statistically sound distributions. Importantly, when BW or bigram achieve lower Hellinger distances, this does not necessarily indicate better performancethe corresponding prediction accuracy graphs often show poor results, highlighting the distinction between distributional similarity and predictive capability. When (normalized) entropy is held constant, varying the number of states does not affect the LLM convergence rate. We provide concrete evidence for this claim in Figure 9, where rows 1, 3, and 6 all have the same normalized entropy H(A) = 0.5. Across each column, the Qwen2.5-7B convergence curves for these three rows exhibit nearly identical shapes, demonstrating that the convergence rate depends primarily on normalized entropy rather than absolute state space size. We emphasize that convergence rate differs from convergence targetthe Viterbi performance. While the rate of improvement remains consistent across different state space sizes (when normalized entropy is fixed), larger state spaces result in lower achievable prediction accuracy due to increased task difficulty."
        },
        {
            "title": "E Ablations on LLMs",
            "content": "In this section, we provide the results on the families, sizes, and tokenization of the LLMs. E.1 LLM Size We compare Qwen and Llama model families with seven different models. We found that their performances are similar, with slight degradation when the model size is small. Figure 15: Accuracies of seven models across different entropy, entropy, number of states, and number of emissions with λ2 = 0.75 and uniform steady state distribution. Lighter color represents smaller models. The two smallest models from each family have suboptimal performance. 27 Figure 16: Hellinger distances of seven models across different entropy, entropy, number of states, and number of emissions with λ2 = 0.75 and uniform steady state distribution. The models converge similarly, especially when entropy is high. 28 Figure 17: Accuracies of seven models across different mixing rates (λ2), entropy, number of states, and number of emissions with uniform steady state distribution and (1, 2, 3) entropy for (4, 8, 16) states respectively. The two smallest models from each family have suboptimal performance, especially when mixing is fast. 29 Figure 18: Hellinger distances of seven models across different mixing rates (λ2), entropy, number of states, and number of emissions with uniform steady state distribution and (1, 2, 3) entropy for (4, 8, 16) states respectively. The models converge similarly, especially when mixing is slow. 30 Figure 19: Accuracies of seven models across different steady state distributions, entropy, number of states, and number of emissions with (0, 0.5, 2) entropy for (4, 8, 16) states respectively and (0.99, 0.95, 0.75) λ2 for (4, 8, 16) states respectively. The poor performance observed in smaller models at short context length under low A&B entropy settings may be attributed to the filtering of repeated n-grams during pretraining, as discussed in Appendix E.2. 31 Figure 20: Hellinger distances of seven models across different steady state distributions, entropy, number of states, and number of emissions with (0, 0.5, 2) entropy for (4, 8, 16) states respectively and (0.99, 0.95, 0.75) λ2 for (4, 8, 16) states respectively. 32 E.2 Tokenization In this section, we evaluate three tokenization strategies: ABC, which encodes emissions as single letters; 123, which encodes them as single digits; and random, which maps emissions to random tokens from the LLMs tokenizer. For the random strategy, we specifically map emissions to special tokens (!@#$). All experiments are conducted using the Qwen2.5-1.5B model, and the results are presented below. We observe that all tokenization methods converge to similar performance levels in terms of accuracy, with ABC converging slightly faster when the entropy of is large. This suggests that the choice of tokenization has limited impact on final performance. In our experiments, we adopt the ABC tokenization for maximum performance on the LLM. However, when the entropy of matrix is low, ABC tokenization exhibits significantly lower initial accuracy and higher Hellinger distance with short context length. We hypothesize that this is due to the increased likelihood of repetitive state sequences early in the sequencefor example, AAAAA.... During pretraining, such repeated n-gram patterns are often filtered out, as they could cause loss spikes [38]. As result, the model may have limited exposure to these patterns, leading to poor initial performance on such inputs. Figure 21: Accuracy of three tokenization methods across different mixing rates (λ2), entropy, and steady states with 4 states, 4 emissions, and 1 for entropy. Figure 22: Hellinger distance of three tokenization methods across different mixing rates (λ2), entropy, and steady states with 4 states, 4 emissions, and 1 for entropy."
        },
        {
            "title": "F Spectral Learning HMMs for Prediction Task",
            "content": "Notations: We use [X]i,j to denote the element of matrix at its i-th row and j-th column. The indicator function 1{x=i} is 1 only when = and is 0 otherwise. We use 1M to denote vector of all 1s with dimension . We use the notation [L] = {1, 2, . . . , L}. denotes the Frobenius norm for matrices, and depending on the context it denotes ℓ1 or ℓ2 norm for vectors. Algorithm 6: Spectral Learning-Based Prediction Input: Number of hidden states , number of observations L, sequence {o1, . . . , oN } Output: Conditional probability distribution ˆP (ON +1O1:N = o1:N ) Estimate empirical probabilities: for all combinations i, j, [L] do (cid:80)N [ ˆP1]i 1 [ ˆP2]i,j 1 [ ˆP3,n]i,j 1 k=1 1{ok=i}; (cid:80)N k=1 1{ok=i,ok1=j}; (cid:80)N k=1 1{ok=i,ok1=n,ok2=j}; end Compute SVD for dimensionality reduction: ˆU left singular vectors of ˆP2 corresponding to largest singular values; Estimate spectral parameters: ˆb1 ˆU ˆP1; ˆb ( ˆP ˆU) ˆP1; 2 for each observation [L] do ˆCo ˆU ˆP3,o( ˆU ˆP2); end Hidden state belief update: ˆb1 initial belief; for τ = 1 to do ˆCoτ ˆbτ +1 ; ˆbτ ˆCoτ ˆbτ ˆb end Conditional probability prediction: for each possible next observation oN +1 [L] do ˆb (cid:80)L ˆP (ON +1 = oN +1O1:N = o1:N ) ; ˆCoN +1 ˆb ˆCk ˆbN +1 ˆbN +1 k= end return ˆP (ON +1O1:N = o1:N ) F.1 Preliminaries For Markov chain with transition matrix A, we let π RM + denote the initial state distribution. We assume that π is also the stationary distribution of the Markov chain. This can be achieved by taking 1λ2(A) . Note that πt = (At)π is essentially samples after burn-in time which is proportional to convex combination of rows of matrix At, then by triangle inequality, we have πt π1 maxi[M ] ([At]i,:) π1. Thus, for an ergodic Markov matrix A, we define the following to quantify the convergence of πt π1. For an ergodic Markov matrix RM , let τMC > 1 and ρMC (λ2(A), 1) be two constants [26, Theorem 4.9] such that + 1 max i[M ] ([At]i,:) π1 τMCρt MC. Furthermore, we define the mixing time of as tMC(ϵ) := min (cid:26) : max i[M ] 1 2 ([At]i,:) π1 ϵ (cid:27) . (4) (5) Note that τ (M) and τMC have similar roles except τ (M) is usually used to study state matrices while τMC is for Markov matrices. For square M, we have Mk τ (M)ρ(M)k, and for Markov matrix, we have At 1M π τMCρt MC. 34 F.2 Sample Complexity Analysis In this section, we analyze the sample complexity of spectral learning algorithm (Alg 6) when the observation sequence is coming from single trajectory. Our proof builds on [21] by modifying their analysis in Appendix to incorporate single trajectory learning. We only present the Sample complexity analysis here and refer the reader to [21] for the remaining proofs. F.3 Proof of Theorem 1 Fix 2 < < , and recall from [21] that [P1]i = E[1{oT =i}], [P2]i,j = E[1{oT =i,oT 1=j}], [P3,k]i,j = E[1{oT =i,oT 1=k,oT 2=j}], for all [L], when the initial distribution π is the stationary distribution of the Markov chain. In the following, we will present three different estimators for each of these quantities and analyze their convergence. Estimation of P1: Let := {o(1) define the following three estimators of P1, , and without loss of generality, suppose is an integer. Suppose } be the i.i.d. samples obtained from independent trajectories of the HMM. We , . . . , o( ) [ ˆP1]i = (cid:80)N k=1 1{ok=i} [ ˆP(ℓ) 1 ]i = (cid:80) k=1 1{okT ℓ=i} [ ˆP() ]i = for all ℓ = 0, . . . , 1. By triangle inequality, we have ˆP1 P1 ˆP1 ˆP() 1 + ˆP() 1 P1. (cid:80) k=1 1{o(k) =i} , (6) (7) [21] showed that, with probability at least 1 δ, we have, ˆP() following, we will upper bound the term ˆP1 ˆP() each ℓ-th subtrajectory as follows: We have 1 P1 . In the 1 by considering entry-wise concentration of + (cid:113) log(1/δ) (cid:113) 1 (cid:80) k=1 ]i = (cid:16) 1{okT ℓ=i} 1{o(k) =i} (cid:17) . (8) [ ˆP(ℓ) 1 ]i [ ˆP() (cid:104) 1 (cid:105) 1{okT ℓ=i} 1{o(k) First, we observe that =i} 1, almost surely. However, the summation in (8) has weakly dependent terms. Therefore, we use the Bernstein type inequality for class of weakly dependent and bounded random variables proposed in [35]. Before that, we need to upper bound the variance of the summation in (8). Observing that 0. Moreover, 1{okT ℓ=i} 1{o(k) =i} (cid:105) (cid:104) = 0, we have, [ ˆP(ℓ) 1 ]i [ ˆP() 1 ]i (cid:16) Var [ ˆP(ℓ) 1 ]i [ ˆP() 1 (cid:17) ]i := (cid:20)(cid:16) [ ˆP(ℓ) 1 ]i [ ˆP() (cid:17)2(cid:21) 1 [ ˆP(ℓ) 1 ]i + (cid:17)2(cid:21) , ]i (cid:20)(cid:16) [ ˆP() ]i (cid:17)2(cid:21) (cid:104) 2 [ ˆP(ℓ) 1 ]i[ ˆP() 1 (cid:105) . ]i (9) (cid:20)(cid:16) = In the following, we will upper bound each term in (9) separately. We begin with, (cid:20)(cid:16) [ ˆP(ℓ) 1 ]i (cid:17)2(cid:21) 1 2 (cid:88) (cid:88) k=1 k=1 1{okT ℓ=i}1{okT ℓ=i} , = = = = 1 2 1 2 (cid:88) (cid:88) k=1 (cid:88) k=1 (cid:88) k=1 k=1 (cid:104) 1{okT ℓ=i,ok ℓ=i} (cid:105) , (OkT ℓ = i, OkT ℓ = i) , (cid:104) (cid:105) Bdiag(π)AkkT . i,i (10) [Bπ]i + 1 2 (cid:88) (cid:88) k=1 k=1 k=k 35 Next, we have, (cid:20)(cid:16) [ ˆP() 1 ]i (cid:17)2(cid:21) = = = 1 2 1 2 1 2 k=1 (cid:88) k=1 (cid:88) k=1 k=1 (cid:88) (cid:88) k=1 k=1 1{o(k) =i}1 {o(k ) =i} , (cid:88) (cid:88) (cid:104) {o(k) =i,o(k ) =i} (cid:105) , (cid:16) = i, O(k) O(k) (cid:17) = = [Bπ]i + ( 1) [Bπ]2 . (11) Lastly, we have (cid:104)(cid:16) [ ˆP(ℓ) 1 ]i (cid:17) (cid:16) [ ˆP() 1 (cid:17)(cid:105) ]i = = = 1 2 1 2 1 k=1 (cid:88) k=1 (cid:88) k=1 k=1 N (cid:88) (cid:88) k=1 k=1 1{okT ℓ=i} {o(k ) =i} , (cid:88) (cid:88) (cid:104) 1 {okT ℓ=i,o(k ) =i} (cid:105) , (cid:16) OkT ℓ = i, O(k) (cid:17) = = [Bπ]2 . (12) Combining (10), (11), and (12) into (9), we get (cid:16) Var [ ˆP(ℓ) 1 ]i [ ˆP() (cid:17) ]i = 2[Bπ]i + 1 N (cid:88) (cid:88) k=1 k=1 k=k ( + 1) [Bπ]2 , (cid:105) (cid:104) Bdiag(π)AkkT i,i = 2([Bπ]i [Bπ]2 ) + 1 2 (cid:88) (cid:88) k=1 k=1 k=k (cid:104) (cid:105) Bdiag(π)AkkT i,i , [Bπ]2 π (b N (cid:88) (cid:88) π)2) ( 1) 2(b = + 1 2 k= k=1 k=k π (b π)2 b diag(π) (cid:16) AkkT 1M π(cid:17) bi, + bi2τMCρT MC (1 ρT MC) π (b i π)2 , (13) where bi denotes the i-th column of and we get the last inequality by choosing, (cid:18) log bi2τMC (b π (b π)2)(1 ρT MC) (cid:19) (cid:14)(1 ρ). (14) Hence, using the Bernstein type inequality for weakly dependent and bounded random variables (Theorem 1 in [35]), together with (13) (14), and the observations we made right after (8), with probability at least 1 δ, we have [ ˆP(ℓ) 1 ]i [ ˆP() (cid:115) ]i (b π (b π)2) log (cid:19) . (cid:18) 1 δ (15) 36 Union bounding over all [L], and ℓ {0, 1, . . . , 1}, with probability at least 1 δ, we have [ ˆP(ℓ) 1 ˆP() 1 (cid:115) 1 Bπ Bπ2 log (cid:18) LT δ (cid:19) , (16) (cid:110) given maxi[L] probability at least 1 δ, the same upper bound also holds for [ ˆP1 ˆP() (7) and [21], with probability at least 1 δ, we have (cid:17)(cid:111) (cid:14)(1 ρ). This further implies that, with 1 . Combining this with π)2)(1ρT bi2τMC π(b MC) log (b (cid:16) ˆP1 P1 (cid:114) log(1/δ) + (cid:114) 1 + (cid:115) 1 Bπ Bπ2 log (cid:18) LT δ (cid:19) . (17) Estimation of P2: Here, we follow similar line of reasoning as above. We begin with defining the three estimators of P2 as follows, [ ˆP2]i,j = [ ˆP() 2 ]i,j = (cid:80)N (cid:80) k=1 1{ok=i,ok1=j} k=1 1{o(k) =i,o(k) 1=j} [ ˆP(ℓ) 2 ]i,j = (cid:80) k=1 1{okT ℓ=i,okT ℓ1=j} , Similar to P1, we consider the entry-wise concentration of each ℓ-th subtrajectory as follows, [ ˆP(ℓ) 2 ]i,j [ ˆP() (cid:16) (cid:80) k=1 ]i,j = 1{okT ℓ=i,okT ℓ1=j} 1{o(k) =i,o(k) 1=j} (cid:17) . Observing that (cid:104) [ ˆP(ℓ) 2 ]i,j [ ˆP() 2 (cid:105) ]i,j = 0, we have, (18) (19) (cid:16) Var [ ˆP(ℓ) 2 ]i,j [ ˆP() 2 (cid:17) ]i,j = = (cid:20)(cid:16) (cid:20)(cid:16) [ ˆP(ℓ) 2 ]i,j [ ˆP() (cid:17)2(cid:21) 2 [ ˆP(ℓ) 2 ]i,j + (cid:17)2(cid:21) , ]i,j (cid:20)(cid:16) [ ˆP() (cid:17)2(cid:21) ]i,j 2 (cid:104) [ ˆP(ℓ) 2 ]i,j[ ˆP() (cid:105) . ]i,j (20) In the following, we will upper bound each term in (20) separately. We begin with, (cid:20)(cid:16) [ ˆP(ℓ) 2 ]i,j (cid:17)2(cid:21) = = = = 1 2 1 2 1 2 (cid:88) (cid:88) k=1 k= 1{okT ℓ=i,okT ℓ1=j}1{okT ℓ=i,ok ℓ1=j} , (cid:88) (cid:88) (cid:104) 1{okT ℓ=i,okT ℓ1=j,ok ℓ=i,okT ℓ1=j} (cid:105) , k=1 (cid:88) k=1 (cid:88) k=1 k=1 (OkT ℓ = i, OkT ℓ1 = j, OkT ℓ = i, OkT ℓ1 = j) , πDj,i1M + 1 N (cid:88) (cid:88) k=1 k=1 k=k πDj,iAkkT 1Dj,i1M , where, given the i-th column bi, and the j-th column bj of B, we define Dj,i := diag (bj) diag (bi) . 37 (21) (22) Next, we have (cid:20)(cid:16) [ ˆP() 2 ]i,j (cid:17)2(cid:21) = = = = Lastly, we have 1 2 1 2 1 N (cid:88) (cid:88) k= k=1 1{o(k) =i,o(k) 1=j}1 , {o(k ) =i,o(k ) 1=j} {o(k) =i,o(k) 1=j,o(k ) =i,o(k ) 1=j} (cid:21) , (cid:88) (cid:88) (cid:20) 1 k=1 (cid:88) k=1 (cid:88) (cid:16) O(k) = i, O(k) 1 = j, O(k) = i, O(k) 1 = k=1 k=1 πDj,i1M + ( 1) (cid:0)πDj,i1M (cid:1)2 . (cid:17) , (23) (cid:88) (cid:88) k=1 k=1 1{okT ℓ=i,okT ℓ1=j}1 , {o(k ) =i,o(k ) 1=j} {okT ℓ=i,okT ℓ1=j,o(k ) =i,o(k ) 1=j} (cid:21) , (cid:104)(cid:16) [ ˆP(ℓ) 2 ]i,j (cid:17) (cid:16) [ ˆP() 2 (cid:17)(cid:105) = ]i,j = = 1 2 1 2 1 2 (cid:88) (cid:88) (cid:20) 1 k=1 (cid:88) k=1 (cid:88) k= k=1 (cid:16) OkT ℓ = i, OkT ℓ1 = j, O(k) = i, O(k) 1 = (cid:17) , = (cid:0)πDj,i1M (cid:1)2 . (24) Combining (21), (23), and (24) into (20), we get Var (cid:16) [ ˆP(ℓ) 2 ]i,j [ ˆP() 2 (cid:17) ]i,j = 2πDj,i1M + 1 2 (cid:88) (cid:88) k=1 k=1 k=k πDj,iAkkT 1Dj,i1M ( + 1) (cid:1) (cid:0)πDj,i1M 2 (cid:0)πDj,i1M (πDj,i1M )2(cid:1) , = + 1 N (cid:88) (cid:88) k=1 k=1 k=k πDj,i (cid:16) AkkT 1 1M π(cid:17) Dj,i1M , 2 (cid:0)πDj,i1M (πDj,i1M )2(cid:1) + πDj,iDj,i1M τMCρT 1 MC (1 ρT MC) , πDj,i1M (πDj,i1M )2 , where we get the last inequality by choosing, (cid:18) 1 + log πDj,iDj,i1M τMC (πDj,i1M (πDj,i1M )2) (1 ρT MC) (cid:19) (cid:14)(1 ρ). (25) (26) Hence, using similar line of reasoning as we did in the case of P1, with probability at least 1 δ, we have [ ˆP(ℓ) 2 ˆP() 2 (cid:115) (cid:80)L i,j=1 (πDj,i1M (πDj,i1M )2) log (cid:18) L2T δ (cid:19) , (27) (cid:110) given 1 + maxi,j[L] that, with probability at least 1 δ, the same upper bound also holds for [ ˆP2 ˆP() this with the triangle inequality and [21], with probability at least 1 δ, we have (πDj,i1M (πDj,i1M )2)(1ρT (cid:17)(cid:111) (cid:14)(1 ρ). This further implies 2 . Combining πDj,iDj,i1M τMC MC) log (cid:16) ˆP2 P2 (cid:114) log(1/δ) + (cid:114) 1 + (cid:115) (cid:80)L i,j=1 (πDj,i1M (πDj,i1M )2) log (cid:18) L2T δ (cid:19) . (28) Estimation of P3: Here, we follow similar line of reasoning as above. We begin with defining the three estimators of P3 as follows, [ ˆP3,n]i,j = (cid:80)N k=1 1{ok=i,ok1=n,ok2=j} [ ˆP(ℓ) 3,n]i,j = (cid:80) k=1 1{okT ℓ=i,okT ℓ1=n,okT ℓ2=j} , [ ˆP() 3,n ]i,j = (cid:80) k=1 1{o(k) 1=no(k) 2=j} =i,o(k) (29) Following the same line of reasoning as we did in the case of P2, with probability at least 1 δ, we have ˆP3,n P3,n (cid:114) log(1/δ) + (cid:114) 1 + (cid:115) (cid:80)L i,j,n=1 (πDj,n,i1M (πDj,n,i1M )2) log (cid:18) L3T δ (cid:19) , (30) provided that, 2 + max i,j,n[L] (cid:26) (cid:18) log πDj,n,iDj,n,i1M τMC (πDj,n,i1M (πDj,n,i1M )2) (1 ρT MC) (cid:19)(cid:27) (cid:14)(1 ρ), (31) where, given the i-th column bi, the j-th column bj and the n-th column bn of B, we define Dj,n,i := diag (bj) diag (bn) diag (bi) (32) Finalizing the proof: Theorem 1 follows by repeating the proof of Theorem 7 in [21], with the i.i.d. estimators replaced by the single trajectory estimators, and the values of ϵ1, ϵ2,1 and ϵ3,x,1 replaced by, (cid:114) ϵ1 log(1/δ) (cid:115) (cid:80)L (cid:115) (cid:114) 1 + + 1 Bπ Bπ2 log (cid:18) LT δ (cid:114) ϵ2,1 log(1/δ) + ϵ3,x,1 (cid:114) log(1/δ) + (cid:114) 1 + (cid:114) 1 (cid:115) (cid:80)L + i,j=1 (πDj,i1M (πDj,i1M )2) i,j,n=1 (πDj,n,i1M (πDj,n,i1M )2) (cid:19) (cid:19) (cid:19) , , , log log (cid:18) L2T δ (cid:18) L3T δ where = distance in terms of KL-distance. = (N (1 λ2(A))). The proof is completed by upper bounding the Hellinger-"
        },
        {
            "title": "G Additional Real World Experiments",
            "content": "We design an additional experiment using real-world datasets to validate our findings. We artificially simulate different emission entropy levels for the same underlying hidden transition process by controlling the amount of information included in the observation sequence. Using complete information corresponds to low emission entropy, while limiting information artificially increases emission entropy. We use the IBL decision-making mice dataset [25]. In our LLM in-context learning experiment, we implement four ablation conditions that vary the information presented in each trial: (i) choice only; (ii) choice reward; (iii) stimulus choice; (iv) stimulus choice reward. Note that the baseline GLM-HMM uses all available information as in condition (iv). These ablations describe the same underlying mouse decision-making sequences but with varying levels of environmental state detail. Figure 23: LLM in-context learning prediction accuracy for mice decision-making task with varying types of information in the observed sequences. Each line is averaged over 7 mice, with 1-σ error bar. The model we use is Qwen2.5-7B. The results shown in Figure 23 reveal significant differences across ablation conditions: while stimulus choice reward achieves performance exceeding GLM-HMM, choice reward is merely at chance level with its convergence trend similar to the synthetic experiments when the transitions or emissions are near random. This demonstrates that accurately modeling mouse decision-making in this task requires both stimulus and reward information. These findings highlight broader principle: obtaining appropriate information (corresponding to low emission entropy) is essential for successful task modeling. This experiment parallels real-world experimental design, where scientists must choose which signals to collect when studying task structure. When researchers omit critical information needed to describe sequence, it can easily lead to incorrect conclusions about the underlying process."
        }
    ],
    "affiliations": [
        "Cornell University"
    ]
}