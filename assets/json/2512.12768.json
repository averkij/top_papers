{
    "paper_title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
    "authors": [
        "Tianjiao Yu",
        "Xinzhuo Li",
        "Yifan Shen",
        "Yuanzhe Liu",
        "Ismini Lourentzou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions."
        },
        {
            "title": "Start",
            "content": "CoRe3D: Collaborative Reasoning as Foundation for 3D Intelligence Tianjiao Yu, Xinzhuo Li, Yifan Shen, Yuanzhe Liu, Ismini Lourentzou {ty41, lourent2}@illinois.edu University of Illinois Urbana-Champaign 5 2 0 2 4 1 ] . [ 1 8 6 7 2 1 . 2 1 5 2 : r Abstract. Recent advances in large multimodal models suggest that explicit reasoning mechanisms play critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions. 1. Introduction Despite rapid progress in 3D generation, most existing methods remain imitation-based, reproducing shapes rather than reasoning about objects [52, 105]. As result, they struggle with prompts that implicitly describe relations, counts, geometry, or physical contacts, concepts that recent unified languagevision models have begun to handle effectively in 2D settings [69, 98]. This progress is largely attributed to the integration of Chain-of-Thought (CoT) reasoning [81], which, when extended to multimodal LLMs [7, 50, 109, 112], improves interpretability and consistency across visual reasoning tasks [34, 48]. However, unified reasoning in the 3D domain remains under-explored; few models are capable of jointly interpreting and constructing 3D objects [80, 104]. To advance this frontier, we propose CoRe3D, framework for collaborative reasoning that unifies semantic understanding and geometric generation within single 3D-LLM. As illustrated in Fig. 1, CoRe3D integrates unified 3D language model with an octant-based 3D VQ-VAE, enabling the model to reason in both language and 3D token space. At its core, our approach couples Semantic CoT for high-level textual planning with novel Geometric CoT for spatial synthesis. The geometric CoT operates autoregressively across octant blocks, addressing the limitations of existing flat voxel representations that waste computation on empty space and fail to capture structured spatial dependencies. Unlike part-level representations [9], which require fixed ontologies and suffer from poor generalization across categories, or voxel-level representations [53, 90], which remain unstructured and semantically agnostic, our octant-based representation remains ontology-free yet structure-aware. To jointly refine both reasoning streams, we further employ Group-Relative Policy Optimization (GRPO) [60], allowing CoRe3D to learn from multi-critic feedback that balances semantic intent, visual quality, and physical coherence. This reasoning-aware framework produces high-fidelity 3D construction with enhanced spatial understandlanguage ing while maintaining strong general abilities. This approach is essential for three reasons: (1) it elicits plans where no gold\" supervision exists; (2) it allows for granular process credit assignment using dense 3D-specific rewards; and (3) it prevents reward hacking and overfitting by leveraging an ensemble of different critics. By rewarding both linguistic reasoning and 3D synthesis, our approach lays the groundwork for general 3D intelligence, unifying understanding and generation. Preprint. Work in progress. Collaborative Reasoning as Foundation for 3D Intelligence Figure 1: We introduce CoRe3D, framework that unifies Semantic CoT and octant-based Geometric CoT through collaborative reasoning. By coupling language-grounded reasoning with shape constructions, CoRe3D enables bidirectional capability in both 3D understanding and generation, allowing the model to interpret objects and construct them within unified framework. Contributions: In summary, our contributions are: We introduce CoRe3D, collaborative reasoning framework that unifies two complementary reasoning levels, Semantic CoT for textual planning, with novel octant-based Geometric CoT that acts as structure-aware yet ontology-free prior, enabling interpretable progressive construction without category-specific part definitions. To the best of our knowledge, we are the first to use Co-GRPO to jointly optimize semantic and geometric reasoning in 3D. This approach elicits plans without direct supervision and effectively assigns credit using dense 3D-specific rewards (e.g., symmetry, physical coherence) for improved alignment, structure, and robustness. We further demonstrate that our unified reasoning paradigm is not limited to generation but naturally extends to reciprocal 3D understanding tasks, such as 3D-to-text captioning, highlighting its potential as scalable foundation for general 3D intelligence. 2. Related Work 3D Generation. Early text-to-3D frameworks [10, 18, 35, 37, 52, 55, 64, 66, 72, 79, 105] formulated 3D synthesis as an optimization problem guided by 2D priors through score distillation sampling (SDS). While this approach enabled cross-modal 3D generation without paired data, it required long per-instance optimization and often produced viewinconsistent geometry. Subsequent methods [8, 54, 62, 73, 78, 103] address view inconsistency by enforcing cross-view semantic constraints within diffusion pipelines. Other works address the inefficiency of iterative optimization [19, 24, 39, 43, 44, 45, 47, 61, 71, 83, 86, 101, 108, 113] by first generating consistent 2D renderings then reconstructing the 3D geometry through fast neural reconstruction. More recently, native 3D diffusion models [16, 32, 36, 76, 87, 90, 99, 102, 110, 115] shifted toward generative modeling within latent 3D spaces, employing VAE-based encoders to learn volumetric or implicit shape priors. In contrast to diffusion models, approaches employ vector-quantized autoencoders [70], casting 3D generation as an autoregressive sequence modeling problem [13, 63, 84]. Later works [12, 14, 27, 67, 80, 85, 114] introduce taskspecific tokenization schemes that directly encode vertexface structures, improving geometric fidelity and local continuity. However, these models still operate as next-token predictors and do not expose an explicit reasoning process. Our method instead pairs an octant-based 3D VQ-VAE with unified 3D-LLM that performs both semantic and geometric chain-of2 Collaborative Reasoning as Foundation for 3D Intelligence thought reasoning, leading to better 3D generation and understanding performance. Unified Generation & Understanding. Recent multimodal LLMs have revealed remarkable capability in jointly processing and generating visionlanguage content. Early frameworks [1, 2, 17] extend LLMs with visual encoders for grounded perception, while more recent systems [40, 68, 77, 91, 116] integrate text and image generation through learned visual tokenizers and mixed-modality training. Extending this paradigm to 3D, emerging studies [11, 29, 53, 94, 95] adapt LLMs for 3D understanding using point-cloud or shape embeddings. While effective for perception tasks, these models largely focus on recognition rather than generation. Subsequent efforts [15, 80, 104, 106, 117] attempt to unify language and 3D modeling by developing generative LLMs that handle both understanding and generation within shared representation space. More interactive paradigms, such as LL3M [49] and L3GO [97], employ agent-based reasoning to iteratively construct or edit 3D scenes, yet rely on symbolic planning rather than token-level 3D reasoning. In contrast, our approach integrates semantic and geometric reasoning within unified 3D-LLM. By explicitly modeling the reasoning process across both language and 3D token spaces, CoRe3D achieves reasoning-aware 3D understanding and generation, bridging the gap between linguistic intent and physically grounded 3D synthesis. 3. Method 3.1. Preliminaries Reinforcement learning has recently become dominant tool for eliciting reasoning behaviors in large models. particularly effective variant is Group Relative Policy Optimization (GRPO) [60], which modifies Policy Proximal Optimization (PPO) by discarding the explicit value function and instead normalizing rewards within sampled group of trajectories. Formally, given promptanswer pair (p, a), the generates group of candidate old policy œÄŒ∏old . Each response is scored by responses {oi} i=1 reward model, yielding ‚Ñõi . To reduce variance and emphasize relative quality, the advantage of the i-th response is defined by standardizing rewards within the group: , (1) j=1) ‚Ñõi j=1) Ai = ¬µ({‚Ñõj} œÉ({‚Ñõj}G where ¬µ and œÉ denote the mean and standard deviation. The learning objective follows the clipped surrogate structure of PPO, but with direct KL penalty that anchors the updated policy œÄŒ∏ to reference distribution œÄŒ∏ref : ùí•GRPO(Œ∏) = ({oi}G i=1 i=1 oi t=1 [ 1 i= oi ( min (ri,t(Œ∏)Ai, clip(ri,t(Œ∏), 1 Œµ, 1 + Œµ)Ai) Œ≤DKL(œÄŒ∏ œÄŒ∏ref))], (2) where the importance ratio at each token step is ri,t(Œ∏) = œÄŒ∏(oi,t p, oi,<t) œÄŒ∏old(oi,t p, oi,<t) . (3) . Here, Œµ controls the clipping range of the importance ratio, and Œ≤ determines the strength of the KL penalty that keeps the updated policy close to the reference policy œÄŒ∏ref While GRPO has been applied mainly in text reasoning (e.g., math or code generation), its principle of relative quality comparison remains under-explored in 3D generation. Leveraging this idea, we evaluate groups of rollout trajectoriesspanning both semantic plans and geometric refinementsagainst one another, where their relative ranking provides stable optimization signal that aligns outputs with semantic intent and geometric plausibility. Octree-based Autoregressive Model. Recent 3D transformers such as OctFormer [74] demonstrate that representing 3D data using an octree hierarchy enables efficient global reasoning while preserving local geometric detail. Instead of processing dense voxel grids, an octree partitions the 3D space into hierarchical cubic cells (octants) of adaptive resolution, allocating finer subdivisions in geometrically complex regions and coarser ones in uniform areas. Collaborative Reasoning as Foundation for 3D Intelligence Figure 2: CoRe3D overview. Semantic and geometric reasoning tokens are generated by our unified 3D-LLM, and the generated 3D object and corresponding multiviews are evaluated by an ensemble of critics. This sparse yet structured representation significantly reduces memory and computation costs compared to dense attention over all voxels. Formally, an octree representation can be described as set of hierarchical nodes ùí™ = {ok ‚Ñì ‚Ñì [0, L), ‚Ñê‚Ñì}, where ‚Ñì denotes the level in the tree and indexes the spatial position at that level. Each node ok encodes geometric and visual features (e.g., oc‚Ñì cupancy, color, or normal) aggregated from its eight child nodes at level ‚Ñì+1. The model applies transformer attention hierarchically across this structure: intra-level attention aggregates context among nodes within the same resolution, while inter-level attention propagates information between parent and child nodes to capture cross-scale dependencies. This design provides two key advantages: (1) it maintains spatial locality, allowing the model to focus computation on occupied regions, and (2) it establishes natural coarse-to-fine reasoning pathway across the 3D volume. In our work, we adopt simplified variant of this idea by discretizing the latent 3D volume into uniform 222 octant blocks rather than an adaptive octree hierarchy. Each octant token thus serves as fixed-resolution counterpart to an octree node, preserving the locality of the model while enabling autoregressive reasoning through the geometric CoT process described in the following sections. 3.2. Semantic and Geometric Representations Semantic-Level Representation. core challenge in 3D generation is translating open-ended language into structured reasoning signals that preserve compositional semantics and physical constraints. Directly mapping prompts into latent 3D tokens is under-specified, as language descriptions typically omit precise geometric, relational, and material cues, resulting in generated shapes that capture coarse appearance but fail to recover consistent structure or texture details. To address this gap, we introduce semantic CoT reasoning stage that expands each textual prompt into an explicit structural plan before geometry generation. Given an input description and an optional reasoning instruction, the unified 3D-LLM first produces detailed natural language description of the object category, spatial layout, 4 Collaborative Reasoning as Foundation for 3D Intelligence materials, and appearance details. This description serves as an interpretable, text-based scaffold that anchors the subsequent geometric reasoning process. Formally, we represent the semantic reasoning trace as sequence of tokens ùíÆsem = [s1, s2, . . . , sN], conditioned on the image prompt and reasoning instruction. These tokens are optimized jointly with 3D generation tokens, enabling semantic intent to directly modulate spatial synthesis during training. Geometric-Level Representation. We represent each 3D object using 643 voxel grid, which provides balanced trade-off between spatial fidelity and computational efficiency. To obtain compact token representation, 3D VQ-VAE encoder maps the voxel grid to 163 latent grid, preserving local geometry and appearance features. The latent grid is serialized into 4096 latent vectors, each corresponding to spatial location. To further reduce sequence length, we group every 222 neighborhood of latent voxels (eight adjacent cells) by concatenating their channels into single vector. This operation transforms the 4096 latent vectors (with 8-D channels) into 512 tokens with 64-D channels, where each token represents local octant block within the 3D volume. vector-quantization module with an 8192-entry codebook discretizes these octant features, resulting in 512 discrete 3D tokens per object. For spatial disambiguation across blocks, we attach learned absolute position embedding to each octant token, keyed by its (xb, yb, zb) index on the 888 block grid (or an equivalent Morton/Z-order code). This embedding is injected post-quantization, so the codebook remains content-centric while the generator remains location-aware. This octant-based representation naturally supports our geometric CoT reasoning: the model iteratively thinks over octant tokens, refining or sampling candidate completions for each sub-cube. We denote the sequence of geometric reasoning tokens as ùí¢geo = [g1, g2, . . . , gM], (4) where each token gi corresponds to discrete octant block produced by the 3D VQ-VAE. While the semantic CoT ùíÆsem expresses the conceptual plan in language space, the geometric CoT ùí¢geo realizes that plan token-by-token in the 3D latent space. Together, these two reasoning levels enable controllable and interpretable 3D generation that preserves both global semantics and local geometric precision. 3.3. Collaborative Reasoning The core innovation of our framework lies in the explicit collaboration between semantic and geometric reasoning. While each level can operate independently, its joint optimization leads to stronger, mutually reinforcing behavior. We unify them through 3D Co-GRPO, reinforcement learning framework that refines both reasoning levels using multi-critic 3Daware rewards, aligning linguistic intent with spatial construction. This results in objects that are semantically faithful, visually compelling, and physically coherent. An overview is shown in fig. 2. and preFormally, given an input prompt Tp generated semantic CoT decoded from ùíÆsem, the unified 3D-LLM produces geometric reasoning sequence ùí¢geo = [g1, g2, . . . , gM] to synthesize the final 3D object ÀÜO. The process can be viewed as sequential reasoning pipeline: the semantic trace ùíÆsem provides global planning cues such as object category, spatial relations, and texture details, while the geometric trace ùí¢geo progressively realizes those cues within the latent 3D token space. We extend the Generalized Reinforcement Preference Optimization (GRPO) paradigm into the 3D domain by introducing four complementary critics, each providing scalar reward that captures distinct dimension of 3D quality. The resulting meshes and multi-view renderings {Ii} are evaluated by an ensemble of 3D experts: Human Preference Critic: evaluates perceptual realism and human aesthetic preference, assessing prompt relevance and overall visual appeal from multi-view renderings [88, 93, 103]. 3D Understanding Critic: verifies attributeand part-level correctness using 3D-VQA models [11, 29, 53, 117] that query geometric, textural, and symmetry attributes derived from the semantic CoT ùíÆsem. Text3D Alignment Critic: measures semantic faithfulness between the textual reasoning trace (prompt and ùíÆsem) and the generated geometry 5 Collaborative Reasoning as Foundation for 3D Intelligence ùí¢geo, using pretrained text3D embedding models [96, 106] to ensure cross-modal coherence and faithful alignment. Physical Coherence Critic: analytically enforces structural stability and physical plausibility through differentiable reward composed of three geometrybased terms: RP = Œª1Rstab + Œª2Rrig + Œª3Rint, where Rstab measures global balance of the center of mass, Rrig promotes topologically connected geometry that maintains physical continuity, and Rint penalizes self-intersection between surfaces. This critic is fully geometry-driven and provides compact measure of physical coherence. Each critic outputs normalized reward Ri [0, 1], and the overall GRPO objective aggregates them via = wH RH + wV RV + wX RX + wPRP, (5) where weights balance human preference, 3D understanding, cross-modal alignment, and physical coherence. The combined reward provides preference signals for policy updates, encouraging the model to improve both reasoning accuracy and geometric fidelity over GRPO iterations. During training, the model first performs forward generation to produce ùíÆsem and ùí¢geo for each prompt . Generated results are rendered into multi-view Tp images and passed through the four critics, producing individual rewards {RH, RV, RX, RP}. The composite reward in Eq. (5) is then used to compute pairwise preferences among samples and update the model using the GRPO objective defined in Eq. (2). 4. Experiments Implementation Details. Our training data is sourced from the 3D-Alpaca dataset [104], which comprises approximately 2.56M multimodal samples spanning text-to-3D, image-to-3D, 3D-captioning, and 3D-editing tasks. Each 3D asset is rendered from four orthogonal views and paired with GPTgenerated captions, providing rich interleaved supervision across modalities. We initialize our 3DULM from ShapeLLM-Omni [104], which we extend with our octant-based 3D VQ-VAE for compact geometry representation and geometric CoT reasoning. Table 1: Evaluation of general conversational and reasoning abilities on standard language benchmarks. We compare CoRe3D against top-tier general vision-language models (VLMs) and 3D-specific language models. Our model demonstrates SoTA or competitive language understanding and reasoning performance. Best and second-best are highlighted. Benchmark Qwen2.5vl-7B LLaMA3.2Vision-11B LLaMAMesh-8B ShapeLLMOmni-7B MMLU PIQA GSM8K SIQA 67.5 81.3 43.2 41.0 66.2 80.1 42.1 40.6 59.8 79.8 37.2 40.3 64.3 78.9 55.6 41. CoRe3D 67.6 79.4 57.3 41.5 Training follows the 3D Co-GRPO framework with 6 and KL regularization base learning rate of 1 10 Œ≤ = 0.01. The model is trained for 40k steps with batch size 256 on 8L40 GPUs. For reward computation, we render multi-view images for each generated mesh and evaluate them using HPS [88] for the Human Preference Critic, ShapeLLM [53] for the 3D Understanding Critic, and ULIP [96] embeddings for the Text3D Alignment Critic. Finally, the Physical Coherence Critic is implemented through geometry-based evaluation on the generated meshes. Given each generated mesh. All critic scores are normalized to [0, 1] and fused as in Eq. 5 to guide policy optimization under the GRPO objective (Eq. 2). More details in the Appendix. 4.1. Quantitative Results General Conversational Abilities. We first evaluate the foundational language understanding of CoRe3D. We compare against leading general-purpose VLMs (Qwen2.5-vl-7B [98], LLaMA3.2-Vision-11B [69]) and 3D-focused multimodal models (LLaMA-Mesh8B [80], ShapeLLM-Omni-7B [104]) on suite of standard language benchmarks. These include MMLU [28], PIQA [6], GSM8K [20], and SIQA [58]. As shown in Table 1, CoRe3D maintains its general conversational abilities with good language understanding and reasoning performance. These results suggest that our co-reasoning mechanism does not diminish the models broader reasoning capabilities but instead strengthens them. 6 Collaborative Reasoning as Foundation for 3D Intelligence Figure 3: Image-to-3D qualitative comparison. Given single input image, CoRe3D produces 3D shapes with higher geometric fidelity, cleaner topology, and stronger semantic alignment compared to baselines. Table 2: 3D object captioning results on the Objaverse benchmark. We evaluate the models 3D caption capability. CoRe3D achieves state-of-the-art performance by significant margin across all n-gram and semantic similarity metrics, demonstrating that our reasoning-driven generative training directly enhances 3D understanding. Best and second-best results are highlighted. Model BLEU-1 ROUGE-L METEOR Sentence-BERT SimCSE LLaVA-13B Qwen2.5-vl-7B 3D-LLM LEO PointLLM-13B ShapeLLM-Omni CoRe3D 4.01 4.05 15.11 16.98 3.18 18.92 24.02 8.18 7.85 17.84 20.12 7.54 21.46 26.45 13.18 14.23 19.22 20.91 12.24 22.12 24.98 46.97 48.90 42.36 48.01 47.89 49.43 51. 48.86 50.86 43.58 47.25 49.01 50.72 52.79 3D object Understanding. Beyond general language abilities, key goal of our work is to excel at 3D understanding. We test this reciprocal capability using the 3D object captioning task on the Objaverse [21] dataset. We compare CoRe3D against prominent general-purpose VLMs (LLaVA-13B [41], Qwen2.5vl-7B) and specialized 3D-language models (3DTable 3: Quantitative comparison of 3D generation quality for Text-to-3D and Image-to-3D tasks. We evaluate CoRe3D against state-of-the-art generative models. Results show CoRe3D achieves competitive performance on all metrics for both tasks. Best and second-best results are highlighted. Method SAR3D CLAY Trellis ShapeLLM-Omni CoRe3D CLIP 23.1 27.3 28.9 27.0 30. Text-to-3D FD incep KD 28.4 23.9 18.6 24.4 18.5 incep CLIP 83.9 0.27 84.6 0.21 85.1 0.19 84.4 0.24 85.9 0.18 Image-to-3D FD incep KD 22.1 13.5 10.9 14.1 11.2 incep 0.18 0.10 0.08 0.09 0. LLM [29], LEO [30], PointLLM-13B [94], ShapeLLMOmni [104]). Performance is measured using n-gram matching (BLEU-1, ROUGE-L, METEOR) and semantic embedding similarity (Sentence-BERT, SimCSE) following the evaluation settings in PointLLM. The results in Table 2 show that CoRe3D decisively outperforms all baselines across all five metrics. These results quantitatively confirm that our co-reasoning method substantially improves 3D understanding. 7 Collaborative Reasoning as Foundation for 3D Intelligence Figure 4: Text-to-3D qualitative comparison. CoRe3D generates 3D objects that more faithfully follow the textual prompt. 3D-Generation. We evaluate the core generative capabilities of CoRe3D on both Text-to-3D and Imageto-3D synthesis. We compare against several leading methods, including SAR3D [15], CLAY [110], Trellis [90], and ShapeLLM-Omni [104]. We use three standard metrics: CLIP Score, Frechet Distance (FD), and Kernel Distance (KD) to assess the generation quality. As shown in Table 3, our method achieves state-of-the-art results in the Text-to-3D task, demonstrating clear gain by ranking first across all three metrics. Our high CLIP score is direct quantitative validation of our core contribution: the Semantic CoT, jointly optimized with GRPO and Text-3D Alignment Critic, successfully made our model more faithful to the text. This strong performance extends to Image-to-3D generation, where our model leads in prompt alignment (CLIP 85.9) and maintains promising quality compared to the state-of-the-art 3D generation-only models. 4.2. Qualitative results Standard 3D Generation. We first evaluate CoRe3D on standard image-to-3D and text-to-3D tasks, comparing it against strong baselines, including the unified 3D models ShapeLLM-Omni [104] and SAR3D [15], as well as the generation-focused models Trellis [90] and CLAY [110]. As illustrated in Figure 3, competing methods often exhibit misalignment with the input image or suffer from geometric artifacts. In contrast, our model generates 3D meshes with high geometric fidelity and semantic coherence, faithfully capturing the complex structures present in the source image. This superior performance extends to text-to-3D generation, as shown in Figure 4. Our model achieves more robust alignment with the text prompt. Notably, CoRe3D successfully interprets and renders fine-grained stylistic details, such as the cartoon\" attribute, producing 3D meshes that align precisely with the prompts intention. Reasoning-based 3D Generation. The robust understanding capabilities of CoRe3D unlock more challenging class of 3D generation: synthesis from complex or indirect prompts. These prompts often require world knowledge and compositional reasoning to infer the users true intent (e.g., inferring Statue of Liberty from colossal copper figure holding torch ... symbolizing freedom and hope). 8 Collaborative Reasoning as Foundation for 3D Intelligence Figure 5: Qualitative results of CoRe3D on challenging prompts that require inferring the correct object or interpreting implicit descriptive cues. Comparing with the base model ShapeLLM-Omni [104] and state-of-the-art generation model Trellis [90], our model successfully inferred the true object from the implicit prompts. (e.g., colossal copper figure holding torch ... symbolizing freedom and hope corresponds to The Statue of Liberty.) Figure 6: Qualitative results on 3D part editing. The collaborative reasoning in our framework enhances yielding instruction comprehension, edits that align more faithfully with the input text and produce 3D shapes that accurately reflect the specified modifications. Standard models, which lack an explicit reasoning stage, fail at this task, generating literal but semantically incorrect shapes. As demonstrated in Figure 5, CoRe3D successfully navigates these challenges. Our models Semantic-CoT first deconstructs the ambiguous prompt into structured steps and effectively infers the underlying intention, allowing our model to produce 3D objects that maintain high faithfulness to the prompts true meaning. 3D Part Editing. Compared with traditional generative models, unified 3D LMs unlock powerful language-driven paradigm for interactive 3D asset Figure 7: Ablating Semantic CoT and Geometric CoT compared against CoRe3D and ShapeLLM-Omni [104]. Results show both CoT contribute significantly towards the final performance. manipulation. As shown in Figure 6, our model can perform fine-grained part-level edits while preserving object identity and structural coherence. Additional results are provided in the Appendix. 4.3. Ablations Semantic-CoT and Geometric-CoT. We ablate CoRe3D by removing either the Semantic-CoT tokens or the Geometric-CoT tokens for the GRPO pipeline. As shown in Figure 7, removing Semantic-CoT leads to structurally plausible objects but lacks categoryand attribute-specific cues. Conversely, removing 9 Collaborative Reasoning as Foundation for 3D Intelligence Table 4: Ablation of different critics. HP, 3DU, TA, and PC correspond to Human Preference, 3D Understanding, Text-3D Alignment, and Physical Best and second-best Coherence respectively. results are highlighted. Critic 3D Captioning HP 3DU TA PC METEOR Sentence-BERT SimCSE CLIP - - - - - - - - - - - - - - - - 12.76 14.12 15.34 13.41 18.92 20.48 19.83 21.55 24.98 44.91 47.88 46.52 45.76 48.77 49.02 49.88 50.41 51. 48.14 49.63 50.11 48.62 51.24 51.89 51.01 51.43 52.79 0.29 0.31 0.33 0.30 0.36 0.37 0.38 0.37 0.40 FD 3D Generation incep KD 23.4 22.7 21.9 20.8 19.7 19.9 20.1 19.4 18.5 incep 0.25 0.23 0.21 0.22 0.19 0.20 0.19 0.20 0.18 Geometric-CoT results in objects with clear geometric distortions and simplified shapes. Reward analysis We analyze the role of each critic and their combinations to better understand how different reward signals shape model behavior. As shown in Table 4, each critic contributes complementary improvements. Text3D alignment yields the largest gains in caption quality, showing its importance for accurate descriptions. 3D Understanding significantly improves generation quality by enforcing stronger object-level structure. Combining multiple critics steadily improves performance: pairs such as (3DU + TA + PC) or (HP + TA + PC) achieve balanced improvements across both captioning and generation tasks. More details in the Appendix 5. Conclusion We introduce CoRe3D, collaborative reasoning framework that unifies semantic planning and geometric construction through dual chain-of-thought process. By coupling these complementary reasoning levels and optimizing them jointly with 3D CoGRPO, our model achieves state-of-the-art performance across both 3D generation and understanding tasks. Beyond producing faithful and physically coherent 3D assets, CoRe3D demonstrates robust interpretive capabilities, successfully handling indirect and referring descriptions, ambiguous prompts, and fine-grained part-level edits. Our results highlight collaborative reasoning as scalable and structureaware foundation for general 3D intelligence."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65 72, 2005. [5] David Benson and Joel Davis. Octree textures. ACM Transactions on Graphics (TOG), 21(3): 785790, 2002. [6] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [7] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/ Deep-Agent/R1-V, 2025. Accessed: 202502-02. [8] Luxi Chen, Zhengyi Wang, Chongxuan Li, Tingting Gao, Hang Su, and Jun Zhu. Mi10 Collaborative Reasoning as Foundation for 3D Intelligence crodreamer: Zero-shot 3d generation in 20 seconds by score-based iterative reconstruction. arXiv e-prints, pages arXiv2404, 2024. toregressive 3d object generation and understanding via multi-scale 3d vqvae. In CVPR, 2025. [9] Minghao Chen, Jianyuan Wang, Roman Shapovalov, Tom Monnier, Hyunyoung Jung, Dilin Wang, Rakesh Ranjan, Iro Laina, and Andrea Vedaldi. Autopartgen: Autogressive 3d part generation and discovery. arXiv preprint arXiv:2507.13346, 2025. [10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to3d content creation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2224622256, 2023. [11] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding In Proceedings of reasoning and planning. the IEEE/CVF conference on computer vision and pattern recognition, pages 2642826438, 2024. [12] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Billzb Wang, Jingyi Yu, Gang Yu, et al. Meshxl: Neural coordinate field for generative 3d foundation models. Advances in Neural Information Processing Systems, 37:9714197166, 2025. [13] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. [14] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555, 2024. [15] Yongwei Chen, Yushi Lan, Shangchen Zhou, Tengfei Wang, and Xingang Pan. Sar3d: Au- [16] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling highquality 3d asset generation via primitive diffusion. arXiv preprint arXiv:2409.12957, 2024. [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [18] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. Text-to-3d using gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2140121412, 2024. [19] Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3d: Video diffusion models are effective 3d generators. arXiv preprint arXiv:2403.06738, 2024. [20] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168, 9, 2021. [21] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaversexl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36: 3579935813, 2023. [22] Kangle Deng, Hsueh-Ti Derek Liu, Yiheng Zhu, Xiaoxia Sun, Chong Shang, Kiran Bhat, Deva Ramanan, Jun-Yan Zhu, Maneesh Agrawala, 11 Collaborative Reasoning as Foundation for 3D Intelligence and Tinghui Zhou. Efficient autoregressive shape generation via octree-based adaptive tokenization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1168511696, 2025. [23] Tianyu Gao, Xingcheng Yao, and Danqi Simcse: Simple contrastive learnChen. ing of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021. [24] Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, and Kai Xu. Partgs: Learning part-aware 3d representations by fusing 2d gaussians and superquadrics. arXiv preprint arXiv:2408.10789, 2024. [25] Ethan Griffiths, Maryam Haghighat, Simon Denman, Clinton Fookes, and Milad Ramezani. Hotformerloc: Hierarchical octree transformer for versatile lidar place recognition across ground and aerial views. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 66486658, 2025. [26] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [27] Zekun Hao, David Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3d mesh generation at scale. arXiv preprint arXiv:2412.09548, 2024. [28] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [29] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36: 2048220494, 2023. [30] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world, 2024. URL https://arxiv.org/ abs/2311.12871. [31] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [32] Zixuan Huang, Mark Boss, Aaryaman Vasishta, James Rehg, and Varun Jampani. Spar3d: Stable point-aware reconstruction of 3d objects from single images. arXiv preprint arXiv:2501.04689, 2025. [33] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [34] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [35] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. arxiv:2310.02596, 2023. [36] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. [37] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, 12 Collaborative Reasoning as Foundation for 3D Intelligence Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution textto-3d content creation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 300309, 2023. [38] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [39] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model, 2024. URL https://arxiv. org/abs/2408.16767. [40] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024. [41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [42] Jiawei Liu, Nirav Diwan, Zhe Wang, Haoyu Zhai, Xiaona Zhou, Kiet Nguyen, Tianjiao Yu, Muntasir Wahed, Yinlin Deng, Hadjer Benkraouda, et al. Purpcode: Reasoning for safer code generation. arXiv preprint arXiv:2507.19060, 2025. [43] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36:2222622246, 2023. [44] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92989309, 2023. [45] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiviewconsistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [46] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an allaround player? In European conference on computer vision, pages 216233. Springer, 2024. [47] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9970 9980, 2024. [48] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun yue Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. ArXiv, abs/2310.02255, 2023. [49] Sining Dinh, Rana Hanocka. guage 3d modelers, https://arxiv.org/abs/2508.08228. Lu, Guan Chen, Nam Anh and Itai Lang, Ari Holtzman, Large lanURL Ll3m: 2025. [50] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based largescale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [51] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [52] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Textto-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Collaborative Reasoning as Foundation for 3D Intelligence [53] Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In European Conference on Computer Vision, pages 214238. Springer, 2024. [54] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99149925, 2024. [55] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 23492359, 2023. [56] Nils Reimers and Iryna Gurevych. Sentenceusing arXiv preprint bert: siamese bert-networks. arXiv:1908.10084, 2019. embeddings Sentence [57] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 35773586, 2017. [58] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [59] Ruwen Schnabel and Reinhard Klein. Octreebased point-cloud compression. PBG@ SIGGRAPH, 2006. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [61] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. [62] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [63] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nie√üner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1961519625, 2024. [64] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. arXiv preprint arXiv:2310.16818, 2023. [65] Richard Szeliski. Rapid octree construction from image sequences. CVGIP: Image understanding, pages 2332, 1993. [66] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. [67] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Auto-regressive autoencoder for artistic mesh generation. arXiv preprint arXiv:2409.18114, 2024. [60] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. [68] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. doi: 10.48550/ 14 Collaborative Reasoning as Foundation for 3D Intelligence arXiv.2405.09818. URL https://github. com/facebookresearch/chameleon. [69] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [70] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [71] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2024. [72] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. arXiv preprint arXiv:2212.00774, 2022. [73] Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201, 2023. [74] Peng-Shuai Wang. Octformer: Octree-based transformers for 3d point clouds. ACM Transactions on Graphics (TOG), 42(4):111, 2023. [75] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-cnn: Octreebased convolutional neural networks for 3d shape analysis. ACM Transactions On Graphics (TOG), pages 111, 2017. [76] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: generative model for sculpting 3d digital avatars using diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45634573, 2023. [77] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [78] Xinzhou Wang, Yikai Wang, Junliang Ye, Zhengyi Wang, Fuchun Sun, Pengkun Liu, Ling Wang, Kai Sun, Xintong Wang, and Bin He. Animatabledreamer: Text-guided nonrigid 3d model generation and reconstruction with canonical score distillation. arXiv preprint arXiv:2312.03795, 2023. [79] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36:84068441, 2023. [80] Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. Llama-mesh: Unifying 3d mesh generation with language models. arXiv preprint arXiv:2411.09595, 2024. [81] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [82] Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, and Peng-Shuai Wang. Octgpt: Octree-based multiscale autoregressive models for 3d shape generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. [83] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, CL Chen, and Lei Zhang. Consistent123: Improve consistency for one Collaborative Reasoning as Foundation for 3D Intelligence image to 3d object synthesis. arXiv preprint arXiv:2310.08092, 2023. [84] Haohan Weng, Yikai Wang, Tong Zhang, CL Chen, and Jun Zhu. Pivotmesh: Generic 3d mesh generation via pivot vertices guidance. arXiv preprint arXiv:2405.16890, 2024. [85] Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Scaling mesh generation via Guo, et al. compressive tokenization. arXiv preprint arXiv:2411.07025, 2024. [86] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [87] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. [88] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [89] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler faster stronger. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4840 4851, 2024. [90] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. scalable and Structured 3d latents versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. for [91] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [92] Bojun Xiong, Jialun Liu, Jiakui Hu, Chenming Wu, Jinbo Wu, Xing Liu, Chen Zhao, Errui Ding, and Zhouhui Lian. Texgaussian: Generating high-quality pbr material via octreebased 3d gaussian splatting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 551561, 2025. [93] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. [94] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. In European Conference on Computer Vision, pages 131147. Springer, 2024. [95] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart√≠n-Mart√≠n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, images, and point clouds for 3d understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11791189, 2023. [96] Le Xue, Ning Yu, Shu Zhang, Artemis Panagopoulou, Junnan Li, Roberto Mart√≠nMart√≠n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, et al. Ulip-2: Towards scalable multimodal pre-training for 3d understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2709127101, 2024. [97] Yutaro Yamada, Khyathi Chandu, Yuchen Lin, Jack Hessel, Ilker Yildirim, and Yejin Collaborative Reasoning as Foundation for 3D Intelligence Choi. L3go: Language agents with chainof-3d-thoughts for generating unconventional objects, 2024. URL https://arxiv.org/ abs/2402.09052. [98] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [99] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. [100] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [101] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM Transactions on Graphics (TOG), 2024. [102] Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging. arXiv preprint arXiv:2503.22236, 3, 2025. [103] Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, and Jun Zhu. Dreamreward: Text-to3d generation with human preference. In European Conference on Computer Vision, pages 259276. Springer, 2024. [104] Junliang Ye, Zhengyi Wang, Ruowen Zhao, ShapellmShenghao Xie, and Jun Zhu. omni: native multimodal llm for 3d generation and understanding. arXiv preprint arXiv:2506.01853, 2025. [105] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67966807, 2024. [106] Fukun Yin, Xin Chen, Chi Zhang, Biao Jiang, Zibo Zhao, Wen Liu, Gang Yu, and Tao Chen. Shapegpt: 3d shape generation with unified multi-modal language model. IEEE Transactions on Multimedia, 2025. [107] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiIn Proceedings of the IEEE/CVF ance fields. international conference on computer vision, pages 57525761, 2021. [108] Tianjiao Yu, Vedant Shah, Muntasir Wahed, Ying Shen, Kiet Nguyen, and Ismini Lourentzou. Part2GS: Part-aware modeling of articulated objects using 3d gaussian splatting. arXiv preprint arXiv:2506.17212, 2025. [109] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via stepwise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [110] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. [111] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. 17 Collaborative Reasoning as Foundation for 3D Intelligence Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [112] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual arXiv preprint arXiv:2407.08739, 2024. instruction tuning. [113] Ruowen Zhao, Zhengyi Wang, Yikai Wang, Zihan Zhou, and Jun Zhu. Flexidreamer: single image-to-3d generation with flexicubes. arXiv preprint arXiv:2404.00987, 2024. [114] Ruowen Zhao, Junliang Ye, Zhengyi Wang, Guangce Liu, Yiwen Chen, Yikai Wang, and Jun Zhu. Deepmesh: Auto-regressive artistmesh creation with reinforcement learning. arXiv preprint arXiv:2503.15265, 2025. [115] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shapeimage-text aligned latent representation. Advances in neural information processing systems, 36:7396973982, 2023. [116] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Predict and Omer Levy. the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. Transfusion: [117] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text In Proceedings of the IEEE/CVF alignment. International Conference on Computer Vision, pages 29112921, 2023. Collaborative Reasoning as Foundation for 3D Intelligence A. Additional Related Work B. Implementation Details Post-Pretraining Reinforcement Learning. Since the introduction of Chain-of-Thought prompting [81], enhancing the reasoning ability of large language models has become an important research focus. More recently, DeepSeek-R1 [26] advanced this direction by proposing rule-based reward design combined with GRPO training, encouraging models to produce explicit intermediate reasoning traces before generating final answers. This paradigm has since been extended to multimodal settings [31, 50, 100, 109], where task-specific rewards guide the learning process. These reasoningdriven methods have achieved notable gains across variety of challenging tasks [46], including mathematical problem solving [111, 112] and code generation [33, 42]. Our proposed CoRe3D framework builds on this line of work by extending GRPO from text-only settings to unified 3D understanding and 3D generation model. We employ collaborative reasoning strategy that tightly couples linguistic semantics with 3D geometry, leading to more coherent and interpretable generation. Octant-based 3D Representations. The concept of octree has been used in wide range of 3D geometric processing applications, including point cloud compression [59], 3D texturing [5, 92], and multi-view scene reconstruction [65, 107]. Beyond these foundational uses, octree structures have become integral to efficient shape analysis in large-scale environments [25, 57, 74, 75, 89]. In the realm of generative modeling, recent advancements [22, 22, 82] employ adaptive tokenization or multi-scale autoregressive strategies resources to allocate computational to geometrically complex regions dynamically. Diverging from these adaptive hierarchical methods, which often result in variable-length sequences or complex tree traversals, our approach utilizes fixedresolution octant-based 3D VQ-VAE to discretize the 3D volume into uniform blocks. This design preserves essential spatial locality while maintaining compact token sequence, thereby facilitating stable and interpretable geometric chain-of-thought reasoning within unified 3D-LLM framework. Octant-based Auto-regressive Model. Autoregressive generation over 3D volumetric representations poses unique challenge: the model must preserve spatial locality while maintaining tractable token length. Traditional raster-order serialization severely disrupts locality in 3D, making next-token prediction unnecessarily difficult. Inspired by ideas from hierarchical octant-based models such as OctFormer [74] and OctGPT [82], we adopt an octant-structured tokenization strategy tailored to our VQ-VAE latent space. Unlike hierarchical octrees, our formulation operates on single-scale 163 latent grid while preserving the locality benefits emphasized in prior octant-based architectures. As shown in Figure 8, our voxel VQ-VAE maps each 643 input volume to 163 latent grid. To reduce sequence length while preserving local geometric structure, we partition this latent grid into nonoverlapping 222 neighborhoods. Each neighborhood forms local octant block that contains eight spatially adjacent latent cells. Concatenating the features of these eight cells produces single octant token, yielding exactly 888 = 512 tokens per object. These tokens summarize compact spatial regions and maintain local geometry and appearance cues. To serialize the latent volume into an autoregressive sequence, we employ Morton (Z-order) space-filling curve, which preserves spatial locality more effectively than raster or lexicographic scanning. Let ùí™ = [o1, o2, . . . , o512] denote the sequence of octant tokens arranged in Morton order. The generative process factorizes the distribution over the latent volume as p(ùí™) = 512 i=1 p(oi o<i, ùíÆsem), (6) where the semantic chain-of-thought ùíÆsem provides high-level cues that guide geometric synthesis. To encode spatial location, we attach learned positional embedding to each octant token, keyed by its block index (xb, yb, zb) in the 888 grid. This embedding is injected after vector quantization, ensuring that the codebook remains content-centric while the autoregressive decoder remains location19 Collaborative Reasoning as Foundation for 3D Intelligence Figure 8: Overview of Octant-based 3D VQVAE. The voxelized geometry is encoded into latent blocks, quantized using shared codebook of 3D embeddings, and decoded back into high-fidelity voxel grid. aware. Together, the octant blocks and positional embeddings introduce two inductive biases: (i) each token carries high-resolution local geometric context, and (ii) the sequence ordering respects 3D locality. During training, the semantic reasoning trace ùíÆsem and the geometric reasoning trace ùí¢geo jointly condition the autoregressive transformer. The semantic trace encodes global planning information such as categories and textures, while the geometric trace provides localized structural cues for neighborhood-level voxel refinement. The decoder predicts discrete codebook index for each octant token, reconstructing the latent volume block-by-block. After predicting all 512 tokens, the VQ-VAE decoder reconstructs dense 643 voxel field, which is subsequently rendered into mesh or multi-view images. Compared to hierarchical octree models [82], which must autoregressively generate thousands of binary split or leaf tokens, our design yields significantly shorter and more expressive sequence. This compact, locality-aware autoregressive formulation is essential to our framework, enabling efficient token-level generation that tightly aligns with semantic and geometric reasoning. Physical Coherence Critic. We use Trimesh to compute physical statistics and PyMeshFix to diagnose self-intersections. Stability is estimated by projecting the mesh center of mass onto the ground plane and checking whether this projection lies inside the convex hull of bottom support vertices extracted from the lowest-z region, yielding normalized stability score. Structural connectivity is measured by splitting the mesh into connected components and computing the fraction of faces belonging to the largest component, which penalizes fragmented or floating parts. To assess self-intersection, we run PyMeshFix on the original mesh and compare the number of faces before and after repair; larger relative change indicates more severe geometric artifacts and results in lower self-intersection score. Hyperparameters. The policy is updated using KLregularized GRPO with ratio clipping. The KL penalty coefficient is set to Œ≤ = 0.01 and the clipping threshold to Œµ=0.1. For each prompt, we sample K=4 candidate generations and compute pairwise preferences using the four critics introduced in 3. The humanpreference, 3D-understanding, Text-3D-alignment, and Physical coherence critics are weighted by (wH, wV, wX, wP) = (0.25, 0.25, 0.25, 0.25). Optimiza6, tion uses AdamW with learning rate of 1 10 Œ≤1=0.9, Œ≤2=0.98, and weight decay 0.01. We apply 2000-step linear warmup followed by cosine decay. For stability, the policy KL is capped at 1.2 its exponential moving average, and gradients are clipped with global norm of 1.0. Evaluation Metrics. We report common textgeneration metrics, i.e.,, BLEU-1 [51], ROUGE-L [38], and METEOR [4]. Although standard, these metrics often favor shorter outputs and may overlook semantic fidelity. To address this, we incorporate embedding-based similarity measures, including Sentence-BERT [56] and SimCSE [23], which evaluate semantic alignment between generated captions and human references more robustly. 20 Collaborative Reasoning as Foundation for 3D Intelligence Figure 9: Additional Results on Challenging Prompts. CoRe3D successfully inferred the true object from the implicit prompts. C. Additional Results Additional Results on Challenging Prompts. Figure 9 presents additional qualitative examples demonstrating CoRe3Ds ability to infer the correct 3D object even when the input prompt provides only indirect or symbolic descriptions. These prompts intentionally avoid naming the target object and instead describe cultural context, functional cues, or high-level visual impressions. Despite this ambiguity, CoRe3D consistently recovers the correct underlying structure. As shown, the model not only identifies the implicit object but also reconstructs spatially coherent and visually faithful 3D shape. To our knowledge, CoRe3D is the first 3D generative framework capable of resolving such implicit, referential prompts through semantic reasoning, rather than relying on explicit object mentions or category supervision. Quantitative Results of Semantic-CoT and Geometric CoT Ablation. We conduct quantitative analysis to isolate the contributions of the two complementary reasoning CoTs. As shown in Table 5, introducing Semantic CoT yields the largest gains in 3D captioning quality, improving METEOR, SentenceBERT, and SimCSE scores by significant margin over ShapeLLM-Omni. This indicates that explicit semantic reasoning helps the model better decompose text prompts into linguistically grounded object attributes. In contrast, Geometric CoT primarily benefits 3D generation. By encouraging structured it spatial reasoning over octant-level geometry, substantially improves CLIP similarity and reduces both FD and KD, demonstrating its effectiveness in guiding the model toward globally coherent 3D shapes. Our full model, CoRe3D, achieves the best overall performance across all metrics. These results verify that collaborative reasoning is essential for robust 3D understanding and generation. Comparison with Zero-shot CoT. Figure 10 and Figure 11 compare our trained semantic-level CoT with zero-shot CoT baseline (Qwen2.5-vl-7B[3]). In the zero-shot setting, the model is prompted to produce free-form structural description before generating the 3D object, but this unguided semantic reasoning pro21 Collaborative Reasoning as Foundation for 3D Intelligence Table 5: Ablation of Semantic CoT and Geometric CoT. Best and second-best results are highlighted. Critic 3D Captioning Model Name ShapeLLM-Omni CoRe3D (w/o Geometric CoT) CoRe3D (w/o Semantic CoT) CoRe3D (Ours) Semantic CoT Geometric CoT METEOR Sentence-BERT SimCSE CLIP 0.29 0.32 0.35 0.38 44.91 50.38 47.32 49.67 12.76 16.42 14.89 17. 48.14 51.14 50.41 52.11 FD 3D Generation incep KD 23.4 21.6 19.8 18.7 incep 0.25 0.21 0.19 0. Table 6: Ablation of Octant Depth in the Octantbased 3D VQ-VAE. Increasing depth increases spatial granularity (8 4096 octants) and improves fidelity up to Depth 3, after which autoregressive instability degrades performance. Best and second-best . Table 7: Ablation of Codebook Size in the Octantbased 3D VQ-VAE. Larger codebooks reduce quantization error but exhibit diminishing returns and instability at extreme scales. Octant depth is fixed at 512 octants (Depth 3). Best and second-best . Architecture 3D Captioning Depth 1 2 3 (Ours) # Octants METEOR SBERT SimCSE CLIP 0.21 0.29 0.38 0.37 43.10 47.88 52.11 51.36 7.34 12.87 17.03 16.44 41.82 46.51 49.67 49.92 8 64 512 4096 FD 3D Generation incep KD 42.3 29.4 18.7 19.0 incep 0.34 0.25 0.17 0.18 Architecture 3D Captioning Codebook Size Octants METEOR SBERT SimCSE CLIP 0.31 0.34 0.38 0.37 2048 4096 8192 (Ours) 47.44 50.03 52.11 51.62 13.81 15.92 17.03 16.77 46.92 48.51 49.67 49.98 512 512 512 512 FD 3D Generation incep KD 27.8 22.1 18.7 19. incep 0.24 0.20 0.17 0.18 vides only shallow, low-information descriptions. For instance, in the compact car example, the zero-shot CoT yields generic outline of smooth white prototype with no color, style cues, or structural modifiers, leading to an oversimplified reconstruction. In contrast, the CoRe3D semantic-level CoT produces richer and more actionable structural cues, such as bright blue body color, semi-transparent windows, stylized panel lines, or the presence of roof rack with parallel bars and fin-like rear element (bottom CoT row), and the model correspondingly reconstructs much more faithful 3D shape. These results show that zeroshot CoT lacks the necessary semantic grounding for 3D generation, whereas CoRe3D learns to produce CoT that is both structurally informative and tightly aligned with the generation process. This highlights the necessity of our collaborative reasoning pipeline. Additional Image-to-3D Results. Figure 12 shows additional qualitative results from our image-to-3D pipeline. Despite being unified model rather than specialized reconstruction system, CoRe3D generates stable and coherent 3D shapes even from visually complex or cluttered inputs. The model maintains globally consistent geometry, plausible spatial structure, and strong color fidelity, delivering reconstructions that capture the essential form, material cues, and overall visual identity of the input objects. Optimal Octant Layers. Table 6 evaluates how the choice of octant depth and therefore the total number of octant tokens affects the performance of our octantbased 3D VQ-VAE. Increasing the depth refines the spatial partitioning of the latent volume (from 8 to 4096 octants), granting the model access to progressively finer geometric detail. We observe consistent upward trend from Depth 1 to Depth 3 across all captioning and generation metrics. Depth 1 (only 8 octants) severely under-parameterizes local geometry, leading to weak semantic alignment and the worst FD/KD scores. Depth 2 (64 octants) recovers coarse global structure but still lacks local detail, resulting in moderate improvements. Depth 3 provides the best balance between spatial granularity and autoregressive stability. It achieves the highest scores for METEOR, SimCSE, CLIP, and the lowest FD/KD, demonstrating that this level of decomposition offers enough local resolution to capture high-frequency geometry without excessively lengthening the autoregressive sequence. We use Depth 3 as our default. At Depth 4, although the finer subdivision (4096 octants) slightly improves SBERT similarity and CLIP alignment, overall performance begins to degrade. Optimal Codebook Size. Table 7 evaluates how the capacity of the VQ-VAE codebook affects both semantic alignment and 3D generation fidelity. small codebook (2048 entries) creates severe quantization bottleneck. This under-expressiveness leads to noticeably lower METEOR, SBERT, and 22 Collaborative Reasoning as Foundation for 3D Intelligence Figure 10: Comparison with Zero-shot CoT. Zero-shot CoT produces shallow, generic structural descriptions, leading to oversimplified 3D shapes. eration can streamline asset creation and accelerate workflows in engineering and education. At the same time, such frameworks carry several potential risks. Unified 3D models may unintentionally reproduce private or proprietary content, and generative pipelines could be misused to create realistic yet deceptive 3D assets. CoRe3D is intended for research and educational use, and we encourage responsible deployment that emphasizes transparency, provenance tracking, consent for training data, and adherence to domain-specific usage guidelines. With such measures in place, we believe the benefits of collaborative reasoning for 3D understanding and generation outweigh the potential risks. SimCSE scores, along with substantially worse FD/KD metrics. Increasing the codebook to 4096 entries alleviates this issue and yields consistent gains across all evaluation dimensions. Our default setting with an 8192-entry codebook achieves the best overall performance. Expanding the codebook further to 16384 entries results in only marginal improvements on SBERT and CLIP similarity, but degrades generation fidelity. These findings indicate that our 8192-entry codebook is the optimal. D. Broader Impacts CoRe3D advances unified 3D intelligence by enabling models to both interpret and construct 3D objects through collaborative semanticgeometricreasoning. Such capabilities can benefit wide spectrum of applications. In robotics and embodied AI, improved spatial reasoning may support better manipulation, affordance understanding, and task planning. In simulation and digital twin systems, controllable 3D gen23 Collaborative Reasoning as Foundation for 3D Intelligence Figure 11: Comparison with Zero-shot CoT (2). Another example showing that zero-shot CoT provides limited guidance, while our trained semantic-level CoT yields detailed structural cues. Collaborative Reasoning as Foundation for 3D Intelligence Figure 12: Additional Image-to-3D Results. We use visually complex image prompts to demonstrate that CoRe3D achieves strong 3D reconstruction capability, producing coherent geometry and spatially consistent shapes even for challenging inputs."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}