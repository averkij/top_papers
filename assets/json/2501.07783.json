{
    "paper_title": "Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding",
    "authors": [
        "Zhaokai Wang",
        "Xizhou Zhu",
        "Xue Yang",
        "Gen Luo",
        "Hao Li",
        "Changyao Tian",
        "Wenhan Dou",
        "Junqi Ge",
        "Lewei Lu",
        "Yu Qiao",
        "Jifeng Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at https://github.com/OpenGVLab/PIIP."
        },
        {
            "title": "Start",
            "content": "Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai 1 5 2 0 2 4 ] . [ 1 3 8 7 7 0 . 1 0 5 2 : r AbstractImage pyramids are widely adopted in topperforming methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose novel crossbranch feature interaction mechanism. To validate PIIP, we apply it to various perception models and representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIPLLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at https://github.com/OpenGVLab/PIIP. Index TermsVision Foundation Models, Visual Perception, Object Detection, Multimodal Large Language Models, Multimodal Understanding I. INTRODUCTION In modern computer vision, advanced image perception and understanding systems heavily depend on large-scale pretrained models, which often require tens of thousands to millions of GPU hours for pretraining [2][5]. To adapt these costly pretrained models for vision perception (e.g., object detection [6][9] and segmentation [10][12]) tasks, the common practice is to combine them with image pyramids [13], [14], i.e. upsample and downsample the image to multiple scales and process them independently, then fuse their outputs. This Zhaokai Wang and Xue Yang are with Shanghai Jiao Tong University. Xizhou Zhu, Wenhan Dou, Junqi Ge, and Jifeng Dai are with Tsinghua University. Gen Luo and Yu Qiao are with Shanghai Artificial Intelligence Laboratory. Hao Li and Changyao Tian are with The Chinese University of Hong Kong. Lewei Lu is with Sensetime. Zhaokai Wang, Xue Yang, Hao Li, Changyao Tian and Jifeng Dai are also with Shanghai Artificial Intelligence Laboratory. Zhaokai Wang, Xizhou Zhu and Xue Yang contribute equally to this work. Corresponding author: Jifeng Dai (daijifeng@tsinghua.edu.cn). preliminary version of this research is published in NeurIPS 2024 [1] as Spotlight. combination is crucial in constructing multi-scale features for visual perception and understanding on high-resolution images. Nonetheless, employing image pyramids with pretrained models incurs substantial computational overhead. Current image pyramids use the same large-scale model to process multiple scales of the same image (Figure 1 (b)), leading to quadratic growth in computational demand as image resolution scales increase. When certain computation budget is imposed, the maximum image resolution is limited, which negatively impacts model performance in visual perception tasks. Although feature pyramids [15][17] aim to reduce this overhead in dense prediction tasks, most top-ranking models [18][21] in the MS COCO challenge [22] still rely on image pyramids due to their performance advantages. Therefore, it is necessary to reduce the computing resources needed to build image pyramids while maintaining high performance for visual perception. High computational costs are also challenge in multimodal understanding. In the field of multimodal large language models (MLLMs) [23][29], previous works have validate that scaling up the resolution of the input image helps to improve the visual understanding ability of MLLMs. Some multi-resolution approaches also adopt image pyramids [30][32] (Figure 1 (f)). Others adopt dynamic high resolution strategy, i.e. partition the image into multiple slices and pass through the vision encoder [33][35] (Figure 1 (g)). However, these methods use visual models with same parameter scale to process large scale and small scale inputs. This inevitably imposes great computational burden, limiting the performance of multimodal understanding under computation budget. Despite that the vision encoder only take up small fraction of the total FLOPs compared with the language model, its depth and input size significantly affect the inference speed, as examined in [25], [36]. Consequently, it is also essential to increase the input resolution while maintaining an acceptable computation cost for multimodal understanding. To address this issue, our primary insight is that it is unnecessary to utilize vision models of identical size across all resolutions (Figure 1(b-c)) or adopt parameter-direct design (Figure 1(d)). Instead, we can adopt parameterinverted design and allow features at different resolutions to complement each other through effective feature fusion. This approach improves computational efficiency and avoids redundant modeling of similar information. In lower-resolution pyramid levels, larger models can efficiently extract rich semantic and contextual features because of the smaller input image size. Conversely, high-resolution branches should 2 Fig. 1. Different multi-resolution designs in visual perception and multimodal understanding. (a)(e) Plain network without multi-scale features. (b)(c)(f) Inefficient image pyramid networks using equivalently large models for all scales, either with shared weights or with separate weights and interactions. (d) Parameter-direct image pyramid network which processes high-resolution images with large models, leading to high computational cost. (g) Multi-resolution approaches on multimodal tasks based on grid partition. (h) Our efficient and effective parameter-inverted image pyramid network (PIIP), which pairs models of increasing parameter sizes inversely with images of decreasing resolution. It achieves better performance with much lower computational cost. focus on capturing the missing detail information without reprocessing semantic data. Thus, high-resolution features can focus on smaller receptive fields with less semantic information, reducing computational costs while retaining important details. Building on this principle, we propose to construct highperformance, low-cost image pyramid network. This design employs series of models with increasing parameter sizes paired with decreasing image resolutions, as illustrated in Figure 1(h). Each resolution level directly leverages existing pretrained vision foundation models for feature extraction, eliminating the need to train multi-scale image pyramid networks from scratch. Additionally, strong feature interactions between levels ensure that features at varying scales remain complementary and avoid redundant feature computation. To implement this, we propose Parameter-Inverted Image Pyramid Networks (PIIP), which leverage the complementary nature of features at different resolutions. This network processes images of various scales using smaller branches for highresolution inputs to capture local details and larger branches for low-resolution inputs to extract global context. Each branch is initialized from pretrained Vision Transformers (ViTs) [37] or Convolutional Neural Networks (CNNs) [38]. Additionally, we introduce feature interaction module that allows features between different resolutions to complement each other. dedicated feature interaction module facilitates the integration of features across multiple scales, reducing the parameter scale of high-resolution branches and ensuring effective information exchange. This approach significantly lowers computational expenses while maintaining superior performance."
        },
        {
            "title": "To validate the capabilities of our method on general visual",
            "content": "perception and understanding tasks, we evaluate its performance on multimodal understanding, object detection, segmentation and image classification. Our method outperforms singlebranch networks and traditional image pyramids while reducing computational costs across all these tasks. PIIP provides new direction for efficient and effective visual computing. Our contributions are as follows1: We identify the key issue of common image pyramids in computer vision, i.e., the expensive computational overhead. To overcome this limitation, we introduce novel framework named Parameter-Inverted Image Pyramid (PIIP) to enhance the multi-scale representational capability of vision backbones while maintaining high computational efficiency. In PIIP, we propose novel strategy to promote the information interactions across different pyramid branches, 1This paper is built upon our work published in NeurIPS 2024 as Spotlight (Top 2.08%) [1] with substantial extensions. Compared to the original version, we extend PIIP in five aspects in terms of model designs and experiments. 1) We extend PIIP to multimodal understanding and propose novel multimodal large language model called PIIP-LLaVA. Experiments demonstrate that our PIIP can significantly help MLLM improve the performance over existing multi-resolution approaches, e.g. +1.4% on average over LLaVA-HR. PIIP-LLaVA achieves 73.0% on TextVQA and 74.5% on MMBench with only 2.8M training data (Section III-C, IV-E). 2) We extend PIIP to different types of visual architectures, including ViT-based architectures and CNNbased architectures. Extensive experiments have validated the generalizability and effectiveness of PIIP (Section III-A, IV-B). 3) PIIP is also adapted to heterogeneous ViT-CNN structures, which can effectively leverage the distinct advantages of ViT and CNN (Section III-A, IV-B, IV-E). 4) More experiments are conducted on different settings, including from-scratch pretraining and stronger detection models. Based on InternViT-6B, PIIP finally achieves 60.0 box AP on object detection (Section IV-B, IV-D). 5) We provide more ablation studies and qualitative analysis to further understand the design principle and the capability of PIIP (Section IV-F, IV-G, IV-H). 3 namely cross-branch interaction. It integrates features of different spatial scales and semantic levels for better visual representation learning. We extend PIIP to various visual architectures, including pureViT, pure-CNN and the heterogeneous ViT-CNN networks. The heterogeneous structure leverages pretrained ViTs like CLIP [5] for global semantic modeling and CNNs for local feature extraction. Based on PIIP, we propose new MLLM termed as PIIPLLaVA, which adopts the PIIP design to achieve efficient and effective high-resolution understanding. Beyond visual perception, PIIP-LLaVA demonstrate the versatility of PIIP in multimodal understanding. We apply our method on various visual perception tasks, including object detection, segmentation, and image classification. Our method surpasses single-branch models and other image pyramid methods with higher performance and lower computation cost. When applied to InternViT-6B [39], largescale vision foundation model, PIIP improves its performance on object detection and semantic segmentation by 1.9% and 1.3% while reducing 43% and 58% of computational costs, respectively. We finally achieve 60.0 APb on COCO [22] and 59.7 mIoU on ADE20K [40]. We also provide extensive ablation studies and valuable design guidelines for PIIP that may benefit future research. We evaluate PIIP-LLaVA on various MLLM benchmarks, which demonstrates superior performance over existing multiresolution approaches, e.g. +1.4% on average over LLaVAHR [30], or 73.0% on TextVQA and 74.5% on MMBench with only 2.8M training data. II. RELATED WORK A. Image Pyramids and Feature Pyramids In order to obtain multi-scale image understanding capabilities for visual perception tasks, especially for dense prediction tasks, modern methods widely adopt image pyramids or feature pyramids. Image pyramids [13], [14], [41] resize the original image into different resolutions and feed into the model separately, allowing for accurately detecting objects at various scales. However, this technique introduces significant computational costs for high-resolution images. Feature pyramids [15][17] represent another method for constructing multi-scale feature representations by combining low-resolution, semantically strong features with high-resolution, semantically weak features. Despite significant reduction in computational costs, feature pyramids cannot fully replace image pyramids when detecting very small or large objects, limiting their performance [41]. Our proposed PIIP adopts the multi-resolution input paradigm from image pyramids while integrating feature interaction from feature pyramids. It introduces the parameter-inverted design to achieve efficient and effective visual perception. B. Multi-Branch Architectures Multi-branch architecture is common approach to combine features from different resolutions in visual perception, including image classification [42], object detection [21], [43][45], semantic segmentation [46], [47] and multimodal understanding [30], [48]. ViT-Adapter [21] and ViT-CoMer [45] use CNN-based branch to insert multi-scale information into pretrained ViT branch for dense prediction tasks. CBNetV2 [44] leverages an assistant backbone and leading backbone in Dense Higher Level Composition manner. These methods adopt asymmetric branch structures, leading to higher design complexity when extending to three or more branches. CrossViT [42] uses two-branch structure with different patch sizes to obtain inputs of various scales, and leverages models of multiple sizes to balance the computational load. HRNet series [43], [46], [47] adopt four-branch architecture for extracting high-resolution features, where the number of branches gradually increases as the layers deepen. However, they do not employ parameter-inverted design and cannot utilize existing pretrained models. In contrast, we propose general visual perception architecture that utilizes pretrained models with different parameter scales to build efficient and powerful image pyramids. C. Multimodal Large Language Models With the rapid development of large language models (LLM) [49][51], multimodal large language models (MLLM) have garnered significant research interest [23], [26], [52], which focus on utilizing pretrained LLMs for multimodal understanding and generation [53][57]. Most existing MLLMs adopt modular structure, i.e. using vision encoder to extract visual features from input images to be fed into the LLM [24], [29], [58]. Some existing approaches attempt to equip MLLMs with high-resolution visual inputs for stronger multimodal understanding ability. LLaVA-UHD [34], [35] employs an image modularization strategy to convert highresolution images into condensed image tokens. LLaVANeXT [33] proposes an AnyRes strategy to use dynamic high resolution image inputs. LLaVA-HR [30] uses two-branch image pyramids with specially designed MR-Adapter module for feature fusion. Mini-Gemini [31] adopts similar twobranch design that injects the high-resolution feature maps into low-resolution ones with patch info mining. MG-LLaVA [32] incorporates object-level features into MLLM through MultiGranularity Vision Flow module to effectively process visual inputs of multiple granularities. However, these methods use vision encoders with shared weights or same parameter scale to process large scale and small scale inputs, instead of the more efficient parameter-inverted design used in PIIP-LLaVA. PIIP-LLaVA also employs cross-branch interactions to combine the multi-scale features for better representation learning. D. Redundancy Reduction for Visual Models. There have been extensive works focus on reducing computational redundancy to accelerate visual models, especially for ViTs. Some work attempts to reduce the number of visual tokens leveraging the sparsity of images to accelerate model inference. For instance, Dynamic ViT [59] and AdaViT [60] predict and prune less informative tokens with lightweight prediction modules. EViT [61] and Evo-ViT [62] identify less informative tokens and adopt accelerated processing strategies by computing attention scores for each token from the class 4 Fig. 2. Overall architecture of PIIP. We use multi-resolution branches to process images of different resolutions, where larger images are handled by smaller models. Each branch leverages pretrained ViTs or CNNs. Interaction units build connections between adjacent branches. Branch merging is inserted after all the blocks or within certain intermediate blocks to combine the features of all branches. images of different resolutions, where larger resolutions are processed by branches with fewer parameters. Cross-branch interactions are inserted every few blocks to fuse features across different feature scales. Branch merging is inserted at the end or within intermediate blocks to combine the outputs from all branches. We use the existing pretrained ViTs [2] [5] or CNNs [38] to initialize the branches, and initialize the interactions and branch merging from scratch. A. Multi-Resolution Branches The multi-resolution branches serve to extract visual representations from different image scales and semantic levels. We first resize the input image to different resolutions through bilinear interpolation, which are then fed into corresponding branches of different scales. Each branch starts with an embedding module, which are patch embedding and position encoding for ViT [37] or patchify stem for ConvNeXt [38]. The number of branches can be 2, 3 or 4. All the branches have the same number of blocks , where each block contains one or multiple ViT or CNN layers. Blocks from different branches usually have different feature dimensions due to the pretrained models, e.g. ViT-T/ConvNeXt-T, ViT-S/ConvNeXtS and ViT-B/ConvNeXt-B. Branches with larger image sizes have smaller number of parameters. For clarity, we refer to the branch with the largest number of parameters (with the smallest image size) as Branch 1, the second largest as Branch 2, and so on. The output of the i-th block of Branch Dj , where Hj, Wj, Pj, Dj is denoted as are the image height, image width, patch size, and feature dimension of Branch j, respectively. RHj Wj /P 2 Typically, all branches are homogeneous, e.g. ViT-T, ViT-S and ViT-B, or ConvNeXt-T, ConvNeXt-S and ConvNeXt-B. Nevertheless, we also extend PIIP to heterogeneous structure, i.e. some branches are ViTs while others are CNNs. Such structure has specific applications. CNN is capable of extracting local features from high-resolution inputs due to its locality Fig. 3. Illustration of PIIP-LLaVA for multimodal understanding. We use one projector after each branch to align the visual features with the language embedding space of the LLM, and combine the features to obtain the visual features. token. Some studies refine the model structure for efficient computation, e.g. attention mechanisms [47], [63], [64], or adopt hierarchical architecture that gradually reduce the spatial resolution as the layers deepen [65], [66]. Orthogonal to these studies, we propose to employ parameter-inverted paradigm to avoid the computational cost of processing highresolution images with large models. III. METHODOLOGY To construct efficient and effective image pyramid networks, we employ multi-branch structure in parameter-inverted manner to handle images of different resolutions with models of different sizes. As shown in Figure 2, our proposed architecture comprises three components: multi-resolution branches, crossbranch interactions, and branch merging. Each branch uses an off-the-shelf pretrained ViT [37] or CNN [38] model to process 5 Fig. 4. Detailed structure of the interaction unit. It consists of two deformable attentions with fully-connect layers and feed-forward networks. Fig. 5. Detailed design of branch merging in different tasks. For detection, segmentation and multimodal understanding, output features from all branches are fused together with projection and upsampling, and fed into the subsequent FPN or LLM. For classification, we employ the original classification heads to compute logits, and average them as the final prediction. and translation invariance, aligning with our intuition that highresolution branch should focus on smaller receptive fields with detailed information. ViT has stronger global modeling ability when the context length is small, i.e. low-resolution inputs, and can be employed to extract rich semantics. This also leverages the powerful semantic representation of ViTs with large-scale pretraining, e.g. CLIP [5] pretrained on the massive image-text paired data. B. Cross-Branch Interactions Branches with different resolutions focus on different spatial scales and semantic levels. To complement the features of various scales, we propose the cross-branch interactions. Each cross-branch interaction consists of several interaction units, where each unit builds connections between outputs from two feature-scale adjacent branches. The structure of an interaction unit is shown in Figure 4. 1 D1 and 2 RH2W2/P 2 1 RH1W1/P 2 Specifically, for the outputs of the i-th block of Branch 1 and 2 D2, 2, denoted as we perform two deformable cross-attention [7] between the two hidden features, denoted as Attn(). Each cross attention is preceded by linear layer FC() to project the feature dimension of key and value into that of the query, i.e. from D1 to D2 or vice versa. feed-forward network FFN() is added after each cross attention to provide channel-wise feature fusion. The hidden dimension ratio of FFN is set to 0.25 to save computational costs. For the first cross-attention in the interaction unit, the interaction process can be formulated as: ˆF 1 = 1 = ˆF 1 + γi 1 + τ 1Attn(norm(F 1FFN(norm( ˆF 1), norm(FC(F 1)), 2))), (1) (2) 1 and γi 1 is the interaction output. τ where norm() is LayerNorm [67], τ 1 are learnable parameters, and 1 and γi 1 are initialized with 0 to ensure that the feature extraction of the original blocks (i.e. distribution of 1) will not be modified drastically during the interactions, thereby better leveraging the pretrained weights. Similarly, the second cross-attention is performed by switch2. The outputs ing the query and key/value to obtain 1 and 2 are used for subsequent feature extractions. We only construct interaction units between each pair of feature-scale adjacent branches, such as Branch 1 & Branch 2 and Branch 2 & Branch 3 for three-branch network. C. Branch Merging The final feature maps of all branches j have different spatial shapes and feature dimensions, where spatially larger feature maps have fewer feature dimensions. Since single feature map fails to provide multi-scale semantic features, we employ branch merging to merge the outputs of all branches into single feature map. In most scenarios, branch merging is added only after all blocks to form the final output. However, for CNN-based detection and segmentation where hierarchical features are required, branch merging is also inserted within intermediate blocks to obtain intermediate outputs, as depicted TABLE COMPARISON WITH BASELINE ON COCO VAL2017. WE REPORT THE NUMBER OF PARAMETERS AND FLOPS OF THE BACKBONE. UNDERLINE INDICATES FLOPS OR METRICS ON PAR WITH THE BASELINE. APb AND APm REPRESENT BOX AP AND MASK AP, RESPECTIVELY."
        },
        {
            "title": "Resolution",
            "content": "#Param #FLOPs"
        },
        {
            "title": "APb APb",
            "content": "Mask R-CNN 1 schedule 50 APb"
        },
        {
            "title": "50 APm\n75",
            "content": "ViTDet-B [68] PIIP-TSB (ours) ViTDet-L [68] PIIP-SBL (ours) 1024 1120/896/448 1568/896/448 1568/1120/672 1024 1120/672/448 1344/896/448 1568/896/ PIIP-TSBL (ours) 1344/896/672/448 1568/1120/672/448 1792/1568/1120/448 90M 146M 147M 149M 308M 493M 495M 497M 506M 507M 512M 463G 243G 287G 453G 1542G 727G 1002G 1464G 755G 861G 1535G 43.8 43.9 45.0 46.6 46.8 46.7 48.2 49.4 46.9 48.2 49.6 67.6 65.7 67.0 68. 70.8 69.0 71.0 71.9 69.9 70.5 72.4 47.7 47.5 48.7 51.1 51.4 50.6 52.8 53.9 50.6 52.7 54.2 39.9 38.6 40.2 41. 42.5 40.8 42.5 43.7 41.6 42.8 44.2 63.6 61.8 63.8 65.2 67.3 65.2 67.3 68.4 65.9 66.9 69.2 42.2 40.6 42.6 44. 45.3 42.8 45.4 46.6 44.1 45.6 47.5 in the bottom of Figure 2. This resembles the classical design of CNN-based detectors [15], [18] where features from middle stages are fed into the feature pyramid network (FPN). The detailed structure of branch merging is illustrated in Figure 5. We elaborate the design in detection and segmentation as an example. All branch outputs except for Branch 1 are first projected to the feature dimension of Branch 1 (the largest feature dimension) with Proj(), which comprises two convolutional layers and GroupNorm [69]. Then, all branch outputs are upsampled by bilinear interpolation Upsample() into the feature map size of the last branch (the largest feature map size). Finally, these outputs, with the same spatial shape and feature dimension, are aggregated with learnable scalar weights wj to form the final output. This process can be formulated as: = Upsample(Proj( F out )), out = (cid:88) j=1 wj out , (3) (4) where is the number of branches. out is the output feature map, which has the largest feature resolution and also the largest feature dimension across all branches. It subsequently serves as input to FPN [15] as in common detection and segmentation models. For image classification, we do not use branch merging module, but append the original classification heads of the pretrained models after each branch. The final predicted score of each class is computed as the average of the output logits of all branches. We observe that using pretrained heads can speed up convergence compared to using randomly initialized head after branch merging. For multimodal understanding, common modular MLLM architectures [26], [29], [70] employ projector between the vision encoder and language model [24], [70] to align the visual features with the LLM embedding space. In our MLLM called PIIP-LLaVA, we add separate projector for each branch inside the branch merging module. The overall structure of PIIP-LLaVA is shown in Figure 3. IV. EXPERIMENTS A. Implementation Details For comparison with base-size models, we use pretrained ViT-T/S/B as the branches to construct three-branch PIIP network PIIP-TSB. Similarly, ViT-S/B/L are used to construct PIIP-SBL to match the computation of large-size models. We also construct four-branch PIIP-TSBL model. For CNN-based and heterogeneous models, we employ ConvNeXt [38] as the branches. Unless specified, PIIP models are ViT-based in perception tasks, i.e. detection, segmentation, and classification. We set the number of interactions (each with 2 interaction units as shown in Figure 2) to 12, i.e. every layer for ViTT/S/B, every two layers for ViT-L, or every three layers for ConvNeXt-S/B/L. We construct multiple variants of two-branch, three-branch and four-branch models with different resolution configurations. For combinations with an inconsistent number of layers, we adopt larger learning rate decay for the backbone with fewer layers. For example, for ViT-S/B (12 layers) and ViT-L (24 layers), the learning rate decay for ViT-S/B is set to be twice that of ViT-L (24/12=2). For object detection and segmentation, we use ViT-S/B/L pretrained on ImageNet [75] from DeiT III [3], ViT-T from DeiT [2]. ViT-H from MAE [76] and InternViT-6B [39] are used for 6B-scale experiments. For all PIIP-SBL models, we use the ImageNet-21K 384-resolution pretrained weights to compare with previous approaches. We adopt AdamW [77] optimizer with layer-wise learning rate decay [78] to train the model on 8 NVIDIA A800 GPUs. For image classification, in base-size experiments we use pretrained ViT-T/S/B weights from DeiT [2]. In large-size experiments, since DeiT does not provide ViT-L models, we use ImageNet-21K pretrained ViT-S/B/L weights from [4]. For multimodal understanding, we CLIP-B/L from [5] and ConvNeXt-B/L pretrained on Laion Aesthetic [79]. 7 Fig. 6. Performance of different PIIP variants by adjusting input resolutions on object detection and instance segmentation. (a) Object detection (b) Instance segmentation TABLE II PERFORMANCE OF CNN-BASED AND HETEROGENEOUS MODELS ON COCO VAL2017."
        },
        {
            "title": "Resolution",
            "content": "#FLOPs Mask R-CNN 1 schedule APb"
        },
        {
            "title": "Model",
            "content": "ViTDet-B [68] ConvNeXt-B [38]"
        },
        {
            "title": "Small",
            "content": ""
        },
        {
            "title": "Tiny",
            "content": ""
        },
        {
            "title": "ConvNeXt",
            "content": ""
        },
        {
            "title": "ViT",
            "content": "1024/672/448 PIIP-TSB (ours)"
        },
        {
            "title": "ConvNeXt",
            "content": "1024/672/"
        },
        {
            "title": "ViT",
            "content": "1024/672/448 1024/672/"
        },
        {
            "title": "ConvNext",
            "content": "1024/672/"
        },
        {
            "title": "ViT",
            "content": "1024/672/"
        },
        {
            "title": "ConvNext",
            "content": "1024/672/"
        },
        {
            "title": "ConvNext",
            "content": "1024/672/448 463G 321G 225G 326G 373G 431G 297G 291G 231G 193G 43. 42.4 44.3 46.4 47.1 46.8 46. 45.4 45.2 44.8 39.9 38.7 39. 41.7 42.4 42.2 42.0 40.9 40. 40.3 We use the FLOPs calculation script from MMDetection [80], with our modifications to accurately calculate FLOPs of selfattention and deformable attention modules. The script is released along with the training code. We have also manually verified the calculations using formulas, and the results are consistent with those produced by the script. B. Object Detection and Instance Segmentation Setting. We conduct object detection and instance segmentation experiments with MMDetection [80] on MS COCO dataset [22]. Three detectors are used, including Mask RCNN [10], Cascade R-CNN [6] and DINO [85]. Following common practices [21], we adopt 1 (12 epochs) or 3 (36 epochs) training schedules and use window attention [68] to save time and memory. The total batch size is 16, and the initial learning rate and weight decay are 1e-4 and 0.05. Comparison with baseline. To demonstrate the performance and computational advantages of PIIP, we validate the effectiveness of PIIP against two baseline models: ViTDet-B and ViTDet-L [68], as shown in Table I. While maintaining similar performance with ViTDet-B, our PIIP-TSB reduces the computational cost by 47.5% (243G vs. 463G) and 38.0% (287G vs. 463G) in object detection and instance segmentation tasks respectively. Similarly, compared with ViTDet-L, our PIIPSBL reduces the computational cost by about 52.9% (727G vs. 1542G) and 35.0% (1002G vs. 1542G) in the two tasks. On the other hand, when keeping similar computational costs as the baseline, PIIP-TSB and PIIP-SBL improve the object detection performance by 2.8% and 2.6%, respectively, and instance segmentation by 1.5% and 1.2%, compared to ViTDet-B and ViTDet-L. To better understand the above conclusion, we depict the trend between the computational cost and performance of different PIIP variants by adjusting the input resolution, as shown in Figure 6. Furthermore, the performance curve for the four-branch structure demonstrates slight improvement over the three-branch structure. Results of CNN-based and heterogeneous models. As given in Table II, we study the effectiveness of CNN-based and CNN-ViT hybrid models. We find that the three-branch ConvNeXt TSB model outperforms the ConvNeXt-B baseline with equivalent computation, demonstrating that PIIP can also effectively reduce computation and improve performance TABLE III OBJECT DETECTION AND INSTANCE SEGMENTATION PERFORMANCE ON COCO VAL2017. MS MEANS USING AUTOAUGMENT [71] FOR MULTI-SCALE TRAINING. LARGE-SIZE MODELS USE VIT WEIGHTS TRAINED ON IMAGENET-21K. SOME RESULTS ARE FROM [21]."
        },
        {
            "title": "Method",
            "content": "PVTv2-B5 [66] ViT-B [72] ViTDet-B [68] Swin-B [65] ViT-Adapter-B [21] PIIP-TSB (ours) ViT-L [72] ViTDet-L [68] ViT-Adapter-L [21] PIIP-SBL (ours) PIIP-SBL-3 (ours) PIIP-H6B-1 (ours)"
        },
        {
            "title": "APb APb",
            "content": "50 APb 75 APm APm 50 APm 75 Mask R-CNN 1 schedule 47.4 42.9 43.2 46.9 47.0 47.9 45.7 46.2 48.7 49. 57.9 60.0 68.6 65.7 65.8 - 68.2 70.2 68.9 69.2 70.1 72.8 76.9 79.0 51.9 46.8 46.9 - 51.4 52.5 49.4 50.3 53.2 54. 42.5 39.4 39.2 42.3 41.8 42.6 41.5 41.4 43.3 44.6 DINO + MS schedule 63.3 65.4 - - 65.7 62.6 62.7 - 65.1 67. 65.6 65.8 67.0 69.3 - - 46.0 42.0 41.4 - 44.9 45.5 44.6 44.1 46.9 47.9 - -"
        },
        {
            "title": "Method",
            "content": "Swin-L [65] ConvNeXt-L [38] PIIP-SBL (ours) Swin-B [65] Shuffle-B [73] ViT-B [72] ViT-Adapter-B [21] PIIP-TSB (ours) Swin-L [65] RepLKNet-31L [74] ConvNeXt-L [38] PIIP-SBL (ours)"
        },
        {
            "title": "APb APb",
            "content": "50 APb 75 APm APm 50 APm 75 Cascade R-CNN 1 schedule 68.4 70.2 70.3 44.9 46.4 46.3 56.2 58.3 57. 71.0 72.8 73.3 Cascade R-CNN 3 + MS schedule 70.9 71.3 69.3 70.6 72.3 72.4 72.5 73.8 73.8 57.0 57.0 54.3 56.5 57.4 58.8 58.6 59.8 59. - - - - 46.5 46.7 46.5 47.6 47.7 - - - - 70.1 70.1 70.0 71.3 71.6 48.9 50.2 50.0 - - - - 51. 50.8 50.6 51.7 52.1 51.8 53.5 53.6 51.9 52.2 50.1 52.1 53.1 53.9 53.9 54.8 54.5 TABLE IV EXPERIMENTS ON THE LARGE-SCALE VISION FOUNDATION MODEL INTERNVIT-6B."
        },
        {
            "title": "Model",
            "content": "#Param Mask R-CNN 1 schedule UperNet 160k #FLOPs"
        },
        {
            "title": "Crop Size",
            "content": "#FLOPs mIoU InternViT-6B [39] PIIP-LH6B (ours) 1024 5919M 24418G 7269M 1280/1024/256 5643G 7271M 10368G 1280/1024/512 7273M 13911G 1280/1024/640 53.8 53.5 54.4 55.7 48.1 47.5 47.8 49. 512 640/512/192 640/512/256 640/512/384 6105G 1903G 2592G 4560G 58.36 57.82 58.42 59.65 on CNN image pyramids. As for heterogenous structures, models with ConvNeXt for higher resolution and ViT for lower resolution achieve the best performance, consistent with our observation in Section IV-E. Results with base-size and large-size models. As shown in Table III, combined with Mask R-CNN, PIIP achieves higher performance than ViT-Adapter by considerable margin, about 0.9% and 1.2% on APb. With more powerful detector Cascade R-CNN and stronger training schedule (3 + MS), PIIP-TSB and PIIP-SBL achieve competitive performance of 53.1% and 54.5% APb, respectively. Finally, we achieve 60.0% APb with the DINO [85] detector. Results with InternViT-6B. In Table IV, we further examine PIIP on an extremely large vision foundation model InternViT6B [39], and achieve 55.7% APb using Mask R-CNN 1 training schedule. In addition, PIIP can save nearly 43% of the computation and achieve better performance than the singlebranch InternViT-6B by 1.9% on APb and 0.9% on APm. Results with different pretraining methods. To study the influence of different pretraining weights, we initialize PIIPSBL with pretrained ViT S/B/L weights from AugReg [4], Uni-Perceiver [81], MAE [76], DeiT III [3], DINOv2 [82] and BEiTv2 [83]. As shown in Table V, the BEiTv2-initialized model achieves the best performance. C. Semantic Segmentation Setting. We use UperNet [11] as the basic framework to train on the ADE20K [40] dataset based on MMSegmentation [94]. We follow the settings of [65] to train the model for 160k iterations. The batch size, initial learning rate and weight decay are 16, 4e-5 and 0.05. Results with base-size and large-size models. In Table VI, PIIP outperforms baseline with fewer computations. Moreover, PIIP-TSB in Table VII attains 51.6% mIoU with UperNet, exceeding InternImage-B [18] by 1.4%. Similarly, PIIP-SBL yields 54.3% mIoU, an outstanding result compared to counterparts like ConvNeXt-XL [38] and InternImage-L [18]. Results with InternViT-6B. Similar to the conclusions in the object detection experiments, InternViT-6B equipped with PIIP achieves performance close to the baseline but saves about 58% FLOPs, and finally achieves 59.65% mIoU. D. Image Classification Results with pretrained models. We load the pretrained models for each branch and train the model for 20 epochs on ImageNet-1K [75]. The batch size, learning rate and weight decay are 1024, 3e-5 and 0.1. The learning rate for the randomly initialized interactions is 10 times the base learning rate, i.e. 3e4. Other settings mainly follow the finetuning recipe of [3]. As shown in Table VIII, when compared with the DeiT baseline, our PIIP-SBL reduces the computational cost by 36.7% (39.0G vs. 61.6G) while maintaining the performance. When using similar computational cost as the baseline models, PIIP-TSB and PIIP-SBL improve the top-1 accuracy by 0.3% and 0.7%, respectively. TABLE EXPERIMENTS OF INITIALIZING WITH DIFFERENT PRE-TRAINED WEIGHTS ON COCO VAL2017 WITH PIIP-SBL 1568/1120/672. TABLE VII SEMANTIC SEGMENTATION PERFORMANCE ON ADE20K USING UPERNET. ViT-S ViT-B / ViT-L"
        },
        {
            "title": "Crop Size",
            "content": "mIoU 9 AugReg [4] AugReg [4] DeiT III [3] Uni-Perceiver [81] DeiT III [3] DeiT III [3] DeiT III [3] DeiT III [3] MAE [76] DeiT III [3] DINOv2 [82] BEiTv2 [83] 48.3 48.8 49.1 50.0 51.0 51. 42.6 42.9 43.0 44.4 44.7 45.4 TABLE VI COMPARISON WITH BASELINE ON ADE20K USING UPERNET."
        },
        {
            "title": "Crop Size",
            "content": "#FLOPS mIoU ViT-B PIIP-TSB (ours) 640 896/448/336 ViT-L PIIP-SBL (ours) 640 1120/448/336 159G 118G 545G 456G 51.0 51.6 53.6 54.3 Swin-B [65] ConvNeXt-B [38] RepLKNet-31B [74] SLaK-B [84] InternImage-B [18] 512 512 512 512 PIIP-TSB (ours) 896/448/336 Swin-L [65] RepLKNet-31L [74] ConvNeXt-L [38] ConvNeXt-XL [38] InternImage-L [18] 640 640 640 640 PIIP-SBL (ours) 1120/448/336 48.1 49.1 49. 50.2 50.2 51.6 52.1 52.4 53. 53.6 53.9 54.3 TABLE VIII IMAGE CLASSIFICATION PERFORMANCE ON IMAGENET. UNDERLINE INDICATES FLOPS OR METRICS ON PAR WITH THE BASELINE. TABLE IX ABLATION ON BRANCH MERGING ON COCO VAL2017 WITH PIIP-TSB 1568/896/672."
        },
        {
            "title": "Resolution",
            "content": "#FLOPs Top-1 Acc"
        },
        {
            "title": "Out Branch APb APm",
            "content": "DeiT-B [2] PIIP-TSB (ours) ViT-L [4] ViT-L [4] (our impl.) PIIP-SBL (ours) PIIP-SBL (ours) 224 368/192/128 224 224 320/160/96 384/192/128 17.2G 17.4G 61.6G 61.6G 39.0G 61.2G 81.8 82.1 84.0 85.2 85.2 85.9 B+S B+T S+T B+S+T 43.1 44.7 45.6 45.4 46.3 46.2 46.6 37.0 39.1 40.6 39.8 41.1 40.9 41.4 From-scratch pretraining. As an preliminary attempt to extend our parameter-inverted image pyramid design to fromscratch pretraining, we design model PIIP-B and evaluate it on ImageNet-1K. The specific configuration of PIIP-B is provided in Table X(a), which is based on the principle that all branches should have similar computational costs, i.e. the number of parameters is inversely proportional to the square of the resolution, while keeping the total number of parameters and FLOPs similar to ViT-B. Contrary to the experiments in Table VIII where the classification heads of the original pretrained models are used, we use branch merging module similar to the one in object detection and append linear layer with GroupNorm [69] for Proj(.), followed by final LayerNorm and linear classification head. We follow the pretraining recipe of [3] and change the number of epochs to 300. The result is provided in Table X(b), where we observe that our three-branch model surpasses the baseline by 0.7%, highlighting the effectiveness of PIIP for from-scratch pretraining. E. Multimodal Understanding Setting. We construct our MLLM, namely PIIP-LLaVA, based on LLaVA-1.5 [24]. We use dual-branch structure with CLIP-L as Branch 1 and CLIP-B/ConvNeXt-B as Branch 2, and Vicuna-1.5-7B/13B as the language model [95], as shown in Figure 3. For the comparison with baseline (Table XI), we only use two interactions to balance computational cost. This is because we empirically find that with certain computational budget, the improvement of using more interactions is smaller in multimodal understanding (Table XIII) than in dense prediction tasks, i.e. detection and segmentation (Figure 7). For the experiments in Table XII, the total number of interactions is set to 12. The training procedure of PIIP-LLaVA consists of two stages, i.e. pretraining stage and instruction tuning stage, following [24]. During pretraining, the vision encoder (branches and interactions) and language model are kept frozen, and only the projectors are optimized. Since γ and τ in the interaction units are fixed as 0, the vision encoder is simply two frozen branches of pretrained models, and each projector serves to align the visual features of one branch with the language 10 TABLE FROM-SCRATCH PRE-TRAINING SETTINGS AND RESULTS ON IMAGENET-1K. (a) Configuration"
        },
        {
            "title": "Module",
            "content": "#Layers Dim #Heads"
        },
        {
            "title": "Resolution",
            "content": "#Param #FLOPs Branch 1 Branch 2 Branch 3 Interactions Branch Merging 12 12 12 12 - 640 320 160 - - 8 4 2 - - 128 256 512 - - 59.6M 15.1M 4.0M 21.2M 0.3M 3.8G 4.3G 4.9G 5.1G 0.2G (b) Performance"
        },
        {
            "title": "Resolution",
            "content": "#Param #FLOPs Top-1 Acc ViT-B (our impl.) PIIP-B (ours) 224 512/256/128 86M 100M 17.5G 18.4G 82.0 82.7 TABLE XI COMPARISON WITH MULTI-RESOLUTION BASELINES ON MULTIMODAL BENCHMARKS. ALL MODELS ARE TRAINED WITH LLAVA-1.5 [24] TRAINING DATA. WE REPORT #FLOPS OF THE VISION ENCODER."
        },
        {
            "title": "Resolution",
            "content": "#FLOPs MMBEN MMVet TextVQA SQAI GQA VQAv"
        },
        {
            "title": "POPE Avg",
            "content": "Models using Vicuna-7B LLaVA-1.5 [70] Fig. 1(e) CLIP-L 336 191G LLaVA-HR [30] Fig. 1(f) ConvNeXt-L, CLIP-L 1024/448 1172G PIIP-LLaVA Fig. 1(h) CLIP-B, CLIP-L 512/256 PIIP-LLaVA Fig. 1(h) ConvNeXt-B, CLIP-L 640/224 PIIP-LLaVA Fig. 1(h) CLIP-B, CLIP-L 512/448 PIIP-LLaVA Fig. 1(h) ConvNeXt-B, CLIP-L 1024/336 Models using Vicuna-13B LLaVA-1.5 [70] Fig. 1(e) CLIP-L 336 LLaVA-HR [30] Fig. 1(f) ConvNeXt-L, CLIP-L 1024/ LLaVA-UHD [34] Fig. 1(g) CLIP-L 1008 PIIP-LLaVA Fig. 1(h) CLIP-B, CLIP-L 512/448 PIIP-LLaVA Fig. 1(h) ConvNeXt-B, CLIP-L 1024/ 193G 191G 422G 598G 191G 1172G 1337G 422G 598G 64.3 66.4 63. 64.5 66.2 67.5 67.7 66.5 68. 67.6 68.5 31.1 31.2 32.0 31. 30.5 31.5 36.1 34.8 36. 37.7 58.2 67.1 57.9 59.0 59. 63.1 61.3 68.1 67.7 61.6 64. 66.8 65.1 68.8 68.3 68.0 68. 71.6 68.1 72.0 71.0 71.1 62. 64.2 62.8 62.1 63.7 62.7 63. 64.8 65.2 64.5 64.2 78.5 81. 79.1 79.9 80.3 81.1 80.0 82. 81.7 81.2 81.8 66.1 64.2 67. 67.5 69.0 69.0 68.2 64.5 69.5 69.3 85.9 87.6 86.5 86. 87.3 87.9 85.9 87.8 89.1 87. 87.9 64.1 66.0 64.8 65.0 65. 66.4 66.8 67.1 67.3 68. embedding space. We follow LLaVA-1.5 to use LCS-558K [24] for pretraining. AdamW [77] is used as the optimizer, and the learning rate and total batch size are set to 1e-3 and 256. During instruction tuning, the entire MLLM is fully optimized, including two branches, interactions, projectors, and the language model. We follow LLaVA-1.5 to use 665k instruction tuning data for finetuning. The learning rate and total batch size are set to 2e-5 and 128, respectively. The training epoch is set to 1 for both the pretraining and instruction tuning stages. When fairly comparing with recent MLLMs like MM1 [93] which are trained with larger data scale, we incorporate additional 1.6M instruction data, including ShareGPT4V [96], LAION-GPT4V [97], ALLAVA [98], LIMA [99], OpenAssistant2 [100], Tabmwp [101], MathQA [102], KVQA [103], Geometry [104], STVQA [105], ChartQA [106], DVQA [107], AI2D [108], LLaVA-Med [109], InfoVQA [110], MathV360k [111], and SQA [112]. Benchmarks. We compare PIIP-LLaVA with other methods on 8 multimodal benchmarks: MMBench-EN [113], MMVet [114], TextVQA [115], SQA-IMG [112], GQA [116], VQAv2 [117], SEED Image [118], and POPE [119]. In particular, MMBench, MMVet and SEED Image evaluate the multimodal perception and cognition abilities of MLLMs. POPE measures the visual hallucinations of MLLMs. TextVQA, SQA-IMG, GQA and VQAv2 evaluate the general visual question answering capability. Specifically, TextVQA contains text-rich images for fine-grained recognition, which requires high-resolution understanding capability. Comparison with multi-resolution baselines. Table XI shows the comparison with other multi-resolution baselines. We observe that with nearly equivalent computation costs, PIIPLLaVA surpasses LLaVA-1.5 [24] on all of the benchmarks, with +1.1% on average and +2.0% on SQA-Image. This is because the parameter-inverted design helps to build efficient image pyramids, thereby allowing for larger input resolution (i.e. 512 or 640 compared with 336 of LLaVA-1.5) and stronger image understanding ability. We also find that heterogeneous structures (ConvNeXt-B + CLIP-L) show advantages when handling high resolution inputs, as CNN contribute to extracting local details and ViT extract high-level semantics, similar to the analysis in Section III-A. TABLE XII COMPARISON WITH EXISTING MLLMS ON MULTIMODAL BENCHMARKS. DATA DENOTES THE DATA SIZE OF ALL TRAINING STAGES. *IMAGES FROM SQA-IMG TEST SET ARE OBSERVED IN TRAINING, SO WE MARK ITS RESULT AND AVG IN GRAY. Method Vision Encoder Resolution LLM Data MMBEN MMVet TextVQA SQAI GQA VQAv2 SEEDI POPE Avg 11 InstructBLIP [86] QwenVL-Chat [29] LLaVA-1.5 [70] ViT-g ViT-G CLIP-L 224 448 Vicuna-7B 130M Qwen-7B Vicuna-7B LLaVA-HR [30] ConvNeXt-L, CLIP-L 1024/448 Vicuna-7B PIIP-LLaVA PIIP-LLaVA BLIP-2 [87] InstructBLIP [86] Shikra [88] LLaVA-1.5 [70] ConvNeXt-B, CLIP-L 1024/336 Vicuna-7B ConvNeXt-L, CLIP-L 1024/336 Vicuna-7B ViT-g ViT-g CLIP-L CLIP-L 224 224 224 336 Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B LLaVA-HR [30] ConvNeXt-L, CLIP-L 1024/448 Vicuna-13B LLaVA-UHD [34] CLIP-L 1008 Vicuna-13B PIIP-LLaVA PIIP-LLaVA ConvNeXt-B, CLIP-L 1024/336 Vicuna-13B ConvNeXt-L, CLIP-L 1024/336 Vicuna-13B"
        },
        {
            "title": "More training data",
            "content": "mPLUG-Owl2 [89] CLIP-L SPHINX-intern2 [90] ConvNeXt-XXL, DINOv2-g Slime [91] LLaVA-NeXT [33] LLaVA-UHD v2 [35] Mini-Gemini [31] MG-LLaVA [32] InternLM-XC [92] MM1 [93] PIIP-LLaVA CLIP-L CLIP-L CLIP-L CLIP-L ConvNeXt-L, CLIP-L, RAMPlus, OWL-ViTv2-L EVA-G CLIP-L 448 2016 1344 1008 Llama-2-7B 400M InternLM2-7B 16M Vicuna-7B Vicuna-7B Vicuna-7B 768/336 Vicuna-7B 768/336 Vicuna-7B 224 1792 InternLM-7B 1.1B 7B 1B ConvNeXt-L, CLIP-L 1024/336 Vicuna-7B 2.7M 1.4B 1.2M 1.2M 1.2M 1.2M 129M 130M 6M 1.2M 1.2M 1.2M 1.2M 1.2M 2M 1.6M 1.4M 2.8M 2.5M 38.3 60.9 64.3 66.4 67.5 67. 39.8 60.2 67.7 66.5 68. 68.5 66.5 64.5 57.9 69.3 68.2 69.3 72.1 74.4 72.3 74. 26.2 31.1 31.2 31.5 31. 22.4 25.6 36.1 34.8 37.7 36.8 36.2 36.5 35.4 43. 40.8 41.0 35.2 42.1 44. 50.1 61.5 58.2 67.1 63.1 67. 42.5 50.7 61.3 68.1 67. 64.2 69.2 58.2 64.4 64. 67.6 65.2 67.3 72.8 73. 60.5 68.2 66.8 65.1 68.1 68. 61.0 63.1 71.6 68.1 72. 71.1 69.8 68.7 70.4 76.8 70. 71.3 70.8 72.6 49. 57.5 62.0 64.2 62.7 63.9 41. 49.5 63.3 64.8 65.2 64. 65.2 56.1 56.2 63.1 64.2 65. 95.0* 62. 78.2 78.5 81.9 81.1 81. 41.0 77.4 80.0 82.3 81. 81.8 82.5 79.4 75.5 80.3 81. 80.2 82.8 82. 65.4 66.1 64.2 69.0 69. 46.4 68.2 64.5 69.3 70.5 57.8 68.8 70. 70.0 68.9 69.4 66.9 69.9 72. 85.9 87.6 87.9 88. 85.3 78.9 85.9 87.8 89. 87.9 87.6 86.9 85.4 86. 86.6 87. 64.1 66.0 66.4 67. 66.8 67.1 68.0 68.5 74.0* When we scale up the input resolution and compare PIIPLLaVA with image pyramid method LLaVA-HR [30] and dynamic high resolution method LLaVA-UHD [34], PIIPLLaVA achieves comparable or better performance with 44% or 51% of the computational costs. These results confirm the generalization of PIIP as an efficient and effective multiresolution approach in multimodal understanding tasks. Comparison with existing MLLMs. In Table XII, we further scale up the ConvNeXt model and the training data size and compare PIIP-LLaVA with existing MLLMs. When trained with 1.2M data, PIIP-LLaVA achieves superior performance over existing MLLMs, e.g. +1.4% over LLaVA-HR-13B. When trained with more data, PIIP-LLaVA outperforms all models, including MM1 [93] with 1B training data and 1792 resolution, e.g. 74.5% on MMBenc, 73.0% on TextVQA, and 87.5% on POPE. These results demonstrate the effectiveness of PIIP on multimodal perception and cognition, general visual question answering and eliminating hallucination. F. Ablation Study We conduct ablation studies to evaluate the impact of different design choices. We first study the influence of unfreezed modules and number of interactions on multimodal understanding, and then provide extensive experimental results on MS COCO object detection. Ablation on multimodal understanding tasks. As shown in Table XIII, we test different numbers of interactions and unfreezed modules of the vision encoder during pretraining, and adjust the resolution to maintain computational cost unchanged. We observe that while 2 interactions achieve better performance than no interactions, 12 interactions result in negative effect. For unfreezed modules, when unfreeze the interactions or the entire vision encoder (branches and interactions) during pretraining, the performance declines significantly. This suggests the importance of training solely the projectors and aligning the vision encoder with the language model. Superiority of parameter-inverted image pyramid networks. In Table XIV, we evaluate the effectiveness of the image pyramid and parameter-inverted design by comparing our method with other methods, e.g. designs in Figure 1. We provide three analyses: 1) single-branch model with multi-scale training represents the most basic implementation of an image pyramid (row 2). Compared with the baseline model, its performance improvement is limited (44.8% vs. 43.8%). 2) Interactions provide cross-branch feature fusion that are beneficial for visual perception (row 3,4,5). 3) We conduct experiments by controlling the scale of the branches and the input resolutions while keeping the computational cost close. Specifically, when using the same input image resolution, the combination of models of different sizes does not bring significant improvements to detection performance (row 6). Correspondingly, when the three branches use the same model (e.g. BBB), the input image resolution is adjusted to the pyramid structure (row 5). The performance of the final model is slightly improved on APb (44.5% vs. 43.8%), but the APm drops significantly (38.7% vs 39.9%) due to the 12 TABLE XIII ABLATION ON MULTIMODAL UNDERSTANDING WITH CLIP-B, CLIP-L AND VICUNA-7B. #Interactions"
        },
        {
            "title": "Pretrain Unfreeze Resolution",
            "content": "#FLOPs MMBEN MMVet TextVQA SQAI GQA VQAv"
        },
        {
            "title": "POPE Avg",
            "content": "2 0 12 2 2 none none none interactions all 512/256 528/ 464/256 512/256 512/256 193G 196G 194G 193G 193G 65.0 65.7 64.6 63. 34.3 34.0 29.5 31.3 25.4 15. 57.9 56.8 57.4 52.4 43.3 68. 69.2 69.5 70.6 67.7 62.8 63. 62.7 61.1 44.6 79.1 78.9 79. 75.9 52.3 67.3 66.8 67.1 62. 41.0 86.5 85.4 86.1 85.5 71. 65.2 64.4 64.7 62.1 46.3 TABLE XIV ABLATION ON IMAGE PYRAMID AND PARAMETER-INVERTED DESIGN. PI, IP AND INTER. REPRESENT PARAMETER-INVERTED, IMAGE PYRAMID AND INTERACTIONS. MS MEANS MULTI-SCALE TRAINING, FOLLOWING [71]."
        },
        {
            "title": "Branches",
            "content": "PI IP Inter."
        },
        {
            "title": "Resolution",
            "content": "#Param #FLOPs"
        },
        {
            "title": "APb APb",
            "content": "Mask R-CNN 1 schedule 50 APb 75 APm APm 50 APm 75 Fig. 1(a) Fig. 1(b) - - Fig. 1(c) - Fig. 1(d) Fig. 1(h) Fig. 1(a) Fig. 1(c) - Fig. 1(h)"
        },
        {
            "title": "L\nLLL\nSBL\nSBL",
            "content": ""
        },
        {
            "title": "1024\nMS\n896/448/224\n896/672/224\n896/448/224\n896/896/896\n448/672/896\n1568/1120/672",
            "content": "1024 896/448/224 848/848/848 1568/896/672 90M 90M 262M 263M 341M 148M 147M 149M 308M 1053M 495M 497M 463G 463G 369G 457G 466G 468G 452G 453G 1542G 1458G 1539G 1464G 43.8 44.8 43.3 43.8 44.5 44.6 42.6 46. 46.8 46.9 47.2 49.4 67.6 69.2 65.8 66.3 66.5 66.4 64.2 68.4 70.8 69.7 69.4 71.9 47.7 49.1 46.6 47.3 48.2 48.3 45.6 51.1 51.4 51.2 51.0 53.9 39.9 41.0 37.9 38.2 38.7 39.0 36.5 41. 42.5 40.8 41.1 43.7 63.6 65.8 61.5 62.2 62.6 62.7 59.5 65.2 67.3 65.3 65.4 68.4 42.2 43.9 39.6 39.7 40.6 41.4 38.0 44.3 45.3 43.3 43.7 46.6 TABLE XV BASELINE WITH HIGHER RESOLUTION."
        },
        {
            "title": "Resolution",
            "content": "#Param #FLOPs APb ViTDet-L ViTDet-L 1024 1792 PIIP-TSBL 1792/1568/1120/448 308M 308M 512M 1542G 6458G 1535G 46.8 48.3 49.4 reduction of the maximum resolution. The former demonstrates the importance of the image pyramid, and the latter further demonstrates the need for the image pyramid to maintain larger image scale range, which is especially essential for the instance segmentation task. Drawing on experience, parameter-inverted image pyramid networks are an efficient design that meets the above requirements, especially when compared to its opposite configuration parameter-direct image pyramid (row 7), i.e. TSB with 448/672/896 resolution (46.6% vs. 42.6%). With less computation than the baseline, PIIP can support image inputs in the maximum range from 672 to 1568 (row 8), and the performance is significantly improved. Baseline with higher resolution. To evaluate the impact of using higher resolutions, we add another baseline ViTDetL with 1792 resolution (matching the largest resolution of PIIP-TSBL 1792/1568/1120/448), as shown in the second row of the Table XV. The first and third rows are from Table 1. We observe that while ViTDet-L with 1792 resolution achieves better performance compared to the 1024 resolution, its FLOPs are approximately 4 times larger. Compared with PIIP-TSBL, ViTDet-L 1792 has lower box AP (-1.3%) and incurs 4 times the computational cost in FLOPs. This experiment explains that the performance improvement does not entirely come from larger image resolution. The structure of PIIP also makes an important contribution in the computational cost and performance improvement. Design guidelines for PIIP. Through extensive practice, we derive empirical design guidelines when scaling up the model: 1) Prioritize increasing the image resolution of the largest image branch: as shown in the blue dashed circle in Figure 7 (a), the input resolution of the largest image branch is greatly increased without causing sharp increase in the total computational cost. 2) The scale of the largest model does not need to exceed the baseline model: the introduction of larger models restricts the resolution range of the image pyramid, e.g. PIIP-TSB is more cost-effective than PIIP-TBL according to Figure 7 (a). Branch merging. We examine the effects of using feature maps from all branches in branch merging. Experiments in Table IX demonstrate that merging all branches yields the best performance by providing multi-scale semantically rich features, compared to using feature maps from only one or two of the branches. TABLE XVI ABLATION ON ATTENTION IMPLEMENTATION AND NUMBER OF INTERACTIONS WITH PIIP-TSB 1120/896/448. #Interaction"
        },
        {
            "title": "Regular Attention",
            "content": "#FLOPs APb APb l"
        },
        {
            "title": "APb",
            "content": "m APb s"
        },
        {
            "title": "Deformable Attention\nAPb",
            "content": "#FLOPs APb APb APb 0 1 2 4 6 12 176G 211G 245G 315G 384G 592G 41.3 41.1 41.7 41.6 42.1 42.0 59.0 59.1 59.5 59.2 59.7 60. 44.6 44.9 45.2 45.3 45.8 45.9 22.5 22.6 22.7 22.8 23.2 23.1 176G 182G 187G 198G 210G 243G 41.3 41.9 42.5 43.0 43.3 43.9 59.0 59.8 60.5 61.0 61.8 62.4 44.6 45.5 46.4 47.3 46.9 47. 22.5 22.4 23.1 23.3 23.6 24.4 (a) Variants with different resolutions (b) Number of interactions Fig. 7. Ablation on model variants and number of interactions. Attention implementation. The core of information interaction between branches is cross-attention mechanism. We adopt PIIP-TSB with resolution 1120/896/448 as the basic model and investigate two different attention implementations. As shown in Table XVI, deformable attention [7] with linear complexity can significantly improve the performance without substantially increasing the computational cost. We opt for using deformable attention as the default configuration. Number of interactions. We test different number of interactions in object detection. As shown in Table XVI, regardless of the attention implementations, the increase in the number of interactions helps to improve the performance to varying degrees. Since this also increases the computational cost, we further explore the cost-effectiveness of the number of interactions. We conduct experiments with multiple resolution combinations on models with different numbers of interactions, and the scatter plot of all results is shown in Figure 7 (b). It can be seen that when the number of interactions is small (i.e. less than 2), the growth trend of model performance with the increase in computational cost is relatively slow. When the number of interactions is 4, 6, or 12, the trend is similar. This is different from our observation in multimodal understanding. We attribute this to the fact that object detection requires higher resolution (e.g. detecting small objects) and therefore more cross-scale feature fusion. For multimodal understanding, the resolution of all branches are closer, and the task itself focuses more on semantics rather than detailed localization. Interaction direction between branches. We compare five different interaction directions in Table XVII. Considering both the computational cost and performance, we ultimately select the fourth method, i.e. bidirectional connections of adjacent branches, as the default choice. As can be seen from Figure 8, all the interaction directions achieve satisfactory performance-computation balance, validating their ability to improve communication between branches. G. Visualization To gain deeper understanding of the internal mechanisms of PIIP, we visualize the attention map and Fourier spectrum of feature maps of two-branch model, i.e. Block 6 of PIIP-H6B in object detection. As shown in Figure 9, the two branches display distinct patterns. Branch 1, the low-resolution one, focuses on the global structure and semantic information. Its attention map highlights key objects in the image, like the dog and cat in the example. The Fourier spectrum demonstrates TABLE XVII ABLATION ON INTERACTION DIRECTIONS WITH PIIP-TSB UNDER RESOLUTION 1120/896/448."
        },
        {
            "title": "Type",
            "content": "#FLOPs APb APm 210G 43.5 38.7 230G 43.2 38.3 230G 43.6 38.6 243G 43.9 38.6 283G 44.0 38. Fig. 8. Performance of different interaction directions. that feature map of Branch 1 contains more low-frequency components. On the contrary, Branch 2 with high resolution concentrates on details of the image with localization information. The attention map emphasizes edges of the objects, and the feature map has more high-frequency components. These findings are consistent with our motivation in Section that the lowresolution branch extracts global semantics and the highresolution branch captures detailed information. H. Qualitative Analysis Figure 10 shows examples of PIIP-SBL on object detection and instance segmentation. Owing to the high-resolution processing capability, our model can detect small objects accurately, like the tiny bench and persons in the background of the upper-left image, or the small cars in the upper-right image, demonstrating its ability in visual perception. In Figure 11, we provide qualitative results of PIIP-LLaVA in multimodal understanding. Compared with the LLaVA-1.5 baseline, PIIP-LLaVA has stronger ability on fine-grained vision-language tasks like text recognition (row 1 left and 3), counting (row 1 right and 5), and fine-grained recognition (row 4). For the challenging task visual information extraction (row 2), PIIP-LLaVA can extract the visual information in the identification card and organize it correctly, which is even comparable with GPT-4v [23]. These results highlight the effectiveness of PIIP in multimodal understanding. Fig. 9. Attention map visualization and Fourier spectrum of feature maps from two branches. Branch 1 with higher resolution focuses on the semantics-rich objects of low-frequency components. Branch 2 with lower resolution highlights localization like edges and details of high-frequency components. V. CONCLUSION In this paper, we propose the Parameter-Inverted Image Pyramid Networks (PIIP), novel architecture that addresses the computational inefficiencies of traditional image pyramid methods by pairing smaller models with high-resolution images and larger models with low-resolution images. PIIP leverages pretrained models, heterogeneous ViT-CNN architectures, and dedicated feature interaction mechanism. Through extensive experiments on visual perception and multimodal understanding tasks, such as object detection, segmentation, image classification, and multimodal understanding, PIIP demonstrates its effectiveness in balancing computational cost and performance. It achieves superior results over single-branch and existing multiresolution approaches, achieving state-of-the-art performance on large-scale vision foundation models and multimodal large language models while significantly reducing computational requirements. Our contributions validate the versatility of PIIP in addressing diverse challenges in vision perception and understanding. We hope this work inspires further exploration into efficient and effective multi-scale feature extraction for future vision and multimodal computing tasks. 15 Fig. 10. Qualitative results on object detection and instance segmentation. Please zoom in to view detailed bounding boxes and masks. High-resolution processing capability enables PIIP to accurately detect small objects in the images. 16 Fig. 11. Qualitative results of PIIP-LLaVA on multimodal understanding. Red denotes incorrect answers and green denotes correct ones. PIIP-LLaVA is capable of tackling fine-grained vision-language tasks like counting, text recognition and visual information extraction."
        },
        {
            "title": "REFERENCES",
            "content": "[1] X. Zhu, X. Yang, Z. Wang, H. Li, W. Dou, J. Ge, L. Lu, Y. Qiao, and J. Dai, Parameter-inverted image pyramid networks, in NeurIPS, 2024. 1, 2 [2] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, Training data-efficient image transformers & distillation through attention, in ICML, 2021. 1, 4, 6, 9 [3] H. Touvron, M. Cord, and H. Jegou, Deit iii: Revenge of the vit, in ECCV, 2022. 1, 4, 6, 8, 9 [4] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and L. Beyer, How to train your vit? data, augmentation, and regularization in vision transformers, arXiv preprint arXiv:2106.10270, 2021. 1, 4, 6, 8, 9 [5] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in ICML, 2021. 1, 3, 4, 5, [6] Z. Cai and N. Vasconcelos, Cascade r-cnn: Delving into high quality object detection, in CVPR, 2018. 1, 7 [7] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, Deformable detr: Deformable transformers for end-to-end object detection, in ICLR, 2021. 1, 5, 13 [8] X. Yang, J. Yang, J. Yan, Y. Zhang, T. Zhang, Z. Guo, X. Sun, and K. Fu, Scrdet: Towards more robust detection for small, cluttered and rotated objects, in ICCV, 2019. 1 [9] X. Yang, J. Yan, Z. Feng, and T. He, R3det: Refined single-stage detector with feature refinement for rotating object, in AAAI, 2021. 1 [10] K. He, G. Gkioxari, P. Dollar, and R. Girshick, Mask r-cnn, in ICCV, 2017. 1, [11] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, Unified perceptual parsing for scene understanding, in ECCV, 2018. 1, 8 [12] Q. Wen, J. Yang, X. Yang, and K. Liang, Patchdct: Patch refinement for high quality instance segmentation, in ICLR, 2023. 1 [13] B. Singh, M. Najibi, and L. S. Davis, Sniper: Efficient multi-scale training, in NeurIPS, 2018. 1, [14] M. Najibi, B. Singh, and L. S. Davis, Autofocus: Efficient multi-scale inference, in ICCV, 2019. 1, 3 [15] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie, Feature pyramid networks for object detection, in CVPR, 2017. 1, 3, 6 [16] G. Ghiasi, T.-Y. Lin, and Q. V. Le, Nas-fpn: Learning scalable feature pyramid architecture for object detection, in CVPR, 2019. 1, 3 [17] M. Tan, R. Pang, and Q. V. Le, Efficientdet: Scalable and efficient object detection, in CVPR, 2020. 1, 3 [18] W. Wang, J. Dai, Z. Chen, Z. Huang, Z. Li, X. Zhu, X. Hu, T. Lu, L. Lu, H. Li et al., Internimage: Exploring large-scale vision foundation models with deformable convolutions, in CVPR, 2023. 1, 6, 8, 9 [19] Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y. Cao, Eva: Exploring the limits of masked visual representation learning at scale, in CVPR, 2023. [20] Z. Zong, G. Song, and Y. Liu, Detrs with collaborative hybrid assignments training, in ICCV, 2023. 1 [21] Z. Chen, Y. Duan, W. Wang, J. He, T. Lu, J. Dai, and Y. Qiao, Vision transformer adapter for dense predictions, in ICLR, 2023. 1, 3, 7, 8 [22] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, Microsoft coco: Common objects in context, in ECCV, 2014. 1, 3, 7 [23] Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, and L. Wang, The dawn of lmms: Preliminary explorations with gpt-4v (ision), arXiv: 2309.17421, 2023. 1, 3, 14 [24] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, in CVPR, 2024. 1, 3, 6, 9, 10 [25] G. Luo, X. Yang, W. Dou, Z. Wang, J. Dai, Y. Qiao, and X. Zhu, Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training, arXiv preprint arXiv:2410.08202, 2024. 1 [26] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu et al., Expanding performance boundaries of opensource multimodal models with model, data, and test-time scaling, arXiv preprint arXiv:2412.05271, 2024. 1, 3, 6 [27] Y. Tang, A. Qu, Z. Wang, D. Zhuang, Z. Wu, W. Ma, S. Wang, Y. Zheng, Z. Zhao, and J. Zhao, Sparkle: Mastering basic spatial capabilities in vision language models elicits generalization to composite spatial reasoning, arXiv preprint arXiv:2410.16162, 2024. 1 17 [28] Y. Luo, G. Luo, J. Ji, Y. Zhou, X. Sun, Z. Shen, and R. Ji, γ mod: Exploring mixture-of-depth adaptation for multimodal large language models, arXiv preprint arXiv:2410.13859, 2024. [29] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, Qwen-vl: frontier large vision-language model with versatile abilities, arXiv preprint arXiv:2308.12966, 2023. 1, 3, 6, 11 [30] G. Luo, Y. Zhou, Y. Zhang, X. Zheng, X. Sun, and R. Ji, Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models, arXiv preprint arXiv:2403.03003, 2024. 1, 3, 10, 11 [31] Y. Li, Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu, and J. Jia, Mini-gemini: Mining the potential of multi-modality vision language models, arXiv: 2403.18814, 2024. 1, 3, 11 [32] X. Zhao, X. Li, H. Duan, H. Huang, Y. Li, K. Chen, and H. Yang, Mg-llava: Towards multi-granularity visual instruction tuning, arXiv preprint arXiv:2406.17770, 2024. 1, 3, 11 and C. Li, [33] B. Li, K. Zhang, H. Zhang, D. Guo, R. Zhang, F. Li, Y. Zhang, supercharge Z. Liu, multimodal capabilities in the wild, 2024. [Online]. Available: https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/ 1, 3, 11 Llava-next: Stronger llms [34] Z. Guo, R. Xu, Y. Yao, J. Cui, Z. Ni, C. Ge, T.-S. Chua, Z. Liu, and G. Huang, Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images, in ECCV, 2024. 1, 3, 10, 11 [35] Y. Zhang, Y. Liu, Z. Guo, Y. Zhang, X. Yang, C. Chen, J. Song, B. Zheng, Y. Yao, Z. Liu et al., Llava-uhd v2: an mllm integrating high-resolution feature pyramid via hierarchical window transformer, arXiv preprint arXiv:2412.13871, 2024. 1, 3, 11 [36] H. Diao, Y. Cui, X. Li, Y. Wang, H. Lu, and X. Wang, Unveiling encoder-free vision-language models, in NeurIPS, 2024. 1 [37] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in ICLR, 2021. 2, 4 [38] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, convnet for the 2020s, in CVPR, 2022. 2, 4, 6, 7, 8, 9 [39] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, B. Li, P. Luo, T. Lu, Y. Qiao, and J. Dai, Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in CVPR, 2024. 3, 6, 8 [40] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, Scene parsing through ade20k dataset, in CVPR, 2017. 3, 8 [41] B. Singh and L. S. Davis, An analysis of scale invariance in object detection snip, in CVPR, 2018. [42] C.-F. R. Chen, Q. Fan, and R. Panda, Crossvit: Cross-attention multiscale vision transformer for image classification, in ICCV, 2021. 3 [43] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang et al., Deep high-resolution representation learning for visual recognition, in TPAMI, 2020. 3 [44] T. Liang, X. Chu, Y. Liu, Y. Wang, Z. Tang, W. Chu, J. Chen, and H. Ling, Cbnet: composite backbone network architecture for object detection, in TIP, 2022. 3 [45] C. Xia, X. Wang, F. Lv, X. Hao, and Y. Shi, Vit-comer: Vision transformer with convolutional multi-scale feature interaction for dense predictions, in CVPR, 2024. 3 [46] Y. Yuan, R. Fu, L. Huang, W. Lin, C. Zhang, X. Chen, and J. Wang, Hrformer: High-resolution vision transformer for dense prediction, in NeurIPS, 2021. [47] J. Gu, H. Kwon, D. Wang, W. Ye, M. Li, Y.-H. Chen, L. Lai, V. Chandra, and D. Z. Pan, Hrvit: Multi-scale high-resolution vision transformer, arXiv preprint arXiv:2111.01236, 2021. 3, 4 [48] W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding et al., Cogagent: visual language model for gui agents, arXiv preprint arXiv:2312.08914, 2023. 3 [49] OpenAI, GPT-4 technical report, arXiv: 2303.08774, 2023. 3 [50] A. Anthropic, The claude 3 model family: Opus, sonnet, haiku, Claude-"
        },
        {
            "title": "3 Model Card, 2024. 3",
            "content": "[51] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, Z. Chen, P. Chu et al., Internlm2 technical report, arXiv preprint arXiv:2403.17297, 2024. 3 [52] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth et al., Gemini: family of highly capable multimodal models, arXiv: 2312.11805, 2023. 3 [53] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu et al., Emu3: Next-token prediction is all you need, arXiv preprint arXiv:2409.18869, 2024. 3 18 [54] H. Li, C. Tian, J. Shao, X. Zhu, Z. Wang, J. Zhu, W. Dou, X. Wang, H. Li, L. Lu et al., Synergen-vl: Towards synergistic image understanding and generation with vision experts and token folding, arXiv preprint arXiv:2412.09604, 2024. 3 [55] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan et al., Janus: Decoupling visual encoding for unified multimodal understanding and generation, arXiv preprint arXiv:2410.13848, 2024. [56] C. Tian, X. Zhu, Y. Xiong, W. Wang, Z. Chen, W. Wang, Y. Chen, L. Lu, T. Lu, J. Zhou et al., Mm-interleaved: Interleaved imagetext generative modeling via multi-modal feature synchronizer, arXiv preprint arXiv:2401.10208, 2024. 3 [57] J. Zhu, X. Ding, Y. Ge, Y. Ge, S. Zhao, H. Zhao, X. Wang, and Y. Shan, Vl-gpt: generative pre-trained transformer for vision and language understanding and generation, arXiv preprint arXiv:2312.09251, 2023. 3 [58] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma et al., How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, arXiv:2404.16821, 2024. 3 [59] Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh, Dynamicvit: Efficient vision transformers with dynamic token sparsification, in NeurIPS, 2021. 3 [60] L. Meng, H. Li, B.-C. Chen, S. Lan, Z. Wu, Y.-G. Jiang, and S.-N. Lim, Adavit: Adaptive vision transformers for efficient image recognition, in CVPR, 2022. 3 [61] Y. Liang, C. Ge, Z. Tong, Y. Song, J. Wang, and P. Xie, Not all patches are what you need: Expediting vision transformers via token reorganizations, in ICLR, 2022. [62] Y. Xu, Z. Zhang, M. Zhang, K. Sheng, K. Li, W. Dong, L. Zhang, C. Xu, and X. Sun, Evo-vit: Slow-fast token evolution for dynamic vision transformer, in AAAI, 2022. 3 [63] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, Linformer: Selfattention with linear complexity, arXiv preprint arXiv:2006.04768, 2020. 4 [64] H. Cai, J. Li, M. Hu, C. Gan, and S. Han, Efficientvit: Lightweight multi-scale attention for high-resolution dense prediction, in ICCV, 2023. 4 [65] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in ICCV, 2021. 4, 8, 9 [66] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao, Pyramid vision transformer: versatile backbone for dense prediction without convolutions, in ICCV, 2021. 4, 8 [67] J. L. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization, arXiv preprint arXiv:1607.06450, 2016. 5 [68] Y. Li, H. Mao, R. Girshick, and K. He, Exploring plain vision transformer backbones for object detection, in ECCV, 2022. 6, 7, 8 [69] Y. Wu and K. He, Group normalization, in ECCV, 2018. 6, 9 [70] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in NeurIPS, 2024. 6, 10, 11 [71] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, Autoaugment: Learning augmentation policies from data, arXiv preprint arXiv:1805.09501, 2018. 8, 12 [72] Y. Li, S. Xie, X. Chen, P. Dollar, K. He, and R. Girshick, Benchmarking detection transfer learning with vision transformers, arXiv preprint arXiv:2111.11429, 2021. [73] Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, and B. Fu, Shuffle transformer: Rethinking spatial shuffle for vision transformer, arXiv preprint arXiv:2106.03650, 2021. 8 [74] X. Ding, X. Zhang, J. Han, and G. Ding, Scaling up your kernels to 31x31: Revisiting large kernel design in cnns, in CVPR, 2022. 8, 9 [75] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in CVPR, 2009. 6, 8 [76] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, Masked autoencoders are scalable vision learners, in CVPR, 2022. 6, 8, 9 [77] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in ICLR, 2019. 6, 10 [78] H. Bao, L. Dong, S. Piao, and F. Wei, Beit: Bert pre-training of image transformers, in ICLR, 2022. 6 [79] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., Laion-5b: An open large-scale dataset for training next generation image-text models, in NeurIPS, 2022. 6 [80] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Xu et al., Mmdetection: Open mmlab detection toolbox and benchmark, arXiv preprint arXiv:1906.07155, 2019. 7 [81] X. Zhu, J. Zhu, H. Li, X. Wu, H. Li, X. Wang, and J. Dai, Uniperceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks, in CVPR, 2022. 8, 9 [82] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. 8, 9 [83] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei, Beit v2: Masked image modeling with vector-quantized visual tokenizers. arxiv 2022, arXiv preprint arXiv:2208.06366, 2022. 8, [84] S. Liu, T. Chen, X. Chen, X. Chen, Q. Xiao, B. Wu, T. Karkkainen, M. Pechenizkiy, D. Mocanu, and Z. Wang, More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity, in ICLR, 2023. 9 [85] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum, Dino: Detr with improved denoising anchor boxes for end-to-end object detection, in ICLR, 2023. 7, 8 [86] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. C. H. Hoi, Instructblip: Towards general-purpose vision-language models with instruction tuning, in NeurIPS, 2023. 11 [87] J. Li, D. Li, S. Savarese, and S. C. H. Hoi, BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models, in ICML, 2023. 11 [88] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, Shikra: Unleashing multimodal llms referential dialogue magic, arXiv preprint arXiv:2306.15195, 2023. 11 [89] Q. Ye, H. Xu, J. Ye, M. Yan, A. Hu, H. Liu, Q. Qian, J. Zhang, and F. Huang, mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 11 [90] D. Liu, R. Zhang, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin, K. Zhang et al., Sphinx-x: Scaling data and parameters for family of multi-modal large language models, arXiv preprint arXiv:2402.05935, 2024. [91] Y.-F. Zhang, Q. Wen, C. Fu, X. Wang, Z. Zhang, L. Wang, and R. Jin, Beyond llava-hd: Diving into high-resolution large multimodal models, arXiv preprint arXiv:2406.08487, 2024. 11 [92] P. Zhang, X. Dong, B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, H. Duan, S. Zhang, S. Ding et al., Internlm-xcomposer: visionlanguage large model for advanced text-image comprehension and composition, arXiv preprint arXiv:2309.15112, 2023. 11 [93] B. McKinzie, Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng, A. Belyi et al., Mm1: methods, analysis and insights from multimodal llm pre-training, in ECCV, 2024. 10, 11 [94] M. Contributors, MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark, https://github.com/open-mmlab/ mmsegmentation, 2020. 8 [95] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez et al., Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, https://vicuna.lmsys.org, 2023. 9 [96] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin, Sharegpt4v: Improving large multi-modal models with better captions, in ECCV, 2024. [97] Laion, Laion-gpt-4v, https://huggingface.co/datasets/laion/ gpt4v-dataset, 2023. 10 [98] G. H. Chen, S. Chen, R. Zhang, J. Chen, X. Wu, Z. Zhang, Z. Chen, J. Li, X. Wan, and B. Wang, Allava: Harnessing gpt4v-synthesized data for lite vision-language model, arXiv preprint arXiv:2402.11684, 2024. 10 [99] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu et al., Lima: Less is more for alignment, in NeurIPS, 2024. 10 [100] A. Kopf, Y. Kilcher, D. von Rutte, S. Anagnostidis, Z. R. Tam, K. Stevens, A. Barhoum, D. Nguyen, O. Stanley, R. Nagyfi et al., Openassistant conversations-democratizing large language model alignment, in NeurIPS, 2024. [101] P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan, Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning, in ICLR, 2023. 10 [102] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu, Metamath: Bootstrap your own mathematical questions for large language models, in ICLR, 2024. 10 [103] S. Shah, A. Mishra, N. Yadati, and P. P. Talukdar, Kvqa: Knowledgeaware visual question answering, in AAAI, 2019. 10 19 [104] P. Lu, R. Gong, S. Jiang, L. Qiu, S. Huang, X. Liang, and S.-c. Zhu, Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning, in ACL, 2021. 10 [105] A. F. Biten, R. Tito, A. Mafla, L. Gomez, M. Rusinol, E. Valveny, C. Jawahar, and D. Karatzas, Scene text visual question answering, in ICCV, 2019. [106] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque, Chartqa: benchmark for question answering about charts with visual and logical reasoning, in ACL, 2022. 10 [107] K. Kafle, B. Price, S. Cohen, and C. Kanan, Dvqa: Understanding data visualizations via question answering, in CVPR, 2018. 10 [108] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi, diagram is worth dozen images, in ECCV, 2016. 10 [109] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao, Llava-med: Training large language-and-vision assistant for biomedicine in one day, in NeurIPS, 2024. 10 [110] M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar, Infographicvqa, in WACV, 2022. 10 [111] W. Shi, Z. Hu, Y. Bin, J. Liu, Y. Yang, S. K. Ng, L. Bing, and R. Lee, Math-llava: Bootstrapping mathematical reasoning for multimodal large language models, in EMNLP, 2024. [112] P. Lu, S. Mishra, T. Xia, L. Qiu, K. Chang, S. Zhu, O. Tafjord, P. Clark, and A. Kalyan, Learn to explain: Multimodal reasoning via thought chains for science question answering, in NeurIPS, 2022. 10 [113] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin, Mmbench: Is your multi-modal model an all-around player? in ECCV, 2024. 10 [114] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang, Mm-vet: Evaluating large multimodal models for integrated capabilities, in ICML, 2024. 10 [115] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach, Towards VQA models that can read, in CVPR, 2019. 10 [116] D. A. Hudson and C. D. Manning, GQA: new dataset for real-world visual reasoning and compositional question answering, in CVPR, 2019. 10 [117] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, Making the in VQA matter: Elevating the role of image understanding in visual question answering, in CVPR, 2017. 10 [118] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, Seed-bench: Benchmarking multimodal llms with generative comprehension, arXiv: 2307.16125, 2023. [119] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J. Wen, Evaluating object hallucination in large vision-language models, in EMNLP, 2023."
        }
    ],
    "affiliations": [
        "Sensetime",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}