{
    "paper_title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models",
    "authors": [
        "Haitang Feng",
        "Jie Liu",
        "Jie Tang",
        "Gangshan Wu",
        "Beiqi Chen",
        "Jianhuang Lai",
        "Guangcong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D ."
        },
        {
            "title": "Start",
            "content": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models Haitang Feng1 Beiqi Chen3 Jie Liu1, (cid:66) Jianhuang Lai4 Guangcong Wang2, (cid:66) Jie Tang1 Gangshan Wu1 1Nanjing University 3Harbin Institute of Technology 2Great Bay University 4Sun Yat-sen University 5 2 0 2 5 2 ] . [ 1 1 7 2 8 1 . 8 0 5 2 : r Project page: https://objfiller3d.github.io Figure 1. We introduce ObjFiller-3D, novel framework capable of reconstructing complete 3D objects from partial inputs. Regions requiring inpainting are explicitly marked in pink. ObjFiller-3D demonstrates superior performance compared to previous state-of-the-art (SOTA) methods across multiple benchmark datasets."
        },
        {
            "title": "Abstract",
            "content": "3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller3D, novel method designed for the completion and edit- (cid:66): Jie Liu and Guangcong Wang are co-corresponding authors. Email: liujie@nju.edu.cn, wanggc3@gmail.com ing of high-quality and consistent 3D objects. Instead of employing conventional 2D image inpainting model, our approach leverages curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of video inpainting model for 3D scene inpainting. In addition, we introduce reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demon1 strates strong potential for practical deployment in realworld 3D editing applications. 1. Introduction Repairing incomplete 3D objects is crucial task in computer graphics and computer vision, particularly in applications such as cultural heritage restoration and digital reconstruction. Environmental constraints during scanning often result in data loss, which poses significant challenges in recovering the original geometry of objects. Despite its importance, research on 3D object completion remains relatively scarce-to the best of our knowledge, only methods such as NeRFiller [49] and Instant3dit [2] focused on this problem-and the task is fundamentally distinct from 3D generation and mainstream editing tasks. 3D generation methods [23, 52] typically aim to produce high-quality 3D assets from noise, single image, or few reference views. These approaches generally assume that the input images depict complete views of the object and are not designed to handle partial or degraded observations. On the other hand, mainstream 3D editing [10] focuses on modifying textures or performing style transfer, without accommodating significant geometric alterations, which making it unsuitable for restoring missing object parts. One straightforward approach is to use 2D inpainting model [27, 41] to restore multiple view images of the incomplete object, and then apply pre-trained 3D generation model to synthesize the full 3D object. key challenge is that conventional 2D inpainting does not account for crossview dependencies of the same object, often resulting in inconsistent outputs across different views (see Figure 5 ). Using these mismatched images for reconstruction inevitably leads to artifacts and poor quality results. ScoreDistillation Sampling (SDS) [32] and Iterative Dataset Update (IDU) [10] have been proposed to address this challenge. However, both SDS and IDU rely on the iterative optimization of 3D models from numerous inconsistent 2D samples. As result, the optimization process is extremely time-consuming and often leads to blurry or inaccurate reconstructions. Moreover, they do not support local highquality 3D editing. Recent studies NeRFiller [49] and Instant3dit [2] focused on 3D Object inpainting based on state-of-the-art (SOTA) 3D representations. NeRFiller observes that denoising four missing view images arranged in 22 grid leads to more consistent multi-view inpainting compared to independent inpainting, referring to this as grid prior. Instant3dit builds upon the grid prior of NeRFiller. They introduced masked object dataset, and trained Stable Diffusion inpainting model [27], specifically designed to generate consistent 22 grid images. The inpainted multiview images are then used as input for the Large Reconstruction Model (LRM) [14] to quickly produce the full 3D reconstruction. Although these approaches have attempted to address 3D inpainting based on SOTA 3D representations, the primary limitation is that they ensure consistency across only four views, thereby restricting the number of supported viewpoints, and thus leading to lack of details in geometry and textures. They are only suitable for objects with simple structure and uniform textures. They struggle with challenging tasks involving complex shapes and many views, failing to guarantee high-quality reconstructions. On the other hand, the current cutting-edge video generation [13, 43, 56], video editing [18, 45], and world models [9] have showcased the strong spatial perception and understanding capabilities of video models. These models excel in maintaining inter-frame consistency, enabling the generation of 3D-consistent and coherent objects. Previous works [42] have already explored using video models for 3D generation tasks but did not consider 3D object inpainting. Motivated by the limitation of the existing 3D inpainting models and the advantages of video generation methods, we analyze the representation gap between 3D and videos. We observe that 3D scenes are typically modeled with 360-degree coverage and uniform spatial sampling, enabling complete view synthesis, whereas real-world videos are limited to partial viewpoints, dynamic content, and non-uniform temporal sampling (e.g., slow-fast encoding), which introduce challenges like deformation, occlusion, and motion blurless prominent in static 3D settings. We adapt the video inpainting model to 3D inpainting model with Low-Rank Adaptation (LoRA) [15]. To guarantee the 360-degree looped video inpainting, we also duplicate the first image frame as the last one. To support referencedriven 3D inpainting, We prepend the reference image and an all-zero mask to the video and mask sequences, respectively, indicating no editing is required for the first frame. The results demonstrate that our method clearly outperforms prior approaches in quality while maintaining short runtime, achieving SOTA performance. Specifically, ObjFiller-3D achieves more faithful and fine-grained 3D reconstructions, outperforming NeRFiller and Instant3dit with PSNR of 26.6 (vs. 15.9) and an LPIPS of 0.19 (vs. 0.25), respectively. To summarize, our main contributions include: 1) We propose highly consistent 3D object inpainting method that generates high-quality results within short time, surpassing previous methods and not being limited by the number of viewpoints. 2) We present new video-to-3D adaptation method that supports high-quality 3D inpainting by efficiently leveraging pre-trained video editing models. Moreover, we introduce reference-based 3D inpainting to enable image-based conditional generation. 3) Extensive experiments validate the effectiveness of our method and highlight its substantial improvement over prior ap2 proaches. 2. Related Work Our objective is to complete the missing regions of an existing 3D object. Several approaches are relative this problem, including 3D object generation and editing as well as video inpainting. 2.1. 3D Object Generation 3D object generation methods are typically categorized into 2D prior-based and feedforward approaches. The former utilizes image generative models (e.g., Stable Diffusion [34], Imagen [35]) trained on large-scale datasets like LAION-400M/5B [36, 37] to extend text or image inputs into 3D. DreamFusion [32] introduced Score Distillation Sampling (SDS) for this purpose, followed by improved but costly variants [16, 48]. Alternatively, some methods [8, 17, 24, 26, 38, 39, 44] directly generate multi-view consistent images using finetuned diffusion models, offering more efficient path to 3D reconstruction.Feedforward generation is another key paradigm for 3D synthesis, often employing encoder-decoder architectures like Latent Diffusion Models (LDMs) [34] to extract image tokens and generate latent 3D representations. Models such as Large Reconstruction Model (LRM) [14], MeshLRM [50], Instant3D [23], and InstantMesh [53] use Transformers to map tokens into implicit triplane representations. 2.2. 3D Editing Recent advances in 3D field editing mainly leverage NeRF [29] and 3DGS [20] for superior realism and flexibility over traditional mesh or point clouds. Instructionguided methods use pre-trained 2D diffusion models due to scarce 3D data, optimizing NeRF via Score Distillation Sampling (SDS) to achieve view-consistent, relightable 3D outputs. Iterative Dataset Update (IDU) approaches like Instruct-NeRF2NeRF [10] train 3D scenes using edited images. Methods [4, 5, 47] extend 3DGS for efficient, controllable editing. For 3D object removal, SPIn-NeRF [30] combines depth and 2D inpainting but struggles with full 360 views. Recent Gaussian Splatting inpainting works like InFusion [25], AuraFusion360 [51], and IMFine [40] address 360 unbounded scene restoration. However, these methods primarily concentrate on maintaining global scene consistency and eliminating visual artifacts after object removal, which differs substantially from the goals of our task. Among the most closely related methods to ours are NeRFiller [49] and Instant3dit [2]. However, NeRFiller still relies on IDU, and Instant3dit is limited to handling only four input views, which constraints their performance. 2.3. Video Inpainting Unlike image inpainting, video inpainting requires consistency across frames, which makes the task more challenging. Methods [7, 54, 59] utilize flow-guided feature propagation in conjunction with video Transformer architectures to reconstruct missing content across frames. The emergence of video foundation models [3, 21, 43] in recent research has demonstrated remarkable competitiveness. Building upon the video generation model WAN [43], VACE [18] demonstrates strong capabilities in the domain of video editing and inpainting. However, these models are trained on real-world, scene-level video datasets [46, 55], which differ fundamentally from object-centric 3D model datasets [6]. The complex, multi-object environments in real-world data introduce inductive biases that challenge direct transfer. Therefore, effective application of such video models to 3D-specific tasks necessitates domain adaptation to mitigate the domain discrepancy. 3. Method We present ObjFiller-3D, new framework that involves controllable modifications of the geometry, appearance, or semantics of 3D scenes or objects. In the following, we detail the process of obtaining 3D-consistent masks, organizing existing datasets [2] for finetuning the VACE model [18], and generating the final 3D object. 3.1. Preliminary and Problem Formulation Preliminary. 3D Gaussian Splatting (3DGS) [20] uses 3D Gaussians as scene primitives and represents the scene with large number of 3D Gaussians. Gaussian ellipsoid centered at point µ R3 (mean) with covariance matrix Σ can be written as: G(x) = 1 2 xT Σ1x, and Σ = RSST RT , (1) where denotes the distance from specific point in space to the mean µ, and R33 represents the Gaussians rotation matrix, while R33 represents the scaling matrix. After formalizing the 3D Gaussians, volumetric rendering techniques [19] can be used to splat these Gaussians onto the 2D image plane. Specifically, the color for pixel along ray is given by: (cid:88) = ciαiG(xi) i1 (cid:89) j=1 (1 αjG(xj)) , (2) where α and R3 represent the opacity and color of the 3D Gaussians, respectively. 3DGS is recognized for its efficiency in real-time radiance field rendering and has thus been integrated into our method. Problem Formulation. We define the task of 3D object inpainting as follows: Given incomplete 3D object O, 3 3D mask region , text description of the object and an optional reference image Ir, our goal is to generate plausible shape contained within , based on and Ir, such that {O O} forms complete 3D object. 3.2. Dataset Preparation Before introducing our proposed method, we first describe the dataset used in Instant3dit. Specifically, Instant3dit utilizes approximately 7k high-quality 3D objects selected from the large-scale synthetic dataset Objaverse [6]. Furthermore, types of 3D masks, namely: convexhull, surface, and volume (see Figure 2). 1) Convexhull: the inpainted part of the object is fully contained inside the mask . 2) Surface: the mask covers small surface region on object O. 3)Volume: the mask tightly encloses the object O. it defines three distinct Figure 2. Three Types of 3D Masks. Although the Instant3dit dataset provides masked objects along with their corresponding 3D masks, it does not include the original complete objects or their associated 2D ground truth images. To address this, we locate the corresponding objects from the Objaverse dataset for each object in Instant3dit, and render 16-view 2D images at resolution of 512 512 using Blender as ground truth. These 16 views are distributed at fixed elevation of 20 degrees, with azimuth angles uniformly sampled from 0 to 2π. Additionally, we employ Cap3D [28] to produce captions for individual 3D objects, which use the pre-trained model in captioning (BLIP-2 [22]), alignment (CLIP [33]), and LLM (GPT4 [1]) to consolidate multi-view information. To leverage the inpainting prior from generative video diffusion model, we need to project the 3D information onto 2D plane. We render each 3D object into set of image frames Fs = {f1, f2, . . . , fn}, and the missing region into set of binary masks Ms = {m1, m2, . . . , mn}, from our predefined camera poses Π = {πi}n i=1. fi, mi = R(O, M, πi), {1, . . . , n}. (3) denotes rendering operator. The complete dataset is approximately 18 GB in size, and we will make it publicly available upon publication. 3.3. Multi-view Consistent 3D Inpainting We leverage the inter-frame coherence of video models to maintain consistent 3D inpainting across multiple views. Specifically, after processing the Instant3dit dataset, each object is represented as tuple Fs, Ms, y, where the 16 rendered views form 360 looped video. To meet VACEs [18] input requirement of 4n + 1 frames, we duplicate the first image and mask as the 17th frame. The image frames Fs and masks Ms are combined into video sequences, whichalong with and Irare fed into the VACE model to generate s, set of temporally consistent and inpainted frames. Analyzing Representation Gap between 3D and Videos. While video-based editing methods offer temporal context, they often struggle to capture the full spatial structure and semantic consistency required in 3D content creation. First, 3D scenes and objects are generally modeled with 360-degree coverage, yielding looped rendered videos, while real-world videos are inherently limited to partial and forward-facing views. Second, real-world videos frequently contain dynamic objects and temporal motions, which introduce complexities such as object deformation, occlusion, and motion blurissues that are less prevalent in static 3D editing scenarios. Third, videos are typically encoded using non-uniform temporal sampling schemes, such as slow-fast strategies that emphasize specific motion segments, whereas 3D data is often uniformly sampled in space for structural completeness. Serving as counterexample, the LoRA ablation in the subsequent experimental section showcases results obtained using the video model without any alterations. Figure 3. VACE model architecture. VACE blocks control the output of the WAN model through adapter tuning, based on control signals such as text prompts and masks, enabling controllable video editing. We inject LoRA into the VACE layers and freeze all other parameters. Adapting Video Inpainting to 3D Inpainting. To bridge the gap between video inpainting and 3D inpainting, we 4 finetune the video diffusion model with Low-Rank Adaptation [15] method. Figure 3 illustrates the main architecture of the VACE model, where LoRA weights are incorporated into every VACE transformer layer. During training, the original parameters of the transformer blocks are kept frozen, and only the low-rank matrices Rdr and Rrk are updated, with the rank min(d, k). We set LoRA rank = 32. To train the video inpainting model parameter θ, we adopt the flow matching loss that aligns the learned velocity (cid:0)t, Fs, Ms, y) field with the ground-truth flow. Let uθ denote the velocity predicted by our diffusion network at time and state x. Let u(t, x) be the true velocity field defined by the underlying interpolation between noise and data distributions. The flow matching loss is then defined as LFM(θ) = EtU (0,1) Expt (cid:13) (cid:13)uθ(t, Fs, Ms, y) u(t, x) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 , (4) where pt denotes the marginal distribution of the noisy data at time t, and U(0, 1) denotes the uniform distribution over the interval [0, 1]. Minimizing LFM encourages the network to recover the true continuous flow from noise to data, yielding temporally coherent inpainted frames. Figure 4. Reference-based 3D Inpainting. Given reference image, the other views are well aligned with the input. Reference-based 3D Inpainting. Without reference images, inpainting results often vary widely and may require multiple generations to achieve desirable outcome. However, in certain scenariossuch as restoring historical architecture or cultural artifactswe typically have access to complete reference image to guide the inpainting process and ensure results align with expectations.This is straightforward for our method, as the VACE model has designed to handle video editing based on reference images. Specifically, we can append the reference image Ir to the input video sequence Fs as the first frame. Correspondingly, we include an all-zero mask as the first frame in the mask sequence Ms, indicating that this frame does not require editing. After generating the video, this auxiliary frame can be discarded. Figure 4 presents an inpainting result with reference image. Object Reconstruction. After the inpainting process, we obtain set of frames along with their corresponding camera poses Π. These inpainted views can then be used to reconstruct the complete 3D object represented by 3D Gaussians G. Ginpainted = arg min (cid:88) πiΠ,f L (R(G, πi), ) , (5) where represents the rendering function, while is 3DGSs loss function. This reconstruction approach is similar to the NeRFiller pipeline, but thanks to the high consistency ensured during our inpainting step, we are able to skip the IDU stage. This significantly reduces reconstruction time: our method completes the process in under 10 minutes, whereas NeRFiller [49] requires over 40 minutes. Moreover, our reconstruction quality substantially surpasses that of NeRFiller, as shown in Figure 1 and Figure 5. Figure 5. Visual comparison of different multi-view inpainting methods Compared to baseline approaches, ObjFiller-3D achieves more consistent and coherent inpainting across different views. 4. Experiments Implementation details. We train our models using 3,000 objects from our reprocessed dataset. During the training phase, we set the LoRA rank to 32, used learning rate of 104, batch size of 4, and trained for 10 epochs. For the VACE 14B model, training was conducted in half-precision, consuming approximately 60 GB of VRAM. The process took approximately 3 days on single NVIDIA A800 GPU. During inference, we used the UniPC sampler [58], with the 5 number of inference steps set to 20, CFG [12] guidance scale of 4, and LoRA scale of 1. Evaluation Metrics. For the metric comparison between ours and Instant3dit, we focus on three key aspects: 1) TextImage Similarity: We measure the semantic relevance between the generated 22 grid images and the given text prompt using the CLIP [33] similarity score. 2) Image Quality and Fidelity: This evaluates the quality of the generated images and the similarity between the inpainted images and the Ground-Truth (GT) images. Specifically, we inpaint 300 grid images from the evaluation dataset and compare them against GT grids using the FID score [11]. 3) multi-view Consistency: To assess consistency across the four views in the inpainted grid image, we feed the four images directly into LRM [53]. If the views lack consistency, LRM will produce distorted object; otherwise, the reconstruction will be coherent. To quantify this, we render the resulting object from the original viewpoints and compute the perceptual LPIPS score [57] between the rendered images and the inpainted ones. lower LPIPS score indicates better multi-view consistency. For the metric comparison between ours and NeRFiller, the final object reconstruction is 3D reconstruction task. Therefore, we evaluate using three standard metrics: SSIM, PSNR, and LPIPS. Figure 6. Qualitative results compare to Instant3dit We apply our method to three different mask types, listed from top to bottom as Convexhull, Surface, Volume. ObjFiller-3D-14B (the rightmost column) exhibits the highest consistency across all the mask types. 4.1. Comparisons with State-of-the-Arts Comparison with Instant3dit. We provide four model variants: VACE1.3B, VACE14B, and their respective LoRA fine-tuned versions. We evaluate them on 300 held-out objects. For Instant3dit, we stack masked multi-view images and corresponding masks from four orthogonal viewpoints (0, 90, 180, and 270) into 300 image grids arranged in 6 22 layout. These image grids, along with text prompts generated by Cap3D [28], serve as input to Instant3dits open-source 2D multi-view inpainting model, which has been trained on SDXL [31]exactly as expected by its design. For the four VACE variants, we respectively concatenate all views and masks into 17-frame videos, and feed them into VACE along with text prompts. From the output video, we extract the same four orthogonal views and arrange them into 22 image grids. Since object reconstruction in later steps is performed using LRM for all methods, our comparison focuses solely on the visual quality of the image grids generated by VACEs and Instant3dit. Comparison with NeRFiller. We choose VACE14BLoRA followed with 3DGS as our main model for comparison with NeRFiller. For more comprehensive evaluation, we additionally consider two alternative methods: 1) Masked NeRF: No inpainting; training is performed only on known pixels. 2) SD Image Cond: Each image is individually inpainted using SD and then directly used for training. The dataset we use was introduced by NeRFiller, which occludes the center of each image with 256256 region to inpaint (see Figure 5 top). All four methods are trained using 180 equally spaced images and evaluate metrics on the remaining 20 images. 4.2. Further Analysis The comparison results with Instant3dit are presented in Table 1, where it is evident that VACE14B with LoRA finetuning achieves the best overall performance. The grid image inpainting results shown in Figure 6 further support this conclusion. In comparison with NeRFiller, our method shows substantial performance advantage, as presented in Table X. This significant lead is well justified, given the quality of our inpainting visual results shown in Figure 1 and Figure 5. FID LPIPS Clip GT Instant3dit (CVPR25) ObjFiller-3D-1.3B (ours) ObjFiller-3D-14B (ours) 100.9 92.07 90.75 0.140 0.253 0.190 0.195 30.53 29.81 29.87 30.19 Table 1. Quantitative result compare to Instant3dit. We compare our result with the latest method Instant3dit on three aspect. FID scores are computed between the synthesized images and the ground truth (GT). Number of Input Views. More incomplete views provide extra object information, but also impose greater constraints. As shown in Table 3, the performance of our inpainter improves with more inputs. Therefore, we argue that our method effectively handles the negative aspects inFigure 7. Qualitative results of scene inpainting. The leftmost column shows the scenes we need to process, where the areas to be repaired are masked in pink. We compare our method, ObjFiller-3D (the rightmost column), with other approaches. PSNR SSIM LPIPS Input Views 100 120 140 Masked NeRF SD Image Cond NeRFiller (CVPR24) Ours 7.76 14.15 15.89 26.62 0.71 0.76 0.82 0. 0.37 0.28 0.23 0.07 Table 2. Quantitative result compare to NeRFiller. All evaluation metrics have improved by large margin compared to baselines. troduced by additional views, while also benefiting from the positive information they provide. LoRA ablation. key component of our method is the LoRA weights, which are fine-tuned using three types of multiview-consistent masks. To assess their effectiveness, we conduct ablation studies focusing on this aspect. Figure 8 shows the difference. The quantitative comparisons are provided in the supplementary material. Extend to 3D Scene. Leveraging our video-based approach, we naturally extend it to 3D scene inpainting, PSNR SSIM LPIPS 22.76 0.89 0.11 25.63 0.92 0. 26.62 0.93 0.07 26.68 0.93 0.06 Table 3. Evaluation of our 3D inpainting method across different input view number. whichlike object-level inpaintingis fundamentally mask-filling task. We evaluate our method on four diverse scenes, comparing it with NeRFiller and SPIn-NeRF [30], as shown in Figure 7. Unlike SPIn-NeRF, which assumes compact mask regions suited for object removal, our approach accommodates broader, unconstrained masks, enabling more general 3D inpainting and demonstrating greater applicability. In complex scenes, our method outperforms NeRFiller, as illustrated in the second row of Figure 7. Applications. Since inpainting and editing are closely related tasks, our model can also be applied to the process of"
        },
        {
            "title": "Acknowledgement",
            "content": "The computational resources are supported by SongShan Lake HPC Center in Great Bay University. This work was also supported by Guangdong Research Team for Communication and Sensing Integrated with Intelligent Computing (Project No. 2024KCXTD047). (SSL-HPC)"
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 4 [2] Amir Barda, Matheus Gadelha, Vladimir Kim, Noam Aigerman, Amit Bermano, and Thibault Groueix. Instant3dit: Multiview inpainting for fast editing of 3d objects. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1627316282, 2025. 2, 3, 1 [3] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Skyreels-v2: Zheng Chen, Chengcheng Ma, et al. arXiv preprint Infinite-length film generative model. arXiv:2504.13074, 2025. 3 [4] Jun-Kun Chen and Yu-Xiong Wang. Proedit: Simple progression is all you need for high-quality 3d scene editing. Advances in Neural Information Processing Systems, 37:4934 4955, 2024. 3 [5] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2147621485, 2024. 3 [6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 3, [7] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf. Flow-edge guided video completion. In European Conference on Computer Vision, pages 713729. Springer, 2020. 3 [8] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything arXiv preprint in 3d with multi-view diffusion models. arXiv:2405.10314, 2024. 3 [9] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. 2 [10] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: EditIn Proceedings of the ing 3d scenes with instructions. IEEE/CVF international conference on computer vision, pages 1974019750, 2023. 2, 3 Figure 8. LoRA ablation. We evaluate both w./w.o. LoRA configurations across different parameter scales. As expected, the 14B variant with LoRA delivers the best performance (bottom-left). object editing. Specifically, we can import our object into Blender, manually place 3D geometric shape as the 3D mask in the desired location. Figure 9 illustrates an replace example. Similarly, ObjFiller-3D supports add and remove. Figure 9. Object editing. Given text prompt, ObjFiller-3D can replace the corresponding part of the object accordingly. 5. Conclusion In this paper, we propose ObjFiller-3D, fast and consistent 3D inpainting framework adapted an off-the-shelf video diffusion model [18]. By bridging the gap between video and 3D data and introducing reference-guided inpainting, our method overcomes limitations of prior approaches that rely on inconsistent view synthesis or restricted viewpoint priors, achieving high-quality object reconstruction and surpasses prior approaches in fidelity, consistency, and speed. Furthermore, our framework generalizes to 3D scene-level inpainting and object editing, paving the way for broader applications in digital content creation, gaming, cultural heritage restoration, and virtual environments. Limitation. Our approach is built upon video foundation models, and thus its capabilities are inherently constrained by the limitations of these models. We believe that with the emergence of more powerful video models in the future, the effectiveness of our method will be further enhanced. 8 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Neural Information Processing Systems,Neural Information Processing Systems, 2017. [12] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6 [13] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Imagen Poole, Mohammad Norouzi, David Fleet, et al. video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [14] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2, 3 [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 2, 5 [16] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, ZhengJun Zha, and Lei Zhang. Dreamtime: An improved optimization strategy for text-to-3d content creation. arXiv preprint arXiv:2306.12422, 3(5):14, 2023. 3 [17] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. arXiv preprint arXiv:2412.03632, 2024. 3 [18] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2, 3, 4, [19] James Kajiya and Brian Von Herzen. Ray tracing volume densities. ACM SIGGRAPH computer graphics, 18(3):165 174, 1984. 3 [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 3 [21] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 4 [23] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 2, 3, 1 [24] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [25] Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu, Yujun Shen, and Yang Cao. Inpainting 3d gaussians via learning depth completion from diffusion prior. arXiv preprint arXiv:2404.11613, 2024. 3 Infusion: [26] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99709980, 2024. 3 [27] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. 2 [28] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36: 7530775337, 2023. 4, 6 [29] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [30] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstantinos Derpanis, Jonathan Kelly, Marcus Brubaker, Igor Gilitschenski, and Alex Levinshtein. Spin-nerf: Multiview segmentation and perceptual inpainting with neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2066920679, 2023. 3, 7 [31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 6 [32] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2, 3 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4, 6 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 9 [36] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 3 [37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 3 [38] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 3 [39] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 3 [40] Zhihao Shi, Dong Huo, Yuhongze Zhou, Yan Min, Juwei Lu, and Xinxin Zuo. Imfine: 3d inpainting via geometry-guided In Proceedings of the Computer multi-view refinement. Vision and Pattern Recognition Conference, pages 26694 26703, 2025. 3 [41] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2149 2159, 2022. [42] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2024. 2 [43] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3 [44] Peng Wang and Yichun Shi. multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 3 Imagedream: Image-prompt arXiv preprint [45] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023. [46] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. arXiv preprint arXiv:2305.10874, 2023. 3 [47] Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, and Hanwang Zhang. View-consistent 3d editing with gaussian splatting. In European conference on computer vision, pages 404420. Springer, 2024. 3 10 [48] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36: 84068441, 2023. 3 [49] Ethan Weber, Aleksander Holynski, Varun Jampani, Saurabh Saxena, Noah Snavely, Abhishek Kar, and Angjoo Kanazawa. Nerfiller: Completing scenes via generative 3d In Proceedings of the IEEE/CVF Conference inpainting. on Computer Vision and Pattern Recognition, pages 20731 20741, 2024. 2, 3, 5, 1 [50] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for highquality meshes. arXiv preprint arXiv:2404.12385, 2024. [51] Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, et al. Aurafusion360: Augmented unseen region alignment for referencebased 360deg unbounded scene inpainting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1636616376, 2025. 3 [52] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2146921480, 2025. 2 [53] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 3, 6, 1 [54] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy. In Proceedings of Deep flow-guided video inpainting. the IEEE/CVF conference on computer vision and pattern recognition, pages 37233732, 2019. 3 [55] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 [56] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 2 [57] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In 2018 IEEE/CVF deep features as perceptual metric. Conference on Computer Vision and Pattern Recognition, 2018. [58] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:4984249869, 2023. 5 [59] Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and Improving propagation In Proceedings of Chen Change Loy. and transformer for video inpainting. Propainter: the IEEE/CVF international conference on computer vision, pages 1047710486, 2023. 3 11 ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "I. Dataset Details Rendering Setup. We generate perspectives of each object against plain white background under uniform lighting conditions using Blenders physically-based path tracer Cycles. Before rendering, the selected 3D models are uniformly resized and aligned at the center of bounded cubic space defined by [1, 1]3. The entire setup conforms to systematic configuration (see Data Preparation section of the main text.), employing fov of 50 from fixed range of 2.7 units to produce 16 distinct renderings. All rendered images and associated metadata used in our experiments will be made publicly available upon publication. We believe this will serve as valuable inspiration for future work. Prompts Used for Dataset. We use the following text prompts for each of the 8 NeRF synthetic object and 4 scenes from NeRFiller [49]. chair: video of green striped chair. drums: video of drums. fics: video of fics. hotdog: video of golden brown hotdogs with musthe pretrained weights CLIP-ViT-L-14-CommonPool.XLs13B-b90K1 from the Hugging Face Hub. CLIP metrics assess semantic consistency between the rendered results and the reference inputs, providing complementary evaluation to LPIPS. Training Setup During our fine-tuning process, the VACE1.3B model was trained using full-precision on an NVIDIA RTX 4090 GPU, with an approximate memory usage of 20 GB. In contrast, the VACE14B model was trained using half-precision on an NVIDIA A800 GPU, requiring approximately 60 GB of memory. Both models were trained for 10 epochs, with each epoch comprising 3,000 steps. The training of single epoch typically required around 6 hours, depending on hardware conditions and model size. FID LPIPS Clip w/o LoRA (1.3B) w/ LoRA (1.3B) 107.2 92.07 0.205 0.190 29.76 29. tard and condiments. Table I. Quantitative result of LoRA ablation on the 1.3B model. lego: video of LEGO bulldozer model. materials: video of 3D models displaying diverse material textures and finishes. mic: video of microphone. ship: video of ship in circular basin. backpack: black backpack and cardboard box rest on the grass in front of an orange plastic fence. billiards: video of vintage-style room. drawing: video of lavish interior. norway: video of an elegant room. II. More Experimental Implementation Evaluation Metrics. To compare perceptual similarity between our method and Instant3dit [2], we adopt the LPIPS metric using VGG as the backbone network. Since the original reconstruction module (LRM: Instant3D [23]) used in Instant3dit is not publicly available, we employ InstantMesh [53] as unified reconstruction backend to ensure fair comparison. InstantMesh reconstructs 3D meshes from four orthographic-view input images and renders synthesized views using triplanebased rendering module and the associated camera parameters. These rendered images are then used for the LPIPS evaluation. For CLIP-based comparisons, we use FID LPIPS Clip w/o LoRA (14B) w/ LoRA (14B) 104.8 90.75 0.219 0.195 30.19 30.19 Table II. Quantitative result of LoRA ablation on the 14B model. III. Additional Results Figure presents additional visualization results for comparative analysis with Instant3dit. Table and Table II show the quantitative comparisons of LoRA ablation. Tables III to present the individual results for each of the eight objects in the NeRFiller dataset. 1https : / / huggingface . co / laion / CLIP - ViT - - 14 - CommonPool.XL-s13B-b90K 1 PSNR SSIM LPIPS PSNR SSIM LPIPS Masked NeRF SD Image Cond NeRFiller (CVPR24) Ours 5.19 13.71 16.59 30.74 0.70 0.75 0.85 0.96 0.40 0.31 0.24 0.03 Masked NeRF SD Image Cond NeRFiller (CVPR24) Ours 8.80 14.79 17.89 26.38 0.73 0.79 0.84 0.91 0.33 0.22 0.18 0.07 Table III. Quantitative evaluation of inpainting consistency on chair data. Table VII. Quantitative evaluation of inpainting consistency on lego data. PSNR SSIM LPIPS Masked NeRF SD Image Cond NeRFiller (CVPR24) Ours 5.77 12.30 13.25 21.65 0.66 0.76 0.79 0.89 0.49 0.27 0.27 0.11 Table IV. Quantitative evaluation of inpainting consistency on drums data. PSNR SSIM LPIPS Masked NeRF SD Image Cond NeRFiller (CVPR24) Ours 6.86 13.40 17.24 26.83 0.73 0.76 0.85 0.94 0.37 0.31 0.21 0. Table V. Quantitative evaluation of inpainting consistency on ficus data. PSNR SSIM LPIPS Masked NeRF SD Image Cond NeRFiller (CVPR24) Ours 8.28 14.53 14.53 26.18 0.72 0.78 0.80 0. 0.33 0.25 0.23 0.06 Table VIII. Quantitative evaluation of inpainting consistency on materials data. PSNR SSIM LPIPS Masked NeRF SD Image Cond NeRFiller (CVPR24) Ours 6.94 13.40 13.26 25. 0.73 0.77 0.81 0.95 0.35 0.28 0.27 0.07 Table IX. Quantitative evaluation of inpainting consistency on mic data. PSNR SSIM LPIPS Masked NeRF SD Image Cond NeRFiller (CVPR24) Ours 10.66 15.95 17.64 26.29 0.69 0.74 0.80 0.88 0.35 0.29 0.24 0.10 PSNR SSIM LPIPS Table X. Quantitative evaluation of inpainting consistency on ship data. Masked NeRF SD Image Cond NeRFiller (CVPR24) Ours 8.84 15.12 16.69 29.67 0.73 0.78 0.86 0.95 0.36 0.31 0.24 0.04 Table VI. Quantitative evaluation of inpainting consistency on hotdog data. Figure I. Additional visualization results compared to Instant3dit. We randomly select several outputs from our method and Instant3dit for qualitative comparison. Overall, our 14B model demonstrates superior consistency across multiple views."
        }
    ],
    "affiliations": [
        "Great Bay University",
        "Harbin Institute of Technology",
        "Nanjing University",
        "Sun Yat-sen University"
    ]
}