{
    "paper_title": "DELIFT: Data Efficient Language model Instruction Fine Tuning",
    "authors": [
        "Ishika Agarwal",
        "Krishnateja Killamsetty",
        "Lucian Popa",
        "Marina Danilevksy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 2 5 2 4 4 0 . 1 1 4 2 : r DELIFT: DATA EFFICIENT LANGUAGE MODEL INSTRUCTION FINE-TUNING Ishika Agarwal1, Krishnateja Killamsetty2, Lucian Popa2, Marina Danilevsky2 1University of Illinois Urbana-Champaign, 2IBM Research 1ishikaa2@illinois.edu 2krishnateja.k@ibm.com, {lpopa, mdanile}@us.ibm.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is pairwise utility metric that quantifies how beneficial data sample is for improving the models responses to other samples, effectively measuring the informational value relative to the models current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy."
        },
        {
            "title": "INTRODUCTION",
            "content": "Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformationfrom general-purpose model to task-specific agentheavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the models subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the models capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains significant challenge, leading to our central research question: How can we create unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction FineTuning), novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the models current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the models learning requirements without incurring unnecessary computational costs. 1 The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the models ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the models expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the models knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the models learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Efficient data subset selection is vital for enhancing training efficiency in deep neural networks while maintaining or improving model performance. This section categorizes existing subset selection methods into model-independent and model-dependent approaches and identifies the gaps 2 (a) (b) (c) Figure 1: DELIFT data selection across fine-tuning stages. (a) Instruction Tuning: Diverse instructions selected; redundant samples pruned. (b) Task-Specific Fine-Tuning: Mutually informative (with benchmark data) and diverse samples are prioritized for selection. (c) Continual Fine-tuning: New samples that are novel are integrated; new samples with overlapping information are pruned. our work addresses. Model-independent subset selection methods focus on selecting representative subsets without model-specific feedback. Common approaches include using pre-trained sentence embeddings with distance or clustering metrics (Bukharin & Zhao, 2024; Sorscher et al., 2023; Killamsetty et al., 2023; Du et al., 2023; Bhatt et al., 2024), as well as employing large models like GPT-4 or pre-trained reward models for high-quality data filtering (Du et al., 2023; Chen et al., 2024). However, these methods often struggle to translate the assessed diversity or quality into downstream utility. Model-dependent subset selection aims to identify data samples beneficial to 3 the downstream model by analyzing features like per-sample gradients or loss values. Methods such as GradMatch (Killamsetty et al., 2021a), CRAIG (Mirzasoleiman et al., 2020), and TAGCOS (Zhang et al., 2024) focus on selecting samples that approximate the gradient updates over the full dataset. GLISTER (Killamsetty et al., 2021b) employs bilevel optimization to align gradients from selected subsets with those of validation set. LESS (Xia et al., 2024) proposes computing gradients through LoRA fine-tuning to reduce the computational cost of gradient computation and utilizes random projection to address gradient dimensionality issues. Li et al. (2024) proposed the IFD score, computationally efficient model-dependent metric that assesses instruction difficulty to filter challenging samples, though it does not guarantee data diversity. While effective in capturing useful samples, these methods often face computational challenges, especially with LLMs. Persistent limitations across these methods include: (i) Limited Adaptability across different fine-tuning stages, (ii) Computational Intensity due to model feedback reliance, (iii) Lack of Unified Solutions applicable across all fine-tuning phases, and (iv) ineffective Redundancy Handling. DELIFT addresses these limitations through novel pairwise utility metric, which effectively aligns with the models evolving capabilities throughout fine-tuning. By integrating submodular optimization with pairwise model-dependent metrics that evaluate relative sample utility, DELIFT minimizes redundancy while maximizing adaptability and computational efficiency. This approach proves effective across diverse use cases including instruction tuning, task-specific fine-tuning, continual fine-tuning, and In-Context Learning (ICL), offering versatile and scalable solution for data subset selection."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "This section presents foundational concepts and the specific approach of DELIFT, focusing on data subset selection through utility-based kernel integrated with submodular optimization techniques."
        },
        {
            "title": "3.1 NOTATION",
            "content": "Let denote the fine-tuning dataset, comprising elements di = (xi, yi), where xi is the input sequence and yi is the corresponding output sequence. Our objective is to select subset that maximizes the models performance while minimizing computational costs. The selection strategy adapts based on the fine-tuning objective, which may include instruction tuning, task-specific adaptation, or continual learning."
        },
        {
            "title": "3.2 UTILITY-BASED KERNEL",
            "content": "At the core of DELIFT lies the utility-based kernel, mechanism designed to quantify the informativeness of one data point when used as an in-context example for another. Consider two data points, (xi, yi) and (xj, yj). The utility of data point relative to data point i, denoted as Fij, is defined as: Fij = d(GTi, p(yi xi)) d(GTi, p(yi xj, yj, xi)), (1) where: d(, ) is length-normalized distance metric between two probability distributions, GTi is the ground truth distribution for the sequence yi, modeled as vector of ones for each token to signify perfect prediction, p(yi xi) is the models predicted probability distribution for yi given only the input xi, p(yi xj, yj, xi) is the predicted distribution for yi when the model is provided with (xj, yj) as an in-context example followed by xi. The distance metric d(p, q) is calculated using the length normalized L2 norm and is defined as: d(p, q) = (cid:115) (cid:80)N k=1(pk qk)2 , (2) where pk and qk are the k-th elements of the flattened probability distributions and q, respectively. Importantly, varies with each data sample and corresponds to the number of tokens in the ground truth response yi. This normalization ensures that the distance measure remains scale-invariant across different sequence lengths and vocabulary sizes. 4 To compute the probability distributions accurately, we employ the teacher forcing technique (Williams & Zipser, 1989). This method ensures that the model uses the ground truth of previous tokens when predicting each subsequent token in the sequence, enabling reliable measurement of prediction accuracy. The utility value Fij measures the improvement in prediction accuracy for the entire sequence (xi, yi) when utilizing (xj, yj) as an in-context example. positive Fij indicates that including data point enhances the models prediction accuracy for i, whereas negative value suggests an adverse effect."
        },
        {
            "title": "3.3 SUBMODULAR FUNCTIONS FOR DATASET SELECTION",
            "content": "To optimize the selection of informative data subsets, DELIFT leverages submodular functions (Fujishige, 2005). Submodular functions are characterized by the property of diminishing marginal returns, making them ideal for selecting diverse, informative, and non-redundant subsets. Submodular function maximization can be efficiently approximated using greedy algorithm, with provable approximation guarantee of 1 1 of the optimal solution (Nemhauser et al., 1978). We employ three tailored submodular functions (Iyer et al., 2021), each suited to specific finetuning stage:"
        },
        {
            "title": "3.3.1 FACILITY LOCATION (FL)",
            "content": "From an information perspective, the Facility Location function maximizes the coverage of the It ensures that the selected subset contains examples that are collectively information space. representative of the entire datasets information content. This is particularly useful in instruction tuning, where we aim to capture diverse range of instruction types and their informational content. It is defined as the following where sij is the similarity measure between data points and j: fF L(A) = (cid:88) iD max jA sij, (3)"
        },
        {
            "title": "3.3.2 FACILITY LOCATION MUTUAL INFORMATION (FLMI)",
            "content": "The FLMI function is designed to maximize the mutual information between the selected subset and the target domain dataset DT . In our context, it ensures that the selected data points are not only informative in general but also particularly relevant to the specific task at hand. This makes it ideal for task-specific fine-tuning, where we want to bridge the gap between general knowledge and task-specific information. It is defined below where η is scaling factor (set to 1 in our experiments): fF LM (A; DT ) = (cid:88) iD max jA sij + η (cid:88) jA max iDT sij, (4)"
        },
        {
            "title": "3.3.3 FACILITY LOCATION CONDITIONAL GAIN (FLCG)",
            "content": "From an information-theoretic standpoint, the FLCG function aims to maximize the conditional information gain of the selected subset given the existing dataset DE . It quantifies how much new information each data point brings, conditional on what the model already knows. This is crucial for continual fine-tuning, where we want to avoid redundancy and focus on novel, complementary information that expands the models knowledge base without unnecessary repetition. It is defined as the following where ν is scaling factor (set to 1 in our experiments). fF LCG(A DE) = (cid:18) (cid:19) max max jA sij ν max kDE sik, 0 , (cid:88) iD (5) Each submodular function, when combined with our utility-based kernel, guides the selection of data subsets tailored to the specific fine-tuning stage. This ensures that DELIFT selects the most informative and diverse examples, maximizing the efficiency and effectiveness of fine-tuning."
        },
        {
            "title": "3.4 UTILITY KERNEL AS FEATURE SPACE",
            "content": "Our approach utilizes the utility-based kernel as feature space for data selection, representing significant departure from traditional semantic similarity-based methods. Traditional methods often rely on sentence embeddings (SE) to capture static semantic similarities between data points. In contrast, our utility-based kernel measures the actual impact of examples on model performance, providing dynamic and task-specific assessment. This distinction is crucial for two main reasons: 1. Semantic Diversity vs. Performance Enhancement: While SE-based methods select diverse examples solely based on semantic content, our utility-based approach selects examples that demonstrably improve model performance across various inputs; 2. Model-Aware Selection: The utility-based kernel is attuned to the models current capabilities and weaknesses, enabling the selection of data points that are most beneficial for enhancing performance on the target task. By integrating the utility-based kernel with the aforementioned submodular functions DELIFT tailors the data selection process to each fine-tuning stage: instruction tuning, task-specific fine-tuning, and continual learning."
        },
        {
            "title": "3.5 DATA SUBSET SELECTION ALGORITHM",
            "content": "To operationalize our data selection approach, we employ **greedy algorithm** that iteratively builds the subset by selecting the data point that offers the maximum marginal gain in the chosen submodular function. Algorithm 1 Greedy Maximization for Submodular Function Require: Dataset D, submodular function , budget 1: Initialize subset 2: for = 1 to do 3: 4: 5: end for 6: return Select = arg maxdDA (f (A {d}) (A)) Update {d} This greedy algorithm ensures that each addition to the subset maximizes the marginal gain in the submodular function . By iteratively selecting the most beneficial data points according to the utility-based kernel and the specific submodular function tailored to the fine-tuning stage, DELIFT efficiently utilizes the data budget to select the most informative examples. The complete subset selection process involves the following steps: 1. Compute the Utility-Based Kernel: Calculate Fij for all relevant pairs of data points in the dataset to assess their informativeness; 2. Select the Appropriate Submodular Function: Depending on the fine-tuning stage (instruction tuning, task-specific fine-tuning, or continual fine-tuning), choose the corresponding submodular function (FL, FLMI, or FLCG); 3. Apply the Greedy Maximization Algorithm: Use Algorithm 1 to iteratively build the subset by selecting data points that offer the highest marginal gain according to the selected submodular function. By synergizing our novel utility-based kernel with submodular optimization, DELIFT achieves dataefficient fine-tuning that effectively addresses both redundancy and informativeness in the data selection process, optimizing the models performance across various tasks and domains."
        },
        {
            "title": "4 EXPERIMENTAL RESULTS",
            "content": "We conducted extensive experiments to evaluate the effectiveness of DELIFT across various finetuning scenarios, model scales, and datasets. This section details our experimental setup, baselines, evaluation metrics, and results analysis. 6 Model Method ICL ROUGE BGE Initial Random SelectIT LESS DELIFT (SE) DELIFT Full Data 37.87 39.00 43.08 42.08 47.43 48.46 58.65 78.92 80.66 84.50 83.24 84.40 85. 88.72 Qwen2 Phi-3 QLoRA ROUGE BGE 36.36 44.45 45.14 45.16 48.22 52.79 65.51 82.55 85.46 85.88 84.95 86.50 88.04 92.24 LAJ 3.02 3.12 3.21 3.28 3.28 3. 3.51 ICL ROUGE BGE 25.76 33.05 36.11 47.10 46.62 49.83 55. 43.34 72.73 76.31 85.94 85.28 85.27 88.26 LAJ 2.98 3.12 3.18 3.26 3.28 3.35 3.45 QLoRA ROUGE BGE 35.50 44.70 49.68 48.68 45.64 50.31 74.98 80.46 83.75 85.84 85.86 83.70 84.40 93. LAJ 2.58 2.95 3.20 3.24 3.27 3.33 3.84 LAJ 1.42 2.92 3.18 3.23 3.24 3.32 3. Table 1: Results on Use Case 1: MixInstruct. Bold indicates the best performance. There is 10.44% performance percentage drop from Full Data to DELIFT after pruning 70% of the data, and 2.27% performance percentage drop from DELIFT to the next best baseline. Model Method ICL ROUGE BGE Initial Random SelectIT LESS DELIFT (SE) DELIFT Full Data 18.03 20.05 31.38 34.59 34.69 35.48 36.43 59.13 59.39 71.08 83.23 83.31 83.69 84. Qwen2 Phi-3 QLoRA ROUGE BGE 20.15 20.29 32.96 35.03 35.46 35. 35.88 58.38 59.39 74.76 83.37 83.43 83.64 76.87 LAJ 1.54 1.79 2.86 3.07 3.43 3.58 3. ICL ROUGE BGE 20.10 20.83 35.37 39.69 37.07 40.66 42.07 48.66 49.92 66.67 72.12 71.49 84. 85.26 LAJ 1.78 1.83 2.90 3.50 3.53 3.54 3.63 QLoRA ROUGE BGE 20.64 24.51 38.98 40.32 38.13 41.91 44.73 49.17 53.41 69.84 70.89 79.68 84.53 87.03 LAJ 1.39 2.36 2.54 3.24 3.74 3.76 3.82 LAJ 1.36 2.24 2.52 3.17 3.52 3.68 3.78 Table 2: Results on Use Case 1: P3. Bold indicates the best performance. There is only 0.76% performance percentage drop from Full Data to DELIFT after pruning 70% of the data, and 3.23% performance percentage drop from DELIFT to the next best baseline."
        },
        {
            "title": "4.1 DATASETS AND USE CASES",
            "content": "We evaluated DELIFT across the three previously described fine-tuning scenarios: Use Case 1: Instruction Tuning We evaluated the effectiveness of DELIFT for use case 1 on two datasets: MixInstruct (Jiang et al., 2023) and P3 (Public Pool of Prompts) (Sanh et al., 2021). We randomly selected 21,000 train, 6,000 valid, and 3,000 test samples. Using the Facility Location (FL) submodular function, we aimed to select subset of training data that was both representative and informative. Use Case 2: Task-Specific Fine-Tuning We evaluated DELIFT for task-specific fine-tuning using two dataset pairs: (1) HotpotQA (Yang et al., 2018) with MMLU (Hendrycks et al., 2021), and (2) MixInstruct with MT-Bench (Zheng et al., 2023). We used the Facility Location Mutual Information (FLMI) submodular function to select the most informative samples from the training datasets (HotpotQA and MixInstruct) that shared relevant information with the target datasets (MMLU and MT-Bench, respectively). Use Case 3: Continual Fine-Tuning We evaluated DELIFT in continual fine-tuning setting using two dataset pairs: (1) SQuAD (Rajpurkar et al., 2016) paired with HotpotQA for general questionanswering, and (2) proprietary query rewriting datasets covering IBM and government domains.1 Our goal was to integrate new knowledge efficiently while minimizing redundancy. We employed the Facility Location Conditional Gain (FLCG) submodular function, selecting complementary samples from the new dataset (HotpotQA and Government query rewrite) that provided additional, nonoverlapping information to the existing dataset (SQuAD and IBM query rewrite). 1In this task, non-standalone questions questions that require previous context to answer must be rewritten to be standalone. For example, How much is it? should be rewritten to How much is the subscription for IBM Cloud? Such queries are common in user-agent conversations where user asks follow-up to an agent. Model Method Initial Random SelectIT LESS DELIFT (SE) DELIFT Full Data Qwen2 PhiQLoRA QLoRA 82.10 79.31 79.13 80.35 80.10 81.70 78.36 69.10 65.16 65.24 66.72 66.36 68.70 64. Table 3: Results on Use Case 2: HotpotQA and MMLU (5-shot) for Qwen2 and Phi-3 models (classification accuracy). Bold indicates the best performance. For Qwen2, DELIFT outperforms Full Data by 3.34%, while for Phi-3, it improves by 4.20%. Model Method ICL ROUGE BGE Initial Random SelectIT LESS DELIFT (SE) DELIFT Full Data 44.32 49.78 54.92 59.63 62.85 64.73 65.89 74.86 79.54 83.71 85.89 86.94 87.82 88. Qwen2 Phi-3 QLoRA ROUGE BGE 47.65 52.91 57.86 62.74 65.83 67. 69.72 77.92 82.67 86.59 88.72 89.76 90.64 91.53 LAJ 2.72 3.05 3.31 3.48 3.57 3.66 3. ICL ROUGE BGE 39.57 44.63 49.75 54.82 57.69 59.58 60.76 69.43 74.28 78.64 81.95 82.87 83. 84.59 LAJ 2.31 2.62 2.91 3.08 3.17 3.26 3.34 QLoRA ROUGE BGE 42.89 47.85 52.68 57.73 60.54 62.47 64.31 72.76 77.39 81.52 84.67 85.59 86.48 87.42 LAJ 2.48 2.83 3.12 3.29 3.38 3.47 3.55 LAJ 2.53 2.84 3.13 3.29 3.38 3.47 3.55 Table 4: Results on Use Case 2: MixInstruct and MT-Bench. Bold indicates the best performance. There is 2.91% performance percentage drop from Full Data to DELIFT after pruning 70% of the data, and 1.14% performance percentage drop from DELIFT to the next best baseline."
        },
        {
            "title": "4.2 EXPERIMENTAL SETUP",
            "content": "Models: We evaluated DELIFT on two state-of-the-art open-source models: Phi-3-mini-128kinstruct (Abdin et al., 2024): 3.8B parameters, Qwen2-72B-Instruct (Yang et al., 2024): 72B parameters. These models were chosen to demonstrate effectiveness across different model scales. Metrics: We use variety of metrics to characterize performance. For n-gram word overlap we use ROUGE (Lin, 2004). For semantic similarity we calculate the dot product between the embeddings from the bge-large-en-v1.5 model (Xiao et al., 2023); the embeddings are normalized to unit vectors, hence the closer the dot product is to 1, the more semantically similar the vectors (the metric is referred to as BGE). Additionally, we use Prometheus (Kim et al., 2023), specifically the prometheus-7b-v2.0 model, as an LLM-as-a-Judge (referred to as LAJ). With our custom rubric outlined in Appendix B, Prometheus assigns scores in range of 1 to 5 (higher scores indicate better performance.) Finally, we use classification accuracy to evaluate MMLU. Baselines: We evaluated DELIFT by comparing it against several baselines to understand its effectiveness in data selection. These baselines included: (1) SelectIT (Liu et al., 2024), which selects data using model feedback at the token, sentence, and model levels to identify useful samples; (2) LESS (Xia et al., 2024), which leverages LoRA approximated gradient-based influence estimation to prioritize impactful data points; (3) Random, which selects fixed percentage (x%) of the dataset randomly, providing benchmark for non-strategic selection; (4) DELIFT with Sentence Embedding Features (SE), which uses DELIFT but substitutes sentence embeddings as the feature space, employing model-independent, pairwise similarity kernel instead of the utility kernel for submodular optimization; and (5) Full Data, where the entire dataset is used for fine-tuning, serving as an upper benchmark for performance. For In-Context Learning (ICL), the selected subsets from each baseline were used as the pool of examples, allowing us to evaluate how effectively each method supports ICL by providing relevant and informative data. 8 Model Method ICL ROUGE BGE Initial Random SelectIT LESS DELIFT (SE) DELIFT Full Data 44.11 55.57 63.07 64.28 61.07 69. 66.08 70.49 85.26 86.38 85.41 85.16 87.94 87.84 Qwen2 Phi-3 QLoRA ROUGE BGE 48.49 55.52 65.42 69.85 74.05 74.19 76.83 80.85 85.53 87.50 89.33 92.47 92.23 92. LAJ 2.62 2.94 3.20 3.45 3.58 3.65 3.74 ICL ROUGE BGE 40.66 45.76 63.49 66.01 68.84 74.11 71.23 58.68 76.19 85.27 87.20 88.46 89.41 91.10 LAJ 1.52 2.45 2.96 3.19 3.32 3. 3.52 QLoRA ROUGE BGE 43.96 58.94 64.09 67.53 69.30 74.38 77. 69.56 82.41 85.07 88.17 88.62 91.55 91.10 LAJ 2.29 2.89 3.16 3.22 3.35 3.57 3.64 LAJ 2.43 2.91 3.18 3.29 3.45 3.60 3.65 Table 5: Results on Use Case 3: IBM and Government. Bold indicates the best performance. There is only 0.31% performance percentage drop from Full Data to DELIFT after pruning 70% of the data, and 3.89% performance percentage drop from DELIFT to the next best baseline. Model Method ICL ROUGE BGE Initial Random SelectIT LESS DELIFT (SE) DELIFT Full Data 51.51 54.38 58.03 67.16 73.75 76.94 77. 66.97 79.12 83.75 85.76 88.01 90.41 90.31 Qwen2 Phi-3 QLoRA ROUGE BGE 54.18 59.23 63.26 69.72 74.84 77.56 78.72 78.27 82.02 84.01 86.63 88.79 89.99 90.77 LAJ 1.77 2.57 2.82 2.94 3.26 3.33 3.35 ICL ROUGE BGE 40.42 44.29 47.35 60.97 64.44 66. 68.47 58.23 59.45 74.15 81.41 83.95 84.65 85.93 LAJ 1.26 1.33 2.54 2.84 3.03 3.25 3. QLoRA ROUGE BGE 40.94 50.29 56.88 61.56 66.35 67.09 70.48 58.12 61.52 80.47 81.53 84.77 85. 86.06 LAJ 2.50 2.66 2.87 3.26 3.30 3.34 3.48 LAJ 1.29 1.60 2.70 2.88 3.14 3. 3.44 Table 6: Results on Use Case 3: SQuAD and HotpotQA. Bold indicates the best performance. There is only 1.94% performance percentage drop from Full Data to DELIFT after pruning 70% of the data, and 2.78% performance percentage drop from DELIFT to the next best baseline."
        },
        {
            "title": "4.3 RESULTS AND ANALYSIS",
            "content": "To ensure fair and comprehensive evaluation of DELIFT, we conducted experiments across three distinct fine-tuning scenarios: instruction tuning, task-specific fine-tuning, and continual fine-tuning. For all subset selection methodsincluding DELIFT, Random, SelectIT, LESS, and DELIFT with Sentence Embdedding Features (SE)we consistently selected 30% of the dataset as subset, enabling direct comparisons between methods and with the full dataset baseline (see Section 4.4 for an ablation study examining the impact of subset size). Use Case 1: Instruction Tuning Our first set of experiments focused on instruction tuning, crucial task to enhancing models ability to follow diverse instructions. As shown in Tables 1 and 2, DELIFT achieved minimal performance drop of only 5.60% compared to using the full dataset while reducing the dataset by 70%. This demonstrates DELIFTs capability to retain the most informative samples essential for instruction tuning. Furthermore, DELIFT outperformed other subset selection methods, achieving 2.74% improvement and substantial 26.21% advantage over the next best and worst-performing baselines, respectively. These results underscore DELIFTs superior ability to maintain high performance with significantly reduced data, highlighting its efficacy in instruction tuning. Use Case 2: Task-Specific Fine-Tuning In the task-specific fine-tuning scenario, we evaluated DELIFT using two dataset pairs: (1) HotpotQA (Yang et al., 2018) with MMLU (Hendrycks et al., 2021), and (2) MixInstruct paired with MT-Bench (Zheng et al., 2023). Results, presented in Tables 4 and 3, demonstrate DELIFTs consistent and competitive performance across different task pairs. particularly noteworthy outcome emerged from the HotpotQA-MMLU pair, where DELIFT not only matched but exceeded the performance of the full dataset, achieving 5.51% improvement. This indicates that DELIFTs selective approach can effectively filter out noise and focus on the most relevant and informative samples, yielding enhanced task-specific adaptation even with reduced data. Use Case 3: Continual Fine-Tuning The third use case examined DELIFTs efficacy in continual fine-tuning, where models need to incorporate new information while retaining previously learned 9 knowledge. As detailed in Tables 5 and 6, DELIFT demonstrated remarkable consistency, showing only marginal 1.13% performance drop compared to using the full dataset. Moreover, DELIFT outperformed the second-best baseline by 3.33% and the worst baseline by 23.88%, highlighting its superiority in data selection. In specialized tasks such as query rewriting, DELIFT even surpassed the performance of the full dataset, suggesting that its selective approach effectively prunes noisy or irrelevant data points, thereby enhancing model performance."
        },
        {
            "title": "4.4 ABLATION STUDY: IMPACT OF SUBSET SIZE",
            "content": "To assess how subset size influences DELIFTs performance, we conducted an ablation study by varying the subset size from 5% to 100% of the full dataset across three use cases. The results, detailed in Appendix and illustrated in Figure 2, show that LAJ scores generally increase with subset size. Utilizing the full dataset consistently yields the highest performance, highlighting the benefits of larger training sets. However, for methods such as DELIFT, SelectIT, and LESS, performance gains plateau or slow beyond 50% subset size, indicating that additional data offers minimal benefits and may introduce redundancy. Importantly, DELIFT outperforms all baselines across subset sizes from 5% to 100%, demonstrating its robustness and effectiveness in selecting informative samples regardless of subset size. These findings suggest that carefully selected smaller datasets can achieve comparable performance to larger, unfiltered datasets, which is particularly valuable for resource-intensive large language models."
        },
        {
            "title": "4.5 DISCUSSION",
            "content": "The comprehensive results across all three use cases highlight DELIFTs effectiveness and versatility. By consistently reducing data requirements by up to 70% while maintainingand in some cases improvingperformance, DELIFT addresses critical challenge in large language model fine-tuning. The superior performance of DELIFT can be attributed to its novel pairwise utility metric and the use of tailored submodular functions for each fine-tuning stage. This approach enables DELIFT to select not only representative and diverse samples but also to reduce noise present in the full dataset. The ability to outperform full datasets in certain scenarios, particularly in niche tasks like query rewriting, underscores DELIFTs capacity to distill the most relevant and informative data points. These findings have significant implications for the accessibility and efficiency of LLM fine-tuning. By dramatically reducing the amount of data required for effective fine-tuning, DELIFT paves the way for more widespread adoption and application of large language models across various domains, especially in resource-constrained environments. Furthermore, DELIFTs consistent outperformance of existing data selection techniques across various fine-tuning scenarios and model scales demonstrates its robustness and broad applicability, making it valuable tool for researchers and practitioners alike. In conclusion, our experimental results firmly establish DELIFT as powerful and efficient method for data selection in LLM fine-tuning. By addressing the critical challenge of optimal data selection, DELIFT not only enhances the efficiency of model training but also opens new possibilities for fine-tuning large language models in domains where data or computational resources may be limited."
        },
        {
            "title": "5 CONCLUSION, LIMITATIONS, AND FUTURE WORK",
            "content": "In this paper, we introduced DELIFT, novel approach to data-efficient fine-tuning of large language models by employing versatile pairwise utility metric combined with submodular optimization techniques for optimal data selection. Empirical evaluations showed that DELIFT can reduce data and computational requirements by up to 70% while achieving performance comparable to the full dataset, and outperforming existing data selection methods by up to 26% in effectiveness. These results suggest that DELIFT offers promising method for improving the accessibility of LLM adaptation, especially for resource-constrained scenarios. However, our approach has limitations, including potential sensitivity to the quality and diversity of initial data and the risk of bias amplification inherent in the selected data. Future work will explore integrating DELIFT with data augmentation techniques to improve robustness, incorporating fairness constraints to mitigate biases, and extending the approach to emerging model architectures and multimodal learning. Our ongoing efforts are directed toward ensuring that DELIFT contributes to responsible and equitable AI development while maximizing efficiency."
        },
        {
            "title": "6 ACKNOWLEDGEMENT",
            "content": "This work used the Delta system at the National Center for Supercomputing Applications through allocation CIS240550 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio Cesar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel PerezBecker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Gantavya Bhatt, Yifang Chen, Arnav Das, Jifan Zhang, Sang Truong, Stephen Mussmann, Yinglun Zhu, Jeff Bilmes, Simon Du, Kevin Jamieson, Jordan Ash, and Robert Nowak. An experimental design framework for label-efficient supervised finetuning of large language models. In LunWei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 65496560, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.390. URL https://aclanthology.org/2024.findings-acl.390. Jeff Bilmes. Submodularity in machine learning and artificial intelligence, 2022. URL https: //arxiv.org/abs/2202.00132. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec 11 Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020b. URL https://arxiv.org/abs/2005.14165. Alexander Bukharin and Tuo Zhao. Data diversity matters for robust instruction tuning, 2024. URL https://arxiv.org/abs/2311.14736. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training better alpaca with fewer data, 2024. URL https://arxiv.org/abs/2307.08701. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423. Qianlong Du, Chengqing Zong, and Jiajun Zhang. Mods: Model-oriented data selection for instruction tuning, 2023. URL https://arxiv.org/abs/2311.15653. Satoru Fujishige. Submodular functions and optimization. Elsevier, 2005. Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Dont stop pretraining: Adapt language models to domains and tasks. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 83428360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.740. URL https://aclanthology.org/2020.acl-main.740. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088. Rishabh Iyer, Ninad Khargoankar, Jeff Bilmes, and Himanshu Asanani. Submodular combinatorial information measures with applications in machine learning. In Vitaly Feldman, Katrina Ligett, and Sivan Sabato (eds.), Proceedings of the 32nd International Conference on Algorithmic Learning Theory, volume 132 of Proceedings of Machine Learning Research, pp. 722754. PMLR, 1619 Mar 2021. URL https://proceedings.mlr.press/v132/iyer21a.html. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. (arXiv:2306.02561), June 2023. URL http:// arxiv.org/abs/2306.02561. arXiv:2306.02561 [cs]. Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. Grad-match: Gradient matching based data subset selection for efficient deep model training, 2021a. URL https://arxiv.org/abs/2103.00123. Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection for efficient and robust learning, 2021b. URL https: //arxiv.org/abs/2012.10630. Krishnateja Killamsetty, Alexandre V. Evfimievski, Tejaswini Pedapati, Kiran Kate, Lucian Popa, and Rishabh Iyer. Milo: Model-agnostic subset selection framework for efficient model training and tuning, 2023. URL https://arxiv.org/abs/2301.13287. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491, 2023. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting LLM performance with selfguided data selection for instruction tuning. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 75957628, Mexico City, Mexico, June 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.naacl-long.421. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013. Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, and Min Zhang. Selectit: Selective instruction tuning for large language models via uncertainty-aware self-reflection, 2024. URL https://arxiv.org/abs/2402.16705. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: designing data and methods for effective instruction tuning. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, Pascale Fung, and Zhiguang Wang. Continual learning in task-oriented diIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wenalogue systems. tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 74527467, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.590. URL https://aclanthology.org/2021.emnlp-main.590. Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning models, 2020. URL https://arxiv.org/abs/1906.01827. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 34703487, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.244. URL https://aclanthology.org/2022.acl-long.244. George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14:265294, 1978. URL https://api.semanticscholar.org/CorpusID:206800425. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, In and Ryan Lowe. Training language models to follow instructions with human feedback. S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2773027744. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf. 13 Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Profor machine comprehension of text. ceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2021. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural scaling laws: beating power law scaling via data pruning, 2023. URL https://arxiv.org/ abs/2206.14486. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=gEZrGCozdqR. Ronald J. Williams and David Zipser. learning algorithm for continually running fully recurrent neural networks. Neural Computation, 1(2):270280, 1989. doi: 10.1162/neco.1989.1.2.270. Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual learning for large language models: survey, 2024. URL https://arxiv.org/ abs/2402.01364. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. LESS: Selecting influential data for targeted instruction tuning. In International Conference on Machine Learning (ICML), 2024. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023. Tianci Xue, Ziqi Wang, Yixia Li, Yun Chen, and Guanhua Chen. PACIT: Unlocking the power of examples for better in-context instruction tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 654665, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.36. URL https://aclanthology.org/2024. findings-acl.36. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. 14 Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/1809.09600. Jipeng Zhang, Yaxuan Qin, Renjie Pi, Weizhong Zhang, Rui Pan, and Tong Zhang. Tagcos: Taskagnostic gradient clustered coreset selection for instruction tuning data, 2024. URL https: //arxiv.org/abs/2407.15235. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke ZettleIn A. Oh, T. Naumoyer, and Omer Levy. mann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 5500655021. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/ac662d74829e4407ce1d126477f4a03a-Paper-Conference.pdf. Lima: Less is more for alignment."
        },
        {
            "title": "A SUBSET SIZE COMPARISON",
            "content": "To assess how subset size influences the performance of DELIFT, we performed an ablation study by varying the subset size from 5% to 100% (specifically 5%, 15%, 30%, 50%, 100%) of the entire dataset across three distinct use cases. Figure 2 illustrates the performance metric LAJ as function of subset size for each fine-tuning scenario. A.1 GENERAL OBSERVATIONS Performance Increases with Subset Size: Across all methods, LAJ scores generally improve as the subset size increases. Utilizing the full dataset consistently yields the highest performance, underscoring the benefits of larger training set. Diminishing Returns Beyond 50%: For methods such as DELIFT, SelectIT, and LESS, performance gains plateau or slow down beyond 50% subset size. This suggests that additional data beyond this point offers minimal benefits and may introduce redundancy. A.2 DETAILED ANALYSIS OF METHODS A.2. INITIAL VS. RANDOM SELECTION Initial Baseline: Consistently records the lowest scores across all subset sizes, indicating that models without data-informed selection struggle to generate quality responses. Random Selection: Slightly outperforms the Initial baseline but maintains relatively flat performance curve. This lack of significant improvement highlights that uninformed data selection does not substantially enhance model quality. A.2.2 SELECTIT AND LESS METHODS LESS: Demonstrates strong upward trend, particularly when subset sizes increase from 15% to 50%. This indicates that LESS effectively selects informative subsets, especially in the mid-range subset sizes, but is sub-optimal with smaller subset sizes. SelectIT: Initially lags behind DELIFT and LESS but shows steady improvement with larger subset sizes. For subset sizes above 50%, SelectIT approaches the performance of DELIFT, suggesting its heuristic-driven selection becomes more effective with more data. A.2.3 DELIFT VARIANTS DELIFT vs. DELIFT (SE): DELIFT consistently outperforms DELIFT (SE), which uses sentence embeddings, highlighting the superiority of DELIFTs utility-based kernel in capturing data informativeness. DELIFT vs. Other Methods: DELIFT outperforms all other subset selection methods across all subset sizes, particularly below 50%. This effectiveness is attributed to DELIFTs strategy of identifying the most informative samples early on, making it ideal for scenarios with limited computational resources. DELIFT vs. Full Data: At smaller subset sizes (e.g., 15%, 30%), DELIFT achieves LAJ scores close to the Full Data baseline. In ICL fine-tuning scenarios, 30% subset size with DELIFT nearly matches Full Data performance, demonstrating its efficiency in data reduction without significant loss in performance. A. IMPACT ON DIFFERENT FINE-TUNING SCENARIOS ICL vs. QLoRA: QLoRA fine-tuning generally yields higher scores than ICL across all methods, suggesting that QLoRA benefits more from effective data selection strategies. DELIFT, in particular, shows more pronounced improvements in QLoRA settings, indicating its subsets are well-suited for efficient parameter tuning. 16 (a) (c) (e) (b) (d) (f) Figure 2: Graphs of LLM-A-J scores (y-axis) of Qwen2-72B-Instruct with varying subset sizes (xaxis) of Use Case 1 on MixInstruct for (a) ICL and (b) QLoRA, Use Case 2 on MixInstruct and MT-Bench for (c) ICL and (d) QLoRA, and Use Case 3 on IBM and Government for (e) ICL and (f) QLoRA. Use Case Comparisons: In Use Case 3 (IBM and Government datasets), DELIFT achieves the highest gains relative to the Initial baseline across both ICL and QLoRA scenarios. This effectiveness is likely due to the nature of query rewriting tasks, where DELIFTs informed data selection effectively eliminates redundant or irrelevant examples, resulting in higherquality training set."
        },
        {
            "title": "B PROMETHEUS RUBRIC",
            "content": "The Prometheus model served as an LLM-as-a-Judge to evaluate response quality from different data selection methods. Table 7 contains the general rubric used for the Prometheus model scoring on all use cases and settings (except for the experiments on the query-rewriting task using the IBMproprietary data). 17 Evaluate the models ability to follow instructions and deliver high-quality response across the following dimensions: 1. Instruction Following: How accurately and fully does the model adhere to the given instruction? 2. Accuracy: Is the information correct, reliable, and factually sound? 3. Relevance: Does the response directly address the question or task without unnecessary information? 4. Completeness: Does the response cover all essential aspects of the instruction or question 5. Depth: How thoroughly does the response explore the topic? Does it demonstrate insightful analysis where appropriate? 6. Clarity: Is the response well-organized, easy to follow, and free from ambiguity or confusion? 7. Creativity: Does the response offer original or innovative approaches where applicable? 8. Helpfulness: Does the response effectively meet the users needs and provide value in solving the problem or addressing the query? Score of 1: The response fails to meet expectations across most or all criteria. It does not follow the instruction, contains significant errors or misinformation, lacks relevance, is incomplete or shallow, unclear, unoriginal, and unhelpful. Score of 2: The response shows major deficiencies across several criteria. It partially follows the instruction but includes significant inaccuracies, is often irrelevant, incomplete, or lacks depth, clarity, creativity, and helpfulness. Score of 3: The response is average, meeting some but not all criteria. It follows the instruction but may fall short in terms of accuracy, depth, relevance, or helpfulness. Improvements in clarity and insightfulness may be needed. Score of 4: The response is strong, performing well across most criteria. It follows the instruction closely, is mostly accurate and relevant, provides good depth, and is well-structured. Minor improvements could enhance clarity, creativity, or helpfulness. Score of 5: The response excels in all or nearly all criteria. It fully follows the instruction, is highly accurate, directly relevant, complete, and demonstrates depth and insight. The response is well-organized, creative where appropriate, and very helpful in addressing the users needs. Table 7: General Prometheus Rubric B.1 USAGE NOTES Each response is evaluated independently based on the criteria above. The cumulative score reflects the overall quality and effectiveness of the response. Final LAJ scores are obtained by averaging the scores across all criteria. LLM-AS-JUDGES SCORES In Tables 8 and 9, we show the distribution of Prometheus scores on one particular setting: Use Case 1, MixInstruct training and MixInstruct validation sets on the Qwen2-72B-Instruct model. These figures make clear that the average LGA scores computed in Tables 1-6 are true averages of distribution of scores, not averages of combination of just 1s and 5s. C."
        },
        {
            "title": "INTERPRETATION OF SCORE DISTRIBUTIONS",
            "content": "C.1.1 OVERALL TRENDS Score Variability: There is significant variability in score distributions across different methods. The Initial and Random baselines show concentration of scores between 2.5 and 3.5, indicating average to subpar performance. Enhanced Performance with Advanced Methods: Methods like SelectIT, LESS, DELIFT (SE), and DELIFT exhibit score distributions skewed towards higher values (3.5 to 4.0), with DELIFT showing the highest concentration above 3.5. This highlights their effectiveness in selecting informative and useful data for fine-tuning."
        },
        {
            "title": "SelectIT",
            "content": "Table 8: LLM-as-Judges score distributions for Use Case 1 with MixInstruct training and validation set on the Qwen2-72B-Instruct model on the Initial, Random, and SelectIT baselines. The corresponding table is Table 1."
        },
        {
            "title": "LESS",
            "content": "DELIFT (SE)"
        },
        {
            "title": "Full Data",
            "content": "Table 9: LLM-as-Judges score distributions for Use Case 1 with MixInstruct training and validation set on the Qwen2-72B-Instruct model on the LESS, DELIFT with Sentence Embedding, DELIFT, and Full Data methods. The corresponding table is Table 1. 20 C.1.2 METHOD-SPECIFIC OBSERVATIONS Initial and Random Methods: Both methods have lower mean scores (around 3.0 to 3.2) with wide spreads, suggesting inconsistent and generally lower-quality responses. SelectIT and LESS Methods: SelectIT: Shows improved mean scores, especially in QLoRA settings, indicating its effectiveness in resource-constrained training scenarios. LESS: Demonstrates significant performance improvements, with mean scores around 3.26 to 3.28, reflecting effective gradient-based data selection. DELIFT Variants: DELIFT (SE): Skews towards higher scores but not as prominently as DELIFT. DELIFT: Achieves the highest average scores (3.35 for ICL and 3.37 for QLoRA), outperforming all other methods and indicating its superior utility-based kernel and submodular optimization. C.1.3 COMPARISON WITH FULL DATA DELIFT vs. Full Data: DELIFT nearly matches Full Data performance with only slight reduction in mean scores (3.35 to 3.37 vs. 3.45 to 3.51). This demonstrates DELIFTs capability to retain most of the models performance while using significantly less data. Efficiency of Data Pruning: Full Data shows modest increase in mean scores compared to DELIFT, but at the cost of substantially higher computational resources. DELIFT offers more efficient alternative without major sacrifices in performance."
        },
        {
            "title": "D LIMITATIONS",
            "content": "Dependence on Initial Data Quality: DELIFTs effectiveness relies on the diversity and quality of the initial dataset. Biases or lack of diversity in the dataset can propagate to the selected subsets. Scalability Constraints: While DELIFT is computationally efficient, extremely large datasets may still present challenges in terms of computation and memory. Domain-Specific Performance: DELIFTs performance may vary across different domains, particularly those requiring specialized knowledge or handling multimodal data. Bias Amplification Risks: The subset selection process may unintentionally amplify existing biases within the data, necessitating careful mitigation strategies."
        },
        {
            "title": "E FUTURE WORK",
            "content": "Integration with Data Augmentation: Combining DELIFT with data augmentation techniques could further enhance the robustness and diversity of selected subsets. Fairness and Bias Mitigation: Incorporating fairness constraints and bias mitigation strategies into the subset selection process to ensure equitable model performance across different groups. Extension to Multimodal Learning: Adapting DELIFT for multimodal data (e.g., text, images, audio) to expand its applicability beyond natural language processing. Theoretical Analysis: Developing deeper theoretical understanding of the utility metric and its properties to further validate and refine the approach. Enhancing Scalability: Exploring methods to scale DELIFT effectively for larger datasets and more complex models without compromising efficiency. Our ongoing efforts aim to ensure that DELIFT contributes to responsible and equitable AI development while maximizing efficiency."
        },
        {
            "title": "F CODE AND DATA AVAILABILITY",
            "content": "To facilitate reproducibility and further research, we will make the DELIFT implementation and the datasets used in our experiments publicly available upon publicaInterested researchers can access these resources through the following repository: tion. https://anonymous.4open.science/r/optimizing-data-selection-0CD0."
        },
        {
            "title": "G HYPERPARAMETER SETTINGS",
            "content": "Consistent hyperparameter settings were maintained across all experiments to ensure reproducibility: Submodular Function: Utilized Facility Location (FL), Facility Location Mutual Information (FLMI), or Facility Location Conditional Gain (FLCG) based on the use case. Utility Metric Scaling Factor: Set η = 1 for FLMI and ν = 1 for FLCG. Budget (% of Data): Fixed at 30% for all subset selection experiments. Optimization Algorithm: Employed greedy maximization with stopping criterion based on the budget. Distance Metric: Used length-normalized L2 norm. Teacher Forcing Technique: Applied during utility metric computation to ensure reliable prediction accuracy measurement."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign",
        "IBM Research"
    ]
}