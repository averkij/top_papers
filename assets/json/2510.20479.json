{
    "paper_title": "RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging",
    "authors": [
        "Bowen Wang",
        "Haiyuan Wan",
        "Liwen Shi",
        "Chen Yang",
        "Peng He",
        "Yue Ma",
        "Haochen Han",
        "Wenhao Li",
        "Tiao Tan",
        "Yongjian Li",
        "Fangming Liu",
        "Yifan Gong",
        "Sheng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 7 4 0 2 . 0 1 5 2 : r RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging Bowen Wang1,, Haiyuan Wan1,2, Liwen Shi1, Chen Yang5, Peng He1,2, Yue Ma1,2, Haochen Han2 Wenhao Li4, Tiao Tan1, Yongjian Li6, Fangming Liu2, 3,, Yifan Gong2,, Sheng Zhang1, 1Shenzhen International Graduate School, Tsinghua University 2Peng Cheng Laboratory, 3Huazhong University of Science and Technology 4Xiamen University, 5The Hong Kong University of Science and Technology, Guangzhou 6School of Biomedical Engineering, Tsinghua University Correspondence: {wangbw23, wanhy24}@mails.tsinghua.edu.cn, fangminghk@gmail.com gongyf@pcl.ac.cn, zhang_sh@mail.tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge and propose RECALLi, novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance tradeoffs, RECALL achieves seamless multi-domain knowledge fusion and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing scalable and data-free solution for evolving LLMs."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved impressive advances across tasks like question answering, text generation, and mathematical reasoning, powering applications such as chatbots, AI business agents, and recommendation systems (Devlin, 2018; Brown et al., 2020; Touvron et al., 2023; Raffel et al., 2020). They are typically trained through unsupervised pre-training on large corpora, followed by supervised fine-tuning (SFT) on task-specific or domain-specific data (Brown et al., 2020; Touvron et al., 2023; Wei et al., 2021; Ouyang et al., 2022). However, LLMs remain susceptible to catastrophic forgetting (CF), where distribution shifts during training lead to parameter Equal contribution Corresponding authors ihttps://github.com/bw-wang19/RECALL updates that overwrite prior knowledge (Mccloskey and Cohen, 1989; Kirkpatrick et al., 2016; Li and Hoiem, 2018). As LLMs are increasingly applied in continual and multi-domain settings, mitigating CF is essential to maintain both specialization and generalization (Brown et al., 2020; Wei et al., 2021; Achiam et al., 2023; Doimo et al., 2024). As illustrated in Figure 1, previous approaches addressing CF generally fall into two categories, each with distinct strengths and limitations: 1) Data-based methods preserve past knowledge by revisiting stored samples from previous tasks during training on new tasks (Lopez-Paz and Ranzato, 2017; Rebuffi et al., 2016; Romanov et al., 2019; Isele and Cosgun, 2018). These methods are effective in retaining task-specific information by directly exposing the model to prior data. However, they require access to historical samples, which may be impractical due to storage constraints or privacy concerns in real-world scenarios. 2) Model-based methods constrain model updates or isolate task-specific knowledge via regularization (Huang et al., 2021; Kirkpatrick et al., 2016; Li and Hoiem, 2018; Wang et al., 2023) or architecture adaptation (Rusu et al., 2016; Fernando et al., 2017; Tian et al., 2024). These approaches enable continual learning without relying on past data, offering better scalability in privacy-sensitive settings. Nonetheless, they often operate within limited optimization spaces and struggle to preserve performance across diverse tasks. Additionally, they may depend on explicit task identifiers and increase model complexity over time. To overcome the limitations of existing continual learning approaches, we aim to combine the strengths of both data-based and model-based methods: retaining prior knowledge without relying on stored data, while enabling flexible model adaptation across tasks. However, without access to historical data, it becomes difficult to assess what knowledge should Figure 1: taxonomy of previous approaches to catastrophic forgetting. Data-based methods (a) rely on stored samples from previous tasks, which are replayed alongside new data during fine-tuning. Model-based methods (b) mitigate forgetting by either constraining parameter updates or isolating task-specific knowledge. In (b), the left side illustrates regularization-based methods that optimize model parameters within the intersection of low-loss regions for both old and new tasks (e.g., Task and Task B), instead of strictly minimizing the loss on the new task. This encourages more stable update trajectory that retains previously learned knowledge while adapting to new tasks. be preserved; and without explicit task boundaries, it is unclear how to guide model updates in structured and generalizable manner. This raises core challenge: how can we identify and preserve useful task knowledge across models in data-free and task-agnostic way? In addressing this question, we observe that internal representations, which reflect how models encode and process inputs, can serve as reliable proxies for their learned knowledge. These representations are inherently shaped by both model architecture and training objectives, making them well-suited for comparing and aligning knowledge across models without requiring access to raw data or task labels. Motivated by this insight and recent advances in model merging (Xiao et al., 2023; Wortsman et al., 2022; Jiang et al., 2023), we propose novel representation-aware model merging strategy that addresses both data availability and optimization flexibility. Our method computes inter-model similarities based on intermediate representations and uses them to guide adaptive, layer-wise parameter merging. By avoiding raw data, we circumvent privacy and accessibility concerns, while our fine-grained integration expands the optimization space beyond traditional methods and enables more effective knowledge fusion. Our main contributions are summarized as follows: We propose novel representation-aware model merging framework to address catastrophic forgetting, by leveraging intermediate representations to guide parameter fusion without relying on raw data or explicit task boundaries. Our method generalizes to the merging of multiple expert models fine-tuned on different domains, enabling effective multi-domain capability fusion through weighted representation alignment. We further demonstrate that the proposed framework can be applied to traditional continual learning benchmarks, including sequential fine-tuning scenarios, achieving strong performance without task-specific modifications. across multiple Extensive datasets the effectiveness and generality of our approach, showing consistent improvements in knowledge retention and transferability. and benchmarks validate experiments"
        },
        {
            "title": "Representation Dynamics in LLMs",
            "content": "Prior studies have shown that different layers of large language models encode distinct types of linguistic and semantic information (Tenney et al., 2019; Starace et al., 2023). Building on this, we analyze hidden representations from transformer layers to examine how they evolve within model and diverge across models fine-tuned on different tasks. 2.1 Layer-wise Representation Shift We first investigate how internal representations evolve across layers within single model for fixed input batch. Specifically, we compute the average RBF kernel similarity between adjacent layers hidden states. The similarity scores exhibit non-monotonic pattern, with noticeable drops Figure 2: Illustration of representation transformation across layers within single LLM. The input progresses through sequence of transformations, and the corresponding hidden states (shown for layers 0, 15, and 32) exhibit distinct structural patterns in the representation space, highlighting the non-uniform nature of internal dynamics. in both early and late layers. This indicates that the transformation of representations varies significantly across the network (see Appendix for details). In addition, as shown in Figure 2, visualization through clustering and dimensionality reduction techniques shows that hidden states at different layers form distinct structural patterns in the representation space. This layer-wise variation suggests that each layer contributes differently to the models behavior. As result, treating all layers uniformly during model mergingsuch as through naive parameter averagingmay overlook the unique functional roles of different layers and lead to suboptimal integration. 2.2 Specialization-induced Model Divergence We next examine how internal representations diverge across models that share the same architecture and initialization but have been fine-tuned on different tasks. Using the same input batch, we extract hidden states from each model and compute the average layer-wise RBF kernel similarity between them. We observe that lower-layer representations remain relatively consistent, while deeper layers diverge significantly across tasksa trend highlighted by the model-wise similarity curves in Appendix D. To further illustrate this phenomenon, Figure 3 visualizes the hidden states from two task-specific Figure 3: Visualization of representation drift between two models fine-tuned on different tasks (SST2 vs. RACE). Despite sharing the same input, their hidden states evolve along different trajectories and form distinct clustering patterns, especially in deeper layers. models. Despite processing the same inputs, their hidden states evolve along different trajectories and form distinct clustering structures, reinforcing the view that fine-tuning induces semantic specialization in deeper layers. These results suggest that naive parameter merging, especially in upper layers, may introduce semantic inconsistency or destructive interference if such representational misalignment is ignored."
        },
        {
            "title": "3 RECALL: REpresentation-aligned",
            "content": "Catastrophic-forgetting ALLeviation In Section 2, we analyze the characteristics of the data representation across models and layers through experimental observations, and illustrate that the knowledge of model is closely related to its data representation. And previous works (Wortsman et al., 2022; Xiao et al., 2023) have nicely illustrated that knowledge fusion and continual learning do not necessarily require fine-tuning stage such as knowledge distillation. Model merging can also directly and effectively achieve the goal. Therefore, inspired by those observations, we propose RECALL in this section, which performs layer-wise model merging by comparing the similarities of data representations between different models, so as to achieve representation alignment. As illustrated in Figure 4, RECALL effectively enhances LLMs abilities in multiple domains and tasks, and mitigates catastrophic forgetting. As prerequisite condition, we have the source Figure 4: Illustration of RECALL, our proposed representation-aware model merging framework. The pipeline consists of four stages: (1) extract hidden states from typical samples using the newly fine-tuned model MN , (2) compute the pairwise representational similarities across all models (including MN ), (3) derive layer-wise adaptive weights based on similarity scores via softmax, and (4) perform hierarchical parameter merging guided by the computed weights. This process enables effective knowledge fusion across models while preserving task-specific features. model M0 and multiple homologous expert models M1, M2, , MN 1, which have the same architecture but different parameters with M0. On the new task TN , we obtain the new model MN by finetuning M0 from the dataset DN . Then, we select typical samples Dtype = {d1, d2, , dm} DN through clustering algorithm. For each dk Dtype, we extract its representations on models M0 MN , analyze the differences between MN and other models in semantic and syntactic knowledge through the similarities between data representations. Finally, we perform hierarchical model merging for knowledge fusion. 3.1 Data Representation Representation Extraction. We extract the hidden states of layer of Mp, which is formulated as: Rn = (r1, r2, , rL) RLE, where is the number of input tokens, is the dimension of embedding vectors, and ri indicates the embedding of the ith token. Referring to the practice of most embedding models (Reimers and Gurevych, 2019; Xiao et al., 2023), we average the hidden states by token to obtain the representation vector: = 1 i=1 ri RE. (cid:80)L Typical Dataset Selection. Our approach does not place restrictions on the composition of datasets, which means that samples from multiple domains and tasks may be included in DN . Therefore, we cluster all data representations of DN , and select samples which are nearest to the cluster centers = {c1, c2, , cm} to form the typical dataset Dtype = {dt1, dt2, , dtm} DN . For [1, m]: dtk = arg mindiDN di ck2, in which ck is the kth cluster center clustered by Kmeans. In order to reduce the number of samples needed to perform forward inference for data representation analysis, we use Dtype as the representative of DN to analyze the knowledge difference of models. 3.2 Similarity Calculation For each sample dk in the typical dataset Dtype and each model Mp, its data representation at layer is rp,k RE. As mentioned above, we measure the difference in knowledge between models by data representations. Specifically, as we select typical samples by Kmeans which is closely related to the norm distance, RBF kernel function is adopted to measure similarity between representation vectors, and we calculate the algebraic average of similarities of all samples in Dtype as the overall similarity. We also carefully discuss the differences using different similarity measures and do experiments to compare them; results and discussions are detailed in Appendix F. The similarity between Mp and Mq on layer is formulated as: Sp,q = 1 (cid:88) k=1 exp ( rp,k rq,k 2σ 2 2 ), (1) in which σ is scaling factor. 3.3 Hierarchical Merging For later narration, we summarize here the paradigm approach to model merging. Current model merging is essentially linear interpolation of the model parameters, which means for each parameter θ in the model and merging weight w, we have: (cid:88) θ = wiθi = wT θ, (2) i=1 in which θ is the vector concatenated by parameter θ of different models, and is the vector of corresponding weights. Furthermore, Eq 2 can be easily extended to the case that group of parameters correspond to the same weights. We can compute linear interpolations of multiple parameters at once via the inner product operation like the following equation, θ = (cid:21) (cid:20)θ1 θ2 = (cid:21) (cid:20)wT θ1 wT θ2 = (cid:21) (cid:20)θT 1 θT 2 w. (3) In Section 3.2, we present the similarity metric to measure the similarities of data representation between models. To align their representations, we use Softmax to normalize representation similarities as merging weights. Then the weight of Mq in layer is as follows: wq = exp Sn,q (cid:80) p=0 exp Sn,p . (4) Therefore, we provide representation-aligned merging method for one layer: θ = (cid:88) q=0 wq θq = (cid:2)θ , θ2 , , θN (cid:3) wi = ΘT wi, (5) where θi denote the models parameters of layer i. According to Eq 3, 5, we perform hierarchical model merging layer by layer: θ = θ 1 θ 2 ... θ = ΘT 1 w1 ΘT 2 w2 ... LwL ΘT = diag(ΘT w), (6) in which θ is the parameter vector of the final merging model. Θ = [Θ1, Θ2, , ΘL] is the parameter matrix of M0N , and wT = [wT 1 , wT ] is the corresponding weight matrix. 2 , , wT As mentioned above, our method enhances the abilities of LLM in multi-domains and resists catastrophic forgetting by performing independent weight calculation between layers and hierarchical merging operations. The detailed procedure of RECALL is presented in Algorithm 1 in the Appendix G, and meanwhile we provide an analysis of runtime, memory usage, and scalability."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we will provide detailed introduction to our implementation and the results of experiments, which are mainly composed of three main parts: Experimental Setup, Different Merging Scenarios, and Sequential Fine-tuning Scenario. Furthermore, we summarize and analyze the results of these experiments, which strongly prove the superiority of our method. 4.1 Experimental Setup Datasets. Considering challenging experimental setup in knowledge fusion and continual learning, we selected 5 datasets as targets from multiple domains and tasks, including text classification, single-choice questions, and text generation, which are SST-2(Socher et al., 2013), SQuAD2.0(Rajpurkar et al., 2016, 2018), MedMCQA(Pal et al., 2022), RACE(Lai et al., 2017) and IWSLT2017(Cettolo et al., 2017). Since these datasets come from different tasks and have different formats, in order to adapt our method, we unify them into QA format by constructing prompts. Examples of the prompts are accessible in Appendix C. Baseline.(1) SFT only: directly fine-tunes the base model on single downstream task without considering any cross-task interactions or parameter sharing. (2) Avg.(Wortsman et al., 2022): averaging their parameters without any alignment or adjustment. (3) DARE(Yu et al., 2023): flexible strategy to combine with other baselines(Average or Task Vector method) and random dropout parameters. (4) LM-Cocktail(Xiao et al., 2023): merges models by comparing loss on validation set. (5) Task Vector(Ilharco et al., 2022): computes the difference between the base model and each finetuned model to perform add, subtraction, or interpolation to construct new task behaviors. (6) EWC(Kirkpatrick et al., 2016): introduces regularization term based on the Fisher Information Matrix to prevent forgetting. We selected the Llama-2-7B-chati(Touvron et al., 2023) as the base model for fine-tuning and weight merging on 8 NVIDIA V100 GPUs, and LoRA(Hu et al., 2022) is deployed for the fine-tuning pipeline. The implementation details of the fine-tuning and evaluation pipeline are provided in Appendix B. All implementation details are supplied in Appendix A. In experiments of comparing with other baselines, our method always uses the same setting: we select 20 typical samples for each layer by the clustering algorithm, and those samples are concatenated to form the typical dataset. Same as Eq 1, we adopt the RBF kernel function as the similarity, of which the scale factor σ is set to 1.0. Then we segment and calculate weights for each layer of the model to merge them independently(taking Llama2-7b-chat as an example, the model will have 33 different groups of merging weights). 4.2 Performance of RECALL in Different Merging Scenarios Firstly, we fine-tune the base model on the above 5 datasets to obtain five corresponding expert models. We then set up two different scenarios depending on the number of models used in the merging, which will be illustrated in the next two subsections. 4.2.1 Single Fine-tuned Model Merging In this study, we consider the case of merging using single fine-tuned model and its base model. With access to the training datasets for both models, we conduct comprehensive experiments to evaluate the proposed approach across different datasets. Our experiments compare the performance of several baselines using different datasets, and the results are presented in Table 1. We draw the following observations from Table 1: Our method RECALL consistently outperforms all baselines across diverse settings, achieving the highest average performance (45.00) and the best generalization to unseen tasks (38.92, +7.86% over the best baseline). It maintains toptier results across all fine-tuning sources and excels in challenging domains such as MEDMCQA and IWSLT2017-EN-FR, demonstrating both robustness and transferability. These results underscore the effectiveness of leveraging representational similarity for model merging and motivate the extension to more complex multi-source integration scenarios. 4.2.2 Multiple Fine-tuned Models Merging To simulate more complex knowledge fusion setting, we simultaneously merge five task-specific expert models. As shown in Table 2, we consider two configurations: merging with and without the inclusion of the base model. From Table 2, we observe: RECALL achieves the best overall performance in both settings, with or without the base model, reaching averages of 56.93 and 62.83, respectively. Notably, it outperforms all other methods even without relying on the base model, demonstrating strong capability in fusing knowledge from multiple fine-tuned experts. These results highlight the advantage of representation-aware merging over both parameter averaging and task-vector-based baselines, and demonstrate that RECALL is not only effective for single-expert scenarios but also scalable to multiexpert merging, showing robust performance in both knowledge preservation and generalization without requiring access to training data. To more effectively demonstrate the effectiveness of RECALL across multiple models, we conducted supplementary experiments using Qwen27B-Instructii. As shown in Table 3, RECALL also demonstrated strong knowledge fusion ability and the ability to resist catastrophic forgetting in this test. 4.3 Sequential Fine-tuning Scenario To further assess the effectiveness of RECALL in realistic continual learning settings, we conduct sequential fine-tuning experiments across five tasks introduced in fixed order. After training on each new task, the current model is merged with the previously accumulated one using different strategies. We compare RECALL against two baselines: standard LoRA-based fine-tuning (LoRA SFT) and Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2016). Figure 5 illustrates the forward forgetting curves over the task sequence, where the y-axis indicates model performance on the current task immediately ihttps://huggingface.co/meta-llama/Llama-2-7b-chat-hf iihttps://huggingface.co/Qwen/Qwen2-7B-Instruct Fine-tuned on Method SST-2 SQuAD2.0 IWSLT2017-en-fr RACE MedMCQA All Average SFT only Avg DARE+Avg LM-Cocktail RECALL(Our) SFT only Avg DARE+Avg LM-Cocktail RECALL(Our) SFT only Avg DARE+Avg LM-Cocktail RECALL(Our) SFT only Avg DARE+Avg LM-Cocktail RECALL(Our) SFT only Avg DARE+Avg LM-Cocktail RECALL(Our) SFT only Avg DARE+Avg LM-Cocktail RECALL(Our) SST-2 SQuAD2.0 95.76 94.95 95.07 95.76 94.50 86.81 89.11 89.11 79.36 86.19 82.68 89.91 89.33 89.91 89.56 18.23 47.36 29.13 30.39 34.93 9.91 0.11 24.36 17.58 70.32 58.68 64.29 65.40 62.60 75.10 31.68 5.21 8.75 25.54 30.72 85.46 80.92 78.67 84.46 84.87 10.72 4.35 10.45 5.23 10.55 50.64 14.80 50.05 51.24 40.27 6.58 5.97 11.99 12.61 18.58 37.02 22.25 31.98 35.82 37.00 Datasets IWSLT2017-en-fr RACE MedMCQA Avg from all tasks Avg from unseen tasks 13.28 12.32 11.68 11.88 12.08 21.28 18.28 19.59 18.38 18.20 45.33 42.01 41.63 43.13 43.09 19.06 22.58 19.55 23.02 23.12 18.22 15.34 14.04 14.07 13.86 23.43 22.11 21.30 22.10 22.07 44.71 32.75 50.44 32.55 47.44 48.78 31.58 50.52 42.24 49.50 29.39 32.85 44.6 30.08 48.73 85.71 73.47 78.68 82.31 79.31 31.76 23.17 57.46 24.89 43.77 48.07 38.76 56.34 42.40 53.75 32.32 34.28 35.14 34.26 34.90 32.61 34.23 34.69 32.66 34.34 33.21 35.45 35.84 35.07 34.43 39.68 34.66 34.97 37.29 36.96 45.54 43.25 42.86 44.18 44.82 36.67 36.37 36.7 36.69 37.09 43.55 35.90 40.22 40.00 43.93 55.00 50.82 54.52 51.42 54.62 40.27 40.91 44.37 40.68 45.27 42.66 38.57 42.48 44.85 42.92 22.40 17.57 30.14 22.67 38.27 40.77 36.76 42.34 39.92 45.00(+6.28%) 30.50 21.14 26.50 26.06 31.29 47.37 43.3 48.48 43.16 47.06 39.00 40.64 45.06 40.07 45.82 31.90 29.85 33.42 35.49 33.82 16.62 11.15 26.96 17.29 36.63 33.08 29.22 36.08 32.41 38.92(+7.86%) Table 1: Performance of merging the base model(Llama-2-7B-chat) and the model fine-tuned on one specific dataset. We compared our method with 4 baselines and marked the best two results in bold and underlined fonts. The average performance on 5 datasets and 4 datasets(except the fine-tuning dataset) is also labeled in the last two columns. after learning it, and the x-axis denotes the task index. Figure 5: Performance curves on SST-2 during sequential fine-tuning with other two baselines. As illustrated in Figure 5, LoRA SFT suffers from dramatic performance decline on the original SST-2 task as training progresses on new tasks, indicating severe forward forgetting phenomenon. EWC alleviates this to some extent, but still shows noticeable downward trend. In contrast, our proposed RECALL method maintains relatively stable performance throughout the sequential fine-tuning process, with only moderate decline toward the final tasks. This suggests that RECALL is more effective at preserving prior task knowledge compared to the other two baselines. These results confirm that RECALL is well-suited for deployment in dynamic learning environments, offering resilience to forgetting while ensuring consistent learning progress. Detailed per-task results are available in Appendix E."
        },
        {
            "title": "5 Related works",
            "content": "Catastrophic forgetting (CF) is particularly severe in realistic deployment settings, where training data from previous tasks may be inaccessible due to privacy concerns, and task boundaries or identifiers are typically unavailable. To address CF, existing continual learning (CL) approaches can be broadly categorized into two classes: data-based methods and model-based methods. Data-based methods leverage stored or generated samples from earlier tasks (Lopez-Paz and Ranzato, 2017; ReMethod With base model Llama2-7B-chat(base model) Avg. DARE+Avg. LM-Cocktail RECALL(Our) Avg. DARE+Avg. Task Vector DARE+Task Vector RECALL(Our) Without base model SST-2 SQuAD2.0 87.96 86.47 86.35 51.38 85.44 91.28 89.6 11.82 16.86 89. 0.94 54.85 63.9 66.74 78.4 67.85 68.01 29 29.34 77.66 Datasets IWSLT2017-en-fr RACE MedMCQA 9.64 27.25 34.24 29.31 28.26 35.87 36.85 9.98 11.11 33.12 50.14 58.65 61.63 68.89 57.9 66.94 69.08 49.64 50.34 74.39 35.91 35.84 36.82 36.07 34.66 37.2 40.96 7.36 9.25 39. Average 36.918 52.612 56.588 50.478 56.932 59.828 60.9 21.56 23.38 62.828 Table 2: Performance of merging multiple models. With base model: Merging the five fine-tuned models and the base model(Llama-2-7B-chat). Without base model: Merging the five fine-tuned models. We compared our method with several baselines and marked the best two results in bold and underlined fonts. Fine-tuned on Without SFT SST-2 SQuAD2.0 IWSLT2017-en-fr RACE MedMCQA RECALL_6merges SST-2 SQuAD2. IWSLT2017-en-fr RACE MedMCQA"
        },
        {
            "title": "Datasets",
            "content": "93 96.79 92.55 92.32 28.9 0.57 94.15 37.02 35 98.51 31.49 25.89 13.43 81.45 41.85 41.43 42.33 52.62 35.28 35.5 45.03 87.66 76.92 50.63 86.34 99.15 88.71 91.93 52.45 46.78 24.38 50.9 54.35 97.7 59."
        },
        {
            "title": "Average",
            "content": "62.396 59.384 61.68 62.734 48.714 47.182 74.34 Table 3: Supplementary Experiments: Performance of Qwen2-7B-Instruct and models fine-tuned on one specific dataset, compared with the model merged with the above six models using RECALL. The best result is marked in bold font. buffi et al., 2016), while model-based methods impose constraints on parameter updates or isolate task-specific modules (Kirkpatrick et al., 2016; Fernando et al., 2017). Some recent work adapts these paradigms to LLMs using parameter-efficient tuning modules (Wei et al., 2025; Tian et al., 2024). 5.1 Model Merging Model merging has emerged as an alternative to traditional CL methods, enabling knowledge integration without access to historical training data. Most methods perform parameter-level fusion, typically via uniform averaging, without accounting for layer-wise functional differences. Task Arithmetic (Ilharco et al., 2022) and ModelSoup (Wortsman et al., 2022) showed that simple weight averaging can yield multi-task models. Fisher Merging (Matena and Raffel, 2022) incorporates importance weights based on Fisher information to preserve task-relevant parameters. RegMean (Jin et al., 2023) formulates merging as regression problem over model outputs, aligning them via low-rank projection. Other works attempt to mitigate interference through more selective merging. TIESMerging (Yadav et al., 2023) trims parameter deltas and aligns signs, while DARE (Yu et al., 2023) sparsifies task-specific shifts to preserve key differences. LM-Cocktail (Xiao et al., 2023) and LLMBlender (Jiang et al., 2023) perform weighted merging or output blending using learned domain signals or generation-based rankers. 5.2 Probing Representations Probing techniques analyze how LLMs internally organize linguistic and task knowledge. Prior work has shown that lower layers tend to encode syntactic information, while upper layers capture semantics and abstract features (Tenney et al., 2019; Starace et al., 2023). Starace et al. (2023) demonstrate that linguistic features are unevenly distributed across layers and can shift during adaptation. Tighidet et al. (2024) find that past knowledge may remain latent but inaccessible, while Kotha et al. (2023) show that representation-level forgetting is limited, with performance loss arising from usage changes rather than loss of internal content. These findings highlight the importance of analyzing internal representations when studying model behavior under adaptation and support representation-driven approaches to knowledge retention and integration."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we first conduct exploratory experiments to explore the phenomenon that data representations drift between layers and models, and relate this phenomenon to knowledge differences and catastrophic forgetting of models. Based on these findings, we propose method to achieve knowledge fusion and resist catastrophic forgetting by aligning the representations of different layers of the model, called RECALL. RECALL does not require past data and only requires hierarchical model aggregation by exploiting the similarity of model representations to achieve the goal effectively. We verify the effectiveness of the method in multiple scenarios, and analyze the details of the method in more depth through ablation experiments and other tests."
        },
        {
            "title": "7 Limitations",
            "content": "While RECALL provides an effective and datafree solution to continual learning in large language models, several limitations remain. First, our method assumes access to multiple fine-tuned models on related tasks, which may not always be available in real-world deployment scenarios. Second, the current implementation relies on clustering and similarity computations over small set of representative samples; while efficient, the selection quality of these typical samples can influence the final merging outcome. Moreover, RECALL is tailored to models with identical architectures and aligned tokenizersextending to heterogeneous model families or multilingual settings poses additional challenges. Finally, although we empirically validate RECALL across diverse NLP tasks, further investigation is needed on scaling to dozens of tasks or integrating with training-time regularization techniques for tighter lifelong learning integration."
        },
        {
            "title": "Acknowledgments",
            "content": "Science and Technology Program under Grant RCJC20231211085918010. We would like to thank our colleagues and collaborators for their valuable feedback and insightful discussions throughout the course of this work. We are also grateful to the open-source community for providing access to pretrained language models and toolkits that significantly accelerated our research."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. 2017. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pages 214, Tokyo, Japan. International Workshop on Spoken Language Translation. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115. Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Diego Doimo, Alessandro Serra, Alessio Ansuini, and Alberto Cazzaniga. 2024. The representation landscape of few-shot learning and fine-tuning in large language models. ArXiv, abs/2409.03662. Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei Rusu, Alexander Pritzel, and Daan Wierstra. 2017. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734. This work was supported in part by the Major Key Project of PCL under Grant PCL2024A06 and PCL2025AS10, and in part by the Shenzhen Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang, and Diyi Yang. 2021. Continual learning for text classification with information disentanglement based regularization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089. David Isele and Akansel Cosgun. 2018. Selective experience replay for lifelong learning. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561. Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. 2023. Dataless knowledge fusion by merging weights of language models. In The Eleventh International Conference on Learning Representations. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, and Agnieszka Grabska-Barwinska. 2016. Overcoming catastrophic forgetting in neural networks. Proc Natl Acad Sci A, 114(13):35213526. Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. 2019. Similarity of neural netIn Proceedings of work representations revisited. the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 35193529. PMLR. Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. 2023. Understanding catastrophic forgetting in language models via implicit inference. arXiv preprint arXiv:2309.10105. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785 794, Copenhagen, Denmark. Association for Computational Linguistics. Zhizhong Li and Derek Hoiem. 2018. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):29352947. David Lopez-Paz and Marc' Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Michael Matena and Colin Raffel. 2022. Merging models with fisher-weighted averaging. In Advances in Neural Information Processing Systems, volume 35, pages 1770317716. Curran Associates, Inc. Michael Mccloskey and Neal J. Cohen. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of Learning and Motivation, 24:109165. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: large-scale multisubject multi-choice dataset for medical domain question answering. In Proceedings of the Conference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Research, pages 248260. PMLR. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia. Association for Computational Linguistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Sylvestre Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph Lampert. 2016. icarl: Incremental classifier and representation learning. IEEE Computer Society. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Alexey Romanov, Anna Rumshisky, Anna Rogers, and David Donahue. 2019. Adversarial decomposition of text representation. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, vol. 2. long and short papers: Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2019), 2-7 June 2019, Minneapolis, Minnesota, USA. Andrei Rusu, Neil Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks. arXiv preprint arXiv:1606.04671. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 16311642, Seattle, Washington, USA. Association for Computational Linguistics. Giulio Starace, Konstantinos Papakostas, Rochelle Choenni, Apostolos Panagiotopoulos, Matteo Rosati, Alina Leidinger, and Ekaterina Shutova. 2023. Probing llms for joint encoding of linguistic categories. arXiv preprint arXiv:2310.18696. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert In Annual rediscovers the classical nlp pipeline. Meeting of the Association for Computational Linguistics. Chunlin Tian, Zhan Shi, Zhijiang Guo, Li Li, and Chengzhong Xu. 2024. Hydralora: An asymmetric lora architecture for efficient fine-tuning. arXiv preprint arXiv:2404.19245. Zineddine Tighidet, Andrea Mogini, Jiali Mei, Benjamin Piwowarski, and Patrick Gallinari. 2024. Probing language models on their knowledge source. arXiv preprint arXiv:2410.05817. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652. Xiwen Wei, Guihong Li, and Radu Marculescu. 2025. Online-lora: Task-free online continual learning via low rank adaptation. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 66346645. IEEE. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. 2022. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pages 2396523998. PMLR. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2023. C-Pack: Packed Resources For General Chinese Embeddings. arXiv e-prints, arXiv:2309.07597. Shitao Xiao, Zheng Liu, Peitian Zhang, and Xingrun Xing. 2023. Lm-cocktail: Resilient tuning of language models via model merging. arXiv preprint arXiv:2311.13534. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. TIES-merging: Resolving interference when merging models. In Thirtyseventh Conference on Neural Information Processing Systems. Le Yu, Yu Bowen, Haiyang Yu, Fei Huang, and Yongbin Li. 2023. Language models are super mario: Absorbing abilities from homologous models as free lunch. ArXiv, abs/2311.03099. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Datasets and Fine-Tuning Settings Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing Huang. 2023. Orthogonal subspace learning for language model continual learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1065810671, Singapore. Association for Computational Linguistics. We fine-tune the LLaMA-2-7B model on 5 different datasets from diverse domains and tasks, including sentiment classification, question answering, medical QA, reading comprehension, and machine translation. Detailed statistics and supervised finetuning (SFT) hyperparameters are presented below. Dataset # train # test Metric SST-2 SQuAD2.0 MedMCQA RACE IWSLT2017-en-fr 60,000 130,000 100,000 80,000 100,000 872 11,873 4,183 4,934 8,597 Accuracy Exact Match Accuracy Accuracy Exact Match Table 4: Statistics for the datasets used to fine-tune LLaMA-2-7B. A.1 Dataset Statistics and Prompt Format Dataset Descriptions. SST-2 (Socher et al., 2013): Binary sentiment classification dataset with movie reviews labeled as positive or negative. SQuAD2.0 (Rajpurkar et al., 2016, 2018): Reading comprehension dataset with both answerable and unanswerable questions. MedMCQA (Pal et al., 2022): Multiplechoice QA dataset from Indian medical entrance exams. RACE (Lai et al., 2017): Reading comprehension dataset from English exams for Chinese middle and high school students. IWSLT2017-en-fr (Cettolo et al., 2017): English-to-French translation dataset from TED talks. A.2 Fine-Tuning Hyperparameters We fine-tune five task-specific models based on LLaMA-2-7B using LoRA (Hu et al., 2022) on 8 NVIDIA V100 GPUs. Each model is trained with distinct hyperparameters tailored to its dataset. The LoRA config is reported as r/α/dropout ((see Table 5 for details)). The LLaMA-2-7B-chat (Touvron et al., 2023) is used as the base model. We intentionally chose diverse datasets to simulate challenging setup for continual learning and knowledge fusion."
        },
        {
            "title": "B Experimental Framework",
            "content": "We adopt llama-factory (Zheng et al., 2024) for instruction tuning. It supports various parameterefficient fine-tuning methods such as LoRA (Hu et al., 2022) and QLoRA (Dettmers et al., 2023), enabling flexible configuration and easy adaptation to various data formats. is Model evaluated performance using OpenCompass (Contributors, 2023), which integrates broad range of standardized benchmarks to ensure consistency and reproducibility. For inference, we deploy models with efficient vLLM (Kwon et al., 2023), providing high throughput and low latency."
        },
        {
            "title": "Details",
            "content": "To illustrate the data used in our experiments, we present two sets of representative samples. Table 6 shows instruction samples used during supervised fine-tuning (SFT)."
        },
        {
            "title": "D Supplementary Similarity Curves",
            "content": "Figure 6: Cosine similarity between adjacent hidden layers within single LLM. The similarity drops in both early and late layers, suggesting non-uniform transformation of representations across the network. Figure 7: Cosine similarity between representations at the same layer across two LLMs fine-tuned on different tasks. Similarity remains high in early layers but decreases in deeper layers, indicating increasing taskspecific divergence. As shown in Figure 6, 7, representational similarity varies across layers and tasks. Models trained on similar tasks (e.g., SST-2 and RACE) show higher alignment in middle and upper layers, while Dataset LoRA (r/α/dropout) Max Len LR Batch Epochs Deepspeed SST-2 SQuAD2.0 MedMCQA RACE IWSLT2017-en-fr 8 / 32 / 0.1 8 / 32 / 0.1 8 / 32 / 0.1 8 / 32 / 0.1 8 / 32 / 0.1 2048 2048 2048 2048 2048 5e-5 5e-5 5e-5 5e-5 5e-5 64 32 64 128 3 4 3 5 5 ZeRO-3 ZeRO-3 ZeRO-3 ZeRO-3 ZeRO-3 Table 5: SFT hyperparameters for each dataset. Task SST-2 SQuAD2. Example Instruction: Statement: the characters in swimfan seem motivated by nothing short of dull, brain-deadening hangover. Whats sentiment should the above sentence be? OPTIONS:- negative.- positive. Answer: Output: negative Instruction: Unpopulated boards are usually bare-board tested for ... the appropriate contact points and only on these. According to the above passage, answer the following question. If it is impossible to answer according to the passage, answer impossible to answer: Question:Whats an absent connection that needs to be linked up on an unpopulated board called? Output: An open MedMCQA Instruction: Question: Which of the following metabolic reactions require vitamin B12 but not folate? Options: A: Conversion of malonic acid to succinic acid B: Conversion of homocysteine to methionine C: Conversion of serine to glycine D: Thymidylate synthesis Choose an correct answer from A/B/C/D.Answer: Output: RACE Instruction: Read the article, and answer the question by replying A, B, or D. Article: Tired of all the pushing in supermarkets? Angry at wasting ... claim it to be. Q:The author agrees with the fact that ... Output: Table 6: Instruction samples used for supervised fine-tuning. those from different domains (e.g., MedMCQA IWSLT) diverge significantly, especially in vs. deeper layers. These patterns are consistent with our main findings and further support the use of representation-aware merging strategies. Sequential Fine-Tuning Results To provide strong baseline for comparison, we conduct sequential fine-tuning (SeqFT) experiments, where single model is trained on multiple datasets in fixed order without revisiting previous ones. This setting simulates continual learning scenario and serves to quantify the extent of catastrophic forgetting. We sequentially fine-tune the LLaMA-2-7B model across five diverse tasks, including sentiment classification, question answering, medical QA, reading comprehension, and machine translation. All models are trained under the same LoRA configuration for consistency. After completing each step in the sequence, we evaluate the model on all previously seen datasets to track performance drop. As shown in Table 7, performance on earlier tasks gradually deteriorates as the model is updated on subsequent ones. The trend clearly reflects catastrophic forgetting and reinforces the need for continual learning strategies such as our proposed representation-aware model merging, which avoids overwriting previous knowledge by aligning and preserving internal representations."
        },
        {
            "title": "F Comparison of Similarity Metrics",
            "content": "To determine the most effective similarity metric for guiding our representation-aware model merging, we conduct comparative study across five widely-used similarity measures. These metrics are used to compute the alignment between hidden representations of models, which in turn inform the layer-wise merging weights. The five similarity metrics evaluated are: Cosine similarity: x, are vectors. Sim = xT x2 y2 (7) Method LoRA SFT EWC RECALL(Our) Task Sequence Datasets SST-2 SQuAD2.0 MedMCQA IWSLT2017 RACE 44.71 32.32 95.76 SST-2 Task 1 58.15 16.88 94.38 Task 2 SQuAD2.0 68.38 42.62 88.3 Task 3 MedMCQA 58.73 42.39 76.38 IWSLT2017 Task 4 86.24 39.8 14.79 RACE Task 5 44.71 32.32 95.76 SST-2 Task 1 51.64 25.77 Task 2 94.27 SQuAD2.0 57.75 42.53 Task 3 MedMCQA 90.47 55.6 41.05 81.59 IWSLT2017 Task 4 87.34 39.68 67.42 RACE Task 5 47.44 34.9 Task 1 94.5 SST-2 57.52 30.79 96.61 SQuAD2.0 Task 2 69.06 40.65 Task 3 MedMCQA 92.89 67.55 38.16 86.31 IWSLT2017 Task 4 88.97 36.22 80.59 RACE Task 13.28 25.06 19.29 45.29 34.85 13.28 20.94 12.44 47.86 33.54 12.08 19.83 18.48 45.73 43.14 31.68 87.42 74.89 75.71 68.05 31.68 88.32 72.12 65.31 64.81 30.72 86.34 71.66 67.09 62.38 Average 43.55 56.378 58.696 59.7 48.746 43.55 56.188 55.062 58.282 58.558 43.928 58.218 58.548 60.968 62.26 Table 7: Detailed performance of sequence training scenario. Euclidean distance (converted to similarity): x, are vectors."
        },
        {
            "title": "G RECALL Algorithm Details",
            "content": "Sim = y2 max ,Y Y2 The Analysis of Runtime, Memory Usage and (8) Scalability Centered Kernel Alignment (CKA) (Kornblith et al., 2019): X, are two distributions. CKA(X, ) = Y 2 XF Y (9) Maximum Mean Discrepancy (MMD): X, are two distributions. MMD2(X, ) = 1 n2 + 1 m2 2 nm (cid:88) (cid:88) i=1 j=1 (cid:88) (cid:88) i=1 j=1 (cid:88) (cid:88) i= j=1 k(xi, xj) k(yi, yj) (10) k(xi, yj) RBF kernel: See Eq 1. For each metric, we compute layer-wise alignment scores between expert models, normalize the weights, and perform hierarchical model merging using the same fusion strategy. The final merged models are evaluated on multiple tasks to assess performance consistency. As shown in Table 8, RBF Kernel yields the highest performance across all evaluation datasets. While CKA and dot product also perform competitively, metrics like Euclidean distance and MMD are less stable. These results support our choice of RBF Kernel as the default alignment metric in our model merging framework. Compared with other model merging methods, our method mainly adds the following steps: (1) Feature extraction; (2) Extracting typical samples; (3) Extracting the representation of typical samples in all models; (4) Similarity calculation; (5) Hierarchical merging. Mathematical notation convention: t: number of iterations; k: number of clusters; n: number of samples; E: dimension of features/hidden layers; l: number of model layers; b: mini-batch size; s: number of GPUs; m: number of typical samples; : number of models to merge. Feature extraction: Feature extraction forwards all samples and saves all hidden layer states. In order to save memory and speed up, we use distributed inference and pass features back to rank0 on each batch, where they are offloaded to CPU memory to reduce GPU memory usage. So the GPU memory complexity is O(bEl), and the CPU memory and subsequent storage complexity is O(nEl). And the time complexity is: O(αl + γnEl), where the first term is the time required to forward samples across GPUs in parallel, The second term is the time it takes for sub nodes to send batch of features to rank 0, the third term is the time it takes to offload all the features from GPU to CPU, and α, β, γare scaling constants. If this distributed offloadbs + βnEl Metric SST-2 SQuAD2.0 IWSLT2017-en-fr RACE MedMCQA Avg. Cosine Euclidean CKA MMD RBF 83.83 88.65 83.94 65.83 89.11 67.99 26.72 68.04 28.93 77.66 33.24 43.64 33.25 41.26 33.12 65.2 38.93 65.16 50.87 74.39 37.03 34.71 36.91 36.58 39.86 57.458 46.53 57.46 44.694 62. Table 8: Performance of merged multiple models(without base model) using different similarity metrics. RBF Kernel similarity consistently achieves the best average performance across tasks. Algorithm 1 RECALL Require: Task dataset DN , source model M0, fine-tuned models M1, M2, . . . , MN 1 with parameters θq (q [0, 1]) Ensure: Merged model parameters θ 1: MN Fine-tune M0 on DN 2: Dtype Kmeans(RN ), which are representations extracted from DN using MN 3: for each expert model Mj, [1, 1] do 4: Rj Extract representations from Dtype using Mj 5: end for 6: for each layer [1, L] do Compute similarity Sp,q 7: Mp and Mq for layer i between models Sp,q = 1 Dtype (cid:88) Dtype RBF(Ri p, Ri q) 8: Normalize similarities to obtain merging weights: wq = ) exp(SN,q q=0 exp(SN,q (cid:80)N ) 9: Merge model parameters at layer i: θ = (cid:88) q=0 θq wq 10: end for 11: return θ ing strategy isnt adopted, the time complexity and space complexity will greatly increase: Space complexity: O(nEl); Time complexity: O(αnl). and time complexity of the one-pass Kmeans algorithm are O(E(n + k))and O(tknE), respectively. The overall complexity is: Space complexity: O(E(n + k)l)(on CPU); Time complexity: O(tknEl). Extracting the representation of typical samples in all models: We use typical samples to perform forward inference on the models to be merged, and save the hidden layer information of all layers. We adopt the same strategy as step (1), so the space complexity and time complexity are: Space complexity: O(bElN )(b will be smaller than because typical sample set generally does not occupy all GPU memory); Time complexity: O((αl bs + βmEl s1 + γmEl)N ). Similarity calculation: We compute the similarity between the representations of the main model and all other models at each layer, using rbf kernel as the similarity metric, and averaging the similarity across all typical samples, so: Space complexity: O(mlN ); Time Complexity: O(mlEN ). Obviously, when m, utilizing typical samples will greatly reduce the memory and time consumption of extracting features and calculating similarity (step (3/4)), which is one of the important reasons for our choice of sampling. Hierarchical merging: Compared with other model merging algorithms, we increase the number of weights in model merging (each model has an independent floating-point weight in each layer), but still take single parameter as the unit for merging. So: Space complexity: O(lN ); Time complexity: O(N ). Extracting typical samples: We perform Kmeans clustering for the features of each layer in step (1), and the space complexity Scalability: When the scale of the model increases, time and space consumption will grow at linear rate."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "Peng Cheng Laboratory",
        "School of Biomedical Engineering, Tsinghua University",
        "Shenzhen International Graduate School, Tsinghua University",
        "The Hong Kong University of Science and Technology, Guangzhou",
        "Xiamen University"
    ]
}