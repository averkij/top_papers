{
    "paper_title": "FactAlign: Long-form Factuality Alignment of Large Language Models",
    "authors": [
        "Chao-Wei Huang",
        "Yun-Nung Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign"
        },
        {
            "title": "Start",
            "content": "FACTALIGN: Long-form Factuality Alignment of Large Language Models Chao-Wei Huang Yun-Nung Chen National Taiwan University, Taipei, Taiwan f07922069@csie.ntu.edu.tw y.v.chen@ieee.org 4 2 0 2 ] . [ 1 1 9 6 1 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FACTALIGN, novel alignment framework designed to enhance the factuality of LLMs long-form responses while maintaining their helpfulness. We introduce fKTO, fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FACTALIGN utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FACTALIGN significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FACTALIGN is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score."
        },
        {
            "title": "Introduction",
            "content": "Generating natural language provides natural interface for humans to communicate with artificial intelligence. With the emergence of large language models (LLM) (Brown et al., 2020), they immediately demonstrate the potential to become the next-generation engine for information access due to their ability to generate long-form natural language response to human queries. Given the largescale pre-training on web-scale datasets, LLMs demonstrate impressive capabilities of answering diverse questions, showcasing the vast amount of 1Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign. Figure 1: An example of the evaluation long-form factuality. The long-form response is broken down into subclaims and verified separately. The factual precision score can be calculated as the precision of all subclaims. knowledge they possess. The post training techniques, i.e., instruction tuning (Wei et al., 2022) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), further train LLMs to respond in more human preferable way, e.g., generating coherent and detailed responses. Despite their impressive reasoning capabilities and wide-range knowledge, research has shown that LLMs still struggle with hallucination (Xu et al., 2024b; Rawte et al., 2023) and generating non-factual content (Min et al., 2023). An example of long-form generation and factuality assessment is illustrated in Figure 1. These issues hinder the reliability of LLMs and make it hard to be adopted to real-world settings where factual accuracy is crucial requirement for most applications. The long-form responses make these issues more complex as it is non-trivial to quantify the level of long-form factuality (Wei et al., 2024), let alone to improve it. Meanwhile, most research focuses on improving the helpfulness of LLM chatbots and their reasoning capabilities, with little emphasis on the factuality of the responses. In this paper, we aim to improve the reliability of LLMs by enhancing the factuality of their long-form responses. Recent advances of automatic factuality evaluators show that they are capable of providing factuality assessment at the atomic fact level (Min et al., 2023; Wei et al., 2024). To leverage those fine-grained factuality assessments, we propose FACTALIGN, an alignment framework designed to improve LLMs long-form factuality while maintaining the same level of helpfulness. We introduce fine-grained alignment algorithm, fKTO, which extends the Kahneman-Tversky Optimization (KTO; Ethayarajh et al. (2024)) alignment algorithm to sentence-level. We conduct experiments on both open-domain prompts and information-seeking questions and demonstrated that our proposed FACTALIGN can effectively improve long-form factuality of LMs while maintaining their helpfulness."
        },
        {
            "title": "Our main contributions can be summarized as",
            "content": "the following: We introduce fKTO, sentence-level alignment algorithm that can leverage fine-grained signals provided by long-form factuality evaluator. We propose FACTALIGN, framework to align LMs with fine-grained signals to generate responses that are more factual, while keeping their helpfulness. The effectiveness of the proposed components are validated through detailed analyses."
        },
        {
            "title": "2.1 Language Model Alignment",
            "content": "Alignment, i.e., aligning language models to human values, has been very popular research field recently. Prior work such as InstructGPT (Ouyang et al., 2022) and LLaMA-2 (Touvron et al., 2023) showcased that RLHF (Bai et al., 2022a) enhances models ability to follow instructions significantly. Fine-grained RLHF (Wu et al., 2024) proposed to leverage fine-grained rewards for better alignment. Constituional AI (Bai et al., 2022b) and RLAIF (Lee et al., 2023) introduced AI feedback to eliminate the requirement of human annotation. Another line of research focused on alignment without RL. DPO (Rafailov et al., 2023) derived simple objective for alignment, thus attracting rapid adoption. KTO (Ethayarajh et al., 2024) eliminated the requirement of pairwise preference data. Our proposed alignment algorithm, fKTO, extends KTO to sentence-level, which can leverage the finegrained signals provided by long-form factuality evaluator. 2.2 Factuality of Langage Models Factuality and hallucination have been longstanding issues for natural language generation (Lee et al., 2022; Ji et al., 2023). Lee et al. (2022), Li et al. (2023), and Chuang et al. (2024) proposed decoding techniques that improved factuality of LMs. Shuster et al. (2021) reduced hallucination by retrieval-augmented generation. (Dhuliawala et al., 2023) proposed chain-of-verification to reduce LLM hallucination. SelfCheckGPT (Manakul et al., 2023) proposed method to selfcheck factuality by sampling multiple generations. FactScore (Min et al., 2023; Chiang and Lee, 2024) and LongFact (Wei et al., 2024) both introduced frameworks for evaluating factuality of long-form generations. FAVA (Mishra et al., 2024) introduced fine-grained hallucination categories to evaluate the models and provided detailed view of the hallucination issues of LLMs. Our proposed method also utilize long-form factuality evaluator, while focusing on leveraging the provided factuality assessments for better factuality alignment. Prior work has also worked on training LMs to be more factual. FactTune (Tian et al., 2024) leveraged FactScore to construct preference pairs and demonstrated improvement on the bio generation task. FLAME (Lin et al., 2024) introduced factuality-aware alignment which combines FactTune with open-domain prompts. KnowTuning (Lyu et al., 2024) proposed knowledge augmentation which constructs synthetic pairs for DPO training. On the other hand, recent work has shown that fine-tuning LMs on new knowledge might encourage hallucinations (Gekhman et al., 2024; Kang et al., 2024). Our work additionally proposes fKTO for fine-grained factuality alignment, which achieves superior performance."
        },
        {
            "title": "3 Preliminaries",
            "content": "In this paper, we aim to improve the long-form factuality of LLMs by factuality alignment. In this section, we introduce an overview of the task of long-form factuality and alignment algorithms. 3.1 Long-form Factuality LLMs excel at generating long-form responses with detailed description and explanation. However, evaluating the factuality of long-form generations is non-trivial. In this paper, we define the factuality score of long-form response as an aggregation of the factuality score of each individual atomic fact, following FactScore (Min et al., 2023) and LongFact (Wei et al., 2024). More formally, given knowledge corpus C, an user prompt and the response = M(x) generated by model M, we first decompose into atomic statements = {a1, , aA}. For each atomic statement ai, its factuality score (ai) is defined as whether it is supported by the knowledge in C, i.e., (ai) = 1[ai is supported by C]. Then, the factuality score of the long-form response can be defined as fA(y) = A({f (a1), , (aA)}), where is an aggregation function that can be defined in various ways. In this paper, we adopt two metrics for longform factuality: factual precision as defined in FactScore (Min et al., 2023) and factual f1 score as defined in LongFact (Wei et al., 2024). Factual precision measures the overall precision of the atomic statements: fprec(y) = (cid:80)S i=1 (ai) . While factual precision is simple, it could be easily exploited. model could obtain very high factual precision score by only generating one statement that has the highest confidence. On the other hand, factual f1 assumes that certain amount of information is desired by the user and additionally considers the factual recall: ff 1@K(y) = (cid:40) 2fprec(y)frec@K (y) fprec(y)+frec@K (y) 0 if > 0 if = 0, where frec@K(y) = min(1.0, ) is the factual recall score assuming that at least statements are desired by the user. Factual f1 is less exploitable than factual precision as it punishes the model when it only generates few statements."
        },
        {
            "title": "3.2 Kahneman-Tversky Optimization",
            "content": "Training LLMs that are aligned to human values typically involves three stages: 1) pre-training, 2) supervised fine-tuning, and 3) reinforcement learning from human feedback (RLHF). The first two stages maximize the sequence generation likelihood of the LM given dataset of either diverse pre-training data or human-annotated instructionfollowing data. The third stage, RLHF, aims to maximize the expected reward of LM generations, where the reward usually is defined as human preferences (Ouyang et al., 2022). As result, the RLHF stage enables LMs to generate responses that are more preferable by humans, which is vital for creating intelligent assistants. While the success of the RLHF framework is eminent, its adoption is hindered by the complexity of the framework, the unstability of the training process, and the increased training time due to the requirement of online sample generation. To this end, prior work has proposed alignment algorithms that do not require RL, thus attracting mass adoption. Direct Preference Optimization (DPO; Rafailov et al. (2023)) derives simpler objective from the RLHF, eliminating the requirement of reward model and the RL optimization process. More recently, Ethayarajh et al. (2024) introduced Kahneman-Tversky Optimization (KTO), which derives family of human-aware alignment loss functions. The objective function of KTO is even simpler than DPO. It only requires binary label for each prompt-response pair (x, y), as opposed to DPO which requires pairwise preference labels for each triplet (x, y1, y2). This relaxed data requirement enables us to extend the algorithm to sentencelevel, which we will discuss in Section 4.2. More formally, the KTO loss is defined as: LKTO = 1 (cid:88) x,yB (λy v(x, y)), where denotes the minibatch, λy denotes the weight of the chosen and rejected samples, and v(x, y) = (cid:40) λcσ(β(rθ(x, y) z0)) if c(x, y) = 1, λrσ(β(z0 rθ(x, y))) if c(x, y) = 0, z0 = EyD[KL(πθ(y x)πref(y x))], rθ(x, y) = log πθ(x, y) πref(x, y) , where c(x, y) denotes the preference function, i.e., c(x, y) = 1 if the response is chosen. Ethayarajh et al. (2024) demonstrated that KTO achieves on par or better alignment performance compared to DPO. KTO also works well under the scenario where the number of chosen and rejected samples are significantly unbalanced, e.g., 1:9."
        },
        {
            "title": "4 FACTALIGN: Aligning Language\nModels for Long-form Factuality",
            "content": "In this section, we introduce our proposed framework FACTALIGN. An overview of our framework is illustrated in Figure 2. 4.1 Automatic Long-form Factuality Evaluator Obtaining fine-grained factuality annotations for long-form responses by human annotation is very costly. For example, Min et al. (2023) estimated that evaluating one generation costs $4. In this work, we employ an automatic factuality evaluator for long-form responses. The factuality evaluator, following the design of FactScore (Min et al., 2023) and SAFE (Wei et al., 2024), is workflow of 4 stages: 1) atomic statement decomposition, 2) query generation, 3) relevant knowledge search, and 4) final factuality assessment. Note that stage 2 and 3 can be run multiple times to enrich the searched knowledge. Atomic Statement Decomposition The reinto sentences = sponse is first split {s1, , sS}, and each sentence is decomposed into atomic facts A. We add an additional step to revise the decomposed atomic statements into selfcontained statements with GPT-3.5-TURBO following SAFE. Query Generation We prompt GPT-3.5TURBO to generate search query given the revised statement and possibly the previously generated queries and found knowledge snippets. Relevant Knowledge Search We employ Wikipedia as the knowledge corpus following FactScore. While the coverage of Wikipedia is more limited compared to commercial search engines like Google Search, we opt for Wikipedia as this reduces cost and allows us to fully manage the knowledge search component under controlled setting. We perform search with the generated query and obtain the top-k most relevant knowledge snippets. Final Factuality Assessment We prompt GPT3.5-TURBO to provide the final factuality assessment of revised statement i, which is either Supported if the statement is supported by the knowledge snippets, or Not Supported otherwise. The statement-level score is then defined as (ai) = 1[ai is Supported]. Note that (ai) represents whether the statement is supported with respect to Wikipedia, not whether it is globally true. 4.2 Long-form Factuality Alignment At the core of the FACTALIGN framework is the alignment algorithm, which operates on two granularities: response-level and sentence-level. 4.2.1 Response-level Alignment We employ the standard KTO loss LKTO for response-level alignment. The preference labels c(x, y) in the KTO loss can be defined and obtained in various ways. For instance, most prior work utilized human-annotated preference labels or pseudo labels provided by LLMs. In order to align for factuality, we treat response as chosen sample if the factual f1 score of the response is greater than threshold t: c(x, y) = 1[ff 1@K(y) > t]. By minimizing the response-level loss, we align the LMs to generate responses that have higher factual f1 scores. In addition to the data for factuality alignment, the response-level loss is compatible to other forms of preference data. For example, in order to make the model more helpful, we can include diverse preference datasets that are based on human preferences. In practice, we include general-domain alignment datasets during training to make sure the model is aligned to diverse human values."
        },
        {
            "title": "4.2.2 Sentence-level Alignment\nSince our factuality evaluator provides assessments\nat a finer granularity, we propose a fine-grained\nalignment algorithm, fKTO, to leverage these sig-\nnals by extending the KTO alignment algorithm to\nsentence-level. The fKTO loss is defined as",
            "content": "LfKTO = 1 (cid:88) x,yB 1 S (cid:88) i=1 (λf v(x s<i, si)), where s<i denotes the concatenation of and s<i which denotes sentences before si. In this objective function, sentence si is treated as the completion given s<i. sentence is chosen if the average precision of its atomic statements is higher than threshold ts. c(x s<i, si) = 1 (cid:34) (cid:80)Asi j=1 (aj) Asi (cid:35) > ts , Figure 2: An overview of our FACTALIGN framework. Top: the pipeline for long-form facutality assessment. Bottom: the long-form factuality alignment process. where Asi = {aj aj si} denotes the atomic statements in sentence si. The sentence-level loss provides training signals at finer-grained level, thus enabling the model to be aligned more effectively. Note that the relaxed data requirement enables KTO to be easily extended to the sentencelevel, as opposed to algorithms that require pairwise preference labels, e.g., DPO. Finally, the loss function we optimize is the combination of the response-level and sentence-level losses: = LKTO + λ LfKTO, where λ is the weight of the sentence-level loss. 4."
        },
        {
            "title": "Iterative Optimization",
            "content": "With the alignment algorithms introduced above, we can align LMs to be more factual and more helpful. However, the responses and factuality assessments are obtained in an offline fashion, i.e., we sample the responses and their factuality labels before training the model and use this data throughout training. This creates discrepancy between the assessed responses and the model being trained, which would hinder the alignment process due to distributional shift. Hence, we employ an iterative optimization procedure, where we periodically sample new responses with the trained model and assess their factuality. The newly generated responses are then included in the training dataset for the next iteration."
        },
        {
            "title": "5 Experimental Stetup",
            "content": "We conduct experiments to validate the effectiveness of our proposed framework FACTALIGN. Furthermore, we perform analyses to discuss the effectiveness of each component in the framework."
        },
        {
            "title": "5.1 Datasets",
            "content": "Supervised Fine-tuning (SFT) We employ the Deita dataset (Liu et al., 2024) for supervised fine-tuning before performing alignment to ensure basic instruction-following capabilities of the model. The Deita dataset consists of high-quality data selected from UltraChat (Ding et al., 2023), ShareGPT2, and WizardLM (Xu et al., 2024a). General-domain Alignment We follow the Zephyr recipe (Tunstall et al., 2023) and employ the UltraFeedback dataset (Cui et al., 2023) as the general-domain alignment dataset. The UltraFeedback consists of prompts across multiple domains and completions generated from multiple LLMs to enrich diversity. We use the binarized version of the dataset3 and decouple the pairs for the KTO loss. Factuality Alignment We generate informationseeking prompts following the data creation procedure from LongFact (Wei et al., 2024). LongFact 2https://sharegpt.com 3https://huggingface.co/datasets/ HuggingFaceH4/ultrafeedback_binarized consists of 38 topics chosen to ensure diverse coverage. For each topic, we generate 30 prompts with GPT-4-TURBO and sample generations with our policy model. The generations are then assessed by the long-form factuality evaluator and labeled with factuality assessments at an atomic statement level. For each iteration of iterative optimization, we generate new set of prompts and sample generations with the currently aligned model. 5.2 Long-form Factuality Evaluator We employ gpt-3.5-turbo to perform atomic statement decomposition, query generation, and final factuality assessment. The generation temperature is set to 0.1. We use the preprocessed Wikipedia corpus from the Dec. 20, 2021 dump released by Izacard et al. (2024) as our knowledge corpus C, which consists of 33 million passages. pre-trained retriever ColBERT-v2 (Santhanam et al., 2022) is used to encode all passages and perform retrieval given query. We retrieve top-3 passages for each query and combine them with the previously retrieved passages for final factuality assessment. At most 2 search steps are performed to retrieve relevant passages for each statement. Detailed prompts can be found in Appendix A."
        },
        {
            "title": "5.3 Models",
            "content": "We employ the pre-trained gemma-2b model (Team et al., 2024) as our policy model, which is an openweight model pre-trained on large-scale datasets across diverse domains. The model is first finetuned with the Deita SFT dataset, and then aligned with the alignment datasets. We also conduct experiments on LLaMA-3 8B (Meta, 2024) and Phi3-Mini models (Abdin et al., 2024), which are both open-weight models which were aligned with proprietary data."
        },
        {
            "title": "5.4 Evaluation Procedure",
            "content": "The trained models are evaluated on two aspects: long-form factuality and helpfulness. Long-form Factuality Evaluation We evaluate models long-form factuality following the procedure of SAFE (Wei et al., 2024)4. We choose the LongFact-object subset following the original work, which consists of 38 topics. We change the Google Search API to our Wikipedia retriever due to resource and budget constraint. In preliminary ex4https://github.com/google-deepmind/ long-form-factuality periments, we find that this change have very little impact on the evaluation outcome. Our evaluator has correlation scores of 0.93 and 0.82 with SAFE for the number of Supported and Not Supported assessments, respectively. We follow SAFE to add an postamble to each prompt to ask for the model to generate as many details and examples as possible. We report 1@100 as the main evaluation metric. We also report the factual precision and factual recall scores. In addition, we evaluate models with FactScore (Min et al., 2023). We run the evaluation from its official implementation5 and use GPT-3.5-TURBO as the evaluator instead of InstructGPT. FactScore can be interpreted as the factual precision of bio generation. Helpfulness Evaluation We evaluate models helpfulness on MT-Bench (Zheng et al., 2023), popular benchmark that includes challenging multiturn open-ended questions for evaluating chat assistants. The automatic judgement is performed by GPT-4 with score of 1 to 10, which is shown to be highly-correlated with human judgement. The evaluation is done with their official implementation6. 5."
        },
        {
            "title": "Implementation Details",
            "content": "We set the threshold to 0.75, meaning that the response is chosen if its f1@100 is higher than 0.75. The threshold for sentences ts is set to 1.0, i.e., the sentence is only chosen if all of its atomic statements are supported. During training, we set β = 0.1 for KTO and βf = 0.5 for fKTO. The weight of LfKTO, λ, is set to 2.0. The learning rate is set to 5e-7 with linear learning rate schedule. We set the effective batch size to 16 and train for 1 epoch for each iteration. In order to reduce GPU memory consumption during training, we optimize the model with the 8-bit version of the AdamW optimizer. We iteratively optimize the LM as described in Section 4.3 for 3 iterations. All experiments are run on 4xV100 GPUs. Each training run takes 1 to 2 hours to finish. We estimate that each evaluation run costs $25 in API credits."
        },
        {
            "title": "6 Results",
            "content": "We present the main results in Table 1, where we contrast FACTALIGN with both proprietary models (GPT-4-Turbo and GPT-3.5-Turbo), prominent 5https://github.com/shmsw25/FActScore 6https://github.com/lm-sys/FastChat/tree/main/ fastchat/llm_judge LongFact 1@100 Precision FactScore # claims FS # claims MT-Bench GPT-4-Turbo GPT-3.5-Turbo Olmo-7B-Instruct LLaMA-2-70B-Chat Gemma-2B-SFT + FACTALIGN Phi3-Mini + FACTALIGN LLaMA3-8B-Chat + FACTALIGN 91.85 61.59 75.42 83.78 61.74 86. 66.91 82.31 79.98 87.32 87.99 89.25 66.8 79.33 77.41 79.59 74.41 78.29 71.89 78. 108.9 52.4 131.6 112.5 66.8 135.1 81.5 106.7 121.5 132.6 83.98 75.79 - 59.06 57.02 59. 58.33 61.21 54.96 62.84 49.7 18.1 - 65.4 27.4 51.7 36.7 49.6 50.6 55. 8.99 7.94 - 6.85 4.41 5.70 6.58 6.63 7.38 7.44 Table 1: Main results of our experiments. FS denotes the FactScore and # claims denotes the average number of claims. We report percentage points for 1@100, precision, and FS. We mark the best scores among the Gemma-2B models in bold. open-weight model (LLaMA-2-70B-Chat (Touvron et al., 2023)), and fully open-source model (Olmo-7B-Instruct) (Groeneveld et al., 2024). The the comparison involves our baseline model, Gemma-2B model7, which has been fine-tuned using our SFT dataset, Deita. This model serves as the foundational policy model for all subsequent aligned models. Additionally, we benchmark against the rejection sampling fine-tuning method (Yuan et al., 2023), involving supervised fine-tuning with selected samples from our alignment dataset. This method shows modest improvements. Remarkably, our FACTALIGN framework significantly improves the long-form factuality and helpfulness of the baseline model, achieving relative improvements of 40.1% and 29.2% in terms of 1@100 and average score on MT-Bench, respectively. These results demonstrate our capability to simultaneously refine LMs for enhanced factuality and utility. Moreover, FACTALIGN also boosts the FactScore of the baseline models and outperforms larger models like GPT-3.5-Turbo and LLaMA2-70B-Chat in both 1@100 and FactScore metrics. This demonstrates the potential for smaller LMs, through precise alignment, to surpass generaldomain large LMs in factual accuracy. With detailed examination of the metrics, it is evident that FACTALIGN primarily improves factual recall, increasing the output of factual claims from 66.8 to 135.1, while slightly improving fac7https://huggingface.co/google/gemma-2b f1@100 Precision"
        },
        {
            "title": "FACTALIGN",
            "content": "- Iterative Optimization - fKTO - General Dataset - Factuality Dataset Rejection Fine-tuning 86.51 77.10 73.12 61.33 68.86 68.33 79.59 78.44 73.27 65.72 69.93 77.86 Table 2: Ablation study on LongFact (%). tual precision from 77.41 to 79.59. This enhancement suggests that FACTALIGN primarily amplifies output volume while maintaining factual precision. This trend echoes findings from general-domain alignment research, which indicates that alignment algorithms typically promote longer outputs, likely due to combined human and LM preference for more extensive responses (Dubois et al., 2024). qualitative example of this can be found in Appendix B."
        },
        {
            "title": "6.1 Ablation Study",
            "content": "To validate the effectiveness of our proposed components, we conduct an ablation study to understand their contribution to the final improvement. The results are reported under FACTALIGN in Table 2. Firstly, we remove the iterative optimization technique, where we only perform 1 iteration of training. As shown in the results, removing iterative optimization significantly degrades the performance, where 1@100 drops by over 10 points. Seen Unseen Gemma-2B-SFT General-domain only Seen topics only 61.97 69.45 76.49 61.36 68.23 72. Table 3: Performance on seen and unseen topics (%). We report the f1@100 score on LongFact. βf 0.5 0.5 0.5 0.1 1.0 threshold 1@100 0.65 0.75 0.85 0.75 0. 74.32 77.10 73.66 75.86 75.12 Table 4: Performance with various number of βf and threshold (%). This result demonstrates that it is crucial to perform iterative optimization or online sampling in order to achieve better performance. We also observe that training on the same dataset for multiple epochs yields worse performance, showcasing that the alignment data quickly becomes stale and no longer is good sample after 1 epoch of training. Note that for all other ablation experiments, we also only performs 1 iteration of training. Next, we remove the fKTO loss LfKTO and align the model with only LKTO. Without LfKTO, the factual f1 score degrades by 4 points from 77.10 to 73.12, demonstrating that the proposed fine-grained alignment objective fKTO can align LMs more effectively. Note that we observe that the fKTO loss occasionally makes the training process unstable. We hypothesize that this is due to the amount of factuality data being much less than the generaldomain data, thus making the instances with finegrained label sparse during training. Hence, the estimation of the fKTO loss becomes slightly unstable. We will also discuss the sensitivity to hyperparameters in Section 6.4. We also conduct an experiment where we exclude the general-domain alignment dataset from our training data. The performance degrades significantly on all datasets after removing the generaldomain alignment dataset. Upon further investigation, we observe that without general-domain data, LMs easily overfit and often generate repetitive outputs. This result indicates that mixture of general-domain datasets and factualy-specific Figure 3: The precision-recall curve with varying ratios of data mixture. SFT denotes the supervised fine-tuned baseline. The labels denote the ratio of the precision data points used. datasets is important to maintain balance and prevent catastrophic forgetting. Finally, we exclude the factuality dataset during training, i.e., only align the LM on generaldomain datasets. As shown in the results, aligning with general-domain dataset also improves the long-form factuality and helpfulness of the baseline model. This indicates that factuality might be encoded in the diverse array of human values present in the general-domain alignment dataset. However, including the factuality dataset sill achieves significantly superior performance for long-form factuality."
        },
        {
            "title": "6.2 Generalization to New Topics",
            "content": "Since the training data is created with the same set of topics in LongFact, all the topics should be considered seen during evaluation. Note that prompts used in evaluation are excluded during training. To validate whether FACTALIGN could generalize to unseen topics, we conduct an additional experiment where we split the topics into 19 seen topics and 19 unseen topics. We only include the data from the seen topics during training and perform evaluation on the unseen topics. The results are reported in Table 3. The results show that FACTALIGN performs slightly worse on unseen topics. Nonetheless, it still outperforms the baseline models significantly, showcasing that the alignment can generalize to unseen topics."
        },
        {
            "title": "6.3 Relationship of Precision-Recall",
            "content": "By varying the ratio of data points using precision as the threshold and those using recall, we can control the tradeoff between the precision score and the recall score. We train models with different data mixture and plot the corresponding precisionrecall curve in Figure 3. The model trained with 100% precision data achieves the highest precision score, and the model trained with 100% recall data achieves the highest recall score. Furthermore, we can achieve specific level of factual precision and recall scores on the curve by changing the ratio. This result demonstrates that FACTALIGN enables control over the desired factual precision and recall scores. 6.4 Sensitivity of Hyperparameters We report the performance of FACTALIGN under various hyperparamter settings. The results are reported in Table 4. We observe that the threshold affects performance slightly, with 0.75 being the best setting. We also notice that with = 0.75, the labels are balanced, i.e., the number of chosen samples is roughly equal to the number of rejected samples. This indicates that constructing balanced dataset perform better for our alignment algorithm. We also vary the hyperparameter βf and notice that it degrades performance slightly. Note that the best βf value is higher than the β value typically set for KTO, i.e., 0.1. Our hypothesis is that since fKTO operates on the sentence-level, the log probability difference naturally has lower magnitude compared to the response-level case. Thus, higher value of βf is needed to promote the fine-grained loss to similar level as the response-level loss."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we address the issue of long-form actuality in LLMs by proposing novel alignment framework, FACTALIGN. Our approach, which incorporates proposed data construction process alongside the fine-grained alignment algorithm fKTO, significantly enhances the factuality of LLMs over long-form responses, while also boosting their helpfulness. Our analysis demonstrates that FACTALIGN enables detailed control over the desired level of factual precision and recall scores. We believe that the insights and methodologies presented in our work can motivate further advancements in the factuality alignment of LLMs."
        },
        {
            "title": "Limitations",
            "content": "Our work focuses on the factuality aspect of LLMs, which we define as whether the generated response is supported by retrieved evidence. This definition makes the performance dependent to the performance of the retriever and the coverage of the knowledge corpus. Moreover, our data creation and evaluation pipeline rely on automatic factuality evaluators. Even though prior work has validated the effectiveness of these evaluators by showing high correlation with human judgements, the automatic evaluators inevitably might make incorrect judgements. While FACTALIGN significantly improves the factuality of LLMs, they still are prone to generate non-factual content. calibration method would be complimentary to our method to ensure the reliability of LLMs. We focus on controlled setting where the information-seeking prompts are all questions about certain object. This is to ensure the reliability of the automatic evaluation process. Future work could extend the coverage of the informationseeking prompts to more diverse user queries."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank the reviewers for their insightful comments. This work was financially supported by the National Science and Technology Council (NSTC) in Taiwan, under Grants 111-2222-E-002-013MY3 and 112-2223-E002-012-MY5. We thank to National Center for High-performance Computing (NCHC) of National Applied Research Laboratories (NARLabs) in Taiwan and Google PaliGemma Academic Program for providing computational and storage resources."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Cheng-Han Chiang and Hung-yi Lee. 2024. Merging facts, crafting fallacies: Evaluating the contradictory nature of aggregated factual claims in long-form generations. arXiv preprint arXiv:2402.05629. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. 2024. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. Preprint, arXiv:2310.01377. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does fine-tuning llms on new knowledge encourage hallucinations? arXiv preprint arXiv:2405.05904. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2024. Atlas: few-shot learning with retrieval augmented language models. J. Mach. Learn. Res., 24(1). Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138. Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. 2024. Unfamiliar finetuning examples control how language models hallucinate. arXiv preprint arXiv:2403.05612. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems, 35:3458634599. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inferencetime intervention: Eliciting truthful answers from language model. In Thirty-seventh Conference on Neural Information Processing Systems. Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, and Xilun Chen. 2024. Flame: Factuality-aware alignment for large language models. arXiv preprint arXiv:2405.01525. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations. Yougang Lyu, Lingyong Yan, Shuaiqiang Wang, Haibo Shi, Dawei Yin, Pengjie Ren, Zhumin Chen, Maarten de Rijke, and Zhaochun Ren. 2024. Knowtuning: Knowledge-aware fine-tuning for large language models. arXiv preprint arXiv:2402.11176. Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 90049017, Singapore. Association for Computational Linguistics. AI Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. Meta AI. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore. Association for Computational Linguistics. Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. 2024. Fine-grained hallucination detection and editing for language models. arXiv preprint arXiv:2401.06855. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems. Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, and Amitava Das. 2023. The troubling emergence of hallucination in large language models - an extensive definition, quantification, and prescriptive remediations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 25412573, Singapore. Association for Computational Linguistics. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. Colretrieval via BERTv2: Effective and efficient lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 37153734, Seattle, United States. Association for Computational Linguistics. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 37843803, Punta Cana, Dominican Republic. Association for Computational Linguistics. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher Manning, and Chelsea Finn. 2024. Finetuning language models for factuality. In The Twelfth International Conference on Learning Representations. Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. 2024. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2024. Finegrained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024a. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations. Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024b. Hallucination is inevitable: An innate limitation of large language models. arXiv preprint arXiv:2401.11817. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685."
        },
        {
            "title": "A Prompts Used",
            "content": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti We use the following prompt for new prompt generation Instructions: 1. Ask general question about specific object (such as person, place, event, act, company etc.). 2. The object should be very specific and niche within the topic of {topic}. 4. Do not ask about any particular aspect of the object (such as history, significance, impact, role, purpose, etc.). 5. Follow the question styles in the provided examples. 6. Wrap the question in square brackets. The following prompt is used for query generation: Instructions: 1. You have been given STATEMENT and some KNOWLEDGE points. 2. Determine whether the given STATEMENT is supported by the given KNOWLEDGE. The STATEMENT does not need to be explicitly supported by the KNOWLEDGE, but should be strongly implied by the KNOWLEDGE. 3. Before showing your answer, think step-by-step and show your specific reasoning. As part of your reasoning, summarize the main points of the KNOWLEDGE related to the STATEMENT. 4. If the STATEMENT is supported by the KNOWLEDGE, be sure to show the supporting evidence. 5. After stating your reasoning, restate the STATEMENT and then determine your final answer based on your reasoning and the STATEMENT. 6. Your final answer should be either \"Supported\" or \"Not Supported\". Wrap your final answer in square brackets."
        },
        {
            "title": "B Qualitative Examples",
            "content": "We include qualitative in Table 5. Instructions: 1. You have been given STATEMENT and some KNOWLEDGE points. 2. Your goal is to try to find evidence that either supports or does not support the factual accuracy of the given STATEMENT. 3. To do this, you are allowed to issue ONE search query that you think will allow you to find additional useful evidence. 4. Your query should aim to obtain new information that does not appear in the KNOWLEDGE. This new information should be useful for determining the factual accuracy of the given STATEMENT. 5. Format your final query by putting it in markdown code block. KNOWLEDGE: {_KNOWLEDGE_PLACEHOLDER} STATEMENT: {_STATEMENT_PLACEHOLDER}"
        },
        {
            "title": "The following prompt is used for final answer",
            "content": "assessment: Model Response # Claims 1@100 Prompt: Could you provide some details about the Hague Conventions? 67 63.4 65 78.9 Gemma-2B-SFT FACTALIGN Sure! The Hague Conventions are set of international treaties that were designed to protect the rights of civilians in international armed conflicts. Here are some specific details and examples related to the Hague Conventions: 1. The First Hague Convention: The First Hague Convention was signed in 1899 and was designed to protect the rights of civilians in international armed conflicts. It was signed by 31 countries and established the principle that civilians should not be targeted by military forces. Example: In 1904, during the Boxer Rebellion in China, British and French troops were sent to suppress the rebellion. However, they were ordered to avoid killing or injuring the Chinese civilians. The British and French troops complied with this principle and did not fire on the civilians. 2. The Second Hague Convention: The Second Hague Convention was signed in 1907 and established the principle that the use of prohibited weapons in war is prohibited. It was signed by 33 countries and established the principle that the use of prohibited weapons in war is prohibited. Example: During World War I, the use of poison gas by both the German and British armies was prohibited under the Second Hague Convention. However, the German army still used poison gas and was criticized for its use. 3. The Fourth Hague Convention: The Fourth Hague Convention was signed in 1907 and established the principle that the use of prohibited weapons in war is prohibited. It was signed by 33 countries and established the principle that the use of prohibited weapons in war is prohibited. Example: During World War II, the use of chemical weapons by both the German and Soviet armies was prohibited under the Fourth Hague Convention. However, the Soviet army used chemical weapons against the Polish army in 1939, which was not violation of the Hague Convention. These are just few examples of how the Hague Conventions have been used to protect the rights of civilians in international armed conflicts. The Hague Conventions have been ratified by more than 190 countries and have been instrumental in shaping international law and protecting the rights of civilians in armed conflicts. The Hague Conventions are series of international treaties that were signed in the Netherlands city of The Hague in 1899. These conventions were designed to address the issue of international law and to promote cooperation between different countries. The Hague Conventions are divided into several different treaties, each addressing different aspect of international law. Some of the most significant Hague Conventions include: 1. The Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict (1954) 2. The Hague Convention Respecting the Laws and Customs of War on Land (1907) 3. The Hague Convention for the Protection of Victims of International Armed Conflicts (1977) 4. The Hague Convention on the Civil Aspects of International Litigation (1971) 5. The Hague Convention on the International Sale of Goods (1980) 6. The Hague Convention on the Protection of the Rights of Visigothic Migrants (1998) These conventions were signed by large number of countries, and they have had significant impact on international law. For example, the Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict was signed in 1954, and it has been ratified by large number of countries. This convention has been used in number of different situations, including the conflict in Bosnia and Herzegovina. In terms of specific examples, the Hague Conventions have been used in number of different situations. For example, the Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict has been used in number of different conflicts, including the conflict in Bosnia and Herzegovina. The Hague Convention Respecting the Laws and Customs of War on Land has been used in number of different situations, including the conflict in Afghanistan. Overall, the Hague Conventions are considered to be some of the most significant international treaties of the 20th century. They have had significant impact on international law, and they continue to be relevant today. Table 5: An example of model generations. The generations are cut short due to space limit."
        }
    ],
    "affiliations": [
        "National Taiwan University, Taipei, Taiwan"
    ]
}