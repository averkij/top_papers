{
    "paper_title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics",
    "authors": [
        "Weida Wang",
        "Dongchen Huang",
        "Jiatong Li",
        "Tengchao Yang",
        "Ziyang Zheng",
        "Di Zhang",
        "Dong Han",
        "Benteng Chen",
        "Binzhao Luo",
        "Zhiyu Liu",
        "Kunling Liu",
        "Zhiyuan Gao",
        "Shiqi Geng",
        "Wei Ma",
        "Jiaming Su",
        "Xin Li",
        "Shuchen Pu",
        "Yuhan Shui",
        "Qianjia Cheng",
        "Zhihao Dou",
        "Dongfei Cui",
        "Changyong He",
        "Jin Zeng",
        "Zeke Xie",
        "Mao Su",
        "Dongzhan Zhou",
        "Yuqiang Li",
        "Wanli Ouyang",
        "Yunqi Cai",
        "Xi Dai",
        "Shufei Zhang",
        "Lei Bai",
        "Jinguang Cheng",
        "Zhong Fang",
        "Hongming Weng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure a deep understanding of the problem-solving process,we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields a more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a significant capability gap, especially for this practical and frontier domain relative to traditional physics. The code anddataset are publicly available at https://github.com/CMPhysBench/CMPhysBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 4 2 1 8 1 . 8 0 5 2 : r CMPhysBench: Benchmark for Evaluating Large Language Models in Condensed Matter Physics Weida Wang1,4*, Dongchen Huang2,3*, Jiatong Li6*, Tengchao Yang5*, Ziyang Zheng2,3*, Di Zhang4, Dong Han1, Benteng Chen1, Binzhao Luo3, Zhiyu Liu3, Kunling Liu3, Zhiyuan Gao3, Shiqi Geng1, Wei Ma5, Jiaming Su5, Xin Li5, Shuchen Pu1, Yuhan Shui1, Qianjia Cheng1, Zhihao Dou1, Dongfei Cui1, Changyong He5, Jin Zeng5, Zeke Xie8, Mao Su1, Dongzhan Zhou1, Yuqiang Li1, Wanli Ouyang1, Yunqi Cai2,3, Xi Dai7, Shufei Zhang1, Lei Bai1, Jinguang Cheng3, Zhong Fang3, Hongming Weng2,3 1Shanghai Artificial Intelligence Laboratory 2Beijing National Laboratory for Condensed Matter Physics and Institute of Physics, Chinese Academy of Sciences 3 Condensed Matter Physics Data Center, Chinese Academy of Sciences 4Fudan University 5Tongji University 6Hong Kong Polytechnic University 7Hong Kong University of Science and Technology 8Hong Kong University of Science and Technology (Guangzhou) *Equal contribution Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure deep understanding of the problem-solving process, we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring significant capability gap, especially for this practical and frontier domain relative to traditional physics. The code and dataset are publicly available at https://github.com/CMPhysBench/CMPhysBench."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models (LLMs) have revolutionized natural language processing, demonstrating exceptional capabilities in understanding and generation tasks [5, 12], particularly in commonsense and mathematical reasoning, often enhanced by reinforcement learning techniques [19, 27]. Leveraging these strengths, LLMs have achieved impressive results in Olympiadlevel mathematics [58], complex programming [14], and even scientific research tasks such as molecule generation [31, 32] and optimization [30], fueling expectations for their applicability in physics. As field grounded in uncovering the fundamental laws of nature, physics imposes uniquely rigorous demands on LLMsrequiring not only advanced reasoning and mathematical precision but also deep conceptual understanding of physical principles, making it an ideal testbed for evaluating whether LLMs truly comprehend the structure of the real world. *These authors contributed equally. Corresponding authors. Figure 1: Example problem from CMPhysBench comparing three metrics for model performance evaluation: Expression Edit Distance (EED) [41], Accuracy (Acc) [21] and the proposed Scalable Expression Edit Distance (SEED). Scores for three different responses to the same problem are shown, where SEED excels at both accuracy and fine-grained evaluation. Previous benchmark efforts, such as SciQ [53] and ScienceQA [44], have played an important role in facilitating the evaluation of LLMs on physics-related questions. However, these benchmarks primarily focus on high school-level content, which may not adequately test the complexity of reasoning or the degree of mathematical rigor required for evaluating advanced understanding in physics. More recent benchmarks, including PHYBench [41] and UGPhysics [54], have made meaningful progress by incorporating undergraduate-level problems. Nonetheless, these benchmarks remain limited in depth, as they often underrepresent the most critical and frontier areas of contemporary physics research. Considering the inherent conceptual and mathematical complexity of physics, broader and more rigorous benchmarks are essential for assessing whether LLMs can support real-world scientific tasks and facilitate cross-disciplinary integration. In this work, we focus on Condensed Matter Physics (CMP), which investigates the physical properties and microscopic structures of condensed phases of matter, namely solids and liquids [33]. As central area of modern physics, condensed matter has become driving force behind many recent theoretical and experimental advances, contributing to our understanding of phenomena such as superconductivity, topological states, and quantum phase transitions. This field integrates concepts from quantum mechanics [34], statistical physics [52], solid-state physics [17], and many-body theory [24], posing significant challenges due to its complexity, inter-disciplinarity, data-scarcity, and demand for precise mathematical formulation evaluation. To address these challenges, we present CMPhysBench, novel benchmark specifically designed to evaluate the problem-solving abilities of LLMs in CMP. It comprises 520 questions, manually authored by Ph.D. students and postdoctoral researchers based on standard graduate textbooks spanning key CMP subfields, with difficulty levels ranging from undergraduate to advanced graduate coursework. Unlike multiple-choice benchmarks [44, 56] that are ignorant of intermediate reasoning and procedural correctness, CMPhysBench emphasizes open-ended calculation problems, requiring models to produce complete solutions that reflect both conceptual understanding and computational precision. Furthermore, inspired by Expression Edit Distance (EED) [41], we propose the Scalable Expression Edit Distance (SEED) metric to quantify the similarity between predicted and reference solutions by evaluating structural differences in mathematical expressions, offering more robust and interpretable performance measure than exact string matching [21]. Different from existing metrics derived from expression tree structures, e.g. EED [41], SEED more accurately handles diverse answer types by providing fine-grained, non-binary scoring. An illustration of SEED and an example problem is shown in Figure 1. Our experimental results reveal notable performance gap: although 2 LLMs demonstrate strong capabilities in general mathematical reasoning, their effectiveness in CMP remains limited. This underscores limitations of LLMs in physics to fully harness their capabilities in complex scientific fields such as CMP. To summarize, our contribution lies in the following aspects: Graduate-level CMP benchmark with open-ended calculation. We release CMPhysBench, 520-question benchmark manually authored by Ph.D. students and postdoctoral researchers based on standard graduate textbooks, spanning core subfields and emphasizing open-ended calculation tasks that require complete, step-by-step solutions across five answer types. SEED: fine-grained, accurate evaluation metric. We propose the Scalable Expression Edit Distance (SEED), which maps diverse answer types to ASTs and computes tree-edit distance with unit conversion, scientific-notation parsing, and rounding within tolerance, yielding non-binary partial credit and interpretable error localization. Comprehensive empirical study and diagnosis. We evaluate 18 proprietary and opensource LLMs on CMPhysBench, finding consistently low performance and pronounced variability across models, and providing quantitative analyses that illuminate failure modes and opportunities for improving domain-specific reasoning in CMP."
        },
        {
            "title": "2.1 Overview",
            "content": "In this section, we introduce the details of CMPhysBench. As shown in Table 1, CMPhysBench covers 520 carefully curated questions with difficulty spanning from introductory undergraduate exercises to advanced graduate-level challenges from CMP. CMPhysBench comprises six representative topics of CMP, structured as follows. Firstly, to ensure domain representativeness, we include four core topics: Magnetism, Superconductivity, Strongly Correlated Systems, and Semiconductors. Secondly, to holistically evaluate LLMs beyond narrow domain expertise, we extend the benchmark with two additional dimensions of CMP. One of the additional categories is Theoretical Foundations, which encompasses crystallography, plasmonics, phase transitions, and condensed matter field theory. The other is Others, which further includes quantum mechanics, statistical physics, electrodynamics, and quantum field theory. This hierarchical categorization allows simultaneous assessment of domain-specific knowledge and general physical reasoning capabilities. Figure 2: Overview of the CMPhysBench dataset and its answer types. (a) Distribution of problem topics across various condensed matter physics domains in CMPhysBench. (b) Distribution of answer types across the dataset, highlighting the prevalence of numeric answers. (c) table displaying examples of each answer type. At the same time, following the settings in OlympiadBench [21], we also categorize these questions based on the answer types. Specifically, there are five answer types in CMPhysBench, including tuple, equation, numeric, expression, and interval. The categorization of the questions is performed by human experts to ensure its correctness. Details of the data categorization and distribution are listed in Figure 2(a) and (b), and our benchmark contains topics across various fields in condensed-matter 3 physics, and the problems can be divided into five types: Tuple, Numeric, Expression, Equation and Interval, and the examples of them are shown in Figure. 2 (c)."
        },
        {
            "title": "2.2 Data Curation",
            "content": "Figure 3: The data curation pipeline of CMPhysBench. We initially collect course materials and exercise problems from 17 textbooks with difficulty spanning from introductory undergraduate exercises to advanced graduate-level challenges. We mainly choose classical textbooks in CMP like An Introduction to Quantum Field Theory [40], Classical Field Theory [45], Condensed Matter Field Theory (3rd edition) [2], Introduction to Many-Body Physics [10], Statistical Physics [28] etc. As shown in Figure 3, the data curation pipeline consists of four stages to ensure the quality and usability of the benchmark. Collection Firstly, the collected textbook materials are first converted from PDF to Markdown format, followed by transformation into structured, machine-readable text formats. Specifically, we convert the PDF documents of textbooks into Markdown format via MathPix1. Structuring Subsequently, we carefully modify the selected the problems relevant to calculation tasks and adapted them to standardized calculation-question format suitable for benchmarking. Specifically, we propose only calculation problems. Quality Control, Expert Review and Annotation Finally, each adapted question is manually checked by Ph.D. students and postdoctoral researchers specialized in Condensed Matter Physics. During this review process, incomprehensible or ambiguous questions are removed and detailed answers and solutions were carefully verified, ensuring that all retained data could be clearly interpreted and evaluated. In addition, all questions are further classified based on the type of answer they require, which can be demonstrated by Figure 2 (c)."
        },
        {
            "title": "2.3 Evaluation Metric: Scalable Expression Edit Distance (SEED)",
            "content": "To provide robust and fine-grained evaluation, we follow the core EED pipeline. We first extract the mathematical expression from the model output and canonicalize it to standard LaTeX; we then convert it to SymPy2 object via latex2sympy_extended, normalize terms to positive canonical form, and apply simplify() to stabilize and accelerate subsequent comparison. While EED struggles with noisy LaTeX and varied answer types, SEED standardizes them and provides fine-grained, physics-aware evaluation. We extend the evaluation in three directions. First, answer-type support and unification (as shown in right side of Figure 4): (1) Expressions are directly parsed into abstract syntax trees (ASTs). (2) Equations are standardized by moving all terms to one side. (3) Tuples are evaluated component-wise by positional matching, and the SEED scores are averaged. (4) Intervals incorporate boundary openness through symbolic representations. (5) Numeric answers are evaluated with attention to unit conversion, scientific notation parsing, and rounding within relative tolerance. Second, expanded symbolic coverage: we add native handling of matrices/vectors and inequalities (<, , >, ), which we canonicalize as () # 0 (with # {< , , >, }) while preserving semantics under operations that flip inequality direction. Third, robust 1https://mathpix.com/ 2https://www.sympy.org/ 4 Figure 4: SEED calculation process for different answer types, including edit-distance examples and rules for expressions, equations, tuples, intervals, and numeric answers. LaTeX preprocessing: we strip wrappers such as boxed{}, remove left and right, normalize implicit multiplication (e.g., 2x, ab), unify Unicode symbols (e.g., the minus sign), standardize function aliases and font commands (mathrm{}, mathcal{}, mathbb{}), discard extraneous natural-language boilerplate (e.g., Final Answer:), and auto-balance parentheses and fractions. These improvements enable SEED to build ASTs reliably from noisy LLM outputs and, via tree-edit distance, deliver non-binary partial credit together with interpretable error localization. Its type-agnostic AST design and pluggable, physics-aware normalization allow easy extension to new answer types and domain rules, enabling application across CMP and other STEM tasks while maintaining unified, fine-grained evaluation."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we introduce the LLMs we benchmarked, detailed experiment setup, and main results on CMPhysBench."
        },
        {
            "title": "3.1 Models",
            "content": "We group models by provider families: OpenAI (GPT-4o [36]; o1 [37]; o3 [38]; o3-mini [39]; o4-mini [38]), Google (Gemini 2.5 Pro, Gemini 2.0 Flash Thinking [47]), Anthropic (Claude 3.7 Sonnet; Claude 3.7 Sonnet Thinking [3]), xAI (Grok 3 Beta [1], Grok 4), Meta/Llama (Llama-3.1-70B-Instruct; Llama-3.3-70B-Instruct [16]), Alibaba/Qwen (Qwen3-32B [48]; QWQ-32B [49]), and DeepSeek (DeepSeek-V3 [11]; DeepSeek-R1 and its distilled variantsR1-Distill-Llama-70B, R1-DistillQwen-32B [19]). This family-based taxonomy spans both proprietary and open-source ecosystems as well as general-purpose and Long-CoT reasoning models, enabling controlled comparisons on CMPhysBench."
        },
        {
            "title": "3.2 Experiment Setup",
            "content": "For proprietary LLMs, we utilize API services to query these models. Meanwhile, for DeepSeek-v3 and DeepSeek-R1, due to their requirement on huge GPU memory, we also adopt API services for the query. In contrast, for the remaining open-source general and reasoning LLMs, we adopt vllm3 for parallel acceleration."
        },
        {
            "title": "3.3 Main Results",
            "content": "As shown in Figure 5, CMPhysBench is challenging across the board. small lead clusterGrok 4, o3, and Gemini 2.5 Proachieves roughly 3036 on the SEED scale with expert-labeled accuracies around 2329% (e.g., Grok 4 36.0 / 28.9), clearly separating from the mid pack. Most remaining 3https://docs.vllm.ai/ 5 systems lie in middle band (2328 SEED, 1620% accuracy), while instruction-tuned opensource baselines fall lower (2022 SEED, 1415% accuracy) and distilled/smaller variants are the weakest (1517 SEED, 1012% accuracy). However, an interesting phenomenon suggests that reasoning LLMs do not necessarily perform better than general LLMs on these challenging domain-specific problems in condensed matter physics, because the problems require domain-specific knowledge and become highly difficult, making it easy for reasoning models to make mistakes during the reasoning process, which then will propagate to the final answer. In this case, the more LLMs think, the more likely they could make mistake. We also observe many near-miss solutions (e.g., unit handling, constants, boundary conditions): expert-labeled accuracy is strict and stays low, whereas SEED systematically yields higher values (typically +59 points) by crediting partial correctness. Together, these patterns give fuller picture of current limitations and highlight the need for physics-aware training and evaluation. Figure 5: Model performance on CMPhysBench. For each model, we report the SEED score along with the expert-labeled accuracy."
        },
        {
            "title": "4.1 Error Analysis",
            "content": "To investigate model failure patterns on CMPhysBench, we conduct detailed error analysis by passing incorrect predictions to GPT-4o and prompting it to infer the underlying reasons. This allows us to categorize error types in scalable and consistent manner. Notably, Grok 4 is excluded from this analysis as it does not generate intermediate reasoning chains, making it difficult to assess its internal logic or attribute specific failure types. The errors are grouped into eight categories: (a) Concept and Model Misuse, incorrect application of scientific principles; (b) Task Misunderstanding, failures in grasping prompt intent; (c) Mathematical or Logical Errors, flawed reasoning or calculations; (d) Notational Inconsistency, misuse of variables or units; (e) Unit or Dimensional Errors, misapplied physical dimensions; (f) Approximation Misuse, inappropriate idealizations; (g) System Limitations, broken or incomplete outputs; and (h) Redundant or Irrelevant Content. As shown in Figure 6 and Table 4, among these errors, the following two errors account for significant proportion:Concept and Model Misuse and Mathematical or Logical Errors. Concept and Model Misuse are the most dominant error type, and account for over 4050% of all normalized errors in models such as GPT-4o (66.5%), Claude 3.7 Sonnet Thinking (51.6%), and DeepSeek-V3 (56.3%). This indicates that many models, even high-performing ones, struggle with the correct application 6 Figure 6: Analysis of error types across models. Left: Error count breakdown by type for each model on CMPhysBench. Right: Representative examples for each error type, with the blue text indicating the specific location of the error in the equation or response. The corresponding reason for the error is listed below each example. of domain-specific physical principles. Another major category is Mathematical or Logical Errors, typically contributing 2030% of total errors. For instance, o4-mini and o3 exhibit logical mistake rates of 31.0% and 29.4%, respectively, despite having relatively good task-following ability. These issues range from incorrect algebraic manipulation to invalid approximations and reveal persistent gaps in symbolic reasoning. In contrast, Task Misunderstanding is particularly notable in instruction-tuned models like Qwen332B (24.2%) and QwQ-32B (27.0%), where the model often responds to the wrong aspect of the question or fails to interpret context-specific constraints. More advanced models such as Gemini 2.5 Pro and o3 demonstrate more balanced error profiles, with concept misuse rates under 35% and lower task misunderstanding rates (e.g., Gemini 2.5 Pro: 7.5%). This result shows that large dataset and good reasoning technique can help understand the physical problem better. Minor but non-negligible error types like Unit Errors and Redundant Content remain relatively rare (<2%), but their presence can still degrade trust in model outputs. Overall, this analysis underscores the need for improved scientific alignment and symbolic precision, particularly in high-stakes physics domains."
        },
        {
            "title": "4.2 Analysis of Different Problem Topics",
            "content": "As shown in Figure 7 and Table 5, performance varies markedly across topics and model families. Grok 4 leads most categories, topping Magnetism (35.30), Superconductivity (43.42), and Theory (41.21), while o3 is strong all-rounderfirst on Others (46.42) and second on Superconductivity/Strongly Correlated Systems/Semiconductors (35.77/37.34/27.80). Topic-specific peaks also emerge: DeepSeek-R1 attains the best SCS score (42.16), Gemini 2.5 Pro leads Semiconductors (29.18) and is competitive in Theory (40.50), and DeepSeek-V3 ranks second in Magnetism (25.75). Notably, even top models display pronounced asymmetries (e.g., Grok 4 strong in SC/Theory but lower on SCS), indicating that strengths do not transfer uniformly across CMP subfields. These patterns highlight the premium on domain-specific reasoning beyond generic mathematical skill. Instruction-tuned open-source baselines generally trail proprietary reasoning models, yet some exhibit localized strengths (e.g., Qwen3-32B performs relatively well on Theory 35.47 but remains weak on Magnetism 8.47), underscoring uneven competencies across topics. Overall, the crossdomain spread suggests that improved handling of physics conventions and subfield-aware training are crucial for closing the gap."
        },
        {
            "title": "4.3 Comparison with Different Metrics",
            "content": "To systematically assess the reliability and alignment of various evaluation metrics, we compare SEED against four widely used alternatives: Expression Edit Distance (EED) [41], GPT-4o-based judgment [36], xVerify-9B-C [6], and the OlympiadBench-rule based metric [21]. As shown in Figure 7(b), we report the Spearman correlation coefficients between these metrics and human expert ratings across diverse set of model responses. 7 Figure 7: Comparison of model performance and metric correlations. (a) Radar chart of model performance across six domains. Abbreviations: Mag = Magnetism, SC = Superconductivity, SCS = Strongly Correlated Systems, Semi = Semiconductors, Theory = Theoretical Foundations, Others = Others. (b) Spearman correlation between human expert ratings and automatic evaluation metrics. Among all methods, SEED exhibits the highest correlation with human experts (ρ = 0.90), demonstrating superior agreement with expert judgment. This performance stems from SEEDs design as discrete, structure-aware metric that supports partial credit and accommodates wide range of symbolic answer types commonly found in condensed matter physics, such as equations, intervals, and tuples. Unlike binary accuracy metrics, SEED distinguishes near-miss cases from completely incorrect outputs, providing more nuanced assessment of symbolic reasoning. Furthermore, SEED is designed for polynomial expression similarity evaluation which is very common in graduate-level condensed-matter physics. In contrast, EED, though fast and interpretable, struggles with generalization beyond simple expressions. It is highly sensitive to LaTeX formatting, and fails to handle complex structures like equations with symbolic manipulations or multi-component answers. GPT-4o and xVerify, while more flexible in language understanding, are less reliable for evaluating highly structured mathematical responses. Their performance (ρ = 0.56 and 0.51, respectively) suggests limitations in symbolic alignment, particularly for multi-step derivations and dense expressions common in CMP problems. And these two evaluation method do not explicitly consider equivalent transformation of expression, making it not be the most suitable metric in condensed-matter physics. OlympiadBench-Rule supports multiple answer types, but its rule-based approach is overly simplistic and often fails to account for meaningful structural or mathematical equivalence, resulting in the lowest correlation (ρ = 0.41). Overall, these findings indicate that SEED provides fine-grained partial credit with higher accuracy and robustness, alongside wide applicability and interpretability, making it stronger metric for domain-specific scientific reasoning."
        },
        {
            "title": "5.1 Existing Scientific Benchmarks",
            "content": "Due to the rapid development of LLMs and their potential in scientific research, there is growing trend toward evaluating their performance on scientific problems. For example, benchmarks such as SciQ [53], ScienceQA [44], ARC [8], OpenBookQA [35], PubMedQA [25], SciBench [51], SciEval [46], and E-Eval [23] provide platforms for testing LLMs on general scientific questions across multiple disciplines. Normally, these benchmarks cover broad spectrum of topics but often cap difficulty at K-12 or introductory college levels and favor multiple-choice formats, which increasingly lag behind frontier models and limit exploration of deeper scientific reasoning, especially in physics. In contrast, emerging benchmarks like UGPhysics [54], GPQA [43], SuperGPQA [13], PHYSICS [60], SciCode [50], PHYBench [41], and PhysReason [59] raise the bar by introducing undergraduateto graduate-level problems, stepor expression-aware grading, and physics-specific evaluation pipelines, which impose stricter requirements on domain knowledge, reasoning, and 8 problem-solving. However, most of these still emphasize broad coverage rather than depth within specific research direction; they do not thoroughly examine sustained knowledge acquisition and structured derivations in narrowly defined subfields. In summary, while existing work has substantially advanced the evaluation of LLMs physics problem-solving abilities, there remains notable gap for benchmarks that probe rigorous, subfield-specific physics tasks with fine-grained, structure-aware scoring."
        },
        {
            "title": "5.2 Metrics for Evaluating Complex Reasoning",
            "content": "Evaluation methods for complex reasoning broadly fall into four families. (1) Outcome-based scoring. Many benchmarks judge only the final answer via exact match (EM), e.g., GSM8K [9] and MATH [22], sometimes with minor normalization, which is simple but brittle to equivalent forms and formatting noise.To reduce false negatives, several pipelines [29, 22] augment EM with CAS-based checks using SymPy to test symbolic/numeric equivalence (and lightweight tolerances), as popularized by Minerva and now embedded in common evaluators. Recent math [26] suites further combine exact, numerical, and symbolic equivalence in single grader. (2) Fine-grained structure-aware similarity. Instead of only the final token string, expression-level metrics compare the structure of predicted and reference solutions. PHYBenchs Expression Edit Distance [41] computes tree-edit distances over SymPy expression trees and converts them to fine-grained score, capturing almost-correct derivations that EM misses. (3) Judgeand verifier-based evaluation. LLM-as-a-Judge [18, 7] offers flexible rubric-style grading but is susceptible to systematic biases (e.g., position/verbosity), motivating protocols and debiasing to improve reliability. In contrast, lightweight answer verifiers target objective tasks by extracting the final answer from long chains and checking equivalence across formats; recent models such as xVerify [6] report strong accuracy across math/short-answer settings. Toolkits like MARIO-Eval [57] unify CAS checks with optional LLM judging to improve robustness across datasets. Overall, recent trends move from brittle EM toward type-aware, fine-grained structure-aware, and process-aware evaluation, often blending CAS equivalence, expression-level distances, and calibrated judges/verifiers to better match expert judgments on complex reasoning."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we have introduce CMPhysBench, novel benchmark tailored to evaluate the proficiency of LLMs in the domain of Condensed Matter Physics. Comprising 520 carefully selected questions based on authoritative textbooks, CMPhysBench encompasses wide range of representative topics such as magnetism, superconductivity, strongly correlated systems, semiconductors, etc. To ensure accurate evaluation, we propose the Scalable Expression Edit Distance (SEED) score to measure the similarity between various mathematical expressions. Our findings reveal significant performance gap, with LLMs excelling in general mathematical tasks yet falling short in the specialized context of Condensed Matter Physics, which further underscores the necessity to enhance the effectiveness of LLMs in this domain. Further, we believe domain-specific dataset is crucial in promoting the performance of LLM in the future."
        },
        {
            "title": "7 Limitations",
            "content": "CMPhysBench prioritizes graduate-level, hand-authored calculation questions distilled from standard textbooks, whichwhile broaddo not aim to cover every real-world workflow (e.g., data-driven pipelines or instrument control). SEED already normalizes units/notation and supports diverse answer types, but the handling of certain higher-order operatorsmost notably symbolic integrals with parameter-dependent limits and nested summations/seriesremains an area of active extension; current releases either reduce such cases to comparable forms or defer them. These points are incremental and do not affect our central findings."
        },
        {
            "title": "8 Broader Impacts",
            "content": "Our results point to concrete directions for advancing scientific LLMs: embed physics-aware verification into decoding (dimension/unit checks, conservation laws, boundary/limit tests) to curb spurious 9 reasoning; couple models with symbolic/numeric tools to enable proposecheckrevise derivations instead of single-pass chains; develop domain-curated curricula emphasizing canonical derivations and common approximations; adopt step-aware supervision and SEED-based partial credit so training aligns with scientific correctness; and evaluate in retrieval-grounded, tool-augmented settings that better reflect real CMP workflows. We further encourage parallel tracks for open-source and proprietary systems with shared diagnostics, enabling reproducible ablations and faster community progress in CMP and broader STEM."
        },
        {
            "title": "References",
            "content": "[1] X. AI. Grok 3 betathe age of reasoning agents, 2025. https://x.ai/news/grok-3. [2] A. Altland and B. D. Simons. Condensed matter field theory. Cambridge university press, 2010. [3] Anthropic. Claude 3.7 sonnet and claude code, 2025. https://www.anthropic.com/news/ claude-3-7-sonnet. [4] D. Arora, H. Singh, and Mausam. Have LLMs advanced enough? challenging problem solving benchmark for large language models. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 75277543, Singapore, Dec. 2023. Association for Computational Linguistics. [5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [6] D. Chen, Q. Yu, P. Wang, W. Zhang, B. Tang, F. Xiong, X. Li, M. Yang, and Z. Li. xverify: Efficient answer verifier for reasoning model evaluations. arXiv preprint arXiv:2504.10481, 2025. [7] G. H. Chen, S. Chen, Z. Liu, F. Jiang, and B. Wang. Humans or llms as the judge? study on judgement biases. arXiv preprint arXiv:2402.10669, 2024. [8] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [9] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] P. Coleman. Introduction to many-body physics. Cambridge University Press, 2015. [11] Deepseek. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional In Proceedings of the 2019 conference of the North transformers for language understanding. American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [13] X. Du, Y. Yao, K. Ma, B. Wang, T. Zheng, K. Zhu, M. Liu, Y. Liang, X. Jin, Z. Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. [14] A. El-Kishky, A. Wei, A. Saraiva, B. Minaiev, D. Selsam, D. Dohan, F. Song, H. Lightman, I. Clavera, J. Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. [15] K. Feng, Y. Zhao, Y. Liu, T. Yang, C. Zhao, J. Sous, and A. Cohan. Physics: Benchmarking foundation models on university-level physics problem solving, 2025. [16] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [17] G. Grosso and G. P. Parravicini. Solid state physics. Academic press, 2013. [18] J. Gu, X. Jiang, Z. Shi, H. Tan, X. Zhai, C. Xu, W. Li, Y. Shen, S. Ma, H. Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [19] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [20] Y. Hao, J. Gu, H. W. Wang, L. Li, Z. Yang, L. Wang, and Y. Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. [21] C. He, R. Luo, Y. Bai, S. Hu, Z. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, 2024. [22] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [23] J. Hou, C. Ao, H. Wu, X. Kong, Z. Zheng, D. Tang, C. Li, X. Hu, R. Xu, S. Ni, et al. E-eval: comprehensive chinese k-12 education evaluation benchmark for large language models. arXiv preprint arXiv:2401.15927, 2024. [24] J. C. Inkson. Many-body theory of solids: an introduction. Springer Science & Business Media, 2012. [25] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. [26] D. Karki, M. Kamphuis, and A. Frey. Easymath: 0-shot math benchmark for slms. arXiv preprint arXiv:2505.14852, 2025. [27] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [28] L. Landau and E. Lifshitz. Statistical Physics. Elsevier Science, 1980. [29] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. [30] J. Li, J. Li, Y. Liu, D. Zhou, and Q. Li. Tomg-bench: Evaluating llms on text-based open molecule generation. arXiv preprint arXiv:2412.14642, 2024. [31] J. Li, Y. Liu, W. Fan, X.-Y. Wei, H. Liu, J. Tang, and Q. Li. Empowering molecule discovery for molecule-caption translation with large language models: chatgpt perspective. IEEE transactions on knowledge and data engineering, 2024. [32] J. Li, Y. Liu, W. Liu, J. Le, D. Zhang, W. Fan, D. Zhou, Y. Li, and Q. Li. Molreflect: Towards in-context fine-grained alignments between molecules and texts. arXiv preprint arXiv:2411.14721, 2024. [33] M. P. Marder. Condensed matter physics. John Wiley & Sons, 2010. [34] A. Messiah. Quantum mechanics. Courier Corporation, 2014. [35] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [36] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [37] OpenAI. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [38] OpenAI. introducing-o3-and-o4-mini/. Introducing openai o3 and o4-mini, 2025. https://openai.com/index/ [39] OpenAI. Openai o3-mini: Pushing the frontier of cost-effective reasoning, 2025. https: //openai.com/index/openai-o3-mini/. [40] M. E. Peskin. An Introduction to quantum field theory. CRC press, 2018. [41] S. Qiu, S. Guo, Z.-Y. Song, Y. Sun, Z. Cai, J. Wei, T. Luo, Y. Yin, H. Zhang, Y. Hu, et al. Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv preprint arXiv:2504.16074, 2025. [42] S. Qiu, S. Guo, Z.-Y. Song, Y. Sun, Z. Cai, J. Wei, T. Luo, Y. Yin, H. Zhang, Y. Hu, C. Wang, C. Tang, H. Chang, Q. Liu, Z. Zhou, T. Zhang, J. Zhang, Z. Liu, M. Li, Y. Zhang, B. Jing, X. Yin, Y. Ren, Z. Fu, W. Wang, X. Tian, A. Lv, L. Man, J. Li, F. Tao, Q. Sun, Z. Liang, Y. Mu, Z. Li, J.-J. Zhang, S. Zhang, X. Li, X. Xia, J. Lin, Z. Shen, J. Chen, Q. Xiong, B. Wang, F. Wang, Z. Ni, B. Zhang, F. Cui, C. Shao, Q.-H. Cao, M. xing Luo, M. Zhang, and H. X. Zhu. Phybench: Holistic evaluation of physical perception and reasoning in large language models, 2025. [43] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [44] T. Saikh, T. Ghosal, A. Mittal, A. Ekbal, and P. Bhattacharyya. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289301, 2022. [45] D. E. Soper. Classical field theory. Courier Dover Publications, 2008. [46] L. Sun, Y. Han, Z. Zhao, D. Ma, Z. Shen, B. Chen, L. Chen, and K. Yu. Scieval: multi-level large language model evaluation benchmark for scientific research. arXiv preprint arXiv:2308.13149, 2023. [47] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [48] Q. Team. Qwen3: Think deeper, act faster, 2025. https://qwenlm.github.io/blog/ qwen3/. [49] Q. Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. https://qwenlm. github.io/blog/qwq-32b/. [50] M. Tian, L. Gao, S. D. Zhang, X. Chen, C. Fan, X. Guo, R. Haas, P. Ji, K. Krongchon, Y. Li, S. Liu, D. Luo, Y. Ma, H. Tong, K. Trinh, C. Tian, Z. Wang, B. Wu, Y. Xiong, S. Yin, M. Zhu, K. A. Lieret, Y. Lu, G. Liu, Y. Du, T. Tao, O. Press, J. Callan, E. A. Huerta, and H. Peng. Scicode: research coding benchmark curated by scientists. arXiv, abs/2407.13168, 2024. [51] X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Loomba, S. Zhang, Y. Sun, and W. Wang. SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. In Proceedings of the Forty-First International Conference on Machine Learning, 2024. [52] G. H. Wannier. Statistical physics. Courier Corporation, 1987. [53] J. Welbl, N. F. Liu, and M. Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. [54] X. Xu, Q. Xu, T. Xiao, T. Chen, Y. Yan, J. Zhang, S. Diao, C. Yang, and Y. Wang. Ugphysics: comprehensive benchmark for undergraduate physics reasoning with large language models. arXiv preprint arXiv:2502.00334, 2025. [55] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. [56] X. Yue, T. Zheng, Y. Ni, Y. Wang, K. Zhang, S. Tong, Y. Sun, B. Yu, G. Zhang, H. Sun, Y. Su, W. Chen, and G. Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of ACL, 2025. [57] B. Zhang, C. Li, and K. Fan. Mario eval: Evaluate your math llm with your math llma mathematical dataset evaluation toolkit. arXiv preprint arXiv:2404.13925, 2024. [58] D. Zhang, J. Wu, J. Lei, T. Che, J. Li, T. Xie, X. Huang, S. Zhang, M. Pavone, Y. Li, et al. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. NAACL, 2025. [59] X. Zhang, Y. Dong, Y. Wu, J. Huang, C. Jia, B. Fernando, M. Z. Shou, L. Zhang, and J. Liu. Physreason: comprehensive benchmark towards physics-based reasoning. arXiv preprint arXiv:2502.12054, 2025. [60] S. Zheng, Q. Cheng, J. Yao, M. Wu, H. He, N. Ding, Y. Cheng, S. Hu, L. Bai, D. Zhou, et al. Scaling physical reasoning with the physics dataset. arXiv preprint arXiv:2506.00022, 2025."
        },
        {
            "title": "A Overview of the Appendix",
            "content": "Section contains details about the composition of CMPhysBench, the data curation process, and comparisons with existing benchmarks, highlighting its uniqueness and advantages in the domain of condensed matter physics. Section introduces the SEED evaluation metric and compares it against EED, EM, and GPT-4obased scoring, demonstrating SEEDs scalability and improved alignment with human judgment in symbolic reasoning tasks. Section outlines the experimental settings, including prompt design, tested models, and implementation details used for both answer generation and error analysis. Section presents an in-depth analysis of model performance on CMPhysBench, including breakdowns by error type and topics, as well as representative case studies of physics problems and model predictions."
        },
        {
            "title": "B CMPhysBench Details",
            "content": "B.1 Composition of CMPhysBench In this study, we categorize the benchmark question set into six major domains: Magnetism, Superconductivity, Strongly Correlated Systems, Semiconductors, Theoretical Foundations and General Concepts, as shown in Figure 2. Each domain encompasses key theoretical frameworks and representative problems appropriate for graduate-level physics education, reflecting progressive trajectory from foundational understanding to advanced modeling. Theoretical Foundations encompass wide range of topics from quantum field theory (e.g., KleinGordon fields, Dirac fields, path integrals, spontaneous symmetry breaking) to statistical physics (e.g., Gibbs distribution, fluctuation theory). Given their central role in supporting advanced topics and their broad applicability, this domain also includes 110 questions, aiming to reinforce systematic understanding of modern theoretical physics. Magnetism and Semiconductors are each represented by 115 questions. These domains focus on phenomena such as spin dynamics, magnetic interactions, charge transport, band theory, and device-level behaviortopics of both fundamental and applied significance in condensed matter physics and materials science. The higher question volume reflects the practical complexity and frequency of these systems in real-world physical problems, encouraging students to develop robust modeling and analytical skills. Superconductivity includes topics such as the macroscopic GinzburgLandau theory, microscopic BCS theory, and related experimental phenomena. Although conceptually challenging, the theory is relatively self-contained and often revolves around paradigmatic problems. Thus, moderate number of questions (55) is sufficient to assess students depth of understanding through carefully selected, representative examples. Strongly Correlated Systems cover advanced topics such as quantum many-body fluctuations, the Hubbard model, and Mott transitions. As one of the most intellectually demanding and researchintensive areas in theoretical physics, it is included as an extension module with 15 high-level questions. These problems are designed to challenge students with strong theoretical backgrounds and facilitate further exploration of frontier topics. Others cover fundamental problems and computational techniques in quantum mechanics, including harmonic oscillators, perturbation theory, and spin systems. As these topics span multiple subfields and serve as essential tools across the curriculum, relatively large number of questions (110) are assigned to this domain to ensure comprehensive training in basic problem-solving skills and physical intuition. Generally, the distribution of questions reflects both the structural organization of knowledge in graduate-level physics and deliberate balance between representativeness, theoretical depth, computational rigor, and pedagogical utility. The design seeks to ensure both breadth and depth, enabling the benchmark to serve as comprehensive tool for assessing general competence while also identifying advanced reasoning capabilities. 14 Furthermore, following the settings in OlympiadBench [21], we also categorize these questions based on the answer types. Specifically, there are five answer types in CMPhysBench, including tuple, equation, numeric, expression, and interval, whose distributions are illustrated in Figure 2. The categorization of the questions is performed by human experts to ensure its correctness. B.2 Comparison with Other Benchmarks Table 1: Comparison of our benchmark with existing datasets. For Level: COMP = Competition level, CEE = University Entrance Exam, K1K12 = Primary and Secondary School. For Question Type: OE = Open-ended, MC = Multiple-choice."
        },
        {
            "title": "Benchmark",
            "content": "JEEBench [4] GPQA [43] SciQ [53] SciEval [46] SciBench [51] ScienceQA [44] MMMU [55] MMMU-Pro [56] OlympiadBench [21] EMMA [20] PHYSICS [15] SciCode [50] PHYBench [42] CMPhysBench"
        },
        {
            "title": "Size",
            "content": "123 227 13,679 1,657 295 617 443 3,460 2,334 156 1,297 338"
        },
        {
            "title": "Scoring Type",
            "content": "CEE Graduate K4K8 University K1K12 University University COMP University University K10COMP Graduate OE, MC OE OE, MC OE, MC OE MC OE, MC MC OE MC OE OE OE OE"
        },
        {
            "title": "Binary\nBinary\nBinary\nBinary\nBinary\nBinary\nBinary\nBinary\nBinary\nBinary\nBinary\nBinary\nDetailed\nDetailed",
            "content": "Table 1 provides comparison between CMPhysBench and range of existing scientific and physicsrelated benchmarks. While earlier benchmarks such as PHYSICS, PHYBench, and SciBench have advanced the development of AI systems capable of handling domain-specific problems, CMPhysBench distinguishes itself through its graduate-level difficulty, richer answer representations, and more robust evaluation protocol. Unlike PHYBench, where open-ended (OE) questions are limited to symbolic expressions and evaluated using EED (Expression Edit Distance), CMPhysBench introduces more powerful and extensible metric named SEED (Scalable Expression Edit Distance). This allows for nuanced grading and flexible equivalence matching beyond symbolic forms. Key distinctions of CMPhysBench include: Advanced Answer Types: Answers are not restricted to expressions or numerics; they also include tuples, intervals, and equation systems, reflecting the diversity of physical reasoning and solution strategies required in real-world scientific practice. Graduate-Level Scope: Questions are curated from advanced textbooks and course materials in theoretical and condensed matter physics, ensuring alignment with the cognitive demands of graduate education and early-stage research, rather than standard undergraduate or competitionlevel problems. Semantic Evaluation Flexibility: The SEED metric enables fine-grained evaluation that supports partial credit, symbolic and numeric equivalence, and structural matchingoffering more meaningful feedback on models reasoning capabilities. In contrast, many prior benchmarks (e.g., PHYSICS, MMMU, ScienceQA) focus on multiple-choice formats or expression-only open-ended questions at the high school or early undergraduate level, and often rely on binary correctness. CMPhysBench, by contrast, aims to bridge the gap between academic problem-solving and scientific reasoning, providing more rigorous, diverse, and research-oriented benchmark for evaluating LLMsin physics and beyond."
        },
        {
            "title": "C Evaluation Metric",
            "content": "C.1 Scalable Expression Edit Distance In this appendix, we briefly introduce the differences and advantages of our proposed Scalable Expression Edit Distance (SEED) compared with the original Expression Edit Distance (EED). The term \"scalable\" refers to our methods capability of extending to more complex and varied answer types, including intervals, tuples, and equations, beyond the simple mathematical expressions handled by EED. Key differences and advantages are listed as follows. 1. Enhanced Expression Parsing: SEED supports parsing and scoring of complex LaTeX structures including matrices, derivative expressions (e.g., dx ), logical relations (=, <, >), and various special formatting cases, significantly extending EEDs capabilities. 2. Extended Node Types in Parse Trees: Beyond basic numeric, constant, and symbolic nodes, SEED introduces dedicated nodes for matrices, inequalities, derivatives, and logical operators, ensuring richer semantic representations. 3. Advanced Preprocessing and Standardization: SEED standardizes special fonts (e.g., mathscr{L}), derivative notations, exponent formats, vector notations, fraction formats, and removes problematic LaTeX commands (e.g., text{}), significantly reducing parsing ambiguities and errors. 4. Support for Varied Answer Types: Expressions: Handled similarly to EED, with improved robustness and accuracy. Equations: SEED extracts both sides of equations separately and then combines them into unified form (typically by subtraction) for scoring. This approach allows direct handling of equation-type answers, addressing EEDs inability to process equations effectively. Tuples: Answers structured as tuples (e.g., (a, b, c) = (1, 2, 3)) are transformed into key-value pairs, allowing structured and accurate component-wise evaluation. Intervals: Interval expressions (e.g., (a, b)) are transformed into evaluable mathematical forms, including explicit handling of open and closed boundaries, to facilitate robust scoring. 5. Robust Symbol and Format Handling: Enhanced recognition logic prevents parsing errors from similar LaTeX commands (e.g., distinguishing left from le), and uniformly standardizes ambiguous formatting and special characters."
        },
        {
            "title": "Supported Structures\nParse Tree Nodes\nPreprocessing\nRobustness",
            "content": "Simple Expressions Basic (symbols/functions) Minimal Limited Expressions, Equations, Tuples, Intervals Extended (Matrices, Derivatives, Inequalities) Extensive Standardization and Disambiguation Enhanced Parsing Robustness Table 2: Comparison of SEED and original EED."
        },
        {
            "title": "D Experimental Details",
            "content": "D.1 Prompts for Response Generation This prompt is designed to assess models ability to perform symbolic, step-by-step reasoning in advanced physics. The model must use only the symbols provided, avoiding any external assumptions, and present the final result in clear LaTeX boxed{} format. This ensures precision, interpretability, and alignment with expert-level problem-solving. D.2 Prompts for Error Analysis This prompt instructs GPT-4o, acting as physics expert, to systematically evaluate model-generated answers by checking correctness, categorizing errors (e.g., conceptual, mathematical, dimensional) 16 Prompts for Response Generation You are condensed matter physics expert. Please read the following question and provide step-by-step solution using only the given symbols. Do not introduce any new symbols that are not provided in the problem statement. Your final answer must be presented as readable LaTeX formula, enclosed in boxed{} environment. and providing concise reasoning. Responses are structured in JSON format, enabling precise and efficient error analysis and scoring. Prompts for Error Analysis You are condensed matter physics expert. Your task is to evaluate model-generated answer to physics question. Please perform the following: 1. Determine whether the models answer is correct. 2. If incorrect, identify which of the following error categories (ah) the answer falls into (multiple selections allowed): a) Conceptual or physical model errors: Misuse or misapplication of core physical principles, laws, or models (e.g., using Newtonian mechanics in relativistic regimes). b) Misinterpretation of the problem: Misunderstanding of what the question is asking (e.g., solving for the wrong quantity, or ignoring critical constraints). c) Mathematical or logical mistakes: Incorrect mathematical manipulations, derivations, or reasoning steps (e.g., algebraic mistakes, sign errors, invalid inferences). d) Symbolic or notational inconsistencies: Incorrect, inconsistent, or ambiguous use of symbols or notation (e.g., mixing variables, wrong subscripts, undefined terms). e) Dimensional or unit errors: Violations of dimensional consistency or incorrect unit conversions (e.g., adding quantities of different dimensions). f) Invalid approximations or assumptions: Applying approximations or assumptions that are unjustified in the given context (e.g., small-angle approximation where angle is large). g) Model or language model limitations: Errors clearly stemming from generation failures, hallucinations, or limitations of the AI system (e.g., nonsensical steps, abrupt output truncation). h) Irrelevant or verbose content: Inclusion of content that is redundant, off-topic, or distracts from the solution (e.g., repeating known facts or copying question text unnecessarily). Respond in JSON format as follows: { \"is_correct\": \"true\" or \"false\", \"error_types\": [\"a\", \"c\", ...], \"explanation\": \"Your reasoning in 12 sentences\" } Question{question} Ground Truth{ground_truth} Model Response: {model_response} D.3 Models and Settings We evaluate diverse set of proprietary and open-source large language models, as summarized in Table 3. For OpenAI (GPT-4o, o1, o3, o4-mini) and Anthropic (Claude 3 series) models etc, we use their official APIs. Google Gemini and xAI Grok models are also accessed via respective APIs. For open-source models such as Qwen, DeepSeek, and LLaMA variants, we employ the vLLM inference engine for efficient batched decoding. In cases where vLLM is not supported (e.g., vision-language models), we fall back to the HuggingFace Transformers library for direct model loading. 17 Model QwQ-32B Param Src URL temperature = 0.6 local checkpoint DeepSeek-R1-Distill-Qwen-32B temperature = 0.6 local checkpoint Qwen3-32B temperature = 0.6 local checkpoint DeepSeek-R1-Distill-Llama-70B temperature = 0.6 local checkpoint Llama-3.1-70B-Instruct temperature = 0.6 local checkpoint Llama-3.3-70B-Instruct temperature = 0.6 local checkpoint Claude-3-7-Sonnet Claude-3-7-Sonnet-thinking GPT-4o o1 o3-mini o3 o4-mini DeepSeek-R DeepSeek-V3 Gemini-2.0-flash-thinking Gemini-2.5-pro Grok-3-Beta Grok-4 - - - - - - - - - - - - - claude-3-7-sonnet-latest claude-3-7-sonnet-thinking OpenAI o1 o3-mini o3 o4-mini deepseek-r deepseek-v3 gemini-2.0-flash-thinking-exp gemini-2.5-pro-preview-03-25 grok-3-beta grok-4 https://huggingface.co/Qwen/ QwQ-32B https://huggingface. co/deepseek-ai/ DeepSeek-R1-Distill-Qwen-32B https://huggingface.co/Qwen/ Qwen3-32B https://huggingface. co/deepseek-ai/ DeepSeek-R1-Distill-Llama-70B https://huggingface. co/meta-llama/Llama-3. 1-70B-Instruct https://huggingface. co/meta-llama/Llama-3. 3-70B-Instruct https://www.anthropic.com/ https://www.anthropic.com/ https://platform.openai.com https://platform.openai.com https://platform.openai.com https://platform.openai.com https://platform.openai.com https://huggingface.co/ deepseek-ai/DeepSeek-R1 https://huggingface.co/ deepseek-ai/DeepSeek-V3 https://ai.google.dev/ https://ai.google.dev/ https://x.ai/ https://x.ai/ Table 3: The sources of models used in the experiments and the hyperparameters configuration. \"-\" stands for default parameters."
        },
        {
            "title": "E Experiment Results",
            "content": "E.1 Error Types Counts Table 4: Error types counts. Abbreviations: CM = Concept and Model Misuse, ML = Mathematical or Logical Errors, UD = Unit or Dimensional Errors, TM = Task Misunderstanding, SL = System Limitations, NI = Notational Inconsistency, RI = Redundant or Irrelevant Content, AM = Approximation Misuse. Model CM ML UD TM QwQ-32B DeepSeek-R1-Distill-Qwen-32B Llama-3.1-70B-Instruct Qwen3-32B Claude 3.7 Sonnet Thinking Llama-3.3-70B-Instruct GPT-4o Gemini 2.0 Flash Thinking Claude 3.7 Sonnet Grok 3 Beta DeepSeek-V3 o1 o3-mini o4-mini DeepSeek-R1 Gemini 2.5 Pro o3 176 260 325 197 123 311 294 249 231 242 240 213 205 176 202 172 147 56 85 105 61 98 95 110 109 134 97 115 91 96 96 102 106 116 56 32 22 54 20 20 12 22 10 41 10 15 21 21 16 24 15 0 3 2 0 0 3 4 2 6 4 3 6 4 4 1 3 18 SL 107 40 2 62 172 4 4 7 0 3 0 3 1 1 11 3 0 NI RI AM 2 3 3 0 1 3 2 7 7 4 7 8 11 11 8 7 9 9 2 3 4 4 1 0 5 0 3 1 4 7 7 2 3 3 0 0 2 1 1 1 0 0 0 2 1 0 0 0 0 0 2 E.2 Model Performance on Different Domains Table 5: Model performance across condensed matter physics domains (normalized scores, two decimal places). Abbreviations: All = SEED of all problems, Mag = Magnetism, SC = Superconductivity, SCS = Strongly Correlated Systems, Semi = Semiconductors, Theory = Theoretical Foundations, Others = Others. Blue = highest, Purple = second highest in each column. Model QwQ-32B DeepSeek-R1-Distill-Qwen-32B Llama-3.1-70B-Instruct Qwen3-32B Claude 3.7 Sonnet Thinking Llama-3.3-70B-Instruct GPT-4o Gemini 2.0 Flash Thinking Claude 3.7 Sonnet Grok 3 Beta DeepSeek-V3 o1 o3-mini o4-mini DeepSeek-R1 Gemini 2.5 Pro o3 Grok 4 All 15.39 16.72 19.61 20.49 20.56 20.87 21.74 23.11 24.14 24.35 25.04 25.63 26.28 26.85 28.86 30.79 34.31 36.04 Mag 8.93 8.41 8.56 8.47 10.68 10.19 19.04 13.85 22.55 17.74 25.75 23.75 19.51 17.50 20.49 19.65 23.68 35.30 SC 8.75 12.65 9.30 15.65 22.38 13.08 18.90 13.47 19.13 26.34 29.67 26.02 27.67 27.63 28.88 25.86 35.77 43.42 SCS 26.29 20.12 29.63 17.25 24.53 24.25 29.03 11.15 34.93 26.74 9.25 28.42 19.08 22.32 42.16 26.67 37.34 20.49 Semi 14.97 12.30 19.05 16.30 13.65 17.68 11.58 26.66 13.61 16.26 15.30 12.72 14.40 18.33 22.50 29.18 27.80 21.01 Theory Others 22.23 24.01 27.24 35.47 33.44 30.10 28.95 29.82 30.05 34.37 27.73 32.39 35.63 38.13 37.10 40.50 38.99 41.21 17.56 24.31 27.92 25.32 23.77 29.58 28.42 28.85 31.93 28.39 31.62 33.78 36.72 34.49 34.18 37.44 46.42 45.79 E.3 CMPhysBench Problem and Responses Demo 51: Interacting Fields and Feynman Diagrams, Decay of scalar particle Problem: Decay of scalar particle. Consider the following Lagrangian, involving two real scalar fields Φ and ϕ : 1 = (µΦ)2 1 2 The last term is an interaction that allows Φ particle to decay into two ϕ s, provided that > 2m. Assuming that this condition is met, calculate the lifetime of the Φ to lowest order in µ. Final Answer: m2ϕ2 µΦϕϕ. (µϕ)2 2Φ2 + 1 2 1 2 τ = 8πM µ2 (1 4m2 2 )1/2 Gemini-2.5-pro Answer: The lifetime of particle is the inverse of its total decay rate Γ. The decay process is Φ ϕ + ϕ. We will calculate Γ to the lowest order in the coupling constant µ. Step 1: Determine the matrix element M. The Lagrangian density is given by: 1 2 1 2 1 2 = 2Φ2 + (µϕ)2 (µΦ)2 m2ϕ2 µΦϕϕ. 1 2 The interaction term responsible for the decay is Lint = µΦϕϕ. This is trilinear interaction. The Feynman rule for the vertex connecting one Φ particle and two ϕ particles is derived from this term. Since the two ϕ fields are identical, there is symmetry factor of 2! = 2 associated with permutations of these fields if they were externally labelled, or equivalently, one considers the derivative of the interaction term with respect to the fields. For an interaction term of the form gabc ! ΦaΦbΦc where is the number of identical fields of certain type, the vertex factor is igabc. Our interaction is µΦϕ2. This can be written as 2µ 2! Φϕ2. Thus, the vertex factor is i(2µ). The matrix element for the decay Φ ϕ + ϕ is therefore 2µ (by convention, the is often factored out). So, M2 = (2µ)2 = 4µ2. 19 Step 2: Write down the formula for the decay rate Γ. For particle of mass decaying from its rest frame into two final state particles with momenta p1 and p2, the decay rate is given by: Γ = (cid:90) 1 2M (cid:90) d3p1 (2π)32E1 d3p2 (2π)32E2 (2π)4δ(4)(P p1 p2)M2S Here, = (M,0) is the four-momentum of the initial Φ particle. p1 = (E1, p1) and p2 = (E2, p2) are the four-momenta of the two outgoing ϕ particles. E1 = (cid:112)p12 + m2 and E2 = (cid:112)p22 + m2. Since the two final state ϕ particles are identical, the statistical factor = 1 2! = 1 2 . Substituting M2 = 4µ2 and = 1/2: Γ = Γ = µ2 1 2M (cid:90) (cid:90) (4µ2) 1 2 d3p1 (2π)32E1 (cid:90) d3p1 (2π)32E1 d3p2 (2π)32E2 (cid:90) d3p2 (2π)32E2 (2π)4δ(4)(P p1 p2) (2π)4δ(M E1 E2)δ(3)(0 p1 p2) Step 3: Evaluate the phase space integral. Let be the phase space integral: (cid:90) (cid:90) d3p1 (2π)32E1 d3p2 (2π)32E = (2π)4δ(M E1 E2)δ(3)(p1 + p2) Integrate over p2 using the δ(3)(p1 + p2), which sets p2 = p1. This implies p2 = p1. Let pf = p1. Then E1 = E2 = Ef = (cid:113) p2 + m2. (cid:90) = d3p1 (2π)6(2Ef )(2Ef ) (2π)4δ(M 2Ef ) = 1 (2π)2 (cid:90) d3p1 4E2 δ(M 2Ef ) Convert d3p1 to spherical coordinates: d3p1 = p2 the integrand is spherically symmetric. dpf dΩ. The angular integration gives 4π as = 4π (2π)2 (cid:90) p2 dpf 4E2 δ(M 2Ef ) = 1 4π (cid:90) 0 p2 dpf E2 δ(M 2Ef ) (cid:113) (cid:113) + m2, which gives 2/4 = p2 p2 The delta function δ(M 2Ef ) constrains Ef = M/2. Since Ef = p2 + m2, we have = 2/4 m2. For pf to be M/2 = real, 2/4 > m2, i.e., > 2m, which is given. The magnitude of the final momentum is pf = (cid:112)M 2/4 m2 = 1 2 4m2. To evaluate the integral over dpf , we use the property (cid:82) g(x)δ(f (x))dx = (cid:80) g(xi) /E2 (xi) where (xi) = 0. Here = pf , g(pf ) = p2 , (cid:113) . At Ef = M/2, and (pf ) = 2Ef = 2 + m2. So, p2 2 + m2. (pf ) = 2 dEf p2 dpf M/2 = 4p . So the integral is p2 = 2 pf Ef 0/E2 (p0) = p2 0 0 (M/2)2 4p0 = pf = p0 = 1 2 4p2 = p0 . Substituting this into the expression for I: 0 2 4m2. (p0) = 2p0 4p0 p0 This is standard result for the two-body phase space factor. 2 4m2 2 1 4πM 1 4π = = = 2 4m2 8πM Step 4: Combine results to get Γ. Substitute the value of back into the expression for Γ: µ2 2 4m2 8πM µ2 Γ = = = µ2 2 4m2 8πM 2 Step 5: Calculate the lifetime τ . The lifetime τ is the inverse of the decay rate Γ: τ = 1 Γ = 8πM 2 2 4m2 µ2 The condition > 2m ensures that the term under the square root is positive, so the decay is kinematically allowed and the lifetime is real and positive. The calculation is to the lowest order in µ, which means considering only the tree-level diagram, resulting in Γ µ2. In natural units (ℏ = = 1), mass M, and coupling µ have Final check of units: units of energy [E]. The kinetic terms (µΦ)2 imply Φ has units [E]. The interaction term µΦϕϕ has units [µ][E][E][E]. So [µ] must be [E]. The decay rate Γ has units of [E] (inverse time). Our result for Γ: [µ]2 The Lagrangian density has units [E]4. [E]2 [M ]2[m]2 [M ] = [E]2 [E]2 [E]1 (time). Our result for τ : = [E]2[E] [E]2 = [E]. This is correct. The lifetime τ has units of [M ]2 [E]2[E] = [E]1. This is correct. [M ]2[m]2 = [E]2 [µ] The final expression for the lifetime τ is: τ = 8πM 2 2 4m2 µ2 SEED score: o3 Answer: Tree-level (lowest order in the coupling mu) 1. Vertex rule From the interaction term µ Φϕϕ the (constant) Feynman amplitude for Φ ϕϕ is µ , so that M2 = µ2 . 2. Twobody phasespace factor For particle of mass decaying at rest into two identical particles of mass m, Lorentzinvariant phase space gives the (cid:90) dPS2 = (cid:114) 1 1 8π 4m2 2 . Because the two final particles are identical, an extra factor 1/2! is required. 3. Decay probability per unit time (cid:16) probability per unit time (cid:17) = 1 2M (cid:17) (cid:16) 1 2! (cid:90) M2 dPS2 = (cid:114) 1 µ2 32πM 4m2 2 . 4. Lifetime The lifetime is the inverse of the above quantity, hence which is valid provided > 2m. µ 1 lifetime = 32π (cid:114) 4m2 2 SEED score: 77.5 Comments: The model answer contains mathematical error in calculating the decay rate: it shows factor of 32 instead of the correct factor of 8 present in the reference answer. The error likely arises from incorrect consideration of symmetry or phase space factors. Error Type: \"c\": Mathematical or logical mistakes DeepSeek-R1 Answer: The lifetime of the Φ particle decaying into two Φ particles is calculated using the decay rate Γ, which is determined by the interaction term in the Lagrangian. The decay rate to lowest order in µ is given by: The lifetime τ is the inverse of the decay rate: µ2 Γ = 2 4m2 32πM 2 Final Answer: τ = 1 Γ = 32πM 2 2 4m2 µ τ = 32πM 2 2 4m2 µ2 SEED score: 77.5 Comments: The models answer is mathematically incorrect. When the ground truth expression is simplified, it becomes τ = 8πM 2 2 4m2 µ2 , which differs from the models answer by factor of 4 in the numerical coefficient (32 in the model vs. 8 in the ground truth). Error Type: \"c\": Mathematical or logical mistakes DeepSeek-V3 Answer: To calculate the lifetime of the Φ particle decaying into two ϕ particles, well follow these steps: 1. Identify the Interaction Term: The interaction term in the Lagrangian is µΦϕϕ, which allows the decay Φ ϕϕ. 2. Compute the Decay Amplitude: The Feynman rule for the vertex Φϕϕ is iµ (since the interaction term is µΦϕϕ, the vertex factor is iµ). 3. Phase Space Calculation: The decay rate Γ for Φ ϕϕ in the rest frame of Φ is given by: Γ = (cid:90) 1 2M d3p1 (2π)32E1 d3p2 (2π)32E2 (2π)4δ4(P p1 p2)M2, where M2 is the squared matrix element for the decay. Matrix Element Squared: For the decay Φ ϕϕ, the matrix element is = iµ. Thus, M2 = µ2. Phase Space Integral: The phase space integral for two-body decay with identical particles (symmetry factor 1/2!) gives: Γ = (cid:19) (cid:90) 1 2M (cid:18) 1 2 d3p1d3p2 (2π)24E1E2 (2π)4δ(M E1 E2)δ3(p1 + p2)µ2. Integrating over p2 and using p1 = p2, we have: Γ = µ2 16πM (cid:90) d3p1 E2 1 δ(M 2E1). Using E1 = (cid:112)m2 + p12, and d3p1 = 4πp12dp1, the delta function enforces: 2E1 = = p1 = (cid:112) 2 4m2. 1 2 22 Evaluating the integral: Γ = µ2 16πM (cid:18) 4π p1 4E2 1 Since E1 = M/2 and p1 = 1 2 4m2, we get: (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)E1=M/2 . 4. Final Decay Rate: Simplifying, the decay rate is: Γ = µ2 16πM (cid:32) π (cid:114) 1 (cid:33) . 4m2 2 Γ = (cid:114) 1 µ2 16πM 4m2 2 . 5. Lifetime Calculation: The lifetime τ of the Φ particle is the inverse of the decay rate: τ = 1 Γ = 16πM (cid:113) 1 4m2 . µ2 The final result for the lifetime of the Φ particle is: τ = 16πM (cid:113) 1 4m2 µ2 µ2 (1 4m2 SEED score: 55 Comments: The models answer for the lifetime τ is twice the correct value. The correct expression is 2 )1/2. The models answer has numerator of 16πM instead of 8πM . This τ = 8πM discrepancy by factor of 2 could arise from conceptual misunderstanding (e.g., incorrect matrix element M2 or symmetry factor) or mathematical error in the phase space integration. Error Type: \"a\": Conceptual or physical model errors \"c\": Mathematical or logical mistakes Grok-4-0709 Answer: SEED score: 100 8πM 2 2 4m2 µ2 228: Movement of electrons in crystal in electric and magnetic fields Problem: atoms form two-dimensional square lattice, with each atom contributing one electron to form two-dimensional free electron gas. The electron energy expression is Derive the formula for the density of states of two-dimensional free gas. E(k) = ℏ2k2 2m + ℏ2k2 2m 23 Final Answer: g(E) = πℏ2 DeepSeek-R1 Answer: To derive the density of states (DOS) formula for two-dimensional free electron gas, we first consider an annulus in k-space corresponding to iso-energy contours. The energy expression is: = ℏ2(k2 + k2 y) 2m = ℏ2k2 2m , + k2 where = (cid:112)k2 y. In k-space, the iso-energy contour is circle of radius k. The area of the annulus corresponding to energies from to + dE (i.e., between radii and + dk) is 2πk dk. The number of allowed k-states per unit area of k-space is (L/(2π))2, where is the linear dimension of the system (and = L2 is the real-space area). The number of states dN within the k-space annulus 2πk dk, per unit real-space area, is: dN = 1 L2 (cid:18) 2π (cid:19)2 2πk dk = dk 2π . From = ℏ2k2/(2m), differentiation yields dk = for the number of states per unit real-space area, we obtain the density of states g(E) = dN ℏ2k dE. Substituting this into the expression AdE : g(E) = = = = (cid:19) (cid:18) dk 2π dk dE 1 dE 2π 2π 2πℏ2 . ℏ2k The final result for the density of states is: g(E) = 2πℏ2 SEED score: 49 Comments: The models answer for the 2D density of states, g(E) = 2πℏ2 , is incorrect because it is missing the spin degeneracy factor of 2, or rather, it has an extra factor of 1/2. The correct density of states for 2D free electron gas, including spin, is g(E) = πℏ2 . Error Type: \"a\": Conceptual or physical model errors DeepSeek-V3 Answer: To derive the density of states g(E) for two-dimensional free electron gas, follow the steps below: 1. Energy dispersion relation: The energy of an electron is given by E(k) = ℏ2(k2 + k2 y) 2m = ℏ2k2 2m where = (cid:112)k2 + k2 y. 2. Number of states in wavevector space: In two-dimensional k-space, the area per state is (2π/L)2, where is the side length of the 2D lattice. For system composed of atoms, the total k-space area is determined by circle with 24 radius from 0 to kF (kF is the Fermi wavevector). The number of states is Number of states = (cid:19)2 (cid:18) 2π πk2 because the area in k-space is πk2. 3. Relation between total number of electrons and energy: Each state can accommodate 2 electrons (spin up and down), so the total number of electrons satisfies: = 2 (cid:19)2 (cid:18) 2π πk2 From this, we find the Fermi wavevector kF as or kF = 2πn, where = N/L2 is the areal electron density. k2 = 2πN L2 4. Definition of density of states: The density of states g(E) is the number of states per unit energy interval (including spin degeneracy), i.e., g(E) = dN dE From steps 2 and 3, we obtain the relation between number of states and energy E: (E) = 2 (cid:19)2 (cid:18) 2π πk Substituting k2 = 2mE ℏ2 , we get (E) = 2 (cid:19)2 (cid:18) 2π (cid:19) (cid:18) 2mE ℏ2 = π mL2 πℏ2 5. Differentiate to obtain the density of states: Differentiating (E) gives g(E): g(E) = dN dE = mL2 πℏ2 This is the total density of states of the system. The density of states per unit area is: g(E) = g(E) L2 = πℏ2 6. Final result: The density of states for two-dimensional free electron gas is constant, independent of energy E: g(E) = πℏ2 SEED score: 100 Grok-4-0709 Answer: The density of states per unit area without magnetic field is g(E). In the presence of magnetic field B, the energy levels condense into Landau levels with energy spacing E. The number of states per unit area in an energy interval without magnetic field is g(E)E. 25 These states condense into one Landau level, so the degeneracy per unit area is = g(E)E. = g(E)E SEED score: 0 Comments: The models answer incorrectly applies the concept of degeneracy, as it relies on the density of states and energy interval, without addressing how the Landau level degeneracy is determined by the magnetic field. The correct expression for Landau level degeneracy depends on magnetic flux through the lattice area, yielding the given formula:D = eB πℏ . Error Type: \"a\": Conceptual or physical model errors 116: Electrostatics of Dielectrics, Thermodynamic relations for dielectrics in an electric field Problem: Determine the height by which the liquid surface inside vertical parallel-plate capacitor rises. Final Answer: = ε 1 8πρg E2 gemini-2.0-flash-thinking Answer: Heres step-by-step solution to determine the height by which the liquid rises inside the capacitor, using only the provided symbols: 1. Energy considerations: The free energy change, , must account for the gravitational potential energy increase due to the liquid rising and the reduction in electrostatic energy due to the introduction of the dielectric. 2. Gravitational Potential Energy: The increase in gravitational potential energy per unit area of the capacitor plates is ρgh(h/2) = 1 2 ρgh2. 3. Electrostatic Energy: The electric field is without the liquid. When the liquid of dielectric constant ε is present, it displaces air (dielectric constant = 1). The energy density reduction is proportional to (ε 1)E2. Integrating this energy density reduction over the volume the liquid occupies, we 2 (ε 1)ϵ0E2h. get the reduction in electrostatic energy per unit area of the capacitor plates as 1 However, we can express this as 1 2 (ε 1)DEh where = ϵ0E is the electric displacement. 2 (ε1)ϵ0E2h Furthermore, because the field is held constant here using voltage source, we have 1 4. Free Energy Minimization: The system will adjust to minimize the free energy . This means the change in free energy due to small change in must be zero, i.e., h = 0. Therefore, we are balancing the gain in gravitational potential energy with the loss of electrostatic energy. 5. Balancing Energies: We set the change in gravitational potential energy equal to the *negative* of the change in electrostatic energy (per unit area) when the liquid rises to height h: 2 ρgh2 = 1 2 (ε 1)ϵ0E2h . However, ϵ0 is not in our permitted variables so we have to remove 1 it. Since is constant, and the parallel plate capacitor can be expressed as = /d, where is the separation between the plates and is the voltage. The capacitance is = ϵA/d. So ϵ represents relative permittivity which is ε = ϵ ϵ0 2 ρgh2 = 1 Then we can get 1 ε E2h 2 ρgh2 = 1 1 so ϵ0 = ϵ ε . 2 (ε 1)ϵ0E2h, which becomes 2 (ε 1) ϵ 6. Solving for h: We solve for by dividing both sides by h/2, we have: 26 7. Final Expression: ρgh = (ε 1) E2 ϵ ε = (ε 1)E2ϵ ρgε = (ε 1)E2ϵ ρgε SEED score: 36.5 Comments: The models derivation is fundamentally incorrect as it starts with dimensionally inconsistent and physically flawed expressions for both gravitational potential energy and electrostatic energy. It also demonstrates severe notational confusion, using the symbol ϵ for both relative and absolute permittivity, which culminates in nonsensical final equation. Error Type: \"a\": Conceptual or physical model errors Gemini-2.5-pro Answer: Here is step-by-step derivation to determine the height h. The liquid rises to height where the total free energy of the system, , is at minimum. This equilibrium condition is found by setting the derivative of the total free energy with respect to the height to zero. The total free energy is sum of the electrical free energy and the gravitational potential energy. Step 1: Gravitational Potential Energy As volume of liquid rises into the capacitor, its gravitational potential energy increases. Lets consider the cross-sectional area of the capacitor gap to be Acs. The volume of the raised liquid is = Acsh. The mass of this liquid is = ρV = ρAcsh. The center of mass of this raised liquid column is at height of h/2. The increase in gravitational potential energy, Ug, is therefore: Ug = mg 2 = (ρAcsh)g 2 = 1 2 ρgAcsh2 Step 2: Electrical Free Energy The problem specifies that is the relevant free energy, which is minimized when the potential across the plates is held constant. When the liquid with dielectric constant ε rises, it displaces the air (with εair 1). This changes the electrical energy stored in the capacitor. The problems use of the symbol π and the absence of the permittivity of free space ε0 indicates that Gaussian units are appropriate. In these units, the energy density of an electric field in linear dielectric is = ε The energy density in the air-filled region is uair = 1 region is uliquid = ε When the liquid rises by height h, volume = Acsh previously filled with air is now filled with the liquid. The change in the stored electrical energy, Uelec, in this volume is: Uelec = (uliquid uair)V = (cid:0) ε For system at constant potential, the change in the relevant free energy, Felec, is the negative of the change in the stored energy: Felec = Uelec = ε1 8π E2. The energy density in the liquid-filled 8π E2(cid:1) Acsh = ε1 8π E2Acsh 8π E2 1 8π E2. 8π E2. 8π E2Acsh Step 3: Total Free Energy and Minimization The total change in the free energy of the system as function of is the sum of the gravitational and electrical contributions: (h) = Ug + Felec = 1 To find the equilibrium height, we minimize this free energy by taking its derivative with 8π E2Acsh(cid:1) = 0 respect to and setting it to zero: 8π E2Acs = 0 ρgAcsh ε1 (cid:0) 1 2 ρgAcsh2 ε 2 ρgAcsh2 ε1 8π E2Acsh = 0 dh d( ) dh Step 4: Solve for The cross-sectional area Acs is common factor and cancels out. ρgh = (ε1)E2 Solving for the height h: = (ε1)E2 8π 8πρg 27 This final expression for uses only the symbols provided in the problem statement. = (ε 1)E2 8πρg SEED score: DeepSeek-V3 Answer: To determine the height by which the liquid surface rises inside vertical parallel-plate capacitor, we analyze the balance between the gravitational potential energy and the electrostatic energy. 1. Gravitational Potential Energy: The work done against gravity to raise the liquid to height is given by the gravitational potential energy: Ugrav = ρgh2 1 2 (The factor of 1 2 arises because the center of mass of the raised liquid column is at height 2 .) 2. Electrostatic Energy: When the liquid rises, it displaces air between the capacitor plates. The change in electrostatic energy due to the presence of the dielectric liquid is: Uelec = 1 2 ε0(ε 1)E2h Here, ε0 is the permittivity of free space, and ε is the relative permittivity (dielectric constant) of the liquid. The negative sign indicates reduction in energy when the dielectric liquid is introduced. 3. Equilibrium Condition: The system reaches equilibrium when the total free energy = Ugrav + Uelec is minimized. Taking the derivative with respect to and setting it to zero: dh = ρgh 1 2 ε0(ε 1)E2 = 0 Solving for h: ε0(ε 1)E2 2ρg Thus, the height by which the liquid surface rises is: = = ε0(ε 1)E2 2ρg SEED score: 36.5 Comments: The model fundamentally misidentifies the energy involved. It calls specific energy term \"electrostatic energy,\" when that term actually represents the total energy change of the entire system, including the work done by the battery. This is critical distinction in physics. After making that initial conceptual mistake, the model performs all the subsequent maththe calculus and algebraperfectly. It followed the recipes instructions (the math) correctly, but it used the wrong ingredient (the physics concept). Error Type: \"a\": Conceptual or physical model errors \"c\": Mathematical or logical mistakes DeepSeek-R1 Answer: To determine the height by which the liquid surface rises inside the vertical parallel-plate capacitor is determined by minimizing the free energy of the system, which is minimized when the potential between the plates is constant. The relevant symbols are provided: h, , ρ, g, ε, π, and E. 28 The free energy includes the Gibbs free energy of the capacitor and the gravitational potential 2 CV 2, energy of the liquid. Under constant voltage, the Gibbs free energy for the capacitor is 1 2 ρgwdh2, where is the capacitance and is the voltage. The gravitational potential energy is 1 where is the width of the plates and is the plate separation. However, and cancel out in the minimization process. The capacitance depends on the height of the liquid. The electric field is constant and uniform between the plates due to the constant voltage and the orientation of the dielectric interface. Specifically, = /d, and this field magnitude remains unchanged regardless of the liquid height because the voltage is constant. In Gaussian units, the expression for is derived by minimizing with respect to h, leading to: = E2(ε 1) 8πρg This result is consistent with the force balance approach, where the electrostatic force per unit area at the liquid-air interface balances the hydrostatic pressure due to the raised liquid column. The symbols used are: (electric field magnitude in air when no liquid is raised), ε (dielectric constant of the liquid, relative permittivity), ρ (density of the liquid), (acceleration due to gravity), and π (mathematical constant). The factor 8π arises from the use of Gaussian units in the formulation. Thus, the height is given by the above expression. = E2 (ε 1) 8πρg = (ε 1)E2 8πρg SEED score: 100 Grok-4-0709 Answer: SEED score:"
        }
    ],
    "affiliations": [
        "Beijing National Laboratory for Condensed Matter Physics and Institute of Physics, Chinese Academy of Sciences",
        "Condensed Matter Physics Data Center, Chinese Academy of Sciences",
        "Fudan University",
        "Hong Kong Polytechnic University",
        "Hong Kong University of Science and Technology",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "Shanghai Artificial Intelligence Laboratory",
        "Tongji University"
    ]
}