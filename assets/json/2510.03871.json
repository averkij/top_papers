{
    "paper_title": "Optimal Scaling Needs Optimal Norm",
    "authors": [
        "Oleg Filatov",
        "Jiangtao Wang",
        "Jan Ebert",
        "Stefan Kesselheim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent progress in optimal hyperparameter transfer under model and dataset scaling, no unifying explanatory principle has been established. Using the Scion optimizer, we discover that joint optimal scaling across model and dataset sizes is governed by a single invariant: the operator norm of the output layer. Across models with up to 1.3B parameters trained on up to 138B tokens, the optimal learning rate/batch size pair $(\\eta^{\\ast}, B^{\\ast})$ consistently has the same operator norm value - a phenomenon we term norm transfer. This constant norm condition is necessary but not sufficient: while for each dataset size, multiple $(\\eta, B)$ reach the optimal norm, only a unique $(\\eta^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient condition, we provide the first measurement of $(\\eta^{\\ast}, B^{\\ast})$ scaling with dataset size for Scion, and find that the scaling rules are consistent with those of the Adam optimizer. Tuning per-layer-group learning rates also improves model performance, with the output layer being the most sensitive and hidden layers benefiting from lower learning rates. We provide practical insights on norm-guided optimal scaling and release our Distributed Scion (Disco) implementation with logs from over two thousand runs to support research on LLM training dynamics at scale."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review."
        },
        {
            "title": "OPTIMAL SCALING NEEDS OPTIMAL NORM",
            "content": "Oleg Filatov, Jiangtao Wang, Jan Ebert, Stefan Kesselheim Julich Supercomputing Centre Forschungszentrum Julich {o.filatov,jian.wang,ja.ebert,s.kesselheim}@fz-juelich.de"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite recent progress in optimal hyperparameter transfer under model and dataset scaling, no unifying explanatory principle has been established. Using the Scion optimizer, we discover that joint optimal scaling across model and dataset sizes is governed by single invariant: the operator norm of the output layer. Across models with up to 1.3B parameters trained on up to 138B tokens, the optimal learning rate/batch size pair (η, B) consistently has the same operator norm value phenomenon we term norm transfer. This constant norm condition is necessary but not sufficient: while for each dataset size, multiple (η, B) reach the optimal norm, only unique (η, B) achieves the best loss. As sufficient condition, we provide the first measurement of (η, B) scaling with dataset size for Scion, and find that the scaling rules are consistent with those of the Adam optimizer. Tuning per-layer-group learning rates also improves model performance, with the output layer being the most sensitive and hidden layers benefiting from lower learning rates. We provide practical insights on norm-guided optimal scaling and release our Distributed Scion (Disco) implementation with logs from over two thousand runs to support research on LLM training dynamics at scale. SDLAML/disco sdlaml-llm/norm-transfer"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in the domain of Large Language Models (LLMs) have been largely driven by the principle of scale. Increasing model size and training dataset volume consistently yields more capable systems (Hoffmann et al., 2022; Kaplan et al., 2020), yet at an increasing computational cost. Consequently, achieving optimal scaling training regime where hyperparameters are optimally configured with growing scale becomes necessary step to push the model frontier further. To address the challenge of hyperparameter tuning, several powerful yet disparate methods have emerged. Theoretically grounded frameworks like Maximum Update Parametrization (µP) (Yang et al., 2022) help transfer optimal hyperparameters with model scaling. Meanwhile, empirical scaling laws (Li et al., 2025) provide rules of thumb for setting hyperparameters optimally when theory is absent, such as with dataset size scaling. Yet, these approaches often feel like pieces of puzzle, with unifying principle for scaling across both model and dataset dimensions remaining elusive. Recently, an emerging paradigm of norm-based optimization (Bernstein & Newhouse, 2024a; Pethick et al., 2025a) has offered new lens through which to view training dynamics: it reframes optimization as process that controls the operator norms of the models weight matrices and gradient updates. This perspective enables monitoring of model properties during training, potentially revealing insights deeper than the loss curve alone. This raises natural question: can the normbased perspective shed light on how to unify optimal model and dataset size scaling? In this work, we argue that the answer is yes. By tracking and analyzing layer norms across thousands of experiments, we have made several discoveries, summarized below: 5 2 0 2 ] . [ 1 1 7 8 3 0 . 0 1 5 2 : r Unifying invariant for optimal scaling. layer WoutRMS (see Definition 2) for the optimal learning rate (η) and batch size (B) configuration has the same value in other words, is invariant or transfers with The operator norm of the output 1 Preprint. Under review. both model scaling (in width and depth) and dataset scaling  (Fig. 2)  . We refer to this phenomenon as norm transfer, and it provides necessary condition for optimality. However, it is not sufficient, as multiple non-optimal (η, B) pairs can reach the same optimal norm value (Fig. 3a). Scaling rules for the Scion optimizer. As sufficient condition for optimality, we empirically measure the relationship between optimal learning rate η, batch size B, and dataset size D. The result is η(B, D) B0.62 D0.56, matching the known square-root scaling rules for the Adam optimizer. We further find that the optimal batch size scales as B(D) D0.450.07, leading to η(D) D0.280.07. For fixed D, one can trade off η via the η rule within low-sensitivity region around the optimal norm (Fig. 3b). While the model performance is insensitive to this change, this freedom can be of computational advantage, allowing for training with larger batch sizes. Optimal per-layer-group learning rate. Performance can be improved by up to 6% in relative loss through additional per-layer-group tuning. We observe that learning rate ratio ηinput : ηhidden : ηoutput = 1 : 1/8 : 1 is consistently optimal across dataset sizes and batch sizes  (Fig. 4)  . We also find the uniform 1 : 1 : 1 layout to be close to the optimal one. Among layer groups, the output layer is the most sensitive to tuning, with sensitivity decreasing gradually for the hidden layers and then the input layer. Distributed Scion/Muon and experimental logs. To facilitate further research on large-scale training dynamics, we release Disco1, distributed implementation of the Scion/Muon optimizer compatible with modern parallelization strategies, along with norm logs2 from over two thousand training runs conducted for this study."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "2.1 BACKGROUND & TERMINOLOGY Recently, fundamental shift in the field of optimal scaling occurred with the work of Yang et al. (2024). It changed the focus from model parametrizations towards the norm perspective by showing that Maximum Update Parametrization (µP ) (Yang et al., 2022) can be derived from more fundamental principle: enforcing spectral condition on the model weights and their updates during the training. We briefly explain the idea behind each of them below. µP introduces theoretically grounded scaling rules for hyperparameters as function of model width in order to ensure maximal feature learning in the infinite width limit. This way, the model is guaranteed to learn meaningful features while remaining stable as one scales up its size. As an important by-product, it was found that models with different widths, once parameterized within µP , all share the same optimal hyperparameters (e.g. learning rate) therefore allowing for what is known as zero-shot hyperparameter transfer. This property has been extensively used for the past years to ensure optimal model scaling by tuning hyperparameters for small (proxy) model, and then effortlessly transferring them to larger one (OpenAI et al., 2024; Gunter et al., 2024; Dey et al., 2024; Meta AI, 2025; Zuo et al., 2025). In turn, the spectral condition specifies bounds on the norms of weights and weight updates that are necessary to ensure feature learning. More formally: Definition 1 (Spectral condition). Consider applying gradient update Wℓ Rdℓ ℓth weight matrix Wℓ Rdℓ should satisfy in to the in for layer ℓ = 1, . . . , L. The spectral norms of these matrices outdℓ outdℓ (cid:32)(cid:115) Wℓ = Θ (cid:33) dℓ out dℓ in and Wℓ = Θ (cid:32)(cid:115) (cid:33) , dℓ out dℓ in (1) where is the spectral norm, also equal to the largest singular value of , and xRMS = d. The symbol Θ is employed following the Big-O notation, indicating scaling behaviour x2/ 1https://github.com/SDLAML/disco 2https://wandb.ai/sdlaml-llm/norm-transfer/reports/ Norm-Transfer--VmlldzoxNDYwNjE2Mw 2 Preprint. Under review. (in this case, constant3) w.r.t. infinite width limit +. If conditions in Definition 1 are met, the zero-shot hyperparameter transfer is guaranteed and the model is being trained in the µP regime. Let us rewrite Definition 1 in more natural way as: WℓRMSRMS = Θ(1) and WℓRMSRMS = Θ(1), (2) where we follow Large et al. (2024) and introduce the core concept of this work: Definition 2 (Induced operator norm4). Given matrix Rdoutdin and two normed vector spaces (Rdin, α) and (Rdout , β), the α to β induced operator norm is given by: (3) (4) αβ = max xRdin xβ xα . The operator norms we are most interested in will be: 1RMS := maxj colj(W )RMS , RMSRMS := (cid:112)din/dout , (5) RMS := maxi din rowi(W )RMS , (6) where rowi(.) and colj(.) denote the i-th row and j-th column of matrix. In order to control the operator norms, Bernstein & Newhouse (2024a) derived duality maps, i.e. transformation rules of the gradients induced by given norm. Applying these transformations not only keeps the gradient updates within the required bound (e.g. Eq. 2), but also ensures the steepest descent under the chosen norm (Bernstein & Newhouse, 2024b). For the norms in Eq. 46, the corresponding duality maps for the gradient with singular value decomposition (SVD) = ΣV are: .1RMS : colj(G) (cid:55) colj(G) colj(G)RMS .RMSRMS : (cid:55) (cid:113) dout din .RMS : rowi(G) (cid:55) 1 din rowi(G) rowi(G)RMS (7) (8) (9) where the .RMS norm was added by Pethick et al. (2025a). Moreover, they wrapped the normbased approach outlined above into Scion optimizer. Within Scion, one has to assign an operator norm to each layer, e.g. out of those in Eq. 46. The corresponding duality maps determine how raw gradients should be transformed for those layers before the optimizer updates the weights. For simplicity, layers are typically grouped as input, hidden, and output, and norms are assigned to these groups. Importantly, model weights are not explicitly transformed within Scion; only the weight updates are, via duality maps. One prominent example of the norm-based view on model optimization is the Muon optimizer (Jordan et al., 2024), which proved to outperform Adam at scale (Liu et al., 2025; Wang et al., 2025) and showed great performance for models up to 1T parameters (Team et al., 2025). Muon can be viewed as specific instantiation of Scion: it optimizes hidden layers under .RMSRMS assumption, and uses Adam for the remaining parameters. However, only in the case with no exponential moving average does Adam coincide with the steepest descent in max-of-max norm (Bernstein & Newhouse, 2024b). Since this is uncommon in practice, no natural norm applies, making Muon hard to analyze through the norm lens. By contrast, Scion naturally incorporates the norm perspective, updating every layer with an assigned, layer-specific norm, making it easy to interpret. In practice, using norm-based optimizers as of now looks like free lunch: they require only one momentum buffer5 (compared to two for Adam), result in better performance with almost no computational overhead in large-scale distributed scenarios, and by design have zero-shot hyperparameter transfer built in. Moreover, the norm-based approach provides more insights into the dynamics of the model training: optimizer-assigned norms can be used naturally to monitor the training dynamics on per-layer basis. This observation leads us to discoveries that we describe in Sec. 3. 3Formally, (x) = Θ(g(x)) if there are constants A, > 0 such that g(x) (x) g(x). 4In the following we will omit induced operator for simplicity. 5Or even none, see ScionLight (Pethick et al., 2025a). 3 Preprint. Under review."
        },
        {
            "title": "2.2 TRAINING SETUP",
            "content": "In all experiments, we use the Llama 3 architecture (Grattafiori et al., 2024) and torchtitan training framework (Liang et al., 2025). Most of the experiments are performed on the small-scale proxy model with total size of 69M trainable parameters, including input/output embedding layers. For additional ablations in Sec. 3.2, we scale up the model up to 12 in width (to 1.3B parameters) and up to 32 in depth (to 168M). Notably, we employ norm-everywhere approach, inspired by the concept of well-normedness in Large et al. (2024) and the recent line of work (Loshchilov et al., 2025; Kim et al., 2025). Effectively, we ensure that the input to every Linear layer is normalized to xRMS = 1 by preceding RMSNorm layer without learnable parameters. More details on model configurations are provided in Appendix A.2 and Appendix A.3. As optimizer, we use Scion without weight decay (i.e. its unconstrained version) (Pethick et al., 2025a) without momentum and with the norm assumptions .1RMS .RMSRMS .RMS for input hidden output layers. Furthermore, we developed its distributed version, which natively integrates into torchtitan, supports FSDP/DDP/TP/EP/CP/PP strategies, and greatly speeds up the training at scale compared to the standard implementation. We make it openly available and provide more details in Appendix A.5. For pretraining, we use high-quality partition of the Nemotron-CC dataset (Su et al., 2025), Llama 3 tokenizer (Grattafiori et al., 2024) with vocabulary size of 128,256 (after padding) and context window of 4096. All the models are pretrained with the causal language modelling task. Unless stated otherwise, constant learning rate schedule without warmup and without decay is used. This allows us, for given set of hyperparameters, to perform single long run and evaluate progressively larger dataset sizes, rather than conducting several runs for each dataset individually, thereby substantially reducing computational costs (Hu et al., 2024; Hagele et al., 2024). 2.3 OPTIMAL NORM MEASUREMENT Our initial intuition was that for given model and data scale, there is always some optimal norm value, corresponding to some optimal hyperparameter choice. To establish this, we focus on the output layer with the Scion-assigned .RMS norm (hereafter referred to as output norm) as being the most natural layer to study.6 The choice of .RMS norm is motivated by Bernstein & Newhouse (2024a) as mapping from natural continuous RMS norm semantics for hidden model representations onto discrete vocabulary, although we also ablate this in Appendix A.11.2. Since by default we disable momentum and any regularization, we are only left with learning rate (η) and batch size (B) as hyperparameters to tune for optimality. To extract the optimal hyperparameter configuration and the corresponding optimal norm, we run an (η, B) grid search for given model and given pretraining dataset size (hereafter referred to as horizon D, measured in tokens), and evaluate the model performance with training loss (crossentropy of the next token prediction). Since we train in non-repeating infinite-data regime, training loss faithfully reflects model performance and its generalization. First, we examine how the optimal norm, associated to (η, B) configuration optimal for given horizon, changes as the horizon increases. Then, we fix the horizon and scale up the model in width and depth, repeating the same optimal norm measurement. This way, we study both model and dataset scaling directions. Practically, for every batch size we are interested in marginalising or profiling across learning rates, i.e. picking the optimal one and the corresponding output norm (see Appendix A.2 for details on the grid and random seed variations). However, an empirically lowest-loss point across the learning rate grid turned out to be statistically noisy estimate; therefore, for each batch size, we perform fit to the distribution of training loss vs. output norm across learning rates. Finally, we extract the optimal norm value from the fitted curve and the corresponding learning rate from the nearest data point to the fitted optimum. We provide more details on the fitting procedure in Appendix A.4. 6The output layer is invariant to both width and depth scaling, it is the most sensitive to learning rate tuning (Sec. 3.4), and it can be viewed as linear classifier on the learned hidden representations. These considerations make us believe that the output layer plays representative role for the entire model, thus making it distinct layer to analyse. 4 Preprint. Under review."
        },
        {
            "title": "3.1 OUTPUT NORM DYNAMICS",
            "content": "First, we describe how the output layer norm evolves depending on the hyperparameter settings. From learning rate scans, we observe that indeed there is an optimal norm value for given batch size and horizon (Fig. 1a). Furthermore, learning rate is positively correlated with the output norm: the higher the learning rate, the higher the norm. Since we use an unconstrained version of Scion, the norms generally grow with the number of gradient steps (Fig. 1b and Appendix A.6). However, we note that norm values can also be constrained during training with weight decay (see Appendix A.9) or with various spectral clipping techniques (Newhouse et al., 2025). Intriguingly, the norm growth is not linear in log-log scale but piecewise linear: with the slope abruptly changing for all batch sizes at the norm value of 26 27 and then at 29 210, where for the latter the dynamics enters the turbulence region. This slope change may have the same nature as recently observed phenomenon in the loss curve dynamics (Mircea et al., 2025). Last but not least, we observe that learning rate controls the offset of norm curves, and batch size controls the decoupling degree of curves: while early in training the curves of same η but different are identical, the slope change at 26 27 norm is more pronounced for larger batch sizes. Interestingly, after decoupling the curves seem to converge again to the same slope, that is lower than the initial one. (a) (b) Figure 1: (a) Interplay of training loss, output layer norm WoutRMS and learning rate. Results are for the proxy model (69M parameters), batch size = 128 samples and horizon = 233 tokens. Points are colored by log2(η) where η is the learning rate. Black dashed lines mark the optimal configuration with minimum training loss. (b) Growth of the output layer norm vs. gradient steps. Each curve corresponds to (learning rate η, batch size B) pair, with measured in samples; colour encodes batch size and line style encodes learning rate. See also the same plot vs. token horizons in Appendix A.6. 3.2 OPTIMAL NORM TRANSFER After analysing learning rate scans across batch sizes, horizons and models of varying width/depth, we visualise results in Fig. 2, with an extended set of plots in Appendix A.7 and Appendix A.13. Each data point corresponds to optimally tuned learning rate η for given batch size, minimising training loss for that horizon and model. We report our observations below, separately for each direction of scaling. Data scaling: After profiling across learning rates and plotting optimal norm against batch size, we observe that for given horizon there is single optimal batch size with the corresponding optimal output norm WoutRMS = 27.00.2. Intriguingly, this norm value transfers across horizons. We refer to this phenomenon as norm transfer: the optimal (η, B) configuration for given horizon must result in the optimal norm of 27. Also note that the optimal batch size grows with horizon scaling, which we discuss in Sec. 3.3. Model width scaling: It is expected to preserve the optimal norm by the design of our optimizer via the spectral condition (Eq. 1). Indeed, in Fig. 2b we observe that scaling up in width by factor 5 Preprint. Under review. (a) (b) Figure 2: Training loss vs. output layer norm across batch sizes. (a) Fixed proxy model (69M parameters) while increasing token horizon from 231 to 237. (b) Fixed token horizon 233 while scaling width/depth of the proxy model as indicated in the legend. Each batch size point (increasing from 32 in 2 steps, reflected by marker size) has its learning rate optimally tuned. The optimal batch size per horizon/model configuration is indicated by the filled marker. All curves share optimal norm at 7.0 0.2 across horizons and 7.4 0.2 across models (grey band). of 12 while keeping the horizon fixed results in the nested µP -style curves, sharing the same optimal norm while resulting in lower loss as we scale up. Model depth scaling: Although not obvious priori, we observe experimentally that scaling up in the number of layers by factor of 32 results in norm transfer. This is quite surprising, since we do not employ any of the established depth-transfer techniques (Bordelon et al., 2023; Yang et al., 2023; Dey et al., 2025). We ablate them in Appendix A.10 and find that in our setup they all induce learning rate transfer, but our strategy (no residual scaling factors, initialization rescaling of layers prior to residuals by 1/(cid:112)2Nlayers) results in the lowest loss. We speculate that this may be related to our norm-everywhere approach (Sec. 2.2) and uniformity in norm treatment by the optimizer and weight initialization. Additional ablations: In practice, one is interested in running Scion with non-zero momentum and with decaying learning rate schedule. We study the impact of these two options in Appendix A.11 and observe that they both show norm transfer. Notably, the addition of momentum largely reduces sensitivity to batch size choice with multiple values resulting in the same optimal norm and loss  (Fig. 9)  . The impact of learning rate decay is also important, as we find it greatly flattens the norm optimum and thus reduces sensitivity to learning rate choice (Fig. 13b). Last but not least, in Appendix A.11.2 we find that norm transfer phenomenon occurs not only in WoutRMS norm, but also in WoutRMSRMS and Win1RMS for the input embedding. Summary I: Within the Scion framework, optimal norm transfers in both model (via width and depth) and data scaling directions: it is necessary to choose the hyperparameter configuration so that the model output norm WoutRMS falls into the optimal region. Substituting alternative norms (WoutRMSRMS or Win1RMS) maintains the consistency of the transfer. The same behaviour holds with non-zero momentum and learning rate decay. 3.3 OPTIMAL (η, B) SCALING RULE Despite the discovered norm guidance, it is still not obvious how to select the corresponding optimal combination of learning rate and batch size for given horizon. Or more generally, what is the sufficient condition for optimality? In this Section, we explore this question. 6 Preprint. Under review. (a) (b) Figure 3: (a) (η, B) combinations that reach the optimal norm WoutRMS = 27.00.2 for given token horizon. Colours denote batch size (B); the y-axis is learning rate (η). Solid and dashed lines denote free and heuristic fits (described in text). (b) Optimal learning rate per batch size across horizons. Circled markers indicate optimal (η, B) with the lowest loss. Within horizon, marker transparency linearly interpolates between the lowestand highest-loss runs, with higher transparency indicating higher training loss. Error bars show systematic variation from the fitting method (Appendix A.4). Dashed lines are joint linear regression with log2 η log2 + log2 D. Fig. 3a illustrates that the optimal norm condition observed in Fig. 2 is necessary but not sufficient. For each token horizon (x-axis), we plot the learning rates (y-axis) and batch sizes (colour) that reach7 the optimal-norm region WoutRMS [26.8, 27.2]. One can observe that for given horizon, every batch size will reach optimal norm with sufficiently high learning rate. We fit the data with linear models log2 η = αfirst log2 + βfirst log2 Dfirst + γfirst (free fit) and log2 η = 1.5 log2 log2 Dfirst + γfirst (heuristic fit). For the free fit, we find the exponents αfirst = 1.32 0.03 and βfirst = 0.96 0.03, which are close to the values from the heuristic fit. Hence, we cannot rely on the output norm as guide to selecting optimal hyperparameters; it is only necessary and not sufficient condition. Let us now study sufficient conditions by first unfolding Fig. 2a and including optimal learning rate information that was profiled away. Specifically, we are interested in how the optimal learning rate η changes within fixed horizon with the batch size change, and then with horizons scaled up. Fig. 3b shows the corresponding data points along with linear regression fit log2 η(B, D) = α log2 +β log2 +γ. Note that only circled markers are per-horizon optima with the lowest loss. We observe several things: The coefficients of the fit α = 0.62 0.05, β = 0.56 0.05 are consistent with well-established square-root scaling with batch size (Malladi et al., 2024) and data horizon (Bjorck et al., 2025) for Adam, respectively. Similar to AI et al. (2025); Sato et al. (2025) we observe no surge phenomenon (Li et al., 2024), i.e. transition for fixed from η scaling rules for batch sizes higher than the critical one (Zhang et al., to η 1/ 2025). Theoretically, Jianlin (2025) explains this from the mean field theory perspective. Different batch sizes result in different losses, and for each horizon there is an optimal one B(D), as emphasized in Fig. 3b with circled markers and marker transparency for relative loss difference. The optimal batch size increases with horizon scaling: in Appendix A.8 we measure with extended set of horizons B(D) D0.450.07, which is consistent with Adam (Li et al., 2025; Bergsma et al., 2025) and intriguingly with D. Using B(D) D0.45 and log2 η(B, D) 0.62 log2 0.56 log2 with the corresponding uncertainties, we obtain for the optimal learning rate scaling η(D) 7Optimal norm will most likely be reached at some point (provided learning rate sweep resolution in Fig. 3a is too small), since in unconstrained Scion the weight norms are growing in time (see Sec. 3.1 and Fig. 1b). 7 Preprint. Under review. D0.280.07. This observation is consistent with Li et al. (2025) but appears to be in tension with Shen et al. (2024); Bergsma et al. (2025), albeit our methodologies are not fully comparable.8 Again, this is interestingly close to η(D) D1/4. Since there exists single optimal batch size for each data horizon, the number of devices usable for training is fundamentally capped: beyond point, increasing the number of devices either hurts throughput (small per-device microbatch size to keep the optimal global batch size) or degrades loss (leaving the optimal batch size region to keep throughput). This hints towards an interesting research direction: if this limit can be bypassed. In fact, for fixed horizon, it is not single optimal (η, B) but an optimal region (η η, B) that results in near-optimality (opacity in Fig. 3b). We relate this to the notion of learning rate sensitivity (Wortsman et al., 2023) that we rephrase as norm sensitivity. We think this region is defined by the flatness of the horizon curve (Fig. 2a) around the optimal norm value. Within this region, one can exchange learning rate for batch size via the η rule, thus allowing for some flexibility in optimal hyperparameter choice, e.g. training with larger batch sizes. Summary II: For Scion, we measure the following hyperparameter scaling rules inducing the sufficient optimal scaling condition: η(D) D0.280.07 and (10) consistent with the Adams scaling exponents. For fixed horizon D, one can trade off η via the η rule within the region of low norm sensitivity, without loss in performance. By Scions design, these observations hold true with model width scaling. B(D) D0.450.07, 3.4 OPTIMAL PER-LAYER-GROUP LEARNING RATE (a) (b) Figure 4: (a) Parallel-coordinates view of per-layer-group learning rate tuning. Results are for the proxy model (69M parameters) and batch size = 128 samples, averaged across random seeds as described in Appendix A.2. Dark gray lines are the top 10% runs (loss 4.114.18); light gray lines are the remainder (loss 4.194.76). Orange traces highlight the three best settings. The inset histogram shows the distribution of top 10% counts for each layer group. (b) Best learning rate layouts per training horizon under the constraint ηinput = ηoutput. Results are for the proxy model (69M parameters) and batch size = 512 samples. All horizons favor V-shaped layout with ηhidden smaller than the input/output learning rates by the same 1/8 factor. In the legend we also report loss for the optimal ηinput = ηhidden = ηoutput η layout (equal @η). So far, we approached scaling from global learning rate point of view. However, this may not be the case, and intricate dynamics can emerge where various layers require different learning rates at 8For example, because of weight decay usage in Bergsma et al. (2025), which significantly affects norm dynamics by constraining it, as we discuss in Sec. 5. 8 Preprint. Under review. different scales to be trained optimally, thus questioning our conclusions so far. In this Section, we explore if this is the case. Fig. 4a presents results for proxy model (69M parameters), fixed data horizon (8.6B tokens) and fixed batch size (B = 128 samples, optimal for this horizon) where we run grid search over learning rate values η {28, 27, . . . , 20} for input (token embedding), output (linear projection onto vocabulary) and hidden (all the other) layers, averaged across random seeds (Appendix A.2). We observe that there is little optimal learning rate imbalance across layer groups, and uniform learning rate assignment results in the same loss as the optimal configurations within uncertainties. Furthermore, from the width of the optimal nodes count histograms per layer groups, we conclude that the output layer is the most sensitive to learning rate mistuning, with the sensitivity progressively decreasing for hidden and then input layers. From analysing Fig. 4a and additional ones for different batch sizes (Appendix A.12) we found that the configuration ηinput : ηoutput : ηhidden = 1 : 1/8 : 1 is always among the top 10%. This symmetry simplifies the learning scan and notably contradicts the optimal configurations suggested in Pethick et al. (2025a) and Riabinin et al. (2025). To study dynamics with horizon scaling, we 9, for perform the learning rate grid scan same as in Fig. 4a but with constraining ηinput = ηoutput the proxy model with = 512. Fig. 4b illustrates the results, where we see the optimal hidden ratio (ηinput/ηhidden = 1/8) transfer across horizons, as well as that it brings loss improvement w.r.t. constant learning rate baseline. Lastly, we note that again, due to the optimizer design, we expect these observations to hold true under model width scaling. Summary III: Uniform learning rate configuration across layers is strong baseline, which still can be improved with additional hidden layer group tuning: ηinput : ηoutput : ηhidden = 1 : 1/8 : 1 yields relative loss improvement of up to 6% and is transferable across dataset sizes."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Hyperparameters with model scaling Yang et al. (2022) showed how to transfer optimal hyperparameters from small to large model in principled way via Maximal Update Parametrization (µP). Everett et al. (2024) later showed that such transfer is also possible in other parametrizations. Yang et al. (2023); Dey et al. (2025) extended the method towards model scaling in depth. Empirically, scaling laws on how to set optimal hyperparameters as function of compute (DeepSeek-AI et al., 2024), loss (Hu et al., 2024) or model size (Porian et al., 2025) were measured. Hyperparameters with data scaling Remains poorly understood theoretically: Smith & Le (2018) showed for SGD how to adjust learning rate and batch size by modelling optimization trajectory as stochastic differential equation (SDE). Largely, the problem has been approached experimentally by measuring hyperparameter scaling rules as function of the dataset size (Shen et al., 2024; Hu et al., 2024; Filatov et al., 2025; Bergsma et al., 2025; Li et al., 2025). (η, B) scaling rules Historically, studies of interaction between learning rate and batch size emerged as an experimental effort to scale batch size without losing performance (Keskar et al., 2017; Goyal et al., 2018; Hilton et al., 2022). Later, deeper understand has emerged from various theoretical angles: SDE (Malladi et al., 2024), loss curvature (McCandlish et al., 2018), random matrix theory (Granziol et al., 2021). Norm-based optimization Starting from the spectral condition (Yang et al., 2024), the approach of transforming gradient updates based on norm assumptions was fully established in Large et al. (2024); Bernstein & Newhouse (2024a), and recently explored in constraining weights themselves (Newhouse et al., 2025). The steepest descent view allowed for connections with manifold learning (Cesista, 2025) and optimizer design (Riabinin et al., 2025). This line of work has led to Muon (Jordan et al., 2024) and Scion (Pethick et al., 2025a;b), along with improvements (Ahn et al., 2025; Amsel et al., 2025), and benchmarks (Wen et al., 2025; Semenov et al., 2025) thereof. 9In terminology of Bernstein & Newhouse (2024a) this corresponds to mass tuning. Preprint. Under review."
        },
        {
            "title": "5 CONCLUSION AND DISCUSSION",
            "content": "In this work, we demonstrate that the operator norm of the output layer is powerful measure that guides joint optimal scaling across both model and dataset dimensions. Informally, one may view our results as: 1. (η, B, D) choice affects=== layer operator norm (Sec. 3.1) 2. optimal loss requires ==== optimal norm (Sec. 3.2) 3. optimal η(D), B(D) scaling rules yield == optimal loss (Sec. 3.3) In words, we empirically (1) study how norms evolve with hyperparameter change and how to tune them to desired values; (2) demonstrate that the optimal hyperparameter configuration must have predefined (output) layer norm in order to be transferable across data and model scales; (3) derive optimal hyperparameter scaling rules resulting in optimal loss. While we are confident that the scaling rules in Sec. 3.3 hold at even larger scales, we still dont know why they are induced in this form, very much resembling square-root and 1/4-power laws. Moreover, how do these rules connect with our main finding, necessary condition of scaling trajectory in (data, model) axes to have the same constant value or one might say, to remain on manifold (Bernstein, 2025). At this point more new questions arise: Why does optimal norm transfer? It is puzzling what makes the optimal scaling trajectory remain on the constant norm manifold, as well as what defines its structure. What is the reason behind optimal scaling rules? While we show how to set hyperparameters optimally, there is something missing in the norm perspective to explain it. Which norm is exactly optimal? We paid most of our attention to WoutRMS, but are the observed phenomena really specific to this one only? And to the Scion optimizer only? How can the constant norm condition be leveraged? It looks like naturally emerging inductive bias that one can take advantage of to optimize the training process. We dont yet have answers to those questions, but we believe our study scratches the surface of exciting phenomena that remain to be fully understood. ACKNOWLEDGMENTS We thank Kaiyue Wen and Ismail Khalfaoui Hassani for helpful discussions and feedback on the manuscript. This research was supported by TrustLLM funded by Horizon Europe GA 101135671, and by the Helmholtz Foundation Model Initiative as part of the Synergy Unit. The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by providing computing time on the GCS Supercomputer JUWELS at Julich Supercomputing Centre (JSC). Parts of computational resources were provided by the German AI service center WestAI."
        },
        {
            "title": "REFERENCES",
            "content": "Kwangjun Ahn, Byron Xu, Natalie Abreu, Ying Fan, Gagik Magakyan, Pratyusha Sharma, Zheng Zhan, and John Langford. Dion: Distributed orthonormalized updates. arXiv preprint arXiv:2504.05295, 2025. Essential AI, :, Ishaan Shah, Anthony M. Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh Shah, Khoi Nguyen, Kurt Smith, Michael Callahan, Michael Pust, Mohit Parmar, Peter Rushton, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, Somanshu Singla, Tim Romanski, Yash Vanjani, and Ashish Vaswani. Practical efficiency of muon for pretraining. arXiv preprint arXiv:2505.02222, 2025. Noah Amsel, David Persson, Christopher Musco, and Robert M. Gower. The polar express: Optimal matrix sign methods and their application to the muon algorithm, 2025. URL https: //arxiv.org/abs/2505.16932. 10 Preprint. Under review. Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, and Joel Hestness. Power lines: Scaling laws for weight decay and batch size in llm pre-training. arXiv preprint arXiv:2505.13738, 2025. Jeremy Bernstein. Modular manifolds. Thinking Machines Lab: Connectionism, 2025. doi: 10. 64434/tml.20250926. https://thinkingmachines.ai/blog/modular-manifolds/. Jeremy Bernstein and Laker Newhouse. Modular duality in deep learning. arXiv preprint arXiv:2410.21265, 2024a. Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology. arXiv preprint arXiv:2409.20325, 2024b. Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, and Xia Song. Scaling optimal lr across token horizons. arXiv preprint arXiv:2409.19913, 2025. Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise arXiv preprint hyperparameter transfer in residual networks: Dynamics and scaling limit. arXiv:2309.16620, 2023. Franz Louis Cesista. Muon and selective survey on Steepest Descent in Riemannian and non-Riemannian Manifolds, April 2025. URL http://leloykun.github.io/ponder/ steepest-descent-non-riemannian/. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Nolan Dey, Quentin Anthony, and Joel Hestness. The practitioners guide to the maximal update parameterization, Sep 2024. URL https://www.cerebras.ai/blog/ the-practitioners-guide-to-the-maximal-update-parameterization. Nolan Dey, Bin Claire Zhang, Lorenzo Noci, Mufan Li, Blake Bordelon, Shane Bergsma, Cengiz Pehlevan, Boris Hanin, and Joel Hestness. Dont be lazy: Completep enables compute-efficient deep transformers. arXiv preprint arXiv:2505.01618, 2025. Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey arXiv preprint Pennington. arXiv:2407.05872, 2024. Scaling exponents across parameterizations and optimizers. Wei Feng, Will Constable, and Yifan Mao. Getting started with fully sharded data parallel (fsdp2), 2025. URL https://docs.pytorch.org/tutorials/intermediate/ FSDP_tutorial.html. Oleg Filatov, Jan Ebert, Jiangtao Wang, and Stefan Kesselheim. Time transfer: On optimal learning rate and batch size in the infinite data limit. arXiv preprint arXiv:2410.05838, 2025. Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2018. 11 Preprint. Under review. Diego Granziol, Stefan Zohren, and Stephen Roberts. Learning rates as function of batch size: random matrix theory approach to neural network training. arXiv preprint arXiv:2006.09092, 2021. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, 12 Preprint. Under review. Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, 13 Preprint. Under review. Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075, 2024. Jacob Hilton, Karl Cobbe, and John Schulman. Batch size-invariance for policy optimization. arXiv preprint arXiv:2110.00641, 2022. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Alexander Hagele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi. Scaling laws and compute-optimal training beyond fixed training durations. arXiv preprint arXiv:2405.18392, 2024. Su Jianlin. Rethinking learning rate and batch size (part 3): Muon, Sep 2025. URL https: //kexue.fm/archives/11285. Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https: //kellerjordan.github.io/posts/muon/. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2017. Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, and Kang Min Yoo. Peri-ln: Revisiting normalization layer in the transformer architecture. arXiv preprint arXiv:2502.02732, 2025. Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, and Jeremy Bernstein. Scalable optimization in the modular norm. arXiv preprint arXiv:2405.14813, 2024. Houyi Li, Wenzhen Zheng, Qiufeng Wang, Hanshan Zhang, Zili Wang, Shijie Xuyang, Yuantao Fan, Zhenyu Ding, Haoying Wang, Ning Ding, Shuigeng Zhou, Xiangyu Zhang, and Daxin Jiang. Predictable scale: Part optimal hyperparameter scaling law in large language model pretraining. arXiv preprint arXiv:2503.04715, 2025. Shuaipeng Li, Penghao Zhao, Hailin Zhang, Xingwu Sun, Hao Wu, Dian Jiao, Weiyan Wang, Chengjun Liu, Zheng Fang, Jinbao Xue, Yangyu Tao, Bin Cui, and Di Wang. Surge phenomenon in optimal learning rate and batch size scaling. arXiv preprint arXiv:2405.14578, 2024. Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, et al. Torchtitan: One-stop pytorch native solution for production ready llm pretraining. In The Thirteenth International Conference on Learning Representations, 2025. 14 Preprint. Under review. Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, and Zhilin Yang. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982, 2025. Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, and Boris Ginsburg. ngpt: Normalized transformer with representation learning on the hypersphere. arXiv preprint arXiv:2410.01131, 2025. Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the sdes and scaling rules for adaptive gradient algorithms. arXiv preprint arXiv:2205.10287, 2024. Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018. Meta AI. Introducing llama 4: Advancing multimodal intelligence, 2025. URL https://ai. meta.com/blog/llama-4-multimodal-intelligence/. Andrei Mircea, Supriyo Chakraborty, Nima Chitsazan, Milind Naphade, Sambit Sahu, Irina Rish, and Ekaterina Lobacheva. Training dynamics underlying language model scaling laws: Loss deceleration and zero-sum learning. arXiv preprint arXiv:2506.05447, 2025. Laker Newhouse, R. Preston Hess, Franz Cesista, Andrii Zahorodnii, Jeremy Bernstein, and Phillip Isola. Training transformers with enforced lipschitz constants. arXiv preprint arXiv:2507.13338, 2025. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Preprint. Under review. Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2024. Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, and Volkan Cevher. Training deep learning models with norm-constrained lmos. arXiv preprint arXiv:2502.07529, 2025a. Thomas Pethick, Wanyun Xie, Mete Erdogan, Kimon Antonakopoulos, Tony Silveti-Falls, and Volkan Cevher. Generalized gradient norm clipping & non-euclidean (l0, l1)-smoothness. arXiv preprint arXiv:2506.01913, 2025b. Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, and Yair Carmon. Resolving discrepancies in compute-optimal scaling of language models. arXiv preprint arXiv:2406.19146, 2025. Artem Riabinin, Egor Shulgin, Kaja Gruntkowska, and Peter Richtarik. Gluon: Making muon & scion great again! (bridging theory and practice of lmo-based optimizers for llms). arXiv preprint arXiv:2505.13416, 2025. Naoki Sato, Hiroki Naganuma, and Hideaki Iiduka. Convergence bound and critical batch size of muon optimizer. arXiv preprint arXiv:2507.01598, 2025. Andrei Semenov, Matteo Pagliardini, and Martin Jaggi. Benchmarking optimizers for large language model pretraining. arXiv preprint arXiv:2509.01440, 2025. Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, and Rameswar Panda. Power scheduler: batch size and token number agnostic learning rate scheduler. arXiv preprint arXiv:2408.13359, 2024. Samuel L. Smith and Quoc V. Le. bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451, 2018. Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2025. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Preprint. Under review. Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Shuche Wang, Fengzhuo Zhang, Jiaxiang Li, Cunxiao Du, Chao Du, Tianyu Pang, Zhuoran Yang, Mingyi Hong, and Vincent YF Tan. Muon outperforms adam in tail-end associative memory learning. arXiv preprint arXiv:2509.26030, 2025. Kaiyue Wen, David Hall, Tengyu Ma, and Percy Liang. Fantastic pretraining optimizers and where to find them. arXiv preprint arXiv:2509.02046, 2025. Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D. CoReyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023. Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning in infinite-depth neural networks. arXiv preprint arXiv:2310.02244, 2023. Greg Yang, James B. Simon, and Jeremy Bernstein. spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2024. Hanlin Zhang, Depen Morwani, Nikhil Vyas, Jingfeng Wu, Difan Zou, Udaya Ghai, Dean FosarXiv preprint ter, and Sham Kakade. How does critical batch size scale in pre-training? arXiv:2410.21676, 2025. Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, Mugariya Farooq, Giulia Campesan, Ruxandra Cojocaru, Yasser Djilali, Shi Hu, Iheb Chaabane, Puneesh Khanna, Mohamed El Amine Seddik, Ngoc Dung Huynh, Phuc Le Khac, Leen AlQadi, Billel Mokeddem, Mohamed Chami, Abdalgader Abubaker, Mikhail Lubinets, Kacper Piskorski, and Slim Frikha. Falcon-h1: family of hybrid-head language models redefining efficiency and performance. arXiv preprint arXiv:2507.22448, 2025. 17 Preprint. Under review."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 LLM USAGE LLMs were used solely to aid in polishing the writing and improving the clarity of exposition. In addition, code-assistant tools were occasionally used for minor programming support, such as code completion and syntax suggestions; they were not employed to design algorithms, generate experiments, or implement the proposed methods from scratch. A.2 MODEL TRAINING CONFIGURATION Proxy model, 69M parameters: 4 hidden layers with dmodel = 256, Multi-Head Attention with nheads = 4 and nkvheads = 4, SwiGLU activation function with MLP expansion factor fMLP = 2.75, RoPE with θ = 10000 (Su et al., 2024), Llama 3 tokenizer with vocabulary size of 128 256 (after padding) (Grattafiori et al., 2024), input and output embedding layers are not tied. 4(12) wider model, 314M (1.3B) parameters: same as proxy, except dmodel = 1024 (3072). In width scaling, we keep fixed dhead = 64 and scale the number of heads accordingly. 8(32) deeper model, 91M (168M) parameters: same as proxy, except 32 (128) hidden layers. Semi-orthogonal initialization for hidden linear layers and row-wise normalized Gaussian initialization for input/output embedding layers (Pethick et al., 2025a). Initialisation of the last layer of both MLP and attention blocks (those with the output being added with the residual stream) is multiplied by 1/(cid:112)2Nlayers. Dropout disabled, no biases in all Linear layers, no weight sharing between input and output embedding layers. norm-everywhere: normalise input to every Linear layer via RMSNorm without learnable parameters with ϵ = 1e20. Effectively, this corresponds to Pre-LN setup with QK-norm plus three additional normalisation layers: V-norm, O-norm (before output projection matrix in Attention block), and MLP-norm (after SwiGLU and before the last MLP layer). Residual connections, including the ones injecting the input embedding layer information, remain intact. Random seeds: For all proxy model runs in Sec. 3.2 and Sec. 3.3: 30 For all width/depth-scaled-up model runs: interleaved 30 + 3034 (every 22 step is 30, every other 22 step is 3034) For layout scans in Fig. 4a and Fig. 11: averaging over 30 + 3034 + 303409 for the three core learning rate values ({24, 26, 28} for = 32, {22, 24, 26} for = 128, {21, 23, 25} for = 512), 3034 + 303409 for the rest For layout scans in Fig. 4b: 30 torchtitan codebase, (Liang et al., 2025), FSDP2 (Feng et al., 2025), FlashAttention- (Dao, 2023) A.3 OPTIMIZER CONFIGURATION Except dedicated ablations, we use the following set of hyperparameters: Unconstrained version (without weight decay), Learning rate η: grid with 20.5 step for the proxy model, and 21 step for the width/depthscaled-up models, momentum µ = 0, without Nesterov momentum, no warmup, constant learning rate schedule, ϵ = 1e20 (used in gradient normalisation), 18 Preprint. Under review. orthogonalization of gradients for hidden layers (.RMSRMS norm assumption) with Newton-Schulz algorithm for niter = 5 with original Muon coefficients a, b, = (3.4445, 4.7750, 2.0315) (Jordan et al., 2024). A.4 OPTIMAL NORM FITTING & LOSS SMOOTHING After naıvely taking the empirical optimum across the learning rate grid (e.g. as the one emphasized with dashed black lines in Fig. 1a), we found that the corresponding norm scans, although still indicating norm transfer, are quite noisy (e.g. compare Fig. 2a vs. Fig. 6a). From Fig. 1a we noted that data points in loss vs. norm plot resemble parabola if plotted in log-log scale. Furthermore, we know by design that at initialization (step 0) the output norm equals to 1, and train loss equals to 11.765. With this, we chose to perform constrained fit with second-order polynomial function in log-log scale log(loss) = log(norm)2 + log(norm) + c, where the free term is fixed at precisely the loss value at initialization. We do this using weighted least squares fitting with np.linalg.lstsq, where the weighting is done with inverse uncertainties coming from loss smoothing, described below. For robustness, only seven data points around the empirical optimum are used in the fit. The optimal loss and norm values are then extracted as the parabola optimum coordinates. Optimal learning rate is taken from the data point closest to the fitted optimum. Results of such fits for Fig. 2 can be found in Fig. 12. Since running several random seeds is computationally intensive, we perform loss smoothing to estimate the loss variance and make loss estimates more robust. Essentially, for given horizon point, instead of taking its loss value, we average it with the previous and next evaluated points (67M tokens away, or e.g. 128 steps from each other with = 128). Empirically estimated standard deviation is then used in the fits as described above. We employ loss smoothing only for small batch sizes 128, as those having large loss variance, and for large token horizons 233, to stay in the region of largely converged loss (which can be locally linearly approximated) and therefore not bias the estimate. In order to get variance estimate in Fig. 3b without running several random seed runs, we vary the fitting procedure outlined above (with/without fitting, with/without loss smoothing, with/without constraint to loss at initialization), thus resulting in 6 total variations. For each of those we track how optimal norm/loss/learning rate changes, and propagate this variance to plotting and downstream analysis. 19 Preprint. Under review. A.5 DISTRIBUTED SCION We implemented distributed version of Scion/Muon. In this section, we briefly describe the implementation. We assume that the vectorized momentum buffer update is performed before applying the actual weight update. A.5.1 DDP-DISCO As warm-up, we first consider the DDP case (note that DDP-based version of Muon has already been implemented in modded-nanogpt10). Our implementation differs slightly from theirs, as we do not explicitly apply communicationcomputation overlap for DDP. i=0 with = {p}, world size , local rank Algorithm 1: Disco step ddp Input: Parameters {pi}P 1 bucket size ; total buckets P/M ; global updates array of length ; /* Step 1: Compute local updates for = 0 to 1 do if mod = then gi GETMOMENTUM(pi) ; ui LMO(gi) ; global updates[i] ui ; /* Step 2: Communicate updates in buckets for = 0 to total buckets 1 do start idx ; end idx min(start idx + M, ); my idx start idx +r; if my idx < end idx then usend global updates[my idx] ; else usend 0 {uj}M 1 j=0 ALLGATHER(usend) ; for = 0 to end idx start idx 1 do global updates[start idx +j] uj ; /* Step 3: Apply updates vectorized APPLYUPDATES({pi}P 1 i=0 , global updates) ; Helper functions: */ */ */ GETMOMENTUM(p): returns the momentum of from the momentum buffer. LMO(g): runs the LMO based on the chosen norm of p. ALLGATHER(u): gathers one tensor from each rank in the data-parallel group. APPLYUPDATES({p}, {u}): applies the global updates {u} to the parameters {p} in single vectorized operation. Notice this version works out-of-the-box for PP+DDP, as we could let each PP(Pipeline Parallelism) stage only manage the parts of the model that the current PP stages needed for forward and backward. To make it work with TP, one needs to do an extra all-gather in the local update loop. 10https://github.com/KellerJordan/modded-nanogpt 20 Preprint. Under review. A.5.2 FSDP-DISCO Here, FSDP refers to combination of FSDP2 with arbitrary parallelisms, including Data Parallelism (DP), Context Parallelism (CP), Expert Parallelism (EP), Tensor Parallelism (TP), and Pipeline Parallelism (PP). In this section, we restrict our discussion to FSDP and EP (via DP2EP). In principle, there is no need to treat DP and PP separately: one only needs to all-gather the full gradient before communication in the FSDP case to ensure compatibility with TP. We assume the design of this work, which applies an .1RMS norm for the LLMs embedding layer and an .RMS norm for the output linear layer. (SignNorm is also acceptable and remains compatible if one strictly follows Scions design.) The FSDP2 implementation in PyTorch shards weights and gradients along the tensors first dimension. We discuss Disco under this assumption and further assume that each tensor or matrix corresponds to single layer. Consequently, fused tensors such as fused QKV in attention layers or fused W13 in SwiGLU are not supported. Under these hypotheses, we can classify parameters into three groups: embedding, experts, and (pure-)fsdp. For updates, no extra communication is required for embedding and experts parameters, thanks to the Shard(0) strategy in FSDP2. Algorithm 2: Disco step embedding Input: Embedding parameters {pi}P 1 i=0 /* Initialise updates storage updates array of length ; /* get momentum and compute LMO update on local shards for = 0 to 1 do gi GETMOMENTUM(pi) ; ui LMO(gi) ; updates[i] ui ; /* Apply updates vectorized APPLYUPDATES({pi}P 1 i=0 , updates) ; Algorithm 3: Disco step experts Input: Expert parameters {pi}P 1 /* Initialise updates storage updates array of length ; /* get momentum and compute LMO update on local shards for = 0 to 1 do i=0 , transpose flag transpose gi GETMOMENTUM(pi) ; ui BATCHEDLMO(gi; transpose experts = transpose) ; updates[i] ui ; /* Apply updates vectorized APPLYUPDATES({pi}P 1 i=0 , updates) ; */ */ */ */ */ */ Noting that MoE expert weights are typically laid out as either (total experts, dout, din) or (total experts, din, dout), we apply transpose in the latter case to ensure that the output dimension comes first. In an FSDP + DP2EP setting, each gradient passed to LMO is therefore 3D tensor with layout (local experts, dout, din). Accordingly, SVD or Newton-Schulz-based algorithms must correctly handle batched inputs. Preprint. Under review. And below is the algorithm for purely fsdp-shard parameters. Algorithm 4: Disco step fsdp Input: FSDP-sharded parameters {pi}P 1 bucket size ; total buckets P/M ; global updates array of length ; i=0 , world size over fsdp, local rank for = 0 to total buckets 1 do start ; my idx start + r; end min(start + M, ); for = 0 to 1 do start + j; if < end then gi GETMOMENTUM(pi) send list[j] gi; else send list[j] 0 // row-sharded by FSDP; // zero padding recv list ALLTOALL(send list) CONCATROWS(recv list) LMO(g) updates send list SPLITROWS(u, ) updates recv list ALLTOALL(updates send list); for = 0 to end start 1 do global updates[start + j] updates recv list[j]; /* Single vectorized apply APPLYUPDATES({pi}P 1 i=0 , global updates); Helper functions: // reconstruct full gradient for pi // split by rows; */ ALLTOALL(list): list-based ALLTOALL over dp shard cp. CONCATROWS(list): concatenates row-shards into full tensor. SPLITROWS(u, ): splits into contiguous row blocks. Preprint. Under review. A.6 OUTPUT NORM EVOLUTION WITH DIFFERENT (η, B) (a) (b) Figure 5: Growth of the output layer norm WoutRMS vs. horizon, in tokens (a) and number of steps (b). Results are for the proxy model (69M parameters). Each curve is (learning rate η, batch size B) pair, with measured in samples: colour encodes batch size and line style encodes learning rate, as described in the legend. Preprint. Under review. A.7 SUPPLEMENTARY PLOTS TO FIG. 2 (a) (b) (c) (d) (e) (f) Figure 6: (a) Fig. 2a with an extended set of horizons, raw data (i.e. no loss smoothing, no fitting, see Appendix A.4 for details on fitting and loss smoothing). (b) Same as (a) + loss smoothing. (c) Same as (a) + fitting. (d) Same as (a) + fitting + loss smoothing. (e) Fig. 2b, raw data (no loss smoothing, no fitting). (f) Same as (e) + loss smoothing. 24 Preprint. Under review. A.8 OPTIMAL B(D) MEASUREMENT (a) (b) Figure 7: (a) Same as Fig. 3b, but with extended set of horizons. (b) Optimal batch size vs. horizon, as extracted from (a)). The line is power-law fit (described in legend). Fig. 3b, for the sake of clarity and simplicity, illustrates only four horizons. This is not really sufficient to extract precisely the scaling of optimal (η, B) (circled markers) with D, as it would mean fitting of four data points. We therefore perform the ordinary least squares (OLS) fit on the extended set of 9 horizons from Fig. 7a, effectively fitting the x-coordinate of the circled markers with line. We model optimal batch size dependency on horizon as power law B(D) = aDb and present results on Fig. 7b. We extract D0.450.07, consistent with the square-root scaling. A.9 NORM CONSTRAINT WITH WEIGHT DECAY (a) (b) Figure 8: Operator norm against number of gradient update steps. Fixed batch size = 32, momentum µ = 0.1, two values of learning rate η = {0.0625, 1.} and two values of weight decay λ = {0.01, 0.1} (applied as in Pethick et al. (2025a)), for proxy model (69M parameters). (a) Win1RMS norm (b) WoutRMS. We see for λ = 0.1 both norms converging to 1/λ, while for λ = 0.01 asymptotic values are not conclusive. 25 Preprint. Under review. A.10 ABLATION OF DEPTH TRANSFER TECHNIQUES Model: same as our proxy model (Appendix A.2), with the only difference in the head configuration: nquery heads = 2, nkv heads = 1. We run combination of two ablations: (i) weight initialisation depth-wise scaling (via gains/variance), and (ii) residual branch summation ratios. For weight initialisation, the depth-wise scaling factors are applied to only the output linear projection of attention and SwiGLU. We compare three flavours of depth init scaling: identity (baseline), total-depth, and relative-depth, defined by multiplying the gain σ by σ = 1/(cid:112)2Nlayers 1/ 2 li scale by total-depth, scale by relative-depth, scale by identity. (11) where Nlayers is the total number of Transformer blocks, and li {1, . . . , 2Nlayers} is the relative , for hidden weights depth of the current block; σ is the scaled orthogonal gain, σ = Rdoutdin. (cid:113) dout din Each transformer block is assigned depth 2, since attention and FFN sub-blocks each count as depth 1. When using relative-depth, the depth of all FFN blocks can be offset by 1. For depth-wise residual scaling, we write the residual connection in transformer as: = α + β Block(cid:0)Norm(X)(cid:1), (12) where is the block input and Block() denotes either self-attention or FFN, and Norm is RMSNorm in our setup. We consider three depth-wise residual scaling schemes: (α, β) = (cid:0) 2Nlayers1 2Nlayers 1 2Nlayers (1, (1, 1) , ) (cid:1) 1 2Nlayers scale by depth-normalized, scale by completeP, scale by identity. (13) depth-normalized Large et al. (2024) scales both the residual and block contributions proportionally to depth. completeP Dey et al. (2025) preserves the residual branch while scaling down the block contribution by depth. identity corresponds to the conventional unscaled residual formulation. We fixed batch size (B) to 32 samples, the sequence length to 4096, and the number of training steps to 2048. Experiments were conducted using proxy models with depths Nlayers {2, 16, 64}. For all models, we performed sweep over the learning rate {24, 23, 22, 21, 20}. We report the final-step losses in Table 1, Table 2, and Table 3 for the three depths, respectively, with the two lowest losses highlighted. From the perspective of learning rate transfer, we find that with our optimizer, the optimal learning rate consistently remains around 22, regardless of weight initialisation or residual scaling. We also observe that combining total-depth weight initialisation with identity residual scaling yields negligible improvement compared to using identity weight initialisation. 26 Preprint. Under review. Table 1: 2 layers (B = 32, steps=2048) Residual init Residual multiplier total-depth total-depth total-depth identity identity identity relative-depth relative-depth relative-depth identity depth-normalized completeP identity depth-normalized completeP identity depth-normalized completeP Learning rate η 21 22 23 4.11 4.12 4.15 4.10 4.12 4.15 4.11 4.13 4.16 4.09 4.11 4.16 4.10 4.12 4.13 4.09 4.11 4.14 4.13 4.17 4.17 4.13 4.13 4.16 4.13 4.16 4.18 2 4.20 4.20 4.22 4.19 4.22 4.21 4.20 4.20 4.21 Table 2: 16 layers (B = 32, steps=2048) Residual init Residual multiplier total-depth total-depth total-depth identity identity identity relative-depth relative-depth relative-depth identity depth-normalized completeP identity depth-normalized completeP identity depth-normalized completeP Learning rate η 21 22 23 3.75 3.79 3.82 3.74 3.78 3.81 3.79 3.79 3.82 3.73 3.80 3.82 3.75 3.78 3.81 3.74 3.80 3.82 3.77 3.84 3.85 3.79 3.83 3.85 3.80 3.83 3.85 24 3.81 3.85 3.87 3.81 3.83 3.86 3.82 3.84 3. Table 3: 64 layers (B=32, steps=2048) Residual init Residual multiplier total-depth total-depth total-depth identity identity identity relative-depth relative-depth relative-depth identity depth-normalized completeP identity depth-normalized completeP identity depth-normalized completeP Learning rate η 21 22 2 3.60 3.65 3.67 3.63 3.64 3.70 3.61 3.65 3.68 3.60 3.65 3.67 3.62 3.64 3.67 3.61 3.65 3.67 3.65 3.69 3.72 3.66 3.69 3.72 3.67 3.69 3.73 24 3.67 3.71 3.72 3.70 3.70 3.70 3.70 3.71 3.72 4.22 4.21 4.28 4.23 4.21 4.24 4.23 4.25 4.24 20 3.88 3.92 3.94 3.89 3.92 3.94 3.90 3.95 3.95 20 3.79 3.80 3.82 3.78 3.80 3.82 3.82 3.80 3.83 Preprint. Under review. A.11 ABLATIONS ON FIG. 2A A.11.1 MOMENTUM & LEARNING RATE DECAY In this set of experiments we set momentum to 0.1 (which is by default disabled in the main text) and firstly run the same horizon scaling experiment for the proxy model (69M parameters) with the constant learning rate schedule and evaluate at the same horizons = {231, 233, 235, 237} as Fig. 2a. The results are presented in Fig. 9a. Here we perform loss smoothing in the same way as for the no-momentum scenario, but do not perform the fitting, i.e. for each batch size we take the optimal norm from the empirically best performing learning rate run. We find that the curves look more like blobs, where multiple batch sizes give almost the same performance and are centered around the optimal norm (which also transfers across horizons). Also the loss difference between horizons is not well-pronounced as in the no-momentum scenario. Then, we add learning rate decay, where we start from checkpoints of the horizons specified above, assume that that constitutes 75% of the total horizon, and linearly decay learning rate to 0 for the rest 25%. Likewise, we smooth loss values and take optimum value per batch size across empirical ones on the learning rate grid. In Fig. 9b we see that there is potentially slight drift of the optimal norm with horizon scaling. However, after examining individual scans  (Fig. 13)  we surprisingly found that for long horizons the learning rate decay smooths out the norm optimum: broad range (factor 4 8 in norm) of learning rates results in the same loss. Hence, there is no longer single optimal norm, but rather sizeable range, indicating that learning rate decay significantly reduces norm sensitivity. Therefore, we conclude that the seaming drift in Fig. 9b is not significant. (a) (b) Figure 9: Same as Fig. 2a but with momentum = 0.1. (a) Without learning rate decay. (b) With linear learning rate decay to 0 for extra 25% of the total horizon. 28 Preprint. Under review. A.11.2 NORM CHOICE In this Section, we ablate if it is only the output layer norm that induces norm transfer. We replot Fig. 2a, with loss smoothing and without fitting, but now where we use .RMSRMS norm of the output or .1RMS of the input layers instead default .RMS of the output layer. We observe in Fig. 10 (see also individual norm scans in Fig. 14) that interestingly both norms induce norm transfer. (a) (b) Figure 10: Same as Fig. 2a but with WoutRMS norm for the X-axis changed to: (a) WoutRMSRMS (output layer). (b) Win1RMS (input layer). A.12 LEARNING RATE LAYOUT FOR ADDITIONAL BATCH SIZES AND HORIZONS (a) (d) (b) (e) (c) (f) Figure 11: Extended version of Fig. 4a with additional batch sizes and horizons. Top (bottom) row: = 231(233) token horizons. Batch sizes, in samples: = 32 (left), = 128 (middle), = 512 (right). Performance is averaged across random seeds as described in Appendix A.2. Note that the optimal is 128 for both = 231 and = 233 according to Fig. 3b. Preprint. Under review. A."
        },
        {
            "title": "INDIVIDUAL NORM SCANS AND FITS",
            "content": "(a) (b) Figure 12: Individual norm scans for various batch sizes (columns), across various horizons in (a), across various models in (b) (rows). We plot train loss (Y-axis) against the output layer operator norm WoutRMS, where each point corresponds to different learning rate run and error bars correspond to loss smoothing variance (see Appendix A.4). The best-loss point for each (B, D) is pinpointed with the blue dashed line, fitted curves are shown with blue solid lines. These fit results are used for: (a) Fig. 2a and Fig. 3b, (b) Fig. 2b, from top to bottom rows: proxy, 4width, 12-width, 8-depth, 32-depth. 30 Preprint. Under review. (a) (b) Figure 13: Individual output norm WoutRMS scans for various batch sizes (columns) across various horizons (rows). (a) with momentum = 0.1, no learning rate decay. (b) with momentum = 0.1, with learning rate decay linearly to 0 for 25% of total horizon. 31 Preprint. Under review. (a) (b) Figure 14: Individual norm scans for various batch sizes (columns) across various horizons (rows). (a) For WoutRMSRMS (output layer). (b) For Win1RMS (input layer)."
        }
    ],
    "affiliations": [
        "Julich Supercomputing Centre, Forschungszentrum Julich"
    ]
}