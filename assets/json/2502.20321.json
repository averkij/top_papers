{
    "paper_title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
    "authors": [
        "Chuofan Ma",
        "Yi Jiang",
        "Junfeng Wu",
        "Jihan Yang",
        "Xin Yu",
        "Zehuan Yuan",
        "Bingyue Peng",
        "Xiaojuan Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The representation disparity between visual generation and understanding imposes a critical gap in integrating these capabilities into a single framework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer that encodes fine-grained details for generation while also capturing high-level semantics for understanding. Despite recent studies have shown that these objectives could induce loss conflicts in training, we reveal that the underlying bottleneck stems from limited representational capacity of discrete tokens. We address this by introducing multi-codebook quantization, which divides vector quantization with several independent sub-codebooks to expand the latent feature space, while avoiding training instability caused by overlarge codebooks. Our method significantly raises the upper limit of unified discrete tokenizers to match or even surpass domain-specific continuous tokenizers. For instance, UniTok achieves a remarkable rFID of 0.38 (versus 0.87 for SD-VAE) and a zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on ImageNet. Our code is available at https://github.com/FoundationVision/UniTok."
        },
        {
            "title": "Start",
            "content": "UniTok: Unified Tokenizer for Visual Generation and Understanding Chuofan Ma 1 2 Yi Jiang 2 Junfeng Wu 2 3 Jihan Yang 1 Xin Yu 1 Zehuan Yuan 2 Bingyue Peng 2 Xiaojuan Qi 1 5 2 0 2 7 2 ] . [ 1 1 2 3 0 2 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The representation disparity between visual generation and understanding imposes critical gap in integrating these capabilities into single framework. To bridge this gap, we introduce UniTok, discrete visual tokenizer that encodes fine-grained details for generation while also capturing highlevel semantics for understanding. Despite recent studies have shown that these objectives could induce loss conflicts in training, we reveal that the underlying bottleneck stems from limited representational capacity of discrete tokens. We address this by introducing multi-codebook quantization, which divides vector quantization with several independent sub-codebooks to expand the latent feature space, while avoiding training instability caused by overlarge codebooks. Our method significantly raises the upper limit of unified discrete tokenizers to match or even surpass domainspecific continuous tokenizers. For instance, UniTok achieves remarkable rFID of 0.38 (versus 0.87 for SD-VAE) and zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on ImageNet. Our code is available at https://github.com/ FoundationVision/UniTok. 1. Introduction Autoregressive modeling has recently extended its predominance in natural language processing to visual domains. This is characterized by the rapid growth of autoregressive models in image and video generation (Kondratyuk et al., 2023; Tian et al., 2024; Li et al., 2024c), as well as the increasing prevalence of Multimodal Large Language Models (MLLMs) for diverse visual understanding tasks (Liu et al., 2024a; Tong et al., 2024a; Wu et al., 2024c). Given these advancements in individual fields, there is growing interest in Project Lead 1The University of Hong Kong 2ByteDance Inc. 3Huazhong University of Science and Technology. Correspondence to: Xiaojuan Qi <xjqi@eee.hku.hk>, Zehuan Yuan <yuanzehuan@bytedance.com>. Preprint. Figure 1. (a): The unified tokenizer training paradigm. (b): Comparison with the unified tokenizer VILA-U in terms of ImageNet zero-shot accuracy and reconstruction FID. developing unified MLLMs by integrating visual generation and understanding within single autoregressive framework (Liu et al., 2024b; Team, 2024; Wang et al., 2024). However, critical challenge persists in unification: the disparity in visual tokenization for understanding and generation. For instance, the CLIP (Radford et al., 2021) tokenizer, widely used in multimodal understanding, does not naturally fit into generative modeling, which requires precise encoding of fine-grained details. Conversely, the VQVAE (Van Den Oord et al., 2017) tokenizer, which is tailored for autoregressive generation, potentially falls short of capturing high-level semantics crucial for visual understanding (Xie et al., 2024). To address this issue, recent studies attempt to use separate tokenizers for each task (Wu et al., 2024a), yet this increases model complexity without fundamentally bridging the gap in representation. These limitations underscore the need for unified visual tokenizer to serve both generation and understanding objectives. Recently, VILA-U (Wu et al., 2024d) has proposed promising paradigm for unified tokenizers, which integrates CLIP supervision into VQVAE training to complement VQ tokens with text alignment and rich semantics, as illustrated in Figure 1(a). Despite this innovation, it has been observed that the unified tokenizer struggles to converge in training and tends to underperform domain-specific tokenizers. This is commonly attributed to loss conflicts arising from divergent optimization goals (Wu et al., 2024d;a). However, recent studies on visual generation suggest that these objectives might not inherently conflict, as evidenced by the alignment UniTok: Unified Tokenizer for Visual Generation and Understanding behavior in generative and discriminative representation learning (Yu et al., 2024). This contrast raises an important question: Do reconstruction and contrastive losses truly conflict in unified tokenizer training, or might there be an underlying bottleneck that remains unidentified? To answer the question, we conduct comprehensive ablation study on the current unified tokenizer training paradigm (see Figure 3), which yields several intriguing findings. First, we show that removing reconstruction supervision, which equals training vector-quantized CLIP model, actually does not lead to better understanding performance over unified tokenizers. This observation indicates that the existing gap between unified and CLIP tokenizers mainly arise from vector quantization rather than the conflict of learning objectives. We further establish that this gap stems from both token factorization which projects tokens into lowerdimensional space for code index lookup (Yu et al., 2021) and discretization. These operations are essential for vector quantization but inevitably compromise the expressiveness of visual tokens. We thus identify that the primary bottleneck of unified tokenizers lies in the limited representational capacity of discrete tokens. To address this limitation, an intuitive approach is to expand the codebook size and latent code dimension of the tokenizer, which enables better approximation of the continuous feature space. However, this in practice could result in low codebook utilization (Zhu et al., 2024) and diminished performance gains (Yu et al., 2023b), while also making the code lookup process more computationally expensive. Drawing inspiration from the divide-and-conquer algorithm, we introduce multi-codebook quantization to alleviate this problem. Specifically, this involves dividing the visual token into multiple chunks and discretizing each with independent sub-codebooks, similar to the multi-head attention mechanism (Vaswani, 2017). Such design effectively scales up the latent code space by increasing the number of sub-codebooks, circumventing the optimization problem associated with large codebooks. Building upon multi-codebook quantization, we train unified tokenizer called UniTok to bridge visual generation and understanding. We conduct thorough evaluation of UniToks reconstruction and classification capabilities, and more importantly, its effectiveness in establishing unified MLLM. Experimental results show that UniTok achieves comparable or even better performance to domain-specific tokenizers, recording an impressive 0.38 reconstruction FID and 78.6% zero-shot accuracy at 256 256 resolution on ImageNet. The foundational capabilities of UniTok further contribute to strong downstream task performance. Our model sets new state-of-the-art among unified autoregressive MLLMs on both multimodal understanding and generation benchmarks. 2. Related Work Image Tokenization for Generation. In the domain of visual generation, image tokenization plays an important role in encoding raw pixels into compact latent features for generative modeling (Van Den Oord et al., 2017; Rombach et al., 2022a). Particularly, among variety of tokenizers, the vector-quantized tokenizer (Van Den Oord et al., 2017) is favored for its discrete latent space and compatibility with autoregressive or masked generative models (Tian et al., 2024; Sun et al., 2024a; Chang et al., 2022; Yu et al., 2023a). The pioneering work VQVAE (Van Den Oord et al., 2017) initially introduced the concept of discretizing continuous tokens by mapping them to the nearest neighbors in learnable codebook. Built on this, VQGAN (Esser et al., 2021) incorporated perceptual loss (Zhang et al., 2018) and discriminator loss (Karras et al., 2019) to significantly improve the reconstruction quality. Subsequently, ViT-VQGAN (Yu et al., 2021) advanced the framework with the transformer architecture. In recent literature, considerable efforts have been devoted to developing better quantization methods such as residual quantization (Lee et al., 2022) and lookupfree quantization (Yu et al., 2023b), which also constitute focal point of this paper. Image Tokenization for Understanding. The unprecedented success of large language models (LLMs) (Wei et al., 2022; Achiam et al., 2023) has catalyzed the development of multimodal large language models (MLLMs) (Liu et al., 2024a; Lin et al., 2024; McKinzie et al., 2024). As critical component of MLLMs, the selection of an effective vision tokenizer has been the subject of extensive study (Wang et al., 2023; Tong et al., 2024a). common choice of the vision tokenizer is the pretrained CLIP model (Radford et al., 2021), which undergoes alignment with language during its pretraining phase. While self-supervised learning models, such as DINOv2 (Oquab et al., 2023), are shown to be advantageous at region-level tasks (Ma et al., 2025). Cambrian-1 (Tong et al., 2024a) further demonstrates that MLLMs can benefit from hybrid representations from mixture of vision encoders. Nonetheless, these tokenizers predominantly encode images into continuous tokens, presenting challenge for uniformly modeling both vision and text tokens. To address this disparity, several works have explored discretizing CLIP tokens (Ge et al., 2023) or employing VQVAE encoders (Liu et al., 2024b; Xie et al., 2024). However, these approaches have been observed to substantially impair understanding performance of MLLM. Unified Vision-Language Models. The rise of MLLMs is not limited to the realm of visual understanding. Recent advancements have witnessed an increasing focus on unifying visual generation and understanding within one MLLM (Dong et al., 2023; Wu et al., 2023; Team, 2024; Zhou et al., 2024; Xie et al., 2024; Tong et al., 2024b). Specifically, 2 UniTok: Unified Tokenizer for Visual Generation and Understanding Figure 2. An overview of UniTok. The tokenizer is trained to faithfully reconstruct the input image while aligning its discrete latent features with the text caption. For vector quantization, each visual token is split into multiple chunks, which then undergo code index lookup on corresponding sub-codebooks concurrently. line of works employs continuous visual tokenizers for image encoding, and leverages pretrained diffusion models for image synthesis (Dong et al., 2023; Ge et al., 2024; Sun et al., 2024b). This approach inevitably disconnects visual token sampling from the MLLM. In contrast, another stream of research adopts VQVAE models to encode images into discrete tokens (Team, 2024; Wang et al., 2024; Xie et al., 2024; Wu et al., 2024d;b). These tokens are subsequently modeled using the same next token prediction loss that is applied to text tokens, facilitating unified approach to multimodal learning. However, as reconstruction-oriented VQVAE does not naturally align with the LLM token space, these models typically suffer from degraded visual comprehension capabilities. Our research aligns with the second approach, with particular focus on the tokenizer design that is suitable for both generation and understanding tasks. 3. Method In this section, we introduce UniTok, unified visual tokenizer well-suited for both generation and understanding tasks. We start with unified training recipe that integrates reconstruction (VQVAE) and contrastive (CLIP) supervisions (Section 3.1). However, we find that simply combining both training objectives leads to severe performance degradation, which can be mainly attributed to limited representational capacity of discrete tokens (Section 3.2). To this end, we propose multi-codebook quantization and attention factorization to enhance the latent feature space and derive unified visual representations (Section 3.3). An overview of the framework is presented in Figure 2. 3.1. Unified Supervision Visual generation and understanding typically impose distinct demands on the visual tokenizer. For instance, generation emphasizes lossless compression for accurate reconstruction, whereas understanding prioritizes semantically meaningful and discriminative features. To accommodate both requirements, we jointly train the tokenizer with (i) VQVAE-based reconstruction loss to preserve low-level information, and (ii) an image-text contrastive loss that enhances high-level semantics of the features. To be specific, the VQVAE-based loss term Lrecon consists of pixel-level reconstruction loss LR, perceptual loss LP based on the LPIPS metric (Zhang et al., 2018), discriminator loss LG to enhance reconstruction fidelity (Karras et al., 2019), and vector quantization loss LVQ to minimize distance between the encoder output and its nearest code entry. It is denoted as: Lrecon = LR + λVQLVQ + λPLP + λGLG, (1) where λ is the weight factor for the corresponding loss term. The image-text contrastive loss term Lcontra is basically the same as in CLIP (Radford et al., 2021). Therefore, the final loss term of UniTok can be written as: = Lrecon + λcontraLcontra. (2) We simply choose λcontra = 1 in this paper. 3.2. Quantization Bottleneck Despite being augmented with CLIP supervision, we find that the unified tokenizer still exhibits unsatisfactory performance in visual understanding tasks, significantly lagging behind the commonly used CLIP tokenizer. To figure out the underlying causes of this underperformance, we break down the key components involved in training unified tokenizer, as illustrated in Figure 3. Starting with the CLIP baseline, we provide step-by-step walk-through and ablation of all changes in the following paragraphs. 3 UniTok: Unified Tokenizer for Visual Generation and Understanding 2024d). We observe similar phenomenon where joint training results in sub-optimal ImageNet zero-shot classification accuracy and reconstruction FID compared to specialized training. However, surprisingly, we find that this degradation has negligible impacts on downstream understanding performance. Moreover, the degradation in classification accuracy and reconstruction FID diminishes after we improve the quantization methods (detailed in the next section). Based on these observations, we speculate that the perceived loss conflict is only superficial issue, and the primary cause of the underperformance lies in the limited representational capacity of discrete tokens. 3.3. UniTok straightforward solution to breaking the quantization bottleneck could be increasing the codebook size and the latent code dimension. However, current studies on VQVAE tokenizers suggest that there is diminishing gain in scaling and the performance saturates after the codebook size reaches 16k (Yu et al., 2023b; Sun et al., 2024a). Continuing expansion results in substantial portion of codes being rarely used or becoming dead during training, which negatively impacts downstream task performance (Yu et al., 2021). To address this issue, we propose multi-codebook quantization and attention factorization in the following paragraphs. Multi-codebook quantization (MCQ) discretizes the latent tokens with set of independent codebooks. Specifically, the latent vector Rd is first evenly split into chunks {f1, f2, ..., fn}, where fi . The subsequent quantization process is denoted as: ˆf = Concat (Q (Z1, f1) , (Z2, f2) , ..., (Zn, fn)) (3) where ˆf is the discretized latent vector, is the code index lookup operation, and Zi is i-th sub-codebook. Compared to conventional quantization methods, the proposed MCQ effectively scales up the vocabulary size. For instance, by increasing the number of sub-codebooks from 1 to 4, and suppose each sub-codebook contains 16k code entries, the theoretical vocabulary size exponentially increases from 214 to 256 (i.e., there are up to 2144 possible combinations of codes for each token). As the size of each individual codebook remains constant, it circumvents the optimization problem associated with large codebooks. Besides, the dimensionality of the latent codes also scales proportionally with the number of codebooks (i.e., increasing from 16-d to 64-d in this case), which further enhances the representational capacity of discrete representations. Figure 3. Roadmap to build UniTok. The blue bars illustrate the progressive changes in VQA performance from the CLIP tokenizer to the unified tokenizer, while the purple bars represent the proposed improvements in UniTok. The VQA score is measured using the average accuracy across the VQAv2, GQA, TextVQA, and POPE benchmarks. All models are trained from scratch on 512m image-text pairs from DataComp. Factorization. Modern VQ-tokenizers typically project continuous tokens to lower-dimensional latent space for code index lookup (e.g. from 768-d to 16-d), known as token factorization (Yu et al., 2021). This increases the relative density of codes by compressing the latent code space, thereby reducing quantization error. To evaluate the impact of factorization in CLIP training, we add two linear projection layers on top of the CLIP vision encoder (right before average pooling), which transforms tokens from 768-d to 16-d and then back to 768-d. Notably, vector quantization and reconstruction supervision are not included at this stage. Surprisingly, it turns out that this channel compression operation significantly compromises the expressiveness of tokens, leading to severe performance degradation in downstream VQA tasks. Discretization. Based on the implementation described above, we further introduce vector quantization to CLIP training, which maps factorized tokens to their nearest code entries. Compared to language tokenizers with vocabularies exceeding 200k entries, the vocabulary size of modern VQ-tokenizers is markedly smaller (i.e., typically ranging from 4k to 16k). Mapping continuous tokens to such small codebook results in considerable information loss. This is validated in our experiment, which demonstrates that discretizing the factorized tokens with 16k codebook causes an average accuracy drop of 2.1 in VQA tasks. Reconstruction Supervision. Finally, we integrate reconstruction losses into the training process to build unified tokenizer, as outlined in Section 3.1. Previous literature suggests that loss conflict between VQVAE and CLIP is major cause of performance degradation in joint training (Wu et al., Attention factorization. Existing VQ methods usually employ linear or convolutional projection layers for token factorization. But as shown in Section 3.2, this over-simplified design fails to preserve rich semantics in original tokens, leading to degraded understanding performance. To allevi4 UniTok: Unified Tokenizer for Visual Generation and Understanding ate this problem, we suggest adapting the multi-head attention modules for factorization, as illustrated in Figure 4. Despite its simplicity, we find this design effectively strengthens the representational power of factorized tokens. Notably, to ensure compatibility with autoregressive generation, the factorization blocks are configured with causal attention. Figure 4. Modified attention blocks for factorization. Modules in yellow indicate change in the number of channels. 3.4. Unified MLLM We proceed to develop unified multimodal model with UniTok. Particularly, we leverage the unified framework introduced in Liquid (Wu et al., 2024b), which models (discrete-valued) vision and language sequences with universal next-token prediction loss. But instead of learning the visual codebook from scratch, we reuse code embeddings of UniTok by projecting them to the MLLM token space with an MLP projector. Notably, despite UniTok encodes an image into codes (where represents the number of sub-codebooks), we simplify this for MLLM input by merging every consecutive codes into single visual token. Similarly, when it comes to visual token prediction, we make each token autoregressively predict the next codes, using depth transformer head as implemented in RQ-Transformer (Lee et al., 2022) and VILA-U (Wu et al., 2024d). This design maintains efficiency for visual generation in the context of multi-codebooks. 4. Experiments 4.1. Implementation Details Tokenizer Setup. Leading VQVAE tokenizers predominantly adopt the CNN architecture, while ViT is preferred in CLIP training for its scalability. To take advantage of both, we choose hybrid architecture, ViTamin-L/16 (Chen et al., 2024), to instantiate UniTok. We configure UniTok with eight sub-codebooks, each containing 4,096 code entries and latent dimension set to 8-d (the global latent dimension is thus 64-d). The discriminator is initialized with pretrained DINOv2-S (Oquab et al., 2023). We train the tokenizer for one epoch on the public dataset DataComp-1B (Gadre et al., 2024) consisting of 1.28B image-text pairs, with all images resized to 256 256 resolution and global batch size of 16k. The learning rate is set to 1e-3 for the tokenizer and 2e4 for the discriminator. Besides, we prepare two settings for evaluation: one with pretrained CLIP weight initialization and one with random initialization (the default setting). MLLM Setup. We instantiate unified MLLM described in Section 3.4 with the Llama-2-7B base model (Touvron et al., 2023). Following Liquid, we first pretrain the model on mix of multimodal data, which is composed of 10M language data from DCLM (Li et al., 2024b), 30M internal MidJourney-style synthetic data, and 30M re-captioned image-text pairs from COYO (Minwoo et al., 2022) and Laion (Schuhmann et al., 2022). Subsequently, we finetune the model on 1.5M text-to-image data and 1.5M multimodal instruction tuning data introduced in Mini-Gemini (Li et al., 2024d). Specifically, the learning rate is set to 5e-5 in the pretraining stage and 2e-5 in the finetuning stage. For visual understanding evaluation, we report results on standard VQA benchmarks including VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), TextVQA (Singh et al., 2019), POPE (Li et al., 2023), MME (Yin et al., 2023), and MM-Vet (Yu et al., 2023d). For visual generation evaluation, we report results on GenAI-Bench (Lin et al., 2025) and MJHQ-30K (Li et al., 2024a). 4.2. Tokenizer Comparison Table 1. Comparison with modern tokenizers on ImageNet reconstruction FID and zero-shot classification accuracy. The rFID is measured at 256 256 resolution with 16 downsample ratio. indicates the model uses pretrained CLIP weights for initialization. Method VQVAE Model VQ-GAN (Esser et al., 2021) RQ-VAE (Lee et al., 2022) VAR (Tian et al., 2024) CLIP Model CLIP (Radford et al., 2021) SigLIP (Zhai et al., 2023) ViTamin (Chen et al., 2024) Unified Model TokenFlow (Qu et al., 2024) VILA-U (Wu et al., 2024d) UniTok UniTok #Tokens rFID Accuracy 256 256 680 256 256 256 680 256 256 256 4.98 1.30 0.90 1.37 1.80 0.39 0.38 76.2 80.5 81.2 73.3 70.5 78.6 UniTok: Unified Tokenizer for Visual Generation and Understanding Table 2. Comparison with unified multi-modal large language models on VQA benchmarks. Method LLM Token Type Res. VQAv2 GQA TextVQA POPE MME MM-Vet Emu (Sun et al., 2023) LaVIT (Jin et al., 2023) DreamLLM (Dong et al., 2023) Unified-IO 2 (Lu et al., 2024) Janus (Wu et al., 2024a) Llama-13B Llama-7B Vicuna-7B Continuous Continuous Continuous 6.8B from scratch Continuous Continuous DeepSeek-1.3B CM3Leon (Yu et al., 2023c) LWM (Liu et al., 2024b) Show-o (Xie et al., 2024) Chameleon (Team, 2024) Liquid (Wu et al., 2024b) VILA-U (Wu et al., 2024d) UniTok 7B from scratch Llama-2-7B Phi-1.5-1.3B 34B from scratch Gemma-7B Llama-2-7B Llama-2-7B Discrete Discrete Discrete Discrete Discrete Discrete Discrete 224 224 224 384 384 256 256 256 512 512 256 256 52.0 66.0 72.9 79.4 77. 47.6 55.8 59.3 69.6 71.3 75.3 76.8 - 46.8 - - 59.1 - 44.8 48.7 - 58.4 58.3 61.1 - - 41.8 - - - 18.8 - - 42.4 48.3 51.6 - - - 87.7 87. - 75.2 73.8 - 81.1 83.9 83.2 - - - - 1338 - - 948 - 1119 1336 1448 - - 26.6 - 34.3 - - - - 27.7 33. Table 3. Comparison with other visual generation methods on GenAI-Bench (advanced prompts). Method Type #Training Images Count Differ Compare Logical Overall Negate Universal SD v2.1 (Rombach et al., 2022a) SD-XL (Podell et al., 2023) Midjourney v6 (Radhakrishnan, 2023) DALL-E 3 (Betker et al., 2023) Diffusion Diffusion Diffusion Diffusion Show-o (Xie et al., 2024) LWM (Liu et al., 2024b) VILA-U (Wu et al., 2024d) Liquid (Wu et al., 2024b) UniTok Discrete Diff. Autoregressive Autoregressive Autoregressive Autoregressive 2000M 2000M 36M 15M 30M 30M 0.68 0.71 0.78 0.82 0.70 0.59 0.70 0.76 0.76 0.70 0.73 0.78 0.78 0.62 0.58 0.71 0.73 0.76 0.68 0.69 0.79 0. 0.71 0.54 0.74 0.74 0.79 0.54 0.50 0.50 0.48 0.51 0.49 0.53 0.46 0.46 0.64 0.66 0.76 0.80 0.65 0.52 0.66 0.74 0.73 0.62 0.63 0.69 0. 0.60 0.53 0.64 0.65 0.67 We benchmark UniTok on ImageNet using two primary metrics: Frechet Inception Distance (FID) to evaluate reconstruction quality, and top-1 zero-shot accuracy to assess image-text alignment. The results are presented in Table 1. Notably, UniTok excels in reconstruction quality compared to both unified and domain-specific tokenizers, recording an impressive 0.38 rFID on ImageNet with 16 downsampling ratio. As discrete tokenizer, UniTok even surpasses the continuous VAE tokenizer from Stable Diffusion v2.1 (Rombach et al., 2022b), showcasing the superiority of the proposed multi-codebook quantization. For the perception performance, we observe that randomly initialized UniTok demonstrates suboptimal zero-shot classification accuracy. This is expected as current training schedule (i.e., one epoch on 1.28B samples) is insufficient for CLIP training to fully converge. It can be seen that initializing the model with pretrained CLIP weights largely alleviates the problem, boosting the zero-shot accuracy from 70.8% to 78.6%. However, we would like to point out that higher ImageNet accuracy does not guarantee superior downstream performance. As demonstrated in Table 7, we find that random initialization actually leads to better understanding performance. In complement to quantitative results, we also provide examples of reconstructed images in Figure 6. 4.3. Visual Understanding Performance We evaluate the understanding performance of UniTok on diverse VQA benchmarks in Table 2. Our unified MLLM showcases clear advantages when compared to other unified models that also utilize discrete visual tokenizer. Specifically, UniTok significantly outperforms the Chameleon model, which relies on traditional VQVAE tokenizer, by 7.2% higher accuracy on VQAv2. Additionally, it surpasses VILA-U, another model with unified tokenizer, by 3.3% in accuracy on the TextVQA benchmark and by notable margin of 112 points on the MME-Perception scores. Furthermore, we can see that UniTok largely narrows the performance gap with MLLMs that incorporate continuous visual tokenizers. These strong results confirm the candidacy of UniTok as unified visual tokenizer for multimodal models. 4.4. Visual Generation Performance Quantitative Results. Table 3 presents the text-to-image generation performance of our unified MLLM on GenAIBench (advanced prompts)1, challenging benchmark that measures the alignment of generated images with text 1Due to paper length limit, we put the results on GenAI-Bench (basic prompts) in the Appendix. 6 UniTok: Unified Tokenizer for Visual Generation and Understanding Figure 5. Images generated in resolution of 256 256 with our unified MLLM. Table 4. Results on MJHQ-30K. Method Type Res. FID SD-XL (Podell et al., 2023) PixArt (Chen et al., 2023) Playground (Li et al., 2024a) Liquid (Wu et al., 2024b) Janus (Wu et al., 2024a) LWM (Liu et al., 2024b) Show-o (Xie et al., 2024) VILA-U (Wu et al., 2024d) Diffusion Diffusion Diffusion Autoregressive Autoregressive Autoregressive Discrete Diff. Autoregressive 1024 1024 1024 512 384 256 256 256 9.55 6.14 4.48 5.47 10.10 17.77 15.18 12.81 UniTok Autoregressive 7.46 prompts in complex dimensions such as counting, differentiation, comparison, and understanding logical relationships. In this demanding context, our model not only consistently outperforms other autoregressive unified models, but also achieves competitive performance against domain experts (diffusion models) trained on billions of images. The strong results underscore the superior capability of our unified MLLM in complex text-to-image generation tasks. We further evaluate the quality of images generated by our model on the MJHQ-30K benchmark, details of which are presented in Table 4. Notably, as this benchmark primarily relies on the FID score for evaluation, high-resolution 7 images are preferred because they potentially capture more fine-grained details. Despite this makes FID across different resolutions less comparable, we show that our model achieves impressive performance even at the the smallest resolution, showcasing its ability to generate high-quality, detail-rich images. Qualitative Results. We present some examples of the images generated by our model in Figure 5, using text prompts sampled from MJHQ-30K. The visualization results demonstrate our model is capable of synthesizing photo-realistic and visually appealing images. Moreover, the model is able to comprehend wide spectrum of concepts, such as Vincent van Gogh painting style and bitcoin, and flexibly combine these concepts to synthesize creative images. 4.5. Ablation Studies Impact of Supervision Types. To ablate the impact of contrastive and reconstruction losses in UniTok training, we conduct experiments on tokenizers trained with different supervision types, as shown in Table 5. It is worth noting that all the tokenizers are vector-quantized even though some do not have reconstruction supervision. First, we show that reconstruction-oriented tokenizer significantly lags behind tokenizers with contrastive supervision in visual understanding performance. This observation evidences the UniTok: Unified Tokenizer for Visual Generation and Understanding Table 5. Impact of different supervision types on downstream generation and understanding performance. The rFID and gFID are measured on the ImageNet (256 256) validation set. LlamaGen-L (Sun et al., 2024a) is adopted as the generator for gFID evaluation. Supervision Generation Understanding rFID gFID VQAv2 GQA SciQA TextVQA POPE MME Contrastive Reconstruction Recon. + Contra. 0.82 0.72 3.59 3. 68.95 56.33 69.14 56.89 47.53 56.06 65.64 63.26 65.25 49.89 43.65 49.22 82.34 77.09 81.42 1373 902 Table 6. Ablation on the number of sub-codebooks. The size of codebook is denoted as B, where is the number of subcodebook and is the size of sub-codebook. Codebook 1 16384 2 8192 4 4096 8 rFID Accuracy 1.50 41.0% 0.98 43.9% 0.54 44.7% 0.33 46.1% Table 7. Comparison of different initialization methods under the LLaVA framework. indicates the model uses CLIP weights for initialization. We highlight the default setting of UniTok in gray. Tokenizer VQAv2 GQA TextVQA POPE MME UniTok UniTok 69.9 72.4 56.2 58.2 49.3 51.6 81.2 82. 1331 1392 limitations of traditional VQVAE. Second, we demonstrate that reconstruction and contrastive training objectives do not inherently conflict, or can be addressed by enhancing discrete feature space. With multi-codebook quantization, the jointly trained tokenizer not only exhibits understanding performance on par with the tokenizer trained solely with contrastive loss, but also slightly improves generation performance over the reconstruction-oriented tokenizer. Number of Sub-Codebooks. To gain deeper insights into multi-codebook quantization, we evaluate how tokenizer performance changes with the number of sub-codebooks. Specifically, for rFID evaluation, we train the tokenizer solely with reconstruction loss on OpenImages (Kuznetsova et al., 2020), and evaluated it on ImageNet (256 256) validation set. While for ImageNet zero-shot accuracy evaluation, the tokenizer is trained on DataComp-1B 128m subset using only contrastive loss. As shown in Table 6, given constant global codebook size, increasing the number of subcodebooks consistently improves reconstruction FID and classification accuracy. This indicates that multi-codebook quantization generally benefits vector-quantized models, independent of the training objectives. CLIP Weight Initialization. In Table 7, we ablate the impact of CLIP weight initialization on visual understanding performance. Specifically, we adopt the classic LLaVA framework for evaluation, replacing the original CLIP tokenizer with UniTok while keeping all other the training settings unchanged. One tokenizer is initialized with the pretrained ViTamin-L-256 (Chen et al., 2024) weights, while the other is randomly initialized. To our surprise, UniTok that is trained from scratch surpasses the one initialized with pretrained CLIP weights, despite the latter actually achieves better zero-shot classification accuracy. This suggests downstream VQA performance may not be highly correlated with ImageNet classification accuracy. More importantly, it also implies that CLIP weight initialization may serve as negative prior for unified tokenizers, as the unified visual feature space could drastically differ from CLIP feature space. 5. Conclusion This paper studies unified visual tokenization for generation and understanding, which serves as the cornerstone of unified multimodal large language models. We investigate the training paradigm of unified tokenizers and identify that the current challenge in unification mainly arises from the limited representational power of discrete tokens. To address this limitation, we introduce multi-codebook quantization and attention-based factorization to build unified tokenizer called UniTok. We show that UniTok achieves comparable or even superior performance than domain-specific tokenizers, and excels in downstream visual generation and understanding tasks. The ablation study further reveals that discriminative and generative representation learning does not inherently conflict. We hope our findings could inspire future research in this domain. However, due to limited computational resources, UniTok is only trained for one epoch, which is not sufficient for CLIP-based semantic representation learning. We believe extending the training schedule could further benefit the tokenizer, especially in understanding performance."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 8 UniTok: Unified Tokenizer for Visual Generation and Understanding"
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3): 8, 2023. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Chen, J., Yu, Q., Shen, X., Yuille, A., and Chen, L.-C. Vitamin: Designing scalable vision models in the visionlanguage era. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1295412966, 2024. Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Gadre, S. Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten, R., Wortsman, M., Ghosh, D., Zhang, J., et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024. Ge, Y., Ge, Y., Zeng, Z., Wang, X., and Shan, Y. Planting seed of vision in large language model. arXiv preprint arXiv:2307.08041, 2023. Ge, Y., Zhao, S., Zhu, J., Ge, Y., Yi, K., Song, L., Li, C., Ding, X., and Shan, Y. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Hudson, D. A. and Manning, C. D. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6700 6709, 2019. Jin, Y., Xu, K., Chen, L., Liao, C., Tan, J., Chen, B., Lei, C., Liu, A., Song, C., Lei, X., et al. Unified language-vision pretraining with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669, 2023. Karras, T., Laine, S., and Aila, T. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019. Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Schindler, G., Hornung, R., Birodkar, V., Yan, J., Chiu, M.-C., et al. Videopoet: large language model for zeroshot video generation. arXiv preprint arXiv:2312.14125, 2023. Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., et al. The open images dataset v4: Unified image classification, object detection, and visual reInternational journal of lationship detection at scale. computer vision, 128(7):19561981, 2020. Lee, D., Kim, C., Kim, S., Cho, M., and Han, W.-S. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022. Li, D., Kamko, A., Akhgari, E., Sabet, A., Xu, L., and Doshi, S. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024a. Li, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre, S., Bansal, H., Guha, E., Keh, S., Arora, K., et al. Datacomplm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024b. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024c. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in large visionarXiv preprint arXiv:2305.10355, language models. 2023. Li, Y., Zhang, Y., Wang, C., Zhong, Z., Chen, Y., Chu, R., Liu, S., and Jia, J. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024d. 9 UniTok: Unified Tokenizer for Visual Generation and Understanding Lin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M., and Han, S. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26689 26699, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Lin, Z., Pathak, D., Li, B., Li, J., Xia, X., Neubig, G., Zhang, P., and Ramanan, D. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pp. 366384. Springer, 2025. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024a. Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024b. Lu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten, R., Hoiem, D., and Kembhavi, A. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2643926455, 2024. Ma, C., Jiang, Y., Wu, J., Yuan, Z., and Qi, X. Groma: Localized visual tokenization for grounding multimodal large language models. In European Conference on Computer Vision, pp. 417435. Springer, 2025. McKinzie, B., Gan, Z., Fauconnier, J.-P., Dodge, S., Zhang, B., Dufter, P., Shah, D., Du, X., Peng, F., Weers, F., et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. Minwoo, B., Beomhee, P., Haecheon, K., Sungjun, L., Woonhyuk, B., and Saehoon, K. Coyo-700m: Image-text pair dataset., 2022. URL https://github.com/ kakaobrain/coyo-dataset. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Radhakrishnan, M. Is midjourney-ai the new anti-hero of architectural imagery & creativity? 11:94114, 01 2023. doi: 10.11216/gsj.2023.01.102270. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022a. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022b. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35: 2527825294, 2022. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024a. Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X. GenarXiv preprint erative pretraining in multimodality. arXiv:2307.05222, 2023. Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Wang, Y., Rao, Y., Liu, J., Huang, T., and Wang, X. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024b. Team, C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Qu, L., Zhang, H., Liu, Y., Wang, X., Jiang, Y., Gao, Y., Ye, H., Du, D. K., Yuan, Z., and Wu, X. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. UniTok: Unified Tokenizer for Visual Generation and Understanding Tong, S., Brown, E., Wu, P., Woo, S., Middepogu, M., Akula, S. C., Yang, J., Yang, S., Iyer, A., Pan, X., et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024a. Tong, S., Fan, D., Zhu, J., Xiong, Y., Chen, X., Sinha, K., Rabbat, M., LeCun, Y., Xie, S., and Liu, Z. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024b. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Wang, G., Ge, Y., Ding, X., Kankanhalli, M., and Shan, Y. What makes for good visual tokenizers for large language models? arXiv preprint arXiv:2305.12223, 2023. Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q., et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 2482424837, 2022. Wu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., Ruan, C., et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024a. Wu, J., Jiang, Y., Ma, C., Liu, Y., Zhao, H., Yuan, Z., Bai, S., and Bai, X. Liquid: Language models are scalable and unified multi-modal generators. arXiv preprint arXiv:2412.04332, 2024b. Wu, J., Zhong, M., Xing, S., Lai, Z., Liu, Z., Wang, W., Chen, Z., Zhu, X., Lu, L., Lu, T., et al. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. arXiv preprint arXiv:2406.08394, 2024c. Wu, S., Fei, H., Qu, L., Ji, W., and Chua, T.-S. NextarXiv preprint llm. gpt: Any-to-any multimodal arXiv:2309.05519, 2023. Wu, Y., Zhang, Z., Chen, J., Tang, H., Li, D., Fang, Y., Zhu, L., Xie, E., Yin, H., Yi, L., et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024d. Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen, E. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. Yu, L., Cheng, Y., Sohn, K., Lezama, J., Zhang, H., Chang, H., Hauptmann, A. G., Yang, M.-H., Hao, Y., Essa, I., et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1045910469, 2023a. Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Birodkar, V., Gupta, A., Gu, X., et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023b. Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2(3), 2023c. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023d. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1197511986, 2023. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. 11 UniTok: Unified Tokenizer for Visual Generation and Understanding Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M., Shamis, L., Kahn, J., Ma, X., Zettlemoyer, L., and Levy, O. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. Zhu, L., Wei, F., Lu, Y., and Chen, D. Scaling the codebook size of vqgan to 100,000 with utilization rate of 99%. arXiv preprint arXiv:2406.11837, 2024. 12 UniTok: Unified Tokenizer for Visual Generation and Understanding A. Image Reconstruction Figure 6 presents qualitative results on the primary image reconstruction task. It can be seen that UniTok can faithfully reconstruct hard examples that contain small texts and human faces. The results further confirm that UniTok preserves detailed information such as texture and shape when encoding images. Figure 6. Qualitative results on image reconstruction in resolution of 256 256. B. More Generation Results We provide the results on GenAI-Bench (basic prompts) in Table 8, which measures the basic skills in understanding attributes, scenes, and relations in text inputs. We demonstrate that UniTok consistently delivers superior generation performance on this benchmark. Table 8. Comparison with other visual generation methods on GenAI-Bench (basic prompts). Method Type #Training Images Attribute Scene Relation Overall Spatial Action Part SD v2.1 (Rombach et al., 2022a) SD-XL (Podell et al., 2023) Midjourney v6 (Radhakrishnan, 2023) DALL-E 3 (Betker et al., 2023) Diffusion Diffusion Diffusion Diffusion Show-o (Xie et al., 2024) LWM (Liu et al., 2024b) VILA-U (Wu et al., 2024d) Liquid (Wu et al., 2024b) UniTok Discrete Diff. Autoregressive Autoregressive Autoregressive Autoregressive 2000M 2000M 36M 15M 30M 30M 0.80 0.84 0.88 0.91 0.72 0.63 0.78 0.84 0.85 0.79 0.84 0.87 0. 0.72 0.62 0.78 0.86 0.87 0.76 0.82 0.87 0.92 0.70 0.65 0.77 0.81 0.86 0.77 0.83 0.87 0.89 0.70 0.63 0.78 0.83 0.86 0.80 0.89 0.91 0. 0.75 0.70 0.79 0.91 0.89 0.78 0.83 0.87 0.90 0.70 0.63 0.76 0.83 0."
        }
    ],
    "affiliations": [
        "ByteDance Inc.",
        "Huazhong University of Science and Technology",
        "The University of Hong Kong"
    ]
}