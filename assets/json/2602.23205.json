{
    "paper_title": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents",
    "authors": [
        "Wenjia Wang",
        "Liang Pan",
        "Huaijin Pi",
        "Yuke Lou",
        "Xuqian Ren",
        "Yifan Wu",
        "Zhouyingcheng Liao",
        "Lei Yang",
        "Rishabh Dabral",
        "Christian Theobalt",
        "Taku Komura"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research."
        },
        {
            "title": "Start",
            "content": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents Wenjia Wang1 Zhouyingcheng Liao1 Liang Pan1 Huaijin Pi1 Yuke Lou1 Xuqian Ren2 Yifan Wu1 Lei Yang3 Rishabh Dabral4 Christian Theobalt4 Taku Komura1 6 2 0 F 6 2 ] . [ 1 5 0 2 3 2 . 2 0 6 2 : r (*: Core contributor.) 1The University of Hong Kong 2Tampere University 3The Chinese University of Hong Kong 4Max-Planck Institute for Informatics Figure 1. Introducing EmbodMocap, portable and low-cost system for simultaneous 4D human and scene reconstruction, deployable anywhere using two moving iPhones. The dataset captured by EmbodMocap benefits three crucial embodied AI tasks: monocular human & scene reconstruction, physics-based character animation, and real-world humanoid motion control. Project page."
        },
        {
            "title": "Abstract",
            "content": "Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within unified metric world coordinate frame. The proposed method allows metric-scale and sceneconsistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research. 1. Introduction Embodied Artificial Intelligence (Embodied AI) aims to build agents that can perceive, understand, and act within real-world environments. Progress in this field relies on datasets that capture both human motion and the surrounding 3D scene, enabling physically grounded perception and action learning. Such scene-aware data allows modeling of realistic humanscene interactions, simulation of lifelike behaviors, and training of humanoids to operate seamlessly in complex environments. They serve as foundation for advancing embodied reasoning and control across robotics, virtual reality, and computer vision. However, collecting high-quality humanscene data remains difficult. Precise 3D motion and scene geometry cannot be automatically obtained from internet videos due to occlusions and depth ambiguity. Existing capture systems that provide high-quality humanscene data typically rely on multi-view camera rigs [11, 74], wearable motion suits [22, 35], or LiDAR scanners [6, 19], which are costly, complex, and limited to controlled studio environments. These constraints hinder scalable and scene-aware data acquisition, limiting the ability of embodied AI models to learn from natural human behavior in diverse indoor and outdoor environments. In this paper, we propose EmbodMocap, an efficient and affordable framework for capturing metrically accurate 4D human and scene using only two iPhones. Our key idea is to jointly calibrate and optimize dual RGB-D inputs to reconstruct both humans and scenes within unified world coordinate frame. Specifically, we first reconstruct the static scene from single RGB-D sequence to define the world scale, then capture synchronized dual-view RGB-D videos of human motion, and finally perform geometric alignment and motion optimization to recover worldanchored human poses. In contrast to existing systems that rely on multi-camera rigs or wearable sensors, our approach achieves high-quality, scene-consistent reconstruction using only moving consumer devices. This design enables scalable, in-the-wild data collection that preserves precise human motion and authentic scene context, supporting realistic humanscene interaction modeling for embodied AI research. Based on the data collected with EmbodMocap, we demonstrate the reliability and versatility of our capture pipeline through three representative applications. The first application verifies geometric consistency, where we finetune reconstruction models to jointly recover humans and scenes in world coordinates. The second validates physical realism, showing that the captured motions enable scalable training of physics-based character skills and scene-aware motion tracking. The third demonstrates embodied transferability, where our data support humanoid robot training through sim-to-real motion tracking framework [26, 44]. These results highlight that EmbodMocap enables scalable and physically grounded data acquisition for embodied AI. In summary, our contributions can be summarized as follows: We introduce EmbodMocap, portable and affordable data collection pipeline that produces high-quality multimodal data for embodied AI applications. We validate our capture pipelines effectiveness across three key embodied AI tasks: monocular human-scene reconstruction, physics-based character animation, and real-world humanoid motion control. We provide scalable and accessible solution that lowers the barrier for embodied AI research, opening new possibilities for real-world applications and further advancements in the field. 2. Related Work Datasets for 4D Human & Scene Capture. Early motion datasets, such as AMASS [10, 36], focus on pure human motion, unifying multiple motion capture sources into large-scale repository. While invaluable for studying human motion, these datasets lack the 3D scene context essential for understanding humanscene interactions. Recent 4D datasets, like PROX [11], RICH [19], and EgoBody [74], combine scanned 3D scenes with motion capture using multi-view camera systems, while EMDB [22] and SPLOPER4D [6], employ IMUs or electromagnetic sensors for motion recording in large-scale environments. Nymeria [35] extends this further with Project Aria glasses and optical marker-based systems for wide-area motion capture. However, these approaches face notable limitations: marker-based and multi-camera systems are expensive and restricted to small studio environments, while IMU and EM-based methods, though more flexible, require extensive manual alignment and post-processing to synchronize motion with 3D scenes. And the wearable devices will influence the human appearance in RGB images. In contrast, our approach uses minimal equipment, operates in diverse environments without static camera setups, and avoids wearable devices, preserving the naturalness of RGB images for authentic humanscene interaction capture. Table 1 compares these datasets. Monocular Human & Scene Reconstruction. Early works [4, 8, 21, 24, 42] on RGB-based human mesh recovery focus on reconstructing 3D pose and shape but often ignore scene context [60] or camera information [25, 63], leading to inconsistencies under camera motion. Recent methods address this by combining motion cues [73], Table 1. Comparison of 4D Human & Scene datasets based on different features. Mocap Suit Scanner Device Static Cam. Dyna. Cam. Total Cost($) Mesh Outcome Dyna.Anno. Outdoor Datasets Publication PROX [11] RICH [19] ICCV2019 CVPR 2022 EgoBody [74] ECCV2022 - - - Structure Sensor Kinetic-One - Leica RTC 6-8Cameras 1IPhone 5Azure Kinect 1Camera Hololens2 SLOPER4D [6] CVPR2023 Noitom PN+NUC11 Ouster-os1 LiDAR EMDB [22] ICCV 2023 EM Sensors Nymeria [35] ECCV2024 2XSens+Aria Wistband - - EmbodMocap - - 1IPhone - - - - DJI-Action2+TLS 1IPhone 2Project Aria 2IPhone 2K 20K+ 9K 20K 15K 60K+ 1K SLAM or visual odometry [55, 65, 72], and human motion priors [54, 73] to recover global trajectories in world coordinates. to occlusion and depth ambiguities. In this paper, we propose method for high-precision human motion and scene reconstruction that overcomes these limitations. Emerging models move toward jointly reconstructing humans and 3D scenes with spatial intelligence models [61, 62]. For example, HSFM [38] combines Dust3R [62] with multi-view correspondence to jointly recover human meshes, scene point clouds, and camera parameters from multi-cameras. HAMSt3R [49] integrates DensePose [9] and multi-view scene reconstruction in one model, with an optimization to get human poses, while JOSH [29] uses MASt3R-SLAM [39] and joint optimization to achieve globally consistent 4D human-scene reconstructions. This trend emphasizes the simultaneous prediction of human motion and scene geometry, which futher requires multimodel data pairs with high-quality annotations. In our paper, we propose monocular human & scene reconstruction pipeline combined with 2 feedforward models, and finetuned it on our proposed dataset to prove the efficiency of our paired data. Training Humanoid from Video Data. Recent advances in physics-based animation and reinforcement learning enable humanoid agents to perform realistic and physically consistent motions using control policies learned from marker-based motion capture data. These methods have shown strong realism in tasks like motion tracking [32, 44], locomotion [33, 45, 46], and humanscene interaction [41, 64], and have been extended to real-world applications in motion tracking [15, 17, 20], locomotion [16], and scene interaction [3, 14]. However, marker-based methods require dedicated studios, expensive hardware, and extensive manual effort, making them costly and hard to scale. Adapting captured motions to new scenes or robot morphologies also demands complex retargeting and re-simulation. To address this, recent works like VideoMimic [2], ASAP [17], and HDMI [67] train humanoid control directly from inthe-wild video data. By using monocular motion capture methods such as TRAM [65] and GVHMR [54], they estimate human motion from videos and retarget it to virtual humanoids for training in physical simulators. This videodriven paradigm leverages diverse real-world data but struggles with capturing complex skills or scene geometries due 3. Proposed Capture System We aim to capture metrically accurate human motion and scene geometry using only two iPhones. As shown in Fig. 2, our capture process consists of four sequential stages that progressively reconstruct and align the scene, cameras, and human motion within unified world coordinate frame. We first reconstruct metrically accurate static scene and establish the world reference using single iPhone RGBD sequence (Sec. 3.1). Then, we use two synchronized iPhones to record dual-view RGB-D videos of human motion and extract per-frame camera poses and human priors with off-the-shelf perception models (Sec. 3.2). Next, we align the dual-view camera trajectories to the reconstructed scene through combination of COLMAP registration and multi-view geometric optimization (Sec. 3.3). Finally, we refine the SMPL parameters by triangulating dual-view 2D keypoints into 3D space and optimizing human poses and translations in the world coordinate system (Sec. 3.4). 3.1. Stage I: Scene Reconstruction In this stage, we aim to reconstruct metrically accurate, Z-up scene mesh that serves as the reference world coordinate system. We first use single iPhone to capture an RGB-D video of the scene, along with synchronized IMU data. The recorded data are processed by the SpectacularAI SDK (SAI) [1], which automatically selects keyframes according to the accumulated camera translation and estimates corresponding camera parameters (Ks, Rs,n, Ts,n) in Z-up world coordinates with metric scale. These trajectories establish consistent world frame for all subsequent stages. Based on the recovered poses, we refine the iPhone LiDAR depth maps using PromptDA [27], unproject them into 3D space, and integrate the point clouds through TSDF fusion [5] to obtain dense and metrically accurate global mesh Mg. Note that the depth maps are truncated based on threshold determined by the effective range of the iPhones depth sensor. Specifically, we use threshold of 3.5m for indoor scenes and 5m for outdoor Figure 2. EmbodMocap: We propose an affordable dataset capture and processing system. From left to right, the four stages (StageI to Stage-IV) illustrate our core logic: leveraging high-quality camera matrices provided by SpectacularAI [1] and aligning sequence coordinates to the scenes world frame. For detailed explanations, please refer to Sec. 3. scenes. We further apply lightweight post-processing such as outlier removal and small-component filtering to clean the mesh. Finally, we extract SIFT features from the same SAI keyframes and run COLMAP [51] with fixed camera parameters to build sparse structure database. This database preserves the metric scale and serves as reference for registering dual-view sequences in later stages. streams. By identifying the frame index where the laser dot disappears, we temporally align both videos and slice all associated image, depth, and parameter data accordingly. This process yields synchronized dual-view RGB-D sequences with calibrated camera trajectories and per-frame human priors, providing clean inputs for subsequent sequence calibration. 3.2. Stage II: Sequence Processing After reconstructing the static scene in Stage I, we proceed to capture and process dual-view human motion sequences within the same environment. In this stage, we use two iPhones to record synchronized RGB-D videos of performer moving inside the reconstructed scene, with each device providing an independent camera coordinate system. The goal is to convert these raw dual-view videos into temporally aligned and metrically consistent per-frame human and camera information, which will serve as the foundation for subsequent calibration and motion optimization. Firstly, we use SAI to obtain per-frame calibrated Let denote the view incameras for each view. dex (v {v1, v2}), and let index time. For each view independently, SAI provides intrinsics and extrinsics (Kv, Rv,t, Tv,t) for every decoded frame Iv,t in the native coordinate system of that view. Next, we extract human-related information using several off-the-shelf models: (i) YOLO [56] for person detection and proposal pruning; (ii) ViTPose [70] for 2D human keypoints with confidence scores; (iii) SAM2 [48] for person segmentation masks; (iv) PromptDA [27] to refine dual-view depths; and (v) VIMO [65] for camera space SMPL parameters. Finally, we employ laser pointer cue for frame-level synchronization between the two camera 3.3. Stage III: Sequence Calibration After obtaining the static scene reconstruction in Stage 3.1 and the dual-view camera trajectories in Stage 3.2, the next step is to align all coordinate systems into unified world frame. At this point, we have three separate coordinate systems: one for the reconstructed scene and two for each iPhone camera trajectory estimated by SAI. Since the dualview coordinate systems differ from the scene coordinate system only by rigid transformations, our goal is to optimize these 2 rigid transformations to unify the dual-view coordinates into the same metric, gravity-aligned world frame. The optimization process is sensitive to the initial values; therefore, it is necessary to first obtain good initial estimate for the rigid transformations. Get Initial Transformation from COLMAP. We register each dual-view sequence to the sparse COLMAP model constructed in Stage 3.1 using the known intrinsics Kv and background-only SIFT features Fv, extracted from images with human regions removed. Matches are established through trained vocabulary tree [52], and images are registered against the sparse COLMAP model to obtain COLMAP camera poses ( ˆRv,t, ˆTv,t) in the same metric, gravity-aligned world coordinates as the scene. To obtain the initial rigid transformation aligning the SAI camera trajectories v, with their COLMAP counterparts ˆT v, t, we solve for an offset (soff , Roff , off ) by minimizing: transformation min soff ,Roff ,T off (cid:88) t=1 (cid:13) (cid:13) ˆTt (soff Roff Tt + off )(cid:13) 2 2, (cid:13) (1) where is the number of frames. After centering the trajectories, we solve this minimization problem using singular value decomposition (SVD). For gravity alignment, Roff is constrained to rotations about the z-axis, ensuring proper alignment of SAI trajectories with the COLMAP coordinate system. Calibration via Multiple Constraints. While the rigid transformations obtained in the previous step provide coarse alignment between the two camera trajectories and the reconstructed scene, this initialization alone is not sufficient to achieve accurate synchronization and metric consistency. To further refine the calibration, we jointly optimize all alignment parameters by introducing multiple geometric and photometric constraints across views. Specifically, we optimize the per-view global offsets Roff (constrained to zv axis rotations) and off , using the initial alignment as the starting value. The aligned camera extrinsics are: the same point across views. The Chamfer distance term dChamfer aligns local pointclouds (v {v1, v2}) with the global reconstruction sampled from Mg in Sec. 3.1, where is obtained by reconstructing the scene using the method from Sec. 3.1 with humans cropped by masks. The Chamfer distance is formally defined as: dChamfer(P v, g) = 1 v (cid:88) min pgP pv pg2 2 pvP (cid:88) + 1 g pgP min pvP pg pv2 2. (6) Finally, Lba,v (v {v1, v2}) ensures reprojection consistency for persistent matches, where the points are obtained from COLMAP image registration: Lba,v = 1 Mv (cid:88) (t,j)Mv (cid:13) (cid:13)xv,t,j π(Kv, Rali v,t, ali v,t, Xj)(cid:13) 2 2. (cid:13) We solve Eq. (3) using the Adam [23] optimizer with gradient clipping. For yaw-only updates, Roff is parameterv ized by single z-axis angle to preserve gravity alignment. (7) Rali v,t = Roff Rv,t, ali v,t = Roff Tv,t + off . (2) 3.4. Stage IV: Motion Optimization The optimization minimizes composite loss of point tracking loss, Chamfer distance, and bundle adjustment loss to ensure spatial consistency between views and the global reconstruction. Lcalib = λtrackLtrack + (cid:88) λchdChamfer + (cid:88) λbaLba,v. (3) Through VGGT tracking, subset of keyframes is selected, yielding accurate dual-view pixel tracking results in the human masks region. The tracked human surface 2D pixel coordinates q(i) v,t, along with their corresponding depth values d(i) v,t, are back-projected into the world frame: Q(i) v,t = d(i) v,tRali v,t K1 (cid:21) (cid:20)q(i) v,t 1 + Rali v,t ali v,t, (4) To enforce track consistency between views, the following loss is minimized: Ltrack = 1 v,t Qv,t (cid:80) (cid:88) (cid:88) w(i) (cid:13) (cid:13)Q(i) 1,t Q(i) 2,t (cid:13) 2 2, (cid:13) (5) 1,t and Q(i) Where Q(i) 2,t are the 3D back-projected coordinates of the i-th point from view 1 and view 2, respectively. The weights w(i) are used to control the contribution of each point based on its tracking confidence. Here w(i) = min(w(i) 2,t) combines the VGGT confidence scores for 1,t, w(i) After obtaining calibrated dual-view trajectories and unified scene coordinate system in Stage 3.3, we further refine the human reconstruction results to achieve accurate and temporally consistent body motions in the world frame. At this stage, both camera poses and scene geometry are fixed, allowing us to focus on optimizing the human parameters. We first triangulate dual-view 2D keypoints into world-space 3D keypoints, which serve as reliable geometric constraints across views. Then, we optimize the SMPL parameters using these triangulated 3D keypoints to recover precise body poses and translations under the unified world coordinate system. 3D Keypoint Triangulation. To triangulate the 3D keypoints Yt,j from their 2D projections {yv,t,j}, we estimate the 3D position by minimizing the weighted reprojection error across all views: min Yt,j (cid:88) v=1 cv,t,j (cid:13) (cid:13)yv,t,j PvYt,j (cid:13) 2 2, (cid:13) (8) where Pv = Kv[Rv,t Tv,t] is the camera projection matrix for the v-th view. The problem can be formulated as weighted least squares optimization. Using SVD, Yt,j is obtained as the right singular vector corresponding to the smallest singular value of A. World-Space SMPLify. Start from initial shape β0 and body pose θb,0 in Sec. 3.2, our World Frame SMPLify [30] jointly optimizes shape β R10, per-frame pose θt = , θb } R72 and root translation γt R3 by minimiz- {θg ing: LSMPLify = L3D + Lsmooth + Lprior + Lreproj (9) We use two-stage optimization phase to ensure the smoothness and alignment with the original dual views. For the first stage, we only fit the body shape and transition, and for the second stage we fit all the parameters. 4. Evaluation In this section, we aim to prove the effectness of our optimization pipeline. We will first ablate different loss functions of the pipeline in Sec. 4.1, then compare ours with the monocular model, single-view only and optical captured ground truth. 4.1. Ablation Study on Loss Functions Ablation on dataset optimization. We conduct an ablation study on four core loss functions that significantly influence performance during data optimization, as described in main paper. These loss functions include tracking loss, Chamfer distance, reprojection loss, smoothness loss and kp3d loss. To evaluate the performance under different optimization settings, we employ four metrics. First, IoU(Intersection over Union) measures the overlap between the rendered SMPL mask and the SAM2 [48] mask. Second, Reproj evaluates the pixel error between the reprojected SMPL joints and the 2D keypoints detected by VITPose [70]. Third, Depth error is computed as the mean squared error (MSE) between the rendered depth from SMPL parameters and the sensor depths refined by PromptDA [27]. Finally, Jitter is quantified using the same temporal foot skating metric as MotionVAE [28]. All metrics are averaged across all sequences and views to ensure robust evaluation. The Ltrack effectively stitches the two views together, significantly improving the overall reconstruction performance, making it highly impactful on the final results. The Lkp3d provides 3D joint positions of the human body, and compared to the reprojection loss, it eliminates the issue of depth ambiguity, thus playing critical role in the overall performance. Table 2. The performance of different optimization settings. Ltrack Lchamf er Lreproj Lsmooth Lkp3d IoU(%) Reproj Depth Jitter 54.3 72.5 72.3 72. 59.3 73.0 44.2 10.9 11.1 10. 20.4 9.3 2.372 0.081 0.079 0. 0.609 0.078 0.0371 0.0131 0.0130 0. 0.0126 0.0128 4.2. Comparison on Capture Methods Direct comparison in optical mocap studio. To evaluate the accuracy of dual view capture system, we set up furniture in mocap studio and use Vicon system to capture ground truth human motion. Two photographers record dual-view videos of the actor with iPhones, while the actor performs basic motions(see Fig. 3, zoom in). We record 5 sequences of one participant with 9420 frames in total. We compare the errors against optical mocap GT of: monocular model GVHMR, our dual-view optimization, and our single-view version(v1 and v2). For the single-view version, we calibrate the actor coordinates to the scene coordinates system using COLMAP and optimize the motion with reprojection, smooth, and prior losses. The optical mocap results are fitted to SMPLX parameters by Mosh [31] and synchronized to dual-view parameters with foot contact keyframs. Results are compared in chunk sizes of 100, 500, and 1000. Our dual-view method outperforms the monocular model and single-view optimization by large margin. As the chunk length increases, our advantage becomes increasingly evident. (see Tab. 3) Figure 3. Our dual view vs. single view results in optical studio. Table 3. Comparision among monocular model, single view optimization, with dual view optimization(ours) Method GVHMR Single-View V1 Single-View V2 Dual View chunk=100 chunk=500 WA-MPJPE W-MPJPE WA-MPJPE W-MPJPE WA-MPJPE W-MPJPE chunk= 66.56 124.68 108.31 56.61 123.44 218.22 211.83 72.86 124.61 233.06 231.41 76.90 333.34 489.11 357.22 99.75 179.47 297.83 338.42 119.45 593.79 768.31 762.80 169. RTE 1.85 2.71 3.65 1.13 The advantage of dual-view over single-view lies in two key aspects: 1)dual-view effectively addresses occlusion and self-occlusion of body joints, 2)it handles the challenging alignment of actor motion coordinates to the scene coordinates. The COLMAP estimates the camera locations for the images but suffers from depth ambiguity in the cameras facing direction. Using single iPhone results in large errors in the depth direction. In contrast, using two iPhones enables pixel-wise dense correspondence(see Eq. (5)), which ensures the rigid transformation between the two cameras during the optimization, and resolves the depth ambiguity in each view. This enables good localization of human trajectories in the scene coordinate system automatically. Our dual view could achieve calibration accuracy to the scene of about 5cm (human touching table in the figure), while the single view is over 30cm, measured in MeshLab by putting markers on the ground for the actors start and end positions. 5. Downstream Tasks In this section, we validate our capture pipelines effectiveness across three key applications. In Sec. 5.1, we propose monocular human & scene reconstruction pipeline and finetune it with our captured RGBD, cameras, and SMPL annotations. In Sec. 5.2, we train several human-object interaction skills and scene-aware motion tracking with our captured motion & scene. In Sec. 5.3, we train humanoid in simulator and deploy it to real-world robot. 5.1. Monocular Human & Scene Reconstruction Motivation. We propose data scheme combining RGBD data from dynamic cameras with camera and human motion parameters to train monocular human and scene reconstruction models. As no feedforward model exists, we establish baseline using π3[66] for SLAM and VIMO[65] for metricscale human motion reconstruction from monocular videos. Implementation. To process long sequences, videos are divided into overlapping chunks, with π3 estimating camera parameters and local point maps per chunk. Adjacent chunks are aligned using Procrustes alignment, and scale/transformations are recursively applied for global consistency. Metric scale is determined as the median ratio of SMPL to π3 depth values. SMPL predictions are then transformed to metric world space. For details, refer to Supp. Mat. We fine-tuned two π3 variants Tab. 4 by adding LoRA [18] layers to the camera and point decoders, supervised with the original π3 loss. For VIMO, we froze the encoder and finetuned the decoder with MSE loss on SMPL parameters. human mask was used to limit supervision to the human region due to our datasets smaller range. Metrics. We evaluate motion and trajectory accuracy on global coordinates using EMDB (subset 2)[22], featuring extended sequences with ground-truth trajectories and meshes. Consistent with prior work[55, 65], each sequence is split into 100-frame chunks, and 3D joint errors are measured using W-MPJPE (aligning the first two frames) and WA-MPJPE (aligning the entire segment), both in millimeters. Additionally, Root Translation Error (RTE) is reported as percentage (%), normalized by total displacement after rigid alignment (excluding scaling). Results. We present 3 variants in Tab. 4: the proposed baseline with the original checkpoints from π3 [66] and VIMO [65], fine-tuning only VIMO, and fine-tuning both π3 and VIMO. The results demonstrate that our approach significantly improves the accuracy of VIMO, as we provide paired high-quality real-world RGB sequences and ground truth SMPL parameters. Additionally, leveraging our highquality RGB-D data and camera parameter pairs, π3s ability to predict in the world coordinate system also shows improvement. Our pipeline shows good performance on largescale real-world videos, see Fig. 4 Table 4. Comparison of Finetuned Models on EMDB Benchmarks Finetuned EMDB Pi3 VIMO WA-MPJPE W-MPJPE RTE 229.04 222.93 220.65 83.56 82.89 82.21 1.78 1.73 1.71 Figure 4. Quality results of proposed 4D Human & Scene Reconstruction pipeline on EMDB dataset. 5.2. Physics-based Character Animation 5.2.1. Human Object Interaction Skill Training Motivation. We train several human-object interaction skills to demonstrate the physical realism of our approach and the scalability of our capture framework to new interaction skills. We aim to prove the efficiency and quality superiority of our framework over optical capture and monocular estimation methods. Implementation. Following [41, 45, 64], we train physical character policies use goal-conditioned reinforcement learning to formulate character control as Markov Decision Process (MDP) defined by states, actions, transition dynamics, reward function r, and discount factor γ. The reward rt is calculated by style reward rstyle [45] and task reward rtask . The policies are trained to maximize the (cid:105) expected discounted return: J(π) = Ep(τ π) , where is the episode length, γ [0, 1] is the discount factor, and rt is the reward at time step t. We use the widely adopted Proximal Policy Optimization (PPO) algorithm [53] to train the control policy model. t=0 γtrt (cid:104)(cid:80)T 1 Following [13, 41, 64], we train set of human object interaction skills in simulator [37], including follow, climb, sit, and lie. These common interaction skills are designed to guide the characters root joint to reach specific target positions in 3D environments while maintaining physically realistic and motion divisty. We train these four common skills on 3 different input data: optical captured, which are collected from AMASS [36] and SAMP [12] following TokenHSI [41]; ours, by segmenting the reconstructed motions into skill clips; monocular, by using the motion predicted by GVHMR [54] which is commonly used in humanoid reference motion prediction[17, 67], segmented with the same temporal slices as ours. We also train 2 extra interaction skills which have not been implemented in previous physics-based human object interaction papers: Prone and Support. We will illustrate the observation, reward designs, and the training details of each skill in Supp.Mat. Metrics. We follow [12, 68] that uses Success Rate and Contact Error as the main metrics to measure the quality of interactions quantitatively. Success Rate records the percentage of trials that humanoids successfully complete the contact within certain threshold. We follow [13, 40, 68] in setting the thresholds for various actions: 20cm for Sit, Follow, and Climb; 30cm for Lie and Prone; and 10cm for Support. For Support, the error is defined as the distance from the object surface center to the hand center, while also taking into account the distance between the two feet. Please see details in Supp.Mat. We evaluate motion diversity using Average Pairwise Distance (APD) [7], which measures the average pairwise distance between joint rotations and positions in generated samples. Higher APD values indicate greater diversity. Results. We can find in Tab. 5, for skills such as Follow, Climb, and Sit, the inherent difficulty is relatively low, and all three data settings achieve good results, very close to 100%. Although the quality of our data is slightly inferior to optically captured data, we provide more variety of task completion trajectories and motion diversities, which contribute to improve task performance. To prove this, we ablate on skills trained with different data proportions. 1X and 2X indicate the ratio of the number of clips relative to the optical capture data. On the 4 common skills, we observe general trend where increased data amount leads to improvements in success rate, contact error, and APD metrics. We also implement 2 extra skills, Prone and Support, demonstrate the versatility of our data collection pipeline. First, these new skills highlight the ability of our approach to generalize to novel interaction tasks. Second, the Support skill significantly increases the level of difficulty. Unlike other tasks, where humanoid only needs to walk or offload the full body weight onto furniture surface, Support requires the hands to bear the weight of the body while the feet remain close together, demanding much higher accuracy in reference motion generation. This experiment shows that our approach outperforms monocular estimation methods by large margin, particularly for high-difficulty interaction skills. The success rate trained on monocular estimated Table 5. Comparison of data duration, Success Rate, Contact Error, and APD for different skills among 3 data settings. Task Data Clips Duration (min) Rate (%) Error (cm) APD Optical Mocap Ours 1X Ours 2X Ours Full Monocular Optical Mocap Ours 1X Ours 2X Ours Full Monocular Optical Mocap Ours 1X Ours 2X Ours Full Monocular Optical Mocap Ours 1X Ours 2X Ours Full Monocular Ours Full Monocular Ours Full Monocular Follow Climb Sit Lie Prone Support 12 12 24 148 148 7 7 14 21 21 20 20 40 80 10 10 20 39 39 3 3 8 8 1.59 1.48 3.06 22.43 22.43 0.28 0.54 0.97 1.54 1.54 4.08 2.11 4.47 8.05 8. 2.52 0.99 2.32 4.25 4.25 0.26 0.26 0.97 0.97 99.9 99.9 99.7 99.8 98.0 99.9 99.8 99.9 99.9 99.2 98.0 99.8 99.9 99.9 98. 89.0 85.3 86.3 89.4 81.2 75.4 71.2 66.0 20.6 6.0 6.7 6.8 6.2 7.2 2.7 1.8 1.8 1.8 1.8 5.5 5.4 5.1 4.7 5. 17.5 20.2 19.8 18.8 21.0 16.5 16.5 4.9 6.4 20.17 0.19 18.42 0.22 18.45 0.17 19.69 0.32 19.85 0.39 22.03 0.30 22.77 0.29 20.72 0.30 22.22 0.27 21.34 0.38 16.07 0.39 14.35 0.27 14.46 0.24 15.90 0.51 15.80 0. 8.76 0.14 7.43 0.10 8.27 0.06 8.57 0.10 8.14 0.10 17.58 0.69 16.18 0.30 21.08 0.59 20.94 0.48 (a) Qualitative comparison on 4 basic skills. (b) Qualitative comparison on 2 additional skills. In Fig. 5b, we motions degrades to only 20% in Tab. 5. can see policy trained on motion estimated from monocular models could not perform standard Support skill. 5.2.2. Scene-aware Motion Tracking Motivation. Recent works [33, 34, 46, 5759, 71] suggest that solving complex tasks requires pre-training on largescale human motion data via motion tracking objectives, in order to obtain reusable and generalizable skill priors. However, existing motion tracking frameworks are mainly built for human-only [32] or single-object interaction [69] scenarios, primarily because current public datasets are conFigure 6. We present qualitative results of scene-aware motion tracking, showing four long-term motion examples in different scenes (a, b, c, and d), including daily indoor and outdoor interactions such as walking, sitting, lying, stair climbing, and touching. Our motion tracking framework not only accurately tracks the reference motion but also ensures physical realism, resolving subtle issues, such as interpenetration and floating artifacts, present in the reference data (see zoomed-in views on the right). Table 6. Quantitative evaluation of scene-aware motion tracking and dataset statistics across four 3D scenes. Scene Clips Duration (min) Status Rate (%) Eps. Len. (s) c 14 6 12 12.31 3.62 7.87 5.06 Succ. Fail. Succ. Fail. Succ. Fail. Succ. Fail. 87.2 12.8 96.7 3.3 95.9 4.1 90.4 9. 9.97 0.21 3.94 2.10 9.99 0.12 4.16 2.38 9.98 0.17 5.43 2.18 9.96 0.21 4.44 1.92 centrated in these settings. We argue that motion tracking pre-training on diverse 3D scenes is equally important, as it also provides rich priorssuch as navigation, interaction, and long-horizon task execution. In this work, we mitigate this gap by: 1) proposing scene-aware motion tracking framework, and 2) supporting it with high-fidelity paired 3D human-scene data captured by our EmbodMocap system. Implementation. We extend MimicKit [43] by incorporating the height map into the observation space to achieve scene-aware tracking (details in the Supp. Mat.). For training, we use four 3D scenes, each containing several minutes of motion clips, and train one policy per scene to track all the motion clips in that scene. Metrics. Policies are evaluated using success rate metric: an episode is initialized from random frame and run for 10s, and is considered successful if tracking exceeds 8s. For each scene, 3,072 episodes are used to compute average success, failure rates, and episode length statistics. Results. The quantitative results in Tab. 6 demonstrate that our data is simulation-ready, enabling the training of sceneaware tracking policies with high success rates. The qualitative results, shown in Fig. 6, further illustrate that the policies not only successfully track the motions but also adapt to subtle imperfections present in the data. 5.3. Real-world Humanoid Robot Control Motivation. Learning from human videos [2, 47, 67] has emerged as crucial paradigm for humanoid robots to learn motor skills at scale. In this section, we demonstrate how EmbodMocap contributes to this paradigm by enabling accurate reconstruction of humans and their interacting 3D environments from videos, while preserving accurate contact information. Implementation. We capture videos of humans performing ground-contact-rich motions, including locomotion and challenging cartwheels that require precise hand-ground contact. EmbodMocap is then used for real-to-sim reconstruction. The produced motions are used to train single tracking policy via sim-to-real RL with domain randomization using BeyondMimic [26]. Results. We deploy the policy on real-world High Torque Hi humanoid robot with 21 joint DoF and height of 80cm. As shown in Fig. 7, the robot successfully replicates human motions from videos, demonstrating that EmbodMocap produces data of sufficient quality for humanoid robot control. 8. Acknowledge We sincerely thank Mr. Xiaohan Ye and Mr. Rui Xu for volunteering as actors during data collection. Figure 7. real-world humanoid robot imitating human motions depicted in videos. 6. Conclusion We propose EmbodMocap, portable and affordable framework for capturing high-quality 4D human & scene data using only two iPhones. Our method enables scalable, metrically accurate reconstruction of human motion and scenes mesh in diverse real-world environments. We directly compare in optical capture studios, and prove the superiority in solving body occlusion and sequence coordinate alignment of our dual view designing. Through downstream applications in monocular human-scene reconstruction, physicsbased character animation, and humanoid robot motion control, we demonstrate the effectiveness and scalability of our approach. By lowering the barrier for embodied AI research, EmbodMocap opens new opportunities for realworld applications. 7. Limitations and Future Work. Our data collection pipeline encounters limitations in specific scenarios. For example, it fails to record depth when the distance exceeds the range of the iPhone LiDAR sensor (approximately 5 meters). Additionally, it struggles with scenes dominated by moving objects, which degrade the results of the SLAM SDK [1]. Extremely bright lighting conditions can also cause COLMAP failures, leading to incorrect registration. Future work could integrate more robust structure-from-motion tools, such as H-Loc [50], to improve reliability. Moreover, incorporating automatic synchronization APPs on iPhone could further reduce human effort."
        },
        {
            "title": "References",
            "content": "[1] Spectacular ai sdk. https://www.spectacularai. com, 2021. 3, 4, 10, 1 [2] Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, and Angjoo Kanazawa. Visual imitation enables contextual humanoid control. arXiv:2505.03729, 2025. 3, 9 [3] Qingwei Ben, Feiyu Jia, Jia Zeng, Junting Dong, Dahua Lin, and Jiangmiao Pang. Homie: Humanoid loco-manipulation with isomorphic exoskeleton cockpit. arXiv:2502.13013, 2025. 3 [4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it smpl: Automatic estimation of 3d human pose and shape from single image. In ECCV, 2016. 2 [5] Brian Curless and Marc Levoy. volumetric method for building complex models from range images. In CGIT, 1996. [6] Yudi Dai, YiTai Lin, XiPing Lin, Chenglu Wen, Lan Xu, Hongwei Yi, Siqi Shen, Yuexin Ma, and Cheng Wang. Sloper4d: scene-aware dataset for global 4d human pose estimation in urban environments. In CVPR, 2023. 2, 3 [7] Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, and Wenping Wang. ase: Learning conditional adversarIn SIGial skill embeddings for physics-based characters. GRAPH, 2023. 8 [8] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In ICCV, 2023. 2 [9] Rıza Alp Guler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 72977306, 2018. 3 [10] Felix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in-betweening. 2020. 2 [11] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael Black. Resolving 3d human pose ambiguities with 3d scene constraints. In ICCV, 2019. 2, 3 [12] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael Black. Stochastic scene-aware motion prediction. In ICCV, 2021. 8 [13] Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, and Xue Bin Peng. Synthesizing physical character-scene interactions. In SIGGRAPH, 2023. 7, [14] Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Universal and dexterous human-to-humanoid whole-body teleoperation and learning. arXiv:2406.08858, 2024. 3 Omnih2o: [15] Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, LearnKris Kitani, Changliu Liu, and Guanya Shi. ing human-to-humanoid real-time whole-body teleoperation. arXiv:2403.04436, 2024. 3 [16] Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, Linxi Fan, and Yuke Zhu. Hover: Versatile neural whole-body controller for humanoid robots. arXiv:2410.21229, 2024. 3 [17] Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbabu, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris Kitani, Jessica Hodgins, Linxi Jim Fan, Yuke Zhu, Changliu Liu, and Guanya Shi. Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills. arXiv preprint arXiv:2502.01143, 2025. 3, 8 [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [19] Chun-Hao Huang, Hongwei Yi, Markus Hoschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael Black. Capturing and inferring dense full-body human-scene contact. In CVPR, 2022. 2, 3 [20] Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin Cheng, and Xiaolong Wang. Exbody2: Advanced expressive humanoid whole-body control. arXiv:2412.13196, 2024. 3 [21] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In CVPR, 2018. 2 [22] Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan Jose Zarate, and Otmar Hilliges. Emdb: The electromagnetic database of global 3d human pose and shape in the wild. In ICCV, 2023. 2, 3, 7 [23] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv:1412.6980, 2014. 5 [24] Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black. VIBE: Video inference for human body pose and shape estimation. In CVPR, 2020. [25] Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch, Lea Muller, Otmar Hilliges, and Michael J. Black. SPEC: Seeing people in the wild with an estimated camera. In ICCV, 2021. 2 [26] Qiayuan Liao, Takara Truong, Xiaoyu Huang, Guy Tevet, Koushil Sreenath, and Karen Liu. Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion. arXiv preprint arXiv:2508.08241, 2025. 2, 9 [27] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1707017080, 2025. 3, 4, 6 [28] Hung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. Character controllers using motion vaes. TOG, 2020. 6 [29] Zhizheng Liu, Joe Lin, Wayne Wu, and Bolei Zhou. Joint optimization for 4d human-scene reconstruction in the wild. arXiv:2501.02158, 2025. 3 [30] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. Smpl: skinned multiperson linear model. TOG, 2015. 5 [31] Matthew M. Loper, Naureen Mahmood, and Michael J. Black. MoSh: Motion and shape capture from sparse markers. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia), 33(6):220:1220:13, 2014. [32] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for real-time simulated avatars. In ICCV, 2023. 3, 8 [33] Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris Kitani, and Weipeng Xu. Universal humanoid motion representations for physics-based control. arXiv:2310.04582, 2023. 3, 8 [34] Zhengyi Luo, Jinkun Cao, Sammy Christen, Alexander Winkler, Kris Kitani, and Weipeng Xu. Grasping diverse objects with simulated humanoids. arXiv:2407.11385, 2024. 8 [35] Lingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov, Yifeng Jiang, Rowan Postyeni, Luis Pesqueira, Alexander Gamino, Vijay Baiyya, Hyo Jin Kim, et al. Nymeria: massive collection of multimodal egocentric daily motion in the wild. In ECCV, 2024. 2, 3 [36] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive of motion capture as surface shapes. In ICCV, 2019. 2, 8 [37] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Isaac Nikita Rudin, Arthur Allshire, Ankur Handa, et al. gym: High performance gpu-based physics simulation for robot learning. arXiv:2108.10470, 2021. 7 [38] Lea Muller, Hongsuk Choi, Anthony Zhang, Brent Yi, Jitendra Malik, and Angjoo Kanazawa. Reconstructing people, places, and cameras. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2194821958, 2025. 3 [39] Riku Murai, Eric Dexheimer, and Andrew Davison. Mast3r-slam: Real-time dense slam with 3d reconstruction priors. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1669516705, 2025. 3 [40] Liang Pan, Jingbo Wang, Buzhen Huang, Junyu Zhang, Haofan Wang, Xu Tang, and Yangang Wang. Synthesizing physically plausible human motions in 3d scenes. In 3DV, 2024. [41] Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, and Jingbo Wang. Tokenhsi: Unified synthesis of physical human-scene interactions through task tokenization. In CVPR, 2025. 3, 7, 8 [42] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from single image. In CVPR, 2019. 2 [43] Xue Bin Peng. Mimickit: reinforcement learning framework for motion imitation and control, 2025. 9 [44] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. TOG, 2018. 2, 3 [45] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. TOG, 2021. 3, 7 [46] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. TOG, 2022. 3, 8 [47] Ri-Zhao Qiu, Shiqi Yang, Xuxin Cheng, Chaitanya Chawla, Jialong Li, Tairan He, Ge Yan, David Yoon, Ryan Hoque, Lars Paulsen, et al. Humanoid policy human policy. arXiv preprint arXiv:2503.13441, 2025. [48] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv:2408.00714, 2024. 4, 6 [49] Sara Rojas, Matthieu Armando, Bernard Ghanem, Philippe Weinzaepfel, Vincent Leroy, and Gregory Rogez. Hamst3r: Human-aware multi-view stereo 3d reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 50275037, 2025. 3 [50] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, 2019. 10 [51] Johannes Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In CVPR, 2016. 4 [52] Johannes Lutz Schonberger, True Price, Torsten Sattler, JanMichael Frahm, and Marc Pollefeys. vote-and-verify strategy for fast spatial verification in image retrieval. In ACCV, 2016. [53] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 7 [54] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia, 2024. 3, 8 [55] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael Black. Wham: Reconstructing world-grounded humans with accurate 3d motion. In CVPR, 2024. 3, 7 [56] Juan Terven, Diana-Margarita Cordova-Esparza, and JulioAlejandro Romero-Gonzalez. comprehensive review of yolo architectures in computer vision: From yolov1 to yolov8 and yolo-nas. Machine learning and knowledge extraction, 2023. 4 [57] Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, and Xue Bin Peng. Maskedmimic: Unified physics-based character control through masked motion inpainting. TOG, 2024. 8 [58] Chen Tessler, Yifeng Jiang, Erwin Coumans, Zhengyi Luo, Gal Chechik, and Xue Bin Peng. Maskedmanipulator: Versatile whole-body control for loco-manipulation. arXiv preprint arXiv:2505.19086, 2025. [59] Andrea Tirinzoni, Ahmed Touati, Jesse Farebrother, Mateusz Guzek, Anssi Kanervisto, Yingchen Xu, Alessandro Lazaric, and Matteo Pirotta. Zero-shot whole-body humanoid control via behavioral foundation models. In The Thirteenth International Conference on Learning Representations. 8 [60] Shashank Tripathi, Agniv Chatterjee, Jean-Claude Passy, Hongwei Yi, Dimitrios Tzionas, and Michael J. Black. DECO: Dense estimation of 3D human-scene contact in the wild. In ICCV, 2023. 2 [61] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, 2025. 3 [62] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 3 [63] Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qingping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, and Taku Komura. Zolly: Zoom focal length correctly for perspectivedistorted human mesh reconstruction. In ICCV, 2023. 2 [64] Wenjia Wang, Liang Pan, Zhiyang Dou, Jidong Mei, Zhouyingcheng Liao, Yuke Lou, Yifan Wu, Lei Yang, Jingbo Wang, and Taku Komura. Sims: Simulating stylized humanscene interactions with retrieval-augmented script generation. ICCV., 2025. 3, 7 [65] Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from inthe-wild videos. In ECCV. Springer, 2024. 3, 4, 7 [66] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Scalable permutation-equivariant visual geometry learning, 2025. [67] Haoyang Weng, Yitang Li, Nikhil Sobanbabu, Zihan Wang, Zhengyi Luo, Tairan He, Deva Ramanan, and Guanya Shi. Hdmi: Learning interactive humanoid whole-body control from human videos. arXiv:2509.16757, 2025. 3, 8, 9 [68] Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, and Jiangmiao Pang. Unified human-scene interaction via prompted chain-of-contacts. In ICLR, 2024. 8 [69] Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, and LiangYan Gui. Towards universal wholebody control for physics-based human-object interactions. arXiv:2502.20390, 2025. 8 Intermimic: [70] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. NeurIPS, 2022. 4, 6 [71] Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, and Libin Liu. Moconvq: Unified physicsbased motion control via scalable discrete representations. ACM Transactions on Graphics (TOG), 43(4):121, 2024. 8 [72] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In CVPR, 2023. 3 [73] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusion-aware human mesh recovery with dynamic cameras. In CVPR, 2022. 2, [74] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Egobody: Human body shape and motion of interacting people from head-mounted devices. In ECCV. Springer, 2022. 2, 3 EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents"
        },
        {
            "title": "Supplementary Material",
            "content": "9. More Details of EmbodMocap 9.1. Capture technique The primary capture technique involves two photographers, each holding an iPhone in vertical orientation. The photographers are required to maintain certain angle relative to each other while following the performer. To achieve optimal triangulation during post-processing, the angle between the two cameras should ideally fall within the range of 60 to 120 degrees. This configuration not only enhances the accuracy of triangulation but also ensures the capture of the performer from multiple perspectives, providing diverse viewpoint information for keypoint detection. Additionally, the photographers should aim to keep the cameras in motion to dynamically adjust their positions and minimize occlusion caused by objects in the environment. Figure 8. Capture technique. 9.2. Human Labor Analysis Temporal Synchronization. This step only needs the operator to identify and input the frame indices where the laser pointers spot disappears into .xlsx file. Typically, this process takes only about 1 minute per sequence. Skill Segmentation. Skill segmentation is only required when training physical interaction skills. The operator annotates each skills category, start, and end times based on the video, typically taking 0.5 to 2 minutes per sequence. Contact Label & Optimization. In the main text, we mention that the alignment between our sequence and the scene coordinate system relies on photometric (COLMAP, pixel tracking) and geometric constraints (chamfer distance). However, this can sometimes result in alignment errors of few centimeters, primarily due to depth inaccuracies in COLMAPs sparse keypoints and depth errors from the iPhone sensor. To address this issue, we propose an optional post-processing solution. During data capture, we place markers in the scene and instruct the performer to begin walking from designated marker and stop on another at the end of the sequence, standing still on the same marker. Annotating contact frame indices costs 1-2 minutes for each sequence. These markers serve as fixed reference In post-processing, we observe the points for alignment. corresponding marker positions on the reconstructed mesh and record their 3D coordinates, along with the frame indices where the performer stands on the markers. Using this information, we optimize rigid transformation to align the center of the performers feet at the specified frame indices to the 3D coordinates of the markers. (cid:21) = (cid:20)R(ϕc) Tc 1 0 Since SAI [1] could generate Z-up metric-scaled camera matrices, we define the rigid transformation in the xyplane, defined by rotation angle ϕc about the z-axis and translation Tc. This can be represented by homogeneous transformation matrix : cos(ϕc) sin(ϕc) 0 tx ty cos(ϕc) sin(ϕc) tz 0 0 1 0 0 (10) This matrix transform the center of lowest point on both feet to match the annotate marker. To robustly solve for the transformation parameters, we employ gradient descent optimization, constrained by minimizing contact loss to match the contact marker: 0 1 0 = Lcontact = 1 Nc (cid:88) (cid:16) iC min (V (i)) c(i) (cid:17)2 (11) For SMPL parameters, the global orientation is updated as θg = Rcθg. For translation, the pelviss world position is transformed as = RcPw + Tc. Re-evaluating the SMPL model with θg gives the local pelvis offset , and the updated translation is γ = . The updated camera rotation and translation are computed as = Tv RvRcT Tc, ensuring alignment and consistency of the scene representation. = RvRcT and 10. More Details of Monocular Human-Scene"
        },
        {
            "title": "Reconstruction Pipeline",
            "content": "reconstruction baseline is modular Our monocular pipeline for reconstructing 3D human pose and scene geometry from monocular video, combining two independent The point clouds and SMPL global orientation and translation are transformed to the world coordinate system with R, following the same formula as Sec. 9.2. 11. More Details of Human-Object Interaction"
        },
        {
            "title": "Skills",
            "content": "11.1. Follow Skill 0.1, xτ 0.1, xτ 0.2, . . . , xτ Definition. The path following task requires the simulated character to move along predefined 2D trajectory. trajectory is represented as τ = {xτ }, where xτ 0.1 denotes 2D waypoint at simulation time 0.1s, and is the episode length. For this task, is set to 10s. The character is expected to follow the trajectory τ as accurately as possible. Task Observation. At each simulation time step t, the character observes 10 future waypoints sampled over the next t+0.8, xτ 1.0s: {xτ t+0.9}. These waypoints are sampled at intervals of 0.1s using linear interpolation from the trajectory τ . The 2D coordinates of these waypoints form the task observation gf R210. Task Reward. The reward for this task, rf , is computed based on the distance between the characters current 2D root position, xroot 2d . The reward is defined as: , and the target waypoint, xτ t+0.1, . . . , xτ , xτ = exp (cid:0) 2.0xroot 2d rf xτ 2(cid:1). (16) Figure 9. An example in finding the contact marker in software (e.g., Meshlab) and corresponding keyframe index(the frames selected here are just for demo). modules: π3 for camera trajectory prediction and scene point cloud reconstruction, and VIMO for SMPL-based human pose estimation. To process long video sequences, π3 divides frames into overlapping chunks, where each chunk independently predicts camera poses Tv RT 44 and local point clouds Plocal RT HW 3. To align these chunks into global coordinate system, Procrustes analysis is applied to the overlapping regions of adjacent chunks. Given two point clouds X, RN 3, the alignment minimizes the error: min s,R,t (sRX + t)2 , (12) 11.2. Sit Skill where is the scale, is the rotation matrix, and is the translation vector. Using SVD, the optimal alignment parameters are computed as: = SU , = , = sR X, trace(Y trace(X RXc) Xc) (13) where Xc, Yc are the centered point clouds, and , are derived from the SVD of the covariance matrix = Yc. After chunk alignment, VIMO predicts SMPL parameters (θ, γ, β), where θ RT 72 represents joint rotations, γ RT 3 is the root translation, and β R10 defines body shape. Using weak perspective camera model, SMPL vertices are projected onto the image plane as: ximg = sxv + (14) where is the scaling factor proportional to 1/z. To resolve scale ambiguity, the pipeline estimates metric scale by matching the predicted depths of SMPL vertices zSMPL (in meters) with the depths of Pi3s point cloud zPi3 (in arbitrary units) on some sampled points. The scale factor is computed as: = median (cid:18) zπ3 zSMPL (cid:19) , (15) Definition. The sitting task requires the character to position its root joint at target 3D sitting location on an object surface. The target position is defined as 10 cm above the center of the top surface of the chair seat. R38 includes Task Observation. The observation gs the 3D target sitting position R3, the 3D root position R3, the root rotation R6, the 2D front-facing direction R2, and the positions of eight corner points of the objects bounding box R38. Task Reward. The sitting task reward rs encourages the character to minimize the distance between its 3D root position, xroot , and the target sitting position, xtar . It is defined as: (cid:40) rs = 0.7 rnear 0.7 rnear + 0.3 rfar , + 0.3, xroot 2d xobj 2d otherwise, > 0.5, (17) where rfar are defined as: and rnear = exp (cid:0) 2.01.5 rfar = exp (cid:0) 10.0xtar rnear Here, xobj 2d is the 2D position of the objects root, xroot 2d is the 2D linear velocity of the characters root, and is horizontal unit vector pointing from xroot 2d xroot 2d xroot 2(cid:1). 2(cid:1), (18) (19) to xobj 2d . 11.3. Climb Skill The near reward focuses on lying accuracy: Definition. The climbing task requires the character to place its root joint at target 3D climbing position on given object. The target position is set 94 cm above the center of the top surface of the object. R27 includes Task Observation. The observation gm the 3D target root position R3 and the 3D coordinates of eight corner points of the objects bounding box R38. Task Reward. The climbing task reward rm 3D distance between the characters root, xroot get location, xtar minimizes the , and the tart . The reward is defined as: rm = (cid:40) 0.5 rnear 0.5 rnear + 0.2 rfar , + 0.2 + 0.3 rfoot xobj 2d , otherwise, xroot 2d > 0.7, where rnear are defined as: , and rfoot , rfar = exp (cid:0) 10.0xtar rnear = exp (cid:0) 2.01.5 rfar xroot 2(cid:1), xroot 2d 2(cid:1), = exp (cid:0) 50.0(xtar rfoot 0.94) xfoot t 2(cid:1). (20) (21) (22) (23) is the height of the target root position, (xtar Here, xtar 0.94) represents the height of the top surface of the target object in world coordinates, and xfoot is the mean height of the characters feet. The reward rfoot encourages the character to lift its feet and is crucial for successful climbing. 11.4. Lie Skill Definition. The lying task requires the character to position its root joint at target 3D lying position on an object, typically centered on the objects surface. The character must first approach designated standing point before transitioning into the lying position. R38 includes Task Observation. The observation gl the 3D target lying position R3, the 3D root position R3, the root rotation R6, the 2D front-facing direction R2, and the positions of eight corner points of the objects bounding box R38. It also includes the chosen standing point R3. Task Reward. The lying reward rl combines rewards for approaching the standing point and accurately lying down: = 0.5 rpos rnear + 0.3 rhead + 0.2 ralignment , (26) minimizes the distance to the target, rhead aligns where rpos head height, and ralignment rewards proper body alignment. 11.5. Prone Skill Definition. The prone task requires the character to position its root joint at designated 3D prone position on an object, typically centered on the objects surface. Unlike the lying task, the character must face downward while maintaining alignment with the target surface. Task Observation. The observation gp R35 includes the 3D target prone position R3, the 3D root position R3, the root rotation R6, the 2D front-facing direction R2, and the positions of eight corner points of the objects bounding box R38. These observations help guide the approach and ensure the correct orientation for prone positioning. Task Reward. The prone reward rp encourages the character to transition smoothly from moving to prone position while maintaining proper alignment and facing downward. The reward is defined as: rp = (cid:40) + 0.3 rfar 0.7 rnear , rnear , t xtar xroot otherwise. > 1.5, (27) The far reward encourages approaching the target prone position: = 0.5 rwalk rfar + 0.2 rvel + 0.2 rfacing + 0.1 rheight , (28) where rwalk aligns velocity with the direction of motion, rfacing proper facing direction, and rheight an appropriate height during approach. rewards moving toward the prone position, rvel ensures encourages maintaining The near reward focuses on prone accuracy: = 0.6 rpos rnear + 0.2 ralignment + 0.2 rface down , (29) where rpos ralignment and rface down down orientation. minimizes the distance to the prone target, ensures proper body alignment with the surface, rewards the character for maintaining face- (cid:40) rl = 0.6 rnear + 0.4 rfar rnear , t , xroot xtar otherwise. > 1.5, (24) 11.6. Support Skill The far reward encourages approaching the standing point: = 0.5 rwalk rfar + 0.2 rvel + 0.2 rfacing + 0.1 rstand , (25) where rwalk aligns velocity, rfacing rstand rewards correct height. rewards walking toward the standing point, rvel ensures proper facing direction, and Definition. The support task encourages the character to approach target object and maintain stable interaction by placing its hands on the top surface while keeping stable foot placement and proper posture. R27 conTask Observation. The task observation gm sists of the 3D target position of the objects top surface R3) and the 3D coordinates of the eight center (xo corner points of the objects bounding box (bt R38). , zo is defined as: > 1.5, (cid:40) rm = , xo + 0.6rs 0.4rf rs , Task Reward. The total reward rm xr otherwise, 2(cid:1) = 0.5 exp (cid:0) 0.5xo rf xr + 0.5 exp (cid:0) 2.01.5 2(cid:1), xr + 0.2ro + 0.15rt = 0.3rh rs + 0.2rg + 0.15rz , (30) (31) (32) (33) where rf and rs encourages the character to approach the object, combines five components for stable interaction: (35) (34) (36) 2(cid:1) 2(cid:1), = 0.6 exp (cid:0) 20zh zo rh + 0.4 exp (cid:0) 5xh2 xo zg2(cid:1), = exp (cid:0) 50zf rg = exp (cid:0) 10xf xf rt = exp (cid:0) 21.0 (ub ro = exp (cid:0) 10zr 2(cid:1). rz and xr denote the 2D positions of the object and the characters root, while zo are their respective and zh heights. xh2 represent the 2D position and height of the hands. Similarly, xf refer to the 2D positions and height of the feet, zg is the ground height, and ub is the vertical component of the bodys up direction. 2(cid:1), t)2(cid:1), , and zf and zr Here, xo zo , xf (37) (38) (39) (a) Camera Trajectory Length Distribution. (b) Human Trajectory Length Distribution. (c) Scene Mesh Area Distribution. (d) Sequence Length Distribution. Figure 10. Statistical information of collected dataset. Evaluation The evaluation of the Support task focuses on the agents ability to position its hands on the top surface of the target object and keep its feet close together. The key metric is the combined XY-plane distance and Z-axis deviation between the hands and the objects top surface. The task is deemed successful if the hands are within predefined thresholds and the feet maintain adequate proximity for stability. 12. More Details of Scene-Aware Imitation Policy 12.1. Representations Character Proprioception. The state describes the proprioception of the characters body, with features consisting of the relative positions of each link with respect to the root (designated to be the pelvis), their rotations expressed in quaternions, and their linear and angular velocities. All features are computed in the characters local coordinate frame, with the root at the origin and the x-axis along the root links facing direction. Height Map. To perceive the surrounding scene geometry, we utilize local egocentric height map. This map is structured as an 11 11 grid spanning 2m 2m area centered on the humanoid, resulting in sampling interval of 0.2m. The grid is defined within the characters local coordinate frame; consequently, the sampling points dynamically translate and rotate with the humanoids movement and heading, consistently covering the immediate vicinity. The height values at these grid points are queried from high-resolution underlying scene mesh (0.05m resolution) using nearest-neighbor interpolation. Target States. The target state ˆq encodes the desired future motion of the character. It is constructed by sampling short trajectory segment from the dataset spanning three consecutive future time steps: T, + 1, and + 2. For each time step, the state comprises the positions, rotations, linear velocities, and angular velocities of all body links. All features are transformed from the world frame into the simulated characters local coordinate frame. This local frame is defined with the characters root located at the origin and the x-axis aligned with the root links facing direction. Action. Our simulated humanoid is constructed based on the SMPL body model, comprising 23 controllable joints. Each joint possesses 3 degrees of freedom (DoF), and we employ Proportional-Derivative (PD) controller for each DoF. Consequently, the action R69 generated by the policy specifies the target orientations for these PD controllers. 12.2. Reward and jitter penalty rsmooth To encourage the character to closely reproduce the reference motion while maintaining motion naturalness, our reward function rt is composed of two terms: tracking reward rtrack . The tracking reward incentivizes the policy to minimize the kinematic error between the simulated character and the reference motion. The jitter penalty is introduced to suppress abnormal shaking generated when the character interacts with objects, which may be induced by instabilities in the physics simulation. The total reward is defined as: rt = rtrack rsmooth . (40) Figure 11. Rendered SMPL and depth images of the captured dataset in camera space. is computed as the weighted sum The tracking reward rtrack of exponential differences across all humanoid links: = wjp exp (cid:0)100 ˆpt pt2(cid:1) rtrack + wjr exp (cid:0)10 ˆqt qt2(cid:1) + wjv exp (cid:0)0.1ˆvt vt2(cid:1) + wjω exp (cid:0)0.1 ˆωt ωt2(cid:1) , where the equation penalizes the differences in translation p, rotation q, linear velocity v, and angular velocity ω for all rigid body links of the humanoid between the simulation and the reference. The jitter penalty penalizes the magnitude of the difference between consecutive actions, defined as: rsmooth = at at12, (42) where at and at1 denote the action at the current and previous time steps, respectively. By minimizing the rate of change of the actions, the policy is incentivized to generate continuous and stable control trajectories, thereby reducing jittery behaviors. (41) 13. More Details of Captured Dataset Used in"
        },
        {
            "title": "Main Paper",
            "content": "We collected data from 23 scenes, each with highprecision mesh, 104 sequences, and approximately 200,000 video frames. Each frame is accompanied by corresponding depth maps, segmentation masks, camera trajectories, and human parameters(bounding boxes, 2D keypoints, SMPL parameters). In Fig. 10a, we present the distribution of camera trajectory lengths, which range from 4 meters to over 30 meters. In Fig. 10b, the human trajectory length distribution is shown, with performers moving between 5 meters and over 30 meters. Figure 10c illustrates the scene mesh area distribution. Indoor scenes are relatively smaller, ranging from 20 to 90 square meters, while outdoor scenes can be as large as 200 square meters. Finally, in Fig. 10d, we show the sequence length distribution, where most sequences have durations ranging from 30 to 60 seconds. 13.1. Qualitative Demonstrations We show camera space results in Sec. 11.6 and world space results in Sec. 13.1 Figure 12. 3D demo of the captured dataset."
        }
    ],
    "affiliations": [
        "Max-Planck Institute for Informatics",
        "Tampere University",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong"
    ]
}