{
    "paper_title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
    "authors": [
        "Qinghe Wang",
        "Xiaoyu Shi",
        "Baolu Li",
        "Weikang Bian",
        "Quande Liu",
        "Huchuan Lu",
        "Xintao Wang",
        "Pengfei Wan",
        "Kun Gai",
        "Xu Jia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework."
        },
        {
            "title": "Start",
            "content": "MultiShotMaster: Controllable Multi-Shot Video Generation Framework Qinghe Wang1 Xiaoyu Shi2(cid:0) Baolu Li1 Weikang Bian3 Quande Liu2 Huchuan Lu1 Xintao Wang2 Pengfei Wan2 Kun Gai2 Xu Jia1(cid:0) 5 2 0 2 ] . [ 1 1 4 0 3 0 . 2 1 5 2 : r 1Dalian University of Technology 2Kling Team, Kuaishou Technology 3The Chinese University of Hong Kong https://qinghew.github.io/MultiShotMaster Figure 1. We propose MultiShotMaster, the first controllable multi-shot video generation framework that supports text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot counts and shot durations are variable. Only the global caption of the first case is shown for brevity."
        },
        {
            "title": "Abstract",
            "content": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these Work done during an internship at Kling Team, Kuaishou Tech. (cid:0) Corresponding authors. challenges, we propose MultiShotMaster, framework for highly controllable multi-shot video generation. We extend pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference 1 tokens and grounding signals, enabling spatiotemporalgrounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven intershot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework. 1. Introduction Recently, riding the wave of success in video generation [6], content creators have produced many interesting videos and attracted massive traffic. Powered by diffusion transformers (DiTs) [27, 36, 45, 60, 69], the semantic information encapsulated in text prompts can be presented in singleshot high-quality generated video. Furthermore, advances in controllability [32, 61, 65] have endowed video generation with increased versatility and creative capabilities by incorporating diverse condition signals such as reference image [21, 29] and object motion [13, 49]. However, realworld films and television series consist of multi-shot video clips with narrative structures that convey coherent stories to audiences. Such storytelling relies on cinematic language, encompassing holistic scenes, character interactions, and microexpressions. Therefore, there is significant gap between current video generation techniques and practical video content creation. Ideally, the AI-powered video generation system should deliver director-level controllable multi-shot functionality to users, encompassing four critical aspects: (1) variable shot counts and flexible shot durations, (2) dedicated text descriptions for each shot, (3) character appearance and scene definition, (4) character movement control. The development of this vision is still in its infancy. Current multi-shot video generation methods typically fall into two paradigms: text-to-keyframe generation followed by imageto-video (I2V) generation [9, 58, 62, 70], and direct endto-end generation [7, 15, 22, 37, 57]. The keyframe-based paradigm first generates set of keyframes with visual consistency and then uses the I2V model to generate each shot with the corresponding keyframe. Although it has decent applicability, the limited conditional capability of sparse keyframes cannot cover the briefly-appearing characters and scene consistency outside the keyframes. The end-toend paradigm can generate multi-shot videos directly, and keep the consistency by using full attention along the temporal dimension. Yet, it is still constrained by the fixed shot duration or limited shot counts. Moreover, both paradigms can only be driven by text prompts. As result, there is an urgent need to investigate comprehensive controllability in multi-shot video generation, covering flexible shot arrangements and more condition signals. Under the DiT architecture, all image patches are projected into token embeddings and concatenated together, typically requiring positional encoding (e.g., Rotary Position Embedding (RoPE) [44]) to preserve their spatiotemporal order. The RoPE-based attention mechanism has crucial property: tokens with closer spatiotemporal distance receive higher attention weights, enabling the model to capture local spatiotemporal correlations. This property inspires our two insights: (1) using continuous RoPE to all frames of multi-shot videos in temporal order will cause the model to confuse intra-shot consecutive frames with intershot frames across shot boundaries. (2) applying the RoPE of specified region to reference features will bridge the connection with the corresponding video tokens. In this work, we present MultiShotMaster, the first framework for highly controllable multi-shot video generation. Specifically, we extend pretrained single-shot text-to-video model to controllable multi-shot generation primarily through improvements in RoPE. First, we introduce Multi-Shot Narrative RoPE that breaks RoPEs continuity at shot transitions and helps the model recognize shot boundaries, enabling controllable shot transitions. Furthermore, we design Spatiotemporal Position-Aware RoPE, which incorporates spatiotemporal-grounded control signals into RoPE for reference tokens (subjects and backIt establishes strong correlations between refgrounds). erence tokens and the corresponding video tokens during attention, allowing reference injection into specified spatiotemporal regions. By specifying multiple frames, users can further control subject motion. To manage the incontext information flows between reference and video tokens, we develop Multi-Shot & Multi-Reference Attention Mask. In addition, we propose an automatic MultiShot & Multi-Reference Data Curation pipeline to provide essential training data. Comprehensive evaluations demonstrate its superior performance and outstanding controllability. We integrate diverse conditions, including text prompts, subjects, grounding signals, and backgrounds for multi-shot video generation with flexible shot arrangement, as shown in Fig. 1. We anticipate that this work could inspire future research in controllable multi-shot video generation. 2. Related Works 2.1. Text-to-Video Generation Early methods inflated the pretrained text-to-image genlayers [14, 56] for eration models [39] with temporal video generation, achieving preliminary short video animation. Recent approaches have employed the diffusion 2 Figure 2. Overview of MultiShotMaster. We extend pretrained single-shot T2V model by two key RoPE variants: Multi-Shot Narrative RoPE for flexible shot arrangement with temporal narrative order, and Spatiotemporal Position-Aware RoPE for grounded reference injection. To manage in-context information flows, we design Multi-Shot & Multi-Reference Attention Mask. We finetune temporal attention, cross attention and FFN, leveraging the intrinsic architectural properties to achieve flexible and controllable multi-shot video generation. transformer (DiT) [36] architectures, which could generate longer, high-quality videos with detailed text description [6, 12, 27, 45, 60, 69]. However, scaling video generation beyond short clips remains an open problem. Existing research focuses on two distinct tasks: single-shot and multi-shot long video generation. The former typically encounters issues of error accumulation and memory loss [8, 19, 20, 63, 64], while the latter focuses on narrative coherence and inter-shot consistency [7, 15, 37, 58]. 2.2. Multi-Shot Video Generation Multi-shot videos should preserve narrative logic and ensure spatiotemporal consistency of character positioning and scene layout across all shots [17, 34]. The existing paradigms mainly comprise text-to-keyframe generation & image-to-video generation [58, 67, 70] and end-toend holistic generation [7, 15, 22, 37, 51, 57]. The former depends on the generation quality of keyframes and cannot cover the character and scene consistency outside the keyframes. The latter exhibits better consistency, benefiting from the full attention along the temporal dimension. We observe that the multi-shot videos with fixed shot duration might contain boring frames. For instance, when director wants to capture close-up of an actor picking up drink, such an insert shot requires only few frames for audiences to understand the content. To tackle this problem, ShotAdapter [22] incorporates learnable transition tokens that interact only with shot-boundary frames to indicate transitions. CineTrans [57] constructs an attention mask to weaken the inter-shot correlations, enabling transitions at predefined positions. In contrast, our work conveys the transition signals by manipulating the RoPE embeddings, which prevents interference with token interactions in pretrained attention and explicitly achieves shot transitions. 2.3. Controllable Video Generation Providing explicit and precise user control is essential for practical content creation [47, 48, 53, 55]. The controllable video generation field supports diverse control signal types such as camera motion [13, 16], object motion [31, 33, 41, 59], reference video [4, 5, 26, 30]. VACE [21] and Phantom [29] support multi-reference video generation and achieve realistic composition. Tora [68] and Motion Prompting [13] control the object motion through point tra3 jectories. However, existing methods typically focus on the single-shot setting and adopt separate adapters for reference injection and motion control. If following the traditional paradigm [29, 49], controllability in multi-shot settings would require larger networks and incur higher computational costs. To address this limitation, we propose the first controllable multi-shot framework that supports reference injection and motion control jointly, requiring no additional adapters. 3. Method 3.1. Evolving from Single-Shot to Multi-Shot T2V Preliminary: Our model is developed upon pretrained single-shot text-to-video (T2V) model with 1B parameters, which consists of 3D Variational Auto-Encoder (VAE) [24], T5 text encoder [38] and latent diffusion transformer (DiT) model [36]. Each basic DiT block contains sequence including 2D spatial self-attention module, 3D spatiotemporal self-attention module, text crossattention module and feed-forward network (FFN). We define straight path from clean data z0 to noised data zτ at timestep τ using Rectified Flow [11]: zt = (1 τ )z0 + τ ϵ, where ϵ (0, I). The denoising process follows the ordinary differential equation: dzτ = vΘ(zτ , τ, ctext)dτ , where vΘ is the denoising network. The training objective is to regress velocity [28]: LLCM = Eτ,ϵ,z0 (cid:2)(z1 z0) vΘ(zτ , τ, ctext)2 (cid:3) (1) As shown in Fig. 2, to adapt the input from single-shot to multi-shot videos with the sudden content changes at shot boundaries, we encode each shot separately through 3D VAE and then concatenate the video latents. During temporal attention, the original 3D-RoPE assigns sequential indices along the temporal dimension, leading to critical issue: the model cannot distinguish between intra-shot consecutive frames with inter-shot frames across shot boundaries. To explicitly help the model perceive shot boundaries, we propose Multi-Shot Narrative RoPE mechanism that introduces an angular phase shift into the original 3D-RoPE for each transition. The query (Q) of i-th shot is computed as follows, and similarly for key (K): Qi = RoPE((t + iϕ) f, f, ) Qi (2) enables users to flexibly configure both the number of shots and their respective durations. Considering that providing users with the capability to customize each shot individually could facilitate content creation, we design hierarchical prompt structure. It includes global caption that describes subject appearances and environments, and per-shot captions that detail subject actions, backgrounds, and camera, following [15]. For each shot, we combine the global caption with the corresponding per-shot caption. In the vanilla T2V model, T5 encoder encodes the text prompts as text embeddings which are then replicated along the temporal dimension to align with the video frame sequence for text-frame cross attention. Accordingly, we replicate each shots text embeddings to align with the corresponding shot frame count, enabling shot-level cross-attention. 3.2. Spatiotemporal-Grounded Reference Injection Users typically require the capability to provide reference images (e.g., subjects and backgrounds) and motion control signal for creating customized video content. To address this requirement, we propose Spatiotemporal Position-Aware RoPE to improve in-context learning for spatiotemporal-grounded reference injection. Specifically, we individually encode each reference image into the latent space through 3D VAE and concatenate them with the noised video latents via token concatenation. In temporal attention, clean reference tokens propagate visual information to noisy video tokens for reference injection. Furthermore, 3D-RoPE enables tokens with closer spatiotemporal distance to attend more to each other. Inspired by this mechanism, we apply 3D-RoPE from specified regions to the corresponding reference tokens, thereby establishing strong correlations between region-specified video tokens and reference tokens, as shown in Fig. 2. Since the subject bounding box region (x1, y1, x2, y2) at t-th frame is smaller than the spatial dimensions (H, ) of reference tokens, we sample the 3D-RoPE by: Qref = RoPE((t + iϕ) f, href f, wref ) Qref , href = y1 + wref = x1 + (y2 y1) (x2 x1) j, [0, 1], (3) k, [0, 1] where (t, h, w) are spatiotemporal position indices, ϕ is the angular phase shift factor, is the decreasing base frequency vector, and denotes the element-wise rotary transformation of query embeddings Qi via complex rotations. This design not only maintains the narrative shooting order of inter-shot frames, but also leverages RoPEs inherent rotational properties to mark shot boundaries through fixed phase shifts, requiring no additional trainable parameters. It In this manner, we can control the subject to appear at specified spatiotemporal position. To control the motion trajectory of subject, we create multiple copies of the subject tokens and assign different spatiotemporal RoPE to each copy. The temporal attention then transfers the subject motion embedded in these copies to video tokens at the corresponding spatiotemporal positions. The copied tokens of each subject will be averaged after attention. Similarly, 4 Figure 3. Data Curation Pipeline: (1) We employ shot transition detection model [43] to cut the collected long videos into short clips, use scene segmentation model [54] to cluster clips within the same scene, and then sample multi-shot videos. (2) We introduce hierarchical caption structure and use Gemini-2.5 [10] in two-stage process to produce global caption and per-shot captions. (3) We integrate YOLOv11 [23], ByteTrack [66] and SAM [25] to detect, track and segment subject images. Then we use Gemini-2.5 to merge the per-shot tracking results by subject appearance. We obtain clean backgrounds by using OmniEraser [52]. to achieve multi-shot scene customization, we copy the 3DRoPE from the first frame of each shot and apply it to the corresponding background tokens. We further clarify the details in temporal attention in Appendix A.1. By incorporating spatiotemporal-controllable multireference injection, we significantly expand the functional boundaries of multi-shot video generation, providing users with more flexible and powerful video creation capabilities. It allows users to customize characters using subject images and control their positions and movements, which improves the practicality of multi-shot video generation. In addition, using multiple background images of scene can achieve customized multi-shot scene consistency, reducing the necessity for location shooting. 3.3. Multi-Shot & Multi-Reference Attention Mask The user-provided reference images and subject copies may lead to excessively long contexts with high computational costs. On the other hand, there are unnecessary interactions between in-context tokens. For example, Subject 0 may only appear in shot 2, but unconstrained token concatenation allows other shots to access Subject 1 as well. Although 3D-RoPE can guide spatiotemporal-specified video tokens to attend to reference tokens, the small but non-zero attention weights still pose content leakage risk. Therefore, we design multi-shot & multi-reference attention mask to constrain information flow and optimize attention allocation. As shown in Fig. 2, we maintain full attention across all multi-shot video tokens for global consistency, and limit each shot to access only the reference tokens within its own shot. Accordingly, the reference tokens of each shot can only attend to each other and the video tokens within the same shot. This attention mask strategy effectively ensures that each shot focuses on the intra-shot reference injection, and keeps global consistency by inter-shot full attention. 3.4. Training and Inference Paradigm Typically, controllable capabilities are trained based on foundation generation model. Considering that the reference injection task requires training on large-scale data to learn diverse subjects, and the construction cost of multishot & multi-reference data is relatively high, we first train spatiotemporal-specified reference injection on 300k single-shot data. We sample bounding boxes with random starting points and 1-second intervals, where each bounding box has 0.5 drop probability. This sparse bounding box sequence allows users to control subjects easily. In the second stage, we train on the constructed multi-shot & multireference data. To enable controllable multi-shot video generation with text-, subject-, background-, and joint-driven modes, we randomly drop the subject and background, each with 0.5 probability. We notice that the training objective Eq. 1 focuses on global consistency and ignores details. Therefore, we propose cross-shot subject-focused posttraining that assigns (2) loss weight to subject regions versus (1) to backgrounds. It not only improves subject consistency but also enables the model to better comprehend how the subjects change across different shots. More details could be found in Appendix A.2. During inference, our framework supports controllable multi-shot video generation with text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene consistency. Both shot count and duration are flexibly configurable. This versatile framework opens new possibilities for diverse multishot video content creation, enabling users to craft highly customized video narratives. 3.5. Multi-Shot & Multi-Reference Data Curation Multi-Shot Videos: To construct multi-shot video dataset, as shown in Fig. 3, we first crawl long videos from the Internet (including diverse types: movies, television series, documentaries, cooking demonstrations, sports and fitness). Then we use TransNet V2 [43] to detect shot transitions and crop out massive single-shot videos. To merge the single-shot videos captured in the same scene, we employ scene segmentation method [54] that could understand the storyline of the video to figure out where scene starts and ends. It might cluster videos spanning tens of minutes within the same scene into single group. We further sample multi-shot videos using the following strategy: shot count ranges from 1 to 5, frame count ranges from 77 to 308 (i.e., 5-20 seconds at 15 fps), with priority given to samples with higher frame counts and more shots. Caption Definition: To provide users with the ability to define characters and customize each shot, we employ hierarchical caption structure: global caption and per-shot captions, following [15]. We first use Gemini-2.5 [10] to understand the entire multi-shot video and produce global caption, where each subject is denoted by Subject X, [1, 2, 3, ]. Subsequently, based on the global caption and each shot video, we employ Gemini-2.5 to reason the per-shot captions, using the predefined Subject across all captions. It helps the model understand crossshot subject consistency. More details could be found in Appendix A.3. Reference Images Collection: We first apply YOLOv11 [23], ByteTrack [66] and SAM [25] to detect, track and segment subject images. Due to the presence of shot transitions, the tracking process is conducted shot-by-shot. This process produces the shot-level track IDs and their bounding box sequences. To merge the tracking results across all shots, we choose the largest subject image of each shot-level track ID from each shot, and group them using GeminiIn this 2.5 (Details could be found in Appendix A.4). way, we obtain complete multi-shot tracking results and the corresponding subject images. In addition, we feed the first frames of each shot and its foreground mask into OmniEraser [52] to obtain clean backgrounds. 4. Experiment 4.1. Experimental Setup Implementation Details. Our framework is based on pretrained single-shot T2V model with only 1B parameters at the resolution of 384 672. We conduct experiments for controllable multi-shot video generation on narrative videos containing 77-308 frames at 15 fps (i.e., 5-20 seconds), with each video comprising 1-5 shots. We encode each shot separately through 3D VAE [24], and employ sliding window strategy to encode and decode shots with > 77 frames, which maintains the alignment between pixel space and latent space for multi-shot videos. We train the model on 32 GPUs, with learning rate of 1 105, batch size 1. The angular phase shift factor of Multi-Shot Narrative RoPE is set to 0.5 by default. During inference, we set the classifierfree guidance scale [18] as 7.5 and DDIM [42] steps as 50. More details can be found in the supplementary materials. Baselines. We compare our work with two multi-shot video generation methods [46, 57]. CineTrans [57] is the latest open-source multi-shot narrative method. EchoShot [46] focuses on identity-consistent multi-shot portrait videos, In addition, considering rather than narrative coherence. that there is no controllable multi-shot method, we employ single-shot reference-to-video methods Phantom [29] and VACE [21] to generate multiple single-shot videos with individual text prompts from story for comparison. The competing baselines are all based on Wan2.1-T2V1.3B [45] at the resolution 480 832. Evaluation. To comprehensively evaluate our work, we design 100 multi-shot prompts using Gemini-2.5 [10]. To ensure fairness, we process the text prompts with the corresponding style for each baseline. Given that both subject consistency and scene consistency are crucial for multi-shot reference injection, we construct 90 cases encompassing three settings: subject injection, background injection, and joint injection, with 30 cases for each setting. Metrics. We evaluate multi-shot narrative video generation from four perspectives: (1) Text Alignment (TA): we calculate the similarity between text features and shot (2) Inter-Shot Confeatures extracted by ViCLIP [50]. sistency: first, we calculate the holistic semantic similarity between ViCLIP shot features. Then, we apply YOLOv11 [23] and SAM [25] to detect and crop subjects and backgrounds from keyframes (first, middle, and last frames), and subsequently employ DINOv2 [35] to measure subject consistency and scene consistency. (3) Transition Deviation: we employ TransNet V2 [43] to detect transitions in the generated videos and calculate the frame count deviation from the ground-truth transition timestamps. (4) 6 Figure 4. Qualitative Comparisons. We compare with two multi-shot video generation methods [46, 57] in the upper part, and two singleshot reference-to-video methods [21, 29] under multi-shot setting in the lower part. [ ] denotes the placeholder of character descriptions for baselines. The character introductions of the bottom part are omitted for brevity. Narrative Coherence: we employ Gemini-2.5 [10] to evaluate the narrative logic of multi-shot videos (Details of this metric could be found in Appendix A.5). In addition, we evaluate the Reference Injection Consistency from two perspectives: (1) We detect and crop the generated subjects and backgrounds, and calculate DINO similarity with the provided references. (2) Grounding: we detect the 2D object bounding boxes and calculate the mean Intersection over Union (mIoU) across keyframes to measure the spatiotemporal-grounded accuracy. 4.2. Qualitative Comparison As shown in Fig. 4, we present two different feature comparisons for multi-shot text-to-video generation and multishot reference-to-video generation. The simplified prompts for each shot are shown in the subtitle. CineTrans [57] uses global caption that focuses on the scene and camera transitions, and per-shot captions. Other baselines follow the individual caption manner for each shot. All these methods need to repeat the character descriptions in every shot caption, which makes it inconvenient for users. In comparison, we adopt the user-friendly hierarchical caption structure that describes subject appearance in the global caption and uses the indexed nouns in per-shot captions. In the upper part, CineTrans [57] shows limited variation in camera positioning across shot clips and fails to preserve character identity consistency. This stems from that CineTrans manipulates the attention score for shot transitions, impeding the original token interactions in pretrained attention. EchoShot [46] also designs RoPE-based shot transition for generating multiple portrait video clips that mainly focuses on identity consistency, ignoring other narrative details such as inconsistent clothing colors. Our method implements text-driven cross-shot subject consistency and scene consistency. Notably, the vehicle roof occupies small area within Shot 3, yet it still maintains consistent 7 Table 1. Quantitative Evaluations. denotes that this feature is not supported. The upper part compares multi-shot text-to-video generation, and the lower part compares multi-shot reference-to-video generation. In comparison, we achieve superior performance across all evaluation metrics, and further provide the exceptional spatiotemporal-grounded reference injection capabilities. Text Align. Inter-Shot Consistency Semantic Subject Scene Transition Deviation Narrative Coherence Reference Consistency Subject Background Grounding CineTrans [57] EchoShot [46] Ours (w/o Ref) VACE [21] Phantom [29] Ours (w/ Ref) 0.174 0.183 0.196 0.201 0.224 0.227 0.683 0.617 0.697 0.599 0.585 0.702 0.437 0.425 0. 0.468 0.462 0.495 0.389 0.346 0.447 0.273 0.279 0.472 5.27 3.54 1.72 1.41 0.496 0.213 0. 0.325 0.362 0.825 0.475 0.490 0.493 0.361 0.328 0.456 0.594 VACE [21] and Phantom [29] are implemented by performing multiple independent inferences for multi-shot reference-to-video generation, so we do not calculate their transition deviation. The inter-shot consistency stems only from the text prompts and reference images, leading to suboptimal inter-shot consistency and poor narrative coherence. These methods struggle to preserve user-provided In backgrounds, resulting in inferior scene consistency. comparison, we deliver outstanding performance across all evaluation metrics while providing additional support for spatiotemporal-grounded reference injection capabilities. Due to space constraints, we present ablation studies about key components and training strategy in Appendix B. 5. Conclusion In this work, we propose MultiShotMaster, the first controllable multi-shot video generation framework. We extend pretrained text-to-video model through two key RoPE improvements: Multi-Shot Narrative RoPE for recognizing shot boundaries and enabling controllable transitions, and Spatiotemporal Position-Aware RoPE for injecting reference tokens (subjects and backgrounds) into specific spatiotemporal regions. We also propose an automatic multishot & multi-reference data curation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our method leverages the intrinsic architectural properties to integrate text prompts, subjects, grounding signals, and backgrounds for flexible multi-shot video generation with superior controllability. We anticipate that this work could inspire future research in controllable multi-shot video generation. Limitation and Future Works. Although our method exhibits strong controllability, several key limitations require additional research to address: (1) We experiment on pretrained single-shot T2V model with only 1B parameters at the resolution of 384 672, which lags behind current open-source models like the WAN family of models [45] at the resolution 480 832 used by baselines. Therefore, the generation quality still needs improvement. In the future, Figure 5. Limitation visualization. We only explicitly control the subject motion, while the camera position is controlled by text prompts, which might cause the motion coupling issue. color with the vehicle roof in shot 2. In the lower part, we feed subject images and background images into VACE [21] and Phantom [29] for multishot reference-to-video generation and perform inference multiple times using individual shot captions for each shot. Since all shots are generated independently, they fail to maintain inter-shot subject consistency. For instance, in the fourth row, the woman wears different clothing between Shot 1 and Shot 3. And these methods fail to fully preserve the user-provided background reference images. In contrast, we achieve satisfactory reference-driven subject consistency and scene consistency, and further support grounding signals to control the subject injection into specified regions and background injection into specified shots. 4.3. Quantitative Comparison We report the quantitative comparison results in Table 1. Since CineTrans [57] adds mask matrix to the attention score, weakening the correlations across different shots, which results in unsatisfactory inter-shot consistency. On the other hand, as shown in the row 1 of Fig. 4, its shot transitions are not significant, leading to inferior transition deviation score and text alignment. EchoShot [46] is designed for generating multiple portrait video clips rather than creating narrative content, therefore it exhibits limited narrative coherence. Benefiting from the effectiveness of the proposed framework, we achieve superior inter-shot consistency, transition deviation score and narrative coherence. we will implement our work on WAN 2.1/2.2 and release code. (2) We explicitly control the subject motion, while the camera position is controlled by text prompts. As shown in Fig 5, although the generated video aligns the grounding signals, this is consequence of the camera and object moving together. We leave this coupling issue as future work."
        },
        {
            "title": "References",
            "content": "[1] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multicamera video generation from diverse viewpoints. arXiv preprint arXiv:2412.07760, 2024. 3 [2] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Cameracontrolled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. [3] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, and Hongsheng Li. Gs-dit: Advancing video generation with dynamic 3d gaussian fields through efficient dense 3d point tracking. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2171721727, 2025. 3 [4] Weikang Bian, Xiaoyu Shi, Zhaoyang Huang, Jianhong Bai, Qinghe Wang, Xintao Wang, Pengfei Wan, Kun Gai, and Hongsheng Li. Relightmaster: Precise video relighting with multi-plane light images. arXiv preprint arXiv:2511.06271, 2025. 3 [5] Yuxuan Bian, Xin Chen, Zenan Li, Tiancheng Zhi, Shen Sang, Linjie Luo, and Qiang Xu. Video-asprompt: Unified semantic control for video generation. arXiv preprint arXiv:2510.20888, 2025. 3 [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2, 3 [7] Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. 2, [8] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets fullsequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. 3 [9] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Shortto-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. 2 [10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Inderjit Ice Pasupat, Noveen Sachdeva, Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 5, 6, 7, 13, 14, 17, 18 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 4 [12] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 3 [13] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Yusuf Aytar, Michael Rubinstein, Chen Sun, et al. Motion prompting: Controlling video generation with motion trajectories. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 112, 2025. 2, [14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without arXiv preprint arXiv:2307.04725, specific tuning. 2023. 2 [15] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. arXiv Long context tuning for video generation. preprint arXiv:2503.10589, 2025. 2, 3, 4, 6 [16] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3 [17] Jingwen He, Hongbo Liu, Jiajun Li, Ziqi Huang, Yu Qiao, Wanli Ouyang, and Ziwei Liu. Cut2next: Generating next shot via in-context tuning. arXiv preprint arXiv:2508.08244, 2025. 3 [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6 [19] Junchao Huang, Xinting Hu, Boyao Han, Shaoshuai Shi, Zhuotao Tian, Tianyu He, and Li Jiang. Memory forcing: Spatio-temporal memory for consistent scene generation on minecraft. arXiv preprint arXiv:2510.03198, 2025. 9 [20] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the trainarXiv test gap in autoregressive video diffusion. preprint arXiv:2506.08009, 2025. 3 [21] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1719117202, 2025. 2, 3, 6, 7, 8 [22] Ozgur Kara, Krishna Kumar Singh, Feng Liu, Duygu Ceylan, James Rehg, and Tobias Hinz. Shotadapter: Text-to-multi-shot video generation with diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2840528415, 2025. 2, 3 [23] Rahima Khanam and Muhammad Hussain. Yolov11: An overview of the key architectural enhancements. arXiv preprint arXiv:2410.17725, 2024. 5, 6 [24] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4, 6 [25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, In Proceedings of the et al. IEEE/CVF international conference on computer vision, pages 40154026, 2023. 5, 6 Segment anything. [26] Baolu Li, Yiming Zhang, Qinghe Wang, Liqian Ma, Xiaoyu Shi, Xintao Wang, Pengfei Wan, Zhenfei Yin, Yunzhi Zhuge, Huchuan Lu, et al. Vfxmaster: Unlocking dynamic visual effect generation via incontext learning. arXiv preprint arXiv:2510.25772, 2025. 3 [27] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: arXiv Open-source large video generation model. preprint arXiv:2412.00131, 2024. 2, 3 [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Flow matcharXiv preprint Maximilian Nickel, and Matt Le. ing for generative modeling. arXiv:2210.02747, 2022. 4 [29] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via cross-modal alignment. arXiv preprint arXiv:2502.11079, 2025. 2, 3, 4, 6, 7, 8 [30] Yawen Luo, Jianhong Bai, Xiaoyu Shi, Menghan Xia, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Tianfan Xue. Camclonemaster: Enabling referencearXiv based camera control for video generation. preprint arXiv:2506.03140, 2025. 3 [31] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Follow Siran Chen, Xiu Li, and Qifeng Chen. your pose: Pose-guided text-to-video generation using In Proceedings of the AAAI Conpose-free videos. ference on Artificial Intelligence, pages 41174125, 2024. [32] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al. Controllable video generation: survey. arXiv preprint arXiv:2507.16869, 2025. 2 [33] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Followyour-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. 3 [34] Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, et al. Holocine: Holistic generation of cinematic multi-shot long video narratives. arXiv preprint arXiv:2510.20822, 2025. 3 [35] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin Dinov2: Learning robust viEl-Nouby, et al. arXiv preprint sual features without supervision. arXiv:2304.07193, 2023. 6 [36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2, 3, 4 [37] Tianhao Qi, Jianlong Yuan, Wanquan Feng, Shancheng Fang, Jiawei Liu, SiYu Zhou, Qian He, Hongtao Xie, and Yongdong Zhang. Maskˆ 2dit: Dual mask-based diffusion transformer for In Proceedings multi-scene long video generation. of the Computer Vision and Pattern Recognition Conference, pages 1883718846, 2025. 2, [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 4 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF conference els. on computer vision and pattern recognition, pages 1068410695, 2022. 2 [40] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large10 scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 14 [41] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-tovideo generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pages 1 11, 2024. [42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 6 [43] Tomas Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition detection. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 11218 11221, 2024. 5, 6 [44] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 2 [45] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 6, 8 [46] Jiahao Wang, Hualian Sheng, Sijia Cai, Weizhan Zhang, Caixia Yan, Yachuang Feng, Bing Deng, and Jieping Ye. Echoshot: Multi-shot portrait video generation. arXiv preprint arXiv:2506.15838, 2025. 6, 7, 8 [47] Qinghe Wang, Xu Jia, Xiaomin Li, Taiqing Li, Liqian Ma, Yunzhi Zhuge, and Huchuan Lu. Stableidentity: Inserting anybody into anywhere at first sight. IEEE Transactions on Multimedia, 2025. [48] Qinghe Wang, Baolu Li, Xiaomin Li, Bing Cao, Liqian Ma, Huchuan Lu, and Xu Jia. Characterfactory: Sampling consistent characters with gans for diffusion models. IEEE Transactions on Image Processing, 2025. 3 [49] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3daware and controllable framework for cinematic textto-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 1 10, 2025. 2, 4 [50] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 6 [51] Cong Wei, Bo Sun, Haoyu Ma, Ji Hou, Felix JuefeiXu, Zecheng He, Xiaoliang Dai, Luxin Zhang, Kunpeng Li, Tingbo Hou, et al. Mocha: Towards moviearXiv preprint grade talking character synthesis. arXiv:2503.23307, 2025. 3 [52] Runpu Wei, Zijin Yin, Shuo Zhang, Lanxiang Zhou, Xueyi Wang, Chao Ban, Tianwei Cao, Hao Sun, Zhongjiang He, Kongming Liang, et al. Omnieraser: Remove objects and their effects in images with paired video-frame data. arXiv preprint arXiv:2501.07397, 2025. 5, [53] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, and Hongming Shan. Dreamvideo-2: Zero-shot subjectdriven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024. 3 [54] Haoqian Wu, Keyu Chen, Yanan Luo, Ruizhi Qiao, Bo Ren, Haozhe Liu, Weicheng Xie, and Linlin Shen. Scene consistency representation learning for video scene segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1402114030, 2022. 5, 6 [55] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized textto-video generation. NeurIPS, 2024. 3 [56] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tunea-video: One-shot tuning of image diffusion models In Proceedings of the for text-to-video generation. IEEE/CVF international conference on computer vision, pages 76237633, 2023. 2 [57] Xiaoxue Wu, Bingjie Gao, Yu Qiao, Yaohui Wang, and Xinyuan Chen. Cinetrans: Learning to generate videos with cinematic transitions via masked diffusion models. arXiv preprint arXiv:2508.11484, 2025. 2, 3, 6, 7, 8 [58] Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, and Lu Jiang. Captain cinema: Towards short movie generation. arXiv preprint arXiv:2507.18634, 2025. 2, 3 [59] Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, and Feng Liu. Motioncanvas: Cinematic shot design with controllable image-to-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 11 [70] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. Advances in Neural Information Processing Systems, 37:110315110340, 2024. 2, 3 [60] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3 [61] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Unic: UniQifeng Chen, and Wenhan Luo. arXiv preprint fied in-context video editing. arXiv:2506.04216, 2025. 2 [62] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwaxl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 2 [63] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141, 2025. 3 [64] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. [65] Ruihan Zhang, Borou Yu, Jiajian Min, Yetong Xin, Zheng Wei, Juncheng Nemo Shi, Mingzhen Huang, Xianghao Kong, Nix Liu Xin, Shanshan Jiang, et al. Generative ai for film creation: survey of recent adIn Proceedings of the Computer Vision and vances. Pattern Recognition Conference, pages 62676279, 2025. 2 [66] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. In European conference on computer vision, pages 121. Springer, 2022. 5, 6 [67] Yuang Zhang, Junqi Cheng, Haoyu Zhao, Jiaxi Gu, Fangyuan Zou, Zenghui Lu, and Peng Shu. Shouldershot: Generating over-the-shoulder dialogue videos. arXiv preprint arXiv:2508.07597, 2025. 3 [68] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion In Proceedings of transformer for video generation. the Computer Vision and Pattern Recognition Conference, pages 20632073, 2025. 3 [69] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 2,"
        },
        {
            "title": "Appendix",
            "content": "A. More Implementation Details A.1. Details in Temporal Attention i=1 To clarify the designs in temporal attention, including Multi-Shot Narrative RoPE and Spatiotemporal PositionAware RoPE, we provide an Algorithm 1. Specifically, the complete in-context latents contain multi-shot video laand reference latents zref = [zm]Nref tents = [zi]Nshot m=1 . Nshot represents shot count, Nref represents the number of input reference images (subjects and backgrounds). The input bounding box sequences of references [boxes]Nbox contain Nbox bounding boxes. Each bounding box is represented as [(m, t, x1, y1, x2, y2)], indicating the bounding box of m-th reference at t-th frame. Note that for background references, the bounding boxes are fixed as (m, t, 0, 0, H, ), where is the first frame of the corresponding shot. In temporal attention, the linear projections to q, to k, to first transform in-context latents to Q, K, . Then, by applying the Multi-Shot Narrative RoPE (i.e., Eq. 2 in the main paper), the query and key of each shot are introduced explicit shot transition signals, while keeping the narrative temporal order. For m-th reference containing box boxes, we copy the query and key of the m-th reference box times. Each copy is then applied with Spatiotemporal Position-Aware RoPE based on the corresponding box in [boxes]N . Since RoPE is not applied to value component in attention mechanism, we copy the value for attention computation. After the attention computation with the proposed multi-shot & multi-reference attention mask, we aggregate the box reference copies for m-th reference by taking their mean. Finally, the multi-shot video ˆz and the reference latents zref are concatenated along the token dimension and fed into the linear projection to out. The attention output maintains the same dimension with the input in-context latents Z. box A.2. Training Paradigm The three-stage training paradigm consists of: (1) we finetune the temporal attention for spatiotemporal-specified reference injection on 300k single-shot video data with 30 epochs, batch size 8, while keeping other model parameters frozen. (2) we finetune temporal attention, cross attention and FFN on 235k multi-shot & multi-reference data with 3 epochs, batch size 1. (3) following the second stage, we assign (2) loss weight to subject regions and (1) to backgrounds to train 0.5 epoch. We conduct ablation study for the training paradigm in Sec B.2 and Table 4. 13 Algorithm 1 Temporal Attention with Multi-Shot Narrative RoPE and Spatiotemporal Position-Aware RoPE. Input: In-context latents containing: Multi-shot video latents = [zi]Nshot i=1 Reference latents zref = [zm]Nref m=1 Bounding box sequences of references (subjects and backgrounds) [boxes]Nbox = [(m, t, x1, y1, x2, y2)]Nbox Output: In-context latents after temporal attention 1: = to q(Z), = to k(Z), = to v(Z) // Apply Multi-Shot Narrative RoPE i=1 = Eq. 2([ Qi]Nshot ) i=1 = Eq. 2([ Ki]Nshot 2: = [Qi]Nshot = [Ki]Nshot = [ Vi]Nshot // Apply Spatiotemporal Position-Aware RoPE i=1 i=1 i=1 ) 3: Qref = [Qref ref = [K ref ref = [ ref b=0 = Eq. 3(Copy( Qref ), [boxes]Nbox ]Nbox ) b=0 = Eq. 3(Copy( ref ), [boxes]Nbox ]Nbox b=0 = Copy( ref , [boxes]Nbox ]Nbox ) b ) // Attention Computataion 4: ˆZ = Attention([Q, Qref ], [K, ref ], [V, ref ], Mask) // Reference Aggregation m=1 = [ mean([ˆzm]N box 5: zref = [zm]Nref 6: = to out([ˆz, zref ]) 7: return ) ]Nref m= A.3. Labeling Hierarchical Captions As introduced in Sec 3.5 of the main paper, we employ Gemini-2.5 [10] to label the global caption and per-shot captions. The prompt template is shown in Fig. 6. We begin by proportionally sampling 20 frames from the multishot video, ensuring at least one frame is extracted from each shot, and use Gemini-2.5 to produce comprehensive global caption. Then we employ Gemini-2.5 to reason the per-shot captions based on the global caption and each shot video (with sampling frame stride of 15). Each subject is denoted by Subject X, [1, 2, 3]. As shown in Fig. 7, the cross-shot consistency of subject annotations is satisfactory due to the powerful Gemini-2.5 and our carefullydesigned prompt template. A.4. Merge Cross-Shot Tracking Annotations As introduced in Sec 3.5 of the main paper, we conduct the tracking process shot-by-shot to obtain the bounding box sequence of each subject. To merge the cross-shot tracking results, we use Gemini-2.5 [10] to group the subject images by prompting with our carefully-designed prompt template Table 2. Ablation study for Multi-Shot RoPE. We experiment on multi-shot text-to-video generation without reference input. Table 3. Ablation study for reference injection. We experiment on multi-shot reference-to-video generation. Inter-Shot Consistency Transition Deviation Semantic Subject Scene Narrative Coherence Aesthetic Score Narrative Coherence Reference Consistency Subject Scene Grounding w/o MS RoPE Ours (w/o Ref) 0.702 0.697 0.486 0.455 0.491 0.447 4.68 1.72 0.645 0.695 w/o Mean w/o Attn Mask w/o STPA RoPE Ours (w/ Ref) 3.84 3.72 3.79 3. 0.796 0.787 0.761 0.825 0.482 0.452 0.468 0.414 0.425 0.363 0.493 0.456 0.557 0.561 0.594 as shown in Fig. 8. A.5. Narrative Coherence To comprehensively assess the narrative coherence of generated multi-shot videos, we employ Gemini-2.5 [10] to construct an automated evaluation metric. We begin by proportionally sampling 20 frames from the multi-shot video, ensuring at least one frame is extracted from each shot. Subsequently, we input these frames and the hierarchical captions as pair into Gemini-2.5. We require Gemini-2.5 to strictly adhere to cinematic narrative logic and scrutinize cross-shot content across four core dimensions: Scene Consistency, Subject Consistency, Action Coherence, and Spatial Consistency, by the constructed elaborate instructions as shown in Fig. 9. Specifically, Scene Consistency verifies the stability of the background, lighting, and atmosphere during transitions to ensure all shots depict the same setting; Subject Consistency strictly scrutinizes identity features and appearance attributes by comparing core objects across different viewpoints to detect unintended deviations; Action Coherence focuses on evaluating the temporal logic of dynamic behaviors to determine whether actions in subsequent shots constitute reasonable continuations of preceding ones; and Spatial Consistency examines whether the topological structure of relative positional relationships between subjects remains constant in accordance with cinematic language. Functioning as binary classifier, the model outputs True or False verdict for each dimension, thereby quantifying the generative models capability in handling complex multishot spatiotemporal consistency. B. Ablation Study B.1. Ablation Study for Network Design We experiment with different settings to validate the effectiveness of the proposed designs in our framework: w/o MS RoPE: without Multi-Shot Narrative RoPE, the shot transitions rely only on the per-shot captions. w/o Mean: this setting randomly selects one copy from multiple copies of subject tokens after 3D attention, instead of averaging. w/o Attn Mask: without Multi-Shot & Multi-Reference Attention Mask, this setting uses full attention along the temporal dimension. w/o STPA RoPE: without the Spatiotemporal PositionAware RoPE, this setting directly concatenates the reference tokens along the temporal dimension and applies the RoPE(t=0,h,w) to each reference. Ours (w/o Ref): this setting is trained using all the proposed designs, and infers multi-shot text-to-video generation without reference input. Ours (w/ Ref): this setting uses the same trained checkpoint as Ours (w/o Ref) and infers multi-shot referenceto-video generation. Since the spatiotemporal-grounded reference injection might facilitate shot transitions, we do not provide reference input to compare with w/o MS RoPE setting. It relies only on the variations between per-shot captions to guide shot transitions, and uses the continuous RoPE to all frames of multi-shot videos in the temporal order. This setting cannot implement precise shot transitions by text prompts only, leading to unsatisfactory transition deviation score, as shown in Table 2. Due to the lack of shot transitions, there is almost no change between shots, resulting in higher semantic and scene consistency scores. With the proposed Multi-Shot Narrative RoPE, we can perform shot transition at user-specified timestamps with superior deviation score. We further evaluate the designs in spatiotemporalgrounded reference injection. In addition to the mentioned metrics in the main paper, we further introduce Aesthetic Score [40] to measure the aesthetic quality of the generated multi-shot videos. w/o Mean might cause information loss, showing suboptimal results, as shown in Table 3. the excessively long contexts in w/o Attn Mask setting have unnecessary interactions between in-context tokens, leading to weak aesthetic score and reference consistency. w/o STPA RoPE cannot designate the specific shot or exact spatiotemporal position where the subjects and backgrounds appear, relying only on text prompts for positioning. It shows poor performance on reference consistency. Taking advantage of the effectiveness of the proposed designs, our method shows best performance on all metrics. B.2. Ablation Study for Training Paradigm We conduct ablation study for the three-stage training paradigm introduced in Sec 3.4 of the main paper. We 14 Table 4. Ablation Study for training paradigm. We experiment on multi-shot reference-to-video generation. The 1st/2nd best results of settings are indicated in underline/bold. I: Multi-Shot+Ref. Injection I: Multi-Shot II: Multi-Shot+Ref. Injection I: Ref. Injection II: Multi-Shot+Ref. Injection I: Ref. Injection II: Multi-Shot+Ref. Injection III: Multi-Shot+Subject-Focused Ref. Injection Text Align. Inter-Shot Consistency Semantic Subject Scene Subject Background Grounding Reference Consistency 0. 0.671 0.464 0.415 0.454 0.426 0. 0.695 0.481 0.433 0.472 0.451 0. 0.578 0.222 0.692 0.484 0.437 0. 0.454 0.583 0.227 0.702 0.495 0. 0.493 0.456 0.594 first explore the order of multi-shot video generation and reference-to-video generation, then shows the performance of the subject-focused post-training. The first setting involves fintuning the pretrained text-to-video generation model to learn both multi-shot task and reference-to-video task simultaneously. However, because the diffusion loss is computed across all frames to optimize global consistency, this unified training paradigm shows inadequate for effectively learning both tasks. The second setting is first learning multi-shot text-to-video generation, followed by multishot reference-to-video generation, both using the curated multi-shot & multi-reference data. This setting achieves slightly lower subject consistency due to insufficient exposure to diverse subjects during training. Since the construction cost of multi-shot & multi-reference data is relatively high, we first train the model to learn spatiotemporalgrounded reference injection task on single-shot data, and then learn both tasks using the curated multi-shot & multireference data. It achieves better results on most metrics. Furthermore, we introduce subject-focused post-training that guides the model to prioritize subjects requiring higher consistency, which also promotes the modeling of crossshot subject variations. 15 Figure 6. Prompts of labeling global caption and per-shot captions. We first label the global caption by sampling frames from the input multi-shot video. Then we label the per-shot caption one by one. 16 Figure 7. Multi-shot video data example. By employing Gemini-2.5 [10] with the carefully-designed prompts as shown in Fig. 6, the labeled subjects could be consistent in global and per-shot captions. Figure 8. By employing Gemini-2.5 [10] to group the subject images, we obtain complete multi-shot tracking results. 17 Figure 9. We require Gemini-2.5 [10] to strictly adhere to cinematic narrative logic and scrutinize cross-shot content across four core dimensions: Scene Consistency, Subject Consistency, Action Coherence, and Spatial Consistency."
        }
    ],
    "affiliations": [
        "Dalian University of Technology",
        "Kling Team, Kuaishou Technology",
        "The Chinese University of Hong Kong"
    ]
}