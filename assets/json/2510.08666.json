{
    "paper_title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
    "authors": [
        "Yuxin Ma",
        "Lun Du",
        "Lanning Wei",
        "Kun Chen",
        "Qian Xu",
        "Kangyu Wang",
        "Guofeng Feng",
        "Guoshan Lu",
        "Lin Liu",
        "Xiaojing Qi",
        "Xinyuan Zhang",
        "Zhen Tao",
        "Haibo Feng",
        "Ziyun Jiang",
        "Ying Xu",
        "Zenan Huang",
        "Yihong Zhuang",
        "Haokai Xu",
        "Jiaqi Hu",
        "Zhenzhong Lan",
        "Junbo Zhao",
        "Jianguo Li",
        "Da Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack of a standardized and efficient inference framework. We present dInfer, an efficient and extensible framework for dLLM inference. dInfer decomposes the inference pipeline into four modular components--model, diffusion iteration manager, decoding strategy, and KV-cache manager--and integrates novel algorithms for each component alongside system-level optimizations. Through this combination of algorithmic innovations and system enhancements, dInfer achieves substantial efficiency gains without compromising output quality on LLaDA-MoE. At batch size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800 tokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to prior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while maintaining similar model performance. Even compared to the AR model (with a comparable number of activation parameters and performance) QWen2.5-3B, which is highly optimized with the latest vLLM inference engine, dInfer still delivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced at https://github.com/inclusionAI/dInfer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 2 6 6 6 8 0 . 0 1 5 2 : r dInfer: An Efficient Inference Framework for Diffusion Language Models Yuxin Ma1,, Lun Du1,, Lanning Wei1,, Kun Chen1,, Qian Xu1,4,, Kangyu Wang1,6,, Guofeng Feng1,5,, Guoshan Lu1,, Lin Liu1, Xiaojing Qi1, Xinyuan Zhang1, Zhen Tao1, Haibo Feng1, Ziyun Jiang1,3, Ying Xu1,5, Zenan Huang1, Yihong Zhuang1, Haokai Xu1,2, Jiaqi Hu1,2, Zhenzhong Lan1,3,, Junbo Zhao1,2,, Jianguo Li1,, Da Zheng1, 1Ant Group, 2Zhejiang University, 3Westlake University, 4Renmin University of China, 5University of Chinese Academy of Sciences, 6Shanghai Jiao Tong University"
        },
        {
            "title": "Abstract",
            "content": "Diffusion-based large language models (dLLMs) have emerged as promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack of standardized and efficient inference framework. We present dInfer, an efficient and extensible framework for dLLM inference. dInfer decomposes the inference pipeline into four modular componentsmodel, diffusion iteration manager, decoding strategy, and KV-cache managerand integrates novel algorithms for each component alongside system-level optimizations. Through this combination of algorithmic innovations and system enhancements, dInfer achieves substantial efficiency gains without compromising output quality on LLaDA-MoE. At batch size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800 tokens per second across six benchmarks on 8 H800 GPUs. Compared to prior systems, dInfer delivers 10 speedup over Fast-dLLM while maintaining similar model performance. Even compared to the AR model (with comparable number of activation parameters and performance) QWen2.5-3B, which is highly optimized with the latest vLLM inference engine, dInfer still delivers 23 speedup. The implementation of dInfer is open-sourced at https://github.com/inclusionAI/dInfer."
        },
        {
            "title": "Introduction",
            "content": "Over the past year, diffusion-based large language models (dLLMs) have gained increasing attention in both academia and industry. Unlike conventional autoregressive (AR) models that generate tokens sequentially, dLLMs refine entire sequences in parallel through iterative denoising. This intrinsic parallelism opens new opportunities for faster decoding and enables better utilization of GPU hardware. Combined with rapid algorithmic progress, these properties make dLLMs compelling alternative to AR LLMs. Recent work has shown that models such as LLaDA(-MoE) (Nie et al., 2025; Zhu et al., 2025) can reach performance levels comparable to strong AR baselines, including Llama (Grattafiori et al., 2024) and Qwen (Yang et al., 2024). Despite these advantages, the practical deployment of dLLMs still faces three critical bottlenecks. First, dLLMs are substantially more computationally expensive than AR models due to iterative denoising steps, making efficiency improvements at both the algorithm and system level essential. Second, while dLLMs have inherent parallelism, scaling parallel decoding remains challenginglarger parallel spans often degrade output quality. Third, the field lacks unified inference framework and standardized evaluation protocol. This gap hinders consistent benchmarking and often leads to incomparable acceleration claims, such as reporting tokens per second (TPS) under varying batch sizes or hardware. To address these challenges, we propose dInfer, an efficient and extensible inference framework for dLLMs. dInfer modularizes inference into four componentsmodel, diffusion iteration manager, decoding strategy, and KV-cache managementand provides well-designed APIs for flexible combinations of algorithms in each component. It supports multiple dLLM variants, including LLaDA (Nie et al., 2025), LLaDA-MoE (Zhu et al., 2025), and LLaDA-MoE-TD (Section C.1). dInfer introduces an iteration smoothing algorithm for smoother Corresponding Authors. Core Contributors. 1 (a). TPS (b). Model performance Figure 1: Benchmark results. We compare dInfer on LLaDA-MoE and LLaDA-MoE-TD with Fast-dLLM and vLLM across six benchmarks and show their average inference speed in tokens per second (TPS) on the six benchmarks and the highest inference speed on the HumanEval dataset. When achieving similar model performance on the benchmarks, dInfer is about 10 faster than Fast-dLLM on the same model and is about 2 3 faster than vLLM on Qwen-2.5-3B. denoising, hierarchical and credit decoding for enhanced parallel decoding, and vicinity refresh strategy for KV-cache management to mitigate cache staleness. Beyond algorithmic improvements, dInfer integrates several system-level optimizations. It supports both tensor parallelism (TP) and expert parallelism (EP) to maximize GPU utilization even at batch size 1. It leverages PyTorch compilation and NVIDIA CUDA Graphs for efficient kernel execution, and introduces loop unrolling mechanism to eliminate CUDA stream bubbles across diffusion iterations. We evaluate inference efficiency using tokens per second (TPS) per sequence, shown in Figure 1. On HumanEval, dInfer achieves over 1,100 TPS at batch size 1, and averages more than 800 TPS across six benchmarks on single node with 8 H800 GPUs. Compared to Fast-dLLM (Wu et al., 2025), dInfer delivers more than 10 speedup while maintaining accuracy; on LLaDA-MoE it provides 2 3 speedup over QWen2.5-3B on vLLM with comparable quality. Our contributions are summarized as follows: We present dInfer, the first modularized dLLM inference framework that integrates algorithmic innovations with system-level optimizations to deliver substantial efficiency gains. We provide the first open-source demonstration that dLLM inference can surpass AR models at batch size 1, establishing new milestone for inference efficiency. The implementation is available at https://github.com/inclusionAI/dInfer."
        },
        {
            "title": "2 Framework Design",
            "content": "A key advantage of dLLMs lies in their ability to perform parallel decoding within each iteration. To fully exploit this capability, advances are required in model design, diffusion iteration strategies, and decoding algorithms. At the same time, dLLMs introduce unique computational challenges. Unlike AR modelswhere previously computed keys and values can be cacheddLLMs employ bidirectional attention, meaning that decoding single token can affect the representations of all tokens in the sequence. This makes straightforward KV-cache reuse infeasible and necessitates specialized cache management for efficient inference. To address these issues, we design dInfer, an inference framework that accelerates dLLMs through four modular components: model, diffusion iteration manager, decoding strategy, and KV-cache management (Figure 2). This modular architecture Figure 2: The architecture of the dInfer framework 2 enables flexible combinations of algorithms across components, allowing users to construct customized inference pipelines that maximize the benefits of parallel decoding while improving computational efficiency. Algorithm 1 Blockwise dLLM Inference Require: Input tokens ZBL (undecided positions marked as mask id); block size S; model M; decoder D; KV-cache manager K; block iteration manager # 1) Iterator: pick next block (blockwise order) [start : end] I.NEXTBLOCK() undecided (X[start : end] = mask id) while any(undecided) do # 2) KV update policy if K.SHOULDUPDATE(loop context, start : end) then Ensure: Completed tokens (cid:98)X ZBL 1: K.CREATE(B, L) 2: while I.HASNEXT() do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end while 17: return K.UPDATE(X, start : end) end while end if # 3) Model forward on this region (with KV Cache) logits M.FORWARD(X, K, start : end) # 4) Decoder: tokens to commit in this block (X, undecided) D.DECODE(logits, X, undecided, start : end) Create KV cache Get next block Boolean mask of undecided positions logits RBLV (cid:98)X"
        },
        {
            "title": "2.1 Diffusion Iteration Manager\nThe diffusion iteration manager acts as the controller of the iterative denoising process, with three main\nresponsibilities: 1) determining the next region of tokens to decode, 2) interacting with the model to obtain\noutputs such as logits and hidden states, 3) maintaining historical predictions to provide a richer context for\nfuture decoding.",
            "content": "dInfer currently implements two algorithms in this component. The blockwise diffusion iteration algorithm performs decoding in fixed-size spans and serves as baseline (Algorithm 1). The iteration smoothing algorithm improves upon this by retaining token representations from the previous iteration and fusing them with the next iterations embeddings. This enables cross-iteration information flow, enriches contextual cues, and empirically boosts token confidence while mitigating the performance degradation typically caused by KV-cache deployment. detailed description is provided in Appendix A.1."
        },
        {
            "title": "2.2 Decoding Strategy\ndInfer supports three strategies for parallel decoding:",
            "content": "Threshold decoding (from Fast-dLLM (Wu et al., 2025)): commits tokens whose confidence exceeds preset threshold. Hierarchical decoding (ours): recursively partitions masked spans, ensuring at least one token is decoded per region, thereby reducing local dependencies and improving efficiency. Credit decoding (ours): accumulates historical confidence scores as credits and preferentially commits tokens with consistently stable predictions, improving reliability across iterations. These algorithms allow dInfer to achieve higher decoding efficiency without retraining the underlying model. Detailed formulations are included in Appendix B.1 and B.2."
        },
        {
            "title": "2.3 KV-cache management\nA central challenge in dLLM inference is KV-cache incompatibility. In AR models, causal attention allows KV\nstates to be computed once and reused; in dLLMs, however, token representations evolve across denoising\nsteps, making static reuse infeasible. Without caching, inference must perform Transformer computations\nover the entire sequence, creating heavy computational overhead.",
            "content": "3 Earlier approaches introduced training-free strategies such as blockwise caching and Dual Cache (Wu et al., 2025), which reuse KV states for decoded tokens or suffixes of masked tokens. However, these methods treat cached states as static, neglecting updates from newly decoded tokens, which often degrades accuracy. To balance cost and performance, dInfer introduces vicinity KV-cache refresh. This method exploits semantic locality by selectively updating small window of tokens adjacent to the current decoding block. During denoising, and states are recomputed for both masked tokens and their immediate neighbors; once block is fully decoded, full cache update ensures global consistency."
        },
        {
            "title": "2.5 An example of the orchestration of the algorithms in dInfer",
            "content": "Figure 3: Orchestration of the algorithms in different dInfer components. Figure 3 shows how the algorithms interact within dInfer. In each iteration, the framework tries to identify the [MASK] tokens in the active decoding block. The sequence is first embedded (potentially leveraging context from previous iterations), followed by forward pass through the model using tensor/expert parallelism to produce logits. During the forward pass, the previous KV cache that does not hit the vicinity refresh strategy will remain unchanged and be reused. After obtaining the logits, the hierarchical/credit decoding algorithm will decide which [MASK] tokens to decode and predict their identities. In addition, the iteration smoothing algorithm will retain the logit-weighted embeddings of the sequence and incorporate them as part of the embedding for the next iteration, ensuring continuity across steps. Together, these components enable dInfer to achieve both efficiency and stability in dLLM inference."
        },
        {
            "title": "Implementation Details",
            "content": "To deliver fast inference speed, dInfer provides system-level optimizations on the components of dInfer. Model computations dInfer builds on vLLMs backend to exploit two complementary forms of parallelism. Tensor parallelism is applied to the linear layers preceding attention modules, distributing dense computations efficiently across multiple GPUs. Expert parallelism is applied to the LLaDA-MoE model and is effective even at batch size of 1unlike in AR models, where expert parallelism typically requires large batch sizes. By combining tensor and expert parallelism, dInfer achieves more than 100% improvement in inference efficiency. To further optimize single-sequence inference, dInfer uses PyTorchs just-in-time (JIT) compiler torch. compile to fuse CUDA kernels and execute them within NVIDIA CUDA Graphs, thereby eliminating 4 PyTorch execution overhead. This compilation technique improves inference efficiency by over 200% when TP and EP are enabled. Diffusion iterations Diffusion iterations can suffer from CUDA stream bubblesidle gaps of consecutive kernel launches between diffusion iterationswhich waste GPU cycles and reduce throughput. To address this, dInfer applies loop unrolling strategy that allows Python to launch CUDA kernels continuously without being blocked by stream synchronization. This reduces launch latency, keeps GPU pipelines fully occupied, and boosts iteration efficiency by about 5-10%. We also introduce an early termination mechanism for blockwise decoding. Once an end-of-sequence (EOS) token is generated within block, subsequent decoding steps on the remaining blocks become redundant. dInfer therefore halts the diffusion loop and fills all remaining blocks with EOS, avoiding unnecessary computation. This optimization improves inference efficiency by 15-40%. Parallel decoding To make loop unrolling effective in diffusion iterations, decoding algorithms in dInfer are implemented without control-flow operations, and data transfer from PyTorch tensors to Python code is eliminated. This design ensures compatibility with system-level optimizations to achieve high decoding throughput."
        },
        {
            "title": "4.1 Datasets and Configurations\nDatasets. We select six datasets from diverse domains with sufficient response lengths: CRUX-O (Gu\net al., 2024), LiveCodeBenchv6 (Jain et al., 2024)(denoted as LCB V6), MBPP (Austin et al., 2021), and\nHumanEval (Chen et al., 2021) for code generation; GSM8K (Cobbe et al., 2021) for mathematical reasoning;\nand IFEval (Zhou et al., 2023) for instruction-following agent tasks.",
            "content": "Evaluation Metric. To evaluate the efficiency of the dInfer framework, we use tokens per forward (TPF) per sequence to measure the parallel decoding capability within single diffusion iteration, and tokens per second (TPS) per sequence to assess overall inference efficiency. To be more specific, TPF can be formally described as TPF = Ti where Ti and Fi are the number of tokens generated before the first EOS and the Fi number of diffusion iterations that run on sequence to generate tokens. TPS can be described as TPS = Ti ti where ti is the time cost to generate tokens for sequence i. Configurations. We compare dInfer with Fast-dLLM (Wu et al., 2025) to demonstrate the effectiveness of both system optimizations and algorithmic innovations in dInfer. Without KV cache, Fast-dLLM employs parallel decoding with threshold of 0.9, setting used in their paper, while dInfer further incorporates credit decoding and iteration smoothing to validate the effectiveness of the proposed decoder. When KV cache is enabled, Fast-dLLM adopts Dual Cache, whereas dInfer integrates the vicinity KV-Cache refresh method with iteration smoothing and threshold decoding. In addition, we evaluate the effectiveness of LLaDA-MoE-TD in dInfer. We report its results under the dInfer optimal setting, which integrates dual-cache, hierarchical decoding, vicinity KV-Cache refresh, and iteration smoothing. Please see more details about the configurations of the experiments in Appendix D. All experiments are conducted on server equipped with 8 NVIDIA H800 GPUs, with PyTorch 2.9.0.dev20250831 and vLLM 0.10.1. We use batch size of 1, generation length of 1024 and block size of 64 for all experiments."
        },
        {
            "title": "4.2 Performance\nAs shown in Table 1, it can be clearly observed that LLaDA-MoE has comparable or higher performances than\nQwen2.5-3B over six different datasets. Under the ”Without KV Cache” setting, dInfer achieves an average\naccuracy of 54.33, which is higher than Fast-dLLM and is comparable to the performance of QWen2.5-3B\nin vLLM and LLaDA-MoE reported in its paper (Zhu et al., 2025), while achieving 6.5× speedup over\nFast-dLLM. When KV cache is enabled, dInfer achieves higher accuracy than Fast-dLLM (53.96 vs. 52.15)\nand delivers 6× speedup over Fast-dLLM. When achieving similar model performance, dInfer achieves over\n10× speedup (a TPS of 680.71) over Fast-dLLM (a TPS of 63.61) across the six benchmarks. dInfer is also 2.5×\nfaster than Qwen-2.5 3B (a TPS of 277.45) in vLLMs.",
            "content": "As shown in Table 2, the LLaDA-MoE model trained by the trajectory distillation technique substantially improves inference efficiency on the six benchmarks. Coupled with the full set of dInfer algorithmic optimizations, the distilled model achieves an average TPS of 847.22, significantly higher than the 680.71 TPS of the non-distilled baseline, which is more than 3x speedup over Qwen2.5-3B in vLLM. 5 Table 1: Evaluations of different framework and configurations in terms of performance, TPF, and TPS on LLaDA-MoE. We can observe that dInfer achieves 2 3 improvement over vLLM (680.71 vs. 277.45). Furthermore, we can see that dInfer provides more than tenfold enhancement over Fast-dLLM (680.71 vs. 63.61) while achieving similar results (53.96 vs. 53.52). Config. Frame. Metric LLaDA-MoE - QWen2.5-3B vLLM Without KV Cache With KV Cache Fast-dLLM dInfer Fast-dLLM dInfer Perf Perf TPF TPS Perf TPF TPS Perf TPF TPS Perf TPF TPS Perf TPF TPS Avg. 54.83 54.44 1 277.45 53.52 2.82 63.61 54.33 4.29 407.36 52.15 2.46 110. 53.96 3.87 680.71 CRUX-O GSM8K HumanEval IFEval MBPP LCB V6 42.38 46.75 1 289. 43.75 2.9 59.79 42.38 4.26 379.62 40.75 2.68 120.57 41.38 4.02 765.3 82.41 86.28 1 294. 82.79 2.28 56.19 82.26 3.76 379.63 79.9 2.09 97.5 80.97 3.42 682.9 61.59 60.37 1 294. 60.98 3.87 90.8 63.41 6.17 606.85 60.37 3.24 143.9 62.2 5.52 1,011.12 59.33 58.2 1 296. 54.53 2.42 60.25 57.49 2.79 285.49 53.97 2.02 95.23 58.78 2.32 444.51 70.02 13. 65.81 1 290.15 66.5 3.01 70.2 67.21 4.82 475.23 65.11 2.55 112.9 67.45 4.54 757.55 9.2 1 200. 12.56 2.46 44.4 13.22 3.92 317.36 12.78 2.19 95.8 13 3.38 422.88 Table 2: Evaluations of different framework and configurations in terms of performance, TPF, and TPS on LLaDA-MoETD. With the introduction of Trajectory Distillation, the TPS for various benchmarks has significantly improved. The average TPS exceeds that of vLLM by more than threefold. Config. Frame. Metric LLaDA-MoE - QWen2.5-3B vLLM With KV Cache dInfer Perf Perf TPF TPS Perf TPF TPS Avg. 54. 54.44 1 277.45 52.72 5.67 847.22 CRUX-O GSM8K HumanEval IFEval MBPP LCB V6 42. 46.75 1 289.53 40.12 6.06 976.66 82.41 86.28 1 294.15 79.15 6.12 1,011.22 61. 60.37 1 294.05 63.41 7.10 1,125.67 59.33 58.2 1 296.7 56.19 2.98 496.92 70. 13.27 65.81 1 290.15 65.11 6.61 906.98 9.2 1 200.12 12.33 5.18 562."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present dInfer, an efficient and extensible inference framework for dLLMs. By decomposing inference into modular components and incorporating optimizations such as hierarchical decoding, credit decoding, iteration smoothing, and vicinity KV-cache refresh, dInfer effectively addresses the key bottlenecks of high computation cost and limited parallel decoding efficiency of dLLMs. Extensive experiments on LLaDA-MoE demonstrate that dInfer achieves state-of-the-art throughputexceeding 1,100 TPS on 8H800 GPUswhile maintaining output quality. We believe that dInfer provides both practical toolkit and standardized platform to accelerate research and development in the rapidly growing field of dLLMs."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 6 Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CruxEval: Benchmark for Code Reasoning, Understanding and Execution. arXiv preprint arXiv:2401.03065, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025. URL https://arxiv.org/abs/2502.09992. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, and Hao Zhou. Seed diffusion: large-scale diffusion language model with high-speed inference, 2025. URL https://arxiv.org/abs/2508.02193. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115, 2024. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, and Ji-Rong Wen. Llada-moe: sparse moe diffusion language model, 2025. URL https://arxiv.org/abs/2509.24389."
        },
        {
            "title": "A Diffusion iteration",
            "content": "A.1 IterSmooth: Iteration Smoothing In conventional dLLM decoding, only small subset of positions is updated at each step by selecting argmax tokens, while the logits for all other positions are discarded. IterSmooth reuses this otherwise wasted information: for positions that remain masked, it converts the logits distribution into an expected embedding and injects it into the corresponding mask-token embedding. This allows uncertain positions to be enriched with dense, distribution-level signals. In our setting, since the output projection and input embedding matrices are not weight-tied, the expected embeddings are computed using the input embedding matrix together with the token probability distribution. We operate only on masked positions to avoid shifting the training distribution elsewhere. Let zt[i] be the logits at step and position i, Wemb the input embedding matrix, and emask the standard mask embedding. Without temperature scaling, we use: pt[i] = softmax(zt[i]), et[i] = pt[i] Wemb, αt = min(αinit + αgrowtht, αpreset) et+1[i] = emask + αt et[i], The mixing weight αt increases from small initial value (e.g., 0.1) over decoding steps toward preset maximum (e.g., 0.20.4), ensuring conservative behavior early. In addition, we adopt decode-threshold schedule that decays from 1.0 toward preset target across steps, decoding only high-confidence positions early while progressively relaxing the criterion, which stabilizes inputs early and increases the contribution of distribution-level guidance later. Our method does not introduce new parameters or retraining, and Wemb is reused directly. The approach increases per-step information by leveraging the full distribution instead of only argmax tokens. Our empirical study shows that this method can increase the average number of tokens decoded in diffusion iteration by 30 40%, and improves the final quality of generated texts."
        },
        {
            "title": "B Decoding strategy",
            "content": "B.1 Hierarchical decoding While dLLMs theoretically allow parallel decoding by predicting multiple tokens at once, naive implementations often suffer from quality degradation. This stems from the violation of the conditional independence assumption among simultaneously generated tokens, which frequently leads to semantic inconsistencies. To overcome this limitation, we propose Hierarchical Decoding, training-free strategy inspired by the divide-and-conquer paradigm. The method recursively partitions masked spans into smaller sub-regions and decodes tokens based on their confidence, attempting to resolve at least one token in each region during every forward pass whenever confidence permits. The key insight is that the spatial distribution of masked tokens has critical impact on prediction stability. This approach provides two notable advantages. First, by promoting non-contiguous decoding, it increases the spacing between masked tokens, thereby reducing local dependencies and improving semantic consistency. Second, when decoding positions are preferentially selected near the center of each span, the undecoded regions shrink recursively, enabling the process to approach O(logn) complexity in the ideal case. Together, these properties allow Hierarchical Decoding to generate more tokens per forward pass than vanilla decoding without fine-tuning the base model. B.2 Credit decoding In standard dLLM inference, text is generated through repeated predictsamplere-mask cycles across multiple denoising steps. Existing parallel decoding strategies typically commit tokens based solely on their current confidence at each step. In practice, however, many tokens that are ultimately correct stabilize early in the generation process but remain below the confidence threshold. These tokens are repeatedly re-masked and re-evaluated, leading to unnecessary computation. We present CreditDecoding, training-free acceleration algorithm for dLLMs that reduces redundant computation and accelerates convergence in parallel decoding. During decoding, we maintain credit Ci,v for each position and token V. This credit which quantifies how consistently token has been favored along the generation process, serving as temporal prior for its likelihood of being correct. Given the input xt, let pi v) denote the models current predictive distribution, where fθ(xt) are the logits. Let = arg maxv pi θ(v xt) be the top candidate token at θ(v xt) = Softmax( fθ(xt)i 8 position i. The credit is updated as follows: (cid:40) Ci,v = t1 + (cid:0)pi θ(v xt)(cid:1)γ β Ci,v β Ci,v t1 = v, otherwise, β (0, 1), γ (0, 1). (1) Here β discounts the earlier confidence and prevents errors from accumulating and influencing future prediction. The concave transformation ()γ (with γ < 1) provides relatively larger boosts to tokens with low or moderate confidence, helping correct but underconfident predictions stabilize earlier. Before making token commitment decision, the accumulated credit is fused with the models logits as prior in the log domain: + α log(cid:0)1 + Ci,v α > 0, (2) θ(v xt) = Softmax(cid:0) fθ(xt)i (cid:1). Intuitively, tokens that have been which yields an enhanced distribution pi consistently predicted across steps receive confidence boost, making them more likely to be committed earlier. In contrast, tokens with fluctuating or transiently high confidence are suppressed. This mechanism enhances decoding stability, particularly in long-sequence and reasoning tasks. = fθ(xt)i fθ(xt)i (cid:1), t Importantly, CreditDecoding does not change the underlying sampling or decoding policy, but instead simply replaces the original distribution pθ with the enhanced pθ. This design ensures the compatibility with standard inference optimizations such as threshold decoding, top-k sampling, KV-cache, and compiler-level optimizations, allowing efficiency gains to accumulate when combined. To balance efficiency and robustness under varying stability conditions, we default to maintaining and updating credits only within the current decoding block. This limits the influence of uncertain future context, reduces interference from under-informed positions, and improves scalability across different model sizes and context lengthsespecially in long-sequence generation scenarios. Post-training to enhance models parallel decoding C.1 Inference Acceleration via Trajectory Compression While dLLMs show promise for non-sequential generation, their practical application is often hindered by high inference latency stemming from the iterative, multi-step sampling process. Inspired by the approach in Seed Diffusion (Song et al., 2025), which demonstrates the value of training on high-quality generation paths, we propose novel second-stage fine-tuning method, termed Trajectory Compression, to explicitly reduce the number of required sampling steps. The core idea is to train the model to jump between non-consecutive states within an optimal generation trajectory, thereby decoding multiple tokens in single forward pass. We refer to this resulting model as LLaDA-MoE-TD. Our method consists of two main stages: high-quality trajectory distillation and compressed transition learning. High-Quality Trajectory Distillation First, we generate dataset of golden trajectories. We use pretrained dLLM to sample large corpus of generation paths, T, on domain-specific dataset (e.g., 200,000 math problems). trajectory τ = (sN, sN1, . . . , s0) represents the sequence of states from the initial fully masked sequence sN to the final generated output s0. Each trajectorys final output s0 is evaluated by an external verifier, V(). For mathematical tasks, we employ math verify function to ascertain the correctness of the solution. We then filter the corpus to retain only the trajectories that result in correct output, forming high-quality dataset gold: gold = {τ V(sτ 0 ) = True} This process ensures that the subsequent fine-tuning stage learns from effective and valid reasoning paths. Compressed Transition Learning In the second stage, we fine-tune the dLLM on new objective. Instead of learning the standard single-step transition pθ(st1st), we train the model to predict multi-step transition from an early state si to later state sj, where > j. For each trajectory τ gold, we construct training instance by randomly sampling two timestamps, and j, where > 0. The pair (si, sj) serves as the input and target, respectively. Since tokens, once revealed, are fixed in subsequent steps, the models task is to predict the tokens that are [MASK] in si but are revealed 9 in sj. Let Mt be the set of indices of [MASK] tokens in state st. The model learns to predict the tokens at indices ij = Mi Mj. The fine-tuning objective is to minimize the negative log-likelihood of this compressed transition. The loss function is defined as: Lcompress(θ) = τT gold, i,jU(τ) ij log pθ(xk = sj[k] si) where sj[k] is the ground-truth token at position in the target state sj. To handle variable-length sequences, both si and sj are padded to the models maximum context length. This fine-tuning process endows the model with the ability to execute large jumps during inference, significantly accelerating generation. We measure this improvement using tokens per forward (TPF). Our experiments show that this method yields 99.8% increase in TPF for mathematical reasoning and an average TPF improvement of 45.3% across other domains, including code generation, confirming Trajectory Compression as an effective technique for reducing dLLM inference latency."
        },
        {
            "title": "D Detailed Configuration of Experiments",
            "content": "To achieve an optimal balance between efficiency and generation quality, different experimental settings adopt distinct combinations of decoding and optimization methods, as summarized in Table 3. Each configuration is tuned for its best trade-off between model performance and inference efficiency given the models characteristics and cache usage. We thus employ tailored algorithms for each setting, based on the ablation results provided in Table 4, Table 5, and Table 6, respectively. The hyperparameter settings for the corresponding methods are as follows: the threshold decoding uses confidence threshold of 0.8; the hierarchical decoding adopts decoding threshold of 0.92 and lower boundary threshold of 0.62; the iteration smoothing employs continuation weight (cont weight) of 0.3; and the vicinity KV-Cache refreshment strategy uses prefix look and after look of 16, with warmup times = 4. Table 3: Experimental settings and enabled methods. checkmark () indicates the method is applied. dInfer Setting Threshold Hier. Credit IterSmooth. Vicinity KV-Ref. LLaDA-MoE w/o KV-Cache LLaDA-MoE with KV-Cache LLaDA-MoE-TD with KV-Cache Table 4: Ablation study of decoding algorithm on LLaDA-MoE w/o KV-Cache setting. Config. Metric Avg. CRUX-O GSM8K HumanEval"
        },
        {
            "title": "Perf\nTPF",
            "content": "54.01 3.67 53.68 3.89 54.33 4.29 42.62 3.03 39.75 3.28 42.38 4. 82.41 3.11 80.97 3.55 82.26 3.76 60.98 5.40 64.02 5.80 63.41 6. IFEval MBPP 67.21 55.64 4.44 2.64 LCB V6 15.2 3.37 57.67 2.34 57.49 2.79 65.81 4.75 67.21 4. 13.88 3.64 13.22 3.92 Table 5: Ablation study of decoding algorithm on LLaDA-MoE with KV-Cache setting. Config. Metric Avg. CRUX-O GSM8K HumanEval"
        },
        {
            "title": "IFEval MBPP",
            "content": "LCB V"
        },
        {
            "title": "Perf\nTPF",
            "content": "53.96 3.87 53.90 3.15 51.56 3.4 41.38 4.02 48.82 3.20 37.88 3. 80.97 3.42 79.3 2.91 80.14 3.19 10 62.2 5.52 64.02 4. 61.59 5.01 58.78 2.32 53.97 1.92 53.97 2.08 67.45 4.54 66.28 3. 63.93 3.99 13 3.38 11.01 2.80 11.87 3.12 Table 6: Ablation study of decoding algorithm on LLaDA-MoE-TD with KV-Cache setting. Config. Metric Avg. CRUX-O GSM8K HumanEval"
        },
        {
            "title": "IFEval MBPP",
            "content": "LCB V"
        },
        {
            "title": "Perf\nTPF",
            "content": "49.56 5.83 52.72 5.67 48.89 5.17 35.25 6.26 40.12 6.06 34.5 4. 76.88 6.33 79.15 6.12 77.18 5.9 57.93 7.27 63.41 7.09 57.32 6. 56.93 3.14 56.19 2.98 50.83 2.77 60.89 6.73 65.11 6.60 62.53 6. 9.47 5.23 12.33 5.18 10.96 4."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Renmin University of China",
        "Shanghai Jiao Tong University",
        "University of Chinese Academy of Sciences",
        "Westlake University",
        "Zhejiang University"
    ]
}