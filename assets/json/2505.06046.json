{
    "paper_title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information",
    "authors": [
        "Joshua Harris",
        "Fan Grayson",
        "Felix Feldman",
        "Timothy Laurence",
        "Toby Nonnenmacher",
        "Oliver Higgins",
        "Leo Loman",
        "Selina Patel",
        "Thomas Finnie",
        "Samuel Collins",
        "Michael Borowitz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics."
        },
        {
            "title": "Start",
            "content": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information Joshua Harris Fan Grayson Felix Feldman Timothy Laurence Toby Nonnenmacher Oliver Higgins Leo Loman Selina Patel Thomas Finnie Samuel Collins Michael Borowitz"
        },
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) become widely accessible, detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics. 5 2 0 2 9 ] . [ 1 6 4 0 6 0 . 5 0 5 2 : r Figure 1: (left) PubHealthBench Full and Reviewed model accuracy, (right) PubHealthBenchFreeForm model accuracy. 95% Wilson CI. *LLM used to generate benchmark, **Judge LLM. corresponding author - joshua.harris@ukhsa.gov.uk Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Public health guidance represents an important source of information for UK residents and experts to inform personal, professional, and clinical decision making. The release of highly capable Large Language Models (LLMs) [1], and particularly chatbots [2], could represent potentially significant shift in how public health guidance is retrieved, analysed, and disseminated. This in turn raises significant opportunities and risks for public health institutions both internally and when engaging the public. Whilst LLMs often undergo broad range of evaluations during development [36], relatively little is currently known about LLMs public health knowledge, including existing UK Government guidance. Furthermore, due to guidance undergoing regular revisions, and differing guidance being issued across institutions and geographies, accurate up to date knowledge of UK public health guidance may be particularly challenging for LLM systems. Therefore, as recently observed for BBC news stories [7], there is risk that LLM based applications and chatbots generate hallucinations [8] or incomplete information regarding UK public health advice. This in turn could potentially have significant impact on the public. These risks, combined with the increasing desire within the UK Government to incorporate LLMs into existing real world processes [9, 10], means comprehensive evaluations of LLMs understanding of UK public health guidance are needed. In this paper we introduce new dataset, Multiple-Choice Question Answering (MCQA) benchmark, and free form response benchmark for assessing LLMs knowledge across broad range of UK public health guidance1. Specifically our contributions include: UK public health guidance dataset2 - We collect, extract, markdown format, and chunk information from over 1000 publicly available UK Health Security Agency (UKHSA) PDF and HTML documents from the UK Government website (gov.uk). We make the extracted text from filtered dataset (687 documents) publicly available in markdown format to enable further research into the use of LLMs on UK public health advice. PubHealthBench3 fully grounded MCQA benchmark - We implement an automated MCQA generation and validation pipeline grounded in the extracted guidance source text. This enables us to generate new benchmark with over 8000 MCQA questions to test LLM knowledge across broad range of current guidance. We provide results for both the full benchmark (PubHealthBench-Full) and manually reviewed subset (PubHealthBench-Reviewed). PubHealthBench-FreeForm - To assess LLMs in more realistic real-world setting we also implement free form response benchmark using the questions from the manually reviewed subset. By utilising the fact that every question can be linked back to the original source chunk and document, we implement grounded LLM judge to assess responses. Initial LLM evaluations - We evaluate 24 private and open-weight LLMs on this new public health benchmark. Given our focus on assessing knowledge, we primarily focus on SOTA non-reasoning models, but also include some leading reasoning models for comparison. Manual expert review and human baseline - To quality assure the benchmark and establish an upper bound for performance, human experts manually reviewed random sample of 800 MCQA examples (approx. 10% of the benchmark). Furthermore, in order to compare to human performance, 5 humans also took sample tests (total 600 MCQA examples) to establish human baseline with the use of search engines. This provides an initial indication of the boundary at which LLMs become similarly accurate to cursory search by non-expert humans."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 MCQA LLM evaluations Using Multiple-Choice Question Answering (MCQA) to assess the knowledge of LLMs is well established in the literature. Earlier work evaluating LLM knowledge in specific domains often used 1The guidance included can cover the entire UK or individual constituent countries, most documents primarily relate to English public health guidance. We also only included English language documents. 2Documents: https://huggingface.co/datasets/Joshua-Harris/PubHealthBench-documents 3Benchmark: https://huggingface.co/datasets/Joshua-Harris/PubHealthBench Figure 2: Example PubHealthBench MCQA benchmark questions. existing MCQA human assessments and exams [1114], for example in the medical domain using evaluations from the US Medical Licensing Examination (USMLE) [15, 16]. Broader evaluations of LLM knowledge and capabilities have also been introduced. One of the most widely adopted being the Massive Multitask Language Understanding (MMLU) benchmark [17]. More recently the MMLU-Pro benchmark by Wang et al. [18], updated the original MMLU evaluations for errors [19] and increased the question difficulty. New domain specific MCQA LLM evaluations have also been created, such as the GPQA [20] and ARC [21] benchmarks within science. 2.2 Synthetic MCQA datasets Aggregating existing examinations often provides the easiest comparison to human expert performance, however, this approach is vulnerable to both data leakage [22] into training datasets and is only possible where appropriate public examinations exist. In contrast, manually creating new MCQA LLM evaluations can solve these limitations but are costly to develop. This has led to research investigating automated approaches to MCQA generation grounded in existing corpora [2325]. To develop MMLU-Pro Wang et al. [18] use GPT4-Turbo to generate answer options, as well as to augment the option lists found in other LLM benchmarks (primarily MMLU) with additional distractors. Similarly combined human-LLM pipeline is used to construct the recent SuperGPQA benchmark [26] and in Asiedu et al. [27]s work evaluating LLMs for tropical and infectious diseases. Concurrent work by Shashidhar et al. [23] on the YourBench framework goes further and provides fully automated approach to generating MMLU style evaluations from new document corpora. 2.3 Public health evaluations In public health the closest evaluations to those in this paper were performed by Davies et al. [28], which assessed ChatGPT 3.5s open-ended responses to the UK Faculty of Public Healths Diplomate exam (DFPH) Paper 1 questions. ChatGPT was evaluated on 119 questions double marked by DFPH examiners. Ayers et al. [29] conduct similar evaluations using 23 questions across 4 public health domains. Finally, previous work by Harris et al. [30] evaluated LLMs on relevant classification and extraction tasks using public health guidance free text."
        },
        {
            "title": "3 Methods",
            "content": "To generate our MCQA benchmark we develop an automated pipeline (Figure 3) to extract free text from documents, chunk it into sections, generate MCQA samples, and filter to high quality subset. We focus on an automated approach for three reasons: (1) generating thousands of MCQA samples manually is highly time consuming, (2) public health guidance is frequently revised and so any approach needs to be amenable to regular updates, and (3) it allows us to easily extend our benchmarking to additional knowledge bases in the future. 3 Figure 3: Overview of MCQA generation pipeline. 3.1 UK public health information Public health covers very broad range of topics from biosecurity to tackling health inequalities. To assess LLMs knowledge of UK guidance across these areas we collect large corpus of 1,150 current UK Government guidance documents from the UK Government website (gov.uk) in HTML and PDF formats. 3.2 Pre-processing and chunking HTML documents are pre-processed and converted into markdown format. PDF document extraction is more challenging. Therefore, we use two stage pipeline to achieve the requisite performance on PDF documents. We first extract the raw text from the PDFs using existing tools. We then use OpenAIs GPT-4o-mini vision LLM via the API, prompting the model to extract the text from the image (including markdown headers). For each page individually we pass: an image of the PDF page, the raw markdown text extracted using existing tools for that page at the first step, and the header hierarchy from all previous pages processed up to that point in the document. This combined approach proved consistent method for generating markdown formatted versions of the text contained in our PDF documents. However, some missing content was still observed, particularly from infographics. We then split the documents into 20,488 smaller section chunks based on the markdown headers of each document, and include the hierarchy of higher level headers into every chunk to ensure relevant wider context and document structure is available. 3.3 Question generation Guidance documents often contain significant background information and operational details that do not directly relate to UK public health recommendations. Therefore, in order to generate relevant questions we first use an LLM to classify each chunk into whether it contains public health recommendation and filter out any chunks that do not (see Harris et al. [30] for background on this classification task). We also filter out chunks exceeding 2000 words. This reduces the total number of chunks to 7,946, which form the source material for our MCQA generation. We then use an LLM (Llama-3.3-70bn-Instruct) to generate two multiple choice questions per chunk in the standard MCQA format (Figure 2), with: single question, single correct answer option, and six incorrect distractor options per question. We use one-shot with Chain of Thought (CoT) prompt instructing the LLM to output its final answer as JSON (see Appendix A.3). To ensure the LLM has the required context it is also provided with the text chunks that appear either side of the target chunk in the document (whether or not these contain recommendations). We run this pipeline across all of the filtered text chunks generating 15,666 correctly formatted candidate MCQA samples. 3.4 Automated MCQA error detection and sampling To check consistency with the source text and improve the quality of our question set, we use LLMs to filter potentially invalid questions. For full details of the approach and evaluation see Appendix A.1. We select the Llama-3-70bn-Instruct model for this error detection step and filter the 15,666 candidate MCQA questions down to 14,440 that were not flagged as containing potential errors. Finally, we 4 remove questions relating to documents that contain guidance that has been withdrawn, and balance our dataset between HTML and PDF documents to ensure more even representation of topics (as PDF documents were often significantly longer). We retain the remaining approximately 4,000 unused questions from PDF source documents as potential internal hold-out set. 3.5 Final MCQA benchmark dataset Overall, the final public benchmark (PubHealthBench) consists of 8,090 MCQA questions covering public, clinical, and professional guidance across 10 public health topic areas, and 352 guidance areas, sourced from 687 documents containing UK Government public health information. Figure 4 provides breakdown of the benchmark by topic area and the intended audience for the guidance. Figure 4: PubHealthBench questions by guidance topic area and guidance audience. Figure 5: Overview of benchmark review steps. 3.6 Human expert quality assurance To quality assure the benchmark and estimate the underlying rate of invalid questions, we manually review random sample of 800 questions (c.10% of the benchmark). Based on this manual review we estimate the rate of ambiguous or invalid questions in the full benchmark to be approximately 5.5% (4.1%-7.3%, 95% Wilson score CI) in the final dataset. See Appendix A.4 for annotation details. However, the majority of questions identified as invalid related to situations where one of the distractor options could be considered an equally good answer to the specified correct answer. Therefore, whilst these questions were deemed ambiguous, the correct answer should remain one of the most likely guesses for LLMs. Accounting for this random guessing over usually two correct answers, we expect the upper bound score on this benchmark to be approximately 97%. This is supported by the observed model performance on questions classified as invalid, as discussed in Section 5."
        },
        {
            "title": "4 Model Evaluation Approach",
            "content": "To understand the level of knowledge current open-weight and private LLMs have about UK Government public health guidance, we assess 24 models on PubHealthBench, including: GPT-4.5 [31], Claude-Sonnet-3.7 [6], Gemma-3 [32], o1 [33], Phi-4 [34], OLMo-2 [35], and Llama-3.3 [3]. 5 4.1 Benchmark subsets We report overall results for three subsets of PubHealthBench: PubHealthBench-Full - the full MCQA benchmark, providing the broadest assessment of LLM capabilities. This enables us to provide results across granular topic areas within public health. PubHealthBench-Reviewed - the random test subset of 760 MCQA questions that have been manually reviewed by human experts, we report results both including and excluding questions classified as ambiguous or invalid so as to make the results comparable to the full benchmark. For the most expensive models we run this subset instead of the full benchmark for cost reasons. PubHealthBench-FreeForm - the same manually reviewed subset as in PubHealthBench-Reviewed but only asking the question (without multiple choice options) and allowing for open-ended free form responses, similar to how chatbot may respond in real world uses cases. We use grounded LLM judge to assess whether the free text answer is consistent with the source material. 4.2 Human baseline In addition to estimating the theoretical upper bound (97%), we also provide baseline human performance for comparability (Figure 5). To make this baseline as relevant for real world use cases as possible we seek to replicate the conditions that existed prior to the creation of LLMs. This should help us to understand the degree to which someone asking an LLM (often chatbot) public health related question is higher or lower risk in terms of information retrieval than human using the previous tools available (e.g Google) to try to find the same information themselves. Human test takers were instructed to answer the MCQA samples with access to search engines but without any LLM or AI enabled tools. Test takers were not trained public health specialists, and were encouraged to take no longer than 2 minutes per question. Under these conditions, humans scored 88% on 600 questions, about 9 percentage points below the potential upper bound. We highlight that this result is meant to simulate member of the public looking for relevant guidance relatively quickly. In practice, within UKHSA, questions about guidance are fielded by relevant domain experts not under timed-conditions, so accuracy would be considerably higher. See Appendix A.4 for further details on the human baseline setup. 4.3 Experimental setup We closely follow the prompts and answer extraction used in the MMLU-Pro benchmark [18]. However, as with the human baseline, in our LLM evaluations we seek to replicate similar query setup to that which might occur when interacting with chatbot or within simple LLM based application. Therefore, we focus on zero-shot prompting (see Appendix A.6 for the prompt templates), and only use CoT when it is the default behavior, as for reasoning models. For comparability across models and to match the highest risk real world deployments, we do not allow any LLMs access to external tools (e.g search) or information repositories. 4.3.1 Free form response evaluation Utilising the fact all questions are directly grounded in specific parts of the original source text we use an LLM as Judge setup [36, 37] for the free form answer evaluation. We prompt judge LLM (GPT-4o-Mini) with the question, ground truth answer, the LLM response, and six retrieved related chunks. The judge is asked to assess the response and provide binary classification for whether it is consistent with the source text and ground truth MCQA answer. For full details see Appendix A.7."
        },
        {
            "title": "5 Results",
            "content": "5.1 PubHealthBench - overall MCQA benchmark results For the 21 models run on the PubHealthBench-Full  (Table 1)  and the PubHealthBench-Reviewed subset  (Table 3)  , we find very high correlation in overall accuracy between the two sets, with correlation coefficient of >0.99, rank correlation of 0.98, and an average absolute score difference 6 Table 1: PubHealthBench-Full - zero-shot accuracy for test set of 7929 questions, refusals included as incorrect responses, and bold indicates the highest score. *LLM used to generate benchmark. Blood Safety, Hepatitis, STIs and HIV Chemicals and Toxicology Climate and Health Gastro and Food Safety HCAI, Fungal, AMR, Antimicrobial Use and Sepsis Health Protection in Inclusion Health Settings Other Radiation Tuberculosis, Travel, Zoonotic, and Emerging infections VPDs and Immunisation Overall Model Name GPT-4.5 o3-Mini Gemini-2.0-Flash Llama-3.3-70B* Phi-4-14B Gemini-Pro-1.5 Mistral-3.1-24B GPT-4o-Mini Claude-Haiku-3.5 Gemma-3-27B Gemma-2-27B Phi-4-4B Gemma-3-12B Command-R-32B GPT-4o Llama-3.1-8B Olmo-2-32B Command-R-7B Olmo-2-13B Gemma-3-4B Gemma-3-1B 90.9 87.7 84.7 86.5 85.4 81.3 85.4 83.1 82.4 84.5 84.2 82.0 80.4 79.7 79.5 79.2 76.7 76.3 76.0 73.5 49.5 91.1 88.5 86.1 86.6 82.7 81.6 82.7 80.1 80.5 79.7 79.3 79.7 79.0 77.2 77.4 77.2 74.7 72.3 69.8 69.7 49. 97.4 94.5 95.3 92.4 92.1 93.9 92.1 91.6 92.4 91.6 91.3 90.3 89.5 89.5 89.2 89.2 88.9 86.3 87.6 85.8 57.9 90.3 88.1 86.0 85.5 85.5 84.1 84.8 81.9 80.3 80.3 82.9 81.2 76.5 76.7 79.1 77.9 75.5 74.1 74.1 71.5 45.1 94.6 92.4 88.2 87.9 90.4 87.3 89.7 88.6 86.2 83.9 83.9 87.1 83.0 83.7 80.8 84.2 82.8 76.3 76.8 73.9 50.9 94.0 90.6 90.6 90.9 88.7 90.0 88.6 88.1 87.6 87.4 86.9 85.7 84.4 84.8 85.9 84.8 83.4 79.0 78.8 79.2 51.4 91.3 87.5 87.1 86.1 89.5 86.1 87.8 86.4 86.4 84.3 84.3 86.4 86.4 83.6 87.5 84.3 80.8 82.9 80.8 81.5 57.1 89.9 87.1 88.5 87.5 84.7 85.0 84.7 82.9 86.1 80.5 80.5 77.4 81.2 77.7 79.8 78.0 77.4 71.4 74.2 67.9 44. 91.8 87.1 86.3 85.3 85.1 84.1 83.0 79.9 81.1 80.1 80.4 78.6 79.1 79.1 79.6 76.2 76.4 70.3 71.4 69.8 44.8 93.0 88.3 87.4 87.2 85.4 86.0 83.5 82.8 81.5 82.0 81.3 80.3 79.2 80.7 76.7 79.3 77.1 73.1 72.8 67.4 42.1 92.5 88.9 87.7 87.4 86.1 85.6 85.1 83.5 83.2 82.7 82.6 81.8 80.8 80.7 80.2 80.0 78.4 74.7 74.4 72.2 47.6 of under 1 percentage point. Therefore, we compare overall results directly for models only run on PubHealthBench-Reviewed for cost reasons. We provide tables with 95% Wilson score confidence intervals in Appendix A.8. On MCQA format questions we find the latest private LLMs perform very strongly, with the highest scoring models GPT-4.5, GPT-4.11, and o11, all achieving over 90% accuracy, above the human baseline and nearing the benchmarks estimated upper bound. Smaller open-weight models also show reasonable degree of knowledge of public health guidance with most 5-15bn parameter models scoring above 75%. However, recent \"reasoning\" models (o1 and o3-Mini) perform similarly to \"non-reasoning\" models with little additional benefit from the extra test time compute in the MCQA setting. As shown in Table 3, due to the nature of the MCQA issues identified during manual review, we find models still achieve on average 60% accuracy on MCQA questions labeled invalid. This adds additional support to our estimated upper bound for the benchmark of approximately 97%. Table 2: PubHealthBench-Full zero-shot accuracy by guidance type. *LLM used to generate benchmark. Table 3: PubHealthBench-Reviewed zero-shot accuracy by question and response type. *LLM used to generate benchmark, **Headline result. Clinical Guidance Multiple Audiences Professional Guidance Public Guidance Unclassified Overall Model Name GPT-4.5 o3-Mini Gemini-2.0-Flash Llama-3.3-70B* Phi-4-14B Gemini-Pro-1.5 Mistral-3.1-24B GPT-4o-Mini Claude-Haiku-3.5 Gemma-3-27B Gemma-2-27B Phi-4-4B Gemma-3-12B Command-R-32B GPT-4o Llama-3.1-8B Olmo-2-32B Command-R-7B Olmo-2-13B Gemma-3-4B Gemma-3-1B 91.5 85.9 84.8 84.9 84.5 82.5 81.4 80.7 79.5 78.7 79.4 78.3 76.9 77.7 73.7 76.8 74.6 70.1 69.6 64.7 40.1 93.7 91.7 89.7 89.5 88.1 90.0 87.2 85.8 85.3 86.0 84.4 84.4 83.2 83.2 83.3 82.3 81.8 73.6 77.2 74.2 45. 91.9 88.7 87.6 87.1 85.5 84.8 84.9 83.1 83.0 82.3 82.1 81.8 80.7 79.5 80.9 80.1 78.1 75.6 74.2 73.1 50.3 96.1 92.8 93.1 91.7 90.5 90.6 90.1 87.9 87.4 87.8 87.5 85.4 87.1 86.9 86.2 82.6 84.9 79.3 81.2 77.7 48.7 92.1 88.9 86.3 87.1 85.0 85.9 86.0 83.9 84.9 83.9 83.4 82.5 80.2 82.6 80.7 81.1 77.4 75.4 74.9 74.7 50.5 92.5 88.9 87.7 87.4 86.1 85.6 85.1 83.5 83.2 82.7 82.6 81.8 80.8 80.7 80.2 80.0 78.4 74.7 74.4 72.2 47.6 Exc. Refusals Inc. Refusals** Invalid MCQA Valid MCQA Model Name GPT-4.5 GPT-4.1 o1 Gemini-2.0-Flash o3-Mini Claude-Sonnet-3.7 Llama-3.3-70B* Phi-4-14B Gemini-Pro-1.5 Mistral-3.1-24B GPT-4o-Mini Claude-Haiku-3.5 Gemma-3-27B Gemma-2-27B Phi-4-4B Llama-3.1-8B Command-R-32B GPT-4o Gemma-3-12B Olmo-2-32B Olmo-2-13B Gemma-3-4B Command-R-7B Gemma-3-1B 92.9 92.2 91.8 88.5 88.3 92.4 87.4 86.8 86.2 84.7 83.9 83.3 82.9 82.9 81.7 81.1 80.8 91.8 80.3 78.2 75.1 73.4 72.9 45.9 92.9 92.2 91.8 88.4 88.3 87.8 87.4 86.8 86.2 84.7 83.9 83.3 82.9 82.9 81.7 81.1 80.8 80.7 80.3 78.2 75.1 73.4 72.9 45.9 71.4 78.6 66.7 61.9 69.0 59.5 61.9 66.7 59.5 61.9 52.4 57.1 59.5 54.8 64.3 57.1 59.5 52.4 61.9 54.8 57.1 54.8 57.1 26. 94.2 93.0 93.3 90.0 89.4 89.4 88.9 88.0 87.7 86.1 85.8 84.8 84.3 84.5 82.7 82.5 82.0 82.3 81.3 79.5 76.2 74.5 73.8 47.1 1Results on the PubHealthBench-Reviewed subset. 7 5.2 PubHealthBench - results by topic area and audience Breaking down PubHealthBench-Full results by topic area  (Table 1)  , we find consistently higher performance across models on Climate and Health and Health Protection in Inclusion Health Settings guidance. While LLMs generally performed worse on Chemicals and Toxicology (Figure 6). Looking at results by guidance audience  (Table 2)  , we find LLMs have better knowledge of guidance intended for the general public, and worse knowledge of clinical guidance (Figure 6). On public guidance the highest performing model (GPT-4.5) scores 96%, close to the estimated upper bound. Figure 6: Average deviation from overall model performance on PubHealthBench-Full by guidance topic (left) and guidance audience (right). 5.3 PubHealthBench-FreeForm results The free form response setup is substantially more challenging for few reasons: (1) it requires recalling the correct guidance information without any hints from MCQA options, (2) it introduces the possibility of LLMs hallucinating additional information that may be inconsistent with the source text, and (3) the correct answer cannot be inferred via elimination of the other answer options. As result, all models achieve substantially lower scores in the free form setting by up to 60 percentage points (ppts). The best performing model (o1) scores 74%  (Table 4)  and also sees the smallest decline from its MCQA performance at -17ppts  (Table 5)  . Notably there is also significant variation across models with some smaller LLMs (e.g Phi-4-14B) showing >45ppts declines from MCQA accuracy, while other similarly sized models (e.g Gemma-312B) see drops of comparable magnitude to SOTA private LLMs. Unlike in the MCQA setup, we see initial indications that \"reasoning\" models (o1 and o3-Mini) start to outperform \"non-reasoning\" models . Also, importantly, as observed in the MCQA setting, LLMs consistently show more accurate knowledge of guidance intended for the general public compared to clinical or professional guidance."
        },
        {
            "title": "6 Discussion",
            "content": "Our results suggest that current SOTA LLMs, both private and open-weight, in general have very high level of knowledge across UK public health guidance. This is particularly notable given 31% of the MCQA questions were based on guidance documents that were at least partially updated within 2024 (see Appendix A.2), after many of the LLMs training data cut-off date. 8 Table 4: PubHealthBench-FreeForm model accuracy by guidance audience. *LLM used to generate benchmark, **Judge LLM. Table 5: Difference in model accuracy between MCQA and Free Form settings. *LLM used to generate benchmark, **Judge LLM. Clinical Guidance Multiple Audiences Professional Guidance Public Guidance Unclassified Total PubHealthBench Reviewed PubHealthBench FreeForm MCQA - FreeForm Difference Model Name o1 GPT-4.1 o3-Mini GPT-4o GPT-4.5 Claude-Sonnet-3.7 Gemini-2.0-Flash Gemma-3-27B Gemini-Pro-1.5 Gemma-3-12B Claude-Haiku-3.5 GPT-4o-Mini** Llama-3.3-70B* Mistral-3.1-24B Phi-4-14B Olmo-2-32B Gemma-3-4B Command-R-32B Olmo-2-13B Gemma-2-27B Command-R-7B Llama-3.1-8B Phi-4-4B Gemma-3-1B 71 65 65 60 59 57 56 50 46 50 51 37 35 36 33 37 25 30 30 27 20 16 16 14 81 71 78 71 71 66 64 61 58 53 61 54 53 39 46 51 39 41 32 36 17 22 24 12 70 71 69 54 54 55 53 53 51 48 40 40 38 36 37 37 35 29 30 32 22 15 17 21 86 82 78 82 77 76 77 73 63 74 68 68 60 64 59 55 58 53 60 49 46 38 29 82 69 74 63 59 57 61 52 61 55 47 41 40 40 40 29 44 42 34 34 19 18 15 14 74 71 70 61 59 59 58 55 53 52 48 43 41 40 39 39 37 34 34 33 23 19 19 18 Model Name o1 o3-Mini GPT-4o GPT-4.1 Gemma-3-1B Gemma-3-12B Gemma-3-27B Claude-Sonnet-3.7 Gemini-2.0-Flash Gemini-Pro-1.5 GPT-4.5 Claude-Haiku-3.5 Gemma-3-4B Olmo-2-32B GPT-4o-Mini** Olmo-2-13B Mistral-3.1-24B Llama-3.3-70B* Command-R-32B Phi-4-14B Command-R-7B Gemma-2-27B Llama-3.1-8B Phi-4-4B 91 88 80 92 45 80 82 87 88 86 92 83 73 78 83 75 84 87 80 86 72 82 81 81 74 70 60 70 18 52 54 58 57 52 59 48 36 38 43 33 39 40 34 39 23 33 18 -17 -18 -19 -21 -27 -27 -28 -29 -30 -33 -33 -35 -36 -39 -40 -41 -45 -46 -46 -47 -49 -49 -62 -63 However, we find that performance is significantly degraded across models in the free form response setting. This is in part due to models often including extraneous recommendations that do not form part of the source UK public health guidance. Interestingly, we find reasoning models see notable relative improvement in the free form response setup compared to the MCQA setup, potentially due to their ability to recall wider set of relevant information as part of the reasoning process before only providing explicit UK public health guidance in the final response. Importantly for real-world use cases and deployments we see large disparities between the private and large open-weight models compared with smaller open-weight LLMs (1-15bn parameters). On MCQA questions this gap is generally 10-20ppts but often grows to >35ppts in the free form setting. Therefore, there still appear to be significant risks around hallucinations relating to UK public health guidance when using smaller LLMs. From public health perspective, it is also an important finding that LLMs consistently performed best on guidance that is intended for the general public. This audience is likely the highest risk set of users for querying general chatbots to retrieve public health information. Therefore, the fact LLMs are observed to have greater knowledge in this area implies the risks may be lower than the overall results would imply. Further work is needed to identify the common types of risk that incorrect MCQA or free form answers could pose and the degree to which LLMs specify lack of knowledge (for example answering \"I dont know\") compared to hallucinating incorrect information."
        },
        {
            "title": "7 Limitations",
            "content": "Whilst we have attempted to include broad range of topics and two LLM response formats (MCQA and free form), limitation is that this still only represents few of the ways LLMs could be used to retrieve public health information. Further work investigating different types of public health query is needed, for example, multi-turn interactions, queries including images, queries about topics that are related to public health but not directly incorporated into UK guidance, or allowing tool use. In this work, we also focus on English-language permissively-licensed online UK Government information, which is likely to be at least partially incorporated into LLM training data. Therefore, limitation is that these results may not extrapolate to LLM performance on other countries guidance or to guidance in other languages."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper we use an automated pipeline to generate new benchmark, PubHealthBench, to assess LLM knowledge of current UK Government public health guidance. 9 We evaluate SOTA LLMs across three versions of the benchmark: full, verified, and free form. By testing LLMs across 10 guidance topic areas and 3 intended audiences in the MCQA and free form response setups, we provide an initial assessment of the performance and risks of using current LLMs in this domain. Our results suggest that the latest private LLMs have high degree of knowledge of UK public health guidance but challenges remain consistently matching the ground truth guidance in the free form setting. We hope this new benchmark and UK Government guidance dataset will enable further research and evaluation of LLMs for public health, and reduce the risks within real-world deployments."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was fully funded by the UK Health Security Agency (UKHSA) an executive agency of the Department of Health and Social Care. No competing of interests were identified. This work was enabled by UKHSA HPC Cloud & DevOps Technology colleagues developing and maintaining internal HPC resources. We would also like to thank our UKHSA colleagues in the Chief Medical Advisor Group for their support and expertise in developing this work."
        },
        {
            "title": "References",
            "content": "[1] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: survey, 2025. URL https://arxiv.org/abs/2402. 06196. [2] OpenAI. Introducing chatgpt, 2022. Introducing ChatGPT, Accessed: 08/04/2025. [3] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng 10 Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [4] Google. Gemini 2.5: Our most intelligent ai model, 2025. Gemini 2.5: Our most intelligent AI model, Accessed: 03/04/2025. [5] OpenAI. Openai o3-mini, 2025. OpenAI o3-mini, Accessed: 03/04/2025. [6] Anthropic. Claude 3.7 sonnet, 2025. Claude 3.7 Sonnet, Accessed: 03/04/2025. [7] BBC. Representation of BBC News content in AI Assistants, 2024. URL https://www.bbc.co.uk/ [Online; accessed 13-Febaboutthebbc/documents/bbc-research-into-ai-assistants.pdf. 2025]. [8] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 11 43(2):155, January 2025. ISSN 1558-2868. doi: 10.1145/3703155. URL http://dx.doi.org/10. 1145/3703155. [9] Innovation Department for Science and Technology. Ai opportunities action plan: government response, 2023. AI Opportunities Action Plan: government response, Accessed: 03/04/2025. [10] UKHSA. Ukhsa advisory board - artificial intelligence discovery exercise, 2023. UKHSA Advisory Board: Artificial Intelligence Discovery Exercise, Accessed: 03/04/2025. [11] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [12] Jillian Bommarito, Michael Bommarito, Daniel Martin Katz, and Jessica Katz. Gpt as knowledge worker: zero-shot evaluation of (ai)cpa capabilities, 2023. URL https://arxiv.org/abs/2301.04408. [13] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams, 2020. URL https://arxiv.org/abs/2009.13081. [14] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa : large-scale multisubject multi-choice dataset for medical domain question answering, 2022. URL https://arxiv.org/ abs/2203.14371. [15] Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor Tseng. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLOS Digital Health, 2(2):112, 02 2023. doi: 10.1371/journal.pdig.0000198. URL https://doi.org/10. 1371/journal.pdig.0000198. [16] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023. [17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL https://arxiv.org/ abs/2009.03300. [18] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. URL https://arxiv.org/abs/2406.01574. [19] Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, and Pasquale Minervini. Are we done with mmlu?, 2024. URL https://arxiv.org/abs/2406.04127. [20] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. [21] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/abs/1803.05457. [22] Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models, 2024. URL https://arxiv.org/abs/ 2311.09783. [23] Sumuk Shashidhar, Clémentine Fourrier, Alina Lozovskia, Thomas Wolf, Gokhan Tur, and Dilek HakkaniTür. Yourbench: Easy custom evaluation sets for everyone, 2025. URL https://arxiv.org/abs/2504. 01833. [24] Gauthier Guinet, Behrooz Omidvar-Tehrani, Anoop Deoras, and Laurent Callot. Automated evaluation of retrieval-augmented language models with task-specific exam generation, 2024. URL https://arxiv. org/abs/2405.13622. [25] Gayane Ghazaryan, Erik Arakelyan, Pasquale Minervini, and Isabelle Augenstein. Syndarin: Synthesising datasets for automated reasoning in low-resource languages, 2024. URL https://arxiv.org/abs/ 2406.14425. 12 [26] Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shawn Gavin, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, David Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tyshawn Hsing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. URL https://arxiv.org/abs/2502.14739. [27] Mercy Asiedu, Nenad Tomasev, Chintan Ghate, Tiya Tiyasirichokchai, Awa Dieng, Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, Eric Ndombi, and Katherine Heller. Contextual evaluation of large language models for classifying tropical and infectious diseases, 2025. URL https://arxiv.org/abs/2409.09201. [28] Nathan Davies, Robert Wilson, Madeleine Winder, Simon Tunster, Kathryn McVicar, Shivan Thakrar, Joe Williams, and Allan Reid. Chatgpt sits the dfph exam: large language model performance and potential to support public health learning. BMC Medical Education, 24(1):57, 2024. [29] John Ayers, Zechariah Zhu, Adam Poliak, Eric Leas, Mark Dredze, Michael Hogarth, and Davey Smith. Evaluating artificial intelligence responses to public health questions. JAMA network open, 6(6): e2317517e2317517, 2023. [30] Joshua Harris, Timothy Laurence, Leo Loman, Fan Grayson, Toby Nonnenmacher, Harry Long, Loes WalsGriffith, Amy Douglas, Holly Fountain, Stelios Georgiou, Jo Hardstaff, Kathryn Hopkins, Y-Ling Chi, Galena Kuyumdzhieva, Lesley Larkin, Samuel Collins, Hamish Mohammed, Thomas Finnie, Luke Hounsome, and Steven Riley. Evaluating large language models for public health classification and extraction tasks, 2024. URL https://arxiv.org/abs/2405.14766. [31] OpenAI. Openai gpt-4.5 system card, 2025. GPT-4.5 System Card, Accessed: 07/05/2025. [32] Gemma Team. Gemma 3. 2025. URL https://goo.gle/Gemma3Report. [33] OpenAI. Openai o1 system card, 2024. o1 System Card, Accessed: 07/05/2025. [34] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024. URL https://arxiv.org/abs/2412.08905. [35] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656. [36] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-ajudge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/abs/2306.05685. [37] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. survey on llm-as-a-judge, 2025. URL https://arxiv.org/abs/2411.15594. [38] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [39] Sam Bowyer, Laurence Aitchison, and Desi R. Ivanova. Position: Dont use the clt in llm evals with fewer than few hundred datapoints, 2025. URL https://arxiv.org/abs/2503.01747."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Automated MCQA error detection Our approach to automated filtering of candidate questions leverages the fact that synthetic MCQA generation allows us to create an essentially arbitrary number of potential questions but with the risk of material question errors. An advantage of this is that we can utilise an LLM classier with high recall (accurately identifying incorrect questions) even if this comes at the expense of low precision (large numbers of valid questions being rejected) to filter our question set and ensure fewer material errors in the final benchmark. To assess whether we can use LLMs for this error classification task on our generated MCQA questions we first manually annotated an evaluation dataset of LLM generated samples. We use samples from the earliest version of the pipeline that contained much higher question error rate to ensure we had enough positive and negative samples. We build on the categorisation approach developed by Gema et al. [19] to annotate each question with one of 5 categories spanning the common errors that can be found in MCQA questions (the reviewer should allocate the first label that applies): 1. Valid Question and Options The question contains all the required context to be able to answer the question and it is clear what information the question is seeking. 2. Ambiguous Question The question cannot be answered standalone for some reason, including: (a) It is missing context necessary to answer the question (e.g., the disease the question refers to, the population the question refers to, etc.). (b) It is poorly formed or does not make sense (e.g., asking about made-up country). (c) It contains material grammatical or typographical errors. 3. Ambiguous Options The question is valid but it is not possible to determine if there is correct option for some reason, including: (a) The options do not make sense given the question. (b) The options do not contain enough context to determine whether any are correct. 4. Incorrect Answer The question is valid and the options are clear but the proposed correct answer (a.) is not valid correct answer to the question, independent of the other options provided. 5. Multiple Correct Answers The question is valid, the options are clear, and the proposed correct answer (a.) is true, but there are also other options that would be equally valid. Two human experts then reviewed random sample of 150 generated questions. We found the level of ambiguity and distractor option differentiation that was permissible for valid question challenging task even for expert human annotators. On the 150 questions inter-annotator agreement was 81% with Cohens Kappa of 0.39. To create final validation set of annotations all disagreements were reviewed and resolved by panel of the two reviewers and 3rd expert. 14 Figure 7: Confusion matrix for LLM MCQA error detection. For the binary classification task of valid vs invalid, on the test set (of low quality questions) shown in Figure 7 using Llama-3-70bn, this approach would in theory of reduced the benchmark error rate from approximately 16% to approximately 8%. However, we note that the limited test sample means further evaluation of approaches is needed. In our final pipeline, this stage was less critical as improving both the prompting approach and the LLM used in the generation process (Llama-3 to Llama-3.3) substantially reduced the rate of invalid MCQA samples generated by the pipeline. This is illustrated by the fact only approximately 8% of generated MCQA samples were classified as invalid by the LLM error detection step in the final pipeline, compared with 44% in the validation test set which was sampled from an early version. 15 A.2 Additional benchmark statistics Figure 8: PubHealthBench-Full MCQA by source guidance document last revision date. Note - the last revision to document may only entail minor changes from the original text. Figure 9: (left) PubHealthBench-Full samples split by source text document type, (right) MCQA quality human annotations - \"Good\" and \"Acceptable\" categories are both treated as valid question samples. 16 A.3 MCQA generation prompts Table 6: MCQA generation system prompt (see Table 7 for user prompt). System Prompt You are top public health expert, creating multiple choice test to assess individuals knowledge of UK Public Health Guidance - you should select questions relating to important parts of the information in the guidance text provided that have material public health implications. The questions should not relate to minor details such as phone numbers or other inconsequential information. To make the questions in the test you are going to be given piece of current UKHSA guidance, within which there will be highlighted passage START OF SOURCE TEXT THE QUESTION SHOULD RELATE TO [PASSAGE] END OF SOURCE TEXT THE QUESTION SHOULD RELATE TO . You will then do five things: 1. You will provide list of bullet points of the key parts of highlighted passage in the guidance text provided that could be potentially important public health information and explain why they could be important. These bullet points should entirely come from the highlighted passage in the guidance text provided, you should not include any context or information not contained within the highlighted passage in the guidance text. 2. You will then make list of bullet points of the relevant pieces of contextual information contained in the highlighted passage of the source text that are necessary to include in future questions - THIS INFORMATION MUST BE INCLUDED IN THE QUESTION ITSELF UNLESS IT IS IRRELEVANT. 3. You will generate question based on the information in the highlighted passage of the text provided (no other information should be needed to give the correct answer). The public health expert being asked the question will not have the source text and so YOU MUST ENSURE that all relevant context required is included in the question (e.g the names of the dieseases the question relates to, the sub-populations being disucssed, the geographical area the guidance relates to, the time period if specified, etc.). 4. You will then generate 7 multiple choice answers (a. to g.). The correct answer should always be the first answer (a.) you provide. The other answers should be incorrect but very plausible and challenging even for public health expert to really test whether they actually know the fact. To construct the distractors you should draw on information in both the highlighted passage and the wider guidance provided. 5. Repeat steps 3., and 4., in order to generate total of 2 questions and answers. You should provide your final questions and options in the following JSON format: {{ \"0\": {{\"question\": [INSERT QUESTION], \"a\": [INSERT CORRECT ANSWER], \"b\": [INSERT INCORRECT ANSWER], \"c\": [INSERT INCORRECT ANSWER], \"d\": [INSERT INCORRECT ANSWER], \"e\": [INSERT INCORRECT ANSWER], \"f\": [INSERT INCORRECT ANSWER], \"g\": [INSERT INCORRECT ANSWER] }}, \"1\" : {{\"question\": [INSERT QUESTION], \"a\": [INSERT CORRECT ANSWER], \"b\": [INSERT INCORRECT ANSWER], \"c\": [INSERT INCORRECT ANSWER], \"d\": [INSERT INCORRECT ANSWER], \"e\": [INSERT INCORRECT ANSWER], \"f\": [INSERT INCORRECT ANSWER], \"g\": [INSERT INCORRECT ANSWER] }} }} 17 Table 7: MCQA generation user prompt (see Table 6 for system prompt). Prompt Content IMPORTANT NOTES YOU MUST FOLLOW: 1. No real phone numbers or urls from the text should be included. 2. In both the question and the answer options you generate you should NOT mention anything relating to the structure of OTHER parts of the source text not included below, for example you should NOT mention things like: other sections of the text (e.g \"refer to section 10\"), question numbers (e.g \"if the patient has answered yes to question 1\"), further reading (e.g \"see appendix for more information\"), etc.. 3. Be very careful not to include correct answers in the distractor options b. to g. (or distractors that are potentially correct based on your wider knowledge). 4. You must only generate question specifically about the passage marked using: START OF SOURCE TEXT THE QUESTION SHOULD RELATE TO [PASSAGE] END OF SOURCE TEXT THE QUESTION SHOULD RELATE TO . You should only use the information outside of this passage for context. 5. Make sure you include all the relevant context in both the questions you generate, these questions will be separated and so should be totally independent. Here is an example you should use as template for the structure and style: ================================ {one shot CoT example} ================================ Now please follow the instructions above and generate the question and answer options for this piece of UKHSA guidance. Guidance text: {guidance text} Answer (Provide the bulleted list, contextual information, then the final JSON): A.4 Human manual annotation In this work we perform two sets of human annotation, the first to estimate the rate of invalid or ambiguous questions, and the second to set human baseline. In both cases, all annotators were full time employees within the organisation with relevant data science or public health experience. A.4.1 Estimating the benchmark error rate Assessing MCQA sample validity is challenging annotation task. For question to be valid, judging the level of context (e.g subpopulation, geography, time period etc.) that is required for the question to be answerable can involve significant degree of subjectivity. For the options to be valid, the boundary between when challenging distractor option crosses over into being potentially equally valid answer to the specified correct answer, rendering the question ambiguous, is also subjective. Therefore, we followed two round annotation process. In the first round, pairs of human experts were assigned total of 150 MCQA samples for double review. Any discrepancies in annotations between reviewers were then assessed by all reviewers and the correct final annotation agreed. This enabled us to identify and rectify inconsistencies in the application of the annotation protocol. In the second round, the remaining 650 MCQA samples were then annotated by single reviewers who participated in the first round. We provide the Wilson score 95% confidence interval for binomial proportion. The full instructions provided to reviewers are shown in the box below: 18 Protocol for Manual Review of Exam Questions This protocol provides structured criteria for assessing the validity of guidance LLM benchmark questions and answer options into three categories: Good, Acceptable, and Incorrect. 1. Good Questions Criteria: 1. Question Valid: The question is answerable and clearly aligned with the guidance document. 2. Best Answer Clearly Identifiable: The correct answer is evidently the best choice compared to the other options and is similar to hypothetical gold standard answer. 3. Other Options Incorrect but Not Trivially Wrong: At least some of the other incorrect options are plausible, not wrong by definition, or so obviously wrong that an uninformed member of the public could say that is not something guidance would ever say. 4. Informative Value: The LLMs performance on this question adds meaningful information about the LLMs knowledge of the guidance. 2. Acceptable Questions Definition: Questions in this category are valid but have limitations that reduce their overall quality or informativeness. These questions are still useful but might not be as robust as \"Good\" questions. Criteria: 1. Question Valid but Some Ambiguity: The question can be understood and is answerable, but it may be missing some context, contain uncommon acronyms, or may assume high degree of knowledge. 2. Gaps in Correct Answer but Still the Best Option: The correct answer is the best choice from the options provided, but may not be the perfect gold standard answer, may lack some detail or nuance from the guidance, and may be poorly phrased. 3. Other Options Incorrect but Potentially Trivially Wrong: All incorrect options are worse options than the correct answer but they may be incorrect trivially, making the correct answer easy to guess. 4. Some Relevance but Lower Informative Value: The LLMs performance on this question has some relevance (even if minor) but may test less critical aspects of the guidance, or the question may cover overlapping points with others in the dataset. Very easy questions would also be acceptable. 5. Not Directly Aligned with the Chunk Provided: In some cases, the question may relate to text found either side of the intended chunk of guidance in the document. This is acceptable so long as the question still meets the criteria above. 3. Invalid Questions Definition: Questions in this category cannot be answered due to material errors and are unsuitable for use in the evaluation. Criteria: 1. Invalidity: The question is not answerable due to ambiguity, misinterpretation of guidance, or grammatical errors. 2. Misleading Answer Options: The correct answer option provided is either: (a) Not valid answer to the question, or (b) Clearly less accurate answer than one or more of the distractor options. 3. Errors in Construction: The question has structural or logical flaws that render it unusable. A.4.2 Human baseline To set the human baseline 600 questions were randomly sampled from the manually reviewed subset of 800 questions. To make the human baseline comparable to the full benchmark scores these samples included MCQA examples that human annotators had classified as invalid. The 600 questions were then randomly allocated among 5 human test takers who had not manually reviewed the questions previously. Results were then aggregated to provide the human baseline. Test takers were provided with the following instructions: Human Test Taking Instructions These multiple choice questions are designed to assess LLM knowledge of UKHSA public health guidance. Understanding the performance of humans on these tests is useful for two reasons: 1. To provide baseline for the overall difficulty of the questions. 2. To understand whether human asking chatbot (an LLM) public health related question is higher or lower risk than human using previous tools (e.g., Google) to try to find the same information themselves. To answer the questions, please follow these instructions: Question Format 1. Standard Multiple Choice Every question has one correct answer option in the provided options list. 2. Select Best Answer There may be multiple options that are technically valid information; you should select the answer that is the best reflection of UKHSA guidance. 3. Dont Know For any questions where you arent confident in the answer, you should provide your best guess. Test Conditions 1. Timing Whilst there is no time constraint on completing the test, we recommend spending no more than 2 minutes on average per question. 2. Open Book Feel free to use your own knowledge and any publicly available online guidance resources to answer the questions. 3. No LLM Tools Please do not use any LLM-based tools or chatbots to search for guidance or question answers. A.5 Experimental setup and compute resources For all experiments and models where it can be specified we use greedy decoding (temperature 0). For all OpenAI reasoning models we use \"reasoning_effort=low\" for cost reasons and due to the knowledge based nature of the task. Private models are accessed via provider APIs, open-weight models are hosted internally on HPC resources using vLLM [38], with up to 2 A100 80GB used per model. Additional compute (also using 2x A100 80GB) was used for benchmark generation pipeline. We used budget of $400 for the private model evaluations, including the LLM judge. A.6 PubHealthBench prompts Table 8: MCQA zero-shot evaluation prompt. System Prompt You are an expert working for Public Health agency. Prompt Content The following are multiple choice questions (with answers) about UK Government public health guidance. Question: This question relates to UK Health Security Agency (UKHSA) guidance that could be found on the gov.uk website as of 08/01/2025. {question} Options: {answer_options} Provide the letter (A, B, C, D, E, F, or G) of the correct answer. You should state \"The answer is (X)\", where the contained in the brackets is the correct letter choice, make sure you include the brackets () around your final answer in your response. DO NOT provide any other information or text in your response. Answer: Table 9: Free form evaluation prompt . System Prompt You are an expert working for Public Health agency. Prompt Content The following is question about UK Government public health guidance. Question: This question relates to UK Health Security Agency (UKHSA) guidance. {question} Please answer the question to the best of your knowledge. Answer: A.7 Free form evaluation - LLM as Judge setup To provide an accurate but cost effective LLM judge to assess unstructured free form LLM responses, we frame it as binary classification task for whether the free form responses align with the correct answer and source text. We provide the judge with the guidance source text that was used to generate the question. This ensures the judge always has access to the ground truth information. To allow the judge to correctly evaluate answers that may include additional information that relates to other aspects of UK public health guidance, we provide 5 additional chunks of relevant guidance in the prompt. These 5 additional chunks for each question are retrieved from the full corpus using hybrid retrieval system that ranked relevance by combining the cosine similarities of text embedding vectors (OpenAIs text-3-embedding-large model) and TF-IDF vectors, between each chunk and given question. Finally, to guide the judge to the most important information to include, we also provide the MCQA correct answer option. The full LLM judge CoT prompt combining these components is shown in Table 10. To create judge evaluation dataset and assess the performance of the LLM judge, we utilise the already generated correct and incorrect MCQA answer options. We insert the known correct or incorrect answer option into variety of free form chat response templates designed to simulate the structure of LLM free form responses. We assess the ability of the judge to distinguish whether the response is valid using the provided source text. In this paper we use gpt-4o-mini-2024-07-18 as the judge with greedy decoding, which achieved >99% accuracy on the judge evaluation set (10,517 samples). Further details of this LLM judge evaluation approach will be published in upcoming work. Table 10: Judge prompt. System Prompt You are an expert in UK public health. You are going to evaluate whether given answer to public health guidance question is correct. Prompt Content You are tasked with evaluating whether given answer is correct based on the ground truth answer and provided context. Carefully analyse the ground truth answer and context and determine whether the given answer correctly answers the question and aligns with the information given. =========== Question: =========== {question} =========== Context: =========== {context} =========== Ground Truth Answer: =========== {ground truth answer} =========== Given Answer: =========== {given answer} For the given answer to be correct it must align with the ground truth without omitting any key details **and** any additional detail in the given answer must be seen in the provided context. Determine, with reasoning, whether the given answer is correct based on the ground truth answer and context. Give your response in the following json format: {{\"reasoning\": Why is the answer correct/incorrect, \"predicted_correct\": true or false}} 22 A.8 Confidence Intervals Table 11: PubHealthBench-Full - zero-shot accuracy Wilson score 95% confidence intervals [39] for test set of 7929 questions, refusals included as incorrect responses, and bold indicates the highest score. *LLM used to generate benchmark. Blood Safety, Hepatitis, STIs and HIV Chemicals and Toxicology Climate and Health Gastro and Food Safety HCAI, Fungal, AMR, Antimicrobial Use and Sepsis Health Protection in Inclusion Health Settings Other Radiation Tuberculosis, Travel, Zoonotic, and Emerging infections VPDs and Immunisation Overall Model Name GPT-4.5 o3-Mini Gemini-2.0-Flash Llama-3.3-70B* Phi-4-14B Gemini-Pro-1.5 Mistral-3.1-24B GPT-4o-Mini Claude-Haiku-3.5 Gemma-3-27B Gemma-2-27B Phi-4-4B Gemma-3-12B Command-R-32B GPT-4o Llama-3.1-8B Olmo-2-32B Command-R-7B Olmo-2-13B Gemma-3-4B Gemma-3-1B (87.8-93.2) (84.3-90.4) (81.0-87.8) (83.0-89.4) (81.8-88.4) (77.4-84.7) (81.8-88.4) (79.3-86.3) (78.6-85.7) (80.8-87.6) (80.5-87.4) (78.1-85.3) (76.4-83.8) (75.7-83.2) (75.4-83.0) (75.2-82.8) (72.5-80.4) (72.1-80.0) (71.8-79.8) (69.2-77.4) (44.9-54.2) (89.5-92.5) (86.7-90.1) (84.1-87.8) (84.7-88.4) (80.5-84.6) (79.5-83.6) (80.6-84.6) (77.9-82.2) (78.3-82.5) (77.5-81.8) (77.1-81.4) (77.5-81.8) (76.7-81.1) (74.9-79.4) (75.1-79.6) (74.9-79.4) (72.3-76.9) (69.9-74.6) (67.3-72.2) (67.2-72.1) (46.7-52.0) (95.2-98.6) (91.7-96.4) (92.6-97.0) (89.3-94.6) (89.0-94.4) (91.1-95.9) (89.0-94.4) (88.4-94.0) (89.3-94.6) (88.4-94.0) (88.1-93.7) (86.9-92.9) (86.0-92.2) (86.0-92.2) (85.7-91.9) (85.7-91.9) (85.4-91.7) (82.5-89.4) (83.9-90.6) (81.9-88.9) (52.9-62.8) (87.1-92.7) (84.7-90.9) (82.3-89.0) (81.8-88.6) (81.8-88.6) (80.3-87.3) (81.1-87.9) (78.0-85.3) (76.2-83.8) (76.2-83.8) (79.0-86.2) (77.2-84.7) (72.2-80.3) (72.5-80.5) (75.0-82.7) (73.7-81.6) (71.2-79.4) (69.7-78.1) (69.7-78.1) (67.0-75.6) (40.4-49.9) (92.2-96.4) (89.6-94.5) (84.8-90.8) (84.6-90.6) (87.3-92.8) (83.9-90.0) (86.6-92.2) (85.3-91.2) (82.7-89.1) (80.2-87.0) (80.2-87.0) (83.6-89.8) (79.3-86.2) (80.0-86.8) (76.9-84.2) (80.5-87.2) (79.0-86.0) (72.2-80.0) (72.7-80.5) (69.6-77.7) (46.3-55.5) (92.4-95.2) (88.8-92.2) (88.8-92.2) (89.1-92.4) (86.7-90.4) (88.1-91.6) (86.6-90.3) (86.1-89.9) (85.6-89.4) (85.4-89.2) (84.8-88.7) (83.5-87.6) (82.2-86.4) (82.6-86.8) (83.8-87.8) (82.6-86.8) (81.1-85.4) (76.6-81.3) (76.3-81.0) (76.7-81.5) (48.5-54.3) (87.5-94.0) (83.1-90.8) (82.7-90.5) (81.6-89.6) (85.5-92.6) (81.6-89.6) (83.5-91.1) (82.0-89.9) (82.0-89.9) (79.7-88.1) (79.7-88.1) (82.0-89.9) (82.0-89.9) (78.9-87.5) (83.1-90.8) (79.7-88.1) (75.9-85.0) (78.1-86.8) (75.9-85.0) (76.6-85.6) (51.4-62.7) (85.9-92.9) (82.7-90.5) (84.3-91.7) (83.1-90.8) (80.0-88.4) (80.4-88.7) (80.0-88.4) (78.1-86.8) (81.6-89.6) (75.5-84.7) (75.5-84.7) (72.2-81.8) (76.3-85.3) (72.5-82.1) (74.8-84.0) (72.9-82.4) (72.2-81.8) (65.9-76.3) (68.9-78.9) (62.3-73.1) (39.0-50.4) (90.1-93.1) (85.2-88.9) (84.3-88.1) (83.3-87.1) (83.0-86.9) (82.1-86.0) (80.8-84.9) (77.6-82.0) (78.8-83.1) (77.9-82.2) (78.2-82.5) (76.3-80.7) (76.8-81.2) (76.8-81.2) (77.3-81.7) (73.8-78.4) (74.1-78.7) (67.7-72.7) (68.9-73.8) (67.3-72.3) (42.1-47.5) (91.8-94.1) (86.7-89.6) (85.8-88.8) (85.6-88.6) (83.7-86.9) (84.4-87.5) (81.7-85.1) (81.0-84.4) (79.7-83.2) (80.2-83.6) (79.4-83.0) (78.4-82.0) (77.3-81.0) (78.8-82.4) (74.7-78.6) (77.4-81.1) (75.1-78.9) (71.1-75.1) (70.7-74.7) (65.3-69.5) (39.9-44.4) (91.9-93.1) (88.2-89.5) (86.9-88.4) (86.7-88.1) (85.3-86.8) (84.8-86.3) (84.3-85.9) (82.7-84.3) (82.4-84.0) (81.9-83.5) (81.7-83.4) (80.9-82.6) (80.0-81.7) (79.8-81.6) (79.3-81.1) (79.1-80.9) (77.5-79.3) (73.7-75.6) (73.5-75.4) (71.2-73.2) (46.5-48.7) Table 12: PubHealthBench-Full zero-shot accuracy Wilson score 95% confidence intervals by guidance type. *LLM used to generate benchmark. Table 13: PubHealthBench-Reviewed Wilson score 95% confidence intervals zero-shot accuracy by question and response type. *LLM used to generate benchmark, **Headline result. Clinical Guidance Multiple Audiences Professional Guidance Public Guidance Unclassified Overall Model Name (90.1-92.8) GPT-4.5 (84.1-87.4) o3-Mini (83.0-86.4) Gemini-2.0-Flash (83.1-86.6) Llama-3.3-70B* (82.7-86.2) Phi-4-14B (80.6-84.2) Gemini-Pro-1.5 (79.5-83.2) Mistral-3.1-24B (78.7-82.5) GPT-4o-Mini (77.5-81.4) Claude-Haiku-3.5 (76.7-80.6) Gemma-3-27B (77.4-81.3) Gemma-2-27B (76.2-80.2) Phi-4-4B Gemma-3-12B (74.9-78.9) Command-R-32B (75.7-79.7) (71.5-75.8) GPT-4o (74.7-78.8) Llama-3.1-8B (72.4-76.6) Olmo-2-32B (67.8-72.2) Command-R-7B (67.3-71.8) Olmo-2-13B (62.3-66.9) Gemma-3-4B (37.8-42.5) Gemma-3-1B (91.5-95.3) (89.3-93.6) (87.1-91.8) (86.9-91.6) (85.4-90.4) (87.4-92.1) (84.4-89.5) (82.9-88.3) (82.4-87.9) (83.1-88.4) (81.4-87.0) (81.4-87.0) (80.1-85.9) (80.1-85.9) (80.3-86.0) (79.1-85.0) (78.6-84.6) (70.1-76.9) (73.8-80.2) (70.7-77.4) (41.4-49.1) (91.0-92.7) (87.6-89.7) (86.4-88.6) (86.0-88.2) (84.3-86.6) (83.6-85.9) (83.7-86.1) (81.8-84.3) (81.7-84.2) (81.0-83.5) (80.9-83.4) (80.5-83.0) (79.4-82.0) (78.2-80.8) (79.6-82.2) (78.8-81.4) (76.7-79.4) (74.2-76.9) (72.8-75.6) (71.7-74.6) (48.7-52.0) (94.7-97.1) (91.0-94.2) (91.3-94.5) (89.8-93.3) (88.5-92.2) (88.6-92.3) (88.0-91.8) (85.7-89.8) (85.2-89.3) (85.6-89.7) (85.3-89.4) (83.1-87.5) (84.8-89.1) (84.6-88.9) (83.8-88.2) (80.1-84.9) (82.5-87.0) (76.7-81.8) (78.6-83.5) (75.0-80.2) (45.5-51.8) (90.2-93.6) (86.8-90.7) (84.1-88.3) (84.9-89.1) (82.7-87.1) (83.6-88.0) (83.7-88.0) (81.5-86.1) (82.6-87.0) (81.5-86.1) (81.0-85.6) (80.0-84.7) (77.6-82.6) (80.1-84.8) (78.1-83.0) (78.6-83.4) (74.7-79.9) (72.6-78.0) (72.1-77.5) (71.9-77.3) (47.3-53.6) (91.9-93.1) (88.2-89.5) (86.9-88.4) (86.7-88.1) (85.3-86.8) (84.8-86.3) (84.3-85.9) (82.7-84.3) (82.4-84.0) (81.9-83.5) (81.7-83.4) (80.9-82.6) (80.0-81.7) (79.8-81.6) (79.3-81.1) (79.1-80.9) (77.5-79.3) (73.7-75.6) (73.5-75.4) (71.2-73.2) (46.5-48.7) Exc. Refusals Inc. Refusals** Invalid MCQA Valid MCQA Model Name GPT-4.5 GPT-4.1 o1 Gemini-2.0-Flash o3-Mini Claude-Sonnet-3.7 Llama-3.3-70B* Phi-4-14B Gemini-Pro-1.5 Mistral-3.1-24B GPT-4o-Mini Claude-Haiku-3.5 Gemma-3-27B Gemma-2-27B Phi-4-4B Llama-3.1-8B Command-R-32B GPT-4o Gemma-3-12B Olmo-2-32B Olmo-2-13B Gemma-3-4B Command-R-7B Gemma-3-1B (90.8-94.5) (90.1-93.9) (89.7-93.6) (86.1-90.6) (85.8-90.4) (90.2-94.1) (84.8-89.5) (84.3-89.1) (83.5-88.5) (82.0-87.1) (81.2-86.4) (80.5-85.8) (80.1-85.4) (80.1-85.4) (78.8-84.3) (78.1-83.7) (77.8-83.4) (89.4-93.6) (77.3-82.9) (75.1-80.9) (71.9-78.1) (70.2-76.4) (69.6-75.9) (42.4-49.5) (90.8-94.5) (90.1-93.9) (89.7-93.6) (86.0-90.5) (85.8-90.4) (85.2-89.9) (84.8-89.5) (84.3-89.1) (83.5-88.5) (82.0-87.1) (81.2-86.4) (80.5-85.8) (80.1-85.4) (80.1-85.4) (78.8-84.3) (78.1-83.7) (77.8-83.4) (77.7-83.3) (77.3-82.9) (75.1-80.9) (71.9-78.1) (70.2-76.4) (69.6-75.9) (42.4-49.5) (56.4-82.8) (64.1-88.3) (51.6-79.0) (46.8-75.0) (54.0-80.9) (44.5-73.0) (46.8-75.0) (51.6-79.0) (44.5-73.0) (46.8-75.0) (37.7-66.6) (42.2-70.9) (44.5-73.0) (39.9-68.8) (49.2-77.0) (42.2-70.9) (44.5-73.0) (37.7-66.6) (46.8-75.0) (39.9-68.8) (42.2-70.9) (39.9-68.8) (42.2-70.9) (15.3-41.1) (92.2-95.6) (90.9-94.7) (91.2-94.9) (87.6-92.0) (87.0-91.5) (87.0-91.5) (86.3-91.0) (85.4-90.2) (85.1-89.9) (83.3-88.4) (83.0-88.2) (82.0-87.3) (81.4-86.7) (81.7-87.0) (79.8-85.3) (79.5-85.1) (79.1-84.7) (79.4-84.9) (78.3-84.0) (76.4-82.3) (72.9-79.2) (71.2-77.6) (70.5-76.9) (43.4-50.7) Table 14: PubHealthBench-FreeForm model accuracy Wilson score 95% confidence intervals by guidance audience. *LLM used to generate benchmark, **Judge LLM. Model Name o1 GPT-4.1 o3-Mini GPT-4o GPT-4.5 Claude-Sonnet-3.7 Gemini-2.0-Flash Gemma-3-27B Gemini-Pro-1.5 Gemma-3-12B Claude-Haiku-3.5 GPT-4o-Mini** Llama-3.3-70B* Mistral-3.1-24B Phi-4-14B Olmo-2-32B Gemma-3-4B Command-R-32B Olmo-2-13B Gemma-2-27B Command-R-7B Llama-3.1-8B Phi-4-4B Gemma-3-1B Clinical Guidance Multiple Audiences Professional Guidance Public Guidance Unclassified Total (64.3-77.6) (57.8-71.8) (57.8-71.8) (52.6-67.0) (51.5-65.9) (49.7-64.2) (48.6-63.1) (42.4-57.0) (38.5-53.1) (43.0-57.6) (44.1-58.7) (30.3-44.5) (28.2-42.2) (29.3-43.3) (26.1-39.8) (30.3-44.5) (19.3-32.1) (23.4-36.9) (24.0-37.5) (20.8-33.9) (14.7-26.5) (11.3-22.2) (11.3-22.2) (9.4-19.6) (69.6-89.3) (58.6-81.2) (65.9-86.6) (58.6-81.2) (58.6-81.2) (53.4-76.9) (51.7-75.4) (48.3-72.4) (44.9-69.4) (40.0-64.7) (48.3-72.4) (41.7-66.3) (40.0-64.7) (27.6-51.7) (33.7-58.3) (38.4-63.2) (27.6-51.7) (29.1-53.4) (21.7-44.9) (24.6-48.3) (9.5-28.5) (13.4-34.1) (14.7-36.0) (5.9-22.5) (64.9-74.4) (66.1-75.5) (63.7-73.3) (49.2-59.5) (48.3-58.6) (49.7-60.0) (47.5-57.8) (47.8-58.1) (46.1-56.4) (42.7-53.1) (35.3-45.5) (34.8-44.9) (32.9-42.9) (31.5-41.5) (32.6-42.6) (31.8-41.7) (30.7-40.6) (24.5-33.9) (25.3-34.8) (27.2-36.8) (18.0-26.6) (12.1-19.6) (13.9-21.8) (16.9-25.4) (76.5-91.9) (72.1-89.0) (67.8-85.9) (72.1-89.0) (66.4-84.9) (65.1-83.8) (66.4-84.9) (62.3-81.7) (51.7-72.7) (63.7-82.7) (57.0-77.3) (57.0-77.3) (49.2-70.4) (53.0-73.9) (47.9-69.2) (44.1-65.7) (46.6-68.0) (41.6-63.3) (49.2-70.4) (37.9-59.6) (35.5-57.1) (28.4-49.6) (20.5-40.4) (17.3-36.3) (72.7-88.3) (58.8-77.3) (64.5-82.0) (53.3-72.5) (49.0-68.6) (46.8-66.6) (51.1-70.6) (41.6-61.5) (51.1-70.6) (44.7-64.6) (37.5-57.4) (31.4-51.0) (30.4-49.9) (30.4-49.9) (30.4-49.9) (20.8-38.9) (34.4-54.2) (32.4-52.1) (25.5-44.5) (25.5-44.5) (12.6-28.5) (11.7-27.3) (9.2-23.7) (8.4-22.5) (71.0-77.2) (67.2-73.7) (66.9-73.4) (57.4-64.3) (55.7-62.6) (55.1-62.1) (54.4-61.4) (51.2-58.2) (49.3-56.4) (48.8-55.9) (44.6-51.7) (39.8-46.8) (37.4-44.3) (36.3-43.3) (36.1-43.0) (35.4-42.3) (33.4-40.2) (30.8-37.5) (30.5-37.3) (29.8-36.4) (20.4-26.4) (16.2-21.7) (15.9-21.5) (15.6-21.1)"
        }
    ],
    "affiliations": [
        "UK Health Security Agency (UKHSA)"
    ]
}