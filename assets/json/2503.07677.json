{
    "paper_title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity",
    "authors": [
        "Kwanyoung Kim",
        "Byeongsu Sim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 7 7 6 7 0 . 3 0 5 2 : r PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity Kwanyoung Kim, Byeongsu Sim Samsung Research {k 0.kim, bs.sim}@samsung.com Figure 1. Qualitative comparison (Top): guidance sampling methods (CFG[18], PAG[1], SEG[20]) (Mid): guidance-distilled models (DMD2[61], SDXL-Lightning [31], Hyper-SDXL[42]) (Bottom): Other backbone such as Stable Diffusion 1.5 [44] and SANA [59] with our proposed method, PLADIS(Ours). PLADIS is compatible with all guidance techniques and also supports guidance-distilled models including various backbone. It provides the generation of plausible and improved text alignment without any training or extra inference."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate First and corresponding author 1 query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering highly efficient and universally applicable solution. Table 1. Comparison of PLADIS with other sampling methods reveals key advantages of ours, with (cid:139) and denoting positive and negative connotations for each category. Method CFG [18] SAG [21] AG [24] PAG [1] SEG [20] PLADIS (Ours) Need extra Training Need heuristic Search Need extra Inference Supports guidanceDistilled Model (cid:139) (cid:139) (cid:139) (cid:139) (cid:139) (cid:139) (cid:139) (cid:139) (cid:139) 1. Introduction Diffusion models have demonstrated remarkable advancements in generating high-quality images and videos [3, 5, 6, 13, 44, 45, 59]. However, when using naıve sampling methods, the quality of the generated samples can be suboptimal. Classifier-Free Guidance (CFG) [18] is prominent technique that increases the likelihood of sample belonging to specific class by calculating the difference between the score functions of conditional and unconditional models, and applying weighted adjustment. While CFG is effective, it needs additional training and inference, and can degrade sample quality when the guidance scale is too high. Inspired by CFG, various guidance sampling methods have been explored [1, 7, 20, 21, 24, 29, 46]. Recent research has focused on creating weak models by intentionally weakening model to guide the stronger, original model. Although these methods generally improve performance, they also come with clear limitations. For example, AutoGuidance (AG) [24] relies on poorly trained version of the unconditional model, which can be challenging and unstable to train. Alternative attention-based guided sampling methods, independent of the training process, have also been explored. For instance, Perturbed Attention Guidance (PAG) [1] disrupts self-attention maps by converting them into identity matrices, while Smooth Energy Guidance (SEG) [20] introduces blurring into attention weights. These methods are heuristic, as they are applied to specific layers, introducing additional hyperparameters that need to be determined through grid search. Furthermore, all existing guidance sampling methods require additional neural function evaluations (NFEs) and are not applicable to guidance-distilled models [31, 35, 42, 48, 56, 61, 62] due to the need to calculate the difference between conditional and unconditional models or weak models. These limitations present challenging and interesting problem: Can we develop universal boosting method that does not require additional training or NFE, can be combined with other guidance sampling methods, and can be applied to guidance-distilled models? In this work, we aim to tackle this challenging problem by adopting attention-based methods in completely different route. One of the most important contributions of this paper is the discovery of the importance of classical result from sparse attention via α-Entmax [39] which includes softmax and sparsemax [36] as particular cases, and is sparse for any α > 1 and produce sparse alignment to assign nonzero probability. Although widely investigated in natural language processing (NLP) [8, 36, 39, 52], sparse attention has not yet been extensively utilized within the realm of computer vision, particularly in diffusion models. Specifically, our findings demonstrate that substituting cross-attentions with sparse counterparts during inference significantly improves overall generation performance. Rather than weakening models via self-attention, which requires additional inference time, modifying the crossattention mechanism circumvents the need for extra inference. This ensures compatibility with other guidance sampling methods and guidance-distilled models. Interestingly, this result can be interpreted through the lens of modern Hopfield Networks [41] and sparse Hopfield Networks (SHN) [23, 57]. In these works, the attention layer mirrors the update rule of Hopfield network to retrieve stored patterns. Moreover, there is noise robustness advantage when we use sparse counterparts, which supports the rationale behind our approach in diffusion models. Building on these findings and insights, we propose novel and straightforward method, referred to as PLADIS, which assigns weights to the differences between sparse and dense attention to emphasize sparsity. As highlighted in Tab. 1, our approach effectively addresses the aforementioned challenges, leading to improved performance and enhanced text-image alignment, as demonstrated by extensive experiments. Our key contributions are as follows: We propose simple but effective method, named PLADIS, which substitutes cross-attention in diffusion models with adjusted attention mechanisms that extrapolating between sparse and dense cross-attentions. We provide thorough theoretical analysis based on our understanding of SHN, and propose the error bound and noise robustness of sparse attention for intermediate sparsity case. To the best of our knowledge, this is the first paper to apply and improve diffusion models from the perspective of SHN. Our method can be combined with other guidance methods and even guidance-distilled models, does not require extra training or NFEs. We have demonstrated these advantages on various benchmark datasets, showing significant improvements in sample image quality, text-image alignment, and human preference evaluation. 2. Preliminary 2.1. Diffusion Models Diffusion models (DM) [19, 50] are class of generative models designed to learn the reverse of forward noise process by leveraging the score function of the data 2 distribution. Specifically, given data distribution x0 q(x0) := qdata(x), the forward process iteratively adds noise to the data according to Markov chain q(xtxt1) 1 βtxt1, βtI) for = 1, . . . , with pre-defined ( schedule {βt}t=1,...,T . Consequently, the distribution of latent variable is q(xt) = ( αtx0, (1 αt)I) and the distribution of last one approximates to an isotropic Gaussian distribution q(xT ) (0, I), where αt = 1 βt, αt = (cid:81)t αi. The reverse process is modeled as pθ(xt1xt) = (µθ(xt, t), Σθ(xt, t)). This model can be trained with variational bound on log likelihood [19] or trained with score function in continuous time formulation [50]. Both training objectives are reformulated with denoising score matching (DSM) [54]: min θ xt= αtx0+ 1 αtϵ,ϵN (0,I) [ϵθ(xt, t) ϵ2 2]. (1) Sampling process is conducted as the learned reverse process starting from the isotropic Gaussian distribution. For instance, given xT (0, I), DDIM [49] samples x0 are computed as follow: xt1 = αt1 ˆx0(t) + (cid:112)1 αt1ϵθ(xt, t), (2) where ˆx0(t) := E[x0xt] = (xt αt is the denoised estimate by Tweedies formula [11, 25]. This process is repeated from to 1. 1 αtϵθ(xt, t))/ 2.2. Guidance Sampling in Diffusion Models In order to generate samples following condition given by users, diffusion models are extended to conditional generative models [18, 43] with additional inputs in the models: min θ Ext,ϵ,c [ϵθ(xt, t, c) ϵ2 2], where xt, ϵ are sampled same as Eq. 1 and denotes specific condition that has, in most cases the embedding of class or text. However, since vanilla sampling often results in suboptimal performance for conditional generation, various guidance sampling methods have been extensively explored to enhance sample quality [1, 7, 10, 18, 20, 21, 24, 46]. For clarity, let us shorten the notation as ϵθ(xt, c) := ϵθ(xt, t, c) and denote the unconditional model as ϵθ(xt, ), where represents the null condition. Classifier-Free Guidance (CFG) adjusts the classconditioned probability relative to the unconditional one, becoming ˆp(xtc) = p(xtc) adjusted sampling process: (cid:16) p(xtc) p(xt) , resulting in an (cid:17)w conditional output as follow: ϵ θ (xt, c) = ϵθ(xt, c) + s(ϵθ(xt, c) ϵθ(xt, c)) (5) where is the guidance weight, and ϵ represents model that is intentionally weakened or perturbed, achieved through various heuristic methods. For instance, AG [24] uses flawed model variant, PAG [1] replaces self-attention weights with an identity matrix, SEG [20] blurs attention weights, Time Step Gudiance (TSG) [46] perturbs timestep embeddings, and SelfGuidance [29] alters noise levels. While effective, these approaches lack clear theoretical foundation and have limitations: 1) they require specific layer identification, 2) increase computational cost with added NFEs, and 3) are incompatible with step-distilled models. Our method overcomes all of these limitations. 2.3. Energy-Based Interpretations of Attention Attention mechanisms, following their distinct success, have recently been applied across various fields, including diffusion models [4, 15, 26, 33, 38, 51]. An energy-based model perspective has revealed their connection to Hopfield energy functions [23, 41, 57]. In Hopfield networks, the goal is to associate an input query with the most relevant pattern ξ by minimizing the energy function E(x) through retrieval dynamics . In modern Hopfield networks [41], energy functions and dynamics has been proposed, which is equivalent to attention mechanisms: E(x)Dense := lse(β, Ξx) + 1 2 TDense(x) := ΞSoftmax(βΞx) x, x, (6) (7) (cid:16)(cid:80)M where Rd, Ξ = [ξ1 , ξM ] RdM , and lse(β, z) := log /β denotes log-sumi=1 exp(βzi) exponential function for any given vector RM and β > 0. It mirrors the attention mechanism in transformers and providing theoretical basis for its success. (cid:17) Since sparse attention was introduced for its efficiency [8, 36, 39, 52], the Sparse Hopfield network (SHN) [23, 57] was also proposed, extending the previous connection. The energy function was modified to make sparse the computation of retrieval dynamics: Eα(x) := Ψ α(β, Ξx) + Tα(x) := Ξα-Entmax(βΞx), x, x, 1 2 (8) (9) αt1 ˆx0(t) + (cid:112)1 αt1ϵ xt1 = θ(xt, c) = ϵθ(xt, c) + w(ϵθ(xt, c) ϵθ(xt, )), ϵ θ(xt, t), (3) (4) α is the convex conjugate of Tsallis entropy [53], and Ψ Ψα, α-Entmax(z), represents the probability mapping: where is the guidance scale. Recently, weak model guidance has been introduced, which weakens the conditional model and computes the difference with the normal Ψα(p) := 3 (cid:40) 1 (cid:80)M i=1(pi pα i=1(pi log pi), α(α1) (cid:80)M ), α = 1, α = 1, (10) Figure 2. Conceptual comparison between other guidance methods [1, 18, 20] and PLADIS: Existing guidance methods require extra inference steps due to undesired paths, such as null conditions or perturbing self-attention with an identity matrix or blurred attention weights. In contrast, PLADIS avoids additional inference paths by computing both sparse and dense attentions within all cross-attention modules using scaling factor, λ. Moreover, PLADIS can be easily integrated with existing guidance approaches by simply replacing the cross-attention module. α-Entmax(z) = arg max [p, Ψα(p)], (11) pM where RM . Here, α controls the sparsity. When α = 1, it is equivalent to dense probability mapping, 1-Entmax = Softmax, and as α increases towards 2, the outputs of α-Entmax become increasingly sparse. Similar to TDense, Tα can be extended to attention mechanisms, establishing strong connection with sparse attention. For α = 2, the exact solution can be efficiently computed using sorting algorithm [14, 37]. For 1 < α < 2, inaccurate and slow iterative algorithm was used for computing α-Entmax [34]. Interestingly, for 1.5-Entmax, an exact solution are derived in simple form [39]. In SHN, sparsity reduces retrieval errors and provide faster convergeness compared to dense retrieval dynamics [23, 57]. As mentioned, the retrieval dynamics of modern and sparse Hopfield energy can be converted into an attention mechanism as follows: At(Qt, Kt, Vt) = Softmax(QtK / Atα(α, Qt, Kt, Vt) = α-Entmax(QtK / d)Vt (12) d)Vt (13) where Atl denotes original (dense) attention layer, and Atα represents sparse attention module with α-Entmax operator at lth layer. Both attention layers can be applied to self and cross-attention layers. Qt, Kt, and Vt represent the query, key, and value matrices at time step t, respectively, and is the dimensionality of the keys and queries. Note d, weight matrices, and operators, TDense that with β = 1/ in Eq. (7) and Tα in Eq. (9) are reduce to the transformer attention mechanism Eq. (12) and Eq. (13), respectively. More details are available in supplement B. Noise robustness of sparse Hopfield network While the sparse extension is an efficient counterpart of dense Hopfield network, it has been discovered that there is more advantages to use sparse one besides efficiency [23, 57]. Theorem 1. (Noise-Robustness) [23]. In case of noisy patterns with noise η, i.e. = + η (noise in query) or ξµ = ξµ + η (noise in memory), the impact of noise η on the sparse retrieval error T2(x) ξµ is linear, while its effect on the dense retrieval error TDense(x) ξµ is exponential. where ξµ is memory pattern and to be considered stored at fixed point of . This theorem suggests that under noisy conditions, sparse attention mechanisms exhibit superior noise robustness compared to standard dense attention, leads the lower retrieval error. 3. Main Contribution : PLADIS Motivated by advantages of sparse attention presented in previous section, we aimed to enhance text to image (T2I) diffusion model by sparsifying attention modules as described in Eq. (13). In the following subsection, we investigate sparse attention in selfand cross-attention for T2I diffusion models (Sec. 3.1), explore the effect of sparsity in α-Entmax for α > 1 (Sec. 3.2) and connect SHNs noise robustness with sparse attention for 1 < α 2 in T2I models (Sec. 3.3). Finally, we introduce PLADIS, cost-effective enhancement method for T2I diffusion models (Sec. 3.4). Figure 3. Qualitative comparison between baseline and variants that substitute self-attention and cross-attention mechanisms with sparse attention methods. 3.1. Sparse Attention for T2I Generation To study the efficacy of the sparse attention mechanism in T2I diffusion models, we initially replace the standard self-attention and cross-attention modules with their respective sparse counterparts using α-Entmax as depicted in Fig. 3. When self-attention modules are replaced, the model is severely damaged, generating no meaningful outputs. Sparse attention ignore immaterial correlations while maintaining stricter ones; for self-attention, which utilizes relation between noisy image patches, such strict association yield unsatisfactory outcomes. Surprisingly, substituting the cross-attention module with its sparse counterpart leads to enhanced generation quality and better text alignment, although the model was not trained with the sparse attention modules. As shown in Fig. 3, the baseline results are unable to accurately generate the text Boost. In contrast, the sparse variants achieve successful and accurate text generation. Further evidence of these improvements can be found in Fig 4. This intriguing discovery regarding the use of sparse cross-attention within T2I diffusion models serves as the primary impetus behind our proposed algorithm. 3.2. Effect of Sparsity in Cross-Attention Module In this section, we explore the effect of sparsity in the sparse attention mechanism within the cross-attention module of T2I diffusion models. Sparsity is controlled by α, with α = 1 refer to softmax and α = 2 to sparsemax, as described in Sec. 2.3. Notably, α-Entmax transforms are sparse for all α > 1. To assess sparsitys impact, we replace standard cross-attention layers with sparse ones and generate 5K samples from the MS-COCO validation dataset using CFG guidance, varying α as shown in Fig.4. Interestingly, increasing sparsity (higher α) improves generation quality, text alignment, and human preference scores without additional training. Cross-attention with softmax results in dense alignments and strictly positive output probabilities, but sparse cross-attention produces sparse alignments, ensuring stricter match between image and text embeddings. It leads to overall improvement in performance. Figure 4. Comparison of α values in α-Entmax on the MSCOCO dataset with CFG and PAG guidance. 3.3. Connection With Noise Robustness of SHN To further verify why performance improves when 1 < α 2, we introduce retrieval error of dynamics for this case: Theorem 2 (Retrieval Error). Let Tα be the retrieval dynamics of Hopfield model with α-Entmax. For 1 < α 2,Tα(x) ξµ + mκ (cid:104) (α 1)β (cid:16) ξν, [Ξx](κ+1) max ν (cid:17) (cid:105) 1 α1 , (14) Here, we abuse the notation [Ξx](d+1) 1α/(α 1). := [Ξx](d) For proof, see supplement B. Based on our proposed error bound, we can derive the noise-robustness for 1 < α 2. Corollary 2.1. (Noise-Robustness) In case of noisy patterns with noise η, the impact of noise on the retrieval error 1 α1 for 1 < α 2. Tα(x) ξµ is polynomial of order This theorem and corollary suggest that Tα also take pleasure in noise robustness for 1 < α 2, leads the lower retrieval error. In T2I diffusion models, cross-attention layers process query, key, and value matrices from noisy images and text prompts. Due to Gaussian noise corruption in the diffusion process, the query matrix is inherently perturbed. Building on this and Theorem 1, 2, and Corollary 2.1, the observed performance improvement, especially with increasing α, reflects the noise robustness of sparse attention, as shown in Fig. 4. By linking these gains to the theoretical guarantees of SHN, we provide stronger foundation for the efficacy of sparse-cross attention in DMs. 3.4. Our Approach : PLADIS Building on our exploration of sparse attention, we propose simple yet more effective approach called PLADIS. Specifically, we aim to enhance the benefits of sparse attention (as shown in Fig. 4) without introducing additional neural function evaluations (NFEs). Inspired by guidance methods like CFG, PAG, and SEG, we extrapolate querykey correlations in both dense and sparse attentions. AtOurs(α, λ, Qt, Kt, Vt) := At(α, Qt, Kt, Vt) + λ(cid:0)Atα(α, Qt, Kt, Vt) At(Qt, Kt, Vt)(cid:1) (15) Algorithm 1: Diffusion Sampling with PLADIS and other guidance methods Input: Diffusion model ϵθ(xt) with cross-attention module At() at layer l, total number of cross-attention layers L, scales λ. 1 for in 1, , , do 2 Replace At() with AtOurs() by Eq. 15 3 xT (0, I) 4 for in T, 1, , 1 do 5 if CFG then 6 8 9 10 Compute ϵθ(xt, c) by Eq. 4 if PAG or SEG then Compute ϵθ(xt, c) by Eq. 1 αtϵθ(xt, c))/ αt αt1 ˆx0(t) + 1 αt1ϵθ(xt, c) ˆx0(t) = (xt xt1 = return: x0 The scale parameters λ is hyperparameter and determine the extent to which sparse attention effects are accentuated. When λ = 0, the formula is equivalent to the baseline model, and when λ = 1, it represents the model in Sec. 3.2. When λ > 1, our PLADIS is applied. The sparsity degree 1 < α 2 is another hyperparameter, but we only consider two options α = 1.5 and α = 2 , where efficient algorithms are known to exist. Here, we emphasize the generalizability of our method. Other methods that modify the attention module require layers. However, for hyperparameter search for target PLADIS, applying Eq. (15) to all cross-attention layers is sufficient, which makes our method more easily extendable to other cases. Nevertheless, we conduct an ablation study in Tab. 8 for varying target layers and find that applying it to all layers is the optimal choice. (See supplement G) Moreover, unlike other guidance formulations, our method is implicit in that it does not require an additional model, enabling our method to be extended to guidance-distilled models. 4. Experiment Implementation Detail In our experiments, we use Stable Diffusion XL (SDXL) [40] as the backbone model to validate the effectiveness of our proposed methods. The results on other backbone is available in supplement E. All experiments are conducted on single NVIDIA H100 GPU. For the calculation of the α-Entmax function, we utilize an open-source library. We set α to 1.5 and the scale λ to 2.0 as the baseline. Evaluation Metric To comprehensively assess our method, we employ various evaluation metrics. For visual fidelity, https://github.com/deep-spin/entmax 6 Table 2. Quantitative results of various guidance methods on the MS-COCO dataset. Bold text indicates the best performance for each metric across the different methods. CFG Method Vanilla + Ours FID 83.68 CLIPScore ImageReward 20. -1.050 79.72 (-3.96) 21.86 (+0.89) -0.858 (+0.19) PAG [1] 29.36 24.03 -0.011 + Ours 24.51 (-4.85) 24.85 (+0.93) 0.251 (+0.31) SEG [20] 38.08 23.71 -0. + Ours Vanilla + Ours 33.19 (-4.89) 24.63 (+1.02) 0.134 (+0.28) 23.39 25.91 0.425 19.01 (-4.38) 26.61 (+0.70) 0.622 (+0.20) PAG [1] 24.32 25.42 0.478 + Ours 20.11 (-4.21) 26.41 (+0.99) 0.726 (+0.25) SEG [20] 26.80 25.39 0. + Ours 22.08 (-4.80) 26.49 (+1.10) 0.689 (+0.26) Table 3. Quantitative comparison of text alignment and human preference across datasets using various guidance methods. For PAG, SEG, CFG guidance is used jointly. Bold text indicates the best performance for each metric. Dataset Method CLIPScore PickScore ImageReward HPSv2 CFG [18] 26.63 21.72 0.198 26.83 + Ours 27.72 (+1.09) 21.94 (+0.22) 0.419 (+0.22) 27.10 (+0.24) PAG [1] 26.19 21. 0.295 28.65 Drawbench [47] + Ours 27.23 (+1.05) 22.16 (+0.22) 0.570 (+0.27) 28.93 (+0.28) SEG [20] 26.06 21.79 0. 28.71 + Ours 27.41 (+1.34) 21.99 (+0.20) 0.497 (+0.21) 29.08 (+0.37) CFG [18] 29.00 21.98 0.567 28.53 + Ours 29.78 (+0.78) 22.11 (+0.13) 0.693 (+0.13) 28.54 (+0.01) PAG [1] 28. 22.13 0.637 30.64 HPD [58] + Ours 28.93 (+0.92) 22.35 (+0.22) 0.828 (+0.19) 31.12 (+0.48) SEG [20] 28.21 21. 0.673 30.48 + Ours 29.21 (+1.00) 22.15 (+0.17) 0.786 (+0.11) 30.75 (+0.27) CFG [18] 27.08 21.30 0.340 28. + Ours 27.97 (+0.89) 21.69 (+0.09) 0.466 (+0.13) 28.14 (+0.09) PAG [1] 26.34 21.49 0.467 29.91 Pick-a-pic [27] + Ours 27.31 (+0.97) 21.67 (+0.18) 0.668 (+0.20) 30.38 (+0.47) SEG [20] 26. 21.36 0.461 29.38 + Ours 27.50 (+1.02) 21.48 (+0.12) 0.613 (+0.15) 30.15 (+0.77) we calculate the Frechet Inception Distance (FID) [17] of images generated from 30K random prompts from the MSCOCO validation set [32]. To evaluate text-image alignment and user preference, we measure CLIPScore [16], ImageReward [60], PickScore [27], and Human Preference Score (HPS v2.1)[58]. Additionally, our model is evaluated using text prompts from not only MS-COCO but also Drawbench [47], HPD [58], and Pick-a-Pic [27]. More details are provided in the supplement C. 5. Results Results with Guidance Sampling To rigorously evaluate the effectiveness of our method, we generate 30K samTable 4. Quantitative comparison across various datasets using 4-steps sampling with the guidance-distilled model. Drawbench [47] HPD [58] Pick-a-pic [27] Method CLIPScore PickScore ImageReward CLIPScore PickScore ImageReward CLIPScore PickScore ImageReward Turbo [48] 27.81 22.11 0.555 29.06 22.39 0. 27.41 21.75 0.625 + Ours 28.55 (+0.73) 22.18 (+0.07) 0.601 (+0.05) 29.56 (+0.50) 22.44 (+0.05) 0.754 (+0.02) 27.92 (+0.52) 21.77 (+0.02) 0.657 (+0.03) Light [31] 26.86 22.30 0.625 28. 22.70 0.931 27.19 22.03 0.827 + Ours 27.70 (+0.84) 22.39 (+0.09) 0.738 (+0.11) 29.41 (+0.64) 22.76 (+0.06) 1.011 (+0.08) 27.91 (+0.72) 22.09 (+0.06) 0.891 (+0.07) DMD2 [61] 28.08 22. 0.829 29.78 22.55 1.002 28.14 21. 0.983 + Ours 28.38 (+0.30) 22.41 (+0.02) 0.919 (+0.09) 29.94 (+0.16) 22.60 (+0.05) 1.043 (+0.04) 28.53 (+0.39) 21.91 (+0.03) 0.993 (+0.01) Hyper [42] 27.51 22.53 0.768 29.27 22.86 1. 27.63 22.15 1.023 + Ours 28.22 (+0.71) 22.60 (+0.07) 0.867 (+0.10) 29.80 (+0.53) 22.96 (+0.10) 1.184 (+0.06) 28.27 (+0.64) 22.23 (+0.08) 1.111 (+0.09) Figure 5. Qualitative comparison by varying the scale λ. As the scale λ increases, images represent improved plausibility and enhanced text alignment. But too high value leads to smoother textures and potential artifacts, similar to those seen in CFG. When λ is greater than 0, our PLADIS method is applied. In our configuration, λ is set to 2.0. ples both with and without CFG, applying various guidance sampling techniques, including PAG [1] and SEG [20]. In this setup, we use 25 sampling steps, and detail setting are available in supplement C. As shown in Tab. 2, the use of PLADIS without any additional guidance sampling noticeably enhances visual quality, text alignment, and user preference. Furthermore, our method integrates seamlessly with different guidance approaches, offering straightforward yet impactful improvements when CFG and weak model guidance are used together. To further substantiate these findings, we conducted experiments on human preference dataset, as illustrated in Tab. 3. Our analysis reveals that ours consistently delivers substantial performance gains across all metrics and guidance techniques. Furthermore, the synergy between our method and existing guidance methods results in more visually appealing outputs and improved text-image coherence, as shown in Fig. 1 and 5. Further comparisons are provided in supplement H. Unleashing restrained concepts In Fig. 5, the baseline model does not produce the concepts correctly. It initially appears that the concept (spatial relation) is difficult for the model to learn and that superior model is required to generate such concepts. However, the model already possesses knowledge of the relation; it merely fails to fully utilize its learned information. All we need is modifying inference steps to enable utilization, effectively surfacing the models pre-existing knowledge and allowing it to fully realize and express previously latent concepts. Results on Guidance-Distilled Model To validate the effectiveness of our method on the guidance-distilled model, we conduct experiments using various baselines with 4-steps sampling across different datasets, as shown in Tab. 4. For the baselines, we employ several stateof-the-art methods, including SDXL-Turbo [48], SDXLLighting (Light) [31], Distribution Matching Distillation 2 (DMD2) [61], and Hyper-SDXL [42]. Notably, our method significantly enhances overall performance, particularly in terms of text alignment and human preference, across all baselines. The introduction of PLADIS improves the visual quality of samples compared to those produced by the baselines, as shown in Fig. 1. Furthermore, we observe that PLADIS also improves performance in one-step sampling. Due to space limitations, further examples and details are provided in the supplement and H. User Preference Study Beyond the automated metrics, we aim to assess the practical effectiveness of PLADIS in terms Figure 6. User Preference Study for PLADIS. Table 5. Ablation study on the α scale for α-Entmax with 25 steps. Inference time is measured per prompt. α 1 1.25 1.5 1. 2 Ours(α = 1.5) Ours(α = 2) FID CLIPScore ImageReward Inference Time (sec) Memory (G) 33.76 25.41 0.478 2.521 16. 32.13 25.76 0.617 9.172 16.56 31.53 25.87 0.647 3.085 16.45 31.11 25.91 0.653 9.097 16. 30.87 25.95 0.648 2.785 16.45 27.87 (-5.89) 26.41 (+1.00) 0.726 (+0.25) 3.087 (+0.56) 16.45 (+0.01) 26.88 (-6.88) 26.56 (+1.15) 0.649 (+0.001) 2.788 (+0.28) 16.45 (+0.01) Figure 7. Ablation study on the scale, λ, for PLADIS. of sample quality and prompt alignment. To evaluate human preference in these aspects, we have evaluators assess pairwise outputs from the model with and without PLADIS, associated with two questions. Fig. 6 presents the user study results. Notably, all guidance methods and distilled models with ours outperform those without ours in both image quality and prompt alignment. Especially, the models with ours significantly improve prompt coherence. Further details of the user preference study are available in supplement D. 6. Ablation Study and Analysis The Effect of α We investigate the impact of α by adjusting its value in α-Entmax, as shown in Fig. 4 and Tab. 5. We generate 5K samples using CFG and PAG guidance on MS-COCO dataset. When α = 1, this corresponds to baseline sampling with the Softmax operation. For α > 1, the cross-attention mechanism is replaced with the corresponding operation in α-Entmax. Notably, introducing sparsity into cross-attention consistently enhances performance across all instances for α > 1, supporting our theoretical findings on noise robustness of sparse attention in diffusion. In PLADIS, α values such as 1.5 and 2 are considered candidates. Our approach (α = 2) provides the best performance in terms of FID and CLIPScore but obtains inferior results for ImageReward. An α value of 1.5 offers balanced improvements across all metrics, making it our default setting. Computation Cost To evaluate the efficiency of PLADIS, we compare inference time and memory usage in VLAM by varying α, as shown in Tab. 5. Unlike other guidance techniques, our PLADIS does not need extra inference at each time step, though it does involve calculating α-Entmax systematically. We observe that our method delivers the 8 best performance while sacrificing minor processing time per prompt (0.56 seconds) and memory consumption (0.01 GB) compared to the baseline. Notably, our default setting (α=1.5) is approximately 3 faster than other α values, except for α = 2, and shows negligible differences compared to α = 1.5 without PLADIS. The Scale λ The scale λ controls how much sparse attention with α-Entmax deviates from dense attention. higher scale increases the influence of sparse attention relative to dense attention during denoising. In our empirical study, we sample 5K images with scales from 1.0 to 3.0, evaluating results using FID, CLIPScore, and PickScore  (Fig. 7)  . Ours achieves peak performance at scale of 2.0 for FID and CLIPScore, and at λ = 1.5 for PickScore. Additionally, increasing the value of (λ), the visual quality and text alignment are improved, as demonstrated in Figure 5. Based on these findings, we set the default configuration to (λ = 2.0). β and temperature Besides the hyperparameters α and d), which corresponds to λ, we can alter β (default = 1/ α-Entmax with different temperatures (often referred to as inverse temperatures) [23]. We find that our method is extendable to different β (temperature). See supplement G.1. 7. Discussion and Limitation While experiments have been conducted under variety of conditions and backbone models, our approach has not yet been applied to complex backbone architectures with transformer structures, such as the Multimodal Diffusion Transformer (MMDiT) [12], employed in Stable Diffusion 3 [12] and the Flux model. Additionally, our experiments are focused exclusively on text-to-image generation tasks. However, the proposed PLADIS has the potential to be extended to other types of tasks, such as text-to-video or even multimodal and language generation. We will focus on applying PLADIS to these structures and these tasks in future work. 8. Conclusion In this study, we introduce PLADIS, novel approach to diffusion sampling that integrates the weight of sparse cross-attention, deviating from the dense cross-attention mechanism. Furthermore, by introducing retrieval error bound in the case of 1 < α 2, we establish connection between the noise robustness of sparse cross-attention in DMs. We provide in-depth analyses of sparsity in the cross-attention module for T2I generation. Building upon these analyses, we achieve significant improvements during inference time in generation across various guidance strategies and guidance-distilled models with our PLADIS. We believe PLADIS paves the way for future research in multimodal generation and alignment, with potential applications in domains requiring precise multimodal alignment via cross-attention."
        },
        {
            "title": "References",
            "content": "[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. In European Conference on Computer Vision, pages 117. Springer, 2025. 1, 2, 3, 4, 6, 7, 16, 24 [2] Adriano Barra, Matteo Beccaria, and Alberto Fachechi. new mechanical approach to handle generalized hopfield neural networks. Neural Networks, 106:205222, 2018. 12 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110, 2023. 3 [5] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 2 [6] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. 2 [7] Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. Cfg++: Manifold-constrained classifier free guidance for diffusion models. arXiv preprint arXiv:2406.08070, 2024. 2, 3 [8] Goncalo Correia, Vlad Niculae, and Andre FT MararXiv preprint Adaptively sparse transformers. tins. arXiv:1909.00015, 2019. 2, 3 [9] Mete Demircigil, Judith Heusel, Matthias Lowe, Sven Upgang, and Franck Vermet. On model of associative memory with huge storage capacity. Journal of Statistical Physics, 168:288299, 2017. 12 [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 3 [11] Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496):1602 1614, 2011. 3 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 8 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik 9 Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 [14] Michael Held, Philip Wolfe, and Harlan Crowder. Validation of subgradient optimization. Mathematical programming, 6:6288, 1974. 4, 13 [15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 3 [16] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 6 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1, 2, 3, 4, 6, 16, 23 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [20] Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. arXiv preprint arXiv:2408.00760, 2024. 1, 2, 3, 4, 6, 7, 16, 25 [21] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7462 7471, 2023. 2, 3 [22] John Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):25542558, 1982. 12 [23] Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopfield model. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 4, 8, 13, 14, 18 [24] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. 2, [25] Kwanyoung Kim and Jong Chul Ye. Noise2Score: Tweedies approach to self-supervised image denoising without clean images. Advances in Neural Information Processing Systems, 34:864874, 2021. 3 [26] Kwanyoung Kim, Yujin Oh, and Jong Chul Ye. OTSeg: Multi-Prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation. In European Conference on Computer Vision, pages 200217. Springer, 2024. 3 [27] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. 6, 7, 16, 17 [28] Dmitry Krotov and John Hopfield. Dense associative memory for pattern recognition. Advances in neural information processing systems, 29, 2016. 12 [29] Tiancheng Li, Weijian Luo, Zhiyang Chen, Liyuan Ma, and Guo-Jun Qi. Self-guidance: Boosting flow and diffusion generation on their own. arXiv preprint arXiv:2412.05827, 2024. 2, [30] Junyang Lin, Xu Sun, Xuancheng Ren, Muyu Li, and Qi Su. Learning when to concentrate or divert attention: Selfadaptive attention temperature for neural machine translation. arXiv preprint arXiv:1808.07374, 2018. 18 [31] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxllightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. 1, 2, 7, 16, 17, 20 [32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6 [33] Bingyan Liu, Chengyu Wang, Tingfeng Cao, Kui Jia, and Jun Huang. Towards understanding cross and self-attention in stable diffusion for text-guided image editing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 78177826, 2024. 3 [34] Jun Liu and Jieping Ye. Efficient euclidean projections in linear time. In Proceedings of the 26th annual international conference on machine learning, pages 657664, 2009. 4, [35] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2 [36] Andre Martins and Ramon Astudillo. From softmax to sparsemax: sparse model of attention and multi-label classification. In International conference on machine learning, pages 16141623. PMLR, 2016. 2, 3, 13 [37] Christian Michelot. finite algorithm for finding the projection of point onto the canonical simplex of n. Journal of Optimization Theory and Applications, 50:195200, 1986. 4, 13 [38] Geon Yeong Park, Jeongsol Kim, Beomsu Kim, Sang Wan Lee, and Jong Chul Ye. Energy-based cross attention for bayesian context update in text-to-image diffusion models. Advances in Neural Information Processing Systems, 36: 7638276408, 2023. 3 [39] Ben Peters, Vlad Niculae, and Andre FT Martins. arXiv preprint Sparse sequence-to-sequence models. arXiv:1905.05702, 2019. 2, 3, 4, 13 [40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 6 [41] Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020. 2, 3, 12, 14 [42] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. 1, 2, 7, 16, 17, [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2 [45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2 [46] Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. arXiv preprint arXiv:2407.02687, 2024. 2, 3 [47] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 6, 7, 16, 17 [48] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2025. 2, 7, 16, 17, 20 [49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3 [50] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. 2, 3 [51] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture. What the daam: Interpreting stable diffusion using cross attention. arXiv preprint arXiv:2210.04885, 2022. 3 [52] Maxat Tezekbayev, Vassilina Nikoulina, Matthias Galle, and Zhenisbek Assylbekov. Speeding up entmax. arXiv preprint arXiv:2111.06832, 2021. 2, 3 [53] Constantino Tsallis. Possible generalization of boltzmanngibbs statistics. Journal of statistical physics, 52:479487, 1988. 3, 13 [54] Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):1661 1674, 2011. 10 [55] Martin Wainwright, Michael Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(12):1305, 2008. 13 [56] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu arXiv preprint Liu, et al. arXiv:2405.18407, 2024. 2 Phased consistency model. [57] Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. Stanhop: Sparse tandem hopfield model for arXiv preprint memory-enhanced time series prediction. arXiv:2312.17346, 2023. 2, 3, 4, 13, 14 [58] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 6, 7, 16, [59] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synarXiv preprint thesis with linear diffusion transformers. arXiv:2410.10629, 2024. 1, 2, 16, 17, 22 [60] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36, 2024. 6 [61] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. 1, 2, 7, 16, 17, 20 [62] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024. 2 11 PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity Supplementary Material A. Supplementary Section In this supplementary document, we present the following: Theoretical background on Hopfield energy networks and sparse Hopfield energy networks, the proof of the noise robustness in the intermediate cases, and the error bound of PLADIS in Section B. Detailed description of the evaluation metrics and implementation in Section C. Further detail and results of the user preference study in Section D. Results for other backbone models including Stable Diffusion 1.5 and SANA in Section E. Results from one-step sampling with guidance-distilled model in Section F. Additional ablation studies, including attention temperature, cross-attention maps, the effect of layer selection in Section G. Additional qualitative results, including interactions with existing guidance sampling approaches, the guidance-distilled model, and further ablation studies in Section H. B. Theoretical Background Notations. For R, a+ := max{0, a}. For z, Rd, z, = zz is the inner product of two vectors. For = (z1, . . . , zd) Rd, we denote the sorted coordinates of as z(1) z(2) z(d), that is, z(ν) is the νth largest element among zis. := {p RM pi 0, (cid:80) pi = 1}, (M 1)-dimensional simplex. In this section, we provide the concept of modern Hopfield network and its sparse extension in simple form, to make readers fully understand the motivation and intuition of our method and encourage further research upon our works. Initially, Hopfiled model was introduced as an associative memory that can store binary patterns[22]. The model is optimized to store patterns in the local minima of associated energy function. Then, given query input, the closest local minimum point of the energy function is retrieved. There were many extensions of the classic model to improve stability and capacity of the model, such as exponential energy functions or continuous state models[2, 9, 28]. Ramsauer et al. proposed modern Hopfield network that can be integrated into deep learning layers [41]. The network is equipped with new energy function and retrieval dynamics that are differentiable and retrieve patterns after one update: EDense : Rd R, (cid:55) lse(β, Ξx) + 1 2 TDense : Rd Rd, (cid:55) ΞSoftmax(βΞx) x, x, (16) (17) where Rd represents query input, Ξ = [ξ1 . . . ξM ] RdM , ξi Rd denotes pattern stored, lse(β, z) := log := (exp(z1), . . . , exp(zd)), for RM . Theoretical results about the energy function and the retrieval dynam1 i=1 exp(zi) function for β > 0 and Softmax(z) /β is log-sum-exponential (cid:17) i=1 exp(βzi) (cid:80)d ics including convergence, properties of states were proposed [41]. (cid:16)(cid:80)M Connection with attention of the Transformer Interesting connection between the update rule and self-attention mechanism used in transformer and BERT models was also proposed [41]. Specifically, we provide the detail derivation of this connection by following [41]. Firstly, we extend TDense in Eq. 17 to multiple queries := {xi}i[N ]. Given any raw query and memory matrix that are input into Hopfield model, we calculate and Ξ as = RWQ := Q, Ξ = YWK := K, using weight matrices, WQ, WK. Therefore, we rewrite TDense as KSoftmax(βKQ). Then, by taking transpose and projecting to with WV , we have TDense : (cid:55) Softmax(βQK)KWV = Softmax(βQK)V, (18) which is exactly transformer self-attention with β = 1/ the Eq. (12), d. In other words, we obtain by employing the notations in TDense : (cid:55) Softmax(QK/ d)V := At(Q, K, V) = At(WQX, WKX, WV X) (19) 12 However, we can extend the interpretation to cross-attention mechanism: TDense : (X, Y) (cid:55) Softmax (cid:16) XWQW KY/ (cid:17) YWV = At(WQX, WKY, WV Y) We find similarity in the above cross-attention formula with inputs X, and weight matrices WQ, WK, WV . As discussed in lines of this paper, we focus on this extension into the cross-attention mechanism. In terms of modern Hopefield network, the input query is processed with additional transformation WQ to increase complexity of network and inner product are computed with stored (learned) WKY patterns (keys). Then, the retrieved patterns (values) for next layers are computed. Different layers can have different patterns, so hierarchical patterns are stored and retrieved in deep layers. Note that while Hopfield network outputs one pattern, the attention yields multiple patterns, so attention corresponds to stack of outputs of Hopfield network. Hence, the attention is multi-level and multi-valued Hopfield network. Sparse Hopfield Network Later, sparse extensions of the modern Hopfield network are proposed [23, 57]. The energy function was modified to make sparse the computation of retrieval dynamics: Eα : Rd R, (cid:55) Ψ α(β, Ξx) + 1 x, x, Tα : Rd Rd, (cid:55) Ξα-Entmax(βΞx), and Ψ α is the convex conjugate of Tsallis entropy [53], Ψα, α-Entmax(z), represents the probability mapping: Ψα(p) := (cid:40) (cid:80)M i=1(pi pα i=1(pi log pi), α(α1) (cid:80)M ), α = 1, α = 1, α-Entmax(z) := arg max [p, Ψα(p)], pM (20) (21) (22) (23) where RM . Here, α controls the sparsity. When α = 1, it is equivalent to dense probability mapping, 1-Entmax = Softmax, and as α increases towards 2, the outputs of α-Entmax become increasingly sparse, ultimately converging to 2-Entmax Sparsemax(z) := arg min z [36]. Notably, when α = 1, Tα becomes equivalent to TDense T1 [55]. pM We have simple formula for α-Entmax[36]. There is unique threshold function τ : RM that satisfies α-Entmax(z) = [(α 1)z τ (z)1]1/(α1) + . (24) From this formula, we know that the entries less than τ /(α 1) map to zero, so sparsity is achieved. We will denote the number of nonzero entries in α-Entmax as κ(z) for later use to derive theoretical results. For α = 2, the exact solution can be efficiently computed using sorting algorithm [14, 37]. For 1 < α < 2, inaccurate and slow iterative algorithm was used for computing α-Entmax [34]. Interestingly, for 1.5-Entmax, an accurate and exact solution are derived in simple form [39]. Similar to TDense, Tα can be extended to attention mechanisms, establishing strong connection with sparse attention. In other words, by following the derivation as provided in Eq. (18), and Eq. (19), we can obtain Tα : (cid:55) α-Entmax(QK/ d)V := Atα(Q, K, V) (25) Furthermore, similar to the dense attention mechanism, we can also extend into cross-attention mechanism with inputs and Y: Tα : (X, Y) (cid:55) α-Entmax XWQW KY/ YWV = Atα(WQX, WKY, WV Y) (cid:16) (cid:17) Noise robustness of sparse Hopfield network In SHN, sparsity reduces retrieval errors and provide faster convergeness compared to dense retrieval dynamics [23, 57]. While the sparse extension is an efficient counterpart of dense Hopfield network, it has been discovered that there is more advantages to use sparse one besides efficiency [23, 57]. 13 Definition 1 (Pattern Stored and Retrieved). Suppose every pattern ξµ is contained in ball Bµ. We say that ξµ is stored if there is single fixed point Bµ, to which all point Bµ converge, and Bµs are disjoint. We say that ξµ is retrieved for an error ϵ if (x) ξµ ϵ for all Bµ For following theorems, := maxν ξν. Theorem 3 (Retrieval Error). [23, 41, 57] Let Tα be the retrieval dynamics of Hopfield model with α-Entmax. For α = 1, For α = 2, For α > α, Tα(x) ξµ 2m(M 1) exp (cid:110) β (cid:16) ξµ, max ν (cid:17)(cid:111) ξµ, ξν . Tα(x) ξµ + mβ κ (cid:20) (cid:16) ξν, [Ξx](κ) max ν (cid:17) + (cid:21) . 1 β Tα(x) ξµ Tα ξ. (26) (27) (28) You can find the result Eq. (26) in [41], Eq. (27) in [23], and Eq. (28) in [23, 57]. Corollary 3.1. (Noise-Robustness) [23, 57]. In case of noisy patterns with noise η, i.e. = + η (noise in query) or ξµ = ξµ + η (noise in memory), the impact of noise η on the sparse retrieval error T2(x) ξµ is linear, while its effect on the dense retrieval error T1(x) ξµ is exponential. where ξµ is memory pattern and to be considered stored at fixed point of . This theorem suggests that under noisy conditions, sparse attention mechanisms governed by Tα with α > 1 exhibit superior noise robustness compared to standard dense attention. Critically, increasing sparsity (via higher α) further diminishes retrieval errors. We propose new theoretical result that completes above theorem by providing error estimation for all intermediate cases that was not given. Theorem 4 (Retrieval Error 2). Let Tα be the retrieval dynamics of Hopfield model with α-Entmax. For 1 < α 2, Tα(x) ξµ + mκ (cid:104) (α 1)β (cid:16) ξν, [Ξx](κ+1) max ν (cid:17)(cid:105) 1 α , (29) Here, we abuse the notation [Ξx](M +1) := [Ξx](M ) 1α/(α 1). Thanks to this new theorem, we can estimate the impact of noise on the sparse retrieval error for all 1 < α < 2. Corollary 4.1. (Noise-Robustness) In case of noisy patterns with noise η, the impact of noise η on the retrieval error Tα(x) ξµ is polynomial of order 1 α1 for 1 < α 2. Remark The proposed theorem includes the case α = 2. In that case, the right hand side becomes (cid:16) (cid:17)(cid:105) (cid:104) mβ κ max ν ξν, [Ξx](κ+1) . Therefore, by combining with previous result, we obtain tighter bound: T2(x) ξν mβ κ max ν ξν, + min (cid:20) (cid:26) κ[Ξx](κ+1), κ[Ξx](κ) + (cid:27)(cid:21) 1 β proof of Thm. 4. Tα(x) ξµ = (cid:13) (cid:13)Ξα-Entmax (βΞx) ξµ (cid:13) (cid:13) = κ (cid:88) ξ(ν) [α-Entmax (βΞx)](ν) ξµ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ν=1 κ (cid:88) ξµ + (cid:13) (cid:13) (cid:13)ξ(ν) (cid:13) (cid:13) [α-Entmax (βΞx)](ν) (cid:13) ν=1 κ (cid:88) + (cid:104) (α 1) (cid:16) [βΞx](ν) [βΞx](κ+1) (cid:17)(cid:105) 1 α ν=1 + mκ max ν (cid:104) (α 1)β (cid:16) ξν, [Ξx](κ+1) (cid:17)(cid:105) 1 α1 . For Eq. (32), we use the following lemma. 14 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (30) (31) (32) (33) Lemma 1. For RM and ν κ(z), [α-Entmax(z)](ν) [(α 1)(z(ν) z(κ+1))]1/(α1). Proof. (i) κ < From the definition of κ, we have following properties. α-Entmax(z)(κ+1) = 0. z(κ+1) τ (z)/(α 1). Keep the last inequality, and now consider the νth largest coordinate of Eq. (24), but we can omit + since it is strictly positive. α-Entmax(z)(ν) = [(α 1)z(ν) τ (z)]1/(α1) = [(α 1)z(ν) τ (z)]1/(α1) [(α 1)z(ν) (α 1)z(κ+1)]1/(α1) + (ii) κ = We use Holder inequality (cid:16)(cid:88) aip(cid:17)1/p (cid:16)(cid:88) biq(cid:17)1/q (cid:88) aibi for p, (1, ), 1/p + 1/q = 1 to estimate lower bound of τ for α = 2. By substituting ai = (α 1)zi τ, bi = 1, = 1/(α 1), = 1/(2 α), (cid:16)(cid:88) (α 1)zi τ 1/(α1)(cid:17)α1 (cid:16)(cid:88) (cid:17)2α 1 (cid:88) (α 1)zi τ . We know that all entries are positive (α 1)zi τ > 0 since κ = . Moreover, (cid:88) [(α 1)zi τ ]1/(α1) = 1 since the left hand side is the sum of the coordinates of α-Entmax output. Therefore, 2α (α 1) (cid:88) τ α 1 (cid:88) zi zi τ 1α α 1 min zi 1α α 1 = z(M ) 1α α 1 We remain the case α = 2. We directly sum up the entries of 2-Entmax: (cid:88) 1 = zi τ = (cid:88) zi τ min zi τ τ z(M ) 1 = z(M ) 1α α 1 We further estimate the retrieval error of retrieval dynamics defined in PLADIS. We use the notation: Then, we have following result for the retrieval error of λ α . λ α (x) := λTα(x) + (1 λ)T1(x). Theorem 5 (Retrieval Error 3). Consider the retrieval dynamics λ α (cid:16) λ α (x) ξµ λm + λmκ (cid:104) (α 1)β Proof. + 1 λ2m(M 1) exp max ν (cid:16) (cid:110) ξν, [Ξx](κ+1) (cid:17)(cid:105) 1 α1 β ξµ, max ξµ, ξν ν (cid:17)(cid:111) . (34) (35) λ α (x) ξν = λTα(x) + (1 λ)T1(x) ξν λTα(x) + ξν + 1 λT1(x) ξν and apply Eq. (26) and Eq. (29)."
        },
        {
            "title": "This theorem suggests that the retrieval dynamics given in PLADIS have the error bound of mixture of polynomial and",
            "content": "exponential terms. C. Metrics and Implementation Detail For image sampling in Table 2, sampling without CFG guidance is conducted using 30,000 randomly selected text prompts from the MSCOCO validation dataset. Conversely, sampling with CFG is performed with uniformly selected values of in the range (3,5). In both cases, the PAG and SEG scales are fixed at 3.0, following the recommended settings from the corresponding paper. For Tables 3 and 4, we use 200 prompts from Drawbench [47], 400 prompts from HPD [58], and 500 prompts from the test set of Pick-a-pic [27], generating 5 images per prompt. Additionally, for the ablation study in Table 5, we generate 5,000 images from the MSCOCO validation set with CFG and PAG guidance. As with Table 2, the CFG scale is uniformly selected within the range of (3,5), while the PAG scale remains set at 3.0. D. User Preference Study As presented in Fig. 6, we employ human evaluation and do not rely solely on automated evaluation metrics such as FID, CLIPScore, ImageReward, etc. Our aim is to assess whether PLADIS truly improves image quality and prompt coherence. To rigorously evaluate these aspects, we categorized caess into two groups: interaction with guidance sampling including CFG [18], PAG [1], SEG [20], and interaction with guidance-distilled models such as SDXL-Turbo [48], SDXLLightening [31], DMD2 [61], and Hyper-SDXL [42]. We evaluate all models based on 20 selected prompts from the randomly selected Drawbench [47], HPD [58], and Pick-a-pic [27]. For the guidance-distilled model, we select half from one-step sampling results and the other half from four-step sampling results. Human evaluators, who are definitely blind and anonymous, are restricted to participating only once. Evaluators are shown two images from model outputs with and without PLADIS based on the same text prompt and measure images with two questions: for image quality, Which image is of higher quality and visually more pleasing? and for prompt alignment, Which image looks more representative of the given prompt. The order of prompts and the order between models are truly randomized. In Fig. 6, we averaged all of the results related to the guidance-distilled model due to limited space. Further presenting in detail, we present user preference study for each guidance-distilled model as shown in Fig. 8. As similar to guidance sampling, guidance-distilled models with PLADIS outperform both image quality and prompt alignment, validating the practical effectiveness of PLADIS. E. Application on Other Backbone To demonstrate the robustness of our proposed method, we perform experiments using additional backbones, including Stable Diffusion v1.5 (SD1.5) and SANA [59]. SANA is recently introduced text-to-image diffusion model that uses linear attention, enabling faster image generation. It is based on the Diffusion Transformer (DiT) architecture. We generate 30K samples from randomly selected MS COCO validation set images and evaluate them using FID, CLIPScore, and ImageReward, as shown in Table 7. For SD1.5, we use CFG, while SANA is tested with its default configuration without modifications. Interestingly, we observe that both SD1.5 and SANA, when integrated with our PLADIS method, consistently improve performance across all metrics. visual comparison is provided in Fig. 11 and Fig. 12. As shown in the figures, the generation with our PLADIS provides more natural and pleasing images and precise matching between images and text prompts on both backbones. As seen in other experiments, our PLADIS enhances both generation quality and text alignment with the given prompts. By confirming these improvements with SD1.5 and SANA, we demonstrate that PLADIS is robust across different backbones, particularly transformer-based architectures. 16 Figure 8. User preference study for PLADIS in the context of guidance-distilled models. We evaluate the two aspects of model output with and without PLADIS such as image quality and prompt alignment. Table 6. Quantitative comparison across various datasets using 1-steps sampling with the guidance-distilled model. Drawbench [47] HPD [58] Pick-a-pic [27] Method CLIPScore PickScore ImageReward CLIPScore PickScore ImageReward CLIPScore PickScore ImageReward Turbo [48] 27.19 21.67 0.305 28.45 21.85 0. 26.89 21.16 0.346 + Ours 27.56 (+0.37) 21.68 (+0.01) 0.390 (+0.08) 28.78 (+0.33) 21.86 (+0.01) 0.517 (+0.04) 27.10 (+0.21) 21.17 (+0.01) 0.378 (+0.04) Light [31] 26.08 21.86 0.428 27. 22.05 0.730 25.73 21.34 0.585 + Ours 26.66 (+0.58) 21.94 (+0.08) 0.558 (+0.13) 28.42 (+1.05) 22.24 (+0.19) 0.830 (+0.10) 26.63 (+0.90) 21.46 (+0.12) 0.680 (+0.10) DMD2 [61] 27.91 22. 0.651 29.95 22.18 0.888 28.14 21. 0.770 + Ours 28.09 (+0.19) 22.05 (+0.01) 0.662 (+0.01) 30.21 (+0.26) 22.20 (+0.02) 0.902 (+0.01) 28.38 (+0.43) 21.58 (+0.01) 0.794 (+0.02) Hyper [42] 27.41 22.27 0.662 29.09 22.61 0. 27.29 21.91 0.812 + Ours 27.80 (+0.39) 22.30 (+0.03) 0.674 (+0.01) 29.42 (+0.33) 22.65 (+0.04) 0.932 (+0.02) 27.85 (+0.56) 21.92 (+0.01) 0.832 (+0.02) Table 7. Application on other BackBone Model on MS COCO validation set. SD1.5 and SANA indicate that Stable Diffusion version 1.5 and SANA 1.6 model, respectively. Resolution BackBone 512 512 SD1. FID 23.88 CLIPScore ImageReward 24.11 -0. + PLADIS (Ours) 22.41(-1.48) 25.09 (+0.98) -0.08 (+0.360) 1024 1024 SANA [59] 28.01 26.61 0.867 + PLADIS (Ours) 27.53(-0.48) 26.83 (+0.21) 0.883(+0.016) Table 8. Ablation study on layer group which is replaced with PLADIS on MS COCO validation dataset. Layer Baseline Up Mid Down Up, Mid Up, Down Mid, Down FID 33.76 29.78(-3.98) 31.76(-2.00) 31.46(-2.30) 30.76(-3.00) 28.46(-5.30) 31.36(-2.40) CLIPScore ImageReward 25.41 0. 25.78 (+0.37) 25.46 (+0.05) 25.43 (+0.02) 25.46 (+0.05) 26.12 (+0.71) 25.52 (+0.11) 0.624(+0.15) 0.496(+0.02) 0.501(+0.02) 0.548(+0.07) 0.658(+0.18) 0.498(+0.02) All (Ours) 27.87(-5.89) 26.41 (+1.00) 0.726(+0.25) F. Comparison Results on One-Step Sampling As discussed in Section 5, we found that our proposed method, PLADIS, is also effective for one-step sampling with guidance-distilled model. Following the experimental settings in Table 4, we generate images from text prompts in human preference datasets such as Drawbench [47], HPD [58], and Pick-a-pick [27]. The generated images are evaluated using CLIPScore, ImageReward, and PickScore, as presented in Table 6. Our method consistently yields performance improvements, particularly in text alignment and human preference, across all baselines. This demonstrates the robustness of our approach for denoising steps and highlights its potential as generalizable boosting solution. 17 Figure 9. Comparison results for various temperatures, with and without PLADIS, are presented, including the baseline (Softmax) and 1.5Entmax. While lower temperatures with the baseline offer benefits in both cases, our proposed method (α = 1.5), with and without PLADIS, outperforms across all temperature settings. G. Additional Ablation Study G.1. Comparison with Attention Temperature In the field of NLP, to improve existing attention mechanisms, temperature scaling [30], also known as inverse temperature, has been extensively studied to adjust the sharpness of attention. It is defined as follows: At(Q, K, V) = Softmax( QK τ ) (36) where τ denotes the temperature, which controls the softness of the attention. lower temperature results in sharper activations, creating more distinct separation between values. Importantly, it is closely related to the β in α-Entmax. In common attention mechanisms, β is typically set to the square root of the dimension, d, which corresponds to τ = 1.0. In modern sparse Hopfield energy functions, β serves as scaling factor for the energy function, influencing the sharpness of the energy landscape and thereby controlling the dynamics [23]. Hu et al. argue that high β values, corresponding to low temperatures (τ < 1), help maintain distinct basins of attraction for individual memory patterns, facilitating easier retrieval. As discussed in the main paper, we provide an ablation study on the hyperparameter τ (which is equivalent to β) by varying τ from 0.9 to 0.1 for Softmax, alongside our default configuration (1.5Entmax). Similar to the previous ablation study, we generate 5K images from randomly selected samples in the MS-COCO validation set under CFG and PAG guidance with our PLADIS, as shown in Fig. 9. We observed that lowering the temperature (increasing β) consistently improved generation performance in both transformations, such as Softmax and 1.5Entmax. In the case without PLADIS, Softmax with lower temperature improved all metrics, but its performance still remained inferior to sparse attention (α = 1.5). When using PLADIS, the trend was similar: Softmax with lower temperature benefited from PLADIS, but it still did not outperform the 1.5Entmax configuration with PLADIS. Furthermore, 1.5Entmax with lowered temperature consistently improves generation quality in terms of visual quality and text alignment, ultimately converging to similar performance. Notably, very low temperatures with Softmax result in nearly identical sparse transformations, but with larger-than-zero intensities. This suggests that lowering the temperature benefits all transformations in α-Entmax for 1 α 2. However, dense alignment with lowered temperature is insufficient, and sparse attention remains necessary in both cases, with and without PLADIS. Additionally, adjusting other hyperparameters is time-consuming, but our PLADIS with 1.5Entmax does not require finding the optimal hyperparameter τ , thanks to the convergence of performance across various τ values. Therefore, these results demonstrate that the noise robustness of sparse cross-attention in diffusion models (DMs) is crucial for generation performance. G.2. Analysis on Cross-Attention Map To analyze the effect of our proposed method in the cross-attention module, we directly visualize the cross-attention maps, as shown in Fig. 10. Each word in the prompt corresponds to an attention map linked to the image, showing that the information 18 Figure 10. Qualitative comparison of cross-attention average maps across all time steps. Top: Baseline. Middle: PLADIS (with λ = 1) represent only use α-Entmax transformation. Bottom: PLADIS (with λ = 2.0). Our PLADIS with λ = 2.0 provides more sparse and sharp correlation with each text prompt, especially rabbit and dog. Furthermore, other approaches yield incorrect attention maps that highlight the space between the dog prompt and rabbit space. However, our method provides an exact attention map. related to the word appears in specific areas of the image. We observe that the baseline (dense alignment with softmax) produces blurrier attention maps for the related words. Moreover, the generated image does not accurately reflect the text prompt of small dog, instead generating small rabbit. The cross-attention map highlights the small rabbit and large rabbit nearby, associated with the dog prompt, resulting in poor text alignment. When replacing the cross-attention with sparse version, the maps become more sparse but still generate small rabbit and incorrect attention maps. In contrast, our PLADIS produces both sparse and sharp attention maps compared to the baseline, and correctly aligns the attention maps with the given text prompts. As result, PLADIS consistently improves text alignment and enhances the quality of generated samples across various interaction guidance sampling techniques and other distilled models. G.3. The Effect of Layer Group Selection To apply PLADIS in the cross-attention module, we incorporate it into all layers, including the down, mid, and up groups in the UNet. In SDXL, each group contains multiple layers; for example, the mid group has 24 layers, while the up group has 36 layers. To examine the effect of layer group selection, we focus on groups like the mid and up, instead of studying each layer ex. the first layer in the up group. We conduct experiments by varying the groups for the application of PLADIS in the cross-attention module, as shown in Tab 8. Similar to previous ablation studies, we generate 5K samples from randomly selected data in the MS COCO validation set under CFG and PAG guidance. We observe that when applied to single group, the up group has the most significant impact compared to others. However, in all cases, the use of PLADIS improves both generation quality and text alignment, as measured by FID and CLIPScore. Finally, combining all groups yields the best performance, confirming that no heuristic search for the target layer is necessary and validating our default configuration choice. H. Additional Qualitative Results In this section, we present additional qualitative results to highlight the effectiveness and versatility of our proposed method, PLADIS, across various generation tasks and in combination with other approaches. Comparison of Guidance Sampling with Our Method Fig. 13, 14, and 15 provide qualitative results demonstrating interactions with existing guidance methods such as CFG, PAG, and SEG, respectively. By combining PLADIS with these guidance approaches, we observe significant enhancement in image plausibility, particularly in text alignment and coherence 19 with the given prompts, including improvements in visual effects and object counting. Through various examples of this joint usage, we demonstrate that PLADIS improves generation quality without requiring additional inference steps. Comparison of Guidance-Distilled Models with Ours Fig. 16 and 17 present qualitative results from applying our method, PLADIS, to guidance-distilled models such as SDXL-Turbo [48], SDXL-Lightening [31], DMD2 [61], and HyperSDXL [42], for both 1-step and 4-step cases. Notably, PLADIS significantly enhances generation quality, removes unnatural artifacts, and improves coherence with the given text prompts, all while being nearly cost-free in terms of additional computational overhead. Ablation Study on Scale λ Fig. 18 shows visual example of conditional generation with controlled scale λ. We generate samples using combination of CFG and PAG, or CFG and SEG. For the ablation study, all other guidance scales are fixed, and only our scale λ is adjusted. Consistent with the results shown in Sec 6, scale λ of 2.0 produces the best results in terms of visual quality and text alignment, which leads to our default configuration. Ablation Study on α in α-Entmax As discussed in Sec. 6, PLADIS offers two options for choosing α: 1.5 or 2. Fig. 19 provides qualitative comparison between the baseline, α = 1.5, and α = 2. Empirically, we adopt α = 1.5 as our default configuration. While PLADIS with α = 2 improves generation quality and text alignment compared to the baseline (dense cross-attention), PLADIS with α = 1.5 offers more stable and natural enhancement in sample quality. 20 Figure 11. Qualitative evaluation of Stable Diffusion 1.5 using our PLADIS method: PLADIS significantly boosts generation quality, strengthens alignment with the given text prompt, and generates visually compelling images. Figure 12. Qualitative assessment of SANA [59] with and without our PLADIS method: PLADIS notably improves generation quality, strengthens alignment with the provided text prompt, and produces visually striking images. 22 Figure 13. Qualitative evaluation of the joint usage CFG [18] with our method: CFG with PLADIS generates more plausible images with significantly improved text alignment based on the text prompt, without requiring additional inference. 23 Figure 14. Qualitative evaluation of the joint usage PAG [1] with our method: Integrating PAG with PLADIS produces highly credible images with markedly enhanced correspondence to the text prompt, all achieved without any further inference steps. Figure 15. Qualitative evaluation of the joint usage SEG [20] with our method: The combination of SEG and PLADIS yields highly convincing image generations with substantially improved alignment to the given text prompt, accomplished without the need for additional inference. 25 Figure 16. Qualitative comparison of the guidance-distilled model with our PLADIS method for one-step sampling: Even with one-step sampling, our PLADIS enhances generation quality, improves coherence with the given text prompt, and produces visually plausible images. 26 Figure 17. Qualitative comparison of the guidance-distilled model using our PLADIS method for four-step sampling: In the case of the four-step sampling approach, PLADIS substantially improves generation quality, enhances alignment with the provided text prompt, and produces visually convincing images. Figure 18. Qualitative comparison by varying the scale λ: As λ increases, the images display greater plausibility and improved text alignment. However, excessively high values lead to smoother textures and potential artifacts, similar to those found in CFG. The first two rows of images are generated using CFG and PAG, while the remaining rows are produced with CFG and SEG. When λ is greater than 1, our PLADIS method is applied. In our configuration, λ is set to 2.0. 28 Figure 19. Qualitative comparison by α in PLADIS: Although PLADIS with α = 2 also sifgnificantly improves generation quality and text alignment compared to the baseline (dense cross-attention), PLADIS with α = 1.5 offers more robust and coherence given text prompts, leads to our base configuration as α = 1.5."
        }
    ],
    "affiliations": [
        "Samsung Research"
    ]
}