{
    "paper_title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models",
    "authors": [
        "Ilgee Hong",
        "Changlong Yu",
        "Liang Qiu",
        "Weixiang Yan",
        "Zhenghao Xu",
        "Haoming Jiang",
        "Qingru Zhang",
        "Qin Lu",
        "Xin Liu",
        "Chao Zhang",
        "Tuo Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning from human feedback (RLHF) has become a powerful post-training paradigm for aligning large language models with human preferences. A core challenge in RLHF is constructing accurate reward signals, where the conventional Bradley-Terry reward models (BT RMs) often suffer from sensitivity to data size and coverage, as well as vulnerability to reward hacking. Generative reward models (GenRMs) offer a more robust alternative by generating chain-of-thought (CoT) rationales followed by a final reward. However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks. Moreover, their pairwise preference outputs are incompatible with standard RLHF algorithms that require pointwise reward signals. In this work, we introduce Think-RM, a training framework that enables long-horizon reasoning in GenRMs by modeling an internal thinking process. Rather than producing structured, externally provided rationales, Think-RM generates flexible, self-guided reasoning traces that support advanced capabilities such as self-reflection, hypothetical reasoning, and divergent reasoning. To elicit these reasoning abilities, we first warm-up the models by supervised fine-tuning (SFT) over long CoT data. We then further improve the model's long-horizon abilities by rule-based reinforcement learning (RL). In addition, we propose a novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion and enabling more effective use of Think-RM outputs. Experiments show that Think-RM achieves state-of-the-art results on RM-Bench, outperforming both BT RM and vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline, it demonstrates superior end-policy performance compared to traditional approaches."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 5 6 2 6 1 . 5 0 5 2 : r Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models Ilgee Hong1 Changlong Yu2 Liang Qiu2 Weixiang Yan2 Zhenghao Xu1 Haoming Jiang2 Qingru Zhang1 Qin Lu Xin Liu2 Chao Zhang1 Tuo Zhao1 1Georgia Institute of Technology 2Amazon"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning from human feedback (RLHF) has become powerful post-training paradigm for aligning large language models with human preferences. core challenge in RLHF is constructing accurate reward signals, where the conventional Bradley-Terry reward models (BT RMs) often suffer from sensitivity to data size and coverage, as well as vulnerability to reward hacking. Generative reward models (GenRMs) offer more robust alternative by generating chain-of-thought (CoT) rationales followed by final reward. However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks. Moreover, their pairwise preference outputs are incompatible with standard RLHF algorithms that require pointwise reward signals. In this work, we introduce Think-RM, training framework that enables long-horizon reasoning in GenRMs by modeling an internal thinking process. Rather than producing structured, externally provided rationales, Think-RM generates flexible, self-guided reasoning traces that support advanced capabilities such as self-reflection, hypothetical reasoning, and divergent reasoning. To elicit these reasoning abilities, we first warm-up the models by supervised fine-tuning (SFT) over long CoT data. We then further improve the models long-horizon abilities by rule-based reinforcement learning (RL). In addition, we propose novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion and enabling more effective use of Think-RM outputs. Experiments show that Think-RM achieves state-of-the-art results on RM-Bench, outperforming both BT RM and vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline, it demonstrates superior end-policy performance compared to traditional approaches. This depth-oriented approach not only broadens the GenRM design space but also establishes new paradigm for preference-based policy optimization in RLHF. The code, datasets, and models are publicly available at https://github.com/IlgeeHong/Think-RM."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning from human feedback (RLHF) has emerged as powerful post-training paradigm for large language models (LLMs), enabling them to better follow instructions [14], reason over multiple steps [58], and comply with safety constraints [911]. By iteratively shaping the model with learned reward signal aligned to human preferences, RLHF bridges the gap between pretraining objectives and real-world usage. central challenge in RLHF lies in constructing an accurate reward signal. conventional approach is to use Bradley-Terry reward model (BT RM), which maps prompt and response pair to Preprint. Under review. (a) Self-Reflection (b) Hypothetical Reasoning (c) Divergent Reasoning Figure 1: Examples of advanced reasoning abilities enabled by Think-RM. single scalar score by minimizing empirical risk over preference data [12, 13]. While BT RM is straightforward to implement and easy to train, they often overfit to specific patterns in the training data and are highly sensitive to dataset size and coverage, frequently leading to reward over-optimization and reward hacking [1418]. Generative reward models (GenRMs) offer promising alternative to conventional discriminative approaches [1822]. By training an LLM to generate chain-of-thought (CoT) explanation followed by final reward or preference, GenRM leverages the pretrained models existing knowledge and shows greater robustness to data scarcity and distribution shifts, demonstrating stronger out-ofdistribution (OOD) performance [2022]. Moreover, GenRM has been shown to further improve its performance through vertical inference-time scaling, where multiple reasoning paths are generated and then aggregated (e.g., by majority voting or averaging) to produce more reliable reward or preference estimate [1922]. This inference-time scaling is not available to discriminative counterparts such as BT RM, highlighting an additional advantage of the generative approach. However, vertical inference-time scaling often fails to improve GenRM performance on nuanced or complex RM tasks, especially those that require deep reasoning. While aggregating outputs from multiple reasoning paths improves self-consistency, each path generated by existing GenRM is typically shallow (limited to few hundred tokens) making it difficult for any single path to fully capture complex or subtle implicit context. For example, in coding or math-related conversations, such shallow paths may not be sufficient to fully understand the users intent. In multi-turn conversations, they often fail to track long-term dependencies across different turns. In addition, shallow CoT reasoning is often insufficient to detect single false statement embedded within an otherwise fluent and well-structured response. Moreover, the outputs of existing GenRMs are typically expressed as pairwise preferences, which are not directly compatible with standard RLHF algorithms that require pointwise reward signals. In contrast, scaling along the horizontal dimension, where the model reasons more extensively within single trajectory, remains largely underexplored, despite its success in improving the quality of reasoning in other language model applications [2325]. In this work, we introduce Think-RM, new training framework that transforms non-reasoning pretrained LLM (e.g., Llama series [26, 27]) into GenRM equipped with long-horizon thinking capabilities by modeling an internal thinking process. Rather than producing structured, externally provided rationales, Think-RM generates flexible, self-guided reasoning traces that support advanced capabilities such as self-reflection, hypothetical reasoning, and divergent reasoning, as illustrated in Figure 1. This enables the model to solve reasoning-heavy RM tasks by extending single CoT trajectory from hundreds to thousands of tokens. To stimulate the reasoning abilities for the non-reasoning model, we first warm up the model with supervised fine-tuning (SFT). We generate multiple long CoT trajectories for each GenRM prompt using pretrained reasoning model and select the longest correct one, which we use to fine-tune the model. After the SFT warm-up, we further refine the models overly long or noisy reasoning process using rule-based reinforcement learning. In addition, we propose novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion and enabling more effective use of Think-RMs outputs. Extensive experiments show that Think-RM outperforms both the BT RM and the vertically scaled standard GenRM on both in-distribution (ID) and OOD tasks. In particular, it outperforms these baselines by 8% on RM-Bench [28], challenging benchmark requiring intensive reasoning, achieving state-of-the-art performance among all publicly available RMs under 10B using only 6K training data. Furthermore, integrating Think-RM into our pairwise RLHF pipeline yields stronger end-policy performance compared to traditional pointwise RLHF with BT RMs. By shifting the modeling paradigm from breadth to depth, Think-RM not only expands the design space of generative reward modeling, but also establishes new foundation for preference-based policy optimization in RLHF, paving the way toward better alignment of LLMs with more complex objectives."
        },
        {
            "title": "2 Related Work",
            "content": "Bradley-Terry (BT) Reward Modeling. The BT framework [29] is conventional approach to reward modeling in RLHF, where discriminative model learns to predict scalar rewards from pairwise preference data. This approach, pioneered in early RLHF work [2, 12], continues to be widely adopted in state-of-the-art language models like GPT-4 [30] and Qwen-2.5 [31]. In the BT framework, reward models are trained using maximum likelihood estimation to map text inputs to scalar scores that preserve the ordering of human preferences. However, this discriminative modeling paradigm faces several key limitations: it requires large amounts of high-quality preference data for reliable training, shows high sensitivity to dataset coverage, and remains vulnerable to reward hacking where models learn to exploit patterns in the training data rather than truly aligning with human preferences. Generative Reward Models. Recent work has shifted towards generative approaches to reward modeling, where LLMs are trained to generate explanatory rationales before making preference decisions [1822]. Unlike discriminative BT models that directly output scalar scores, GenRMs leverage the reasoning capabilities of LLMs through chain-of-thought generation, leading to several key advantages: they require less training data, demonstrate stronger OOD generalization, and provide interpretable reasoning traces for their decisions. These works show that GenRMs can be effectively trained using standard next-token prediction objectives and can benefit from test-time compute through majority voting over multiple reasoning paths. However, current GenRMs typically generate relatively shallow reasoning paths limited to few hundred tokens, which can be insufficient for complex tasks requiring deeper analysis or long-term dependency tracking. LLM-as-a-Judge for Response Evaluation. parallel line of work explores using LLMs directly as judges to evaluate model outputs [32]. While sharing similar goals with reward modeling, this approach differs by using off-the-shelf LLMs without additional training, relying instead on careful prompt engineering to elicit evaluation capabilities. Although strong LLM judges like GPT-4 can achieve high agreement with human preferences on controlled evaluations, recent studies have revealed significant limitations: position and verbosity biases, inconsistent judgments across different models, and most importantly, an inherent ceiling where an LLMs ability to judge is fundamentally limited by its own ability to solve the underlying tasks. These challenges are particularly pronounced in complex reasoning problems requiring detailed analysis or long-term dependency tracking [32, 33]. These systematic limitations in LLM judges align with our observations about shallow reasoning in GenRM, further motivating the need for models specifically trained for deep analytical evaluation. Long Chain-of-Thought Reasoning. The development of chain-of-thought (CoT) prompting has been crucial for improving language models reasoning capabilities [34]. While early CoT approaches and their extensions like self-consistency [35] and tree-of-thought search [36] showed promise, they typically operated within relatively short reasoning horizons of few hundred tokens. Recent breakthroughs have demonstrated the importance of extending reasoning chains to much longer horizons, with OpenAIs o1/o3 series [23, 37] showcasing remarkable performance on complex mathematical and coding tasks through extended multi-step reasoning. This capability was subsequently replicated and made public through DeepSeeks R1 model [24], which demonstrated that long-horizon reasoning abilities could be systematically trained using reinforcement learning. Similar approaches have been adopted by subsequent models like QwQ [38] and Grok [39], establishing long-horizon reasoning as crucial capability for tackling complex tasks. This evolution in reasoning capabilities directly informs 3 Figure 2: Overview of the Think-RM training framework. our approach to reward modeling, suggesting that extending reasoning horizons could similarly benefit preference learning and evaluation tasks."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce Think-RM, training framework that enables long-horizon reasoning in GenRM. Our approach begins with pretrained LLM that initially lacks sophisticated reasoning capabilities for reward modeling tasks. Building upon this foundation, we stimulate the models reasoning ability through two-stage process: first warming up the LLM using SFT on carefully curated set of long-horizon CoT data, and then refining its reasoning process through rule-based RL. Figure 2 presents the complete pipeline of Think-RM, illustrating the flow from initial task specification through preference dataset creation to the final training stages. 3.1 Preliminaries 3.1.1 Generative Reward Modeling GenRMs are LLMs trained to evaluate responses through natural language reasoning. Let denote the space of natural language task instructions that define the behavior of the GenRM. Each task specifies how the model should evaluate responses based on specific criteria. For example, scoring task asks the model to evaluate single response by outputting numerical reward, while preference task requests comparative judgment between two responses and outputs which one is better. The task instruction is typically provided to the GenRM as system message. In this paper, we focus on the preference task and use five HelpSteer attributes [40] as well as safety as the evaluation criteria, but the proposed method can be generalized to other . Details of our task instructions are provided in Appendix A. We consider pairwise GenRM Gθ, which takes as input triplet (x, ya, yb), where is the prompt context and ya and yb are two different responses to x. The model generates corresponding internal reasoning process and final preference for ya and yb, denoted as (R, s) Gθ(t, x, ya, yb). For the type of s, we consider two cases: (1) binary pairwise GenRM, where {a, b} indicates which response is preferred; and (2) multiclass pairwise GenRM, where {3, 2, 1, 1, 2, 3} represents the strength and direction of preference. In the multiclass case, the magnitude of indicates preference strength, with negative values favoring ya and positive values favoring yb. 3.1.2 Reinforcement Learning from Human Feedback RLHF is training paradigm that aligns language models with human preferences through reward optimization. The core RLHF objective aims to maximize expected rewards provided by pointwise scalar reward model : with respect to the policy language model Pϕ, defined as: xD,yPϕ(x) (cid:2)r(x, y)(cid:3). max ϕ (1) To optimize (1), we can apply the PPO algorithm [41], which iteratively minimizes the following surrogate function: LCLIP(ϕ) = Et (cid:104) min (cid:0) Pϕ(ytx, y<t) Pϕold(ytx, y<t) (cid:98)At, clip(cid:0) Pϕ(ytx, y<t) Pϕold (ytx, y<t) , 1 ϵ, 1 + ϵ(cid:1) (cid:1)(cid:105) (cid:98)At β DKL (Pϕ Pϕref ) , where β 0 is hyperparameter controlling the strength of the KL regularization, and (cid:98)At represents the advantage estimates for the t-th token. These advantage estimates can be computed using established methods such as generalized advantage estimation (GAE) [42] or group relative policy optimization (GRPO) [24]. 3.2 Warm-up Supervised Fine-Tuning To equip reasoning capabilities in non-reasoning LLMs for reward modeling tasks, we first warm up the model through fine-tuning on small set of long CoT trajectories corresponding to task t. To prepare the warm-up CoT data from the preference dataset Dpref = {(xi, ya,i, yb,i, si)}n i=1, we use an off-the-shelf pretrained reasoning model π to generate CoT trajectories for each instance, denoted as {( (cid:98)Rij, (cid:98)sij)}M j=1 π(t, xi, ya,i, yb,i). To equip the LLM with long-horizon reasoning capabilities spanning thousands of tokens, we select the trajectory with the longest (cid:98)Rij among those satisfying (cid:98)sij = si for each instance. Importantly, these longer trajectories naturally incorporate diverse forms of self-reflection and analytical depth, providing strong foundation for developing sophisticated reasoning abilities tailored to reward modeling tasks. Once the long CoT data is prepared, we optimize the following maximum likelihood objective that combines preference prediction and reasoning generation: LSFT(θ) = (x,ya,yb, (cid:98)R,s)D[ log Gθ(st, x, ya, yb, (cid:98)R) log Gθ( (cid:98)Rt, x, ya, yb)]. 3.3 Rule-based Reinforcement Learning While long CoT trajectories provide rich reasoning patterns for reward evaluation, the models used to curate such data are not specifically optimized for this task. This leads to training data that, despite being informative, often contains redundant reasoning steps. After SFT on these trajectories, our GenRM model Gθ naturally inherits this verbose reasoning style. To refine the reasoning process while preserving its effectiveness, we further fine-tune the GenRM with rule-based RL. Specifically, we adopt GRPO from Guo et al. [24], but restrict the reward to be based solely on accuracy. For notational simplicity, we define τ = (t, x, ya, yb). The GRPO loss is then given by: LGRPO(θ) = i=1Gθold (τ ) (cid:34) (cid:88) (x,ya,yb,s)Dpref , { (cid:98)Ri,(cid:98)si}G Gθ( (cid:98)Ri, (cid:98)si τ ) Gθold ( (cid:98)Ri, (cid:98)si τ ) 1 β DKL (Gθ Gθref ) , min i= (cid:32) Ai, clip Gθ( (cid:98)Ri, (cid:98)si τ ) Gθold ( (cid:98)Ri, (cid:98)si τ ) (cid:33) (cid:35) , 1 ϵ, 1 + ϵ Ai (cid:113) where denotes the number of samples per prompt, Ai = (ri r)/((cid:98)σr + ϵ), = (1/G) (cid:80)G (cid:98)σr = the binary output {a, b}, the rule-based reward ri is defined as: j=1 rj, j=1(rj r)2, and DKL( ) denotes the Kullback-Leibler divergence. For (1/(G 1)) (cid:80)G For the multiclass output {3, 2, 1, 1, 2, 3}, the rule-based reward ri is defined as: ri = (cid:26)1.0, (cid:98)si = si 0.0, otherwise ri = 1.0, (cid:98)si = si 0.5, 0.0, otherwise sign((cid:98)si) = sign(si) This reward design ensures strong learning signals by assigning full reward for exact predictions and partial reward for correctly identifying preference direction. The RL training phase is an essential step for Think-RM, as it enables the discovery of effective long-horizon reasoning paths through systematic exploration of diverse trajectories. 5 3.4 Pairwise RLHF with GenRMs Given trained pairwise GenRM (cid:98)Gθ, we propose new direct preference-based approach to fine-tune target policy Pϕ, eliminating the need to recover pointwise rewards for individual prompt-response pairs (x, y). During training, each RL iteration processes mini-batch of prompts, with sampled responses per prompt, yielding GB total responses. We present our advantage estimation method for GRPO below, noting that similar principles apply to GAE-based approaches. Pairwise Preference Strength Matrix. We construct skew-symmetric matrix R(GB)(GB) indexed by responses y1, . . . , yGB. For each pair (yi, yj) sharing prompt x, we obtain single GenRM evaluation ( (cid:98)R, s) (cid:98)Gθ( t, x, yi, yj). For multiclass output s, we define matrix entries as: dij = s, dji = dij. For binary preferences, we first map to (cid:101)s {1, +1} and incorporate the reasoning length (cid:98)R as confidence measure: dij = (cid:101)s/ (cid:98)R, dji = dij. This formulation ensures that longer reasoning chains, which often indicate more ambiguous comparisons, result in smaller reward differences. Advantage Estimation using Preference Strength. We can compute (cid:98)Ai based on our pairwise difference matrix D: (cid:80)G j=1 dij (cid:98)Ai = (cid:114) (cid:80) i,j ij + Gϵ 2(G 1) vs. ri (cid:98)σr + ϵ (cid:125) (cid:123)(cid:122) Standard advantage estimation in GRPO (cid:98)Ai = (cid:124) . This formulation derives from the following relationships: ri = 1 (cid:88) j=1 (ri rj) = 1 (cid:88) j=1 dij and (cid:98)σr = (cid:118) (cid:117) (cid:117) (cid:116) 1 1 (cid:88) (ri r)2 = (cid:115) i=1 1 2G(G 1) (cid:88) i,j d2 ij."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we investigate the effectiveness of long-horizon reasoning in Think-RM on reasoningintensive tasks. We evaluate the performance of various RMs on both in-distribution (ID) and out-of-distribution (OOD) tasks. To ensure fair comparison, we conduct head-to-head evaluations between RMs trained from the same backbone model using the same amount of data. Additionally, we implement our pairwise RLHF framework, which directly uses pairwise preference rewards from CoT-GenRMs and Think-RMs to train the policy. We compare this approach against the traditional pointwise RLHF method, which relies on pointwise rewards (e.g., from BT RMs), to evaluate the relative effectiveness of the two strategies. 4.1 Experiment Setup Training Data and Baselines. We use HelpSteer2-Preference [40] as the training data for all baseline methods as well as Think-RMs. HelpSteer2-Preference consists of 10k high-quality prompt and response pairs covering diverse topics and complexities, including multi-turn contexts. In addition, it provides human preference annotations in the form of preference strengths and corresponding human-written justifications. We use QwQ-32B [38] to generate set of long internal thinking CoT trajectories {( (cid:98)Rij, (cid:98)sij)}M j=1 for each instance in the original HelpSteer2-Preference dataset. We set = 10 and discard any instance if all generated preferences disagree with the ground truth (i.e., (cid:98)sij = si for all j). This results in 6K training examples for binary preference and 4K for multiclass preference. Due to the heterogeneity between the binary and multiclass training sets, our focus is not to compare binary and multiclass GenRMs directly. Instead, we focus on comparing different types of RMs within each category. For all head-to-head comparisons, we use the same training data across baselines within the respective category. For the baselines, we consider BT RM and two CoT-GenRMs [20]: one trained on ground-truth, human-written CoT rationales from the HelpSteer2-Preference dataset, denoted as CoT-GenRM 6 (ground truth), and the other trained on explicit CoT rationales generated by QwQ-32B (after the internal reasoning process), denoted as CoT-GenRM (model-generated). We emphasize that CoTGenRM (ground truth) is strong GenRM baseline, as collecting human-written CoT rationales and further postprocessing them by expert researchers is prohibitively expensive. In addition, we evaluate these CoT-GenRMs under vertical inference-time scaling: for each sample, the model generates independent judgments, and the final prediction is determined by majority voting. This serves as point of comparison to Think-RMs, which adopt different inference time scaling approach based on long-horizon inference scaling. To compare pairwise RLHF with traditional pointwise RLHF, we integrate RMs trained on the filtered HelpSteer2-Preference data into both pipelines and evaluate their end-policy performance. Specifically, we use BT RMs for pointwise RLHF and GenRMs for our pairwise RLHF. We conduct these experiments on the HH-RLHF dataset [1], using 3K randomly sampled prompts for training. Base Model. We use Llama-3.1-8B-Instruct1 as the backbone model for all experiments. We choose this small-sized model because integrating larger one into full pairwise RLHF pipeline is prohibitively expensive in terms of computation and memory. Evaluation Benchmarks. We use HelpSteer3-Preference [43] as benchmark to evaluate generalization under moderate distribution shift. Although it shares similar prompt sources and response pair generation methods with HelpSteer2-Preference, which we use for ID evaluation via its validation set, HelpSteer3-Preference includes more diverse and challenging examples that go beyond ID settings. For OOD evaluation, we consider two additional benchmarks: RewardBench [44] and RM-Bench [28]. RM-Bench is specifically designed to evaluate robustness to subtle content variations and resistance to stylistic biases. It is widely regarded as one of the most challenging benchmarks for RMs, requiring fine-grained judgment and extensive reasoning. For evaluating end-policy performance after RLHF training, we use AlpacaEval2 [45] with GPT-4-as-a-judge. Implementation Details. Our implementation is based on OpenRLHF [46] for training BT RMs and all SFT models, and on VeRL [47] for all RL experiments, including both the rule-based RL stage of Think-RMs and pairwise RLHF with GenRMs. For warm-up SFT, we fine-tune the model for 5 epochs using learning rate of 1e5 for binary outputs and 5e6 for multiclass outputs. At the rule-based RL stage, we set the rollout batch size to 512, the learning rate to 2e6, the KL loss coefficient to β = 1e4, and the group size to = 8 for both binary and multiclass settings. For RLHF experiments, we use the same hyperparameters as in rule-based RL, except we reduce the group size to = 4. Additional implementation details are provided in Appendix B. 4.2 Main Experiment 4.2.1 Evaluation on In-Distribution and Moderately Shifted Tasks In Table 1, we report the preference accuracy of different RMs on ID (HelpSteer2-Preference) and moderately shifted (HelpSteer3-Preference) tasks. Since the models are trained with only 6K examples for binary cases and 4K for multiclass cases, we observe that binary models generally achieve higher accuracy across all subdomains. Due to the small amount of training data, BT RMs underperform compared to GenRMs, even on ID tasks, highlighting the sensitivity of BT RMs to data size and coverage, and the robustness of GenRMs in low-data regimes. CoT-GenRMs (ground truth) outperform CoT-GenRMs (model-generated) on ID tasks due to the higher quality of human-written rationales, but their performance becomes comparable under moderate distribution shift. Vertical inference-time scaling using majority voting over 16 judgments provides minimal improvement. Think-RMs, trained with both SFT and RL, significantly outperform their SFT-only counterparts while also reducing average response length, indicating refinement of the long and noisy reasoning trajectories introduced during SFT warm-up. Notably, binary Think-RM outperforms all baselines on both ID and moderately shifted settings, even surpassing CoT-GenRM (ground truth). For the multiclass case, Think-RM performs slightly worse than CoT-GenRM (ground truth), but outperforms or matches CoT-GenRM (model-generated), even when vertical inference-time scaling is applied. In the reasoning-heavy code domain of HelpSteer3-Preference, binary Think-RM achieves the highest score among all baselines, demonstrating its effectiveness on complex tasks. 1https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct 7 Table 1: Reward model evaluation on HelpSteer2-Preference (in-distribution) and HelpSteer3Preference (moderate distribution shift). Bolded numbers indicate the best performance within each type, and underlined numbers indicate the second best. Type Model HelpSteer2-Preference HelpSteer3-Preference Validation Avg. Len Code General Multilingual Stem AVG Avg. Len Base BT RM CoT-GenRM (ground truth) CoT-GenRM (model-generated) Binary CoT-GenRM (ground truth) w/ vertical inference-time scaling (m = 16) CoT-GenRM (model-generated) w/ vertical inference-time scaling (m = 16) Think-RM (SFT) Think-RM (SFT + RL) Base BT RM CoT-GenRM (ground truth) CoT-GenRM (model-generated) Multiclass CoT-GenRM (ground truth) w/ vertical inference-time scaling (m = 16) CoT-GenRM (model-generated) w/ vertical inference-time scaling (m = 16) Think-RM (SFT) Think-RM (SFT + RL) 67.47 75.57 80.97 76.14 80.11 77.56 74.86 81. 58.81 75.00 78.12 73.30 78.84 75.43 74.86 76.70 354.01 - 97.98 383.33 67.48 72.92 75.23 78. 64.43 70.71 69.84 69.29 66.52 75.48 75.45 76.21 62.96 69.57 69.96 64.61 65.29 71.88 72.03 72.01 353.01 - 100.93 411.29 1561. 75.00 70.11 75.30 70.58 72.16 1596. 6181.76 78.12 69.51 76.21 66.26 72. 6529.28 1335.80 1018.76 351.76 - 98.32 379.16 76.04 80.32 56.13 71.76 78.12 77.20 68.80 70. 57.21 67.21 68.42 68.52 76.52 75.15 59.85 69.1 74.55 73.94 66.67 67.90 58.85 68.75 66.67 69.14 71.48 73. 57.63 68.75 71.43 71.48 1587.74 1124.98 353.10 - 108.89 390.16 1492.64 78.59 67. 75.00 66.05 71.22 1532.8 6064.8 79. 69.18 1573.41 1184.26 72.45 78.36 65.41 67.49 74.24 72.12 75. 67.08 72.03 6280.8 63.58 68.52 67.92 71.41 1665.39 1333. Table 2: Reward model evaluation on RewardBench. Bolded numbers indicate the best performance within each type, and underlined numbers indicate the second best. Type Model Base BT RM CoT-GenRM (ground truth) CoT-GenRM (model-generated) Binary CoT-GenRM (ground truth) w/ vertical inference-time scaling (m = 16) CoT-GenRM (model-generated) w/ vertical inference-time scaling (m = 16) Think-RM (SFT) Think-RM (SFT + RL) Base BT RM CoT-GenRM (ground truth) CoT-GenRM (model-generated) Multiclass CoT-GenRM (ground truth) w/ vertical inference-time scaling (m = 16) CoT-GenRM (model-generated) w/ vertical inference-time scaling (m = 16) RewardBench Chat Chat Hard Reasoning Safety AVG Avg. Len 89.53 88.32 93.85 93.85 45.18 66.89 66.01 62.06 68.46 80.71 76.29 73.24 77.23 76.35 87.97 85. 70.95 78.07 80.79 78.26 381.56 - 113.10 453.10 93.02 65.57 79.33 87. 81.81 1596.8 95.25 63.71 74.33 84. 79.28 6767.2 94.97 94.41 69.13 92.25 95.95 94.69 75.44 77.85 48.14 66.01 64.25 65. 85.03 85.23 63.46 80.83 75.50 75.34 84.86 86.35 58.51 75.14 87.03 85.95 85.44 86.35 61.42 78.56 80.72 79. 2267.46 1422.93 358.82 - 115.77 399.88 95.53 63.27 76.13 85. 80.37 1529.12 95.81 63.38 74.92 87. 80.39 5820.64 Think-RM (SFT) Think-RM (SFT + RL) 90.78 94.27 76.21 75.33 81.11 82. 84.73 86.35 83.17 84.49 2514.14 1635.57 4.2.2 Evaluation on Out-of-Distribution Tasks In Tables 2 and 3, we report the preference accuracy of different RMs on OOD tasks, including RewardBench and RM-Bench. Notably, the Chat Hard and Reasoning subcategories of RewardBench, as well as all domains in RM-Bench, are known to require extensive reasoning. From the tables, we observe that Think-RMs significantly outperform all baselines on these OOD tasks for both binary and multiclass settings, with average improvements of up to 5% on RewardBench and 8% on RM-Bench, even compared to CoT-GenRMs (ground truth) with vertical inference-time scaling using 16 judgments. In particular, Think-RMs achieve improvements of more than 10% and 5% in the Chat Hard and Reasoning subcategories, respectively, and 12% improvement in the Math domain of RM-Bench compared to CoT-GenRMs. These results demonstrate that long-horizon reasoning via internal thinking processes outperforms vertical inference scaling through structured external reasoning when solving complex, reasoning-intensive tasks. Think-RMs trained with both SFT and RL substantially outperform their SFT-only counterparts while also reducing the average response length, consistent with the observation in Section 4.2.1. Table 3: Reward model evaluation on RM-Bench. Bolded numbers indicate the best performance within each type, and underlined numbers indicate the second best. Type Model Base BT RM CoT-GenRM (ground truth) CoT-GenRM (model-generated) Binary CoT-GenRM (ground truth) w/ vertical inference-time scaling (m = 16) CoT-GenRM (model-generated) w/ vertical inference-time scaling (m = 16) Think-RM (SFT) Think-RM (SFT + RL) Base BT RM CoT-GenRM (ground truth) CoT-GenRM (model-generated) Multiclass CoT-GenRM (ground truth) w/ vertical inference-time scaling (m = 16) CoT-GenRM (model-generated) w/ vertical inference-time scaling (m = 16) RM-Bench Chat Code Math Safety AVG Avg. Len 49.10 61.11 60.90 59.35 51.19 53.9 51.88 52.05 57.10 59.48 59.30 58.31 82.63 88.33 88.21 88. 63.79 68.27 67.79 67.51 344.40 - 102.39 400.80 60.16 53.61 59.15 88. 68.11 1534.72 59.73 53.27 60.42 88. 68.55 6340.8 67.92 66.41 52.28 62.05 60.38 60.98 56.29 54.68 51.97 55.58 53.12 53. 71.73 72.25 53.81 57.93 61.32 60.01 91.23 91.51 62.35 82.92 88.52 89.70 75.19 75.06 56.18 66.28 68.86 68. 3228.61 1798.86 334.29 - 104.16 420.19 59.04 53.05 61.06 88. 68.75 1545.92 61.28 54.19 60.40 90. 69.36 6348.48 Think-RM (SFT) Think-RM (SFT + RL) 64.51 64.17 52.00 51.07 66.99 67. 90.38 91.62 71.95 72.37 3092.62 1690.38 4.3 Ablation Study Figure 3 presents the preference accuracy and average response length of binary Think-RM across all benchmarks, comparing two warm-up data selection strategies: using the longest versus the shortest CoT per instance. As shown, training with the longest CoT data consistently achieves higher preference accuracy across all benchmarks, demonstrating the effectiveness of length-based CoT filtering for enhancing reasoning quality. However, this comes at the cost of increased response length, indicating trade-off between accuracy and inference efficiency in selecting warm-up CoT data strategies. Figure 3: Comparison of two CoT filtering strategies for warm-up data selection. 4.4 Pairwise vs. Pointwise RLHF Table 4 shows the end-policy performance of models trained using two different RLHF approaches: traditional pointwise RLHF with BT RMs and our proposed pairwise RLHF with CoT-GenRMs and Think-RMs. As shown, the policies trained using the pairwise RLHF + GenRMs pipeline outperform those trained with pointwise RLHF + BT RMs, demonstrating the effectiveness of our approach. The similar length-controlled win rate (LC WR) between the pairwise RLHF models using CoT-GenRMs 9 and Think-RMs can be explained by their comparable preference accuracy on the HH-RLHF test set (64.42 for CoT-GenRMs vs. 65.03 for Think-RMs), likely due to the relatively easy nature of the prompts in this dataset. Nevertheless, the model trained with Think-RMs achieves higher overall win rate (WR) than the one trained with CoT-GenRMs. Table 4: Comparison of pointwise and pairwise RLHF approaches using different reward models. Model Base Pointwise RLHF with BT RM AlpacaEval2 LC WR WR Avg. Len 24.54 27.49 31.11 33. 2296 2300 Pairwise RLHF with CoT-GenRM (ground truth) w/ vertical inference scaling (m = 4) Pairwise RLHF with CoT-GenRM (model-generated) w/ vertical inference scaling (m = 4) Pairwise RLHF with Binary Think-RM Pairwise RLHF with Multiclass Think-RM 31. 40.30 2430 31.72 31.56 31.94 41.06 47.20 42. 2408"
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced Think-RM, framework for training LLMs as GenRMs with longhorizon reasoning capabilities enabled by an internally modeled thinking process. To elicit advanced reasoning skills such as self-reflection and hypothetical reasoning, we applied supervised fine-tuning on long CoT data, followed by rule-based reinforcement learning. We also proposed pairwise RLHF pipeline that directly leveraged pairwise preference rewards, removing the need for pointwise reward conversion. Experiments showed that Think-RMs outperformed both BT RMs and vertically scaled CoT-GenRMs, and achieved superior end-policy performance when combined with our pairwise RLHF approach. Overall, our framework provided depth-oriented approach for GenRM design and established new paradigm for preference-based RLHF."
        },
        {
            "title": "References",
            "content": "[1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [3] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, 2023. [4] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [5] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [6] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [7] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. 10 [8] Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane DwivediYu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. [9] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of LLM via human-preference dataset. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [10] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe rlhf: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, 2024. [11] Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex Beutel, John Schulman, and Lilian Weng. Rule based rewards for language model safety. arXiv preprint arXiv:2411.01111, 2024. [12] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [13] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. Advances in Neural Information Processing Systems, 2020. [14] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex DAmour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023. [15] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. [16] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024. [17] Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, et al. Rrm: Robust reward model training mitigates reward hacking. arXiv preprint arXiv:2409.13156, 2024. [18] Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, Improving reward models with synthetic critiques. arXiv preprint and Matthias Gallé. arXiv:2405.20850, 2024. [19] Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. Critique-out-loud reward models. arXiv preprint arXiv:2408.11791, 2024. [20] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. [21] Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, et al. Self-generated critiques boost reward modeling for language models. arXiv preprint arXiv:2411.16646, 2024. [22] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, 2025. [23] OpenAI. Learning to reason with llms. OpenAI Blog, 2024. [24] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [25] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. 11 [26] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [28] Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. RM-bench: Benchmarking reward models of language models with subtlety and style. In The Thirteenth International Conference on Learning Representations, 2025. [29] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [30] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [31] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [32] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [33] Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, and Ding Chen. xfinder: Robust and pinpoint answer extraction for large language models. arXiv preprint arXiv:2405.11874, 2024. [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. Chain of thought prompting elicits reasoning in large language models. In NeurIPS, 2022. [35] Xuezhi Wang, Jason Wei, Dale Schuurmans, et al. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [36] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [37] OpenAI. Introducing openai o3 and o4-mini. OpenAI Blog, 2025. [38] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [39] xAI. Grok 3 beta the age of reasoning agents. xAI Blog, 2025. [40] Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. Helpsteer2-preference: Complementing ratings with preferences. In The Thirteenth International Conference on Learning Representations, 2025. [41] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [42] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [43] Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, and Oleksii Kuchaiev. Dedicated feedback and edit models empower inferencetime scaling for open-ended general-domain tasks. arXiv preprint arXiv:2503.04378, 2025. [44] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. [45] Yann Dubois, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple debiasing of automatic evaluators. In First Conference on Language Modeling, 2024. [46] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [47] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [48] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017."
        },
        {
            "title": "A Task Instruction Template",
            "content": "Binary Think-RM System: You are an impartial judge, tasked with evaluating the quality of the two AI assistants responses to the context displayed below. Your evaluation should be based on the following six criteria: - Helpfulness: Overall helpfulness of the response to the users question or instruction. - Correctness: Inclusion of all pertinent facts without errors. - Coherence: Consistency and clarity of expression. - Complexity: Intellectual depth required to write response (i.e., whether the response can be written by anyone with basic language competency or requires deep domain expertise). - Verbosity: Amount of detail included in the response, relative to what is asked for in the context. - Safety: Whether the response is free of any kind of harmful, toxic, or illegal content. After carefully considering these criteria, determine which assistants Begin your evaluation by thinking through the response is superior. problem step by step. Then output your final verdict by strictly following this format: <answer>A</answer> if assistant is better, and <answer>B</answer> if assistant is better. User: [The Start of Context] {context} [The End of Context] [The Start of Assistant As Response] {response1} [The End of Assistant As Response] [The Start of Assistant Bs Response] {response2} [The End of Assistant Bs Response] 14 Multiclass Think-RM System: You are an impartial judge, tasked with evaluating the quality of the two AI assistants responses to the context displayed below. Your evaluation should be based on the following six criteria: - Helpfulness: Overall helpfulness of the response to the users question or instruction. - Correctness: Inclusion of all pertinent facts without errors. - Coherence: Consistency and clarity of expression. - Complexity: Intellectual depth required to write the response (i.e., whether the response can be written by anyone with basic language competency or requires deep domain expertise). - Verbosity: Amount of detail included in the response, relative to what is asked for in the context. - Safety: Whether the response is free of any kind of harmful, toxic, or illegal content. After carefully considering these criteria, determine which assistants response is better and how much better it is using the scale below: -3 if Assistant As response is much better than Assistant Bs response -2 if Assistant As response is better than Assistant Bs response -1 if Assistant As response is slightly better than Assistant Bs response 1 if Assistant Bs response is slightly better than Assistant As response 2 if Assistant Bs response is better than Assistant As response 3 if Assistant Bs response is much better than Assistant As response Begin your evaluation by thinking through the problem step by step. Then output your final score inside the <answer></answer> tag. User: [The Start of Context] {context} [The End of Context] [The Start of Assistant As Response] {response1} [The End of Assistant As Response] [The Start of Assistant Bs Response] {response2} [The End of Assistant Bs Response]"
        },
        {
            "title": "B Additional Implementation Details",
            "content": "Training. We train Think-RMs using eight A100 GPUs (1 node), each with 80GB of memory. The warm-up SFT phase takes approximately one hour, while the rule-based RL phase takes about three hours. For pairwise RLHF training, we use sixteen A100 GPUs (2 nodes), each with 80GB of memory: one node is allocated for RLHF training and the other for GenRM inference. For warm-up SFT, we use the Adam optimizer [48] with β1 = 0.9 and β2 = 0.95, which are the default settings in OpenRLHF [46], and tune the learning rate from {5e6, 1e5}. We apply cosine learning rate scheduler with warmup ratio of 0.03. The number of epochs is tuned over {1, 3, 5, 10}, and we set the batch size to 256 and the maximum sequence length to 16,384 tokens. For rule-based RL, we use the AdamW optimizer [49] with β1 = 0.9 and β2 = 0.999, following the default settings in VeRL [47]. We tune the learning rate in {1e6, 2e6} and use constant learning rate scheduler with no warmup (warmup ratio 0). We tune the number of epochs over {1, 2}, the rollout batch size over {256, 512}, and set the training batch size to 128. The maximum prompt and response lengths are both set to 4,096 tokens. We use KL loss coefficient of β = 1e4 and group size = 8. For baselines including BT RMs and CoT-GenRMs, we tune the number of epochs over {1, 2, 3, 5, 10} and the learning rate over {5e6, 1e5}, and set the batch size to 256. The maximum sequence length is set to 8,192 tokens. All selected hyperparameters are summarized in Table 5. Table 5: Summary of selected hyperparameters across binary and multiclass setups. Type Model Num Epochs LR Rollout Batch Size Binary Multiclass Think-RM (SFT) Think-RM (RL) CoT-GenRM (ground truth) CoT-GenRM (model-generated) BT RM Think-RM (SFT) Think-RM (RL) CoT-GenRM (ground truth) CoT-GenRM (model-generated) BT RM 5 1 5 10 3 5 2 5 10 5 1e-5 2e-6 1e-5 1e-5 1e-5 5e-6 2e-6 1e-5 5e-6 1e- - 512 - - - - 512 - - - For pairwise RLHF experiments with GenRMs, we reuse all hyperparameters selected for the rulebased RL setup, as listed in Table 5. For policy rollout, we set the temperature to 1.0, with maximum prompt and response lengths of 1,024 and 2,048 tokens, respectively. For GenRMs inference, we use temperature of 0.6 and generate up to 2,048 response tokens. To reduce computational cost, we set the group size to = 4. Evaluation. For the inference of GenRMs in Tables 1, 2, and 3, we use temperature of 0.6, top-p of 1.0, and maximum response length of 16,384 tokens. These settings are applied to both Think-RM and CoT-GenRMs. For inference with the RLHF-trained models reported in Table 4, we use temperature of 0.6, top-p of 0.9, and maximum response length of 4,096 tokens."
        }
    ],
    "affiliations": [
        "Amazon",
        "Georgia Institute of Technology"
    ]
}