{
    "paper_title": "MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency",
    "authors": [
        "Nicolas Dufour",
        "Lucas Degeorge",
        "Arijit Ghosh",
        "Vicky Kalogeiton",
        "David Picard"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them to a reward, typically user preference. This discarding of informative data together with the optimizing for a single reward tend to harm diversity, semantic fidelity and efficiency. Instead of this post-processing, we propose to condition the model on multiple reward models during training to let the model learn user preferences directly. We show that this not only dramatically improves the visual quality of the generated images but it also significantly speeds up the training. Our proposed method, called MIRO, achieves state-of-the-art performances on the GenEval compositional benchmark and user-preference scores (PickAScore, ImageReward, HPSv2)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 7 9 8 5 2 . 0 1 5 2 : r MIRO: MULTI-REWARD CONDITIONED PRETRAINING IMPROVES T2I QUALITY AND EFFICIENCY Nicolas Dufour1,2, Lucas Degeorge,1,2,3, Arijit Ghosh,1, Vicky Kalogeiton,2, David Picard,1 1LIGM, ENPC, IP Paris, CNRS, UGE 2LIX, École Polytechnique, IP Paris {nicolas.dufour, arijit.ghosh, david.picard}@enpc.fr {lucas.degeorge,vicky.kalogeiton}@polytechnique.edu 3AMIAD Figure 1: Images from our MIRO Synth model on PartiPrompt(Yu et al., 2022)."
        },
        {
            "title": "ABSTRACT",
            "content": "Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them to reward, typically user preference. This discarding of informative data together with the optimizing for single reward tend to harm diversity, semantic fidelity and efficiency. Instead of this post-processing, we propose to condition the model on multiple reward models during training to let the model learn user preferences directly. We show that this not only dramatically improves the visual quality of the generated images but it also significantly speeds up the training. Our proposed method, called MIRO, achieves state-of-the-art performances on the GenEval compositional benchmark and userpreference scores (PickAScore, ImageReward, HPsV2). Code and Models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Aligning with human preferences. How can apply it to my generative AI problem? There are many success stories in LLMs (Christiano et al., 2017; Rafailov et al., 2023) and even text-to-image generation (Fan et al., 2023), where this alignment has been excelling. In fact, todays best text-toimage generation systems are typically trained in three stages: large-scale pretraining on noisy web data followed by post-hoc alignment using curated subsets and then reinforcement learning from human feedback (RLHF)(Esser et al., 2024; Labs, 2024). While effective, this paradigm carries well-known downsides: it discards informative low-quality data (Dufour et al., 2024), complicates 1 Scoring the dataset with all the rewards score with rj Reward models r1, . . . , rN r1 r2 r3 r4 r5 r6 scores ˆs ϵ caption: scenic volcano add noise condition on ˆs image + caption MIRO xt = (1 t)x + ϵ Flow matching training denoised image Figure 2: MIRO training pipeline. Top: dataset scoring with multiple rewards r1, . . . , rN produces scores vector ˆs. Bottom: during training, the model conditions on ˆs and noisy input xt = (1 t)x + ϵ to learn to denoise toward high-reward regions. training with an additional optimization stage, and tends to overfit to single reward, often harming diversity (mode collapse) or semantic fidelity and efficiency. We ask simple question: rather than correcting pre-trained text-to-image model, can we teach it how to trade off multiple rewards from the beginning? Our answer is MultI-Reward cOnditioning (MIRO) pretraining, framework that integrates multiple reward signals directly into the pretraining objective for text-to-image generation. Similar to Dufour et al. (2024), we condition the generative model on vector of reward scores per text-image pair, and the model thus learns an explicit mapping from desired reward levels to visual characteristics. The rewards span aesthetics, user preference, semantic correspondence, visual reasoning, and domainspecific correctness. This simple change has powerful consequences. First, it preserves the full spectrum of data quality instead of filtering it out, allowing the model to learn how different reward levels manifest visually. Second, it turns alignment into controllable variable at inference time: users can dial individual rewards up or down, or recover multi-reward analogue of classifier-free guidance that steers towards jointly high-reward regions. Third, by providing rich supervision at scale, MIRO accelerates convergence and improves sample efficiency. Empirically, small model trained with MIRO on 16M-image setup outperforms no reward conditioning and single-reward baselines: it converges up to 19 faster on AestheticScore (Schuhmann et al., 2022), HPSv2 (Wu et al., 2023), PickScore (Kirstain et al., 2023), and ImageReward (Xu et al., 2023), mitigates reward hacking, and improves compositional alignment. It outperforms much bigger models like Flux-dev on GenEval (Ghosh et al., 2024) and user preference scores (Xu et al., 2023; Wu et al., 2023; Kirstain et al., 2023), while remaining substantially more compute-efficient. Our contributions are the following: We propose MIRO: reward-conditioned pretraining that integrates rewards directly during training alleviating post-hoc processing, MIRO achieves state-of-the-art scores on GenEval and user-preference metrics, outperforming much bigger models trained for much longer, MIRO converges up to 19 faster than regular training and achieves the same quality with orders of magnitude less inference compute (370 less than Flux for example). scores ˆs+ caption: scenic volcano condition ˆs+ image + caption MIRO noisy input xt condition ˆs scores ˆs + ω guided image ˆvθ vθ(xt, c, ˆs+) vθ(xt, c, ˆs) = vθ(ˆs+) vθ(ˆs) guidance Figure 3: MIRO inference overview (single model). The previous step xt and caption are fed to one MIRO model while conditioning on two reward histograms: ˆs+ (top) and ˆs (bottom), producing vθ(xt, c, ˆs+) and vθ(xt, c, ˆs). The guidance direction = vθ(ˆs+) vθ(ˆs) is scaled by ω and added to the high-reward output to obtain the guided image ˆvθ."
        },
        {
            "title": "2 METHOD",
            "content": "We introduce MultI-Reward cOnditioning Pretraining (MIRO), framework for conditional image generation that incorporates multiple reward signals directly into the pretraining phase. Our key insight is that by conditioning the generative model on explicit reward scores during training, we can preserve the full spectrum of quality levels while enabling fine-grained control over multiple objectives at inference time. This approach eliminates the need for separate alignment stages (Fan et al., 2023) while providing unprecedented flexibility in reward trade-offs. Method Overview Our method consists of three key components: (1) Dataset Augmentation, where we enrich the pretraining dataset with reward annotations across multiple quality dimensions; (2) Multi-Reward Conditioned Training, where we modify the flow matching objective to incorporate reward signals directly into the generative process; and (3) Reward-Guided Inference enables fine-grained control over generation quality through explicit reward conditioning during sampling. Problem Formulation Let = {(x(i), c(i))}M i=1 be large-scale pretraining dataset where x(i) RHW 3 represents an image and c(i) represents the corresponding text condition (e.g., caption, prompt). Traditional pretraining learns generative model pθ(xc) that captures the joint distribution of images and text without explicit quality control. In contrast, we consider set of reward models = {r1, r2, . . . , rN } where each rj : RHW 3 evaluates different aspects of image quality, with being the associated conditioning space. Our goal is to learn conditional generative model pθ(xc, s) where = [s1, s2, . . . , sN ] represents the desired reward levels, enabling controllable generation across multiple quality dimensions. 2.1 DATASET AUGMENTATION WITH REWARD SCORES The first step of MIRO involves augmenting the pretraining dataset with comprehensive reward annotations. For each sample (x(i), c(i)) D, we compute reward scores across all reward models: s(i) = rj(x(i), c(i)) {1, 2, . . . , } (1) This process transforms our dataset into an enriched version = {(x(i), c(i), s(i))}M s(i) = [s(i) ] contains the multi-dimensional quality assessment for each sample. 2 , . . . , s(i) 1 , s(i) i=1 where Score Normalization and Binning. Raw reward scores often exhibit different scales and distributions across reward models, making direct conditioning challenging. To address this, we employ uniform binning strategy into bins that ensures balanced representation across quality levels. Details are found in the Supplementary Material. 3 MIRO vs. Image Reward MIRO vs. HPSv2 MIRO vs. Aesthetic MIRO vs. SciScore Reward Image 1.1 0.22 LIP Reward Image 1.1 0.22 LIP Pick 0.23 0. 0.2 5 0.17 0.24 0.25 0. HPSv2 MIRO vs. CLIP Pick 0.23 0.21 0. 5 0.17 0.24 0.25 0.30 HPSv Aesthetic 6.8 Reward Image 1.1 0.28 LIP OpenAI 0.22 LIP Aesthetic 6.8 Reward Image 1. 0.28 LIP OpenAI 0.22 LIP Pick 0. 0.21 0.2 5 0.17 0.24 0. 0.30 HPSv2 MIRO vs. VQA Pick 0.23 0. 0.2 5 0.17 0.24 0.25 0. HPSv2 Aesthetic 6.8 Reward Image 1.1 0. LIP OpenAI 0.22 LIP Aesthetic 6.8 Reward Image 1.1 0.28 LIP OpenAI 0.22 LIP Pick 0.23 0.21 0.2 5 0.17 0. 0.25 0.30 HPSv2 MIRO vs. Pick Pick 0. 0.21 0.2 5 0.17 0.24 0. 0.30 HPSv2 Aesthetic 6.8 Reward Image 1. 0.28 LIP OpenAI 0.22 LIP Aesthetic 6. Reward Image 1.1 0.28 LIP OpenAI 0.22 LIP Pick 0.23 0.21 0.2 5 0. 0.24 0.25 0.30 HPSv2 MIRO vs. Baseline Pick 0.23 0.21 0.2 5 0.17 0. 0.25 0.30 HPSv2 Aesthetic 6.8 0. LIP OpenAI Aesthetic 6.8 0.28 LIP OpenAI Figure 4: Comparison of the MIRO model against eight other specialist/baseline models. Each radar plot shows MIRO versus comparison model across six metrics. 2.2 MULTI-REWARD CONDITIONED FLOW MATCHING Having augmented our dataset with reward scores, we now incorporate these signals into the generative model architecture. We build upon flow matching Lipman et al. (2023), powerful framework for training continuous normalizing flows that has shown excellent performance in high-resolution image generation. Training Objective. Following the standard flow matching formulation, we sample noise ϵ (0, I) and time U(0, 1), then compute the noisy sample xt = (1 t)x + tϵ. The multi-reward flow matching loss becomes: = (x,c,ˆs) D,ϵN (0,I),tU(0,1) (cid:104) vθ(xt, c, ˆs) (ϵ x)2 2 (cid:105) (2) This objective trains the model to predict the difference between the noise and the clean image, conditioned on both the text prompt and the desired quality levels. The model learns to associate different reward levels with corresponding visual characteristics, enabling reward-aware generation. Training Dynamics. During training, the model observes the full spectrum of quality levels for each reward dimension. This exposure allows it to learn the relationship between reward values and visual features, from low-quality samples that may exhibit artifacts or poor composition to high-quality samples with superior aesthetics and text alignment. 2. INFERENCE WITH REWARD-GUIDED SAMPLING At inference time, MIRO provides unprecedented control over the generation process through explicit reward conditioning. This section details the various sampling strategies enabled by our approach. High-Quality Generation. For generating high-quality samples, we condition the model on maximum reward values across all dimensions: ˆsmax = [B 1, 1, . . . , 1]. This instructs the model to generate samples that maximize all reward objectives simultaneously. Multi-Reward Classifier-Free Guidance. We extend classifier-free guidance to the multi-reward setting by leveraging the reward conditioning mechanism. Following the Coherence-Aware CFG approach (Dufour et al., 2024), we compute guidance using the contrast between positive direction and negative direction in the reward space. We introduce positive and negative reward target, denoted ˆs+ and ˆs, which can be chosen by the user for controllability. By default, we use ˆs+ = ˆsmax = [B 1, . . . , 1] and ˆs = ˆsmin = [0, . . . , 0] and ω is the guidance scale: 4 Aesthetic Score Image Reward Pick Score HPSv2 Score 19.1x faster 6 4 1 0 1 2 3.7x faster 0.22 0.21 0.2 0.19 3.5x faster 0. 0.22 0.16 6.3x faster 0 1 3 4 5 0 1 3 4 5 0 1 3 4 5 0 1 3 4 5 Train steps 105 Train steps 105 Train steps 105 Train steps 105 Figure 5: Training curves showing reward evolution during training. Baseline, MIRO. ˆvθ(xt, c) = vθ(xt, c, ˆs+) + ω (cid:0)vθ(xt, c, ˆs+) vθ(xt, c, ˆs)(cid:1) (3) Theoretical Interpretation. This guidance formulation can be interpreted as approximating the gradient of an implicit joint reward function. Specifically, the guidance direction vθ(xt, c, ˆsmax) vθ(xt, c, ˆsmin) points toward regions of the latent space where all rewards are simultaneously high, effectively steering generation away from low-quality outputs and toward samples that satisfy multiple quality criteria jointly. By amplifying this direction with the guidance scale ω, we push the generated samples toward parts of the distribution characterized by superior aesthetic quality, text alignment, and other desired attributes. Similar to the weak guidance framework (Karras et al., 2024), where bad version of the model is used to guide the good version, here the guidance is provided by the contrast between high-reward and low-reward conditioning. Flexible Reward Trade-offs. key advantage of MIRO is the ability to specify custom reward targets at inference time. Users can set ˆscustom = [ˆs1, ˆs2, . . . , ˆsN ] where each represents the desired level for reward for image i. This enables control over trade-offs between different quality or preference aspects. 2.4 ADVANTAGES OF MIRO OVER TRADITIONAL ALIGNMENT APPROACHES MIRO offers several key advantages over traditional alignment approaches, stemming from its unified training paradigm and explicit reward conditioning mechanism. Training Efficiency. By incorporating reward alignment directly into pretraining, MIRO eliminates the need for separate fine-tuning or reinforcement learning stages. MIRO converges to reward-aligned behavior without additional training phases achieving faster convergence than regular pretraining and higher quality samples. The single-stage training also reduces the complexity of the training pipeline and eliminates hyperparameter tuning for multiple stages. Full-Spectrum Data Utilization. In contrast to post-hoc fine-tuning and RL pipelines that filter or concentrate training on narrow slice of high-reward data, MIRO retains every sample and trains across the entire reward spectrum. Each example contributes signal together with its associated reward vector, so low-, medium-, and high-scoring regions are all modeled. This spectrum-wide supervision reduces collapse toward narrow high-reward modes, yields representations that generalize across quality levels, and produces single model that can intentionally generate at any desired reward level at inference time. Reward Hacking Prevention. Traditional single-objective optimization often leads to reward hacking, where models exploit specific reward metrics at the expense of overall quality (Luo et al., 2025). MIROs multi-dimensional conditioning naturally prevents this by requiring the model to balance multiple objectives simultaneously. Users can detect and mitigate reward hacking by adjusting individual reward levels and observing the resulting trade-offs. Controllability and Interpretability. The explicit reward conditioning provides interpretable control over generation quality. Users can understand and predict the effect of different reward settings, enabling more intuitive interaction with the model. This controllability extends beyond simple quality scaling to nuanced trade-offs between different aspects of visual quality. 5 Step 25k Step 50k Step 75k Step 100k Step 200k Step 400k \"a tiger wearing tuxedo\" i a I i a I \"panda mad scientist mixing sparkling chemicals \" Figure 6: Training progression visualization showing generated images at different training steps for the same prompt. Top row shows baseline model outputs, bottom row shows MIRO model outputs."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 REWARD-CONDITIONED PRETRAINING IMPROVES MODEL QUALITY We demonstrate that pretraining with MIRO produces superior models compared to traditional approaches. We evaluate three training configurations: (1) baseline model trained without reward conditioning, (2) single-reward models conditioned on individual rewards (similar to Coherence Aware Diffusion Dufour et al. (2024) but using our reward suite instead of CLIP score), and (3) MIRO conditioned on all seven rewards simultaneously. MIRO outperforms single-reward approaches across all metrics. Figure 4 presents results on the CC12M+LA6 dataset, evaluating models across AestheticScore, PickScore, ImageReward, HPSv2, and JINA CLIP score. We also include OpenAI CLIP score as an out-of-distribution evaluation metric not used during training. MIRO consistently outperforms all baselines across aesthetic and preference metrics, demonstrating the effectiveness of multi-reward conditioning. Multi-reward conditioning mitigates reward hacking. Crucially, we observe that leveraging multiple rewards mitigates reward hacking compared to single-reward optimization. This is particularly evident with AestheticScore: while the single-reward model achieves high aesthetic scores, it severely degrades performance on other metrics. Models trained on ImageReward and HPSv2 show more balanced trade-offs but still underperform MIROs comprehensive optimization. MIRO dramatically accelerates training convergence. Figure 5 reveals substantial training efficiency gains from multi-reward conditioning. MIRO reaches the baseline models final performance dramatically faster: 19 speedup for AestheticScore, 6.2 for HPSv2, 3.5 for PickScore, and 3.3 for ImageReward. This acceleration occurs because reward conditioning provides dense supervisory signals throughout training that guide the model toward high-quality generations, rather than requiring the model to discover these qualities through the diffusion objective alone. Position 50 80 Colors GenEval Metrics Two Objects 14 44 75 43 29 76 20 Counting Aesthetic Metrics Pick 0. 0.19 0.2 5 0.17 0.24 0. 0.32 HPSv2 Aesthetic 7.0 0.30 LIP OpenAI Single Object 100 Reward Image 1.2 60 Attribution Color 0.24 LIP (a) 76 74 70 68 r l n 66 0 0.250. 0.751.0 6.4 5.9 5.3 4.7 4. 3.5 e e p P a Aesthetic weight in prompt (b) Figure 7: (a) MIRO vs baseline trained with real vs synthetic captions on GenEval and Aesthetic metrics. (b) Trading off GenEval and Aesthetic scores with Synth MIRO by adjusting the aesthetic weight in the prompt, i.e., varying the positive target ˆs+ aesthetic while keeping the other components of ˆs+ equal to 1 and ˆs fixed. Legend: Baseline (real), MIRO (real), Synth Baseline (50% real + 50% synth), Synth MIRO (50% real + 50% synth); lines in (b): GenEval Overall, Aesthetic. Qualitative results confirm accelerated high-quality generation. Figure 6 provides qualitative evidence of MIROs accelerated convergence. For the tiger in tuxedo prompt, MIRO establishes proper compositional layout and generates visually appealing tiger within 50k training stepsa level of quality that requires 200k steps for the baseline model to achieve. Similarly, for the mad scientist panda prompt, MIRO rapidly converges to aesthetically pleasing results while the baseline model fails to generate recognizable panda until 400k steps. These qualitative improvements complement our quantitative findings, demonstrating that MIROs multi-reward conditioning enables both faster convergence and superior generation quality. 3.2 IMPROVING TEXT-IMAGE ALIGNMENT Beyond optimizing for specific reward metrics, MIRO demonstrates significant improvements in text-image alignment, as measured by comprehensive evaluation benchmarks. Table 1 presents detailed results on GenEval, comparing MIRO against baseline models and single-reward approaches. MIRO enhances compositional understanding. Our multi-reward approach substantially improves the models ability to generate images that accurately reflect textual descriptions. MIRO achieves an overall GenEval score of 57, representing 9.6% improvement over the baseline score of 52. This enhancement is particularly pronounced in challenging compositional reasoning tasks: Color Attribution improves from 29 to 38 (+31%), Two Objects from 55 to 68 (+24%), and Counting from 49 to 55 (+12%). These results demonstrate that MIROs multi-reward conditioning enables better understanding of complex spatial relationships, object interactions, and numerical concepts. Single-reward models exhibit varying alignment capabilities. Our analysis reveals that different reward models contribute differently to text-image alignment. Models optimized solely for aesthetic appeal (AestheticScore) achieve poor GenEval performance (33.0), suggesting that aesthetic optimization can come at the expense of semantic fidelity. In contrast, rewards more directly related to text-image correspondencesuch as CLIP score, VQA score, and JINA CLIP scoreachieve GenEval scores of 57, matching MIROs performance. Notably, the SciScore model achieves the highest single-reward GenEval score of 58.0, though this comes with reduced aesthetic quality as shown in Figure 4. Multi-reward conditioning prevents overfitting. The superior performance of MIRO compared to single-reward models highlights key advantage of our approach: by optimizing across multiple complementary objectives simultaneously, MIRO avoids the overfitting that occurs when models focus exclusively on single reward signal. This balanced optimization leads to models that excel across diverse evaluation criteria while maintaining strong performance on individual metrics. 7 GenEval PartiPrompts Position Counting Colors Color Aesthetic Image HPSv2 PickAScore Model SD v1.5 SD v2.1 PixArt-α PixArt-Σ CAD Sana-0.6B Sana-1.6B SDXL SD3-medium FLUX-dev Image Reward HPSv2 Aesthetic SciScore CLIP VQA Pick Baseline MIRO Baseline MIRO MIRO Params (B) Inference Overall TFLOPs Single Two Obj. Obj. 0.9 0.9 0.6 0.6 0.35 0.6 1.6 2.6 2.0 12.0 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 - - - - 20.8 - - - - 4.16 4.16 4.16 4.16 4.16 4.16 4.16 4.16 4.16 4.16 4.16 4.16 SOTA Baselines 97 98 98 98 95 99 99 98 98 99 38 51 50 59 56 71 79 74 74 4 7 8 10 11 16 18 15 34 20 35 44 44 50 40 63 63 39 63 79 CAD-like Models (our models) 97 95 74 94 97 97 93 59 63 37 62 63 58 62 21 15 6 24 24 20 56 52 24 61 57 57 58 Real Caption Models (our models) 94 92 55 68 18 19 49 76 85 80 80 76 91 88 85 67 74 76 78 42 72 70 76 75 68 69 43 50 48 52 50 64 66 55 62 67 57 56 33 58 57 57 57 52 Synthetic Caption Models (50% Real + 50% Synth) (our models) 57 68 75 93 97 98 59 73 79 30 46 58 44 61 74 77 85 Attr. 6 17 7 15 22 42 47 23 36 47 33 31 15 35 32 37 34 29 38 43 52 5.68 5.81 6.47 6.44 5.56 6.31 6.36 5.94 6.18 6.56 5.31 5.47 6.65 4.62 5.04 4.88 5.16 5.18 6.28 4.96 6.28 5.24 Inference Scaled + Synthetic Caption Models (MIRO + 128 samples inference scaled) (our models) Aesthetic Scaled MIRO Image Reward Scaled MIRO HPSv2 Scaled MIRO PickAScore Scaled MIRO 0.36 0.36 0.36 0.36 532 532 532 532 63 75 74 74 97 98 98 98 68 84 83 83 40 52 47 57 69 74 76 75 82 80 81 45 65 65 59 6.81 6.28 6.28 6.27 0.24 0.38 0.97 1.02 0.69 1.23 1.23 0.46 1.15 1.19 1.04 0.90 0.00 0.56 0.73 0.64 0. 0.52 1.06 0.52 1.11 1.18 1.04 1.61 1.35 1.32 0.25 0.26 0.29 0.29 0.26 0.30 0.30 0.25 0.30 0.30 0.27 0.29 0.26 0.24 0.25 0.25 0.26 0.25 0. 0.24 0.29 0.29 0.29 0.30 0.32 0.31 0.213 0.215 0.226 0.225 0.214 0.228 0.228 0.220 0.225 0.229 0.214 0.215 0.209 0.209 0.214 0.212 0.216 0.212 0.220 0.211 0.220 0. 0.219 0.223 0.225 0.229 Table 1: GenEval and PartiPrompts Results Comparison Across All Models. Unless noted, inference uses the positive/negative targets ˆs+ = [1, 1, . . . , 1] and ˆs = [0, 0, . . . , 0]. denotes custom positive target with all rewards set to 1 except the aesthetic reward set to 0.625 (i.e., ˆs+ aesthetic = 0.625), with ˆs fixed. 3.3 MIRO AND SYNTHETIC CAPTIONS Synthetic captioning has emerged as the go-to method for improving text-image alignment in generative models. This approach offers the advantage of retaining all training data without requiring filtering based on caption quality. While CAD Dufour et al. (2024) proposes method to avoid filtering, it does not demonstrate results on synthetic captions. We evaluate MIRO using mixture of 50% synthetic and 50% real captions (captioning details provided in the Supplementary Material). Technical implementation. Applying MIRO to synthetic captions presents challenge: some reward models cannot process captions longer than 77 tokens, while our synthetic captions are extensive (approximately 200 tokens). To address this limitation, we generate both long synthetic captions for training and shorter versions for reward model evaluation. MIRO outperforms synthetic captioning alone. Our results demonstrate that MIRO without synthetic captions achieves comparable GenEval performance to baseline models trained with synthetic captions. More importantly, Figure 7a shows that MIRO without synthetic captions significantly outperforms the synthetic caption baseline across rewards metrics. This finding suggests that MIRO provides more effective approach to improving text-image alignment than synthetic captioning alone, while being computationally more efficient. Indeed, reward model scoring requires substantially less compute than recaptioning with large vision-language models. MIRO unlocks synthetic caption potential. Combining MIRO with synthetic captions yields the strongest overall performance as shown in Table 1. While maintaining equivalent aesthetic quality to MIRO without synthetic captions, this combined approach achieves remarkable GenEval score of 68, substantially improving over the synthetic caption baseline of 57 (+19%). The improvements are consistent across all compositional reasoning metrics: Position increases from 30 to 46 (+53%), Color Attribution from 43 to 52 (+21%), Single Object from 93 to 97 (+4%), Two Objects from 58 to 73 (+26%), and Counting from 44 to 61 (+39%). These comprehensive gains across all compositional aspects demonstrate that MIRO effectively benefits massively from synthetic captions for text-image alignment, achieving superior compositional understanding while preserving aesthetic quality. 8 Aesthetic Score Image Reward Pick Score HPSv2 Score 6.5 6 5.5 1.5 1 0. 0.23 0.225 0.22 0.215 0.32 0. 0.28 0.26 32 faster 16 faster 0 4 2 3 5 Best-of-2N 6 7 0 4 2 3 5 Best-of-2N 6 7 0 4 2 3 5 Best-of-2N 6 7 0 4 2 3 5 Best-of-2N 6 7 Figure 8: Test-time scaling showing performance vs. Best-of-2N sampling. Baseline, MIRO."
        },
        {
            "title": "3.4 SYNERGIZING WITH TEST-TIME SCALING",
            "content": "Test-time scaling has emerged as popular method to improve reward performance by generating multiple samples and selecting the best one Ma et al. (2025). We demonstrate that MIRO achieves superior sample efficiency compared to baseline models when combined with test-time scaling. Experimental setup. We evaluate both baseline and MIRO models using the Random Search protocol from Ma et al. (2025). Figure 8 presents performance across varying sample counts (1 to 128 samples, displayed on log-2 scale). For each evaluation, we generate samples and select the highest-scoring sample according to the respective reward model. MIRO demonstrates superior sample efficiency. Our results reveal that MIRO consistently outperforms the baseline across all reward metrics, often by substantial margins. Most remarkably, for Aesthetic Score and HPSv2 metrics, MIRO achieves with single sample what the baseline cannot reach even with 128 samples. This dramatic efficiency gain highlights MIROs ability to generate high-quality samples without requiring extensive test-time computation. Quantifying inference-time efficiency improvements. The efficiency gains are particularly striking for specific metrics: For ImageReward, MIRO with 8 samples matches the performance of the baseline with 128 samples, representing 16 efficiency improvement. For PickScore, MIRO achieves equivalent performance with only 4 samples compared to the baselines 128 samples, demonstrating remarkable 32 efficiency gain. These results establish MIRO as not only superior training approach but also more efficient inference-time method. 3.5 COMPARISON TO STATE-OF-THE-ART MODELS In Figure 1, we evaluate MIRO against state-of-the-art text-to-image models on GenEval, demonstrating superior performance while maintaining significantly lower computational costs. GenEval results demonstrate exceptional training efficiency. MIRO achieves GenEval score of 68, outperforming FLUX-dev (12B parameters) which scores 67, while requiring dramatically less computation: 4.16 TFLOPs vs 1540 TFLOPs for FLUX-dev, representing remarkable 370 efficiency improvement. This demonstrates that MIROs multi-reward conditioning enables compact models to surpass much larger architectures. MIRO sets new benchmarks for compositional reasoning. Beyond overall performance, MIRO excels on challenging compositional metrics that have historically been difficult for text-to-image models. On the Position metric, MIRO achieves score of 46, improving upon the previous stateof-the-art of 34 (SD3-Medium) by 31%. For Color Attribution, MIRO advances from FLUX-devs previous best of 47 to 52 (+11%). User preference evaluation confirms scalable efficiency. On PartiPrompts, MIRO consistently outperforms larger models across multiple reward metrics, leveraging inference time scaling. When optimizing for Aesthetic Score with 128-sample inference scaling, MIRO achieves state-of-the-art score of 6.81 compared to FLUX-devs 6.56. For ImageReward optimization, MIRO scores 1.61 versus Sana-1.6Bs 1.23. Remarkably, even with this 128-sample inference scaling strategy, MIRO maintains 3 efficiency advantage over FLUX-dev (532 TFLOPs vs 1540 TFLOPs) while achieving superior performance across all metrics. 9 Aesthetic CLIP HPSv2 Image Reward Pick SciScore VQA All Graffiti of rocket ship on brick wall in vibrant, high-contrast neon pop-art colors Robots meditating on skyscraper rooftop under neon rain with deep blue and magenta glow Figure 9: Generations from the Synth MIRO model using multi-reward classifier-free guidance (see notation in Section 2.3). For each column j, we sample with positive target ˆs+ = [1, . . . , 1] and negative target ˆs = [1, . . . , 1] except ˆs = 0. This makes the guidance vector point purely toward reward j. The All column uses ˆs = 0, guiding toward simultaneously high values for all rewards. Multi-reward conditioning enables cross-metric generalization. Notably, MIRO demonstrates strong performance even when not explicitly optimized for specific metrics. For instance, when optimizing for HPSv2, MIRO achieves an ImageReward score of 1.35, outperforming models specifically trained for that metric. This cross-metric generalization highlights the robustness of MIROs multi-reward approach and its ability to achieve state-of-the-art results with substantially reduced computational requirements. 3.6 FLEXIBLE REWARD TRADE-OFFS AT INFERENCE Reward weighting exposes controllable trade-offs between aesthetics and alignment. Our testtime scaling results (Figure 8) show that selecting samples by Aesthetic Score can reduce GenEval performance, indicating trade-off between aesthetic quality and semantic alignment. Sweeping the aesthetic weight identifies an optimal balance. We vary the aesthetic reward weight at inference and observe the highest GenEval score at weight of 0.625, at the cost of lowering the Aesthetic Score to 5.24 (Figure 12). Optimized weighting rivals heavy test-time scaling. Using this inference strategy, MIRO match the GenEval performance of ImageReward-based selection with 128-sample test-time scaling, while using single weighted selection. Other metrics also improve; for example, ImageReward reaches 1.18, matching FLUX-dev without test-time scaling. Visualizing per-reward controllability. In Figure 9, we visualize this controllability with Synth MIRO using multi-reward classifier-free guidance (Section 2.3). For column j, we set ˆs+ = [1, . . . , 1] and ˆs = [1, . . . , 1] with ˆs = 0, which cancels the shared direction and isolates reward while keeping the other rewards anchored to ˆs+. Pairwise reward exploration. To explore the trade-offs between two specific rewards, we perform pairwise interpolation while keeping all other rewards fixed. For rewards and B, we set ˆs+ = [1, . . . , 1] and ˆs = [1, . . . , 1], except for the two rewards of interest: ˆs = 1 t, where [0, 1] controls the interpolation. This configuration enables smooth exploration of the reward space between two objectives while maintaining high values for all other rewards, revealing the models ability to navigate trade-offs between specific quality dimensions. = and ˆs User-controlled rewards at inference. MIRO allows choosing reward weights at test time, enabling principled trade-offs across capabilities, giving users control and reducing reward hacking."
        },
        {
            "title": "4 CONCLUSION",
            "content": "We presented Multi-Reward cOnditioning (MIRO), simple pretraining framework that conditions on vector of reward scores to integrate alignment into training rather than as post-hoc stage. By learning p(x c, s) and exposing reward targets as controllable inputs, MIRO disentangles content from quality, offering precise and interpretable control at inference time. Empirically, on 16M-image setup, MIRO outperforms no-conditioning and single-reward baselines, converges substantially faster, mitigates reward hacking, strengthens compositional alignment, and achieves state-of-the-art results on PartiPrompts with inference-time scalingwhile being markedly more compute-efficient. Notably, despite being much smaller, our MIRO model surpasses FLUX-dev on GenEval and PartiPrompts at fraction of the compute. We hope that this work will pave the way for this alternative line of research on how to exploit rewards at pre-training. Acknowledgements. This work was supported by ANR project TOSAI ANR-20-IADJ-0009, CIEDS, Hi!Paris grant and was granted access to the HPC resources of IDRIS under the allocation 2024-A0171014246 made by GENCI. We would like to thank Alyosha Efros, Tero Karras and Luca Eyring for their helpful comments and Yuanzhi Zhu and Xi Wang for proofreading."
        },
        {
            "title": "REFERENCES",
            "content": "Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow: Differentiating through flows for controlled generation. In arXiv preprint, 2024. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. OpenAI, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In CVPR, 2024. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pp. 35583568, 2021. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, 2024a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024b. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Giannis Daras, Adrian Rodriguez-Munoz, Adam Klivans, Antonio Torralba, and Constantinos Daskalakis. Ambient diffusion omni: Training good models with bad data. arXiv preprint arXiv:2506.10038, 2025. Lucas Degeorge, Arijit Ghosh, Nicolas Dufour, David Picard, and Vicky Kalogeiton. How far can we go with imagenet for text-to-image generation? arXiv preprint, 2025. Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, and Tingbo Hou. Prdp: Proximal reward difference prediction for large-scale reward finetuning of diffusion models. In CVPR, 2025. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021. Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky T. Q. Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. In ICLR, 2025. Nicolas Dufour, Victor Besnier, Vicky Kalogeiton, and David Picard. Dont drop your samples! coherence-aware training benefits conditional diffusion. CVPR, 2024. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICLR, 2024. Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. NeurIPS, 2024. Luca Eyring, Shyamgopal Karthik, Alexey Dosovitskiy, Nataniel Ruiz, and Zeynep Akata. Noise hypernetworks: Amortizing test-time compute in diffusion models. NeurIPS, 2025. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Ddpok: Reinforcement learning for fine-tuning text-to-image diffusion models. In NeurIPS, 2023. 12 Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: Open diffusion models trained on creative-commons images. In CVPR, 2024. Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind, and Navdeep Jaitly. Matryoshka diffusion models. In ICLR, 2023. Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. Proc. EMNLP, 2020. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. Emiel Hoogeboom, Jonathan Heek, , and Tim Salimans. Simple diffusion: End-to-end diffusion models for image generation. In Proc. ICML, 2023. Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In Proc. ICML, 2023. Zhiwei Jia, Yuesong Nan, Huixi Zhao, and Gengdai Liu. Reward fine-tuning two-step diffusion models via learning differentiable latent-space surrogate reward. In CVPR, 2024. Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. NeurIPS, 2024. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-apic: An open dataset of user preferences for text-to-image generation. NeurIPS, 2023. Andreas Koukounas, Georgios Mastrapas, Michael Günther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Martínez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, and Han Xiao. Jina clip: Your clip model is also your text retriever. arXiv preprint, 2024. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Jialuo Li, Wenhao Chai, Xingyu Fu, Haiyang Xu, and Saining Xie. Science-t2i: Addressing scientific illusions in image synthesis. CVPR, 2025. Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility. In NeurIPS, 2024. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Vqascore: Evaluating text-to-visual generation with image-to-text vqa. ECCV, 2024. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint, 2023. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint, 2023. Yihong Luo, Tianyang Hu, Weijian Luo, Kenji Kawaguchi, and Jing Tang. Reward-instruct: reward-centric approach to fast photo-realistic image generation. arXiv preprint, 2025. Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. ECCV, 2024. Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Saining Xie. Inference-time scaling for diffusion models beyond scaling denoising steps. CVPR, 2025. 13 Zichen Miao, Zhengyuan Yang, Kevin Lin, Ze Wang, Zicheng Liu, Lijuan Wang, and Qiang Qiu. Tuning timestep-distilled diffusion model using pairwise sample optimization. In arXiv preprint, 2024. Owen Oertell, Jonathan D. Chang, Yiyi Zhang, Kianté Brantley, and Wen Sun. Rl for consistency models: Faster reward guided text-to-image generation. In arXiv preprint, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. In NeurIPS, 2023. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. Noam Shazeer. Glu variants improve transformer. arXiv preprint, 2020. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. Proc. ICML, 2015. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS, 2019. Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. NeurIPS, 2020. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. ICLR, 2021. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint, 2023. Wenpin Tang and Fuzhong Zhou. Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond. In arXiv preprint, 2025. Zhiwei Tang, Jiangweizhi Peng, Jiasheng Tang, Mingyi Hong, Fan Wang, and Tsung-Hui Chang. Inference-time alignment of diffusion models with direct noise optimization. In arXiv preprint, 2024. Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. In Communications of the ACM, 2016. 14 Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropy-regularized control. In arXiv preprint, 2024. Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, and Tommaso Biancalani. Inference-time alignment in diffusion models with reward-guided generation: Tutorial and review. In arXiv preprint, 2025. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, 2024. Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoders. In ICCV, 2023. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint, 2023. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. ICLR, 2024. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and leveraging human preferences for text-to-image generation. NeurIPS, 2023. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint, 2022. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv, 2024. Biao Zhang and Rico Sennrich. Root mean square layer normalization. NeurIPS, 2019. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023."
        },
        {
            "title": "A RELATED WORK",
            "content": "A.1 DIFFUSION, FLOW MATCHING, AND CONDITIONAL GENERATION Modern T2I builds on diffusion/score models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021; Song & Ermon, 2019; 2020; Dhariwal & Nichol, 2021) and their latent and text-conditional variants (Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022). Transformer-based diffusion backbones (Peebles & Xie, 2023; Ma et al., 2024) and flow matching (Lipman et al., 2023) further improve scalability and training dynamics. Alternative backbones and simplifications broaden design space (Peebles & Xie, 2023; Hoogeboom et al., 2023; Jabri et al., 2023), while control modules and training/sampling variants provide additional levers (Zhang et al., 2023; Song et al., 2023; Luo et al., 2023). Practical recipes add stable normalization/activation and attention scaling (Zhang & Sennrich, 2019; Shazeer, 2020; Henry et al., 2020). Coherence-aware conditioning improves conditional generation without discarding data (Dufour et al., 2024) and Ambient Diffusion Omni (Daras et al., 2025) improves training with bad data. A.2 EFFICIENT TEXT-TO-IMAGE GENERATION Efficiency arises from data, objectives, and architectures. Compact, public datasets enable reproducible training with lower compute (Changpinyo et al., 2021; Schuhmann et al., 2022; Thomee et al., 2016; Gokaslan et al., 2024; Deng et al., 2009; Degeorge et al., 2025). Representation-focused objectives and recipes accelerate convergence (Wei et al., 2023; Yu et al., 2024). Transformer-based diffusion/flow models (Peebles & Xie, 2023; Ma et al., 2024) and latent training (Rombach et al., 2022; Chen et al., 2024b; Gu et al., 2023) reduce cost while preserving quality; large-scale systems highlight the upper bound in capability and compute (Betker et al., 2023; Esser et al., 2024). Coherence-aware training further improves sample efficiency without filtering (Dufour et al., 2024). A.3 ALIGNING T2I MODELS WITH REWARD SIGNALS AND TEST-TIME SCALING Reward models span complementary axes for alignment and evaluation (AestheticScore, HPSv2, ImageReward, PickScore, VQAScore, JINA CLIP, SciScore) (Schuhmann et al., 2022; Wu et al., 2023; Xu et al., 2023; Kirstain et al., 2023; Lin et al., 2024; Koukounas et al., 2024; Li et al., 2025). Training-time alignment either fine-tunes diffusion models with reward feedback via RL (Black et al., 2024; Fan et al., 2023; Deng et al., 2025)effective but compute-heavy and sometimes unstableor learns from preferences using pairwise objectives such as DPO (Rafailov et al., 2023; Wallace et al., 2024; Li et al., 2024). Control-theoretic formulations optimize continuous-time dynamics with reward guidance (Uehara et al., 2024; Tang & Zhou, 2025; Domingo-Enrich et al., 2025) but are costly; lighter approaches avoid full trajectory gradients (Oertell et al., 2024; Miao et al., 2024; Jia et al., 2024). At inference, test-time scaling boosts rewards via sample-and-select (Ma et al., 2025; Uehara et al., 2025) or reward-guided refinement (Ben-Hamu et al., 2024; Tang et al., 2024), at higher runtime cost. Complementary gradient-based alignment optimizes the initial noise using reward gradients (ReNO) and amortizes such test-time compute via Noise Hypernetworks (Eyring et al., 2024; 2025). MIRO conditions on multiple rewards during pretraining, enabling controllable trade-offs and strong single-sample quality, and complements test-time scaling by achieving higher scores with fewer samples and better GenEval alignment (Ghosh et al., 2024). Trading-off multiple rewards during inference has been explored by weight averaging methods like Rewarded Soups (Rame et al., 2023) but this approaches requires 1 model per reward, making it impractical for large number of rewards. Furthermore, changing the mix of rewards at inference time requires changing the model averaging parameters, which requires having all the models in memory."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "Architecture Modifications. Our flow matching network vθ takes as input the noisy sample xt, text condition c, and the binned reward vector ˆs = [ˆs1, ˆs2, . . . , ˆsN ]. The reward conditioning is implemented through: Sinusoidal embeddings: Each reward bin index ˆsi is encoded using sinusoidal position embeddings, similar to those used in transformer architectures 16 Token space mapping: The sinusoidal reward embeddings are projected to the same dimensional space as text tokens Token concatenation: The projected reward embeddings are concatenated to the text token sequence, allowing the model to process rewards and text through the same attention mechanism Rewards preprocessing For each reward model rj, we: 1. Compute scores on representative subset Dcal of the training data 2. Sort the scores and divide them into bins with equal population 3. Map each raw score s(i) to its corresponding bin index ˆs(i) {0, 1, . . . , 1} This binning approach provides several advantages: (1) it normalizes different reward scales into common discrete space, (2) ensures balanced training across all quality levels, and (3) provides interpretable conditioning signals where higher bin indices correspond to better quality. Experimental Setup We used the TextRIN architecture Dufour et al. (2024) with several modifications: FFN layers replaced with SwiGLU Shazeer (2020), LayerNorm replaced with RMSNorm Zhang & Sennrich (2019), and QK-Norm Henry et al. (2020) in attention mechanisms. We employed flow matching instead of diffusion for generation. Models were trained for 500k steps with batch size 1,024 and learning rate 1e-3. We train our model in 256px resolution. We combined CC12M Changpinyo et al. (2021) and LAION Aesthetics 6+ Schuhmann et al. (2022) for 16M total image-text pairs, following Dufour et al. (2024). We used seven reward models for MIRO: Aesthetic Score Schuhmann et al. (2022) for visual appeal, HPSv2 Wu et al. (2023) for human preference alignment, ImageReward Xu et al. (2023) for text-image correspondence and user preference, PickScore Kirstain et al. (2023) for user preference, VQAScore Lin et al. (2024) for visual comprehension, JINA CLIP Score Koukounas et al. (2024) for long captions CLIP score, and SciScore Li et al. (2025) for scientific accuracy. SOTA Baselines We compare MIRO against the following baselines: SD v1.5: Rombach et al. (2022) SD v2.1: Rombach et al. (2022) PixArt-α: Chen et al. (2024b) PixArt-Σ: Chen et al. (2024a) CAD: Dufour et al. (2024) Sana-0.6B: Xie et al. (2024) Sana-1.6B: Xie et al. (2024) SDXL: Podell et al. (2024) FLUX-dev: Labs (2024) SD3-Medium: Esser et al. (2024)"
        },
        {
            "title": "C ADDITIONAL RESULTS",
            "content": "Aesthetic Score CLIP Score HPSv2 Score a a 5 0 0 a a 0.28 0. 0.24 0.22 a a 0.2 0. 0.16 0.3 0.28 0.26 0.24 a d e 10 20 CFG rate (a) 30 0 10 20 CFG rate (b) 0 10 20 CFG rate (c) 30 OpenAI CLIP Score Pick Score Image Reward 0.22 0.22 0.21 0.21 a a 10 20 CFG rate (d) 30 0 10 20 CFG rate (e) 30 a a 0.5 0 0.5 0 10 20 CFG rate (f) image_reward hpsv2 aesthetic miro sciscore clip vqa Pick Baseline Figure 10: Score plots for different reward functions: (a) Aesthetic Score, (b) CLIP Score, (c) HPSv2 Score, (d) OpenAI CLIP Score, (e) Pick Score, and (f) Image Reward. Each plot shows all models color-coded according to the legend. MIRO vs. ImageReward Two Objects 14 44 75 43 29 76 20 Counting MIRO vs. CLIP Two Objects 44 75 43 29 65 20 Position 50 80 Colors Position 50 80 Colors 100 60 60 Single Object Position Color Attribution Colors Single Object Position Color Attribution Colors 50 80 80 MIRO vs. HPSv2 Two Objects 14 44 43 29 65 76 20 Counting MIRO vs. VQA Two Objects 14 44 75 29 65 76 20 100 100 60 Single Object Position Color Attribution Colors Single Object Position Color Attribution Colors 50 50 80 MIRO vs. Aesthetic Two Objects MIRO vs. SciScore Two Objects 14 44 75 43 29 76 20 Counting MIRO vs. Pick Two Objects 44 75 43 29 65 20 100 60 100 60 Single Object Position Color Attribution Colors Single Object Position Color Attribution Colors 50 80 50 80 44 75 43 29 65 20 Counting MIRO vs. Baseline Two Objects 14 75 43 29 65 76 Single Object 100 60 Color Attribution Single Object 60 Color Attribution Counting Counting Counting Counting Figure 11: Comparison of the MIRO model against eight other specialist/baseline models on GenEval metrics. Each radar plot shows the MIRO model (orange) versus comparison model across six GenEval categories: Single Object, Two Objects, Position, Counting, Colors, and Color Attribution. Scores range from 0 to 100 for all categories. Min and max values on each axis show the range of actual metric scores and are consistent across all plots. 18 r l n 76 72 70 68 66 0 max at 0. Synth MIRO 0.125 0.25 0.375 0.5 Aesthetic weight in prompt 0.625 0.75 0.875 1.0 Figure 12: GenEval Overall vs aesthetic prompt weight for Synth MIRO. We vary the positive target ˆs+ aesthetic while keeping the other components of ˆs+ equal to 1 and ˆs fixed. Higher is better. t s 6 5 4 1.2 1. 1 0.9 w g 0.3 0. 0.26 2 H 0 0.875 0.625 0.375 0.125 0.75 0.25 1.0 0.5 Aesthetic weight 0 0.875 0.625 0.375 0.125 0.75 0.25 1.0 0.5 Aesthetic weight 0 0.875 0.625 0.375 0.125 0.75 0.25 1.0 0.5 Aesthetic weight (a) Aesthetic (b) ImageReward (c) HPSv2 c i 0.22 0.22 0.21 c L 0.21 0. 0.29 0.28 0.27 C e 0 0.8751.0 0.3750.5 0.625 0.125 0.75 0.25 Aesthetic weight 0 0.875 0.625 0.375 0.125 0.75 0.25 1.0 0.5 Aesthetic weight 0 0.875 0.625 0.375 0.125 0.75 0.25 1.0 0.5 Aesthetic weight (d) Pick (e) CLIP (f) OpenAI CLIP Figure 13: Six metrics vs aesthetic prompt weight for Synth MIRO. Each subplot shows the metric value over aesthetic weight in the prompt."
        },
        {
            "title": "D CAPTIONING DETAILS",
            "content": "We caption images using the model google/gemma-3-12b-it available on HuggingFace. To generate long captions, we use the following prompt: \"Analyze the following image in detail. Identify all prominent objects, their attributes (color, material, shape, size, texture), their spatial relationships, the overall scene and setting, the lighting conditions, and any relevant style or composition details.\" \"Based on your analysis, generate caption of the image. It should be descriptive enough to allow diffusion model to accurately reconstruct the image. Include specific details rather than general descriptions. For example, instead of blue car, describe it as shiny, dark blue vintage sedan with chrome bumpers parked on cobblestone street. \"Please ensure the caption is enclosed within <CAPTION> and </CAPTION> tags. \" 19 7 6 4 c t s r P I p 0. 0.2 (a) (b) (c) c L 0. 0.15 0 1 2 3 Train steps 4 5 105 0 1 2 3 Train steps 4 5 105 c 2 H 0.3 0.25 0.2 0. 0 1 2 3 Train steps 4 5 (d) (e) (f) c i 0. 0.21 0.2 0.19 0 1 3 Train steps 4 5 105 0 1 3 Train steps 4 5 105 w g 0 1 2 0 1 3 Train steps 4 5 105 baseline image_reward hpsv aesthetic miro sciscore clip vqa pick Figure 14: Training curves for different reward functions: (a) Aesthetic Score, (b) CLIP Score, (c) HPSv2 Score, (d) OpenAI CLIP Score, (e) Pick Score, and (f) Image Reward. Each plot shows the reward value progression across Train steps for different models including image_reward, hpsv2, aesthetic, miro, sciscore, clip, vqa, and pick. \"Example of lengths for the caption:\" \"<CAPTION> plump gray domestic shorthair cat with symmetrical white paws sleeps curled into tight circle on sunlit oak windowsill, its body occupying about two-thirds of the surface. The cats head rests on its hind legs, with its tail wrapped neatly around its body. The windowsill shows distinct wood grain patterns and sun-bleached patch where sunlight consistently hits. To the left, semi-sheer white lace curtains with small floral pattern hang from wooden rod, partially billowing inward from 30-centimeter-wide open window that reveals an out-of-focus garden with green foliage. On round wooden side table to the right, transparent glass vase holds five pink peonies and three white snapdragons in water, with visible pollen grains floating on the surface. The tables surface shows faint circular water stains and light dusting of pollen. Behind the table, an armchair with beige linen upholstery features folded gray knit blanket draped over its back. vintage wall clock with Roman numerals and brass hands is mounted above the windowsill. Sunlight streams through the window. </CAPTION> \" \"<CAPTION> Space Gray iPad Pro displays vibrant beach sunset, positioned on rustic walnut table. The attached Magic Keyboard is folded back, and Apple Pencil rests diagonally across an open leather folio case, revealing its suede-lined interior. To the left, double-walled glass mug of black coffee sits on cork coaster with thin ring of condensation and light sprinkle of cinnamon on the foam. small ceramic pot contains jade pothos plant with six visible leaves, two of which trail over the tables edge. The tables surface shows natural wood grain variations, including dark, heart -shaped knot near the center. In the background, mid-century modern sofa in teal velvet has two throw pillows with geometric patterns. 20 bookshelf against the far wall holds mix of books, brass desk lamp, and stacked stone decoration. Natural light filters through casement window with slightly wavy glass panes, creating visible light refractions on the table. ceiling fan casts moving shadows, and seashell wind chime hangs outside the window, occasionally tinkling in the breeze.</CAPTION>\" \"<CAPTION> rectangular farmhouse table is covered with pressed linen tablecloth (ivory with subtle gray stripes) and meticulously set for eight guests. Each place setting includes plate with Wild Strawberry pattern, five-piece sterling silver flatware set, an water goblet, and wine glass, all arranged with precise alignment. cloth napkin is folded into rectangle and tied with burgundy silk ribbon. The centerpiece is floral arrangement in mercury glass compote, featuring six red roses, four white peonies, eight pine sprigs, and three cinnamon sticks. Eight tapered candles in brass holders are placed among the flowers. Wooden dining chairs with navy velvet upholstery have wool throws draped over their backs. wrought iron chandelier with six Edison bulbs hangs above the table, casting warm light that reflects off the crystal glassware. The walls are adorned with cedar garlands embedded with 50 white fairy lights, and three framed botanical prints hang in horizontal row. In the background, fireplace with visible flame and stack of birch logs adds warmth to the scene. The air smells faintly of pine, cinnamon, and beeswax polish.</CAPTION> \" \"An alt-text corresponding to the image is: <ALT-TEXT> </ALT-TEXT>\" To compute reward scores, we generate short captions of the images. We use the following prompts : \"Generate short caption of the image. Please ensure the caption is enclosed within <CAPTION> and </CAPTION> tags. \" \"Example of lengths for the caption:\" \"<CAPTION>A cat sleeping on windowsill.</CAPTION> \" \"<CAPTION>A beautiful sunset over the mountains with clear sky.</ CAPTION> \" \"<CAPTION>A group of people enjoying picnic in the park on sunny day .</CAPTION> \" \"<CAPTION>A boy playing with ball in the backyard.</CAPTION> \" Table 2 shows examples of long and short captions."
        },
        {
            "title": "E TRAINING PROGRESSION ADDITIONAL EXAMPLES",
            "content": "21 Images Captions Short: modern living room features gray sofa, vibrant watercolor garden print, and soft throws under minimalist floor lamp. Long: bright, contemporary living room showcases spacious gray three-seater sofa with subtly textured, linen-blend upholstery, positioned against pristine white wall. The wall is adorned with large, rectangular canvas print displaying vibrant, watercolor-inspired illustration of whimsical garden scene, featuring diverse array of colorful birds (including blue, orange, yellow, and green-bodied birds) amidst pink, yellow, and white flowers and lush greenery. tall, minimalist black floor lamp with geometric, faceted metal shade casts soft light onto the light gray hardwood floor to the right of the sofa. dark blue knitted throw is casually draped over the sofas left armrest, and lighter blue/gray throw is folded on the right. The overall aesthetic is clean, modern, and inviting, highlighting the artwork as striking focal point in minimalist setting. Short: scowling Santa in velvet suit glares intensely from inside wooden hut. Long: strikingly serious portrait of man portraying Santa Claus, captured in tight close-up from within rustic wooden sauna. He is attired in bright red Santa suit made of textured velvet-like fabric, complete with white fur trim around the collar and cuffs, and traditional conical hat featuring large, plush white pom-pom. His long, thick, and unkempt white beard covers significant portion of his face. His dark, bushy eyebrows are heavily furrowed, conveying palpable sense of discontent or annoyance, and his dark eyes gaze directly at the viewer with alarming intensity. The sauna is constructed from light-colored pine planks, exhibiting natural wood grain and slightly rough texture, creating warm but somewhat enclosed feeling. Dramatic directional lighting from the left aggressively illuminates his face, casting heavy shadows to the right, accentuating the wrinkles and emphasizing the seriousness of his expression. The overall effect is jarring juxtaposition of the familiar Christmas icon with an unsettling and unexpected mood, suggesting Santa Claus far removed from the joyful spirit typically associated with the holiday. Short: Newlyweds share tender embrace on lush green lawn, she in lace and flowers, he in navy and pink. Long: heartwarming candid moment featuring bride and groom embracing on vibrant green lawn, set before stately two-story white house constructed in classic colonial architectural style with dark blue, evenly spaced shutters. The bride has light brown hair elegantly styled in an updo accented with small white floral detail. She wears flowing white wedding dress with delicate lace overlay and low, open back, revealing glimpse of her skin. Her arms are wrapped tightly around her groom, who is dressed in navy blue suit, crisp white dress shirt, and light pink tie. shiny silver wedding band adorns his left ring finger. The bride holds bouquet consisting of mix of white and pale pink roses interspersed with lush greenery. The background features meticulously trimmed hedges and mature tree with thick, textured gray trunk. Soft, diffused natural light bathes the scene, creating gentle shadows across the lawn. The overall impression is one of joy, love, and timeless elegance, characteristic of wedding day celebration. Table 2: Example of captions used in the training set. 22 Step 25k Step 50k Step 75k Step 100k Step 200k Step 400k l B M i a I i a I i a O \"a panda\" \"a city\" \"a clock tower\" \"a ladder\" Figure 15: Additional training progression examples showing generated images at different training steps. Each row pair shows baseline (top) and MIRO (bottom) model outputs for the same prompt across training. 23 Step 25k Step 50k Step 75k Step 100k Step 200k Step 400k \"a taxi\" \"an elephant\" \"A dignified beaver wearing glasses, vest, and colorful neck tie. He stands next to tall stack of books in library.\" l B I i a I i a I e e O \"A raccoon wearing formal clothes, wearing tophat and holding cane. The raccoon is holding garbage bag. Oil painting in the style of abstract cubism.\" Figure 16: Additional training progression examples showing generated images at different training steps. Each row pair shows baseline (top) and MIRO (bottom) model outputs for the same prompt across training."
        }
    ],
    "affiliations": [
        "AMIAD",
        "LIGM, ENPC, IP Paris, CNRS, UGE",
        "LIX, École Polytechnique, IP Paris"
    ]
}