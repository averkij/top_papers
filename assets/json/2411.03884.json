{
    "paper_title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
    "authors": [
        "Zhijian Zhuo",
        "Ya Wang",
        "Yutao Zeng",
        "Xiaoqing Li",
        "Xun Zhou",
        "Jinwen Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the $\\textbf{optimal approximation rate}$, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 4 8 8 3 0 . 1 1 4 2 : r Under Peer Review. POLYNOMIAL COMPOSITION ACTIVATIONS: UNLEASHING THE DYNAMICS OF LARGE LANGUAGE MODELS Zhijian Zhuo12 Ya Wang2 Yutao Zeng2 Xiaoqing Li3 Xun Zhou2 Jinwen Ma1 1 School of Mathematical Sciences, Peking University 2 Seed-Foundation-Model, ByteDance 3 Capital University of Economics and Business"
        },
        {
            "title": "ABSTRACT",
            "content": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom. Figure 1: Training loss, validation perplexity (PPL), and downstream performance of 1B dense models. We compare models employing different activation functions, including SwiGLU, GELU, ReLU, PolyReLU, and PolyNorm. It indicates that models using PolyReLU and PolyNorm exhibit lower training loss and validation PPL, alongside better downstream performance."
        },
        {
            "title": "INTRODUCTION",
            "content": "Transformers (Vaswani et al., 2017) have revolutionized the field of deep learning, facilitating unprecedented advancements in natural language processing (Radford et al., 2019), computer vision Corresponding author: Yutao Zeng (yutao.zeng@outlook.com) and Jinwen Ma (jwma@math.pku.edu.cn). Equal contribution. 1 Under Peer Review. (Dosovitskiy et al., 2021), and beyond (Dong et al., 2018; Arnab et al., 2021). Characterized by their attention mechanisms, transformers excel at capturing intricate relationships within data, making them indispensable in contemporary machine learning applications. However, despite their widespread success, there remain opportunities for further refinement, particularly concerning the selection of activation functions. The activation function plays crucial role in determining the output of each neuron within neural network. Traditionally, simple nonlinearities such as Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) and its variants (Hendrycks & Gimpel, 2016; So et al., 2021) have been favored due to their computational efficiency and ease of implementation. Although effective, these activation functions are inherently limited in their ability to model complex higher-order relationships within data. This limitation can be particularly restrictive in transformer architectures, where the ability to capture subtle and complex dependencies is essential. In this paper, we introduce novel category of polynomial composition activation functions (PolyCom), specifically engineered to enhance the performance of transformer architectures. In contrast to conventional activation functions, which are predominantly linear or piecewise linear, polynomial composition activations facilitate the modeling of more complex patterns within data. This augmentation in the activation functions expressiveness endows the model with superior expressive capacity, enabling it to capture higher-order interactions that might otherwise be neglected. Unlike other forms of polynomials ((Hornik et al., 1989; Trefethen, 2019)) that suffer from inadequate approximation, exploding values, and oscillatory behavior, we demonstrate that PolyCom possesses more potent expressive capability than both ReLU and traditional polynomials and achieves optimal approximation within Sobolev space. We posit that the integration of polynomial composition activations within transformer models can lead to enhanced performance in tasks requiring intricate data interpretation. To evaluate this hypothesis, we conducted comprehensive experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. These evaluations were performed across various benchmarks, assessing the performance of transformers employing polynomial composition activations in comparison to those utilizing traditional activation functions. The results indicate that the proposed method not only improves model accuracy, but also accelerates convergence rates, thereby suggesting that polynomial composition activations provide substantive advantage in deep learning applications. The main contributions of this paper are summarized in the following. We propose new activation function PolyCom which is composition of the polynomial and other types of function. In particular, we introduce two instances of PolyCom: PolyReLU and PolyNorm, and details its integration into the transformer architecture. Theoretically, we derive bounds on the number of trainable parameters required for PolyReLU networks to approximate ReLU networks, and vice versa. Additionally, we show that PolyReLU network of size O(ϵd/n) can approximate any function in Sobolev spaces with error tolerance ϵ, achieving optimal approximation rates. Empirically, we validate the effectiveness of this new activation function on LLMs with both 1B dense models and MoE models with 1B active and 7B total parameters. The results of both models demonstrate that PolyCom can accelerate the converging speed and significantly outperform SwiGLU, GELU, and ReLU et al. The outline of this paper is structured as follows: In Section 2, we present the mathematical formulation of PolyCom and discuss its integration within transformer architectures. Section 3 delivers comprehensive theoretical analysis of PolyCom, emphasizing its enhanced expressivity and effectiveness. In Section 4, we provide detailed account of our experimental results involving large language models (LLMs). Section 5 provides an overview of related work in the field of activation functions and their applications in transformer models. Finally, we conclude the paper and outline potential directions for future research."
        },
        {
            "title": "2 POLYNOMIAL COMPOSITION ACTIVATION FUNCTION",
            "content": "In this section, we present the mathematical formulation of the polynomial composition activation function (PolyCom) and detail its integration into the transformer architecture. 2 Under Peer Review. Figure 2: Block diagrams of Transformer MLP blocks utilizing ReLU/GELU, SwiGLU, PolyReLU and PolyNorm. FC stands for Fully Connected layer. xi represents the i-th power of the input tensor x, while aj denotes the j-th element of the learnable weight vector a. indicates normalization operation. PolyCom. The study of the polynomial activation function can be traced back to the seminal work of Hornik et al. (1989), which showed that neural networks with polynomial activation are not dense within the space of continuous functions. Additionally, empirical evidence has shown that deep neural networks employing pure polynomial activations tend to underperform (Trefethen, 2019). To overcome these limitations, we propose PolyCom, novel composition of polynomial and other functions. Specifically, we explore two composition approaches: (cid:26)Type I: (cid:55) (cid:80)r Type II: (cid:55) (cid:80)r i=0 aiρi(x), i=0 aiρ(xi), ai R, (1) where denotes the order of PolyCom and ρ represents an arbitrary functions such as ReLU, PReLU, Sigmoid, SiLU, or normalization. The key distinction between the two approaches lies in whether the function is composed before or after the power operation. It can be theoretically shown that both approaches have equivalent expressivity, provided that ρ is non-linear function. This is because polynomial terms are symmetric with respect to composition, allowing both Type and Type II to approximate similar function classes. In other words, rearranging the order of ρ and the polynomial powers does not affect the ability to approximate complex non-linear functions. In practice, we use third-order PolyCom (r = 3) with trainable coefficients ai. For initialization, we set ai = 1/r for = 1, 2, . . . , and a0 = 0. For Type PolyCom, we specifically consider composition involving the ReLU function due to its simplicity, which we term PolyReLU. An r-order PolyReLU is defined as: PolyReLU(x) = (cid:88) i=0 aiReLUi(x), (2) whereReLUi(x) = max{x, 0}i. This formulation can be seen as an extension of both ReLU and square ReLU. For Type II PolyCom, we introduce PolyNorm, which normalizes the powers to ensure consistent magnitudes across terms: PolyNorm(x) = (cid:88) i=0 ai xi xi2 , (3) where xi = [xi normalization. 1, xi 2, , xi d] represents element-wise exponentiation, and 2 denotes the L2 Integration into Transformer. The transformer architecture (Vaswani et al., 2017) consists of two alternating modules, Multi-Head Attention (MHA) and position-wise Feed-Forward Networks (FNN). Activation functions predominantly influence the performance of FFN layers. We begin by formalizing the common paradigm of FFN, FFNρ(x) = ρ(xW1)W2 (4) where ρ represents the activation function such as ReLU, GeLU, PolyReLU, or PolyNorm. We replace the traditional activation function with our proposed PolyCom variants to enhance model capacity and performance, as illustrated in Figure 2. 3 Under Peer Review."
        },
        {
            "title": "3 THEORETICAL ANALYSIS",
            "content": "As discussed in Section 2, PolyReLU and PolyNorm have equivalent expressivity. To streamline the analysis, we focus solely on the theoretical properties of PolyReLU, specifically its expressivity and effectiveness. Additional, nonlinear activations such as GeLU and SwiGLU can be locally approximated by Taylor polynomials around the origin, which allows us to primarily compare PolyReLU with ReLU and polynomial activations. To avoid confusion, we refer to networks that use ReLU activations as ReLU networks, and those that use PolyReLU activations as PolyReLU networks."
        },
        {
            "title": "3.1 APPROXIMATING RELU NETWORKS BY POLYRELU\nIn this subsection, we present theoretical results on approximating ReLU networks using PolyReLU\nnetworks. The following lemma shows that ReLU, ReLU2, and polynomial activation are special\ncases of PolyReLU activation, highlighting the superior expressivity of PolyReLU. This implies\nthat PolyReLU has stronger approximation abilities with fewer trainable parameters compared to\nReLU and other polynomial activations.",
            "content": "Lemma 1. ReLU, ReLU2 and polynomial activation can be represented by PolyReLU. Proof of Lemma 1. For ReLU activation, set a1 = 1, ai = 0, = 1, leading to PolyReLU(x) = ReLU(x). For ReLU2 activation, set a2 = 1, ai = 0, = 2, giving PolyReLU(x) = ReLU2(x). For general polynomial activation, observe that for and N: xi = ReLUi(x) + (1)iReLUi(x), R, N. Thus, for any polynomial activation of order r, where PolyReLU1(x) = (cid:80)r Poly(x) = PolyReLU1(x) + PolyReLU2(x), i=0 aiReLUi(x) and PolyReLU2(x) = (cid:80)r i=1(1)iaiReLUi(x). (5) (6) Building on Lemma 1, we can formally prove that any ReLU network can be exactly represented by PolyReLU network of the same size, as stated in the following theorem. Theorem 1. Let : [1, 1]d [1, 1] be ReLU network with depth and width K. Then, there exists PolyReLU network : [1, 1]d [1, 1] of size O(LK) such that (x) = g(x), forx [1, 1]d. (7) This theorem, proved in Appendix A, shows that PolyReLU networks can exactly match the representational power of ReLU networks without increasing the model size. 3.2 APPROXIMATING POLYRELU WITH RELU NETWORKS In this part, we give theoretical results on approximating PolyReLU networks using ReLU networks. The following Lemma 2 demonstrates that the PolyReLU activation can be approximated by ReLU network within given error tolerance. Lemma 2. For the activation PolyReLU(x) = (cid:80)r i=0 aiReLUi(x), [1, 1] with ai [1, 1]. Given any ϵ (0, 1), there exists ReLU network : [1, 1] [1, 1] with size O(ln2(1/ϵ)), such that max x[1,1] (x) PolyReLU(x) < ϵ. (8) The proof is provided in the Appendix A. Lemma 2 establishes an upper bound on the size of ReLU network needed to approximate PolyReLU activation function. This result highlights that while ReLU networks can approximate PolyReLU activations, they require significantly larger number of parameters. 4 Under Peer Review. Building on Lemma 2, we derive the following theorem, which provides both upper and lower bounds for approximating PolyReLU networks with ReLU networks. Theorem 2. Let : [1, 1]d [1, 1] be PolyReLU network with depth and width K, and PolyReLU activation with order and Lipschitz constant α. Suppose each neuron computes (cid:55) PolyReLU(ax + b) with the pair (a, b) satisfies a1 + 1 and PolyReLU : [1, 1] [1, 1] (a, b, and PolyReLU are possibly distinct across neuron). For any given ϵ (0, 1), there exists ReLU network : [1, 1]d [1, 1] of size such that (cid:18) LK ln (cid:18) LαL ϵ (cid:19)(cid:19) , max x[1,1]d (x) g(x) < ϵ. (9) (10) Conversely, the size of any ReLU network that approximates PolyReLU network within tolerance ϵ, must be at least (cid:18) (cid:19)(cid:19) (cid:18) 1 ϵ Ω KL ln . (11) Theorem 2 tells us that the total number of trainable parameters required by ReLU networks to approximate PolyReLU neural network within tolerance of ϵ is O(ln2(1/ϵ)). Conversely, there exists PolyReLU network that can not be approximated by ReLU networks of size less than Ω(ln(1/ϵ). Combined with theorem 1, we conclude that PolyReLU networks are more efficient in terms of representational capacity than ReLU networks. 3.3 APPROXIMATION OF GENERAL SMOOTH FUNCTION Similar to Yarotsky (2017); Boulle et al. (2020), we also explore the universal approximation capabilities of PolyReLU networks in the context of Sobolev spaces (Adams & Fournier, 2003). Specifically, we show that PolyReLU networks achieve the optimal approximation rate within these spaces, meaning that PolyReLU networks require minimum parameters to approximate general smooth functions in Sobolev spaces, compared with networks with the other activation. The definition of Sobolev space n, (cid:0)[1, 1]d(cid:1) is stated below. The set [1, 1]d can be replaced by any compact set in Rd, we use it just for the sake of brevity. Definition 1 (Sobolev Spaces). For n, N, Sobolev space n, (cid:0)[1, 1]d(cid:1) is defined as n, (cid:0)[1, 1]d(cid:1) = (cid:8)f (cid:0)[1, 1]d(cid:1) W n,([1,1]d) < (cid:9) , with the norm which defined as the following W n,([1,1]d) = max n:n1n ess sup x[1,1]d Dnf (x) , (12) (13) where Nd and Dnf is the respective weak derivative of , and ess sup means the essential supremum in functional analysis. Intuitively, Sobolev space is space of functions endowed with weaker notion of smoothness compared to differentiability and possessing generalized derivatives. The Sobolev space n, (cid:0)[1, 1]d(cid:1) contains functions from n1 (cid:0)[1, 1]d(cid:1) which consists of functions whose derivatives of order 1 are Lipschitz continous. In the sequel, we mainly consider the unit ball within n, (cid:0)[1, 1]d(cid:1), which is defined as follows Fn,d = {f n, (cid:0)[1, 1]d(cid:1) W n,([1,1]d) 1}. With the above definitions established, we can present the following main results. We provide an upper bound on the size of PolyReLU networks required to approximate any function in Fn,d. 5 Under Peer Review. Theorem 3. Suppose that d, and ϵ (0, 1). For any Fd,n, there exists PolyReLU network with size O(ϵd/n) that can approximate at given error tolerance ϵ, i.e., max x[1,1]d (x) g(x) < ϵ. (14) Theorem 3 indicates that PolyReLU networks can achieve an optimal approximation rate of O(ϵd/n). In contrast, previous works by Yarotsky (2017) demonstrated that ReLU networks require O(ϵd/n ln(1/ϵ)) parameters to achieve similar approximation error. Similarly Boulle et al. (2020) showed that rational neural networks need O(ϵd/n ln(ln(1/ϵ))) parameters for the same task. Therefore, the approximation ability of PolyReLU networks is superior to that of both ReLU networks and rational networks. Furthermore, Theorem 4.2 in DeVore et al. (1989) shows that the total number of parameters required by neural networks to approximate functions in Fn,d is Ω(ϵd/n). Therefore, our PolyReLU networks achieve the optimal approximation rate in the context of Sobolev spaces."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we demonstrate the expressivity and effectiveness of PolyCom within the transformer through experiments on LLMs. 4.1 SETUP Baseline. We evaluate PolyCom across two series of models: 1B dense model and Mixture of Experts (MoE) model with 1B active and 7B total parameters. The 1B dense model contains approximately 1.3 billion parameters with an architecture similar to Llama 2 (Touvron et al., 2023). For the MoE model, we use the OLMoE framework (Muennighoff et al., 2024), which activates 1.3B parameters out of total of 6.9B parameters. Both models are trained from scratch. We compare the performance of PolyCom with several activation functions, including ReLU, square ReLU, GELU, and SwiGLU. All experiments are conducted on Nvidia A100-80G GPUs, 32 GPUs for the dense model, and 64 GPUs for the MoE model. Model Configuration. For the dense model, the transformer consists of 24 layers with hidden size dmodel = 2048 and 16 attention heads. In the MoE model, the transformer is composed of 16 layers, with hidden size of dmodel = 2048, 16 attention heads, and 64 experts. To maintain consistent number of trainable parameters across all activation functions, we adjust the intermediate size accordingly. Specifically, for SwiGLU, the intermediate size is set to two-thirds that of the other activations in all experiments. Datasets. The dense model is trained on the RedPajama-1T dataset 1 (Computer, 2023), which was developed by the open-source AI community to enable competitive performance against proprietary models. The MoE model is trained on the OLMoE Mix dataset 2 (Muennighoff et al., 2024). Hyperparameters. Unless otherwise specified, we use third-order PolyCom by default and initialize the coefficients as ai = 1/3 for = 1, 2, 3 and set a0 = 0. Model weights are randomly initialized. For optimization, we apply the AdamW optimizer with β1 = 0.9 and β2 = 0.95. All models are trained on sequences of 4096 tokens. For the dense model, we set the initial learning rate to 3e-4, decaying to 1.5e-5 using cosine scheduler. The MoE model starts with learning rate of 4e-4, also decaying according to cosine schedule. Evaluation To evaluate the performance of LLMs with PolyCom, we use wide range of open benchmarks, including ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), CoQA (Reddy et al., 2019), Winogrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), COPA (Gordon et al., 2012), CSQA (Talmor et al., 2019), OBQA (Mihaylov et al., 2018), and SocialIQA (Sap et al., 2019). We utilize the LM Eval Harness (Gao et al., 2023) for standardized performance evaluation. 6 Under Peer Review. Table 1: Overall results of the 1B dense model with different activation functions, reported in terms of training loss, validation perplexity, and downstream accuracy (%). ARC-E and ARC-C refer to ARC-Easy and ARC-Challenge, respectively. The best results in each column are highlighted in bold. Avg. denotes the average accuracy of all downstream tasks. Loss PPL ARC-E ARC-C HellaSwag PIQA SciQ Winograde Avg. SwiGLU 2.19 GELU 2.20 ReLU 2.21 PolyReLU 2.17 PolyNorm 2. 3.22 3.24 3.26 3.18 3.17 56.61 55.43 55.68 57.53 59.68 27.47 27.73 28.50 27.99 29.01 49.23 48.42 48.59 50.19 50.86 68.61 86.10 68.12 87.40 68.39 87.10 70.29 87.60 69.15 87.20 56.83 54.78 54.85 55.72 56. 57.47 56.98 57.18 58.22 58.68 Figure 3: Traning and validation loss on C4 and Wikipedia for MoE models with 200 billion training tokens. We compare models using SwiGLU and PolyNorm activation functions. PolyNorm demonstrates lower training and validation losses, indicating faster convergence. 4.2 RESULTS ON DENSE MODEL Training Dynamics of 1B Dense Model. Figure 1 compares the training dynamics of the 1B dense model across different activation functions. As shown in the figure, models using PolyReLU and PolyNorm exhibit lower training loss and validation perplexity throughout the training process compared to models utilizing other activation functions. This indicates that PolyCom accelerates the convergence of LLMs. The models with PolyReLU and PolyNorm also consistently outperform others in downstream tasks by large margins, highlighting the advantage of PolyCom in improving the overall expressivity and effectiveness of LLMs. Dowmstream Evaluation. Table 1 presents the training loss, validation perplexity, and downstream task accuracy (%) after processing 250 billion training tokens. The downstream tasks include ARCEasy, ARC-Challenge, HellaSwag, PIQA, SciQ, and Winograde. More detailed results are provided in Appendix D. The results clearly demonstrate that the PolyCom family (PolyReLU and PolyNorm) outperforms the other activation functions. For instance, PolyNorm outperforms SwiGLU by an average margin of 1.21% across six downstream tasks. This underscores the expressivity and efficiency of PolyCom as an activation function in transformer models. 4.3 RESULTS ON MOE MODEL Our experiments with MoE modes are based on OLMOE-1B-7B, which has 1 billion activate parameters and 7 billion total parameters (Muennighoff et al., 2024). Due to computational constraints, we compare only PolyNorm activation function, shown to perform best in dense models, with the widely used SwiGLU activation function, which is commonly employed in current LLM architectures. Training dynamics of MoE model. In Figure 3, we report the training and validation loss of MoE models trained on 200 billion tokens. Models using PolyNorm consistently show lower losses com1RedPajama-1T is available at https://github.com/togethercomputer/RedPajama-Data. 2OLMoE Mix dataset at https://huggingface.co/datasets/allenai/ available is OLMoE-mix-0924. 7 Under Peer Review. Figure 4: Downstrean performance dynamics on HellaSwag, MMLU Var, ARC-Challenge, and SciQ for MoE models with 200 billion training tokens. Models with PolyNorm significantly outperform those with SwiGLU on downstream tasks. Table 2: Validation losses of MoE models with different activation functions. CC denotes Common Crawl. Best results per column are bold. Methods C4 Books CC peS2o Reddit Stack Wikipedia ICE M2D2 Pile Wikitext Avg. SwiGLU 2.72 PolyNorm 2.71 2.59 2.57 2.79 2.78 2.16 2.15 2.93 2. 1.01 1.00 2.30 2.50 2.29 2.49 3.07 3.06 2.07 2.37 2.03 2.34 2.41 2.39 Table 3: Downstream evaluation results of MoE models with different activation functions. ARC-C, ARC-E, OQA denote ARC-Challenge, ARC-Easy, and OpenbookQA, respectively. Best results per column are in bold. Tasks MMLU Var HellaSwag SciQ ARC-C ARC-E PIQA WinoGrande OQA COPA Avg. SwiGLU 37.07 PolyNorm 37.27 66.49 90.60 37.12 67.63 92.40 38.46 71.58 70.70 76.61 77.04 62.75 62.19 39.80 83.00 62.78 40.60 84.00 63. pared to those using SwiGLU, indicating that PolyNorm enables faster learning. Figure 4 shows the downstream performance on HellaSwag, MMLU Var3, ARC-Challenge, and SciQ. PolyNorm outperforms SwiGLU on all tasks, with notable improvements, demonstrating superior generalization capabilities. Dowmstream Evaluation. Table 2 presents the validation losses on 11 datasets. PolyNorm consistently achieves lower validation losses than SwiGLU across all datasets, with an average improvement of 0.02. In Table 3, we also observe that PolyNorm outperforms SwiGLU on 8 downstream tasks. These results highlight the superior performance of models using the PolyNorm activation function. Additional results can be found in Appendix E. 4.4 ABLATIONS AND ANALYSIS. Order of PolyCom. We first investigate the effect of different orders of PolyCom. We vary the order of PolyReLU in the set {2, 3, 4} and plot the results in Figure 5(a). As seen, the convergence speed improves as the order increases. However, there is no noticeable difference between orders 3 and 4 in terms of convergence speed. Additionally, increasing the order can lead to computational overhead and overflow issues, particularly when using low-precision arithmetic. Based on these observations, we select = 3 as the default order for PolyCom in our experiments, balancing both performance and computational efficiency. Different Polynomial Composition Functions. We evaluate the impact of different polynomial composition functions by comparing PolyReLU, PolyPReLU, PolyNorm, and PolyReLUNorm in Figure 5(b). Our results indicate that PolyNorm, which uses normalization as the composition func3MMLU Var is variant of MMLU (Hendrycks et al., 2021) using varied few-shots (Muennighoff et al., 2024). 8 Under Peer Review. (a) Different orders of PolyReLU (b) Different compositions (c) ReLU variants Figure 5: Training loss for 1B dense models with different activation functions. 5(a): We compare different orders of PolyReLU. 5(b): Comparison of PolyCom with different composition functions. 5(c): Comparison of different variants of ReLU activation function. tion, achieves the lowest training loss and best overall performance. This suggests that normalization plays key role in stabilizing training and enhancing the models ability to generalize. In contrast, combining ReLU with normalization (PolyReLUNorm) provides intermediate results, suggesting that more complex compositions do not always lead to better outcomes. Variants of ReLU. In Figure 5(c), we compare different variants of the ReLU activation function, including ReLU and ReLU2. PolyReLU consistently outperforms both ReLU and ReLU2 across all tasks, highlighting the benefits of using polynomial composition. This result reinforces the hypothesis that introducing higher-order terms through PolyCom enables the model to capture more complex data interactions, thus improving the expressivity of the activation function without significantly increasing model size or complexity. Rank of Weights. To understand how PolyCom enhances model performance, we analyze the rank of the weights in each FNN layer of the transformer. We use the effective rank (Roy & Vetterli, 2007) to measure the effective dimensionality of weights and its definition is in Appendix C.2. Figure 6 shows that PolyReLU and PolyNorm result in higher weight ranks compared to other activation functions such as SwiGLU, GELU, and ReLU. higher rank in the weight matrices indicates greater capacity for representing complex patterns in the data. These findings suggest that PolyCom improves the expressibility of transformers by allowing the FNN layers to better utilize their parameters, ultimately leading to better generalization on downstream tasks. Layer-wise Similarity. We further analyze the layer-wise similarity of hidden states using cosine similarity, as illustrated in Figure 7. For both dense and MoE models, we compare SwiGLU with PolyNorm. The results reveal that PolyNorm consistently maintains lower layer-wise similarity compared to SwiGLU, indicating that PolyNorm promotes greater diversity between layers. This diversity likely enables the model to learn more complex representations, as deeper layers are not merely replicating the functionality of earlier ones. Notably, the gap in cosine similarity between PolyNorm and SwiGLU widens in the deeper layers, which are generally more crucial for downstream task performance. This increased diversity across layers enhances the models ability to capture complex relationships, thereby improving the overall effectiveness of LLMs."
        },
        {
            "title": "5 RELATED WORK",
            "content": "The design of activation functions has been critical area of research in neural networks, directly influencing the performance and capabilities of deep learning models. Early activation functions like Sigmoid and Tanh were widely used due to their smooth nonlinear transformations (Goodfellow et al., 2016). However, these functions faced challenges such as vanishing gradients, making it difficult to train deep networks effectively. The introduction of the Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) mitigated some of these issues by offering simple, non-saturating nonlinearity, which has since become standard in many deep learning applications. Variants of ReLU, such as Leaky ReLU (Maas et al., 2013) and Parametric ReLU (PReLU) (He et al., 2015), were 9 Under Peer Review. (a) Rank of Wup, dense (b) Rank of Wdown, dense (c) Rank of Wup, MoE (d) Rank of Wdown, MoE Figure 6: Rank of weights in each FNN. 6(a) & 6(b) for the dense model, 6(c) & 6(d) for the MoE model. (a) SwiGLU, dense (b) PolyNorm, dense (c) SwiGLU, MoE (d) PolyNorm, MoE Figure 7: Layer-wise cosine similarity of hidden states. 7(a) &7(b): for 1B dense models with SwiGLU and PolyNorm, respectively. 7(c) & 7(d): for MoE models with SwiGLU and PolyNorm, respectively. developed to address the dying ReLU problem by allowing small, non-zero gradient when the input is negative. Other functions, like the Exponential Linear Unit (ELU) (Clevert, 2015), aimed to provide smoother activation profiles, resulting in better generalization and faster convergence in certain tasks. Moreover, Manessi & Rozza (2018) proposed combination of weighted base activation functions for further enhancement. Polynomial activation functions (Hornik et al., 1989; Oh et al., 2003), although less commonly used, have been studied in various contexts for their ability to model higher-order, complex relationships more effectively. For instance, Lokhande et al. (2020) introduced Hermite polynomial activations to improve pseudo-label accuracy, while Chrysos et al. (2020) proposed polynomial networks, Π-nets, which apply to various domains such as image and audio processing. Building on this, Chrysos et al. (2023) utilized regularization techniques to enhance the performance of polynomial networks. These works highlight the potential of polynomial functions to increase the expressiveness of neural networks by capturing intricate, higher-order interactions. On the theoretical front, the expressivity and approximation power of polynomial functions have been rigorously explored (Kileel et al., 2019; Kidger & Lyons, 2020; Kubjas et al., 2024). The choice of activation function in transformers has also become an important area of research. Originally developed for natural language processing, transformers (Vaswani et al., 2017) have been effectively adapted for diverse tasks, including image recognition, speech processing, and reinforcement learning. Despite their broad applicability, the activation functions predominantly utilized in transformers, ReLU and GELU, have seen minimal evolution. Recent studies, however, have begun to explore alternatives to these conventional activations. For example, the Swish activation (Ramachandran et al., 2017; Shazeer, 2020) and the Mish activation (Misra, 2019) are smooth and non-monotonic functions that offer potential benefits in model performance and training stability. Additionally, Gated Linear Units (GLU) were proposed by Dauphin et al. (2017), with SwiGLU (Shazeer, 2020), prominent variant, being used in models such as LLaMA-Series (Touvron et al., 2023). 10 Under Peer Review."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "In this paper, we introduce the Polynomial Composition Activation (PolyCom) and demonstrate its effectiveness within transformer models. By enabling the capture of higher-order interactions, PolyCom enhances both the accuracy and convergence rates of these models. Our experiments, conducted across different large language model architectures and multiple benchmarking datasets, confirm that PolyCom consistently outperforms conventional activation functions. Furthermore, ablation studies indicate that PolyCom increases model expressivity by elevating weight rank and reducing redundancy across layers. These findings underscore the significant potential of polynomialbased activations to improve transformer models, thereby paving the way for future research endeavors."
        },
        {
            "title": "REFERENCES",
            "content": "Robert Adams and John JF Fournier. Sobolev spaces. Elsevier, 2003. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇcic, and Cordelia Schmid. Vivit: video vision transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 68366846, 2021. Jonathan Barron. Continuously differentiable exponential linear units. arXiv preprint arXiv:1704.07483, 2017. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Nicolas Boulle, Yuji Nakatsukasa, and Alex Townsend. Rational neural networks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, pp. 1424314253, 2020. Grigorios Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, In Proceedings of the and Stefanos Zafeiriou. P-nets: Deep polynomial neural networks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73257335, 2020. Grigorios Chrysos, Bohan Wang, Jiankang Deng, and Volkan Cevher. Regularization of polynomial networks for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1612316132, 2023. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Djork-Arne Clevert. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015. Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Yann Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933941. PMLR, 2017. Ronald DeVore, Ralph Howard, and Charles Micchelli. Optimal nonlinear approximation. Manuscripta mathematica, 63:469478, 1989. 11 Under Peer Review. Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: no-recurrence sequence-to-sequence model for speech recognition. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 58845888. IEEE, 2018. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/ 10256836. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 315323. JMLR Workshop and Conference Proceedings, 2011. Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT Press, 2016. Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In * SEM 2012: The First Joint Conference on Lexical and Computational SemanticsVolume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 394398, 2012. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 10261034, 2015. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359366, 1989. Patrick Kidger and Terry Lyons. Universal approximation with deep narrow networks. In Conference on learning theory, pp. 23062327. PMLR, 2020. Joe Kileel, Matthew Trager, and Joan Bruna. On the expressive power of deep polynomial neural networks. Advances in neural information processing systems, 32, 2019. Alex Krizhevsky et al. Convolutional deep belief networks on cifar-10. 2010. Kaie Kubjas, Jiayi Li, and Maximilian Wiesmann. Geometry of polynomial neural networks. arXiv preprint arXiv:2402.00949, 2024. Shiyu Liang and Srikant. Why deep neural networks for function approximation? In International Conference on Learning Representations, 2017. Vishnu Suresh Lokhande, Songwong Tasneeyapant, Abhay Venkatesh, Sathya Ravi, and Vikas Singh. Generating accurate pseudo-labels in semi-supervised learning and avoiding overconfident predictions via hermite polynomial activations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1143511443, 2020. Andrew Maas, Awni Hannun, Andrew Ng, et al. Rectifier nonlinearities improve neural network acoustic models. In Proc. ICML. Atlanta, GA, 2013. 12 Under Peer Review. Franco Manessi and Alessandro Rozza. Learning combinations of activation functions. In 24th international conference on pattern recognition (ICPR), pp. 6166. IEEE, 2018. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, 2018. Diganta Misra. Mish: self regularized non-monotonic activation function. arXiv preprint arXiv:1908.08681, 2019. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. Olmoe: Open mixture-of-experts language models, 2024. URL https://arxiv.org/abs/2409. 02060. Vinod Nair and Geoffrey Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, pp. 807814, 2010. Sung-Kwun Oh, Witold Pedrycz, and Byoung-Jun Park. Polynomial neural networks architecture: analysis and design. Computers & Electrical Engineering, 29(6):703725, 2003. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Prajit Ramachandran, Barret Zoph, and Quoc Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017. Siva Reddy, Danqi Chen, and Christopher Manning. Coqa: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. Olivier Roy and Martin Vetterli. The effective rank: measure of effective dimensionality. In EUSIPCO, 2007. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. David So, Wojciech Manke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc Le. Searching for efficient transformers for language modeling. Advances in neural information processing systems, 34:60106022, 2021. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, 2019. Matus Telgarsky. Neural networks and rational functions. In International Conference on Machine Learning, pp. 33873393. PMLR, 2017. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Lloyd Trefethen. Approximation theory and approximation practice, extended edition. SIAM, 2019. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 13 Under Peer Review. Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 94106, 2017. Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network (2015). arXiv preprint arXiv:1505.00853, 2015. Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural networks, 94: 103114, 2017. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, 2019. 14 Under Peer Review."
        },
        {
            "title": "A OMITTED PROOFS",
            "content": "In this section, we provide the proofs that were omitted in the main body of the paper. The following proofs build upon the work of Yarotsky (2017); Telgarsky (2017); Boulle et al. (2020). A.1 PROOF OF LEMMA 2 The proof of Lemma 2 leverages Lemma 3.4 from Telgarsky (2017), which we state below. Lemma A.1 (Lemma 3.4 in Telgarsky (2017)). Let ϵ (0, 1) be given. Suppose : [0, 1]d [1, 1] be order polynomial with monomials and coefficients within [1, 1]. Then there exists ReLU network : [0, 1]d [1, 1] of size O(min{sr ln(sr/ϵ), sd ln2(dsr/ϵ)}) such that maxx[0,1]d p(x) (x) < ϵ. Using this result, we now proceed with the proof of Lemma 2. Proof of Lemma 2. First, we observe that PolyReLU(x) = Poly(ReLU(x), where oly(x) = (cid:80)r i=0 aixi for [1, 1]. By Lemma A.1, there exists ReLU network f1 : [0, 1] [1, 1] of size O(ln2(1/ϵ)) such that: max x[0,1] f1(x) Poly(x) < ϵ. Thus, we construct = f1 ReLU for inputs [1, 1]. This yields that: max x[1,1] (x) PolyReLU(x) = max x[1,1] f1 ReLU(x) PolyReLU(x) = max x[1,1] f1(ReLU(x)) Poly(ReLU(x)) f1(x) Poly(x) = max x[0,1] < ϵ. (15) (16) Since f1 is ReLU network, the constructed function = f1 ReLU is also ReLU network, completing the proof. A.2 PROOF OF THEOREM 1 The proof is an elementary extension of Lemma 1. Proof of Theorem 1. Using Lemma 1, we can represent the ReLU activation on using PolyReLU activation. Thus, we replace each ReLU activation in the ReLU network with PolyReLU to construct new network g. Obviously, such satisfies the above requirements. Hence, the size and structure remain equivalent, and serves as the PolyReLU network equivalent to the ReLU network. A.3 PROOF OF THEOREM 2 The lower bound of Theorem 2 follows directly from Theorem 11 in Liang & Srikant (2017), restated here for clarity: Lemma A.2 (Theorem 11 in Liang & Srikant (2017)). Suppose function : differentiable and strongly convex. Let ϵ (0, 1) be given and be ReLU network. maxx[0,1]d (x) (x), then the network size of is at least Ω(ln(1/ϵ)). Lemma A.2 shows that approximating the quadratic function x2 with an error tolerance ϵ requires network of size at least Ω(ln(1/ϵ)). Since x2 on [0, 1]d is degradation case of PolyReLU, any ReLU network approximating PolyReLU with error ϵ must also be at least Ω(ln(1/ϵ)) in size. The upper bound is proved in the following. [0, 1]d is If Proof of Theorem 2. Denote gi as the i-th layer of PolyReLU neteeork for 1 L, such that: = gL gL1 g1. 15 Under Peer Review. For each neuron, since a1 + 1, it follows that: ax + ax + a1x + 1, {xx 1}. (17) Additionally, note that the range of PolyReLU is [1, 1]. Hence, by induction, the output of each neuron remains within [1, 1]. For each subnetwork gi, by applying Lemma 2, we can construct corresponding ReLU network fi by replacing each PolyReLU activation pi,j in gi with ReLU activation. Specifically, for any [L]4 and [K], there exists ReLU network fi,j : [1, 1] [1, 1] that approximates the PolyReLU activation pi,j with given tolerance ϵi > 0. Thus, the network fi is obtained by replacing each PolyReLU activation pi,j in gi with its ReLU approximation fi,j. Obviously, fi is ReLU network whose output dimensions are in range [1, 1]. Next, we give the approximation error bound. Denote hg = fi f1 for [L]. For the sake of brevity, we assume hg 0 as the identity map in [1, 1]d. Hence, we have hg = gi g0 and hf i1 + bi,j) be the output of j-th neuron of gi. Denote the approximation between the PolyReLU network and the ReLU network at i-th layer and j-th neuron as ei,j. And we use ei = maxj[K] ei,j to denote the approximation error between the PolyReLU network and the ReLU network at i-th layer. Then for any [L], we have that = fi f0. Suppose (cid:55) pi,j(a = gi g1 and hf 0 = hf i,jhg i,j(x) hf (cid:12) (cid:12) i,j(x) (cid:12) ei,j = max x[1,1]d = max x[1,1]d = max x[1,1]d (cid:12) (cid:12)hg (cid:12) (cid:12) (cid:12)pi,j(a (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) pi,j(a i,jhg i1(x) + bi,j) fi,j(a i,jhf (cid:12) (cid:12) i1(x) + bi,j) (cid:12) i,jhg i1(x) + bi,j) pi,j(a i,jhf i1(x) + bi,j) (cid:12) (cid:12) i1(x) + bi,j) (cid:12) (cid:12) + pi,j(a i1(x) + bi,j) fi,j(a i,jhf max x[1,1]d i,jhg i,jhf (cid:12) (cid:12)pi,j(a (cid:12) (cid:12) (cid:12)pi,j(a (cid:12) x[1,1]d (cid:12) (cid:12)(a (cid:12) α + max max x[1,1]d i1(x) + bi,j) pi,j(a i,jhf (cid:12) (cid:12) i1(x) + bi,j) (cid:12) (18) i,jhf i1(x) + bi,j) fi,j(a i,jhf (cid:12) (cid:12) i1(x) + bi,j) (cid:12) (cid:12) (cid:12) (cid:12) + ϵi i1(x) + bi,j) i,jhg i1(x) + bi,j) (a (cid:13) (cid:13)hg (cid:13) i1(x) hf i,jhf (cid:13) i1(x) hf (cid:13) i1(x) (cid:13) (cid:13) (cid:13) i1(x) (cid:13) + ϵi. ai,j1 α max x[1,1]d α max x[1,1]d (cid:13) (cid:13)hg (cid:13) + ϵi The first inequality is using the triangular inequality. The second inequality holds because the Lipschitz constant of pi,j is α and the ReLU subnetwork fi,j approximates pi,j with error ϵi. In the fourth inequality, we used Holders inequality. Since ai,j1 ai,j1 + bi,j 1, the fifth inequality holds. Therefore, we derive the following approximation bound: ei = max j[K] ei,j α max x[1,1]d (cid:13) (cid:13)hg (cid:13) i1(x) hf i1(x) (cid:13) (cid:13) (cid:13) + ϵi = αei1 + ϵi, (19) for [L]. Since hg 0 = hf 0 , we have e0 = 0. Let ϵi = ϵ/(LαLi) for [L]. It follows that: ei iϵ LαLi , [L]. (20) Hence, the final error at the last layer is bounded by eL ϵ. 4We use notation [L] to denote the set {1, 2, . . . , L}. Under Peer Review. Last, we need to estimate the size of the ReLU network . By Lemma 2, the size of each ReLU subnetwork fi,j is O(ln2(LαLi/ϵ)). Therefore, the total size of the ReLU network is: (cid:32) (cid:88) i=1 ln (cid:18) LαLi ϵ (cid:19)(cid:33) (cid:18) KL ln2 = (cid:18) LαL ϵ (cid:19)(cid:19) , where we use the fact that: (cid:18) LαLi ϵ (cid:88) ln2 i= (cid:19) = (cid:88) (cid:18) i=1 ln (cid:19) (cid:18) LαL ϵ (cid:19)2 ln α = (cid:18) ln2 (cid:18) LαL ϵ (cid:19)(cid:19) . (21) (22) This completes the proof. A.4 PROOF OF THEOREM 3 Before proving Theorem 3, we begin by introducing few useful lemmas. Lemma A.3 (Proposition 1 in Yarotsky (2017)). Let and ρ : be any continuous piece-wise linear function with breakpoints. Then the following two statements hold: For network with activation ρ, depth and width K, there exists ReLU network with the same depth and width O(M K) that computes the same function as the original network. Conversely, if ReLU network has depth and width K, there exists network with activation ρ, depth and width that computes the same function on bounded input domain D. This result, Combined with Lemma 1, directly leads to the following corollary, which demonstrates that PolyReLU networks can represent any piece-wise linear function exactly on R. Corollary A.1. Let and ρ : be any continuous piece-wise linear function with breakpoints. Then there exists PolyReLU network of size O(M ) such that: ρ(x) = g(x), R. In similar manner to Proposition 10 in Boulle et al. (2020), we can show that PolyReLU networks can represent powers xn exactly for any N. Lemma A.4. Suppose n, and 2. Then xn can be represented exactly by PolyReLU network with an r-th order PolyReLU activation and size O(ln2(n)). Proof of Lemma A.4. We first prove that xn can be represented exactly by polynomial network ˆg with r-th order polynomial activation and having size O(ln2(n)). Based on ˆg, we construct PolyReLU network that satisfies the requirements. By expressing in base r, we have that: xn = (cid:89) i=0 xciri = (cid:89) i=0 (cid:16) xci (xr)i(cid:17) , (23) where = logr n, = (cid:80)k can be represented by polynomial network with + 1 layers and width 1. It follows that xn can be represented by polynomial network of size i=0 ciri, and ci {0, 1, 2, . . . , 1}. Each xciri (cid:88) (i + 1) = O(k2) = O(ln2(n)). i=0 (24) By Lemma 1, we know that PolyReLU activation can represent polynomial activation. Hence, there exists PolyReLU network with an r-th order activation and size O(ln2(n)) such that g(x) = xn R. Under Peer Review. With the above lemmas, we can now prove Theorem 3. Proof of Theorem 3. The proof is composed of two parts. We first approximate by local Taylor polynomials and continuous piece-wise linear functions and then represent these functions using PolyReLU networks, following Yarotsky (2017); Boulle et al. (2020). Part 1. Suppose is positive integer. We begin by dividing [1, 1]d into grid of (2N + 1)d functions: (cid:88) ϕm(x) = 1, ϕm(x) = (cid:89) i=1 (cid:16) φ 3N (cid:16) xk (cid:17)(cid:17) , mk = (x1, x2, . . . , xd) [1, 1]d, where = (m1, m2, . . . , md) {N, (N 1), . . . , 0, . . . , }, and φ is defined as: φ(x) = 1, 0, 2 x, 1 2. < 1, 2 < x, This function has the following properties: max xR φ(x) = 1, max x[1,1]d ϕm(x) = 1, (cid:26) supp ϕm = (cid:12) (cid:12) (cid:12) (cid:12) (cid:13) (cid:13) (cid:13)x (cid:13) (cid:13) (cid:13) < 2 3N (cid:27) , {N, (N 1), . . . , }d. Part 2. We use degree-(n 1) local Taylor approximation of the function , defined as fN (x) = (cid:88) ϕm(x)Pm(x), m{N,...,N }d where Pm is the degree-(n 1) Taylor polynomial of at = m/N , i.e., Pm(x) = (cid:88) n:n1<n 1 n! Dnf (cid:17) (cid:16) (cid:16) (cid:17)n , (25) (26) (27) (28) with conventions n! = (cid:81)d (cid:1)ni. i=1 ni! and (cid:0)x The approximation error between and fN can be bounded as follows: (cid:0)xi mi = (cid:81)d (cid:1)n i= f (x) fN (x) = (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) m{N,...,N }d (cid:88) (cid:12) (cid:12) (cid:12) ϕm (f (x) Pm(x)) (cid:12) (cid:12) (cid:12) (x) Pm(x) m:x 3N 3N < 2 max < 2 m:x (cid:19)n (cid:18) 2d 3N (cid:18) 2d 3N (cid:19)n . 2d 2d n! 2d n! (x) Pm(x) (29) max n:n1=n ess sup x[1,1]d Dnf (x) The first inequality is because of the triangular inequality and Eq. (25). In the second inequality, we used the fact that [1, 1]d belongs to the support of at most 2d functions ϕm. The third inequality is bound for the Taylor remainder and the fourth inequality uses the definition of Fn,d. Let + 1, (30) = (cid:37) (cid:19) (cid:36) 2d 3 (cid:18) 2d n!ϵ 18 Under Peer Review. we have that max x[1,1]d (x) fN (x) < ϵ. (31) Next, we construct PolyReLU network gN to represent fN exactly. Let am,n = 1 Since W n,([1,1]d) 1, am,n 1 for any m, n, we rewrite fN as: n! Dnf (cid:0) (cid:1). fN (x) = (cid:88) (cid:88) m{N,...,N }d n:n1<n am,nϕm(x) (cid:16) (cid:17)n . (32) i=1 φ (cid:0)3N (cid:0)xk mk Therefore, fN is composed of at most dn(2N + 1)d functions ϕm(x) (cid:0)x . Since ϕm(x) = (cid:1)(cid:1) is continuous piece-wise linear function, (cid:1)(cid:1) and each φ (cid:0)3N (cid:0)xk mk (cid:81)d we can apply Corollary A.1, which guarantees that there exists PolyReLU network ˆϕm of size O(d) that can exactly represent ϕm on Rd, i.e., ˆϕm(x) = ϕm(x), Rd. For (cid:0)x = (cid:1)ni, by Lemma A.4, we know that there exists PolyReLU network gm of size at (cid:81)d most O(d ln2(n)) such that gm(x) = (cid:0)x (cid:1)n , xRd. Combining these results, we can now construct larger PolyReLU network gn as follows: (cid:0)xi mi (cid:1)n (cid:1)n i=1 N gN (x) = (cid:88) (cid:88) am,n ˆϕm(x)gm(x), (33) m{N,...,N }d n:n1<n where the total size of the network is: (cid:0)dn(2N + 1)d(d + ln2(n))(cid:1) = O(ϵ ). Here, we use Eq. (30) to determine the size bound in terms of the error tolerance ϵ. Clearly, we have: Hence, we conclude: fN (x) = gN (x), Rd. max x[1,1]d (x) gN (x) = max x[1,1]d (x) fN (x) < ϵ. (34) (35) This completes the proof."
        },
        {
            "title": "B ACTIVATION FUNCTIONS",
            "content": "We provide definitions of several commonly used non-linear activation functions in Table 4."
        },
        {
            "title": "C EXPERIMENTAL DETAILS",
            "content": "C.1 ARCHITECTURE Table 5 outlines the model architecture used for the 1B dense model. To ensure comparable numbers of training parameters across different activation functions, we adjust the intermediate sizes accordingly. For SwiGLU, the intermediate size is set to 5504, while for other activation functions, it is set to 8256. Table 6 outlines the model architecture used for the MoE models. Similarly, the intermediate size for SwiGLU is set to 1024, while for other activation functions, it is set to 1536. C.2 DEFINITION OF EFFECTIVE RANK We adopt the concept of effective rank from Roy & Vetterli (2007) to measure the effective dimensionality of matrix. Given matrix with Singular Value Decomposition (SVD) = ΣV , where Σ is diagonal matrix containing singular values σ1 σ2 σn 0. we define the singular value distribution as pi = σi/ (cid:80)n j=0 σj, [n]. The effective rank of is then given by: (cid:32) Erank(A) = exp (cid:33) . pi ln pi (cid:88) i=0 (36) Under Peer Review. Table 4: Definition of activation functions."
        },
        {
            "title": "Activation",
            "content": "ReLU (Nair & Hinton, 2010) ReLU2 (So et al., 2021) ReLU6 (Krizhevsky et al., 2010) Leaky ReLU (Maas et al., 2013) RReLU (Xu et al., 2015) Parametric ReLU (PReLU) (He et al., 2015) Tanh Softplus (Glorot et al., 2011) Mish (Misra, 2019) Sigmoid SiLU(Swish) (Ramachandran et al., 2017) ELU (Clevert, 2015) CELU (Barron, 2017) GELU (Hendrycks & Gimpel, 2016) GLU (Dauphin et al., 2017) SwiGLU (Shazeer, 2020) Poly"
        },
        {
            "title": "Definition",
            "content": "LeakyReLU (x) = , (0, 1) is constant ReLU (x) = max{x, 0} ReLU2 (x) = max{x, 0}2 ReLU6 (x) = min(max{x, 0},6 ) if 0 otherwise (cid:26)x ax if 0 otherwise, (cid:26)x, ax, RReLU(x) = PReLU (x) = is randomly sampled from uniform distribution (cid:26)x, ax, is learnable parameter Tanh(x) = exp(x)exp(x) exp(x)+exp(x) if 0 otherwise , Softplus (x) = 1 log(1 + exp(ax)), is constant (default 1.0) Mish (x) = Tanh(Softplus(x)) Sigmoid(x) = σ(x) = 1 1+exp(x) SiLU(x) = σ(x) (cid:26)x, if > 0 if 0, ELU(x) = (exp(x) 1), is constant (default 1.0) CELU (x) = max(0, x) + min(0, α (exp(x/a) 1)), is constant (default 1.0) GELU(x) = Φ(x), Φ(x) is CDF for Gaussian distribution GLU(x) = σ(xW ) (xV ) SwiGLU(x) = SiLU(xW ) (xV ), W, are learnable parameters i=0 aixi, ai, [r] are learnable parameters. Poly(x) = (cid:80)r Table 5: Model architecture of the 1B dense model. Params Hidden size Context Length Intermediate size Attention heads Hidden Layers 1.3B 4096 5504/8256 16 24 Table 6: Model architecture of MoE model. Activate Params Total Params Hidden size Intermediate size Attention heads 1.3B Hidden Layers 16 6.9B Exports 2048 Context Length 4096 1024/"
        },
        {
            "title": "D ADDITIONAL RESULTS ON DENSE MODEL",
            "content": "More detailed results from our ablation studies are shown in Figures 8, 9, and 10. These figures illustrate the training loss, validation loss, and validation perplexity (PPL) for the 1B dense model under different configurations. 20 Under Peer Review. Figure 8: Traning loss, validation loss, and validation perplexity (PPL) for the 1B dense model with different orders of PolyReLU activation functions. Figure 9: Traning loss, validation loss, and validation perplexity (PPL) for the 1B dense model with different polynomial compositions. Figure 10: Traning loss, validation loss, and validation perplexity (PPL) for the 1B dense model with different variants of ReLU activation functions."
        },
        {
            "title": "E ADDITIONAL RESULTS ON MOE MODEL",
            "content": "More results for the MoE model are provided in Figure 11, , showcasing validation losses and downstream evaluations after 200 billion training tokens. The comparison highlights models with different activation functions, such as SwiGLU and PolyNorm. As shown, models with PolyNorm exhibit lower training and validation losses, along with superior downstream performance. 21 Under Peer Review. Figure 11: Validation loss and downstream evaluations for MoE models with 200 billion training tokens, comparing SwiGLU and PolyNorm activation functions. PolyNorm shows superior performance in terms of lower loss and better downstream results."
        }
    ],
    "affiliations": [
        "School of Mathematical Sciences, Peking University",
        "Seed-Foundation-Model, ByteDance",
        "Capital University of Economics and Business"
    ]
}