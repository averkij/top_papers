{
    "paper_title": "EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models",
    "authors": [
        "Hu Yue",
        "Siyuan Huang",
        "Yue Liao",
        "Shengcong Chen",
        "Pengfei Zhou",
        "Liliang Chen",
        "Maoqing Yao",
        "Guanghui Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (EWMBench), a dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages a meticulously curated dataset encompassing diverse scenes and motion patterns, alongside a comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset and evaluation tools are publicly available at https://github.com/AgibotTech/EWMBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 4 9 6 9 0 . 5 0 5 2 : r EWMBENCH: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models Hu Yue1,4, Siyuan Huang2, Yue Liao3 Shengcong Chen1 Pengfei Zhou1 Liliang Chen1, Maoqing Yao1, Guanghui Ren1,"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (EWMBENCH), dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages meticulously curated dataset encompassing diverse scenes and motion patterns, alongside comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset and evaluation tools are publicly available at https://github.com/AgibotTech/EWMBench."
        },
        {
            "title": "Introduction",
            "content": "Figure 1: Comparison between general video generation and embodied video generation. Unlike general videos, embodied videos typically feature more structured scenes, consistent motion patterns, and clearer task logic. *Equal contribution. Project leader. Corresponding authors. 1AgiBot. 2SJTU. 3MMLab-CUHK. 4HIT. Preprint. Creative AI has advanced rapidly in recent years, propelled by innovations in model architecturessuch as variational autoencoders (VAEs) and diffusion modelsincreased parameter scaling, and the availability of large-scale, high-quality datasets. These developments have empowered generative models to synthesize images and videos conditioned on language instructions with unprecedented fidelity and controllability. Building on the momentum of text-to-video diffusion models, recent efforts have expanded their scope from generating high-fidelity, high-resolution videos to serving as embodied world models (EWMs) capable of synthesizing physically actionable scenes from language instructions (e.g., move the robot arm approaching the cup) or physical action instructions, i.e., an action policy sequence. This emerging capability establishes critical link between vision and action in embodied AI, facilitating applications such as robotic manipulation, where instruction-conditioned trajectories must conform to physical and kinematic constraints. Despite advancements in EWMs, fundamental question remains unresolved: How can we determine whether video generation model qualifies as good embodied world model, beyond merely serving as general-purpose video generator? Addressing this question is essential for guiding model development and assessing models ability to produce physically grounded, action-consistent behaviors. While existing video generation benchmarks Huang et al. [2024a] focus on perceptual metrics like visual fidelity, language alignment, and human preference, these criteria are insufficient for evaluating EWMs. Embodied generation tasks have unique requirements, such as coherence in embodiment motion and plausibility in action execution, as illustrated in Figure 1. For instance, in robotic manipulation scenarios, the background, object configuration, and embodiment structure (e.g., robot morphology) are expected to remain static, while only the robots pose and interactions evolve according to instructions. This structured realism sets EWMs apart from general video generators and demands evaluation beyond conventional criteria. In this work, we introduce dedicated benchmark, Embodied World Model Benchmark (EWMBENCH), to systematically assess embodiment motion fidelity and spatiotemporal consistency in robotic manipulation. We first formalize the benchmarking setup for EWMs. Given an initial video segment that specifies the embodiment (e.g., robotic arm) and the environment, along with manipulation instruction, the candidate EWM is tasked with autoregressively generating future frames depicting the embodiments motion until the instruction is completed. We design an evaluation protocol based on three key aspects: (1) Visual Scene Consistency, ensuring static elements like the background, objects, and embodiment structure remain unchanged during motion; (2) Motion Correctness, requiring the generated embodiment trajectory to be coherent and aligned with the task objective; and (3) Semantic Alignment and Diversity, assessing the models alignment with linguistic instructions and its ability to generalize across diverse tasks. For these aspects, we develop systematic evaluation tools, including prompt engineering with video-based MLLMs. To benchmark EWMs under our proposed criteria, we construct comprehensive colosseum consisting of curated benchmark dataset and open-source evaluation tools. The dataset is built on AgibotWorld AgiBot [2024], the largest real-world robotic manipulation dataset, featuring diverse tasks at scale. We select 30 candidate samples across ten tasks with clear action-ordering constraints, where correct execution requires understanding logical dependencies and affordancesposing significant challenges for embodied video generation. For each sample, static initial frames are clipped to ensure subsequent frames strictly reflect annotated language instructions without redundant movements. To reflect task diversity, we account for cases where multiple trajectories achieve the same goal and incorporate voxelized scoring to encourage variation. With this dataset, initial frames and language instructions are fed into different video generators, and the generated videos are compared against ground truth (GT) using various metrics. Contributions: We summarize our contributions as follows: (1) We propose the first world generation benchmark tailored for embodied tasks, EWMBENCH. (2)We curate high-quality, diverse dataset for our benchmark evaluation. (3) We introduce and open-source the systematical evaluation metrics, which covers key aspects in the embodied world model generation. (4) we provide insights into the performance of existing video models on embodied generation tasks."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Video Generative Models Diffusion-based video generation models have made significant advances in recent years, particularly in text-to-image (T2I) generation Blattmann et al. [2023], Ho et al. [2020], Song et al. [2020]and text-to-video (T2V) Zhang et al. [2023], Chen et al. [2023], Ren et al. [2024], Guo et al. [2023], Xing et al. [2025] generation. Recent works Kong et al. [2024], Zheng et al. [2024], Bao et al. [2024] have explored the usage of Diffusion Transformers in the denoising process. Some researchers Cheang et al. [2024] in robot learning aim to leverage this knowledge to address the challenge of data scarcity in robotic data collection. In some works Brooks et al. [2024], Yang et al. [2023]video models are employed to predict and simulate future states of dynamic systems. In robotics, video models have been utilized to predict future frames based on textual and visual inputs Zhou et al. [2024], Huang et al. [2025], Chi et al. [2024]. 2.2 Evaluation of Video Generative Models Benchmark Motion Scene Semantic TrajD TrajP SceneC DivM TempC InterL ImgC SemS TC-Bench Feng et al. [2024] Physics-IQ Motamed et al. [2025] VBench Huang et al. [2024a] VBench++ Huang et al. [2024b] PhyGenBench Meng et al. [2024] T2V-CompBench Sun et al. [2024] VMBench Ling et al. [2025] EvalCrafter Liu et al. [2024] T2VBench Ji et al. [2024] EVA Chi et al. [2024] Ours Table 1: Comparison of video generation benchmarks on 8 key evaluation dimensions: trajectory dynamics (TrajD), trajectory plausibility (TrajP), scene consistency (SceneC), diversity (DivM), temporal causality (TempC), interaction logic (InterL), image quality (ImgC), and semantic similarity (SemS). With the rapid development of video generation models and their broad applications across various domains, evaluating these models has become increasingly important. Earlier approaches primarily relied on conventional metrics such as Fréchet Inception Distance (FID)Heusel et al. [2017], Inception Score (IS)Salimans et al. [2016], and Fréchet Video Distance (FVD) Unterthiner et al. [2019]. However, these metrics primarily focus on visual appearance and offer limited insights into the diverse and complex capabilities of modern video generation models. Recent evaluation frameworks Huang et al. [2024a,b] introduced more structured approach that considers multiple capability dimensions. Nevertheless, these benchmarks still emphasize visual quality, including aesthetic appeal and motion smoothness. To address these limitations, specialized benchmarks have emerged. For instance, PhyGenBench Meng et al. [2024] evaluates models understanding of physical laws using VisionLanguage Models (VLMs), while T2V-CompBench Sun et al. [2024] assesses compositionality, covering motion, actions, spatial relationships, and attributes. Although these efforts have significantly expanded evaluation dimensions, they remain focused on general video generation. In the context of world models, particularly in the embodied domain, video generation requires enhanced controllability, physical plausibility, and robust object interactions. However, previous works have generally overlooked critical factors such as action plausibility, object interactions, and manipulation. To bridge this gap, we propose EWMBENCH, systematic framework for evaluating embodied world models. detailed comparison with existing benchmarks is provided in Table 1."
        },
        {
            "title": "3 The EWMBENCH Benchmark",
            "content": "Figure 2: Overview of the EWMBENCH benchmark design. The framework begins with unified world initialization, where generative models are instructed to produce predictive video frames based on initial scene images, task instructions, and optional action trajectories. These generated video frames are subsequently evaluated using multi-dimensional metrics, focusing on scene consistency, motion dynamics, and semantic alignment. 3.1 EWMBENCH Overview Overview The primary objective of this work is to establish comprehensive evaluation benchmark for embodied world model generation. EWMBENCH introduces three core components: (1) unified world initialization, (2) meticulously curated dataset for embodied manipulation tasks, and (3) systematic evaluation metrics. The complete pipeline is illustrated in Figure 2. The world is initialized using initial scene images and corresponding task instructions, with the sampled action trajectory being optional for generative models. Leveraging these unified input modalities, various generative models are instructed to produce video frames, while some contextual modalities may remain optional depending on the model. The outputs generated are evaluated using EWMBENCH metrics, which focus on three critical factors: scene, motion, and semantics. These factors collectively form the foundation for evaluating robotic tasks. In this work, we primarily focus on robotic manipulation tasks, which are the most dominant and representative within the domain of embodied tasks. We plan to extend this framework to broader tasks in future work. Evaluation Task Formulation An embodied world model generates video as expressed in Equation 1, where I, L, and represent the input context image, language, and trajectory, respectively. We provide this unified information, including up to four initial images. The action trajectory, formatted as sequence of 6D poses, is optional for generation model inference. The function fproc represents model-specific preprocessing. To ensure fairness, we apply vnorm to normalize the raw generated video frames before evaluation. The evaluation framework comprises multi-modal LLM for high-level semantic analysis, trajectory detector for low-level trajectory-based evaluation, and several visual foundation models for visual feature processing. = vnorm(gworld(fproc(I, L, ))) (1) 3.2 Dataset Construction We developed our evaluation dataset using the open-source Agibot-World dataset. Ten tasks were carefully selected based on their clear operational goals and sequential dependencies, covering both household and industrial contexts. These tasks emphasize action-ordering constraints that require reasoning about affordances and procedural contexts. To ensure diverse motion patterns, action trajectories were encoded into voxel grids, and greedy algorithm was employed to select the most diverse trajectories for each task. An overview of the constructed dataset is in Figure 3. Details on task descriptions can be found in the Appendix. For fine-grained evaluation, we adopted task-oriented decomposition strategy. Each high-level task was broken down into sequence of 4 to 10 atomic sub-actions, with each sub-action paired with step-level caption. This approach guarantees one-to-one alignment between video segments, sub-action labels, and their corresponding linguistic descriptions. 4 Figure 3: Overview of the constructed dataset. Left: Task scenes spanning household, commercial, and industrial environments. Middle: Diverse task-specific trajectory variations within each scene. Right: Broad semantic coverage across various manipulation contexts. 3.3 Evaluation Metrics EWMBENCH systematically evaluates three dimensions to ensure the generated outputs are visually realistic, action plausible, and semantically meaningful. We leave the metric definition details in the Appendix. A. Scene. We introduce the Scene Consistency metric, which examines visual layouts, object permanence, and viewpoint coherence. DINOv2, fine-tuned on an embodied dataset, extracts patchlevel frame representations. Cosine similarity between patch embeddings of consecutive and initial frames quantifies frame-to-frame consistency. Higher scores indicate stable scene structures and coherent viewpoints throughout the video. B. Action Motion. The quality of generated motions is evaluated through Trajectory-Based Evaluation, which compares generated trajectories with ground truth trajectories. The trajectories capture physical consistency, task logic, and interaction constraints. In our setup, we use the endeffectors (EEF) trajectory as the evaluation target and EEF is detected with our finetuned detector. Symmetric Hausdorff Distance (HSD) measures spatial alignment by calculating the maximum deviation between points on generated and GT trajectories. Normalized Dynamic Time Warping (NDTW) captures spatial-temporal alignment, ensuring correct sequence and timing of motions. Dynamic Consistency (DYN) evaluates motion dynamics, such as velocity and acceleration, using Wasserstein distance with motion normalization. To ensure fairness, generative models are required to produce three candidate trajectories for each task. The best trajectory is selected based on Hausdorff distance. C. Semantics. Semantic evaluation focuses on (1) alignment between task instructions and generated videos and (2) diversity within the task space. For semantic alignment, we use the generated videos language caption as an intermediate representation, comparing them to ground truth annotations to compute an alignment score. Captions are extracted at three levels, with details on the prompt design provided in Section 3.4. For semantic diversity, we use CLIP model, global video features are extracted, and the diversity score is computed as 1 similarity. This reflects the models ability to generalize and produce varied outputs. 3.4 MLLM Prompt Suite Design EWMBENCH Prompt suite is designed to be compact yet representative. We perform this evaluation across three levels of language analysis. Full prompts are in the project page. Global Video Caption Representation: At the global level, video MLLM generates compact caption summarizing the entire video. This caption is compared with the raw task instruction to evaluate overall alignment between the task goal and the generated videos content using BLEU score. 5 Figure 4: Evaluation Results of Video Generative Models. Key Steps Description: Robot tasks often involve multiple key steps that may be lost in global representations. To address this, the video MLLM produces detailed, step-by-step description of the tasks key steps. These descriptions are compared with GT step descriptions generated by the MLLM using CLIP score. Logical Error Punishment: Logical errors, such as hallucinations or spatial inconsistencies, are critical in robotic applications as they can lead to unsafe outcomes. The MLLM evaluates generated videos for commonsense violations, explicitly penalizing errors like hallucinated object manipulations or illogical spatial relationships. These penalties ensure that the model prioritizes realistic and coherent task execution."
        },
        {
            "title": "4 Experiments\nModels We evaluate EWMBENCH across seven video generation models categorized as open-source,\ncommercial, and domain-adapted. The open-source models include OpenSora 2.0Peng et al. [2025],\nwhich demonstrates strong performance on VBench with low training costs; LTXHaCohen et al.\n[2024], capable of real-time generation for interactive tasks; and COSMOS-7BAgarwal et al. [2025],\npre-trained for digital twin applications. Commercial models optimized for zero-shot generation\ninclude Kling-1.6Kuaishou [2025] and Hailuo I2V-01-liveHailuo [2025], both of which rank highly\nin recent benchmarks. Domain-adapted models fine-tuned for embodied scene understanding and\naction prediction include LTX_FT, a fine-tuned version of LTX, and EnerVerse, which is specifically\ndesigned for embodied scenarios. At present, we primarily focus on the Image-Text-to-Video setting,\nas no open-source action-conditioned video generation models are currently available. The evaluation\nof such models is left for future work. Nonetheless, with our unified input format and visual-space\nevaluation operations, EWMBENCH could also support this evaluation in the future. For the current\nevaluation, we tested 10 tasks, each consisting of 10 ground-truth episodes. Three videos were\ngenerated per model per episode, and the best prediction was selected using a best-of-three strategy,\nresulting in a total of 2,100 videos.",
            "content": "4.1 Evaluation Results We evaluate models across dimensions using normalized scores between 0 and 1, where higher values indicate better performance. Results in Table 2 show that domain-adapted models, such as EnerVerse and LTX_FT, consistently outperform commercial models (e.g., Kling, Hailuo) and open-source models (e.g., COSMOS, OpenSora, LTX). This highlights the effectiveness of domain-specific finetuning in capturing motion dynamics and task semantics. Notably, EnerVerse and Kling demonstrate strong semantic alignment, reflecting solid understanding of task logic. To validate the reliability of our evaluation, Figure 5 provides representative examples. Low scene consistency is marked by changes in spatial layout and object presence, while high scene consistency preserves both. Poor trajectory consistency features mismatched end-effector motion and task failure, whereas good cases exhibit motion patterns closely aligned with the ground truth, ensuring task success. Importantly, scene and trajectory consistency are complementary: visually plausible but static videos may score high in scene consistency while lacking meaningful motion. This emphasizes the need for systematic, multi-dimensional evaluation approach. 6 Type Model Scene Motion Semantics SceneC HSD Dyn nDTW Avg. Diversity BLEU CLIP Logics Avg. Dom. EnerVerse_FTHuang et al. [2025] LTX_FT Comm. KlingKuaishou [2025] HailuoHailuo [2025] Open. COSMOSAgarwal et al. [2025] OpenSoraPeng et al. [2025] LTXHaCohen et al. [2024] 0.9427 0.9436 0.8888 0. 0.7963 0.9210 0.9156 0.5356 0.4758 0.3231 0.2229 0.2500 0.1548 0.1575 0.5363 0.6197 0.3047 0. 0.2052 0.0474 0.1002 0.5957 0.5208 0.3162 0.1789 0.2533 0.1420 0.1425 1.6676 1.6163 0.9440 0. 0.7085 0.3442 0.4002 0.0691 0.0162 0.0493 0.0370 0.0803 0.0415 0.0174 0.1800 0.1740 0.1675 0. 0.1230 0.1598 0.0687 0.8638 0.8548 0.8535 0.8857 0.8458 0.8505 0.8324 0.9778 0.9444 0.9667 0. 0.7333 0.8222 0.7333 2.0907 1.9894 2.0370 2.0186 1.7824 1.8739 1.6518 Overall 4.7010 4. 3.8698 3.4125 3.2872 3.1392 2.9676 Table 2: Evaluation results categorized into task scene, action motion, and semantics. Figure 5: Typical examples. EWMBENCH scores align well with scene and motion accuracy, demonstrating the interpretability and robustness of the proposed metrics. 4.2 Human Evaluation To evaluate the alignment between automated metrics and human judgment, we conducted human evaluation on videos generated by four representative models: LTX_FT, Kling-1.6, Hailuo I2V-01live, and OpenSora-2.0. Annotators ranked the predictions based on overall quality, assigning 3 points to the best, 2 to the second-best, and 0 to the worst. Final rankings were derived by aggregating scores across all annotators and samples, with multiple review rounds ensuring annotation reliability. We then compared this aggregated human rankings (Figure 6 (A)) with those produced by EWMBENCH and VBench Huang et al. [2024a], one of the most popular video generation evaluation benchmarks. As shown in Figure 6 (B), EWMBENCHs rankings align more closely with human judgments than VBench rankings, indicating stronger consistency with human perception. 4.3 Complementarity of Trajectory Metrics To validate the necessity of employing all three trajectory consistency metricsHSD, nDTW, and DYNwe conducted controlled experiments involving sequence reversal, outlier insertion, and frame repetition. As shown in Figure 6 (C), each metric responds uniquely, demonstrating its specific strengths. In the sequence reversal test, only nDTW showed significant drop due to its sensitivity to temporal order, underscoring its role in detecting alignment errors. In the outlier test, HSD and DYN experienced substantial declines, reflecting their focus on spatial accuracy and motion integrityboth essential for safe and precise embodied execution. In the frame repetition test, nDTW increased due to repeated alignment, while DYN decreased, highlighting its sensitivity to motion smoothness. These findings confirm the complementary roles of the three metrics in providing comprehensive evaluation of trajectory quality. 4.4 Further Analysis and Discussions Characteristics of SOTA Models We present key findings from EWMBENCH evaluation, focusing on trade-offs, model characteristics, and the impact of domain adaptation in embodied video generation. Qualitative examples illustrating these failure modes are provided in Appendix A.4. Figure 6: (A) Aggregated human rankings of model predictions. (B) Comparison of rankings produced by EWMBENCH and VBench, highlighting EWMBENCHs closer alignment with human judgments.(C) Complementarity of trajectory metrics. Domain-adapted models show the best overall performance, particularly in semantic and dynamic dimensions, demonstrating that targeted fine-tuning significantly enhances task understanding and motion alignment. However, they occasionally exhibit empty grasping behaviors, revealing limitations in fine-grained action grounding. Kling achieves the best overall performance among general commercial and open-source video models, demonstrating strong and robust capabilities. Hailuo performs reasonably well in zero-shot embodied scenarios, but its generated scenes often appear cartoon-like, limiting visual realism. COSMOS and LTX display bias toward human hand representations and frequently fail to adapt semantic understanding to robotic contexts. LTX, in particular, suffers from abrupt scene transitions, inconsistent task execution, and and tendency to generate stationary states in action sequences. In contrast, COSMOS struggles to maintain consistent viewpoints, highlighting inadequate control of camera parameters. OpenSora shows partial understanding of task scenes, action motions, and semantic alignment in manipulation tasks. However, it suffers from jittery robotic arm movements and frequently generates static videos. Comparison with VBench Metrics Our experiments reveal that VBench struggles to separate foreground and background features, limiting the effectiveness of its subject-level metrics. In contrast, our Scene Consistency metric, leveraging fine-tuned DINOv2, excels at capturing layout structure and is more sensitive to viewpoint changes. This heightened sensitivity enables the detection of visual instability, critical factor in embodied video generation. Additional details and feature map visualizations are provided in the Appendix."
        },
        {
            "title": "5 Conclusions and Limitations\nIn this work, we propose EWMBENCH, a comprehensive benchmark suite for evaluating embodied\nworld generation models. With its multi-dimensional, human-aligned metric design and a motion-\ndiverse, multi-scene dataset, EWMBENCH serves as a valuable tool for measuring progress in\nembodied world model development.",
            "content": "Limitations and Future Work. First, our method currently focuses on the trajectory of the robotic arms end-effector, but future work will incorporate the state and configuration of the entire arm. Second, the current evaluation is conducted in fixed-viewpoint scenes; future research will explore flexible viewpoints, such as dynamic camera setups. Lastly, we aim to extend the scope of embodied tasksfrom the current manipulation tasks to more diverse domains, including navigation and mobile manipulation."
        },
        {
            "title": "References",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. AgiBot. Agibot world. https://agibot-world.com, 2024. Nir Aharon, Roy Orfaig, and Ben-Zion Bobrovsky. Bot-sort: Robust associations multi-pedestrian tracking. arXiv preprint arXiv:2206.14651, 2022. Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024. URL https://openai. com/research/video-generation-models-as-world-simulators, 3:1, 2024. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. Xiaowei Chi, Hengyuan Zhang, Chun-Kai Fan, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-min Chan, Wei Xue, Wenhan Luo, Shanghang Zhang, et al. Eva: An embodied world model for future video anticipation. arXiv preprint arXiv:2410.15461, 2024. Weixi Feng, Jiachen Li, Michael Saxon, Tsu-jui Fu, Wenhu Chen, and William Yang Wang. Tc-bench: Benchmarking temporal compositionality in text-to-video and image-to-video generation. arXiv preprint arXiv:2406.08656, 2024. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Hailuo. Hailuoai. https://hailuoai.video/, 2025. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In Advances in neural information processing systems, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Yue Liao, Peng Gao, Hongsheng Li, Maoqing Yao, et al. Enerverse: Envisioning embodied future space for robotics manipulation. arXiv preprint arXiv:2501.01895, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024a. 9 Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024b. Pengliang Ji, Chuyang Xiao, Huilin Tai, and Mingxiao Huo. T2vbench: Benchmarking temporal dynamics for text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53255335, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Kuaishou. Kling. https://app.klingai.com/cn/, 2025. Xinrang Ling, Chen Zhu, Meiqi Wu, Hangyu Li, Xiaokun Feng, Cundian Yang, Aiming Hao, Jiashu Zhu, Jiahong Wu, and Xiangxiang Chu. Vmbench: benchmark for perception-aligned video motion generation. arXiv preprint arXiv:2503.10076, 2025. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models learn physical principles from watching videos? arXiv preprint arXiv:2501.09038, 2025. Meinard Müller. Dynamic time warping. Information retrieval for music and motion, pages 6984, 2007. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training commercial-level video generation model in 200k. arXiv preprint arXiv:2503.09642, 2025. Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324, 2024. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In Advances in neural information processing systems, 2016. Jean Serra. Hausdorff distances and interpolations. Computational Imaging and Vision, 12:107114, 1998. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. FVD: new metric for video generation. In ICLRW, 2019. Cédric Villani and Cédric Villani. The wasserstein distances. Optimal transport: old and new, pages 93111, 2009. Andrew Vince. framework for the greedy algorithm. Discrete Applied Mathematics, 121(1-3):247260, 2002. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2025. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1(2):6, 2023. 10 Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qing, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models, 2023. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. URL https: //github.com/hpcaitech/Open-Sora. Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional Details on World Specification We provide implementation details for the pre-processing module. These components ensure consistency across different models and support metric-specific evaluation. Pre-Processing We resize all reference images to fixed resolution of 640 480. For the task prompt L, we use the aligned step-level caption corresponding to the current sub-action. To ensure viewpoint and temporal consistency, we append the following constraint to the prompt: Keep the first-person view of the robot unchanged. Keep the first frame of this video unchanged. Video Normalization All generated videos are resized to 640 480 and resampled to 30 FPS. The normalized video is used to evaluate task scene consistency and semantic alignment. Trajectory Extraction We extract 2D trajectories of both end-effectors using consistent detection pipeline. Specifically, we apply fine-tuned YOLO-World Cheng et al. [2024] model for per-frame detection and use BoT-SORT Aharon et al. [2022]for temporal association. To ensure fair evaluation, we compute the convex hull of each hands trajectory and select the hand with the largest spatial extentmeasured by the maximum Euclidean distance between any two points on the hullas the primary trajectory for motion evaluation. YOLO-World Training Details. We fine-tune yolov8s-worldv2 on 1451 manually annotated frames from Agibot-World. Two tasks (Freezer Restocking and Factory Packing) are held out for validation to construct hard-sample set. The model is trained for 100 epochs and achieves final performance of Recall: 0.91667, Precision: 1.0. A.2 Additional Details on Dataset Curation A.2.1 Task Selection and Scene Diversity To support evaluation across wide range of manipulation scenarios, we select 10 representative tasks from the Agibot-World AgiBot [2024] dataset. The selection prioritizes tasks with clear operational goals, sequential dependencies, and diverse object interactions. These tasks span both household and industrial contexts and are designed to challenge models across spatial reasoning, tool-use, and action ordering. Retrieving toast from toaster (Toaster) Pouring water (Pour Water) Setting cutlery (Place Cutlery) Restocking freezer (Restock Freezer) Producing ice (Produce Ice) Packing laundry detergent (Factory Packing) Cleaning bottles (Brush Bottle) Heating food in microwave (Heat Food) Installing showerhead (Hang Showerhead) Storing objects in drawer (Store in Drawer) These tasks exhibit substantial variation in manipulated object types (e.g., rigid, deformable, articulated), spatial layouts, and interaction complexity. Figure 7 visualizes the task scenes, their corresponding trajectory overlays, and object property distributions. A.2.2 Action Motion Sampling Strategy To construct comprehensive and diverse evaluation set, we uniformly sample 100 video instances per task. Each task is decomposed into fine-grained primitive actions, ensuring one-to-one correspondence between video segments, sub-action labels, and textual descriptions. 12 Figure 7: Dataset overview. The first two rows display selected task scenarios and their associated action motion trajectories. The third row categorizes object properties (e.g., fluid, articulated, rigid, deformable, multi-body) using color-coded legends and representative examples. For trajectory analysis, we extract the left and right end-effector positions and convert them into voxel grid representations. pairwise similarity matrix is computed using 3D Intersection over Union (IoU): IoUi,j = (cid:80) min(V , ) + (cid:80) min(V ) + (cid:80) max(V , ) ) + ϵ , , i (cid:80) max(V (2) where and denote the voxel grids for the left and right end-effectors, respectively. To promote trajectory diversity in the final evaluation subset, we adopt greedy selection algorithm Vince [2002]: Start by selecting the trajectory with the lowest average IoU relative to all others. Iteratively select the trajectory with the lowest average IoU to the already selected set. Repeat until 10 representative trajectories are selected for each task. This sampling strategy ensures that the evaluation set includes both common and atypical motion patterns, providing broad spectrum of behavioral diversity to test the generalization ability of generative models. The resulting distribution is reflected in Figure. 7. A.3 Additional Details on Metrics A.3.1 Scene Consistency: DINOv2 vs. VBench Analysis Fine-tuning DINOv2 for Embodied Scenes. To enhance the extraction of task-relevant visual features in embodied operation scenarios, we fine-tune DINOv2 Oquab et al. [2023] on the AgibotWorld dataset using 20,000 iterations of unsupervised training. We use the dinov2-vitb14-reg 13 Figure 8: Feature map comparison across models. DINOv2 fine-tuned on embodied data captures agents and tools with sharper spatial coherence, enabling more reliable scene stability evaluation. Figure 9: Scene Consistency failure cases. Despite camera movement and background drift, VBench assigns high scores (0.880.91). Our metric, however, detects the instability through decreased cosine similarity. checkpoint as the initialization. We visualize feature maps from three model variants: (1) the original DINO Zhang et al. [2022] (ViT-B/16, used in VBench), (2) pre-trained DINOv2, and (3) our fine-tuned DINOv2. As shown in Figure. 8, only the fine-tuned DINOv2 consistently focuses on task-relevant agents and manipulated tools, while the others fail to highlight key foreground regions or exhibit backgroundforeground entanglement. This justifies the necessity of adapting foundation models to the embodied task domain. Failure Cases of VBench Background Consistency. We further visualize representative cases in which VBench assigns high background consistency scores despite significant viewpoint changes or layout shifts. As shown in Figure. 9, the cosine similarity of our metric drops significantly in these cases, whereas VBench remains insensitive. This highlights the limitation of VBenchs DINO-ViT-B/16 features in separating foreground and background cues. A.3.2 Action Motion Metrics Symmetric Hausdorff Distance Consistency(HSD Consistency) The Symmetric Hausdorff DistanceSerra [1998] (SymH) measures the maximum spatial deviation between the generated trajectory and the corresponding ground truth trajectory. This distance represents the greatest of the minimum distances between points from both trajectories. HSD is particularly useful for evaluating the spatial alignment of the generated trajectory with the true trajectory, ensuring that the generated path adheres to expected movement patterns and does not deviate significantly from the true action trajectory. To ensure that the score is positively correlated with consistency, we take the reciprocal of this value: HSDscore = 1 dsymH(G, ) (3) Where represents the ground truth trajectory, and represents the generated trajectory. Normalized Dynamic Time Warping Distance Consistency(NDTW Consistency) NDTWMüller [2007] is used to evaluate the overall shape similarity and temporal alignment of trajectories. While HSD focuses on spatial deviations, NDTW evaluates the overall trajectory shape and how well the generated actions align with the timing and sequence of the true actions. This metric is particularly useful for capturing the temporal causality and task sequencing that the model should learn from the true trajectory. 14 By aligning both the spatial and temporal dimensions of the trajectories, NDTW assesses whether the generated sequence matches the timing and order of the ground truth actions. The similarity score is then calculated as the reciprocal of this value: NDTWscore ="
        },
        {
            "title": "1\nNDTW(G, P )",
            "content": "(4) Dynamic Consistency (DYN) Velocity and acceleration are critical components in robotic control, directly affecting the physical feasibility of the generated actions. To evaluate how well the predicted trajectories align with the real-world motion characteristics, we extract the 2D velocity and acceleration time series from both predicted and ground-truth trajectories. We then compute the Wasserstein distanceVillani and Villani [2009] (Earth Movers Distance, EMD) between these distributions to quantify their differences. The Wasserstein distance captures the global distributional alignment between sequences, it allows for soft matching and does not require strict temporal alignment, which makes it more robust and better suited to capturing continuous motion trends across the entire trajectory. To enhance the robustness of this metric across different motion amplitudes, inspired by the IoU calculation method, we introduce an amplitude normalization factor that uses the difference between the maximum and minimum velocity/acceleration to construct the following ratio: min (cid:0)max(vgt) min(vgt), max(vpred) min(vpred)(cid:1) + ϵ max (max(vgt) min(vgt), max(vpred) min(vpred)) + ϵ min (cid:0)max(agt) min(agt), max(apred) min(apred)(cid:1) + ϵ max (max(agt) min(agt), max(apred) min(apred)) + ϵ AR = VR = (6) (5) where ϵ = 1 108. To account for variations in motion amplitudes, we introduce amplitude normalization factors. The final dynamic consistency score is defined as: DYNscore = α VR 1 (v) + β AR 1 (a) , α = 0.007, β = 0.003 (7) Here, () represents the Wasserstein distance, and VR, AR are the amplitude normalization factors for velocity and acceleration, respectively. These corrections ensure that low-amplitude trajectories do not introduce numerical amplification, thereby preserving the accuracy of dynamic consistency assessment. A.4 Visual Examples of Model Generation Results To complement the discussion in Section 5, we provide qualitative examples illustrating the observed characteristics of recent SOTA models evaluated under WMBM. Domain-Adapted Models. Despite strong semantic alignment, domain-adapted models sometimes fail in action grounding. As shown in Figure.10, the generated video depicts precise scene and goal instruction, but the agent executes an empty grasp motion without interacting with the object. Figure 10: Domain-adapted model failure case. The robot hand moves toward the correct region but fails to close the gripper on the object, resulting in empty grasping. LTX and COSMOS. Figure. 12 shows LTX generating abrupt scene transitions and failing to maintain object continuity. LTX and COSMOS, as in Figure. 13, frequently renders human hands 15 Figure 11: Despite being given explicit camera viewpoint control instructions, COSMOS fails to maintain consistent viewpoint throughout the video. This indicates limitation in its ability to follow spatial constraints, leading to unstable or drifting perspectives. Figure 12: Examples illustrating the poor task understanding and temporal instability of the LTX model. The middle column shows LTX-generated frames, which often exhibit abrupt visual changes and scene inconsistencies. In contrast, the rightmost column presents examples with better scene preservation, highlighting the gap in temporal coherence. instead of robot arms, revealing semantic adaptation failure.A failure of viewpoint control in COSMOS is illustrated in Figure. 11. OpenSora.As discussed in the main text, OpenSora shows partial understanding of task semantics but struggles with motion control. Figure. 14 presents representative example highlighting this issue: while the generated scene correctly reflects the intended manipulation context, the robotic arm undergoes significant jitter and fails to execute smooth movements. This supports our observation that OpenSoras motion instability remains key limitation in embodied video generation. Figure 13: Generated videos from COSMOS and LTX models often depict human hands instead of robotic arms, indicating bias toward human hand representations in their training data. This bias hinders the models ability to correctly generalize to robotic manipulation tasks, where accurate mapping to robotic arms is essential. 16 Figure 14: An example from OpenSora illustrating unstable robotic arm motion. Although the scene is semantically aligned with the manipulation task, the arm exhibits visible jitter and lacks smooth trajectory control, consistent with the motion instability discussed in the main text."
        }
    ],
    "affiliations": [
        "AgiBot",
        "HIT",
        "MMLab-CUHK",
        "SJTU"
    ]
}