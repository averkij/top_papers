{
    "paper_title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation",
    "authors": [
        "Xiuwei Xu",
        "Angyuan Ma",
        "Hankun Li",
        "Bingyao Yu",
        "Zheng Zhu",
        "Jie Zhou",
        "Jiwen Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training a generalized visuomotor policy via imitation learning. Prior works explore a promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose a real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulator- and rendering-free, thus being efficient and plug-and-play. Specifically, given a single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. A group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 7 4 5 8 0 . 0 1 5 2 : r Preprint. Work in progress R2RGEN: REAL-TO-REAL 3D DATA GENERATION FOR SPATIALLY GENERALIZED MANIPULATION Xiuwei Xu1, Angyuan Ma1, Hankun Li1, Bingyao Yu1, Zheng Zhu2, Jie Zhou1, Jiwen Lu1 1Tsinghua University, 2GigaAI"
        },
        {
            "title": "ABSTRACT",
            "content": "Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training generalized visuomotor policy via imitation learning. Prior works explore promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulatorand rendering-free, thus being efficient and plug-andplay. Specifically, given single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation. Website."
        },
        {
            "title": "INTRODUCTION",
            "content": "Robotic manipulation with visuomotor policy Chi et al. (2023); Zhao et al. (2023); Fu et al. (2024) has achieved great progress in recent years, while the reliance on large amount of human-collected data during imitation learning becomes the main bottlenecks for application and further scaling up Lin et al. (2024). Unlike most prior work focused on fixed-tabletop arms, this paper studies more general manipulation setting involving mobile manipulators. Since the mobile base may be located at arbitrary positions, the resulting viewpoint variation further increases the policys reliance on extensive training data Tan et al. (2024). Spatial generalization constitutes the primary factor driving the substantial data demand during visuomotor policy learning. As pointed out by Garrett et al. (2024), control difficulty is not uniformly distributed among the human-collected trajectory. Note trajectory can be divided into two categories of segments: contact-rich segments involving the interaction between robotic arm and objects, and other segments simply indicating the movement of robotic arm in free space, which are also known as skill and motion segments respectively. Skill segments are generally more challenging, whereas motion segments can often be handled effectively through motion planning. However, even thought skill segments are more informative, the majority of human demonstration effort is typically devoted to teaching motion behaviors. For instance, in task such as put apple on plateeven with identical apple, plate, and pick and place skillhundreds of demonstrations may be needed to cover varying objects spatial arrangements and robots base positions to learn generalized policy. Therefore, spatial generalization remains fundamental bottleneck in data efficiency. To reduce redundant human effort on ensuring spatial generalization, MimicGen Jiang et al. (2024) and follow-up works Hoque et al. (2024); Garrett et al. (2024); Jiang et al. (2024) replace the tedious Equal contribution. Corresponding author. Preprint. Work in progress Figure 1: R2RGen is simulator-free data generation framework. Given one human-collected demonstration, R2RGen directly parses and edits both pointcloud observations and action trajectories in shared 3D space. R2RGen achieves strong spatial generalization on diverse complex tasks. relocate-and-recollect data collection procedure with automatic demonstration generation. These methods only require few human-collected data, based on which they augment object configurations and apply transformation and interpolation to generate diverse trajectories with different motion patterns. Though achieving satisfactory performance in sumulation, these methods require on-robot rollouts to collect real-world observation-action pair, which takes much more time and relies on human supervision. Recently, DemoGen Xue et al. (2025) introduces 3D-based data generation method that builds on point-cloud input policy Ze et al. (2024b). By operating directly in the 3D domain, the approach augments object point clouds to synthesize varied trajectories along with their corresponding visual observations. This pipeline is simulatorand rendering-free, thus being very efficient and avoiding sim-to-real gap. However, DemoGen exhibits several critical limitations that restrict its practical use: (1) It focuses on fixed-base setting, so viewpoint change is not taken into consideration; (2) it imposes strong assumption on input data, where the pointcloud of environment should be cropped, up to 2 objects are supported, and each skill must involve only one target object; (3) it suffers from visual mismatch problem, i.e., large augmentation leads to incomplete pointcloud observation. Due to these constraints, DemoGen does not fully achieve practical real-to-real generation and remains limited in handling mobile manipulation and diverse task configurations. In this paper, we propose R2RGen, real-to-real 3D data generation framework which is applicable for mobile robot Fu et al. (2024), works on raw pointcloud observation and supports any number of objects and interaction modes. Given pre-segmented source demonstration, previous methods apply spatial transformation on each object individually. This object-centric paradigm can only handle skills relevant to only one target object. To overcome this problem, we propose group-wise data augmentation, which links each skill to group of objects rather than single target to maintain necessary object combination for complicated skills. It also leverages backtracking mechanism to augment the 3D observation without disturbing the causal order of each operation. Moreover, since pointcloud from 3D sensor (e.g. RGB-D camera) is incomplete, large transformation (especially rotation) will make the augmented 3D observation unreasonble. E.g., points should be observed are missing, while points should be occluded exists. To this end, we further present camera-aware 3D post-processing to ensure the 3D observation after augmentation obey the distribution of 3D sensor. Through extensive real-world evaluation, we show that R2RGen, trained with only one human demonstration, outperforms policies trained with 25 more human-collected data. Furthermore, R2RGen exhibits strong scalability with additional demonstrations, generalizes beyond spatial variation, and demonstrates promising potential for applications such as mobile manipulation."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Imitation learning for robotic manipulation: With the development of robotic data collection system and model architecture, using imitation learning to train visuomotor policies Zhao et al. 2 Preprint. Work in progress (2023); Chi et al. (2023); Prasad et al. (2024); Wang et al. (2024a;b); Ze et al. (2024b;a), which end-to-end predict actions from visual observation, becomes promising way to learn dexterous manipulation skills from human demonstrations. Inspired by the success of large language models (LLM) and vision language models (VLM), there are multiple recent works exploring scaling up the model size and data amount to train generalist robot policies with imitation learning. One promising approach for training such generalist are vision-language-action models (VLA) Brohan et al. (2023); Kim et al. (2024b); Black et al. (2024); Cheang et al. (2024); Zhang et al. (2024); Cheng et al. (2024a); Li et al. (2024), which finetunes VLM pre-trained on internet-scale data for robot control. Though being flexible, imitation learning methods are data-intensive due to the lack of skill priors. In fact, to achieve strong generalization ability, visuomotor policy requires large amount of data for training / finetuning. Since largest embodied datasets ONeill et al. (2024); Khazatsky et al. (2024) are still much smaller than the counterparts in vision and language fields Deng et al. (2009); Schuhmann et al. (2022), current works manage to solve this problem with advanced data collection systems like UMI Chi et al. (2024); Ha et al. (2024) and VR Cheng et al. (2024b); Ding et al. (2024), or empirical studies on data scaling Zhao et al. (2024); Lin et al. (2024); Zha et al. (2025) and data selection Hejna et al. (2024); Zhang et al. (2025) techniques. Data generation for visuomotor policy: In order to train generalized manipulation visuomotor policy with less human labor, automatic data generation has been paid increased attention in recent years. branch of works Hua et al. (2024); Wang et al. (2023a;b); Katara et al. (2024) utilize the common knowledge from LLM / VLM and privileged information from simulatorfor zero-shot task and motion planning. To improve data quality, some works generate robotic data from human demonstration video Duan et al. (2023); Lepert et al. (2025); Yu et al. (2025), which fully exploits structured skill primitives from human to acquire reasonable data. However, these methods still rely on vision foundation models Kim et al. (2024a); Kerr et al. (2024) to estimate and track poses of hand and object / part, which may not be accurate enough. Different from generating robotic data in robot-free manner, MimicGen Mandlekar et al. (2023) and its follow-up works Hoque et al. (2024); Garrett et al. (2024); Jiang et al. (2024) expand real-world demonstrations acquired from teleoperation by synthesizing different execution plans in simulator. These methods work well for simulation, but suffer from time-consuming on-robot rollouts to acquire real-world observation-action pairs. More recently, DemoGen Xue et al. (2025) proposes to apply augmentation on real-world 3D visual input as well as the trajectory. By using 3D policy, DemoGen directly takes the augmented 3D pointcloud as input, thus being simulatorand rendering-free. The real-to-real generation paradigm is efficient and plug-and-play without simulator setup, but currently DemoGen struggles on strong input assumption and visual mismatch problems which severely hinder its application."
        },
        {
            "title": "3 APPROACH",
            "content": "3.1 PROBLEM STATEMENT Visuomotor policy learning: Robotic manipulation task can be modeled as Partially Observable Markow Decision Process (POMDP) with visuomotor policy π : (cid:55) A, which defines function that maps current RGB-D observation ot to the robots action at A. ot = (It, Pt), where It is RGB observation and Pt is the pointcloud in camera coordinate system lifted from depth observation and camera intrinsics. To train this policy, large dataset of demonstrations = }N {oi i=1 should be collected. To reduce human labor, we aim to improve data efficiency by generating spatially diverse data with only one human-collected source demonstration Ds D. Formally written as: 1, ..., oi Hi 1, ai , ai Hi g, D2 g, ..., DN = {Ds, D1 }, {Di It is expected that we can train spatially generalized visuomotor policy purely from D. As real-to-real framework, we directly augment the 3D observation as well as the action trajectory to generate diverse observation-action pair. Therefore, π should be 3D policy that directly takes in pointcloud as visual input. We opt for iDP3 Ze et al. (2024a) as our policy, which consumes the egocentric pointcloud Pt without requirement on camera pose. i=1 = R2RGen(Ds) g}N (1) Assumptions: We make the following assumptions: (1) The visuomotor policy only predicts the actions of robotic arm. Although we support manipulation with different base positions, the mobile base remains fixed during each individual task execution. (2) The visual observation is captured with 3 Preprint. Work in progress RGB-D camera, which should be fixed along with the base during task execution. (3) Similar to Mandlekar et al. (2023), the actions in demonstration can be treated as sequence of continuous end effector pose and discrete gripper state. Formally, at = (Aee is SE(3) end-effector pose. We align the coordinate system of Aee with the camera coordinate system to maintain consistent representation. ), where Aee t , agrip t"
        },
        {
            "title": "3.2 SOURCE DEMONSTRATION PRE-PROCESSING",
            "content": "Given the source demonstration Ds = {o1, a1, ..., oHs , aHs}, we need to fully parse the 3D observations {Pt} into editable composition of objects, and parse the action trajectory {at} into motion and skill segments, which facilitates further data generation. Formally, the goal of this pre-processing stage is: (1) Scene parsing. Assume the task of Ds involves relevant objects. So for each timestamp t, the pointcloud observation ot should be segmented into object pointclouds {P 1 }, one environment pointcloud and the pointcloud of robots arm. (2) Trajectory parsing. Same with previous definition Garrett et al. (2024); Xue et al. (2025) of motion and skill, {at} should be segmented into sequence of interleaved motion / skill segments {a1, ..., am1 } (motion-1), {am1+1, ..., as1} (skill-1), {as1+1, ..., am2} (motion-2), {am2+1, ..., as2 } (skill-2), etc. The objects relevant to each skill should also be known. , ..., For scene parsing, we can segment the objects in the first frame I1 and track through the whole video. The 2D masks can be projected into 3D to obtain pointcloud of each object. However, only segmenting each objects from {Pt} is insufficient due to the incompleteness of RGBD observation. For instance, the pointcloud of cup may only represent the side facing the camera, leaving the occluded side unobserved. As result, the observation will be incomplete when generating demonstrations from novel viewpoint relative to the object. To this end, in addition to segmentation, we further complete the object pointclouds (cid:101)P ). We adopt template-based 3D object tracking system Wen et al. (2024) to achieve this, which generates complete object pointt }Hs clouds { (cid:101)P 1 t=1 for all frames given object masks in the first frame I1. We also need to acquire the complete environment pointcloud (cid:101)P e. Since the camera is fixed during task execution, (cid:101)P is static regardless of time. Therefore, we can remove the objects and get an observation o0 before we collect Ds. Then simply set (cid:101)P as P0. Given the complete objects { (cid:101)P 1 } and environment (cid:101)P e, the arm pointcloud can be obtained by ). We do not complete since we empirically find it brings negligible influence. As result, the pointcloud observation Pt for each frame is parsed into the combination of complete objects, environment and robots arm. Figure 2: Pre-processing results. The 3D scene is parsed into complete objects, environment and robots arm. The trajectory is parsed into interleaved motion and skill segments. = Pt ( (cid:101)P (cid:101)P 1 ... (cid:101)P t = C(P , ..., (cid:101)P , ..., (cid:101)P For trajectory parsing, we introduce lightweight annotation system. The interface plays the RGB video {I1, ..., IHs} and asks the annotator to label the start frame and end frame of each skill. The intermediate trajectories between two skill segments are classified as motion segments. Apart from annotating skill segments, the annotator also specifies the object IDs (ranging from 1 to K) associated with each skill. Specifically, for every skill, both target object IDs and in-hand object ID are provided: the in-hand object refers to the item being held by the gripper during the skill execution (if any), while the target objects denote the entity with which the gripper interacts. IDs may be null, single object, or multiple objects, depending on the task structure. The entire process uses only the RGB video as input and requires less than 60 seconds per demonstration, making it efficient and minimally labor-intensiveparticularly suitable for settings with very few source demonstrations. Figure 2 illustrates the parsed results of source demonstration. More details about the object parsing and trajectory parsing systems can be found in Appendix A.1. Preprint. Work in progress Figure 3: The pipeline of R2RGen. Given processed source demonstration, we backtrack skills and apply group-wise augmentation to maintain the spatial relationships among target objects, where fixed object set is maintained to judge whether the augmentation is applicable. Then motion planning is performed to generate trajectories that connect adjacent skills. After augmentation, we perform camera-aware processing to make the pointclouds follow distribution of RGB-D camera. The solid arrows indicate the processing flow, while the dashed arrows indicate the updating of fixed object set. 3.3 GROUP-WISE DATA AUGMENTATION To synthesize new demonstrations, we randomly augment the location and rotation of objects and environment to acquire new scene configurations, and generate the corresponding action trajectories. Previous pointcloud-based data generation methods Xue et al. (2025) assume only one target object per skill and transform skill segments solely based on that objects transformation. Motion segments are then generated using planner to connect adjacent skills into complete trajectory. However, this approach fails when skill involves multiple target objects whose spatial relationships must be preserved. For instance, for task \"build bridge\" shown in Figure 3, placing the bridge deck (object-3) requires the two bridge piers (object-1 and object-2) to be positioned at specific relative distance. Independently augmenting each pier would disrupt this relationship and prevent successful execution of the final skill. To support arbitrary interaction modes, we propose group-wise augmentation strategy that maintains structural constraints among multiple objects during data generation. Group-wise backtracking: Instead of modeling skills as object-centric, we assign each skill to group of objects consists of the annotated target and in-hand objects. All objects within the same group undergo identical geometric transformations (i.e., the same translation and rotation). Augmentations are performed in backtracking manner to avoid causal conflicts among object states. Formally, we begin from the last skill (skill-n) {amn+1, ..., asn }. Denote On = Otar is the ID set of target and in-hand objects of current skill, and On = is the ID set of fixed objects (i.e., objects cannot be augmented). We decide whether to augment current group according to On On. If the intersection is , we randomly sample transformation matrix Tn R44 to apply XY-plane translation and Z-axis rotation on group On (XY plane is fitted through the tabletop point cloud). Otherwise this group is fixed at current time and we cannot augment it. After applying the group transformation, the fixed object set is updated as follows: Ohand On1 = (On Otar ) Ohand (2) where current group is appended into the fixed set to maintain spatial relationships, while in-hand object is released since its state before grasped is independent of current skills constraints. We then proceed to skill-(n 1) and repeat above operations until all skills are traversed. Skill augmentation: For skill-i, if the corresponding group is not fixed, we apply transformation Ti to augment the end effectors pose while remain the gripper state: ˆA ee = Aee Ti, ˆagrip = agrip , [mi + 1, si] (3) 5 Preprint. Work in progress Then the pointclouds of objects (cid:101)P are transformed with Ti in the same way. Motion augmentation: For motion-i, we apply motion planning to generate trajectory that starts at the end of skill-(i 1) and ends at the beginning of skill-i: (k Oi) and arm ˆA ee t1:t2 = MotionPlan( ˆAsi1 , ˆAmi+1), ˆagrip t1:t2 = agrip t1:t2, t1 = si1 + 1, t2 = mi (4) Then the pointclouds of in-hand objects (cid:101)P ee relative pose transformation (Aee . )1 ˆA (k Ohand ) and arm are transformed with the Finally, we apply random transformation Te on the environment (cid:101)P for all frames, which simulates the viewpoint change of robot. An algorithm diagram of the group-wise augmentation pipeline is detailed in Appendix A.2. For bimanual manipulation, we additionally introduce constraints to ensure the generated demonstrations executable. Refer to Appendix A.3 for more details on bimanual tasks."
        },
        {
            "title": "3.4 CAMERA-AWARE 3D POST-PROCESSING",
            "content": "After training on generated demonstrations D, the 3D policy is deployed in real-world with RGB-D camera as input sensor. Therefore, the pointcloud observation in should be similar to the raw RGB-D observation. Currently, there are two main differences: (1) The generated pointcloud is over-complete. While for given perspective of RGB-D camera, raw pointcloud converted from depth image is complete only at this viewpoint. (2) Due to the augmentation on environment, the spatial distribution of generated pointcloud is shifted. To solve this, we propose camera-aware 3D post-processing to adjust the distribution of generated pointcloud observations { ˆPt}: ˆP adjust = 1(Fill(Z-buffer(Crop({(ui, vi, di)})))), {(ui, vi, di)} = P( ˆPt) (5) where projects 3D pointcloud to image plane with camera intrinsics. The Crop operation removes pixels {(ui, vi, di)ui < 0 or ui or vi < 0 or vi H} which are out of image boundary. Z-buffer processes overlapped pixels and only keep one pixel with smallest depth value, which removes hidden points at current viewpoint. In practice, we notice the density of pointclouds may not be so high, making front surface unable to hide all points behind. Therefore, we propose patch-wise Z-buffer operator, where each point with small depth value can hide deeper points in r-radius neighborhood on image plane. Since the environment is augmented, pixels near the image boundary may be empty (i.e., no point is projected to these pixels). So we Fill the empty pixels by either shrinking the image size or expanding the environment pointcloud, which we detail in Section 4.4. Finally, after post-processing in the image plane, we project the pixels back to camera coordinate system. The adjusted pointcloud ˆP adjust well matches the distribution of RGB-D camera and can be directly fed into our 3D policy during training. t"
        },
        {
            "title": "4 EXPERIMENT",
            "content": "In this section, we evaluate R2RGen through extensive real-world experiments, demonstrating its effectiveness on one-shot imitation learning and how it scales up with more source demonstrations. We also conduct comprehensive ablation studies to explore the optimal design choices. Furthermore, we show R2RGen can be extended to facilitate appearance generalization and mobile manipulation. 4.1 EXPERIMENTAL SETUP Policy. We select iDP3 Ze et al. (2024a) as the visuomotor policy, which takes egocentric pointclouds (i.e., pointclouds in camera coordinate system) and proprioception state as inputs without requirement on camera pose and calibration. Details on training iDP3 on each task are described in Appendix A.4. Hardware. We utlize two robot platforms: single-arm and bimanual. The single-arm platform consists of 7-DoF UR5 arm equipped with parallel jaw gripper and mobile base. An ORBBEC femto bolt camera is rigidly affixed via mounting bracket to the mobile base, which provides the RGB-D observations. The bimanual platform adheres to the MobileAloha architecture Fu et al. (2024), employing dual AgileX PiPER arms integrated with HexFellow omnidirectional mobile base. head-mounted RGB-D camera provides egocentric perception. See Appendix for further details. 6 Preprint. Work in progress Table 1: Real-world evaluation of R2RGen for spatial generalization. Success rate is reported. Open-Jar Place-Bottle Single-Arm Task Pot-Food Hang-Cup Stack-Brick Build-Bridge Grasp-Box Store-Item Dual-Arm Task 1 Source +DemoGen +R2RGen 10 Source 25 Source 40 Source 3.1 18.8 50.0 56.3 78.1 87.5 3.1 15.6 50. 34.3 53.1 68.8 3.1 37.5 9.4 21.9 28.1 3.1 34.4 15.6 43.8 43.8 3.1 43. 9.4 40.6 50.0 3.1 34.4 9.4 28.1 43.8 4.2 16.7 41.7 25.0 29.2 37.5 4.2 16.7 33. 20.8 33.3 41.7 Averaged 3.4 40.3 22.5 41.0 50.2 Figure 4: Visualization of our real-world tasks. We show the start and end moments of each task. Tasks and evaluation. We design 8 representative tasks for evaluation, including 2 simple tasks (Open-Jar, Place-Bottle), 4 complex tasks (Pot-Food, Hang-Cup, Stack-Brick, Build-Bridge) and 2 bimanual tasks (Grasp-Box, Store-Item), as visualized in Figure 4. We compare with DemoGen Xue et al. (2025) on the two simple single-arm tasks and two bimanual tasks. While for other tasks involving complex spatial relationships among objects, DemoGen fails to generate reasonable data. We evaluate different methods on diverse objects locations and rotations as well as robots viewpoints, including portion of out-of-distribution samples not encountered during training. Refer to Appendix for detailed task definition and evaluation protocol. 4.2 RESULTS: ONE-SHOT IMITATION LEARNING For one-shot imitation learning, we only collect one human demonstration for R2RGen to generate new data. Similar to DemoGen, we replay the collected human demonstration twice to acquire diverse pointcloud observations, which significantly reduce the impact of sensor noise. The three pointcloud trajectories are all used for 3D data generation. Then we compare the policy trained on purely generated data with ones trained on different number of human demonstrations, as shown in Table 1. When trained with only one human demonstration, the policy succeeds merely at the demonstrated pose but fails to generalize. Both DemoGen and R2RGen improve its performance. It is shown that R2RGen consistently outperforms DemoGen across all tasks, even though DemoGen crops pointclouds of background while we do not. This advantage primarily stems from our scene parsing and camera-aware processing techniques, which enable generating high-quality data under large variations in object location / rotation and robot viewpoint. In contrast, DemoGen suffers from significant visual mismatch under such challenging evaluation. R2RGen achieves performance comparable to policies trained with 25 human demonstrations, and even surpasses 40 demonstrations on several difficult tasks, which validates its effectiveness on spatial generalization. 4.3 RESULTS: PERFORMANCE-ANNOTATION TRADEOFF We further study how R2RGen scales up with more source demonstrations. For each task, we run R2RGen to generate data from 1, 2, 3 and 5 human demonstrations respectively. We then report the policy performance boost w.r.t. the increase of synthetic data under different numbers of source demonstrations. As shown in Figure 5, the success rate gradually saturates as the number of generated demonstrations increasesa trend also observed with human-collected data. This behavior is due to the limited capacity of the iDP3 policy, which uses lightweight PointNet encoder. Further scaling beyond this plateau would require policies with larger 3D backbones. We also note that more source demonstrations lead to higher saturation performance, demonstrating R2RGens ability to effectively leverage additional data for improved results. 7 Preprint. Work in progress Figure 5: Effects of the number of generated demonstrations and source demonstrations on the final performance of R2RGen. Table 2: Effects of the pointcloud processing. Table 3: Effects of camera-aware processing. Method Remove pointcloud completion Remove environment pointcloud Remove environment augmentation R2RGen SR 12.5 18.8 28.1 50.0 Method Remove Crop operation Remove Z-buffer operation Remove Fill operation R2RGen SR 34.4 15.6 28.1 50.0 4.4 ABLATION STUDY We further conduct ablation experiments to study the effects of each design choice. The analysis is conducted on the Place-Bottle task. Pointcloud processing. We first study how object and environment pointclouds affect the final performance, as shown in Table 2. We notice removing pointcloud completion will lead to unrealistic data under large spatial augmentation, while removing operations on environment reduces the policys robustness to viewpoint changes. Camera-aware processing. We then ablate camera-aware processing by removing one of the key operations at each time. As shown in Table 3, these operations play critical role in determining final performance. For the Fill operation, we further compare two design choices in Figure 6: by shrinking the image size or expanding the environment pointcloud to fill the black dashed outline (i.e., the valid-depth area of the RGB-D camera). If shrinking is adopted, we will apply the same shrinking to raw RGB-D observations when deploying the policy trained with R2RGen. Empirically, both methods achieve comparable performance; we ultimately adopt shrinking due to its operational simplicity and lack of additional processing requirements. Figure 6: Two implementations of Fill operation, i.e., shrinking and expanding. 4.5 EXTENSION AND APPLICATION Extension: appearance generalization. Beyond spatial generalization, robotic manipulation tasks involve other forms of generalization, such as appearance generalization (i.e., adapting to novel object instances and environments) and task generalization. Among these, spatial generalization serves as the fundamental prerequisite for other generalization capabilities. Since this work focuses on single-task visuomotor policy learning, we investigate whether the spatial generalization enabled by R2RGen can further facilitate appearance generalization. As shown in Figure 7, we design more 8 Preprint. Work in progress Figure 7: Extension on appearance generalization. The spatial generalization of R2RGen can serve as foundation to achieve other kinds of generalization with much less data. challenging Place-Bottle task with four distinct bottle-base appearance combinations (2 bottle types 2 base types). We observe that achieving both appearance and spatial generalization significantly increases data demand. Even with 40 human demonstrations (10 per bottle-base pair), the policy only reaches 25% success rate. In contrast, using R2RGen, only 1 demonstration per bottle-base pair (4 in total) is needed to achieve success rate of 43.8%, demonstrating its efficiency in handling combined generalization challenges. Application: mobile manipulation. The strong spatial generalization ability achieved by R2RGen can also facilitate the challenging mobile manipulation task. Being generalized to different camera views, we solve mobile manipulation as simple combination of navigation and fixed-base manipulation. Here we adopt MoTo Wu et al. (2025) as the navigation strategy to approach the target object, and then apply policy trained with R2RGen to complete the task. The experimental video demo is shown in our project page. Refer to Appendix for more details."
        },
        {
            "title": "5 CONCLUDING REMARK",
            "content": "R2RGen introduces real-to-real 3D data generation framework that generalizes beyond prior pointcloud-based methods such as DemoGen Xue et al. (2025). Specifically, it supports mobile manipulators, raw sensor inputs, arbitrary numbers of objects, and diverse interaction modes, overcoming key limitations of existing approaches. With only one human demonstration, our method directly augments the 3D observation as well as the action trajectories to generate large amount of pointcloudaction pairs, which are utilized to train spatially generalized 3D policy. The overall pipeline is carefully designed with user-friendly pre-processing, group-wise augmentation and camera-aware post-processing, which ensures the visual observations are complete and spatial relationships between objects are maintained. Extensive experiments on multiple real-world tasks validate the effectiveness of R2RGen. We further extend its application to appearance generalization and mobile manipulation scenarios, demonstrating its strong generalizability and potential for broad real-world deployment. Potential Limitations. Currently, there are two major limitations of R2RGen. (1) The RGB-D sensor should be fixed during task execution. So R2RGen cannot be applied to source demonstration collected with wrist camera or moving base. (2) During pre-processing of source demonstration, we apply template-based 3D object tracking model Wen et al. (2024) to acquire complete object pointclouds for all frames, which only works for rigid objects. For non-rigid objects, please refer to Appendix A.1 for more details. 9 Preprint. Work in progress"
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To support reproducibility, we provide detailed description of the R2RGen framework in the main text and appendix, including algorithm outline and implementation specifics. minimal runnable example demonstrating the core data generation process is available on our anonymous project page: https://r2rgen.github.io. This demo allows users to execute key stages of R2RGen in predefined scenario and adjust hyperparameters interactively. Due to the integrated nature of the full pipelinewhich involves template reconstruction, data annotation, trajectory generation, and real-robot deploymentthe complete code is still being cleaned. We commit to releasing the full codebase (covering both R2RGen method and real-robot deployment) upon acceptance of the paper."
        },
        {
            "title": "REFERENCES",
            "content": "Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, and Xiaolong Wang. Navila: Legged robot vision-language-action model for navigation. arXiv preprint arXiv:2412.04453, 2024a. Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and Xiaolong Wang. Open-television: Teleoperation with immersive active visual feedback. arXiv preprint arXiv:2407.01512, 2024b. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. IJRR, 2023. Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and Shuran Song. Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. arXiv preprint arXiv:2402.10329, 2024. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pp. 248255. Ieee, 2009. Runyu Ding, Yuzhe Qin, Jiyue Zhu, Chengzhe Jia, Shiqi Yang, Ruihan Yang, Xiaojuan Qi, and Xiaolong Wang. Bunny-visionpro: Real-time bimanual dexterous teleoperation for imitation learning. arXiv preprint arXiv:2407.03162, 2024. Jiafei Duan, Yi Ru Wang, Mohit Shridhar, Dieter Fox, and Ranjay Krishna. Ar2-d2: Training robot without robot. arXiv preprint arXiv:2306.13818, 2023. Zipeng Fu, Tony Zhao, and Chelsea Finn. Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. arXiv preprint arXiv:2401.02117, 2024. Caelan Garrett, Ajay Mandlekar, Bowen Wen, and Dieter Fox. Skillmimicgen: Automated demonstration generation for efficient skill learning and deployment. arXiv preprint arXiv:2410.18907, 2024. Huy Ha, Yihuai Gao, Zipeng Fu, Jie Tan, and Shuran Song. Umi on legs: Making manipulation policies mobile with manipulation-centric whole-body controllers. arXiv preprint arXiv:2407.10353, 2024. Joey Hejna, Chethan Bhateja, Yichen Jiang, Karl Pertsch, and Dorsa Sadigh. Re-mix: Optimizing data mixtures for large scale imitation learning. arXiv preprint arXiv:2408.14037, 2024. 10 Preprint. Work in progress Ryan Hoque, Ajay Mandlekar, Caelan Garrett, Ken Goldberg, and Dieter Fox. Intervengen: Interventional data generation for robust and data-efficient robot imitation learning. In IROS, pp. 28402846. IEEE, 2024. Pu Hua, Minghuan Liu, Annabella Macaluso, Yunfeng Lin, Weinan Zhang, Huazhe Xu, and Lirui Wang. Gensim2: Scaling robot data generation with multi-modal and reasoning llms. arXiv preprint arXiv:2410.03645, 2024. Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, and Yuke Zhu. Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning. arXiv preprint arXiv:2410.24185, 2024. Pushkal Katara, Zhou Xian, and Katerina Fragkiadaki. Gen2sim: Scaling up robot learning in simulation with generative models. In ICRA, pp. 66726679. IEEE, 2024. Justin Kerr, Chung Min Kim, Mingxuan Wu, Brent Yi, Qianqian Wang, Ken Goldberg, and Angjoo Kanazawa. Robot see robot do: Imitating articulated object manipulation with monocular 4d reconstruction. arXiv preprint arXiv:2409.18121, 2024. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. Chung Min Kim, Mingxuan Wu, Justin Kerr, Ken Goldberg, Matthew Tancik, and Angjoo Kanazawa. Garfield: Group anything with radiance fields. In CVPR, pp. 2153021539, 2024a. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024b. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, pp. 40154026, 2023. Marion Lepert, Jiaying Fang, and Jeannette Bohg. Phantom: Training robots without robots using only human videos. arXiv preprint arXiv:2503.00779, 2025. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, and Yang Gao. Data scaling laws in imitation learning for robotic manipulation. arXiv preprint arXiv:2410.18647, 2024. Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. arXiv preprint arXiv:2310.17596, 2023. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In ICRA, pp. 68926903. IEEE, 2024. Aaditya Prasad, Kevin Lin, Jimmy Wu, Linqi Zhou, and Jeannette Bohg. Consistency policy: Accelerated visuomotor policies via consistency distillation. arXiv preprint arXiv:2405.07503, 2024. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 11 Preprint. Work in progress Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 35:2527825294, 2022. Hengkai Tan, Xuezhou Xu, Chengyang Ying, Xinyi Mao, Songming Liu, Xingxing Zhang, Hang Su, and Jun Zhu. Manibox: Enhancing spatial grasping generalization via scalable simulation data generation. arXiv preprint arXiv:2411.01850, 2024. Dian Wang, Stephen Hart, David Surovik, Tarik Kelestemur, Haojie Huang, Haibo Zhao, Mark Yeatman, Jiuguang Wang, Robin Walters, and Robert Platt. Equivariant diffusion policy. arXiv preprint arXiv:2407.01812, 2024a. Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, and Xiaolong Wang. Gensim: Generating robotic simulation tasks via large language models. arXiv preprint arXiv:2310.01361, 2023a. Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, and Chuang Gan. Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. arXiv preprint arXiv:2311.01455, 2023b. Zhendong Wang, Zhaoshuo Li, Ajay Mandlekar, Zhenjia Xu, Jiaojiao Fan, Yashraj Narang, Linxi Fan, Yuke Zhu, Yogesh Balaji, Mingyuan Zhou, et al. One-step diffusion policy: Fast visuomotor policies via diffusion distillation. arXiv preprint arXiv:2410.21257, 2024b. Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. In CVPR, pp. 1786817879, 2024. Zhenyu Wu, Angyuan Ma, Xiuwei Xu, Hang Yin, Yinan Liang, Ziwei Wang, Jiwen Lu, and Haibin Yan. Moto: zero-shot plug-in interaction-aware navigation for general mobile manipulation. In CoRL, 2025. Zhengrong Xue, Shuying Deng, Zhenyang Chen, Yixuan Wang, Zhecheng Yuan, and Huazhe Xu. Demogen: Synthetic demonstration generation for data-efficient visuomotor policy learning. arXiv preprint arXiv:2502.16932, 2025. Justin Yu, Letian Fu, Huang Huang, Karim El-Refai, Rares Andrei Ambrus, Richard Cheng, Muhammad Zubair Irshad, and Ken Goldberg. Real2render2real: Scaling robot data without dynamics simulation or robot hardware. arXiv preprint arXiv:2505.09601, 2025. Yanjie Ze, Zixuan Chen, Wenhao Wang, Tianyi Chen, Xialin He, Ying Yuan, Xue Bin Peng, and Jiajun Wu. Generalizable humanoid manipulation with improved 3d diffusion policies. arXiv preprint arXiv:2410.10803, 2024a. Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. arXiv preprint arXiv:2403.03954, 2024b. Lihan Zha, Apurva Badithela, Michael Zhang, Justin Lidard, Jeremy Bao, Emily Zhou, David Snyder, Allen Ren, Dhruv Shah, and Anirudha Majumdar. Guiding data collection via factored scaling curves. arXiv preprint arXiv:2505.07728, 2025. Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and He Wang. Navid: Video-based vlm plans the next step for vision-andlanguage navigation. arXiv preprint arXiv:2402.15852, 2024. Yu Zhang, Yuqi Xie, Huihan Liu, Rutav Shah, Michael Wan, Linxi Fan, and Yuke Zhu. Scizor: self-supervised approach to data curation for large-scale imitation learning. arXiv preprint arXiv:2505.22626, 2025. Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. Tony Zhao, Jonathan Tompson, Danny Driess, Pete Florence, Kamyar Ghasemipour, Chelsea Finn, and Ayzaan Wahid. Aloha unleashed: simple recipe for robot dexterity. arXiv preprint arXiv:2410.13126, 2024. 12 Preprint. Work in progress"
        },
        {
            "title": "APPENDIX",
            "content": "Please visit our anonymous website to view robot videos: https://r2rgen.github.io. Implementation Details A.1 Annotation System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Pipeline of Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Bimanual Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Data Generation and Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Hardware Setup Tasks and Evaluations C.1 Task Definition . . . C.2 Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Application Usage of LLM 14 15 16 16 17 17 18 19 19 13 Preprint. Work in progress"
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "A.1 ANNOTATION SYSTEM There are three stages to collect and process source demonstration, as shown below. Template and environment scanning: To collect demonstration, we first move the robot to the front of table or any other platform. Then the robots base and head camera are fixed during task execution. Before we perform teleoperation, we first remove all relevant objects from the table and take RGB-D image o0 as the complete environment. Then we individually scan each object to acquire their 3D template via the RealityComposer App on iPad. Annotation with UI: After scanning, we put objects back to the table and start teleoperation to collect sequence of observation-action pair. After that, our UI plays the RGB video and ask user to annotate. As shown in Figure 8. The user first draws boxes on the initial frame to label each object with index, which is then processed with SAM Kirillov et al. (2023) to get object masks. Then user watches the video and is able to click Play, Stop or Rollback at anytime to capture key frames (i.e., the start / end of skill segments). When the user stops at key frame, they can press Annotate and enter annotation mode. In this mode, the user is asked to type in the start and end frame of each skill segment, as well as the target and in-hand object IDs corresponding to each skill. The annotations will be processed into json file. We show an example as below. { aa\"masks\": [ aaaa\"mask_gripper.png\", aaaa\"mask_1.png\", aaaa\"mask_2.png\" aaaa\"mask_3.png\" aa], aa\"arms\": 2, aa\"annotations\": [ aaaa{ aaaaaa\"frame\": 4, aaaaaa\"type\": \"motion\" aaaa}, aaaa{ aaaaaa\"frame\": 12, aaaaaa\"type\": \"skill\", aaaaaa\"target\": [2], aaaaaa\"left_hand\": null, aaaaaa\"right_hand\": null aaaa} aaaa{ aaaaaa\"frame\": 23, aaaaaa\"type\": \"motion\" aaaa}, aaaa{ aaaaaa\"frame\": 31, aaaaaa\"type\": \"skill\", aaaaaa\"target\": [1,3], aaaaaa\"left_hand\": [2], aaaaaa\"right_hand\": null aaaa} aa] } Object tracking and completion: With the object masks in first frame, the whole RGB-D video and the 3D templates of all objects, we run FoundationPose Wen et al. (2024) to track each object across all frames. FoundationPose can accurately predict the 6-DoF pose of each object. So we use the pose to transform object template into world coordinate system to acquire complete object pointclouds. 14 Preprint. Work in progress Figure 8: The annotation UI. The users first segment all relevant objects in the first frame. Then they click Play, Stop or Rollback to capture key frames for skill / motion annotation. With the pointclouds of complete enironments (cid:101)P and complete objects {{ (cid:101)P 1 }}, we finally calculate set difference between the above complete pointclouds and the raw observation to isolate the robots arm {P , ..., (cid:101)P } for all frames. Parsing non-rigid objects: Our R2RGen is general framework that can also handle non-rigid objects. However, since currently FoundationPose only supports rigid objects, for non-rigid ones (assume with ID J), we instead apply SAM2 Ravi et al. (2024) to track and segment the object across all frames. Then we query the object pointclouds {P } from the RGB-D images according to the object masks. With this simple modification, our framework is able to support non-rigid objects as well. Different from {{ (cid:101)P 1 } are incomplete pointclouds, so strong augmentation on them will lead to visual mismatch as in DemoGen. Therefore, we only apply weak augmentation on non-rigid objects and skip camera-aware processing on their pointclouds. This limitation may be solved by 3D pointcloud completion methods or an improved FoundationPose model, which we leave for future work. }}, {P , ..., (cid:101)P A.2 PIPELINE OF AUGMENTATION An algorithm diagram of the group-wise augmentation pipeline is shown in Algorithm 1. To augment pointcloud observations, we can divide the pointcloud of whole scene into three parts, i.e., environments, robots arm and objects: ˆPt = (cid:101)P Te (Aee )1 ˆA ee ObjectAugmentK k=1( (cid:101)P , {ˆat}) (6) Here the environment pointcloud is static across all frames. The arm pointcloud is correlated to the pose of end effector. However, different objects are involved in different motion and skill segments, which cannot be augmented as whole. Therefore, we traverse all objects. For object k, we backtrack all skills that contain this object: 2, ... Then we augment (cid:101)P according to the timestamp: 1, ˆP t = (cid:40) )1 ˆA (Aee (cid:101)P Tk (cid:101)P 1 ee , MotionHand(k) , otherwise , End(S 2) < (7) ˆP = (cid:40) (Aee (cid:101)P Tk (cid:101)P )1 ˆA 1 Tk 2 ee , MotionHand(k) , otherwise ... , End(S 3) < End(S 2) where End() means the end frame of segment. If the segment does not exist, the value will be set to 1. MotionHand(k) represents the set of timestamps that is in-hand during motion (the in-hand object of motion Mi equals to that of skill Si). During backtracking, every skill involving the target object triggers cumulative application of its spatial transformation to itself and prior timestamps. 15 Preprint. Work in progress Algorithm 1: Pipeline of Group-wise Augmentation. Input: Trajectory of motion {M1, ..., MH } and skill {S1, ..., SH }; Pointclouds of enironments (cid:101)P e, objects {{ (cid:101)P 1 in-hand object {Ohand , ..., (cid:101)P }} and robots arm {P } and } of each skill Si; Transformation on each skill {T1, ..., TH }. }; Set of target objects {Otar Output: Augmented 3D observation { ˆPt} and action {ˆat}. Initialize fixed object set OH = , backtracking index = H; // Backtrack skills while > 0 do // If current group is not fixed, augment the trajectory if (Otar ) OT = then Ohand Augment ST with Eq (3); // Otherwise just copy the trajectory else Set {ˆatt Timestamp(ST )} = ST ; Update OT with Eq (2); = 1; // Interpolate motions while < do Interpolate motion {ˆatt Timestamp(MT +1)} with Eq (4); = + 1; // Augment 3D observations according to new trajectories Acquire { ˆPt} with Eq (6); A.3 BIMANUAL TASKS As general framework, R2RGen can also support bimanual manipulation with less modification. (1) For source pre-processing: we do not segment skill and motion for each arm separately. We just extend the in-hand information to in-left-hand and in-right-hand when annotate each skill. In this way, both single-arm and dual-arm operations can be unified. Then for the arm pointcloud , we cluster it into two parts to get pointclouds of left arm la . (2) For group-wise augmentation: if in skill Si, in-left-hand and in-right-hand objects are the same (not null), this means this object is also held by both arms during motion Mi. Therefore, the trajectories of both arms during Mi should follow fixed spatial relationship to ensure the object can be stably grasped in both hands. For other cases, we individually interpolate the trajectories for both arm during motion. and right arm ra A.4 DATA GENERATION AND TRAINING Since real-world data always has random noise, one source demonstration may not be enough to cover the distribution of pointclouds. In our experiments, we still collect only one human demonstration, but replay the action trajectory for three times as did in DemoGen. Then we generate demonstrations based on all three source demonstrations. For each new demonstration generated from source demonstration, we randomly add small perturbations on the augmented locations and rotations for three times. Specifically, we add random tranlsation within circle of 1.5cm radius and random rotation within 20. The total number of generated demonstrations is calculated as 3N 3, where is the number of combinations of all augmented locations and rotations. To train iDP3, denote To as the observation horizon, Tp as the action prediction horizon, and Ta as the action execution horizon, we set To = 2, Tp = 16, and Ta = 8. The visuomotor policy is run at 5 Hz. Training was performed for 6,000 epochs on single RTX 4090 GPU (batch size 64) using Adam (learning rate 1 104, weight decay 1 106). Validation performance plateaued after approximately 2,500 epochs, and we selected the checkpoint with the lowest validation loss. Preprint. Work in progress Figure 9: Robot platform overview. We employ two robot platforms: (a) single-arm UR5e system and (b) dual-arm Mobile Aloha system."
        },
        {
            "title": "B HARDWARE SETUP",
            "content": "We utilize two robot platforms. The primary platform (Figure 9, a) is single 7-DoF UR5e arm equipped with Weiss WSG-50 parallel-jaw gripper. ORBBEC femto bolt RGB-D camera is mounted beside to acquire visual observations. The arm is fixed on height-adjustable table with movable base, which makes it possible for us to evaluate policy with different viewpoint and height. The action space is 7-dimensional (6-DoF end-effector pose plus gripper width). We use Xbox controller to teleoperate the robotic arm to collect demonstrations. The second platform (Figure 9, b) follows the design paradigm of Mobile Aloha Fu et al. (2024), using four AgileX PiPER Arms (two for teleoperation, two for manipulation) and HexFellow omnidirectional mobile base. We mount one ORBBEC femto bolt RGB-D camera on the robots head to acquire visual observation. Each robotic arm has 7 dimensions (6-DoF end-effector pose plus gripper width) and the overall action space is 14-dimensional."
        },
        {
            "title": "C TASKS AND EVALUATIONS",
            "content": "C.1 TASK DEFINITION We carefully design 8 tasks for evaluation, including 6 single-arm tasks and 2 bimanual tasks. task summary is provided in Table 4. We describe these tasks in the text as follows, where skill and motion verbs are highlighted as orange and blue respectively: (A) Open-Jar. The gripper moves above the jar and lowers to an appropriate height. Then it opens, moves down and closes to grasp the handle. It further rotates to open the jar. (B) Place-Bottle. The gripper first moves to the bottle and grasps it. Then it lifts the bottle up and places it on the base. Table 4: summary of our real-world tasks. #Obj: number of manipulated objects. #Eval: number of evaluated configurations. #Demo: number of generated demonstrations. Task Platform #Obj #Eval #Demo Open-Jar Place-Bottle Pot-Food Hang-Cup Stack-Brick Build-Bridge Grasp-Box Store-Item Single-arm Single-arm Single-arm Single-arm Single-arm Single-arm Dual-arm Dual-arm 1 2 3 3 3 3 1 2 32 48 244 244 244 244 24 38 144 144 144 144 144 144 108 108 17 Preprint. Work in progress Figure 10: Protocol for evaluating spatial generalization. We evaluate policies on different robots viewpoints, object locations and rotations. Black crosses indicate seen locations (if human demonstrations are sufficient to cover) and red ones denote unseen locations during training. (C) Pot-Food. The gripper first moves to the food and grasps it. Then it moves towards the pot and puts food into pot. Next it moves to the pot lid and picks it up. It finally moves towards the pot again and covers the pot with the lid. (D) Hang-Cup. The gripper moves to the first cup and picks it up. Then it moves towards the shelf and hangs the cup on the shelf. It repeats the same operation on the second cup, but this time the cup should be hanged on different position of the shelf. (E) Stack-Brick. The gripper moves to the first brick and picks it up. Then the gripper moves to designated place and places the brick there. It repeats the same operation on the other two bricks to stack them one-by-one. (F) Build-Bridge. The gripper first moves to white box and picks it up. Then it brings the box to designated place and places it there. The same operation is repeated on the second white box, where the two white boxes (i.e., the bridge piers) should be placed in proper distance. Then the gripper moves to and grasps the yellow box (i.e., the bridge deck), moves towards the bridge piers and puts deck on piers to build bridge. (G) Grasp-Box. The left gripper moves to the left side of the box. Then the right gripper moves to the right side. After that, two grippers simultaneously grasp the box and lift it up. (H) Store-Item. The left gripper moves to the left side of the box. Then it grasps the box and lifts it up. At the same time, the right gripper moves to the banana and grasps it. It then brings the banana to the box and stores the banana into box. C.2 EVALUATION PROTOCOL To evaluate spatial generalization, we define large planar evaluation workspaces as illustrated in Figure 10. For each test trial, the initial positions of the objects are determined by sampling distinct locations from pre-defined set of 32 points on the workspace. Each object is also assigned random rotation sampled from the range of -20 to 20 degrees, while the robots base is initialized at one of three distinct locations. For tasks involving more than one object, we constrain the range of locations for each object to reduce the overall number of combinations. We demonstrate the range of object locations of single-arm tasks in Figure 10 (a): (A) Open-Jar. The range of jar is {1, 2, ..., 32}. (B) Place-Bottle. The range of bottle is {4, 10, 8, 17, 22, 24, 28, 29}, and the range of box is {11, 13, 25, 26}. (C) Pot-Food. The range of food is {27, 31}, the range of lid is {6, 13, 19, 21}, and the range of pot is {9, 17, 23, 29}. (D) Hang-Cup. The range of the first cup is {4, 16, 22, 29}, the range of the second cup is {13, 14, 20, 27}, and the range of shelf is {11, 18}. (E) Stack-Brick. The range of the first brick is {8, 9, 22, 23}, the range of the second brick is {13, 19, 21, 27}, and the range of the third brick is {10, 25}. 18 Preprint. Work in progress Figure 11: Visualization of mobile manipulation results. The policy trained with R2RGen successfully generalizes to different camera views with only one human-collected demonstration. (F) Build-Bridge. The range of the first bridge pier is {8, 9, 22, 23}, the range of the second pier is {13, 14, 26, 27}, and the range of the bridge deck is {11, 24}. The range of object locations of dual-arm tasks is shown in Figure 10 (b): (G) Grasp-Box. The range of box is {1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32}. (H) Store-Item. 19, 23, 26, 30, 32}. The range of box is {1, 14, 27}, and the range of banana is {4, 6, 17, Note the black crosses in Figure 10 indicate possibly seen locations during training, where human demonstrations are collected within these locations. The red crosses denote unseen locations which the training set does not cover."
        },
        {
            "title": "D APPLICATION",
            "content": "R2RGen makes our 3D policy achieve strong spatial generalization across different viewpoints without camera calibration, so we can achieve mobile manipulation by simply combining navigation system Wu et al. (2025) and manipulation policy trained with R2RGen. Since the termination condition of navigation is relatively loose, the robot may stop at different docking point around the manipulation area, which imposes great challenges on the manipulation policy. According to Figure 11, using iDP3 trained with R2RGen, the policy successfully generalizes to different docking points with maximum distance larger than 5cm. Different from DemoGen (DP3) which requires careful calibration of the camera pose to crop environment pointclouds, our method directly applies on raw RGB-D observations during both data generation and policy training / inference stages, which is more practical in real-world applications."
        },
        {
            "title": "E USAGE OF LLM",
            "content": "In the preparation of this manuscript, large language models (LLMs) were employed solely for the purpose of linguistic polishing and refinement of partial sentences. Specifically, LLMs were used to improve grammatical accuracy, enhance sentence fluency, and ensure terminological consistency in certain paragraphs. All conceptual development, theoretical analysis, experimental design, result interpretation, and methodological discussions remain entirely the work of the human authors. The 19 Preprint. Work in progress use of LLMs did not contribute to the generation of original ideas, technical content, or scientific conclusions presented in this paper. All authors take full responsibility for the integrity and validity of the research and the content of this publication."
        }
    ],
    "affiliations": [
        "GigaAI",
        "Tsinghua University"
    ]
}