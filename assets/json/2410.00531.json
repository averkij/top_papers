{
    "paper_title": "TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices",
    "authors": [
        "Zonghang Li",
        "Wenjiao Feng",
        "Mohsen Guizani",
        "Hongfang Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large model inference is shifting from cloud to edge due to concerns about the privacy of user interaction data. However, edge devices often struggle with limited computing power, memory, and bandwidth, requiring collaboration across multiple devices to run and speed up LLM inference. Pipeline parallelism, the mainstream solution, is inefficient for single-user scenarios, while tensor parallelism struggles with frequent communications. In this paper, we argue that tensor parallelism can be more effective than pipeline on low-resource devices, and present a compute- and memory-efficient tensor parallel inference system, named TPI-LLM, to serve 70B-scale models. TPI-LLM keeps sensitive raw data local in the users' devices and introduces a sliding window memory scheduler to dynamically manage layer weights during inference, with disk I/O latency overlapped with the computation and communication. This allows larger models to run smoothly on memory-limited devices. We analyze the communication bottleneck and find that link latency, not bandwidth, emerges as the main issue, so a star-based allreduce algorithm is implemented. Through extensive experiments on both emulated and real testbeds, TPI-LLM demonstrated over 80% less time-to-first-token and token latency compared to Accelerate, and over 90% compared to Transformers and Galaxy, while cutting the peak memory footprint of Llama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . [ 1 1 3 5 0 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Under review",
            "content": "TPI-LLM: SERVING 70B-SCALE LLMS EFFICIENTLY ON LOW-RESOURCE EDGE DEVICES Zonghang Li Department of Machine Learning MBZUAI zonghang.li@mbzuai.ac.ae Wenjiao Feng School of Info & Comm Engineering UESTC wenjiaofeng@std.uestc.edu.cn Mohsen Guizani Department of Machine Learning MBZUAI mohsen.guizani@mbzuai.ac.ae Hongfang Yu School of Info & Comm Engineering UESTC yuhf@uestc.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Large model inference is shifting from cloud to edge due to concerns about the privacy of user interaction data. However, edge devices often struggle with limited computing power, memory, and bandwidth, requiring collaboration across multiple devices to run and speed up LLM inference. Pipeline parallelism, the mainstream solution, is inefficient for single-user scenarios, while tensor parallelism struggles with frequent communications. In this paper, we argue that tensor parallelism can be more effective than pipeline on low-resource devices, and present computeand memory-efficient tensor parallel inference system, named TPILLM, to serve 70B-scale models. TPI-LLM keeps sensitive raw data local in the users devices and introduces sliding window memory scheduler to dynamically manage layer weights during inference, with disk I/O latency overlapped with the computation and communication. This allows larger models to run smoothly on memory-limited devices. We analyze the communication bottleneck and find that link latency, not bandwidth, emerges as the main issue, so star-based allreduce algorithm is implemented. Through extensive experiments on both emulated and real testbeds, TPI-LLM demonstrated over 80% less time-to-first-token and token latency compared to Accelerate, and over 90% compared to Transformers and Galaxy, while cutting the peak memory footprint of Llama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, Large Language Models (LLMs) have been widely deployed in the cloud for inference. User inputs are uploaded to the cloud, where high-performance GPUs are used to compute output sequences, and then sent back to user devices for display. This process poses privacy risks, as user prompts are exposed to network intermediaries and clouds. Therefore, there is an increasing need to shift LLM services to the network edge, such as on laptops, hand phones, tablets, and desktop computers. However, edge devices have very limited memory (4-16 GB) and computing power (often CPU-only). Even with quantization, running Llama 3.1-70B model still requires at least 40 GB of memory, which far exceeds the capacity of most edge devices. Besides, running Bert-L on one Nano-M device results in latency that is 120 longer than on one A100 GPU. This gap requires the use of more edge devices to support and speed up LLM inference on the network edge. While advanced LLM serving systems (Shoeybi et al., 2019; Rasley et al., 2020; Li et al., 2023; Agrawal et al., 2024; Miao et al., 2024) have been designed for high-performance GPU clusters, recent efforts (Zhang et al., 2024; Mei et al., 2024; Borzunov et al., 2024) are adapting these systems to edge environments, by adaptively partitioning model between edge devices and optimizing Zonghang Li and Wenjiao Feng contribute equally."
        },
        {
            "title": "Under review",
            "content": "schedulers to boost token throughput. However, in smart home scenarios like smart speaker, edge LLM systems often handle one user request at time, making them degrade from pipeline to model parallelism and leaving devices idle most of the time. Thus, tensor parallelism is preferred for better efficiency. For instance, Ye et al. (2024) combine tensor and sequence parallelism to reduce token latency and Wei et al. (2024) introduce block parallelism to restructure Transformer layers. However, even with 8 devices sharing the load, running full-precision Llama 2-70B still requires 35 GB per device, memory remains shortage. Solutions like memory block paging (Kwon et al., 2023) and optimized KVCache storage (Jin et al., 2023; Lee et al., 2024) help schedule data between GPUs and CPUs, but unfortunately, GPUs are not available on most edge devices. As popular alternative, Accelerate (Gugger et al., 2022) can offload model data from CPU to disk to run larger models, but its blocking I/O drastically slows inference, with token latency increases to 30 seconds per token on Llama 3.1-8B. In this work, we analyze why tensor parallelism is more effective than model parallelism on lowresource edge devices and present TPI-LLM, computingand memory-efficient tensor parallel inference framework to serve LLM models. Constrained by the high link latency, star-based allreduce algorithm is implemented. To address the memory shortage, sliding window memory scheduler is further introduced. We build prototype of TPI-LLM with 3K LoC and two testbeds using Klonet (Ma et al., 2024) and 4 laptops. Extensive results on Llama 3.1-8B/70B (Dubey et al., 2024), Llama 2-3B/7B/13B/70B (Touvron et al., 2023) and Yi-34B (AI et al., 2024) demonstrate the significant reduction of the memory footprint and faster inference speed compared to Transformers (Wolf et al., 2020), Acclerate (Gugger et al., 2022), and Galaxy (Ye et al., 2024). We summarize the main contributions of this work as follows: We design TPI-LLM for edge LLM serving, which keeps prompt privacy in mind to allow edge devices with limited computing power collaborate to deliver faster inference. We find that network bandwidth is no longer an issue. Instead, link latency causes high delays in advanced allreduce algorithms. Thus, star-based allreduce algorithm is implemented, which greatly outperforms ringand tree-based methods. We introduce sliding window memory scheduler, which asynchronously loads and unloads layer weights and overlaps disk I/O latency with computations and communications, enabling the inference of larger models on low-memory devices. We prototype TPI-LLM and show that it reduces time-to-first-token and token latency by over 80% compared to Accelerate and over 90% compared to Transformers and Galaxy. It serves Llama 2-70B with peak memory footprint of 3.1 GB across 8 low-resource devices."
        },
        {
            "title": "2 OBSERVATIONS AND MOTIVATIONS",
            "content": "Before presenting our TPI-LLM system, we address two questions that guide our design: Q1: On low-resource edge devices, which dominate inference time: computation or communication? Which is more efficient, tensor parallelism or model parallelism? On the network edge, the balance between computation and communication differs from that in high-performance GPU clusters. To determine whether tensor or model parallelism offers more benefits, it is essential to identify whichcomputation or communicationtakes up more time. For this purpose, we examine the Llama 3.1-8B model on LAN network with 4 laptops of 8 cores. The network bandwidth between them is 178 Mbps, and the devices implement allreduce communications using parameter server architecture (Li et al., 2014). Figures 1a and 1b show the timeline and computing-communication time ratio for model and tensor parallelism during inference. In model parallelism, communication accounts for only 2% of the time, with most spent on computation. However, when one device is computing, others are idle, creating pipeline bubbles and resource waste. In tensor parallelism, communication rises to 70%, but all devices compute simultaneously, and the speed boost outweighs the communication cost, leading to less overall inference time. This makes tensor parallelism the preferred choice. Q2: Is tensor parallelism enough for edge LLM serving?"
        },
        {
            "title": "Under review",
            "content": "(a) (b) (c) Figure 1: Comparison of (a,b) tensor and model parallelism in terms of computational and communication time and (c) memory footprint each device with increasing tensor parallel nodes. Tensor parallelism does reduce memory footprint each device by sharing model parameters across multiple devices, but it doesnt fully address the memory shortage. Figure 1c shows that even with 4 tensor parallel nodes, memory footprint remains at 12 GBstill too high for most edge devices. This is because memory footprint includes not just model parameters but also intermediate results, key value cache, libraries, etc., causing the actual usage to exceed the theoretical value. Besides, other apps on the device also compete for memory, which worsens the shortage. Thus, even with tensor parallelism, memory scheduler is still needed to avoid out-of-memory (OOM) issues."
        },
        {
            "title": "3 TPI-LLM FRAMEWORK WITH SLIDING WINDOW MEMORY SCHEDULING",
            "content": "In typical inference workflow, many users send their prompts to cloud-based service. These prompts are pooled and scheduled in batches, undergoing dozens of Transformer layers, and converted into probabilities to predict the next token. This process repeats until the generated sequence is finished. While the fundamental workflow on the cloud and edge are similar, key differences arise: (a) Keep prompts and generated sequences on users device. In cloud setup, user prompts are sent to remote servers for processing, which result in exposure of private data. Edge LLM serving systems are required to keep prompts and generated sequences in users own devices to ensure raw data never get exposed to external unknown environments. (b) More single-prompt serving. Current LLM serving systems are typically optimized for batched prompts using pipeline scheduling. However, these optimizations lead to resource underutilization in edge scenarios like smart speakers, where only one prompt is processed at time. (c) Low-resource devices without CUDA support. Edge devices, unlike cloud GPUs, have very limited memory and low computing power. Many of them lack CUDA support or do not have GPUs at all, and they often prioritize full precision to ensure faster computations. 3.1 THE PARALLEL FRAMEWORK DESIGN OF TPI-LLM SYSTEM The proposed tensor parallel inference system (TPI-LLM) tackles these challenges by using tensor parallel framework that distributes attention heads across multiple nodes. As depicted in Figure 2, it involves master node, typically the users device that initiates the prompt, and several worker nodes that share the computational load. Their pseudo codes are given in Algorithms 1 and 2. Step 1: The master node partitions and distributes model weights. Before inference begins, the master node partitions the pretrained model weights , such as attention heads and FFN weights, among the worker nodes. Workers with greater computing power and larger memory are allocated more attention heads and FFN weights. This ensures no single device bears the full burden. Step 2: The master node initiates prompt and broadcast the input embedding to workers. The inference process starts at the master node, where user prompt is tokenized into list of token indices and then transformed into input embeddings 0 = xWemb. The embedding is then broadcast to all worker nodes 0 ffn = 0 to initiate the tensor parallel workflow."
        },
        {
            "title": "Under review",
            "content": "Figure 2: Overview of the TPI-LLM parallel framework. Step 3: All nodes perform tensor parallel computing. The tensor parallel computing follows cycle of four operations: attention computing, allreduce, FFN computing, and allreduce. These operations together constitute Transformer block. Devices compute attention and FFN with partitioned weights in parallel, reducing the computing delays on low-power devices. In the attention computation phase of the l-th Transformer block, device processes only subset of attention heads Qh,l = norm = norm(H l1 are downloaded from the master node in Step 1. Once Qh,l, Kh,l, h,l are computed, we apply the scaled dotproduct attention to calculate the attention score, and the result is then synchronized across devices: ffn ) is the normed hidden state and weight partitions h,l normW h,l , h,l , Kh,l = K , h,l = , where , h,l normW h,l normW h,l H attn = all reduce(softmax( Qh,l(Kh,l) )V h,l) + l1 ffn , (1) where is the dimension for attention head. Here, attention is computed in parallel across devices, followed by an allreduce to aggregate their hidden states and shortcut connection. The key-value pair (Kh,l, h,l) is cached locally on device to reduce redundant computations. This distributed KVCache partitions the cache across devices, so memory cost is reduced on individual device. After the attention computation and allreduce, the process continues with the FFN computation: ffn = all reduce(W h,l down (σ(W h,l gate norm) (W h,l up norm))) + attn, (2) where FFN weights h,l attn), σ represents the activation function such as SiLU (Elfwing et al., 2018). Similar to the attention computation stage, the FFN is computed in parallel, followed by an allreduce and shortcut connection. down are also partitioned weights, norm = norm(H up , h,l gate, h,l Step 4: The master node reduces tensor parallel results and calculates the next token. After each node completes its part of computation within the backbone network, the result is sent to the master node. The summed results ffn are then passed through task head Whead and softmax to obtain the probability distribution of the next token = softmax(H ffnWhead), which is then sampled. Steps 2 to 4 repeat until an EOS token is generated or the length limit is reached. (i) The user prompt {x1, x2, } and generated sequence TPI-LLM provides three benefits: {z1 z1, z2 z2, } are processed only on the master node, keeping them hidden from workers. Even if workers reverse-engineer input embeddings 0, they cannot recover the raw prompt or next token since the weights of input embedding Wemb and task head Whead reside solely on master. (ii) The inference speed is often limited by the computational latency, but in TPI-LLM, it is accelerated via parallel computing. (iii) Unlike other systems that use mix of communication primitives (reduce & broadcast (Shoeybi et al., 2019), reducescatter & allgather (Ye et al.,"
        },
        {
            "title": "Under review",
            "content": "Algorithm 1: Master (with rank 0): Algorithm 2: Worker (with rank k): 1 Split and distribute pretrained weight files to 1 Download sliced weight files from the master worker nodes; 2 Tokenize user prompt into indices; 3 Start memory scheduler; 4 while generated sequence not finished do 5 Preprocess: Convert indices to input and node; 2 Start memory scheduler; 3 while generated sequence not finished do position embeddings, calculate causal mask and cache position; Broadcast: Send embeddings, causal mask, and cache position to workers; foreach decoder layer do Attention: Execute layernorm, self-attention, and store (K 0 KVCache D0; Allreduce: Aggregate attention outputs; FFN: Execute layernorm and FFN; Allreduce: Aggregate FFN outputs; , 0 ) in end Reduce: Sum final outputs with others; Postprocess: Execute layernorm, MLP, softmax, and sample next token; 6 7 8 9 10 11 12 13 14 5 6 7 8 9 10 11 Broadcast: Receive embeddings, causal mask, and cache position from master; foreach decoder layer do Attention: Execute layernorm, self-attention, and store (K KVCache Dk; Allreduce: Aggregate attention outputs; FFN: Execute layernorm and FFN; Allreduce: Aggregate FFN outputs; , l ) in end Reduce: Send final output to master; 15 end 12 end 2024), etc.), TPI-LLM standardizes communications to allreduce. This enhances compatibility with broader communication libraries like PS-LITE (Chen et al., 2015) and NetStorm (Li et al., 2024), leveraging their optimized implementations for edge conditions. 3.2 ALLREDUCE LATENCY ANALYSIS Given the dynamic and heterogeneous nature of edge networks, we tested NetStorm (Li et al., 2024) as the communication backend, but unfortunately, it resulted in high token latency. After further validation, we confirmed that this latency was not due to network bandwidth, but due to link latency. To analyze the impact of network bandwidth and link latency, we make the following assumption. Assumption 1. Assume that the edge network adopts physical topology as shown in Appendix A.7, the network links have the same latency τ , the allreduce algorithm follows tree-based structure of depth 2 for aggregation, and each device has the same computing power. The allreduce latency can be expressed as tall reduce = 2L(tdata + tlink + tbarrier + taggr), where is the number of Transformer layers, tdata is the cumulative data transfer latency, tlink is the cumulative link latency, tbarrier is the cumulative barrier latency during aggregation, and taggr is the cumulative latency for aggregation calculation. Here we ignore taggr as it takes only 0.1 ms and thus negligible compared to other factors. Proposition 1. The bottleneck in allreduce is not network bandwidth, but link latency. Proof. The data transfer latency tdata = 2 (cid:80) depends on the size 32H of the data {ij}Ph being transmitted and the bandwidth Bij of the links in the path Ph, here Ph is an index sequence from device to the master device. For example, in the case of Llama 2-70B with hidden size = 8192 and network bandwidth of 300 Mbps, the data transfer latency is only tdata = 3.4 ms, which is negligible compared to other latencies. In addition, experiment results in Figure 5 show that increasing the network bandwidth does not significantly reduce token latency, further confirming that data transfer and network bandwidth is not the bottleneck. 32H Bij The link latency tlink, which is often neglected, emerges as the main issue. For example, the path from device h2 to h1 via h8 follows the route h2 r2 r9 r8 h8 r8 r9 r1 h1, resulting in total link latency of 16τ , where τ is the per-hop link latency. To isolate the impact of"
        },
        {
            "title": "Under review",
            "content": "link latency, we ran allreduce with only 4 bytes of data, excluding data transfer tdata and barrier latencies tbarrier. The results, shown in Figure 3, demonstrate that the per-link latency τ significantly impacts allreduce latency. This indicates that an inefficient allreduce algorithm, where multiple hops are required (e.g., ring (Ye et al., 2024; Shoeybi et al., 2019) or tree-based (Zhou et al., 2021; Li et al., 2024) algorithms), will further amplifies this impact. For example, with the ring algorithm, allreduce requires 7 communication steps for reducescatter and 7 for allgather, resulting in total link latency of 56τ , which is 3.5 higher than the tree-based setup. Figure 3: Impact of link latency τ . The barrier latency, tbarrier, arises from synchronization delays during data aggregation. Given the assumption that all devices have equal computing power and network links have equal latencies, the barrier latency can be approximated as negligible: tbarrier = max{ (cid:88) (ij)P 32H Bij , P} min{ (cid:88) (ij)P 32H Bij , P} 0. (3) Thus, link latency tlink emerges as the key factor in allreduce latency. Proposition 2. The star-based allreduce is more effective for TPI-LLM in high-latency networks. Despite past criticism, the star-based allreduce, where workers push data directly to the master for aggregation and pull the result back (Chen et al., 2015), stands out as the best choice (see Appendix A.1 for detailed proof). It has minimal hops (8), lowest link latency (8τ ), zero intermediate barriers, and avoids the single-point issue due to the small data size (256 KB per device), making it the preferred allreduce algorithm for TPI-LLM. 3.3 SLIDING WINDOW MEMORY SCHEDULING Quantizations like FP16 and INT8 are common for NVIDIA GPUs with CUDA support, but most edge devices lack CUDA and prefer full precision for faster computation due to their general-purpose CPU design. As result, while tensor parallelism helps distribute memory costs across devices, the memory load remains high. Thus, memory scheduling is still required to manage these loads. We introduce memory scheduler, which manages memory by dynamically loading and unloading model weights during inference, ensuring that only the necessary parts are kept in memory (see Appendix A.2 for potential use). The memory scheduler operates on daemon thread to asynchronously handle memory operations. To maintain the peak memory footprint, it uses sliding window and preloads weights for upcoming layers while unloading those that have been processed. As mentioned in Section 3.1, each Transformer layer is divided into attention computing, allreduce, FFN computing, and allreduce. For simplicity, in Figure 4, we assume the delays for these stages and weight loading to be equal. In each time slot, the memory scheduler asynchronously loads weights for either an attention or FFN block. By overlapping weight loading with ongoing computations and communications, it hides the I/O latency associated with loading weights from disk. For example, in Figure 4, the memory scheduler loads one more block during each allreduce until the sliding window reaches its size. As computations and communications proceed, we ensure weights are always ready when needed, allowing for seamless inference without computational stalls. Next, we provide the conditions for this mechanism to reach steady state, under which all required weights are loaded before computation starts. Proposition 3 (Loose Steady Condition). The memory scheduler reaches steady state when the following condition is met: tattn + tffn + 2tall reduce τffn + τattn, (4) and one of the following conditions is met: tattn + (l 1) tffn + (2l 1) tall reduce τffn + (l 1) τattn, {1, , L}, (l 1) tattn + tffn + (2l 1) tall reduce (l 1) τffn + τattn, {1, , L}, (5) (6)"
        },
        {
            "title": "Under review",
            "content": "Figure 4: An illustration of the sliding window memory scheduling. Blue blocks indicate the blocks currently executed, with numbered blocks for attention or FFN computing and unnumbered blocks for allreduce communication. Green blocks indicate loaded model weights. The dashed box represents the sliding window, with size 4 in this case. where tattn and tffn are times required for attention and FFN computation, tall reduce is the allreduce latency, τffn and τattn are times required to load attention and FFN weights, and is the number of Transformer layers. This condition is loose but bit hard to assess, so we present tighter, more intuitive condition. Proposition 4 (Tight Steady Condition). tattn + tall reduce τffn and tffn + tall reduce τattn. The proofs can be found in Appendices A.3 and A.4. This conclusion is straightforward. If the previous blocks computation and allreduce time cover the current blocks weight loading time, the memory scheduler can fully hide the disk I/O latency. As an example, in Section 4.4, we use 4 laptops with Llama 2-7B, setting pi = 0.25 and = 4. We measured tattn = 11 ms, tffn = 17 ms, tall reduce = 14 ms, τattn = 18 ms, and τffn = 30 ms. While the tight steady condition is not met, the loose steady condition is met, allowing the memory scheduler to achieve steady state. Proposition 5 (Peak Memory Footprint). If the memory scheduler reaches steady state, the peak memory footprint of the master and worker can be expressed as Mmaster = γ Mworker = γ hv + h, 2hv + h, 2hv + + (cid:4) w2 2 (cid:18)(cid:106) (cid:5) (cid:0)2(1 + )h2pi + h(cid:1) + (cid:4) w1 (cid:22) + 1 (cid:23) 2 2 (3hspi + h) , (cid:5) (3hspi + h), (cid:19) (cid:107) (2(1 + )h2pi + h) + if = 1 if = 2 if (7) (8) where is the hidden size, is the vocabulary size, is the number of attention heads, is the number of key-value heads, is the intermediate size, pi is the proportion of parameters handled by device i, is the memory window size, and γ is memory scaling factor. The proof can be found in Appendix A.5. However, if slow disk I/O disrupts the steady state, the memory scheduler will retain some FFN blocks in memory to reduce disk access frequency. Proposition 6 (Loose Steady Condition with Block Retention). Let the memory scheduler retain one FFN block in memory every FFN blocks, the condition to reach steady state is then tattn + tffn + 2l tall reduce (l tattn + (l 1) tffn + (2l 1) tall reduce (l (cid:25) (cid:25) (cid:24) (cid:24) ) τffn + τattn, (9) ) τffn + (l 1) τattn. (10) The proof can be found in Appendix A.6. By setting an appropriate , idle memory can help the scheduler reach steady state, thus achieving tradeoff between memory use and inference speed."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Prototype and Testbed. We implemented the prototype of TPI-LLM1 with 3K LoC using PyTorch and Transformers to provide flexible support for various sizes and versions of pretrained LLMs. Our testbed, illustrated in Appendix A.7, was built upon Klonet (Ma et al., 2024) to create an edge network environment, emulating realistic conditions with configurable properties like network topology, bandwidth, and latency. By default, 8 edge devices were emulated on 2 Intel Xeon Gold 5220R CPUs, each limited to 8 logical cores, 8 GB of memory, and 4 GB of swap. Network bandwidth between devices was set to 300 Mbps with 1 ms latency. Models. The inference speed of TPI-LLM is significantly affected by the model architecture. Deeper layers, more parameters, larger hidden sizes, and more attention heads increase the computational latency. Additionally, deeper layers result in more allreduce communications, and larger hidden size leads to greater traffic. We tested with various models of different sizes, including Llama 23B/7B/13B/70B, Llama 3-8B/70B, and Yi-34B. See Appendix A.8 for their configuration details."
        },
        {
            "title": "4.1 OVERVIEW OF TPI-LLM PERFORMANCE",
            "content": "Fit 70B LLMs into edge devices and run in high efficiency. We tested the performance of TPILLM with focus on 3 key metrics: time-to-first-token (TTFT), token latency, and peak memory footprint per device. The memory window size is set to 2 by default. As shown in Table 1, without the memory scheduler, the full weights are loaded into the memory at once. Despite that these weights have been distributed across multiple devices, the memory is still insufficient for larger models like Yi-34B and Llama 2/3/3.1-70B. Instead, enabling our memory scheduler significantly reduces the peak memory footprint, allowing larger models to run efficiently. For example, the Llama 2-70B model requires just 3.1 GB of memory per device, and the Llama 3.1-70B model fits within device limits. The results are summarized in Table 1. Table 1: The TTFT, token latency, and peak memory footprint per device of TPI-LLM. Model (FP32) Llama 2-3B Llama 2-7B Llama 2-13B Llama 2-70B Llama 3.1-8B Llama 3.1-70B Yi-34B Memory Scheduler Disabled Memory Scheduler Enabled TTFT 2.3 3.1 5.1 OOM 4.5 OOM OOM Latency Memory 2.8 GB 1.0 s/token 4.5 GB 1.2 s/token 1.9 s/token 8.1 GB 34.9 GB OOM 1.5 s/token 8.5 GB 42.3 GB OOM 20.4 GB OOM TTFT 2.0 3.0 5.8 29.4 4.5 32.9 15.7 Latency 1.9 s/token 2.6 s/token 2.9 s/token 26.1 s/token 4.3 s/token 29.9 s/token 13.7 s/token Memory 1.4 GB 1.7 GB 2.1 GB 3.1 GB 5.4 GB 11.3 GB 4.9 GB Table 2: Peak memory footprint per device with the memory window size set to 2. Memory Scheduler Disabled (GB) Memory Scheduler Enabled (GB) Model (FP32) = 2 = 4 = 6 = 8 = 2 = 4 = 6 = 8 Llama 2-3B Llama 2-7B Llama 2-13B Llama 2-70B Llama 3.1-8B Llama 3.1-70B Yi-34B 7.3 13.7 25.7 130 18.4 137.7 67 4.3 7.7 13.9 66.6 11.8 74.0 36.4 2.8 4.5 8.1 34.9 8.5 42.3 20.4 3.2 5.5 9.8 46.6 9.4 51.1 23. 1.4 1.8 2.2 3.3 5.6 10.5 5.0 1.5 2.0 2.3 3.7 5.9 10.8 5.0 1.4 1.7 2.2 3.3 5.5 11.4 5.0 1.4 1.7 2.1 3.1 5.4 11.3 4.9 No need for dozens of devices, one or two are enough to run 70B models. We used 8 devices by default, but can fewer devices run 70B-scale models? Table 2 gives detailed peak memory footprints with varying number of devices. Without the memory scheduler, full weights are loaded onto the devices, and with fewer devices, the memory load increases. For instance, using only 2 devices limits users to smaller models, like those between 3B and 7B. However, with the memory scheduler enabled, only few layers weights are loaded and distributed across devices. This allows even 1Open available at: https://anonymous.4open.science/r/tpi-llm."
        },
        {
            "title": "Under review",
            "content": "Figure 5: Token latency over varying number of devices, CPU cores, and network bandwidth on Llama 2-70B. Figure 6: Comparison of TPI-LLM with three benchmarks. larger models, such as 70B, to run smoothly on just 2 devices. Appendix A.9 shows the case with memory window size of 4, which requires slightly more memory but faster speed. The peak memory footprint in TPI-LLM is primarily determined by the product of vocabulary size and hidden size, which is detailed in equation (7) and can be further reduced in our future work. 4.2 SCALING OVER VARYING EDGE CONDITIONS Computation remains the bottleneck, not network bandwidth. In this experiment, we examined the token latency of TPI-LLM under different edge conditions, the results are shown in Figure 5. As expected, increasing the number of devices reduces the computing load on each device, significantly lowering token latency, and more CPU cores also contribute to reduced latency. Instead, limited network bandwidth was no longer bottleneck, boosting it from 300 Mbps to 1 Gbps had little effect on latency due to the tiny data size (only 256 KB) during each allreduce. Thus, the main bottleneck remains in the computation, which our future work should focus on. 4.3 COMPARISON WITH BENCHMARKS We compared the TPI-LLM with 3 benchmarks: (a) Standalone: LLM inference is executed only on single edge device using Transformers (Wolf et al., 2020). (b) Model Parallelism (MP): Since only one user is served at time, the pipeline parallelism (Zhang et al., 2024; Mei et al., 2024; Borzunov et al., 2024) degrades to the model parallelism, where different layer sequences are distributed across multiple devices. Each device computes its layers and passes the result to the next device until the entire inference is complete. (c) Galaxy (Ye et al., 2024) combines tensor and sequence parallelism and overlaps communication and computation to accelerate inference. They all run in FP32 mode. Run larger models with lower latency and memory usage. As shown in Figure 6, limited memory on single device makes it challenging to run even 3B models in standalone mode. MP addresses this by the collaboration of 8 devices, allowing models up to 13B, but suffers from high latency due to pipeline bubbles. Galaxy tries to reduce such latency by combining tensor and sequence parallelism. However, in Section 3.2, we concluded that the network bandwidth was no longer the issue, and the real problem is the link latency. Galaxys use of ring algorithm for reducescatter and allgather forces each link to be traversed at least 14 times. This causes high link"
        },
        {
            "title": "Under review",
            "content": "Table 3: Comparison of Transformers, Accelerate, Transformers+MS, and TPI-LLM on 4 laptops. Model (FP32) Llama 2-3B Llama 2-7B Llama 2-13B Llama 3.1-8B Yi-34B Transformers Accelerate TTFT (s) 61 115 OOM 133 OOM Latency (s/token) 30 56 OOM 65 OOM TTFT (s) 24 30 OOM 37 OOM Latency (s/token) 16 26 OOM 31 OOM Transformers + MS TTFT (s) 4 13 22 20 185 Latency (s/token) 3 8 18 12 55 TPI-LLM TTFT (s) 2.5 6 10 11 33 Latency (s/token) 2 5 9 8 29 latency and outweighs the benefits of parallel computing, ultimately resulting in higher token latency than MP. In contrast, TPI-LLM adopts star-based allreduce algorithm, minimizing hops and cumulative link latency. Combined with the blocking-free memory scheduler, TPI-LLM delivers significantly lower token latency and memory footprint, even with larger 70B models. 4.4 REAL CASE STUDY In this study, we used 4 laptops with different CPU architectures and memory capacities, connected via local Wi-Fi router. The testbed and configurations are detailed in Appendix A.10. Macbook Pro was used by default. Due to the lack of CUDA, all computations were performed in full precision. As shown in Table 3, Transformers loaded the entire model into the CPU memory, and when memory was insufficient, the operating system offloaded data to the swap. This frequent swap exchange significantly increased TTFT and token latency, even for smaller 3B models. As the model size grows, the swap space overflowed, finally leading to OOM errors. As more efficient alternative, Accelerate (Gugger et al., 2022) instantly loads layer weights only when required for the computation and reduces unnecessary data I/O. While it speeds up inference, due to implementation flaws on disk offloading, it still requires loading full weights before splitting and offloading them to disk. This results in OOM errors when the model size reaches 13B. TPI-LLM stands out in TTFT, token latency, and model size. Our memory scheduler (Transformers+MS) outperforms Transformers and Accelerate in both TTFT and token latency across all model sizes. This is because our memory scheduler employs sliding window mechanism, where daemon thread asynchronously preloads the weights needed for upcoming computations. By overlapping data I/O with computations and communications, the scheduler avoids delays caused by disk I/O blocks, ensuring smoother and faster inference. To further speed up inference, we integrate the computing power of 4 laptops to serve TPI-LLM. By distributing the computational load across 4 laptops, the reduction in computing time far exceeds communication delays, so both TTFT and token latency are further reduced. The results from using 3 laptops are shown in Appendix A.11, indicating slightly higher latency due to reduced parallelism."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we concluded that tensor parallelism can be more effective than pipeline parallelism on low-resource devices, and presented computeand memory-efficient tensor parallel inference system, named TPI-LLM, to serve 70B-scale LLMs. TPI-LLM is designed with user prompt and generated sequence privacy in mind, by keeping sensitive raw data local in the users devices. It leverages sliding window memory scheduler to dynamically manage layer weights during inference with disk I/O latency overlapped by onging computations and communications, allowing larger models to run smoothly on devices with very limited memory. Our analysis showed that link latency, not bandwidth, emerges as the main issue, so TPI-LLM implements star-based allreduce algorithm, rather than the commonly used ringand tree-based algorithms. Through extensive experiments on emulated and real testbeds, TPI-LLM demonstrated significantly lower TTFT, token latency, and peak memory footprint compared to Transformers, Accelerate, Galaxy, and enabled serving largerscale LLMs such as Yi-34B and Llama 2/3/3.1-70B on low-memory devices."
        },
        {
            "title": "REPRODUCIBILITY",
            "content": "We have made efforts to ensure reproducibility by providing the source code at https:// anonymous.4open.science/r/tpi-llm, with detailed README for guidance included. To ease the use, prebuilt Docker image is also provided. Key experimental setups are given in Section 4 of the paper."
        },
        {
            "title": "REFERENCES",
            "content": "Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee. Taming Throughput-Latency tradeoff in LLM In 18th USENIX Symposium on Operating Systems Design and inference with Sarathi-Serve. Implementation (OSDI 24), pp. 117134, Santa Clara, CA, July 2024. USENIX Association. ISBN 978-1-939133-40-3. URL https://www.usenix.org/conference/osdi24/ presentation/agrawal. 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, and Colin Raffel. Distributed inference and fine-tuning of large language models over the internet. Advances in Neural Information Processing Systems, 36, 2024. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:311, 2018. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. s3: Increasing gpu utilization during generative inference for higher throughput. Advances in Neural Information Processing Systems, 36:1801518027, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023. Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. Infinigen: Efficient generative inference of large language models with dynamic kv cache management. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pp. 155172, 2024. Mu Li, David Andersen, Jun Woo Park, Alexander Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In 11th USENIX Symposium on operating systems design and implementation (OSDI 14), pp. 583598, 2014."
        },
        {
            "title": "Under review",
            "content": "Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. AlpaServe: Statistical multiplexing with model parallelism for deep learning serving. In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), pp. 663679, Boston, MA, July 2023. USENIX Association. ISBN 978-1-939133-34-2. URL https://www.usenix.org/conference/ osdi23/presentation/li-zhouhan. Zonghang Li, Wenjiao Feng, Weibo Cai, Hongfang Yu, Long Luo, Gang Sun, Hongyang Du, and Dusit Niyato. Accelerating geo-distributed machine learning with network-aware adaptive tree and auxiliary route. IEEE/ACM Transactions on Networking, 2024. Tie Ma, Long Luo, Hongfang Yu, Xi Chen, Jingzhao Xie, Chongxi Ma, Yunhan Xie, Gang Sun, Tianxi Wei, Li Chen, et al. Klonet: an easy-to-use and scalable platform for computer networks education. In 21st USENIX Symposium on Networked Systems Design and Implementation, pp. 20252046, 2024. Yixuan Mei, Yonghao Zhuang, Xupeng Miao, Juncheng Yang, Zhihao Jia, and Rashmi Vinayak. Helix: Distributed serving of large language models via max-flow on heterogeneous gpus. arXiv preprint arXiv:2406.01566, 2024. Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin Cui, and Zhihao Jia. Spotserve: Serving generative large language models on preemptible instances. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pp. 11121127, 2024. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 35053506, 2020. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Yuanxin Wei, Shengyuan Ye, Jiazhi Jiang, Xu Chen, Dan Huang, Jiangsu Du, and Yutong Lu. Communication-efficient model parallelism for distributed in-situ transformer inference. In 2024 Design, Automation & Test in Europe Conference & Exhibition, pp. 16. IEEE, 2024. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art In Proceedings of the 2020 conference on empirical methods in natural language processing. natural language processing: system demonstrations, pp. 3845, 2020. Shengyuan Ye, Jiangsu Du, Liekang Zeng, Wenzhong Ou, Xiaowen Chu, Yutong Lu, and Xu Chen. Galaxy: resource-efficient collaborative edge ai system for in-situ transformer inference. arXiv preprint arXiv:2405.17245, 2024. Mingjin Zhang, Jiannong Cao, Xiaoming Shen, and Zeyang Cui. Edgeshard: Efficient llm inference via collaborative edge computing. arXiv preprint arXiv:2405.14371, 2024. Huaman Zhou, Weibo Cai, Zonghang Li, Hongfang Yu, Ling Liu, Long Luo, and Gang Sun. TsenIEEE gine: Enable efficient communication overlay in distributed machine learning in wans. Transactions on Network and Service Management, 18(4):48464859, 2021."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PROOF OF PROPOSITION 2 In conventional data parallel systems, each device sends several gigabytes of data, putting significant pressure on network bandwidth. This makes data transfer latency major concern, while link latency becomes negligible. Then, tree and ring-based algorithms are introduced to optimize the data transfer. However, they do not apply to our case. In TPI-LLM, each device only sends small amount of data, usually just tens of kilobytes. This tiny data size does not strain the network, so data transfer latency is minimal. Instead, in edge networks where wireless communication causes higher transmission delays, link latency becomes more significant than data transfer latency. As result, the commonly used tree and ring-based allreduce algorithms are less effective. Let us consider 1 master and 2 workers connected via router. In Figure 7, we compare the traffic models of star, tree, and ring-based algorithms. In star-based allreduce, worker 1 sends data directly to the master via the router, and the allreduce latency (includes reduce and broadcast) is tstar = 2(tdata + tlink) + tbarrier + taggr. In this model, the router only forwards data packets. Figure 7: Comparison of traffic models for star, tree, and ring-based allreduce algorithms. In tree-based allreduce, data from worker 1 must first go through worker 2 before reaching the master, so there are 2 hops involved. In this process, worker 1 sends its data to worker 2, which aggregates it and forwards the result to the master. Once the global aggregation is complete, the final result is broadcast back to all workers. The total time for this process is ttree = 3tdata + 4tlink + 2tbarrier + 2taggr. In ring-based allreduce, each device communicates directly with its neighbors in ring topology. Data is divided and sent in sequence around the ring, with each device receiving, aggregating, and passing the data to the next device. Unlike star or tree-based methods, there is no central device, and data flows continuously between the devices. The total time for the ring-based allreduce is tring = 4 Assume that all devices are homogeneous, i.e., tbarrier 0, and tdata 0, taggr 0 because the data size is very small. Then we have latencies simplified as follows: 3 tdata + 4tlink + 3tbarrier + 2 3 taggr. Thus, the star-based allreduce is the most efficient method because it minimizes link latency. tstar = 2tlink < ttree = tring = 4tlink. (11) A.2 SIMPLE-TO-USE MEMORY SCHEDULER In our implementation, context manager is used to ensure that the required block weights are loaded correctly and unload the used weights to free up memory for subsequent blocks. This simplifies the deployment of large-scale LLMs on low-memory edge devices, requiring just one additional line of code:"
        },
        {
            "title": "Under review",
            "content": "1 with memory_manager.wait_and_release(f\"self_attn.0\"): 2 hidden_states = self_attn(hidden_states) A.3 PROOF OF PROPOSITION 3 We start with the first attention block and end with the final FFN block. Time slot 1 (attention computation): In this initialization step, 1 attn must be loaded before computing the first attention block, taking τattn + tattn. During the computation time tattn, the next FFN weights, 1 ffn, are loading in parallel. Time slot 2 (allreduce): The attention block is followed by allreduce communication, which takes tall reduce, with the next FFN weights, 1 Time slot 3 (FFN computation): By this time, the FFN weights 1 ffn should be fully loaded. If not, the computation must wait for loading to complete. Let = tattn + tall reduce τffn, if 0, no blocking occurs; otherwise, the computation is delayed by t. Once loaded, compute the FFN block in tffn. ffn, loading in parallel. During this time slot, the waiting, computation of the current FFN block and the weight loading of the next attention block occur simultaneously. By the time the current FFN block finishes, the next attention blocks weights 2 attn have been loading for max{0, tattn + tall reduce τffn} + tffn. Time slot 4 (allreduce): The FFN block is followed by allreduce communication, which takes tall reduce, with the next attention weights, 2 Time slot 5 (attention computation): Ensure that the attention weights 2 attn are fully loaded. Let = max{0, tattn + tall reduce τffn} + tffn + tall reduce τattn. If 0, the computation proceeds without blocking. Then, 2 ffn have been loading for max{0, max{0, tattn + tall reduce τffn} + tffn + tall reduce τattn} + tattn. attn is computed in tattn, and the next FFN weights 2 attn, loading in parallel. Time slot 6 (allreduce): The allreduce communication takes tall reduce, while the next FFN weights 2 ffn are loading in parallel. ffn are fully loaded. Let = Time slot 7 (FFN computation): Ensure that the FFN weights 2 max{0, max{0, tattn + tall reduce τffn} + tffn + tall reduce τattn} + tattn + tall reduce τffn. If 0, the computation proceeds without blocking. This process repeats, until the generation task is finished. For the system to reach steady state where computation is not blocked by weight loading at any time, the following conditions must hold. Case 1: tattn + tall reduce τffn 0. Time slot 3 (l = 1): Time slot 5 (l = 1): Time slot 7 (l = 2): 2tattn + tffn + 3tall reduce 2τffn τattn 0, Time slot 9 (l = 2): 2tattn + 2tffn + 4tall reduce 2τffn 2τattn 0. tattn + tall reduce τffn 0, tattn + tffn + 2tall reduce τffn τattn 0, We repeat these conditions and derive the following patterns. tattn + tffn + 2tall reduce τffn + τattn, tattn + (l 1) tffn + (2l 1) tall reduce τffn + (l 1) τattn. Case 2: tattn + tall reduce τffn < 0. Time slot 3 (l = 1): Time slot 5 (l = 1): Time slot 7 (l = 2): Time slot 9 (l = 2): Time slot 11 (l = 3): tattn + tall reduce τffn < 0, tffn + tall reduce τattn 0, tattn + tffn + 2tall reduce τattn τffn 0, tattn + 2tffn + 3tall reduce 2τattn τffn 0, 2tattn + 2tffn + 4tall reduce 2τattn 2τffn 0. (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22)"
        },
        {
            "title": "Under review",
            "content": "Similarly, repeat these conditions and derive the following patterns. tattn + tffn + 2tall reduce τffn + τattn, (l 1) tattn + tffn + (2l 1) tall reduce (l 1) τffn + τattn. (23) (24) Thus, the proposition is proved. A.4 PROOF OF PROPOSITION Let α = tattn + (l 1) tffn + (2l 1) tall reduce τffn (l 1) τattn > 0, and we derive the following inequality from inequality (16): tattn + tffn + 2l tall reduce τffn τattn > 0. (25) By substituting α into this inequality, we have α + tffn + tall reduce τattn > 0. Let α > 0 > τattn tffn tall reduce, we obtain the first condition: tffn + tall reduce > τattn. (26) Let β = tffn + tall reduce τattn > 0, and substitute β into inequality (16), then we have β + tattn + tall reduce τffn > 0. Let β > 0 > τffn tattn tall reduce, we obtain the second condition: tattn + tall reduce > τffn. (27) Thus, the proposition is proved. A.5 PROOF OF PROPOSITION 5 In this section, we analyze the peak memory footprint on both the master and worker nodes to estimate the largest model size that our memory scheduler can handle. Let us use the Llama model as an example, assume the vocabulary size is v, hidden size is h, number of attention heads is a, number of key-value heads is b, and intermediate size is s. Let = [p1, p2, , pn] be vector representing the proportion of parameters handled by devices, and be the window size of the memory scheduler. Following the block definition in Figure 2, the parameter counts for each block are detailed in Table 4: Table 4: Parameter counts for the main blocks (e.g., = 4, pi = 0.25, Llama 2-7B). Block Preprocess Attention FFN Postprocess Parameters hv 2(a + b)h2pi/a + 3hspi + hv + Block Size 500 MB 64 MB 129 MB 500 MB The memory footprint is affected by parameters, activation storage, temporary tensors, memory management, and caching, making precise quantification challenging. To estimate peak memory, we apply an empirical rule: multiply the parameter size by scaling factor γ. Figure 8: Illustration of the memory window at the peak memory footprint."
        },
        {
            "title": "Under review",
            "content": "From the memory window at the peak memory footprint shown in Figure 8, we can derive the following equations. Mmaster = γ hv + h, 2hv + h, 2hv + + (cid:4) w2 (cid:5) (cid:0)2(1 + )h2pi + h(cid:1) + (cid:4) 2 (cid:5) (3hspi + h), 2 if = 1 if = 2 if 3 For any worker node, the memory footprint does not include the preprocess and postprocess blocks. Therefore, the peak memory footprint Mworker can be expressed as: (cid:18)(cid:106) (cid:22) + 1 2 2 )h2pi + h) + Mworker = γ (3hspi + h) (2(1 + (cid:19) (cid:23) (cid:107) . Thus, the proposition is proved. A.6 PROOF OF PROPOSITION 6 When the memory scheduler reaches steady state, the overlap between computation, communication, and disk I/O is optimized, ensuring that weights are always pre-loaded before they are needed for computations. However, if disk I/O becomes bottleneck and disrupts the steady state (e.g., due to high disk latency), the scheduler must adapt by selectively retaining certain blocks in memory to reduce disk access frequency. In our preliminary experiments, we measured tattn = 11 ms, tffn = 17 ms, τattn = 18 ms, τffn = 30 ms, and observed that FFN blocks generally exhibit higher computation and weight loading latency. By retaining some FFN blocks in memory, we can reduce the need to reload large weights. Let the memory scheduler retain one FFN block in memory every FFN blocks, and I{l=1+kT } = (cid:26)1, if = 1 + kT and 0, 0, otherwise. Similar to the analysis in Appendix A.3, we have Time slot 3 (l = 1): Time slot 5 (l = 1): tattn + tall reduce (1 I{l=1+kT })τffn 0, tattn + tffn + 2tall reduce (1 I{l=1+kT })τffn τattn 0, 2 (cid:88) Time slot 7 (l = 2): 2tattn + tffn + 3tall reduce (1 I{i=1+kT })τffn τattn 0, i= Time slot 9 (l = 2): 2tattn + 2tffn + 4tall reduce Time slot 11 (l = 3): 3tattn + 2tffn + 5tall reduce 2 (cid:88) (1 I{i=1+kT })τffn 2τattn 0, i=1 3 (cid:88) (1 I{i=1+kT })τffn 2τattn 0. i=1 By repeating these conditions, we derive the following patterns: tattn + tffn + 2l tall reduce (cid:88) (1 I{i=1+kT })τffn τattn 0, i=1 tattn + (l 1) tffn + (2l 1) tall reduce Since (cid:80)l i=1 I{i=1+kT } = (cid:6) (cid:7), we have (cid:88) i=1 (1 I{i=1+kT })τffn (l 1) τattn 0. tattn + tffn + 2l tall reduce (l tattn + (l 1) tffn + (2l 1) tall reduce (l Thus, the proposition is proved. (cid:25) (cid:25) (cid:24) (cid:24) ) τffn + τattn, ) τffn + (l 1) τattn."
        },
        {
            "title": "Under review",
            "content": "A.7 KLONET TESTBED One of our testbed, as shown in Figure 9, was built upon Klonet (Ma et al., 2024) to create an edge network environment. Klonet is network emulation platform designed to support the development and testing of new network protocols and applications in realistic environment. It can emulate various network scenarios, such as wireless, mobile, satellite, and optical networks, and provide fine-grained control over the network parameters, such as bandwidth, delay, jitter, and packet loss. It can also integrate with real devices and applications, such as routers, switches, sensors, and smartphones, to create hybrid network experiments. Klonet is based on the Linux operating system and uses virtualization and containerization technologies to create isolated network nodes and links. It provides both GUI and CLI to help users configure and manage their network experiments. Figure 9: Testbed built upon Klonet. This testbed includes 8 user devices (devices 1 to 8), 8 home gateways (routers 1 to 8), and 1 core router (router 9). User devices connect to their home gateways via wired or wireless connections, and these home gateways are interconnected through routers or switches in the edge network. This topology reflects real-world household network interconnections. In addition, the CPU cores, memory, swap limits, bandwidth, and latency settings in Section 4 are based on measurements from the authors edge network. A.8 CONFIGURATIONS OF THE USED MODELS Table 5: Configurations of the used Llama models. Model (FP32) Llama 2-3B Llama 2-7B Llama 2-13B Llama 2-70B Llama 3.1-8B Llama 3.1-70B Yi-34B Layers 26 32 40 80 32 80 60 Params 3 billion 7 billion 13 billion 70 billion 8 billion 70 billion 34 billion Hidden Size Heads KV Heads Required Mem 32 32 40 64 32 64 56 8 8 8 14 GB 26 GB 50 GB 257 GB 31 GB 266 GB 130 GB 3200 4096 5120 8192 4096 8192"
        },
        {
            "title": "Under review",
            "content": "A.9 PEAK MEMORY FOOTPRINT WITH MEMORY WINDOW SIZE 4 Table 6: Peak memory footprint per device with the memory window size set to 4. Memory Scheduler Disabled (GB) Memory Scheduler Enabled (GB) Model (FP32) = 2 = 4 = 6 = 8 = 2 = 4 = 6 = 8 Llama 2-3B Llama 2-7B Llama 2-13B Llama 2-70B Llama 3.1-8B Llama 3.1-70B Yi-34B 7.3 13.7 25.8 129.9 18.4 137.8 67 4.3 7.7 13.9 66.5 11.8 74.0 36. 3.2 5.5 9.7 46.7 9.4 51.4 23.9 1.7 2.4 2.8 4.5 6.3 10.8 6.0 1.5 2.1 2.5 3.1 5.8 10.5 5.6 2.8 4.5 8.0 35.0 8.5 42.5 20.4 1.5 1.8 2.3 3.1 5.6 11.5 5.3 1.5 1.8 2.2 3.1 5.5 11.4 5. A.10 REAL TESTBED AND CONFIGURATIONS The real testbed consists of 4 laptops, all connected via local Wi-Fi router, as shown in Figure 10. Table 7 details the hardware and network configurations of these laptops. In this case study, the laptop in the lower right serves as the master, while the other three laptops act as workers. The workers are connected to the master, and the master is currently generating the output sequence. The generated sequence is identical to that of single-server inference. Figure 10: real testbed composed of 4 laptops connected via local Wi-Fi. Table 7: Hardware and network configurations of the laptops. CPU Model Device Mac Pro Apple M1 Mac Air Dell Intel Core i5 Intel i7-1165G7 Cores Memory 8 4 8 8 GB 8 GB 16 GB Bandwidth 510 Mbps 320 Mbps 610 Mbps Latency CUDA Number No No No 5 ms 7 ms 3 ms 1"
        },
        {
            "title": "Under review",
            "content": "A.11 CASE STUDY WITH 3 LAPTOPS In this case, only 3 out of the 4 laptops are used: one MacBook Pro, one MacBook Air, and one Dell laptop. The results are given in Table 8, indicating slightly higher latency due to reduced parallelism. Table 8: Comparison of Transformers, Accelerate, Transformers+MS, and TPI-LLM on 3 laptops. Model (FP32) Llama 2-3B Llama 2-7B Llama 2-13B Llama 3.1-8B Yi-34B Transformers Accelerate TTFT (s) 61 115 OOM 133 OOM Latency (s/token) 30 56 OOM 65 OOM TTFT (s) 24 30 OOM 37 OOM Latency (s/token) 16 26 OOM 31 OOM Transformers + MS TTFT (s) 4 13 22 20 Latency (s/token) 3 8 18 12 55 TPI-LLM TTFT (s) 3 7 14 13 48 Latency (s/token) 2 6 12 9"
        }
    ],
    "affiliations": [
        "Department of Machine Learning MBZUAI",
        "School of Info & Comm Engineering UESTC"
    ]
}