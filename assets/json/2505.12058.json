{
    "paper_title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation",
    "authors": [
        "Vincent Koc"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 8 5 0 2 1 . 5 0 5 2 : r Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation Vincent Koc Comet ML, Inc. New York, NY, USA vincentk@comet.com Abstract Tiny QA Benchmark++ (TQB++) is an ultra-lightweight evaluation suite designed to expose critical failures in Large Language Model (LLM) systems within seconds, contrasting with extensive benchmarks like MMLU or BIG-Bench. At its heart lies <20KB golden dataset of 52 hand-crafted English Question-Answering (QA) triples, ideal for rapid CI/CD checks and prompt engineering. This paper details the evolution into Tiny QA Benchmark++, which significantly expands upon the original TQB. Key enhancements include: (i) synthetic, on-demand generation toolkita Python LiteLLM script (<300 lines)that produces schema-validated micro-benchmarks in any language, domain, or difficulty, with SHA-256 hashing for provenance; and (ii) pre-built multilingual packs <20KB for Arabic (AR), German (DE), English (EN), Spanish (ES), French (FR), Japanese (JA), Russian (RU), Korean (KO), Portuguese (PT), Turkish (TR), and Chinese (ZH), enabling immediate cross-lingual smoke tests. We position TQB++ as the LLM analogue of software unit tests. Empirically, top-tier models achieve high accuracy ( 90% Exact Match) on the core English set, while performance significantly varies for low-resource languages, demonstrating TQB++s utility in rapidly detecting regressions or quality shifts in LLMOps workflows. The dataset, generator script, and related tools are released under open-source licenses (see Section 1.1 for details) and hosted on the Hugging Face Hub (https://huggingface.co /datasets/vincentkoc/tiny_qa_benchmark_pp) and GitHub (https://github.com/v incentkoc/tiny_qa_benchmark_pp) (Koc, 2025e), promoting accessible and continuous quality assurance in modern LLMOps. Keywords: LLM Evaluation, QA Benchmarks, Synthetic Data, LLMOps, Continuous Integration, Smoke Testing, Croissant Metadata"
        },
        {
            "title": "1 Introduction and Motivation",
            "content": "Large Language Models (LLMs) are typically evaluated on extensive benchmarks like MMLU (Massive Multitask Language Understanding) (Hendrycks et al., 2021) and BIG-Bench (Srivastava et al., 2023), which cover dozens or even hundreds of tasks with thousands of queries. These comprehensive evaluations demand significant time and compute resources; for example, evaluating single model across all 57 tasks of MMLU or 204 tasks of BIGBench can cost many GPU-hours or API fees. In fast-paced development and deployment of LLM-powered applications, often termed LLMOps (the DevOps-inspired practice of deploying, monitoring, and iterating on LLM applications) teams need something far lighter to guard continuous integration (CI) / continuous deployment (CD) pipelines and interactive development loops. 2025 Vincent Koc. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Koc This need for rapid feedback became particularly apparent during projects involving iterative prompt optimization, such as the development of prompt engineering solutions for Comets Opik Optimizer (Comet ML, 2025), where repeatedly running large evaluation suites proved to be significant bottleneck, highlighting the demand for quick yet realistic feedback mechanisms. The Need for Tiny Benchmarks. The original Tiny QA Benchmark (TQB) (Koc, 2025d) was created to address this gap by providing minimal English QA set for rapid CI/CD validation and prompt debugging. Analogous to software unit tests, TQB provides quick, low-cost signal for basic regressions or integration errors (e.g., broken prompt formats, retrieval failures) by covering elementary general knowledge questions that competent models should answer correctly. Its small size (17KB) allows instant loading and evaluation in seconds, making it practical first-line check before invoking resource-intensive benchmarks. While effective as an initial sanity check, TQB intentionally does not aim to replace rigorous benchmarks or provide statistically significant model ranking. Limitations of Fixed Core Set. While the original TQB performs admirably as baseline health check, its fixed, English-only nature presents limitations in real-world deployments which are often multilingual, domain-specific, and may have unique regulatory constraints. static set may miss regressions in, for example, French legal chatbots or Turkish banking agents. Furthermore, many teams require the ability to maintain private, scenario-tailored tests that cannot be easily open-sourced or derived from fixed public benchmark. Contributions of TQB++. To address these limitations and enhance utility for broader LLMOps, we present TQB++. Its key contributions and design goals are: 1. Keep the gold standard. The 52-item English core remains immutable for deterministic regression. 2. Enable synthetic customisation. Provide simple, licence-compliant generator to mint tiny datasets on demand for any language or any topic. 3. Standardise metadata. Package artefacts in Croissant JSON-LD (Akhtar et al., 2024) so tools and search engines can discover and auto-load them. 4. Promote Open Science. Release all code, the core dataset, and generation tools under open-source licenses (see Section 1.1 for details), fostering community adoption and extension. 5. Align with LLMOps. Document CI/CD integration, prompt-engineering workflows, cross-lingual drift detection, and observability dashboards. Together these features deliver practical, extensible resource for continuous evaluation. By providing an easily accessible, standardized mini-dataset hosted on the Hugging Face Hub (https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark_pp) and GitHub (https://github.com/vincentkoc/tiny_qa_benchmark_pp) under open-source licenses (see Section 1.1 for details) (Koc, 2025e), we aim to encourage the community to adopt more robust testing and CI practices for LLM-based applications. 2 Tiny QA Benchmark++: Micro Gold for LLMOps 1.1 Ethics, Licensing, and Provenance The TQB core dataset questions and answers were hand-crafted by the dataset creator from well-known public-domain facts, representing canonical pieces of knowledge that are unlikely to be incorrect or disputed. Since the content is drawn from general knowledge, there are no privacy or sensitivity concerns. The project utilizes multi-licensing strategy for its components: The core English dataset (core en), all software (including the generator script), and the Python client library (tinyqabenchmarkpp on PyPI) are licensed under Apache-2.0. Synthetically generated dataset packs, such as those provided on the Hugging Face Hub or generated by users for distribution, are licensed under custom Eval-Only, Non-Commercial, No-Derivatives license. Croissant JSON-LD metadata files are available under CC0-1.0. The TQB++ generator stamps each synthetic item with SHA-256 hash for provenance tracking. Comprehensive licensing information for each component is available in the projects repository. The Python client library is Apache-2.0 (same as core codebase)."
        },
        {
            "title": "2 Dataset Structure and Schema",
            "content": "2.1 Human-Curated Core (TQB) The human-curated core of TQB consists of 52 question-answer pairs covering general knowledge domains such as geography, history, science (physics, biology, chemistry), mathematics (including basic arithmetic and calculus), technology (computer science), literature, art, logic puzzles, and temporal/calendar trivia. Each example is represented as JSON object with the following fields: text (string): The question prompt, e.g., What is the capital of France? label (string): The gold answer, e.g., Paris. metadata.context (string): one-sentence factual statement supporting the answer (serving as brief evidence or explanation). For example, France is country in Europe. Its capital is Paris. tags.category (string): broad category for the question (e.g., geography, history, math, computer science, etc.). There are roughly 15 distinct categories in the set, spanning STEM fields and the humanities. tags.difficulty (string): rough difficulty level of the question (easy or medium in this dataset; none are labeled hard as the dataset was designed to run quickly). All questions have concise answers (mostly single words, numbers, or short phrases). The context field typically restates known fact and includes the answer, making it useful for extractive QA evaluation or as verification for closed-book answers. For instance, question Who wrote Romeo and Juliet? has context Romeo and Juliet is famous play 3 Koc written by William Shakespeare. and answer William Shakespeare. About two-thirds of the items are labelled easy, and one-third medium, reflecting an intentional bias toward catch-all simplicity. There are no ambiguous prompts or trick questions; every answer is unique in the given context. The entire dataset is single JSON file (approximately 17 KB, 52 entries). It is trivial to load in Python via the Hugging Face datasets library or via plain JSON parsing. Being so small, it loads almost instantly, and evaluations over this set complete in negligible time (a modern LLM API can answer all 52 questions in few seconds). This design choice was deliberate to facilitate fast iteration. The datasets simplicity ensures that virtually all well-performing models should achieve high accuracy, making any failure very conspicuous, aligning with its role as canary for regressions. 2.2 Multi-Lingual Extensions (TQB++) Using the generator in TQB++ releases packs in ten languages: English (EN), French (FR), Spanish (ES), Portuguese (PT), German (DE), Chinese (ZH), Japanese (JA), Russian (RU), Korean (KO), Turkish (TR), and Arabic (AR). Each pack typically contains 50 QA items (e.g., 10 categories 10 questions per category), though this is configurable. Users can regenerate these packs, extend them with more questions, or create packs for new languages and domains with single CLI call using the provided toolkit. The aim for these multi-lingual extensions is not necessarily to ensure identical human-perceived difficulty across languages, which can be highly complex (Koc, 2025b), but rather to evaluate models consistency and capability on conceptually similar tasks that are formulated in different linguistic contexts through standardized generation process. This approach can help identify language-specific performance disparities or weaknesses in multilingual models (Koc, 2025c)."
        },
        {
            "title": "3 Synthetic Data Generation Toolkit (TQB++)",
            "content": "3.1 Design core component of TQB++ is its synthetic data generation toolkit. Implemented in approximately 40 lines of Python code using LiteLLM for provider-agnostic LLM calls (see Appendix ??), the generator can mint bespoke tiny QA datasets. Users can specify parameters such as --num (number of questions), --languages (comma-separated list of language codes), --categories (topics for questions), --difficulty, and --provider (the LLM endpoint to use, e.g., OpenAI, Anthropic, Cohere, or any OpenAI-compatible API). The generation process, illustrated in Figure 1, involves: 1. Crafting system prompt that instructs the LLM to output structured JSON, adhering to the TQB schema (text, label, context, tags). 2. Providing two few-shot exemplars within the prompt to guide the LLM on the desired format and content style. 3. Sending the generation request to the chosen LLM and parsing the response. 4. Basic validation of the generated JSON structure, with retry mechanism (up to = 3 attempts) if the output is malformed. 4 Tiny QA Benchmark++: Micro Gold for LLMOps 5. Storing SHA-256 hash of each generated item for provenance tracking and reproducibility. Figure 1: Workflow of the TQB++ synthetic data generator, illustrating the process from user inputs (language, topic, difficulty) through LLM-based generation with system prompts and few-shot examples, to JSON validation and final QA item output with provenance tracking."
        },
        {
            "title": "4 Practical Usage Scenarios",
            "content": "TQB and TQB++ are designed to slot easily into various LLMOps and evaluation workflows. 4.1 CI/CD Pipeline Testing The dataset can be used as unit test for an LLM service. For example, nightly build or deployment pipeline can automatically run the 52 TQB QA pairs (or relevant TQB++ pack) through the latest model or agent and compare responses to the expected answers. Any incorrect responses can fail the pipeline, alerting engineers before faulty model is released. PyTest fixture might load TQB and assert exact match accuracy 0.95. The check runs in approximately 0.5 seconds on CPU, enabling per-commit gating. Frameworks like Comets Opik explicitly support this pattern, allowing users to store small eval sets and run them as part of CI (via PyTest integration), with automated upload of traces and metrics to monitoring server dashboard. 4.2 Prompt Engineering and Agent Development When iterating on prompts or multi-step agents (e.g., using LangChain or DSPy), developers can rerun the tiny set after each edit. Because the categories are broad, failure instantly localises issues: e.g., if the math category drops, one might check calculator tool integration; if history questions fail, retrieval chain bug might be the cause. If an advanced multi-step agent fails any TQB question that basic single-call model could handle, that signals problem in the orchestration. LangChain can directly ingest the Hugging Face dataset object or converted Pandas DataFrame. DSPy can also utilize these QA pairs as test cases to verify that its compiled strategies yield correct answers. Koc Figure 2: Conceptual diagram of TQB++ in CI/CD Pipeline, showing code commit triggering test execution, results logging, decision points, and feedback loops. 4.3 Evaluation Harness Integration TQB can be encoded as an OpenAI Evals YAML or LangSmith dataset, providing dashboards of accuracy over time. Category tags permit fine-grained tracking (e.g., the science subset of French TQB++ pack). LLM observability platforms increasingly emphasize fine-grained tracing and evaluation (J, 2024), and fixed tiny test set is low-noise signal for such monitoring. One could create an eval with TQB to regularly track models accuracy on these 52 questions as baseline health metric (Lam, 2024). Figure 3: Example Observability Dashboard for TQB++ Monitoring. 4.4 Cross-Lingual Drift Detection (TQB++) By replaying multi-lingual TQB++ packs hourly or daily in production, teams can detect localisation regressions. For example, after an update to Turkish tokenizer, if accuracy on the TR pack dropped significantly (e.g., 18 percentage points), this would be caught proactively, potentially before widespread user impact. 6 Tiny QA Benchmark++: Micro Gold for LLMOps 4.5 Demonstrations and Adaptive Testing (TQB++) The 52-item TQB set is small enough for live demos when showcasing new LLMs, toolchains, or evaluation methodologies. It spotlights differences in model capabilities or the effects of prompt changes without overwhelming audiences. For highly specialized or rapidly evolving domains, TQB++ can also support an adaptive testing paradigm through test-time dynamic generation. Instead of relying solely on pre-generated static datasets, new micro-benchmarks can be synthesized on-the-fly, tailored to the specific features, code changes, or even production data drifts being evaluated. For example, when testing new module that handles niche topic, small, highly relevant TQB++ pack can be generated just before the test execution. This ensures that the evaluation directly targets the functionality in question with fresh, specific examples. The LiteLLM integration, with its potential for caching similar generation requests, can help manage the latency of such on-demand generation, making it viable for CI/CD environments. This approach moves towards pseudo LLM-as-a-judge for dataset creation at the point of testing, offering higher degree of agility and relevance for complex, fast-moving projects. 4.6 Monitoring Fine-tuning Dynamics (TQB++) key challenge in fine-tuning LLMs is preventing catastrophic forgetting or unintended erosion of general knowledge and capabilities. TQB++, with its ability to generate targeted micro-benchmarks across diverse categories and languages, offers lightweight mechanism for monitoring these dynamics. By regularly evaluating model undergoing fine-tuning against suite of relevant TQB++ packs (both the core English set and synthetic variants for specific domains or languages of interest), developers can gain rapid insights into how the fine-tuning process affects different knowledge areas. For example, significant drop in accuracy on general science TQB++ pack after fine-tuning on narrow legal domain might indicate knowledge erosion. This signal, obtained quickly and cheaply, can inform adjustments to the fine-tuning strategy (e.g., data mixture, regularization) or serve as an early warning before more extensive evaluations are conducted. This approach is particularly valuable for resource-constrained teams or when developing smaller, open-source models where extensive post-fine-tuning evaluations for every epoch or checkpoint are infeasible. The observed log-loss or accuracy degradation on these targeted tiny benchmarks can even be candidate signal for integration into RLHF processes to help preserve desired general capabilities. 4.7 Domain-Specific Smoke Tests from Telemetry (TQB++) Building on adaptive testing, TQB++ can be enhanced to generate domain-specific smoke tests by integrating with LLM operational telemetry or domain-specific knowledge bases. For instance, an observability platform monitoring an LLM application might detect emerging themes in user queries or identify specific types of interactions where the LLM struggles. This telemetry could then seed the TQB++ generator, prompting it to create small, highly relevant set of QA pairs focused on these new themes or problematic areas. Such an approach would allow for the automated creation of dynamic, targeted smoke tests that directly validate the systems handling of observed real-world challenges or evolving domain Koc requirements, providing powerful tool for proactive quality assurance and rapid response to changing operational landscapes. 4.8 Implications for LLMOps and Future Work The experimental validation reinforces the potential of TQB++ as practical tool for LLMOps. The ability to quickly generate targeted, multilingual micro-benchmarks that are sensitive to model capabilities, language differences, and question difficulty offers several advantages for continuous evaluation and quality assurance: Efficient CI/CD Gates: Small TQB++ packs can act as rapid, low-cost checks in CI/CD pipelines, catching regressions before deployment without the overhead of large benchmark suites. Agile Prompt Engineering: Developers can iterate on prompts and agent designs, using relevant TQB++ sets for immediate feedback on how changes impact core QA performance across different categories or languages. Targeted Drift Detection: Custom-generated TQB++ packs for specific domains or languages of interest can monitor for performance drift in production systems, as discussed in Section 4. Resource-Constrained Evaluation: Teams with limited access to compute or expensive model APIs can still perform meaningful, albeit coarse-grained, evaluations and comparisons. The possibility of theoretically feeding domain knowledge or LLM call telemetry into the TQB++ generation process to create highly contextualized smoke tests, as suggested during our research, presents pertinent direction for future work. This could involve: Developing workflows where production data drifts (e.g., new topics appearing in user queries) trigger the generation of fresh, relevant TQB++ micro-benchmarks to ensure the system handles these new scenarios correctly. Integrating the TQB++ generator with LLM observability platforms, allowing automated creation of small validation sets based on monitored operational data or identified failure modes. This would enable form of self-healing or adaptive evaluation where the test suite evolves with the application and its usage patterns. Further research could also explore more sophisticated synthetic generation techniques, potentially incorporating LLM-as-a-judge mechanisms directly into the generation loop for on-the-fly quality filtering and refinement (Koc et al., 2025). Enhancing the generator to allow for more fine-grained control over the distribution of generated question types (e.g., factual, inferential, mathematical) within categories could also increase its utility. The current study focused on o3-mini for generation; exploring the quality and characteristics of TQB++ datasets generated by other LLMs (including open-source models) would also be valuable contribution. Overall, the experiments demonstrate that TQB++ provides flexible and effective solution for lightweight LLM smoke testing, bridging gap between rapid iteration needs and the requirement for continuous quality assurance in the LLMOps lifecycle. 8 Tiny QA Benchmark++: Micro Gold for LLMOps"
        },
        {
            "title": "5 Evaluation Philosophy",
            "content": "TQB++ follows small-but-sufficient ethos for early-stage validation, in contrast to exhaustive benchmarks aimed at fine-grained model differentiation. While the field currently lacks comprehensive formal theory of LLM smoke testing or minimal competency evaluation, TQB++ serves as practical case study and step towards defining such framework. Its design and application motivate several desirable properties (desiderata) for effective smoke tests in LLMOps: Rapid Execution Low Cost: Tests should complete in seconds, incurring minimal computational or financial overhead, to be viable for per-commit checks. High Sensitivity to Basic Flaws: The benchmark should target foundational capabilities where regressions are clear indicators of systemic problems (e.g., broken prompt formats, context retrieval failures, severe performance degradation). Broad Initial Coverage: While tiny, the test should span diverse range of general knowledge or core functionalities to catch variety of potential issues early. Configurability Extensibility: The ability to generate targeted variants (e.g., different languages, domains, difficulties, as with TQB++s generator) allows tailoring smoke tests to specific project needs. Clear Interpretation of Results: Outcomes should be easily understandable, often binary (pass/fail) or near-binary, facilitating quick decision-making in CI/CD pipelines. TQBs design, where high accuracy is expected, makes deviations highly salient. Deterministic Core with Stochastic Options: An immutable core set (like TQBs 52 items) ensures consistent regression detection, while synthetic generation (TQB++) allows for randomized variants to combat overfitting to the test set. This approach is about ensuring minimal competence and catching glaring problems. Binary safety and Signal in Deviations. The set is designed to be easy enough that any error on the core TQB signals potentially serious regression. We expect any advanced model to nearly ace this test; thus, even one failure is noteworthy. If models performance drops from 98% to 90% on TQB, that is clear signal to investigate. Evaluation is typically done via exact-match or simple string comparison, making automated grading straightforward. Cheap Redundancy with Synthetic Variants. The TQB++ generator allows teams to create unlimited synthetic variants. This enables randomisation of tests, which can help combat overfitting to fixed small set and tailor evaluations to specific domains or languages not covered by the core set. For multilingual variants, the focus is on assessing models consistent performance on tasks generated with similar logic across different languages, rather than strictly calibrated cross-lingual human difficulty, thereby aiding in the detection of language-specific capability gaps (see also 2 and (Koc, 2025c,b)). 9 Koc Two-stage Pipeline. TQB is intended as first-line check. Models or pipelines should pass tiny tests first, then proceed to more comprehensive and resource-intensive benchmarks (e.g., MMLU, BIG-Bench, HELM). This mirrors the unit testing integration testing performance testing progression in traditional software development. This philosophy is supported by recent findings that tiny targeted benchmarks can be useful. For instance, Polo et al. (2024) show that evaluating an LLM on as few as 100 curated examples can approximate its performance on much larger benchmark with surprising accuracy for ranking purposes. The goal of TQB is less about estimating full benchmark scores and more about acting as safety net or detector of severe issues: coarse but quick. While very small benchmarks can yield high variance and are not statistically reliable for measuring incremental improvements between top-tier models (Hochlehnert et al., 2025), TQBs purpose is largely binary for basic models: detect disasters, not measure fine-grained triumphs. After passing the TQB test, model should still be put through rigorous, large-scale evaluations."
        },
        {
            "title": "6 Experimental Setup",
            "content": "Performance was primarily assessed using two metrics. For clarity in reporting in this section, all scores (EM and LR) are generally presented on 0-100 scale (e.g., an EM of 0.904 is reported as 90.4), unless specified otherwise by the original metric definition (e.g. F1-scores which are typically 0-1). When LR Score or LR Accuracy is reported in tables (such as Table 6) and general performance discussions, it refers to the percentage of items achieving rsaw Levenshtein Ratio of 0.75 or higher, effectively representing an accuracy based on this threshold. Raw Levenshtein Ratios themselves are used directly in the threshold calibration analysis (Section 7). Exact Match (EM) Accuracy: As defined in Section 5, after normalizing both predicted and gold answers (lowercase, removal of articles, punctuation, and extra whitespace). 6.1 Metrics and Test Criteria For the core TQB (52 items), the primary metric is Exact Match (EM) accuracy. Given question qi with gold answer ai and model prediction pi, EM is defined as: EMi = (cid:40) 1 if normalize(pi) = normalize(ai) 0 otherwise where normalize(s) is function that typically converts to lowercase, removes articles (a, an, the), punctuation, and extra whitespace. Overall accuracy is 1 i=1 EMi for = 52. (cid:80)N pass/fail threshold can be set, e.g., Accuracy 0.95 (allowing for 2 errors). While EM is stringent and unambiguous metric suitable for the deterministic nature of the core TQB, it may penalize responses that are semantically correct but differ slightly in phrasing. For generated TQB++ datasets, or for use-cases where minor answer variations are acceptable, alternative metrics can be more appropriate, such as the Levenshtein Ratio (LR). 10 Tiny QA Benchmark++: Micro Gold for LLMOps The validation (Section 7.1) employed both EM and LR. Although LR can offer more nuanced scoring, EM is often preferred for its simplicity and speed in high-frequency smoke testing scenarios, where clear, binary signal is most valuable. The choice of metric can depend on the specific goals of the evaluation. For instance, Levenshtein distance (Levenshtein, 1966) quantifies the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word into the other. This raw distance is often normalized into similarity ratio, e.g., 1 (lev(a, b)/ max(a, b)). For such normalized ratio, suitable pass threshold might be lower than for EM, for example, around 0.75, though an empirically calibrated threshold (see Section 7) is recommended. This threshold could be calibrated based on human feedback assessing acceptable answer variations (e.g., initial calibration for TQB based on sample of = 5 human evaluations per item suggested that certain semantic equivalents would pass at this level but fail EM). For deeper semantic understanding, especially with more complex or nuanced answers that might arise in custom-generated TQB++ packs, embedding-based distance (e.g., cosine similarity between answer embeddings) can provide measure of semantic equivalence. If and are the n-dimensional vector embeddings of the gold answer and the predicted answer respectively, the cosine similarity is defined as: extcos(A, B) = AB = (cid:80)n k=1 AkBk (cid:113)(cid:80)n (cid:113)(cid:80)n k=1 A2 k=1 B2 While these alternative metrics offer valuable flexibility, particularly for the diverse outputs possible with TQB++ variants, the core TQB evaluation in this paper primarily relies on EM for its strictness and clarity as smoke test. These more nuanced metrics are often more practical for multilingual contexts, particularly with non-Latin scripts, compared to n-gram overlap metrics like BLEU or ROUGE which, while common in other NLP tasks, may be less suitable for short, factual answers and can face challenges with morphologically rich languages (Koc, 2025b,c)."
        },
        {
            "title": "7 Results and Discussion",
            "content": "The models evaluated in these experiments are detailed in Appendix A.2  (Table 4)  . 7.1 Overall Performance Trends One of the primary goals for TQB++ is to serve as rapid evaluation tool sensitive enough to detect known performance variations, such as those between LLMs of different sizes within the same model family, or across diverse datasets. The experiments aimed to validate this sensitivity. We anticipated that larger, more computationally intensive models would generally outperform their smaller, distilled counterparts, and that performance would vary based on language and the nature of the questions. TQB++ should surface these expected gradations quickly. The experiments revealed clear performance gradations across models and datasets, underscoring TQB++s ability to differentiate model capabilities even with small test sets. Figure 4 presents heatmap of EM scores, providing visual overview. Models are grouped 11 Koc by family (e.g., Gemma, Mistral) and generally ordered by size/capability, allowing for visual inspection of performance hierarchies. similar heatmap for LR scores (using an empirically determined threshold, see Section 7) is provided in Appendix B.1 (Figure 6). Figure 4: Heatmap of Exact Match (EM) scores across models and datasets (including supplementary challenge datasets sup-ancientlang en 10 and sup-medicine en 10). Darker shades indicate higher accuracy. Models are grouped by family and generally ordered by size/capability within families, illustrating performance degradation with decreasing model weight (e.g., Gemma-3 12b vs. 4b) and variance across datasets. As anticipated, and visibly in Figure 4, larger and more capable models within the same family generally outperformed their smaller counterparts. For instance, gemma-3-12b consistently scored higher than gemma-3-4b across most datasets (e.g., on core en, EM 90.4 vs 86.5; on pack fr 40, EM 80.0 vs 75.0). Similarly, mistral-24b-instruct surpassed mistral-7b-instruct (e.g., on core en, EM 84.6 vs 50.0). This trend is key indicator that TQB++ effectively reflects model scale and capability. This pattern is also observed with LR metrics (see Appendix B.1). For comprehensive breakdown of EM scores for all models across all datasets, including calculated deltas between intra-family model variants, see Table 5 in Appendix C. The core en dataset (52 human-curated items), generally elicited high scores from the top-performing models, establishing baseline of expected performance. The synthetically generated English packs of varying sizes (10, 20, 30, and 40 items, denoted pack en 10, pack en 20, pack en 30, and pack en 40 respectively) also showed similar performance patterns, indicating that even small synthetic sets can effectively rank models by capability. Performance on the specialized supdatasets (ancient languages and medicine, each 10 items) was notably lower across all models, reflecting their hard difficulty 12 Tiny QA Benchmark++: Micro Gold for LLMOps and niche topics, and demonstrating TQB++s utility in creating targeted challenging evaluations. 7.2 Multilingual Performance Analysis key goal of TQB++ is to facilitate quick assessment of multilingual capabilities. The experiments compared performance on the English datasets (core and synthetic) with newly generated 40-item packs for French (FR), Japanese (JA), and Turkish (TR). Table 1 summarizes the mean EM scores (averaged across all models, excluding specialized supdatasets). More revealingly, it also shows EM scores for specific model pair (gemma-3-12b vs. gemma-3-4b) on representative datasets for each language, highlighting how the performance between model variants can change across languages. English datasets consistently yielded the highest scores. Japanese and Turkish presented more significant challenges for most models, and the performance between model sizes can become more pronounced for these languages. Table 1: Mean Exact Match (EM) Scores by Language (0-100 Scale) and Example Model Pair Performance. Language ISO Code Avg. EM (all models) gemma-3-12b (on 40-52 items) gemma-3-4b (on 40-52 items) (12b-4b) English French Japanese Turkish en fr ja tr 86.1 60.0 29.1 36.3 90.4 (core en) 80.0 (pack fr 40) 50.0 (pack ja 40) 50.0 (pack tr 40) 86.5 (core en) 75.0 (pack fr 40) 37.5 (pack ja 40) 50.0 (pack tr 40) 3.9 5.0 12.5 0.0 Further analysis of EM scores by language and category (details available in supplementary materials) reveals nuances. For instance, while some categories might see relatively stable performance across EN and FR, the drop-off can be more pronounced for JA and TR. This degradation is often more severe for smaller model variants within the same family. For example, while gemma-3-12b achieved an EM of 50.0 on pack ja 40 and 50.0 on pack tr 40, the smaller gemma-3-4b scored lower at 37.5 and 50.0 respectively. An even smaller model like llama-3.2-1b-instruct scored only 12.5 on pack ja 40 and 7.5 on pack tr 40. This suggests that while larger models may possess some zero-shot or few-shot capabilities in these languages, smaller models struggle significantly more, making TQB++ effective for detecting such weaknesses rapidly. The performance decay underscores the importance of specific multilingual evaluations, as strong English performance does not guarantee comparable efficacy in other languages, especially those typologically distant from English or less represented in common training corpora. These findings align with the motivation for TQB++s multilingual generation capability: providing simple way to create targeted smoke tests that can flag potential issues in nonEnglish language handling before more exhaustive (and expensive) multilingual benchmarks are run. 13 Koc 7.3 Impact of Data Characteristics The TQB++ framework allows for tagging items with metadata like difficulty and category, enabling more granular performance analysis. Table 2: EM Scores by Difficulty. Table 3: EM Scores by Category. Model Family Variant Easy Medium Hard Variant Art Math GemmaMistral gemma-3-12b gemma-3-4b (12b-4b) mistral-24b (it) mistral-7b (it) (24b-7b) 84.8 84.8 0.0 82.4 54.4 28.0 80.2 74.5 5. 72.6 45.3 27.3 49.0 29.4 19.6 58.8 9.8 49.0 gemma-3-12b gemma-3-4b (12b-4b) mistral-24b (it) mistral-7b (it) (24b-7b) 88.2 82.4 5. 82.4 47.1 35.3 85.3 79.4 5.9 82.4 50.0 32.4 Mean Exact Match (EM) Scores (0-100 Scale). Left  (Table 2)  : by Stated Question Difficulty. Right  (Table 3)  : by Selected Question Categories. (it) denotes instruct-tuned models. Performance by Difficulty. As expected, model performance generally decreased as the stated difficulty of questions increased from easy to medium to hard. Table 2 illustrates this trend for EM scores, highlighting how TQB++ can detect performance variations even between models of the same family. Most models maintained higher accuracy on easy questions, but scores typically dropped for medium and more significantly for hard questions. This differentiation is noticeable even between closely related models; for example, the between gemma-3-12b and its smaller counterpart gemma-3-4b widens as questions become harder, key sensitivity TQB++ aims to expose. This demonstrates that the synthetic generator, guided by simple difficulty prompts, can produce datasets that elicit differential performance reflective of question complexity. The two supplementary datasets (sup-ancientlang en 10 and sup-medicine en 10), which were entirely composed of hard questions, proved challenging for all models (scores often below 50.0, refer back to Figure 4). Performance by Category and Generator Bias. Analyzing scores by category (full details in supplementary materials, with summary heatmap in Appendix B.1 (Figure 7)) reveals that model performance can vary significantly across different knowledge domains. These patterns can indicate relative strengths or weaknesses in models training data or reasoning capabilities for specific topics. Table 3 shows EM scores and deltas for illustrative categories for the same model pairs, again showing TQB++s ability to surface fine-grained differences. 7.4 Levenshtein Ratio Calibration Details While Exact Match (EM) is stringent metric, it can be overly punitive for answers that are semantically correct but differ slightly in phrasing or due to minor OCR-like errors in model outputs. The Levenshtein Ratio (LR) offers more flexible alternative. However, choosing an appropriate acceptance threshold for LR is crucial. An LR threshold that is too low might accept incorrect answers, while one that is too high approaches EM in strictness. 14 Tiny QA Benchmark++: Micro Gold for LLMOps To determine data-driven optimal LR threshold, bootstrapping analysis was performed. For each potential LR threshold from 0.00 to 1.00 (in increments of 0.01), EM scores were treated as ground truth (1 for match, 0 otherwise). Precision, recall, and F1-score were calculated by comparing LR-based classifications (answer considered correct if its LR score threshold) against these EM-based ground truths across all model predictions on all datasets. This process was repeated with 1000 bootstrap samples to establish confidence intervals, although the primary interest was the threshold maximizing the F1-score. Figure ?? plots the F1-score against varying LR thresholds. The analysis indicated that an LR threshold of 0.95 maximized the F1-score (achieving an F1 of 1.000 in this specific analysis, suggesting very high agreement with EM at this tight threshold, while still allowing for very minor variations). Your initial analysis output confirms this optimal point. This empirically derived threshold provides more robust alternative to an arbitrarily chosen one, such as the 0.75 threshold used for generating the LR Accuracy scores presented in the overview heatmap (Figure 6 in Appendix B.1) and the detailed LR score table (Table 6 in Appendix C). The 0.75 threshold was used for those summary tables to align with common practice or initial exploratory settings before this specific F1-based calibration. This finding suggests that for the kind of short, factual answers typical of TQB++, relatively high LR threshold (e.g., 0.95) is needed if the goal is to closely approximate EM while tolerating minimal differences. For practical purposes, an LR threshold between 0.85 and 0.95 might be reasonable range depending on the desired trade-off between precision and recall. It is also critical to reiterate that for multilingual LR calculations (especially with FR, JA, TR), Unicode normalization (e.g., to NFC or NFKC form) of both gold and predicted strings before computing Levenshtein distance is essential to prevent spurious differences due to character encoding variations. Figure 5: F1-score versus Levenshtein Ratio (LR) threshold. The plot shows how the F1-score (comparing LR-based acceptance to EM ground truth) changes as the LR threshold is varied. The peak indicates the optimal LR threshold. 7.5 Efficacy of Micro-Benchmarks core proposition of TQB++ is that even very small, synthetically generated datasets can serve as effective sense-checking or smoke tests. The experimental results support this. Koc Performance trends observed on larger datasets (e.g., core en with 52 items or pack en 40 with 40 items) regarding model ranking and sensitivity to difficulty or language were largely mirrored on the smaller English packs like pack en 10 and pack en 20. For example, referring back to Figure 4 and Appendix B.1, the relative performance of different model families and sizes is generally discernible even with the 10-item pack en 10. On the 10-item English pack (pack en 10), larger models showcased strong performance. For instance, gemma-3-12b achieved an EM of 100.0, while mistral-24b-instruct scored EM 90.0. In comparison, smaller counterparts such as llama-3.2-1b-instruct scored EM 70.0, and mistral-7b-instruct obtained EM 60.0. More subtly, performance differences between variants of the same model family, such as gemma-3-12b versus gemma-3-4b, can also be detected. On pack en 10, gemma-3-12b achieved an EM of 100.0, while gemma-3-4b scored 90.0. While the absolute scores might have higher variance with fewer items, the directional signal regarding severe performance regressions or basic capability checks, including clear differences between model sizes and more subtle ones between intra-family variants, remains. The stability of EM scores across small English synthetic packs further illustrates this. For instance, gemma-3-12b scored EM 100.0 on pack en 10, 90.0 on pack en 20, and 93.3 on pack en 40. Similarly, mistral-24b-instruct scored 90.0, 85.0, and 93.3 on these respective datasets. While not identical, these scores are comparable and show that 10 or 20-item pack can provide reasonable indication of performance that is not dramatically different from 40-item pack for these types of general knowledge questions. This reinforces the idea that small, well-constructed (even if synthetically generated) set can be surprisingly informative for quick checks, capable of surfacing both coarse and finer-grained performance signals. This rapid feedback is invaluable in CI/CD pipelines or iterative prompt engineering, where waiting for extensive benchmark results is impractical. 10 or 20-item TQB++ pack can flag major issues in seconds."
        },
        {
            "title": "8 Related Work",
            "content": "The evaluation of LLMs is rapidly evolving field. TQB++ situates itself within this landscape by addressing specific need for lightweight, continuous testing. Large Benchmarks. Comprehensive benchmarks like MMLU (Hendrycks et al., 2021), BIG-Bench (Srivastava et al., 2023), and HELM (Liang et al., 2022) are essential for robustly assessing model capabilities across wide array of tasks. However, their computational cost and time requirements make them unsuitable for high-frequency testing during development. Tiny and Efficient Benchmarks. The need for more efficient evaluation has spurred research into smaller, targeted benchmarks. The tinyBenchmarks project (Polo et al., 2024) demonstrates that carefully selected small subsets of major benchmarks can reliably approximate full benchmark performance for model ranking. TQB++ shares this spirit of efficiency but targets an even smaller scale, focusing on CI smoke testing and regression detection rather than comparative model ranking. Other efforts like OpenAI Evals (OpenAI, 2024) provide frameworks for running custom evaluations, and TQB can serve as readymade eval set for such systems. Synthetic QA Generation. The generation of synthetic data for training and evaluation is growing area. Surveys like Long et al. (2024) cover the landscape of LLM-generated 16 Tiny QA Benchmark++: Micro Gold for LLMOps synthetic data. Some works focus on privacy-preserving synthetic data generation, such as differential privacy pipelines (Kurakin et al., 2024). The TQB++ generator, while also leveraging LLMs, emphasizes simplicity, rapid generation via minimal prompting, strict schema adherence for CI/CD compatibility, and provider-agnosticism through LiteLLM. This contrasts with other synthetic data efforts that might focus on generating highly complex reasoning chains, achieving state-of-the-art performance on specific benchmarks through synthetic training data, or involving more intricate generation and filtering pipelines. TQB++s approach is tailored for creating readily usable, schema-consistent micro-benchmarks for continuous evaluation and smoke testing. Tool-Augmented QA and Agent Evaluation. As LLMs are increasingly used as reasoning engines in autonomous agents that can use tools, benchmarks like ToolQA (Zhuang et al., 2023) are emerging to evaluate their ability to correctly use external tools. Furthermore, evolving comprehensive benchmarks such as AgentBench (Liu et al., 2023), which has incorporated increasingly complex tool-use sub-benchmarks, assess broader agent capabilities across diverse environments. TQB++, while not directly testing tool use or complex agency, can act as preliminary gate: if an agent fails basic factual recall or reasoning on TQB, its more complex tool-use or autonomous capabilities might also be compromised. LLMOps and Continuous Evaluation. The MLOps community has recognized the critical need for continuous monitoring of model quality. LLM observability tools and platforms (J, 2024; Lam, 2024) often integrate evaluation data. TQB was initially motivated by such use-cases, providing concrete dataset for these frameworks. Dataset Inference and Provenance. Concerns about data leakage and provenance in LLM training and evaluation are addressed by works like Maini et al. (2021). While TQB is based on public domain facts, the TQB++ generators inclusion of SHA-256 hashes for synthetic items is step towards better provenance for generated test sets. Furthermore, ensuring fairness and mitigating pre-existing biases in models themselves is critical aspect of the LLMOps lifecycle, with dedicated frameworks proposed for their detection and mitigation (Koc, 2025a). Metadata Standards for Datasets. The Croissant initiative (Akhtar et al., 2024) aims to standardize ML dataset metadata for better discovery and interoperability. TQB++ adopts this standard to improve the usability of its artefacts."
        },
        {
            "title": "9 Conclusion",
            "content": "TQB++ extends the original Tiny QA Benchmark by blending deterministic 52-item gold standard English dataset with an extensible synthetic data generation toolkit and machine-readable Croissant metadata. The core TQB provides diverse basic QA coverage in minimal footprint, ideal for rapid sanity checks. TQB++ enhances this with multi-lingual capabilities and custom test generation, aligning with modern LLMOps practices. It is designed to slot into CI/CD pipelines, prompt-engineering workflows, and cross-lingual deployment monitoring to catch regressions or logic bugs before more extensive and costly benchmarks are run. Limitations. TQB++ is intentionally designed as lightweight smoke test focusing on factual QA. Consequently, its small scale and narrow task definition mean it cannot, by 17 Koc itself, detect broader issues such as complex instruction-following regressions, nuanced reasoning failures beyond factoid retrieval, or the generation of plausible-sounding but incorrect information (hallucinations) outside the specific QA context. Its utility is as rapid first-pass filter, not comprehensive measure of all desirable LLM capabilities. All original code, the core dataset, Croissant files, and supplementary analysis materials are released under the Apache-2.0 license. Specific licensing details for all components, including synthetically generated datasets and the Python client library, are provided in Section 1.1. The resources are available on the Hugging Face Hub at https://huggingf ace.co/datasets/vincentkoc/tiny_qa_benchmark_pp and via GitHub repository at https://github.com/vincentkoc/tiny_qa_benchmark_pp. The Python package for the generator can be installed from PyPI: https://pypi.org/project/tinyqabenchmarkpp/. We invite the community to utilize TQB++, fork the generator to add new domains and languages, and contribute to the development of robust, low-friction evaluation practices for LLMs."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We thank the anonymous reviewers for their insightful feedback. This work was supported in part by Comet ML, Inc. The author also acknowledges: the use of OpenAIs o3-mini for generating some of the synthetic datasets used in the validation study. the use of OpenRouter for accessing the models used in the evaluation. the contributions of co-authors on the authors foundational publications in LLM evaluation, which have informed the direction of the current research. Appendix A. Generation and Evaluation Protocol This appendix details the specifics of the synthetic data generation process and the evaluation protocol used in the experiments described in Section 6. A.1 Synthetic Pack Generation Details The synthetic TQB++ datasets were created using Python script. (The script, generate py, is in the suplimentary directory included with this paper). The key aspects of the generation process are as follows: Generator Script. The script (approx. 300 lines of Python, and <200 when its not packaged for PyPi is leveraging litellm) utilizes the litellm library to interact with various LLM providers. It takes parameters such as the number of questions, target language(s), categories, difficulty, and the specific LLM provider and model to use for generation. Prompting Strategy. structured prompting approach was employed: System Prompt: You are dataset generator that outputs JSON with keys: text, label, context, tagscategory, difficulty[easy, medium, hard]. The context MUST be 18 Tiny QA Benchmark++: Micro Gold for LLMOps one-sentence fact that contains the label verbatim. An optional domain context can be appended if provided by the user. Few-shot Examples: Two examples are provided to the model within the user message to guide the output format and style: Example 1: { \"text\": \"What is 2 + 2?\", \"label\": \"4\", \"context\": \"2 + 2 equals 4.\", \"tags\": { \"category\": \"math\", \"difficulty\": \"easy\" } } Example 2: { \"text\": \"Who wrote 1984?\", \"label\": \"George Orwell\", \"context\": \"The novel 1984 was written by George Orwell.\", \"tags\": { \"category\": \"literature\", \"difficulty\": \"easy\" } } User Prompt: template like Generate diff questions in lang about category. Return JSON list ONLY. is used, filled with the user-specified parameters. Generation Model and Hyperparameters. For generating all synthetic datasets used in this papers experiments, the following setup was used: Model: OpenAIs gpt-3.5-turbo-0125 (referred to as o3-mini in Section 6), accessed via an OpenAI-compatible API endpoint. Temperature: 1.0 as its set for OpenAI reasoning models, otherwise would use 0.0 (as mentioned in Section 6, to increase reprodceability in generation). Max Tokens: 4096 (default for the generation script, generator.py). Seed: While the script supports seed argument for providers that use it, specific seed values were generally used during batch generation runs to aim for reproducibility where possible, though fixed seed was not strictly enforced for every single generation instance if not supported by particular endpoint or if exploratory generation was performed. For the reported benchmark packs, best efforts were made to use consistent generation parameters. Response Format: JSON object mode was requested from the API where available (e.g., {\"type\": \"json object\"}). 19 Koc Post-filtering and Validation. The primary validation performed by the generate py script is schema adherence. It checks if the LLM output is valid JSON and if the expected keys (text, label, context, tags with sub-keys category and difficulty) are present in each generated item. If the output is not valid JSON or list of items, it may attempt to parse content from provider-specific refusal fields or retry (though retry logic was not heavily relied upon for the main benchmark generation). No further automated post-filtering, such as self-consistency checks or external factuality verification against knowledge base, is implemented directly within this basic generation script. The quality relies on the LLMs ability to follow instructions and the schema validation. SHA-256 hash is computed for each generated item for provenance. A.2 Evaluation Protocol Models Evaluated. The models used in the evaluation (Section 7) were accessed via LiteLLM, typically through an OpenRouter endpoint or directly if applicable. Table 4 lists the models, their providers/full IDs used in eval batch py, and relevant notes. Table 4: Models Used in Evaluation. Model Alias (in Paper) Provider Notes gemma-3-12b OpenRouter gemma-3-4b OpenRouter ministral-8b OpenRouter ministral-3b OpenRouter mistral-24b-instruct OpenRouter mistral-7b-instruct OpenRouter llama-3.2-3b-instruct OpenRouter llama-3.2-1b-instruct OpenRouter Note: Specific API access costs were not tracked for this study, but choices generally favored accessible models. Latency can vary based on provider and real-time load. OpenAI models were not used for the evaluation runs presented in Section 7. This decision was due to the general lack of publicly disclosed model parameters and comprehensive model cards for OpenAI models, which hinders comparative analysis across model families with varying weights. Additionally, this choice helps to mitigate potential self-reinforcement bias, as an OpenAI model was used for the synthetic data generation. Rationale for Generation Model Choice (o3-mini). The synthetic datasets were primarily generated using OpenAIs gpt-3.5-turbo-0125 model (aliased as o3-mini). This choice was based on pragmatic balance of factors: Accessibility and Cost: OpenAI models are widely accessible via API. For instance, the gpt-3.5-turbo series offers good balance of capability versus cost for generating moderately sized datasets. Reasoning Capability for Quality Data: To ensure the generated synthetic data was of reasonable quality, model with strong reasoning and instruction-following capabilities was preferred. While more advanced models like GPT-4 or Claude 3 Opus might 20 Tiny QA Benchmark++: Micro Gold for LLMOps offer higher quality, gpt-3.5-turbo variants are often considered highly performant for structured data generation tasks (as also indicated by general leaderboards like those on the Artificial Analysis website (Artificial Analysis, 2024)). Using capable model was deemed important for the integrity of the benchmark, even if it was not the absolute top-tier model, to ensure the questions and answers were coherent and factually plausible for smoke test. Speed of Generation: For rapidly creating multiple dataset variants across languages and categories, model with reasonable generation speed was also factor. Alternative high-quality reasoning models like Googles Gemini 1.5 Pro or Anthropics Claude series could also be suitable candidates for generation, contingent on API access, cost, and specific feature requirements (e.g., JSON mode support). Evaluation Script and Hyperparameters. All evaluations were conducted using the eval py script. For these runs, the temperature was set to 0.0 (for OpenAI reasoning models we had to use 1.0) to ensure deterministic outputs from the models under test, and consistent seed (defaulting to 42 in the script if not overridden) was used for reproducibility where supported by the LLM provider and model. Appendix B. Additional Experimental Results B.1 Levenshtein Ratio (LR) Score Heatmap Figure 6: LR Scores by Model/Dataset. Figure 7: EM Scores by Model/Category. Figure 6 (left) shows Levenshtein Ratio (LR) scores (threshold 0.75). Figure 7 (right) shows mean Exact Match (EM) scores by category. Darker shades indicate higher accuracy. Figure 6 shows the Levenshtein Ratio (LR) scores, using an LR threshold 0.75 as discussed in Section 7. B.2 Category Score Heatmap The full heatmap of mean EM scores by model and category is shown in Figure 7. 21 Koc Appendix C. Detailed Score Matrix with Intra-Family Deltas This appendix presents comprehensive score matrices for all evaluated models across the primary TQB and TQB++ datasets (excluding the supplementary challenge datasets: sup-ancientlang en 10 and sup-medicine en 10, which are, however, included in overview Figure 4). For model pairs within the same family, row is included to highlight the performance difference between the larger and smaller variant on each dataset. Table 5 presents the detailed Exact Match (EM) scores, where larger model variants generally outperform smaller ones within the same family, though with some variance by language and dataset size. Table 5: Detailed Exact Match (EM) Scores (0-100 Scale) by Model and Dataset (Supplementary Datasets Excluded), with Intra-Family . Model Family Model Variant en 10 en en 30 en 40 fr 40 ja 40 tr 40 core en Gemma-3 Ministral Mistral Llama-3.2 gemma-3-12b gemma-3-4b (12b-4b) ministral-8b ministral-3b (8b-3b) mistral-24b-instruct mistral-7b-instruct (24b-7b) llama-3.2-3b-instruct llama-3.2-1b-instruct (3b-1b) 100.0 90.0 10.0 80.0 70.0 10.0 90.0 60.0 30.0 80.0 70.0 10. 90.0 90.0 0.0 90.0 90.0 0.0 85.0 70.0 15.0 90.0 65.0 25.0 93.3 96.7 -3.4 90.0 93.3 -3. 93.3 90.0 3.3 93.3 80.0 13.3 93.3 96.7 -3.4 90.0 93.3 -3.3 93.3 83.3 10.0 93.3 73.3 20. 80.0 75.0 5.0 72.5 65.0 7.5 72.5 32.5 40.0 57.5 25.0 32.5 50.0 37.5 12.5 35.0 20.0 15. 47.5 7.5 40.0 22.5 12.5 10.0 50.0 50.0 0.0 37.5 40.0 -2.5 55.0 12.5 42.5 37.5 7.5 30. 90.4 86.5 3.9 80.8 76.9 3.9 84.6 50.0 34.6 84.6 53.8 30.8 Levenshtein Ratio (LR) scores  (Table 6)  , using 0.75 threshold, show similar trend, often with slightly higher scores than EM due to partial credit for near matches. Table 6: Detailed Levenshtein Ratio (LR) Scores (0-100 Scale, using LR threshold 0.75) by Model and Dataset (Supplementary Datasets Excluded), with Intra-Family . Model Family Model Variant en 10 en 20 en en 40 fr 40 ja 40 tr 40 core en GemmaMinistral Mistral Llama-3.2 gemma-3-12b gemma-3-4b (12b-4b) ministral-8b ministral-3b (8b-3b) mistral-24b-instruct mistral-7b-instruct (24b-7b) llama-3.2-3b-instruct llama-3.2-1b-instruct (3b-1b) 100.0 90.0 10.0 80.0 70.0 10.0 90.0 60.0 30.0 80.0 70.0 10.0 90.0 90.0 0. 90.0 90.0 0.0 90.0 70.0 20.0 90.0 65.0 25.0 22 93.3 96.7 -3.4 90.0 93.3 -3. 93.3 93.3 0.0 93.3 80.0 13.3 93.3 96.7 -3.4 90.0 93.3 -3.3 93.3 86.7 6.6 93.3 73.3 20. 80.0 75.0 5.0 82.5 77.5 5.0 77.5 40.0 37.5 65.0 30.0 35.0 65.0 47.5 17.5 37.5 25.0 12. 65.0 17.5 47.5 35.0 17.5 17.5 60.0 62.5 -2.5 47.5 45.0 2.5 62.5 15.0 47.5 42.5 10.0 32. 96.2 88.5 7.7 82.7 84.6 -1.9 88.5 51.9 36.6 86.5 55.8 30.7 Tiny QA Benchmark++: Micro Gold for LLMOps The difference between LR and EM scores  (Table 7)  highlights how much each model benefits from the more lenient LR metric, often indicating generation of semantically close but not identical answers, particularly in non-English languages. Table 7: Difference Between Detailed LR and EM Scores (LR - EM) by Model and Dataset (Supplementary Datasets Excluded). Model Family Model Variant en 10 en 20 en en 40 fr 40 ja 40 tr 40 core en GemmaMinistral Mistral Llama-3.2 gemma-3-12b gemma-3-4b ministral-8b ministral-3b mistral-24b-instruct mistral-7b-instruct llama-3.2-3b-instruct llama-3.2-1b-instruct 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.4 0.0 0.0 0.0 0. 10.0 12.5 5.0 7.5 7.5 5.0 15.0 10.0 2.5 5.0 17.5 10. 12.5 5.0 10.0 12.5 10.0 5.0 7.5 2.5 5.0 2.5 5.8 2. 1.9 7.7 3.9 1.9 1.9 2.0 Appendix D. Sample Dataset Items This section provides few illustrative examples from the core TQB dataset and some of the generated TQB++ packs to give qualitative sense of the data. Sample Dataset Items core en.json (Human-Curated English Core) {\"text\":\"What is the capital of France?\",\"label\":\"Paris\",\"metadata\":{\"context\":\"France is country in Europe. Its capital is Paris.\"},\"tags\":{\"category\":\"geography\",\"difficulty \":\"easy\"}} pack en 10.json (Synthetically Generated English, n=10) {\"text\":\"In which year did the Titanic sink?\",\"label\":\"1912\",\"context\":\"The Titanic sank in 1912.\",\"tags\":{\"category\":\"history\",\"difficulty\":\"medium\"},\"id\":\"ecfc9a30\",\"lang\":\"en \",\"sha256\":\"5599c977...40bcf6a1\"} pack fr 40.json (Synthetically Generated French, n=40) {\"text\":\"Combien font 5 + 7 ?\",\"label\":\"12\",\"context\":\"5 + 7 = 12.\",\"tags\":{\"category\":\" math\",\"difficulty\":\"easy\"},\"id\":\"292402c2\",\"lang\":\"fr\",\"sha256\":\"762e734d...b8b6085\"}"
        },
        {
            "title": "References",
            "content": "Omar Akhtar, Ruanne de Castro, Romain Egele, Osma endangering Trazona, Stephane Massonnet, Sarah Moir, David Kanter, Joaquin Vanschoren, Max Pagels, Vojtech Hudecek, Andrew Zaldivar, Been Kim, and Nicolas Passat. Croissant: metadata format for machine learning datasets. arXiv preprint arXiv:2401.12982, 2024. 23 Koc Artificial Analysis. Independent analysis of ai models and api providers. https://artifi cialanalysis.ai/, 2024. Accessed: 2025-05-15. Inc. Comet ML. Cometopik optimizer documentation. https://www.comet.com/docs/opi k/agent_optimization/opik_optimizer, 2025. Accessed: 2025-05-15. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2021. URL https://arxiv.org/abs/2009.03300. Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility, 2025. URL https://arxiv.org/abs/2504.07086. Jaikanth J. Understanding llm observability - key insights, best practices, & tools. https: //signoz.io/blog/llm-observability/, September 2024. Accessed: 2025-05-15. V. Koc. Framework for fairness in machine learning using detecting and mitigating bias in ai algorithms. In 2025 3rd IEEE International Conference on Business Analytics for Technology and Security (ICBATS-2025), Dubai, United Arab Emirates, May 2025a. V. Koc, K. Alang, J. I. Janjua, and S. B. Peta. Leveraging multiple llm evaluators for scalable and fair language model assessments. In 2025 IEEE International Conference on Metaverse and Current Trends in Computing (ICMCTC), Subang Jaya, Malaysia, April 2025. Vincent Koc. Complexities for non-latin languages & llm evaluations, 03 2025b. URL https://www.comet.com/site/blog/complexities-for-non-latin-languages-llm -evaluations/. Accessed on the urldate. Vincent Koc. Generative ai and large language models in language preservation: Opportunities and challenges, 01 2025c. URL https://arxiv.org/abs/2501.11496. Vincent Koc. tiny qa benchmark (revision ff9143f), 04 2025d. URL https://huggingface. co/datasets/vincentkoc/tiny_qa_benchmark. Vincent Koc. Tiny qa benchmark++ (tqb++) datasets and toolkit. https://huggin gface.co/datasets/vincentkoc/tiny_qa_benchmark_pp, 2025e. See also: https: //github.com/vincentkoc/tiny_qa_benchmark_pp. Alexey Kurakin, Natalia Ponomareva, Umar Syed, Liam MacDermed, and Andreas Terzis. Harnessing large-language models to generate private synthetic text, 2024. URL https: //arxiv.org/abs/2306.01684. Lina Lam. The ultimate guide to prompt evaluation frameworks. https://www.helicone .ai/blog/prompt-evaluation-frameworks, 2024. Accessed: 2025-05-15. Vladimir I. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707710, 1966. 24 Tiny QA Benchmark++: Micro Gold for LLMOps Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents, 2023. URL https: //arxiv.org/abs/2308.03688. Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. On llms-driven synthetic data generation, curation, and evaluation: survey, 2024. URL https://arxiv.org/abs/2406.15126. Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution in machine learning, 2021. URL https://arxiv.org/abs/2104.10706. OpenAI. Openai evals framework. https://github.com/openai/evals, 2024. Accessed: 2025-05-15. Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer examples, 2024. URL https: //arxiv.org/abs/2402.14992. Aarohi Srivastava et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023. URL https://arxiv.org/abs/2206.04615. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: dataset for llm question answering with external tools, 2023. URL https://arxiv.org/abs/23 06.13304."
        }
    ],
    "affiliations": [
        "Comet ML, Inc. New York, NY, USA"
    ]
}