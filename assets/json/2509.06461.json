{
    "paper_title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning",
    "authors": [
        "Yuyao Ge",
        "Shenghua Liu",
        "Yiwei Wang",
        "Lingrui Mei",
        "Baolong Bi",
        "Xuanshan Zhou",
        "Jiayu Yao",
        "Jiafeng Guo",
        "Xueqi Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 1 6 4 6 0 . 9 0 5 2 : r Preprint. Under review. FOCUSING BY CONTRASTIVE ATTENTION: ENHANCING VLMS VISUAL REASONING Yuyao Ge1 Shenghua Liu1 Yiwei Wang2 Lingrui Mei1 Baolong Bi1 Xuanshan Zhou1 Jiafeng Guo1 Xueqi Cheng1 1Institute of Computing Technology, Chinese Academy of Sciences 2University of California, Merced {geyuyao24z, liushenghua}@ict.ac.cn Jiayu Yao"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), trainingfree method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-Language Models (VLMs) have achieved remarkable success across diverse tasks (Radford et al., 2021; Jia et al., 2021; Alayrac et al., 2022). However, in human vision, complex visual features frequently divert attention from task-relevant regions (Treisman & Gelade, 1980). Given this cognitive parallel, question arises: Similarly, do complex images interfere with VLMs attention mechanisms, making it difficult for them to focus on task-relevant regions? To answer this question, we investigate the relationship between visual complexity and attention patterns via quantitative experiments. Specifically, we define visual complexity as texture and color dimensions, revealing significant positive correlation between both factors and attention entropy. Furthermore, our analysis shows that attention entropy negatively correlates with accuracy on visual reasoning tasks. Through this two-stage analysis, we establish that complex visual information impairs VLMs reasoning performance via attention distribution (detailed in Section 3). Based on these findings, we conduct preliminary experiment on TextVQA (Singh et al., 2019) by first applying progressive masking to obscure background regions, then cropping to retain only task-relevant regions and adaptively magnifying them to the original image size. Figure 1 presents two representative samples where cluttered visual environments initially cause incorrect predictions. While incorrect token probability initially prevails in both samples, correct token probability surpasses incorrect probability at mask ratios of approximately 0.02 and 0.65 respectively. These results provide initial validation that masking visual noise can improve correct token probability. Corresponding author. 1 Preprint. Under review. Figure 1: The effect of manually progressive masking on candidate tokens probabilities predicted by QWEN2.5-VL-3B. The x-axis represents mask ratio and y-axis shows log10 probability. To automate the visual noise masking process, we leverage contrasting attention maps between general instructions and task-specific questions to distinguish semantic signal from visual noise. To this end, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), contrastive method for visual extraction. By masking with contrastive attention maps, CARVE crops and magnifies semantic regions to focus on essential semantic signal (detailed in Section 4)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Contrastive Learning in LLMs. Liu et al. (2023b) pioneered contrastive objectives for aligning LLMs with human preferences, establishing the foundation for RECIPE (Chen et al., 2024b) which trains Knowledge Sentinel to determine when queries trigger knowledge updates. Building on alignment challenges, Jiang et al. (2024) employ hallucinated text as hard negatives while Zhang et al. (2024b) apply contrastive learning in hidden representations to suppress hallucinations. Zhai et al. (2025) traces critical transmission paths across all layers, treating less important pathways as negatives, which Pan et al. (2024) further adapted to multimodal LLMs through UniKEs semantictruthfulness space disentanglement. Departing from embedding-space modifications, DeCK (Bi et al., 2025a) shifts contrastive logic to the decoding stage by comparing token probabilities with and without injected knowledge, while parallel applications emerged in DistiLLM-2 (Ko et al., 2025) for knowledge distillation and Zhu et al. (2024) for factual consistency enhancement. Attention-based LLM Optimization. Alayrac et al. (2022) established multimodal foundations through perceiver resampler architecture, which Li et al. (2023a) refined via lightweight Querying Transformer for parameter-efficient visual extraction. Extending attention optimization to text modality, Chen et al. (2025) exploit attention scores for dynamic prompt compression through importance sampling at both token and sentence levels. Ma et al. (2024a) eliminated redundant visual token computations while Acharya et al. (2024) introduced block-sparse mechanisms for parallel processing, and Liu et al. (2025b) bypassed attention bottlenecks through sequential chunk processing in Recurrent LLMs. Yao et al. (2025b) identified position bias in multimodal RAG where models over-focus on boundary items. Zhang et al. (2025) discovered that models consistently know where to look, even when they provide the wrong answer. Our method eliminates visual noise by contrasting attention maps to distinguish semantic pixels from noise pixels without requiring training."
        },
        {
            "title": "3 ATTENTION MAY GO MYOPIC: WHEN, WHY, AND HOW",
            "content": "3.1 DISCOVERY: WHEN VISUAL FOCUS FAILS Building upon the question in Section 1, we aim to investigate the underlying causes of VLMs answering failures. We conduct experiments on TextVQA dataset using QWEN2.5-VL-3BINSTRUCT. As shown in Figure 2, we visualize attention maps during inference and find two interesting phenomena. Attention progressively refines from broad global scanning in shallow layers to regional localization in the middle layers, culminating in focused convergence in deep layers. The degree of convergence varies based on input images. Moreover, visual complexity critically influences attention convergence. In simple scenes with clear targets and minimal distractors, the high-attention regions successfully narrow as layers deepen, 2 Preprint. Under review. Figure 2: Attention maps across different layers during inference. Each row represents visual question-answering task: row 1 shows simple scene with clear targets, while rows 2-3 depict complex scenes with dense textures and multiple similar objects. From left to right, the columns display: input images, shallow layer attention, middle layer attention, and deep layer attention. aligning with task-relevant regions. Conversely, in complex scenes with rich textures and colors, the high-attention regions still attempt to narrow as layers deepen, yet the resulting attention weights remain more diffused compared to simple scenes. As indicated by the annotation Confused where to look, this attention dispersion resembles human hesitation when confronting crowded shelves and ultimately manifests as reasoning failures. These observations lead us to formulate question: Does visual complexity affect VLMs attention distribution, and does this attention distribution further influence VLMs performance? To answer it, we conduct two deeper experiments: First, we quantify the relationship between visual complexity and attention entropy to establish whether complex inputs produce dispersed attention (Section 3.2); Second, we examine the correlation between attention entropy and model performance to determine whether dispersed attention contributes to reasoning failures (Section 3.3). 3.2 ROOT CAUSE: WHY VLMS GO MYOPIC In Figure 2, images in rows 2-3 differ from row 1 by displaying numerous colorful bottles, containing significantly more textures and colors. Therefore, we decompose visual complexity into two dimensions: texture and color, and investigate their respective impacts on attention. We define the texture complexity and the color complexity as follows: Texture Complexity. Let RHW 3 denote an input image. We define the texture complexity Tc(I) using Canny edge detection (Canny, 1986), where E(I) {0, 1}HW represents the resulting binary edge map. The texture complexity is then defined as: Tc(I) = 1 HW (cid:88) (cid:88) i=1 j= E(I)ij = E(I)1 HW [0, 1] (3.1) 3 Preprint. Under review. (a) Original (b) Edge detection (c) Hue distribution (d) Hue histogram Figure 3: Visualization of texture and color complexity analysis. Each row represents sample image: (a) Original image I, (b) Canny edge map E(I) for texture complexity Tc, (c) Spatial hue distribution ζ in HSV space, and (d) Hue statistic (x-axis: hue value, y-axis: ratio). (a) Texture-entropy correlation (b) Color-entropy correlation Figure 4: Correlation analysis between visual complexity and attention entropy. Both attention entropy and complexity are normalized to the [0, 1], divided into intervals of 0.1, and the average attention entropy is calculated within each interval. Color Complexity. Let ζij = Hue(ΨRGBHSV (Iij)) denote the hue value at pixel (i, j) after applying the RGB to HSV transformation operator Ψ. The color complexity is then defined as: Cc(I) = 1 ln B1 (cid:88) b=0 ρb ln ρb, where ρb = nb HW , nb = {(i, j) : ζij = b} (3.2) with = 180 hue bins, yielding Cc [0, 1] where higher values indicate greater color diversity. Figure 3 visually validates the effectiveness of our complexity measurement approach. The first row demonstrates high texture complexity with dense edge networks in E(I) and diverse color distribution across the hue spectrum. In contrast, the second row exhibits minimal edge density and concentrated hue values, indicating lower complexity scores. We therefore proceed to quantitatively investigate the correlation between complexity metrics and attention distribution. For measuring the distribution of attention across visual tokens, inspired by Yao et al. (2025b), we employ Shannon entropy (Shannon, 1948) as our quantification metric. Let Nv denote the number of visual tokens in the models representation. We denote the attention map as A(Q) l,t RNv , where indicates the layer index, the generation time step, and the input question. For entropy analysis, we focus on the final generation step tend and define the overall attention entropy as: = 1 (cid:88) lL H(A(Q) l,tend ) = 1 (cid:88) lL (cid:32) Nv(cid:88) i=1 (cid:33) al,tend,i ln al,tend,i (3.3) where = [Lstart, Lend] represents the layer range under consideration, and al,t,i denotes the contrasted attention weight for the i-th visual token. Higher entropy indicates more dispersed attention, while lower entropy indicates more concentrated focus. Figure 4 presents the correlation analysis between our defined complexity metrics and computed attention entropy. Both texture complexity (Figure 4a) and color complexity (Figure 4b) exhibit strong positive linear relationships with attention entropy. This monotonic trend indicates that complex visual features lead to dispersed attention patterns in VLMs. 4 Preprint. Under review. (a) Entropy-accuracy correlation (b) Layer-wise entropy correlation Figure 5: Attention entropys correlation with accuracy and its evolution across layers. Shaded regions indicate 95% confidence intervals computed as t0.975,n1 s/ n. (a) Shows accuracy for samples grouped by overall attention entropy H. (b) Displays mean entropy across samples per layer as 1 is sample is attention at the final generation step. ), where A(Q,i) l,tend i=1 H(A(Q,i) l,tend (cid:80)N"
        },
        {
            "title": "3.3 CASCADE: HOW PERFORMANCE GOES ASTRAY",
            "content": "Figure 5(a) reveals strong negative correlation between attention entropy and accuracy. As attention entropy increases from 5.1 to 6.8, performance decreases from approximately 76% to 65%, confirming that increased attention dispersion directly impairs visual reasoning capabilities in VLMs. To investigate the hierarchical evolution of attention entropy, we present mean entropy and its distribution across layers in Figure 5(b). The results reveal two notable characteristics: (1) attention entropy monotonically decreases with layer depth, consistent with Figure 2. (2) The 95% confidence intervals progressively widen with increasing depth, indicating enhanced inter-sample variability. For samples with clear visual targets, deep layers achieve highly concentrated attention. In contrast, for noisy samples, the model maintains dispersed attention patterns even in deep layers."
        },
        {
            "title": "4 CONTRASTIVE ATTENTION REFINEMENT FOR VISUAL ENHANCEMENT",
            "content": "4.1 THEORETICAL FOUNDATION: NOISE SUPPRESSION AND VISUAL REFINEMENT Based on our findings in Section 3, where we demonstrated that visual complexity causes attention dispersion and performance degradation, we seek to extract pure task-related semantic signal. Therefore, we first formally define the attention signal decomposition mechanism. Definition 1 (Attention Decomposition): Attention distributions are influenced by inherent visual noise (detailed in Appendix A.2) of the image and task-related semantic signal. The attention map A(Q) l,t (I) decomposes as: A(Q) l,t (I) = Fvis(I) Fsem(Q, I) (4.1) where Fvis(I) RNv captures image-inherent visual noise, Fsem(Q, I) RNv captures task-related semantic signal, and denotes the Hadamard product. When using general instructions G, due to the absence of specific tasks to introduce semantic information, the semantic signal function reduces to uniform distribution (Fsem(G, I) 1Nv ), making general instruction attention predominantly capture visual noise: A(G) l,t (I) Fvis(I) 1Nv = Fvis(I) (4.2) Definition 2 (Semantic Extraction Based on Attention Decomposition): To extract semantic signal function Fsem(Q, I) from A(Q), we define estimated semantic attention ˆA RNv + as our estimate of Fsem(Q, I), which is the solution to the following optimization problem: where the objective function is constructed based on Definition 1s decomposition: ˆA = arg min AA ( A; A(Q), A(G)) (4.3) Preprint. Under review. Figure 6: CARVE comprises three stages: Stage 1 generates general attention distribution A(G) with general instructions; Stage 2 extracts task-specific attention A(Q) ; Stage 3 applies contrasted attention ˆAi to generate enhanced masked images for noise suppression. ( A) = Nv(cid:88) i=1 (cid:124) (cid:16) Ai Fvis,i(I) [Fvis,i(I) Fsem,i(Q, I)] (cid:17)2 + λ Nv(cid:88) i Fvis,i(I) (4.4) (cid:123)(cid:122) Semantic reconstruction error (cid:125) i=1 (cid:124) (cid:123)(cid:122) Visual suppression regularization (cid:125) Theorem 3 (Closed-form Solution for Semantic Extraction): Substituting Definition 1s relationships A(Q) Fvis,i into the optimization objective yields: Fvis,i Fsem,i and A(G) ( A) = Nv(cid:88) i=1 (cid:16) Ai A(G) A(Q) (cid:17) + λ Nv(cid:88) i=1 A2 A(G) (4.5) where λ > 0 is regularization parameter that controls the strength of visual noise suppression. Solving the first-order optimality conditions yields the closed-form solution: ˆAi = A(Q) A(G) + λ = Fvis,i Fsem,i Fvis,i + λ Fsem,i when Fvis,i λ (4.6) Equation 4.6 demonstrates that normalization suppresses the influence of Fvis,i when it dominates (i.e., Fvis,i λ), approximating the semantic signal Fsem,i (detailed analysis in Appendix C). 4.2 CONTRASTIVE ATTENTION-BASED VISUAL ENHANCEMENT Having obtained the semantically refined attention maps { ˆA}, as shown in Algorithm 1, we now proceed to generate attention masks that physically remove visual noise from the input image. Attention Maps Fusion. Since different layers and time steps capture complementary information, we fuse attention maps across the layer range and generation time steps = [tstart, tend] through weighted aggregation. Later tokens encode richer contextual information by accessing complete preceding sequences during inference, thus receiving higher fusion weights. Mask Generation and Visual Extraction. Task-relevant regions are identified by applying the top-p percentile threshold τ = Qp(S), which retains the top (0, 1] proportion of pixels from attention map S. Connected component analysis extracts coherent regions from the thresholded map. 6 Preprint. Under review. Model Step: A-OKVQA POPE QWEN2.5-VL-3B QWEN2.5-VL-7B LLAVA1.5-7B LLAVA1.5-13B w/o CARVE tstart tend Tfull w/o CARVE tstart tend Tfull w/o CARVE tstart tend Tfull w/o CARVE tstart tend Tfull 73.0() 76.5(4.79) 79.2(8.49) 78.3(7.26) 75.0() 77.0(2.67) 78.3(4.40) 78.0(4.00) 71.5() 73.9(3.36) 78.2(9.37) 75.4(5.45) 75.7() 76.2(0.66) 76.9(1.59) 76.5(1.06) 86.9() 87.1(0.23) 88.4(1.73) 87.9(1.15) 87.0() 87.9(1.03) 89.7(3.10) 88.6(1.84) 83.6() 86.8(3.83) 89.0(6.46) 89.0(6.46) 84.6() 90.0(6.38) 90.7(7.21) 90.1(6.50) 50.3() 56.0(11.33) 57.1(13.52) 56.5(12.33) 50.8() 58.6(15.35) 59.7(17.52) 58.1(14.37) 38.7() 57.1(47.55) 66.5(71.83) 66.5(71.83) 42.4() 65.4(54.25) 74.3(75.24) 70.0(65.09) TextVQA 72.8() 76.1(4.53) 76.4(4.95) 76.3(4.81) 75.0() 80.7(7.60) 81.9(9.20) 81.7(8.93) 47.8() 57.9(21.13) 58.2(21.76) 57.9(21.13) 57.1() 59.2(3.68) 61.2(7.18) 61.2(7.18) Table 1: Accuracy comparison of CARVE across VLMs on four datasets. We evaluate three temporal configurations: tstart uses attention from initial generated tokens, tend from final tokens, and Tfull applies weighted fusion across all tokens. We use layer range = [20, 25] for attention fusion. Algorithm 1 CARVE: Contrastive Attention Refinement for Visual Enhancement Notation: M: VLM model; Ξ: attention extraction; πHW : spatial reshape;Qp: top-p threshold; Φ: visual extraction (mask, crop, resize); G: general instruction; τ : threshold; R: connected regions; K: max regions to keep Require: RHW 3, Q, M, Θ = {L, , p, λ, K} 1: Inference: AQ {A(Q) l,t }lL,tT = Ξ(M, I, Q) 2: Inference: AG {A(G) l,t }lL,tT = Ξ(M, I, G) A(Q) 3: Contrast: ˆAl,t l,t A(G) l,t +λ 4: Fuse: (cid:80) (cid:80) 5: Threshold: τ Qp(S) 6: Mask: = (cid:83)K k=1 7: Extract: Irefined Φ(I, ) 8: Inference: return M(Irefined, Q) Weighted attention fusion Compute threshold to retain top percentile (i,j)R S(i, j) with from τ lL πHW ( ˆAl,t), wt = tstart + 1 Visual extraction Final inference Question-specific attention = arg maxRR following Eq. 4.6 General attention for all L, where tT wt (cid:80) We select the top-K regions ranked by cumulative attention scores and generate the enhanced image through Irefined = Φ(I, ), where Φ applies masking, cropping, and resizing, and controls the maximum number of regions to preserve. This refinement eliminates visual noise while magnifying task-relevant content, enabling focused attention on task-related areas. As shown in Figure 6, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), method that contrasts attention maps to distinguish semantic pixels from noise, preserving only task-relevant regions for enhanced model focus (detailed in Appendix B)."
        },
        {
            "title": "5 METHOD ANALYSIS",
            "content": "5.1 EXPERIMENTAL SETUP Datasets. We conduct our experiments on four datasets: A-OKVQA (Schwenk et al., 2022), POPE (Li et al., 2023b), (Wu & Xie, 2023), and TextVQA (Singh et al., 2019), which cover multiple task dimensions including visual reasoning, visual understanding, and visual knowledge reasoning. For TextVQA, we evaluate the models intrinsic visual text recognition capabilities by providing only images and questions without external OCR augmentation (detailed in Appendix E). Models. We conduct experiments on four VLMs: QWEN2.5-VL-3B-INSTRUCT, QWEN2.5-VL7B-INSTRUCT (Qwen, 2025), LLAVA-1.5-7B, and LLAVA-1.5-13B (Liu et al., 2023a). The 7 Preprint. Under review. Model QWEN2.5-VL-3B QWEN2.5-VL-7B LLAVA1.5-7B LLAVA1.5-13B Layer(s): w/o CARVE Single Layer Multi-Layers 14 20 25 [10, 15] [15, 20] [20, 25] w/o CARVE Single Layer Multi-Layers 14 20 25 [10, 15] [15, 20] [20, 25] w/o CARVE Single Layer Multi-Layers 14 20 25 [10, 15] [15, 20] [20, 25] w/o CARVE Single Layer Multi-Layers 14 20 [10, 15] [15, 20] [20, 25] A-OKVQA POPE 73.0() 74.3(1.78) 76.5(4.79) 76.7(5.07) 74.0(1.37) 76.8(5.21) 78.3(7.26) 75.0() 75.2(0.27) 76.9(2.53) 77.0(2.67) 75.0(0.00) 77.1(2.80) 78.0(4.00) 71.5() 71.7(0.28) 74.0(3.50) 74.1(3.64) 71.5(0.00) 74.2(3.78) 75.4(5.45) 75.7() 75.8(0.13) 76.2(0.66) 76.2(0.66) 75.7(0.00) 76.8(1.45) 76.9(1.59) 86.9() 87.1(0.23) 87.4(0.58) 87.5(0.69) 86.9(0.00) 87.7(0.92) 87.9(1.15) 87.0() 87.5(0.57) 87.9(1.03) 88.2(1.38) 87.0(0.00) 88.4(1.61) 88.6(1.84) 83.6() 85.1(1.79) 87.2(4.31) 87.1(4.19) 84.5(1.08) 87.5(4.67) 89.0(6.46) 84.6() 86.1(1.77) 88.2(4.26) 88.1(4.14) 85.0(0.47) 88.6(4.73) 90.1(6.50) 50.3() 53.9(7.16) 56.0(11.33) 56.0(11.33) 53.4(6.16) 56.0(11.33) 57.1(13.52) 50.8() 54.5(7.28) 56.5(11.22) 57.0(12.20) 51.3(0.98) 57.6(13.39) 58.1(14.37) 38.7() 63.4(63.82) 65.4(68.99) 65.4(68.99) 48.2(24.55) 65.4(68.99) 66.5(71.83) 42.4() 66.5(56.84) 68.6(61.79) 69.0(62.74) 52.9(24.76) 69.1(62.97) 70.0(65.09) TextVQA 72.8() 73.6(1.10) 74.7(2.61) 75.9(4.26) 73.0(0.27) 76.0(4.40) 76.3(4.81) 75.0() 75.2(0.27) 77.9(3.87) 78.4(4.53) 75.0(0.00) 79.5(6.00) 81.7(8.93) 47.8() 54.0(12.97) 56.1(17.36) 56.2(17.57) 49.2(2.93) 56.4(17.99) 58.2(21.76) 57.1() 58.2(1.93) 59.1(3.50) 59.2(3.68) 57.4(0.53) 59.4(4.03) 61.2(7.18) Table 2: We investigate CARVEs accuracy using both single-layer and multi-layer intervention strategies at shallow, middle, and deep model depths, where single-layer interventions use attention maps ˆAi from individual layers, while multi-layer interventions fuse maps across multiple layers to guide masking decisions. We employ Tfull as the time step configuration. Qwen family processes images at 448 448 resolution, while the LLaVA-1.5 family operates at 336 336 resolution. All models employ greedy decoding. 5.2 RESULTS CARVE Enhances VLMs Visual QA Performance. Tables 1 and 2 demonstrate CARVEs consistent performance enhancement across all evaluated models and datasets. Earlier-generation models exhibit substantially greater improvements than their more recent counterparts. For instance, LLAVA1.5-7B achieves 71.83% relative improvement on V, whereas QWEN2.5-VL-7B shows 17.52% gain. This pattern indicates that limited-capability models suffer more from visual complexity interference and benefit more from contrastive attention-guided focusing mechanisms. Ablation Study on the Time Step. Table 1 reveals consistent performance hierarchy across various time step selection strategies. Specifically, tend generally outperforms Tfull, which in turn surpasses tstart across most experimental configurations. This pattern is exemplified by QWEN2.5VL-7Bs performance on TextVQA, where tend achieves 81.9% accuracy, followed by Tfull at 81.7% and tstart at 80.7%. This phenomenon aligns with architectural principles. Later tokens encode richer contextual information by accessing complete preceding sequences during inference. Consequently, the final tokens attention maps accurately localize target objects, providing prerequisite conditions for CARVEs noise masking mechanism. Ablation Study on the Layer Selection. To investigate layer selection effects on attention pattern extraction, we conduct systematic experiments as shown in Table 2. Across all tested model architectures, the layer-wise performance demonstrates the following general ordering: [20, 25] [15, 20] single layer 25 single layer 20 single layer 14 [10, 15]. This pattern is exemplified by LLAVA1.5-7Bs performance on TextVQA, where the multi-layer [20,25] achieves 21.76% improvement, the [15,20] reaches 17.99% improvement, while the early-layer [10,15] 8 Preprint. Under review. (a) Accuracy TextVQA for QWEN2.5-VL-3B on (b) Accuracy TextVQA for QWEN2.5-VL-7B on Figure 7: Impact of mask generation hyperparameters on TextVQA accuracy for QWEN family. Results show performance across varying top-p threshold and maximum keep regions K. Method Original SAM YOLO CLIP ViCrop rel-att grad-att pure-grad CARVE Accuracy GPU Time 47.80 0.17 49.42 3. 48.84 0.35 48.55 1.07 55.17 1.16 56.06 0.89 51.67 2.36 58.2 1. Table 3: Performance comparison of CARVE against external tool-based approaches and ViCrop on TextVQA: accuracy (%) and inference time overhead per sample (seconds). attains only 2.93% improvement. Multi-layer fusion outperforms single-layer alternatives by capturing complementary information and providing robustness against individual layer randomness. This phenomenon aligns with our findings in Figure 5(b): early layers perform global scanning with high entropy, while middle-to-deep layers focus on task-relevant patterns. Sensitivity Analysis of Mask Generation. We examine 1,000-instance subset randomly sampled from TextVQA. As shown in Figure 7, when = 1.0, corresponding to no masking intervention, performance remains at original levels. However, when is set within [0.2, 0.6] combined with {2, 3}, the model achieves optimal performance, as these settings maintain balance between preserving object representations and suppressing visual noise. In contrast, aggressive masking strategies manifest detrimental effects: retention ratios set to 20% and single-region constraints lead to degradation, since such aggressive configurations discard essential visual information. Comparative Analysis with Alternative Methods. As shown in Table 3, CARVE substantially outperforms external tool-based approaches: SAM (Kirillov et al., 2023), YOLO (Redmon et al., 2016), CLIP (Radford et al., 2021) and recent ViCrop (Zhang et al., 2025) variants across diverse baseline methodologies (conducted on NVIDIA RTX A6000). External tools rely on generic segmentation algorithms that lack question-image context awareness. While ViCrop effectively reduces visual noise through strategic cropping, it lacks pixel-level noise masking. Regarding computational efficiency, CARVE requires 1.34 seconds of GPU processing time, exceeding simpler approaches such as YOLO (0.35 seconds) but remaining within practical deployment constraints."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we demonstrate that visual complexity correlates with attention entropy, which in turn negatively impacts VLMs performance. Theoretically, we prove that contrasting attention maps between general and specific instructions enables effective decomposition of visual signal into semantic signal and visual noise components. To this end, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), training-free method that leverages this theoretical insight to extract task-relevant signal through attention contrasting and pixel-level masking. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning without training. 9 Preprint. Under review."
        },
        {
            "title": "REFERENCES",
            "content": "Shantanu Acharya, Fei Jia, and Boris Ginsburg. Star attention: Efficient llm inference over long sequences. arXiv preprint arXiv:2411.17116, 2024. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736, 2022. Baolong Bi, Shaohan Huang, Yiwei Wang, Tianchi Yang, Zihan Zhang, Haizhen Huang, Lingrui Mei, Junfeng Fang, Zehao Li, Furu Wei, et al. Context-dpo: Aligning language models for context-faithfulness. arXiv preprint arXiv:2412.15280, 2024. Baolong Bi, Shenghua Liu, Lingrui Mei, Yiwei Wang, Junfeng Fang, Pengliang Ji, and Xueqi Cheng. Decoding by contrasting knowledge: Enhancing large language model confidence on edited facts. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1719817208, Vienna, Austria, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.841. URL https://aclanthology.org/2025.acl-long.841/. Baolong Bi, Shenghua Liu, Xingzhang Ren, Dayiheng Liu, Junyang Lin, Yiwei Wang, Lingrui Mei, Junfeng Fang, Jiafeng Guo, and Xueqi Cheng. Refinex: Learning to refine pre-training data at scale from expert-guided programs. arXiv preprint arXiv:2507.03253, 2025b. John Canny. computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, 8(6):679698, 1986. Lizhe Chen, Yan Hu, Yu Zhang, Yuyao Ge, Haoyu Zhang, and Xingquan Cai. Frequency-importance gaussian splatting for real-time lightweight radiance field rendering. Multimedia Tools and Applications, 83(35):8337783401, 2024a. Lizhe Chen, Binjia Zhou, Yuyao Ge, Jiayi Chen, and Shiguang Ni. Pis: Linking importance sampling and attention mechanisms for efficient prompt compression. arXiv preprint arXiv:2504.16574, 2025. Qizhou Chen, Taolin Zhang, Xiaofeng He, Dongyang Li, Chengyu Wang, Longtao Huang, and Hui Xue. Lifelong knowledge editing for LLMs with retrieval-augmented continuous prompt learning. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1356513580, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi: 10.18653/ v1/2024.emnlp-main.751. URL https://aclanthology.org/2024.emnlp-main. 751/. Zenghao Duan, Wenbin Duan, Zhiyi Yin, Yinghan Shen, Shaoling Jing, Jie Zhang, Huawei Shen, and Xueqi Cheng. Related knowledge perturbation matters: Rethinking multiple pieces of knowledge editing in same-subject. arXiv preprint arXiv:2502.06868, 2025. Honghao Fu, Yufei Wang, Wenhan Yang, Alex Kot, and Bihan Wen. Dp-iqa: Utilizing diffusion prior for blind image quality assessment in the wild. arXiv preprint arXiv:2405.19996, 2024. Honghao Fu, Junlong Ren, Qi Chai, Deheng Ye, Yujun Cai, and Hao Wang. Vistawise: BuildarXiv preprint ing cost-effective agent with cross-modal knowledge graph for minecraft. arXiv:2508.18722, 2025. Haonan Ge, Yiwei Wang, Ming-Hsuan Yang, and Yujun Cai. Mrfd: Multi-region fusion decoding with self-consistency for mitigating hallucinations in lvlms, 2025a. URL https://arxiv. org/abs/2508.10264. Yuyao Ge, Zhongguo Yang, Lizhe Chen, Yiming Wang, and Chengyang Li. Attack based on data: novel perspective to attack sensitive points directly. Cybersecurity, 6(1):43, 2023. 10 Preprint. Under review. Yuyao Ge, Shenghua Liu, Baolong Bi, Yiwei Wang, Lingrui Mei, Wenjie Feng, Lizhe Chen, and Xueqi Cheng. Can graph descriptive order affect solving graph problems with llms? ACL 2025, pp. 64046420, 2025b. Yuyao Ge, Shenghua Liu, Yiwei Wang, Lingrui Mei, Lizhe Chen, Baolong Bi, and Xueqi Cheng. Innate reasoning is not enough: In-context learning enhances reasoning large language models with less overthinking. arXiv preprint arXiv:2503.19602, 2025c. Yan Hu, Lizhe Chen, Hanna Xie, Yuyao Ge, Shun Zhou, and Xingquan Cai. Real-time nonphotorealistic rendering method for black and white comic style in games and animation. Journal of System Simulation, 36(7):16991712, 2024. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pp. 49044916. PMLR, 2021. Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. Hallucination augmented contrastive learning for multimodal large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2703627046, 2024. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Jongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, and SeYoung Yun. Distillm-2: contrastive approach boosts the distillation of llms. arXiv preprint arXiv:2503.07067, 2025. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. Tianhao Li, Jingyu Lu, Chuangxin Chu, Tianyu Zeng, Yujia Zheng, Mei Li, Haotian Huang, Bin Wu, Zuoxian Liu, Kai Ma, et al. Scisafeeval: comprehensive benchmark for safety alignment of large language models in scientific tasks. AAAI 2025 AI for Cybersecurity, 2024. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023b. Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, and Yiwei Wang. Texture or semantics? vision-language models get lost in font recognition. In Conference on Language Modeling COLM, 2025., 2025. Chang Liu, Hongkai Chen, Yujun Cai, Hang Wu, Qingwen Ye, Ming-Hsuan Yang, and Yiwei Wang. Structured attention matters to multimodal llms in document understanding. arXiv preprint arXiv:2506.21600, 2025a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2629626306, June 2024. Kai Liu, Zhan Su, Peijie Dong, Fengran Mo, Jianfei Gao, ShaoTing Zhang, and Kai Chen. Smooth reading: Bridging the gap of recurrent llm to self-attention llm on long-context tasks. arXiv preprint arXiv:2507.19353, 2025b. Yixin Liu, Kejian Shi, Katherine He, Longtian Ye, Alexander Fabbri, Pengfei Liu, Dragomir Radev, and Arman Cohan. On learning to summarize with large language models as references. arXiv preprint arXiv:2305.14239, 2023b. 11 Preprint. Under review. Feipeng Ma, Yizhou Zhou, Zheyu Zhang, Shilin Yan, Hebei Li, Zilong He, Siying Wu, Fengyun Rao, Yueyi Zhang, and Xiaoyan Sun. Ee-mllm: data-efficient and compute-efficient multimodal large language model. arXiv preprint arXiv:2408.11795, 2024a. Weizhi Ma, Yujia Zheng, Tianhao Li, Zhengping Li, Ying Li, and Lijun Wang. comprehensive review of deep learning in eeg-based emotion recognition: classifications, trends, and practical implications. PeerJ Computer Science, 10:e2065, 2024b. Weizhi Ma, Ying Li, Tianhao Li, Haowei Yang, Zhengping Li, Lijun Wang, and Junyu Xuan. Sfswts: spatial-frequency shifted windows and time self-attention network for eeg emotion recognition. Neurocomputing, pp. 130309, 2025. Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng. Slang: New concept comprehension of large language models. arXiv preprint arXiv:2401.12585, 2024a. Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Jiayi Mao, and Xueqi Cheng. not aligned is not malicious: Being careful about hallucinations of large language models jailbreak. arXiv preprint arXiv:2406.11668, 2024b. Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, and Xueqi Cheng. Hiddenguard: Fine-grained safe generation with specialized representation router, 2024c. URL https://arxiv.org/abs/2410.02684. Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Yuyao Ge, Jun Wan, Yurong Wu, and Xueqi Cheng. a1: Steep test-time scaling law via environment augmented generation. arXiv preprint arXiv:2504.14597, 2025a. Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, et al. survey of context engineering for large language models. arXiv preprint arXiv:2507.13334, 2025b. Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. When do llms need retrieval augmentation? mitigating llms overconfidence helps retrieval augmentation. arXiv preprint arXiv:2402.11457, 2024a. Shiyu Ni, Keping Bi, Lulu Yu, and Jiafeng Guo. Are large language models more honest in their probabilistic or verbalized confidence? In China Conference on Information Retrieval, pp. 124 135. Springer, 2024b. Kaihang Pan, Zhaoyu Fan, Juncheng Li, Qifan Yu, Hao Fei, Siliang Tang, Richang Hong, Hanwang Zhang, and Qianru Sun. Towards unified multimodal editing with enhanced knowledge collaboration. Advances in Neural Information Processing Systems, 37:110290110314, 2024. Qwen. Qwen2.5-vl: powerful vision-language model for seamless computer interaction. arXiv preprint arXiv:2409.12191, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779788, 2016. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. In Computer A-okvqa: benchmark for visual question answering using world knowledge. VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part VIII, pp. 146162. Springer, 2022. Claude Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. Preprint. Under review. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. URL https://arxiv.org/abs/2507.01006. Anne Treisman and Garry Gelade. feature-integration theory of attention. Cognitive psychology, 12(1):97136, 1980. Yuanyuan Wei, Xianxian Liu, Yao Mu, Changran Xu, Guoxun Zhang, Tianhao Li, Zida Li, Wu Yuan, Ho-Pui Ho, and Mingkun Xu. From droplets to diagnosis: Ai-driven imaging and system integration in digital nucleic acid amplification testing. Biosensors and Bioelectronics, pp. 117741, 2025. Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. arXiv preprint arXiv:2312.14135, 2023. Qian Xiong, Yuekai Huang, Ziyou Jiang, Zhiyuan Chang, Yujia Zheng, Tianhao Li, and Mingyang Li. Butterfly effects in toolchains: comprehensive analysis of failed parameter filling in llm tool-agent systems. arXiv preprint arXiv:2507.15296, 2025. Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, and Xueqi Cheng. Who is in the spotlight: The hidden bias undermining multimodal retrievalaugmented generation. arXiv preprint arXiv:2506.11063, 2025a. Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, and Xueqi Cheng. Who is in the spotlight: The hidden bias undermining multimodal retrievalaugmented generation, 2025b. URL https://arxiv.org/abs/2506.11063. Songlin Zhai, Yuan Meng, Yuxin Zhang, and Guilin Qi. Parameter-aware contrastive knowledge editing: Tracing and rectifying based on critical transmission paths. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. ISBN 2818928200, Vienna, Austria, July 2025. Association for Computational Linguistics. 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1367. URL https://aclanthology. org/2025.acl-long.1367/. Guangzi Zhang, Lizhe Chen, Yu Zhang, Yan Liu, Yuyao Ge, and Xingquan Cai. Translating words to worlds: zero-shot synthesis of 3d terrain from textual descriptions using large language models. Applied Sciences, 14(8):3257, 2024a. Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. arXiv preprint arXiv:2502.17422, 2025. Shaolei Zhang, Tian Yu, and Yang Feng. Truthx: Alleviating hallucinations by editing large language models in truthful space. arXiv preprint arXiv:2402.17811, 2024b. Preprint. Under review. Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, et al. Are all prompt components value-neutral? understanding the heterogeneous adversarial robustness of dissected prompt in large language models. arXiv preprint arXiv:2508.01554, 2025. Rongxin Zhu, Jey Han Lau, and Jianzhong Qi. Factual dialogue summarization via learning from large language models. arXiv preprint arXiv:2406.14709, 2024. 14 Preprint. Under review."
        },
        {
            "title": "A DEFINITION AND EXPLANATION",
            "content": "A.1 DEFINITION l,t RNv l,t RNv Symbol RHW 3 H, A(Q) A(G) Nv tend H() Tc(I) Cc(I) E(I) {0, 1}HW ΨRGBHSV ζij ρb Fvis(I) RNv + Fsem(Q, I) RNv + 1Nv ˆA RNv + λ > 0 (0, 1] wt RHW Qp() τ {1..H} {1..W } Rk Φ(I, ) πHW Ξ Ltotal Nq A.2 EXPLANATION Definition Description Input image Image dimensions Task-specific question General instruction Question attention map General attention map Visual tokens Layer range Time range Final step Shannon entropy Overall entropy Texture complexity Color complexity Edge map Color transform Hue value Hue proportion Hue bins Visual noise factor Semantic signal factor Uniform vector Estimated attention Regularization Top-p percentile Max regions Temporal weights Fused map Percentile function Threshold Final mask Connected region Visual extraction Spatial reshape Attention extractor VLM Total layers Text tokens Image with height and width Height and width in pixels Task-specific question General instruction Attention map at layer l, step General question attention map Number of visual tokens Layer indices Generation time step Final generation step Attention distribution entropy Layer-averaged attention entropy Edge density from Canny detection Hue diversity measure Binary edge map from Canny RGB to HSV transformation operator Hue value at pixel (i, j) Fraction of pixels in hue bin Number of hue bins Image-inherent visual noise component Task-related semantic signal component Vector of ones Estimated semantic attention Regularization parameter Percentile threshold for masking Maximum regions to preserve Later token weighting with wt = tstart + 1 Spatially reshaped attention map Top-p percentile operator Computed threshold value Union of top-K regions Connected component from thresholding Masking, cropping and resizing Token to image projection Function to extract attention maps Vision-language model Number of model layers Number of query tokens Time Step (t): In the autoregressive generation process of vision-language models, time step denotes the sequential position index in the output token sequence. The model generates responses token-by-token, where = 1 corresponds to the first generated token and = tend represents the final token. At each time step, the model produces an attention distribution A(Q) l,t RNv over visual tokens. Visual Tokens (Nv): Visual tokens constitute the discrete representational units obtained after processing an input image through visual encoder. An image of dimensions is partitioned and encoded into Nv visual tokens, which form the fundamental units for visual information processing. The attention mechanism allocates weights across these Nv tokens to determine which image regions to attend to. Semantic Signal Factor (Fsem(Q, I)): The semantic signal factor represents the question- + . This factor specific component in the attention decomposition framework, valued in RNv 15 Preprint. Under review. quantifies the semantic signal between each visual token and the given question Q. Under general instructions (e.g., describe this image), this factor approximates uniform distribution (Fsem(G, I) 1Nv ), whereas task-specific questions yield elevated values in semantically relevant regions. Visual Noise Factor (Fvis(I)): The visual noise factor captures the image-inherent, question-independent attention component, valued in RNv + . This factor, determined by texture complexity and color diversity of the image, reflects the influence of visual content characteristics on attention distribution. Under general instructions, the attention distribution is predominantly governed by this factor: A(G) l,t (I) Fvis(I)."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "We conduct our experiments on server with 4 NVIDIA RTX A6000 GPUs. In practical implementation, CARVE requires three inference passes; however, the first two passes (extracting general instruction and task-specific question attention maps) can be terminated early. Specifically, when we require attention maps only from layers = [Lstart, Lend], the first two inference processes can halt upon completing layer Lend computation, eliminating the need for full Ltotal layer forward propagation. The third inference must run to completion to generate the final answer."
        },
        {
            "title": "C PROOFS AND ADDITIONAL THEOREMS",
            "content": "C.1 MATHEMATICAL BASIS OF ATTENTION DECOMPOSITION Theorem B.1 (Existence of Attention Decomposition): For any attention distribution A(Q) RNv + , there exists unique decomposition: l,t (I) A(Q) l,t (I) = Fvis(I) Fsem(Q, I) (C.1) Proof: Define logarithmic space mapping ϕ : R+ where ϕ(x) = log(x). Under this transformation, the decomposition becomes additive in logarithmic space: ϕ(A(Q) l,t (I)) = ϕ(Fvis(I)) + ϕ(Fsem(Q, I)) (C.2) Given the boundary condition that Fsem(G, I) = 1Nv when = (general instruction), we obtain: ϕ(Fvis(I)) = ϕ(A(G) l,t (I)) Consequently, through substitution: ϕ(Fsem(Q, I)) = ϕ(A(Q) l,t (I)) ϕ(A(G) l,t (I)) The unique solution is obtained via the inverse mapping ϕ1(x) = exp(x). C.2 CONVEXITY ANALYSIS OF THE OPTIMIZATION PROBLEM Theorem B.2 (Strict Convexity of Objective Function): The optimization objective Nv(cid:88) ( A) = i=1 is strictly convex with respect to A. (cid:16) Ai A(G) A(Q) (cid:17)2 + λ Nv(cid:88) i=1 A2 A(G) (C.3) (C.4) (C.5) Proof: Computing the Hessian matrix reveals its structure. Since the objective function is separable across components Ai, the Hessian is diagonal with elements: 2J A2 = 2A(G) )2 + 2λA(G) = 2(A(G) + λ) (A(G) Hii = (C.6) Preprint. Under review. Given that A(G) > 0 and λ > 0, all diagonal elements are positive, thus 0 (positive definite). According to convex optimization theory, twice continuously differentiable function with positive definite Hessian everywhere is strictly convex. C.3 DERIVATION AND UNIQUENESS OF CLOSED-FORM SOLUTION Theorem B.3 (Closed-form Expression of Optimal Solution): The optimization problem admits unique global optimum: ˆAi = A(Q) A(G) + λ Proof: Applying first-order optimality conditions (KKT conditions): = 2( Ai A(G) ) A(G) A(Q) + 2λ Ai A(G) Ai = 0 Rearranging terms yields: Ai A(G) Solving for Ai: (A(G) + λ) = A(Q) A(G) Ai = A(Q) A(G) + λ (C.7) (C.8) (C.9) (C.10) By Theorem B.2s strict convexity, this solution represents the unique global optimum. C.4 ERROR BOUNDS AND CONVERGENCE ANALYSIS Theorem B.4 (Approximation Error Bound): Let Fsem(G, I) = 1Nv + ϵ where ϵ δ. Then the estimation error satisfies: ˆA Fsem(Q, I) δ Fsem(Q, I) 1 δ Proof: Under perturbation A(G) = Fvis,i (1 + ϵi), the estimate becomes: ˆAi = Fvis,i Fsem,i(Q, I) Fvis,i (1 + ϵi) + λ Fsem,i(Q, I) 1 + ϵi when Fvis,i λ Using the Taylor expansion ˆAi = Fsem,i(Q, I) (cid:80) k=0(ϵi)k and truncating to first order yields: ˆAi Fsem,i(Q, I) Fsem,i(Q, I) ϵi 1 ϵi Taking the infinity norm completes the proof. C.5 THEORETICAL SELECTION OF REGULARIZATION PARAMETER Proposition B.5 (Optimal Regularization Parameter): In the mean squared error sense, the optimal regularization parameter satisfies: λ = arg min λ (cid:104) ˆA(λ) Fsem(Q, I)2 2 (cid:105) Analysis: Defining the bias-variance decomposition: MSE(λ) = Bias2(λ) + Variance(λ) (C.11) (C.12) where the bias term Bias(λ) = E[ ˆA(λ)] Fsem(Q, I) and the variance term Variance(λ) = E[( ˆA(λ) E[ ˆA(λ)])2]. As λ 0, variance increases due to numerical instability while bias decreases. Conversely, as λ , variance decreases but bias increases due to over-smoothing. The optimal λ achieves the bias-variance trade-off point. 17 Preprint. Under review. C.6 HIERARCHICAL EVOLUTION OF ATTENTION ENTROPY Theorem B.6 (Monotonicity of Entropy): For layer sequence l1 < l2 < ... < ln, attention entropy satisfies: H(A(Q) l1,t ) H(A(Q) l2,t ) ... H(A(Q) ln,t) (C.13) Proof sketch: Applying the Data Processing Inequality, we treat each layer as an information processing channel. Since deeper networks progressively extract high-level features and focus on taskrelevant regions, information entropy decreases monotonically. This aligns with the principle of maximum entropy: systems tend toward maximum entropy states under constraints, where deeper layers impose stronger task constraints. C.7 COMPUTATIONAL OPTIMIZATION POTENTIAL OF CARVE This section analyzes the computational optimization potential of the CARVE algorithm. While CARVE requires three inference passes, its structural properties enable significant optimization opportunities. The key observation is that the first two inference passes (general instruction and task-specific question) only require extracting attention maps from intermediate layers, without completing full forward propagation or generating complete responses. This characteristic enables early termination strategies. Furthermore, the general attention maps A(G) depend solely on the input image and are independent of specific questions, creating opportunities for caching and reuse. Let the forward propagation : RNv RNv at layer have computational cost cl = Θ(N 2 baseline complexity without optimization is: ). The Cbaseline = 3Ltotal Θ(N 2 ) + Θ(L Nv) Early Termination Strategy. Since only attention maps from layers = [Lstart, Lend] are required, the first two inference passes can terminate after layer Lend: Cearly = (2Lend + Ltotal) Θ(N 2 ) + Θ(L Nv) The relative computational savings rate is: η1 = Cbaseline Cearly Cbaseline = 2(Ltotal Lend) 3Ltotal = 2(1 α) 3 where α = Lend/Ltotal. For practical configurations with = [20, 25] and Ltotal = 28, we have α = 25/28 0.89, yielding theoretical savings of η1 7.3%. Attention Caching Mechanism. The general attention maps A(G) depend only on the image and can be reused across multiple questions. Define cache mapping : {A(G) }lL. For different questions {Q1, ..., Qn} on the same image, the total computational cost is: Ccached(n) = Lend Θ(N 2 ) + (Lend + Ltotal) Θ(N 2 ) compared to 3n Ltotal Θ(N ) for the baseline approach. The average cost per question becomes: Ccached = Lend Θ(N 2 ) + (Lend + Ltotal) Θ(N 2 ) As , the average cost approaches (Lend + Ltotal) Θ(N 2 baseline: ), yielding speedup ratio relative to Scache = 3Ltotal Lend + Ltotal = 3 1 + α For α = 0.89, this gives Scache 1.59, representing approximately 37% computational savings. 18 Preprint. Under review. Combined Optimization Analysis. When processing batches containing repeated images, combining both strategies yields: Ccombined = (1 ρ)Lend Θ(N 2 ) + (Lend + Ltotal) Θ(N 2 ) where ρ [0, 1] denotes the cache hit rate. The relative speedup becomes: Scombined = 3Ltotal (2 ρ)Lend + Ltotal = 3 (2 ρ)α + 1 Under practical scenarios with α = 0.89 and ρ = 0.3, we obtain Scombined 1.24, corresponding to approximately 19% computational savings. The space complexity remains S(CARVE) = Θ(L Nv). For typical configurations (L = 5, = 10, Nv = 1024), this requires approximately 200KB of additional memory, which is negligible on modern hardware."
        },
        {
            "title": "D PROMPT DESIGN",
            "content": "General Instruction Accuracy (%) Std Dev (%) Relative Gain (%) w/o CARVE Write general description of the image. Describe this image in detail. Provide comprehensive overview of the image. What do you see in this image? Explain what appears in the image. 72.4 77.2 75.8 75.2 74.9 74.8 0.8 0.6 0.9 1.4 1.2 1. +6.63 +4.70 +3.87 +3.45 +3.31 Table 4: Comparison across general instructions. To identify the optimal general instruction for inducing uniform attention distributions, we conducted experiments on randomly sampled subset of 1000 instances from the TextVQA dataset using the QWEN2.5-VL-3B. Our objective was to identify prompts that encourage global image scanning without focusing on specific semantic regions. To assess stability, we performed ten independent trials and computed standard deviations across runs. To avoid discrepancies arising from layer and time step variations, we conduct experiments using Tfull and = [20, 25] as hyperparameters. As shown in Table 4, Write general description of the image achieves both the highest accuracy (77.2%) and the lowest standard deviation (0.6%), indicating superior stability. Meanwhile, What do you see in this image? and Explain what appears in the image. are excluded due to their poor stability. Beyond considering accuracy and stability, we also need to consider the number of tokens generated by the VLM. Specifically, Describe this image in detail. and Provide comprehensive overview of the image. are excluded because they output significantly more tokens than Write general description of the image.. Based on the above considerations, we ultimately adopt Write general description of the image. as the general instruction for CARVE."
        },
        {
            "title": "E DATASETS",
            "content": "For A-OKVQA (Schwenk et al., 2022), we utilize the validation split containing 1,145 questions across 1,122 images that require integrating visual perception with commonsense reasoning, evaluated using VQA-score accuracy. For POPE (Li et al., 2023b), we employ 500 distinct images paired with 9,000 binary questions systematically designed to detect hallucination phenomena through polling-based object probing. For (Wu & Xie, 2023), we evaluate on 191 image-question pairs that demand fine-grained visual reasoning capabilities. For TextVQA (Singh et al., 2019), we test on 3,166 images with 5,000 questions focusing on text comprehension abilities. For TextVQA evaluation, we adopt the protocol established by Zhang et al. (2025), deliberately excluding OCR-extracted tokens from model inputs. We treat TextVQA identically to other visual reasoning benchmarks, providing only the image and question without auxiliary text annotations. While this configuration yields marginally reduced accuracy compared to OCR-augmented baselines in original implementations, it enables unbiased assessment of models intrinsic visual text 19 Preprint. Under review. recognition capabilities, eliminating confounding factors from external OCR systems. This evaluation strategy ensures that performance metrics genuinely reflect the visual perception and text understanding abilities inherent to the vision-language models."
        },
        {
            "title": "F VISUALIZATIONS",
            "content": "This section presents visual analysis of masked images generated by CARVE across different threshold values τ from 1.0 (no masking) to 0.1 (aggressive masking). Figures 8 and 9 show two representative TextVQA samples where visual complexity initially causes incorrect predictions. Figure 8 shows street scene where the model fails to detect the Bridgestone sign at τ = 1.0. Progressive masking removes background buildings and vehicles, enabling correct recognition at τ = 0.3. In Figure 9, multiple decorative mugs cause shape misidentification through the cup handle. At τ = 0.2, only the relevant mug remains, yielding the correct star answer. Across both samples, optimal performance occurs within τ [0.2, 0.4], where contrastive attention effectively preserves semantic signal while eliminating visual distractors. (a) τ = 1.0 (b) τ = 0.9 (c) τ = 0.8 (d) τ = 0. (e) τ = 0.6 (f) τ = 0.5 (g) τ = 0.4 (h) τ = 0.3 (i) τ = 0.2 (j) τ = 0. Figure 8: Images masked with CARVE. The caption of each subfigure shows the computed threshold value τ . (a) τ = 1.0 (b) τ = 0.9 (c) τ = 0.8 (d) τ = 0.7 (e) τ = 0. (f) τ = 0.5 (g) τ = 0.4 (h) τ = 0.3 (i) τ = 0.2 (j) τ = 0.1 Figure 9: Images masked with CARVE. The caption of each subfigure shows the computed threshold value τ ."
        }
    ],
    "affiliations": [
        "Institute of Computing Technology, Chinese Academy of Sciences",
        "University of California, Merced"
    ]
}