{
    "paper_title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence",
    "authors": [
        "Xuming He",
        "Zehao Fan",
        "Hengjia Li",
        "Fan Zhuo",
        "Hankun Xu",
        "Senlin Cheng",
        "Di Weng",
        "Haifeng Liu",
        "Can Ye",
        "Boxi Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 2 2 6 2 0 . 2 1 5 2 : r RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence Xuming He1* Zehao Fan1* Hengjia Li1* Fan Zhuo1 Hankun Xu1 Senlin Cheng2 Di Weng1 Haifeng Liu1 Can Ye2 Boxi Wu1 1Zhejiang University 2Ant Group https://hexmseeu.github.io/RULER-Bench-proj/ Figure 1. Overview of RULER-Bench. We propose RULER-Bench, comprehensive benchmark designed to evaluate the rule-based reasoning abilities of video generation models. Left: Grounded in three fundamental domains, we formulate rule-based reasoning ability into six categories: Science, Vision, Hypothesis, Game, Semantics, and Humanity. These categories are further subdivided into 40 tasks. Center: Using the collected samples, we evaluate 10 video models based on the corresponding checklist across four metrics. Each checklist question is scored by GPT-o3 with discrete labels. To validate the reliability of the evaluator, we conduct human alignment study, in which GPT-o3 achieves 85% agreement with human judgments. Right: Extensive experiments demonstrate that Veo3.1 achieves the best performance. However, all models exhibit limited reasoning ability across different rule categories."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack fine-grained decomposition of reasoning capabilities and comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, benchmark designed to evaluate the reasoning ability of *equal contributions. project lead. video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence. 1. Introduction Video generation focuses on producing video clips that exhibit strong temporal consistency and high visual quality, 1 serving as fundamental technique for downstream applications such as customization [43, 44, 75, 85] and world modeling [1, 17, 74]. Empowered by advances in generative frameworks such as diffusion models [8, 23, 32, 59, 61, 88] and autoregressive [40, 66, 67] approaches, recent video generation systems have achieved remarkable progress on perceptual and understanding abilities, such as aesthetic quality and instruction adherence. However, with the rapid scaling of high-quality datasets and model parameters, state-of-the-art models such as Sora2 [58], Veo3 [26], and Wan2.5 [2] have nearly saturated on these dimensions. Mirroring the evolution of foundation language models [5, 7, 25, 27, 68, 83] in natural language processing, video models are expected to gradually advance from perception and understanding to reasoning, thereby paving the way for ultimately becoming the foundation model for vision. Fig. 1 illustrates the current stage of this evolution. Given the instruction drop piece of sodium into the phenolphthalein solution and an input image, all four generation models produce visually coherent video clips. However, their reasoning abilities differ significantly. For instance, Veo3.1 [26] accurately infers that sodium reacts violently with phenolphthalein solution, releasing gas and forming an alkaline product. Therefore, the generated video depicts vigorous bubbling and the distinctive red coloration. In contrast, Seedance1.0-pro [24], Wan2.5, and Sora2 exhibit limited reasoning ability, capturing only subset of the expected reaction phenomena. This capability gap reveals fundamental challenge: How to assess whether nextlevel video generation models possess the reasoning abilities necessary to achieve vision foundation intelligence? Recently, Wiedemer et al. [76] have conducted an initial exploration into whether Veo3 could serve as vision foundation model. They constructed instances spanning four aspects: perception, modeling, manipulation, and reasoning, and then measured Veo3s success rates on each dimension. However, their exploration lacks fine-grained decomposition of reasoning characteristics and systematic evaluation framework. Additionally, VBench-2.0 [94] and PhyGenbench [53] target intrinsic faithfulness and physical laws, but remain limited to physics and commonsense reasoning, overlooking the diversity of reasoning dimensions. To bridge this gap, we conceptualize reasoning as cognitive rule-based prediction, the ability of generative model to infer rules from inputs and apply them to predict plausible phenomena in videos. Building on this formulation, we organize reasoning scenarios into three fundamental domains: Nature, Society, and Virtuality, which correspond to real-world, human-centered, and virtual environments, respectively. Within these domains, we further define six categories of cognitive rules: Vision, Science, Semantics, Hypothesis, Game, and Humanity, which collectively span diverse reasoning scenarios in video generation. Based on these categories, we propose RULER-Bench (RULE-based Reasoning Benchmark for video generation), comprehensive benchmark designed to evaluate the reasoning capabilities of next-level video generation text-to-video models in two typical generation scenarios: and image-to-video. RULER-Bench adopts hierarchical paradigm, organizing 40 tasks within six rule categories. To ensure evaluation reliability, RULER-Bench comprises 622 high-quality instances, evenly distributed across tasks. Furthermore, large language leveraging multimodal models (MLLMs), we introduce comprehensive evaluation protocol that assesses generated videos across four metrics: Instruction Following, Visual Consistency, Visual Fidelity, and Rule Coherence. Unlike subjective continuous scoring, we construct checklist of objective questions for each instance based on these four metrics. Each question is then rated on discrete three-point scale: good, medium, or bad. To validate the reliability of our evaluation protocol, we manually annotate 813 checklist questions based on generated videos and verify the consistency between MLLM responses and human judgments. As shown in Fig. 1, our evaluation protocol achieves alignment with human annotation on 85% of the checklist questions, which confirms its effectiveness. Building upon RULER-Bench, we further conduct systematic evaluation of 10 state-of-the-art video generation models. Extensive experiments show that all models consistently exhibit limitations in rule coherence, demonstrating substantial potential for enhancing the reasoning capability of next-level video models. The main contributions of this work are: We conceptualize reasoning in video generation as cognitive rule-based prediction and formulate the first taxonomy that organizes reasoning into six rule categories. We introduce RULER-Bench, comprehensive benchmark specifically designed to evaluate the reasoning abilities of video generation models, and systematically covering 40 tasks and 622 high-quality instances. We conduct extensive experiments on 10 state-of-the-art video generation models, revealing substantial limitations across all models across different rule types. 2. Related Work Video Generation Models. Recent advancements in diffusion models [18, 3234, 55, 62, 63, 88, 91, 92] and autoregressive approaches [10, 35, 40, 46, 67, 80] have led to rapid progress in video generation [8, 28, 30, 31, 39, 47, 48, 52, 73, 77]. By leveraging large-scale, high-quality training data and expanding model capacity, recent systems such as Sora2, Veo3, and Wan have achieved remarkable performance in the dimensions of perception and understanding, including visual consistency, aesthetic quality, and instruction adherence. As visual fidelity continues to improve, research attention has begun to shift focus towards exploring reasoning capabilities [1, 12, 22, 49, 54, 72, 81, 82, 89, 90], such as understanding physical laws and performing logical inference. NewtonGen [86] introduces Neural Newtonian Dynamics to model and predict Newtonian motions, while V-Chain [37] incorporates chain-of-visual-thought mechanism to inject visual reasoning signals into generative processes. However, existing benchmarks lack systematic framework to assess the reasoning abilities of video models. To address this challenge, RULER-Bench provides comprehensive benchmark for evaluating the emerging capabilities of video generation models for rule-based reasoning. Benchmarks for Video Generation Methods. To effectively evaluate the capabilities of video generation models, variety of benchmarks have been proposed. VBench [36], EvalCrafter [51], and FetV [50] primarily focus on Textto-Video (T2V) tasks, assessing fundamental technical attributes. In contrast, T2V-CompBench [64], PhyGenbench [53], and VBench-2.0 [94] focus on deeper principles such as compositionality, physical law, and intrinsic faithfulness. For Image-to-Video (I2V) tasks, benchmarks such as AIGC-Bench [19] and AnimateBench [19] evaluate the video generation models along perceptual dimensions, including instruction following and visual consistency, while UI2V-Bench [87] evaluates models from general understanding perspective. Regarding Video-to-Video (V2V) generation, VE-Bench [65] and EditBoard [13] cover general editing scenarios, including subject, style, and attribute editing, whereas IVE-Bench [14] integrates traditional metrics with large language model-based assessments across multiple editing categories. However, as shown in Tab. 1, these benchmarks are typically constrained to single task type and primarily concentrate on fundamental abilities or specific emerging capabilities. Systematic evaluation of reasoning dimensions remains largely unexplored, with insufficient evaluation data [29] and lack of formal rulebased frameworks [76]. RULER-Bench unifies two task paradigms: T2V and I2V, and provides comprehensive evaluation of rule-based reasoning in video models, addressing limitations in existing evaluation practices and advancing the field toward vision foundation models. 3. RULER-Bench 3.1. Rule-Based Task Formulation Inspired by KRIS-Bench [78], benchmark that evaluates image editing models through the lens of knowledge, we conceptualize reasoning in video generation as cognitive rule-based prediction. Specifically, video models are required to infer implicit rules from the given input, predict possible outcomes, and present them through the generated video. Grounded in the three fundamental domains of the world: Nature, Society, and Virtuality, we define six rule categories that collectively characterize diverse and comTable 1. Comparison of open-source video generation benchmarks. represents insufficient reasoning dimensions. Benchmark Size Categories Tasks Type Reason Rule FetV [50] EvalCrafter [51] TC-Bench [20] VBench-2.0 [94] VideoPhy [6] UI2V-Bench [87] MME-CoF [29] 619 700 270 1260 688 500 59 RULER-Bench 3 4 - 5 3 4 - 6 22 12 3 18 6 21 12 40 T2V T2V T/I2V T2V T2V I2V T/I2V T/I2V plex reasoning scenarios in video generation. Natural Domain requires video generation models to perform reasoning grounded in real-world rules, thereby generating videos with plausible visual phenomena. The rule categories in this domain include: Vision. The ability of video models to reason about realworld visual compositions, such as appearance variations, spatial arrangements, and dynamic transformations. Science. The ability of models to reason about scientific phenomena by leveraging the underlying laws of nature. Social Domain challenges video models to reason grounded in rules derived from human society, generating videos exhibiting socially consistent behaviors and interactions. The rule categories in this domain encompass: Humanity. The ability of video models to infer human behaviors by leveraging principles of social dynamics, such as sports rules, traditional customs, and festivals. Semantics. The ability of video models to reason about authentic expressions through semantic regularities. Virtual Domain requires video models to perform reasoning grounded in rules of virtual environments, generating videos that follow the internal logic of these environments. The rule categories within this domain include: Game. The ability of video models to perform rational actions according to game rules to win matches or complete levels, for example, executing checkmate in chess. Hypothesis. The ability of video models to reason about and predict phenomena based on hypothetical premises. For each rule category, we carefully design distinct tasks to enable finer-grained decomposition. As shown in Fig. 1, RULER-Bench encompasses total of 40 tasks, with detailed descriptions provided in the Appendix. 3.2. Data Construction RULER-Bench encompasses two task paradigms: Textto-Video (T2V) and Image-to-Video (I2V). To construct benchmark with high-quality and diverse coverage, we curate the data through hybrid process that integrates human annotation with GPT-5 [56] generation. In this section, we provide detailed elaboration on the data construction procedures for each task paradigm. An overview of the data construction pipeline is illustrated in Part of Fig. 2. 3 Figure 2. Overview of dataset construction and validation. First, we formulate our tasks based on the six rule categories. Second, we design task-specific data construction pipelines for T2V and I2V tasks. Third, we leverage MLLM to construct checklist questions across four evaluation metrics. Finally, we conduct quality control and data refinement for the constructed dataset and checklists. Seed sample collection. We first manually curate seed samples for each task to provide GPT-5 with in-context guidance, which enables it to better comprehend task requirements and thereby generate more reliable samples. T2V data collection. For each T2V instance, we first define tuple (prompt, implicit explanation), where the prompt serves as the sole input to the video generation models, and the implicit explanation specifies the expected outcome along with the underlying reasoning principles. Second, we provide GPT-5 with detailed task requirements and set of seed samples, and instruct it to generate new samples that strictly adhere to the defined criteria, thereby ensuring both diversity and consistency in the dataset. I2V data collection. Traditional image collection for generative model benchmarks relies on web-sourced images [78, 95]. However, these images are largely confined to generic scenes, such as animals or cultural landscapes, while providing minimal coverage of domain-specific content. With the recent advances in image generation, stateof-the-art models [11, 16, 42] can generate richly detailed images based on highly customizable prompts. Therefore, we collect I2V data by integrating web-sourced and generated images based on the characteristics of each task. For instances containing web images, we adopt the same procedure as in T2V by defining tuple (prompt, implicit explanation). For each image, GPT-5 is used to design the prompt and implicit explanation based on the image content and task requirements. However, for samples in the Game category, MLLMs often struggle to generate implicit explanations that correctly reflect the underlying game strategies. To address this limitation, we collect ground truth images representing the final frame of the expected outcome, and manually create the corresponding implicit explanations to accurately capture the underlying rules. For samples containing generated images, we define triplet (initial scenario, prompt, implicit explanation), with the initial scenario serving as the input to the text-to-image (T2I) model. To ensure consistency, we instruct GPT-5 to leverage the initial scenario as the starting conditions when generating the prompt. Additionally, to maintain high visual quality, GPT-5 is guided to design initial scenarios that prevent the T2I model from generating images containing textual content, which remains challenging in current T2I systems. We employ the powerful nano banana [21] for image generation, providing both the generated image and the corresponding prompt as inputs to the I2V tasks. 3.3. Quality Control and Data Refinement To ensure high data quality, we design meticulous quality control pipeline, illustrated in Part D1 of Fig. 2. First, we use robust embedding model to encode all prompts into text embeddings. Second, we compute cosine similarity be4 tween prompts in the feature space. Highly similar prompts are deduplicated. To further ensure content fidelity, we instruct MLLM to verify consistency among the prompt, implicit explanation, and the corresponding input image. We then perform human refinement on instances that are deduplicated and self-consistent, with each sample carefully validated for rule correctness, task diversity, and scenario typicality. Finally, we pre-generate the verified samples using Sora2 to filter out instances subject to privacy or ethical constraints. During generation, we observe that Sora2 generates videos with fixed aspect ratios (16:9 or 9:16), which may cause stretching or cropping. To prevent information loss, we pad each input to match the target aspect ratio while preserving all critical visual content. 4. Evaluation Protocol 4.1. Evaluation Metrics To comprehensively evaluate the performance of state-ofthe-art video models on RULER-Bench, we consider 3 commonly used metrics: Instruction Following, Visual Consistency, and Visual Fidelity [29, 78, 93]. Additionally, we introduce novel rule-based dimension, termed Rule Coherence, which assesses whether the generated video adheres to the implicit rules underlying the given prompt. The detailed definitions of the 4 metrics are presented below. Instruction Following evaluates whether video models generate videos that faithfully follow the users instructions. It focuses on the semantic alignment with the input prompt and is independent of implicit rules. For instance, given the prompt place small wooden sphere and an iron sphere into water, the Instruction Following metric examines whether the balls are placed into water, without considering the subsequent phenomena or physical principles. Visual Consistency evaluates whether video generation models preserve identities of elements that are expected to remain unchanged throughout the generated video. For example, in sports scenes, the Visual Consistency dimension focuses on whether visual attributes, such as stadium color, remain consistent across the entire video. Visual Fidelity assesses the overall visual quality of the generated video, including whether it is visually clear, stable, and free from noise, artifacts, or distortions. Rule Coherence assesses whether the generated video adheres to scene-specific rules. It requires video models to leverage implicit rules to predict or infer visual phenomena from the given instructions. For example, given the prompt add extra water to the left arm of transparent U-shaped tube and observe the water levels in both arms, generation models are expected to apply the principle of communicating vessels to infer that the final water levels remain equal and depict this phenomenon accurately in the video. Inspired by [45], we design checklist-based evaluation Figure 3. Evaluation pipeline of RULER-Bench. for each sample. As shown in Fig. 3, each checklist contains multiple questions derived from the four evaluation metrics, and each question is answered using one of the three options: good, medium, or bad. Compared to traditional scoring methods [64, 78], the checklist protocol provides greater interpretability, allowing evaluators to make more consistent and human-aligned judgements. Given the recent advances in MLLMs of video understanding, we employ o3 [57] as the evaluator to assess the rule-based reasoning ability of video generation models. 4.2. Checklist Construction and Quality Control As shown in Fig. 2, we leverage MLLMs to generate evaluation checklists. For each sample, we instruct GPT-5 to generate multiple questions based on the input image, prompt, and implicit explanation, covering the four evaluation metrics. Each question is designed to evaluate the generated video from distinct perspective, ensuring that diverse aspects of video content are systematically assessed. Detailed instruction prompts are provided in the Appendix. To ensure the quality of the checklists, we perform careful manual verification for each sample. We instruct human annotators to review each checklist question and make revisions based on accuracy, clarity, consistency, and completeness. After manual verification, we finalize 6,500 checklist questions for 622 samples, ensuring comprehensive coverage across all four evaluation metrics. 5. Experiments and Evaluations General Settings. We evaluate the rule-based reasoning capabilities of 10 state-of-the-art video models on RULERBench, including six closed-source models: Veo3.1 [26], Veo2 [69], Sora2 [58], PixelVerse-V5 [60], Wan2.5 [2], and Seedance1.0-pro [24], and four open-source models: HunyuanVideo [41], CogVideoX1.5-5B [84], Wan2.1-14B [71], and Wan2.2-A14B [71]. Notably, the four open-source models provide separate T2V and I2V versions, whose results are reported as unified entry in our experiments. We use the default implementations of these video models, with detailed configurations provided in the Appendix. 5 Table 2. Evaluation result across different rule categories and metrics, including Instruction Following (IF), Visual Consistency (VC), Visual Fidelity (VF), and Rule Coherence (RC). All models exhibit limited rule-based reasoning ability. The performance of closed-source models and open-source models is separately marked with the best results in bold, and the second best underlined. Rule Categories Metric Closed-Source Models Open-Source Models Veo3.1 Veo2 Sora Pixel Verse-V5 Wan 2.5 Seedance 1.0-pro Hunyuan Video CogVideoX 1.5 5B Wan 2.2 A14B Wan 2.1 14B IF VC VF RC Avg IF VC VF RC Avg IF VC VF RC Avg IF VC VF RC Avg IF VC VF RC Avg VC VF RC Avg IF VC VF RC Avg Science Rule Game Rule Semantics Rule Hypothesis Rule Humanity Rule Vision Rule Average Win Rate 65.05 83.18 91.37 50.97 72.64 39.75 51.45 77.95 17.70 46. 71.83 92.65 91.62 67.57 80.92 86.97 85.90 92.20 46.79 77.96 79.90 87.37 94.49 61.23 80.75 59.53 72.67 48.94 60.38 68.70 76.68 86.72 48.87 70.24 0. 42.17 73.3 82.33 22.16 54.99 24.25 36.33 59.15 8.17 31.98 56.44 91.18 82.50 44.13 68.56 58.55 64.32 81.54 12.50 54.23 53.46 73.10 84.38 35.23 61.54 46.19 57.63 30.58 44. 46.97 64.07 74.59 25.46 52.77 66.00 88.01 89.49 47.09 72.65 39.19 72.33 88.18 19.97 54.92 68.12 90.85 83.43 53.69 74.02 72.44 77.35 82.50 41.35 68.41 80.04 88.06 88.08 56.78 78. 57.77 57.77 28.50 48.02 65.16 79.06 81.58 41.23 66.76 0.186 0.340 57.13 80.76 89.74 41.41 67.26 30.10 67.09 80.59 13.06 47. 65.08 91.18 89.02 56.80 75.52 80.13 81.62 85.73 46.69 73.54 72.87 84.25 89.65 50.63 74.35 56.14 71.61 40.47 56.07 61.06 76.84 84.39 41.51 65.95 0. 57.38 80.48 85.35 33.64 64.21 26.59 72.71 86.28 15.45 50.26 59.91 87.33 82.19 49.95 69.84 71.93 66.45 76.86 18.31 58.39 63.28 79.83 83.90 33.41 65.10 70.04 68.32 42.24 60. 55.82 76.14 80.48 32.17 61.15 0.257 58.86 80.99 87.69 31.96 64.87 24.26 68.79 88.39 15.61 49.26 61.28 87.67 84.55 49.42 70.73 64.32 67.74 79.66 28.31 60. 68.93 83.13 88.52 38.75 69.83 61.86 76.06 41.74 59.89 55.53 75.03 84.14 34.30 62.25 0.267 24.92 48.46 71.29 12.64 39.33 14.75 40.07 59.13 6.98 30. 38.51 80.39 79.17 32.01 57.52 44.44 51.92 73.89 9.62 44.97 46.56 70.60 80.94 27.78 56.47 43.49 52.94 18.91 38.45 33.84 55.82 69.56 17.99 44.30 0. 27.97 48.84 70.96 13.70 40.37 22.75 55.29 69.45 7.56 38.76 46.06 75.82 70.69 37.34 57.48 41.45 50.43 63.8 11.00 41.67 42.32 54.23 67.76 20.60 46.23 24.79 29.41 14.78 22. 36.11 51.57 62.01 17.50 41.80 0.151 37.15 68.52 80.37 17.16 50.80 16.29 64.56 80.13 14.12 43.77 48.77 82.35 82.70 37.73 62.89 61.11 64.74 77.03 12.93 53. 49.76 72.34 83.15 30.21 58.86 59.03 65.55 29.34 51.31 42.62 68.59 78.15 23.58 53.24 0.193 35.80 65.74 81.93 15.90 49.84 19.30 37.52 72.20 10.48 34. 46.27 80.72 83.09 38.40 62.12 61.75 55.56 75.17 17.84 52.58 52.28 70.47 82.32 29.21 58.57 51.26 49.58 23.25 41.36 43.08 60.21 74.05 22.51 49.96 0. Metrics. To enable straightforward comparison, we normalized all scores to 100-point scale. In particular, we map good, medium, and bad ratings to 100, 50, and 0, respectively. For each checklist, the score of each metric is computed as the average rating across all associated questions. To obtain the overall score, we assign equal weights to the four metrics and compute their mean score. 5.1. Main Results We evaluate the performance of 10 state-of-the-art video generation models across six rule categories under four evaluation metrics. Our quantitative results in Tab. 2 demonstrate that RULER-Bench effectively reveals finegrained differences across reasoning capabilities and evalTable 3. Human alignment comparison across different MLLMs. Model KRCC SRCC PLCC ACC Claude-haiku-4-5 Claude-sonnet-4-5 Doubao-seed-1-6 Gemini-2.5-flash Gemini-2.5-pro Grok-4 GPT-4o GPT-5 GPT-o3 0.2941 0.3143 0.1892 0.3436 0.3586 0.3522 0.3044 0.4204 0.4622 0.5128 0.5356 0.4827 0.5976 0.6073 0.5504 0.5564 0.7295 0. 0.5199 0.5442 0.5157 0.6023 0.6112 0.5612 0.5615 0.7289 0.8042 0.6728 0.7036 0.7632 0.7368 0.7208 0.6820 0.7282 0.7995 0.8512 uation metrics. We summarize the key observations below. Observation 1. Video models exhibit limited performance on the Rule Coherence metric compared to other 6 Figure 4. Average performance of video generation models across different tasks on RULER-Bench. Video models generally perform best on tasks in Humanity and Hypothesis, while showing lower performance on Vision and Game categories. evaluation metrics. As shown in Tab. 2, all video models achieve their lowest performance on the Rule Coherence metric, highlighting the significant challenge of rule-based reasoning for current generation models. Moreover, we observe consistent decline in scores as the metrics reflect more complex cognitive skills. For instance, Veo3.1 obtains an average score of approximately 80 on the perceptionbased metrics, including Visual Consistency and Visual Fidelity. The score drops to 68.7 on the Instruction Following metric, which evaluates visual understanding, and further decreases to 48.87 on the Rule Coherence metric. In addition, open-source models generally exhibit lower reasoning capabilities compared to closed-source models, suggesting that higher-quality training data and larger model capacity can enhance rule-based reasoning ability. Observation 2. Video generation models exhibit diverse performance patterns across different categories. As illustrated in Tab. 2, video models achieve the highest scores on the Humanity and Semantics categories, with Veo3.1 reaching an average of 80. This indicates that current models exhibit strong generative competence in the social domain, demonstrating solid understanding of human customs, language, and behaviors. In contrast, all models achieve an average Rule Coherence score below 15 on the Game category, suggesting limited generalization ability in customized, strategy-driven scenarios. As shown in Fig. 4, we further analyze the average performance of different video models across tasks. We observe that models also exhibit considerable intra-category variations in reasoning ability. For example, under the Humanity category, video models generally achieve higher scores on the dress task than on other tasks, reaching an average of 84.9, which can be attributed to the prevalence of clothing-related cues in human-centric scenes, thus enabling video models to generalize dress-related rules through training. Observation 3. Video models struggle with visual understanding and reasoning. As shown in Tab. 2 and Fig. 4, generative models perform worst on the Game and Vision categories, both of which consist entirely of I2V instances. This indicates that, beyond their limited capabilities of ruleTable 4. Effect of Prompt Enhancement (PE) on Rule Coherence. (Game category is excluded since most samples rely heavily on ground truth images, thus less affected by PE). Method Science Vision Semantics Hypothesis Humanity Veo3.1 Veo3.1+PE Sora2 Sora2+PE 50.97 62.62 +9.65 47.09 55.45 +8.36 48.94 52.61 +3.67 28.50 36.06 +7.56 67.57 72.19 +4. 53.69 63.37 +9.68 46.79 58.12 +11.33 41.35 63.35 +22.0 61.23 67.40 +6.17 56.78 65.15 +8.37 based reasoning, the models also exhibit insufficient reasoning of image content, which further constrains their capability to generate high-quality videos. To obtain deeper insight into this limitation, we analyze the result across task types and find that video generation models achieve an average score of 65.55 on T2V tasks, significantly higher than 48.56 on I2V tasks, indicating that an increase in input modalities leads to sharp decline in reasoning performance. 5.2. Analysis Impact on Prompt Enhancement. To analyze the impact of prompt enhancement at test times, we use GPT-o3 to generate an enhanced prompt based on the prompt and implicit explanation of each sample, explicitly incorporating the expected outcomes. As shown in Tab. 4, prompt enhancement substantially improves the performance of Veo3.1 and Sora2 on Rule Coherence. However, these results also suggest that even with explicit guidance on the expected phenomena, current video models still struggle to generate ruleconsistent video clips based on their reasoning abilities, revealing remaining gap toward zero-shot reasoners. Case study. Fig. 5 presents qualitative case studies of video models performance on RULER-Bench across different rule categories. In row #1, all models misinterpret the metaphor planting seeds for the future literally, depicting the sowing and watering scenes. In row #2, Veo3.1 fails to infer the correct placement of AED electrodes, while the other two models show limited understanding of the devices function. In row #3, Veo3.1 and Sora2 demonstrate 7 Figure 5. Case studies on three closed-source models across six rule categories. Each sample is provided with the Rule Coherence aspects derived from the checklist questions. The three video models exhibit varying performance across different instances. scientific reasoning abilities, with Veo3.1 exhibiting finergrained details. Meanwhile, in row #4, all models correctly infer the summer setting, yet their abilities to reason about human clothing vary. In row #5, Sora2 successfully performs game-strategy reasoning, whereas the other models fail to comprehend the task instruction. Finally, in row #6, Veo3.1 and PixelVerse-v5 effectively detect and rectify visual anomalies, while Sora2 struggles to reason over the visual context. These results show notable differences in reasoning competence across models, reflecting their varying abilities to integrate conceptual cues. Human alignment validation. To evaluate the reliability of our evaluation protocols, we conduct human preference alignment experiment. First, we randomly select 80 generated videos along with the corresponding 813 checklist questions. Second, human annotators are invited to answer each question based on the video content. Third, we evaluate the consistency between outputs of different closed-source MLLMs and human annotations, reporting Kendall Rank Correlation Coefficient (KRCC), Spearman Rank Correlation Coefficient (SRCC), Pearson Linear Correlation Coefficient (PLCC), and the overall Accuracy (ACC). As shown in Tab. 3, all models demonstrate strong video understanding capabilities. Among them, GPT-o3 achieves the highest accuracy of 0.8512 and outperforms other MLLMs across all three correlation metrics, demonstrating the reliability of using GPT-o3 as the evaluator. 6. Conclusion In this paper, we introduce RULER-Bench, comprehensive benchmark designed to evaluate the rule-based reasoning abilities of video generation models. RULER-Bench comprises 622 high-quality instances spanning 40 tasks across six rule categories, addressing the critical need for fine-grained evaluation of reasoning capabilities in video To ensure objective evaluation, we design models. checklist-based protocol from four evaluation metrics. Finally, we conduct extensive experiments on 10 state-of-the8 art video generation models and provide detailed analyses based on the results. We hope our work will provide valuable insights for advancing reasoning-aware video generation towards vision foundation intelligence."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 2, 3 [2] Alibaba Cloud (Wan Series). Wan 2.5, 2025. 2, 5 [3] Anthropic. Introducing claude haiku 4.5, 2025. 17 [4] Anthropic. System card: Claude sonnet 4.5. Technical report, Anthropic, 2025. Technical report, Anthropic, September 2025. 17 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2 [6] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. 3 [7] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek LLM: Scaling opensource language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. 2 [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. [9] ByteDance Seed. Introducing to seed1.6 series, 2025. 17 [10] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 2 [11] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 4 [12] Yunuo Chen, Junli Cao, Anil Kag, Vidit Goel, Sergei Korolev, Chenfanfu Jiang, Sergey Tulyakov, and Jian Ren. Towards physical understanding in video generation: 3d point regularization approach. arXiv preprint arXiv:2502.03639, 2025. 3 [13] Yupeng Chen, Penglin Chen, Xiaoyu Zhang, Yixian Huang, and Qian Xie. Editboard: Towards comprehensive evaluation benchmark for text-based video editing models. In AAAI, 2025. 3 Ivebench: Modern benchmark Hu, and Shuicheng Yan. suite for instruction-guided video editing assessment. arXiv preprint arXiv:2510.11647, 2025. 3 [15] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [16] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 4 [17] Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. 2 [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2 [19] Fanda Fan, Chunjie Luo, Wanling Gao, and Jianfeng Zhan. Aigcbench: Comprehensive evaluation of image-to-video content generated by ai. BenchCouncil Transactions on Benchmarks, Standards and Evaluations, 2023. 3 [20] Weixi Feng, Jiachen Li, Michael Saxon, Tsu-jui Fu, Wenhu Chen, and William Yang Wang. Tc-bench: Benchmarking temporal compositionality in text-to-video and image-tovideo generation. arXiv preprint arXiv:2406.08656, 2024. 3 [21] Alisa Fortin, Guillaume Vernade, Kat Kampf, and Ammaar Reshi. Introducing gemini 2.5 flash image: our state-of-theart image model, 2025. Google Developers Blog, May 2025. 4 [22] Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, and Lin Shao. Flip: Flow-centric generative planning as general-purpose manipulation world model. arXiv preprint arXiv:2412.08261, 2024. 3 [23] Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 2 [24] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 2, [25] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. ChatGLM: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. 2 [26] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, 2025. 2, 5 [14] Yinan Chen, Jiangning Zhang, Teng Hu, Yuxiang Zeng, Zhucun Xue, Qingdong He, Chengjie Wang, Yong Liu, Xiaobin [27] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 2025. 2 [28] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2 [29] Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, and Pheng-Ann Heng. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark. arXiv preprint arXiv:2510.26802, 2025. 3, 5 [30] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. In Photorealistic video generation with diffusion models. ECCV, 2024. 2 [31] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In CVPR, 2025. 2 [32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [33] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [34] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In NeurIPS, 2022. 2 [35] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Cogvideo: Large-scale pretraining for and Jie Tang. text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [36] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 3 [37] Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul Debevec, and Ziwei Liu. Vchain: Chain-of-visualthought for reasoning in video generation. arXiv preprint arXiv:2510.05094, 2025. [38] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 17 [39] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. In CVPR, 2024. 2 [40] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 2 [41] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 5 [42] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [43] Hengjia Li, Lifan Jiang, Xi Xiao, Tianyang Wang, Hongwei Yi, Boxi Wu, and Deng Cai. Magicid: Hybrid preference optimization for id-consistent and dynamic-preserved video customization. arXiv preprint arXiv:2503.12689, 2025. 2 [44] Hengjia Li, Haonan Qiu, Shiwei Zhang, Xiang Wang, Yujie Wei, Zekun Li, Yingya Zhang, Boxi Wu, and Deng Cai. Personalvideo: High id-fidelity video customization without dynamic and semantic degradation. In ICCV, 2025. 2 [45] Ouxiang Li, Yuan Wang, Xinting Hu, Huijuan Huang, Rui Chen, Jiarong Ou, Xin Tao, Pengfei Wan, Xiaojuan Qi, and Fuli Feng. Easier painting than thinking: Can text-to-image models set the stage, but not direct the play? arXiv preprint arXiv:2509.03516, 2025. 5 [46] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Zhe Lin, Rita Singh, and Bhiksha Raj. Controlvar: Exploring conarXiv preprint trollable visual autoregressive modeling. arXiv:2406.09750, 2024. 2 [47] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 2 [48] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Menghan Xia, Xintao Wang, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. 2 [49] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded imageto-video generation. In ECCV, 2024. 3 [50] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: benchmark for fine-grained evaluation of open-domain text-tovideo generation. NeurIPS, 2023. [51] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In CVPR, 2024. 3 [52] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2 [53] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. 2, 3 [54] Antonio Montanaro, Luca Savant Aira, Emanuele Aiello, Diego Valsesia, and Enrico Magli. Motioncraft: Physicsbased zero-shot video generation. In NeurIPS, 2024. 3 10 [55] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. 2 [56] OpenAI. Gpt-5 system card. Technical report, OpenAI, 2025. 3, [57] OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025. 5, 17 [58] OpenAI. Sora 2 system card. Technical report, OpenAI, 2025. 2, 5 [59] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2 [60] PixVerse Team. Pixverse v5, 2025. 5 [61] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [62] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2 and Stefano Ermon. arXiv preprint [63] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [64] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. In CVPR, 2025. 3, 5 [65] Shangkun Sun, Xiaoyu Liang, Songlin Fan, Wenxu Gao, and Wei Gao. Ve-bench: Subjective-aligned benchmark suite for text-driven video editing quality assessment. In AAAI, 2025. [66] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 2 [67] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. 2 [68] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [69] Aaron van den Oord, Elias Roman, Olivier Lacombe, Alisa Fortin, and Guillaume Vernade. Veo 2, 2024. Google Research Blog, December 2024. 5 [70] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models, 2022. 15 [71] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 5 [72] Jing Wang, Ao Ma, Ke Cao, Jun Zheng, Zhanjie Zhang, Jiasong Feng, Shanyuan Liu, Yuhang Ma, Bo Cheng, Dawei Leng, et al. Wisa: World simulator assistant for physics-aware text-to-video generation. arXiv preprint arXiv:2503.08153, 2025. 3 [73] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. IJCV, 2025. 2 [74] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 2 [75] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Biao Gong, Longxiang Tang, Xiang Wang, Haonan Qiu, Hengjia Li, Shuai Tan, Yingya Zhang, et al. Dreamrelation: Relation-centric video customization. arXiv preprint arXiv:2503.07602, 2025. 2 [76] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 2, 3 [77] Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. In AAAI, 2025. 2 [78] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025. 3, 4, [79] xAI. Grok 4 model card. Technical report, xAI, 2025. 17 [80] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2 [81] Tianyi Xie, Yiwei Zhao, Ying Jiang, and Chenfanfu Jiang. Physanimator: Physics-guided generative cartoon animation. In CVPR, 2025. 3 [82] Qiyao Xue, Xiangyu Yin, Boyuan Yang, and Wei Gao. Phyt2v: Llm-guided iterative self-refinement for physicsgrounded text-to-video generation. In CVPR, 2025. 3 [83] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2 [84] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 5 [85] Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video with artistic generation and translation. In CVPR, 2025. 2 [86] Yu Yuan, Xijun Wang, Tharindu Wickremasinghe, Zeeshan Nadir, Bole Ma, and Stanley Chan. Newtongen: Physicsconsistent and controllable text-to-video generation via neural newtonian dynamics. arXiv preprint arXiv:2509.21309, 2025. [87] Ailing Zhang, Lina Lei, Dehong Kong, Zhixin Wang, Jiaqi Xu, Fenglong Song, Chun-Le Guo, Chang Liu, Fan Li, and Jie Chen. Ui2v-bench: An understanding-based arXiv preprint image-to-video generation benchmark. arXiv:2509.24427, 2025. 3 [88] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In CVPR, 2023. 2 [89] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In ECCV, 2024. 3 [90] Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, and Yu Cheng. Videorepa: Learning physics for video generation through relational alignment with foundation models. arXiv preprint arXiv:2505.23656, 2025. 3 [91] Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, and Zhou Zhao. Tcsinger 2: Customizable multilingual zero-shot singing voice synthesis. arXiv preprint arXiv:2505.14910, 2025. 2 [92] Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Ruiqi Li, Jingyu Lu, Rongjie Huang, Ruiyuan Zhang, Zhiqing Hong, Ziyue Jiang, et al. Versatile framework for song generation with prompt-based control. arXiv preprint arXiv:2504.19062, 2025. [93] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. 5 [94] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, WeiShi Zheng, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. 2, 3 [95] Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Se norita-2m: high-quality instructionbased dataset for general video editing by video specialists. arXiv preprint arXiv:2502.06734, 2025. 4 12 RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence"
        },
        {
            "title": "Appendix Contents",
            "content": "The Appendix of RULER-Bench is structured as follows: Sec. A: Detailed task explanations. Sec. B: Implementation settings and configurations of different video generation models. Sec. C: Explorations on Video-to-Video tasks. Sec. D: More detailed configuration and experimental results of human annotation and user study. Sec. E: Prompts used for evaluation and generation. Sec. F: Experimental results across tasks. Sec. G: More visualization results. Sec. H: Limitations and future works. Sec. I: Potential social impact of RULER-Bench. A. Detailed Tasks Explanations RULER-Bench evaluates the reasoning abilities of the stateof-the-art video generation models across six rule categories: Vision, Science, Semantics, Hypothesis, Game, and Humanity. To enable more fine-grained assessment, we design suite of 40 tasks distributed over these categories, spanning broad spectrum of scenarios across the nature, society, and virtuality domains. In the following section, we comprehensively define each task. A.1. Vision Rule The Vision category comprises 10 distinct tasks, covering physical, spatial, and temporal visual attributes. Tasks in this category are designed to evaluate whether video generation models can accurately infer the visual composition of the real world from given image and generate videos that adhere to fundamental visual principles. Anomaly task focuses on visual plausibility reasoning. Given an image containing clear visual anomaly, generative models are required to infer the correct appearance of the object based on implicit visual rules and generate videos that restore visual coherence. For example, when the input image depicts cat with four tails, the model should reason that cat normally has only one tail and accordingly correct the anomaly in the generated video clip. Color task focuses on color reasoning and manipulation. Given an input image, generative models are required to reason about the color attributes of specific objects and modify them while preserving spatial-temporal coherence. Size task targets object-scale adjustment and proportional reasoning. Given an instruction to resize particular object, the model should modify its scale appropriately while preserving scene geometry and physical plausibility. Count task evaluates numerical reasoning and controllability. Models are required to adjust the number of instances of given object according to the instruction while preserving spatial realism and temporal coherence. Direction task focuses on directional reasoning and motion control. Given an input image and directional instruction, generative models are required to adjusting the objects facing and orientation, ensuring temporal coherence. Shape task evaluates geometric reasoning and controllability. Video models are required to reason about the specified objects geometry and modify its structure accordingly. Position task evaluates the reasoning ability of video models on spatial attributes. The model needs to reason about the objects location and relocate it accordingly. View task focuses on viewpoint reasoning and perspective adaptation. Given target view specification, video models should generate videos from the new viewpoint. Motion task targets dynamic prediction and temporal reasoning. Models are required to infer plausible motion trajectories from given static or partial motion input and predict temporally coherent frames. For instance, when observing basketball player suspended in mid-air, the model should predict and generate the subsequent motion that reflects realistic landing and continuation of the action. Style task focuses on appearance reasoning and stylistic transformation. Given an input image and style transformation instruction, video models are required to apply the target style while preserving object identity. A.2. Science Rule The Science category consists of 10 tasks spanning multiple disciplines within the natural sciences. These tasks are designed to evaluate whether video generation models have acquired fundamental understanding of natural laws and scientific principles, enabling them to accurately simulate experimental phenomena, biological behaviors, and other science-driven processes in the real world. Physics task focuses on scenes that involve fundamental physical principles, covering multiple domains such as mechanics, optics, and electromagnetism. Video models are required to predict physically consistent phenomena based on the implicit laws embedded within each sample. Chemistry task focuses on fundamental chemical reaction principles and their observational experimental manifestations. Video generation models are required to infer plausible reaction processes and products by reasoning over 13 the chemical rules, given the reactants and conditions. Biology task focuses on fundamental biological principles. Video models are required to infer plausible experimental outcomes based on organism-specific characteristics, covering variety of scenarios such as controlled biological experiments, genetics, and conditioned reflexes. Earth task focuses on geographical and environmental phenomena driven by factors such as location, climate variation, and seasonal cycles. Video generation models are required to infer long-term scene evolution according to underlying geographical principles and reflect these changes coherently within the generated video clip. Math task focuses on the application of fundamental mathematical principles. Given mathematical problem, video generation models are required to apply the relevant principles, derive the solution step by step, and visually illustrate the reasoning process clearly and coherently. Medicine task focuses on the correct application and demonstration of medical protocols. Given clinical scenario, video generation models are required to follow the specified procedural standards, such as venipuncture guidelines, the seven-step handwashing protocol, or basic surgical procedures, and accurately depict these procedures. Life task focuses on fundamental principles of animal behavior. Video generation models are required to infer plausible behavioral responses when experimental disturbances occur, reasoning according to species-specific behavioral patterns and ecological logic. A.3. Hypothesis Rule The Hypothesis category includes two tasks that encompass diverse scenarios, designed to evaluate whether video generation models can reason according to newly defined rules and assumptions beyond real-world rules. Unlike other categories, the governing rules here are explicitly provided in the prompt, requiring the model to perform zero-shot reasoning under newly introduced conditions. Subjective Scenario task focuses on human-defined hypothetical worlds. These scenarios violate real-world logic, requiring video generation models to reason about state changes according to the provided subjective rules. For example, given the rule lying causes the nose to grow, the model must recognize lying event in the scene and apply the specified rule to generate video in which the characters nose lengthens accordingly. Objective Law task focuses on modifying objective real-world principles. Unlike subjective scenarios, Objective Law tasks remain grounded in real physical principles but introduce deliberate modifications to them. Video models are required to abandon the original rule, adopt the newly specified one, and perform reasoning under the modified physical dynamics. For example, given the instruction assume the density order is reversed, the model must reason according to the new law and generate outcomes that contradict the behavior dictated by real-world physics. A.4. Game Rule The Game category comprises 10 tasks that cover wide range of logical reasoning and game scenarios. These tasks are designed to evaluate whether video generation models can reason according to game strategies, execute rational moves, and ultimately achieve victory. To ensure controllable complexity, each sample is constrained to the minimal number of required moves and simplified difficulty settings. Chinese Chess task focuses on one-move checkmate scenarios. Models are required to generate single, legally valid move that adheres to the piece-specific movement rules and delivers decisive checkmate against the opponents General, ensuring that the resulting board state is both legally valid and tactically forced. Gomoku task focuses on fundamental offensive and defensive patterns such as open-three formations, doublethree configurations, or immediate winning opportunities. Models must infer the optimal placement to either complete five-in-a-row or block the opponents winning threat. Go task focuses on one-move capture scenarios. Models are required to reason over the current board configuration and place single stone that removes one or more opponent groups that are in atari with only one remaining liberty, thereby executing valid and tactically justified capture. Sudoku task evaluates logical deduction under strict numerical constraints. Given partially filled grid, models must infer the correct number placement that simultaneously satisfies row, column, and subgrid consistency rules. Chess task targets tactical positions where checkmate can be achieved in the next move. Models must analyze the board configuration, identify forced-mate patterns, and generate the correct move that results in checkmate. Minesweeper task focuses on number-based spatial constraints. Models must analyze the revealed mine counts and infer which unrevealed tiles must contain mines with logical certainty, subsequently producing move that marks all deterministically identifiable mine locations. Maze task evaluates pathfinding and spatial reasoning under structural constraints. Models are required to generate sequence of movements that leads the agent from the starting point to the goal while avoiding dead ends. Puzzle task focuses on identifying spatial correspondences and assembling fragmented pieces into unified structure. Models must infer the correct arrangement of puzzle pieces by reasoning about shape compatibility and boundary alignment to form coherent final pattern. Number Sliding task evaluates sequential planning under movement constraints. Models must generate series of sliding operations that reposition numbered tiles toward the target configuration while preserving the adjacency rules. 14 Matchsticks task focuses on geometric and arithmetic reasoning under limited move constraints. Models must determine which matchsticks to reposition in order to form valid equation or geometric configuration. A.5. Semantics Rule The Semantics category comprises 3 tasks, designed to evaluate whether video generation models can infer high-level semantic information from contextual cues and generate videos that accurately reflect the intended visual meaning. Definition task focuses on the accurate illustration of provided term or concept. Models are required to infer the essential semantic attributes and contextual nuances of the term and produce visual content that aligns with its correct definition rather than surface-level associations. Metaphor task focuses on the interpretation of metaphorical expressions Given metaphorical phrase, the models are required to generate video that represents the underlying conceptual meaning, rather than literal depiction of the words, demonstrating understanding of figurative language and abstract semantic mapping. Idiom task focuses on culturally conventional idiomatic expressions. Models need to avoid literal interpretations and instead generate scenes that correctly express the idioms figurative semantic content, reflecting understanding of linguistic conventions and contextualized meaning. A.6. Humanity Rule The Humanity category includes 8 tasks that encompass diverse social scenarios and human conventions. These tasks are designed to evaluate whether video generation models can reason about societal norms, cultural practices, and customs to generate plausible human behaviors. Dress task focuses on culturally appropriate clothing choices and outfit consistency. Models are required to generate videos where characters wear attire suitable for the given context, season, or activity. Food task examines dietary norms and context-aware food selection. Models must generate scenes where the food type, preparation style, or eating behavior aligns with common culinary conventions and the specified scenario. Emotion task evaluates the understanding of human affect and expressive behavior. Models are required to generate facial expressions, gestures, and body movements that reflect the specified emotional state in natural manner. Festival task focuses on cultural celebrations and associated symbolic practices. Models must incorporate appropriate festival elements, such as decorations, attire, or rituals, corresponding to the designated cultural event. Safety task assesses whether models can adhere to basic safety norms in daily life. Given specific situations, models must avoid unsafe behaviors and instead generate actions aligned with conventional safety practices. Table A1. Details of the configurations of generated videos. Model Resolution Frames FPS Durations (s) Closed-Source Model Veo3.1 Veo2 Sora2 PixelVerse-V5 Seedance1.0-pro Wan2.5 1280720 1280720 1280704 1280720 1248704 1280720 192 192 300 121 121 121 Open-Source Model CogVideoX1.5 5B HuyuanVideo Wan2.1 14B Wan2.2 A14B 1360768 832480 832480 832480 161 81 81 81 24 24 30 24 24 24 16 15 16 15 8.00 8.00 10.00 5.04 5.04 5. 10.06 5.40 5.06 5."
        },
        {
            "title": "Social",
            "content": "task focuses on everyday social conventions. Models are required to generate actions that adhere to common social norms, engaging in other routine prosocial behaviors that reflect cultural expectations. Sports task targets sport-specific conventions. Models must generate videos where human movements, equipment usage, and gameplay dynamics align with the rules and common practices of the designated sport. Transportation task focuses on concrete road-traffic rules. Models are required to generate plausible interactions with real-world traffic systems, such as obeying traffic lights and road signs, or navigating multi-lane roads according to standard driving conventions. B. Implementation Detail We follow the official configurations of different video generation models. As summarized in Tab. A1, the videos have duration of 510 seconds and resolution of either 720P or 480P. Specifically, for open-source models, we implement them using the Diffusers framework [70]. C. Explorations on Video-to-Video tasks To provide more systematic and comprehensive evaluation, we further explore the rule-based reasoning abilities of video generation models on the Video-to-Video (V2V) tasks. Following the same data collection pipeline as in the I2V task, we manually curate web videos and use GPT-5 for captioning. This web video set covers eight tasks from the Vision category and consists of 81 high-quality samples. We leverage Wan2.1 14B to perform rule-based reasoning based on these V2V instances. We find that in most cases, Wan2.1 14B struggles to perform plausible reasoning based on the prompt and input video, exhibiting relatively poor generative performance on V2V tasks, as illustrated in Tab. A3 Surprisingly, the model achieves substantially higher scores on the Visual Consistency and Visual Fidelity metrics compared to I2V instances in the same task. We attribute this to the presence of the input video, which 15 Table A2. Human alignment comparison across MLLMs. KRCC, SRCC, PLCC, and ACC are reported across the four evaluation metrics. Model KRCC SRCC PLCC ACC IF RC VC VF IF RC VC VF IF RC VC VF IF RC VC VF Claude-haiku-4-5 Claude-sonnet-4-5 Grok-4 doubao-seed-1-6 Gemini-2.5-flash Gemini-2.5-pro GPT-4o GPT-5 GPT-o3 .2655 .2998 .3656 .2540 .3725 .4012 .3336 .4598 .5281 .3913 .3913 .4129 .3785 .4032 .4426 .3670 .4923 .5218 .2912 .3375 .3874 .1387 .3238 .3224 .3064 .4152 .4564 .1410 .1414 .2184 -.0031 .1874 .1708 .1535 .1831 .2419 .4186 .4852 .5680 .5473 .6021 .6639 .5869 .7495 . .5805 .5832 .6200 .5521 .6091 .6926 .5512 .7751 .8188 .5139 .5809 .5990 .3904 .5647 .5478 .5661 .6898 .8010 .3647 .3397 .4097 -.0385 .4791 .3843 .4209 .5301 .6315 .4384 .4909 .5798 .5569 .6113 .6559 .5777 .7309 .8765 .5789 .5794 .6150 .5521 .6063 .6921 .5534 .7750 .8171 .5028 .5640 .6042 .4143 .5649 .5431 .5516 .6869 . .3514 .3630 .4251 -.0385 .4805 .3974 .4037 .5284 .5996 .6456 .6519 .6603 .6471 .7025 .7152 .6962 .8038 .8924 .6188 .6733 .7050 .6875 .7129 .7525 .6584 .7871 .8218 .6312 .7188 .7006 .6875 .7250 .6937 .7125 .7625 .8500 .7474 .7440 .6678 .9259 .7782 .7167 .8020 .8259 .8498 Table A3. Quantitative results of Wan2.1 14B for V2V tasks. Metrics Color Count Direction Position Shape Size Style View RC VC VF Avg 3.57 18.18 42.86 22.73 89.29 64.39 45.24 35.10 10.00 55.00 82.50 49.17 12.50 71.67 77.50 53.89 18.18 14.58 7.5 5.00 52.5 42.50 62.5 31.82 59.09 72.92 50.83 51.67 36.36 50.00 36.94 33. Figure A2. Annotation interface for human alignment evaluation. question. For each sample, we provide detailed explanations of the associated metadata, such as the prompt and implicit explanation, and standardize the annotation requirements to ensure reliable judgments. Each annotators are required to make revisions based on the following criteria: Accuracy evaluates whether the question accurately reflects the expected video content based on specific rules. Clarity considers whether the checklist question is stated clearly and without ambiguity. Relevance measures whether the question is relevant to the scenario and prompt of the given sample. Consistency assesses whether the question type aligns with the corresponding evaluation dimension. Completeness ensures whether the checklist covers all necessary aspects for thorough evaluation. D.2. User Study To verify the effectiveness of our GPT-o3-based automated evaluation, we conduct human preference alignment experiment. First, we randomly sample 80 generated videos along with their corresponding checklist questions. Second, we develop an annotation interface to improve labeling efficiency. As shown in Fig. A2, annotators are provided with the generated video, the corresponding prompt, and the checklist questions. Third, annotators need to complete each checklist question based on the visual content of the video. After annotation, we aggregate all responses and compare human annotation with the output from stateFigure A1. Qualitative results of Wan2.1 14B for V2V tasks. likely serves as strong visual reference and encourages the model to align both visual quality and temporal consistency closely with the provided input. However, in few instances, as shown in Fig. A1, Wan2.1 14B is able to produce results that meet expectations, although this capability is observed only on sporadic samples. D. Details of Human Annotation We conduct large-scale human annotation for two aspects: checklist validation and human alignment experiments. In the checklist validation stage, human annotators are employed to assess the soundness of the proposed checklist and refine it when necessary. In the human alignment experiment, annotations are collected to support user study designed to evaluate the alignment between the output of GPTo3 and human preferences. The overall annotation process is collaboratively conducted by the authors. In this section, we present the detailed annotation procedures and results. D.1. Checklist Validation To validate the effectiveness of the constructed checklist, we perform quality control for each individual checklist of-the-art MLLMs, including Claude Haiku 4.5 [3], Claude Sonnet 4.5 [4], Doubao Seed 1.6 [9], Gemini 2.5 flash [15], Gemini 2.5 pro [15], Grok-4 [79], GPT-4o [38], GPT-5 [56], and GPT-o3 [57]. KRCC, SRCC, PLCC, and ACC are reported. As shown in Tab. A2, GPT-o3 demonstrates significant performance advantage across all four evaluation metrics. Its alignment with human preferences exceeds 85%, indicating its effectiveness as an evaluator. consistency. Future work may extend this framework to all three task paradigms, enabling unified assessment for the next-level video models that natively support T2V, I2V, and V2V in comprehensive manner. Potential for both depth and breadth expansion. While RULER-Bench covers broad spectrum of rule-based reasoning scenarios, its breadth can be further extended to tasks such as GUI interaction, classical algorithms, and table reasoning, which are left for future work. E. Prompt for Generation and Evaluation I. Social Impacts This work may positively influence the development of more reliable video generation by encouraging consistent rule-based reasoning, thereby supporting better downstream applications. However, stronger generative abilities may also increase the risk of misuse that obeys ethical norms and legal requirements. These risks highlight the importance of ethical deployment practices and appropriate oversight to ensure that progress in video generation technology benefits society while maintaining responsible use. Fig. A11 illustrates the prompt used to evaluate the performance across four evaluation metrics of MLLMs. Fig. A14 presents the prompt for generating checklist based on the provided metadata of the specific instance. Fig. A12 demonstrates the system prompt used for our Image-toscenario pipeline. GPT-5 is required to generate the corresponding prompt and implicit explanation based on the instruction and provided image. Finally, we use the prompt presented in Fig. A13 to generate initial scenario, prompt, and implicit explanation according to the task property. F. Experimental Results per Task We summarize the performance of different video generation models across the evaluation tasks. As shown in Tab. A4, Veo3.1 demonstrates impressive generative capability on multiple tasks, and closed-source models consistently outperform open-source models. Moreover, we find that although prompt enhancement leads to observable performance gains, current video models still fall short of fully achieving reliable rule-based reasoning. G. More Visualization Results Additional qualitative results are presented as follows: Fig. A3: Qualitative results of the Anomaly task. Fig. A4: Qualitative results of the Safety task. Fig. A5: Qualitative results of the Earth task. Fig. A6: Qualitative results of the Life task. Fig. A7: Qualitative results of the Physics task. Fig. A8: Qualitative results of the Chemistry task. Fig. A9: Qualitative results of the Emotion task. Fig. A10: Qualitative results of the Color task. H. Limitations and Future Works While RULER-Bench provides comprehensive evaluation for the rule-based reasoning ability of video generation models, there are still many challenges: Lack of unified evaluation that jointly assesses T2V, I2V, and V2V tasks. Since most existing video models support either T2V and I2V jointly or V2V exclusively, we focus our evaluation on the T2V and V2V settings for 17 Table A4. Evaluation result across tasks in six rule categories. Category Task Closed-Source Models Open-Source Models Prompt Enhancement Veo3.1 Veo2 Sora2 Pixel Verse-V5 Seedance 1.0-pro Wan2.5 CogVideoX 1.5 5B Hunyuan Video Wan2.1 14B Wan2.2 A14B Veo3.1 PE Sora2 PE 37.15 44.98 36.54 45.46 48.28 72.76 57.68 33.68 45.90 34.78 47.30 39.42 32.78 36.18 42.77 28.28 27.03 46.61 46.81 28.89 47.66 26.39 38.96 51.37 44.42 35.56 46.91 51.13 67.98 58. 8.12 22.22 8.33 16.88 21.88 18.06 20.83 7.50 19.38 27.29 48.72 58.74 49.49 47.79 61.81 81.97 66.03 42.89 36.87 36.18 40.95 39.12 39.48 44.91 37.21 31.09 24.06 32.24 34.72 26.67 47.81 22.22 23.44 25.82 33.11 38.50 50.52 52.21 68.93 55. 24.06 29.17 23.61 17.50 28.13 27.78 42.71 20.62 42.50 31.25 53.24 56.06 51.42 52.93 61.71 81.25 65.61 52.07 46.25 54.30 55.81 50.27 42.68 46.72 52.43 21.88 38.65 36.98 43.33 38.89 50.78 33.06 33.39 27.47 22.20 46.24 58.02 59.05 71.52 58. 21.56 30.56 22.22 30.00 32.50 31.94 36.98 25.62 31.88 42.71 50.67 59.66 51.72 59.36 68.03 85.74 60.97 40.12 51.62 54.17 52.44 48.94 43.33 47.84 55.62 33.28 36.67 44.90 53.61 44.51 50.47 46.11 47.55 34.10 51.64 44.90 61.72 57.83 70.54 64. 28.44 33.33 38.89 44.38 36.88 37.50 41.67 32.50 38.12 50.21 78.50 82.36 75.08 85.49 90.03 94.15 93.36 59.54 86.59 74.49 81.15 64.35 61.28 68.14 83.68 23.44 53.54 46.82 51.67 59.31 57.66 32.22 40.99 31.36 32.74 75.36 81.91 83.82 79.73 86. 38.44 62.50 26.39 36.88 28.12 50.78 46.35 47.50 51.25 57.92 69.38 79.65 75.22 87.88 89.74 93.67 85.68 55.57 81.63 82.25 76.38 51.12 72.92 80.62 73.50 50.78 49.95 71.72 67.50 51.25 78.12 52.22 57.24 48.68 59.30 73.02 84.32 75.13 82.07 79. 35.62 38.19 38.19 28.75 42.50 40.28 31.25 46.88 32.50 53.65 Humanity Science Game Hypothesis Semantics Vision transportation sport social safety festival dress food emotion chemistry physics biology earth math medicine life chess puzzle gomoku sudoku maze minesweeper number puzzle sticks xiangqi Go subjective objective idioms metaphors definition anomaly color count direction position shape size style view motion 80.00 78.08 74.88 82.79 80.83 90.87 90.94 69.21 81.13 79.09 83.31 57.27 61.52 63.38 74.51 20.94 64.43 39.43 52.36 56.60 70.00 50.83 39.53 34.27 41.44 74.06 81.31 76.53 81.36 87. 42.19 63.89 20.14 32.50 37.50 46.88 47.92 47.50 51.25 55.31 54.50 71.56 62.80 73.48 51.09 77.77 48.18 80.13 60.43 84.35 87.18 89.82 78.25 88.86 56.67 63.60 71.20 85.78 56.99 76.41 52.48 84.14 35.46 52.92 55.34 70.09 53.53 64.64 51.50 65.86 16.25 45.94 41.56 44.95 41.67 56.98 30.69 71.81 39.17 54.44 42.50 71.41 30.83 46.11 21.98 54.17 19.41 47.64 38.10 58.26 46.08 62.73 61.21 73.27 60.89 72.14 78.75 70.37 71.50 80. 34.69 32.81 26.39 22.92 18.75 34.03 30.00 30.00 43.12 41.88 27.34 39.58 30.73 38.02 23.12 43.75 29.38 31.25 51.04 41.77 73.44 71.66 65.23 75.13 80.03 86.86 78.65 66.34 75.21 72.32 73.63 52.02 57.86 64.18 67.54 28.12 58.39 41.20 64.31 45.28 68.75 32.50 54.01 39.09 43.53 58.87 86.12 72.11 80.56 76. 33.12 36.81 26.56 35.00 44.38 47.22 43.75 36.88 50.62 56.88 68.19 65.87 64.24 65.79 78.65 88.02 72.90 59.85 66.61 73.44 75.68 47.81 57.31 61.11 65.28 36.84 52.63 51.72 51.76 52.64 68.44 41.67 43.54 44.08 46.15 49.91 68.66 65.56 74.04 76. 37.19 51.39 38.89 48.75 37.50 54.17 43.75 31.25 45.62 56.88 67.53 74.95 61.18 57.40 61.54 85.10 67.67 45.46 65.73 76.34 74.97 49.12 65.76 56.09 55.44 28.91 59.90 46.51 51.11 57.57 73.44 47.78 55.31 32.95 49.40 49.42 65.64 60.00 72.12 82. 36.25 54.17 42.36 32.81 45.62 36.11 51.14 43.75 53.75 53.12 18 Figure A3. Qualitative results of the Anomaly task across 10 video models. Prompt: Please correct the bicycles wheel shape anomaly so that the vehicle can function properly. 19 Figure A4. Qualitative results of the Safety task across 10 video models. Prompt: In an office, thick smoke and small flames are rising from pile of paper near power socket. Several employees notice the fire spreading quickly across nearby papers and office supplies. They look around the room, searching for way to extinguish it. Figure A5. Qualitative results of the Earth task across 10 video models. Prompt: Observe the lava flow over several years under continuous volcanic activity, noting changes along its path. 21 Figure A6. Qualitative results of the Life task across 10 video models. Prompt: The process of tadpole transforming into frog. 22 Figure A7. Qualitative results of the Physics task across 10 video models. Prompt: Fill U-shaped transparent tube with colored water so that both arms have the same initial height. Add extra colored water to the left arm and observe the water levels in both arms. Figure A8. Qualitative results of the Chemistry task across 10 video models. Prompt: First, add 0.5 mL of 0.1 dilute hydrochloric acid and observe the litmus color change; then add 1 mL of 0.1 dilute sodium hydroxide, recording the color change. 24 Figure A9. Qualitative results of the Emotion task across 10 video models. Prompt: student has just learned that their exam results are excellent, exceeding expectations. 25 Figure A10. Qualitative results of the Count task across 10 video models. Prompt: Increase the number of pink paperclips in the input image so that there are exactly five partially overlapping paperclips, arranged in visually natural stacked formation where each clip slightly overlaps the next."
        },
        {
            "title": "System Prompt for Evaluation",
            "content": "You are video quality assessment expert. Respond using the following format exactly: <think></think> <answer></answer> In <think></think>, first provide concise, concrete description of the task. Next, give an objective understanding/summary of the input video (what happens in the video, salient visual / content elements). For each checklist question, give brief factual justification (one or two short sentences) explaining why you rated that question as you did. Do not reveal private chain-of-thought; provide only concise, professional justification for each answer. In <answer></answer>, write the answers to all checklist questions as JSON-style list of labels in order, for example: [\"Good\", \"Medium\", \"Poor\", ...] The list length must exactly match the number of checklist questions. Each item must be one of Good, Medium, or Poor. Do not include any other text, commentary, or explanation in <answer>. You will be provided: video the generation prompt an optional implicit explanation an optional input image an optional ground-truth image set of checklist questions"
        },
        {
            "title": "Definitions",
            "content": "generation prompt: the single textual instruction given to the video-generation model. implicit explanation: an optional detailed explanation of the intended outcome. input image: optional visual input provided to the model. ground truth image: the expected final frame or expected motion trajectory. checklist questions: specific quality or content questions to assess."
        },
        {
            "title": "Notes",
            "content": "Good The video clearly and fully satisfies the question. Medium The video partially satisfies the question or has minor issues. Poor The video fails to satisfy the question or has major issues."
        },
        {
            "title": "Rules",
            "content": "1. If the premise of question is not met by the video (e.g., the question asks about an element that never appears), rate that question Poor. 2. Evaluate each question independently; do not let other answers influence questions rating. 3. Provide concise justifications in <think> and only the ordered list of labels in <answer>. 4. The response must strictly follow the <think></think><answer></answer> format and include nothing outside those tags. Figure A11. System prompt template for the evaluation of generated videos. System Prompt for Image-to-scenario Pipeline For each sample, you must output two keys: prompt and implicit explanation, based on the provided image. Follow the detailed rules below precisely to ensure clarity and consistency. 1. Prompt: Describe the action or external event applied to the provided image the only textual input to the model. Requirements: Begin with single sentence briefly describing the visual content of the initial scenario (e.g., In the image, the beaker on the left contains hydrochloric acid, and the beaker on the right contains sodium hydroxide solution.) Must contain exactly one main action or event. Exclude results, explanations, or causes. Keep it precise and maintain the same style and perspective as the initial scene. Use unambiguous, temporally clear language. Must not leak any information from implicit explanation. Ensure standalone completeness: the description must remain logically self-contained even when no initial scenario is provided. Clearly specify all references in each sentence; avoid vague terms such as some, certain, kind of, or unknown always use explicit, concrete nouns. 2. Implicit Explanation: Explain the hidden reasoning behind the prompt: the mechanism that will unfold after the prompts action. Requirements: Describe factual mechanisms and expected consequences logically. Include clear cause process result sequence. Use precise, domain-correct terminology (no metaphors or emotional tone). Maintain full consistency with both initial scenario and prompt. Cautions: All samples must maintain high-school level reasoning difficulty: complex enough to require logical understanding but not specialized technical expertise. Scenarios must remain within the real-world physical scale: avoid microscopic (e.g., atomic, cellular) and macroscopic (e.g., planetary, galactic) contexts. Ensure internal logical coherence: objects, actions, and outcomes must follow physically plausible relationships. All components (initial scenario, prompt, implicit explanation) must align stylistically and factually. Avoid fantasy or science-fiction elements unless explicitly grounded in real-world analogs. Maintain visual realism and neutral tone no symbolic, emotional, or metaphorical descriptions. Each generated sample should strictly correspond to the specific task requirements and concepts, with no ambiguity in objects, actions, or reasoning related to the task. Figure A12. System prompt template for Image-to-scenario pipeline designed for I2V tasks. 28 System Prompt for Scenario-to-image Pipeline For each sample, you must output 3 keys: initial scenario, prompt, and implicit explanation. Follow the detailed rules below precisely to ensure clarity and consistency. 1. initial scenario: Describe the static starting state of the scene in precise, visual, and objective language. This defines what the model initially sees before any change occurs. Requirements: The description should be detailed and concise, including visual objects, spatial arrangement, lighting, composition, and style, etc. No actions, no temporal hints, no implied future events. Should logically support the descriptions in prompt. Do not include any visible text, letters, characters, or numbers in the scene description (e.g., avoid book with the title History on the cover or clock showing the number 12). Clearly specify all references in each sentence; avoid vague terms such as some, certain, or unknown always use explicit, concrete nouns. 2. prompt: Describe the action or external event applied to the initial scene the only textual input to the generative model. Requirements: Begin with single sentence briefly describing the visual content of the initial scenario (e.g., In the image, the beaker on the left contains hydrochloric acid, and the beaker on the right contains sodium hydroxide solution.) Must contain exactly one main action or event. No results, explanations, or causes. Maintain the same style and perspective as the initial scene. Must not leak any information from implicit explanation. Standalone completeness: The description must be logically self-contained even when no initial scenario is provided. Ensure continuity between actions and the initial scene. Clearly specify all references in each sentence; avoid vague terms such as some, certain, kind of, unknown always use explicit, concrete nouns. 3. implicit explanation: Explain the hidden reasoning behind the prompt: the mechanism that will unfold after the prompts action. Requirements: Describe factual mechanisms and expected consequences logically. Include cause process result sequence. Use precise, domain-correct terminology (no metaphors or emotional tone). Must stay fully consistent with initial scenario and prompt."
        },
        {
            "title": "Cautions",
            "content": "Scenarios must remain strictly within the real-world physical scale: avoid microscopic (e.g., atomic, cellular) and macroscopic (e.g., planetary, galactic) contexts. Ensure internal logical coherence: objects, actions, and outcomes must follow physically plausible relationships. All components (initial scenario, prompt, implicit explanation) must align stylistically and factually with one another. Each generated sample should strictly correspond to the specific task requirements and concepts, with no ambiguity in objects, actions, or reasoning related to the task. Figure A13. System prompt template for Scenario-to-image pipeline. Initial scenarios are instructed to T2I systems to generate images."
        },
        {
            "title": "System Prompt for Checklist Generation",
            "content": "You are an expert evaluator specialized in assessing video generation models. Your task is to create checklist composed of questions that will be used by multimodal model to judge the quality of generated video."
        },
        {
            "title": "Inputs",
            "content": "prompt: the only textual instruction used by the video generation model. input image or video (optional): the only visual input provided to the model. gt image (optional): the expected final frame or the expected motion trajectory. implicit explanation (optional): detailed explanation of the intended outcome."
        },
        {
            "title": "Evaluation Dimensions",
            "content": "Instruction Following Whether the video correctly follows the prompts instruction. Visual Consistency Whether objects, actors, and camera motions remain coherent across space and time including alignment, motion realism, and smooth scene transitions. Rule Coherence Whether the video logically reflects causal, physical, or rule-based reasoning implied by the prompt. Visual Fidelity Whether the video maintains high visual quality including clarity, color stability, and absence of artifacts or distortions."
        },
        {
            "title": "Checklist Design Rules",
            "content": "1. Include at least five questions. 2. Each question must refer to specific objects, actions, or outcomes described in the inputs. 3. Include at least one question that explicitly tests the models reasoning ability. 4. Each reasoning-related question must focus on only one reasoning aspect (e.g., causal reasoning, physical reasoning, spatial reasoning) do not combine multiple reasoning types in single question. 5. Phrase every question so that answering Yes means the generated video meets the expected criteria, and No means it does not. 6. Use only information that appears in the prompt, input image / video, gt image, and explicit explanation. 7. If part of the explicit explanation describes something that is not necessary result of the prompt, you may ignore it. 8. If multiple reasoning aspects are involved, design separate questions for each reasoning aspect. 9. The final checklist must include at least one question corresponding to each of the four dimensions: Instruction Following, Visual Consistency, Rule Coherence, and Visual Fidelity."
        },
        {
            "title": "Output Format",
            "content": "Return the checklist as valid Python list of JSON objects, where each JSON object uses the dimension name as the key and the corresponding Yes/No question as the value. Figure A14. System prompt template for checklist generation across 4 evaluation metrics."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Zhejiang University"
    ]
}