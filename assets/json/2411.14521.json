{
    "paper_title": "MyTimeMachine: Personalized Facial Age Transformation",
    "authors": [
        "Luchao Qi",
        "Jiaye Wu",
        "Bang Gong",
        "Annie N. Wang",
        "David W. Jacobs",
        "Roni Sengupta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20$\\sim$40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 1 1 2 5 4 1 . 1 1 4 2 : r MyTimeMachine: Personalized Facial Age Transformation Luchao Qi1 Jiaye Wu2 Bang Gong1 Annie N. Wang1 David W. Jacobs2 Roni Sengupta1 1University of North Carolina at Chapel Hill {lqi, gongbang, awang13, ronisen}@cs.unc.edu 2University of Maryland, College Park {jiayewu, djacobs}@cs.umd.edu SAM [2] CUSP [14] AgeTransGAN [17] FADING [7] + Dreambooth [49] MyTimeMachine Figure 1. We introduce MyTimeMachine to perform personalized age regression (top) and progression (bottom) by training personspecific aging model from few (50) personal photos spanning over 20-40 year range. Our method outperforms existing age transformation techniques to generate re-aged faces that closely resemble the characteristic facial appearance of the user at the target age."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Facial aging is complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the persons appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to personal photo collection of the user depicting aging in small time interval (2040 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines global aging prior with personal photo collection (using as few as 50 images) to learn personalized age transformation. We introduce novel Adapter Network that combines personalized aging features with global aging features and generates re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive wnorm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches. What makes face aging so challenging? Virtual age transformation algorithms aim to digitally simulate the physical aging process of an individuals face. The goal of these methods [15, 17, 28, 32, 39, 62, 68] is to modify the shape and texture of the face to create the desired re-aging effect, while preserving the individuals unique identity, along with the pose, lighting, and style of the input image. However, facial aging is often highly dependent on several factors, such as ethnicity, gender, genetics, lifestyle, and health conditions [33, 52], which makes it challenging to model. Existing age transformation algorithms [2, 14, 17] learn generative global prior, modeling how an average face ages, using datasets like FFHQ [20]. While these methods have advanced in generating visually pleasing re-aging effects, they often fail to accurately depict how an individual actually ages. For instance, when provided with an image of Al Pacino at 68 years old, state-of-the-art techniques [2, 7, 14, 17] can produce realistic yet inaccurate version of his appearance at 30, as shown in Fig. 1. For many practical applications, e.g. re-aging actors in films, it is crucial to accurately re-age individuals to the target age, as viewers are often familiar with the actors appearance at various stages of life. However, accurately predicting an individuals re-aged appearance from single image is highly illposed and challenging, since aging is person-specific [10]. 1 In this paper, we show that accurate age synthesis can be performed when an algorithm has access to as few as 50 photos of an individual across 2040 year time range. Personal photo collections are often available in many practical applications of virtual aging, and utilizing them can significantly improve the result, see MyTimeMachine in Fig 1. For example, consider de-aging effects often used in movies where particular actor at 60 years old is shooting scene where they need to be rendered as 30 years old. We can easily access the past 2040 years of photos of the actor to learn an accurate aging model. Similarly, consider an individual interested in simulating how photograph of their loved one at 40 years would appear at 60 years old or beyond. We can also easily access the past 10-20 years of photo collection of their loved ones to understand the aging process and more accurately simulate their future appearance at 60 years and beyond. We therefore create personalized aging method that can transform an input image to any target age, both within and beyond the age range represented in the personal photo collection used for training. Simply personalizing generic global age transformation algorithm, e.g. FADING [7], with Dreambooth [49] is ineffective. Personal photo collections often cover limited range of age, pose, lighting, and style variations compared to large-scale facial datasets like FFHQ. Consequently, naive fine-tuning typically results in overfitting, limiting the models ability to generalize to unseen ages, poses, styles, and lighting conditions, as shown by the extrapolation failure of FADING + Dreambooth in row 4 of Fig. 4. Additionally, FADING is built on diffusion, facing the typical inversion-editability trade-off problem [16, 57]. Specifically, re-aging requires both high fidelity to the input face at similar ages and high editability as the target age diverges. In contrast, such trade-offs have been more well explored in StyleGAN2s well-trained latent space [5, 46, 57, 61]. Therefore, we demonstrate an effective approach to personalized age transformation based on StyleGAN2. Our proposed personalized age transformation network, MyTimeMachine (MyTM), introduces novel adapter network that updates global facial aging features with personalized aging characteristics, trained on personal photo collection using custom loss functions. Built on top of SAM [2], global age transformation network capable of continuous aging without per-image optimization, MyTM enhances SAMs global age encoder, which projects an input image into StyleGAN2s latent space with specified target age. We design personalized adapter network that learns to adjust the global aging features. To train this adapter, we introduce three loss functions: personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. The personalized aging loss ensures that identitypreserving features of the reaged image closely resemble those in reference image from the personal photo collection at similar target age. Extrapolation regularization controls aging effects beyond the training age range using global priors, while adaptive w-norm regularization addresses StyleGANs inversion-editability trade-off, ensuring distinct shape and texture changes due to aging while preserving identity. We then extend MyTM to video reaging by utilizing face-swapping techniques to generate temporally consistent and identity-preserving results. We curated longitudinal aging dataset comprising highquality images of 12 celebrities, captured under diverse conditions, including varying poses, expressions, and lighting. Inspired by real-world applications of personalized aging, we train our model on this dataset and establish two experimental frameworks to evaluate its performance: one for age regression, where 70-year-old is rendered younger, and another for age progression, where 40-year-old is rendered older. Our method outperforms existing global age transformation and naive personalization techniques, delivering high-quality, identity-preserving aging effects in both images and videos that closely resemble each individuals actual appearance at the target age. In summary, our contributions are as follows: (i) We demonstrate that with access to few (50) personal images spanning few decades (2040 years), we can achieve high-quality, identity-preserving facial age transformations. These transformations accurately reflect the persons appearance at the target age while maintaining the style of the input image. (ii) We introduce several key technical advancements that integrate global aging prior with personal photo collection to enable personalized aging. Our approach trains an adapter network to adjust the global aging prior, utilizing three custom loss functions: personalized age loss, extrapolation regularization, and adaptive wnorm regularization. (iii) We show that MyTM can also be extended to perform temporally consistent and identitypreserving reaging in videos, which is important for many VFX applications. 2. Related Work Traditional age transformation methods fall into two categories: prototype-based [23, 56] and physical model-based approaches [51, 54]. For detailed overview, we refer readers to the survey by [12]. Recently, generative models have shown impressive results in synthesizing and editing highresolution face images, inspiring their use in aging tasks. Global Age Editing. Global age editing refers to age transformation without personal data. Prior works [37, 50] leverage StyleGAN2s well-trained latent space, identifying and traversing linear age editing direction within it. However, this assumption often fails with larger age changes, especially across lifespan, and can entangle other attributes (e.g., gender or glasses) [18]. To address this, recent methods [39, 65] introduce nonlinear aging paths in the latent space by training separate age encoders. In diffusion models, several methods [4, 25, 27] perform age editing through latent manipulation guided by CLIP [42]. However, these methods continue to struggle with attribute entanglement in the latent space. FADING [7] improves disentanglement by projecting the input face into the diffusion models latent space using NTI [34] and applying age editing through p2p [16]. However, FADING focuses on textural changes to facial shape achieve re-aging, neglecting the broader changes that occur over persons lifespan [14]. This limitation arises because p2p identifies age-related pixels through attention control, resulting in localized edits rather than facial structural changes [48]. To address this, we build on StyleGAN2, leveraging its well-trained latent space for both fine-grained textural control and structural changes. Personalization of Generative Models. Personalization involves tuning face models with personal images. PTI [46] fine-tunes StyleGAN2 generator anchored by an inverted latent code. Other approaches [36, 41, 66] adapt the generator on small set of personal images (50100) to create personalized prior. In diffusion models, Dreambooth [49] optimizes the weights of the text-to-image network to adapt to specific subject through prompt identifier. In the context of lifespan age transformation, these personalization techniques often overfit to the few training images in limited range (e.g., ages 50 to 70), making it challenging to extrapolate to ages beyond the training range (e.g., 20 years old). We demonstrate that MyTM produces personalized face aging results within the training age range and generalizes to ages beyond it. Video Re-aging. recent video re-aging approach, FRAN [71], applies facial masks to predict age-related changes within masked regions per frame. However, similar to FADING, such method often overlooks structural changes in facial shape that naturally occur over persons lifetime, such as the widening of previously narrow face due to bone growth and shifts in facial fat distribution with age [14]. It also suffers with temporal consistency since it is trained on static images. To address this, Muqeet et al. [35] propose re-aging model trained on synthetic video data, generating re-aged keyframes and interpolating between them to enhance temporal consistency. However, neither of these approaches is open-sourced or supports personalized video re-aging. Recent work [19] seeks to enhance temporal consistency in identity-specific face-swapping by personalizing models for individual users. However, this approach demands around 5,000 images of the persons face captured under various conditions, all at similar age, limiting its effectiveness for lifespan aging transformations. To address this, we follow face-swapping techniques [6, 64], using our personalized re-aged face as the source for swapping. This approach eliminates the need for training dedicated model on large number of personal images. 3 3. Method In Sec. 3.1, we begin by outlining the fundamentals of global aging, SAM [2]. Next, Sec. 3.2 introduces our personalized aging adapter, MyTM, and explains how we integrate personal aging with global aging. Finally, we present the training losses for MyTM in Sec. 3.3. 3.1. Preliminaries Here we provide brief overview of SAM [2], global age transformation network that forms the building block of our proposed personalized network, MyTM. SAM trains an age encoder (Eθ) that maps an input image (x) into the latent space + of StyleGAN, aligning with the desired target age (atgt). The latent code is then processed through the pre-trained StyleGAN (Dθ) to generate the age-transformed face (ytgt). SAM is trained on the FFHQ dataset [21], where the training procedure involves producing an agetransformed output ytgt = Dθ(Eθ(x, atgt)) in forward pass. This process is supervised by the loss Lforward, encouraging the re-aged image to be similar to the input image: Lforward(ytgt) =λl2L2(ytgt) + λlpipsLLPIPS(ytgt) + λidLID(ytgt) + λageLage(ytgt) (1) L2, Llpips and LID matches age-transformed image (ytgt) to input image (x) in pixel space, LPIPS feature space [67] and ArcFace [8] feature space respectively. Lage matches the predicted age by face age detector, DEX() [47], with target age atgt: Lage (ytgt) = atgt DEX (ytgt) 2 After the forward pass, SAM encourages the transformed image (ytgt) to be re-transformed back to the input image. This process helps ensure cycle consistency [70] and can be formally described as ycycle = Dθ(Eθ(ytgt, ax)) with the same loss: Lcycle(ycycle) = Lforward(ycycle). The complete training loss is then given by: (2) Lsam = Lforward(ytgt) + Lcycle(ycycle) (3) 3.2. MyTM: Designing Personalized Age Adapter Training personalized aging prior from scratch is suboptimal due to the limited availability of personal aging data. To address this, we introduce MyTM, which personalizes pre-trained age encoder by combining two components: 1) SAM, pre-trained age encoder that captures shared global aging prior learned from diverse set of identities, and 2) Age Adapter, personalized age adapter network trained exclusively on an individual. Specifically, we assume the individual has personal photo collection of RGB images, xi, each with an associated ground truth age, ai, represented as = {(xi, ai)}N i=1. Our key idea is to update the age-transformed latent code + tgt produced by the global age transformation network, SAM, using personalized adapter network, AN (). The adapter takes the latent vector + tgt predicted by SAM and computes an offset latent vector + tgt = Eq. 6 Eq. 7 Eq. 5 Figure 2. Given an input face of Oprah Winfrey at 70 years old, our adapter re-ages her face to resemble her appearance at 30, while preserving the style of the input image. To achieve personalized re-aging, we collect 50 images of an individual across different ages and train an adapter network that updates the latent code generated by the global age encoder SAM. Our adapter preserves identity during interpolation when the target age falls within the range of ages seen in the training data, while also extrapolating well to unseen ages. AN(Eθ(x, atgt), atgt). As shown in Fig. 2, we then combine this personalized latent adaptation + tgt with the global latent code + tgt and pass the result through the pre-trained StyleGAN2 decoder. Formally, yp tgt = Dθ( Eθ(x, atgt) + AN(Eθ(x, atgt), atgt) ) (4) global aging personal aging By doing so, MyTM enables the integration of partially observed, personalized aging information of an individualusing only images of that personinto the global aging trajectory. Our adapter is based on an MLP architecture, with detailed implementation of the age adapter network available in the supplementary material. 3.3. MyTM: Loss Funtions Our adapter is trained on personal photo collection = {(xi, ai)}N i=1. We introduce three loss functionspersonalized aging loss, extrapolation regularization, and adaptive w-norm regularizationto integrate global priors with personal data. Additionally, we also use the loss function Lsam from Sec. 3.1, based on SAM, to mitigate the forgetting of global priors [24, 38]. Personalized Aging Loss. After examining the loss formulation of SAM [2], we notice that the primary source of aging information is the aging loss Lage, which relies on pre-trained age classifier. The problem with this global aging loss is that the age classifier is not robust across different ethnicities, styles, and individual aging patterns [29]. It is often impossible to train robust aging detector that works well for every individual. Rather than relying on the power of global age classifier, we propose personalized aging loss that encourages facial features of the transformed face to be similar to reference images in similar age range in the training dataset. This encourages the re-aged image to closely resemble how the person looked at that age, ignoring the pose, lighting, and style variations. We denote the minimum and maximum age of the traini=1 as amin = min(a1...an) ing dataset = {(xi, ai)}N 4 and amax = max(a1...an). During training, we randomly sample target age atgt between minimum and maximum age atgt U(amin, amax). We create reference set, Dtgt = {(xj, aj)}M j=1, which contains actual images of the individual near the target age (atgt 3-years). We then employ facial recognition network, Arcface [9], to extract identity features and compute the similarity between the age-transformed image ytgt and all images in the reference set Dtgt, and only consider the maximum similarity. Considering maximum similarity over the reference set ensures that the identity recognition networks are not significantly influenced by stylistic differences between images. Formally, we define the personalized aging loss as: tgt), R(xj)(cid:11)(cid:9)M Lpers-age = 1 max (cid:8)(cid:10)R(yp (5) j=1 R() is pretrained ArcFace [8] network for facial feature recognition, , computes the cosine similarity between its argument [40], and is the number of images in reference set Dtgt with faces near the target age (atgt 3-years). Extrapolation Regularization. When training the adapter network with personalized age loss, we observe that the networks performance can degrade when atgt falls outside the training age range [amin, amax]. Specifically, this degradation manifests as the generated images continuing to resemble the appearance at the boundaries of the training age range (amin, amax), rather than appropriately aging or deaging. For instance, as illustrated in row 4 of Fig. 4 (FADING + Dreambooth), when the training set covers faces aged 30 to 70, the model may overfit, generating faces that still resemble 30-year-old when atgt = 10. To prevent this extrapolation failure, we enforce the preservation of the pre-trained SAMs output during extrapolation. We apply experience replay [38, 49], which encourages the output of our personalized age encoder (yp tgt) to be similar to that produced by the pre-trained SAM (ytgt): tgt, ytgt) tgt, ytgt) tgt, ytgt) + λLPIPSLLPIPS(yp +λIDLID(yp Lreg-extra = λl2L2(yp (6) Adaptive w-norm regularization. During personalization, we observed that SAM struggles to capture distinct facial feature changes across ages, as illustrated in row 2 of Fig. 4. We attribute this issue to the inversion-editability trade-off [46, 57]. Specifically, the latent codes predicted by SAM are distant from the training distribution and the center of the latent space, W, reducing their editing capacity and making personalization challenging. This trade-off is particularly relevant in facial aging tasks. When the target age is close to the input age, we encourage the latent codes close to SAMs pre-trained output, preserving inversion accuracy while staying distant from the average latent code W. As the target age diverges from the input age, greater deformations in head shape and facial texture are needed, requiring latent codes nearer to to facilitate editing. To address this, we propose adaptive W-norm regularization inspired by [45], where Lreg = λregW + tgt is used to constrain the latent codes. We further enhance this by making λreg cosine function, , , of the difference between input and target age age = ai atgt: Lreg = λreg(age)W + tgt λreg(age) = 1 π age/100 (7) 4. Experiments In Sec. 4.1 we first introduce our experimental setup, including datasets, experimental framework, state-of-the-art aging algorithms, and evaluation metrics. In Sec. 4.2 we present comparisons with state-of-the-art baselines, followed by the application of MyTM for video re-aging in Sec. 4.3. Finally, we present ablation studies in 4.4. 4.1. Experimental Setup Dataset. We curated dataset of images featuring 12 celebrities spanning wide age range, including 7 males and 5 females from diverse ethnic backgrounds such as Caucasian, African American, Hispanic, and Asian. For further details, please refer to Table 3 in the supplementary material. For each celebrity, we train MyTM using 50 images, as discussed in Sec. 4.4. We then evaluate the performance of MyTM using test images of the same celebrity at either 40 or 70 years old, depending on the task outlined later in the experimental framework. Experimental Framework. We consider the following two real-world scenarios where age transformation techniques are heavily used and demand high quality. 1) Age regression or de-aging renders images of an individual to go back in time and is heavily used in VFX for movies and TV shows [58]. Motivated by this, we design an experimental setup where we personalize our aging model by training on images from either 3070 or 5070 age range and then evaluate de-aging performance on an unseen image at 70 years old to target age (atgt 70 years old). We sample the target age every 10 years where atgt {0, 10, 20, 30, 40, 50, 60, 70}. 2) Age progression or aging renders images of an individual going forward in time and is used for forensic investigations, missing person searches, or as an emotional support tool to visualize departed loved ones. We design an experimental framework where we personalize our aging model by training on images from 2040 years old and evaluate on unseen faces at 40 years old to generate target age (atgt 40), where atgt {40, 50, 60, 70, 80, 90, 100}. State-of-the-art Aging Algorithms. We compare our results with the following state-of-the-art aging methods: (i) SAM [2], which uses pre-trained StyleGAN2 decoder and trains an age encoder on FFHQ [22]. It treats aging as continuous process, enabling fine-grained control over trans- (ii) CUSP [14], which jointly trains both an formations. age encoder and decoder on FFHQ-Aging [39]. While effective for age transformations, it lacks fine-grained control due to its reliance on predefined age group-based transformations, limiting editing capabilities and supporting resolutions only up to 256 256. (iii) AgeTransGAN [17], an encoder-decoder architecture, also limited by age groupbased transformations, similar to CUSP. (iv) FADING [7], which inverts images into the latent space of face diffusion model using NTI [34], then edits them through p2p [16], allowing the aging diffusion model to be personalized to the input image. We also introduce additional personalization f.t., naively fine-tune SAM on baselines: (v) SAM Pers. personal images; (vi) FADING + Dreambooth, personalizes FADING by following the Dreambooth approach [49] with the prompt photo of [atgt] year old [sks] man/woman. Evaluation Metrics. Following the evaluation protocols in prior aging baselines [2, 7, 14, 17], we evaluate our personalized age transformation results in terms of age accuracy and identity preservation. We use the following metrics to evaluate the re-aged results quantitatively: Age Accuracy (AgeM AE). Following previous works [2, 14], we define age mean absolute error as AgeM AE = ˆatgt atgt, where ˆatgt is predicted by FP-Age [29]. Identity Preservation (IDsim). Previous works [55, 71] evaluate identity preservation by comparing the re-aged face to the input face. However, facial recognition systems, such as ArcFace [8], demonstrate strong dependence on age and thus favor age consistency between the re-aged face and the input face [2], favoring algorithms that perform small changes. We address this problem by creating reference image sets of the individual near the target age which are not used in training, and then calculate the identity similarity to the reference images at the target age, in contrast to using the input image. Formally, IDsim (ytgt) = max {R(ytgt), R(xj)}M j=1 where R() is pretrained ArcFace [8] network for facial feature recognition and xj belongs in reference image set near the target age (atgt 3-years). We report the average IDsim across all sampled target ages. (8) 5 Figure 3. Performance of age transformation techniques for age regression (first two rows) and age progression (last two rows). The first column shows the input image, and the second column provides reference image of the same person at the target age. MyTM (Ours) is compared against other state-of-the-art methods including SAM [2], CUSP [14], AgeTransGAN [17], and FADING [7]. 4.2. Comparison with Age Transformation Methods Age Regression. We use two age ranges of personal photos40 years (ages 3070) and 20 years (ages 5070)to examine the impact of training age span. Results are presented in Table 1, with visual examples of other pre-trained methods in Fig. 3 and other personalized methods in Fig. 4. For detailed visual comparison across all ages (0100), please refer to Fig. 12 in the supplementary material. Compared to pre-trained baselines (SAM, CUSP, AgeTransGAN, and FADING), our method achieves superior identity preservation (IDsim), with an 11.7% improvement (0.67 vs. 0.60) in IDsim over the best-performing method, FADING. This improvement is also maintained during interpolation (e.g., when atgt 30 70), producing 9.0% increase in IDsim (0.72 vs. 0.66) compared to FADING, even when FADING overfits to the input image via NTI [34], favoring its IDsim score for smaller age gaps. Compared to other personalized methods, our approach achieves both high age accuracy (AgeM AE) and strong identity preservation. SAM + Pers. f.t. shows minimal improvement over SAM alone, underscoring the effectiveness of our proposed loss function in Sec. 3.3. While FADING + Dreambooth [49] (5070) records slight improvement over ours in IDsim (0.78 vs. 0.76), it fails to maintain age accuracy (MAE 25.9 vs. 7.7) and overfits to the training age range, limiting its ability to generalize to unseen ages. Method AgeM AE() IDsim() atgt 70 atgt 50 70 atgt 30 70 0.49 0.48 0.49 0.39 0.53 0.60 0.63 0.64 0.65 0.67 8.1 8.2 9.2 11.0 11.1 8.9 25.9 23.0 7.7 7.8 0.58 0.58 - 0.44 0.65 0.72 0.78 - 0.76 - SAM [2] + Pers. f.t. (5070) + Pers. f.t. (3070) CUSP [14] AgeTransGAN [17] FADING [7] + Dreambooth [49] (5070) + Dreambooth [49] (3070) Ours (5070) Ours (3070) Table 1. Performance of age regression where an input test image at 70 years old is de-aged to target age atgt 70. We also evaluate MyTM (Ours) using 20-year (atgt 50 70) and 40-year (atgt 30 70) age ranges in the training data. Bold indicates the best results, while underlined denotes the second-best. 0.53 - 0.53 0.42 0.58 0.66 - 0.70 - 0.72 Age progression. We perform age progression with 20year (ages 20 40) range of personal photos. Age progression specifically evaluates the extrapolation ability of our technique to ages not seen in training. Quantitative results are presented in Table 2, with visual comparisons to other pre-trained methods shown in Fig. 3, and full visual comparison provided in Fig. 12 in the supplement. Our model outperforms pre-trained baselines, achieving the highest age accuracy (6.3) and best identity preservation (0.70 for atgt 40 and 0.78 for atgt 40 60), due to the benefits of personalization. As shown in Fig. 12, FADING often produces poor results when the target age differs greatly from the input age, due to NTI + p2p editing [48]. 6 Figure 4. Performance of age transformation techniques for age regression, where an input test image around 70 is transformed to all target ages between 0 and 100. We show MyTM (Ours) trained on 40 years of data (ages 30 70), with the age range included in the personal training data highlighted in red. An example image of the same person within 3 years of the target age is provided as reference at bottom. Method AgeM AE() IDsim() atgt 40 atgt 40 60 SAM [2] + Pers. f.t. (2040) CUSP [14] AgeTransGAN [17] FADING [7] + Dreambooth [49] (2040) Ours (2040) 6.9 10.3 7.3 8.5 7.6 20.2 6.3 0.54 0.56 0.44 0.61 0.62 0.72 0.70 0.58 0.59 0.48 0.65 0.71 0.77 0.78 Table 2. Performance of age progression where an input test image at 40 years old is aged to target age atgt 40. We also evaluate MyTM (Ours) using 20-year (atgt 40 60) and atgt 40 age ranges in the training data. Bold indicates the best results, while underlined denotes the second-best. Note that FADING + Dreambooth has the lowest aging accuracy, as measured by AgeM AE. We evaluate our method across two age regression tasks (3070 and 5070) and one age progression task (2040), totaling three tasks. We then sample one input and re-aged image pair per celebrity, resulting in 10 pairs for each age regression task and 8 pairs for the age progression task. For each pair, we collect 24 user responses for comparisons with FADING and 29 user responses for comparisons with SAM. As shown in Fig. 5, our method is significantly preferred over the baselines across all re-aging tasks. 4.3. MyTM for Video Re-aging Having established personalized aging prior, we extend our focus to video re-aging. Instead of training separate re-aging model specifically for video re-aging, we build Figure 5. User Study comparing our method with baselinesFADING and SAMfor age regression (atgt 70) and age progression (atgt 40). We present the percentage of user preference for our method over the baselines. Compared to other personalized methods, FADING + Dreambooth achieves slightly better IDsim than our model (0.72 vs. 0.70). However, it struggles to extrapolate to unseen ages, resulting in high AgeM AE of 20.2. User Studies. We conduct user studies to qualitatively evaluate our method through pairwise human evaluations. In each pair, users see the original input image alongside two re-aged results at the target age atgtone generated by an existing method and the other by ours, presented in random order. Users also receive reference images showing the persons face near atgt and are asked to select the result that best matches the reference images while preserving the style of the original input image. 7 Figure 6. We apply video re-aging on video of Jackie Chan from the movie Bleeding Steel. Left: The keyframe from the source video that we re-age with MyTM. Right: The re-aged face is mapped onto other frames of the source video via face-swapping. upon face-swapping techniques by utilizing Inswapper1, widely adopted black-box model for face swapping [59]. Given source video, we manually select keyframe in near-frontal pose with minimal occlusion and motion blur as the basis for re-aging. MyTM is then applied to this keyframe to transform the face image to the desired target age, generating personalized re-aged face. Next, for each video frame, the face in the current frame and the re-aged face are input into the swapping model to generate the final re-aged result. This re-aged face is then pasted back onto the current frame using landmark-based warping. This framework requires only single re-aged face for swapping, ensuring strong temporal consistency while preserving personalized facial identity. Our video re-aging pipeline is illustrated in Fig. 6. For further details on temporal consistency, please refer to the supplement. 4.4. Ablation Study Effect of Dataset Size. We investigate the impact of training dataset size on MyTM by sampling subsets of images for each celebrity, with sizes of 10, 50, and 100. We then assess MyTMs performance on the age regression task (ages 3070), which demands the largest training age range. We report the average IDsim in Fig. 7. Results indicate significant performance improvement from 10 to 50 images, with minimal gains from 50 to 100 images. Consequently, we use 50 images for personalization, unless otherwise noted. Effect of Proposed Loss Functions and Architecture. We analyze the effectiveness of our proposed network architecture and loss functions by conducting an ablation study in Fig. 8. We begin with SAM and progressively introduce each proposed component, including custom loss terms and the adapter network. For the age regression task, we train MyTM on ages spanning 30 to 70, testing with target ages atgt 70. Our proposed Personalized Aging Loss yields the most improvement in IDsim. 5. Conclusion We present MyTimeMachine, personalized facial age transformation technique that combines an individuals longitudinal photo collection (as few as 50 images) with global 1https://github.com/deepinsight/insightface Dataset Size Ablations Metric SAM = 10 = 50 = 100 AgeM AE() IDsim() 8.1 0. 8.5 0.58 7.8 0.67 8.0 0.67 Figure 7. Effect of training dataset size on personalization. MyTM is trained on ages 3070 and tested for atgt 70. Visual examples of Robert De Niro are shown at the top, with quantitative results displayed below. Model Ours A) B) C) SAM Pers. f.t. SAM Adapter Network - - - - - Extrapolation Reg - - - - Personalized Aging Loss - - - Adaptive W-norm Reg - - IDsim() 0.67 0.65 0.65 0.55 0.46 0. Figure 8. Contributions of our proposed loss functions and the adapter network for the age regression task, trained on ages 3070 and tested for atgt 70 on Al Pacino. aging priors. Our main technical contributions, which enable the learning of personalized aging prior, include novel Adapter Network architecture, personalized aging Loss, extrapolation regularization, and adaptive w-norm regularization. Extensive quantitative and qualitative evaluations demonstrate that our method outperforms existing age transformation and naive personalization approaches. Limitations. While our model effectively performs age transformations, as shown in Fig. 13 in the supplement, it can struggle with accessories (e.g. glasses) due to the limitations of the e4e encoder [57]. Furthermore, the pretrained SAM model has difficulty in 1) modifying hair color to or from whitea common challenge in other aging works [28, 53]. 2) producing red-eye artifacts when generating older faces. Although our proposed w-norm regularization mitigates these issues, they are not fully resolved. Ethical Considerations. Facial aging is complex and inherently challenging problem, and even with personalization, our model may lack robustness across all underrepresented populations. Our approach also has the potential to produce manipulated images of real individuals, which poses significant societal risk. This issue is common across generative models, underscoring the need for future research in detecting synthetic image composites."
        },
        {
            "title": "References",
            "content": "[1] Mehran Aghabozorgi, Shichong Peng, and Ke Li. Adaptive IMLE for Few-shot Pretraining-free Generative Modelling. 2023. 2 [2] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Only matter of style: age transformation using style-based regression model. ACM Transactions on Graphics, 40(4):1 12, 2021. TLDR: An image-to-image translation method that learns to directly encode real facial images into the latent space of pre-trained unconditional GAN subject to given aging shift, which learns more disentangled, nonlinear path. 1, 2, 3, 4, 5, 6, 7 [3] David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba. Semantic Photo Manipulation with Generative Image Prior. ACM Transactions on Graphics, 38(4):111, 2019. arXiv:2005.07727 [cs] TLDR: This paper adapts the image prior learned by GANs to image statistics of an individual image and can accurately reconstruct the input image and synthesize new content, consistent with the appearance of theinput image. 1 [4] Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, and Bjorn Ommer. Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions, 2024. arXiv:2403.17064 [cs] TLDR: It is shown that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models and it is demonstrated that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in compositional manner. 3 [5] Anand Bhattad, Viraj Shah, Derek Hoiem, and D. A. Forsyth. Make It So: Steering StyleGAN for Any Image Inversion and Editing, 2023. arXiv:2304.14403 [cs]. [6] Renwang Chen, Xuanhong Chen, Bingbing Ni, and Yanhao Ge. SimSwap: An Efficient Framework For High Fidelity Face Swapping, 2021. arXiv:2106.06340. 3 [7] Xiangyi Chen and Stephane Lathuili`ere. Face Aging via Diffusion-based Editing, 2023. arXiv:2309.11321 [cs]. 1, 2, 3, 5, 6, 7 [8] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF Conface recognition. ference on Computer Vision and Pattern Recognition, pages 46904699, 2019. 3, 4, 5 [9] Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, and Stefanos Zafeiriou. ArcFace: Additive Angular MarIEEE Transactions gin Loss for Deep Face Recognition. on Pattern Analysis and Machine Intelligence, 44(10):5962 5979, 2022. arXiv:1801.07698 [cs]. 4 [10] Julien Despois, Frederic Flament, and Matthieu Perrot. AgingMapGAN (AMGAN): High-Resolution Controllable Face Aging with Spatially-Aware Conditional GANs, 2020. arXiv:2008.10960. 1 [11] Amil Dravid, Yossi Gandelsman, Kuan-Chieh Wang, Rameen Abdal, Gordon Wetzstein, Alexei A. Efros, and Kfir Aberman. Interpreting the Weight Space of Customized Diffusion Models, 2024. arXiv:2406.09413 [cs] TLDR: It is shown that inverting single image into this space reconstructs realistic identity, even if the input image is out of distribution (e.g., painting). 2 [12] Yun Fu, Guodong Guo, and Thomas S. Huang. Age synthesis and estimation via faces: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(11):1955 1976, 2010. 2 [13] Gege Gao, Huaibo Huang, Chaoyou Fu, Zhaoyang Li, and Ran He. Information Bottleneck Disentanglement for Identity Swapping. pages 34043413, 2021. 3 [14] Guillermo Gomez-Trenado, Stephane Lathuili`ere, Pablo Mesejo, and Oscar Cordon. Custom Structure Preservation in Face Aging, 2022. arXiv:2207.11025 [cs]. 1, 3, 5, 6, 7 [15] Zhenliang He, Meina Kan, Shiguang Shan, and Xilin Chen. S2GAN: Share Aging Factors Across Ages and Share Aging Trends Among Individuals. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 94399448, Seoul, Korea (South), 2019. IEEE. TLDR: The proposed method can achieve continuous face aging with favorable aging accuracy, identity preservation, and fidelity, and befitted from the effective design, unique model is capable of all ages and the prediction time is significantly saved. 1 [16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-toPrompt Image Editing with Cross Attention Control, 2022. arXiv:2208.01626 TLDR: This paper analyzes textconditioned model in depth and observes that the crossattention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt, and presents several applications which monitor the image synthesis by editing the textual prompt only. 2, 3, 5 [17] Gee-Sern Hsu, Rui-Cang Xie, Zhi-Ting Chen, and Yu-Hong Lin. AgeTransGAN for Facial Age Transformation with Rectified Performance Metrics. In Computer Vision ECCV 2022, pages 580595. Springer Nature Switzerland, Cham, 2022. Series Title: Lecture Notes in Computer Science. 1, 5, 6, 7 [18] Erik Harkonen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. GANSpace: Discovering Interpretable GAN Controls, 2020. arXiv:2004.02546 [cs]. [19] iperov. iperov/DeepFaceLive, 2024. original-date: 2020-1215T12:19:22Z. 3 [20] Tero Karras, Samuli Laine, and Timo Aila. StyleBased Generator Architecture for Generative Adversarial Networks, 2019. arXiv:1812.04948 [cs, stat]. 1 [21] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and Improving the Image Quality of StyleGAN, 2020. arXiv:1912.04958 [cs, eess, stat]. 3 [22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In Proc. CVPR, 2020. 5, 1 [23] Ira Kemelmacher-Shlizerman, Supasorn Suwajanakorn, and Steven M. Seitz. Illumination-Aware Age Progression. pages 33343341, 2014. 2 [24] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka GrabskaBarwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic Proceedings of the Naforgetting in neural networks. tional Academy of Sciences, 114(13):35213526, 2017. arXiv:1612.00796 [cs, stat]. 4 [25] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion Models already have Semantic Latent Space, 2023. arXiv:2210.10960 [cs] TLDR: Asyrp is proposed which discovers the semantic latent space in frozen pretrained diffusion models, named h-space, which has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. 3 [26] Jason Lee, Kyunghyun Cho, and Douwe Kiela. Coun2019. tering Language Drift via Visual Grounding, arXiv:1909.04499 [cs]. 2 [27] Peipei Li, Rui Wang, Huaibo Huang, Ran He, and Zhaofeng Pluralistic Aging Diffusion Autoencoder, 2023. He. arXiv:2303.11086 [cs]. [28] Zeqi Li, Ruowei Jiang, and Parham Aarabi. Continuous Face Aging via Self-estimated Residual Age Embedding. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1500315012, Nashville, TN, USA, 2021. IEEE. 1, 8 [29] Yiming Lin, Jie Shen, Yujiang Wang, and Maja Pantic. FPAge: Leveraging Face Parsing Attention for Facial Age Estimation in the Wild, 2022. arXiv:2106.11145 [cs]. 4, 5, 1 [30] Mingcong Liu, Qiang Li, Zekui Qin, Guoxin Zhang, Pengfei Wan, and Wen Zheng. BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation, 2021. arXiv:2110.11728 [cs]. 1 [31] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering Language Drift with Seeded Iterated Learning. In Proceedings of the 37th International Conference on Machine Learning, pages 6437 6447. PMLR, 2020. ISSN: 2640-3498. 2 [32] Farkhod Makhmudkhujaev, Sungeun Hong, and In Kyu Park. Re-Aging GAN: Toward Personalized Face Age TransforIn 2021 IEEE/CVF International Conference on mation. Computer Vision (ICCV), pages 38883897, Montreal, QC, Canada, 2021. IEEE. TLDR: This work proposes ReAging GAN (RAGAN), novel single framework considering all the critical factors in age transformation that achieves state-of-the-art personalized face age transformation by compelling the input identity to perform the self-guidance of the generation process. 1 [33] Bryan Mendelson and Chin-Ho Wong. Changes in the Facial Skeleton With Aging: Implications and Clinical Applications in Facial Rejuvenation. Aesthetic Plastic Surgery, 36 (4):753760, 2012. [34] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Inversion for Editand Daniel Cohen-Or. ing Real Images using Guided Diffusion Models, 2022. arXiv:2211.09794 [cs]. 3, 5, 6, 2 Null-text [35] Abdul Muqeet, Kyuchul Lee, Bumsoo Kim, Yohan Hong, Hyungrae Lee, Woonggon Kim, and KwangHee Lee. Video Face Re-Aging: Toward Temporally Consistent Face ReAging, 2023. arXiv:2311.11642 [cs] TLDR: novel synthetic video dataset that features subjects across diverse range of age groups is proposed that outperforms existing approaches in age transformation accuracy and temporal consistency and the development of novel metrics tailored explicitly for evaluating the temporal consistency of video reaging techniques is developed. 3 [36] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and Daniel Cohen-Or. MyStyle: Personalized Generative Prior. ACM Transactions on Graphics, 41(6):206:1206:10, 2022. TLDR: This work introduces MyStyle, personalized deep generative prior trained with few shots of an individual that allows to reconstruct, enhance and edit images of specific person, such that the output is faithful to the persons key facial characteristics. 3, 2 [37] Yotam Nitzan, Rinon Gal, Ofir Brenner, and Daniel CohenOr. LARGE: Latent-Based Regression through GAN Semantics. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1921719227, New Orleans, LA, USA, 2022. IEEE. TLDR: This work proposes novel method for solving regression tasks using few-shot or weak supervision for GANs, and makes the simple observation that distances traversed along such directions are good features for downstream tasks - reliably gauging the magnitude of property in an image. 2 [38] Yotam Nitzan, Michael Gharbi, Richard Zhang, Taesung Park, Jun-Yan Zhu, Daniel Cohen-Or, and Eli Shechtman. Domain Expansion of Image Generators, 2023. arXiv:2301.05225 [cs]. [39] Roy Or-El, Soumyadip Sengupta, Ohad Fried, Eli Shechtman, and Ira Kemelmacher-Shlizerman. Lifespan Age Transformation Synthesis, 2020. arXiv:2003.09764 [cs]. 1, 2, 5 [40] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery, 2021. arXiv:2103.17249 [cs]. 4, 1 [41] Luchao Qi, Jiaye Wu, Annie N. Wang, Shengze Wang, and Roni Sengupta. My3DGen: Scalable Personalized 3D Generative Model, 2023. arXiv:2307.05468 [cs]. 3, 2 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision, 2021. arXiv:2103.00020 [cs]. 3 [43] Xiaohang Ren, Xingyu Chen, Pengfei Yao, Heung-Yeung Shum, and Baoyuan Wang. Reinforced Disentanglement for Face Swapping without Skip Connection, 2023. arXiv:2307.07928 [cs]. 3 [44] K. Ricanek and T. Tesafaye. MORPH: longitudinal image In 7th Internadatabase of normal adult age-progression. tional Conference on Automatic Face and Gesture Recognition (FGR06), pages 341345, 2006. 1 [45] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in Style: StyleGAN Encoder for Image-to-Image TransIn 2021 IEEE/CVF Conference on Computer Vilation. sion and Pattern Recognition (CVPR), pages 22872296, TLDR: This work Nashville, TN, USA, 2021. IEEE. presents generic image-to-image translation framework, pixel2style2pixel (pSp), based on novel encoder network that directly generates series of style vectors which are fed into pretrained StyleGAN generator, forming the extended latent space. 5 [46] Daniel Roich, Ron Mokady, Amit H. Bermano, and Daniel Cohen-Or. Pivotal Tuning for Latent-based Editing of Real Images, 2021. arXiv:2106.05744 [cs]. 2, 3, 5 [47] Rasmus Rothe, Radu Timofte, and Luc Van Gool. DEX: Deep EXpectation of Apparent Age from Single Image. In 2015 IEEE International Conference on Computer Vision Workshop (ICCVW), pages 252257, Santiago, Chile, 2015. IEEE. TLDR: The proposed method, Deep EXpectation (DEX) of apparent age, first detects the face in the test image and then extracts the CNN predictions from an ensemble of 20 networks on the cropped face, significantly outperforming the human reference. 3 [48] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations, 2024. arXiv:2410.10792. 3, 6, 2 [49] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, 2023. arXiv:2208.12242 [cs]. 1, 2, 3, 4, 5, 6, 7 [50] Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou. InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs, 2020. arXiv:2005.09635 [cs, eess]. 2 [51] Jinli Suo, Song-Chun Zhu, Shiguang Shan, and Xilin Chen. Compositional and Dynamic Model for Face Aging. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(3):385401, 2010. Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence. [52] Arthur Swift, Steven Liew, Susan Weinkle, Julie Garcia, and Michael Silberberg. The Facial Aging Process From the Inside Out. Aesthetic Surgery Journal, 41(10):1107 1119, 2020. 1 [53] Xu Tang, Zongwei Wang, Weixin Luo, and Shenghua Gao. Face Aging with Identity-Preserved Conditional Generative Adversarial Networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7939 7947, Salt Lake City, UT, USA, 2018. IEEE. 8 [54] Yusuke Tazoe, Hiroaki Gohara, Akinobu Maejima, and Shigeo Morishima. Facial aging simulator considering geomIn ACM SIGGRAPH 2012 etry and patch-tiled texture. Posters, page 1, New York, NY, USA, 2012. Association for Computing Machinery. 2 [55] Qianrui Teng, Rui Wang, Xing Cui, Peipei Li, and Exploring 3D-aware Lifespan Face AgZhaofeng He. ing via Disentangled Shape-Texture Representations, 2023. arXiv:2312.16881 [cs]. 5 [56] B. Tiddeman, M. Burt, and D. Perrett. Prototyping and transforming facial textures for perception research. IEEE Computer Graphics and Applications, 21(5):4250, 2001. Conference Name: IEEE Computer Graphics and Applications. 2 [57] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder for StyleGAN image manipulation. ACM Transactions on Graphics, 40(4): 133:1133:14, 2021. 2, 5, [58] Ben Travis. The Irishman: Robert De Niro Recreated Iconic GoodFellas Scene To Test De-Ageing Tech Exclusive, 2019. 5 [59] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. InstantID: Zero-shot Identity-Preserving Generation in Seconds, 2024. arXiv:2401.07519 [cs]. 8 [60] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards Real-World Blind Face Restoration with Generative Facial Prior, 2021. arXiv:2101.04061 [cs]. 1 [61] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. GAN Inversion: Survey, 2022. arXiv:2101.05278 [cs]. 2 [62] Jiu-Cheng Xie, Jun Yang, Wenqing Wang, Feng Xu, and Hao Gao. Diverse and Lifespan Facial Age Transformation Synthesis with Identity Variation Rationality Metric, 2024. arXiv:2401.14036 [cs]. 1 [63] Chao Xu, Jiangning Zhang, Yue Han, Guanzhong Tian, Xianfang Zeng, Ying Tai, Yabiao Wang, Chengjie Wang, and Yong Liu. Designing One Unified Framework for HighFidelity Face Reenactment and Swapping. In Computer Vision ECCV 2022, pages 5471, Cham, 2022. Springer Nature Switzerland. TLDR: This paper proposes an effective end-to-end unified framework to achieve both face reenactment and swapping, and sufficiently transfers identity and attribute based on learned disentangled representations to generate high-fidelity faces. [64] Zhiliang Xu, Zhibin Hong, Changxing Ding, Zhen Zhu, Junyu Han, Jingtuo Liu, and Errui Ding. MobileFaceSwap: Lightweight Framework for Video Face Swapping, 2022. arXiv:2201.03808. 3 [65] Xu Yao, Gilles Puy, Alasdair Newson, Yann Gousseau, and Pierre Hellier. High Resolution Face Age Editing, 2020. arXiv:2005.04410 [cs]. 2 [66] Libing Zeng, Lele Chen, Yi Xu, and Nima Kalantari. MyStyle++: Controllable Personalized Generative Prior, 2023. arXiv:2306.04865 [cs]. 3 [67] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 3 [68] Zhifei Zhang, Yang Song, and Hairong Qi. Age Progression/Regression by Conditional Adversarial Autoencoder, 2017. arXiv:1702.08423 [cs]. 1 [69] Yan Zheng and Lemeng Wu. InverseMeetInsert: Robust Real Image Editing via Geometric Accumulation Inversion in Guided Diffusion Models, 2024. arXiv:2409.11734 version: 1 TLDR: GEO is introduced, an exceptionally versatile image editing technique designed to cater to customized user requirements at both local and global scales and is driven by 11 novel geometric accumulation loss that enhances DDIM inversion to faithfully preserve pixel space geometry and layout. 2 [70] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 22422251, 2017. 3 [71] Gaspard Zoss, Prashanth Chandran, Eftychios Sifakis, and Derek Bradley. Markus Gross, Paulo Gotardo, Production-Ready Face Re-Aging for Visual Effects. ACM Transactions on Graphics, 41(6):112, 2022. TLDR: This paper presents the first practical, fully-automatic and production-ready method for re-aging faces in video images, and demonstrates how the simple U-Net allows for unprecedented temporal stability and preservation of facial identity across variable expressions, viewpoints, and lighting conditions. 3, 5 MyTimeMachine: Personalized Facial Age Transformation"
        },
        {
            "title": "Supplementary Material",
            "content": "Along with this supplemental PDF, we provide additional visual materials (e.g., images and videos) in an HTML file, accessible via index.html . We highly recommend readers to refer to the accompanying videos for comprehensive examination of the visual outcomes. A. Overview of Appendices Our appendices contain the following additional details: Sec. provides an overview of our data preprocessing pipeline, the curated dataset (summarized in Table 3), and additional details about the celebrities used in experiments Sec. 4.2. Sec. provides implementation details of our personalized adapter network, including hyperparameters and training configurations. Sec. presents benchmarking results against other pretrained aging methods, with qualitative results shown in Fig. 12. Sec. includes benchmarking results against alternative naive personalization techniques, with both quantitative and qualitative results displayed in Fig. 9 and Fig. 10. Sec. explains our choice of using StyleGAN2s aging encoder for personalization over encoder-decoder GAN models or diffusion models. Sec. discusses the design rationale behind our video reaging pipeline. B. Dataset Curation Existing in-the-wild aging datasets [29, 44, 68] lack longitudinal data for individual subjects, as they do not offer multiple high-quality images of the same person over several decades. To address this limitation, we collected new dataset as summarized in Table 3. To better illustrate the age distributions, we count the number of images within the age ranges 2040, 4060, and 6080, and report these numbers in the table. These ranges differ from the training ranges of 2040, 5070, and 3070. For each celebrity, we first gathered facial images, then enhanced older images to improve visual quality, compensating for the limitations of earlier camera technology and image processing methods. Following [60], we restored grayscale or low-quality images to ensure more consistent and enhanced visual representation over time. Faces were then cropped and aligned according to the FFHQ [22] standard. While downloading publicly available images, we extracted metadata, such as the time of capture, to calculate each subjects age. For re-aging tasks in Sec. 4.2, the available age distribution of the collected celebrities varies; for instance, some celebrities have fewer than 50 images in the 20 to 40 age range. Therefore, we conduct age regression tasks for the"
        },
        {
            "title": "Age range",
            "content": "2040 4060 6080 Al Pacino Charles III Elizabeth II Robert De Niro Jennifer Aniston Oprah Winfrey Morgan Freeman Jackie Chan Chow Yun-fat Elaine Chao Diego Maradona Margaret Thatcher 2184 0176 0396 2781 0255 2470 2087 2170 2068 1671 1760 2087 89 219 65 121 375 163 4 31 91 14 165 56 409 116 340 322 529 136 444 109 117 301 270 198 530 539 286 - 315 290 201 60 123 - 268 Table 3. longitudinal facial aging dataset featuring images of 12 celebrities. The number of images for each celebrity is reported across different age ranges. Unless stated otherwise, 50 images are selected for training. following 10 celebrities: Al Pacino, Charles III, Elizabeth II, Robert De Niro, Oprah Winfrey, Morgan Freeman, Jackie Chan, Chow Yun-fat, Elaine Chao, and Margaret Thatcher, and age progression tasks for the following 8 celebrities: Al Pacino, Charles III, Elizabeth II, Jennifer Aniston, Oprah Winfrey, Chow Yun-fat, Diego Maradona, and Margaret Thatcher. These results also correspond to the number of pairs used in user studies, as discussed in Sec. 4.2. For dataset size ablation studies in Sec. 4.4, we use the same celebrities selected in the regression task. C. Implementation Details Personalized Age Adapter Network. Inspired by [3, 30, 40], our adapter network is built on multi-layer perceptron (MLP) architecture that takes as input the latent vector + tgt and the target age (atgt), and outputs the offset vector + tgt. Specifically, the 18 512 dimensional latent code + tgt is first processed through Global MLP, which produces down-sampled global representation Wglobal of dimension 18 32, flattened to 1 512. Next, we design an Aging MLP that takes the scalar target age as input and generates 1 16 dimensional age feature, atgt-feat. We then train 18 independent Style MLPs, each operating on one of the [1, 18] styles in the W+ space, to produce an offset vector for each style, + tgt(k). Each Style MLP receives the 1 512 dimensional age-transformed latent code from SAM, + tgt(k), the 1 512 dimensional global representation Wglobal, and the target aging feature atgt-feat, and then outputs the per-style offset code + tgt(k). Both the Global, Aging, and 18 Style MLPs are designed as 2layer neural networks with ReLU activation. This architecture enables the network to subtly and effectively adjust the latent representation, preserving the individuals identity while incorporating personalized aging characteristics. For each celebrity, we train our adapter network on GPU A6000 for 10,000 iterations, which takes approximately 4 hours. We inherit SAMs hyperparameters, including its original loss weights. Additionally, we set λpers-age = 1 for Eq. 5, λreg-extra = 1 for Eq. 6, and λreg = 1 for Eq. 7. D. Comparison with SOTA Methods without"
        },
        {
            "title": "Personalization",
            "content": "As discussed in Sec. 4.2, we benchmark our approach against all other pre-trained baselines, including SAM, CUSP, AgeTransGAN, and FADING, as shown in Fig. 12. For baseline methods like CUSP and AgeTransGAN, which utilize pre-defined age groups based on FFHQ-Aging [39], we interpolate between these age groups to demonstrate continuous aging, following the approach used by SAM [2]. E. Comparison with Naive Personalization Techniques. We perform additional ablation studies using alternative personalization approaches on data for Al Pacino aged 30 70 years, with results shown in Fig. 9. SAM Pers. f.t. behaves similarly to the pre-trained SAM, as the latent codes are far from the latent center, limiting its editing capabilities. This aligns with the inversion-editability trade-off discussed in Sec. 3.3. SAM Pers. ft. + MyStyle [36] first personalizes the SAM encoder, then tunes the decoder following the PTI pipeline [46]. However, this introduces significant artifacts due to changes in the latent distribution, which diverges from the pre-trained StyleGAN2 distribution. In SAM, global aging knowledge is learned with fixed StyleGAN2 decoder, and modifying decoder weights distorts the latent space distribution, compromising the aging knowledge and introducing decoding artifacts. FADING + Dreambooth [49] overfits the aging results to the input image, especially when the target age lies outside the training age range. Additionally, this approach neglects age-related facial shape transformations, such as toddlers rounder face or proportional changes in facial features over time, which are caused by NTI + p2p as discussed in Sec. 2. Additionally, as discussed in Sec. 4.2, we show visual comparisons against other naive personalization methods for age progression in Fig. 10. F. Why Personalizing the Encoder SAM? Finetuning encoder-decoder GAN with limited personal data often leads to overfitting, mode collapse [1], and data drift [26, 31], preventing the model from generalizing to unseen test images of an individual [41]. Therefore, encoderdecoder GAN structures, like AgeTransGAN, necessitate substantial amount of paired data to achieve effective personalization in aging transformations. For instance, personalizing the appearance of celebrity such as Al Pacino Experiment SAM SAM Pers. ft. SAM Pers. ft. + MyStyle FADING + Dreambooth Ours (3070) IDsim() 0. 0.49 0.60 0.64 0.66 Figure 9. We compare MyTM (Ours) with naive personalization techniques: SAM Pers. ft., SAM Pers. ft. + MyStyle, and FADING + Dreambooth, trained on ages 3070 and tested within the same age range for Al Pacino. While SAM Pers. ft. + MyStyle achieves high IDsim score, it suffers from poor visual quality, resulting in adversarial examples for ArcFace. would necessitate images of him at both ages 20 and 70, with consistent pose, lighting, and expression. However, acquiring such data in real-world conditions is extremely challenging, as it demands rare and specific longitudinal images that capture individuals across wide age span under controlled settings. This limitation makes encoder-decoder GANs less practical for applications where personalized aging transformations are desired. For diffusion models, there are several limitations in reaging tasks: (1) They lack the latent space, which enables fine-grained continuous aging control and editing [11]. (2) Models like FADING, which use NTI + p2p for age editing, often struggle with the trade-off between inversion accuracy and editability [48]. Additionally, FADING frequently produces unstable results, as shown in Fig. 11, which we attribute to the unstable NTI optimization [48, 69]. New stable optimization-free methods could be explored for diffusion models in the future. (3) VQ auto-encoders, commonly used in diffusion to encode images, can introduce artifacts, particularly in the human face domain [34]. These issues highlight the need for an alternative approach, such as utilizing StyleGAN2s well-trained latent space and optimization-free e4e encoder [57], to achieve high-quality, artifact-free re-aging transformations. 2 Figure 10. Performance of personalized age transformation techniques for age progression, where an input test image is transformed to all target ages between 0 and 100. MyTM (Ours) is trained on 20 years of data (ages 2040). The age range included in the personal training data is highlighted in red. We also provide an example image of the same person within 3 years of the target age as reference. Figure 11. Visual results of FADING using identical input and inference code. The instability in age transformation arises from the optimization of NTI [34], leading to inconsistencies. cant style differences between the source and target faces can cause artifacts like flickering, particularly in real-world video scenarios [43]. G. Why Not Use Reference Image for FaceSwapping? Firstly, obtaining images of person at any arbitrary age is often challenging, particularly high-quality images comparable to our synthesized faces at 1024x1024 resolution. Even if reference images at the target age are available, face-swapping techniques [6, 13, 63] generally yields optimal results when the source and target faces share similar styles, such as pose, expression, and lighting. Signifi3 Figure 12. Performance of age transformation techniques for age regression (top) and age progression (bottom). The input test images match those in Fig. 4 and Fig. 10 for consistency. For age regression, MyTM (Ours) is trained across 40-year range (ages 30 to 70), while for age progression, it is trained over 20-year range (ages 20 to 40). Personalized training data age ranges are marked in red. reference image of the same person, taken within three years of the target age, is included for comparison. 4 Figure 13. Limitations of MyTM. Our method may struggle with accessories (e.g., glasses), as these elements are not consistently handled by the e4e encoder [57]."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park",
        "University of North Carolina at Chapel Hill"
    ]
}