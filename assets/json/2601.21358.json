{
    "paper_title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
    "authors": [
        "Jiecong Wang",
        "Hao Peng",
        "Chunyang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search."
        },
        {
            "title": "Start",
            "content": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization Jiecong Wang1, Hao Peng1, Chunyang Liu2 1Beihang University, 2Didi Chuxing {jcwang, penghao}@buaa.edu.cn, liuchunyang@didiglobal.com 6 2 0 2 9 2 ] A . [ 1 8 5 3 1 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as deterministic trajectory of latent planning states, while separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns robust, broader solution space, offering transparent and scalable foundation for inference-time search."
        },
        {
            "title": "Introduction",
            "content": "Chain-of-Thought (CoT) reasoning (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2023; Zhang et al., 2023) has revolutionized the landscape of Large Language Models (LLMs) by decomposing intractable problems into sequences of intermediate steps (Zhou et al., 2023). This paradigm has unlocked impressive capabilities across complex domains, serving as the backbone for modern applications ranging from code generation (Chen, 2021; Chen et al., 2023; Li et al., 2025a; Liu et al., 2025) 1 Figure 1: Comparison of PLaT and other reasoning strategies. CoT is an explicit chain-of-thought reasoning method, and the rest are implicit latent reasoning methods. to autonomous agents (Yao et al., 2022; Shinn et al., 2023; Schick et al., 2023). However, it faces fundamental theoretical bottleneck: reasoning path collapse. At every generation step, the model is forced to sample discrete token from the vocabulary, thereby pruning the probability of alternative valid reasoning paths (Yao et al., 2023a; Zhang et al., 2025; Chen et al., 2025). This nature restricts the model from maintaining superposition of multiple potential reasoning strategies in highdimensional space, often leading to irrecoverable errors once suboptimal token is chosen. Additionally, current models incur high costs by generating prohibitively long sequences of intermediate tokens (Zhang et al., 2025; Sui et al., 2025; Wang et al., 2025; Feng et al., 2025). To mitigate the inefficiency, recent works have explored latent reasoning, where the model evolves hidden states internally before outputting final answer (Zhang et al., 2025; Xu et al., 2025; Hao et al., 2025; Shen et al., 2025; Tan et al., 2025). While promising, they predominantly adopt an end-to-end implicit paradigm, optimizing latent states directly for the final generation. This leads to two critical limitations. First, the reasoning process is opaque: the intermediate states function as black boxes that cannot be reliably interpreted. Second, and more critically, these methods rely on fixed number of latent steps during inference. This forces the model to expend the same computational effort regardless of problem difficulty, lacking the flexible nature of human System 2 thinking (Li et al., 2025b). We argue that robust reasoning system should mirror the cognitive distinction between thought and language. From cognitive perspective, language serves merely as low-dimensional projection (interface) of high-dimensional thought; the core reasoning process often occurs implicitly without verbalization (Varley and Siegal, 2000; Fedorenko and Varley, 2016; Coetzee et al., 2022; Fedorenko et al., 2024). Ideally, the brain should maintain superposition of potential reasoning trajectories within continuous latent space, collapsing to discrete decisions only when an interface with the external world (the mouth) is required. Motivated to replicate this implicit process computationally, we draw inspiration from Multi-Token Prediction (MTP) (Stern et al., 2018; Qi et al., 2020; Gloeckle et al., 2024; Nagarajan et al., 2025; Samragh et al., 2025) (that transformer hidden states are able to encode information about future tokens before they are generated) and propose to model reasoning as sequence of latent planning states. In this work, we introduce PLaT (Planning with Latent Thoughts), framework that fundamentally decouples the reasoning process from verbalization. Our architecture comprises two distinct components: latent Planner and Decoder for verbalization. The Planner autoregressively evolves trajectory of states in high-dimensional continuous manifold, maintaining probabilistic density over multiple logical possibilities until decision is required. The Decoder grounds these latent plans onto the language space via reconstruction objective. This empowers PLaT with dynamic termination of latent planning and intermediate interpretability of latent states, unlike prior methods that use fixed number of latent steps. Our empirical evaluations on mathematical benchmarks reveal distinctive behavioral pattern. We identify trade-off between greedy precision and exploration potential: while PLaT achieves lower greedy accuracy compared to baselines, it exhibits superior scalability in reasoning diversity. PLaT outperforms baselines in Pass@k metrics with steeper scaling slope, indicating that it learns broader 2 solution space rather than overfitting to narrow trajectory. Furthermore, the latent states in PLaT can be decoded into text for interpretability without disrupting the continuous reasoning flow. Our main contributions are summarized as follows: We reformulate latent reasoning as planning over latent space, shifting from implicit pattern matching to planning in continuous space. We introduce decoupled Planner-Decoder architecture that separates latent reasoning from language generation. This design naturally enables interpretable intermediate reasoning and dynamic inference termination. Experiments show superior Pass@k scaling and reduced diversity saturation under searchbased inference, framing distinct trade-off between greedy precision and exploration potential."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Chain-of-thought Reasoning Chain-of-Thought (CoT) prompting enables large language models to solve complex problems by explicitly decomposing them into intermediate reasoning steps (Wei et al., 2022). Subsequent work shows that CoT can be activated with minimal prompting by single instruction such as Lets think step by step (Kojima et al., 2022). Beyond simple CoT strategies, self-consistency (Wang et al., 2023) samples multiple CoT trajectories and marginalizes over final answers, improving robustness by approximating distribution over reasoning paths. Tree-of-Thought (ToT) (Yao et al., 2023a; Long, 2023; Mo and Xin, 2024) generalizes CoT into an explicit tree search, allowing the model to branch, evaluate, and backtrack over intermediate states. Graph-of-Thought (GoT) (Yao et al., 2023b; Besta et al., 2024; Yao et al., 2024) further extends this paradigm to graph-structured reasoning, enabling the reuse and recombination of intermediate conclusions across different branches. Orthogonal to structured search, other works focus on refining intermediate reasoning steps through iterative revision or verification. These approaches improve correctness by critiquing, editing, or validating generated reasoning traces, often in multipass manner (Madaan et al., 2023; Lyu et al., 2023; Yang et al., 2023). Figure 2: Framework of the proposed PLaT paradigm. (1) SFT Stage: the Planner autoregressively steps forward to generate the latent states in the context of the question. The Decoder then utilizes the projected latent states as the prefix to verbalize them. (2) RL Stage: the Decoder decodes the same states with sampling strategy to roll out different results. Equations that are valid in the corresponding reasoning process and correct answers reinforce the Decoder as policy. 2.2 Latent Reasoning"
        },
        {
            "title": "3 Method",
            "content": "Recent latent reasoning methods aim to reduce the cost and path collapse of explicit CoT by shifting part of the reasoning process into continuous hidden states, while retaining the final answer in natural language. Early approaches explore partial internalization of reasoning, where additional nonsemantic structures are introduced during inference. For example, pause tokens or planning tokens allow models to suspend surface-level generation and perform internal processing before producing the next reasoning step (Goyal et al., 2024; Wang et al., 2024). second line of work focuses on compressing explicit CoT into latent representations, progressively removing or softening intermediate textual steps. Curriculum-based approaches such as Coconut gradually replace explicit reasoning tokens with continuous latent states, enabling the model to internalize multi-step reasoning (Hao et al., 2025). Related techniques distill explicit CoT trajectories into latent spaces or employ continuous vectors for intermediate reasoning steps (Zhang et al., 2025; Shen et al., 2025). Other approaches further explore latent compression objectives to stabilize and optimize implicit reasoning processes (Tan et al., 2025). While these methods significantly improve efficiency, they typically treat latent states as end-to-end optimized carriers of reasoning, offering limited interpretability of intermediate plans. 3 In this section, we first formalize reasoning as latent autoregressive process. We then detail the PLaT architecture, the training via reconstruction, the efficient Lazy Decoding strategy, and the policy refinement via reinforcement learning. All notations are listed in Appendix A. 3.1 Problem Formulation: Reasoning as Latent Planning Standard CoT models the probability of reasoning chain = (y1, . . . , yT ) autoregressively in the discrete token space: p(yx) = (cid:89) k=1 p(yky<k, x) (1) This formulation enforces reasoning path collapse at every step k, as the model must commit to specific tokens and potentially prune other reasoning paths. PLaT introduces sequence of continuous latent variables, which map each textual step yk to sequence of NL latent planning states. Let Sk = (sk,1, . . . , sk,NL) denote the raw latent trajectory corresponding to the k-th reasoning step. Let Hk,i = {S<k, sk,<i, x} be the causal history. The joint distribution is factorized as: p(y, Sx) = p(s1,1x) (cid:125) (cid:124) (cid:123)(cid:122) Encoder (cid:89) k= p(SkHk,1) (cid:123)(cid:122) (cid:125) Planner (cid:124) p(ykSk) (cid:125) (cid:123)(cid:122) (cid:124) Decoder (2) where p(SkHk,1) = (cid:81)NL i=I(k=1)+1 p(sk,iHk,i) represents the latent reasoning process at step k, and I(k = 1) is the indicator function that equals 1 when = 1 and 0 otherwise. Here, the Planner operates at fine-grained resolution, evolving raw states unaffected by the aggregation. Then the Decoder aggregates these states to verbalize the coarse-grained reasoning step yk (detailed in Section 3.2). 3.2 Architecture The PLaT architecture implements the above formulation through two distinct modules: the Planner and the Decoder. They interact via dedicated linear projectors (ϕEnc, ϕH2L, ϕL2H, ϕDec) bridging the LLM backbone dimension (Rdm) and the latent dimension (Rds). Planner. The Planner is responsible for evolving the reasoning trajectory autoregressively on the latent manifold. First, to initialize the trajectory, an encoder projector ϕEnc maps the hidden state of the input question (at the special token tenc) to the 1. Then, at each step k, the Planinitial state s1,1 ner predicts the next planning state based on the history. The latent history {S<k, sk,<i} is mapped to the model dimension via ϕL2H and fed into the backbone M. delimiter token tplan separates the text context from the latent states: hnext = M([x tplanϕL2H(S<k), . . . , ϕL2H(sk,<i)])1 (3) Then the next state is gained by sk,i = ϕH2L(hnext). It is important to note that the Planner generates deterministic vectors, unlike previous methods that sample from distribution for RL training (Tan et al., 2025). Decoder. To stabilize the planning trajectory and synthesize information from the NL micro-steps, we introduce an Exponential Moving Average (EMA) mechanism. We maintain NL independent aggregators. For the i-th slot in step k, the aggregator ak,i is updated as: ak,i = αEM sk,i + (1 αEM A) ak1,i 1We empirically found that using separate ϕEnc rather than sharing weights with the Planner projector ϕH2L yields superior performance. It is likely due to the distinct distributional properties of the initial context versus intermediate reasoning states. where αEM [0, 1] is the smoothing coefficient (set to 1 for the first step). This mechanism acts as temporal memory, allowing the i-th latent slot to aggregate information specifically from the i-th channel of previous steps. The final planning state for step is the concatenation of these stabilized aggregators: Sk = [ak,1, . . . , ak,NL]. The Decoder serves as the interface to the textual world. It takes the aggregated state Sk as input. Sk is projected via ϕDec and acts as soft prefix for generating the text segment yk: (ykSk) = yk (cid:89) j=1 PM(yk,jyk,<j, [ϕDec(Sk); tdec]) The Decoder strictly conditions only on the current aggregated state Sk. This bottleneck forces the Planner and aggregators to encapsulate all necessary historical context into Sk, ensuring semantic completeness. 3.3 Supervised Training via Reconstruction During Supervised Fine-Tuning (SFT), we optimize the entire pipeline end-to-end using reconstruction loss. The loss is calculated as the crossentropy between the ground-truth text yk and the models prediction conditioned on the state Sk: LSFT = (cid:88) yk (cid:88) k=1 j=1 log (yk,jSk, yk,<j). (4) This formulation treats intermediate reasoning steps and the final answer uniformly within the latent space, eliminating the need for modeswitching mechanisms and the constraint of fixed number of latent steps. To improve the robustness of the Decoder and force it to learn the manifold structure rather than memorizing point-wise mappings, we inject Gaussian Noise into the accumulated states during training: ϵnoise (0, σ2). 3.4 Efficient Inference via Lazy Decoding The decoupling of latent reasoning and verbalization enables highly efficient inference protocol, which we term Lazy Decoding. Since the Planner operates in the latent space, we can generate the trajectory of states (s1,1, s1,2, . . . ) without generating full text. To determine when to terminate reasoning or output the answer, we do not need to fully decode each Sk. We perform semantic probe by 4 decoding only the first token (greedy decoding for example): ˆyk,1 = argmaxvV (vϕDec(Sk), tdec). The inference logic proceeds as follows: (1) If ˆyk,1 = tans: The model is in an intermediate reasoning stage. We discard the token and proceed to generate the next latent state. (2) If ˆyk,1 = tans: The model has reached the conclusion. We pause the reasoning and fully decode the final answer from Sk. This strategy significantly reduces computational overhead, as the costly token-by-token generation is skipped for all intermediate steps. It still retains the ability to inspect the reasoning chain on demand for interpretability. 3.5 Policy Refinement via Reinforcement Learning While SFT establishes the capability for latent planning, we employ Reinforcement Learning (RL) to refine the search policy. key theoretical advantage of our framework is the decoupling of planning stability from exploration, since the latent states are deterministic, and exploration is induced during the verbalization phase. We freeze all Planner parameters to maintain the structural integrity of the learned latent manifold and optimize only the Decoder parameters. This constraint ensures that the underlying reasoning topology remains stable, preventing the RL process from distorting the semantic consistency of the latent space, while focusing solely on refining the decoding policy. Decoupled GRPO. For given question x, the Planner generates deterministic latent trajectory {S1, . . . , ST }. Diversity is introduced by enabling temperature sampling in the Decoder. From the same fixed latent states Sk, the Decoder explores different verbalization paths {y(i) i=1. We employ Group Relative Policy Optimization (GRPO) objective. Let πθ denote the policy of the Decoder. The objective is to maximize: }G (θ) = (cid:34) 1 (cid:88) i=1 min (ri(θ)Ai, clip(ri(θ), ϵ)Ai) (cid:35) (5) πθold is the probability ratio, where ri(θ) = πθ(y(i) Sk) (y(i) Sk) and ϵ is the clipping hyperparameter. The advantage Ai is computed by normalizing the rewards within each group: Ai = (Ri R)/σR, where and σR are the mean and standard deviation of rewards {Rj}G j=1 sampled from the same state. Each Ri is assigned based on answer correctness and format validity (detailed in Appendix B.1). This objective allows the model to explore the superposition of meanings within the fixed Sk and converge onto the verbalization that maximizes the likelihood of correct solution."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup. Model. Following the experimental protocols established in recent latent reasoning research (Hao et al., 2025; Shen et al., 2025), we employed GPT2 (small) (Radford et al., 2019) as our backbone LLM. This choice was primarily made to ensure strictly fair comparison with state-of-the-art latent baselines that utilize this specific architecture. Datasets. Our training was conducted on GSM8k-Aug (Deng et al., 2023), an augmented version of the GSM8k dataset (Cobbe et al., 2021) where the chains-of-thought are formatted as equations generated by GPT-4 (Achiam et al., 2023). The data is structured as follows: Question Step1 Step2 Answer, which naturally supports the segmentation of latent planning steps. To evaluate the generalization and robustness of PLaT, we further test the trained models on three outof-distribution (OOD) benchmarks: GSM-HARD (Gao et al., 2022), SVAMP (Patel et al., 2021), and MultiArith (Roy and Roth, 2015). Baselines. We benchmark PLaT against three representative paradigms: (1) CoT-SFT (Wei et al., 2022): Fine-tunes the model with reasoning chains, and the trained models generate step-by-step during inference. (2) Coconut (Hao et al., 2025): curriculum-based latent reasoning method that progressively replaces explicit tokens with implicit hidden states. (3) CODI (Shen et al., 2025): latent reasoning framework that employs hiddenstate distillation from an explicit reasoning teacher. We excluded CoLaR (Tan et al., 2025) from our comparison because the authors reported that its latent compression mechanism is ineffective on smaller models like GPT-2, limitation confirmed by our internal replication attempts 2. Evaluation. We evaluate model performance in two dimensions: (1) Greedy Accuracy: The correctness of the most probable output under greedy 2https://github.com/xiaomi-research/colar/issues/ 5 Figure 3: Scaling properties of reasoning diversity across datasets. PLaT-1 and PLaT-2 are the results of PLaT when NL = 1 and NL = 2, respectively. decoding. This measures the models exploitation capability on its primary reasoning path. (2) Pass@k (k = 32, 64, 128): The probability that at least one of sampled reasoning chains yields the correct answer. Pass@k serves as critical metric for exploration capability, reflecting the quality and diversity of the solution space learned in the latent manifold. Implementation Details. The Planner and Decoder share the backbone parameters of M. To increase the Planners capacity for planning, we append two additional transformer layers at the output of the backbone. In the SFT stage, PLaT is initialized from CoT-SFT checkpoint. We finetune for 25 epochs with learning rate of 5e-4 and latent dimension ds = 2048. More implementation details can be found in Appendix B.1. 4.2 Effectiveness Performance of Supervised Fine-Tuning. We fine-tuned PLaT on GSM8k with latent states NL = 1 and NL = 2. We evaluated the models on the in-domain test set and three OOD datasets. Figure 3 illustrates the performance scaling. distinct crossover phenomenon is observed. In greedy decoding, PLaT generally underperforms compared to Coconut and CODI. However, in terms of diversity scaling (Pass@k), PLaT exhibits steeper upward slope. On Pass@128, PLaT surpasses both Coconut and CODI across GSM8k, GSM-HARD, and SVAMP. For instance, on GSM8k Pass@128, PLaT-2 reaches 74.2%, outperforming Coconut (66.7%) and CODI (70.1%) by substantial margin. This indicates that PLaTs latent space supports efficient sampling of diverse answers, whereas baselines show signs of saturation (flattening curves) at higher k. PLaT-2 achieves higher diversity (Pass@128) 6 Figure 4: Impact of Reinforcement Learning on PLaT performance. We report results on GSM8k (indomain) and three OOD datasets. than PLaT-1 on the in-domain GSM8k (74.2% vs 72.8%) and MultiArith. However, on OOD datasets SVAMP and GSM-HARD, PLaT-1 performs slightly better or comparably. This suggests that while increasing NL increases theoretical capacity, it may also introduce optimization challenges or overfitting to the source domain. Explicit CoT remains the performance upper bound. While PLaT improves over latent baselines, notable gap persists between PLaT and CoT. This confirms that mapping reasoning to compressed latent space inevitably incurs information loss compared to full-text reasoning, though PLaT minimizes this loss with the large sampling budget. Impact of Reinforcement Learning. Figure 4 reports the results after applying GRPO on the SFT checkpoints. RL training leads to consistent improvement in greedy accuracy on GSM8k but decrease in Pass@128. This confirms that the RL signal successfully collapses the high-entropy planning distribution towards high-likelihood correct trajectories. While in-domain (GSM8k) greedy performance improves, we observe performance degradation on OOD tasks (SVAMP, MultiArith) after RL. This suggests that the policy overfits to the reward signal of the training domain, common behavior in RL that highlights the need for multi-task reward modeling in future work. Although RL improves accuracy on the indomain task, the gain is relatively marginal (around 1%). We attribute this limitation to the parameter bottleneck of the GPT-2 Small backbone. The model likely lacks sufficient parameter space to disentangle complex reasoning boundaries required for high greedy precision. We hypothesize that scaling up the backbone in future work would raise this capacity ceiling, allowing RL to yield more significant accuracy gains. 4.3 Efficiency Table 1 presents the computational efficiency comparison. PLaT achieves significant speedup compared to Explicit CoT. PLaT-1 (152.6ms) reduces inference latency by approximately 56% compared to CoT (349.6ms) by skipping intermediate token generation. PLaT incurs moderate latency overhead compared to Coconut (100.6ms) and is faster compared to CODI (240.0ms). The overhead stems from the additional forward passes required by the Decoder to check for termination. Although Table 1: Efficiency comparison. We measure the number of forward passes (Fwd.) and average inference time per question. The Fwd. values of PLaT are reported in the form of Planner forward passes + Decoder forward passes. Method CoT Coconut CODI PLaT-1 PLaT-2 Fwd. 25. 6.00 6.00 Time (ms) 349.68.9 100.63.2 240.017. 4.00+4.00 7.90+3.95 152.614.3 206.46.5 Figure 5: Evolution of exploration during reasoning. PLaT maintains consistently higher branching factor throughout the reasoning process, evidencing active exploration. Crucially, the number of semantically valid branches in PLaT converges with or surpasses CoT in later stages. Figure 6: Scatter plot of Branching Factor vs. Valid Step Count for PLaT and CoT-SFT. Samples without intermediate steps are excluded. The figures are segmented into 16 zones. The number of samples in each zone and the difference relative to the other method are annotated in the upper-right corner of each zone. PLaT is not the fastest latent method, it offers interpretability. Unlike Coconut, which is opaque, PLaT allows for on-demand inspection of intermediate states. The efficiency results indicate that PLaT provides favorable balance, delivering transparent, high-diversity reasoning at speed significantly faster than standard CoT. 4.4 Analysis of Latent States key advantage of PLaT is the interpretability of its intermediate latent states, feature absent in previous methods like Coconut and CODI. This allows us to analyze the reasoning topology. We compared the branching characteristics of PLaT against explicit CoT on the GSM8k test set (we did not use the OOD datasets because they do not contain intermediate-step annotations for reference). For both methods, we sampled 10 reasoning paths per question (temperature=0.9) and analyzed them. We employed GPT-4o-mini (Hurst et al., 2024) to cluster semantically distinct reason7 ing steps and verify their logical validity (prompt details are in Appendix C). To ensure evaluation reliability, we manually verified the agreement between human judges and the LLMs judgment (detailed in Appendix B.2). Evolution of Reasoning Diversity. We define the Branch Count as the number of semantically unique reasoning steps generated from the same latent states across all samples. Figure 5 (a) visualizes the evolution of this metric over normalized reasoning progress. Both CoT and PLaT exhibit an inverted-U pattern, where the search space initially expands and then narrows. PLaT maintains consistently higher average branching factor (offset by +1.0) compared to CoT throughout the process. Raw diversity is insufficient unless the generated paths are logically sound. Figure 5 (b) tracks the Valid Branch Count. PLaT starts with lower count of valid branches than CoT, but its count decays more slowly and eventually surpasses CoT. This suggests that PLaT retains broader range of potential paths deep within the reasoning process. We also provide an entropy analysis in Appendix B.3. Distribution of Exploration vs. Exploitation. Figure 6 presents more fine-grained scatter plot of Branch Count vs. Valid Step Count for individual problem instances. CoT samples are heavily concentrated in the top-left quadrant (low branching, high validity) and tend to aggressively prune search paths, collapsing into single, often valid trajectory. In contrast, PLaT shifts the density toward the center-right (high branching and moderate validity) and accumulates more, but still valid, samples (e.g., +77, +71, +59) than CoT. This distribution shift statistically proves that PLaT prioritizes the coverage of the solution space (recall) over the precision of single trajectory (precision). This characteristic makes PLaT particularly suitable as generator for search-based inference algorithms (e.g., Tree-ofThoughts or rejection sampling), where diversity is the bottleneck. 4.5 Ablation Study We conducted ablation studies to validate our architectural design choices. Results are reported in Table 2. Contextualization (w/o context): Initializing the reasoning without attending to the full question context ([x; tdyn]) leads to the most significant drop in greedy accuracy (28.66% 21.30%). Table 2: Ablation study on architectural components and training strategies. We evaluate the contribution of context injection, EMA aggregation, denoising, and parameter sharing strategies on GSM8k. Greedy accuracy is deterministic under different seeds. Method Acc. Pass@128 PLaT - w/o context - w/o EMA - w/o denoising 28.660.00 21.300.00 26.460.00 26.990.00 74.160.74 74.680.82 72.720.68 71.421.01 Residual 23.810. 68.920.42 Indep. Decoder 26.910.00 74.390.64 However, interestingly, this setting yields the highest Pass@128, suggesting that reduced contextual constraints may inadvertently encourage wilder exploration at the cost of precision. State Aggregation (w/o EMA) & Noise (w/o denoising): Removing EMA or training noise both degrade performance across all metrics, confirming their roles in stabilizing the trajectory and smoothing the manifold. Architectural Variants: The Residual variant (adding previous state to current before decoding instead of employing EMA) performs worst in exploration and second worst in greedy accuracy. The Independent Decoder (untied parameters) achieves competitive Pass@128 but lower greedy accuracy, suggesting that parameter sharing effectively regularizes the latent space. 4.6 Analysis of Hyperparameters We performed an analysis over the latent sequence length (NL), EMA coefficient (αEMA), and latent dimension (ds). Detailed sensitivity plots and analyses are provided in Appendix B.4. We observed that: (1) NL = 2 achieves superior balance between greedy precision and diversity compared to NL = 1, whereas longer trajectories (NL > 2) lead to optimization degradation. (2) αEMA = 0.5 for NL = 2 yields the best results, and longer latent chains require stronger smoothing to filter high-frequency noise. (3) We fixed ds = 2048 with the most robust performance. Further increasing dimensions yielded negligible gains while raising computational costs. 8 Figure 7: Visualization of PLaTs reasoning process. GT, Greedy, and Sample are the ground truth reasoning steps in the dataset, the greedy decoding results of each group of states, and the sampling results of each group of states, respectively. The green boxes indicate the equations or answers in them are valid, while the red ones indicate invalidity. Each group of states can be decoded into various equations or answers. 4.7 Case Study To validate our hypothesis that PLaT learns reasoning search space rather than memorizing single path, we visualize the decoding tree of an error case in Figure 7. As shown in the second row, the greedy decoding path fails: the model correctly identifies the 2nd months downloads (180) but deviates in the 3rd steps calculation. In standard explicit CoT model, this error would be irreversible due to probability collapse. We observe diverse set of branched reasoning paths by sampling (Sample). Notably, the latent states encode superposition of strategies: For instance, in the second step, the model simultaneously considers calculating the reduction via decimals (180*0.7), fractions (30/100), or direct subtraction (180-30/100*180). Although the greedy result led to an error, the correct logical paths are preserved within the same latent state. This confirms that the information about correct paths is encoded in the planning states, but that it generates incorrect greedy results due to pattern failure in verbalization."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced PLaT, framework that fundamentally reformulates latent reasoning by decoupling the reasoning of latent thought from the process of verbalization. Unlike prior blackbox approaches that treat latent states as mere compression artifacts for end-to-end prediction, PLaT enforces glass-box paradigm where latent states are modeled as explorable planning trajectories anchored to the language manifold. This structural shift brings two pivotal advancements. First, it enables dynamic termination of the latent reasoning process without relying on static hyperparameters. Second, our empirical findings reveal critical precision-diversity trade-off. While baseline methods achieve higher greedy accuracy, they suffer from saturation in diversity. In contrast, PLaT sacrifices some deterministic accuracy and demonstrates superior scalability in reasoning diversity. This suggests that PLaT effectively learns broad, explorable solution manifold rather than narrow memorized path. By providing transparent architecture where internal thoughts are continuous, explorable, dynamic, and interpretable, PLaT offers robust foundation for future System 2 reasoning systems. It shifts the focus from memorizing golden traces to learning generalizable search spaces, paving the way for advanced inference-time scaling and search-based reinforcement learning."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Pi9 otr Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 1768217690. Mark Chen. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research. Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. 2025. Reasoning beyond language: comprehensive survey on laarXiv preprint tent chain-of-thought reasoning. arXiv:2505.16782. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. John Coetzee, Micah Johnson, Youngzie Lee, Allan Wu, Marco Iacoboni, and Martin Monti. 2022. Dissociating language and thought in human reasoning. Brain Sciences, 13(1):67. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. 2023. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460. Evelina Fedorenko, Steven Piantadosi, and Edward AF Gibson. 2024. Language is primarily tool for communication rather than thought. Nature, 630(8017):575586. Evelina Fedorenko and Rosemary Varley. 2016. Language and thought are not the same thing: evidence from neuroimaging and neurological patients. Annals of the New York Academy of Sciences, 1369(1):132 153. Sicheng Feng, Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2025. Efficient reasoning models: survey. arXiv preprint arXiv:2504.10903. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Roziere, David Lopez-Paz, and Gabriel Synnaeve. 2024. Better & faster large language models via multi-token prediction. In Forty-first International Conference on Machine Learning. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. 2024. Think before you speak: Training lanIn The Twelfth guage models with pause tokens. International Conference on Learning Representations. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2025. Training large language models to reason in continuous latent space. In Second Conference on Language Modeling. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Richard Landis and Gary Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159174. Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2025a. Structured chain-of-thought prompting for code generation. ACM Transactions on Software Engineering and Methodology, 34(2):123. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. 2025b. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419. Ren-Biao Liu, Anqi Li, Chaoding Yang, Hui Sun, and Ming Li. 2025. Revisiting chain-of-thought in code generation: Do language models need to learn reasoning before coding? In Forty-second International Conference on Machine Learning. Jieyi Long. 2023. Large language model guided tree-ofthought. arXiv preprint arXiv:2305.08291. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. In The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLPAACL 2023). 10 Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594. Shentong Mo and Miao Xin. 2024. Tree of uncertain thoughts reasoning for large language models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1274212746. IEEE. Vaishnavh Nagarajan, Chen Henry Wu, Charles Ding, and Aditi Raghunathan. 2025. Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction. In Forty-second International Conference on Machine Learning. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20802094, Online. Association for Computational Linguistics. Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. 2020. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training. arXiv preprint arXiv:2001.04063. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 17431752. Mohammad Samragh, Arnav Kundu, David Harrison, Kumari Nishu, Devang Naik, Minsik Cho, and Mehrdad Farajtabar. 2025. Your llm knows the future: Uncovering its multi-token prediction potential. arXiv preprint arXiv:2507.11851. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. 2025. Codi: Compressing chain-of-thought into continuous space via selfdistillation. arXiv preprint arXiv:2502.21074. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, et al. 2025. Stop overthinking: survey on efficient reasonarXiv preprint ing for large language models. arXiv:2503.16419. Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Ruihua Song, and Jian Luan. 2025. Think silently, think fast: Dynamic latent compression of LLM reasoning chains. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Rosemary Varley and Michael Siegal. 2000. Evidence for cognition without grammar from causal reasoning and theory of mindin an agrammatic aphasic patient. Current Biology, 10(12):723726. Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, and Bang Liu. 2025. System-1.5 reasoning: Traversal in language and latent spaces with dynamic shortcuts. Preprint, arXiv:2505.18962. Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. 2024. Guiding language model reasoning with planning tokens. In First Conference on Language Modeling. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. 2025. Softcot: Soft chain-of-thought for efficient reasoning with llms. arXiv preprint arXiv:2502.12134. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. In The Twelfth International Conference on Learning Representations. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language 11 models. In The eleventh international conference on learning representations. Yao Yao, Zuchao Li, and Hai Zhao. 2023b. Beyond chain-of-thought, effective graph-of-thought arXiv preprint reasoning in language models. arXiv:2305.16582. Yao Yao, Zuchao Li, and Hai Zhao. 2024. Got: Effective graph-of-thought reasoning in language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 29012921. Zhen Zhang, Xuehai He, Weixiang Yan, Ao Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, and Xin Eric Wang. 2025. Soft thinking: Unlocking the reasoning potential of llms in continuous concept space. arXiv preprint arXiv:2505.15778. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2023. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations."
        },
        {
            "title": "A Notations",
            "content": "Table 1: Summary of notations and special tokens used in PLaT. Symbol Definition Note Token sequence = (y1, y2, . . . , yT ) Text segment delimited by special tokens {1, . . . , } {1, . . . , NL} Typically ds = dm Number of latent states representing CoT step Rdm Output of Planner, Rds Sequence (sk,1, . . . , sk,NL) Output of EMA Sequence (ak,1, . . . , ak,NL) Rdm Rds (Init: s0) Rdm Rds (Planner Output) Rds Rdm (Planner Input) Rds Rdm (Verbalization) yk dm ds NL αEM sk,i Sk ak,i Sk ϕEnc ϕH2L ϕL2H ϕDec tenc tplan tdec tstep tans Input question sequence Complete reasoning chain The k-th explicit textual step Index of reasoning steps Index of latent aggregator slots Pre-trained LLM backbone Hidden dimension of the backbone Dimension of the latent space Number of reasoning steps Number of latent states The coefficient of EMA Hidden state in LLM backbone Latent state at step k, slot Sequence of latent states at step Aggregated latent state Input of Decoder for step Encoder Projector Hidden-to-Latent Projector Latent-to-Hidden Projector Decoder Projector Appended to Question to extract s0 Delimiter between and s0 Start-of-decoding token Start-of-step delimiter Start-of-answer delimiter"
        },
        {
            "title": "B Extra Information of Experiments",
            "content": "B.1 More Implementation Details General Details We used LoRA (Hu et al., 2022) with rank = 128 and α = 32 (the extra layers of Planner are fully trained). We fine-tuned the model for 25 epochs in the CoT-SFT setting with learning rate of 1e-4 following Coconut (Hao et al., 2025). Standard deviation ϵnoise of the denoising mechanism during training is set to 0.1. All projectors have single linear layer. Results of checkpoints with the highest validation greedy accuracy are reported. The sampling temperature for Pass@k is set to 0.9. All results are averaged across 5 random seeds to ensure statistical significance. Reinforcement Learning To maintain stable latent planning space while refining the verbalization policy, we employed decoupled parameter management strategy. During the SFT phase, shared set of LoRA weights was trained for the backbone. Upon transitioning to the RL phase, we created two distinct instances of these LoRA weights: (1) Frozen Planner LoRA: The LoRA weights associated with the 13 Figure 1: Confusion matrix of Human-LLM agreement. Planner were frozen. This ensures that the latent manifold remains intact, preventing the reasoning logic from collapsing due to reward hacking. (2) Trainable Decoder LoRA: The LoRA weights associated with the Decoder were initialized from the SFT checkpoint and remain trainable. This allows the model to explore different linguistic realizations of the fixed latent plans. This separation ensures that RL optimizes the mouth rather than the brain. To prevent the policy from deviating too far from the initial SFT distribution, we incorporate KL divergence penalty in the objective: LKL = βDKL(πθπref), where πref is the frozen SFT policy. In terms of hyperparameters, the batch size is set to 64, the group size (G) is set to 8, the learning rate is 5 106, the KL coefficient β is 0.01, the sampling temperature is 0.9, and the clip ϵ is 0. Reward Function We utilize rule-based reward function to provide dense supervision for both intermediate reasoning steps and the final answer. The total reward for rollout is determined by its semantic validity and mathematical correctness. For any intermediate latent state Sk that does not signal an answer start, the reward Rstep is calculated based on the decoded equations validity: 1. Equation Presence: If the step yk contains mathematically extractable equation, reward rvalid_eq = 0.2 is granted. 2. Equation Correctness: If the extracted equation is mathematically sound, an additional reward rcorrect_eq = 0.2 is added. For the final step yT signaling the answer, the reward Rans is defined as: 1. Format Validity: An answer is considered valid if numerical result can be successfully extracted and it contains no illegal special tokens. Valid formatting receives rvalid_ans = 0.2. 2. Correctness: If the extracted answer matches the ground truth y, primary reward rcorrect_ans = 1.0 is granted. Otherwise, an incorrect answer may receive small penalty 0.2. B.2 Agreement of LLM Judgments Table 2: Human-LLM Agreement Metrics. Metric Cohens Kappa (κ) Accuracy Value 0.8721 0.9500 14 Figure 2: Evolution of Token Distribution Entropy over Normalized Reasoning Progress. The X-axis represents the relative progress of the reasoning chain generation (0 100%), and the Y-axis represents the entropy of the Decoders output distribution. To validate the reliability of using GPT-4o-mini as an automated evaluator for reasoning step validity, we conducted human verification study. We randomly sampled 200 reasoning steps generated by PLaT and CoT. PhD annotator manually labeled these steps based on mathematical correctness and logical coherence. Appendix Figure 1 and Appendix Table 2 summarize the alignment between Human and LLM judgments. The automated evaluator achieves an overall accuracy of 95.0% and Cohens Kappa (κ) of 0.8721, indicating perfect agreement (Landis and Koch, 1977). As shown in the confusion matrix, the discrepancies (10 samples) are exclusively False Positives (which humans consider invalid and the LLM considers valid). There are zero false negative cases, meaning the LLM never incorrectly rejects valid reasoning step. This suggests that GPT-4o-mini acts as slightly lenient but highly consistent judge. For our analysis of Valid Branching Count in the main text, this leniency implies that our reported values might be slightly upper-bounded. But since the same evaluator is applied to both CoT and PLaT, the relative comparison remains fair and robust. B.3 Entropy Analysis To investigate the internal decision-making process, we analyzed the Shannon entropy of the token distribution at each decoding step. For given latent state Sk at reasoning step k, let (v sk) denote the probability of token being the first token generated by the Decoder. The reasoning entropy of PLaT H(Sk) is defined as: H(sk) = (cid:88) vV (v ϕDec(Sk), tdec) log (v ϕDec(Sk), tdec) (6) where is the vocabulary and ϕDec is the Decoder projector. In our analysis, we normalize the reasoning progress of each sample to [0, 100%] to aggregate samples with varying lengths. Appendix Figure 2 visualizes the entropy evolution for PLaT compared to baselines. Explicit CoT and CODI exhibit rapid decay in entropy after the initial steps (progress 10%-30%). This drop indicates that the models quickly lock into specific, narrow probability path, effectively pruning alternative logical branches early in the generation. In contrast, PLaT maintains significantly higher entropy throughout the majority of the reasoning process (20% - 90%). This entropy curve suggests that PLaTs latent states do not collapse to single mode but rather maintain superposition of multiple potential verbalizations until the final termination signal is required. Figure 3: Impact of Latent Sequence Length (NL). Performance varies across datasets, with NL =1 or 2 generally providing the best balance between accuracy and diversity, suggesting that compact latent trajectories are sufficient for current reasoning tasks. Figure 4: Hyperparamaters analysis of αEM when NL = 1. B.4 Detailed Analysis of Hyperparameters This section details the sensitivity analysis supporting the hyperparameter choices. Sensitivity to Latent Sequence Length (NL). As illustrated in Appendix Figure 3, the models performance does not scale monotonically with the number of latent states. On GSM8k, NL = 2 provides the optimal balance, achieving higher greedy accuracy than NL = 1 while maintaining superior Pass@128. However, increasing NL beyond 2 leads to consistent degradation in both precision and diversity. We hypothesize that longer latent trajectories, while offering higher theoretical information capacity, introduce significant optimization challenges, such as vanishing gradients through the latent chain, in the absence of intermediate token-level supervision. Impact of EMA Coefficient (αEMA). Appendix Figures 4 and 5 visualize the effect of temporal memory aggregation. We observe clear interaction between NL and αEMA: For NL = 1, higher αEMA = 0.9 is preferred, suggesting that when planning steps are sparse, the model benefits from retaining more Figure 5: Hyperparamaters analysis of αEM when NL = 2. 16 Figure 6: Hyperparamaters analysis of ds when NL = 2 and αEM = 0.5. immediate, raw state information. For NL = 2, moderate value (αEMA = 0.5) yields more robust results. This indicates that for longer planning horizons, stronger smoothing is required to stabilize the information flow across steps. Effect of Latent Dimension (ds). As shown in Appendix Figure 6, the choice of ds reflects an information bottleneck trade-off. dimension of 2048 serves as robust sweet spot across all benchmarks. Lower dimensions restrict the expressivity of the planning states, particularly hindering the models ability to maintain the superposition of complex reasoning paths (reflected in lower Pass@128). Conversely, increasing the dimension to 4096 does not yield substantial gains and increases the computational overhead, suggesting that the reasoning manifold for these tasks is sufficiently captured by 2048-dimensional space."
        },
        {
            "title": "C Prompts for LLM Judgments",
            "content": "C.1 Prompt for Clustering Equations Prompt for Clustering Equations ## Task You are given list of mathematical reasoning steps. Some steps may be semantically equivalent (express the same meaning, even if worded differently). Please group the steps that are semantically equivalent together. ## Reasoning Steps to Group {steps_text} ## Instructions 1. Identify which steps are semantically equivalent (express the same mathematical operation or reasoning) 2. For each group of equivalent steps, keep only ONE representative step (prefer the clearest/most complete one) 3. Return the result in the following JSON format: {{ \"groups\": [ {{ \"representative\": \"step text here\", \"indices\": [1, 3, 5] }}, {{ \"representative\": \"another step text\", \"indices\": [2, 4] }} ] }} Note: Each step index should appear in exactly one group. Steps that are unique should form their own group with single index. 17 C.2 Prompt for Validating Equations Prompt for Validating Equations ## Task Determine whether the following mathematical reasoning step is VALID or INVALID for solving the given problem. IMPORTANT: When in doubt, prefer VALID. step should be marked INVALID only if it is clearly wrong or irrelevant. ## Criteria step is VALID if ANY of the following conditions are met: 1. The mathematical calculation/formula itself is CORRECT (e.g., \"2*1/2=1\", \"3+4=7\", \"163=13\" are all VALID because they are mathematically correct) 2. The step uses numbers/quantities that are mentioned in the problem OR derived from previous valid steps 3. The step could reasonably be part of solution path (even if its an intermediate calculation) 4. The step is subequation or intermediate step that helps break down larger calculation 5. The step combines numbers from the problem in mathematically valid way (e.g., adding, subtracting, multiplying , dividing numbers that appear in the problem) step is INVALID only if: The calculation itself is mathematically WRONG (e.g., \"2+2=5\") The step uses numbers that are completely unrelated to the problem AND cannot be derived from previous steps The step is clearly random calculation with no logical connection to solving the problem ## Important Guidelines Intermediate calculations are VALID: If step like \"3+4=7\" or \"2*1/2=1\" is mathematically correct and uses numbers from the problem, it is VALID even if you cannot immediately see how it contributes to the final answer Subequations are VALID: Breaking down complex calculation into smaller steps is valid reasoning strategy When uncertain, mark as VALID: Its better to accept potentially useful step than to reject valid intermediate calculation ## Examples of VALID steps \"3+4=7\" or \"163=13\" or \"164=12\" are VALID for the GT step <<1634=7> \"2*1/2=1\" is VALID for the GT step <<2/2=1>> \"60*3=180\" followed by \"180*3=540\" are VALID for the GT steps <<3*3=9>><<9*60=540>> ## Examples of INVALID steps \"2+2=5\" is INVALID (mathematically wrong) \"100*100=10000\" when the problem has nothing to do with 100 is INVALID \"3*16=48\" when the problem doesnt require multiplying 3 and 16 AND theyre not derived from previous steps is INVALID ## Question {question_context} ## Ground Truth Steps (Reference to understand the logical flow) {gt_steps_text if gt_steps_text else \"N/A\"} ## Previous Predicted Steps that have been validated {previous_steps_text} ## Reasoning Step to Validate {step_text} ## Answer Please only answer \"VALID\" or \"INVALID\", and then briefly explain the reason (one line). Answer format: VALID/INVALID: [reason]"
        },
        {
            "title": "D Limitations and Future Work",
            "content": "While PLaT introduces promising paradigm for decoupled latent planning, there are several limitations in our current implementation that outline directions for future research. First, regarding Reinforcement Learning, our current exploration is preliminary. We froze the Planner and restricted optimization to the Decoder to ensure the semantic stability of the latent manifold. While 18 this successfully aligns verbalization with the fixed latent plan, it prevents the Planner from learning new reasoning topologies or correcting fundamental logic errors via trial and error. Future work could investigate joint optimization strategies or iterative updates to refine the Planner alongside the Decoder. Second, the scaling laws of latent states remain to be fully characterized. Although our theory suggests that increasing the number of latent states per step (NL) should enrich information capacity, our experiments showed performance saturation beyond NL = 2. This is likely an optimization challenge rather than fundamental theoretical bottleneck, and advanced training techniques are needed to unlock the potential of deeper latent trajectories. Finally, our evaluation is currently concentrated on mathematical reasoning, where logical validity is strictly defined. Its efficacy in less-structured domainssuch as creative writing, common-sense reasoning, or complex code generationremains to be empirically validated. Extending this paradigm to broader spectrum of tasks is key objective for our future work."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Didi Chuxing"
    ]
}