{
    "paper_title": "ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing",
    "authors": [
        "Ziteng Wang",
        "Jianfei Chen",
        "Jun Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 1 1 7 4 1 . 2 1 4 2 : r REMOE: FULLY DIFFERENTIABLE MIXTURE-OFEXPERTS WITH RELU ROUTING Ziteng Wang, Jianfei Chen, Jun Zhu Tsinghua University wangzite23@mails.tsinghua.edu.cn; {jianfeic, dcszj}@tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, fully differentiable MoE architecture that offers simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the routers sparsity while balancing the load among experts. ReMoEs continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE."
        },
        {
            "title": "INTRODUCTION",
            "content": "Transformer models (Vaswani, 2017) consistently improve performance as the number of parameters increases (Kaplan et al., 2020). However, scaling these models is constrained by computation resources. Sparsely activated Mixture-of-Experts (MoE) (Shazeer et al., 2017) mitigates this challenge by employing sparse architecture that selectively activates subset of parameters during both training and inference. This conditional computation allows MoE models to expand model capacity without increasing computational costs, offering more efficient alternative to dense models. The key component in MoE is the routing network, which selects the experts to activate for each token. Various routing methods (Shazeer et al., 2017; Lewis et al., 2021; Roller et al., 2021; Zhou et al., 2022) have been proposed, with TopK routing (Shazeer et al., 2017) being the most commonly adopted. However, the vanilla TopK router introduces discrete and non-differentiable training objective (Shazeer et al., 2017; Zoph et al., 2022), limiting the performance and scalability. Recent works on fully-differentiable MoE aim to overcome this limitation. Soft MoE (Puigcerver et al., 2023) introduces token merging, while SMEAR (Muqeeth et al., 2023) proposes expert merging. However, both approaches break token causality, making them unsuitable for autoregressive models. Lory (Zhong et al., 2024) improves upon SMEAR and is applicable to autoregressive models. But it underperforms vanilla MoE with TopK routing. In this work, we address the discontinuities by introducing ReMoE, an MoE architecture that incorporates ReLU routing as simple yet effective drop-in replacement for TopK routing. Unlike TopK routing, which computes softmax distribution over the experts and calculates weighted sum of the largest experts, ReLU routing directly controls the active state of each expert through ReLU gate. The number of active experts is determined by the sparsity of the ReLU function. To maintain the desired sparsity, we propose adding load-balancing refined L1 regularization to the router outputs, with an adaptively tuned coefficient. This approach ensures that ReMoE maintains the same computational costs as TopK-routed MoE. Compared to TopK routing, ReLU routing is continuous and fully differentiable, as the ReLU function can smoothly transition between zero and non-zero values, indicating inactive and active. AdCorresponding author 1 Figure 1: Compute flows of vanilla MoE with TopK routing and ReMoE with ReLU routing. Positive values are shown in orange, and negative values in blue, with deeper colors representing larger absolute values. Zeros, indicating sparsity and computation savings, are shown in white. The red dash arrows in TopK routing indicate discontinuous operations. Compared with TopK routing MoE, ReMoE uses ReLU to make the compute flow fully differentiable. ditionally, ReLU routing manages the on/off state of each expert independently, offering greater flexibility. Moreover, the number of activated experts can vary across tokens and layers, enabling more efficient allocation of computational resources. Further analysis reveals that ReMoE effectively learns to allocate experts based on token frequency and exhibits stronger domain specialization. Our experiments on mainstream LLaMA (Touvron et al., 2023) architecture demonstrate that ReLU routing outperforms existing routing methods including TopK routing and fully-differentiable Lory. Through an extensive investigation across model structures, we find that ReMoE consistently outperforms TopK-routed MoE across broad range of active model sizes (182M to 978M), expert counts (4 to 128), and levels of granularity (1 to 64) (Krajewski et al., 2024). Notably, in terms of scaling behavior, we observe that ReMoE exhibits steeper performance improvement as the number of experts scales up, surpassing traditional MoE models."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 MOE FOR DECODER-ONLY TRANSFORMER typical decoder-only Transformer model consists of layers, each containing Self-Attention module and Feed-Forward Network (FFN) module. MoE modifies this structure by replacing each FFN module with an MoE module, which comprises small router and several experts FFN1, . . . , FFNE, where each expert is equivalent to the original FFN and denotes the number of t=1 RT of the layer l, where is the number of tokens in experts. Given the input xl = (xl batch and is the hidden size, the output yl = (yl t=1 is computed as: t)T t)T yl = (cid:88) e=1 R(xl t)eFFNe(xl t; df n) (1) Here, R() represents the routing function, and df is the intermediate size of the FFN, typically set to df = 4d. 2.2 TOPK ROUTING TopK routing (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022) is the most commonly used method for defining the routing function R(). It introduces sparsity in the MoE computation 2 by forcibly zeroing out smaller elements: (2) where Wl RdE is the routers weight matrix, and TopK(, k) retains the top largest values while setting the rest to zero. This mechanism allows for skipping the computation of the FFNe functions corresponding to the zeroed-out R(xl t)e values in both the forward and backward passes. t) = TopK(Softmax(xl tWl), k) R(xl"
        },
        {
            "title": "3.1 MOTIVATION: FROM TOPK TO RELU",
            "content": "For given token = (xe)E e=1 after Softmax, TopK introduces jump discontinuity at the k-th largest value, denoted as x[k], by zeroing out the values smaller than x[k]. This can be expressed as: (3) TopK(x, k)e = xe 1{xe t(x, k)}, t(x, k) = x[k] where 1{} is the indicator function, returning 1 if the condition is met and 0 otherwise. As shown in Figure 2, the jump discontinuity can be eliminated by setting the breakpoint t(x, k) 0, which actually corresponds to the ReLU function: ReLU(x)e = xe 1{xe 0} (4) Figure 2: Comparison between TopK and ReLU. At high level, ReLU improves upon TopK by aligning the breakpoints of all inputs and setting them to 0. This ensures that the output is continuous at 0, where the experts transition between active and inactive. As result, the training pipeline becomes fully differentiable. 3.2 DIFFERENTIABLE RELU ROUTING We define the ReLU routing function as follows: R(xl t) = ReLU(xl tWl) (5) with (1 ) being the desired sparsity of ReLU, where is the number of active experts and is the total number of experts. This ensures that the computational cost remains equivalent to that of TopK routing. In vanilla TopK routers, the Softmax outputs sum to 1, representing the probabilities of selecting each expert, after which TopK eliminates those with lower probabilities. In contrast, ReLU routers discard the Softmax function, relying on ReLUs naturally non-negative outputs. The outputs of ReLU routers represent the weights assigned to each expert, which can include 0. Instead of hardcoding expert selection with discontinuous TopK function, ReLU allows the router to learn which experts to activate (i.e., when to produce 0s) in fully differentiable manner. Another key difference is that in TopK routing, each token is routed to exactly experts, whereas in ReLU routing ReMoE, the routing decisions are independent, allowing tokens to be routed to variable number of experts. This flexibility is advantageous, as not all tokens have the same level of difficulty. ReMoE can allocate more computational resources to more challenging tokens, dynamic allocation strategy that we explore further in Section 5.1. TopK routing introduces discrete loss function when the set of activated experts changes, whereas ReLU routing remains continuous and fully differentiable. For instance, in two-expert Top1routing model, small weight update that alters the softmax result from x1 = (0.51, 0.49) to x2 = (0.49, 0.51) shifts the TopK output from (0.51, 0) to (0, 0.51), creating discontinuity. In contrast, ReLU routing only changes the activated experts when the routing output is near zero. For example, an output shift from (0.01, 0) to (0, 0.01) remains continuous. Further details on the stability analysis of these two routers can be found in Appendix A. comparison of the compute flow between ReMoE and MoE is shown in Figure 1."
        },
        {
            "title": "3.3 CONTROLLING SPARSITY VIA ADAPTIVE L1 REGULARIZATION",
            "content": "ReMoE controls computational costs by managing the sparsity of the ReLU output, targeting sparsity level of (1 ). However, directly training the ReLU router often results in lower sparsity, as the model tends to activate more experts to increase capacity. To meet the desired budget, we need to enforce higher sparsity in the ReLU output. We achieve this by introducing regularization loss, Lreg, to the loss of language model, Llm: where λi is an adaptive coefficient based on the current training step i. Initially, we set λ0 to small value and employ simple zeroth-order algorithm to update it: = Llm + λiLreg, (6) Here, α > 1 is preset update multiplier, and Si denotes the average sparsity of all router outputs at the step i: λi+1 = λi αsign((1 )Si) (7) Si ="
        },
        {
            "title": "1\nLT E",
            "content": "L (cid:88) (cid:88) (cid:88) l=1 t=1 e= 1{R(xl t)e > 0} (8) The idea behind Equation 7 is that when the average sparsity Si falls below the target sparsity (1 ), we increase λi by factor of α, strengthening the regularization and encouraging higher sparsity. Conversely, if the sparsity exceeds the target, λi is reduced. We heuristically set λ0 = 1e8 and α = 1.2 in all our experiments, and demonstrate the robustness of these hyperparameters in Appendix B. The regularization term Lreg uses the L1-norm, following prior work (Li et al., 2022; Song et al., 2024), to effectively encourage sparsity: Lreg = 1 LT (cid:88) (cid:88) l=1 t=1 (cid:13) (cid:13)R(xl t)(cid:13) (cid:13)1 = 1 LT (cid:88) (cid:88) (cid:88) l= t=1 e=1 R(xl t)e (9) The second equation holds because the output of the ReLU function is non-negative. The term Lreg represents the average value of all router outputs, including zeros. By taking the derivative of λiLreg, we observe that the regularization effect adds λi LT to the gradient of each non-zero router output, effectively driving the outputs toward zero and enhancing sparsity. With this L1 regularization, we can control the sparsity around the desired level of (1 ) with only minor fluctuations, as shown in Figure 3. Consequently, ReMoE ensures that, on average, tokens are routed to experts across different layers and tokens, maintaining the same FLOPs as vanilla TopK-routed MoE from statistical perspective. Our benchmarking results in Appendix demonstrate that ReMoE can achieve nearly identical training and inference throughputs as conventional MoE, providing an efficient alternative without compromising speed. Figure 3: The sparsity of ReMoE with = 8, = 1 is effectively maintained around the desired target. Sparsity values for all steps are plotted without averaging or sampling. The mean and standard deviation are calculated excluding the first 100 warm-up steps. 3.4 INTEGRATE LOAD BALANCING INTO L1 REGULARIZATION Load imbalance is significant issue in MoE design, potentially leading to routing collapse (Shazeer et al., 2017; Muennighoff et al., 2024) and uneven computational distribution across multiple devices. The L1 regularization in Equation 9 treats the router output for each expert and each layer equally, which can contribute to load balancing problems. 4 (a) Sparsity Si (b) Coefficient term λi and regularization term Lreg (c) Language model loss Llm and overall regularization λiLreg Figure 4: Natural Three Stage Training in ReMoE. To address this, we introduce load-balancing refinement to the L1 regularization: Lreg,lb = fl,e ="
        },
        {
            "title": "1\nLT",
            "content": "E kT (cid:88) (cid:88) (cid:88) l=1 t= e=1 fl,eR(xl t)e (cid:88) t=1 1{R(xl t)e > 0} (10) (11) Here, fl,e is non-differentiable and represents the average activation ratio of expert in layer l, relative to the desired ratio . This serves as weight for the corresponding router output, modifying the added gradient of non-zero router outputs to fl,eλi LT . This mechanism penalizes experts receiving more tokens by driving their router outputs toward zero more rapidly. Although derived from regularization, this formulation is identical to the load-balancing loss in vanilla TopK routing (Fedus et al., 2022). In TopK routing, the outputs of Softmax sum to 1, giving the loss lower bound of 1. In contrast, ReLU routing outputs can be arbitrarily small, making Lreg,lb trivially bounded at 0. Therefore, unlike in MoE, we cannot fix the coefficient λi in ReMoE, as this would lead to routing collapse toward 0. Thanks to the adaptive update of λi, we can balance sparsity control and load balancing within single formulation, as given in Equation 10. Further discussion on load balancing in ReMoE can be found in Section 5.2, and we adopt this load-balancing refined L1 regularization in our later experiments. 3.5 NATURAL THREE-STAGE TRAINING IN REMOE With the regularization scheme described above, we observe clear and naturally occurring threestage separation during the training of ReMoE as is depicted in Figure 4. The first stage is the warm-up stage, or the dense stage. During this stage, λi is small, while Llm is large and decreases rapidly. Training ReMoE at this stage is nearly equivalent to training its dense counterpart with the same total number of parameters. Each expert processes more than half of the tokens, allowing the experts to diversify from their random initializations. The second stage is the sparsifying stage, or the dense to sparse stage. At this point, the sparse regularization term λiLreg becomes significant, causing the ReLU routers to activate fewer experts. This forces the experts to become more diverse without causing an increase in Llm. The third stage is the stable stage, or the sparse stage. In this phase, the sparsity Si stabilizes at the preset target. During this stage, Llm is optimized while being softly guided along the sparse subspace by Lreg. Both Lreg and λi change very slowly, with Lreg gradually decreasing and λi gradually increasing. However, the overall regularization term, λiLreg, remains relatively constant. It should be noted that Stages and II introduce additional computational cost and memory consumption since more experts are activated. However, the time overhead is negligible since they generally require only 100 iterations (0.17% of the total steps in our setting, benchmarking results are detailed in Appendix D). The memory overhead can be minimized by temporarily reducing the micro-batch size or by employing the activation checkpointing technique that avoids storing intermediate results of activated experts by recomputing them on-the-fly during the backward pass. 5 Model ARC-c ARC-e BoolQ HellaSwag LAMBADA PIQA RACE Avg. Dense Hash Lory SparseMixer-v2 EC dMoE ReMoE 19.45 19. 20.31 19.80 18.86 20.05 20.22 43. 45.45 42.97 46.72 42.97 45.16 46. 54.40 54.95 49.54 45.96 60.21 57. 54.16 28.61 29.68 28.75 30.24 29. 29.83 30.26 31.09 31.44 32.35 34. 29.26 32.97 35.94 61.97 63.06 62. 62.89 61.92 63.55 63.55 28.52 27. 27.75 29.00 27.37 28.33 29.38 38. 38.79 37.70 38.39 38.53 39.67 40. Figure 5: Training curves of different routing methods. Table 2: Zero-shot accuracy of different routing methods on downstream tasks."
        },
        {
            "title": "4.1 SETUP",
            "content": "Infrastructure We leverage Megatron-LM (Shoeybi et al., 2019) as our code base and implement ReLU routing as drop-in replacement for the original TopK routing, supporting all forms of model parallelism: Data, Tensor, Pipeline, and Expert Parallelism (Shoeybi et al., 2019; Narayanan et al., 2021; Korthikanti et al., 2023). Model Architecture. We experiment with the mainstream LLaMA (Touvron et al., 2023) architecture, featuring grouped query attention (GQA) (Ainslie et al., 2023), SwiGLU (Shazeer, 2020) activation function, RoPE (Su et al., 2024) position embedding, and RMSNorm (Zhang & Sennrich, 2019). The context length is set to 1024, and the batch size is 512. We experiment with three different dense backbone sizes as shown in Table 1. For vanilla MoE we adopt load balancing loss of weight 0.01 following Fedus et al. (2022). For ReMoE we use the adaptive load balancing L1 regularization in Equation 10. Training Settings. We train the models on The Pile (Gao et al., 2020), an 800 GB diverse corpus. All models are trained for 60k steps ( 30B tokens), which exceeds the compute-optimal dataset size predicted by Krajewski et al. (2024) and is enough to converge. The byte pair encoding (BPE) tokenizer (Sennrich, 2015) is used. We adopt AdamW (Loshchilov, 2017) as the optimizer with β1 = 0.9, β2 = 0.999 with ZeRO optimization (Rajbhandari et al., 2020). The learning rate is set to be 5e4 with cosine scheduler. All models are trained with 8 NVIDIA A100 GPUs. Size #Parameters hidden size num layers num heads num groups GFLOPs Small Medium Large 182M 469M 978M 768 1024 12 24 24 12 16 16 4 4 4 995 2873 5991 Table 1: Configurations for the dense backbones. FLOPs are calculated with single sequence according to Narayanan et al. (2021). 4.2 COMPARISON WITH OTHER ROUTING METHODS (i) Token-choice dropless TopK routing We compare ReMoE against the following methods: (dMoE) (Gale et al., 2023) (ii) Expert-choice TopK routing (EC) (Zhou et al., 2022) (iii) Deterministic hash routing (Hash) (Roller et al., 2021) (iv) Fully-differentiable expert-merging routing (Lory) (Zhong et al., 2024) (v) TopK routing with improved gradient estimate (SparseMixer-v2) (Liu et al., 2024). The performance of these methods is evaluated with active parameters = 182M and the expert count = 8. We fix the active expert count to = 1 for straightforward comparison with the dense counterpart. For the Hash method, we use mod hashing function. And for Lory, the segment length is set to 256, following the original paper. 6 (a) Scaling in (b) Scaling in (c) Scaling in Figure 6: Scalability of ReMoE with respect to the number of active parameters (N ), expert count (E), and granularity (G). Default config is = 182M, = 8, = 1, = 1. The Y-axis represents the validation loss of each model after training on 30B tokens. ReMoE consistently outperforms MoE across all configurations. These models are trained on 30B tokens, with the training curves shown in Figure 5, We evaluate the zero-shot performance of the trained models on the following downstream tasks: ARC (Clark et al., 2018); BoolQ (Clark et al., 2019); HellaSwag (Zellers et al., 2019); LAMBADA (Paperno et al., 2016); PIQA (Bisk et al., 2020); RACE (Lai et al., 2017). The downstream accuracy results are summarized in Table 2. Our results show that all MoE models outperform the dense model. Deterministic hash routing performs worse than the learned routing methods. Among the Top-K approaches, token-choice dMoE outperforms expert-choice MoE and SparseMixer-v2 in evaluation. The differentiable routing method Lory surpasses Hash routing in training but underperforms in downstream tasks, with both methods falling short of the standard Top-K routing. Notably, ReMoE outperforms all methods, including the mainstream Top-K routing, while benefiting from differentiability. 4.3 SCALABILITY OF REMOE In this section, we compare ReMoE with state-of-the-art dMoE (hereinafter referred to simply as MoE) across varying model parameters , expert counts E, and granularity levels to demonstrate its scalability and universal superiority. Since ReMoE demands more computation in both Stage and Stage II, we increase the number of training steps for the MoE baseline to match the total computation in each setting, ensuring more equitable comparison. We present the final validation losses in Figure 6, with comprehensive downstream evaluation results available in Appendix E. Scaling in active parameters . To assess scalability with respect to the number of parameters , we fix = 8 and = 1, while varying active parameters from 182M to 975M, corresponding to the dense counterpart configurations in Table 1. The total parameters are 777M, 2.58B, 5.73B respectively. The results, shown in Figure 6a, indicate that ReMoE consistently outperforms MoE across all model sizes. The performance gap does not diminish as the model size increases, suggesting that ReMoE maintains its advantage at larger scales. Scaling in expert count E. In this experiment, we fix the number of parameters at = 182M and set the number of active experts = 1, while varying the total number of experts from 4 to 128. The scaling curve in Figure 6b reveals that ReMoE consistently outperforms the standard MoE across all configurations of E. Moreover, key observation is the steeper slope in ReMoEs performance as increases, compared to MoE. This suggests that ReMoE scales more effectively with the number of experts and derives greater benefits from larger expert pools. ReMoEs differentiable routing strategy appears better suited for leveraging large expert groups, leading to significant improvements in model expressivity and generalization. Scaling in granularity G. We also evaluate ReMoE and MoE in fine-grained settings. Finegrained MoE (Dai et al., 2024; Krajewski et al., 2024) with granularity is constructed by dividing 7 each expert into smaller experts, as formulated below: yl = EG (cid:88) e=1 R(xl t)eFFNe(xl t; df n/G) (12) R(xl t) = TopK(Softmax(xl (13) Fine-grained MoE outperforms vanilla MoE from scaling law perspective (Krajewski et al., 2024) and has been adopted in subsequent works (Dai et al., 2024; Tan et al., 2024; Muennighoff et al., 2024). For fine-grained ReMoE, the routing function remains identical to Equation 5, and the target sparsity is still (1 ). The only distinction lies in the shape of the weight matrix, with Wl RdEG. tWl), kG) We conduct experiments with = 182M and = 8, varying from 1 to 64 for both fine-grained MoE and fine-grained ReMoE. In addition to comparing these models against the dense baseline with the same number of active parameters, we also evaluate their dense counterpart with the same total number of parameters. This is achieved by expanding the intermediate size of the FFN by factor of E, which we denote as Dense8. This configuration represents the strict upper bound for MoE and ReMoE, as it is equivalent to Mixture-of-Experts with all experts activated (Dai et al., 2024). As illustrated in Figure 6c, fine-grained ReMoE consistently outperforms fine-grained MoE. Moreover, fine-grained ReMoE of = 32 and = 64 reach the performance of the theoretical upper bound, Dense8, while requiring significantly fewer FLOPs during both training and inference. In contrast, fine-grained MoE is unable to match in all settings, making ReMoE more efficient and effective choice."
        },
        {
            "title": "5 DISCUSSION",
            "content": "5.1 DYNAMIC EXPERT ALLOCATION IN REMOE In ReMoE, each token dynamically activates subset of experts, allowing the model to adaptively allocate resources. We evaluate the performance of the = 182M, = 8, = 1 ReMoE model and analyze the relationship between token frequency and the average number of active experts. As illustrated in Figure 7, the model tends to assign higher number of experts to rarer tokens, such as , OTAL, and @#, while reducing the number of active experts for more frequent tokens like , n, and the. This adaptive behavior mirrors the principles of Huffman tree Huffman (1952), where more frequent symbols are assigned shorter codes, and rarer symbols are assigned longer codes. Similarly, ReMoE tends to cluster on common tokens by activating fewer experts, effectively compressing the representation of these frequent tokens. In contrast, for rarer tokens, ReMoE activates more diverse set of experts, encoding them as richer linear combination at the expert level. This suggests that ReMoE learns to dynamically allocate computational resources, achieving an efficient balance between resource usage and the models capacity, optimizing performance under constrained expert budget. Dynamic expert allocation is also evident at the domain level, as detailed in Appendix G. Figure 7: Correlation between expert allocation and token frequency in ReMoE. X-axis is sorted by average active expert count and token frequency is in log-scale. 5.2 THE ROLE OF LOAD BALANCING IN REMOE Load imbalance can lead to routing collapse in the vanilla TopK-routed MoE, where the router tends to assign the same expert to all inputs, in which scenario the training objective becomes continuous and fully differentiable. As is shown in Figure 8a, there is significant performance gap between MoE models with and without load balancing (LB). 8 (a) Training curves of MoE and ReMoE with and without load balancing (b) Average routed tokens ratio of ReMoE w.o. LB (c) Average routed tokens ratio of ReMoE w. LB (d) Sparsity across different layers in ReMoE Figure 8: Observations on the role of load balancing in MoE and ReMoE. White squares in (b) represent inactive experts with fewer than 1/64 tokens routed to them. While in ReLU routing, thanks to its differentiablity, even applying the L1 regularization from Equation 9 without load balancing yields comparable results with well-tuned MoE with LB. However, some experts in ReMoE without LB remain inactive, illustrated as white squares in Figure 8b which shows the heat map of the average routed tokens ratio (i.e., the fraction of tokens routed to the e-th expert in the l-th layer) over 50M tokens in test set. This inactivity can limit the models capacity. When load balancing is incorporated into the refined L1 regularization (Equation 10), the experiments show more even distribution of token assignments across experts, with all experts being utilized, as shown in Figure 8c. The final loss in ReMoE decreases after introducing load balancing. Besides, we observe ReMoE with LB can produce smoother sparsity distribution across layers as depicted in Figure 8d. This is because fl,e is computed based on the absolute number of routed tokens, meaning denser layers receive stronger penalties. Note that even ReMoE with load balancing (LB) does not yield perfectly even distribution. However, the trade-off between load balancing and performance can be easily adjusted by modifying the L1 regularization in Equation 10. For instance, changing fl,e to 2 l,e would make the model more sensitive to load imbalance. Additionally, device-level load balancing techniques, as proposed in Dai et al. (2024), could also be employed. Since load imbalance in ReMoE does not lead to severe routing collapse, it primarily becomes hardware utilization issue. As such, we leave the exploration of these variants for future work. (a) Domain specialization of MoE (b) Domain specialization of ReMoE Figure 9: Average routed tokens ratio for MoE and ReMoE across 12 layers and 8 experts in different domains. The gray dashed lines indicate uniform distribution. ReMoE shows stronger domain specialization."
        },
        {
            "title": "5.3 DOMAIN SPECIALIZATION IN REMOE",
            "content": "The differentiability and dynamic allocation strategy of ReMoE facilitates the development of diverse experts that specialize in different domains. This allows the router to effectively perform ensemble learning by leveraging the expertise of various experts, as demonstrated in our experiments. In Figure 9, we plot the average routed tokens ratio across different experts, layers, and domainsnamely Arxiv, Books, C4, Github, Stackexchange, and Wikipediafor MoE and ReMoE models with = 182M, = 8. We focus on the first, middle, and last layers (with IDs 0, 5, and 11). The results for most experts in MoE (Figure 9a) show roughly uniform distribution across all domains. In contrast, experts in ReMoE (Figure 9b) exhibit clear domain specialization, being activated with varying frequencies across different domains. For example, more than half of the tokens from Arxiv, Github, and StackExchangedomains that emphasize structured, non-natural languages like LaTeX and Pythonare routed to Expert 6 in Layer 5, significantly more than in other domains. more detailed result of domain specialization can be found in Appendix F."
        },
        {
            "title": "6 RELATED WORKS",
            "content": "6.1 MIXTURE-OF-EXPERTS Mixture-of-Experts (MoE) was initially proposed in the early 1990s (Jacobs et al., 1991; Jordan & Jacobs, 1994) and later introduced into large-scale neural networks as sparse submodule for efficiency (Shazeer et al., 2017). Advances like GShard (Lepikhin et al., 2020) and Switch Transformer (Fedus et al., 2022) integrated sparse MoE into Transformer models, achieving significant results. More recently, MoE has been used in commercial-scale language models such as Mixtral-8x7B (Jiang et al., 2024), DeepSeekMoE 16B (Dai et al., 2024), and Snowflake Arctic 17B (Snowflake, 2024). 6.2 ROUTING MECHANISMS IN MOE Various routing methods have been developed for expert selection. Static routers, such as BASE (Lewis et al., 2021), use predefined rules like combinatorial optimization, while Hash routing (Roller et al., 2021) relies on deterministic hash functions, and THOR (Zuo et al., 2021) assigns experts randomly with regularization. Learned routers adaptively select experts based on token input, using approaches like REINFORCE (Bengio et al., 2013; Schulman et al., 2015; Clark et al., 2022) for reinforcement learning, and TopK routing (Shazeer et al., 2017; Zhou et al., 2022) for token or expert selection, though TopK introduces discontinuities that hinder gradient estimation. 6.3 DIFFERENTIABLE MIXTURE-OF-EXPERTS Recent work on fully differentiable MoE models addresses the challenges of discrete optimization, basically through token merging and expert merging approaches. Soft MoE (Puigcerver et al., 2023) uses token merging, assigning fixed slots to each expert as linear combination of input tokens. SMEAR (Muqeeth et al., 2023) merges experts into an ensemble via weighted averaging. However, both methods require full probability map of input tokens, making them unsuitable for autoregressive models. Lory (Zhong et al., 2024) preserves autoregressiveness by segmenting sentences to merge experts but underperforms compared to TopK routing."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we propose ReMoE, fully differentiable MoE architecture with ReLU routing. The simple yet effective ReLU routing function acts as drop-in replacement for the conventional TopK+Softmax routing, offering (i) continuity and differentiability, and (ii) dynamic expert allocation across tokens and layers. With the adaptive load balancing L1 regularization, ReMoE universally outperforms TopK-routed MoE across various model sizes, expert counts, and levels of granularity, demonstrating sharper performance gains as the number of experts scales."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International conference on machine learning, pp. 4057 4086. PMLR, 2022. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. Proceedings of Machine Learning and Systems, 5:288304, 2023. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. David Huffman. method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9):10981101, 1952. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Michael Jordan and Robert Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214, 1994. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5:341353, 2023. 11 Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pioro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Krol, Tomasz Odrzygozdz, Piotr Sankowski, et al. Scaling laws for fine-grained mixture of experts. arXiv preprint arXiv:2402.07871, 2024. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785794, 2017. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pp. 62656274. PMLR, 2021. Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. The lazy neuron phenomenon: On emergence of activation sparsity in transformers. arXiv preprint arXiv:2210.06313, 2022. Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, et al. Grin: Gradient-informed moe. arXiv preprint arXiv:2409.12136, 2024. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060, 2024. Mohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing. arXiv preprint arXiv:2306.03745, 2023. Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. In ProceedEfficient large-scale language model training on gpu clusters using megatron-lm. ings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 115, 2021. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15251534, 2016. Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts. arXiv preprint arXiv:2308.00951, 2023. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 34:1755517566, 2021. John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation graphs. Advances in neural information processing systems, 28, 2015. Rico Sennrich. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. 12 Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Snowflake."
        },
        {
            "title": "Arctic",
            "content": "open: snowflake, URL arctic-open-efficient-foundation-language-models-snowflake/. 2024."
        },
        {
            "title": "Efficient",
            "content": "foundation at https://www.snowflake.com/blog/ language models Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, et al. Prosparse: Introducing and enhancing intrinsic activation sparsity within large language models. arXiv preprint arXiv:2402.13516, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Shawn Tan, Yikang Shen, Rameswar Panda, and Aaron Courville. Scattered mixture-of-experts implementation. arXiv preprint arXiv:2403.08245, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Zexuan Zhong, Mengzhou Xia, Danqi Chen, and Mike Lewis. Lory: Fully differentiable mixture-ofexperts for autoregressive language model pre-training. arXiv preprint arXiv:2405.03133, 2024. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114, 2022. Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and Jianfeng Gao. Taming sparsely activated transformer with stochastic experts. arXiv preprint arXiv:2110.04260, 2021."
        },
        {
            "title": "A STABILITY ANALYSIS OF TOPK AND RELU",
            "content": "We introduce two metrics, flip rate and flip count, to evaluate the routing stability: (cid:80)L l=1 flip rate = (cid:13) (cid:13)vec(M LT l i1)(cid:13) (cid:13)1 flip count = flip rate (14) (15) where step i, computed using fixed calibration set of tokens. RT denotes the 0-1 mask matrix of the output of the router at layer and training The metric flip rate represents the percentage of expert activation states that change (from active to inactive or conversely) in single update, while flip count indicates the average number of experts whose activation states change. We measure the two metrics on MoE and ReMoE with =182M and {8, 16, 32} training for 10B tokens. The results are presented in Figure 10, indicating that the ReLU router is more stable than the TopK router: Figure 10: Flip rate and flip count of MoE and ReMoE When = 8, we find the flip rate of MoE is higher than ReMoE, though the gap narrows as training progresses and the learning rate decreases. While for = 16 and = 32, the flip rate of MoE remains consistently 2 3 higher compared to ReMoE throughout training. Moreover, the flip count of ReMoE is invariant with respect to E, whereas the flip count of MoE is highly sensitive to the total number of experts and keeps increasing as grows. Notably, the flips in TopK-routed MoE are discontinuous (e.g.(0.51, 0) (0, 0.51)), while those in ReLU-routed ReMoE are continuous(e.g.(0.01, 0) (0, 0.01)), further underscoring the superiority of the ReLU router. INSENSITIVITY TO λ0 AND α 1e16 1e12 λ0 Valid Loss 2.031 Settling time 138 Overshoot observed in 8-92 steps. 2.029 136 1e 2.032 110 1e4 2.036 55 1 2.032 92 α 1.1 1.05 1.5 2.057 Valid Loss 2.028 Settling time 211 52 large oscillation amplitude in sparsity is observed. 2.032 110 2.029 80 2.033 1.3 1.2 Table 3: Valid loss and settling time for different values of λ0 with α = 1.2. Table 4: Valid loss and settling time for different values of α with λ0 = 1e8. The ReMoE adaptation algorithm in Equation 7 includes two hyperparameters: λ0 and α. Settling time, defined as the total number of steps required in Stage and Stage II (as outlined in Section 3.5), is influenced by these parameters. For all experiments, we set λ0 = 1e8 and α = 1.2, but we show that performance remains stable as long as λ0 is small and α is close to 1. Our experiments with = 182M, = 8, = 1, and = 1 ReMoE models trained for 20k steps (10B tokens) reveal only minor variations in validation loss for different λ0 values  (Table 3)  and α values  (Table 4)  , except for α = 1.5 which caused rapid regularization changes and excessive oscillation. Besides, although different λ0 and α values affect settling time, the impact is minor compared to the overall training steps, proving the insensitivity."
        },
        {
            "title": "C PERFORMANCE FOR LONGER TRAINING",
            "content": "We conduct experiments of training MoE and ReMoE for longer duration. We experiment with =469M, = 8, = 1 and train the models with batch size of 4M tokens and training over 120B tokens. The results, as shown in Table 5, indicate that the superiority of ReMoE persists in longer training. Model Valid Loss ARC-c ARC-e BoolQ HellaSwag LAMBADA PIQA RACE Avg. MoE ReMoE 1.716 1.689 23.62 25. 52.40 55.22 53.94 55.96 35.43 36.76 43.64 45.82 68.34 68.93 31.48 30. 44.12 45.49 Table 5: Performance of training =469M, = 8, = 1 models for 120B tokens."
        },
        {
            "title": "D SPEED COMPARISON OF REMOE AND MOE",
            "content": "We measure the end-to-end training time for MoE and ReMoE with models of =469M training over 120B tokens. The time consumption across stages is summarized in Table 6. We find Stage and Stage II account for 1.02% of the total training time and incur 0.58% overhead. Model Stage Stage II Stage III Total MoE ReMoE 0.12 0.32 0.41 0.91 119.12 119.25 119.65 120.48 Table 6: End-to-end training time comparison across stages (in hours). The time is measured on = 469M, = 8, = 1 models training over 120B tokens. # Parameters TP Model Train TFLOPS Train Diff. Infer TFLOPS Infer Diff. 182M 469M 978M 978M 978M 1 1 2 4 MoE ReMoE MoE ReMoE MoE ReMoE MoE ReMoE MoE ReMoE 103.49 105.38 138.58 136.69 160.46 157.61 133.40 132.49 103.61 101. 1.82% 1.37% 1.77% 0.68% 2.29% 78.47 80. 107.52 111.71 153.11 152.76 118.55 117.27 85.96 87.96 2.19% 3.89% 0.23% 1.08% 2.33% Table 7: Throughput comparison between TopK-routed MoE and ReLU-routed ReMoE models. TP indicates the tensor parallel size. Train Diff. and Infer Diff. indicate the relative TFLOPS difference of ReMoE compared to MoE, where denotes ReMoE is faster, and denotes it is slower. 15 We further measure the throughput of ReMoE against TopK-routed MoE across different model sizes and tensor parallel sizes during Stage III. The results, presented in Table 7, indicate that ReMoE achieves comparable training and inference speeds with MoE, with minor deviation ranging from 2.29% to +3.89%. This speed consistency is desirable, as ReMoE introduces only minimal modification to the standard MoE architecture by adjusting the routing function, thereby avoiding additional computational overhead."
        },
        {
            "title": "E DOWNSTREAM EVALUATION RESULTS",
            "content": "This section provides the detailed downstream evaluation results for the main experiments of scalability of ReMoE in Section 4.3 and ablations on load balancing in Section 5.2. E.1 SCALING IN ACTIVE PARAMETERS The downstream evaluation results for scaling with respect to the parameter count , as discussed in Section 4.3, are presented in Table 8. These results highlight the performance comparison with increasing model parameters. Model ARC-c ARC-e BoolQ HellaSwag LAMBADA PIQA RACE Avg. Dense MoE ReMoE 182M 19.45 469M 21.50 978M 21.93 182M 20.82 469M 23.63 978M 23.81 182M 20.22 469M 21.67 978M 24. 43.35 49.12 50.88 45.03 52.40 52.90 46.68 53.16 55.26 54.40 56.88 60.24 57.55 53.94 58.90 54.16 58.75 57. 28.61 31.12 32.42 29.84 32.43 35.01 30.26 33.80 35.93 31.09 36.74 41.06 31.81 43.64 44.42 35.94 40.66 44. 61.97 64.47 67.46 63.28 68.34 67.90 63.55 67.95 68.99 28.52 30.53 31.77 28.42 31.48 31.48 29.38 31.20 30. 38.20 41.48 43.68 39.53 43.69 44.91 40.03 43.88 45.20 Table 8: Downstream results of scaling in active parameters . E.2 SCALING IN EXPERT COUNT Table 9 contains the downstream evaluation results for scaling with respect to the expert count E, as examined in Section 4.3. This analysis illustrates how varying the number of experts influences the overall model effectiveness of MoE and ReMoE. Model Dense MoE ReMoE - 4 8 16 32 64 128 4 8 16 32 64 128 ARC-c ARC-e BoolQ HellaSwag LAMBADA PIQA RACE Avg. 19.45 20.73 20.82 20.90 19.54 19.88 20.99 19.88 20.22 20.90 20.56 20.82 19. 43.35 44.49 45.03 45.29 47.35 46.63 47.69 46.46 46.68 49.28 48.11 50.51 51.05 54.40 59.63 57.55 46.36 52.29 60.06 56.73 57.43 54.16 53.36 59.54 57.80 56. 28.61 29.14 29.84 30.50 31.12 31.47 32.00 29.64 30.26 30.85 31.42 32.17 32.40 31.09 31.40 31.81 33.22 35.63 36.33 36.62 33.57 35.94 37.09 37.84 36.74 37. 61.97 63.33 63.28 64.96 64.25 65.07 65.67 62.95 63.55 65.83 65.18 65.78 66.70 28.52 29.19 28.42 28.33 28.23 28.04 28.04 27.66 29.38 30.05 28.42 27.46 29. 38.20 39.70 39.53 38.50 39.77 41.06 41.10 39.66 40.03 41.05 41.58 41.61 42.12 Table 9: Downstream results of scaling in expert count E. 16 E.3 SCALING IN GRANULARITY The downstream evaluation results for scaling with respect to the granularity are shown in Table 10, based on the experiments in Section 4.3. These results demonstrate the superiority of finegrained ReMoE over fine-grained MoE. Model ARC-c ARC-e BoolQ HellaSwag LAMBADA PIQA RACE Avg. Dense Dense8 MoE ReMoE - - 1 2 4 8 16 32 64 1 2 4 8 16 32 64 19.45 22.78 20.82 21.42 20.99 21.59 19.80 21.67 20.14 20.22 20.14 20.39 20.82 21.25 20.90 20. 43.35 48.11 45.03 46.55 46.09 47.73 48.82 48.78 48.74 46.68 47.39 47.94 48.36 49.41 48.86 48.74 54.40 59.66 57.55 54.25 55.90 60.70 57.34 57.85 61.50 54.16 57.95 55.35 60.49 56.06 55.81 60. 28.61 31.11 29.84 29.95 30.52 30.83 30.64 31.27 31.03 30.26 30.60 31.04 30.90 30.91 31.14 31.56 31.09 35.65 31.81 32.52 35.16 36.41 36.00 37.10 36.31 35.94 34.52 36.11 36.06 36.23 36.58 36. 61.97 65.02 63.28 64.09 63.98 64.69 64.74 64.69 63.93 63.55 63.71 64.64 63.87 64.91 64.69 65.40 28.52 29.57 28.42 28.61 29.28 28.04 28.71 28.52 27.85 29.38 28.52 29.00 28.90 29.95 30.05 29. 38.20 41.70 39.53 39.62 40.27 41.42 40.86 41.41 41.35 40.03 40.40 40.64 41.34 41.25 41.15 41.69 Table 10: Downstream results of scaling in granularity G. E.4 LOAD BALANCING ABLATIONS Table 11 presents the downstream evaluation results for the load balancing ablations, as discussed in Section 5.2. These results compare performance with and without load balancing, offering insights into the different roles of load balancing in MoE and ReMoE. Model LB ARC-c ARC-e BoolQ HellaSwag LAMBADA PIQA RACE Avg. - Dense MoE MoE ReMoE ReMoE 19.45 19.20 20.05 19.45 20.22 43.35 44.74 45.16 46.34 46.68 54.40 50.80 57.83 56.94 54. 28.61 28.60 29.83 30.19 30.26 31.09 30.18 32.97 31.79 35.94 61.97 62.24 63.55 63.33 63.55 28.52 27.94 28.33 28.61 29.38 38.20 37.67 39.67 39.52 40.03 Table 11: Downstream results of training with or without load balancing."
        },
        {
            "title": "F DETAILED RESULTS FOR DOMAIN SPECIFICATION",
            "content": "Figure 11 shows the average routed tokens ratio of MoE and ReMoE across all layers. ReMoE demonstrates significantly stronger domain specialization compared to MoE, where certain experts are more frequently activated for specific domains. This suggests that ReMoE is better at learning and exploiting the unique characteristics of different domains, allowing it to allocate computational resources more effectively. In contrast, MoE exhibits more uniform expert activation across domains, indicating less differentiation in its expert specialization. 17 (a) Domain specialization of MoE Figure 11: Detailed results of average routed tokens ratio for MoE and ReMoE in different domains. (b) Domain specialization of ReMoE We further analyze the experts in Layer 5 of ReMoE and observe that certain highly related, domainspecific vocabularies are consistently routed to the same expert. To investigate this, we calculate the routing probabilities of different tokens based on their IDs, defined as the ratio of the number of times specific expert is utilized to the total occurrences of the token. The results are summarized in Table 12. Our findings reveal that the vocabularies exhibit clear specialization, reflecting domain-specific characteristics. For example, Expert 1, which is more frequently assigned to natural language domains (e.g., Books, C4), tends to route tokens such as husband, wife, and lover. In contrast, Expert 6, which is associated with non-natural language domains (e.g., Arxiv, Github, StackExchange), predominantly routes code-related tokens like variable, env, and HEAD. Expert ID Routed Tokens With High Probability 0 1 3 4 5 6 7 End(100%); folding(100%); Fill(100%); FILE(100%); NULL(100%); byte(100%); Release(99.36%); Del(99.80%) husband(100%); ife(100%); baby(100%); human(100%); lover(99.60%); ).(99.86%); ),(99.71%); )...(98.425%) invest(100%); Fortune(100%); exec (100%); 0000(100%); Sorry(100%); bye(97.82%); If(97.74%); (97.63%) Conversely(100%); Methods(100%); flower(100%); Blossom(99.93%); Argentina(100%); Georgian(100%); Uruguay(98.90%); African (100%) Spring(100%); Winter(100%); seasons(99.02%); Temperature (100%); hot(97.98%); cold(100%) `e(100%); æ(99.80%); a(98.59%); Æ(97.67%) ]);(100%); gif(100%); size(100%); variable(100%); env(100%); begin(97.95%); HEAD(97.94%); (97.83%) Kuala(100%); Tus(100%); Lama(100%); Riley(98.94%) Autumn(100%); Summer(100%) Table 12: Routed tokens with high probability for experts in Layer 5 of ReMoE DOMAIN-LEVEL DYNAMIC EXPERT ALLOCATION IN REMOE We measure the average active expert count across different domains, as shown in Figure 12, and find that the computation allocation in ReMoE also varies at the domain level. Furthermore, this variation increases in deeper layers closer to the output. This is reasonable because deeper layers tend to capture more abstract and domain-specific features, leading to more pronounced specialization in expert activation. Figure 12: Domain-level dynamic expert allocation 19 TRAINING MOE WITH NEAR-DENSE WARMUP In ReMoE, the training process naturally progresses through three stages, with the first two involving near-dense training where the majority of experts are active. To facilitate fairer comparison, in Section 4.3, we train the MoE model for additional tokens to match the overall computational cost. In this section, we explore an alternative approach by introducing similar near-dense warmup phase for MoE, referred to as MoE with warmup, to align its computational footprint with ReMoE across each stage. Specifically, we train the MoE with = 182M, = 8, and = 6approximately matching the average sparsity of ReMoE during Stages and II, as depicted in Figure 4afor the first 100 steps, before transitioning to = 1 for the remainder of the training process. Table 13 compares this warmup variant to both standard MoE and ReMoE. The results indicate that the warmup phase provides modest improvement in validation loss compared to standard MoE, despite matching the overall computational cost. Nonetheless, ReMoE consistently outperforms both variants. This suggests that the three-stage training pipeline learned by ReMoE, with Stages and II comprising only the first 100 steps, is beneficial to overall performance. Model Valid Loss 1. 1.928 MoE MoE with warmup ARCc 20.82 20. ARCe 45.03 46.38 BoolQ HellaSwag LAMBADA PIQA RACE Avg. 57.55 52.35 29.84 30.28 31. 33.90 63.28 63.76 28.42 27.66 39. 39.29 ReMoE 1.921 20.22 46.68 54.16 30. 35.94 63.55 29.38 40.03 Table 13: Performance of MoE with near-dense warmup We further extend our experiments with MoE using warmup to configurations with larger E, which increases the computational cost of near-dense training. The results, summarized in Table 14, show that as increases, the warmup setting consistently improves performance. However, ReMoE still outperforms both variants, maintaining steeper performance scaling with respect to E. Valid Loss 1.936 1.928 Avg. Acc. 39.53 39. Model, =8 MoE MoE with warmup Model, =32 MoE MoE with warmup Valid Loss 1.874 1.869 Avg. Acc. 39.77 40. Valid Loss 1.852 1.841 Avg. Acc. 41.10 41. Model, =128 MoE MoE with warmup ReMoE 1.921 40. ReMoE 1.852 41.58 ReMoE 1.815 42. Table 14: Results for MoE with warmup under different expert count To further investigate the impact of warmup steps on MoE performance, we vary the number of warmup steps for the = 8 MoE configuration among 50, 100, 500, and 1000. The training curves of these models, along with standard MoE and ReMoE, are shown in Figure 13, and the final validation losses are summarized in Table 15. Our results reveal that performance does not improve monotonically with an increasing number of warmup steps, despite the additional computation. This behavior arises due to the discrepancy between the training objectives of = 6 (warmup phase) and = 1 (post-warmup phase). For instance, when warmup concludes after 100 steps, the transition between phases is smooth, with the loss changing minimally from 6.491 6.751. However, extending warmup to 500 or 1000 steps leads to more pronounced loss gap of 3.101 5.827 and 2.695 4.428, respectively. 20 Model Warmup Steps Valid Loss 0 50 100 500 -"
        },
        {
            "title": "ReMoE",
            "content": "1.937 1.930 1.928 1.930 1.931 1.921 Table 15: Final validation loss of MoE with different warmup steps Figure 13: Training curves of MoE with different warmup steps In summary, near-dense warmup can enhance the performance of TopK MoE when training from scratch by providing better initialization for the experts. However, the warmup phase should conclude while the language model loss is still decreasing rapidly. Prolonging the warmup can exacerbate the gap between the warmup and subsequent training phases, ultimately degrading performance. In contrast, ReMoE naturally determines the appropriate warmup steps and sparsity levels due to its continuous and differentiable training dynamics."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}