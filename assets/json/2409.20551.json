{
    "paper_title": "UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models",
    "authors": [
        "Qiaojun Yu",
        "Siyuan Huang",
        "Xibin Yuan",
        "Zhengkai Jiang",
        "Ce Hao",
        "Xin Li",
        "Haonan Chang",
        "Junbo Wang",
        "Liu Liu",
        "Hongsheng Li",
        "Peng Gao",
        "Cewu Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Previous studies on robotic manipulation are based on a limited understanding of the underlying 3D motion constraints and affordances. To address these challenges, we propose a comprehensive paradigm, termed UniAff, that integrates 3D object-centric manipulation and task understanding in a unified formulation. Specifically, we constructed a dataset labeled with manipulation-related key attributes, comprising 900 articulated objects from 19 categories and 600 tools from 12 categories. Furthermore, we leverage MLLMs to infer object-centric representations for manipulation tasks, including affordance recognition and reasoning about 3D motion constraints. Comprehensive experiments in both simulation and real-world settings indicate that UniAff significantly improves the generalization of robotic manipulation for tools and articulated objects. We hope that UniAff will serve as a general baseline for unified robotic manipulation tasks in the future. Images, videos, dataset, and code are published on the project website at:https://sites.google.com/view/uni-aff/home"
        },
        {
            "title": "Start",
            "content": "UniAff: Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models Qiaojun Yu1, Siyuan Huang1,7, Xibin Yuan1, Zhengkai Jiang2, Ce Hao3, Xin Li1, Haonan Chang4, Junbo Wang1, Liu Liu5, Hongsheng Li6, Peng Gao7(cid:66) and Cewu Lu1(cid:66) 4 2 0 2 0 3 ] . [ 1 1 5 5 0 2 . 9 0 4 2 : r Abstract Previous studies on robotic manipulation are based on limited understanding of the underlying 3D motion constraints and affordances. To address these challenges, we propose comprehensive paradigm, termed UniAff, that integrates 3D object-centric manipulation and task understanding in unified formulation. Specifically, we constructed dataset labeled with manipulation-related key attributes, comprising 900 articulated objects from 19 categories and 600 tools from 12 categories. Furthermore, we leverage MLLMs to infer objectcentric representations for manipulation tasks, including affordance recognition and reasoning about 3D motion constraints. Comprehensive experiments in both simulation and real-world settings indicate that UniAff significantly improves the generalization of robotic manipulation for tools and articulated objects. We hope that UniAff will serve as general baseline for unified robotic manipulation tasks in the future. Images, videos, dataset and code are published on the project website at:https://sites.google.com/view/uni-aff/home. I. INTRODUCTION Mastering the manipulation of tools and articulated objects is crucial for embodied robots in real-world environments. This requires understanding physical constraints and interaction regions in 3D space [1, 2]. For effective manipulation, identifying movable parts, joint types, 3D joint parameters, and affordances in articulated objects is essential [3, 4]. Similarly, modeling 6D pose, grasping regions and functional areas is vital for tool use in specific tasks [5, 6]. By integrating task-related 6D pose, 3D motion constraints, and affordances predictions, embodied robots can further enhance their adaptability and efficiency. Most existing approaches focus exclusively on either articulated objects [4, 7, 8] or tools [911], which limits their ability to generalize across tasks. By leveraging the reasoning capabilities of large language models (LLMs), two-stage methods are employed: the first stage predicts manipulationrelated parameters using vision model [2, 12], while the second stage utilizes the LLMs reasoning. Additionally, they address case-by-case problems without offering general task reasoning capabilities. To overcome these limitations, we propose UniAff, unified representation of affordances 1Qiaojun Yu, Siyuan Huang, Xibin Yuan, Xin Li, Junbo Wang, Cewu Lu are with the Shanghai Jiao Tong University, China. 2Zhengkai Jiang is with Hong Kong University of Science and Technology, HongKong. 3Ce Hao is with National University of Singapore, Singapore. 4Haonan Chang is with Rutgers University, United States of America. 5Liu Liu is with Hefei University of Technology, China.6Hongsheng Li is with CUHKMMLab, China. 7Siyuan Huang and Peng Gao are with the Shanghai AI Lab, China. * indicates the equal contribution. (cid:66)Peng Gao and Cewu Lu are the equal corresponding authors, gaopeng@pjlab.org.cn, lucewu@sjtu.edu.cn. Fig. 1. UniAff demonstrates its ability to unify tool usage and articulation understanding in VQA format, predicting part bounding boxes, 6D poses, grasp affordances, functional affordances, and manipulation types, etc for effective robotic manipulation tasks. for both tools and articulated objects, powered by visionlanguage models, as illustrated in Figure 1. UniAff combines object-centric manipulation with task understanding, utilizing multimodal large language models (MLLMs) to improve comprehension of 3D motion constraints and affordances. For the training of UniAff, We developed comprehensive dataset for articulated object manipulation and tool-use tasks, including 900 articulated objects over 19 categories and 600 tools in 12 categories. Each objects part-level 6D pose, grasp affordances, manipulation types, and functional affordances are labeled, forming multifunctional dataset for robot learning. By unifying the formulation of manipulation tasks, we incorporate object-centric 3D motion constraints and affordances. Near-realistic simulations generate pre-scanned meshes or URDF models [13], efficiently producing largescale dataset with automatically labeled information. To equip the MLLM with unified affordance capabilities, we fine-tune the SPHINX model [14] on our dataset, enabling UniAff to predict object-centric 3D representations and infer affordances for both tools and articulated objects. To our best knowledge, UniAff is the first model offering unified understanding of both categories, representing significant advancement in object-centric robotic manipulation. Extensive experiments in both simulation and real-world settings demonstrate UniAffs effectiveness. On the HANDAL dataset [10], UniAff outperformed LISA [15] by 11.5% and closely matched ManipVQA [9], with only 2.2% IOU difference, showcasing strong adaptation to real-world tasks. For articulated object manipulation, UniAff improved the success rate by 7.07% on unseen instances and 9.60% on unseen categories compared to A3VLM [7]. These results demonstrate UniAffs ability to generalize and adapt across tasks. In summary, our contributions are: 1) Introducing UniAff, an MLLM that facilitates understanding of fine-grained physical properties, 3D motion constraints, and affordances for manipulation. 2) Developing dataset with 1,500 objects across 19 categories of articulated objects and 12 categories of tools, labeled with part-level 6D poses, manipulation types and affordances. 3) Conducting comprehensive experiments that demonstrate UniAffs significant improvement in robotic manipulation generalization across articulated objects, and tools. II. RELATED WORK Datasets for Object-centric Manipulation. Objectcentric manipulation focuses on understanding object poses and affordances for various tasks. By focusing on object representation, object-centric policies are more transferable across robots, but they also require high-quality, diverse data on object poses and affordances. For affordance data, the RGB-D Part Affordance dataset [11] provides partlevel labels for 105 tools, while PhysObjects [16] offers annotations of household objects, focusing on physical properties. In object pose estimation, Omni6DPose [17] provides extensive pose annotations for real and simulated images. HANDAL [10], an early attempt to combine object affordance and pose estimation, includes graspable regions but is limited to 210 objects and only grasp affordances. To address these limitations, UniAff introduces large-scale synthetic dataset, utilizing 230 real-world scanned tools from the PACE [18] and OmniObject3D [19] datasets, articulated data from PartNet-Mobility dataset [20], along with 370 newly scanned tools. Additionally, we include 900 articulated object manipulations across 19 categories and 600 tool-use tasks across 12 categories. arios.** Instruction-based Robotic Manipulation. Instructionbased robotic manipulation connects abstract commands to low-level control. Early works like CLIPort [21] and 6D-CLIPort [22] utilize pre-trained text encoders, such as CLIP [23], for task-specific policies. Hiveformer [24] integrates language, observations, and action history, while Perceiver Actor [25] uses voxelized 3D data for efficient learning. However, pre-trained language encoders are limited to simple instructions and lack deeper reasoning for complex tasks. To address these limitations, LLMs have been applied to task planning [2628] and robot control code generation [29, 30]. Recent advances, such as RT-2-X [31], ManipLLM [8], and AIC-MLLM [32], incorporate multimodal LLMs (MLLMs) like LLaVA [33] and Sphinx [34], improving reasoning from both language instructions and visual input. However, these methods rely on modeling robot actions directly, requiring extensive real-world interaction data, which limits transferability across different robots. To overcome this, works like ManipVQA [9] and A3VLM [7] emphasize affordance reasoning and articulation awareness. Yet, ManipVQA focuses only on tool use, and A3VLM on articulated objects, restricting their real-world applications. To address this, we propose UniAff, unified representation for both tools and articulated objects. III. METHOD In this section, we introduce our method, which presents structured 3D spatial formulation for object representation in robotic manipulation. Our approach unifies task formulation by incorporating object-centric 3D spatial motion constraints and their corresponding affordances (Section III-A). To further enhance spatial intelligence, we developed synthetic dataset that applies this unified formulation to both tools and articulated objects (Section III-B). Finally, we finetune MLLMs to integrate object-centric formulations through Visual Question Answering (VQA) (Section III-C). A. Formulation of Structured Manipulation Task We define the manipulation task formulation as follows. An unknown object consists of movable parts, represented as = {mi}K i=1. We observe the object through an image and depth map D. Furthermore, we define the object structure for each part ψi as = {ψi}K i=1. The parameters for each part can be defined as ψi = {Ai, Bi, Gi, Fi, Ji, Li}, where Ai R43 represents the 6D pose of the part in the 3D space, Bi R42 represents the parts bounding box (BBOX), Gi R42 denotes the grasp affordance BBOX, and Fi R42 specifies the functional affordance BBOX, Ji indicates the joint type, and Li describes the parts state or function. Notably, for tools that consist of only single part, the entire object is treated as one part. To accurately represent each part, we use rotated BBOX defined by four key points to better fit its geometry and orientation. The B, and are defined by their four vertices (xi, yi)i=0,...,3, where each (xi, yi) represents the 2D coordinates of i-th vertex in the image. Since most articulated objects consist of one-dimensional prismatic or revolute joints, or combination of both [20], we categorize into four distinct manipulation types based on the manipulation policy, as illustrated in Figure 4, including bottle caps, revolute parts, sliding lids, and prismatic parts, respectively. For tools with 6 DOF, we introduce fifth manipulation type, termed as the freedom object. B. Synthetic Data Generation Based on the unified structure of part representation formulation ψ, which incorporates object-centric 3D spatial motion constraints and corresponding affordances as defined in the previous section, we generate synthetic data in the defined format ψ for tools and articulated objects. Acquiring real-world data is both costly and time-consuming, particularly due to the challenges in obtaining and annotating Fig. 2. The architecture of UniAff. The image features are first extracted using Mixed Visual Encoder, such as DINOv2, CLIP, or Q-Former, followed by an MLP projector. Next, language instructions are used to extract features with the Llama Tokenizer. Finally, the output of the structured manipulation tasks, such as Part BBOX, Affordance, and Revolute Parts, is used to execute robotic instructions. tive manipulation, as the use of tool is strongly correlated with its 6D pose in 3D space [5]. In our work, we utilized 230 tools from real-world scanned data sourced from the PACE [18] and OmniObject3D [19] datasets, supplemented by 370 additional tools that we scanned ourselves, covering 12 categories as shown in Figure 3: brush, razor, screwdriver, hair dryer, hammer, knife, spoon, spatula, power drill, flower shovel, fork, and ladle. To ensure consistency, all tools were aligned to common axis. Using Blender, we segmented the textured meshes into grasp affordance parts and functional affordance parts, with each tools segmentation process taking approximately 5 minutes. Building on these labels and leveraging near-realistic rendering technology in simulation [20], we rendered diverse cluttered scenes. For more details on data rendering, please refer to our website. Each scene was automatically labeled with key attributes defined in the formulation {A, B, G, F, , L}. The manipulation type of the tools is identified as the freedom object. 6D Pose (A): The is represented as 4 3 matrix, where the first 13 row represents the tools 3D spatial position, capturing the translational degrees, and the remaining 3 3 matrix captures its rotational degrees in 3D. Grasp Affordances (G): 2D BBOX defined by four vertices (xi, yi)i=0,...,3 is used to delineate the grasp area of the tool. Functional Affordances (F): 2D BBOX defined by four vertices (xi, yi)i=0,...,3 is used to represent the functional area of the tool. By varying the scene configurations, tool orientations, and surrounding objects, we ensured that the dataset encompassed wide range of scenarios, enabling the model to generalize effectively to real-world manipulation tasks. 2) Articulated Objects: To manipulate articulated objects effectively, it is crucial to develop comprehensive understanding of each parts manipulation type (J ), its corresponding joint axis (A) in 3D space, and the grasp affordances (G) associated with various object states. To establish unified representation structure, we relaIllustration of tools. The blue box indicates grasp affordance, the Fig. 3. red box indicates functional affordance and the orientation axis illustrates the objects pose. Illustration of manipulation types.(a) bottle cap, (b) revolute part, Fig. 4. (c) sliding lid, (d) prismatic part. The yellow box represents the object part, the blue box indicates grasp affordance, the red arrow marks the joint parameter, and the green arrow illustrates the manipulation trajectory. it. By leveraging near-realistic simulations to generate prescanned meshes or URDF models of objects, we efficiently create large-scale data across diverse scenes and object states. This synthetic data enables us to fine-tune VLM models while leveraging the capabilities of large models to achieve generalization from simulation to real-world tasks. Since the formulation of B, remains consistent, we present unified representation here and will not reiterate it below. Part BBOX (B): 2D BBOX defined by four vertices (xi, yi)i=0,...,3 is used to delineate the region of the part in the image. Descriptive Sentence (L): An explicit description of the parts state or function is essential for VLMs to interpret it effectively. 1) Tools: Modeling tools 6D pose, along with its grasp and functional affordances, is essential for precise and effecTABLE OVERVIEW OF THE STRUCTURED MANIPULATION TASKS. Capabilities Part Grounding. Tasks 2D-Part-Detection 6D Pose and Manipulation Type Understand. 6D-Pose-Detection Grasp-Affordance Understand. 2D-Grasp-Affordance Functional-Affordance Understand. 2D-Functional-Affordance Examples of Task Templates User: Please detect all manipulable parts and provide their 2D rotated bounding boxes. UniAff: There are manipulable object parts, each with corresponding 2D bounding box: B1, B2, ..., BN. User: Please detect the manipulation type of the object and provide the 6D pose. UniAff: Manipulation Type and its 6D Pose A. User: Please detect the grasp region of part with BBox and provide the 2D bounding box. UniAff: Here is the grasp affordance region BBox G. User: Please detect the functional region of part with BBox and provide the 2D bounding box. UniAff: Here is the functional affordance region BBox . Num. 130K 130K 130K 10K beled 900 objects across 19 categories from the PartNetMobility Dataset [20], including bottle, box, bucket, dispenser, door, folding chair, kitchen pot, laptop, microwave, refrigerator, safe, storage furniture, trash can, faucet, oven, table, toilet, kettle, and washing machine. Since the handles of some objects are not separate meshes or links, we modified the meshes or links of these objects to separate the handles into individual links. This modification allows for more accurate grasp affordance annotation, better aligning with our specific formulation ψ. Based on the definition of ψ in our task formulation, we rendered objects in diverse states. For more details on data rendering, please refer to our website. We present the detailed auto-labeled dataset as follows: Manipulation Type (J ): Each objects movable parts are classified based on their specific 3D motion properties, with joint types divided into four manipulation types (J ): bottle caps, sliding lids, revolute parts, and prismatic parts, as illustrated in Figure 4. Joint Axis (A): For articulated parts with 4 degrees of freedom (DOF), the joint axis (A) is defined by two points (xi, yi, zi)i=0,1, representing the joints axis in 3D space, while the remaining DOF components are masked. Grasp Affordances (G): An articulated objects affordances change based on its state. For example, closed door requires the handle to open, while an open door can be manipulated by the handle or edge. To account for this, we use sampling weights that prioritize the doors edge based on the joint state. The sampling weight is calculated as: weight = θ 180 θ , (1) where represents the current joint state and θ is predefined threshold value. 3) VQA Design: In our task formulation, we define six key parameters to describe each part: 6D Pose (A), Bounding Box (B), Grasp Affordance Region (G), Functional Affordance Region (F), Manipulation Type (J ), and Sentence Describing the Parts State or Function (L). To effectively address these six parameters, we utilize multiple Visual Question Answering (VQA) prompts to design four distinct tasks, as outlined in Table I. By guiding the model through series of task-specific prompts, we develop multi-task learning framework. By adopting this VQAbased approach, we integrate diverse manipulation-related prior knowledge into unified model, enabling it to handle complex manipulation tasks across wide range of object categories and scenarios. These multi-task MLLMs not only improve task efficiency but also enhance the models capacity to generalize across diverse real-world robotic manipulation tasks. 2D-Part-Detection Task: This the models ability to identify object parts and their corresponding bounding boxes (B). task improves 6D-Pose-Detection Task: This task enables the model to detect both the manipulation type (J ) and the 6D pose (A) of each part. Accurately determining the joint axis of articulated parts or the 6D pose of tools is essential for guiding the robots interactions with them, ensuring precise and efficient task execution. 2D-Grasp-Affordance Task: This task enhances the models ability to recognize the grasp region of the specified part (G). Accurately identifying these regions is crucial for the robot to effectively grasp the target object and successfully complete the task. 2D-Functional-Affordance Task: This task equips the model with the ability to recognize the functional region of the specified part (F), enabling accurate tool usage to complete tasks effectively. C. MLLMs-based Manipulation and Model Fine-tuning Using the Any Resolution strategy from [14], We propose novel robot manipulation algorithm, termed UniAff, as illustrated in Figure 2. Our model architecture is built upon SPHINX [14] and utilizes LLaMA2 [35] as the language backbone, enabling robust multimodal interaction between visual and linguistic inputs. The design is optimized for fine-grained visual analysis, focusing on capturing detailed regional object features critical for manipulation tasks. input images with 448 448 resolution are divided into smaller sub-images to preserve fine-grained details. To ensure comprehensive global and local visual grounding, we integrate the visual encoder from CLIP [23] to extract local semantic features. Additionally, DINOv2 [36] is incorporated to further enhance the models capacity for capturing detailed local semantics, while Q-Former [37] is employed to summarize global visual information. The local and global visual features are concatenated along the channel dimension to ensure thorough feature integration and improve visual understanding in manipulation tasks. Our fine-tuning strategy encodes affordances and 3D physical information within natural language representations, aligning training samples with the VQA framework. Consequently, we adopt cross-entropy loss as our primary training objective. To preserve the models broad visual reasoning capabilities, we incorporate general visual reasoning tasks alongside those specifically focused on predicting robotic affordances. The visual projection layers and the language model are jointly fine-tuned to ensure effective alignment between visual and linguistic modalities for affordance-based reasoning. Finally, the decoded manipulation information is combined with visual data using De-Tokenizer, enabling the completion of the specified task. IV. EXPERIMENTS In this section, we conduct comprehensive experiments in both simulation and real-world settings. We compare the performance of UniAff with several baseline models to address the following questions: 1) Can UniAff effectively ground the grasp and functional affordances of tools? 2) Can UniAff simultaneously construct 3D spatial motion constraints along with their corresponding affordances? 3) Can UniAff effectively generalize from perceptual priors to real-world applications? A. Experimental Settings Model Setting. We fine-tuned the vision-language model based on the SPHINX model [14], utilizing eight NVIDIA A100 GPUs, each with 80 GB of memory. The fine-tuning process was completed over three epochs, with total runtime of approximately 10 hours. To maintain the quality of the pre-trained features, the visual encoders remained frozen throughout the fine-tuning phase. The SPHINX model, obtained directly from the official repository, served as our pretrained foundation. Training was conducted with batch size of 4, and the learning rate was set to 2 105. Dataset Setting. We selected 9 categories from brush to power drill, creating training set of 450 tools, including 70 unseen instances. An additional 3 unseen categories include flower shovel, fork, and ladle, totaling 80 tools. We generated 10,000 diverse scenes using the training tools and 3,000 images for unseen instances. We selected 13 categories for articulated objects from bottle to trash can, resulting in training set of 502 objects and 160 unseen instances. Unseen categories are 6, from faucet to washing machine, totaling 238 objects. Each training object was rendered in 20 states from 5 perspectives, producing total of 50,200 images. The rendered data were translated according to the structured task formulation ψ to train UniAff, as shown in Table I. B. Robotic Affordance Detection Result Baselines and Metrics. Our robotic affordance evaluation leverages the HANDAL dataset [10], utilizing pixel-wise segmentation AP. While the baseline model in HANDAL identifies detects only whole objects, our UniAff model both complete objects and manipulable affordance regions effectively. However, because our model generates rotated bounding boxes, direct comparison of bounding box AP with the ground truth data, which uses standard bounding boxes, is not feasible. Instead, we provide qualitative comparison with visual examples of our models performance on our website. To facilitate comparison, we convert the rotated bounding boxes into standard ones and employ SAM [38] to TABLE II ROBOTIC AFFORDANCE EVALUATION RESULTS ON HANDAL DATASET LISA [15] ManipVQA [9] Ours Ha 0.671 0.745 0.742 Pd 0.426 0.439 0.394 Sd 0.624 0.799 0.747 La 0.407 0.620 0.650 Pan 0.453 0.622 0.680 Sp 0.578 0.646 0.621 St 0.397 0.638 0.597 Ut 0.494 0.584 0.435 Wh 0.494 0.683 0.712 AVG 0.505 0.642 0.620 Object abbreviations are listed in sequence: Hammer, Power Drill, Screwdriver, Ladle, Pan, Spatula, Strainer, Utensil, and Whisk. Results are reported as IOU (). generate segmentation masks. This enables us to compare our approach with LISA [15], which integrates LLM and SAM decoder, as well as ManipVQA-SAM [9], where SAM [38] converts the bounding box into segmentation masks. Results. As shown in Table II, UniAff demonstrates competitive performance even in zero-shot setting associated with SAM [38]. Trained solely on the simulation dataset, UniAff outperformed LISA by significant margin of 11.5% and achieved performance comparable to ManipVQA, with only 2.2% difference in IOU. Notably, UniAff does not have access to the HANDAL dataset, unlike ManipVQA. C. Tool Usage Understanding Evaluation Baselines and Metrics. We evaluate UniAffs tool usage understanding on the test splits of our dataset. Unlike the general affordance evaluation on HANDAL, this evaluation requires grounding both grasp affordance (e.g., grasp area) and functional affordance. To achieve more compact representation, we use rotated bounding boxes as ground truth. We compare our method against ManipVQA. Results. As shown in Table IV, UniAff excels at detecting both grasp affordances, with 32.5% improvement, and functional affordances of tools, with 56.9% improvement in IOU, compared to ManiVQA, which struggles with functional affordance reasoning. Along with the results in Table II, UniAff demonstrates superior generalization, likely due to its larger and more diverse dataset. D. Articulation Manipulation Evaluation Task and Metrics. We evaluated articulation tasks, including opening bottle caps, sliding lids, and both revolute and prismatic parts on seen categories. For unseen categories, tasks involved manipulating sliding lids and revolute/prismatic parts. The evaluation covered 160 unseen instances from trained categories and 238 objects from new categories. Success was defined as binary measure, with joint state change exceeding the threshold δ = 0.1: success = 1(δchange δ). Baselines. We compare our proposed method with three different baselines under setting: the (1)Where2Act [39] identifies high-actionability manipulation points and generates short-term actions (e.g., pushing, pulling) at each point for interacting with articulated objects. (2) UMPNet [41] infers action sequences for manipulating objects through self-guided exploration and an Arrow-ofTime attribute. We adapted it by replacing suction with gripper. (3)A3VLM[7] translates object representations into robot actions utilizing simple primitives. We modified it by using gripper and GraspNet[42] to generate and select the highest-scoring grasp pose. identical TABLE III ARTICULATED OBJECT MANIPULATION RESULTS Where2Act [39] UMPNet [40] A3VLM [7] UniAff (w/o afford.) UniAff (ours) Bottle Cap 0.2034 0.2787 0.4895 0.5166 0.5259 Unseen Instances Sliding Lid 0.1921 0.2535 0.5969 0.6321 0.6642 Revolute Part 0.0897 0.3521 0.4397 0.4506 0. Unseen Categories Prismatic Part 0.1093 0.3000 0.5231 0.5360 0.6001 Sliding Lid 0.1535 0.2789 0.4535 0.4820 0.5730 Revolute Part 0.0822 0.3201 0.4906 0.5341 0.5913 Prismatic Part 0.0861 0.3158 0.4387 0.4519 0.5064 Unseen instances consist of data from four object categories utilized in model training. Unseen categories encompass data from completely new categories. Results are reported as success rates (). TABLE IV TOOL USAGE UNDERSTANDING EVALUATION RESULTS Task Grasp Function Model ManipVQA [9] 2 Points BBox 224 Resolution UniAff (ours) ManipVQA [9] 2 Points BBox 224 Resolution UniAff (ours) Sl 0.329 0.346 0.263 0.751 0.203 0.610 0.267 0. Sp 0.487 0.511 0.446 0.692 0.271 0.690 0.527 0.735 Kf 0.449 0.463 0.377 0.768 0.234 0.484 0.413 0.779 Unseen Instances Pd 0.461 0.596 0.478 0.758 0.043 0.624 0.254 0.749 Ra 0.343 0.375 0.333 0.717 0.068 0.476 0.233 0.652 Hd 0.455 0.487 0.406 0.768 0.303 0.657 0.491 0.767 Ha 0.309 0.394 0.357 0.684 0.126 0.502 0.201 0. Br 0.360 0.445 0.385 0.740 0.106 0.544 0.413 0.755 Sd 0.534 0.556 0.390 0.834 0.058 0.234 0.219 0.689 Unseen Categories Fr Ld Fs 0.319 0.304 0.420 0.330 0.290 0.420 0.298 0.236 0.319 0.641 0.606 0.713 0.140 0.169 0.298 0.551 0.686 0.594 0.333 0.349 0.482 0.703 0.745 0.780 AVG 0.398 0.434 0.357 0.723 0.168 0.554 0.349 0.737 Object abbreviations are listed in sequence: Spatula, Spoon, Knife, Razor, Power Drill, Hair Dryer, Hammer, Brush, Screwdriver, Flower Shovel, Ladle, and Fork. Results are reported as IOU (). Results. We present the success rates in Table III. By explicitly modeling the articulation structure, both A3VLM [7] and UniAff demonstrate superior performance. The performance of UniAff is further enhanced by improved affordances. UniAff achieved 7.07% improvement in success rates for unseen instances and 9.60% improvement for unseen categories compared to A3VLM. Fig. 5. Implementation of UniAff in real-world experiments progressed from tool manipulation to articulated object interaction, encompassing tasks such as striking designated target with hammer, opening drawer, refrigerator, microwave, pot, and lifting bucket handle. E. Ablation Studies To assess UniAffs design contributions, we conducted ablation studies. Results in Table IV indicate that omitting the Any Resolution method (e.g., using 224 224 resolution) significantly degrades performance, highlighting the necessity of high-resolution input for partial object understanding. Additionally, the use of 2-point (standard bounding box) representation also reduces performance, as rotated bounding boxes provide more compact representation for affordance understanding. F. Real-World Experiments We conducted real-world experiments using 7-DoF Flexiv robot. First, we mounted an RGB-D RealSense D435 camera on the robots wrist to capture RGB-D images of various objects. We then applied UniAff to predict objectcentric 3D motion constraints and corresponding affordances. The experiments involved six tasks: striking designated target with hammer, opening drawer, opening refrigerator, opening microwave, opening pot, and lifting bucket handle. These tasks demonstrated UniAffs ability to generalize effectively to real-world manipulation tasks, as illustrated in Figure 5. Due to space constraints, detailed information is provided in the website. V. CONCLUSION In this paper, we propose UniAff, novel approach integrating object-centric 3D motion constraints and affordances for manipulation tasks. By leveraging multimodal large language models (MLLMs), UniAff improves manipulation knowledge and generates precise 3D motion constraints and affordances for diverse objects. Our extensive dataset labeled with manipulation-related key attributes, comprising 900 articulated objects and 600 tools, serves as foundation for training UniAff to generalize across wide range of tasks. Experiments show UniAff significantly outperforms existing methods in both simulation and real-world environments, demonstrating superior generalizability in manipulation tasks involving tools and articulated objects. REFERENCES [1] L. P. Kaelbling, The foundation of efficient robot learning, Science, vol. 369, no. 6506, pp. 915916, 2020. [2] H. Geng, S. Wei, C. Deng, B. Shen, H. Wang, and L. Guibas, Sage: Bridging semantic and actionable parts for generalizable articulated-object manipulation under language instructions, arXiv preprint arXiv:2312.01307, 2023. [3] H. Geng, H. Xu, C. Zhao, C. Xu, L. Yi, S. Huang, and H. Wang, Gapartnet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 7081 7091. [4] Q. Yu, J. Wang, W. Liu, C. Hao, L. Liu, L. Shao, W. Wang, and C. Lu, Gamma: Generalizable articulation modeling and manipulation for articulated objects, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 54195426. [5] G. Li, N. Tsagkas, J. Song, R. Mon-Williams, S. Vijayakumar, K. Shao, and L. Sevilla-Lara, Learning precise affordances from egocentric videos for robotic manipulation, arXiv preprint arXiv:2408.10123, 2024. [6] H. Huang, F. Lin, Y. Hu, S. Wang, and Y. Gao, Copa: General robotic manipulation through spatial constraints of parts with foundation models, arXiv preprint arXiv:2403.08248, 2024. [7] S. Huang, H. Chang, Y. Liu, Y. Zhu, H. Dong, P. Gao, A. Boularias, and H. Li, A3vlm: Actionable articulation-aware vision language model, arXiv preprint arXiv:2406.07549, 2024. [8] X. Li, M. Zhang, Y. Geng, H. Geng, Y. Long, Y. Shen, R. Zhang, J. Liu, and H. Dong, Manipllm: Embodied multimodal large language model for object-centric robotic manipulation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 061 18 070. [9] S. Huang, I. Ponomarenko, Z. Jiang, X. Li, X. Hu, P. Gao, H. Li, and H. Dong, Manipvqa: Injecting robotic affordance and physically grounded information into multi-modal large language models, arXiv preprint arXiv:2403.11289, 2024. [10] A. Guo, B. Wen, J. Yuan, J. Tremblay, S. Tyree, J. Smith, and S. Birchfield, Handal: dataset of real-world manipulable object categories with pose annotations, affordances, and reconstructions, in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 11 42811 435. [11] A. Myers, C. L. Teo, C. Fermuller, and Y. Aloimonos, Affordance detection of tool parts from geometric features, in 2015 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2015, pp. 13741381. [12] W. Xia, D. Wang, X. Pang, Z. Wang, B. Zhao, D. Hu, and X. Li, Kinematic-aware prompting for generalizable articulated object manipulation with llms, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 20732080. [13] D. Tola and P. Corke, Understanding urdf: dataset and analysis, IEEE Robotics and Automation Letters, 2024. [14] Z. Lin, C. Liu, R. Zhang, P. Gao, L. Qiu, H. Xiao, H. Qiu, C. Lin, W. Shao, K. Chen et al., Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models, arXiv preprint arXiv:2311.07575, 2023. [15] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, and J. Jia, Lisa: Reasoning segmentation via large language model, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 95799589. [16] J. Gao, B. Sarkar, F. Xia, T. Xiao, J. Wu, B. Ichter, A. Majumdar, and D. Sadigh, Physically grounded vision-language models for robotic manipulation, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 12 46212 469. [17] J. Zhang, W. Huang, B. Peng, M. Wu, F. Hu, Z. Chen, B. Zhao, and H. Dong, Omni6dpose: benchmark and model for universal 6d object pose estimation and tracking, arXiv preprint arXiv:2406.04316, 2024. [18] Y. You, K. Xiong, Z. Yang, Z. Huang, J. Zhou, R. Shi, Z. Fang, A. W. Harley, L. Guibas, and C. Lu, Pace: Pose annotations in cluttered environments, Springer, 2024. [19] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian et al., Omniobject3d: Largevocabulary 3d object dataset for realistic perception, reconstruction and generation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 803814. [20] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su, SAPIEN: simulated part-based interactive environment, in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [21] M. Shridhar, L. Manuelli, and D. Fox, Cliport: What and where pathways for robotic manipulation, in Conference on robot learning. PMLR, 2022, pp. 894906. [22] K. Zheng, X. Chen, O. C. Jenkins, and X. Wang, Vlmbench: compositional benchmark for vision-and-language manipulation, Advances in Neural Information Processing Systems, vol. 35, pp. 665678, 2022. [23] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PMLR, 2021, pp. 87488763. [24] P.-L. Guhur, S. Chen, R. G. Pinel, M. Tapaswi, I. Laptev, and C. Schmid, Instruction-driven history-aware policies for robotic manipulations, in Conference on Robot Learning. PMLR, 2023, pp. 175187. [25] M. Shridhar, L. Manuelli, and D. Fox, Perceiver-actor: multi-task transformer for robotic manipulation, in Conference on Robot Learning. PMLR, 2023, pp. 785799. [26] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser, Tidybot: Personalized robot assistance with large language models, Autonomous Robots, vol. 47, no. 8, pp. 10871102, 2023. [27] W. Cai, S. Huang, G. Cheng, Y. Long, P. Gao, C. Sun, and H. Dong, Bridging zero-shot object navigation and foundation models through pixel-guided navigation skill, arXiv preprint arXiv:2309.10309, 2023. [28] H. Chang, K. Gao, K. Boyalakuntla, A. Lee, B. Huang, H. U. Kumar, J. Yu, and A. Boularias, Lgmcts: Languageguided monte-carlo tree search for executable semantic object rearrangement, arXiv preprint arXiv:2309.15821, 2023. [29] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, Code as policies: Language model programs for embodied control, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 94939500. [30] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei, Voxposer: Composable 3d value maps for robotic manipulation with language models, arXiv preprint arXiv:2307.05973, 2023. [31] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan et al., Open x-embodiment: Robotic learning datasets and rt-x models, arXiv preprint arXiv:2310.08864, 2023. [32] C. Xiong, C. Shen, X. Li, K. Zhou, J. Liu, R. Wang, and H. Dong, Autonomous interactive correction mllm for robust robotic manipulation, in 8th Annual Conference on Robot Learning, 2024. [33] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, Advances in neural information processing systems, vol. 36, 2024. [34] P. Gao, R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin et al., Sphinx-x: Scaling data and parameters for family of multi-modal large language models, arXiv preprint arXiv:2402.05935, 2024. [35] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [36] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. ElNouby et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. [37] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in International conference on machine learning. PMLR, 2023, pp. 19 73019 742. [38] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., Segment anything, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 40154026. [39] K. Mo, L. J. Guibas, M. Mukadam, A. Gupta, and S. Tulsiani, Where2act: From pixels to actions for articulated 3d objects, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 68136823. [40] R. Wu, Y. Zhao, K. Mo, Z. Guo, Y. Wang, T. Wu, Q. Fan, X. Chen, L. Guibas, and H. Dong, Vat-mart: Learning visual action trajectory proposals for manipulating 3d articulated objects, arXiv preprint arXiv:2106.14440, 2021. [41] Z. Xu, Z. He, and S. Song, Universal manipulation policy network for articulated objects, IEEE robotics and automation letters, vol. 7, no. 2, pp. 24472454, 2022. [42] H.-S. Fang, C. Wang, M. Gou, and C. Lu, Graspnet-1billion: large-scale benchmark for general object grasping, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 44411 453."
        }
    ],
    "affiliations": [
        "CUHKMMLab, China",
        "Hefei University of Technology, China",
        "Hong Kong University of Science and Technology, HongKong",
        "National University of Singapore, Singapore",
        "Rutgers University, United States of America",
        "Shanghai AI Lab, China",
        "Shanghai Jiao Tong University, China"
    ]
}