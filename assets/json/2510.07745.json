{
    "paper_title": "Parallel Test-Time Scaling for Latent Reasoning Models",
    "authors": [
        "Runyang You",
        "Yongqi Li",
        "Meng Liu",
        "Wenjie Wang",
        "Liqiang Nie",
        "Wenjie Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \\ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS."
        },
        {
            "title": "Start",
            "content": "Parallel Test-Time Scaling for Latent Reasoning Models Runyang You1, Yongqi Li1, Meng Liu2, Wenjie Wang3, Liqiang Nie4, Wenjie Li1 1The Hong Kong Polytechnic University 2Shandong Jianzhu University 3University of Science and Technology of China 4Harbin Institute of Technology (Shenzhen) runyang.y@outlook.com liyongqi0@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Parallel test-time scaling (TTS) is pivotal approach for enhancing large language models (LLMs), typically by sampling multiple tokenbased chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open new direction for scalable inference in continuous spaces. Code released at here. 5 2 0 2 9 ] . [ 1 5 4 7 7 0 . 0 1 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved remarkable performance on challenging tasks through test-time scaling (TTS), where more inference compute leads to better results. Such scaling is often realized through explicit Chain-of-Thought (CoT) reasoning (Wei et al., 2022), where models verbalize intermediate solving steps in natural language, generate long token sequences over which Corresponding author. 1 compute can be scaled. One promising direction is to scale in parallel (Fu et al., 2025; Qu et al., 2025; Snell et al., 2024), which samples multiple reasoning paths and aggregates outcomes via methods such as majority voting (Wang et al., 2022), bestof-N (Zhou et al., 2022), or guided search (Uesato et al., 2022). Through parallel scaling, models transform extra inference compute directly into stronger capabilities, without any need for parameter updates or retraining. Moving beyond explicit token-by-token reasoning, recent works show that reasoning can instead unfold in latent space, with vector representations replacing tokens as the intermediate steps in autoregressive generation (Li et al., 2025; Chen et al., 2025a). This paradigm, also known as continuous CoT (CCOT), has the potential to match or even surpass explicit CoT while being more compact and efficient, resembling intuitive human reasoning (Hao et al., 2024; Shen et al., 2025; Tan et al., 2025; Wu et al., 2025). By compressing or distilling explicit reasoning into continuous representations, these methods not only speed up inference but also capture abstract patterns difficult to express in language, positioning latent reasoning as promising direction for advancing LLM capabilities. Given the promising potential of latent reasoning, and that parallel TTS can effectively utilize additional generated tokens to deliver superior performance (Wang et al., 2022; Muennighoff et al., 2025; Snell et al., 2024), natural question arises: can latent reasoning models also benefit from parallel TTS? Extending parallel TTS into latent reasoning models is appealing but non-trivial. First, latent reasoning models lack the fundamental sampling capability: while token-based models generate logits that enable commonly-used sampling strategies like top-k or nucleus samplingas illustrated in Figure 1a, latent models operate on continuous vectors without an explicit probability distribution, making it difficult to produce diverse reasoning paths. Second, the aggregation mechanism used in token-based models does not directly apply to latent reasoning models: while token-based methods typically utilize token-level probabilities to rank reasoning trajectories, latent reasoning models provide no inherent likelihoods or stepwise scores, making it difficult to evaluate or aggregate the sampled latent reasoning paths. To address these challenges, we re-think both sampling and aggregation for latent reasoning, proposing effective solutions tailored to the continuous setting. For sampling, to ensure informativeness and controllability of the sampling space, we draw on uncertainty estimation theory (Gawlikowski et al., 2023; Liu et al., 2019) and propose two simple yet effective strategies to introduce stochasticity into latent reasoning: Monte Carlo dropout (MC-dropout) to capture epistemic uncertainty; and Additive Gaussian Noise (AGN) to simulate aleatoric uncertainty, as illustrated in Figure 1. For aggregation, we propose the Latent Reward Model (LatentRM), dedicated scorer that evaluates and guides the progression of latent reasoning at each intermediate step. LatentRM is trained via step-wise contrastive objective that discriminates among candidate thoughts at each reasoning step, enabling fine-grained, position-sensitive scoring. Building on these designs, extensive experiments and visualization analyses reveal that both proposed sampling strategies not only scale effectively with increased compute but also exhibit distinct exploration dynamics in latent space: MC-Dropout promotes structured, directed expansion toward unconventional solutions, resulting in higher coverage, whereas Additive Gaussian Noise drives broad and isotropic exploration that enriches diversity around the deterministic center. For aggregation, LatentRM enables consistent gains with best-of-N and beam search across compute budgets. Ablation studies confirm that contrastive supervision and stochastic rollouts are both crucial. Overall, our findings demonstrate that parallel TTS can transfer to latent reasoning models through redesigned sampling and aggregation, opening new pathway for scalable inference in latent space. Collectively, core contributions are as follows: We introduce parallel test-time scaling into latent reasoning models, enabling key scaling capability that was previously exclusive to token-based reasoning paradigms. We address the fundamental challenge of sampling in continuous latent space by proposing two complementary strategiesMonte Carlo Dropout and Additive Gaussian Noise to enable controlled and informative stochastic latent reasoning. We design the Latent Reward Model (LatentRM), dedicated scorer under step-wise contrastive supervision to evaluate and guide latent reasoning, enabling effective aggregation in the latent setting."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Test-Time Scaling Large language models increasingly rely on testtime scaling (TTS)allocating more computation at inference to improve reasoning quality. Two main axes have emerged. The sequential axis advances by generating longer reasoning traces (Muennighoff et al., 2025; Wang et al., 2025), strategy employed in recent reasoning-oriented models (DeepSeek-AI et al., 2024; Yang et al., 2025; You et al., 2025; Fu et al., 2025; Lee et al., 2025). The parallel axis generates multiple reasoning trajectories and aggregates them, as in SelfConsistency (Wang et al., 2022), Best-of-N (Zhou et al., 2022), and tree-structured exploration methods (Uesato et al., 2022). These two axes are often combined, enabling flexible accuracycompute trade-offs under deployment constraints. Recent works have sought to improve the efficiency of TTS primarily by refining aggregation strategies and decision rules, such as adaptive voting and confidence-based stopping criteria (Snell et al., 2024; Brown et al., 2024). Yet all of these approaches ultimately rest on token-level sampling, decoding by drawing tokens from the predictive distribution. Such mechanism is intrinsic to tokenbased reasoning, but is fundamentally incompatible with latent reasoning, which operates over continuous representations."
        },
        {
            "title": "2.2 Latent Reasoning",
            "content": "Chain-of-Thought (CoT) reasoning has proven highly effective, yet it remains constrained by its reliance on natural language (Zhu et al., 2025). This introduces inefficiencymost tokens add little value and fail to capture cognition like abstract insights or intuitive leaps, making step-by-step verbalization an unnatural limit. These limitations moti2 (a) Token-based Sampling (b) Monte Carlo Dropout (c) Additive Gaussian Noise Figure 1: Sampling mechanisms for token-based generation (1a) and our proposed approaches for the latent setting (1b & 1c). (1a): Multinomial sampling over token probabilities at each step. (1b): Monte Carlo Dropout (MC-dropout): Stochastic inference via randomly sampled dropout masks. (1c): Additive Gaussian Noise (AGN): independent Gaussian perturbations injected to each latent thought. vate the shift toward latent reasoning (Chen et al., 2025c,a). One line of work pursues architectural modifications, such as dynamically skipping or repeating transformer layers, or introducing auxiliary modules to adjust computational cost (Chen et al., 2025b; Geiping et al., 2025; Mohtashami et al., 2024). More recently, research has shifted to latent autoregressive reasoning, where models build trajectories directly in hidden space. COCONUT (Hao et al., 2024) pioneered latent generation using the last hidden state as the next thought, training via curriculum learning, while CODI (Shen et al., 2025) introduced latent autoregression through self-distillation. More recently, CoLaR (Tan et al., 2025) applied reinforcement learning with dynamic latent compression. CoLaR (Tan et al., 2025) and SoftCoT++ (Xu et al., 2025) made initial attempts to inject noise for stochastic latent reasoning, but these efforts remain preliminary. Overall, existing methods leave open the question of how latent reasoning can fully benefit from stochastic exploration and TTS. To address this gap, we develop and systematically analyze stochastic sampling and aggregation strategies in latent space, establishing foundational methods and findings for parallel TTS with latent reasoning models."
        },
        {
            "title": "3 Preliminaries",
            "content": "We begin by formalizing latent reasoning and outlining the inference framework that underlies our test-time scaling approach. Latent Reasoning. latent reasoning model performs autoregressive generation in continuous hidden space. At each step, it produces latent vectorthe last hidden state from the transformer backbonethat represents an intermediate reasoning step. This formulation bypasses token-level verbalization, providing more compact and efficient alternative to standard explicit CoT reasoning. Inference Process. Let the input sequence be = [x1, x2, . . .], the latent reasoning trajectory as h1:T = [h1, h2, . . . , hT ] RT , where each ht Rd represents the hidden state (thought) at step t. At inference time, the model generates the next latent thought autoregressively: ht+1 = fθ(h1:t, x), where fθ is the LLM transformer blocks parameterized by θ. This process ends with the endof-thinking token (<EoT>), which triggers the transition to explicit token generation for the final answer. Reasoning Length Control. Different latent reasoning models control the reasoning length in distinct ways. COCONUT (Hao et al., 2024) and CODI (Shen et al., 2025) use predetermined sequence lengths, where fixed is set and the <EoT> is manually inserted. CoLaR (Tan et al., 2025) introduces dynamic latent compression: the reasoning length (or thinking speed) is adjusted by compression factor specified in the prompt (e.g., thinking speed = 5), allowing the model to adaptively decide when to emit the EoT token and terminate reasoning."
        },
        {
            "title": "4 Method",
            "content": "Building on the latent reasoning formulation above, we now introduce our test-time scaling framework, which consists of two key components: stochastic sampling and latent trajectory aggregation."
        },
        {
            "title": "4.1 Stochastic Sampling in Latent Space",
            "content": "To scale latent reasoning at test time, stochasticity must be introduced into the latent generation process, enabling the model to produce di3 verse set of trajectories {h(n)}N n=1, where each h(n) = [h(n) 1 , . . . , h(n) ] represents sampled sequence of latent thoughts. However, arbitrary noise can easily distort reasoning process. To ensure that sampling is both controllable and meaningful, we draw on uncertainty estimation theory, which delineates two sources of uncertainty that provide principled probabilistic spaces for sampling: (1) epistemic uncertainty, reflecting variability due to the models limited knowledge, and (2) aleatoric uncertainty, arising from noise or ambiguity inherent in the inputs. Accordingly, we propose two complementary strategies: Monte Carlo dropout (MCdropout) to capture epistemic uncertainty, and Additive Gaussian Noise (AGN) to simulate aleatoric uncertainty. An overview of the two mechanisms is shown in Figure 1b and Figure 1c. We now detail the sampling procedures, and their grounding in uncertainty, with formal derivations in Appendix A. MC-dropout. At inference, we keep dropout active with rate by sampling binary mask m(n) Bernoulli(p) and applying it to the model weights θ(n) during each forward pass, as illustrated in Figure 1b. Formally, this yields t+1 = fθ(n)(h(n) h(n) 1:t , x). Following prior practice (Gao et al., 2021; Vaswani et al., 2017), dropout is applied after the feedforward layer within each Transformer block. Such placement captures epistemic uncertainty, which is the variability arising from the model itself, as each thought corresponds to different plausible configuration of its weights. AGN. As complementary sampling strategy, we add isotropic Gaussian noise directly to the latent thoughts to induce controlled stochasticity, as illustrated in Figure 1c. Specifically, random perturbation is drawn and applied at each reasoning step as ϵ(n) (0, σ2I), h(n) = h(n) + ϵ(n) , where is the identity matrix and ϵ(n) denotes zeromean Gaussian noise with standard deviation σ. The model then continues autoregressive inference based on the perturbed trajectory: h(n) t+1 = fθ (cid:16) h(n) 1:t (cid:17) , . This procedure introduces variance controlled solely by the noise scale σ, independent of the model parameters, thereby modeling aleatoric uncertainty. 4.2 Latent Trajectories Aggregation key challenge in extending parallel TTS to latent reasoning lies in aggregation. Unlike tokenbased TTS, where log-likelihoods provide natural scoring mechanism, latent trajectories are continuous vectors without explicit scores. This prevents direct application of best-of-N or beam search. While process reward models (PRMs) (Zhang et al., 2024; Wang et al., 2024) could be natural choice for evaluating intermediate reasoning steps, latent thoughts are abstract vectors without linguistic form and cannot be interpreted or scored by existing PRMs. To this end, we propose the Latent Reward Model (LatentRM), dedicated reward model for latent thoughts that enables effective aggregation strategies. Architecture. We extend the latent reasoning backbone with an additional scoring head that maps hidden states to scalar. This scoring head, LatentRM gϕ, takes in the input prompt and the generated latent trajectory up to step t, and outputs score: rt = gϕ(x, h1:t) R, which estimates the promise of continuing from the current thought ht. Inference. During inference, LatentRM evaluates each candidate trajectory by computing the sum of logits (cid:80) rt over the generated sequence, serving as proxy for the relative quality of thought h1:t. This logit-summing strategy is justified in Appendix B, which shows that cumulative logits can solely determine trajectory ranking. Data. To supervise the learning of LatentRM, we construct thought-level quality labels by estimating the empirical correctness of each intermediate thought. Specifically, for each input x, we sample trajectories = {h(n)}N n=1, and for every thought within every trajectory, we rollout stochastic completions, obtaining set of final answers{am}M m=1, and compute ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) m=1 {am = a} , as proxy for the quality of thought, where denotes the ground-truth answer and I{} as the indicator function. Objective. straightforward approach is to treat each thought independently and optimize LatentRM with binary cross-entropy (BCE) loss, which is commonly used for training PRMs. However, this approach performs poorly in practice, as it provides only isolated supervision per candidate and lacks relative comparison across thoughts at the same step. To address this, we adopt step-wise contrastive formulation. At each step t, the scores of all candidates are compared via softmax, p(n) = ) exp(r(n) n=1 exp(r(n) (cid:80)N . ) LatentRM is then trained with the negative loglikelihood loss = (cid:88) (cid:88) n=1 y(n) log p(n) ."
        },
        {
            "title": "5.1 Benchmarks and Models.",
            "content": "Benchmarks. Following previous works (Shen et al., 2025; Tan et al., 2025; Hao et al., 2024), evaluation is conducted on three benchmarks: (1) GSM8K-Test, the official test split of GSM8K comprising 1.3k samples; (2) GSM8K-Hard (Gao et al., 2022), more challenging version of GSM8K-Test where numbers are scaled to larger magnitudes to increase problem difficulty, and (3) MultiArith (Uesato et al., 2022), dataset with 600 test samples focusing on multi-step arithmetic reasoning. Latent reasoning models/backbones. We evaluate three representative latent reasoning models: (1) COCONUT (Hao et al., 2024), which progressively replaces CoT steps with latent thoughts; (2) CODI (Shen et al., 2025), which self-distills CoT into latent space; and (3) CoLaR (Tan et al., 2025), which performs dynamic latent compression with reinforcement learning. All experiments leverage officially released checkpoints, where COCONUT and CODI are backboned on GPT-2, and CoLaR on Llama-3.2-1B. Following the default configurations in the original papers, for COCONUT and CODI, we fix the number of latent thoughts to = 6; for CoLaR, thinking speed set to 2 with maximum upper limit of 64 latent thoughts. 5 5.2 Sampling Evaluation To evaluate the effectiveness of different stochastic sampling methods, we measure how well each method scales with the number of sampled trajectories. Specifically, coverage quantifies the fraction of problems for which at least one of the sampled trajectories yields the correct answer: coverage = 1 (cid:88) xD I{n : a(n) = a}, This metric is equivalent to pass@k when sampling size and cutoff are identical (Zhang et al., 2024; Brown et al., 2024). Higher coverage indicates stronger sampling effectivenessi.e., greater ability to uncover correct reasoning paths as the inference budget increases. All sampling schemes are evaluated under equal for fair comparison."
        },
        {
            "title": "5.3 Aggregation Evaluation",
            "content": "We evaluate the proposed LatentRM through two of its supported aggregation strategies: (1) best-ofN selection scored by LatentRM, (2) beam search guided by LatentRM, compared against majority voting as non-parametric baseline. To ensure fairness, we match all methods under the same compute budget: best-of-N and majority voting use independent samples, while beam search for comparable decodadopts beam size of ing cost (Zhang et al., 2024; Snell et al., 2024). Training configurations and optimization details for LatentRM are provided in Appendix D, and full inference procedures for aggregation strategies are described in Appendix C."
        },
        {
            "title": "6.1 Main Results",
            "content": "We systematically evaluate both stochastic sampling strategies by varying the sample count (N) and measuring solution coverage. We tune the AGN hyperparameter σ over [0.01, 1.5] and the MAC-Dropout probability over [0, 1], using binary search within each interval to maximize coverage@64. We then plot two curves using the optimal hyperparameters for each method. Figure 2 presents four key findings: (1) both MC-dropout and AGN can effectively scale, as coverage increases monotonically with larger sample sizes; (2) the marginal improvement diminishes as grows, suggesting saturation effect where additional samples contribute less to coverage gains; (3) increased (a) GSM-Test (b) GSM-Hard Figure 3: Coverage versus diversity for MC-dropout (red) and AGN (blue) with {4, 8, 16, 32} by sweeping and σ to span range of diversity values. Darker shades indicate larger . Results are shown for COCONUT (left) and CODI (right) on GSM-Test. (c) MultiArith Figure 2: Coverage (%) versus plot for COCONUT, CODI, and CoLaR on GSM-Test (2a), GSM-Hard (2b) and MultiArith (2c). Each subplot compares MCdropout and AGN using the optimal hyperparameter. Higher coverage indicates larger fraction of problems solved by attempts. Results are reported as the mean over three runs. sampling narrows inter-model performance gaps. Notably, at N=64, COCONUT and CODI achieve nearly equivalent coverage, despite CODIs clear superiority at N=1. (4) MC-dropout consistently achieves higher coverage across nearly all , highlighting its advantage as more reliable stochastic sampling approach."
        },
        {
            "title": "6.2 Analysis on Diversity",
            "content": "The key question to ask is: what makes good sampling strategy for latent reasoning? Beyond simply scaling the sampling budget to boost problem-solving success, good sampling strategy should encourage diverse reasoning trajectories rather than expending computational effort on redundant or overly similar paths. Importantly, this pursuit of diversity must not come at the cost of overall coverage. Definition. To formalize this intuition, we introduce the notion of sampling diversity as the average Figure 4: Diversity of latent trajectories across reasoning steps on GSM-Test with COCONUT. Left: MCdropout (p = 0.1 - 0.5). Right: AGN (σ = 0.1 - 0.5). pairwise cosine dissimilarity among latent thoughts across reasoning steps: diversity = 1 DT (cid:88) (cid:88) xD t=1 dt(x), where dt(x) = 2 (N 1) (cid:80) i<j(1 cos(h(i) , h(j) )), measures the dissimilarity at step t. higher diversity score reflects greater variability in the reasoning paths taken, suggesting richer exploration of the problem space. Diversity vs. coverage. To understand the interplay between diversity and performance, we plot coverage against diversity across different sampling method, while sweeping and σ under varying . The resulting trade-off curves are visualized in Figure 3. Key insights are as follow. (1) Across models and methods, clear sweet spot emerges, with coverage peaking at moderate diversity. This suggests that while some level of stochasticity is beneficial, excessive or insufficient diversity can hinder performance; and (2) at larger diversity levels, AGN tends to maintain or even improve coverage, whereas MC-dropout shows sharp decline. 6 This highlights AGNs superior ability to preserve solution quality even when the injected randomness is high, making it more robust choice for highdiversity exploration. Overall, if high-diversity exploration is desired, AGN serves as more reliable choice. Step-wise dynamics. To gain deeper insight into how different sampling methods influence the reasoning process over time, we further analyze diversity at each individual reasoning step. As illustrated in Figure 4, we find that MC-dropout maintains relatively similar diversity across steps under same p, reflecting its adaptive nature; its stochasticity is inherently tied to the models own assessment, allowing for consistent and \"self-aware\" exploration; whereas AGN exhibits pronounced fluctuations, due to its use of fixed scale σ that perturbs latent vectors of varying magnitudes unevenly across steps, ultimately resulting in inconsistent stochastic influence. These observations align with the theoretical mechanisms of each method: MC-dropout modulates its stochastic influence based on the models epistemic uncertainty, thereby supporting more effective exploration; whereas AGN, while allowing diversity to be increased by manually setting σ, introduces noise in less adaptive manner, leading to variable impact across reasoning steps."
        },
        {
            "title": "Landscape",
            "content": "To unveil how stochasticity reshapes the hidden landscape of reasoning, we cast sampled latent thoughts into 2D stage via t-SNE (Figure 5). What emerges is strikingly distinct: Dropout produces directional driftdense and contiguous along specific directions, whereas AGN yields an isotropic radial dispersion, firework pattern with broader area but lower local density. These geometric signatures explain their different behaviors on easy and hard questions. For easy cases (Figure 5a), where correct regions lie near the deterministic latent, AGN maintains accuracy by keeping more probability mass around the center, whereas large dropout drifts away and degrades performance. For hard cases (Figure 5b), where correct regions are farther away, dropouts larger displacement and denser exploration increase the chance of reaching the correct solution. Ultimately, the geometry of exploration explains their complementary strengths: MC-dropout excels on harder tasks, whereas AGN maintains robustness even unVariant Best-of-8 with LatentRM w/o contrastive (BCE) w/o stochastic rollouts random scalar head (untrained) Majority Voting Test Hard 35.4 33.5 30.7 28.924 33.6 7.8 7.4 6.0 5.8 6.1 Table 1: Ablation studies of LatentRM. We report accuracy (%) on GSM-Test and GSM-Hard under Bestof-N aggregation (N = 32). Each variant isolates one design choice in LatentRM. der strong stochasticity."
        },
        {
            "title": "7 Results of Aggregation",
            "content": "7.1 Main Results We report the TTS results using the three abovementioned aggregation methods in Figure 6. First, accuracy increases monotonically with across all three datasets, demonstrating that latent reasoning can effectively scale with more inference compute. Second, both Best-of-N and Beam Search consistently outperform Majority Voting, confirming that LatentRM can effectively distinguish promising reasoning trajectories. Third, Beam Search performs comparably with Best-of-N on GSM-Test and MultiArith but trails on GSM-Hard, suggesting that early-step score noise can cause premature pruning in more challenging problems. Finally, the gains are most pronounced on MultiArith, highlighting the generalization ability of the learned scorer across arithmetic reasoning patterns. Overall, LatentRM enables scalable and reliable aggregation, with Best-of-N emerging as the most effective route for latent test-time scaling."
        },
        {
            "title": "7.2 Ablation Studies",
            "content": "To understand the contribution of each component in LatentRM, we conduct ablation studies under the Best-of-N setting with = 8 on GSM-Test and GSM-Hard  (Table 1)  . Each variant disables one design element of LatentRM while keeping all other configurations identical. (1) Removing the step-wise contrastive loss (w/o contrastive) causes noticeable drop, showing that relative supervision among concurrent thoughts provides stronger learning signals than isolated binary labels. (2) Excluding stochastic rollouts (w/o stochastic rollouts), where each thought is labeled only by the final trajectory correctness instead of intermediate estimates, leads to further decline, highlighting the importance of Monte Carlo estimation for reliable 7 (a) Easy (b) Hard Figure 5: t-SNE visualization of latent thoughts sampled with different dropout rates (red; from light to dark) and Gaussian-noise scales (blue; σ from light to dark). The green marker denotes the deterministic latent thought (no stochasticity). Diamonds () indicate correct reasoning trajectories; crosses () indicate incorrect ones. (5a): an easy question. (5b): hard question. (a) GSM-Test (b) GSM-Hard (c) MultiArith Figure 6: Test-time scaling results with majority voting, best-of-N , and beam search for latent reasoning. Accuracy(%) versus compute budget under three aggregation strategies on (6a) GSM8K-Test, (6b) GSM8K-Hard, and (6c) MultiArith. Results are reported as the mean over three runs. thought-level annotation. (3) Using an untrained random scalar head (random head) performs even worsebelow Majority Votingindicating that the gain stems from learned evaluation rather than architectural modification. Together, these findings verify that LatentRM effectively learns to assess latent trajectories and is crucial for successful aggregation in latent test-time scaling."
        },
        {
            "title": "8 Conclusion and Future Work",
            "content": "This paper proposed parallel test-time scaling framework for latent reasoning models, addressing the central challenges of sampling in continuous latent spaces and aggregating latent trajectories. By introducing two principled stochastic sampling methodsMonte Carlo Dropout and additive Gaussian noiserooted in uncertainty theory, and developing latent Reward Model trained with step-wise contrastive objective for effective trajectory aggregation, our approach enables scalable and robust parallel inference in the latent regime. Extensive experiments and analyses demonstrate distinctive exploration behaviors, robust aggregation through latentRM, and consistent performance scaling across compute budgets, yielding new insights into test-time scaling for latent reasoning. Building on this foundation, future work will seek to integrate sampling and aggregation into reinforcement learning framework, where latent trajectories are optimized through iterative feedback and reward shaping. Such formulation could transform test-time scaling from static inference procedure into an adaptive reasoning processcapable of learning exploration strategies, adjusting compute allocation dynamically, and generalizing across tasks."
        },
        {
            "title": "Limitation",
            "content": "This work provides an effective framework for parallel test-time scaling in latent reasoning models, but there are certain limitations: (1) Engineering considerations for real-time deployment. The framework shows consistent performance and generality across benchmarks and compute budgets, but real-time use may require additional engineering optimization. Nevertheless, as the core contribution is proof-of-concept and in-depth analysis of parallel scaling for latent reasoning models, these engineering considerations do not impact the validity or the key insights presented in this study. (2) Hyperparameter sensitivity. Our sampling strategies demonstrate strong scaling results with controllable randomness. But optimal performance depends on tuning the dropout rate (p) and noise scale (σ). While this could be seen as an additional configuration burden, we note that the hyperparameter search process is relatively straightforward and does not require extra model training or complex optimization. The main contribution of this work remains focused on the theoretical framework and empirical performance, thus the potential overhead from hyperparameter tuning is minor compared to the overall benefits."
        },
        {
            "title": "Ethics Statement",
            "content": "All data, models and other related experimental artifacts used in this study are publicly released and accessible under permissive licenses for research purposes. The research procedures, data handling, and analyses do not involve harm to individuals, communities, or the environment, and no personal or sensitive information is included."
        },
        {
            "title": "References",
            "content": "Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large Language Monkeys: Scaling Inference Compute with Repeated Sampling. arXiv preprint. Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. 2025a. Reasoning Beyond Language: Comprehensive Survey on Latent Chain-of-Thought Reasoning. arXiv preprint. Foster Adaptive Internal Thinking. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2824128259, Vienna, Austria. Association for Computational Linguistics. Yuxin Chen, Yiran Zhao, Yang Zhang, An Zhang, Kenji Kawaguchi, Shafiq Joty, Junnan Li, Tat-Seng Chua, Michael Qizhe Shieh, and Wenxuan Zhang. 2025c. The Emergence of Abstract Thought in Large Language Models Beyond Any Language. arXiv preprint. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2024. DeepSeek-V3 Technical Report. arXiv preprint. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. 2025. Deep Think with Confidence. arXiv preprint. Yarin Gal and Zoubin Ghahramani. 2016. Dropout as Bayesian approximation: representing model uncertainty in deep learning. In Proceedings of the 33rd international conference on international conference on machine learning - volume 48, ICML16, pages 10501059, New York, NY, USA. JMLR.org. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. PAL: Program-aided language models. arXiv preprint arXiv:2211.10435. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 68946910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. 2023. survey of uncertainty in deep neural networks. Artificial Intelligence Review, 56(Suppl 1):15131589. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. 2025. Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach. arXiv preprint. Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, and Haifeng Wang. 2025b. Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training Large Language Models to Reason in Continuous Latent Space. arXiv preprint. 9 Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. 2025. Evolving Deeper LLM Thinking. arXiv preprint. Jian Wang, Boyan Zhu, Chak Tou Leong, Yongqi Li, and Wenjie Li. 2025. Scaling over Scaling: Exploring Test-Time Scaling Plateau in Large Reasoning Models. arXiv preprint. Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, and Rex Ying. 2025. Implicit Reasoning in Large Language Models: Comprehensive Survey. arXiv preprint. Jeremiah Liu, John Paisley, Marianthi-Anna Kioumourtzoglou, and Brent Coull. 2019. Accurate uncertainty estimation and decomposition in ensemble learning. In Advances in neural information processing systems, volume 32. Curran Associates, Inc. Amirkeivan Mohtashami, Matteo Pagliardini, and Martin Jaggi. 2024. CoTFormer: Chain-of-Thought Driven Architecture with Budget-Adaptive Computation Cost at Inference. arXiv preprint. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint. Yuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. 2025. Optimizing Test-Time Compute via Meta Reinforcement FineTuning. arXiv preprint. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. 2025. CODI: Compressing Chain-of-Thought into Continuous Space via SelfDistillation. arXiv preprint. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. arXiv preprint. Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, and Ruihua Song. 2025. Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains. arXiv preprint. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024. Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-Consistency Improves Chain of Thought Reasoning in Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Haoyi Wu, Zhihao Teng, and Kewei Tu. 2025. Parallel Continuous Chain-of-Thought with Jacobi Iteration. arXiv preprint. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. 2025. SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning. arXiv preprint. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 Technical Report. arXiv preprint. Runyang You, Yongqi Li, Xinyu Lin, Xin Zhang, Wenjie Wang, Wenjie Li, and Liqiang Nie. 2025. $text{R}^2text{ec}$: Towards Large Recommender Models with Reasoning. arXiv preprint. Kaiyan Zhang, Jiayuan Zhang, Haoxin Li, Xuekai Zhu, Ermo Hua, Xingtai Lv, Ning Ding, Biqing Qi, and Bowen Zhou. 2024. OpenPRM: Building Opendomain Process-based Reward Models with Preference Trees. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcomebased feedback. arXiv preprint. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, pages 60006010, Red Hook, NY, USA. Curran Associates Inc. Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, and 14 others. 2025. Survey on Latent Reasoning. arXiv preprint."
        },
        {
            "title": "A Theoretical Perspectives on Sampling",
            "content": "in Latent Space We here briefly derive how MC-dropout can be interpreted as an approximate Bayesian estimator of epistemic uncertainty, while additive Gaussian noise acts as perturbation mechanism that simulates aleatoric variability in latent representations. MC-dropout can be interpreted as an approximate Bayesian method, where stochastic forward passes approximate the posterior predictive distribution given the training dataset (Gal and Ghahramani, 2016). Let the predictive distribution be p(y x, D) = (cid:90) p(y x, ω)p(ω D)dω, where ω denotes network weights and the training data. Since the posterior p(ω D) is intractable, dropout approximates it with variational distribution q(ω) : Wi = Midiag(zi), zi,j Bernoulli(pi). At inference, performing stochastic forward passes with dropout corresponds to sampling ω(t) q(ω) and evaluating y(n) = (x; ω(n)). The empirical mean and variance over samples approximate the true Bayesian predictive distribution: E[y x]"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) t=1 (x; ω(n)), Var[y x]"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) t=1 (x; ω(n))2 (cid:16) (cid:17)2 E[y x] . As , this Monte Carlo procedure converges to the predictive distribution under the variational approximation. Applying dropout at inference effectively samples from distribution over model weights and creating an ensemble of predictions for each input. The variability across these predictions arises from the uncertainty in model parameters due to limited knowledge gained from training data D, this is defined as epistemic uncertainty. 11 AGN perturbs the latent representations directly, injecting variance proportional to the models local sensitivity. = + ϵ, ϵ (0, σ2I). The predictive distribution then becomes (cid:90) p(y x) = p(y + ϵ)p(ϵ)dϵ, with p(ϵ) an isotropic Gaussian prior. Expanding to first order (via Taylor approximation around x), (x + ϵ) (x) + Jf (x)ϵ, where Jf (x) is the Jacobian of the network output w.r.t. x. Taking expectation yields E[f (x + ϵ)] (x), Var[f (x + ϵ)] Jf (x), ΣϵJf (x) Σϵ = σ2I. AGN injects variance proportional to the networks local sensitivity. Since this variability is induced externally by perturbations to the latent inputs, it corresponds to aleatoric uncertainty, i.e., randomness arising from inherent noise in the input space rather than uncertainty about the model itself."
        },
        {
            "title": "B Derivation of Cumulative Scoring",
            "content": "Here we derive in detail why summing the logits over each candidate trajectory serves as valid proxy for the relative quality of the generated thought sequence. Consider reasoning trajectories, where each trajectory 1, . . . , produces sequence of latent scores r(n) At each step t, the probability assigned to thought under the softmax over all candidates is t=1. p(n) = exp(r(n) ) n=1 exp(r(n) . ) (cid:80)N The log-probability is log p(n) = r(n) log (cid:88) n=1 exp(r(n) )."
        },
        {
            "title": "The cumulative log probability over the first t",
            "content": "time steps for thought is: log p(n) 1:t = = (cid:88) t=1 (cid:88) t=1 log p(n) r(n) (cid:88) log (cid:88) t=1 n=1 exp(r(n) ). t=1 log (cid:80)N n=1 exp(r(n) The second term (cid:80)t ) is identical for all trajectories at step t. Therefore, when comparing trajectories, the relative ordering of log p(n) 1:t depends only on the first term (cid:80)t t=1 r(n) t ."
        },
        {
            "title": "C Inference Procedures for Aggregation",
            "content": "We detail the inference algorithms used to aggregate multiple latent reasoning trajectories, corresponding to the three strategies evaluated in Section 7. All procedures operate on sampled latent trajectories {h(n) n=1 obtained via the stochastic sampling methods introduced in Section 4.1. Each latent thought h(n) is scored by the Latent Reward Model (LatentRM) as r(n) = gϕ(x, h(n) 1:t ). 1:T }N Best-of-N . Each trajectory is evaluated independently using the cumulative latent reward R(n) = (cid:88) t=1 r(n) . The trajectory with the highest total reward is selected for final decoding: 1:T = arg max R(n). The final answer token sequence is then produced by the reasoning model conditioned on 1:T : = fθ(x, 1:T ). This mirrors best-of-N decoding in token-based TTS, but replaces token log-likelihoods with learned latent rewards. Beam Search. We further employ beam search guided by LatentRM to explore high-reward reasoning paths. At step t, the model expands each partial latent trajectory in the beam Bt1 by one autoregressive step, producing candidate extensions via stochastic sampling: 1:t = [h(b) h(k) 1:t1, ˆh(k) ], fθ(h(b) ˆh(k) 1:t1, x), where indexes beam element. LatentRM assigns scores r(k) to all candidates, and their cumulative rewards are updated: = R(b) R(k) t1 + r(k) . The top-B candidates by cumulative reward are retained as the next beam: Bt = TopB({h(k) 1:t }, R(k) ). 12 Decoding terminates when all beams emit the endof-thinking token or reach the maximum latent step . The best final trajectory is selected by its cumulative reward. In experiments, we set = to match compute cost with best-of-N as discussed in Section 5.3. Majority Voting. As non-parametric baseline, each latent trajectory is independently decoded into an answer a(n). The final output is the most frequent answer among the candidates: = mode({a(n)}N n=1). This strategy disregards latent scores and serves to isolate the benefit of learned aggregation via LatentRM. Implementation Details. All aggregation procedures are executed under identical compute budgets. For best-of-N and majority voting, full trajectories are sampled; for beam search, the beam size = and per-step expansion = ensure comparable total forward passes. Cumulative rewards (cid:80) are pre-normalized by trajectory length to prevent bias toward longer reasoning chains. r(n) t"
        },
        {
            "title": "D Training Configuration for LatentRM",
            "content": "We train Latent Reward Model (LatentRM) for COCONUT using the step-wise contrastive learning objective introduced in Section 4.2. model initialized from COOCNUT checkpoint, GPT-2 backbone with 124 million parameters (Radford et al.), with architecture modifications detailed therein. Training data. The training data consists of samples generated from the GSM8K training set (385K examples). For each input problem, we sample = 8 reasoning trajectories via MC-dropout with dropout probability = 0.2 from COCONUT, where was tuned for optimal performance as mentioned in Section 6.1. Labeling. For each intermediate reasoning step within trajectory, we perform = 128 stochastic rollouts to empirically estimate the correctness of that step. trajectories that are either too easy (i.e., all trajectories are correct) or too difficult (i.e., all trajectories are incorrect) are excluded to improve training stability and focus on informative examples. Training configuration. Batch size: 2048 Optimizer: Paged AdamW Learning rate: 1 105 Epochs: 10 Hardware: 2 NVIDIA H100 GPUs"
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology (Shenzhen)",
        "Shandong Jianzhu University",
        "The Hong Kong Polytechnic University",
        "University of Science and Technology of China"
    ]
}