{
    "paper_title": "Stepsize anything: A unified learning rate schedule for budgeted-iteration training",
    "authors": [
        "Anda Tang",
        "Yiming Dong",
        "Yutao Zeng",
        "zhou Xun",
        "Zhouchen Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets.While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations.In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient.In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets.First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations.From this framework, we derive the UBA schedule, controlled by a single hyper-parameter $\\varphi$ that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between $\\varphi$ and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of $\\varphi$.We offer practical guidelines for its selection via theoretical analysis and empirical results.xtensive experimental results show that UBA \\textit{consistently surpasses} the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 2 5 4 4 2 . 5 0 5 2 : r Stepsize anything: unified learning rate schedule for budgeted-iteration training Anda Tang1 Yiming Dong1 Yutao Zeng2 Xun Zhou2 Zhouchen Lin1,3,4 1State Key Lab of General AI, School of Intelligence Science and Technology, Peking University 2ByteDance Seed 3Institute for Artificial Intelligence, Peking University 4Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China tanganda@pku.edu.cn, yimingdong ml@outlook.com, yutao.zeng@outlook.com, zhouxun@bytedance.com, zlin@pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets. While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations. In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient. In this work, we propose the Unified Budget-Aware (UBA) schedule, theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets. First, we bridge the gap by constructing novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations. From this framework, we derive the UBA schedule, controlled by single hyper-parameter φ that provides trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish theoretical connection between φ and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of φ. We offer practical guidelines for its selection via theoretical analysis and empirical results. Extensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets."
        },
        {
            "title": "Introduction",
            "content": "Deep learning has achieved remarkable success across various across wide range of domains, including computer vision and natural language processing. However, despite continual advancements in hardware technologies [44, 52], the training cost of neural networks has increased dramatically due to the growing scale of models and datasets [4, 8, 46, 47]. As result, resource constraints, including computational power, memory, energy consumption, and time budgets, are emerging as significant bottlenecks in the training process [40, 56]. These challenges highlight the pressing need for budgeted training, which aims to achieve optimal model performance under fixed hardware and limited time. Work was done during an internship at ByteDance Seed. Corresponding author. Preprint. Under review. While existing budgeted training studies broadly address resource efficiency, critical yet underexplored direction within budgeted training is achieving the best possible model performance under strictly fixed iteration constraints. This scenario is common and practically significant where practitioners work under limited computational or time budgets [29], and in extreme cases, models have to be completed within few training iterations due to resource exhaustion. To formalize this specific research problem, we introduce the term budgeted-iteration training , distinguishing it from the broader scope of budgeted training. Budgeted-iteration training has received growing attention in research, given its significant real-world applicability. Several studies have developed relevant techniques that align with its goals. Smith et al. [42] propose the cyclical learning rate schedule (CLR), improving accuracy in fewer iterations without tuning [43]. Li et al. [29] introduce budget-aware adaptations for existing learning rate schedules. Chen et al. [5] propose novel learning rate schedule called Reflected Exponential (REX). These approaches are primarily based on learning rate designing. Learning rate scheduling highlights key advantages: (i) It plays critical role in general training of diverse neural architectures across tasks. (ii) It has demonstrated suitable and competitive in fixed training iteration budgets [19]. (iii) It is plug-and-play, requiring minimal adjustments to the underlying model architecture, which makes them easily adaptable to various deep learning frameworks. Leveraging these advantages, we adopt the learning rate design approach for budgeted-iteration training. Despite their advantages, most learning rate schedules, whether tailored for budgeted-iteration training or standard training performance, are still heuristic and lack rigorous theoretical grounding. In addition, existing schedules typically rely on manually designed rules or empirical tuning. Consequently, selecting an optimal schedule often involves extensive trial-and-error, incurring substantial cost in terms of time and computation [33]. natural question arises: Does there exist theoretically grounded, unified schedule that eliminates heuristic selection while maintaining robust performance across tasks, networks, scales and training budgets? In this paper, we provide an affirmative answer by proposing theoretically grounded schedule. The proposed learning rate schedule should consistently outperforms existing schedules among diverse architectures and tasks under different constrained training budgets. By doing so, it avoids choosing suitable learning rate schedule after multiple trials for network training. To achieve this, we first bridge the gap by constructing unified budget-aware training optimization framework, which incorporates the robustness to landscape curvature variations induced by data distribution, sampling, network architectures and optimization. Then we obtain numerical solutions by gradient projection methods. To eliminate the need for repeated numerical optimization when applying our method to different networks, we propose universal parametric function that approximates numerical solutions. We nominate the resulting schedule Unified Budget-Aware (UBA) schedule. It requires tuning only single hyper-parameter φ, reducing the overhead of per-network numerical optimization. Moreover, we establish theoretical connection between φ and the condition number, adding interpretation and optimization difficulty-aware theoretical grounding. Besides, we prove the convergence for different values of φ. These theoretical analysis along with empirical results offer practical guidelines for φ selection. We evaluate UBA through comprehensive experiments across vision and language tasks, spanning diverse architectures and iteration budgets. Specifically, for vision tasks, the UBA schedule demonstrates consistent superiority over baselines across all evaluated datasets and model scales under varying training iterations. For language tasks, we validate the effectiveness of UBA through extensive benchmarks with OLMo model(36M, 73M, 150M and 300M parameters). Results show that UBA achieves state-of-the-art performance across on approximately half of the benchmarks, and consistently outperforms baselines on the average scores. Main contributions: 1. We construct unified budget-aware training optimization framework that inherently adapts to landscape curvature variations, enhancing training robustness of leaning rates. 2. We propose the Unified Budget-Aware (UBA) schedule from our constructed optimization problem, controlled by single hyper-parameter φ that provides trade-off between flexibility and simplicity, i.e. adaptive curvature adjustment and minimal tuning cost. 2 3. We prove the convergence under φ and derive practical guidelines for its selection through analysis and experiments. Besides, theoretical analysis and empirical results show that φ is related to optimization difficulty such as condition number. 4. We perform experiments and demonstrate that UBA surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets."
        },
        {
            "title": "2 Related work",
            "content": "Budgeted training Researchers face significant challenges in achieving optimal model performance under fixed hardware and limited time. To address these challenges, the concept of budgeted training has gained increasing attention, exploring techniques including computation efficiency, model compression, training stability and convergence improvement [40]. It focuses on: (i) emphasizing the allocation of resources, such as the balance between the model size and the amount of data [2, 6, 23, 27]. (ii) finding optimal configurations or improving performance within the given compute or time budget, such as memory efficiency and computation reduction [16, 25, 30, 37], optimization learning rate schedules [5, 29, 42, 43], batch size [16] and other weight averaging method [25, 26]. Within the context of budgeted training, key area that remains under-explored is achieving the best possible model performance within fixed number of iterations, i.e. budgeted-iteration training. In this domain, learning rate scheduling based methods are particularly aligned with the objectives of budgeted-iteration training. Smith et al. [42] propose new learning rate schedule, named cyclical learning rates (CLR). It improves accuracy in fewer iterations without tuning and is relevant to superconvergence phenomenon [43]. Li et al. [29] introduce an alternative setting of existing learning rate schedules for budgeted training. Chen et al. [5] propose the reflected exponential schedule (REX) via profile and sampling fashion. Learning rate based approaches achieve robust and high-performing results under various lengths of training iterations, which corresponds to our purpose in this work, i.e. budgeted-iteration training [33]. Besides, this approach is plug-and-play, requiring no substantial alterations to the underlying model structure, making it readily adaptable to various deep network frameworks. Therefore, we explore the proper learning rate schedule to achieve budgeted-iteration training. Learning rate schedule The learning rate plays pivotal role in controlling the optimization process during network training. The common scheme is the step decay schedule. typical instance decreases the learning rate by decaying scalar 0.1 after 50% epochs and by decaying scalar 0.01 after 75% epochs [20]. Then Loshchilov et al. [32] observe that sharp decreases may prevent models from escaping local minima and propose the cosine schedule function, which is the most popular schedule for language model pretraining. Although some schedules include the CLR [42], REX [5], Warmup-Stable-Decay (WSD) [24] and schedule from multi-power law [33], there is no consensus on the optimal choice. In addition, the detail can be found in Appendix F, including some works focus on adaptive learning rate methods. Learning rate design is not only significant in general training, but also critical in budgeted-iteration training which still remains topic of debate. Some analyses advocate for small, constant learning rates to ensure stability and convergence [11]. On the contrast, one prevailing hypothesis suggests that large learning rates may facilitate crossing over sharp local minima in the optimization landscape [55]. Despite the lack of comprehensive theoretical explanations, range of learning rate schedules inspired by the above analyses as heuristic guidelines has been widely adopted in practice, using variable learning rates to budgeted-iteration training [5, 29]. In this work, we explore learning rate schedule from optimization problem tailored to budgeted-iteration training, aiming to balance iteration budget constraints and generalization."
        },
        {
            "title": "3 Budgeted-iteration training",
            "content": "3.1 Finite optimization under limited training iterations To design one-size-fits-all learning rate schedule, we construct robust optimization model of learning rates across varying training conditions (see more conceptual illustration in Appendix A). 3 Specifically, we aim to guarantees minimal loss within constrained training iterations under the worst-case conditions, proposing budget-iteration-aware framework for learning rate optimization. Definition (Finite optimization): Let (W, D) denote the function parameterized by given neural network with parameters RN on the dataset D, ξ denotes the data sampling on the dataset D, and let be function class.Let be the loss function. Let ηt be the learning rate at the t-th iteration, be maximum number of training iterations, and be the current learning step. finite optimization is min η1,η2,...,ηT max L(f (WT , ξ)) s.t. Wt+1 = Wt ηtL(f (Wt, ξ)) = 0, 1, 2, ..., 1 (1) In the optimization model 1, the constraint represents the stochastic gradient descent process. The maximizing of L(f (WT , ξ)) represents the the worst-case among the training process on the network . Then it minimizes the worst-case loss within given iterations, embodying its budget-aware property. By formulating the problem as min-max optimization, we identify learning rates ηt(t = 1, 2, , 1) that are resilient to the uncertainties introduced by different training configuration, uniformly throughout the optimization trajectory. The challenge lies in characterizing the within the optimization process. is primarily determined by variations in parameter configuration, datasets characteristics, batch ordering and network architecture. From an optimization standpoint, we assume that the characteristics of shaped by these factors can be captured by the loss landscape of . Therefore, we can analyze by approximating its loss using quadratic expansion around nearby strict local optima. Specifically, during the optimization process, the loss surface in the vicinity of the optimization trajectory can be approximated by sequence of strict local optima (or at least by one optimum), denoted as (k) (k = 1, 2, . . . K) Z+, with the final optimum represented as K. This approach enables us to capture the key features of the loss surface near these points. Therefore, the trajectory of optimization is impacted by the characteristics of the nearby strict local optima, since the optimization process is inherently shaped by the loss landscape and the key features of the surface are captured by these optima. Consequently, we can derive the learning rate within the optimization model by the information of these nearby strict local optima. According to the second-order necessary condition for the strict minimum (k), this approximation is achieved through the positive semi-definite Hessian matrix (k) (ξ) RN . In addition, the optimization problem (1) will be programmed sequentially. Then objective function of the optimization problem (1) can be reformulated as L(f ( (k), ξ)) + 1 (ξ)(WTk+1 (k)) (k = 1, 2, . . . K). Given that the networks under consideration possess sufficient capacity to fit the data, the loss at the optimal point for different can be made small enough. Thus the term L(f ( (k), ξ)) can be reasonably neglected in our analysis. We obtain the following sequential optimization problem. 2 (WTk+1 (k))H (k) min η1+Tk ,η2+Tk ,...,ηTk+1 max (ξ) 1 2 s.t. Wt+1 = Wt ηtH (k) (WTk+1 (k))H (k) (ξ)(WTk+1 (k)) (ξ)(Wt (k)) (2) = Tk + 1, Tk + 2, ..., Tk+1 (k = 1, 2, . . . 1) where T1 = 0 and TK = 1. By the first constraint Wt+1 = Wt ηtH (k) derivation) (ξ)(Wt (k)), we can obtain (see Appendix for (cid:13) (cid:13) (cid:13)WTk+1 (k)(cid:13) 2 (cid:13) (cid:13) 2 max λ(k) λ(k) λ(k) Tk+1 (cid:89) t=1+Tk 2 (1 ηtλ(k) ) (cid:13) (cid:13) (cid:13)WTk (k)(cid:13) 2 (cid:13) (cid:13) (3) where λ(k) satisfy 0 < λ(k) (i = 1, 2, , ) are the eigenvalues of (k) λ(k) (f (ξ)) λ(k) for all i, u(k) i (ξ) around the k-th optimum, which (i = 1, 2, , ) denote linearly 4 independent eigenvectors and s(k) Since the term (cid:13) are the coefficients corresponding to the eigenvector components. (cid:13)WTk (k)(cid:13) 2 2 is certain constant, the optimization process of weights WTk+1 is (cid:13) equivalent to the least upper bound of the rate schedule can be formulated as follow, Tk+1(cid:81) t=1+Tk (1 ηtλ(k) ). Thus, the optimization of learning Tk+1 (cid:89) (cid:104) (1 ηtλ(k) (cid:105)2 ) min η1+Tk ,η2+Tk ,...,ηTk+1 s.t. max λ(k) λ(k) λ(k) t=1+Tk η1+Tk , η2+Tk , ..., ηTk+1 [ηmin, ηmax] = 1, 2, . . . (4) To solve the constrained min-max problem (4), we adopt an iterative projected gradient method that alternates between minimizing over the variables ηt and maximizing over the parameters λ(k) . To avoid repeated numerical optimization, we fit the solutions with parametric function. Details regarding the numerical solution and curve fitting process are provided in Appendix B. Then, we obtain the ηt within the interval [1 + Tk, Tk+1] as follows 2(1 + cos( (2(tTk)1)π 2φ + (2 φ)(1 + cos( (2(tTk)1)π 2(Tk+1Tk) + (k 1)π)) ηt = (ηmax ηmin) + ηmin, (5) 2(Tk+1Tk) + (k 1)π) where φ is the hyper-parameter controlling the variation speed of the learning rate ηt. When setting (K > 2), UBA extends to multi-phase formulation, which offers three potential advantages. (i) Hierarchical optimization: by partitioning the budget-aware optimization into multiple phases, each near different local minima, it improves approximation accuracy and captures the dynamic features of the loss surface. (ii) It helps escape saddle points, (iii) It generalizes the singlephase method, allowing for future extensions. Notably, single-phase approach (K = 2) remains effective, as the robust budget-aware model inherently derives the optimal schedule function, as shown in Figure 1(a). For consistency with common practice and to better isolate scheduling effects, this paper focuses primarily on the single-phase implementation. The multi-phase approach is also compared in Appendix, with its schedule shown in Figure 1(b). We name the proposed schedule Unified Budget-Aware (UBA) schedule for reasons. The robust budget-aware model minimizes the loss function uniformly across the optimization trajectory, resulting in stable performance. UBA provides reliable, unified choice for practitioners, eliminating the need for case-by-case baseline comparisons and delivering consistent superiority across datasets, architectures, and training budgets. Lastly, UBA can uniformly approximate the behavior of existing schedules through simple parameter adjustments. (a) (b) Figure 1: Evolution of the learning rate in UBA schedule across training iterations. 3.2 Theoretical analysis Proposition 1. The fit function (5) is the exact closed-form solution to the min-max optimization problem: min η1+Tk ,η2+Tk ,...,ηTk+1 max λ(k) λ(k) λ(k) Tk+1 (cid:89) t=1+Tk (cid:34)(cid:32) (cid:32)(cid:32) 1 1 λ(k) 1 λ(k) (cid:33) ηt + 1 λ(k) (cid:33) (cid:33)(cid:35)2 λ(k) (6) when the hyper-parameter φ are determined by λ(k) ηmax = 1, ηmin = 0. and λ(k) through the relation φ = 2 λ(k) λ(k) and The min-max model in Proposition 1 represents special case of our generalized optimization framework. We show that UBA is the exact solution to this special case optimization problem (cid:19) (cid:18)(cid:18) (cid:19) when the learning rate is scaled by . It provides theoretical foundation 1 λ(k) 1 λ(k) ηt + 1 λ(k) for our choice of learning rate instead of choosing it heuristically or empirically, adding rigor and justification to our approach. Moreover, in this case, φ is linked condition number. It indicates how the learning rate is shaped by the local curvature of the model, which could guide the optimization process more effectively. The relationship between φ and condition number suggests an adaptive nature for the learning rate. In regions with sharp curvatures (large condition number), φ reduces the learning rate more rapidly to avoid overshooting. Conversely, in flatter regions, φ allows the learning rate to remain large for several iterations, facilitating faster convergence. The transformation of the learning rate trend is shown in Figure 1. This is step toward establishing principled connection between learning rate and local loss landscape geometry along with optimization difficulty. By this special case, we generalize that φ is related to optimization difficulty. The empirical results support these conclusions, with further details in Section 4.3 and Appendix E.6. Proposition 2. Consider the training process within the interval [1 + Tk, Tk+1]. When the hyper-parameter φ is set sufficiently close to 2, the proposed learning rate scheduling formula reduces to the cosine learning rate schedule. By Proposition 1, the hyper-parameter φ is related to the optimization difficulty. In that case, the optimization difficulty is explicitly quantified as the condition number and φ = 2κ, where κ = λ(k) . λ(k) Furthermore, Proposition 2 shows that when φ approaches 2, the proposed learning rate schedule converges to the standard cosine schedule. It means that, in regions where the optimization difficulty is not large (i.e., the curvature is relatively flat), setting φ 2 allows the learning rate schedule to naturally reduce to the cosine form. Besides, our schedule can approximate the behavior of existing schedules (e.g.step decay, cosine annealing, cyclic schedule or Rex schedule) through simple parameter adjustments, shown in Table 1. The detail can be found in Appendix C. More importantly, it outperforms these schedules by training convergence and final accuracy, supporting results can be found in experiment sections 4.1,4.2 and Appendix E.5. Table 1: The adaptive simulation of existing schedules. Schedule Parameter adjustments Schedule Cosine φ = 2 Exponential φ = 30 Rex φ = 0.8 Step Cyclic Parameter adjustments φ = 0, ηmax = 0.5k, Tk+1 Tk =decaying step,k = 1, 2, ... φ = 2, + 1, Tk+1 Tk =cyclic step ,k = 1, 2, ... OneCycle φ = 2, + 1, T2 T1 = pct_start step Theorem 1. Let nt = (k) introduced by sampling at iteration t. Assume that the sampling Hessian satisfies: Eξ (ξ)(Wt (k)) be the stochastic curvature noise (cid:3) (Wt (k)) (k) (cid:2)ntn for some constant σ. Denote τ := σ2H (k) . If we set the learning rate as the proposed form (5), the loss uncertainty introduced by stochastic gradient method within the interval [1 + Tk, Tk+1] can be bounded by two terms, (φ2)π 4λ(k) (ηmaxηmin) 1+cos (cid:16) (cid:16) (2(tTk )1)π 2(Tk+1Tk ) (cid:17)(cid:17) (Tk+1Tk) For φ > 2: (cid:104) L(f (Wt, ξ)) L(f ( (k), ξ)) (cid:105) (cid:16) 4(Tk+1 Tk) + (φ 2)π 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:17)τ (cid:16) exp 2λ(k) (cid:17) ηmin(t Tk) W1+Tk (k)2 λ(k) 2 (cid:88) +σ2 i=1+Tk η2 (cid:88) j=1 (λ(k) )2 exp (cid:16) 2λ(k) ηmin(t i) (cid:17) (cid:18) 4(Tk+1 Tk) + (φ 2)π(i Tk) 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:19)τ (7) For φ < 2: (cid:104) L(f (Wt, ξ)) L(f ( (k), ξ)) (cid:105) (cid:16) (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (cid:17)τ (2φ + 2π φπ) + (2φ)π 2(Tk+1Tk) (cid:16) exp 2λ(k) ηmin(t Tk) (cid:17) W1+Tk (k)2 λ(k) (cid:88) +σ2 i=1+Tk η2 (cid:88) j= (λ(k) )2 exp (cid:16) 2λ(k) (cid:17) ηmin(t i) (2φ + 2π φπ) (2φ)π (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (Tk+1Tk) (i Tk 0.5) τ (8)"
        },
        {
            "title": "4 Experiment results",
            "content": "We conduct comprehensive evaluations along three dimensions: (i) Modality diversity, including vision and language tasks; (ii) Training budget, including model scales (small to large) and training iterations (short to long); (iii) Ablation studies, such as parameter sensitivity and cross-optimizer performance. This evaluation strategy ensures the robustness and generalization of our findings. key advantage of UBA lies in its critical parameter φ. While an optimal φ can further improve model performance, we intentionally fix its value across all tasks and architectures to ensure fair evaluation. We fix φ = 5 for SGD and φ = 0.5 for AdamW (see learning rate variations in Figure 1(a)). The rationale for these choices is systematically analyzed in our ablation study 4.3. Baselines Research on budgeted-iteration training remains limited, with most existing approaches focusing on learning rate scheduling strategies [5, 29, 43]. To provide fair comparison, we adopt several widely used and empirically effective learning rate schedules as baselines: Step(SS) [15], Cosine(CS) [32], Cyclical (CLR) and OneCycle (1C) [42, 43], Budgeted training(BT) [29] and Reflected Exponential (REX) [5]. Details of these baselines are provided in Appendix E.1. 4.1 Experiments for Vision Classification Tasks We evaluate our proposed UBA schedule on vision benchmarks (e.g., CIFAR10/100 and ImageNet) using different architectures (VGG16, ResNet18, ResNet34, ResNet50). In addition, we independently train models using fixed epoch budgets of 25%, 50%, and 100% of the maximum training epochs, without reusing or interpolating results from longer training runs. This setup ensures that each budget setting is evaluated in isolation, thereby preserving the integrity of comparisons across low and high training budgets. Table 2 presents the validation accuracy across different training budgets, comparing UBA against six baseline schedules (see detailed results in Appendix E.4). We find that: (i) UBA demonstrates the strongest performance across all training budgets, achieving superior results on both small-scale (CIFAR10/100 with ResNet18/34) and large-scale (ImageNet with ResNet50) benchmarks. This consistent improvement highlights the generalizability across model and dataset scales. (ii) UBA outperforms baselines not only at 100% training budget but also at 25% and 50% iteration budgets, demonstrating its budget efficiency, i.e. effectiveness in computation-constrained scenarios. (iii)Notably, UBA shows robust performance even while the second-best schedule varies depending on both the datasets, architectures and training budgets. For practitioners seeking reliable schedules without extensive method selection, UBA provides defaultstrong choice. UBA eliminates the need for case-by-case baseline comparison and delivers stable superiority and reliability regardless of datasets, architectures, training budgets and scales. 4.2 Experiments for language models We evaluate UBA schedule on the OLMo[18], truly open language model based on decoder-only transformer architecture, across diverse benchmarks in language model evaluation. Since large language models commonly provide multiple scales to accommodate different compute constraints. We adjust both model size and training steps to explore budgets. We evaluate UBA on OLMo networks spanning four parameter scales: 36M, 73M, 151M, and 300M, covering normal to large-scale models. 7 Table 2: Validation accuracy for vision classification tasks. We present validation accuracy on vision benchmarks (e.g., CIFAR10/100 and ImageNet) using different architectures (ResNet18, ResNet34, ResNet50) under fixed epoch budgets of 25%, 50%, and 100% of the maximum training epochs. Schedule CIFAR10-ResNet18 training budget (epoch(%)) CIFAR100-ResNet34 training budget (epoch(%)) ImageNetResNet50 training budget (epoch(%)) 75 (25%) 150 (50%) 300 (100%) 75 (25%) 150 (50%) 300 (100%) 75 (25%) 150 (50%) 300 (100%) SS CS CLR 1C BT REX UBA(ours) 92.41 93.66 91.89 94.36 93.24 94.79 94.54 93.90 94.15 92.23 95.02 94.28 94.96 95.26 93.94 95.40 95.32 95.48 95.55 95. 95.74 70.85 63.88 72.63 73.72 72.53 73.12 74.57 75.58 68.84 73.81 75.53 75.40 75.48 76.68 77.28 70.43 74.80 77.90 78.49 77. 78.97 74.84 75.99 72.87 75.28 75.71 75.28 76.00 76.96 77.79 74.88 77.37 77.48 77.03 77.99 78.10 79.10 76.91 78.79 78.66 78. 79.32 Due to space limitation, we defer details such as benchmarks introduction, experimental setting and overall results in Appendix E.5). Figure 2: Training dynamics and performance for language tasks under 150B tokens on 300M OLMo. We present the training loss, validation loss, and downstream performance on HSWAG and ARC-E, demonstrating that UBA schedule achieves superior performance. From Table 3, UBA achieves state-of-the-art performance on approximately 50% of the benchmarks across all scales while the second-best schedule varies. Furthermore, it achieves consistent superior average performance among all baselines. It highlights the stable superiority and reliability of UBA, providing default-strong choice. Moreover, it demonstrates significant improvements in SciQ-73M(+1.7) and ARC-E-300M(+2.63), highlighting its ability to enhance generalization across diverse benchmarks. Besides, in Figure 2, UBA consistently achieves lower training loss and validation loss throughout the training, indicating the efficient training ability and downstream performance enhancement. Notably, while task-specific tuning of φ can further improve model performance, we intentionally fix its value across all tasks and architectures to ensure fair evaluation. Remarkably, even with this universal φ setting, UBA consistently outperforms baselines on diverse benchmarks, demonstrating inherent robustness to varying benchmarks and model scales. 4.3 Ablation Study Performance across different optimizers UBA originates from the optimization problem under gradient descent dynamics. While modern optimizers (e.g., AdamW [31]) introduce momentum and adaptive mechanisms, they maintain the fundamental property of gradient-based updates. To verify the cross-optimizer performance, we conduct ablation studies between SGD and AdamW optimizers(see detailed results in Appendix E.6). The results show that UBA achieves SOTAs on both SGD and AdamW optimizers, demonstrating the cross-optimizer robustness of UBA. This highlights its broad applicability beyond standard gradient descent. Parameter analysis of φ We perform sensitivity analysis of φ {0.25, 0.5, 1.0, 2.5, 5, 10} in equation (5) , which controls the variation speed of learning rate (see detailed setting, results and analyses in Appendix E.7). Experiments 11 show AdamW prefers smaller φ while SGD requires larger φ. We attribute this phenomenon to the preconditioning effect of AdamW. As the relationship 8 Table 3: Performance comparison between UBA and the best-performing baseline schedules on OLMo. We report the results of top-performing baseline schedule for the corresponding benchmark and model scale. The names of the top-performing baseline schedules are listed below the results. Bold values indicate UBAs superiority (See overall results of each schedule in Appendix E.5). Size Sched. PIQA HSWAG OBQA SciQ ARC-E ARC-C COPA SIQA SOC OTH Avg. Benchmark(accuracy %) 6 3 3 7 Best 61.15 27.91 baseline (CLR) (1C) 27.80 68.10 45.26 (REX) (REX) (SS) UBA 60.39 27.98 27.40 68.20 45.79 Best 62.46 30.22 baseline (REX) (REX) 28.80 (CS) 72.60 47.54 (1C) (CS) UBA 63.17 30.09 28.80 74.30 45.79 Best baseline (CS) 66.00 35.72 (REX) 0 5 1 32.80 (1C) 78.20 53.16 (1C) (BT) UBA 65.23 35. 29.80 78.30 50.35 23.41 (1C) 21.74 26.42 (1C) 22. 26.42 (BT) 27.42 63.00 41.45 25.18 29.86 40.92 (REX) (CLR) (REX) (1C) (SS) 63.00 40.69 24.33 30.14 40.97 66.00 41.97 26.32 29.34 42.53 (REX) (CS) (SS) (BT) (BT) 65.00 41.45 27.13 28.85 42.73 69.00 43.71 27.68 32.64 45.91 (REX) (CLR) (REX) (REX) (REX) 67.00 43.50 29.29 32.72 45.91 Best 70.18 46.30 baseline (REX) (REX) 0 0 33.40 (CS) 84.40 57.72 (REX) (REX) 28.43 (REX) 72.00 44.58 29.50 36.74 49.70 (REX) (REX) (REX) (BT) (CS) UBA 69.48 46.44 34.60 83.90 60.35 29.10 72.00 44.42 28.53 35.41 50.42 formalized in Proposition 1), smaller φ is favored for low optimization difficult, while larger φ is beneficial for difficult optimization. AdamW adapts the learning rate by scaling gradients with the vt, implicitly reducing the condition number of the optimization landscape. second moment estimate Thus the schedule with smaller φ is preferred. In contrast, SGD lacks such adaptive mechanisms, thus larger φ is more suitable for this situation. This alignment between theory and experiment underscores the importance of tailoring φ to the optimizers characteristics. Overall, φ is related to generalized optimization difficulty, where optimization precondition effect, datasets distribution and network architecture are all related to optimization difficulty. Performance across different periods Our schedule has periodic phase-based learning rate adjustment setting, where the learning rate at the k-th phase is dynamically determined by the k-th local minimum of the loss landscape. To validate our scheduling strategy, we conduct experiments by varying (see detailed results in Appendix E.8). The results suggests that multi-phase scheduling captures the dynamic features of the loss surface more finely, but it needs careful selection for φ, where φ reflects optimization difficulty. However, selecting optimal φ values per phase for multi-phase remains non-trivial, which motivates future work on automated landscape-aware φ tuning."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we construct unified budget-aware training optimization framework that inherently adapts to landscape curvature variations, enhancing training robustness. From this optimization framework, we propose the Unified Budget-Aware (UBA) schedule. Extensive experiments demonstrate UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets. Theoretically and empirically, we observe that the parameter φ correlates with the optimization difficulty of the training process, influences the optimal choice of φ and UBAs performance. However, the explicit relationship between φ and optimization difficulty remains unexplored and no established evaluation metric exists to quantify the optimization difficulty. These limitations motivate future work on optimization difficulty-aware φ tuning. 9 Despite these open questions, the effectiveness, implementation simplicity and ease of tuning make the UBA practical, must-try schedule for deep learning practitioners. We hope this work could motivate more studies on learning rate scheduling."
        },
        {
            "title": "References",
            "content": "[1] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023. [2] Eric Arazo, Diego Ortego, Paul Albert, Noel OConnor, and Kevin McGuinness. How important is importance sampling for deep budgeted training? arXiv preprint arXiv:2110.14283, 2021. [3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [4] Tom Brown. Language models are few-shot learners. In Advances in Neural Information Processing Systems, pages 18771901, 2020. [5] John Chen, Cameron Wolfe, and Tasos Kyrillidis. Rex: Revisiting budgeted training with an improved schedule. Proceedings of Machine Learning and Systems, 4:6476, 2022. [6] Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang. Data-efficient gan training beyond (just) augmentations: lottery ticket perspective. Advances in Neural Information Processing Systems, 34:2094120955, 2021. [7] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems, 36, 2024. [8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. [9] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [11] Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns onehidden-layer cnn: Dont be afraid of spurious local minima. In International Conference on Machine Learning, pages 13391348. PMLR, 2018. [12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. [13] Donald A. Flanders and George Shortley. Numerical determination of fundamental modes. Journal of Applied Physics, 21(12):13261332, 1951. [14] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, and Jeffrey Hsu. framework for few-shot language model evaluation. December 2023. URL https://zenodo.org/records/10256836. [15] Rong Ge, Sham Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: near optimal, geometrically decaying learning rate procedure for least squares. In Advances in Neural Information Processing Systems, volume 32, 2019. [16] Jonas Geiping and Tom Goldstein. Cramming: Training language model on single gpu in one day. In International Conference on Machine Learning, pages 1111711143. PMLR, 2023. [17] Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In * SEM 2012: The First Joint Conference on Lexical and Computational SemanticsVolume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 394398, 2012. [18] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. 11 [19] Alex Hägele, Elie Bakouch, Atli Kosson, Leandro Von Werra, Martin Jaggi, et al. Scaling laws and compute-optimal training beyond fixed training durations. Advances in Neural Information Processing Systems, 37:7623276264, 2024. [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [22] Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 14(8):2, 2012. [23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [24] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. [25] Peter Izsak, Moshe Berchansky, and Omer Levy. How to train bert with an academic budget. arXiv preprint arXiv:2104.07705, 2021. [26] Jean Kaddour. Stop wasting my time! saving days of imagenet and bert training with latest weight averaging. arXiv preprint arXiv:2209.14981, 2022. [27] Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. Gradmatch: Gradient matching based data subset selection for efficient deep model training. In International Conference on Machine Learning, pages 54645474. PMLR, 2021. [28] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [29] Mengtian Li, Ersin Yumer, and Deva Ramanan. Budgeted training: Rethinking deep neural network training under resource constraints. arXiv preprint arXiv:1905.04753, 2019. [30] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez. Train big, then compress: Rethinking model size for efficient training and inference of transformers. In International Conference on machine learning, pages 59585968. PMLR, 2020. [31] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [32] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. [33] Kairong Luo, Haodong Wen, Shengding Hu, Zhenbo Sun, Zhiyuan Liu, Maosong Sun, Kaifeng Lyu, and Wenguang Chen. multi-power law for loss curve prediction across learning rate schedules. In International Conference on Learning Representations (ICLR), 2025. [34] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [35] Rui Pan, Haishan Ye, and Tong Zhang. Eigencurve: Optimal learning rate schedule for sgd on quadratic objectives with skewed hessian spectrums. arXiv preprint arXiv:2110.14109, 2021. [36] Rui Pan, Shizhe Diao, Jianlin Chen, and Tong Zhang. Extremebert: toolkit for accelerating pretraining of customized bert. arXiv preprint arXiv:2211.17201, 2022. [37] Xuran Pan, Xuan Jin, Yuan He, Shiji Song, Gao Huang, et al. Budgeted training for vision transformer. In The Eleventh International Conference on Learning Representations, 2022. [38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [39] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [40] Li Shen, Yan Sun, Zhiyuan Yu, Liang Ding, Xinmei Tian, and Dacheng Tao. On efficient training of large-scale deep learning models: literature review. arXiv preprint arXiv:2304.03589, 2023. [41] Yong Shi, Anda Tang, Lingfeng Niu, and Ruizhi Zhou. Sparse optimization guided pruning for neural networks. Neurocomputing, 574:127280, 2024. [42] Leslie Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV), pages 464472. IEEE, 2017. [43] Leslie Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pages 369386. SPIE, 2019. [44] Vivienne Sze, Yu-Hsin Chen, Joel Emer, Amr Suleiman, and Zhengdong Zhang. Hardware for machine learning: Challenges and opportunities. In 2017 IEEE custom integrated circuits conference (CICC), pages 18. IEEE, 2017. [45] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. [46] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 1034710357. PMLR, 2021. [47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [49] Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. [50] Ashia Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. Advances in neural information processing systems, 30, 2017. [51] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan. Adan: Adaptive nesterov momentum IEEE Transactions on Pattern Analysis and Machine algorithm for faster optimizing deep models. Intelligence, 2024. [52] Kh Shahriya Zaman, Mamun Bin Ibne Reaz, Sawal Hamid Md Ali, Ahmad Ashrif Bakar, and Muhammad Enamul Hoque Chowdhury. Custom hardware architectures for deep learning on portable devices: review. IEEE Transactions on Neural Networks and Learning Systems, 33(11):60686088, 2021. [53] Matthew Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. [54] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [55] Jian Zhang, Lei Qi, Yinghuan Shi, and Yang Gao. Exploring flat minima for domain generalization with large learning rates. IEEE Transactions on Knowledge and Data Engineering, 2024. [56] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. survey on model compression for large language models. Transactions of the Association for Computational Linguistics, 12:15561577, 2024."
        },
        {
            "title": "A Methodology",
            "content": "Conceptual illustration The optimization landscape of neural networks is highly dynamic due to data distribution and sampling, and sensitive to various factors, such as network architecture, optimization and hyper-parameters. Figure 3(a) illustrates the inherent uncertainties during the training process. The black-and-white surfaces reflect the loss landscape processing one batch, while the colored surface represents the landscape following the next batch. The substantial variations in the landscape between consecutive batches significantly affect the training trajectory, making it challenging to design one-size-fits-all learning rate strategy that can effectively adapt to such fluctuations. Therefore, we aim to develop strategy robust approach to be scheduled leaning rate across diverse optimization landscapes. We construct budget-iteration-aware framework for learning rate optimization model that guarantees minimal loss within constrained training iterations under the worst-case conditions. We analyze loss landscape by quadratic approximation around nearby strict local optima. This approach enables us to capture the key features of the loss surface near these points. Therefore, the trajectory of optimization is impacted by the characteristics of the nearby strict local optima, since the optimization process is inherently shaped by the loss landscape and the key features of the surface are captured by these optima, as illustrated in Figure 3(b), Consequently, we can derive the learning rate within the optimization model by the information of these nearby strict local optima. (a) (b) Figure 3: Conceptual illustration: visualization of the motivations behind the proposed modeling approaches. (a) shows the rationale for the modeling approach in (1), while (b) explains the modeling approach for (2). The solid line represents the optimization trajectory, while the dashed circles indicate the local approximation around corresponding minima. Assumptions The optimization behavior of neural network is fundamentally governed by the geometry of its loss landscape, which emerges from the interaction of parameter configuration, datasets characteristics, sampling, network architecture, optimization preconditioning effect. In the vicinity of any strict local minimum, the loss surface can be approximated by quadratic method whose curvature is determined by the local Hessian matrix. Thus the optimization trajectory is consequently dominated by the geometric properties (e.g., Hessian features, curvature profiles) of the nearest strict local minima. Many tasks train models using epochs, where one epoch represents full pass through the datasets. Under this circumstance, an iteration occurs when the model processes single batch. While the number of iterations per epoch varies with batch size, training with fixed epoch budget becomes equivalent to using fixed iteration count when batch size remains constant. For consistency, this paper uses iterations as our primary unit of optimization model construction and networks training. 14 (cid:3) σ2H (k) The sampling Hessian satisfies: Eξ for some constant σ. This assumption is followed by the work [15, 35]. Here, σ2 measures the degree of Hessian inconsistency across data samples, i.e., the variance in the per-sample Hessian matrices. Besides, smaller σ2 indicates more stable curvature landscape between each data sampling, leading to lower noise amplification during optimization. (cid:2)ntn Optimization problem derivation for Section 3.1 Considering the first constraint Wt+1 = Wt ηtH (k) (ξ)(Wt (k)) in min-max optimization (2), we can reformulate it as Wt+1 (k) = (I ηtH (k) (ξ))(Wt (k)), where denotes the identity matrix. By iteratively applying this equation (9), we obtain (WTk+1 (k)) = (I ηTk+11H (k) (ξ)) (I ηTk+1H (k) (ξ))(WTk+1 (k)). (9) (10) the matrix (k) it is positive semi-definite, , which satisfy 0 < λ(k) possesses eigenvalues λ(k) (ξ) (f (ξ)) abbreviated as λ(k) 2 (f (ξ)), , λ(k) (f (ξ)) and λ(k) denote the bounds on the non-zero eigenvalues of the Hessian around η where η is the learning rate to guarantee the convergence [55]. 1 (f (ξ)), u(k) (f (ξ)) , which can form the basis for the dimension vector space. Then we have (i = 1, 2, , ) and the initial deviation from the optimal solution can Since 1 (f (ξ)), λ(k) λ(k) λ(k) for all i. Here, λ(k) k-th optimum. It also satisfies λ(k) Moreover, there exist linearly independent eigenvectors u(k) abbreviated as u(k) = λ(k) (ξ)u(k) (k) be expressed as WTk (k) = eigenvector components. Thus, we obtain are the coefficients corresponding to the 2 (f (ξ)), , u(k) where s(k) u(k) s(k) u(k) 2 (cid:80) i=1 i (cid:88) (s(k) i=1 (cid:88) (s(k) i= (cid:13) (cid:13) (cid:13)WTk+1 (k)(cid:13) 2 (cid:13) (cid:13) 2 = = )2u(k) 2 2 Tk+1 (cid:89) t=1+Tk 2 (1 ηtλ(k) ) )2u(k) 2 max λ(k) λ(k) λ(k) Tk+1 (cid:89) 2 (1 ηtλ(k) ) (11) t=1+Tk (1 ηtλ(k) ) (cid:13) (cid:13) (cid:13)WTk (k)(cid:13) 2 (cid:13) (cid:13) max λ(k) λ(k) λ(k) Tk+1 (cid:89) t=1+Tk"
        },
        {
            "title": "B Numerical solution and curve fitting",
            "content": "To solve the constrained min-max problem (4) as follow, Tk+1 (cid:89) (cid:104) (1 ηtλ(k) (cid:105)2 ) min η1+Tk ,η2+Tk ,...,ηTk+1 s.t. max λ(k) λ(k) λ(k) t=1+Tk η1+Tk , η2+Tk , ..., ηTk+1 [ηmin, ηmax] = 1, 2, . . . we adopt an iterative projected gradient method that alternates between minimizing over the variables ηt and maximizing over the parameters λ(k) . Specifically, at each iteration, we update ηt with fixed λ(k) as follows ηt P[ηmin,ηmax] For fixed ηt, we update λ(k) via (cid:16) ηt αηj L(ηt, λ(k) (cid:17) ) . λ(k) P[λl,λu] (cid:16) λ(k) + βλ(k) L(ηt, λ(k) (cid:17) ) , where is the projection over box constraints, α and β are step sizes. This approach requires per-network numerical optimization to determine optimal learning rates case-by-case. To circumvent repeated numerical optimization for learning rate scheduling whenever new network is trained, we propose to fit these solutions with parametric function universally across networks. We note that the solution of optimization model is an unordered set {η1+Tk , η2+Tk , ..., ηTk+1}, meaning that any permutation of these values yields the same objective value. To facilitate function approximation and improve the training stability of the network, we sort the solution set in both descending and ascending orders before fitting them with smooth function. This pre-processing step preserves the optimality of the solution while enhancing the stability and interpretability of the fitted function. Due to the symmetry between the descending and ascending orders, and without loss of generality, we first fit function to the descending order solution. The corresponding function for the ascending order can then be easily obtained via simple transformation. Our key observation is that the numerical solutions under varying λl, λu configurations exhibit two characteristic decay modes. As shown in Figure 4, these transformations manifest through two distinct mechanisms: (i) gradual-to-accelerated decay: gentle initial decrease transitioning to rapid drop (as shown in Figures. 4(d),4(e) and 4(f)); (ii) rapid-to-gradual decay: Initial steep decline followed by slow decay (as shown in Figures. 4(g),4(h) and 4(i)). These pattern can be formulated as transformations of base function, which resembles the curvature variation of the transformed ℓ1 norm modulated by parameter [41]. We first isolate the base function by examining the limiting case where λl λu. In this regime, the numerical solution within each interval [1 + Tk, Tk+1] manifests cosine-like profile (Figures. 4(a),4(b) and 4(c)), suggesting the base form: 1 + cos( (2(t Tk) 1)π 2(Tk+1 Tk) ) Then we construct the fitting function as: (ηmax ηmin) a(1 + cos( (2(tTk)1)π 2(Tk+1Tk) )) + c(1 + cos( (2(tTk)1)π 2(Tk+1Tk) ) + ηmin. (12) where {a, b, c} control learning rate decay modes. In addition, through systematic fitting across nine configurations  (Table 4)  , we discover universal scaling relationship among the parameters: + 2c 2 16 (13) Table 4: The parameter details of curve fitting. hyper-parameters fitted parameters objective value(log)(4) ηmin ηmax 0 0 0 0 0 0 1 2 3 10 10 10 10 5 2 10 10 λ(k) 1.89 2.39 5.29 18.89 28.89 50.89 10.01 18.89 20.01 λ(k) 2.60 2.61 5.61 260.11 200.11 150.11 150.11 50.11 180.11 c 0.26 4.16 4.84 1124.67 755.91 0.27 77.45 0.11 0.07 0.51 7.35 7.32 212.28 85.01 0.05 684.15 2.52 3.07 0.00 0.53 1.28 1011.94 719.40 0.24 -268.20 -1.15 -1.47 φ 1.99 1.80 1.56 0.19 0.12 0.17 8.40 21.02 38.34 by numerical solution by fitting function 792.67 798.38 1118.79 2933.34 2607.11 2031.66 2373.58 2007.68 2604.98 750.83 770.38 1105.80 2933.62 2591.51 2075.50 2431.42 2044.84 2632.21 Table 4 lists the fitted parameters {a, b, c}, where the mean absolute error is 0.04. Figure 5 visualizes the variability of the b+2c by shading the region within 1 standard deviation from the theoretical value in Figure 5. The empirical results falls within 1σ of the predicted value (2.0 0.04) in most of cases, demonstrating strong agreement with the assumption relation (13). This empirical relationship (13) allows us to reduce the original 3-parameter system to 2 degrees of freedom as follows, (ηmax ηmin) 2a(1 + cos( (2(tTk)1)π 2b + (2a b)(1 + cos( (2(tTk)1)π 2(Tk+1Tk) ) 2(Tk+1Tk) )) + ηmin. (14) Actually, it is 1-parameter system (ηmax ηmin) 2(1 + cos( (2(tTk)1)π 2d + (2 d)(1 + cos( (2(tTk)1)π 2(Tk+1Tk) ) 2(Tk+1Tk) )) + ηmin, (15) where φ := b/a. We list the value of fitted parameter φ in Table 4. Since the equation (14) and the equation (15) are mathematically equivalent, we adopt the equation (15) as the canonical representation throughout this work. Then the corresponding function for the ascending order can then be easily obtained via phase shift of π as follow, (ηmax ηmin) 2(1 + cos( (2(tTk)1)π 2(Tk+1Tk) + π)) 2φ + (2 φ)(1 + cos( (2(tTk)1)π 2(Tk+1Tk) + π) + ηmin, (16) In summary, we obtain the ηt within the interval [1 + Tk, Tk+1] as follows ηt = (ηmax ηmin) 2(1 + cos( (2(tTk)1)π 2φ + (2 φ)(1 + cos( (2(tTk)1)π 2(Tk+1Tk) + (k 1)π)) 2(Tk+1Tk) + (k 1)π) + ηmin, (17) where is the hyper-parameter controlling the variation speed of ηt. 17 (a) (d) (g) (b) (e) (h) Figure 4: The curve fitting of numerical solutions. (c) (f) (i) Figure 5: Visualization of relation between parameter a, and c."
        },
        {
            "title": "C Adaptive simulation of existing schedules",
            "content": "Our schedule has adaptability owing to the parameter φ, enabling it to approximate the behavior of existing schedules (e.g.step decay, cosine annealing, cyclic schedule or Rex schedule) through simple parameter adjustments. More importantly, it outperforms these schedules by jointly optimizing training stability and final accuracy. We visualize the adaptive simulation of in Figure 6. By setting our parameter φ to 2, the UBA schedule degenerates to cosine schedule (figure 6(a)). By setting φ to large value such as 30, the UBA schedule mimics exponential schedule (figure 6(b)). By setting φ to small value such as 0.8, the UBA schedule mimics Rex schedule (figure 6(c)). By setting φ to 0 , the proposed schedule degenerates to constant. Therefore, we can control ηmax as piece-wise value depending on iteration, and the UBA schedule degenerates to step schedule (figure 6(d)). By setting to any integer larger than 1, the UBA schedule possesses cyclic characters, thus it mimics cyclic schedule (figure 6(e) 6(f)). Since the OneCycle learning rate schedule can be viewed as single-period version of cyclical learning rates, the UBA schedule mimics the OneCycle approach by emulating the cyclical schedule. (a) (d) (b) (e) Figure 6: Adaptive simulation by UBA schedule. (c) (f)"
        },
        {
            "title": "D Propositions and lemmas",
            "content": "D.1 Proof of Proposition 1 Proposition 1 The fitted function (5) is the exact closed-form solution to the min-max optimization problem: 2(1+cos( (2(tTk )1)π 2φ+(2φ)(1+cos( (2(tTk )1)π 2(Tk+1Tk ) ) 2(Tk+1Tk ) )) min η1+Tk ,η2+Tk ,...,ηTk+ max λ(k) λ(k) λ(k) Tk+1 (cid:89) (cid:34) (1 (cid:32) ( t=1+Tk 1 λ(k) 1 λ(k) )ηt + 1 λ(k) (cid:33) (cid:35)2 ) λ(k) (18) when the hyper-parameter φ are determined by λ(k) ηmax = 1, ηmin = 0. and λ(k) through the relation φ = 2 λ(k) λ(k) and Proof. According to the theorem in [13], among all polynomials of degree in µ that take the value +1 at µ = > 1, the Chebyshev polynomial Sn(µ) = Cn(µ) Cn(d) is the unique solution that minimizes the maximum absolute value over the interval (1, +1). Here, Cn(µ) = cos(n arccos(µ)) is the n-th order Chebyshev polynomial. We define the polynomial Q(λ(k) ) as: Q(λ(k) ) := Tk+1 (cid:89) (cid:34) (cid:32)(cid:32) 1 t=Tk+1 1 λ(k) (cid:33) 1 λ(k) ηt + 1 λ(k) (cid:33) (cid:35) λ(k) . which is degree = Tk+1 Tk polynomial in λ(k) Next, we introduce the variable transformation: 2λ(k) γ = + λ(k) + λ(k) λ(k) λ(k) to the interval 1 γ 1, such that λ(k) = λ(k) which maps the interval λ(k) corresponds to γ = 1 and λ(k) λ(k) λ(k) = λ(k) corresponds to γ = 1. The polynomial (γ) = Q(λ(k) (cid:32) ) now satisfies the condition in the theorem from [13], i.e., + λ(k) λ(k) λ(k) λ(k) = 1 since Q(0) = 1. (cid:33) Therefore, the original min-max problem min ηTk +1,ηTk +2,...,ηTk+1 max λ(k) λ(k) λ(k) Tk+1 (cid:89) (cid:34) (cid:32)(cid:32) 1 t=Tk+1 1 λ(k) 1 λ(k) (cid:33) ηt + 1 λ(k) (cid:33) (cid:35) λ(k) is equivalent to the problem of finding polynomial in γ of degree Tk+1 Tk that minimizes the maximum absolute value in the interval 1 γ 1. By the theorem in [13], the desired solution to the problem is the Chebyshev polynomial: STk+1Tk (γ) = CTk+1Tk (γ) (cid:18) (cid:19) +λ(k) λ(k) λ(k) λ(k) CTk+1Tk To match (γ) = STk+1Tk (γ), we equate the corresponding zeros. The roots of Q(λ(k) given by: ) = 0 are λ(k) = (cid:18)(cid:18) 1 1 λ(k) 1 λ(k) (cid:19) ηt + 1 λ(k) (cid:19) for = 1, 2, . . . , Tk+1 Tk. 20 On the other hand, the zeros of (γ) correspond to the values of γ where the polynomial (γ) vanishes, and these zeros are given by: γt = λ(k) + λ(k) λ(k) λ(k) (cid:18)(cid:18) 1 λ(k) 1 λ(k) (cid:19) (cid:19) ηt + 1 λ(k) (λ(k) λ(k) for = 1, 2, . . . , Tk+1 Tk. ) The zeros of the Chebyshev polynomial STk+1Tk (γ) are given by the values: cos (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19) for = 1, 2, . . . , Tk+1 Tk. Equating the zeros of STk+1Tk (γ) and (γ), we have: λ(k) + λ(k) λ(k) λ(k) (cid:18)(cid:18) 2 1 λ(k) 1 λ(k) (cid:19) (cid:19) ηt + 1 λ(k) Solving this equation for ηt, we obtain: (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19) = cos ) (λ(k) λ(k) ηt = (1 + cos (cid:16) (2(tTk)1)π 2(Tk+1Tk) (cid:17) ) 2 λ(k) λ(k) (cid:19) (cid:16) (cid:18) λ(k) λ(k) 1 1 + cos (cid:16) (2(tTk)1)π 2(Tk+1Tk) . (cid:17)(cid:17) D.2 Proof of Proposition 2 Proposition 2 Consider the training process within the interval [1 + Tk, Tk+1]. When the hyper-parameter φ is set sufficiently close to 2, the proposed learning rate scheduling formula reduces to the cosine learning rate schedule. Proof. lim φ2 (ηmax ηmin) 2(1 + cos( (2(tTk)1)π 2φ + (2 φ)(1 + cos( (2(tTk)1)π 2(Tk+1Tk) ) 2(Tk+1Tk) )) + ηmin = (ηmax ηmin)( 1 2 + 1 2 cos( (2(t Tk) 1)π 2(Tk+1 Tk) )) + ηmin = 1 2 1 2 (ηmax ηmin)(1 + cos( (ηmax ηmin)(1 + cos( (t Tk)π Tk+1 Tk (t Tk)π Tk+1 Tk ) cos( π 2(Tk+1 Tk) ) + sin( (t Tk)π Tk+1 Tk ) sin( π 2(Tk+1 Tk) )) + ηmin )) + ηmin (19) The final approximation holds because each training phase typically contains many iterations, i.e. , making 0. Consequently, sin( 2(Tk+1Tk) ) 0 and cos( 2(Tk+1Tk) ) 1. π Tk+1Tk π π D.3 Proof of Theorem 1 Lemma 1. Let ηj be the learning rate at the iteration [1 + Tk, t], defined by the proposed form (5). Let λ(k) be the eigenvalues of the Hessian matrix (k) (ξ) at the k-th strict minimum (k). Then for any [1 + Tk, Tk+1], the following inequalities hold, i 21 Case 1 (φ > 2): (cid:89) (cid:104) (1 ηjλ(k) (cid:105)2 ) j=1+Tk (cid:16) exp 2λ(k) (cid:17) ηmin(t Tk) (cid:16) 4(Tk+1 Tk) + (φ 2)π 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:17) Case 2 (φ < 2): (cid:89) (cid:104) (1 ηjλ(k) ) (cid:105)2 j=1+Tk (cid:16) exp 2λ(k) ηmin(t Tk) (cid:17) 4λ (k) (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (φ2)π (20) (cid:16) (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (cid:17) (2φ + 2π φπ) + (2φ)π 2(Tk+1Tk) 4λ (k) (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (2φ)π (21) (22) Proof. By the fact that 1 exp(x), we have (cid:89) (cid:104) (1 ηjλ(k) (cid:105)2 ) j=1+Tk exp (cid:88) ηjλ(k) j=1+Tk Substituting the expression for ηj by the proposed form (5) yields: exp 2 (cid:88) ηjλ(k) j=1+Tk 2λ(k) = exp (cid:88) j=1+Tk (ηmax ηmin) 2(1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:17) ) 2φ + (2 φ)(1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:17) + ηmin = exp (cid:16) 2λ(k) (cid:17) ηmin (t Tk) exp 2λ(k) (cid:88) j=1+Tk (ηmax ηmin) (cid:16) 2 1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:17)(cid:17) 2φ + (2 φ) (cid:16) 1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (23) (cid:17)(cid:17) The behavior of this term depends on the value of the parameter φ. We consider the following cases: if φ 2, 2 φ 0 Since cos( (2(t Tk) 1)π 2(Tk+1 Tk) (cid:18) (j Tk) 1 π (Tk+1 Tk) (cid:19) , ) cos( (2(j Tk) 1)π 2(Tk+1 Tk) ) 1 (cid:18) (2(j Tk) 1)π 2(Tk+1 Tk) (cid:19) (24) we have (cid:88) j=1+Tk 2φ + (2 φ) 1 (cid:16) 1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:17)(cid:17) = (cid:88) j=1+Tk 2φ + (2 φ) 1 (cid:16) 1 + 1 π (cid:16) (jTk) (cid:17)(cid:17) (Tk+1Tk) (25) (cid:88) j=1+Tk 1 4 + (φ2)π (Tk+1Tk) (j Tk) Then the equality (23) becomes exp (cid:16) 2λ(k) (cid:17) ηmin (t Tk) exp 2λ(k) (cid:88) j=1+Tk (ηmax ηmin) (cid:16) 2 1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:17)(cid:17) 2φ + (2 φ) (cid:16) 1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:17)(cid:17) (cid:16) exp 2λ(k) ηmin(t Tk) (cid:17) exp (cid:34) 4λ(k) (cid:18) (ηmax ηmin) 1 + cos (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19)(cid:19) (cid:88) j=1+Tk 2φ + (2 φ) 1 (cid:16) 1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:17)(cid:17) (cid:35) exp (cid:16) 2λ(k) (cid:17) ηmin (t Tk) exp (cid:34) 4λ(k) (cid:18) (ηmax ηmin) 1 + cos (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19)(cid:19) (cid:88) j=1+Tk 1 4 + (φ2)π (Tk+1Tk) (j Tk) (cid:35) Note that the function h(j) := 1 4+ (φ2)π Tk+1Tk so we can bound the summation from below by the integral: (26) is monotone decreasing over [1+Tk, t], (jTk) (cid:88) j=1+Tk 1 4 + (φ2)π Tk+1Tk (j Tk) dj dj (27) 1 4 + (φ2)π Tk+1Tk (j Tk) 1 4 + (φ2)π Tk+1Tk (j Tk) t+1 (cid:90) 1+Tk (cid:90) = 1+Tk (Tk+1 Tk) (φ 2)π ln (cid:18) 4(Tk+1 Tk) + (φ 2)π(t Tk) 4(Tk+1 Tk) + (φ 2)π (cid:19) 23 Thus the inequality (26) becomes exp (cid:16) 2λ(k) (cid:17) ηmin (t Tk) exp (cid:34) 4λ(k) (cid:18) (ηmax ηmin) 1 + cos (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19)(cid:19) (cid:88) j=1+Tk 1 4 + (φ2)π (Tk+1Tk) (j Tk) (cid:35) (cid:16) exp 2λ(k) ηmin(t Tk) (cid:17) exp (cid:34) 4λ(k) (cid:18) (ηmax ηmin) 1 + cos (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19)(cid:19) (Tk+1 Tk) (φ 2)π ln (cid:18) 4(Tk+1 Tk) + (φ 2)π 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:19)(cid:35) (cid:16) = exp 2λ(k) ηmin(t Tk) (cid:17)(cid:16) 4(Tk+1 Tk) + (φ 2)π 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:17) (cid:16) exp 2λ(k) ηmin(t Tk) (cid:17)(cid:16) 4(Tk+1 Tk) + (φ 2)π 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:17) 4λ (k) (cid:32) (cid:32) (ηmaxηmin ) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (φ2)π 4λ (k) (cid:32) (cid:32) (ηmaxηmin ) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (φ2)π (28) The last inequality is because λ(k) λ(k) and 4(Tk+1Tk)+(φ2)π 4(Tk+1Tk)+(φ2)π(tTk) 1. if φ 2, 2 φ 0 Since 1 + cos( (2(j Tk) 1)π 2(Tk+1 Tk) ) π (2(j Tk) 1)π 2(Tk+1 Tk) . (29) We obtain exp (cid:16) 2λ(k) (cid:17) ηmin (t Tk) exp 2λ(k) (cid:88) j=1+Tk (ηmax ηmin) (cid:16) 2 1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:17)(cid:17) 2φ + (2 φ) (cid:16) 1 + cos (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:17)(cid:17) (cid:16) exp 2λ(k) ηmin(t Tk) (cid:17) exp (cid:34) 4λ(k) (cid:18) (ηmax ηmin) 1 + cos (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19)(cid:19) (cid:88) 1 j=1+Tk 2φ + (2 φ)(1 + cos (cid:16) exp 2λ(k) ηmin (t Tk) (cid:17) (cid:16) (2(jTk)1)π 2(Tk+1Tk) (cid:34) 4λ(k) exp (cid:35) (cid:17) (ηmax ηmin) (cid:18) 1 + cos (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19)(cid:19) (cid:88) j=1+Tk 1 (2φ + 2π φπ) (2φ)π (Tk+1Tk) (j Tk 0.5) (cid:35) 24 (30) because function h(j) := (2φ+2πφπ) (2φ)π (Tk+1Tk ) (jTk0.5) range [1 + Tk, +), then it holds that is monotone increasing in the (cid:88) 1 j=1+Tk (2φ + 2π φπ) (2φ)π (Tk+1Tk) (j Tk 0.5) (cid:90) Tk (2φ + 2π φπ) (2φ)π (Tk+1Tk) (j Tk 0.5) dj (31) = (Tk+1 Tk) (2 φ)π ln (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (2φ + 2π φπ) + (2φ)π 2(Tk+1Tk) Thus, we have exp (cid:16) 2λ(k) (cid:17) ηmin (t Tk) exp (cid:34) 4λ(k) (cid:18) (ηmax ηmin) 1 + cos (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19)(cid:19) (cid:88) j=1+Tk (2φ + 2π φπ) (2φ)π 1 (cid:16) exp 2λ(k) ηmin(t Tk) (cid:17) exp (Tk+1Tk) (j Tk 0.5) (cid:18) (cid:34) 4λ(k) (ηmax ηmin) 1 + cos (cid:35) (Tk+1 Tk) (2 φ)π ln (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (2φ + 2π φπ) + (2φ)π (cid:17) 2(Tk+1Tk) (cid:16) exp 2λ(k) ηmin(t Tk) (cid:18) (2(t Tk) 1)π 2(Tk+1 Tk) (cid:19)(cid:19) (cid:35) (cid:16) (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (cid:17) (2φ + 2π φπ) + (2φ)π 2(Tk+1Tk) (cid:16) exp 2λ(k) ηmin(t Tk) (cid:17) (cid:16) (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (cid:17) (2φ + 2π φπ) + (2φ)π 2(Tk+1Tk) 4λ (k) (cid:32) (cid:32) (ηmaxηmin ) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (2φ)π 4λ (k) (cid:32) (cid:32) (ηmaxηmin ) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (2φ)π The last inequality is because λ(k) λ(k) and (cid:16) (2φ+2πφπ) (2φ)π (Tk+1Tk ) (tTk0.5) (cid:17) (2φ+2πφπ)+ (2φ)π 2(Tk+1Tk ) (32) 1. Theorem 1 Let nt = (k) introduced by sampling at iteration t. Assume that the sampling Hessian satisfies: Eξ σ2H (k) (ξ)(Wt (k)) be the stochastic curvature noise (cid:3) (Wt (k)) (k) for some constant σ. (cid:2)ntn If we set the learning rate as the proposed form (5) where hyper-parameter φ > 2, the loss uncertainty introduced by stochastic gradient method within the interval [1 + Tk, Tk+1] 25 can be quantified as two terms, bias term and variance term, bounded as follows, (cid:104) L(f (Wt, ξ)) L(f ( (k), ξ)) (cid:105) (cid:16) 4(Tk+1 Tk) + (φ 2)π 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:17) 4λ (k) (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (φ2)π exp (cid:16) 2λ(k) (cid:17) ηmin(t Tk) W1+Tk (k)2 λ(k) 2 + σ2 (cid:88) i=1+Tk η2 (cid:88) j=1 (λ(k) )2 exp (cid:16) 2λ(k) ηmin(t i) (cid:17) (cid:16) 4(Tk+1 Tk) + (φ 2)π(i Tk) 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:17) 4λ (k) (cid:32) (cid:32) (ηmaxηmin ) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (φ2)π (33) If we set the learning rate as the proposed form (5) where hyper-parameter φ < 2, the loss uncertainty introduced by stochastic gradient method within the interval [1 + Tk, Tk+1] can be quantified as two terms, bias term and variance term, bounded as follows, (cid:105) (cid:104) L(f (Wt, ξ)) L(f ( (k), ξ)) (cid:16) (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (cid:17) 4λ (k) (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (2φ)π (2φ + 2π φπ) + (2φ)π exp (cid:16) 2λ(k) (cid:17) ηmin(t Tk) 2(Tk+1Tk) W1+Tk (k)2 λ(k) 2 + σ2 (cid:88) i=1+Tk η2 (cid:88) j=1 (λ(k) )2 exp (cid:16) 2λ(k) (cid:17) ηmin(t i) (cid:16) (2φ + 2π φπ) (2φ)π (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (Tk+1Tk) (i Tk 0.5) (cid:17) 4λ (k) (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (2φ)π (34) denote full batch Hessian at k-th minimum, such that E(H (k) (Wt (k)) (k) , and (ξ)(Wt (k)), which indicates the bias. The uncertainty (ξ)) = (k) f Proof. Let (k) denote nt = (k) brought by stochastic gradient method is measured as follows, (ξ)(Wt (k)) (Wt (k)) Wt+1 (k) = Wt (k) ηtH (k) = Wt (k) ηtH (k) + ηtH (k) = (I ηtH (k) = (I ηtH (k) )(Wt (k)) + ηtnt )(I ηt1H (k) (Wt (k)) ηtH (k) f (ξ)(Wt (k)) ) (I ηtqH (k) )(Wtq (k)) (35) (cid:88) + i=tq (I ηtH (k) )(I ηt1H (k) ) (I ηi+1H (k) )ηini := (k) (k) t1 (k) tq(Wtq (k)) + (cid:88) i=tq (k) (k) t1 (k) i+1ηini where (k) := (I ηiH (k) ) . 26 According to the approximation approach of the objective function (1) in Section 3.1 , i.e. L(f (W, ξ)) L(f ( (k), ξ)) + 1 (ξ)(W (k)), the loss uncertainty introduced by stochastic gradient method within the interval [1 + Tk, Tk+1] is quantified as E[(WTk+1 (k))H (k) (WTk+1 (k))], where the loss function is approximated by leveraging the full batch Hessian. 2 (W (k))H (k) (cid:104) L(f (Wt, ξ)) L(f ( (k), ξ)) (cid:105) (cid:104) = (Wt (k))H (k) (Wt (k)) (cid:105) (cid:34) (k) = (k) t1 (k) 1+Tk (W1+Tk (k)) + (cid:88) i=1+Tk (k) (k) Tk+11 (k) i+1ηini (k) (k) t1 (k) 1+Tk (W1+Tk (k)) + (cid:88) (k) (k) t1 (k) i+1ηini (cid:104) (Wt (k))P (k) 1+Tk (Wt (k))P (k) 1+Tk (k) (k) (k) (k) (k) i=1+Tk (k) 1+Tk (cid:105) (Wt (k)) (cid:88) (k) (k) i+1ηini (k) = + 2E + (cid:88) i=1+Tk (k) (k) i+1ηini (k) i=1+Tk (cid:88) i=1+Tk (k) (k) i+1ηini (36) (cid:35) (cid:33)(cid:35) (cid:34) (Wt (k))P (k) 1+Tk Since [nt] = 0, while, we notice the data samplings between different iteration are independent, thus (cid:2)(ni)nj 0 (i = j). We obtain i+1ηini (k) (k) (k) (k) = 0. Mean- (cid:3) = (cid:32) (cid:80) i=1+Tk t (cid:88) (k) (k) i+1ηini (k) (cid:88) i=1+Tk (k) (k) i+1ηini i=1+Tk (cid:88) = ηin (k) i+1 (k) (k) (k) (k) j+1ηjnj (37) i,j=1+Tk (cid:88) (cid:16) = i=1+Tk ηin (k) i+1 (k) (k) (k) (k) i+1ηini (cid:17) Therefore, Eq.(36) can be reformulated as follows, (cid:88) (k) (k) i+1ηini (k) (cid:88) (k) (k) i+1ηini i=1+Tk (cid:104) = (Wt (k))P (k) 1+Tk (k) (k) (k) 1+Tk (cid:105) (Wt (k)) (38) i=1+Tk (k) ηin (k) i+1 (k) (k) (k) (k) i+1ηini (cid:17) (cid:88) (cid:16) + i=1+Tk := + where we denote := (cid:16) term and := (cid:80) i=1+Tk (cid:104) (Wt (k))P (k) 1+Tk (k) (k) i+1 (k) (k) ηin (k) (k) (k) (cid:17) (k) i+1ηini (k) 1+Tk (cid:105) (Wt (k)) bias variance term. 27 Denote u(k) for the dimension vector space. Then we have (k) {1, 2, , } are linearly independent eigenvectors, which can form the basis (i = 1, 2, , ) and (ξ)u(k) = λ(k) u(k) f the initial deviation from the optimal solution can be expressed as Wt (k) = (cid:80) i=1 u(k) s(k) where are the coefficients corresponding to the eigenvector components. Combined with Lemma 1, we s(k) obtain Case 1 (φ > 2): = (Wt (k))P (k) 1+Tk (k) (k) (k) (k) 1+Tk (Wt (k)) (cid:104) = (I ηtH (k) ) (I η1+Tk (k) )(W1+Tk (k)) (cid:105) (k) (I ηtH (k) ) (I η1+Tk (k) )(W1+Tk (k)) = (cid:88) i=1 (cid:88) i= (s(k) (s(k) )2u(k) 2 2λ(k) )2u(k) 2 2λ(k) (cid:89) (cid:104) (1 ηjλ(k) (cid:105)2 ) j=1+Tk exp (cid:16) 2λ(k) (cid:17) ηmin(t Tk) (cid:16) 4(Tk+1 Tk) + (φ 2)π 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:17) 4λ (k) (cid:32) (cid:32) (ηmaxηmin ) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (φ2)π (cid:16) 4(Tk+1 Tk) + (φ 2)π 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:17) 4λ (k) (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (φ2)π exp (cid:16) 2λ(k) (cid:17) ηmin(t Tk) W1+Tk (k)2 λ(k) (39) In addition, (cid:88) (cid:16) = ηin (k) i+1 (k) (k) (k) (k) i+1ηini (cid:17) i=1+Tk (cid:88) i=1+Tk (cid:88) i=1+Tk (cid:88) = = = (cid:16) (cid:104) tr η2 (cid:16) (cid:104) tr η2 (k) i+1 (k) (k) (k) (k) i+1ni (cid:17)(cid:105) i+1 (k) (k) (k) (k) (k) i+1nin (cid:17)(cid:105) η2 tr (cid:104) i+1 (k) (k) (k) (k) (k) i+1 (cid:0)nin (cid:1)(cid:105) i=1+Tk (cid:88) σ2 η2 tr (cid:104) i+1 (k) (k) (k) (k) (cid:105) (40) (k) i+1H (k) i=1+Tk (cid:88) i=1+Tk (cid:88) η2 η2 = σ σ2 (cid:88) (λ(k) )2 (cid:89) (cid:16) 1 ηqλ(k) (cid:17)2 j=1 q=i+1 (cid:88) (λ(k) (cid:16) )2 exp 2λ(k) (cid:17) ηmin(t i) i=1+Tk j=1 (cid:16) 4(Tk+1 Tk) + (φ 2)π(i Tk) 4(Tk+1 Tk) + (φ 2)π(t Tk) (cid:17) (k) 4λ (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (φ2)π 28 where the second equality comes from cyclic property of trace and the first inequality comes from Eξ . (cid:3) σ2H (k) (cid:2)ntn Case 2 (φ < 2): = (Wt (k))P (k) 1+Tk (k) (k) (k) (k) 1+Tk (Wt (k)) (cid:104) = (I ηtH (k) ) (I η1+Tk (k) )(W1+Tk (k)) (cid:105) (k) (I ηtH (k) ) (I η1+Tk (k) )(W1+Tk (k)) = (cid:88) i=1 (cid:88) i=1 (s(k) (s(k) )2u(k) 2 2λ(k) )2u(k) 2 2λ(k) (cid:89) (cid:104) (1 ηjλ(k) (cid:105)2 ) j=1+Tk exp (cid:16) 2λ(k) (cid:17) ηmin(t Tk) (cid:16) (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (cid:17) (2φ + 2π φπ) + (2φ)π 2(Tk+1Tk) 4λ (k) (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (2φ)π (cid:16) (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (cid:17) 4λ (k) (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (2φ)π (2φ + 2π φπ) + (2φ)π exp (cid:16) 2λ(k) (cid:17) ηmin(t Tk) 2(Tk+1Tk) W1+Tk (k)2 λ(k) 2 In addition, (cid:88) (cid:16) V = ηin (k) i+1 (k) (k) (k) (41) (k) i+1ηini (cid:17) i=1+Tk (cid:88) i=1+Tk (cid:88) i=1+Tk (cid:88) = = = (cid:16) (cid:104) tr η2 (cid:16) (cid:104) tr η2 (k) i+1 (k) (k) (k) (k) i+1ni (cid:17)(cid:105) i+1 (k) (k) (k) (k) (k) i+1nin (cid:17)(cid:105) (cid:104) η2 tr i+1 (k) (k) (k) (k) (k) i+1 (cid:0)nin (cid:1)(cid:105) i=1+Tk (cid:88) σ (cid:104) η2 tr i+1 (k) (k) (k) (k) (k) i+1H (k) (cid:105) i=1+Tk (cid:88) i=1+Tk (cid:88) η2 η2 = σ2 σ2 (cid:88) (λ(k) )2 (cid:89) (cid:16) 1 ηqλ(k) (cid:17) j=1 q=i+1 (cid:88) (λ(k) (cid:16) )2 exp 2λ(k) (cid:17) ηmin(t i) i=1+Tk j=1 (cid:16) (2φ + 2π φπ) (2φ)π (2φ + 2π φπ) (2φ)π (Tk+1Tk) (t Tk 0.5) (Tk+1Tk) (i Tk 0.5) 4λ (k) (cid:17) (cid:32) (cid:32) (ηmaxηmin) 1+cos (2(tTk )1)π 2(Tk+1Tk ) (cid:33)(cid:33) (Tk+1Tk ) (2φ)π (42) where the second equality comes from cyclic property of trace and the first inequality comes from Eξ (cid:3) σ2H (k) (cid:2)ntn . 29 (cid:2)ntn In this proof, we employ assumption Eξ acknowledge that the idea of the proof is inspired by the approach introduced in [15, 35] (cid:3) σ2H (k) used in the work [15, 35]. Moreover, we"
        },
        {
            "title": "E Experimental details",
            "content": "E.1 Baselines We detail the baselines as follows, Step schedule(SS) reduces the learning rate in piecewise manner. It implements discrete learning rate drops at predefined epoch intervals following schedule g(t) = dt/ts, where ts is the predefined decaying step and di(di di+1) is predefined decaying scalar [15]. typical implementation decreases the learning rate by factor of 0.1 after 50% of the epochs and further by factor of 0.01 after 75% of the epochs [20]. However, it requires careful tuning of step timing since abrupt changes may destabilize optimization. In this paper, we employ such setting of step schedule ( 0.1 after 50% and 0.01 after 75%) for all our experiments. 2 (η0ηmin)(1+cos( tπ Cosine schedule(CS) controls the t-th iteration learning rate ηt following the mathematical formulation as ηt = ηmin+ 1 )) where is total iterations. It provides smooth transition between learning rates, shown to improve final model accuracy step schedule. Moreover, it is proposed to mitigate abrupt decreases in the learning rate that could hinder models from escaping local minima. This approach has demonstrated effectiveness, particularly in transformer training [32]. Besides, it includes restart mechanisms where becomes the cyclic period. Cyclical Learning Rates (CLR) oscillates between base learning rate ηmin and maximal learning rate ηmax with triangular and triangular2 policies. The cyclic property help traverse loss landscape barrier, so that it enables neural networks to train significantly faster, often by an order of magnitude, compared to standard training methods [42, 43]. Besides, the commonly-used OneCycle learning rate schedule can be seen as the one period version of Cyclical Learning Rates. In our experiments, this baseline CLR includes Cyclical and OneCycle schedules(1C). For CLR, we set linear mode for each period and period is two. Budgeted training(BT) is conducted by the proposed budget-aware setting of existing learning rate schedule under epoch budget [29]. It reformulates standard schedules such as linear, step and cosine schedule as functions of remaining budget. The classic linear decay version is formulated as ηt = η0 (1 t/T ) where is total iterations. It is particularly effective for hyper-parameter tuning. Reflected Exponential (REX) controls the t-th iteration learning rate ηt following the mathematical formulation as ηt = η0 . It is inspired by the performance of delayed linear schedules, aggressively reducing the learning rate near the end of training and reflecting an exponential decay [5]. 1t/T 0.5+0.5(1t/T ) (cid:17) (cid:16) E.2 Evaluation benchmarks of language task We evaluate models on diverse set of open benchmarks. The benchmarks are listed in Table 5. For language models evaluation, we adopt the [14] for standardized performance comparisons. E.3 Implementation Details Consistent with CIFAR, ImageNet and OLMo practices, we use step-wise scheduling for all training procedures. Vision tasks on CIFAR CIFAR10/100 are simple benchmark datasets that are widely used for quick and efficient evaluation of deep learning tasks. CIFAR10 consists of training split of 50000 examples and test split of 10000 examples. Each example is 3232 color image in 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). CIFAR100 comprises 60000 3232 pixels color images which are divided into 100 classes. Each class contains 600 images and the classes are grouped into 20 super-classes. Each image comes with fine label (the class to which it belongs) and coarse label (the super-class to which it belongs). We use the torch.optim.SGD and torch.optim.AdamW API to configure the optimizer. For vision tasks in Section 4.1, we all adopt the SGD optimizer. For ablation studies in Section 4.3, we adopt the AdamW optimizer. We 31 Table 5: Evaluation benchmarks of language task"
        },
        {
            "title": "Datasets name",
            "content": "abbreviation ARC-Easy [10] ARC-E ARC-Challenge [10] ARC-C HellaSwag [54] HSWAG PIQA [3] PIQA WinoGrande [38] WG OpenbookQA [34] OBQA BoolQ [9] BoolQ SciQ [49] SciQ COPA [17] COPA CommonsenseQA [45] CSQA SocialIQA [39] SIQA STEM MMLU stem [21] MMLU humanities [21] HUM SOC MMLU social sci [21] OTH MMLU other [21] initialize the learning rate to 1e-1, set the batch size to 128. Each epoch consists of 50000/128 + 1 = 391 steps. We leave the weight decay value as 5e-4, and complete the training task on PyTorch 2.4.1+cu118 with RTX 5880 and RTX 3090. Vision tasks on ImageNet To evaluate the scalability of our schedule on larger-scale dataset, we conduct experiments on the ImageNet dataset, dataset that more closely reflects real-world visual recognition challenges. ImageNet consists of approximately 1.28 million training images and 50000 validation images across 1000 classes, which also come with an official dataset split. This benchmark, particularly with the ResNet-50 architecture, has been extensively studied and is widely regarded as standard evaluation setup. We adopt the common ResNet50 architecture, and separately train it with different learning rate schedulers. To provide comprehensive evaluation, we conduct experiments using both the classical SGD optimizer and the popularized AdamW optimizer. For the experiments with SGD, we set the peak learning rate to 0.5 and apply the weight decay factor of 1e-4. For the experiments using AdamW, the peak learning rate is set to 3e-3, with decoupled weight decay of 0.02. Following default configuration which requires learning rate warmup, we integrate 10% warmup phases of the total training tokens into all schedules. For schedules that inherently include warmup-like behavior, such as the 1C schedule, where the learning rate initially rises from zero to target value, we retain their original settings during the ascending phase. We set the batch size to 2048 and complete these training tasks with 8 NVIDIA A100 GPUs. Language tasks For language tasks, we use the OLMo model. OLMo [18] is decoder-only transformer-based language model that builds upon the vanilla Transformer architecture [48]. It incorporates several key improvements inspired by recent large language models (LLMs) such as PaLM [8], the LLaMA family [47], OpenLM and Falcon [1]. Notably, OLMo stands out as truly open language model, offering full transparency through its open training data, released training/evaluation code, intermediate model checkpoints, and comprehensive training logs, making it valuable resource for reproducibility and further research. For the training hyper-parameters on OLMo, we primarily adopt the configuration outlined in OLMo [18]. We conduct experiment on H100 and A100. Following default configuration which requires learning rate warmup, we integrate 10% warmup phases of the total training tokens into all schedules. For schedules that inherently include warmup-like behavior, such as the 1C schedule, where the learning rate initially rises from zero to target value, we retain their original settings during the ascending phase. Besides, large language models commonly provide multiple scales to accommodate different compute constraints. We adjust both model size and training steps to explore budgets. To ensure robust conclusions, we maintain data-to-parameters ratio (D/N) of 500 across scales. The specific configurations are as follows, 36M-parameter model is trained on 50 billion tokens, 73M-parameter model is trained on 50 billion tokens, 151M-parameter model is trained on 75 billion tokens, 300M-parameter model is trained on 150 billion tokens. This scaling strategy allows us 32 to systematically study the relationship between model size, training data, and performance while controlling for the D/N ratio. The OLMo model is trained using AdamW optimizer. The detailed configuration are shown in Table 6. Table 6: Configurations of OLMo"
        },
        {
            "title": "Model scale",
            "content": "36M 73M 151M 300M token number Layers (L) Hidden dim (D) Attention heads Inner dim (H) Vocab size (V ) 50B 12 512 8 1280 50B 16 640 10 1536 75B 20 768 8 2240 100, 150B 24 1024 8 2688 The code of UBA schedule is available at https://github.com/Ttt-answer/UBA.git. E.4 Overall experiment results for Vision Classification Tasks Overall results for the vision classification tasks are presented in Table 7, depicting validation accuracy on vision benchmarks (e.g., CIFAR10/100 and ImageNet) using different architectures (VGG16, ResNet18, ResNet34, ResNet50). It is obvious that our proposed UBA schedule outperform baselines, achieving the highest validation accuracy over both small-scale and large-scale benchmarks across all training budgets. Our analysis reveals the following key findings. (i) On CIFAR10-ResNet18 and CIFAR100-ResNet34 task, UBA shows the strongest performance across all training budgets. On the large-scale ImageNet benchmark with ResNet50, UBA maintains consistent superiority. The consistent improvements on both small-scale and large-scale benchmarks highlight UBAs generalizability across scales and data regimes. (ii) UBA outperforms baselines not only at 100% training budget but also at 25% and 50% iteration budgets, demonstrating its budget efficiency, i.e.,effectiveness in computation-constrained scenarios. (iii)Notably, UBA shows robust performance even while the second-best schedule varies depending on both the datasets, architectures and training budgets. This instability among competing schedules suggests their performance is highly sensitive to datasets-architectures characteristics and training dynamics at different learning phases. Therefore, for practitioners seeking reliable schedules without extensive method selection, UBA provides default-strong choice. UBA eliminates the need for case-by-case baseline comparison and delivers stable superiority and reliability regardless of datasets, architectures, training budgets and scales. E.5 Overall experiment results for language models For most tasks, we conduct comprehensive evaluation across six representative baselines to ensure broad coverage. For large model with 300M parameters, we focus our comparison on the top three performing baselines due to the computational tractability. The overall performance of the OLMo models is presented in Table 8. We also plot training curves in Figure 7,8, 9 and 10, showing training and validation losses along with downstream evaluation results across model sizes ranging from 36M to 300M parameters and training data scales from 50B to 150B tokens. As shown, UBA schedule consistently achieves lower training and validation losses while delivering superior downstream performance. From Table 8, UBA achieves state-of-the-art performance on approximately 50% of the benchmarks across all scales while the second-best schedule varies. Furthermore, it achieves consistent superior average performance among all baselines. It highlights the stable superiority and reliability of UBA, providing default-strong choice. Moreover, it demonstrates significant improvements in SciQ-73M(+1.7) and ARC-E-300M(+2.63), highlighting its ability to enhance generalization across diverse benchmarks. Besides, in Figure 2, UBA consistently achieves lower training loss and validation loss throughout the training, indicating the efficient training ability and downstream performance enhancement. Notably, while task-specific tuning of φ can further improve model performance, we intentionally fix its value across all tasks and architectures to ensure fair evaluation. 33 Table 7: Generalization accuracy. Dataset Network Method training budget (epoch(%)) 75 (25%) 150(50%) 300(100%) CIFAR10 ResNet18 VGG16 CIFAR100 ResNet34 ImageNet ResNet50 SS CS CLR 1C BT REX UBA(ours) 92.41 93.66 91.89 94.36 93.24 94.79 94.54 93.90 94.15 92.23 95.02 94.28 94. 95.26 93.94 95.40 95.32 95.48 95.55 95.59 95.74 75 (25%) 150(50%) 300(100%) SS CS CLR 1C BT REX 67.97 71.07 70.38 70.69 69.05 69.84 UBA(ours) 71.75 69.91 71.37 69.90 71.28 70.90 71.46 73. 72.17 72.16 70.91 73.24 71.74 72.73 75.16 30 (25%) 60(50%) 120(100%) SS CS CLR 1C BT REX 70.85 63.88 72.63 73.72 72.53 73.12 UBA(ours) 74.57 75.58 68.84 73.81 75.53 75.40 75.48 76.68 77.28 70.43 74.80 77.90 78.49 77. 78.97 75 (25%) 150(50%) 300(100%) SS CS CLR 1C BT REX 74.84 75.99 72.87 75.28 75.71 75. UBA(ours) 76.00 76.96 77.79 74.88 77.37 77.48 77.03 77.99 78.10 79.10 76.91 78.79 78.66 78.46 79. Remarkably, even with this universal φ setting, UBA consistently outperforms baselines on diverse benchmarks, demonstrating inherent robustness to varying benchmarks and model scales. E.6 Performance across different optimizers Our scheduling strategy originates from the solution to optimization problem under gradient descent dynamics. While modern optimizer (e.g., AdamW [31]) introduce additional momentum and adaptive mechanisms, they maintain the fundamental property of performing gradient-based updates. To verify the performance of schedule, we conduct cross-optimizer ablation studies. We evaluate the model on CIFAR100 and ImageNet datasets using AdamW optimizer. For AdamW, we use learning rate of 0.003 and weight decay of 0.0001. We report the validation accuracy for each configuration. The results are summarized in Table 9. As shown in Table 9, our UBA schedule achieves state-of-the-art validation accuracy on both SGD and AdamW optimizers, consistently outperforming baselines across CIFAR100-ResNet34 and ImageNet-ResNet50 benchmarks. This demonstrates that although our schedule is theoretically derived from standard gradient descent dynamics, it generalizes effectively to modern optimizers like AdamW despite their additional momentum and adaptive mechanisms highlighting its broad applicability. 34 Figure 7: Overall performance for language tasks on OLMo-36M. E.7 Parameter analysis of φ In this subsection, we perform sensitivity analysis of the key parameter φ in equation (5) , which controls the variation speed of learning rate. Our experiments systematically evaluate 35 Figure 8: Overall performance for language tasks on OLMo-73M. φ {0.25, 0.5, 1.0, 2.5, 5, 10} across different optimization scenarios. The results are plotted in Figure 11 and listed in Table 10). Observations From the experimental results, we find an interesting dichotomy of φ. Phenomenon(a): By choose proper φ, UBA demonstrates consistent performance gains over all baselines 36 Figure 9: Overall performance for language tasks on OLMo-150M. across the full spectrum of training budgets (25%, 50% and 100%), irrespective of optimizer choice (SGD/AdamW). This suggests the effectiveness of UBA is orthogonal to specific optimization strategies. Besides, with optimizer changed, UBA maintains comparable accuracy on the same dataset-architecture, which shows that UBA can exhaustively exploit the models capacity regardless of optimizer variants. Phenomenon(b): As shown in Figure 11 and Table 10, on CIFAR100 dataset, 37 Figure 10: Overall performance for language tasks on OLMo-300M. the best performance for AdamW is achieved with φ = 1, whereas SGD performs optimally with φ = 2.5. On ImageNet dataset, the best performance for AdamW is achieved with φ = 0.5, whereas SGD performs optimally with φ = 10. Our experiments reveal notable trend: when using AdamW, smaller φ yields better performance, whereas larger φ is preferred for SGD. 38 Table 8: Performance comparison on OLMo model Benchmark(accuracy %) Size Sched. PIQA HSWAG OBQA SciQ ARC-E ARC-C COPA SIQA SOC OTH Avg. 6 3 3 7 0 5 1 CS BT SS REX CLR 1C UBA CS BT SS REX CLR 1C UBA CS BT SS REX CLR 1C UBA 59.74 27.36 59.41 27.33 60.12 27.63 60.17 27.82 61.15 27.21 60.17 27.91 26.80 27.40 27.20 27.80 27.60 25.80 66.50 44.74 67.20 43.68 65.30 45.26 68.10 45.09 67.50 42.11 67.40 43. 60.39 27.98 27.40 68.20 45.79 63.06 29.68 61.70 29.79 61.53 29.30 62.46 30.22 62.30 29.38 61.81 29.90 28.80 28.00 28.20 27.60 27.80 27.80 72.60 45.09 71.40 47.54 69.40 47.19 72.20 45.09 70.40 45.44 71.10 47. 63.17 30.09 28.80 74.30 45.79 66.00 35.19 64.85 34.95 65.45 34.76 65.56 35.72 65.07 34.76 65.29 35.16 32.00 29.80 30.60 29.40 31.60 32.80 76.60 49.82 78.20 48.95 77.10 49.65 78.30 52.46 77.20 50.70 76.30 53. 65.23 35.50 29.80 78.30 50.35 CS BT REX 0 0 3 68.88 45.32 69.64 45.21 70.18 46. 33.40 30.80 33.00 83.20 57.19 83.70 57.37 84.40 57.72 UBA 69.48 46.44 34.60 83.90 60. 21.74 21.40 22.41 22.41 22.07 23.41 21.74 25.08 23.41 24.41 26.09 24.08 26.42 22.74 25.42 26.42 24.75 25.08 25.08 25.75 27. 27.76 26.42 28.43 29.10 60.00 40.58 24.33 29.09 40.09 59.00 39.82 23.48 29.13 39.79 63.00 40.89 23.87 29.62 40.53 62.00 40.63 25.18 30.02 40.92 62.00 41.45 24.29 29.38 40.48 55.00 40.02 22.26 29.86 39.55 63.00 40.69 24.33 30.14 40.97 65.00 41.56 26.24 28.17 42.53 66.00 41.97 25.05 26.48 42.13 65.00 41.76 26.32 28.33 42.14 65.00 41.81 25.52 29.34 42.53 62.00 42.02 25.05 29.05 41.75 63.00 40.79 24.37 28.81 42.15 65.00 41.45 27.13 28.85 42. 67.00 42.84 26.24 32.27 45.34 67.00 42.99 26.83 31.87 45.19 68.00 42.37 26.58 31.59 45.09 69.00 43.30 27.68 32.64 45.91 68.00 43.71 27.30 31.23 45.47 67.00 43.45 27.64 30.62 45.72 67.00 43.50 29.29 32.72 45.91 69.00 44.58 29.25 34.57 49.32 72.00 43.96 28.66 35.49 49.33 67.00 43.71 29.50 36.74 49.70 72.00 44.42 28.53 35.41 50.42 Analysis Furthermore, we intend to explain the dual behavior of φ through theoretical analyses (Proposition 1 and Theorem 1). We attribute Phenomenon (a) to UBAs optimal scheduling dynamics, governed by the φ value. Moreover, Phenomenon (b) reveals fundamental dichotomy in φ value selection between optimizers. We attribute this phenomenon(b) to the preconditioning effect of AdamW. Unlike SGD, AdamW adapts the learning rate by scaling gradients with the square root of vt. In regions where gradients gt are large, the surrounding landscapes the second moment estimate vt is also large, effectively reducing the learning rate. This adaptive are steep. The denominator behavior mimics preconditioning, implicitly lowering the condition number of the optimization landscape. Our theoretical framework (Proposition 1) establishes connection between parameter φ and the condition number of the landscape around local minima: smaller φ is favored for well-conditioned problems (low condition number), while larger φ is beneficial for large condition number scenarios. Since preconditioning effect of AdamW naturally mitigates ill-conditioning, the schedule with smaller φ is preferred. In contrast, SGD lacks such adaptive mechanisms, thus larger φ is more suitable for this situation. This alignment between theory and experiment underscores the importance of tailoring φ to the optimizers characteristics. Thus, we recommend selecting higher values of φ when facing challenging optimization difficulty, while employing lower φ values in scenarios with smoother optimization difficulty. E.8 Performance across different periods Our schedule has periodic phase-based learning rate adjustment setting, where the learning rate at the k-th phase is dynamically determined by the k-th local minimum of the loss landscape. In the original conception, this design captures the optimization dynamics around successive local minima, with the hyper-parameter φ related to the difficulty of each phase. Specifically, φ controls the trade-off between exploration (large steps) and exploitation (small steps) within each phase. 39 Table 9: Cross-optimizer ablation studies. We list validation accuracy for vision classification tasks across SGD and AdamW. Dataset -network Optimizer Schedule training budget (epoch(%)) 30 (25%) 60(50%) 120(100%) CIFAR100 -ResNet34 ImageNet -ResNet50 SGD AdamW SGD AdamW SS CS CLR 1C BT REX UBA(ours)φ = 5 SS CS CLR 1C BT REX UBA(ours)φ = 0.5 SS CS CLR 1C BT REX UBA(ours)φ = 5 SS CS CLR 1C BT REX 70.85 63.88 72.63 73.72 72.53 73.12 74. 64.01 62.56 62.87 64.27 64.01 65.19 64.44 75.58 68.84 73.81 75.53 75.40 75.48 76.68 70.93 68.91 70.13 71.60 71.44 71.72 72. 77.28 70.43 74.80 77.90 78.49 77.99 78.97 73.65 72.98 73.22 74.06 74.34 74.08 74.48 75 (25%) 150(50%) 300(100%) 74.84 75.99 72.86 75.28 75.71 75.28 76.00 74.64 75.68 74.93 76.38 76.10 76.42 76.96 77.79 74.88 77.37 77.48 77.03 77. 77.51 78.10 77.20 78.11 78.14 78.28 78.37 78.10 79.10 76.91 78.79 78.66 78.46 79.32 79.01 79.04 78.76 79.21 79.05 78.86 79. UBA(ours) φ = 0.5 76.57 Table 10: Cross-φ ablation studies. Dataset Network Epoch Optimizer UBA CIFAR100 ResNet34 60 ImageNet ResNet50 φ=0.25 φ=0.5 φ=1 φ=2.5 φ=5 φ=10 SGD AdamW SGD AdamW 75.10 72.10 78.35 79.26 75. 71.34 78.75 79.38 76.31 76.40 76. 75.64 71.54 71.21 70.40 69.01 78. 79.24 79.32 79.40 79.12 79.10 78. 78.47 To validate our scheduling strategy, we conduct experiments by varying (see detailed results in Appendix E.8). We observe that when roughly setting φ as constant in each phase, performance degrades as training progresses. This suggests that fixed φ strategy cannot adapt to the changing landscape of multi-phase scheduling. Since the optimization difficulty should decrease during training process, we drop φ as phase increases. By finely evaluate φ in each phase, it achieves performance improvement, confirming the need for dynamic control. It makes sense since multi-phase captures the dynamic features of the loss surface more finely, it needs careful selection for φ, where φ reflects optimization difficulty. These results highlight fundamental trade-off: when φ values per phase for multi-phase scheduling can be finely evaluated, multi-phase scheduling better captures the loss landscapes non-stationary behavior than single-phase scheduling. However, selecting optimal φ 40 (a) (b) Figure 11: Visualization of cross-φ performance. values per phase for multi-phase remains non-trivial, which motivates future work on automated landscape-aware φ tuning. Table 11: Performance on multi-phase version UBA. Dataset Network CIFAR100 ResNet34 φ 5 5 0.8k Original 76.68 Multi-phase K= K=6 K=8 76.54 75.37 75.05 76. 76.60 76."
        },
        {
            "title": "F Related work",
            "content": "Budgeted training Researchers face significant challenges in achieving optimal model performance under fixed hardware and limited time. To address this, budgeted training has emerged as broad research domain focusing on optimizing performance while minimizing resource usage. This research spans multiple aspects, including computation efficiency, model compression, training stability, convergence improvement, and optimization [40]. By strategically integrating efficient training approaches, budgeted training advances cost-effective model development, offering pathways to achieve high performance under realistic constraints. Budgeted training can be divided from three main perspectives: data, model, and optimization. From the data perspective, budgeted training emphasizes the allocation of resources between the dataset and the algorithms that process it, such as the balance between the model size and the amount of data [2, 6, 23, 27]. From the model perspective, budgeted training searches the optimal number of model parameters within the given compute budget [16, 25, 30, 37]. Beyond data and model perspective, optimization based methods guide budgeted training based on learning rate schedules [5, 29, 42, 43], batch size [16] and other weight averaging method [25, 26]. Among these three aspects, optimization-based methods are particularly aligned with the objectives of budgeted-iteration training. In this domain, learning rate scheduling based methods are particularly aligned with the objectives of budgeted-iteration training. Smith et al. [42] propose new learning rate schedule, named cyclical learning rates (CLR). It improves accuracy in fewer iterations without tuning and is relevant to super-convergence phenomenon [43]. Li et al. [29] introduce an alternative setting of existing learning rate schedules for budgeted training. Chen et al. [5] propose the reflected exponential schedule (REX) via profile and sampling fashion. Learning rate based approaches achieve robust and high-performing results under various lengths of training iterations, which corresponds to our purpose in this work, i.e. budgeted-iteration training. Besides, this approach is plug-and-play, requiring no substantial alterations to the underlying model structure, making it readily adaptable to various deep network frameworks. Therefore, we explore the proper learning rate schedule to achieve budgeted-iteration training. Learning rate schedule The learning rate plays pivotal role in controlling the non-convex optimization process during network training. Generally, the learning rate is gradually adjusted progressively alongside the iterations, which can be represented as η = g(t)η0, where g(t) is the schedule function which control the variation of learning rate, is the current iteration and η0 is the initial learning rate. Learning rate schedules have been extensively studied to improve training efficiency and model performance. The common scheme is the step decay schedule. Its schedule function can be represented as g(t) = dt/ts, where ts is the predefined decaying step and di(di di+1) is predefined decaying scalar. typical instance decreases the learning rate by decaying scalar 0.1 after 50% epochs and by decaying scalar 0.01 after 75% epochs [20]. Then Loshchilov et al. [32] observe that sharp decreases may prevent models from escaping local minima. Thus they propose cosine schedule function as 1 )), where ts is the predefined stage to restart the learning rate schedule, which has proven effective in transformer training. In addition, cosine schedule is commonly-used schedule nowadays. Pan et al. [35, 36] further introduce learning rate schedule called elastic step decay as ηt = η0/2k, if [(1 rk)T, (1 rk+1)T ) to accelerate the pre-training for Berts, where and are hyper-parameters that control learning rate interval. Hu et al. [24] propose Warmup-Stable-Decay(WSD) for large model pretraining and promoting continuous training. WSD consists of three consecutive phases: warmup, stable constant, and decay. Luo et al. [33] discover new schedule that outperforms the cosine schedule, resembling WSD but with lower final loss. 2 (1 + cos( tπ ts Adaptive learning rate Compared to schedule methods, adaptive learning rate based methods focus on the learning rate adaptation based on local loss landscape statistics. Typical methods includes AdaGrad [12], Adadelta [53], RMSprop [22] and Adam [28]. These methods have been shown stabilizing the training process while effectively optimizing learning rate. Then Loshchilov et al. [31] propose AdamW optimizer which improves the performance in the transformer training. Recently, Chen et al. [7] discovers Lion optimizer for more memory-efficiency. Xie et al. [51] propose the Adan optimizer which implements 50% faster training than Adam and AdamW on commonly-used networks. Despite these improvements, some studies suggest that adaptive methods may under42 perform momentum SGD in terms of generalization [29, 50], critical factor for budgeted-iteration training. Learning rate design is not only significant in general training, but also critical in budgeted-iteration training which still remains topic of debate. Some analyses advocate for small, constant learning rates to ensure stability and convergence [11]. On the contrast, one prevailing hypothesis suggests that large learning rates may facilitate crossing over sharp local minima in the optimization landscape [55]. Despite the lack of comprehensive theoretical explanations, range of learning rate schedules inspired by the above analyses as heuristic guidelines has been widely adopted in practice, using variable learning rates to budgeted-iteration training [5, 29]. In this work, we explore learning rate schedule from optimization problem tailored to budgeted-iteration training, aiming to balance iteration budget constraints and generalization."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Institute for Artificial Intelligence, Peking University",
        "Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China",
        "State Key Lab of General AI, School of Intelligence Science and Technology, Peking University"
    ]
}