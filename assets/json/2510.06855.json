{
    "paper_title": "Online Generic Event Boundary Detection",
    "authors": [
        "Hyungrok Jung",
        "Daneul Kim",
        "Seunggyun Lim",
        "Jeany Son",
        "Jonghyun Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. However, current GEBD methods require processing complete video frames to make predictions, unlike humans processing data online and in real-time. To bridge this gap, we introduce a new task, Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries of generic events immediately in streaming videos. This task faces unique challenges of identifying subtle, taxonomy-free event changes in real-time, without the access to future frames. To tackle these challenges, we propose a novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST) which explains how humans segment ongoing activity into events by leveraging the discrepancies between predicted and actual information. Our framework consists of two key components: the Consistent Event Anticipator (CEA), and the Online Boundary Discriminator (OBD). Specifically, the CEA generates a prediction of the future frame reflecting current event dynamics based solely on prior frames. Then, the OBD measures the prediction error and adaptively adjusts the threshold using statistical tests on past errors to capture diverse, subtle event transitions. Experimental results demonstrate that Estimator outperforms all baselines adapted from recent online video understanding models and achieves performance comparable to prior offline-GEBD methods on the Kinetics-GEBD and TAPOS datasets."
        },
        {
            "title": "Start",
            "content": "Hyungrok Jung1 Daneul Kim2 1GIST jhrock2001@gm.gist.ac.kr Seunggyun Lim1 2Seoul National University carpedkm@snu.ac.kr Jeany Son3 Jonghyun Choi2 3POSTECH sk000514@gm.gist.ac.kr jeany@postech.ac.kr jonghyunchoi@snu.ac.kr 5 2 0 2 8 ] . [ 1 5 5 8 6 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. However, current GEBD methods require processing complete video frames to make predictions, unlike humans processing data online and in real-time. To bridge this gap, we introduce new task, Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries of generic events immediately in streaming videos. This task faces unique challenges of identifying subtle, taxonomyfree event changes in real-time, without the access to future frames. To tackle these challenges, we propose novel On-GEBD framework, ESTimator, inspired by Event Segmentation Theory (EST) [50] which explains how humans segment ongoing activity into events by leveraging the discrepancies between predicted and actual information. Our framework consists of two key components: the Consistent Event Anticipator (CEA), and the Online Boundary Discriminator (OBD). Specifically, the CEA generates prediction of the future frame reflecting current event dynamics based solely on prior frames. Then, the OBD measures the prediction error and adaptively adjusts the threshold using statistical tests on past errors to capture diverse, subtle event transitions. Experimental results demonstrate that ESTimator outperforms all baselines adapted from recent online video understanding models and achieves performance comparable to prior offline-GEBD methods on the Kinetics-GEBD and TAPOS datasets. Figure 1. Comparison of offline-GEBD and human perception with illustration of Event Segmentation Theory (EST). (a) In conventional GEBD task, all the event boundaries are determined by utilizing all past and future frames. However, human segments event sequentially relying only on visuals available at the current moment. (b) The illustration of EST shows how humans perceive events. When we perceive visuals, we naturally expect continuous visuals to be recognized. When significant difference from the given visual input occurs, we perceive it as an event boundary. 1. Introduction In the era of diverse video content from platforms such as YouTube, TikTok and Netflix, the growing importance of processing long-form videos has spurred growth of interest in video understanding tasks. While recent research has Equal Contribution, Corresponding Authors Work done while at GIST. Daneul Kim is with CSE in SNU, and was previously with IPAI. Jonghyun Choi is with ECE, ASRI, and IPAI in SNU. actively investigated tasks such as action recognition and action detection [4, 16, 18, 28, 36, 51, 54], these studies typically focus on limited set of pre-defined action classes and short, trimmed video clips. Consequently, the field of long-form video understanding, which involves analyzing extended video content with complex narratives and diverse actions, remains largely unexplored. In this regard, Generic Event Boundary Detection (GEBD) [34] introduces new perspective to understand long-form videos. Originating from cognitive science, the 1 term event is an entity of which humans naturally segment continuous visual information, maintaining semantic continuity. As humans perceive broad and diverse visuals, events have the characteristic of being taxonomy-free with various levels of granularity. For example, as illustrated in Figure 1(b), pre-defined action like grooming can be partitioned into multiple sub-events, while frames that do not correspond to any pre-defined action label still be delineated as generic events. By aiming to detect changes between these events, GEBD attempts to analyze long-form videos from human-like perspective. However, the current GEBD task aims to identify multiple event boundaries at once, within fully given chunk of video, which differs significantly from how humans perceive events in an online manner (refer to Figure 1(a)). By relying on both past and future frames to determine event boundaries, GEBD does not fully reflect natural human perception, as humans process visual information from time to time without looking at the future. To address this limitation and to closely mimic human cognition, we propose new challenging task, Online Generic Event Boundary Detection (On-GEBD). In On-GEBD, the model must process streaming video and decide immediately whether each incoming frame is an event boundary, relying solely on past and present information without access to future frames. On-GEBD not only inherits the challenges from previous offline-GEBD but also faces more complex challenges due to the limited information (i.e. past-only information) when determining event boundaries. Prior offline-GEBD methods effectively address the challenges in detecting subtle semantic changes of different events by generating multi-level difference map among several frames [39] or creating the temporal similarity matrix for the entire video frame [17]. However, due to their reliance on complete sequence of video frames, these methods are not suitable for On-GEBD, where only past information is available to determine whether streamed frame is boundary or not. Also, since an online setting requires immediate determination of boundary as each frame is streamed, it is essential to devise method specially tailored for On-GEBD. To address these issues, we propose simple yet effective framework ESTimator inspired by Event Segmentation Theory (EST) in cognitive science. The EST states that humans continuously make predictions consistent with an ongoing event and detect changes when these predictions diverge from the actual information (refer to Figure 1(b)). We design Consistent Event Anticipator (CEA) module to reflect the essence of the EST. To make CEA predict consistent events robustly, we propose the two training objectives to derive the model to predict visual information consistent with the current ongoing event. ESTimator detects event boundaries by assessing the discrepancy between the visual information predicted by CEA and the actual information. Moreover, determining boundaries based on fixed threshold presents challenges in distinguishing events that are not constrained by particular taxonomy and have varying degrees of granularity. Therefore, we propose an Online Boundary Discriminator (OBD) module that determines boundaries by comparing the distribution of visual discrepancies from the surrounding frames. OBD stores historical discrepancies in fixed-size queue and conducts statistical testing to provide dynamic threshold that reflects the surrounding context. Our method successfully tackles the unique challenges of On-GEBD and shows its effectiveness on two GEBD benchmark datasets, Kinetics-GEBD [34] and TAPOS [32]. We show that our model not only outperforms baseline methods that utilize existing online video understanding methods [2, 41, 44, 54], but also achieves comparable results to prior offline methods that are tested under the original GEBD setting. We summarize our contribution as follows: We present new challenging task, On-GEBD, designed to align closer to the actual human perception. To address the unique challenges posed by On-GEBD, we propose novel framework, ESTimator, comprising with Consistent Event Anticipator (CEA) and Online Boundary Discriminator (OBD). Our model outperforms various baselines based on traditional online video models and achieves performance on par with offline setting [34, 45]. 2. Related Work Generic Event Boundary Detection. Generic Event Boundary Detection (GEBD), introduced in [34], aims to detect event boundaries aligning with human perception. Unlike traditional video understanding tasks (e.g., TAL [25, 51, 53], Action Recognition [15, 42]), GEBD deals with continuous semantics without taxonomy, enabling the understanding of complex video. Recent GEBD works aim to detect boundaries by processing entire videos or analyzing surrounding frame context. For instance, UBoCo [17] uses temporal similarity matrices and contrastive learning, while DDM-Net [39] employs progressive attention on multi-level dense difference maps. CoSeg [45], inspired by cognitive modeling [50], introduces boundary detection through event reconstruction. While offline-GEBD methods [12, 17, 23, 24, 31, 38, 39, 45, 52, 55, 56] have shown improvements, our work adapts GEBD to an online setting, addressing the challenge of making instant decisions without future frames. Video Understanding. Video understanding encompasses various tasks, including Temporal Action Detection (TAD) [27, 33, 47], Temporal Action Localization (TAL) [25, 51, 53], and Video Instance Segmenta2 Figure 2. Overview of our ESTimator framework. Our framework consists of three major components: Consistent Event Anticipator (CEA) which generates consistent future frame feature using learnable token (Left). EST-inspired training objective that accumulates frame-level (EST loss) and region-level (REST loss) prediction errors derived from the discrepancy between the generated future frame from CEA with the actual input frame (Upper right). Online Boundary Discriminator (OBD) with queue that stores past error prediction values to conduct statistical testing on the error derived from the current input frame for inference (Lower right). tion (VIS) [20, 49]. Recent advancements utilize transformers [51], memory structures [5], and end-to-end approaches [25]. However, these tasks often rely on predefined classes, limiting real-world applicability. While open-set tasks in TAL [3] have been proposed, they still focus on class-based localization. GEBD [34] addresses this limitation by dealing with taxonomy-free events. Online Video Understanding. Online video understanding focuses on streaming content, sharing commonalities with On-GEBD. Tasks include Online Action Detection (OAD) [7], Online Temporal Action Localization (On-TAL) [16], and Online Detection of Action Start (ODAS) [35]. Recent works like LSTR [48], TeSTra [54], and MiniROAD [2] have demonstrated the effectiveness of transformers and GRUs in OAD. For On-TAL, methods like CAG-QIL [16] and OAT [21] have been proposed. While ODAS may seem similar to On-GEBD, it differs significantly as it deals with fixed action classes. On-GEBD focuses on detecting subtle changes among taxonomy-free events, requiring distinct approach from conventional online video understanding methods. 3. Online Generic Event Boundary Detection In this section, we present new task of Online Generic Event Boundary Detection (On-GEBD), focusing on the identification of event boundaries in streaming videos, which is distinct from the traditional offline-GEBD task. The offline-GEBD [34] considers video consisting of frames, = {vt}N t=1, segmented into multiple events with distinct generic event boundaries denoted as = {bj}M j=1, where bj represents the timestamp of the j-th event boundary. However, unlike the human perception process, where incoming visual information is segmented into events instantaneously, the offline-GEBD task allows to utilize all frames in to determine boundaries bj. The On-GEBD task imposes two key constraints on the conventional offline-GEBD: (1) the video is streamed sequentially frame-by-frame; (2) the model must make immediate decisions for each incoming frame on whether vt is an event boundary as soon as the frame is received. These constraints limit the model to use only the past and current frames (i.e., v1 vt) for decision-making, without access to the future context of vt+1 vN . This online, constrained setting makes the model to closely resemble the real-time, causal nature of human event perception. By virtue of its online setting, On-GEBD presents novel and more demanding set of challenges than the previous offline-GEBD task. The absence of future context exacerbates the issues inherited from offline-GEBD in detecting diverse, generic event boundaries. Since the model is compelled to make instant decisions based on limited information, it must balance rapid boundary detection with the risk of false positives. Consequently, On-GEBD necessitates the development of novel algorithms that robustly detect the event transitions in streaming visual data with lowlatency, which closely aligns with human perception. 4. Method Our framework, ESTimator, addresses the unique challenges of On-GEBD by drawing inspiration from the Event 3 Segmentation Theory (EST), which explains human event perception [30]. ESTimator incorporates two key principles from the EST: (1) humans continuously anticipate consistent information about ongoing events; (2) perceive event changes when there is significant discrepancy between anticipated and actual information (refer to Figure 1(b)). These principles offer an effective approach to overcome the key challenges of On-GEBD. The continuous anticipation of event information allows the model to make decisions, even without access to future frames. Moreover, by focusing on the trends in discrepancies between anticipated and actual information, the model can effectively detect subtle and diverse event boundaries without relying on pre-defined taxonomies, thereby addressing the ambiguous and varied nature of generic events in streaming video. Regarding the principles, our framework consists of two components: Consistent Event Anticipator (CEA) and the Online Boundary Discriminator (OBD). CEA is trained with two novel training objectives to anticipate consistent visual information of the ongoing event robustly. Since the consistent information anticipated from CEA diverges from actual information at event boundaries, we leverage this discrepancy as reliable cue for event boundary detection in online scenario. For clarity, we define the term error to denote the discrepancy between the actual and anticipated visual information in the following section. Also, OBD incorporates prior errors to effectively detect diverse forms of event boundaries, enabling more refined and precise detection based on prior semantic changes. In the following section, we explore each component in detail. 4.1. Consistent Event Anticipator Transformers have proven their effectiveness in handling sequential data for video understanding tasks, particularly in predicting future states based on past and present information [9, 10, 37, 40, 46]. Especially, transformer decoders endowed with auto-regressive causal masking excel at nextinformation prediction, as exemplified by Large Language Models (LLMs). In this context, we utilize the transformer decoder [43] in Consistent Event Anticipator (CEA) to construct model that anticipates consistent frame information aligned with ongoing events. Transformer Decoder. Following previous GEBD work [17], we first extract frame features, ft RD, using pre-trained ResNet-50 image encoder [13]. We concatenate single learnable token RD with extracted frame features Ft = {fi}t1 i=tL and forward them into transformer decoder layers Mθ to predict streamed frame features. Xt = concat(Ft, T), ˆft = Mθ(Xt), (1) 4 In Eq. 1, ˆft denotes the output of the learnable token after processing through Mθ, which encapsulates prediction for the upcoming frame feature (refer Figure 2). With the causal-attention mask in the transformer decoder, we ensure that succeeding tokens only attend to the preceding ones. Objective Functions. The main objective of CEA is to maximize errors at semantically inconsistent event boundaries while minimizing errors within consistent event segments, thereby embodying the core principles of EST. We preliminarily define the error εt between the prediction ˆft and the actual frame feature ft with the cosine distance, and scale it between 0 and 1 as follows: (cid:32) (cid:33) εt = distcos(ft, ˆft) = 1 . (2) 1 2 ft ˆft ftˆft To achieve the objective of CEA following EST, we use binary cross-entropy loss for our EST loss, as follows: LEST (εt, t) = yt log εt (1 yt) log(1 εt), (3) where yt = 1 if B, otherwise 0. The EST loss encourages Mθ to maximize the errors at event boundaries while minimizing them elsewhere, effectively distinguishing boundary frames from non-boundary frames. However, strict frame-wise binary supervision would be sub-optimal in videos where consecutive frames contain continuous semantic flow. This is because consecutive frames are smoothly connected without abrupt changes, except shot changes. To address this issue, we propose region-level training scheme that considers temporal context flow. Since this training scheme shares the mechanism of the EST loss, we named it REST loss (Region EST loss). Incorporating errors derived from nearby regions, REST loss aims to give soft supervision of the abrupt label transitions in the near future. Assuming the size of region as K, we collect the series of errors εtK, . . . , εt from the inputs XtK, . . . , Xt, and compute the average of these consecutive past errors εt as follows: εt ="
        },
        {
            "title": "1\nK",
            "content": "t (cid:88) εi. i=tK The REST loss is then defined as follows: LREST (εt, t) = LEST (εt, t), (4) (5) where the boundary label at is used for the loss. The REST loss allows the predicted frame features, ˆftK . . . ˆft, to be softly trained with additional future information, making them less sensitive to noise and more effective in anticipating future events."
        },
        {
            "title": "Our final loss function is computed as a weighted sum of",
            "content": "two losses, EST and REST losses, as follows: L(εt, t) = α LREST (εt, t) + (cid:88) i=tK LEST (εi, i). (6) In our experiments, we set the hyperparameter α to 0.5. Batch-wise Loss Weighting. In the video, frames that correspond to boundaries account for significantly smaller proportion compared to non-boundary frames. Previous offline-GEBD studies utilize balanced samplers [34, 39] or weighted loss terms [14] with manually tuned values to alleviate issues arising from such imbalanced data distribution. To achieve similar effect during training, we utilize batch-wise loss weighting technique. We first calculate the ratio of boundary and non-boundary targets within single batch. This ratio is then multiplied by the loss calculated for boundary targets, allowing the batch-wise loss weighting to balance the impact of boundary and non-boundary samples during training. This approach not only eliminates the need for manually tuned scaling values but also aims to achieve similar effect to that of using batch sampler. 4.2. Online Boundary Discriminator Conventional offline-GEBD methods typically rely on either static thresholds [14, 34, 39] or dynamic criteria based on peak detection [45] to identify boundaries by analyzing the entire sequence of framesincluding future ones. However, these approaches are not well-suited to the OnGEBD: static thresholds fail to capture diverse form of semantic changes, and peak detection is impractical due to its reliance on future frame information. These limitations highlight the need for novel approach tailored to the immediacy and dynamic nature of On-GEBD. Figure 3. An illustration of how Online Boundary Detector (OBD) applies dynamic threshold to capture diverse event transitions. To address these unique challenges, propose we an Online Boundary Dis- (OBD), which criminator dynamically discriminates event boundaries by leveraging recent semantic flow. First, OBD stores errors up to the current frame in fixedsize memory queue, denoted as Q, which is managed in First-In, First-Out (FIFO) manner. When the t-th frame vt is streamed, contains past errors εt, . . . , εt1, where is the size of the Q. Using the errors stored in memory queue, OBD conducts statistical testing with the Gaussian distribution derived from errors in the Q. By normalizing the incoming error εt with statistical information derived from Q, the OBD determines boundary if the incoming error is identified as an outlier compared to the close region errors. Figure 3 exemplifies how the underlying mechanism of OBD dynamically modulates the detection threshold. ζt = εt µQ σQ , OBD(Q, εt) = 1[ζt > τ ], (7) In Eq. 7, τ is threshold value for an indicator function 1[] and ζt is the normalized error at frame by the mean µQ and the standard deviation σQ derived from errors stored in (refer lower right side of the Figure 2). In our experiments, we set τ to 1.5 empirically. By dynamically adjusting the error threshold through OBD, our ESTimator successfully identifies wide range of event boundaries, as demonstrated in Figure 4. 5. Experiment 5.1. Setup Benchmark Dataset. The Kinetics-GEBD dataset [34] is composed of approximately 60K videos selected from the Kinetics-400 dataset [19]. The selected videos are divided into units of taxonomy-free events by specially trained annotators. On average, each video contains about 5 different events. The train, validation and test videos are almost equally distributed with 18,794, 18,813 and 17,725 videos, respectively. Since annotations for the test set are not available, we report the results evaluated on the validation set, following prior works [17, 34, 39]. For cross-validation, we randomly partitioned the dataset into training (80%) and validation (20%) subsets for the experiments. TAPOS dataset [32] is composed of Olympic sports videos that are annotated with 21 different actions with 13,094 training action instances and 1,790 validation action instances. Following [34], we re-purpose TAPOS for the GEBD task by obscuring the action labels of sub-actions. Additionally, we present results for INRIA [1] in supplementary material. Evaluation Metric. Relative Distance (Rel.Dis) is metric that measures the relative difference between the detected boundary timestamps and the ground truth boundary timestamps in GEBD. We calculate the metric by dividing the distance between predictions and ground-truths by the union of predicted and ground-truth event instances. When the model predicts consecutive frames as boundaries, as discussed in [34], we set the center of these frames as the predicted boundary timestamps. Relative distance (Rel.Dis) of detected timestamps is evaluated under 10 thresholds with intervals of 0.05, ranging from 0.05 to 0.5. We consider the result to be positive if it is below each threshold. Following the prior works, we report the F1 scores on each threshold, as well as their average in our main experiments. Baseline. Directly extending the offline-GEBD methods into an online setting poses challenges as they are not designed to process streaming videos. Therefore, we utilize state-of-the-art online video understanding models, which are natively designed for action detection or localization, as our baseline for On-GEBD. Specifically, we utilize TeSTra [54], OadTR [44], and MiniROAD [2] from Online Action Detection, and Sim-On [41] from Online Temporal Action Localization as our naıve baselines. Only the head of each model is modified to perform binary classification 5 Table 1. Quantitative comparison with other online baselines in On-GEBD task. BC denotes binary classifier attached to the last layer of each model to solve On-GEBD. We denoted with bold for the highest F1 score and second with underline."
        },
        {
            "title": "Dataset",
            "content": "Rel. Dis. threshold 0.05 0.1 0.15 0.2 0. 0.3 0.35 0.4 0.45 0."
        },
        {
            "title": "Avg",
            "content": "Kinetics-GEBD TeSTra - BC [54] Sim-On - BC [41] OadTR - BC [44] MiniROAD - BC [2] 0.438 0.461 0.474 0.569 0.488 0.534 0.512 0.622 0.521 0.579 0.535 0.649 0.545 0.610 0.552 0. 0.564 0.633 0.565 0.691 0.580 0.651 0.575 0.704 0.593 0.664 0.583 0.714 0.604 0.675 0.590 0.722 0.614 0.685 0.596 0.729 0.622 0.692 0.601 0. 0.557 0.618 0.558 0.681 ESTimator (Ours) 0.620 0.687 0.724 0. 0.762 0.774 0.782 0.789 0.795 0. 0."
        },
        {
            "title": "TAPOS",
            "content": "TeSTra - BC [54] Sim-On - BC [41] Oad-TR - BC [44] MiniROAD - BC [2] 0.364 0.225 0.263 0.422 0.417 0.269 0.319 0.472 0.452 0.303 0.361 0.502 0.478 0.329 0.394 0.522 0.496 0.350 0.422 0. 0.511 0.367 0.445 0.549 0.523 0.381 0.465 0.558 0.533 0.394 0.483 0.566 0.542 0.405 0.497 0.572 0.550 0.415 0.510 0.578 0.487 0.344 0.416 0. ESTimator (Ours) 0.394 0.455 0.499 0.532 0. 0.578 0.594 0.608 0.619 0.629 0. Table 2. Quantitative comparison with offline methods. Note that we report the performance of the models in offline setting from their original literature. Also, we indicate the highest F1 score with bold, second with underline and third with . Dataset Setting Supervision Rel. Dis. threshold G - t K P Offline Supervised Unsupervised BMN [26] BMN-StartEnd [34] TCN-TAPOS [34] TCN [22] PC [34] SceneDetect [34] PA-Random [34] PA [34] CoSeg [45] Online Supervised ESTimator (Ours) Supervised Offline Unsupervised ISBA [34] TCN [34] CTM [34] TransParser [22] PC [34] SceneDetect [34] PA-Random [34] PA [34] Online Supervised ESTimator (Ours) 0. 0.186 0.491 0.464 0.588 0.625 0.275 0.336 0.396 0.656 0.620 0.106 0.237 0.244 0.289 0.522 0.035 0.158 0.360 0.394 0.1 0.204 0.589 0.560 0.657 0. 0.300 0.435 0.488 0.758 0.687 0.170 0.312 0.312 0.381 0.595 0.045 0.233 0.459 0.455 0.15 0.213 0.627 0.602 0.679 0.804 0.312 0.484 0.520 0.783 0.724 0.227 0.331 0.336 0.435 0.628 0.047 0.273 0.507 0.499 0.2 0.220 0.648 0.628 0.691 0.829 0.319 0.512 0.534 0.794 0.746 0.265 0.339 0.351 0.475 0. 0.051 0.310 0.543 0.532 0.25 0.226 0.660 0.645 0.698 0.844 0.324 0.529 0.544 0.799 0.762 0.298 0.342 0.361 0.500 0.659 0.053 0.331 0.567 0.558 0.3 0.230 0.668 0.659 0.703 0.853 0.327 0.541 0.550 0.803 0.774 0.326 0.344 0.369 0.514 0.665 0.054 0.347 0.579 0.578 0. 0.233 0.674 0.669 0.706 0.859 0.330 0.548 0.555 0.804 0.782 0.348 0.347 0.374 0.527 0.671 0.055 0.357 0.592 0.4 0.237 0.678 0.676 0.708 0. 0.332 0.554 0.558 0.806 0.789 0.348 0.348 0.381 0.534 0.676 0.056 0.369 0.601 0.45 0.239 0.681 0.682 0.710 0.867 0.334 0.558 0.561 0.807 0.795 0.348 0.348 0.383 0.540 0.679 0.057 0.376 0.609 0.5 0.241 0.683 0.687 0.712 0.870 0.335 0.561 0.564 0.809 0.799 0.348 0.348 0.385 0.545 0. 0.058 0.384 0.615 Avg 0.223 0.640 0.627 0.685 0.817 0.318 0.506 0.527 0.782 0.748 0.330 0.330 0.350 0.474 0.642 0.051 0.314 0.543 0.594 0.608 0.619 0.629 0.547 (BC), as it is necessary for the model to distinguish whether streamed input is boundary or not. Implementation Details. We preprocess the video data by sampling at 24 FPS for the Kinetics-GEBD dataset and 6 FPS for the TAPOS dataset. Following [17], we use the features extracted from the ResNet-50 encoder, which is pre-trained on ImageNet [8] with feature dimension = 2,048. For our experiments, we employ 3 transformer decoder layers in both datasets. Additionally, we train our model with batch size of 512 using the AdamW [29] optimizer in training with learning rate of 1e-4. 5.2. Main Result Table 1 shows that our framework outperforms baseline models for On-GEBD on both Kinetics-GEBD and TAPOS datasets. Traditional online methods like TeSTra-BC, OadTR-BC, and Sim-On-BC show lack of model capacity in learning to discriminate generic event boundaries, limited by their model design to learn pre-defined action classes. While MiniROAD-BC demonstrates higher performance than other baselines, our approach still surpasses them on both datasets. The results highlight that simply adapting existing approaches is insufficient; dedicated method like ESTimator is necessary to detect subtle semantic changes during the event transition. In Table 2, we compare our framework with models evaluated in an offline setting. ESTimator performs on par with or exceeds most offline methods, achieving higher Avg. F1 scores on Kinetics-GEBDexcept for PC [34] and CoSeg [45]. Similarly, on the TAPOS, ESTimator outperforms all baselines from the original GEBD, with the sole exception of the PC method [34]. Table 3. Benefit of the proposed components. As baseline, we utilize the transformer decoder with binary classifier. We gradually add each of the proposed components to investigate its effect on improving performance."
        },
        {
            "title": "Method",
            "content": "Baseline 1 2 1+ 2 1+ 3 2+ 3 1+ 2+ 3 (Ours) 1 EST 2 REST 3 OBD F1 @ 0.05 Avg 0.483 0.571 0.504 0.544 0.604 0.621 0.620 0.607 0.698 0.654 0.691 0.659 0.692 0.748 Table 4. Comparison on different metric for calculating the error. We compare Avg. F1 scores using different distance metrics, with min-max normalization applied to each batch during training. Cosine distance is the best choice due to its bounded nature."
        },
        {
            "title": "Error Calculation Metric",
            "content": "L1 Distance (min/max normalized) L2 Distance (min/max normalized) KL Divergence (min/max normalized) Cosine Distance (Ours) Avg F1 0.733 0.733 0.734 0.748 5.3. Ablation Study Impact of Proposed Components. To observe the benefits of each proposed component, we build our components on top of the simple transformer model with binary classifier, which we refer to as Baseline in Table 3. The experimental results present the effectiveness of error-based detection of event boundaries, as the model trained with either EST or REST loss consistently outperforms the baseline. Naıvely applying the REST loss on top of the EST loss degrades the models performance, as using both objectives together tends to reduce errors on boundary frames. Furthermore, the OBD proved to be less effective when applied to CEA trained exclusively with either EST or REST loss. The performance gap between our full proposed framework and its ablated versions demonstrates the synergistic effect of our individual components. Metric for Error Calculation. We investigate the impact of the metric used for error computation on the performance of our framework. Unlike the cosine distance, which is bounded, other widespread metrics such as the L1/L2 distance and the KL Divergence are generally unbounded. Therefore, to facilitate the application of our proposed EST and REST losses, we experiment these metrics with minmax normalization per batch while training the CEA. As shown in Table 4, the cosine distance demonstrates superior performance compared to the L1/L2 distance and the KL divergence. Furthermore, we note that these alternative metrics remain relatively viable due to our proposed OBD. OBD provides reliable criteria even for error values with Table 5. Comparison of real-time performance (in FPS) with other baselines, which utilize other online video understanding models. We denoted with bold for the highest FPS and performance, and underlined for the second highest. ESTimator shows the highest Avg. F1 with compatible overall FPS compared to MiniROAD-BC [2]. All experiments were conducted on single NVIDIA RTX A6000 GPU."
        },
        {
            "title": "Overall",
            "content": "Avg F1 TeSTra - BC Sim-On - BC OadTR - BC MiniROAD - BC ESTimator (Ours) 181 177 275 100 3069 481 72.5 76.3 48.9 99.8 96.3 0.557 0.618 0.558 0.681 0. Table 6. Video feature comparison. We compare the performance of our method and baselines using video features extracted from TSN networks pre-trained on Something Something v2 (SSv2) and Kinetics datasets."
        },
        {
            "title": "Backbone",
            "content": "Avg F"
        },
        {
            "title": "Backbone",
            "content": "Avg F1 TeSTra - BC Sim-On - BC OadTR - BC MiniROAD - BC ESTimator (Ours) TSN (SS-v2) 0.653 0.524 0.700 0.684 0.741 TSN (Kinetics) 0.654 0.501 0.699 0.689 0. Table 7. Comparison on outlier handling in OBD. Removing outliers from the queue significantly reduces performance, demonstrating our OBD functions as dynamic threshold adaptation."
        },
        {
            "title": "OBD Setting",
            "content": "Avg F1 Using Only Inliers Full OBD (Ours) 0.663 (-11.4%) 0.748 unbounded ranges, thereby mitigating the challenges of selecting an appropriate threshold. Real Time Performance. We report runtime analysis of our method with other baselines in Table 5 and demonstrate the feasibility of our approach for real-time processing. For all methods, we utilize the ResNet-50 encoder pretrained on ImageNet [8], which operates at 181 FPS in our experimental setting. ESTimator not only demonstrates superior performance, but also achieves higher FPS compared to other transformer-based baselines (i.e., TeSTra-BC, SimOn-BC, OadTR-BC). Even with superior performance, its overall FPS is on par with that of MiniROAD-BC, which is based on GRU [6] architecture. We further demonstrate the analysis on computation cost in supplementary material. Utilizing Video Feature. To ensure fair comparison, we utilize ResNet-50 as the feature extractor, in accordance with previous offline-GEBD studies. However, since the baseline models were originally developed with different feature extractors and may underperform with ResNet7 Figure 4. Qualitative result. Comparison between our proposed framework and baseline from other online video understanding task. Note that baseline here refers to TeSTra [54] with binary classifier head attached for event boundary detection. features. Table 6 presents the results using the TSN network as backbone, which was originally used by these baselines. Despite utilizing different backbone architectures, our framework still outperforms all other baselines, thereby proving its robust effectiveness. Error Handling in OBD. key challenge in On-GEBD is balancing sensitivity to upcoming boundaries while preventing false alarms. OBD addresses this challenge by storing past errors to adjust its sensitivity to boundaries. However, the persistence of elevated error values from previous event boundary within the queue may lead to interference, hampering the detection of subsequent event boundaries. To investigate this potential issue, we conducted an ablation study in which error values identified as event boundaries were excluded from the OBD queue. As shown in Table 7, excluding outliers severely degraded performance, reducing the Avg F1 from 0.748 to 0.663 (11.4% relative drop). This finding denotes that including errors from event boundaries is an essential mechanism for robust event boundary detection in OBD while ensuring computational efficiency. We believe that one factor contributing to the effectiveness of this mechanism is its cognitive plausibility, as human perception adopts adaptive criteria for event segmentation when exposed to rapid changes [11]. 5.4. Qualitative Result In Figure 4, we present qualitative comparisons with the In contrast to the baseline, which TeSTra-BC baseline. shows noisy predictions as in all of the examples in Figure 4, ESTimator detects boundaries more accurately that align closely with the ground truth. Specifically, in the first example (upper panel), there are 5 ground-truth boundaries; ESTimator identifies spikes at 4 of these boundaries within the gray-shaded ground-truth area, while TeSTra-BC detects only 3 boundariesone of which falls inside an event segment rather than on an actual boundary. In the second example, which contains 7 ground-truth regions, TeSTraBC successfully detects only 2 boundaries, whereas our method predicts 8 boundaries, 7 of which are correct. 6. Conclusion We introduce challenging new task, On-GEBD, designed to bring GEBD closer to human perceptual processes. To address this task, we present ESTimator, inspired by Event Segmentation Theory (EST) from cognitive science, which explains how humans perceive and segment events. Our Consistent Event Anticipator (CEA) is trained using two lossesEST loss and REST lossto effectively predict future frames consistent with current events, thereby maximizing errors at event boundaries. Additionally, the Online Boundary Discriminator (OBD) employs dynamic criteria to distinguish boundaries based on error values produced by the CEA. Our work demonstrates superior performance compared to baselines adapted from other online video understanding tasks and shows almost comparable performance to some recent work in the offline-GEBD task. 8 Acknowledgments. This work was supported by the IITP grants (RS-2019-II191842, RS-2021-II212068, RS2022II220926 (30%), RS-2022-II220077, RS-2022-II220113, RS-2022-II220959, RS-202502263598 (10%), RS-2021-II211343 (SNU AI), RS-2021II212068 (AI Innovation Hub), RS-2025-25442338 (AI Star Fellowship-SNU)) funded by MSIT, and the GIST-MIT Research Collaboration grant funded by GIST (10%), Korea. RS-2022-II220871,"
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Learning from narrated instruction videos. In CVPR, 2016. 5, 3 [2] Joungbin An, Hyolim Kang, Su Ho Han, Ming-Hsuan Yang, and Seon Joo Kim. Miniroad: Minimal rnn framework for online action detection. In ICCV, pages 1034110350, 2023. 2, 3, 5, 6, 7 [3] Wentao Bao, Qi Yu, and Yu Kong. Opental: Towards open set temporal action localization. In CVPR, pages 29792989, 2022. 3 [4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, page 4, 2021. 1 [5] Feng Cheng and Gedas Bertasius. Tallformer: Temporal action localization with long-memory transformer. In ECCV, pages 503521. Springer, 2022. 3 [6] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. [7] Roeland De Geest, Efstratios Gavves, Amir Ghodrati, Zhenyang Li, Cees Snoek, and Tinne Tuytelaars. Online action detection. In ECCV, pages 269284, 2016. 3 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. 6, 7 [9] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In ICCV, pages 1350513515, 2021. 4 [10] Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha, and Minsu Cho. Future transformer for long-term action anticipation. In CVPR, pages 30523061, 2022. 4 [11] Urit Gordon, Shimon Marom, and Naama Brenner. Visual detection of time-varying signals: Opposing biases and their timescales. PLOS ONE, 14(11):e0224256, 2019. 8, 2 [12] Sourabh Vasant Gothe, Vibhav Agarwal, Sourav Ghosh, Jayesh Rajkumar Vachhani, Pranay Kashyap, and Barath Raj Kandur Raja. Whats in the flow? exploiting temporal motion cues for unsupervised generic event boundary detection. In WACV, pages 69416950, 2024. 2, [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. pages 770778, 2016. 4 event boundary detection cvpr2022 kinetics-gebd challenge. arXiv preprint arXiv:2206.08610, 2022. 5 [15] Ziyuan Huang, Shiwei Zhang, Liang Pan, Zhiwu Qing, Mingqian Tang, Ziwei Liu, and Marcelo Ang Jr. Tada! temporally-adaptive convolutions for video understanding. In ICLR, 2022. 2 [16] Hyolim Kang, Kyungmin Kim, Yumin Ko, and Seon Joo Kim. Cag-qil: Context-aware actionness grouping via imitation learning for online temporal action localization. In ICCV, pages 1372913738, 2021. 1, 3 [17] Hyolim Kang, Jinwoo Kim, Taehyun Kim, and Seon Joo Kim. Uboco: Unsupervised boundary contrastive learning for generic event boundary detection. In CVPR, pages 2007320082, 2022. 2, 4, 5, 6, [18] Hyolim Kang, Hanjung Kim, Joungbin An, Minsu Cho, and Seon Joo Kim. Soft-landing strategy for alleviating the task discrepancy problem in temporal action localization tasks. In CVPR, pages 65146523, 2023. 1 [19] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 5 [20] Lei Ke, Martin Danelljan, Henghui Ding, Yu-Wing Tai, ChiKeung Tang, and Fisher Yu. Mask-free video instance segmentation. In CVPR, pages 2285722866, 2023. 3 [21] Young Hwi Kim, Hyolim Kang, and Seon Joo Kim. sliding window scheme for online temporal action localization. In ECCV, pages 653669. Springer, 2022. 3 [22] Colin Lea, Austin Reiter, Rene Vidal, and Gregory Hager. Segmental spatiotemporal cnns for fine-grained action segmentation. In ECCV, pages 3652. Springer, 2016. 6, 2 [23] Congcong Li, Xinyao Wang, Dexiang Hong, Yufei Wang, Libo Zhang, Tiejian Luo, and Longyin Wen. Structured context transformer for generic event boundary detection. arXiv preprint arXiv:2206.02985, 2022. 2, 3 [24] Congcong Li, Xinyao Wang, Longyin Wen, Dexiang Hong, Tiejian Luo, and Libo Zhang. End-to-end compressed video representation learning for generic event boundary detection. In CVPR, pages 1396713976, 2022. [25] Chuming Lin, Chengming Xu, Donghao Luo, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Learning salient boundary feature for anchor-free In CVPR, pages 33203329, temporal action localization. 2021. 2, 3 [26] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. Bmn: Boundary-matching network for temporal action proposal generation. In ICCV, pages 38893898, 2019. 6, 2 [27] Xiaolong Liu, Song Bai, and Xiang Bai. An empirical study In CVPR, pages of end-to-end temporal action detection. 2001020019, 2022. 2 [28] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In CVPR, pages 32023211, 2022. 1 [14] Rui He, Yuanxi Sun, Youzeng Li, Zuwei Huang, Feng Hu, Xu Cheng, and Jie Tang. Masked autoencoders for generic [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 9 [45] Xiao Wang, Jingen Liu, Tao Mei, and Jiebo Luo. Coseg: Cognitively inspired unsupervised generic event segmentation. IEEE Trans. Neural Netw. Learn. Syst., 2023. 2, 5, 6, 3 [46] Junfeng Wu, Yi Jiang, Song Bai, Wenqing Zhang, and Xiang Bai. Seqformer: Sequential transformer for video instance segmentation. In ECCV, pages 553569. Springer, 2022. 4 [47] Mengmeng Xu, Chen Zhao, David Rojas, Ali Thabet, and Bernard Ghanem. G-tad: Sub-graph localization for tempoIn CVPR, pages 1015610165, 2020. ral action detection. 2 [48] Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu Li, Wei Xia, Zhuowen Tu, and Stefano Soatto. Long short-term transformer for online action detection. NeurIPS, 34:10861099, 2021. [49] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In ICCV, pages 51885197, 2019. 3 [50] Jeffrey Zacks and Khena Swallow. Event segmentation. Curr. Dir. Psychol. Sci., 16(2):8084, 2007. 1, 2 [51] Chen-Lin Zhang, Jianxin Wu, and Yin Li. Actionformer: Localizing moments of actions with transformers. In ECCV, pages 492510. Springer, 2022. 1, 2, 3 [52] Libo Zhang, Xin Gu, Congcong Li, Tiejian Luo, and Heng Fan. Local compressed video stream learning for generic event boundary detection. IJCV, 132(4):11871204, 2024. 2, [53] Chen Zhao, Shuming Liu, Karttikeya Mangalam, and Bernard Ghanem. Re2tal: Rewiring pretrained video backbones for reversible temporal action localization. In CVPR, pages 1063710647, 2023. 2 [54] Yue Zhao and Philipp Krahenbuhl. Real-time online video detection with temporal smoothing transformers. In ECCV, pages 485502. Springer, 2022. 1, 2, 3, 5, 6, 8 [55] Ziwei Zheng, Lijun He, Le Yang, and Fan Li. Fine-grained dynamic network for generic event boundary detection. In ECCV, pages 107123. Springer, 2024. 2, 3 [56] Ziwei Zheng, Zechuan Zhang, Yulin Wang, Shiji Song, Gao Huang, and Le Yang. Rethinking the architecture design for efficient generic event boundary detection. In ACMMM, pages 12151224, 2024. 2, 3 [30] Gabriel Radvansky and Jeffrey Zacks. Event perception. Wiley Interdiscip. Rev. Cogn. Sci., 2(6):608620, 2011. 4 [31] Ayush Rai, Tarun Krishna, Julia Dietlmeier, Kevin McGuinness, Alan Smeaton, and Noel OConnor. Motion aware self-supervision for generic event boundary detection. In WACV, pages 27282739, 2023. 2 [32] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. inter-action understanding via temporal action parsing. CVPR, pages 730739, 2020. 2, 5, 3 Intra-and In [33] Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, and Dacheng Tao. Tridet: Temporal action detection with relative boundary modeling. In CVPR, pages 1885718866, 2023. 2 [34] Mike Zheng Shou, Stan Weixian Lei, Weiyao Wang, Deepti Ghadiyaram, and Matt Feiszli. Generic event boundary deIn ICCV, tection: benchmark for event segmentation. pages 80758084, 2021. 1, 2, 3, 5, 6 [35] Zheng Shou, Junting Pan, Jonathan Chan, Kazuyuki Miyazawa, Hassan Mansour, Anthony Vetro, Xavier Giro-i Nieto, and Shih-Fu Chang. Online detection of action start in untrimmed, streaming videos. In ECCV, pages 534551, 2018. [36] Haisheng Su, Weihao Gan, Wei Wu, Yu Qiao, and Junjie Yan. Bsn++: Complementary boundary regressor with scalebalanced relation modeling for temporal action proposal generation. In AAAI, pages 26022610, 2021. 1 [37] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In CIKM, pages 14411450, 2019. 4 [38] Jing Tan, Yuhong Wang, Gangshan Wu, and Limin Wang. Temporal perceiver: general architecture for arbitrary IEEE Transactions on Pattern Analyboundary detection. sis and Machine Intelligence, 45(10):1250612520, 2023. 2, 3 [39] Jiaqi Tang, Zhaoyang Liu, Chen Qian, Wayne Wu, and Limin Wang. Progressive attention on multi-level dense difference maps for generic event boundary detection. In CVPR, pages 33553364, 2022. 2, 5, 3 [40] Song Tang, Chuang Li, Pu Zhang, and RongNian Tang. Swinlstm: Improving spatiotemporal prediction accuracy usIn ICCV, pages 13470 ing swin transformer and lstm. 13479, 2023. 4 [41] Tuan Tang, Jungin Park, Kwonyoung Kim, Kwanghoon Sohn. online temporal action localization. arXiv:2211.04905, 2022. 2, 5, 6 and Simon: simple framework for arXiv preprint [42] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In ICCV, pages 44894497, 2015. 2 [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017. 4 [44] Xiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Zhengrong Zuo, Changxin Gao, and Nong Sang. Oadtr: Online action detection with transformers. In ICCV, pages 75657575, 2021. 2, 5,"
        },
        {
            "title": "Supplementary Material",
            "content": "Table 8. Ablation study of Online Boundary Discriminator. Each number denotes an Avg. F1 score for the range of τ and . Bold number denotes the best Avg. F1 among the given same size of queue. τ 1.0 1.5 2.0 2.5 3.0 Size of Queue 15 0.705 0.701 0.715 0.728 0. 18 0.741 0.743 0.740 0.716 0.670 21 0.748 0.748 0.735 0.701 0.649 24 0.751 0.747 0.726 0.684 0. 12 0.705 0.701 0.715 0.731 0.711 A. Ablation Study on Both τ and δ in OBD In Table 8, we conduct an in-depth analysis on the interaction between the threshold τ and the queue size for the Online Boundary Discriminator (OBD). This table highlights the effects of varying these two parameters on the average F1 (Avg. F1) score in Kinetics-GEBD dataset [34]. We observe that for given queue size , increasing the threshold τ initially leads to improvements in performance up to certain point, after which further increases in τ lead to decline in the Avg. F1 score. For instance, when the queue size is fixed at = 18, the peak performance is achieved at τ = 1.5, with an Avg. F1 score of 0.743. Increasing the threshold means selecting more severe outliers compared to the past errors stored in the OBD. Thus, setting criterion that is either too strict or not would naturally result in decline in overall performance. We have determied τ as 1.5 throughout the entire experiment, since it demonstrates satisfactory performance as shown in the Table 8. Additionally, we can observe that performance gets better with lower τ values when increases. For example, at queue size of = 24, the highest F1 score is 0.751, which occurs at the lowest examined threshold of τ = 1.0. This trend suggests that larger queues are better with lower thresholds, potentially due to the greater amount of past errors available in OBD queue when determining event boundaries. We choose queue size of = 21 and threshold of τ = 1.5, where the model achieves its optimal performance with an Avg. F1 score of 0.748. B. Further Experiments on in REST Loss The Regional EST (REST) loss is core component in training our Consistent Event Anticipator (CEA), designed to enhance the models ability to detect subtle changes at event boundaries. The parameter determines the size of the temporal region considered in the REST loss calculaTable 9. Ablation study of in REST loss. Adjusting the range of REST loss in training CEA. 3 5 9 11 13 15 17 Avg F1 0.724 0.733 0.743 0.748 0.756 0.756 0.754 0.749 0.746 Table 10. Comparison of different lengths, Avg F1 scores, and VRAM usage. We denote the highest Avg F1 in bold."
        },
        {
            "title": "Length",
            "content": "Avg F1 VRAM (GB) 4 8 16 32 0.728 (Ours) 0.748 0.742 0.745 5.2 9.0 14.8 27.9 tion, controlling the range of frames that influences the loss computation. To better understand the impact of this parameter, we conducted additional experiments varying the size of K, with results presented in Table 9. These experiments reveal clear trend in model performance as changes. The Avg. F1 score shows consistent increase as grows from 3 to 11, indicating that larger temporal context benefits the models ability to detect event boundaries. This improvement can be attributed to the models enhanced capacity to capture longer-range dependencies and more complex temporal patterns within the video sequences. Interestingly, our experimental result shows that the models performance peaks when is set to 11 or 13, with both values yielding an Avg. F1 of 0.756. However, we observe decline in performance for values beyond 13, suggesting that excessively large temporal regions may introduce noise or irrelevant information into the loss calculation. Despite the highest performance at = 11 and 13, we opted to use = 9 for all experiments reported in the main manuscript. This decision was primarily due to practical considerations, considering the trade-off between model performance and computational resources. Larger values require more GPU VRAM during training, which can limit batch sizes or necessitates more powerful hardware. C. Ablation on Length The choice of input video sequence length impacts both the performance and computational efficiency of our model. longer input sequence provides more temporal context, potentially improving boundary detection accuracy but at the cost of increased VRAM consumption and inference time. Conversely, shorter sequences are computationally efficient but may lack sufficient context for detecting subtle event transitions. 1 Table 11. Quantitative comparison with additional offline methods. In addition to the offline GEBD methods presented in Table 2 of our original manuscript, we include additional results from more recent offline approaches to highlight the robustness of our model, even as an online method. Note that we report the performance of the models in an offline setting from their original literature. Also, we indicate the highest F1 score with bold for each dataset."
        },
        {
            "title": "Supervision",
            "content": "Rel. Dis. threshold 0.05 0.1 0.15 0.2 0. 0.3 0.35 0.4 0.45 0."
        },
        {
            "title": "Offline",
            "content": "D - t K"
        },
        {
            "title": "Unsupervised",
            "content": "BMN [26] BMN-StartEnd [34] TCN-TAPOS [34] TCN [22] PC [34] Temporal Perceiver [38] SBoCo-Res50 [17] DDM-Net [39] SC-Transformer [23] EfficientGEBD [56] LCVSL [52] DyBDet [55] SceneDetect [34] PA-Random [34] PA [34] CoSeg [45] UBoCo-Res50 [17] FlowGEBD [12] 0.186 0.491 0.464 0.588 0.625 0.748 0.732 0.764 0.777 0.783 0.768 0.796 0.275 0.336 0.396 0.656 0.703 0.713 0.204 0.589 0.560 0.657 0.758 0.828 - 0.843 0.849 0.851 0.848 0.858 0.300 0.435 0.488 0.758 - 0. 0.213 0.627 0.602 0.679 0.804 0.852 - 0.866 0.873 - 0.872 0.880 0.312 0.484 0.520 0.783 - 0.850 0.220 0.648 0.628 0.691 0.829 0.866 - 0.880 0.886 - 0.885 0.893 0.319 0.512 0.534 0.794 - 0.858 0.226 0.660 0.645 0.698 0.844 0.874 - 0.887 0.895 - 0.892 0.901 0.324 0.529 0.544 0.799 - 0. 0.230 0.668 0.659 0.703 0.853 0.879 - 0.892 0.900 0.901 0.896 0.907 0.327 0.541 0.550 0.803 - 0.864 0.233 0.674 0.669 0.706 0.859 0.883 - 0.895 0.904 - 0.899 0.911 0.330 0.548 0.555 0.804 - 0.866 0.237 0.678 0.676 0.708 0.864 0.887 - 0.898 0.907 - 0.901 0.915 0.332 0.554 0.558 0.806 - 0. 0.239 0.681 0.682 0.710 0.867 0.890 - 0.900 0.909 - 0.903 0.917 0.334 0.558 0.561 0.807 - 0.868 0.241 0.683 0.687 0.712 0.870 0.892 - 0.902 0.911 0.913 0.906 0.919 0.335 0.561 0.564 0.809 - 0.869 0.223 0.640 0.627 0.685 0.817 0.860 0.866 0.873 0.881 0.883 0.877 0.890 0.318 0.506 0.527 0.782 0.867 0."
        },
        {
            "title": "Supervised",
            "content": "ESTimator (Ours) 0.620 0.687 0.724 0.746 0. 0.774 0.782 0.789 0.795 0.799 0."
        },
        {
            "title": "Offline",
            "content": "S T"
        },
        {
            "title": "Unsupervised",
            "content": "ISBA [34] TCN [34] CTM [34] TransParser [22] PC [34] DDM-Net [39] Temporal Perceiver [38] SC-Transformer [23] EfficientGEBD [56] LCVSL [52] DyBDet [55] SceneDetect [34] PA-Random [34] PA [34] FlowGEBD [12] 0.106 0.237 0.244 0.289 0.522 0.604 0.552 0.618 0.631 0.618 0.625 0.035 0.158 0.360 0.375 0.170 0.312 0.312 0.381 0.595 0.681 0.663 0.694 0.705 0.694 0.701 0.045 0.233 0.459 0. 0.227 0.331 0.336 0.435 0.628 0.715 0.713 0.728 - 0.728 0.734 0.047 0.273 0.507 0.569 0.265 0.339 0.351 0.475 0.646 0.735 0.738 0.749 - 0.749 0.756 0.051 0.310 0.543 0.624 0.298 0.342 0.361 0.500 0.659 0.747 0.757 0.761 - 0.761 0.767 0.053 0.331 0.567 0. 0.326 0.344 0.369 0.514 0.665 0.753 0.765 0.767 0.774 0.767 0.772 0.054 0.347 0.579 0.677 0.348 0.347 0.374 0.527 0.671 0.757 0.774 0.771 - 0.771 0.775 0.055 0.357 0.592 0.695 0.348 0.348 0.381 0.534 0.676 0.760 0.779 0.774 - 0.774 0.779 0.056 0.369 0.601 0. 0.348 0.348 0.383 0.540 0.679 0.763 0.784 0.777 - 0.777 0.781 0.057 0.376 0.609 0.711 0.348 0.348 0.385 0.545 0.683 0.767 0.788 0.780 0.786 0.780 0.784 0.058 0.384 0.615 0.717 0.330 0.330 0.350 0.474 0.642 0.728 0.732 0.742 0.748 0.742 0.747 0.051 0.314 0.543 0."
        },
        {
            "title": "Supervised",
            "content": "ESTimator (Ours) 0.394 0.455 0.499 0.532 0. 0.578 0.594 0.608 0.619 0.629 0. To achieve balance between performance and efficiency, we set the input length to an optimal value based on empirical results. As shown in Table 10, we compare different sequence lengths in terms of Avg F1 score and VRAM usage. Our selected input length achieves the highest Avg F1 score while maintaining reasonable VRAM footprint, making it suitable for real-time processing. Our OBD is designed to dynamically adapt to recent boundary patterns, reducing false positives during frequent changes while maintaining sensitivity in stable periods. This design aligns with human perception, as studies suggest that when individuals are exposed to rapidly changing visuals, they naturally adjust their threshold for identifying meaningful event boundaries [11]. The ability to incorporate past outliers ensures that the model remains adaptable to varying event structures without excessive desensitization to new transitions. Table 12. Ablation on batch-wise weighted loss. Batch-wise loss Avg F1 0.743 0. These findings reinforce the necessity of including outliers in the queue to maintain robust event boundary detection, making our approach both computationally effective and cognitively plausible. D. Additional Offline GEBD performance table We further report the performance of models developed and evaluated under an offline setting in Table 11. Compared 2 Table 13. Quantitative comparison for generalization ability. Results on Youtube-INRIA-Instructional dataset with online and offline baselines."
        },
        {
            "title": "Pretrained",
            "content": "Precision@0.05 Recall@0.05 F1@0.05 U-Net CoSeg [41]"
        },
        {
            "title": "INRIA\nINRIA",
            "content": "TeSTra BC Sim-On BC OadTR BC Kinetics-GEBD Kinetics-GEBD Kinetics-GEBD MiniROAD - BC Kinetics-GEBD"
        },
        {
            "title": "Ours",
            "content": "Kinetics-GEBD - 0.467 0.181 0.099 0.348 0.209 0.411 - 0.633 0.748 0.068 0.526 0. 0.666 0.299 0.537 0.291 0.080 0.419 0.306 0.508 [39], SC-Transformer to the Table 2 in our main manuscript, Table 11 additionally include Temporal Perceiver [38], SBoCo-Res50 [17], DDM-Net [23], UBoCo [17], Efficient-GEBD [56], LCVSL [52], DyBDet [55] and FlowGEBD [12] for the Kinetics-GEBD dataset. For the TAPOS dataset, we have additionally included DDM-Net, Temporal Perceiver, SC-Transformer, Efficient-GEBD [56], LCVSL [52], DyBDet and FlowGEBD [12] as UBoCo do not report performance for this dataset. E. Ablation on Batch-wise Weighted Loss Table 12 presents the Avg. F1 score on the Kinetics-GEBD dataset, evaluating the impact of batch-wise weighted loss in our model. This technique addresses the imbalance between boundary and non-boundary frames in the training data, common challenge in event boundary detection tasks. By dynamically adjusting the importance of samples within single batch during training, the batch-wise weighted loss aims to improve the models sensitivity to boundary frames without manual hyper-parameter tuning."
        },
        {
            "title": "The results",
            "content": "indicate that incorporating batch-wise weighted loss yields 0.5%p increase in the Avg. F1 score. This improvement may seem trivial, but considering the sensitivity of detecting generic event boundaries, we conjecture that batch-wise weighting is showing noticeable improvement in accuracy. F. Zero-shot Ability of Our Framework To further demonstrate the generalization capability of our framework, we evaluate our framework on the challenging YouTube-INRIA-Instructional dataset [1]  (Table 13)  , which was used in [45] and consists of long-form, multiminute instructional videosmarkedly different in nature from Kinetics-GEBD. Without any additional finetuning, our model pretrained solely on Kinetics-GEBD achieves an F1@0.05 score of 0.508. This result is competitive with, or even superior to, existing offline methods, and it consistently outperforms all online baselines. These results highlight the strong zero-shot generalization ability of our model to previously unseen, complex video domains. 3 G. Additional Details on Computational Cost In Table 5 of the main manuscript, we analyze the real-time performance of our proposing model, focusing on its inference speed (i.e. FPS). For completeness, we provide additional real-time metrics including computational cost details (e.g., GFLOPs and memory usage) in Table 14, highlighting the efficiency of our method in online scenario. As showcased in the Table 14, our model achieves best performance despite having compatible number of GFLOPs and parameters compared to the most efficient baselines (i.e., Sim-OnBC, MiniROAD-BC), demonstrating the effectiveness. H. Additional Qualitative Result We illustrate more qualitative results of our model compared to one of baselines (TeSTra-BC [54]), on both Kinetics-GEBD and TAPOS [32] datasets. In Figure 5, we present two cases of abrupt scene changes (i.e., first and second row) and two cases of subtle changes (i.e., third and fourth row) in Kinetics-GEBD dataset. The first row shows distinct transition such as shot changes between events in video. In this straightforward scenario, both the baseline and our method yield results that are close to the ground truth. However, the error plot of our method for each frame shows sharp peaks, distinctively indicating the boundary locations, in contrast to the baselines, which presents nearly flat distribution. In the second row, there are changes of scene not only at event boundaries but also within each event. While TeSTra-BC fails to recognize the semantic continuity at the first event of the video and raises numerous false alarms, our framework recognizes the boundaries successfully. The third and fourth example present cases where the transition of events is subtle, requiring deeper understanding of granular details to detect event boundaries. Our model also outperforms the baseline in identifying event boundaries. In Figure 6, we present comparison between TeSTraBC and our framework on the TAPOS dataset. As mentioned in our main manuscript, the TAPOS dataset consists of Olympic sport videos annotated with 21 action classes, where each action is further divided into multiple subactions. Since these sub-actions are re-purposed as single Table 14. Comparison of real-time performance with computational cost. Note that bold refers to the best and underline refers to the second best."
        },
        {
            "title": "Method",
            "content": "# of param. GFLOPs VRAM (MB) FPS Avg. F1 TeSTra BC Sim-On BC OadTR BC MiniROAD - BC"
        },
        {
            "title": "Ours",
            "content": "48.73M 24.70M 97.10M 37.15M 42.41M 17.0 8.2 13.0 8.2 10.3 354 134 385 134 72.5 76.3 48.9 99.8 96.3 0.557 0.618 0.558 0.681 0.748 All experiments were conducted on single NVIDIA RTX A6000 GPU. event in our experiment, the semantic changes between subactions within the single video tend to be subtle. As shown in Figure 6, TeSTra-BC fails to detect event boundaries in all four cases, particularly failing to detect any boundaries in the third and fourth cases. In contrast, our framework successfully detects the subtle semantic changes occurring at event boundaries in all videos. I. Limitation and Social Impact Although the Kinetics-GEBD and TAPOS dataset are the only datasets available for testing the GEBD task, they consist exclusively of sports or exercise-related videos. In this context, OBD, which introduces novel criterion for defining event boundaries, may exhibit bias toward sports or exercise contexts. To ensure robust performance across diverse range of domains, it may be necessary to construct variety of datasets for GEBD and perform tuning of corresponding parameters (e.g., , τ ). Since the On-GEBD solver is able to process diverse long-form videos in real time, it has the potential to impact fields that require continuous monitoring and rapid analysis within the previously unobserved video streams such as public safety and surveillance. 4 Figure 5. Additional qualitative result on Kinetics-GEBD dataset. Comparison between our proposed framework and the baseline (TeSTra-BC [54]). Figure 6. Additional qualitative result on TAPOS dataset. Comparison between our proposed framework and the baseline (TeSTraBC [54])."
        }
    ],
    "affiliations": [
        "GIST",
        "POSTECH",
        "Seoul National University"
    ]
}