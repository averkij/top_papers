{
    "paper_title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training",
    "authors": [
        "Pengkai Wang",
        "Qi Zuo",
        "Pengwei Liu",
        "Zhijie Sang",
        "Congkai Xie",
        "Hongxia Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown substantial advances through reinforcement learning (RL), particularly in domains where rewards can be programmatically verified, such as mathematics and code. In these areas, models benefit from a well-defined operational base guided by explicit rule-based objectives. However, this progress reveals a significant limitation: in open-ended domains where rewards are ambiguous, subjective, or context-dependent, such as creative writing, scientific reasoning, and notably medical consultation, robust reward functions are lacking, making these areas challenging for current RL strategies. To bridge this gap, we introduce ORBIT, an open-ended rubric-based incremental training framework specifically designed for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue generation with the dynamic creation of rubrics, employing these rubrics to direct an incremental RL process. In particular, this approach does not depend on external medical knowledge or manual rules, instead utilizing rubric-guided feedback to shape learning. When implemented on the Qwen3-4B-Instruct model, our method can greatly enhance its performance on the HealthBench-Hard benchmark from 7.0 to 27.2 using only 2k samples, thus achieving state-of-the-art results for models of this scale. Our analysis confirms that rubric-driven RL fos-ters consistent performance gains across diverse consultation scenarios, going beyond simple numerical improvements. These findings underscore rubric-based feedback as a scalable strategy for advancing LLMs in intricate, open-ended tasks."
        },
        {
            "title": "Start",
            "content": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Pengkai Wang * 1 2 Qi Zuo * Pengwei Liu * 3 Zhijie Sang 2 Congkai Xie 2 Hongxia Yang"
        },
        {
            "title": "Abstract",
            "content": "pidneuralode/ORBIT. 5 2 0 2 7 1 ] . [ 1 9 5 8 5 1 . 0 1 5 2 : r Large Language Models (LLMs) have shown substantial advances through reinforcement learning (RL), particularly in domains where rewards can be programmatically verified, such as mathematics and code. In these areas, models benefit from well-defined operational base guided by explicit rule-based objectives. However, this progress reveals significant limitation: in open-ended domains where rewards are ambiguous, subjective, or context-dependent, such as creative writing, scientific reasoning, and notably medical consultation, robust reward functions are lacking, making these areas challenging for current RL strategies. To bridge this gap, we introduce ORBIT, an open-ended rubric-based incremental training framework specifically designed for highstakes medical dialogue. ORBIT integrates synthetic dialogue generation with the dynamic creation of rubrics, employing these rubrics to direct an incremental RL process. In particular, this approach does not depend on external medical knowledge or manual rules, instead utilizing rubric-guided feedback to shape learning. When implemented on the Qwen3-4B-Instruct model, our method can greatly enhance its performance on the HealthBench-Hard benchmark from 7.0 to 27.2 using only 2k samples, thus achieving state-of-the-art results for models of this scale. Our analysis confirms that rubric-driven RL fosters consistent performance gains across diverse consultation scenarios, going beyond simple numerical improvements. These findings underscore rubric-based feedback as scalable strategy for advancing LLMs in intricate, open-ended tasks. Code is available at https://github.com/ *Equal contribution 1Department of Computing, University of Hong Kong Polytechnic University, Hong Kong, China 2InfiX.ai 3Department of Control Science and Engineering, Correspondence to: Hongxia Yang Zhejiang University. <hongxia.yang@polyu.edu.hk>. 1. Introduction In recent years, the significance of the post-training phase in the development of Large Language Models (LLMs) has grown substantially, trend underscored by the emergence of models like DeepSeek-Math (Shao et al., 2024) and corresponding body of literature (Yu et al., 2025; Zheng et al., 2025). prevailing finding in this research is that Supervised Fine-Tuning (SFT) is particularly effective for knowledge memorization, while Reinforcement Learning (RL) excels at enhancing generalization (Chu et al., 2025; Shenfeld et al., 2025). Through the use of comprehensive and diverse SFT corpus, model can acquire domain-specific knowledge and, simultaneously, adapt rapidly to the instructional formats of downstream tasks (Zhang et al., 2024; Liu et al., 2025b). The subsequent RL phase, building upon this strong foundation, then assists the model in exploring the boundaries of its capabilities through reward signals based on interactions with given corpus. As result of extensive practical application, the SFT+RL sequence has become the predominant post-training paradigm, although alternative approaches, such as zero-RL methods (Shao et al., 2024), also exist. RLVR(Reinforcement with Verifiable Rewards) shows promising enhancement on tasks like Math, Code, and other STEM subjects with verifiable format. Despite the quick development of scientific-related reinforcement learning, we have many open-ended subjects in real world scenarios like medicine, psychology, and sociology. Benchmarks which adopt rubrics as assessment criterion are proved to be useful to evaluate the both personality and professional level of LLMs. Among open-ended benchmarks, Healthbench (Arora et al., 2025) pave the way for evaluating the medical ability of LLMs with expert-designed hand-written rubrics for special clinical cases. Many medical expert-level medical QA models (Chen et al., 2024; Huang et al., 2025; Wu et al., 2025) can only get zero points on HealthBench Hard, indicating the huge gap between QA benchmarks and open-ended benchmarks. 1 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training To enhance the open-ended medical ability of LLMs, we propose ORBIT, post-training system that can greatly enhance the performance of LLMs on HealthBench. We build an automatic rubrics generation strategy without finetuning LLM or requiring human efforts. We first adopt RAG module to recall the top query related to specific clinical query we collected and utilize the in-context learning ability of LLMs to generate corresponding rubrics. We have systematically reviewed the selection of LLMs for the best rubric design. In order to improve both efficiency and performance, we carefully design sample-level and rubric-level selection module. Finally, we conduct comprehensive ablation on variables among the RL training process including rubric design, judge model selection, and data curation. In summary, our contributions include: fully automated rubrics generation paradigm, which can be potentially useful in enhancing performances of various open-ended evaluation benchmarks. systematic verification of strategies to filter data from both sample-level and rubric-level. comprehensive ablation study on components of the ORBIT system to optimize the performance of the rubrics-based RL training paradigm. 2. Related Work 2.1. Open-Ended Benchmarks The evaluation of large language models (LLMs) on openended generation tasks is undergoing paradigm shift, moving from conventional automated metrics toward more holistic, rubric-based frameworks. Early benchmarks were often limited to short-form text grading, fixed entity extraction, or weakly correlated metrics from natural language generation. These methods proved insufficient for capturing the sophisticated, multi-faceted capabilities of modern LLMs. This limitation has spurred the development of new generation of evaluation suites that employ multi-dimensional, fine-grained rubrics. Prominent examples include HealthBench (Arora et al., 2025), VISTA (Scale AI, 2025), PaperBench (Starace et al., 2025), WildBench (Lin et al., 2024), AMEGA (Fast et al., 2024), and MultiChallenge (Deshpande et al., 2025). By systematically defining thousands of evaluation criteria across diverse scenarios, these benchmarks enable more precise and comprehensive assessment of model behavior. HealthBench, in particular, demonstrates how large-scale, rubric-based evaluation can delineate performance boundaries in the medical consultation domain, task that remains significant challenge for current models. 2 2.2. Rubric-based RL in LLM The role of Reinforcement Learning (RL) in the posttraining of Large Language Models (LLMs) has become increasingly critical. Early methodologies, such as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), mainly relied on holistic feedback from preference data to assess the overall quality of models response. However, this approach often provides only coarse-grained evaluation. Subsequently, shift occurred towards more granular rule-based RL techniques. These methods began by rewarding specific structured components of an answer and later evolved to incorporate rewards for adherence to particular output formats (Chen et al., 2024; Zhang and Zhang, 2024). This was followed by the development of RL methods centered on even more fine-grained semantic reward signals, as exemplified by recent work in (Bhaskar et al., 2025; Jayalath et al., 2025; Viswanathan et al., 2025). This trend has also been reflected in the medical domain, with several specialized approaches emerging (Gunjal et al., 2025; Dou et al., 2025). Despite this progress, significant challenges remain. Key open questions include: how to construct appropriate and scalable rubric data for reward modeling; how to design effective and reproducible pipelines; and how to make these resources open-source to foster broader research. Crucially, determining how to rationally structure the rubric input process to guide model to more rapidly and efficiently probe the capability boundaries of its base architecture during RL continues to be vital research frontier. 2.3. LLM for Health The successful scaling of language models has spurred significant interest in their application to healthcare (Singhal et al., 2023; 2025; Tanno et al., 2025; Thirunavukarasu et al., 2023). Prior work has demonstrated their potential across range of targeted tasks, including generating differential diagnoses, assisting with clinical documentation, supporting mental health treatment, and drafting radiology reports (McDuff et al., 2025; Oh et al., 2024; Liu et al., 2025c). As large language models become more deeply integrated into clinical workflows, the need for agentic architectures has become critical. The inherent complexity of healthcaredemanding multi-task reasoning and the use of diverse external toolsrequires more adaptive and interactive paradigm. Consequently, LLM-driven agents (Ferber et al., 2025; Lu et al., 2024; Tang et al., 2024) have emerged as foundational framework for translating these models into actionable intelligence within real-world medical environments. ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Figure 1. The overall pipeline of ORBIT, useful method to align LLMs on open-ended complex tasks via rubrics-based incremental training. (a) Sampling medical consultation cases, truncating dialogues to form realistic queries, and generating distilled supervision data for SFT training. (b) During the RL phase, we adopt the GRPO framework, where dynamic, query-specific rubric is generated for each medical case. Each response is evaluated by an external Judge model based on this rubric, and the resulting score serves as the reward signal for policy optimization. (c) For each medical consultation case, Retrieval-Augmented Generation (RAG) framework constructs tailored, multi-dimensional rubric. The retrieval module references HealthBench rubrics as exemplars, while the generation model is guided to avoid direct duplication. These rubrics are used solely by the external Judge model for evaluation, ensuring that the agent remains unbiased and enabling stable policy exploration. The RAG framework is also extensible, supporting integration of larger and more diverse medical consultation datasets. 3. ORBIT 3.1. Dialogue QA simulation turn it Problems setup Given clinical case in realistic world, whether in chat format or in outpatient chart format, ORBIT will into practical RL training data. Through filter strategies from different levels, the final < dialogue, rubrics > pair is fed for RL training to optimize the policy gradient of the base LLM. Basically it follows three steps: 3.1 Dialogue QA simulation; 3.2 Rubrics Generator with In-Context Learning; 3.3 Rubrics-Based Reinforcement Learning. Whether starts from chat format or in outpatient chart format, we can prompt LLM to generate feasible multi-turn dialogue. The agentic synthesis methods are practical ways to produce multi-turn dialogue data. For simplicity, we finish the main experiments using the testset dialogue data from DoctorAgent-RL (Feng et al., 2025). After we prepare the dialogue QA data, we utilize the in-context learning ability of advanced LLMs to generate rubrics step by step. 3 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training 3.2. Rubrics Generator with In-Context Learning 3.2.1. DIAGNOSTIC DATABASE CONSTRUCTION Given seed dataset = {(qi, Ri)}N i=1 derived from the HealthBench rubrics and an embedding-based LLM Memb, the goal is to construct semantically enriched diagnostic database. Here, qi denotes the textual dialogue history of the i-th medical consultation case, and Ri = {ri,1, ri,2, . . . , ri,n} represents the corresponding set of diagnostic rubrics. Each dialogue qi and rubric ri,j is first transformed into its semantic representation through Memb, producing embedding vectors eqi = Memb(qi) and eri,j = Memb(ri,j). These embeddings are then stored in structured vector database to support efficient semantic retrieval and matching. Two distinct data pools are then constructed: caserubric pair pool caserubric pairs, formally defined as (i) The (Pcr), which consists of all Pcr = {(qi, Ri, eqi, (cid:88) rRi er) (qi, Ri) D}. (1) (ii) The rubric pool (Pr), which collects all unique rubrics and their embeddings across the dataset, defined as Pr = {(r, er) (cid:91) (qi,Ri)D Ri}. (2) This yields semantically aligned diagnostic database, facilitating fine-grained retrieval, similarity reasoning, and embedding-based analysis of caserubric relationships. 3.2.2. DIAGNOSTIC CANDIDATES SEARCHING Given new query q, representing the dialogue history of patients medical consultation, we first obtain its semantic embedding eq = Memb(q) using an embedding model Memb. Then we compute the similarity between eq and all embeddings stored in our diagnostic database, which comprises caserubric pair pool (Pcr) and rubric pool (Pr). Based on the similarity scores, the top-tcases most relevant cases are retrieved from Pcr, and the top-trubrics initial candidate rubrics are selected from Pr. Subsequently, reranker model Mre is employed to refine the ranking of these initial candidates, balancing computational efficiency with matching accuracy. Through this two-stage retrieval and reranking process, we obtain the set of top-tcases relevant cases, Cq, and the set of top-trubrics semantically aligned rubrics, Rq. 3.2.3. RUBRICS GENERATION PROCESS The retrieved cases Cq and rubrics Rq serve as in-context input for the generation process. We employ generative model, G, and provide it with prompt constructed from Cq, Rq, and task-specific instructions. This guides the model to produce mg rubric candidates for the query q: G(q) = {r1, . . . , rmg }. (3) This generated set of rubrics serves as checklist for evaluating the performance of models response to the input medical query. 3.2.4. DIFFICULTY FILTER WITH PASS@K To improve the quality of our training data and enhance reinforcement learning efficiency, we introduce two-stage filtering strategy. First, we filter the input cases to select for intermediate difficulty. This step isolates productive learning zone by removing problems that are either too trivial to be informative or too complex to be solvable. Second, we filter the generated rubrics, retaining only those that set sufficiently high performance standard. This is critical for creating strong optimization gradient, as overly lenient rubrics fail to drive meaningful model improvement. These filters can be designed to ensure that the model trains on solvable yet challenging problems, guided by rigorous evaluative criteria, thereby maximizing the efficiency of the RL process. This process begins with scoring step. For each query q, we prompt the current model to generate nrollout responses, denoted as Yq = {y1, y2, . . . , ynrollout}. judge model then evaluates each response yi against the set of generated rubrics Rq. This yields satisfaction score S(yi, r) for each response-rubric pair. Based on these scores, we apply two distinct filtering methods in parallel. Sample-Level Filtering: Selecting Moderately Difficult Cases. This filter aims to identify queries that are at an appropriate difficulty level for the model. We first calculate an average score for each sample by averaging the satisfaction scores across all its rollouts and rubrics: sq = 1 nrollout Sq nrollout(cid:88) (cid:88) i=1 rRq S(yi, r), (4) where Sq represents the set of candidate positive scores for this query. sq reflects the models overall performance on the query q. high score suggests the case is too easy, while low score indicates it is too hard. We then apply dualthreshold filter to retain only the moderately challenging samples: Qfiltered = {q τ low sq τ high }. (5) By keeping only the samples whose scores fall within the range [τ low ], we focus the training process on the most instructive examples where the model has room to improve. , τ high Rubric-Level Filtering: Refining Rubric Quality This filter is designed to discard rubrics that are too simple and 4 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training fail to challenge the model. For each rubric r, we calculate its pass rate, defined as the proportion of rollouts that satisfy the rubrics criteria. We consider rubric passed if its satisfaction score exceeds threshold τs (e.g., 0.8). The pass rate for rubric across the nrollout responses for query is: (r, q) = 1 nrollout nrollout(cid:88) i=1 1{S(yi, r) τs}, (6) where 1{} is the indicator function. Rubrics with very high pass rate are considered too easy and provide little value for reinforcement learning. We filter them out using pass rate threshold τr: Rq,filtered = {r Rq (r, q) < τr}. (7) This ensures that the retained rubrics are relatively challenging to guide the models optimization effectively. Ultimately, this filtering strategy enhances the quality of both the training samples and their corresponding rubrics, yielding more robust and efficient dataset for the subsequent reinforcement learning stage. 3.3. Rubrics-Based Reinforcement Learning Group Relative Policy Optimization (GRPO) (Shao et al., 2024) is memory-efficient variant of PPO (Schulman et al., 2017) that avoids value network by using group baseline. For each query q, we can produce rollouts from the original policy model πold and we need to optimize the following tagert function JGRPO(θ) = Eq,{oi} 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 Pt(θ) βDKL[πθπref] (8) where the clipped Pt(θ) can be presented as Qt(θ) = min (cid:16) rt(θ) ˆAi,t, clip(rt(θ), 1 ε, 1 + ε) ˆAi,t (cid:17) (9) where the importance weighting token-level ratio and the group-normalized advantage are rt(θ) = πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) , ˆAi,t = R(q, oi) RG σG (10) (cid:80)G Here RG = 1 j=1 R(q, oj) is the group mean reward and σG its standard deviation and the KL term discourages large policy drift from the reference model πref . While the GRPO framework is highly unified and simplified representation for reinforcement learning training of LLMs. However, designing appropriate reward functions for open-ended scenarios remains challenging problem. Conventional reward function design predominantly relies on rule-based matchingfor instance, extracting the answer from the final boxed segment in response and assigning binary reward {0, 1} based on its correctness, after which advantages are defined via group-wise comparisons. Such rule-based paradigms face significant difficulties in scaling to open-ended benchmarks. Moreover, an oversimplified reward function can lead to training instability and generate uninformative feedback. To address these limitations, we propose reward framework that treats rubrics as dynamic, semantic scoring rules. As detailed in Sec 3.2, we use RAG system as rubrics generator model, denoted as Mrubrics, to produce tailored set of rubrics for each input query q: Mrubrics(q, task) {r1, r2, . . . , rn}. (11) Each rubric ri = {criterioni, pointi} consists of specific evaluation criterion and its corresponding importance score. To calculate the reward for given response, we employ separate large language model (LLM) as Judge Model. This judge evaluates the responses (oi) generated by the current policy πold against each criterion. The overall reward for query and response oi is the sum of points from all satisfied criteria, computed as: R(q, oi) = = (cid:88) j=1 (cid:88) j=1 match(q, oi, rj) match(q, oi, criterionj) pointj. (12) Here, the Judge Model executes the match() function, producing binary output (1 if the criterion is satisfied, 0 otherwise). Each response thereby accumulates points based on the number of criteria it fulfills. This aggregated score forms the rollout reward, which in turn is utilized to calculate the advantage function ˆAi,t and construct the policy update for our rubric-based reinforcement learning. 4. Experiments 4.1. Related Settings Medicine Dialogue Datasets. Our primary analyses were performed on focused corpus of 2,082 test samples, meticulously curated from three prominent Chinese medical benchmarks: IMCS21 (Chen et al., 2023), CHIPMDCFNPC (Zhu et al., 2023), and MedDG (Liu et al., 2022). This dataset was designed to cover diverse range of classic, multi-dimensional medical consultation scenarios, which is also the test dataset of doctorAgent-RL (Feng et al., 2025). ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Benchmark. HealthBench (Arora et al., 2025), an openended medical benchmark from OpenAI, comprises 5,000 full consultation cases. It includes HealthBench Hard, particularly challenging subset of 1,000 cases designed to test the limits of even state-of-the-art general-purpose models. We therefore focus our core experiments on this demanding benchmark (Healthbench Hard) to provide rigorous validation of our approach. Baselines. The capabilities of smaller language models on complex, open-ended tasks are often underestimated. To challenge this assumption, we select Qwen34B-Instruct (Yang et al., 2025) as our base model. We then design comprehensive suite of experiments to systematically investigate its performance limits and highlight its potential. Hardware settings. To ensure fair comparison, we maintain consistent batch size across all methods within each task. All experiments were conducted on cluster of 8 NVIDIA H800 GPUs, each with 80 GB of memory. Four GPUs were allocated for training the primary models, while the remaining four were dedicated to serving the evaluation model with generated rubrics. 4.2. Quantitative Results As shown in Tab. 1, our empirical results unequivocally demonstrate the effectiveness of our proposed ORBIT framework, which is grounded in rubrics-based reinforcement learning (rubrics-RL). To ensure rigorous and consistent evaluation, we employ GPT-4.1 as the sole evaluator for this evaluation result. It aligns with the official protocol of HealthBench (Arora et al., 2025), and we utilize the provided the public codes for all assessments1. Our method substantially elevates the performance of smaller open-source models, enabling them to outperform significantly larger counterparts. Specifically, the ORBIT training paradigm yields substantial performance gains. Our Qwen3-4B-ORBIT model achieves total score of 20.3, 78% relative improvement over its supervised fine-tuning baseline of 11.4. Furthermore, our SFT-4B-ORBIT model attains score of 27.2, establishing it as the top-performing model in the sub-10 billion parameter class. Its performance surpasses that of much larger models, including Qwen3-30B-A3B-Thinking (16.1) and the proprietary GPT-4.1 (13.2). Furthermore, we visualize the performance of the model in multiple dimensions, shown as in Fig. 2. Collectively, these results demonstrate that ORBIT offers parameter-efficient pathway for developing specialized models that can excel at complex, open-ended tasks such as medical consultation. 1https://github.com/openai/simple-evals 4.3. Ablation Experiments 4.3.1. RUBRICS GENERATION MODEL In the previous sections, we introduced the system and task prompts for our in-context learning framework, which integrates RAG mechanism with seed dataset of rubrics. The specific prompts used in our experiments are illustrated in Fig. 4 and Fig. 5. While these components provide explicit procedural guidance and high-quality reference examples, the intrinsic capability of the underlying generation model remains decisive factor influencing the quality of the resulting rubrics. To systematically evaluate this effect, we benchmark four state-of-the-art large language models on the rubric generation task: DeepSeek-R1 (Guo et al., 2025), Gemini-2.5Pro (Comanici et al., 2025), GPT-OSS-120B (Agarwal et al., 2025), and GPT-5-Chat (Arora et al., 2025). For considerations of computational efficiency and cost, our primary analysis focuses on GPT-OSS-120B, evaluated under two configurationswith and without activation of its middle reasoning mode. Our rationale for selecting GPT-OSS-120B as the evaluation model is discussed in Sec. 4.3.2. As shown in Tab. 2, the improvements are particularly pronounced for rubrics derived from DeepSeek-R1 and Gemini2.5-Pro, which consistently enhance scores across multiple evaluation dimensions. Nevertheless, trade-off emerges: the current rubric formulation tends to encourage more verbose responses, resulting in slightly lower scores on the response depth metric and certain instruction-following benchmarks relative to the base model. We attribute the underperformance of GPT-5-Chat to its intrinsic safety mechanisms. Its generated rubrics yield only marginal improvements on health-specific evaluation tasks and lack sufficiently stringent criteria to constrain model uncertainty. We think that the content restrictions imposed by the OpenAI APIparticularly those governing sensitive medical topicslimit GPT-5-Chats ability to generate rigorous evaluative rubrics necessary for effectively assessing instruction fidelity in the medical domain. Given its superior performance and balanced trade-off between quality and coverage, we adopt DeepSeek-R1 as the default rubric generator for all subsequent experiments. The dataset it produces serves as standardized and reproducible baseline for evaluating the different training strategies explored in this study. 4.3.2. THE SELECTION OF EVALUATION MODEL The choice of evaluation model exerts critical influence on the measured performance of open-ended benchmarks such as HealthBench. As summarized in Appendix B, our comparative analysis shows that GPT-4.1the model employed in the official OpenAI evaluationserves as particularly 6 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Models GPT-4.1 GPT-5 (thinking) Qwen-3-4B-Instruct Qwen3-4B-Thinking Qwen-2.5-7B-Instruct Qwen3-4B-SFT Qwen3-4B-ORBIT SFT-4B-ORBIT Qwen3-30B-Instrcut Qwen3-30B-A3B-Thinking GPT-oss-120B (high) Baichuan-M2-32B Table 1. Overall model performance on Healthbench Hard By Theme By Axis Emergency referrals Context seeking Global health Health data tasks Communication Hedging Response depth Accuracy Completeness Communicat. quality Context awareness Instruction following Total Score 20.5 - 9.3 14.4 0 19.7 31.2 36.1 18.3 21.4 - 45. 12.3 - 8.5 12.5 0 12.8 27.3 34.8 12.9 20.4 - 39.5 12.1 - 7.1 2.4 13.3 22.4 30.7 14.7 15.0 - 35.6 9.7 - 0 0 0 0 3.4 5.0 17.9 8.9 - 21. Proprietary Models 14.9 - 12.3 - 17.5 - Open-source Models (< 10B) 8.6 3.5 9.5 19.4 23.9 12.2 8.5 0 16.4 31.5 36.1 5.1 0 0 4.3 8.2 8.4 Open-source Models (> 10B) 19.4 16.7 - 32.0 9.5 20.4 - 40.9 28.5 6.5 - 19.9 30.5 - 24.1 23.2 6.4 25.5 31.9 32. 28.5 33.7 - 41.3 0 - 0.8 0 0 9.6 22.7 35.1 0 11.6 - 44.6 70.6 - 57.5 42.5 45.2 55.5 52.6 44.5 45.2 53.0 - 51.6 0 - 0 0 0 0 11.8 19. 0 0 - 19.3 60.5 - 45.0 39.6 33.7 43.6 51.5 45.4 33.7 45.5 - 48.0 13.2 46. 7.0 5.2 0 11.4 21.6 27.2 13.1 16.1 30.0 34.5 Figure 2. Multi-dimensional performance comparison of ORBIT models. This figure presents comprehensive performance analysis categorized by Theme and Axis. The results are divided into two groups: (a, c) Internal methodological comparisons among the base instructor-tuned model (Qwen3-4B-Instruct), the supervised fine-tuning baseline (Qwen3-4B-SFT), and our proposed models (Qwen34B-ORBIT and SFT-4B-ORBIT). (b, d) Large-scale benchmark comparisons between our best-performing model, SFT-4B-ORBIT, and significantly larger proprietary and open-source models, including Qwen3-30B-Instruct, GPT-4.1, Baichuan-M2-32B, and Qwen3-30BThinking. For brevity, Health Data T. refers to Health Data Tasks, and Emerg. Referrals denotes Emergency Referrals. stringent and reliable evaluator. Notably, the scores produced by GPT-OSS-120B exhibit strong consistency with this rigorous standard, demonstrating its ability to approximate GPT-4.1-level assessments while maintaining substantially lower computational cost. Given its close alignment with the GPT-4.1 baseline and superior computational efficiency, we adopt GPT-OSS-120Bmiddle as the primary evaluation model during the algorithm design process. However, for the final performance assessment presented in Tab. 1, we employ GPT-4.1 to ensure our results are directly comparable with those of mainstream benchmark models. 4.3.3. SFT+RL VS ZERO-RL We investigate the role of Supervised Fine-Tuning (SFT) as precursor to the Reinforcement Learning (RL) phase. The SFT process is widely employed to equip base model with the fundamental structural and stylistic priors required for effective query response generation. This initial training provides crucial cold start, enabling the model to explore the solution space more efficiently during the subsequent RL stage. To quantify this effect, we conduct series of controlled experiments. Specifically, we fine-tune the base model on SFT data generated by Baichuan-M2 for our query set (Sec. 3.1), using range of learning rates. As illustrated in Fig. 3, the results confirm that the SFT stage effectively establishes stable and coherent response pattern, thereby streamlining optimization in the later RL phase. Moreover, we find that even when the model is initialized with SFT data from different source model, our rubric-guided RL framework continues to yield consistent performance gains. However, the choice of learning rate in the SFT phase proves critical: an excessively high rate can lead to overfitting to the SFT data distribution, which ultimately constrains exploration and diminishes performance in the subsequent RL phase. 7 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Table 2. The evaluation results of ORBIT models with different rubrics models. By Theme By Axis Judge Models Emergency referrals Context seeking Global health Health data tasks Communication Hedging Response depth Accuracy Completeness Communicat. quality Context awareness Instruction following Total Score Qwen3-4B-Instruct ORBIT-Deepseek-r1 ORBIT-Gemini-2.5-Pro ORBIT-GPT-OSS-120B ORBIT-GPT-5-Chat Qwen3-4B-Instruct ORBIT-Deepseek-r1 ORBIT-Gemini-2.5-Pro ORBIT-GPT-OSS-120B ORBIT-GPT-4.1 ORBIT-GPT-5-Chat 9.4 26.2 21.3 21.9 13.7 6.6 22.6 23.8 20.4 7.7 13.7 10.5 26.5 27.1 19.6 17.4 10.4 26.9 27.0 23.1 15.3 15.4 10.4 22.6 23.5 21.9 16.3 8.3 24.7 25.0 23.7 11.8 13. 0.3 5.5 8.0 3.6 0 0 3.0 9.1 1.1 0.8 0 Evaluation model: GPT-OSS-120B 8.3 20.8 15.6 15.5 11.1 10.8 26.3 29.2 26.2 16.2 0 0.7 0 0 20.5 24.0 23.5 23.5 20.6 Evaluation model: GPT-OSS-120B-middle 9.1 18.2 20.1 20.4 11.0 11.0 12.6 29.1 28.7 27.5 14.8 17.3 0 0 0 0 0 0 19.7 25.6 24.8 25.0 21.5 19. 3.9 23.8 25.3 21.7 13.6 3.5 23.0 26.7 22.5 7.2 11.5 54.2 53.3 47.6 51.1 53.5 57.7 48.5 48.9 55.4 57.3 53.4 0 10.9 13.0 7.2 0.1 0 11.5 13.5 7.1 0.3 0. 47.6 46.9 45.6 45.8 44.4 47.9 46.9 49.7 45.1 43.9 42.6 8.1 20.2 20.3 17.5 12.3 7.2 20.3 21.3 18.8 10.0 11.0 Table 3. Evaluation results of ORBIT models based on either SFT models (with varying learning rates) or Instruct models. Judge Models Emergency referrals Context seeking Global health Health data tasks Communication Hedging Response depth Accuracy Completeness Communicat. quality Context awareness Instruction following Total Score By Theme By Axis Qwen3-4B-Instruct Qwen3-4B-ORBIT SFT-4B-ORBIT (107) SFT-4B-ORBIT (106) SFT-4B-ORBIT (105) 6.6 26.2 28.4 28.9 28.6 10.4 26.5 31.0 30.0 27.4 8.3 22.6 27.9 27.4 22. 0 5.5 10.0 7.6 13.6 9.1 20.8 26.1 22.6 14.1 12.6 26.3 33.8 27.9 25.8 0 0.7 3.6 1.8 0 19.7 24.0 26.3 23.8 23.2 3.5 23.8 32.4 34.7 29. 57.7 53.3 47.8 41.6 43.9 0 10.9 15.2 10.2 8.2 47.9 46.9 45.8 46.7 41.3 7.2 20.2 25.2 23.1 20.3 Evaluation model: GPT-OSS-120B-middle 4.3.4. PASS@K SETTINGS To enhance the training efficiency of our zero-rubric RL framework, we conduct series of experiments to examine the effectiveness of selective data filtering. This study aims to investigate how strategically pruning specific samples or rubrics influences the Reinforcement Learning process, with particular focus on accelerating training without degrading the models final performance. Our experiments begins with an initial dataset comprising 2,082 dialogue samples and 25,020 associated rubrics in Sec. 3.1. For each query, the base model first generates eight candidate responses. These responses are then evaluated by high-capacity judge model, Qwen-3-30B-Instruct2507, against the predefined rubrics to compute their scores and pass rates. Based on this evaluation, we explore two complementary filtering strategies: Sample-Level Filtering: We filter queries based on the aggregate pass rate of their eight rollouts. Starting from 2,082 samples, we apply two thresholds: moderate range of [0, 0.75], which retains 1,403 samples, and stricter range of [0, 0.5], which retains 701 samples. Rubric-Level Filtering: We filter rubrics according to their overall pass rate across all samples. From the initial 25,020 rubrics, we apply thresholds of [0, 0.25], [0, 0.5], and [0, 0.75], resulting in retained rubrics of 10,055, 12,352 and 14,411, respectively. Based on these filtering settings, we train the Qwen3-4BORBIT model for ten epochs under all experimental conditions. The baseline modeltrained without filteringis benchmarked at 320 RL steps. Because sample filtering reduces the total number of available training samples, we proportionally evaluate the filtered configurations at 110 and 220 steps for the [0, 0.5] and [0, 0.75] thresholds, respectively, corresponding to ten complete epochs over the filtered datasets. As shown in Tab. 4 and Fig. 3, applying strategic filtering at the sample level substantially enhances training efficiency while preserving model performance. However, this method presents two potential limitations. First, with small number of rollouts (k = 8), our estimation of the policy space may be incomplete. Second, overly aggressive filtering could prematurely narrow exploration, hindering optimal policy discovery and degrading performance. Similarly, our experiments with rubric-level filtering, also detailed in Tab. 4, reveal consistent benefit. By excluding rubrics with exceptionally high pass rates, we lower the computational cost of reward calculation without noticeable drop in performance. This is particularly advantageous because it minimizes the substantial latency introduced by the frequent interactions required between the policy and judge models during rubric evaluation. In conclusion, our results demonstrate that the judicious use of filtering thresholds, at both the sample and rubric levels, is potent strategy for boosting training efficiency without sacrificing model quality. ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Table 4. Evaluation results of ORBIT models across different pass@k thresholds, assessed using the GPT-OSS-120B-middle model. Performance is compared under two filtering strategies: sample-based filtering and rubric-based filtering. Judge Models Emergency referrals Context seeking Global health Health data tasks Communication Hedging Response depth Accuracy Completeness Communicat. quality Context awareness Instruction following Total Score By Theme By Axis Qwen3-4B-Instruct (Base) Qwen3-4B-ORBIT (no pass) Qwen3-4B-ORBIT (0 0.75) Qwen3-4B-ORBIT (0 0.50) Qwen3-4B-ORBIT (0 0.25) Qwen3-4B-ORBIT (0 0.75) Qwen3-4B-ORBIT (0 0.50) 6.6 26. 20.0 20.5 18.7 21.7 18.2 10.4 26.5 27.7 25.4 24.1 25.3 19.7 8.3 22. 21.4 21.5 21.9 23.3 18.2 0 5.5 8.6 2.0 3.6 5.2 2.2 Base models 12.6 26.3 Pass@k for rubrics 28.0 26.1 27.3 9.1 20.8 17.3 15.3 15.5 Pass@k for samples 16.1 14.0 29.8 17.9 0 0.7 0.1 0 0 0 0 19.7 24. 23.4 24.6 23.7 24.5 22.2 3.5 23.8 25.0 21.2 23.1 22.3 15.4 57.7 53. 51.8 50.0 50.0 48.7 51.6 0 10.9 10.7 8.2 7.6 10.4 2.7 47.9 46. 47.0 42.6 44.8 48.5 45.7 7.2 20.2 19.9 17.9 18.7 19.7 14.5 Figure 3. Training dynamics under different pass@k filtering strategies. Both rubricand sample-based filtering notably enhance training efficiency, while adjustable thresholds enable fine-grained control over the trade-off between computational cost and final performance. learning framework to general-purpose domains and assess its performance on broader spectrum of open-ended tasks. 5. Conclusion In this paper, we present rubrics-based RL training paradigm with systematic design on automated rubrics generation and data selection. Using only 2k samples for learning, we can achieve much better performance(7 27.2) on the hard subset of leading open-ended medical benchmark Healthbench, which also paves the way for other open-ended benchmarks from different subjects. Moreover, the data creation pipeline does not involve any human efforts, and it is easy to enlarge the amount of data for better performance. In fact, numerical correctness is also special case of rubrics and future directions may try to represent any verifiable conditions as rubrics for RL training. We hope our findings in medical area can bring inspiration to other areas which also require both personality of LLMs and accuracy. 6. Limitation Though we automate the data creation pipeline, it needs human-crafted rubrics as examples. In medical areas, there exists many guideline books and best practices which the doctor should follow when starting the clinical dialogue with the patient. We will try to build meaningful rubrics on these guidelines in the future. Beyond healthcare, key objective is to extend our rubric-guided reinforcement ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training"
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gptoss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Rahul Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quinonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025. Adithya Bhaskar, Xi Ye, and Danqi Chen. Language models that think, chat better. arXiv preprint arXiv:2509.20357, 2025. Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024. Wei Chen, Zhiwei Li, Hongyi Fang, Qianyuan Yao, Cheng Zhong, Jianye Hao, Qi Zhang, Xuanjing Huang, Jiajie Peng, and Zhongyu Wei. benchmark for automatic medical consultation system: frameworks, tasks and datasets. Bioinformatics, 39(1):btac817, 2023. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Kaustubh Deshpande, Ved Sirdeshmukh, Johannes Baptist Mols, Lifeng Jin, Ed-Yeremai Hernandez-Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier llms. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1863218702, 2025. Chengfeng Dou, Chong Liu, Fan Yang, Fei Li, Jiyuan Jia, Mingyang Chen, Qiang Ju, Shuai Wang, Shunya Dang, Tianpeng Li, et al. Baichuan-m2: Scaling medical capability with large verifier system. arXiv preprint arXiv:2509.02208, 2025. Dennis Fast, Lisa Adams, Felix Busch, Conor Fallon, Marc Huppertz, Robert Siepmann, Philipp Prucker, Nadine Bayerl, Daniel Truhn, Marcus Makowski, et al. Autonomous medical evaluation for guideline adherence of large language models. NPJ Digital Medicine, 7(1):358, 2024. Yichun Feng, Jiawei Wang, Lu Zhou, Zhen Lei, and Yixue Li. Doctoragent-rl: multi-agent collaborative reinforcement learning system for multi-turn clinical dialogue. arXiv preprint arXiv:2505.19630, 2025. Dyke Ferber, Omar SM El Nahhas, Georg Wolflein, Isabella Wiest, Jan Clusmann, Marie-Elisabeth Leßmann, Sebastian Foersch, Jacqueline Lammert, Maximilian Tschochohei, Dirk Jager, et al. Development and validation of an autonomous artificial intelligence agent for clinical decision-making in oncology. Nature cancer, pages 113, 2025. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645 (8081):633638, 2025. Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, and Yuyin Zhou. m1: Unleash the potential of test-time scaling for medical reasoning with large language models. arXiv preprint arXiv:2504.00869, 2025. Dulhan Jayalath, Shashwat Goel, Thomas Foster, Parag Jain, Suchin Gururangan, Cheng Zhang, Anirudh Goyal, and Alan Schelten. Compute as teacher: Turning inference compute into reference-free supervision. arXiv preprint arXiv:2509.14234, 2025. Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild. arXiv preprint arXiv:2406.04770, 2024. Che Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, and 10 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Rossella Arcucci. Beyond distillation: Pushing the limits of medical llm reasoning with minimalist rule-based rl. arXiv preprint arXiv:2505.17952, 2025a. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less. arXiv preprint arXiv:2509.04259, 2025. Fenglin Liu, Hongjian Zhou, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam Chen, Yining Hua, Peilin Zhou, et al. Application of large language models in medicine. Nature Reviews Bioengineering, pages 120, 2025b. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620 (7972):172180, 2023. Wei Liu, Jun Li, Yitao Tang, Yining Zhao, Chaozhong Liu, Meiyi Song, Zhenlin Ju, Shwetha Kumar, Yiling Lu, Rehan Akbani, et al. Drbioright 2.0: an llm-powered bioinformatics chatbot for large-scale cancer functional proteomics analysis. Nature communications, 16(1):2256, 2025c. Wenge Liu, Jianheng Tang, Yi Cheng, Wenjie Li, Yefeng Zheng, and Xiaodan Liang. Meddg: an entity-centric medical consultation dataset for entity-aware medical dialogue generation. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 447459. Springer, 2022. Meng Lu, Ho Brandon, Ren Dennis, and Xuan Wang. Triageagent: Towards better multi-agents collaborations for large language model-based clinical triage. In ICML 2024 AI for Science Workshop, 2024. Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, et al. Towards accurate differential diagnosis with large language models. Nature, pages 17, 2025. Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Yeona Cho, Ik Jae Lee, Jin Sung Kim, and Jong Chul Ye. Llm-driven multimodal target volume contouring in radiation oncology. Nature Communications, 15(1):9186, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, et al. Toward expert-level medical question answering with large language models. Nature Medicine, 31(3):943950, 2025. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning. In Findings of the Association for Computational Linguistics ACL 2024, pages 599621, 2024. Ryutaro Tanno, David GT Barrett, Andrew Sellergren, Sumedh Ghaisas, Sumanth Dathathri, Abigail See, Johannes Welbl, Charles Lau, Tao Tu, Shekoofeh Azizi, et al. Collaboration between clinicians and vision language models in radiology report generation. Nature Medicine, 31(2):599608, 2025. Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine, 29(8):19301940, 2023. Vijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tongshuang Wu. Checklists are better than reward models for aligning language models. arXiv preprint arXiv:2507.18624, 2025. Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, et al. Medreason: Eliciting factual medical reasoning steps in llms via knowledge graphs. arXiv preprint arXiv:2504.00993, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Brian Zhang and Zhuo Zhang. Detecting bugs with substantial monetary consequences by llm and rule-based reasoning. Advances in Neural Information Processing Systems, 37:133999134023, 2024. Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. In European conference on computer vision, pages 5270. Springer, 2024. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Wei Zhu, Xiaoling Wang, and Huanran Zheng. Promptcblue: chinese prompt tuning benchmark for the medical domain. urlhttps://github.com/michael-wzhu/PromptCBLUE, 2023. 12 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training A. Rubrics Generator Our methodology utilizes Retrieval-Augmented Generation (RAG) system to retrieve reference cases that correspond to given medical scenario. These cases are then provided as in-context examples to rubric generation model, which in turn produces specific, actionable scoring criteria. The prompt we engineered for this process is described in the following. It is important to note that we explicitly instruct the model to generate rubrics with both positive (credit-assigning) and negative (deductive) criteria to more effectively constrain the medical models behavior. A.1. System Prompt For the rubric generation task, we employ rigorously engineered system prompt explicitly crafted to steer the model toward precise and reliable outputs. This prompt is designed with two core objectives: (1) to guarantee strict compliance with user-defined instructions, and (2) to embed an priori understanding of the relative importance assigned to key scoring dimensions. The full structure of the system prompt, reflecting these design principles, is illustrated in Fig. 4. Figure 4. Overall details of the designed system prompt for the rubrics geneation pipeline. A.2. Rubrics Generation Prompt central challenge in this work lies in designing task-specific prompt for each medical case that can effectively guide the model in generating highly contextualized and clinically grounded rubrics. To address this, we leverage the 5,000 curated rubrics from HealthBench as seed dataset and employ RAG framework to retrieve semantically relevant reference information. To mitigate potential data contaminationwhere the model might inadvertently reproduce examples from the seed setwe introduce anti-copying constraints in the rubrics generation. It is important to emphasize that this seed corpus serves purely as reference foundation and researchers can seamlessly replace it with their own manually annotated rubric repositories without altering the underlying pipeline. The task prompt, illustrated in Fig. 5, is systematically structured to ensure modular adaptability. It includes three core placeholders: (1) {query}, representing the specific input medical case; (2) {top cases text}, containing textual details from the three most semantically aligned reference cases; and (3) {candidate rubrics text}, providing thematically related rubric candidates retrieved from the database. In parallel, the prompt encodes explicit instructions for multi-dimensional assessment, ensuring that the generated rubrics are not only comprehensive and context-aware, but also consistent and objectively evaluative across diverse medical scenarios. 13 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Figure 5. Overall details of the designed rubrics generation prompt for the input medical case. Retrieved reference cases from the RAG system are used as in-context examples to guide rubric-generation model in producing multi-dimensional scoring rubrics with both positive and negative criteria. 14 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training B. The Selection of Evaluation Model We have observed that for open-ended question-answering benchmarks such as HealthBench, the choice of the evaluation model substantially influences the final assessment scores. Our comparative analysis indicates that GPT-4.1, the model used in the official OpenAI and Baichuan-M2 evaluations, provides particularly stringent assessment shown as Tab. 5. From the table below, we found that the results of GPT-OSS-120B closely align with this rigorous standard, whereas other evaluation models consistently exhibit tendency to produce more favorable judgments. Table 5. Here, we present comparative analysis of several open-source models against GPT-4.1, evaluating their performance based on benchmark scores. Additionally, we further assess reasoning models across different parameter tiers. Judge Models Emergency referrals Context seeking Global health Health data tasks Communication Hedging Response depth Accuracy Completeness Communicat. quality Context awareness Instruction following Total Score By Theme By Axis GPT-4.1 GPT-OSS-120B GPT-OSS-120B (middle) Deepseek-R1 Deepseek-V3 Qwen2.5-72B GPT-4.1 GPT-OSS-120B GPT-OSS-120B (middle) GPT-4.1 GPT-OSS-120B Qwen2.5-72B GPT-4.1 Qwen2.5-72B Baichuan-M2-Report GPT-4.1 GPT-oss-120B 9.3 9.4 6.6 8.1 23.7 26.4 14.4 15.0 15.3 18.3 15.3 29 20.5 32 - 45.6 39. 8.5 10.5 10.4 11.3 12.7 22.6 12.5 18.0 13.9 12.9 17.2 26.9 12.3 23.4 - 39.5 32.4 7.1 10.4 8.3 11.7 21.4 24. 2.4 13.0 13.1 14.7 15.8 29.1 12.1 26.2 - 35.6 33.7 0 0.3 0.0 0.9 13.1 17.8 0 1.3 0 0 16.7 9.7 24.4 - 21.3 15 Infer model: Qwen-3-4B-Instruct 8.6 8.3 9.1 11.8 20.9 25.7 12.2 10.8 12.6 17.8 23.7 28. 5.1 0 0 3.6 24.2 25.2 Infer model: Qwen-3-4B-Thinking 3.5 7.8 6.9 8.5 11.4 12.2 0 0 0 Infer model: Qwen-3-30B-Instruct 17.9 17.5 29.7 14.9 31.1 19.4 16.9 30.3 9.5 3.8 33.1 Infer model: GPT-4.1 12.3 28. 17.5 32.9 Infer model: Baichuan-M2 - 32 26.1 - 40.9 36.9 - 19.9 10 24.1 20.5 19.7 25.6 41.4 43. 23.2 21.4 21.1 28.5 24.1 49.6 30.5 48.5 - 41.3 31.9 0.8 3.9 3.5 6.1 17.3 17 0 8.0 6. 11.3 12.7 20.4 0 14.9 - 44.6 41.9 57.5 54.2 57.7 64.2 68.3 70.7 42.5 58.8 60.8 61.7 55.4 73. 70.6 77.7 - 51.6 54.4 0 0 0 0.4 2.6 7.7 0 2.1 0 0 0 8.3 0 8. - 19.3 14.3 45.0 47.6 47.9 47.1 57.6 59.9 39.6 42.6 44.7 48.9 48.3 59.6 60.5 62.8 - 48 47. 7.0 8.1 7.2 10.5 21.4 24.4 5.2 10.6 10.1 13.1 13.1 27.9 13.2 27.4 34.7 34.5 29.4 C. The Evaluation of Some Medical Models Currently, we evaluated several smaller-scale open source medical models on the HealthBench Hard benchmark with GPT-4.1 as the evaluation model and the result is presented in Tab. 6. We observe that these models perform poorly on this open-ended benchmark, result we attribute to their fine-tuning and reinforcement learning data, which consists predominantly of structured question-answer formats. However, this poor performance does not imply that incorporating medical knowledge yields no benefit. In fact, our experiments demonstrate that the integration of medical knowledge in the post-training phase, such as through supervised fine-tuning, provides the model with significantly greater capacity for exploration when addressing open-ended problems. Table 6. For the evaluation of these open-source medical models, all assessments were performed using GPT-4.1 as the judge, methodology selected to ensure alignment with the evaluation standards set by OpenAI. Models Emergency referrals Context seeking Global health Health data tasks Communication Hedging Response depth Accuracy Completeness Communicat. quality Context awareness Instruction following Total Score By Theme By Axis m1-7B-23K (Huang et al., 2025) HuatuoGPT-o1-7B (Chen et al., 2024) AlphaMed-7B (Liu et al., 2025a) HuatuoGPT-o1-8B (Chen et al., 2024) MedReason-8B (Wu et al., 2025) 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.1 8.9 6.2 7.9 5.5 0 0 0 0 0 45.5 47 45.1 51 25.1 0 0 0 0 0 31.7 32.6 31.9 26.8 15. 0 0 0 0 0 D. Details of Some Parameters Settings in Training and Evaluation Processes To ensure the reproducibility of the ORBIT models training and evaluation, we provide detailed breakdown of the parameters used. It is crucial to note that because we assess performance on an open-ended question benchmark, the final scores are highly sensitive to the specific model parameters and the evaluation protocol employed. 15 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training D.1. Evaluation Settings We must first clarify that our evaluation framework is built on the original source code from (Arora et al., 2025). We have subsequently adapted this evaluation framework to meet our specific research requirements, with key enhancements including the implementation of framework for batch evaluation of locally hosted models. The complete source code will be made publicly available in our official open-source repository. Recognizing that the reproducibility of evaluations for open-ended questions is critically dependent on the models generation parameters, we have detailed our specific configurations in Tab. 7. To enhance the evaluation stability, low temperature of 0.1 was used for the Qwen series and other selected open-source models. For all other models, the generation parameters were set to align with those specified in the official HealthBench protocol. Table 7. Evaluation Model Parameter Settings Models Temperature top-p max token API Type GPT-4.1 Claude series Qwen series Medical domain model ORBIT (ours) 0.5 0.5 0.1 0.1 0.1 0.9 0.9 0.9 2048 2048 2048"
        },
        {
            "title": "API model\nAPI model\nLocal model\nLocal model\nLocal model",
            "content": "To clarify the terminology in the accompanying table: API Model denotes closed-source models that were evaluated via their respective APIs, whereas Local Model indicates open-source models that were deployed and evaluated on our local infrastructure. It is critical to note that, to maintain consistent methodology, the fine-grained scoring for all evaluation results presented in the main text was conducted uniformly by single tool: GPT-4.1 (Achiam et al., 2023). E. Case Study 16 ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training Figure 6. The case study for Qwen3-4B-Instruct and our Qwen3-4B-ORBIT for the input medicine consultation dialogue problem."
        }
    ],
    "affiliations": [
        "Department of Computing, University of Hong Kong Polytechnic University, Hong Kong, China",
        "Department of Control Science and Engineering, Zhejiang University",
        "InfiX.ai"
    ]
}