{
    "paper_title": "Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences",
    "authors": [
        "Julius Pesonen",
        "Arno Solin",
        "Eija Honkavaara"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D object localisation based on a sequence of camera measurements is essential for safety-critical surveillance tasks, such as drone-based wildfire monitoring. Localisation of objects detected with a camera can typically be solved with dense depth estimation or 3D scene reconstruction. However, in the context of distant objects or tasks limited by the amount of available computational resources, neither solution is feasible. In this paper, we show that the task can be solved using particle filters for both single and multiple target scenarios. The method was studied using a 3D simulation and a drone-based image segmentation sequence with global navigation satellite system (GNSS)-based camera pose estimates. The results showed that a particle filter can be used to solve practical localisation tasks based on camera poses and image segments in these situations where other solutions fail. The particle filter is independent of the detection method, making it flexible for new tasks. The study also demonstrates that drone-based wildfire monitoring can be conducted using the proposed method paired with a pre-existing image segmentation model."
        },
        {
            "title": "Start",
            "content": "Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences Julius Pesonen1,2, Arno Solin2, and Eija Honkavaara1 5 2 0 2 5 2 ] . [ 1 6 0 9 0 2 . 9 0 5 2 : r Abstract 3D object localisation based on sequence of camera measurements is essential for safety-critical surveillance tasks, such as drone-based wildfire monitoring. Localisation of objects detected with camera can typically be solved with dense depth estimation or 3D scene reconstruction. However, in the context of distant objects or tasks limited by the amount of available computational resources, neither solution is feasible. In this paper, we show that the task can be solved using particle filters for both single and multiple target scenarios. The method was studied using 3D simulation and drone-based image segmentation sequence with global navigation satellite system (GNSS)-based camera pose estimates. The results showed that particle filter can be used to solve practical localisation tasks based on camera poses and image segments in these situations where other solutions fail. The particle filter is independent of the detection method, making it flexible for new tasks. The study also demonstrates that drone-based wildfire monitoring can be conducted using the proposed method paired with pre-existing image segmentation model. I. INTRODUCTION This work addresses the problem of locating distant objects from series of camera-based detections from known locations and orientations. At glance, the problem of locating target objects based on multiperspective imagery seems fairly well-addressed. The earlier proposed methods typically operate as optimisation problems where the object locations of individual keypoints are determined using set of images and known camera poses. Alternatively, full scene 3D reconstruction or dense depth estimation methods have been used for similar problems. However, the task at hand, presented in this work, is specific to far-away objects which have been detected with separate image segmentation models, such as neural networks which inherently produce noisy segments. Other sources of noise occur from possible target object dynamics and the camera-pose estimation methods. In addition, due to the nature of the target detection models, direct correspondence between features from consecutive frames does not hold. The motivation for this work originates from drone-based wildfire detection, in which the position of the drone-carried This research was funded by the Research Council of Finland within the projects DRONE4TREE - Autonomous drone solutions for single treebased forest management (decision no. 359404) and Fireman (decision no. 346710) and with grants (no. 339730 and 362408). The FireMan project is funded under the EUs Recovery and Resilience Facility, that promotes the green and digital transitions through research. 1Julius Pesonen and Eija Honkavaara are with the Department of Photogrammetry and Remote Sensing, Finnish Geospatial Research Institute, National Land Survey of Finland, Espoo, Finland julius.pesonen@nls.fi 2Julius Pesonen and Arno Solin are with the Department of Computer Science, Aalto University, Espoo, Finland Moving camera Estimated positions Fig. 1. We propose hybrid approach for localising distant objects/events (such as wildfire smoke) from sequences of frames and GNSS-estimated poses from moving RGB camera (example frames with masked smoke shown on top-right). camera is estimated using GNSS measurements and known dynamics of the drone camera setup. Our previous work showed that wildfire smoke can be detected from almost ten kilometres away using only drone-carried resources [1]. Pairing the segmentation model with lightweight target localisation method enables fully on-board wildfire detection and localisation. This enables the wildfire detection system to be used in areas of poor telecommunication where cloudbased computing can not be relied on. The sketch in Figure 1 illustrates use case of UAV scanning for wildfires (sketch by DALL-E 3) with masked RGB images (real data). To combat the noise induced by the camera and image observation systems, we focus on Bayesian filters. They enable reliable modelling of various noise sources, and with particle filters in particular, more complex observation and target dynamics can be modelled. The use of particle filters in camera-based 3D localisation literature has been limited. Specifically, for locating distant objects from moving camera, there appears to be lack of extensive research. This work extends the literature on filter-based target localisation from moving camera by addressing object the problem of distant objects detected by separate models. We propose using particle filters to iteratively improve the localisation and uncertainty estimation of the target objects 3D position. Using both simulations and drone-captured image sequence with GNSS-estimated camera positions, we show that the method can locate multiple target objects from moving camera segmentation sequences despite the various sources of noise. II. RELATED WORK The task at hand can be thought of as form of camerabased multiview 3D reconstruction. The earlier developments of 3D object reconstruction from multiview imagery have been well documented by Hartley and Zisserman [2]. These methods relied on point correspondence, well-estimated camera parameters, and bundle adjustments. Later developments, such as COLMAP [3] and 3D Gaussian splatting [4], have greatly improved the structure from motion map generation and 3D modelling, respectively. Unfortunately, the methods still require finding large number of point correspondences between frames, making them computationally heavy for iterative real-time localisation using edge devices, which is desirable in, for example, aerial robotic applications. The 3D reconstruction methods have been poorly studied in the context of smaller and more distant objects. In such situations, any noise in the camera pose estimation causes larger errors in the final reconstruction. This suggests that Bayesian filtering-based solutions could offer feasible solutions. Besides the camera pose estimation errors, typical modern camera-based sensing solutions leverage neural network models for detecting key features, which complicates keypoint matching between frames. Even though the observed object, in this study, is assumed to be static, the problem relates to object tracking, as were interested in obtaining position for an object in 3D space from set of camera observations. Like 3D reconstruction, camera-based object tracking has been studied since the dawn of time, and survey covering the earlier developments has been written by Yilmaz et al. [5]. More recent surveys also consider neural network-based approaches and list large number of methods, evaluation metrics, and datasets for the study of camera-based object tracking [6], [7], [8] even for real-time scenarios [9]. However, even these more recent surveys only consider metrics based on image-based labels, failing to consider the 3D localisation errors. This limitation also applies to survey on video object segmentation and tracking [10]. Multitarget detection and tracking from monocular camera has also been studied specifically in the context of drones, but again, the tracking accuracy has only been evaluated in the camera plane [11]. Still, the 3D object tracking problem is not entirely devoid of resources. One example where the problem has been studied is in autonomous driving, in which benchmarks and datasets such as the KITTI dataset [12] and NuScenes [13] have enabled major progress. The autonomous driving datasets, however, typically include other sensor modalities such as lidar or stereo cameras, which are not feasible to be used in more distant localisation scenarios. Some studies consider only the monocular camera view [14], but the focus on nearby objects still makes the problem statement very different. Other examples of works in the 3D space include tracking of people in small-scale indoor scenes with static camera views, also using particle filters [15], [16], and small object tracking with single static 360 field of view (FOV) camera [17]. Even though its difficult to find direct correspondence for the problem in camera-focused literature, object localisation and tracking in 3D space is largely studied problem in the context of other sensor modalities, particularly radars. Different types of multitarget tracking algorithms are well covered by survey by Vo et al. [18] and later survey focusing on particle filter-based methods by Wang et al. [19]. While new algorithms have been proposed since these surveys, they cover the basics in high detail and provide good overview of the different approaches. The use of particle filters for tracking multiple targets has been proposed at least as early as 2002 by Hue et al. [20], and for cameras in the 2D camera plane, particle filter has been suggested for the problem as early as 2005 by Yang et al. [21]. Ever since, the methods have evolved, particularly in the way that multiple objects are represented. survey by Rao et al has covered techniques used in particle filters for tracking of visual targets [22]. Examples of multitarget presentation methods include multiple filter methods, which were originally introduced for the problem of high dimensionality [23]. The multiple filter methods split the task into multiple single-target filtering tasks. Using multiple filters introduces the problem of assigning the correct observations to each filter. This has been solved, for example, with the symbiotic particle filter [24] or by estimating unknown parameters using fused information from multiple particle filters [25]. Another solution for multiple targets is presented by finite-set statistics developed by Ron Mahler [26], which are used for the PHD (probability hypothesis density) particle filters. These have also been applied to 2D multitarget tracking in camera images [27]. Tracking multiple objects using multiple sensors has also been the focus of many of the more recent works, also using random finite set presentations, as covered in survey by Da et al. [28]. In the multisensor context, the use of cameras has been slightly more common as well. However, in the multicamera scenario, the problem is inherently different from that of single camera, due to the possibility of using the discrepancies between the multiple sensor locations. Besides, the metrics in the literature have been focused on the 2D labels as with individual cameras [29]. Filter-based methods have also been applied to dronebased tasks, and benchmarks have been presented for localising human or vehicle targets based on separate views from multiple drones [30], [31]. The work by Liu and Zhang [32] presents very similar task to the one at hand, where objects such as cars were tracked and localised from drone-captured imagery with combination of neural network and particle filter. However, the localisation was simplified by assuming flat ground and triangular relationship between the ground plane, the target, and the drone-based camera. In addition, the scale of the task is still very different from what is of interest in this study, meaning the range of multiple kilometres. III. MATERIALS AND METHODS A. Problem Setup To study the problem in detail, we defined simulation in which target object is projected onto camera plane using pinhole camera model. The simulation is flexible and allows quick testing of different targets, distances, and noise variations. The simulated target was defined as three-dimensional cube for simplicity. The cube was defined by its eight corners, which were used to project the cube into the simulated two-dimensional camera image, where single point projection was computed as = KMx, (1) where is the projected point in homogeneous coordinates, is the intrinsic camera matrix, is the extrinsic camera matrix, and is the original 3D point in homogeneous coordinates. The homogeneous coordinates use an extra dimension to simplify the matrix operations, such that = (y1, y2, y3) and = (x1, x2, x3, x4), where the pixel coordinates of the projection are: ˆy = (y1/y3, y2/y3), and the corresponding 3D world coordinates are: ˆx = (x1/x4, x2/x4, x3/x4). (2) (3) The intrinsic and extrinsic camera matrices, and M, describe the physical parameters of the camera. is 33 matrix: = fx 0 0 fy 0 cx cy 1 , (4) where fx and fy describe the focal length of the camera in terms of pixels, and cx and cy describe the position of the principal point of the camera. The extrinsic matrix is 34 matrix consisting of 33 rotation matrix, R, and 31 translation component, t, corresponding to each 3D translation axis: = (cid:2)R t(cid:3) = r11 r21 r31 r12 r22 r32 r13 r23 . tx ty tz (5) After the projection, the image coordinates were discretised to integer values corresponding to camera pixel coordinates. This was essential to simulate the loss of information that results from using both cameras and segmentation models with limited number of pixels, in distant observation scenario. To produce the actual simulated segment from the pixel projections, convex hull was used to obtain the area that covers the whole view of the cube in the camera frame. This convex hull then represented the perfect segmentation result. To simulate the noise caused by non-perfect camera pose estimation, we injected the camera extrinsics corresponding to each frame with random amount of translation noise, denoted νt, in each coordinate axis and rotated the matrix over each axis separately, again with random amount, denoted Nr. This resulted in noisy extrinsic matrix Mν, defined as Mν = (cid:2)NrxNryNrzR + νt rν12 rν22 rν32 rν11 rν21 rν31 = (cid:3) rν13 rν23 rν33 , tx + νtx ty + νty tz + νtz (6) where Nrx, Nry, and Nrz correspond to the separately drawn rotation noise matrices, rνn to each resulting noisy rotation element, and νtx, νty, and νtz to each separately drawn translation noise element. The false positives in the simulated segments were generated by defining random rectangular sections of the image to the real positive segand setting these pixels equal ments. These false-positive rectangles were defined by their height and width in pixel dimensions. They were generated randomly for each image, based on false positive rate ρF and the maximum number of false positives axF . Correspondingly, the false positives were removed from the following frames based on false positive dismissal rate δρF . Unless the false positive was removed, it was kept for the subsequent images. The false negatives were simulated in two ways. First, by simply setting all the target segment pixels to zero, equalling the value of the background or by setting only some randomly selected section of the target pixels to zero. While the false positives and partial false negatives were kept in consequent frames until dismissal defined by another random draw, the false negatives were generated independently for each frame. Thus, the false negative appearance and disappearance were defined by the false negative rate ρF , by the partial false negative rate ρP , and the partial false negative dismissal rate δρP . B. Single Target Particle Filter We defined the filter such that the particles were initialised based on traditional least squares estimate between the means of two observations, after threshold number, τminobs, of consecutive positive observations. a) Initialisation: was done by defining the particles as Monte-Carlo samples of three-dimensional Gaussian distribution, scaled in each dimension based on the distance between the camera and the observed target. Thus, the initial distribution took the form πinit (minit, Pinit), (7) where minit is the middle point between the two camera rays, r1 and r2, computed using the inverse operation of the camera matrices, and the middle point of each observation in the camera frame. Pinit is the covariance of the initial distribution, which was set at constant that was determined by adjusting the model such that the initial distribution would be very likely to contain particles located at the target position while simultaneously remaining dense enough to not cause issues in convergence. b) Prediction step: was first taken after the initialisation. In it, the particle distribution was updated based on new observations. At each update step, the particles were injected with independent Gaussian noise, based on the distance between the point and the camera and an optimised constant, which meant that after each observation step, the full distribution could be expressed as combination of three-dimensional Gaussians, where is the number of particles. c) Update step: required that positive pixels were observed. Given this, the pixel distances between the projected particles and the positive observations were compared. Each particle, projected inside the camera frame, was assigned weight, ωp, relative to the distance from the positive pixels such that: ωp = min((obspproj )2), (8) where obs is an array of the positive observations and pproj is the projected particle in the pixel coordinates. d) Resampling: took place after weighing each particle. The resampling was performed by using the weights as the probabilities of drawing the corresponding points from the set of particles. The same number of particles as the previous set was drawn in this manner, resulting in new set where the highest weighted particles were repeated multiple times. This resulting distribution was then used in the next prediction step, where particles were first updated randomly, effectively doing Gaussian kernel smoothing for the distribution, before projecting them again to the camera frame. For the study, we used the bootstrap version of the particle filter, due to better initial results, meaning that the resampling was done at every observation. C. Multitarget Extension Extending the particle filter for multiple targets is possible using various formulations as introduced in Section II. In this work, we solved the multiple target scenario using multiple particle filters. We opted for this particular method to avoid using an unnecessarily large distribution for the particle filter prediction steps, and instead, used larger number of more focused distributions of separate particle filters. In practice, the multiple target localisation was realised by computing the pixels at each observation that were located beyond dynamic threshold, θpo, from the nearest projected particles. Given that these out-of-distribution (OOD) observations appeared at certain number of consecutive observation steps, τminobs, new particle filter was initialised based on the OOD pixels. The separate particle filters were also dismissed in case no positive observations were found for similar threshold number of observation steps, nθdm. D. Metrics The performance of the proposed method was quantified using two measures, the root mean square error (RMSE) and negative log predictive density (NLPD), of which the RMSE was computed between the predicted particles, pi, and the mean of the target location, mt: (cid:114) (cid:80)np RMSE = , (9) i=1(pi mt)2 where np is the number of particles. This is equal to the Euclidean distance between the predicted and ground truth means. The NLPD was computed based on the likelihood that the target mean would be drawn from the distribution of particles as if they presented normal distribution, such that: (cid:80)np i=1 log p(mt p) NLPD = . (10) np The RMSE presents an intuitive linear measure for the distance between the predicted target centre point and the true target centre point, while the NLPD better takes into account the uncertainty presented by the variation of the particles. For both metrics, we report the minimum obtained value during the test, corresponding to the converged filter estimate, and for the RMSE, we also report the mean between 200 and 1000 metres of camera translation. The first 200 metres were dismissed due to being mostly dependent on the initialisation, and 1000 metres was the maximum translation in the experiments. For all results, the metrics were computed over an average of ten simulations to reduce noise caused by the randomness of the experiments. E. Empirical Data To validate that the performance of the proposed method holds for real-world applications, we tested the system with drone-captured video sequence of telecommunication mast. The position of the drone was recorded using differential GNSS, and the geolocation of the target object was known. To capture the sequence, we used DJI Matrice 350, equipped with an AR0234 camera and an Applanix APX-15 UAV GNSS. An NVIDIA Jetson Orin NX was used to record the data. We performed no calibration for the camera or the GNSS. This means that for the camera, only the parameters provided by the camera manufacturer were used [33]. The camera was used to record at full HD (1080 by 1920 pixels) resolution with horizontal FOV of 90 and vertical FOV of 50.625. This means that the intrinsic parameters were set as fx = 1200, fy = 1200, cx = 960, and cy = 540. The lens was assumed to cause no distortions. The GNSS antenna was mounted approximately 30 centimetres away from the camera with an external IMU attached to the camera. Neither the boresight nor the lever arm of the mounting was taken into account for the experiments. This means that the camera pose estimation had at least systematic error in the range of tens of centimetres in addition to any sensor-caused errors. The manufacturer reported RMSE for the position is 0.02 to 0.05 metres, for the roll and pitch 0.025 degrees, and for the heading 0.080 degrees [34]. The images were post-processed using horizontal Sobel operation with image erosion and dilation. This way, we obtained sequence of binary segments with known target location and estimated camera positions, corresponding to TABLE SIMULATED FILTER POSITION ESTIMATION RESULTS WITH DIFFERENT NOISE AND TARGET CONFIGURATIONS. NT Max νrot () Max νt (m) ρF δρF Max ρF ρP δρP RMSE min (m) RMSE 2001k (m) NLPD min 1 1 1 1 1 3 3 3 3 3 0 0.1 0.1 0.1 0.1 0 0.1 0.1 0.1 0.1 0 0.5 0.5 0.5 0.5 0 0.5 0.5 0.5 0.5 0 0 0.1 0.1 0.1 0 0 0.1 0.1 0. - - 0.2 0.2 0.2 0 0 0.2 0.2 0.2 0 0 3 3 3 0 0 3 3 3 0 0 0 0.1 0.1 0 0 0 0.1 0.1 0 0 0 0 0.1 0 0 0 0 0.1 - - - - 0.2 - - - - 0.2 37.81 36.93 47.54 52.44 80.00 171.56 158.25 213.41 231.05 265. 140.57 141.04 168.19 168.19 198.03 264.87 239.19 296.38 361.40 484.00 14.42 14.31 14.44 14.44 18.10 15.80 16.79 16.99 46.17 25.74 drone-based wildfire localisation task. The known target geolocation also enabled measuring the quality of the localisation in ground coordinates. IV. EXPERIMENTS AND RESULTS A. Simulation Simulation offers possibility to study the method extensively by enabling testing in an unlimited number of different scenarios varying by, for example, the camera trajectories, noise levels, and observation distances. Here, the simulation results are presented in an order of increasing complexity. Since the number of possible variations is infinite, we tried to limit the results to scenarios which could offer the most insight into the methods performance in the expected realworld tasks as well as its robustness to the different noise sources. The simulation results are presented in Table I. simulated observation the optimal and simplest The camera intrinsic parameters were set as fx = 1200, fy = 1200, cx = 960, and cy = 540, to imitate the camera used in the empirical test. The discretisation caused by the camera pixels was taken into account in all simulations. perpendicular trajectory The target observation presents this presents an exceedingly scenario. Without noise, to confirm optimal situation, and we mainly used it that the filter performs as expected and to analyse the minimum uncertainty and errors which could theoretically be obtained when locating visual targets using the particle filter. We performed these tests with single 100 by 100 by 100 metre target, with the target position in metres, tp = (x, y, z) = (500, 200, 2000). The trajectory of the camera positions in metres, cp, started from cp = (0, 0, 0) and continued linearly to cp = (1000, 0, 0). The camera angle remained stationary in the simulation, and it was defined such that when the target and camera positions x-coordinates were equal, the target was projected in the horizontal middle axis of the camera frame. The pitch angle of the camera was set such that the horizon was in the middle of the camera frames vertical axis. Even with perfectly noiseless segmentation and an optimal trajectory, the camera requires some translation for the position estimate to converge to the correct position. However, its worth noting that the range of RMSEs is still only between approximately 15% and 2% relative error in the target position estimate for target at two-kilometre distance. The main simulation results are collected in Table with the corresponding simulation experiments explained in the following paragraphs. The convergence of the filter in the corresponding experiments with single and multiple targets are visualised in Figure 2 and Figure 3, respectively. a) Multiple targets: were also simulated with the simple linear trajectory. In this scenario, we placed three targets in positions tp1 = (500, 200, 2000) , tp2 = (750, 200, 5000), and tp3 = (375, 200, 1000). The targets were placed such that Target 1 appeared at the start of the simulation, Target 2 after 200 metres of camera translation, and Target 3 after 500 metres of camera translation. Each target stayed visible for the rest of the simulation. The metrics for the multitarget simulation were measured by assigning the filter to the target that was nearest to the mean of the particles and computing the mean of each filters metric. The results are shown for the different noise scenarios in Table and Figure 3. In addition, the simulation is visualised in the supplementary video. The multitarget tests showed that with multiple particle filters, the method can easily be extended for locating multiple separate targets, even when the segmentation method only produces binary segments. b) Noisy camera pose: was used as the first noise addition to the simulations. The camera pose noise was included in all of the more complex simulations, as it was assumed that it appears the most consistently in any realworld application due to being the most dependent on real sensor noise. The noise only made the model converge slightly slower. c) False positive segments: are inherently easy to handle with the particle filter, as after convergence, any far false positives will not affect the filter updates. The biggest threat caused by false positives is during the initialisation step, where if false positive appears during one of the frames that are used for initialisation, the single error alone can cause the initial distribution to be off by kilometres. However, it can be accounted for by adjusting the size of the initial distribution. Another scenario where the false positives have negative effect is when they appear connected or only few pixels away from true positive target. In those 0.0 125.0 250.0 375.0 500.0 625.0 750.0 875.0 Fig. 2. Single target simulation results. From top to bottom: Simulated camera translation from the start of the sequence, noiseless single target simulation sample frames, fully noisy simulation samples, RMSEs of the single target simulation experiments over the camera translation. The lower noise labels indicate addition to the above ones as per the order of Table I. 0.0 125.0 250.0 375.0 500.0 625.0 750.0 875.0 Fig. 3. Multitarget simulation results. From top to bottom: Simulated camera translation from the start of the sequence, noiseless multitarget simulation sample frames, fully noisy simulation samples, RMSEs of the single target simulation experiments over the camera translation. The red vertical lines mark where the new targets appeared. The lower noise labels indicate addition to the above ones as per the order of Table I. situations, the filter will produce erroneous updates, but given that the amount of noise is sensible, these only cause the filter to produce momentary errors and converge slightly slower. d) False negatives: affecting all the target pixels, could quite easily be taken into account by adjusting the threshold after how many negative observations the filter would be dismissed. This means that false negatives would not affect the filter but only delay its convergence by as many time steps as the target would remain unobserved. Thus, the only negative effect this type of noise has is on the convergence of the filter, which is directly proportional to the ratio of fully false negative frames, as during those frames, the filter is not updated at all. In some scenarios, the false negatives even improved the convergence as they added variation to the length of the camera translation between updates. In the multitarget scenario, however, it appears that the full false negatives caused major issues. This might have been because they cause all the targets to disappear simultaneously, making it significantly harder to distinguish the targets from each other. The false negatives also caused filters to disappear more often in the multitarget scenario. e) Partial false negatives: caused the most negative effects on the filter after the initialisation. In situations where part of the true positive pixels were covered for multiple consecutive frames, the filter started converging towards the wrong position, significantly reducing the target position estimation accuracy. This highlights that the model performs the best when used alongside detection or segmentation model that can capture as many of the true positive pixels of the target object as possible. B. Parameter Optimisation and Recommendations Through trial and error with various experiment configurations, we decided on reliable set of default parameters. The chosen parameters were: τminobs = 5, SDinit = 1000, Ts = 10 m, Np = 100 000, θpo = 1 SD (standard 0.0 15.0 30.0 45.0 60.0 Fig. 4. Empirical sequence results. From top to bottom: Time from the start of the sequence, drone-captured RGB image, segments, mean RMSE (black) and the 95% confidence interval (grey) of the filter over the translation from the empirical data sequence. The segment has been dilated for an additional ten steps for visualisation. deviation), nθdm = 5, and nθf use = 5. These settings enabled the filters to locate multiple targets at different distances under realistic amounts of each type of noise. All the simulation results were obtained with these parameters, and for the real data tests, only the translation step size Ts and the number of particles Np were adjusted. Each of the parameters is adjustable, and their effects are fairly intuitive. The Initialisation, fusion, and dismissal parameters adjust how the model behaves when targets appear or disappear, both intentionally or due to misdetections. With better-performing segmentation or detection models, the parameters can be set to lower values to obtain faster corrections, while in the presence of larger amounts of segmentation noise, the values should be higher to avoid generating false positive target locations or removing true target locations. With smaller Ts, depending on the segmentation frame rate and camera translation speed, the thresholds can also be set higher, to correspond to larger total translation for each behaviour. However, setting the Ts value too small can negatively affect the convergence of the filters. C. Empirical Data Visualised results from the empirical test are shown in Figure 4 and in the supplementary video. The Sobel and morphological operations that were used only separated the mast from the sky pixels, enabling the filter to locate the top part of the mast in 3D. The lower part of the mast had darker background, making it less visible. Thus, the accuracy was measured only in the two ground plane dimensions. As shown in Figure 4, the segmentation method also caused some false positives elsewhere in the image. For this test, the translation parameter was adjusted to approximately Ts = 2 m, as the test sequence was fairly short with maximum translation of only 250 metres. The target object was also closer to the filming drone than in the simulations, at approximately 700 metres away from the sequence starting position. The number of particles was increased to million, as with the 100 000 particles, the results were unstable, presumably due to the target object being very thin. With the chosen Ts, the filter was updated 140 times, achieving mean RMSE of 92.00 metres after 50 metres of translation, estimating the post-convergence accuracy over ten different random seeds. The minimum mean RMSE from the sequence was 76.88 metres. The test confirmed that the particle filter is capable of localising target object from real camera pose and segmentation sequence despite significant noise caused by both the camera pose estimation and segmentation methods. We analysed that the systematic error was caused by the particle filters poor ability to shift the predicted distribution closer or further from the camera in situations with poor initialisation, presumably worsened by the thin shape of the target object. Overall, the test was success as the < 15% relative RMSE suggests that the method is suitable for practical systems with minimal physical calibration. V. CONCLUSIONS Both the simulated and empirical tests showed that the task of localising distant objects in 3D using segments and known poses from moving camera can be solved using particle filter. The results so far were limited to small offline dataset and simulations. Next steps for deploying the method on real applications require extending the simulation tests to an even larger variety of scenarios, including more realworld data testing, and implementing the algorithm on an embedded sensing system. In addition, defining and studying model behaviour in more complex multiple target scenarios is required, such as scenarios with disappearing, fusing or separating targets. Overall, the task presented in the study has very little representation in prior literature. This study shows that existing Bayesian filtering methods pose one solution, and that the presented simulation method can be used to study the alternatives effectively. Finally, the study implies that for the task of drone-based wildfire detection, the presented particle filter paired with pre-existing segmentation model can solve the issue of finding wildfire geolocations at detection time. VI. ACKNOWLEDGEMENTS We thank Teemu Hakala for setting up the instrumentation for the empirical test data collection and Roope Nasi for conducting the data collection flights."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Pesonen, T. Hakala, V. Karjalainen, N. Koivumaki, L. Markelin, A.-M. Raita-Hakola, J. Suomalainen, I. Polonen, and E. Honkavaara, Detecting wildfires on UAVs with real-time segmentation trained by larger teacher models, in Proceedings of the Winter Conference on Applications of Computer Vision (WACV), February 2025, pp. 5166 5176. [2] R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer Cambridge University Press, ISBN: 0521540518, Vision, 2nd ed. 2004. [3] J. L. Schonberger and J.-M. Frahm, Structure-from-motion revisited, in Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [4] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis, 3D gaussian splatting for real-time radiance field rendering, ACM Transactions on Graphics, vol. 42, no. 4, [Online]. Available: https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/ July 2023. [5] A. Yilmaz, O. Javed, and M. Shah, Object tracking: survey, ACM computing surveys (CSUR), vol. 38, no. 4, pp. 13es, 2006. [6] G. Ciaparrone, F. Luque Sanchez, S. Tabik, L. Troiano, R. Tagliaferri, and F. Herrera, Deep learning in video multiobject tracking: survey, Neurocomputing, vol. 381, pp. 6188, 2020. [Online]. Available: https://www.sciencedirect.com/science/ article/pii/S0925231219315966 [7] F. Chen, X. Wang, Y. Zhao, S. Lv, and X. Niu, Visual object tracking: survey, Computer Vision and Image Understanding, vol. 222, p. 103508, 2022. [Online]. Available: https://www.sciencedirect. com/science/article/pii/S1077314222001011 [8] M. A. Awal, M. A. R. Refat, F. Naznin, and M. Z. Islam, particle filter based visual object tracking: systematic review of current trends and research challenges. International Journal of Advanced Computer Science & Applications, vol. 14, no. 11, 2023. [9] L. Kalake, W. Wan, and L. Hou, Analysis based on recent deep tracking: learning approaches applied in real-time multi-object review, IEEE Access, vol. 9, pp. 32 65032 671, 2021. [10] R. Yao, G. Lin, S. Xia, J. Zhao, and Y. Zhou, Video object segmentation and tracking: survey, ACM Transactions on Intelligent Systems and Technology (TIST), vol. 11, no. 4, pp. 147, 2020. [11] J. Li, D. H. Ye, T. Chung, M. Kolsch, J. Wachs, and C. Bouman, Multi-target detection and tracking from single camera in unmanned aerial vehicles (UAVs), in 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016, pp. 49924997. [12] A. Geiger, P. Lenz, and R. Urtasun, Are we ready for autonomous driving? the KITTI vision benchmark suite, in Conference on Computer Vision and Pattern Recognition (CVPR), 2012. [13] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, nuScenes: multimodal dataset for autonomous driving, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 62111 631. [14] S. Wang, Y. Liu, T. Wang, Y. Li, and X. Zhang, Exploring objectcentric temporal modeling for efficient multi-view 3D object detection, arXiv preprint arXiv:2303.11926, 2023. [15] A. Lopez, C. Canton-Ferrer, and J. R. Casas, Multi-person 3D tracking with particle filters on voxels, in 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP07, vol. 1. IEEE, 2007, pp. I913. [16] Y. Salih and A. S. Malik, 3D tracking using particle filters, in 2011 IEEE International Instrumentation and Measurement Technology Conference. IEEE, 2011, pp. 14. [17] M. Taiana, J. Gaspar, J. Nascimento, A. Bernardino, and P. Lima, 3D tracking by catadioptric vision based on particle filters, in Robot Soccer World Cup. Springer, 2007, pp. 7788. [18] B.-n. Vo, M. Mallick, Y. Bar-Shalom, S. Coraluppi, R. Osborne, R. Mahler, and B.-t. Vo, Multitarget tracking, Wiley encyclopedia of electrical and electronics engineering, no. 2015, 2015. [19] X. Wang, T. Li, S. Sun, and J. M. Corchado, survey of recent advances in particle filters and remaining challenges for multitarget tracking, Sensors, vol. 17, no. 12, p. 2707, 2017. [20] C. Hue, J.-P. L. Cadre, and P. Perez, Tracking multiple objects with particle filtering, IEEE Transactions on Aerospace and Electronic Systems, vol. 38, pp. 791812, 2002. [Online]. Available: https://api.semanticscholar.org/CorpusID:14175805 [21] C. Yang, R. Duraiswami, and L. Davis, Fast multiple object tracking via hierarchical particle filter, in Tenth IEEE International Conference on Computer Vision (ICCV05) Volume 1, vol. 1. IEEE, 2005, pp. 212219. [22] G. M. Rao and C. Satyanarayana, Visual object target tracking using particle filter: survey, International Journal of Image, Graphics and Signal Processing, vol. 5, no. 6, p. 1250, 2013. [23] P. M. Djuric, T. Lu, and M. F. Bugallo, Multiple particle filtering, in 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP07, vol. 3. IEEE, 2007, pp. III1181. filtering, in 2010 IEEE Aerospace Conference. [24] M. F. Bugallo and P. M. Djuric, Target tracking by symbiotic particle IEEE, 2010, pp. 17. [25] X. Zhao, M. Iloska, Y. El-Laham, and M. F. Bugallo, Fusion of information in multiple particle filtering in the presence of unknown static parameters, in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 15. [26] R. Mahler, Statistical multisource-multitarget information fusion. Artech, 2007. [27] E. Maggio, E. Piccardo, C. Regazzoni, and A. Cavallaro, Particle PHD filtering for multi-target visual tracking, in 2007 IEEE International Conference on Acoustics, Speech and Signal ProcessingICASSP07, vol. 1. IEEE, 2007, pp. I1101. [28] K. Da, T. Li, Y. Zhu, H. Fan, and Q. Fu, Recent advances in multisensor multitarget tracking using random finite set, Frontiers of Information Technology & Electronic Engineering, vol. 22, no. 1, pp. 524, 2021. [29] T. I. Amosa, P. Sebastian, L. I. Izhar, O. Ibrahim, L. S. Ayinla, A. A. Bahashwan, A. Bala, and Y. A. Samaila, Multi-camera multiobject tracking: review of current trends and future advances, Neurocomputing, vol. 552, p. 126558, 2023. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S0925231223006811 [30] P. Zhu, J. Zheng, D. Du, L. Wen, Y. Sun, and Q. Hu, Multidrone based single object tracking with agent sharing network, IEEE Transactions on Circuits and Systems for Video Technology, 2020. [31] Z. Liu, Y. Shang, T. Li, G. Chen, Y. Wang, Q. Hu, and P. Zhu, Robust multi-drone multi-target tracking to resolve target occlusion: benchmark, IEEE Transactions on Multimedia, vol. 25, pp. 1462 1476, 2023. [32] X. Liu and Z. Zhang, vision-based target detection, tracking, and positioning algorithm for unmanned aerial vehicle, Wireless Communications and Mobile Computing, vol. 2021, no. 1, p. 5565589, 2021. [33] Arducam, AR0234 - Arducam Wiki docs.arducam.com, https://docs.arducam.com/Nvidia-Jetson-Camera/Jetvariety-Camera/ AR0234/, 2022, [Accessed 04-09-2025]. [34] Applanix, Trimble APX UAV, https://applanix.trimble.com/en/ products/hardware/trimble-apx-uav, [Accessed 04-09-2025]."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Aalto University, Espoo, Finland",
        "Department of Photogrammetry and Remote Sensing, Finnish Geospatial Research Institute, National Land Survey of Finland, Espoo, Finland"
    ]
}