{
    "paper_title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
    "authors": [
        "Andrei Panferov",
        "Jiale Chen",
        "Soroush Tabesh",
        "Roberto L. Castro",
        "Mahdi Nikdan",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the \"optimal\" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations. We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the \"true\" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST."
        },
        {
            "title": "Start",
            "content": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations Andrei Panferov 1 Jiale Chen 1 Soroush Tabesh 1 Roberto L. Castro 1 Mahdi Nikdan 1 Dan Alistarh 1 2 5 2 0 2 7 ] . [ 1 3 0 0 5 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, recent study (Kumar et al., 2024) put the optimal bit-width at which models can be trained using QAT, while staying accuracycompetitive with standard FP16/BF16 precision, at 8-bits weights and activations. We advance this state-of-the-art via new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the true (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https: //github.com/IST-DASLab/QuEST. 1. Introduction The massive computational demands of large language models (LLMs), e.g. (Dubey et al., 2024), have made AI ef1ISTA 2Red Hat AI. Correspondence to: Andrei Panferov <andrei@panferov.org>, Dan Alistarh <dan.alistarh@ist.ac.at>. 1 Figure 1. The scaling law induced by QuEST when training Llamafamily models from 30 to 800M parameters on C4, with quantized weights and activations from 1 to 4 bits, in the 100 tokens/parameter regime (higher compression uses proportionally more data at fixed memory). QuEST allows for stable training at 1bit weights and activations (W1A1), and the QuEST W4A4 model is Pareto-dominant relative to BF16, with lower loss at lower size. ficiency critical challenge. One popular pathway to increased efficiency has been reducing numerical precision, usually done via post-training quantization (PTQ) methods for compressing weights (Frantar et al., 2022; Lin et al., 2024; Chee et al., 2024; Tseng et al., 2024) or both weights and activations (Ashkboos et al., 2023; 2024; Zhao et al., 2023). Quantizing both operands is necessary to leverage hardware support for low-precision multiplications, which extends down to 4-bit (NVIDIA, 2024). However, stateof-the-art PTQ methods are still far from recovering full accuracy for 4-bit precision (Ashkboos et al., 2024; Liu et al., 2024), leaving gap between computational support and achievable accuracy. One alternative is quantization-aware training (QAT) (Rastegari et al., 2016; Jacob et al., 2018) where models are trained from scratch with low-precision weights and activations on the forward pass, but with full-precision backward passoffering the potential for superior accuracy-vscompression trade-offs, as gradient optimization can correct compression errors. Despite promising results for weightonly quantization (Wang et al., 2023; Kaushal et al., 2024), it is currently not known whether QAT can produce accurate LLMs with low-bitwidth weights and activations. Here, the key metric is the Pareto-optimal frontier, i.e., the minimal representation size (or inference cost) for the model to Training Accurate LLMs with Low-Bit Weights and Activations achieve certain accuracy under fixed data or training budget. Recently, Kumar et al. (2024) identified 8-bit precision as Pareto-optimal for QAT methods on LLMs. Contribution. We present QuEST, new QAT method that brings the Pareto-optimal frontier to around 4-bit weights and activations and enables stable training at 1-bit precision for both operands. As shown in Figure 1, when data and compute are scaled proportionally to model size, QuEST can train models with 4-bit weights and activations that have superior accuracy relative to BF16 models almost 4x in size. We achieve this by re-thinking two key aspects of QAT methods: 1) the forward step, in which continuous-to-discrete tensor distribution fitting is performed on the forward pass, and 2) the backward step, in which gradient estimation is performed over the discrete representation. For the forward step, QuEST works by approximating the optimal continuous-to-discrete mapping by first applying normalizing Hadamard Transform, and then computing an MSEoptimal quantization for the resulting distribution. This replaces the prior learned normalization approaches (Choi et al., 2018; Bhalgat et al., 2020). The key remaining question is how to find an accurate gradient estimator over weight or activation tensor quantized as above. Here, prior work leverages the Straight-Through Estimator (STE) (Bengio et al., 2013), augmented with learnable components, e.g. (Bhalgat et al., 2020). We propose different approach called trust estimation, which seeks to minimize the difference between the true gradient (taken over high-precision weights) and its estimate taken over lower-precision weights and activations. To do this, trust estimator diminishes the importance of the gradient for some components depending on their quantization error on the forward step, following the intuition that entries with large errors lead to significant deviations in the gradient. Next, we focus on the following question: assuming that training computation is not limiting factor, what is the optimal precision in terms of accuracy-vs-model-size? To address this, we implement QuEST in Pytorch (Paszke et al., 2019) and train Llama-family models (Dubey et al., 2024) of up to 800M parameters on up to 80B tokens from the standard C4 dataset (Raffel et al., 2019), across precisions from INT1 to INT8. Results show that QuEST provides stable and accurate convergence across model sizes and precisions down to 1-bit weights and activations. This induces new scaling laws, which we study across model sizes in the large-data (100 tokens/parameter) regime. QuEST leads INT4 weights and activations to be Pareto-optimal in terms of accuracy at given model size and inference cost, suggesting that the limits of low-precision training are lower than previously thought. In addition, we provide GPU kernels showing that models produced by QuEST can be run efficiently on commodity hardware. 2. Background and Related Work Hubara et al. (2016) and Rastegari et al. (2016) were among the first to consider training neural networks with highlycompressed internal states, focusing primarily on weight compression. Later work focused on quantization-aware training (QAT) (Jacob et al., 2018; Choi et al., 2018; Esser et al., 2019; Bhalgat et al., 2020) in the form considered here, where the model weights and activations (i.e. the forward pass) are quantized, but the backward pass is performed in full-precision, using variants of the straight-through estimator (STE) (Bengio et al., 2013). (The variant where all states, including gradients, are quantized (Wortsman et al., 2023; Xi et al., 2024) is beyond the scope of this paper.) Broadly, QAT considers the problem of finding quantized projection over standard-precision tensor x, representing part of the weights or activations, minimizing output error. For symmetric uniform quantization, the projection onto the quantized tensor ˆx is defined as: ˆx = α (cid:22) clip(x, α) α (cid:25) , (1) where the clip function performs clamping operation over the value distribution for all values above the clipping parameter α, which also acts as scaling factor, normalizing values to to [1, 1], and the function rounds each value to its nearest quantization point, defined as uniform grid whose granularity depends on the number of available bits b. Most QAT methods propose to learn the factor α, for instance, via gradient-based optimization. For example, QAT methods usually keep standard-precision version of the weights; the STE gradient is computed over the quantized weights (cid:98)w, and then added to the full-precision accumulator, possibly also updating the clipping factor α. Recent work such as BitNet (Wang et al., 2023; Ma et al., 2024) and Spectra (Kaushal et al., 2024) showed that weightonly quantization is viable for smalland medium-scale LLMs. The concurrent work presents BitNet a4.8 (Wang et al., 2024), hybrid scheme that combines ternary weights with mixed 4and 8-bit activations, applied selectively to different matrices. In parallel, Kumar et al. (2024) investigated scaling laws for GPT-type models with quantized states, concluding that the Pareto-optimal point for current QAT methods is around 8-bit weights and activations. Prior work by Frantar et al. (2023); Jin et al. (2025) studied scaling laws specifically for sparse foundation models, establishing that the loss can be stably predicted across parameter and data scales when the model weights are sparse. Recently, Frantar et al. (2025) generalized these laws to unify both sparsity and quantization, allowing to compare the effective parameter count for these two types of representations. Our work focuses on improved training methods 2 Training Accurate LLMs with Low-Bit Weights and Activations for highly-compressed representations, leading to improved scaling laws relative to standard dense training, and can be applied to both sparsity and quantization. 3. QuEST Motivation. simple way of describing current QAT methods is that, given standard-precision tensor w, we first try to get an accurate discrete approximation (cid:98)w by optimizing parameters such as the clipping factor α in Equation 1 to minimize some loss target, such as the mean-square-error (MSE), and then rely on STE to estimate wL, the gradient over w, by (cid:98)wL, the gradient taken w.r.t. the quantized weights (cid:98)w. Yet, the difference between these two gradients, which correlates to the gap in optimization trajectory, could be unbounded, specifically because of large errors in small subset of entries. Instead, in this paper, we seek to minimize the difference between the true and discrete gradients, measured, e.g. as (cid:13) (cid:13) (cid:13) wL (cid:98)wL (cid:13) 2 (cid:13) (cid:13) . (2) Let us define the quantization error for each entry wk as errk = (cid:12) (cid:12) (cid:12). We can partition the weight indices based on whether the quantization error errk is smaller or larger than some trust factor threshold . Denote: (cid:12) wk ˆwk Ssmall = { : errk }, Slarge = { : errk > }. Then, the squared gradient difference in (2) decomposes as: (cid:88) kSsmall (cid:124) (wLk (cid:98)wLk)2 (cid:88) + (wLk (cid:98)wLk)2 . (cid:123)(cid:122) () kSlarge (cid:124) (cid:125) (cid:123)(cid:122) () (cid:125) Assuming that the loss is γ-smooth, the () small error term would be upper bounded by γ2w (cid:98)w2 2 γ2T 2Ssmall. Intuitively, this term is minimized in standard QAT methods distribution fitting step. Yet, distribution fitting does not address the large error term (): specifically, outlier entries clipped in the fitting step can lead to extremely large gradient estimation errors. QuEST takes this into account by balancing estimation errors due to minor but persistent quantization errors in (), with the significant outlier errors incorporated by term (). For this, we propose an efficient fitting mechanism that minimizes persistent errors, coupled with trust gradient estimator step aimed at bounding outlier errors. 3.1. Step 1: Distribution Fitting While optimizing the quantization grid to best fit the underlying tensor is core idea across all quantization methods, PTQ methods traditionally use more complex and computationally heavy approaches (Dettmers et al., 2024; Malinovskii et al., 2024). In contrast, QAT methods rely on backpropagation through the scaling factor for errorcorrection (Esser et al., 2019; Bhalgat et al., 2020) while performing re-fitting. To avoid backpropagation errors impacting the forward pass, we do not use backpropagation for distribution fitting. Instead, we start from the empirical observation that the distribution of weights and activations during LLM training is sub-Gaussian but with long tails (Dettmers et al., 2022; 2023). Gaussian Fitting. Specifically, we choose to optimize the grid to explicitly fit Gaussian distribution with the same parametrization as the empirical distribution of the underlying tensor x. Concretely, we use root mean square (RMS) normalization to first align the empirical distribution of with (0, 1) Gaussian distribution (Frantar et al., 2025). We then perform the projection operation with the scale α chosen to minimize the L2 error resulting from projecting (0, 1). Formally: (cid:98)x = α RMS(x) (cid:22) clip (x/RMS(x), α) α (cid:25) = := projα (x), where α := arg min αR EξN (0,1) (cid:13) (cid:13) (cid:13) (cid:13) ξ α (cid:22) clip(ξ, α) α (cid:25)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 If were Gaussianis the MSE-optimal scaling factor. distributed, this would produce an MSE-optimal projection. Hadamard Preprocessing. Yet, the natural distribution of tensor values may not be Gaussian, especially given the emergence of outlier values (Dettmers et al., 2022; Nrusimha et al., 2024). To mitigate this, we add Hadamard Transform (HT) step before Gaussian Fitting. Thus, our forward pass projection becomes: ˆxh = projα HT(x). (3) In other words, we transform the target tensor via multiplication with Hadamard matrix of appropriate shape, applied along the matrix-multiplication dimension, and then project it to an MSE-optimal grid in the Hadamard domain. Here, we leverage 1) the fact that, roughly, multiplication of matrix with the Hadamard Transform leads the weight distribution to better match Gaussiant (Ailon & Chazelle, 2009; Suresh et al., 2017); 2) the existence of fast Hadamard multiplication kernels (Tri Dao), and 3) the fact that the HT is orthogonal, so it can be easily inverted. While this HT effect has been utilized in PTQ (Tseng et al., 2024; Ashkboos et al., 2024; Malinovskii et al., 2024), we believe we are the first to harness it for QAT. 3 Training Accurate LLMs with Low-Bit Weights and Activations 3.2. Step 2: Trust Gradient Estimation Algorithm 1 QuEST Training Forward Next, we focus on the backward pass. For simplicity, we first describe the variant without the Hadamard Transform step and then integrate this component. Trust Estimators for the Basic Projection. First, assume that (cid:98)x = projα (x). Since the projection operation x, is not differentiable w.r.t. x, we need robust way to estimate our gradient. Expressed as an operator, STE can be written as during the backward pass, allowing gradients to propagate through the network, but can lead to large errors due to components with large quantization error. Specifically, the factor α, chosen to minimize the weight fitting error, acts as natural scale for how far off their real value the majority of quantized values can be: for values below the scaling factor, this error is not larger than = α 2b1 , the half-width of quantization interval. This gives natural bound for the () term in our analysis of Equation 2. To bound the second term (), we choose to not trust the gradient estimations for weights with large errors { (cid:98)wLk : Slarge}. Choosing = α 2b1 and masking gradients for elements in Slarge we obtain the gradient operator: IˆxxT ˆx where IˆxxT is the standard indicator operator. We will refer to Mα as the trust mask; this gradient estimation operator will be called the trust estimator. := Mα (x; ˆx) ˆx , Trust Estimators for the Hadamard Projection. We now interface the trust estimator with the Hadamard Transform (HT) and its inverse (IHT) to obtain the following forward scheme: xh = HT(x) and ˆxh = projα xh. Then, the natural approach is to perform trust estimation directly in the Hadamard domain, where quantization takes place: (cid:18) IHT Mα (xh; ˆxh) (cid:19) . ˆxh In other words, after deriving the trust mask w.r.t. distribution fitting in the Hadamard domain, we apply the resulting mask Mα (xh; ˆxh) onto the gradient w.r.t. quantized weights in the Hadamard domain. Gradient Effects. Notice that, in the absence of the HT or regularization effects (e.g., weight decay), the untrusted weights in Slarge would receive no gradient and may be permanently removed from optimization. Yet, the addition of the HT means that the trust mask is no longer binary in the standard domain, allowing for gradient flow towards all model weights. We validated this effect empirically by 1: Input: Input activations x, row-major weight 2: xh = HT(x) 3: ˆxh = projα xh 4: wh = HT(w) 5: ˆwh = projα wh 6: = ˆxh ˆwT 7: Return: y, ˆxh, ˆwh, Mα (xh; ˆxh), Mα (wh; ˆwh) observing that the HT reduced the final cardinality of the untrusted weights set Slarge by 4x, aligning it with the number of values we would expect to be outside the trust set at every step, for weights from normal distribution. This is investigated in more depth in Appendix A.1. 3.3. Discussion Implementation. In practice, we use identical Hadamard Transforms along the matrix-multiplication dimension for both the weights and the activations x. Since the Hadamard Transform is unitary, the quantized matrix multiplication output = ˆx ˆwT is aligned with the full precision output xwT it approximates. The algorithm 1 describes the forward pass over linear layer actively quantized with QuEST for row-major weight representation. The algorithm 2 describes the backward pass over the same layer using the quantized weight and activations from the forward pass as well as error gradient w.r.t y. We note that, although the backward computation is performed w.r.t. the quantized weights and activations, the multiplications and gradient operands are performed in standard 16-bit precision. , ˆxh, ˆwh, Mα (xh; ˆxh), Mα (wh; ˆwh) ˆwh (cid:16) Algorithm 2 QuEST Training Backward 1: Input: = 2: ˆxh 3: x = IHT = ˆxT ˆwh 5: w = IHT 6: Return: Mα (xh; ˆxh) ˆxh y (cid:16) Mα (wh; ˆwh) ˆwh 4: (cid:17) (cid:17) , w Training Complexity. In total, during training, for each original matrix multiplication (e.g., xwT ), we need only two Hadamard Transforms on the forward pass and two Inverse Hadamard transforms on the backward pass. For Transformer model (Vaswani, 2017) with blocks and hidden dimension h, and batch containing tokens, the MatMul complexity of the forward pass can be estimated as: h2. Then, the asymptotic cost of the Hadamard Training Accurate LLMs with Low-Bit Weights and Activations ment) only in the extreme 1-bit compression regime. This is discussed further in Appendix A.2. 4. Experimental Validation 4.1. Implementation Details Models and Hyperparameters. We tested our method on pre-training decoder-only Transformers (Vaswani, 2017) following the Llama architecture (Touvron et al., 2023), in the range of 30, 50, 100, 200, 430 and 800 million non-embedding parameters. Please see Appendix B.1 for architecture and hyper-parameter details. We trained all models on tokens from the C4 (Dodge et al., 2021) dataset, tokenized with the Llama 2 tokenizer. We used the AdamW (Loshchilov & Hutter, 2019) optimizer with cosine learning rate schedule and 10% warmup period, with gradient clipping (1.0 threshold, decoupled weight decay of 0.1). We identified the learning rate optimally for 50M FP16 model via learning-rate sweep. For other models, as standard, we scale the learning rate inverse-proportionally to the number of non-embedding parameters. We reuse the exact learning rates for all QuEST training runs. Please see https://github.com/IST-DASLab/QuEST for reference implementation. Unless stated otherwise, we train every model on number of tokens equal to 100x its number of free parameters, e.g., 10B tokens for Llama 100M model, regardless of precision. This allows us to explore the data-saturation regime. We aim for comparisons that are iso-size: That is, to match the size / FLOPs of 100M FP16 Llama model (trained on 10B parameters), we will train 400M-parameter model with 4-bit weights and activations, using 40B total tokens. This allows us to explore accuracy for fixed model sizes, across compression ratios (see Figure 1). We discuss different D/N regimes in Appendix C.2. 4.2. Comparison to Prior QAT Methods We compare QuEST to two widely used QAT baselines: PACT (Choi et al., 2018) and LSQ (Esser et al., 2019), both of which rely on learnable quantization parameters. While both methods can still converge at higher bitwidths, PACT struggles in the low-precision setting (below INT4), where it fails to produce viable models. Thus, we omit PACT from our comparison in Figure 3 (its PPL values exceed 150). Instead, we compare highly-tuned version of LSQ and QuEST on 30M models trained on different bandwidth configurations. While LSQ maintains reasonable performance across bitwidths, QuEST consistently achieves lower final perplexity. We found that QuEST exhibits stable convergence across all precisions and model sizes tested, with smooth loss trajectories and no signs of divergence. detailed analysis of its convergence compared to LSQ and PACT is provided in Appendix B.2. Figure 2. Gradient alignment comparison for 30M Llama model after training on 2.7B tokens in 8-bit precision. Transform is the quantity log + h2 log h, which is asymptotically negligible with > log h. Activation Effects. It is well-known (Choi et al., 2018) that activation quantization has major impact on training, possibly due to compounding with model depth. To test the effect of different gradient estimators on backpropagation, we empirically examine gradient quality as follows: we calculate intermediate gradients ˆaℓL with respect to activations after the ℓ-th Transformer block. For the same input, we disable activations quantization and calculate the true gradients aℓ L. We then define the gradient alignment as the cosine similarity between gradients: Ξ(ˆaℓ L, aℓL) = (ˆaℓL aℓL)/(ˆaℓL2 aℓ L2). While low similarity does not necessarily indicate poor gradient estimation (as the quantized forward pass might have utilized slightly different pathways, leading to discrepancy), high similarity clearly indicates that the estimator produces high-quality gradients relative to full precision. Figure 2 compares the gradient alignment for the STE relative to QuEST, with and without the HT. QuEST leads to remarkably-high and well-concentrated alignment ( 0.8), even at larger depths. By contrast, standard trust estimation degrades alignment with depth but has good concentration, whereas the STE has poor alignment and high variance. The 1-bit Case. In our original trust estimation formulation, we proposed to set the trust factor as half the quantization interval, = α 2b1 . Thus, the trust regions increase exponentially as the bitwidth decreases. In particular, for 1-bit weights and activations, QuEST will suffer from trust regions that extend out of the grid by whole α. To fix this, we reduce the size of the outermost trust regions, outside the clipping factor, by scaling factor s. Through small-scale experiments, we determined the optimal value of to be 1.30. We use this scaling factor for all the 1-bit QuEST runs in this paper (unless stated otherwise). This modification is necessary (and leads to an improve5 Training Accurate LLMs with Low-Bit Weights and Activations 1 3 4 8 16 eff(P ) 0. 0.16 0.43 0.70 1.02 1.00 Table 1. Fitted scaling-law effective parameter multipliers. Figure 3. Perplexity (PPL) across bit-widths with QuEST vs. tuned variant of LSQ on 30M model. QuEST leads to consistently lower PPL, with the advantage growing with compression. 4.3. Scaling Laws Background. Hoffmann et al. (2022) proposed to model loss scaling as function of the number of parameters in the model and the number of tokens it was trained on, in the form of parametric function: L(N, D) = AN α + BDβ + E, (4) where A, B, E, α, and β are the scaling law parameters that can be fit empirically. Following Frantar et al. (2025), we modify this formula assuming that the training precision only affects the parameter count as multiplicative factor eff(P ), which, for given quantization method, depends only on the training precision: L(N, D, ) = (N eff(P ))α + Dβ + E. (5) If we take eff(16) = 1.0, we recover the law in Equation 4. Fitting process. To estimate A, B, E, α, β and eff(P ) for every quantization precision we need, we fit this parametric function by minimizing the Huber loss (Huber, 1964) between the predicted and the observed log loss. Our process is detailed in the Appendix, and closely follows the setup of Hoffmann et al. (2022), including the grid search and the loss hyper-parameters. Specifically, we fit the model on the range of parameters {1, 2, 3, 4, 16}, {30, 50, 100, 200, 430, 800} 106 and = 100 . The resulting fit is presented on Figure 1. To capture larger range of D, we fit the model on additional runs with {2, 3, 4}, {30, 50, 100} 106 and D/N {25, 50}. We additionally fit the extensions of our method described in Sections 5 and 4.6. Appendix Figure 13 illustrates the quality-of-fit. Results. The overall results were presented in Figure 1, illustrating loss vs. model size. First, we observe that, Figure 4. Illustration of the efficiency factors eff(P )/P , arising from our analysis, for different numerical precisions and formats (INT, FP, INT+sparse). Higher is better. INT4 appears to have the highest efficiency among hardware-supported formats. remarkably, QuEST provides stable training down to 1-bit weights and activations, across model sizes, following stable scaling law. Second, examining the Pareto frontier, we observe that 4-bit precision is slightly superior to 3-bit, and consistently outperforms all higher precisions. Overall, these results show that QuEST can lead to stable scaling laws, which consistently improve upon prior results (Kumar et al., 2024), moving the Pareto-optimal line to around 4-bit. 4.4. Finding the Optimal Precision The Overtraining (OT) regime. The goal of standard scaling law (Equation 4) is to determine the optimal model size and training duration under fixed pre-training compute = 6N D. For instance, Hoffmann et al. (2022) estimated the Chinchilla-optimal ratio to be around D/N 20. Yet, it is now common to train (often smaller) models way beyond this ratio, effectively spending additional training compute (relative to optimal) to minimize deployment costs by executing smaller model. For example, recent models are trained with D/N 1000 (Dubey et al., 2024; Team et al., 2024). With test-time compute (Snell et al., 2024), there is an incentive to increase this even further. If we extrapolate and take D/N , Equation 5 takes the simplified form: LOT (N, ) = (N eff(P ))α + E. (6) We refer to this as the overtraining (OT) regime, where the training compute is less relevant, and is only bounded by factors such as the available amount of filtered training data. The focus is on minimizing runtime/inference compute, measured for example by model latency. This problem Training Accurate LLMs with Low-Bit Weights and Activations can be formulated as finding the optimal model size and precision that minimizes certain runtime compute limit. Runtime Cost Estimate. Since we focus on quantizing both weights and activations, the matrix multiplications can be performed directly in lower-precision, providing linear speedups in the precision (Abdelkhalik et al., 2022). As such, we can roughly estimate the runtime cost, up to constants, as the precision-weighted number of basic operations (FLOPs) in forward pass = . Then, the problem of minimizing loss while staying within certain runtime (FLOP) constraint can be re-written as: min N,P LOT (N, ) = (cid:16) eff(P ) (cid:17)α + s.t. Fmax. From this formulation, if we fix Fmax, maximizing eff(P ) becomes the key factor that influences the optimal pre-training precision in the OT regime. Recall that we can estimate eff(P ) from the empirical scaling law (obtained in Section 4.3 and shown in Table 1). Thus, we can calculate eff(P ) for any precision. Figure 4 suggests that 4-bit appears to be the optimal pre-training precision in this regime. 4.5. Extensions to Different Formats The FP4 Format. We can use the same framework to compare the effective parameter count for INT, INT + sparse, and the lower-precision FP format supported by NVIDIA Blackwell (NVIDIA, 2024). QuEST can be extended to this data type by replacing the rounding operation with rounding to the FP4 grid FP4 scaled to fit the same [1, 1] interval. The optimal scaling factor α FP4 would be defined by simply replacing with FP4 in the original definition. We choose the trust factor for Mα (x; ˆx) = IˆxxT as the largest half-interval of the FP4 grid. To determine the eff(P ) parameter for FP4, we train 30, 50, 100, and 200M models with QuEST in FP4 precision and aggregate results in Figure 5(a), comparing them with the original uniform grid results. We observe that FP4 performs slightly worse than INT4. We also fit FP4 with the scaling law in Equation (5) and present the resulting eff(P )/P in Figure 4 (red dot). The results show that, indeed, FP has lower parameter efficiency than INT at 4-bit precision. We hypothesize that this is correlated with the fact that, when clipping is allowed, FP4 has higher MSE than INT4 when fitting Gaussian-distributed data. Extension to sparsity. QuEST can also be extended to sparsity. Then, the trust estimator will mask out sparsified elements with absolute value above the trust mask; specifically, this covers the majority of sparsified elements, except 7 (cid:104) α (cid:105) . In practice, for the small elements within we still keep the whole weight matrix in full precision during training. On the forward pass, we first sparsify and then quantize. On the backward pass, we apply the trust mask as usual. 2b1 , + α 2b1 Figure 5(a) illustrates the scaling law induced by the 50% sparse + INT4 of NVIDIA Ampere (Abdelkhalik et al., 2022), while Figure 4 (green dot) shows its parameter efficiency relative to INT and FP. With QuEST, this format can provide better scaling than FP4, but slightly inferior to INT4. (While this format is known as 2:4 sparsity, for INT4 + 2:4 it requires 4:8 mask with some additional constraints.) 4.6. Additional Experiments Weight-only quantization. In addition to the comparison with the baseline presented in Section 4.2, we present full scaling for weight-only QuEST quantized training. We train models with 30, 50, 100, and 200 million parameters in 1,2,3, and 4 bits in the same general setup as Figure 1. The results in Figure 5(b) show that our approach leads to stable scaling laws in the weight-only case as well. Interestingly, here 2-bit weights appear to be Pareto-dominant, while 1-bit is surprisingly competitive with 3-bit weights. Hadamard ablation. Finally, we examine the impact of the Hadamard transform by removing it while maintaining the trust technique, as described in Section 3.2. In Figure 5(c), we present the results in the same setup as Figure 1 for simplified trust scheme without the Hadamard Transform. Specifically, 1) training remains stable across all precisions, although W1A1 is now inferior to BF16; 2) W4A4 remains Pareto-dominant, suggesting that the Hadamard transform improves the coefficients but does not alter the scaling laws. 5. GPU Execution Support for QuEST Models Kernel Overview. Finally, we describe GPU kernel support. Our forward-pass pipeline for the quantized linear layer in QuEST consists of three main stages: (1) applying the Hadamard transformation to the BF16 activations, (2) quantizing the BF16 activations into INT4 and packing them into the low-precision format, and (3) performing INT4 matrix multiplication on the quantized activations and weights, followed by dequantization of the result back to BF16. For the first stage, we utilize an existing Hadamard kernel (Tri Dao). We developed custom Triton kernel for the second stage to fuse the quantization and data formatting. This kernel computes MSE-optimal group scales and performs centered quantization on the activations. It also packs the INT4 elements into UINT8, with additional intermediate results prepared for matrix multiplication and dequantization. The third stage involves fused matrix multiplication and dequantization using our enhanced CUTLASS kernel. Training Accurate LLMs with Low-Bit Weights and Activations Figure 5. Additional scaling laws induced by QuEST: (a, left) compares INT, FP, and INT+sparse formats at 4-bit precision, (b, middle) shows the scaling laws for weight-only quantization, where 2-bit appears to be Pareto-dominant, while (c, right) shows that trust estimation benefits significantly from Hadamard normalization. Figure 6. Per-layer speedups for QuEST INT4 vs BF16, on single RTX 4090 GPU. The results take into account quantization/dequantization costs for QuEST, and include the cost of the Hadamard transform (orange bar). We present results for the 800M 4-bit QuEST model we trained, as well as inference speedups for proportional 7B-parameter model. In this stage, both activations and weights are read and processed as integers to exploit the higher GPU throughput. The results are then dequantized back to BF16 within the same kernel. We also apply CUDA Graph end-to-end to further reduce the kernel launching overhead. To optimize GEMM performance, we carefully tuned the CUDA thread-block and warp tile sizes and leveraged the high levels of the memory hierarchy to fuse the dequantization step before writing the results back to Global Memory in custom CUTLASS epilogue. By performing dequantization at the register level, we minimize data movement, reduce GMEM memory access overhead, and minimize the number of kernel launches. Runtime Results. The per-layer speedups achievable using our kernel at 4-bit precision, relative to standard 16-bit matrix multiplications on the corresponding layers, are illustrated in Figure 6. We provide breakdown across layers of the same shape, for 800M (which we have already trained), and proportionally-scaled 7B model (which we plan to train in future work). These measurements include Figure 7. End-to-end prefill speedups for QuEST INT4 vs BF16, across different batch sizes, using the 800M parameter model on single RTX 4090 GPU. As expected, QuEST is most effective for larger batch sizes, where the workload is more compute-bound. all auxiliary overheads (e.g. quantization/dequantization) for QuEST; in addition, we separate out the performance impact of the Hadamard transform. For the smaller 800M model, the per-layer speedups vary between 1.2 (on the smallest layers, with Hadamard) and 2.4 (largest down-projection layer, no Hadamard). The largest overhead of the Hadamard transform, of around 30%, is on the down-projection layer, which presents the largest dimension for the Hadamard. The speedups increase significantly (2.3-3.9) when we move to the 7B-parameter model, as the MatMuls are much more expensive. Figure 7 shows the end-to-end inference performance at 800M using our kernels vs. the BF16 baseline, showing speedups of 1.3-1.5 in the less memory-bound regime. 6. Discussion and Future Work We introduced QuEST, new QAT method that achieves stable LLM training of in extremely low precision (down to 1-bit) weights and activations. Our results demonstrate that, if data and compute are appropriately scaled, 4-bit models can outperform standard-precision baselines in terms of accuracy and inference cost, suggesting that the fundamental 8 Training Accurate LLMs with Low-Bit Weights and Activations limits of low-precision QAT are much lower than previously thought. Further, our analysis provides new insights into the relationship between training precision and model efficiency, suggesting that low-precision may be good target for largescale training runs in the overtrained regime. Third, we have shown that our approach can lead to inference speedups. Several promising directions emerge for future work. First, while we demonstrated QuESTs effectiveness up to 800M parameters, its scaling behavior for much larger models is an interesting direction we plan to pursue in future work. Second, our work focused primarily on decoder-only architectures; extending QuEST to encoder-decoder models and other architectures could broaden its applicability. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:3031830332, 2022. Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. Spqr: sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024."
        },
        {
            "title": "References",
            "content": "Abdelkhalik, H., Arafa, Y., Santhi, N., and Badawy, A.- H. Demystifying the nvidia ampere architecture through microbenchmarking and instruction-level analysis, 2022. URL https://arxiv.org/abs/2208.11174. Ailon, N. and Chazelle, B. The fast johnsonlindenstrauss transform and approximate nearest neighbors. SIAM Journal on Computing, 39(1):302322, 2009. doi: 10.1137/060673096. Ashkboos, S., Markov, I., Frantar, E., Zhong, T., Wang, X., Ren, J., Hoefler, T., and Alistarh, D. Towards end-toend 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259, 2023. Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. Bengio, Y., Leonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Bhalgat, Y., Lee, J., Nagel, M., Blankevoort, T., and Kwak, N. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2020. Chee, J., Cai, Y., Kuleshov, V., and De Sa, C. M. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024. Choi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srinivasan, V., and Gopalakrishnan, K. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. Dodge, J., Sap, M., Marasovic, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., and Gardner, M. Documenting large webtext corpora: case study on the colossal clean crawled corpus, 2021. URL https: //arxiv.org/abs/2104.08758. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning, 2017. URL https://arxiv. org/abs/1702.03118. Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., and Modha, D. S. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. GPTQ: Accurate post-training quantization for arXiv preprint generative pre-trained transformers. arXiv:2210.17323, 2022. Frantar, E., Riquelme, C., Houlsby, N., Alistarh, D., and Evci, U. Scaling laws for sparsely-connected foundation models, 2023. URL https://arxiv.org/abs/ 2309.08520. Frantar, E., Evci, U., Park, W., Houlsby, N., and Alistarh, D. Compression scaling laws: Unifying sparsity and quantization, 2025. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models, 2022. URL https://arxiv.org/ abs/2203.15556. 9 Training Accurate LLMs with Low-Bit Weights and Activations Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y. Binarized neural networks. Advances in neural information processing systems, 29, 2016. Huber, P. J. Robust Estimation of Location Parameter. The Annals of Mathematical Statistics, 35(1):73 101, 1964. doi: 10.1214/aoms/1177703732. URL https: //doi.org/10.1214/aoms/1177703732. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D. Quantization and training of neural networks for efficient integerarithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Jin, T., Humayun, A. I., Evci, U., Subramanian, S., Yazdanbakhsh, A., Alistarh, D., and Dziugaite, G. K. The journey matters: Average parameter count over pretraining unifies sparse and dense scaling laws, 2025. URL https://arxiv.org/abs/2501.12486. Kaushal, A., Vaidhya, T., Mondal, A. K., Pandey, T., Bhagat, A., and Rish, I. Spectra: Surprising effectiveness of pretraining ternary language models at scale. arXiv preprint arXiv:2407.12327, 2024. Kumar, T., Ankner, Z., Spector, B. F., Bordelon, B., Muennighoff, N., Paul, M., Pehlevan, C., Re, C., and Raghunathan, A. Scaling laws for precision. arXiv preprint arXiv:2411.04330, 2024. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. Liu, Z., Zhao, C., Fedorov, I., Soran, B., Choudhary, D., Krishnamoorthi, R., Chandra, V., Tian, Y., and Blankevoort, T. Spinquantllm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019. URL https://arxiv.org/abs/ 1711.05101. Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., and Wei, F. The era of 1-bit llms: All large language models are in 1.58 bits, 2024. URL https://arxiv.org/abs/2402.17764. Malinovskii, V., Panferov, A., Ilin, I., Guo, H., Richtarik, P., and Alistarh, D. Pushing the limits of large language model quantization via the linearity theorem. arXiv preprint arXiv:2411.17525, 2024. Nrusimha, A., Mishra, M., Wang, N., Alistarh, D., Panda, R., and Kim, Y. Mitigating the impact of outlier channels for language model quantization with activation regularization, 2024. URL https://arxiv.org/abs/ 2404.03605. NVIDIA. brief. en-us-blackwell-architecture\", 2024. Nvidia blackwell architecture technical \"https://resources.nvidia.com/ Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv e-prints, 2019. Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. Xnor-net: Imagenet classification using binary convoIn European conference on lutional neural networks. computer vision, pp. 525542. Springer, 2016. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/ 2104.09864. Suresh, A. T., Yu, F. X., Kumar, S., and McMahan, H. B. Distributed mean estimation with limited communication, 2017. URL https://arxiv.org/abs/1611. 00429. Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi`ere, M., Kale, M. S., Love, J., et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, 10 Training Accurate LLMs with Low-Bit Weights and Activations A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Tri Dao, Nikos Karampatziakis, H. C. Fast hadamard interface. https://github.com/Dao-AILab/ transform in URL fast-hadamard-transform. cuda, with pytorch Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa, C. Quip#: Even better llm quantization with hadamard arXiv preprint incoherence and lattice codebooks. arXiv:2402.04396, 2024. Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. URL https://arxiv.org/ abs/1706.03762. Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023. Wang, H., Ma, S., and Wei, F. Bitnet a4. 8: 4-bit activations for 1-bit llms. arXiv preprint arXiv:2411.04965, 2024. Wortsman, M., Dettmers, T., Zettlemoyer, L., Morcos, A., Farhadi, A., and Schmidt, L. Stable and low-precision training for large-scale vision-language models. Advances in Neural Information Processing Systems, 36: 1027110298, 2023. Xi, H., Chen, Y., Zhao, K., Zheng, K., Chen, J., and Zhu, J. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. arXiv preprint arXiv:2403.12422, 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Zhao, Y., Lin, C.-Y., Zhu, K., Ye, Z., Chen, L., Zheng, S., Ceze, L., Krishnamurthy, A., Chen, T., and Kasikci, B. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023. 11 Training Accurate LLMs with Low-Bit Weights and Activations A. Additional Trust Details A.1. Trust Mask Analysis For the purposes of weight trust masks interpretation, we trained 30M model over 3B tokens (11,444 iterations at bs=512) with QuEST weights and activations quantization to 8-bit with and without the Hadamard Transform (HT). We logged the trust masks every 500 iterations. Figure 8 shows the fraction of masked weights. We can see that adding the HT leads to an 4x decrease in the amount of masked values, corresponding to the fraction of expected clipped weights for standard normal distribution. We can also see that without the HT the fraction deviates significantly from the expected fraction under the assumption of weights normality. Figure 8. Fraction of weights for which Mα = 0 as function of number of training iterations for 30M model trained with QuEST. Moreover, we looked at the percentage of masked elements at fixed iteration in the past, that remain masked at fixed later iteration. We plot these percentages in Figure 9. As we can see, for the run without the HT, around 69% of masked elements at iteration 6000 (roughly halfway through training) remain masked at iteration 10000 (towards the end of the training). This percentage is more than twice as small for the run with the HT at 30%. This implies that the HT makes masks less persistent, as expected. In addition, we note that weight decay is applied on all weights (including masked ones). Thus, masked weight will slowly decay until it may exit the masked interval, obtaining gradient again. Figure 9. Fraction of masked values retained from an old iteration to new iteration for 30M model trained with QuEST W8A8. A.2. The 1-bit Case To determine the optimal outer trust scaling factor s, discussed in Section 3.3, we conduct sweep over s, varying the outer size of the outermost trust regions as = α 2b1 . The results for 1-bit, shown in Figure 10, indicate that = 1.30 for the standard QuEST setup and = 1.25 for the setup without the Hadamard Transform (HT), corresponding to exactly quarter of the quantization interval. 12 Training Accurate LLMs with Low-Bit Weights and Activations Figure 10. Performance of QuEST as function of the outer trust scaling factor for 30M model pretraining. A.3. Zero-shot Evaluation of QuEST Models To assess the effectiveness of QuEST beyond perplexity, we conducted zero-shot evaluation on the HellaSWAG benchmark (Zellers et al., 2019), which tests commonsense reasoning capabilities. We compared an 800M parameter model trained with QuEST in 4-bit precision against its BF16 counterpart, both of which were trained on 80B tokens. Table 2 presents the accuracy results. The near-identical performance between the two models indicates that QuESTs quantization-aware training is essentially lossless, preserving the models ability to generalize while significantly reducing precision and computational costs. Model HellaSWAG Accuracy (%) BF16 (800M, 80B tokens) QuEST 4-bit (800M, 80B tokens) 39.52 39.22 Table 2. Zero-shot evaluation on HellaSWAG comparing QuEST 4-bit to its BF16 counterpart. The results are nearly identical, confirming that training with QuEST is lossless. B. Additional Information about the Experimental Setup B.1. Model Hyper-parameters For our experiments, we chose to use the Llama 2 (Touvron et al., 2023) model as the base architecture. For the attention block, this architecture utilizes multi-head attention (Vaswani et al., 2023) with rotary positional embeddings (Su et al., 2023). For the MLP block, it uses additional gate projection and SiLU (Elfwing et al., 2017) activation function. We kept the MLP intermediate dimension equal to 8/3 of the hidden size, padding it to 256 for increased kernel compatibility. For the AdamW optimizer, we used β1 = 0.90 and β2 = 0.95. We did not apply weight decay to any biases and layer normalizations. Table 3 describes size-specific models and optimizer hyper-parameters for all model sizes used in this work. Model size Num. Blocks Hidden Size Num. Attn. Heads Learning Rate Num. Tokens 30M 6 640 5 0.0012 3B 50M 7 768 6 0.0012 5B 100M 200M 8 1024 8 0.0006 10B 10 1280 10 0.0003 20B 430M 13 1664 13 0.00015 43B 800M 16 2048 16 0.000075 80B Table 3. Hyper-parameters used for each model size. 13 Training Accurate LLMs with Low-Bit Weights and Activations B.2. Training Stability and Convergence Here we present the loss curves for BF16, LSQ, PACT, and QuEST (ours) to analyze training stability and convergence. As shown in Figure 11(a), QuEST smoothly converges throughout training, closely tracking the BF16 baseline while consistently outperforming LSQ. Meanwhile, PACT struggles with much higher loss, indicating poor convergence. To better highlight the differences between QuEST and LSQ in the later stages of training, Figure 11(b) focuses on steps after 1000, removing PACT for clarity. This zoomed-in view shows that QuEST maintains consistently lower loss trajectory than LSQ, further reinforcing its superior stability and accuracy across training. (a) (b) Figure 11. Training loss curves for 30M model trained on 3B tokens with W4A4 bitwidth, comparing QuEST (ours), LSQ, PACT, and BF16. (a) Full training loss curves, showing that QuEST closely follows BF16 and consistently outperforms LSQ, while PACT struggles with high loss. (b) Zoomed-in view of training steps after 1000, excluding PACT for clarity, highlighting that QuEST maintains lower loss than LSQ throughout training. B.3. Hyper-parameter Search for Baseline Methods Figure 12. Hyperparameter search for PACT on 30M parameter model with 4-bit weights and activations, trained on 10% of the dataset. The search explores different values for learning rate scaling (LR Scale) and alpha weight decay, with validation loss indicated by the color gradient. Lower validation loss (darker colors) corresponds to better configurations. To ensure fair comparisons between QuEST and prior QAT methods, we conducted hyperparameter searches for both PACT and LSQ. Given PACTs instability at lower bitwidths, we extensively tuned two key hyperparameters: weight decay and learning rate scaling for the quantization parameter α (i.e., ηα = η). Figure 12 shows the loss achieved across different weight decay and LR scale values. For LSQ, we only tuned weight decay, as the LSQ formulation already applies scaling internally to the gradient of α, making additional learning rate adjustments unnecessary. Table 4 summarizes the results of the weight decay search across 2-bit, 3-bit, and 4-bit LSQ models, where the best-performing configuration (highlighted in bold) was used for final model comparisons. Training Accurate LLMs with Low-Bit Weights and Activations"
        },
        {
            "title": "Weight Decay",
            "content": "2-bit PPL 3-bit PPL 4-bit PPL 0.001 0.01 0.1 1.0 37.02 36.91 36.54 38.12 31.10 30.89 30.26 31. 27.93 27.72 27.51 28.67 Table 4. Weight decay hyperparameter search results for LSQ across different bitwidths of 30M model. The best-performing setting is highlighted in bold. Our hyperparameter search ensured that LSQ and PACT were tuned optimally before comparing against QuEST, leading to fair evaluation of performance across all tested quantization methods. C. Scaling Laws C.1. Description of the Fitting Procedure As described in Section 4.3, we closely follow the fitting procedure of Hoffmann et al. (2022) for the scaling law (5) fitting. Specifically, we copied their grid of initialization given by: α {0., 0.5, . . . , 2.}, β {0., 0.5, . . . , 2.}, {1., .5, . . . , 1.}, {0, 5, . . . , 25}, and {0, 5, . . . , 25}. We also reuse their δ = 103 for the Huber loss. In addition, we fit the eff(P ) coefficient for number of quantization schemes described below: QuEST for {1, 2, 3, 4, 8}. Weight-only QuEST for {1, 2, 3, 4}. QuEST without the HT for {1, 2, 3, 4, 8}. QuEST with FP4 grid. QuEST with 2:4 INT4. C.2. Analysis of the Transitory Data Regime Figure 13. Scaling law (5) fit for 3 and 4 bit QuEST with tokens/parameters ratios in {25, 50, 100}. The results in Section 4.4 suggest that 4-bit training is optimal in the D/N regime. Here, we use the fitted scaling law (5) to verify that 4 bit is also close to optimal for D/N ratios that are reasonable in practice. We formulate the question as follows: for fixed model size (e.g. in Gb), for which amount of compute is QuEST 4-bit the optimal precision? Figure 14 demonstrates the (predicted) dependence of performance as function of 2 . For BF16, this quantity becomes D/N . For other , it ensures the same amount of training computed ( D). As such, models there are compared at both the same size and the same training compute. We can see that 4-bit quantization becomes optimal after it passes certain compute threshold that depends on model size. We can also see that the threshold value decreases as the model size (in Gb) grows. For 14.0Gb model (corresponding to 7B parameters in BF16), the threshold is around D/N 30, which is significantly below the amount of data that models of that size are currently trained on (see Section 4.3). For even larger models, the threshold eventually becomes less than the Chinchilla-optimal ratio of D/N 20. This validates that the regime in which 4-bit pre-training is optimal can, in fact, be easily achieved in practice. 15 Training Accurate LLMs with Low-Bit Weights and Activations Figure 14. Different QuEST precision performance as function of tokens-to-parameters ratio at fixed model memory footprint. The gray line indicates 4-bit optimality threshold."
        }
    ],
    "affiliations": [
        "ISTA",
        "Red Hat AI"
    ]
}