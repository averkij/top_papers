{
    "paper_title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis",
    "authors": [
        "Yikang Ding",
        "Jiwen Liu",
        "Wenyuan Zhang",
        "Zekun Wang",
        "Wentao Hu",
        "Liyuan Cui",
        "Mingming Lao",
        "Yingchao Shao",
        "Hui Liu",
        "Xiaohan Li",
        "Ming Chen",
        "Xiaoqiang Liu",
        "Yu-Shen Liu",
        "Pengfei Wan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis."
        },
        {
            "title": "Start",
            "content": "September 12, 2025 Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis Yikang Ding Jiwen Liu Wenyuan Zhang Zekun Wang Wentao Hu Liyuan Cui Mingming Lao Yingchao Shao Hui Liu Xiaohan Li Ming Chen Xiaoqiang Liu Yu-Shen Liu Pengfei Wan Kling Team, Kuaishou Technology 5 2 0 2 1 1 ] . [ 1 5 9 5 9 0 . 9 0 5 2 : r Figure 1: Conditioned on audio, image, and user prompts, Kling-Avatar generates high-fidelity portrait animations through instruction grounding and semantic planning. The results exhibit vivid emotions, rich actions, and precise lip synchronization, while also showing strong generalization to open scenarios such as anime, cartoons, and stylized characters. Figure 2: Benchmark performance of Kling-Avatar against its counterparts in terms of GSB metrics. We achieve superior performance on the overall metric as well as across most of sub-dimensions. Equal contribution. Sorted in alphabetical order by surname."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts two-stage pipeline. In the first stage, we design multimodal large language model (MLLM) director that produces blueprint video conditioned on diverse instruction signals, thereby governing highlevel semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis. Demonstration videos are available in our project page: https://klingavatar.github.io/."
        },
        {
            "title": "Introduction",
            "content": "Avatar animation synthesis translates multimodal references into temporally coherent facial expressions, lip movements and body gestures, enabling interactions with machines that feel conversational and embodied. As communicative medium, speaking avatar can convey intent and affect with high fidelity, turning abstract ideas into vivid, situated performances that maintain user attention and improve comprehension. This capability opens broad opportunities across virtual assistants, education, media content creation, and immersive telepresence. Building such avatars requires models that couple realism with fine-grained controllability and reliable synchronization, which define the core challenge and motivate the approach developed in this work. Recently, Video Diffusion Transformers (DiT) (Chen et al., 2025c; Cui et al., 2025b; Jiang et al., 2024; Tian et al., 2024; Peng et al., 2025; Wei et al., 2025) have emerged as general paradigm for generating visually compelling content conditioned on multimodal signals such as images, speech, and prompts. Prior work has advanced precise facial expression and lip synchronization (Jiang et al., 2024; Fei et al., 2025; Tian et al., 2024), coordinated body motion (Wang et al., 2025a; Gan et al., 2025), and data scaling (Lin et al., 2025; Jiang et al., 2025). However, these advances remain insufficient for highly realistic portrait synthesis, as we want the system not only to hear and to read but also to understand these inputs, so that it can produce natural and empathetic videos aligned with user intents. Without such understanding, current approaches often treat each conditional signal independently and capture only shallow correlations, which leads to semantic conflicts across modalities and affect. For example, an avatar may sing sorrowful song while smiling, which is visually polished yet inconsistent with human expectations. In addition, existing approaches often rely on motion frames for video continuation, which poses significant challenges for maintaining consistency and stability in long-duration generation. To bridge this gap, we introduce Kling-Avatar, novel cascaded framework for portrait animation that faithfully follows multimodal instructions and synthesizes high-quality and long-duration avatar videos. Drawing inspiration from the capabilities of unifying understanding and generation of multimodal large language models(MLLMs) (Team et al., 2023; Xu et al., 2025; Hong et al., 2025), we design an MLLM Director that consolidates multimodal instructions into structured storyline. This storyline encodes high-level plans such as scene layout, camera positioning, character motions, as well as implicit emotions and atmosphere, ensuring that the generated content aligns with the intended narrative arc and expressive trajectory. blueprint video is first generated conditioned on the global script, followed by 2 multiple sub-clips generation in parallel conditioned on the blueprint keyframes. The MLLM Director continuously provides fine-grained guidance based on multimodal context, ensuring local dynamics and visual details. By densely selecting anchor frames and enabling parallel generation, our cascaded framework supports fast and stable synthesis of arbitrarily long videos, offering promising solution for long-term digital human video generation. For data preparation, we collect dataset covering diverse scenarios such as dialogues, films, and speeches. To ensure data quality, we employ series of expert models for rigorous filtering, including mouth-clarity recognition, stage-cut detection, audio-lip synchronization checking, and video quality scoring. To validate the effectiveness of our method, we construct unique benchmark containing 375 reference frameaudio pairs. For these cases, we carefully design challenging instructions that include images from diverse categories, audio spanning different languages and speech rates, and text prompts with explicit control over emotion and dynamics. This benchmark builds up comprehensive evaluation of different methods across multiple dimensions. We highlight representative generation results in Fig. 1, where Kling-Avatar produces expressive, vivid, long-duration portrait animations with rich emotions and dynamics, while maintaining strong generalization to open-domain scenarios. As indicated by the comparisons in Tab. 2 against leading competitors OmniHuman-1 (Lin et al., 2025) and HeyGen (hey), Kling-Avatar achieves superior performance in terms of lip synchronization accuracy, visual fidelity, instruction conditioned expressiveness, and identity preservation over long-duration generation. These results establish Kling-Avatar as the new benchmark for controllable, high-fidelity digital portrait animation synthesis. We summarize our contributions as follows: MLLM Director with unified instruction grounding. We introduce an MLLM Director that grounds multimodal instructions into unified global plan, providing new perspective that lifts portrait video generation from tracking low-level cues to semantic and intent understanding. Cascaded avatar animation synthesis framework. We design two-stage generation pipeline that first establishes high-level semantic guidance and then refines local dynamics, enabling long-duration video generation with coherent and expressive performances. Curated data construction pipeline. We develop data filtering pipeline powered by expert models for quality control, and further construct challenging benchmark to enable comprehensive evaluation of digital human generation systems. High-fidelity performance and strong generalization. Kling-Avatar produces state-of-the-art coherent and vivid portrait animations with precise lip synchronization, rich facial expressions and accurate response for multimodal instructions across diverse scenarios."
        },
        {
            "title": "2 Method",
            "content": "Given conditioning image, audio, and text prompt, Kling-Avatar aims to generate fluent and lifelike portrait animations with precise lip synchronization, accurate instruction following, and support for long-term extrapolation. As illustrated in Fig. 3, our framework is two-stage generation pipeline guided by an MLLM Director. In the following sections, we first present the motivation and implementation of using MLLMs for instruction grounding and control (Sec. 2.1). We then introduce the cascaded generation framework (Sec. 2.2) for long-duration video synthesis, followed by our efforts in data construction for training and benchmarking (Sec. 2.3). Finally, we describe several key strategies for training and inference (Sec. 2.4). 2.1 Grounding Multimodal Instructions with MLLMs Current digital human video generation methods focus on conditioning strategies such as sliding windows or multi-scale injection, to better align input signals with the denoising diffusion process (Gao et al., 2025; Fei et al., 2025; Wang et al., 2025a). However, this alignment is typically performed per modality, relying on local cues such as acoustic features or pixel structures, followed by shallow fusion at the generation stage. While effective at reproducing observable details, this paradigm lacks coordination across multimodal inputs, leading to semantic conflicts or impoverished camera language. For instance, when the input contains angry speech but the text imposes no such constraint, the emotion may be significantly weakened in the final output. To enable the model to truly understand the intent behind the instructions, drawing inspiration from multimodal large language models (MLLMs) (Bai et al., 2025; Hong et al., 2025; Qi et al., 2025), we unify 3 Figure 3: Illustration of Kling-Avatars cascaded generation pipeline. An MLLM Director first interprets multimodal instructions into high-level semantics and tells storyline. Guided by this global planning, the first stage generates blueprint video. In the second stage, keyframes are extracted from the blueprint and used as firstlast frame conditions for parallel sub-clip generation, refining local details and dynamics to synthesize long-duration videos. evidence from multimodal inputs into shared semantic space, producing high-level control signals as global planning for the generation process. Specifically, we use Qwen2.5-Omni (Xu et al., 2025) to extract the transcription and emotion from audio as the audio caption, and Qwen2.5-VL (Bai et al., 2025) to generate descriptions from images as the image caption. These captions are then combined with the user prompt and processed by our MLLM Director, to output coherent storyline. We explicitly specify the storyline template for the MLLM Director using three-shot in-context learning manner. This storyline, prioritized by user knowledge, audio, and image references, tells key elements such as character features, background layout, actions, visual style, camera planning, and emotional shifts. All of these elements are organized into unified textual prompt, which is injected into the video diffusion model through text cross-attention layer to generate blueprint video. 2.2 Cascaded Generation for Long-Duration Generation In the first stage, we generate blueprint video that tells storyline to reflect semantic user intent. The blueprint is then leveraged in the second stage to produce video sub-clips that refine local dynamics and visual details. To this end, we evenly segment the video according to the desired number of clips. Around each segmentation point, we then select high-quality frame that preserves identity consistency, exhibits significant motion, avoids occlusion, and conveys expressive facial details. These frames serve as anchor keyframes for first-last-frame conditioned generation of adjacent sub-clips. During sub-clip synthesis, the MLLM Director decomposes the global storyline into temporally localized semantic plans. This localized narrative, combined with time-aligned audio conditioning, provide fine-grained guidance to ensure expressive coherence and visual consistency throughout the generated sequence. To avoid misalignment between anchor frames and the actual speech timing, we employ an audio-conditioned interpolation strategy to synthesize transition frames. This ensures precise frame synchronization with the input audio, enabling seamless and temporally coherent transitions across sub-clips. The pipeline can be easily implemented in parallel manner since the clips are generated independently. By increasing the anchor number, arbitrarily long videos can be generated with nearly the same runtime as producing single clip. This cascaded framework with first-last-frame conditioned generation highlights our unique advantage in generating long-duration videos and provides promising solution for downstream applications such as digital human podcasting, public speaking, and online education. 2.3 Data Preparation Training Data. We collect thousands of hours of audiovisual content from multiple sources, including publicly available datasets as well as self-collected videos such as film clips, speeches, monologues, interviews, and singing performances, covering wide range of scenarios, linguistic styles and character dynamics. All videos are carefully processed with audio extraction and captioning. In our practice, we emphasize that data quality, rather than data scale, plays decisive role in final performance: smaller amount of high-quality talking segments proves more effective than indiscriminately enlarging 4 the dataset with long-tail samples. To this end, we design suite of expert models to classify and filter low-quality data along multiple dimensions: Lip-clarity filtering. We synthetically perturb mouth regions in high-quality talking-head videos to construct positive/negative pairs. binary discriminator is trained to classify lip-region clarity and filter out videos with visually ambiguous or motion-blurred lip movements. Temporal-continuity detection. We manually assemble different video segments to build negative samples, paired with original clips as positives. We then train temporal coherence discriminator, along with PySceneDetect (Breakthrough, 2023), to identify and remove discontinuous clips. Audiovisual synchronization. We employ SyncNet (Chung &Zisserman, 2016) to assess framelevel audio-visual synchronization confidence scores, and discard videos that fall below calibrated thresholds. Aesthetic quality assessment. We adopt video aesthetic scoring methods (Schuhmann, 2022) to evaluate visual composition and appeal. Only videos exceeding calibrated quality threshold are included in the final training set. After filtering the data using expert models, we further perform manual curation on the retained samples, ultimately assembling hundreds of hours of high-quality human portrait videos, which provide reliable supervision for training our model. Benchmark. To comprehensively evaluate the performance of Kling-Avatar, we construct challenging benchmark comprising 375 imageaudio-prompt pairs. The dataset is carefully designed with the following composition: Images. Reference images are sourced equally from real videos and AI-generated content. The set includes 340 human portraits of different races in both full-body and half-body formats, as well as 35 non-human cases covering cartoon, anime, and animal characters. Image resolutions span vertical, horizontal, and square formats, ranging from 480p to 1080p. Audio. Audio tracks are extracted from real videos and cover both speeches and songs. The collection includes 150 Chinese, 150 English, 35 Korean, and 40 Japanese samples, with clip lengths ranging from 8 seconds to 2 minutes. The audios span multiple speaking rates and expressive styles, ensuring diversity in linguistic and prosodic conditions. Prompt. Text prompts are manually annotated with diverse and explicit specifications on emotional expression, character actions, camera movements, and background layouts. Emotion categories include calm, excitement, confusion, sadness, surprise, and anger, each with multiple intensity levels. Camera instructions specify operations such as panning and zooming. Action descriptions encompass turning, raising hands, head shaking, and other expressive gestures, ensuring broad coverage of dynamic behaviors. This benchmark establishes demanding testbed for existing methods by requiring vivid and coherent portrait generation under complex multimodal instruction control."
        },
        {
            "title": "2.4 Training and Inference Strategy",
            "content": "Training Strategy. We design several training strategies to strengthen the alignment between lip movements and the corresponding speech. First, we adopt sliding window scheme to inject audio features into the audio cross-attention layer, where each video token attends only to its temporally aligned audio tokens with small padding, thereby reinforcing local phase consistency. Second, we employ DWPose (Yang et al., 2023) to locate the mouth region and assign higher weight to its diffusion denoising loss. Third, we randomly pad empty pixels around video frames during training to reduce the proportion of the face in the image, which encourages the model to remain robust under small-face and long-shot conditions. Finally, to preserve the text controllability of the base video generation model and concentrate on audiovisual interaction, we freeze the parameters of the text cross-attention layer during training, effectively preventing the base model from collapsing into overfitting the specific talking-head data. Collectively, these strategies substantially improve lip synchronization accuracy by enhancing visual-audio alignment. Inference Strategy. Our first-last-frame conditioned parallel generation framework alleviates the identity drift problem that commonly arises in existing methods which rely on motion frames for long video 5 Table 1: Numerical evaluations on GSB metrics between our method and competitors. Category GSB Overall Lip Sync Visual Quality Control Response ID Consistency Overall Speech-En Speech-Ch Sing-En/Ch Ours vs. OmniHuman Ours vs. HeyGen Ours vs. OmniHuman Ours vs. HeyGen Ours vs. OmniHuman Ours vs. HeyGen Ours vs. OmniHuman Ours vs. HeyGen 2.39 1.37 1.41 0. 4.53 1.22 2.69 2.90 1.77 2.35 1.00 1.22 3.90 2.26 2.03 7. 2.06 1.76 2.18 1.51 2.44 1.93 1.72 1.89 1.17 0.76 1.06 0. 1.13 0.79 1.35 0.97 1.37 0.86 1.27 0.76 1.47 0.82 1.38 0. Figure 4: Overall GSB evaluation results on our benchmark across various dimensions against OmniHuman-1 and HeyGen. continuation. To further improve identity consistency within each segment, we introduce negative frame Classifier-Free Guidance (CFG) mechanism. Through statistical analysis, we find that identity drift artifacts typically manifest as texture distortions, blurring, exaggerated contrast and saturation, and color shifts. To counter this, we manually corrupt the reference image according to these observed patterns to simulate an enhanced identity drift. The degraded image is then used as negative CFG signal to guide the denoising process toward identity-consistent directions. In addition, since no ground truth frames are available for mouth region masking during inference, we instead increase the audio cross-attention values to strengthen the lip-audio alignment."
        },
        {
            "title": "3.1 Experimental Settings",
            "content": "Implementation Details. Our implementation is based on Video Diffusion Transformer architecture which was pretrained on large-scale dataset. We extend it with an audio cross-attention layer to support audio-to-video generation. Audio features are extracted via pre-trained Whisper encoder (Radford et al., 2022), and text conditioning utilizes T5 encoder (Raffel et al., 2020). The model is optimized using AdamW (Loshchilov &Hutter, 2017) with learning rate of 1e-5. During training, our framework supports arbitrary video resolutions ranging from 480p to 1080p, and at inference it produces fluent videos with up to 1080p at 48 fps. Evaluation Metrics. We design human preferencebased subjective evaluation protocol as our primary metric, aiming to better reflect user-perceived semantics and aesthetic quality. For each sample in the benchmark, three participants independently provide Good/Same/Bad (GSB) judgment by comparing the results of our method against baseline methods, and the final GSB label is determined by majority vote. We report (G+S)/(B+S) as the main metric, reflecting the proportion of cases where our method is judged as \"better or not worse\" than the baseline. In addition to the overall evaluation, we also conduct 6 Figure 5: Comparison of lip synchronization between Kling-Avatar and baselines. We produce accurate lip movements for characters across different scenarios. GSB assessments on four specific dimensions, including: Lip Synchronization. Assesses the naturalness of lip movements, accuracy of audiovisual alignment, and plausibility of facial expressions. Visual Quality. Evaluates overall aesthetic appeal, structural coherence, and visual clarity of the generated video. Control Response. Examines whether emotions, actions, and camera movements in the generated video accurately reflect the textual instructions. Since OmniHuman-1 does not support prompt input, this metric is instead used to evaluate how effectively audio conditions control the body movements. Identity Consistency. Measures how well the generated video preserves identity traits and dynamic characteristics that are consistent with the reference image. This GSB protocol provides unified and intuitive framework of evaluating key aspects such as multimodal instruction following, avatar expressiveness, and visual coherence, which better reflects user subjective experience in real-world scenarios. We plan to incorporate additional objective metrics in the future to complement and extend our assessment. 7 Figure 6: Our generated videos with multimodal instruction conditioning. We highlight our results in generating vivid and coherent portrait animations with strong control over emotions, camera movements, lip synchronization and motion dynamics. 8 Figure 7: Visualization of generated long-duration videos with high consistency, coherence and vividness. Baselines. We select OmniHuman-1 (Lin et al., 2025) and HeyGen (hey) as our primary baselines, since they represent the most competitive state-of-the-art systems currently available on the market. In the future, we plan to extend our comparisons to other commercial solutions such as Higgsfield (hig) and Hedra (hed). 3.2 Experimental Results Comparison with Baselines. Tab. 1 summarizes the GSB evaluation results on the benchmark, comparing our method with OmniHuman-1 (Lin et al., 2025) and HeyGen (hey). In addition to the overall benchmark score, we report results for three sub-categories: English speeches (Speech-En), Chinese speeches (SpeechCh), and bilingual singing (Sing-En/Ch). Since the number of Japanese and Korean samples is relatively small, making their GSB statistics less reliable, we include them only in the overall scores. Fig. 4 further visualizes the GSB comparison on the full benchmark. Numerical results show that Kling-Avatar consistently outperforms OmniHuman-1 across all dimensions, highlighting our superior performance. Compared with HeyGen, our method achieves notable improvements in Lip Synchronization and Visual Quality. Notably, HeyGen produces videos by repeatedly looping five-second action pattern, which enhances motion stability and identity consistency but significantly harms vividness and diversity. In addition, HeyGen crops the reference image to fixed horizontal or vertical resolutions for generation, while our method supports arbitrary input and output resolutions, producing videos up to 1080p at 48 fps. Moreover, HeyGen is tailored for digital human scenarios, whereas our approach is developed on top of general video generation foundation model, making it more extensible and adaptable to broader future applications. We further provide visual comparison of lip synchronization accuracy in Fig. 5. Our method demonstrates precise correspondence between lip shapes and various syllables, whereas the baseline methods struggle with accurate alignment and sometimes even fail to respond. Results on Diverse Scenarios. Fig. 6 showcases our diverse generation results. Benefiting from the high-level planning produced by the MLLM Director through the understanding and integration of multimodal instruction intents, our method faithfully adheres to the input signals and delivers vivid character emotions, actions, camera movements, and accurate, fine-grained lip synchronization. Moreover, it demonstrates strong generalization to various open scenarios, including multi-persons, cartoon and anime styles, and even non-human characters. Please refer to our project page for more compelling video results. Long-Duration Video Synthesis. We further demonstrate the advantages of our cascaded parallel generation framework in long-duration video synthesis. As shown in Fig. 7, we sample one frame every 10 seconds to visualize the results. The generated frames exhibit stable identity preservation, coherent visual quality, and rich character dynamics. Notable examples include the background lighting changes in the first line, the head movements in the second, and the hand gestures in the third."
        },
        {
            "title": "4 Related Work",
            "content": "4.1 Video Generation Breakthroughs in diffusion models for image synthesis (Ho et al., 2020; Dhariwal &Nichol, 2021; Rombach et al., 2022) have driven an evolution in video generation, where scalable training paradigms based on noise inversion and conditional denoising have made high-fidelity appearance and controllability attainable. Early video generation approaches typically extend pretrained image-based U-Nets by stacking temporal modules or inserting temporal attention into spatial backbones to capture crossframe relations (Ho et al., 2022; Singer et al., 2022; Blattmann et al., 2023). However, such designs face limitations in scalability in terms of resolution and sequence length. Recently, the growth of training data and computational resources has shifted the focus toward Diffusion Transformers (DiT) (Peebles &Xie, 2023; Yang et al., 2025; Wan et al., 2025). This paradigm compresses videos into spatiotemporal tokens via 3D VAEs, and leverages the large context capacity and scalable attention of transformers to capture temporal dynamics, thus supporting stable large-scale video generation and establishing itself as the emerging mainstream approach. It also demonstrates strong potential for long-term generation (Kong et al., 2024; Zhang &Agrawala, 2025; Teng et al., 2025; Chen et al., 2025b), real-time synthesis (Zhao et al., 2025; Gu et al., 2025), and world modeling (Yu et al., 2025; Team et al., 2025). These methods are primarily designed for general video generation yet remain inadequate for speech-driven digital portrait modeling. 4.2 Audio-Driven Digital Human Synthesis Audio-driven digital human synthesis aims to generate realistic and expressive talking videos conditioned on speech signals and reference portrait image. One line of work employs explicit intermediate representations, such as facial landmarks or 3D head models, to drive facial expressions and lip movements (Chen et al., 2025a; Hu et al., 2025; Guo et al., 2024; Cui et al., 2025c). However, these approaches are typically limited to facial animation and cannot produce natural upper-body motion or hand gestures. More recent studies leverage diffusion models for end-to-end audio-driven video generation. By directly injecting speech as condition into diffusion transformers, these methods achieve joint alignment and control of audio, expressions, and motion within unified attention framework (Jiang et al., 2025; Gan et al., 2025; Peng et al., 2025; Wang et al., 2025a; Guo et al., 2024), enabling realistic and coherent video synthesis without relying on 3D priors, and showing advantages in expression detail and lipaudio alignment. To further support hand motion and humanobject interactions, methods such as Emo2 (Tian et al., 2025) and HunyuanVideo-HOMA (Huang et al., 2025) incorporate pose sequences as conditions alongside speech and body dynamics. Other approaches such as Mocha (Wei et al., 2025), MultiTalk (Kong et al., 2025) and InteractHuman (Wang et al., 2025b), learn identity information or memory-slot IDs to enable speaker switching and cross-shot localization. Additional efforts explore data scaling (Lin et al., 2025), audiovideo alignment strategies (Gao et al., 2025), and direct performance optimization (Cui et al., 2025a). Despite these advances, existing methods still rely on local cues for alignment within each modality, and thus struggle with multimodal instruction understanding and consistent long-duration generation. To address these challenges, we explore the use of multimodal large language models for instruction grounding and propose cascaded framework for fast synthesizing vivid, long-duration portrait animations."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce Kling-Avatar, cascaded framework that unifies multimodal instruction understanding with long-duration generation of lifelike portrait videos. Our two-stage pipeline first employs an MLLM director to produce blueprint video that encodes high-level semantic intent into coherent storyline, and then synthesizes long videos through parallel sub-clip generation guided by blueprint keyframes to refine local dynamics. Coupled with carefully curated data and practical training and inference strategies, our framework preserves fine-grained details while faithfully realizing global semantics. To evaluate the effectiveness, we construct 375-sample benchmark spanning diverse instructions and challenging scenarios. Experiments demonstrate that Kling-Avatar delivers vivid, fluent videos up to 1080p and 48 fps, with precise lip synchronization, strong controllability, and robust generalization to open scenarios. Human preferencebased metric comparisons further confirm our superior performance. We believe our exploration of instruction-grounded, long-duration avatar video generation represents promising step toward broad real-world applications and future research."
        },
        {
            "title": "References",
            "content": "Hedra AI. URL https://www.hedra-ai.com/. HeyGen. URL https://www.heygen.com/. Higgsfield AI. URL https://higgsfield.ai/avatars. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Breakthrough. Pyscenedetect. 2023. URL https://github.com/Breakthrough/PySceneDetect. Hejia Chen, Haoxian Zhang, Shoulong Zhang, Xiaoqiang Liu, Sisi Zhuang, Pengfei Wan, Di ZHANG, Shuai Li, et al. Cafe-talk: Generating 3d talking face animation with multimodal coarse-and fine-grained control. In The Thirteenth International Conference on Learning Representations, 2025a. Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Xiaoqiang Liu, and Pengfei Wan. Midas: Multimodal interactive digital-human synthesis via real-time autoregressive video generation. arXiv preprint arXiv:2508.19320, 2025b. Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 24032410, 2025c. Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian conference on computer vision, pp. 251263. Springer, 2016. Jiahao Cui, Yan Chen, Mingwang Xu, Hanlin Shang, Yuxuan Chen, Yun Zhan, Zilong Dong, Yao Yao, Jingdong Wang, and Siyu Zhu. Hallo4: High-fidelity dynamic portrait animation via direct preference optimization and temporal motion modulation. arXiv preprint arXiv:2505.23525, 2025a. Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2108621095, 2025b. Liyuan Cui, Xiaogang Xu, Wenqi Dong, Zesong Yang, Hujun Bao, and Zhaopeng Cui. Cfsynthesis: Controllable and free-view 3d human video synthesis. In Proceedings of the 2025 International Conference on Multimedia Retrieval, pp. 135144, 2025c. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Zhengcong Fei, Hao Jiang, Di Qiu, Baoxuan Gu, Youqiang Zhang, Jiahua Wang, Jialin Bai, Debang Li, Mingyuan Fan, Guibin Chen, et al. Skyreels-audio: Omni audio-conditioned talking portraits in video diffusion transformers. arXiv preprint arXiv:2506.00830, 2025. Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, and Steven Hoi. Omniavatar: Efficient audio-driven avatar video generation with adaptive body animation. arXiv preprint arXiv:2506.18866, 2025. Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, et al. Wan-s2v: Audio-driven cinematic video generation. arXiv preprint arXiv:2508.18621, 2025. Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. arXiv preprint Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv:2407.03168, 2024. 11 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pp. arXiv2507, 2025. Wentao Hu, Shunkai Li, Ziqiao Peng, Haoxian Zhang, Fan Shi, Xiaoqiang Liu, Pengfei Wan, Di Zhang, and Hui Tian. Ggtalker: Talking head systhesis with generalizable gaussian priors and identity-specific adaptation. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. Ziyao Huang, Zixiang Zhou, Juan Cao, Yifeng Ma, Yi Chen, Zejing Rao, Zhiyong Xu, Hongmei Wang, Qin Lin, Yuan Zhou, et al. Hunyuanvideo-homa: Generic human-object interaction in multimodal driven human animation. arXiv preprint arXiv:2506.08797, 2025. Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634, 2024. Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, and Mingyuan Gao. Omnihuman-1.5: Instilling an active mind in avatars via cognitive simulation. arXiv preprint arXiv:2508.19209, 2025. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let them talk: Audio-driven multi-person conversational video generation. arXiv preprint arXiv:2505.22647, 2025. Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Ziqiao Peng, Jiwen Liu, Haoxian Zhang, Xiaoqiang Liu, Songlin Tang, Pengfei Wan, Di Zhang, Hongyan Liu, and Jun He. Omnisync: Towards universal lip synchronization via diffusion transformers. arXiv preprint arXiv:2505.21448, 2025. Ji Qi, Yuan Yao, Yushi Bai, Bin Xu, Juanzi Li, Zhiyuan Liu, and Tat-Seng Chua. An lmm for efficient video understanding via reinforced compression of video cubes. arXiv preprint arXiv:2504.15270, 2025. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. URL https://arxiv.org/abs/2212.04356. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Christoph Schuhmann. Aesthetic score predictor. 2022. URL https://github.com/christophschuhmann/ improved-aesthetic-predictor. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2022. 12 Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, et al. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint arXiv:2507.21809, 2025. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pp. 244260. Springer, 2024. Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng Bo. Emo2: End-effector guided audio-driven avatar video generation. arXiv preprint arXiv:2501.10687, 2025. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, and Mu Xu. Fantasytalking: Realistic talking portrait generation via coherent motion synthesis. arXiv preprint arXiv:2504.04842, 2025a. Zhenzhi Wang, Jiaqi Yang, Jianwen Jiang, Chao Liang, Gaojie Lin, Zerong Zheng, Ceyuan Yang, and Dahua Lin. Interacthuman: Multi-concept human animation with layout-aligned audio conditions. arXiv preprint arXiv:2506.09984, 2025b. Cong Wei, Bo Sun, Haoyu Ma, Ji Hou, Felix Juefei-Xu, Zecheng He, Xiaoliang Dai, Luxin Zhang, Kunpeng Li, Tingbo Hou, et al. Mocha: Towards movie-grade talking character synthesis. arXiv preprint arXiv:2503.23307, 2025. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 42104220, 2023. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. In The Thirteenth International Conference on Learning Representations, 2025."
        }
    ],
    "affiliations": [
        "Kuaishou Technology"
    ]
}