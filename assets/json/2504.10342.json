{
    "paper_title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge",
    "authors": [
        "Yueqi Song",
        "Tianyue Ou",
        "Yibo Kong",
        "Zecheng Li",
        "Graham Neubig",
        "Xiang Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with \"thinking\" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 2 2 4 3 0 1 . 4 0 5 2 : r Submission In Progress VISUALPUZZLES: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue {yueqis,tianyueo,gneubig,xyue2}@cs.cmu.edu"
        },
        {
            "title": "Carnegie Mellon University",
            "content": "https://neulab.github.io/VisualPuzzles/"
        },
        {
            "title": "Abstract",
            "content": "Current multimodal benchmarks often conflate reasoning with domainspecific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VISUALPUZZLES, benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VISUALPUZZLES consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VISUALPUZZLES requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VISUALPUZZLES, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with thinking modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VISUALPUZZLES compared to benchmarks with heavier emphasis on knowledge. VISUALPUZZLES offers clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge. Figure 1: Model accuracy on VISUALPUZZLES compared to human performance percentiles. All evaluated models fall below the human 5th percentile (57.5%), highlighting the difficulty of VISUALPUZZLES. Interestingly, models with explicit thinking modes do not consistently outperform their base versions, suggesting that current reasoning strategies do not yet generalize well to VISUALPUZZLESs scenarios, even though these strategies have proven effective in existing reasoning tasks that often rely heavily on domain-specific knowledge. Equal Contributions. Equal Contributions. 1 Submission In Progress Figure 2: Example VISUALPUZZLES instances within each reasoning category"
        },
        {
            "title": "Introduction",
            "content": "Reasoning is cornerstone of both human and artificial intelligence, enabling systems to solve problems, draw inferences, and make decisions from information. Recent advances in multimodal large language models (MLLMs) (OpenAI, 2024; Liu et al., 2023a; Li et al., 2024; Dubey et al., 2024; Qwen Team, 2025a; Yue et al., 2025) exhibit early signs of reasoning in tackling complex tasks such as answering expert-level visual questions (Yue et al., 2024a;b), interpreting scientific diagrams (Roberts et al., 2024), and solving challenging math word problems (Lu et al., 2023). Many of the tasks mentioned above are inherently knowledge-intensive; large amounts of knowledge in domains such as science or math are necessary to answer questions correctly (Yue et al., 2024a). However, in reality, reasoning does not necessitate knowledge. Even non-expert humans can successfully solve logic puzzles, spatial reasoning problems, and analogical tasks using general inferential skills, without requiring deep domain expertise. This raises an important question: Can we measure MLLMss reasoning ability independently of measuring their acquisition of domain-specific knowledge? This question is particularly important with the recent rapid development of reasoning models in the textual domain (Jaech et al., 2024; DeepSeek-AI, 2025; Qwen Team, 2025b), and emerging application to the visual domain (Qwen Team, 2024). To address this question, we introduce VISUALPUZZLES, multimodal benchmark explicitly crafted to assess reasoning capabilities independent of specialized knowledge. VISUALPUZZLES comprises 1,168 carefully curated puzzle-like questions that span five distinct categories of reasoning: algorithmic, analogical, deductive, inductive, and spatial, each annotated with varying difficulty levels. VISUALPUZZLES only requires basic common knowledge and the information presented in the question to solve problems, disentangling reasoning from domain-specific knowledge recall. Our experiments show that VISUALPUZZLES requires significantly fewer domain-specific knowledge concepts compared to benchmarks like MMMU, and models have sufficient knowledge required to solve VISUALPUZZLES questions, enabling us to better assess multimodal reasoning versus pretrained factual knowledge. While VISUALPUZZLES minimizes reliance on domain expertise, its reasoning complexity exceeds that of existing benchmarks: in VISUALPUZZLES, 82.1% of models solution steps are logical reasoning steps, compared to 71.5% in MMMU. Additionally, no current MLLM surpasses even the 5th-percentile human performance, highlighting the benchmarks difficulty and the limitations of todays models in general-purpose visual reasoning. 2 Submission In Progress Our experiments with VISUALPUZZLES reveal critical limitations in current MLLMs multimodal reasoning ability by factoring out domain-specific knowledge requirements and only focusing on reasoning. Specifically, we uncover four key findings: Strong performance on knowledge-heavy benchmarks does not transfer well. Models that rank highly on MathVista and MMMU often experience substantial performance drops on VISUALPUZZLES, highlighting disconnect between knowledge-rich and knowledge-light multimodal reasoning tasks. Humans outperform models on easy and medium tasks, while both degrade on harder ones. Human participants show strong and consistent performance on easy and mediumlevel questions across reasoning categories. In contrast, models struggle even on simpler tasks. Reasoning enhancements (e.g., long CoT and thinking mode) yield inconsistent gains. While explicit reasoning strategies help certain models tackle complex reasoning tasks, these techniques do not consistently improve performance across all model families and task types. Scaling model size does not ensure stronger reasoning. We observe no clear trend indicating that larger models outperform smaller ones on VISUALPUZZLES, suggesting that scaling up parameters alone is insufficient to improve domain-agnostic multimodal reasoning."
        },
        {
            "title": "2 VISUALPUZZLES",
            "content": "2.1 Motivation and Design Principles of VISUALPUZZLES Existing benchmarks often conflate multimodal reasoning with domain-specific knowledge, making it difficult to isolate and measure the pure reasoning capabilities of these models. VISUALPUZZLES is designed to explicitly address this issue by providing testbed focused on evaluating multimodal reasoning in isolation from specialized knowledge. Specifically, VISUALPUZZLES centers on puzzle-like questions that rely solely on the provided image, question text, and basic common-sense reasoning. The core design principle behind VISUALPUZZLES is to limit the need for external or pretrained domain knowledge. Figure 2 shows examples of VISUALPUZZLES within each reasoning category. 2.2 Data Collection and Curation We curated VISUALPUZZLES using multi-stage pipeline. The process involved sourcing, adapting, and validating questions with an emphasis on reasoning quality and minimal reliance on specialized knowledge. Question Sourcing. We collected questions from three primary sources: (1) online resources and textbooks focused on logical, visual, and spatial puzzles, (2) synthesized items using images from large-scale vision datasets paired with text prompts, and (3) carefully repurposed items from existing multimodal reasoning benchmarks. Each source was selected to ensure wide variety of reasoning challenges while avoiding trivial or fact-heavy questions. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination1. Other sources are listed in Appendix A. Format Adaptation. All collected items were adapted into consistent multiple-choice format with four options, balancing between text-based and image-based answer choices. This modality balance allows us to better test models abilities to perform reasoning across diverse formats. Data Validation. During curation, we applied strict filtering criteria to eliminate questions requiring advanced mathematical knowledge, specialized domain knowledge and facts. Questions were retained only if they could be solved using information present in the image, 1 Chinese Civil Service Examination (Logic Test), 中国国家公务员考试行测逻辑推理 3 Submission In Progress the question prompt, and basic common sense. multi-round validation process was conducted by human annotators, focusing on question clarity, solvability, and reasoning type classification. Attribute Annotation. Finally, each question was annotated with two key attributes: Reasoning Category: Each item was categorized as algorithmic, analogical, deductive, inductive, or spatial reasoning. These five categories were selected as they represent fundamental forms of reasoning widely discussed in literature (Liu et al., 2020; Lu et al., 2023; Yue et al., 2024a; Gao et al., 2023). At the same time, we aimed to balance comprehensiveness with conciseness, avoiding an overly fine-grained taxonomy that could dilute the benchmarks clarity and usability. This categorization ensures that VISUALPUZZLES covers broad yet manageable set of reasoning skills relevant to multimodal LLM evaluation. Algorithmic Reasoning involves reasoning over algorithmic rules. Analogical Reasoning requires analyzing the relationships between pair of entities. Deductive Reasoning involves logically drawing conclusions from known premises. Inductive Reasoning focuses on generalizing rules from observed patterns. Spatial Reasoning requires interpreting and manipulating spatial relationships. Difficulty Level: Labeled as easy, medium, or hard, based on annotators estimated cognitive load and time-to-solve metrics. This pipeline ensures that VISUALPUZZLES presents diverse set of high-quality questions designed to challenge multimodal LLMs on their reasoning abilities without involving pretrained domain knowledge. 2.3 Dataset Statistics VISUALPUZZLES comprises 1,168 multimodal reasoning puzzles. It is designed to provide balanced distribution across different reasoning categories, difficulty levels, and option formats for comprehensive evaluation. The statistics of VISUALPUZZLES are shown in Table 1. Category Statistics Total Questions - Algorithmic Reasoning - Analogical Reasoning - Deductive Reasoning - Inductive Reasoning - Spatial Reasoning 1168 262 211 200 209 286 Easy/Medium/Hard Option Type (Image/Text) AVG. Question Length % Easy Words 46%/39%/15% 57%/43% 154.9 54% Across the five reasoning types, we maintain roughly even distribution, ensuring that no single reasoning style dominates the benchmark. Similarly, we balanced the dataset across the three difficulty levels (easy, medium, hard) to capture wide spectrum of cognitive demands. Approximately half of the answer choices in the dataset are image-based and the other half are text-based, enabling evaluation of models abilities to reason across diverse query formats. Table 1: Statistics of VISUALPUZZLES In terms of language complexity, VISUALPUZZLES was constructed with an emphasis on accessibility. Most of the question text uses Basic English vocabulary2 to minimize the impact of linguistic complexity on reasoning performance, focusing the evaluation strictly on multimodal reasoning. Compared to prior benchmarks, VISUALPUZZLES is unique in that it explicitly minimizes domain-specific knowledge requirements while maintaining high reasoning complexity. We demonstrate these traits of VISUALPUZZLES in Section 5."
        },
        {
            "title": "3 Experiments and Results",
            "content": "3.1 Experimental Setup We comprehensively evaluated the reasoning abilities of variety of MLLMs on VISUALPUZZLES. Additionally, we performed human evaluations to better understand the gap between human and models reasoning capabilities. 2https://en.wiktionary.org/wiki/Appendix:Basic English word list 4 Submission In Progress We selected diverse set of proprietary and open MLLMs to ensure broad coverage in terms of model architecture, training scale, and intended application domains. This diversity allows us to capture wide spectrum of current approaches and capabilities in the field. We integrated VISUALPUZZLES into Lmms-eval (Li* et al., 2024). Proprietary Models. We evaluate several leading proprietary models that represent the current state of the art: (1) GPT-4o (OpenAI, 2024) and o1 (Jaech et al., 2024); (2) Gemini1.5-Pro, Gemini-2.0-Flash, Gemini-2.0-Flash-Thinking, and Gemini-2.5-Pro (Gemini et al., 2023); (3) Claude-3.5-Sonnet and Claude-3.7-Sonnet (Anthropic, 2022). Among these, o1 is explicitly optimized for reasoning, while Gemini-2.0-Flash-Thinking and Claude-3.7-Sonnet incorporate dedicated modules for extensive step-by-step problem-solving. Open Models. We further evaluate widely used open MLLMs to gauge how open models compare against proprietary models: (1) LLaVA Series (Liu et al., 2023a; 2024a; Li et al., 2024): LLaVA-1.5 (7B/13B), LLaVA-1.6 (7B/13B/34B), and LLaVA-OV (0.5B/7B/72B); (2) Llama-3.2-Vision-Instruct (11B/90B) (Dubey et al., 2024); (3) Qwen-VL Series (Bai et al., 2024; Yang et al., 2024; Qwen Team, 2025a; 2024): including Qwen-VL, Qwen2-VL (2B/7B/72BInstruct), Qwen2.5-VL (3B/7B/72B-Instruct), and QvQ-72B-Preview; (4) Cambrian (8B/13B) (Tong et al., 2024); (5) Pangea-7B (Yue et al., 2025). We apply both direct multiple-choice prompting and Chain-of-Thought (CoT) prompting to each model, following recent findings that CoT can significantly enhance model reasoning on complex multimodal tasks. For each model we report the best performance, whether achieved by direct multiple-choice prompting or CoT prompting. Human Performance. To establish strong baseline for comparison, we conducted human evaluations with 70 college-level volunteers. Human performance provides valuable upper-bound reference for assessing the current capabilities and limitations of multimodal reasoning models. While this serves as benchmark for present-day systems, it is possible that future models could surpass this level of performance. Each participant was randomly assigned subset of the puzzles and completed them under the same resource-constrained conditions as the models (i.e., without access to external tools or the internet). On average, participants completed each puzzle in 78 seconds, reflecting the typical cognitive load and time demands imposed by VISUALPUZZLES. 3.2 Overall Results Table 2 and Figure 1 compare the performance of humans and selected set of models.3 All evaluated models, even the proprietary ones, perform below the 4th percentile of human accuracy, underscoring the significant gap in multimodal reasoning abilities. These results reinforce our finding that, although models have made progress in multimodal understanding, there remains substantial margin for improvement before they can match or surpass human performance on multimodal reasoning. This pattern holds across categories as well. In Table 2, top human participants (95th percentile) exhibit near-perfect accuracy on multiple reasoning categories, while model performance remains substantially lower, even lower than the worst human performance (5th percentile). These results emphasize the need for continued innovation in model architectures and training paradigms if we aim to close the gap between model and human intelligence on complex multimodal reasoning."
        },
        {
            "title": "4 Disentangling Reasoning from Domain Knowledge",
            "content": "4.1 Knowledge Intensity of VISUALPUZZLES Is VISUALPUZZLES less knowledge-intensive than existing reasoning benchmarks? This question is central to our goal of disentangling reasoning ability from domain-specific knowledge. Many current benchmarks blur this line, making it difficult to assess general 3Full results for every model discussed in Section 3 are provided in Appendix D, including separate performance outcomes for both direct multiple-choice and CoT prompting. 5 Submission In Progress Model Algorithmic Analogical Deductive Inductive Spatial Overall Random Choice Human (95th Percentile) Human (50th Percentile) Human (5th Percentile) GPT-4o o1 Gemini-2.0-flash Gemini-2.0-flash-thinking Gemini-2.5-pro Claude-3.7-Sonnet Claude-3.7-Sonnet-Thinking LLaVA-OV-7B Pangea-7B Qwen2.5-VL-7B-Instruct LLaVA-OV-72B QvQ-72B-Preview Qwen2.5-VL-72B-Instruct 25.0 100.0 88.0 68.1 49.2 63.7 55.3 46. 60.0 64.5 67.2 25.0 100.0 66.0 25.0 Proprietary Models 58.3 68.3 58.8 70. 64.0 48.3 44.1 25.0 100.0 80.0 37.0 49.0 67.5 57.0 49.0 60. 65.0 61.5 Open Models (Qwen-Based) 27.5 32.4 38.2 34.7 44.8 53.4 28.0 23.7 23.7 26.5 43.6 46.9 40.5 38.5 51.5 37.0 44.0 58.0 Open Models (Llama-Based) Cambrian-8B Llama-3.2-11B-Vision-Instruct Llama-3.2-90B-Vision-Instruct 31.3 31.0 45.0 24.2 30.8 23.2 36.0 39.0 43.0 25.0 81.6 50.0 0.0 27.3 29. 24.4 24.9 29.7 26.8 31.1 24.4 28.7 24.9 27.3 26.8 25.8 24.0 21.1 26.3 25.0 100.0 90.0 59. 26.2 34.3 31.8 25.5 36.4 37.4 37.1 28.0 32.5 31.1 28.7 30.8 29.5 29.0 26.2 31. 25.0 89.3 75.0 57.5 41.3 51.8 45.0 42.2 49.5 48.3 48.2 29.4 31.3 33.7 30.8 37.8 42. 28.9 29.4 34.1 Table 2: Performance (%) comparison of humans and selected models on VISUALPUZZLES. We report the best performance resulting from direct multiple-choice prompting and CoT prompting for each method. We highlighted all the reasoning models . reasoning in non-expert settings. VISUALPUZZLES was designed to target visual reasoning skills while deliberately minimizing reliance on specialized knowledge. To test whether VISUALPUZZLES achieves this goal, we prompted GPT-4o to generate knowledge concept checklists for 50 randomly selected questions from widely-used knowledge-intensive reasoning dataset MMMU and 50 from VISUALPUZZLES. We manually verified each question as discussed in subsection E.3. Each checklist comprises knowledgespecific questions intended to assess whether model possesses the background information required to solve the original problem. For example, if question depends on understanding two distinct physics laws, its checklist would include question to explain each. The number of checklist items per instance serves as proxy for knowledge intensivity. Benchmark We found that MMMU problems resulted in significantly more checklist items on average (3.9) compared to VISUALPUZZLES (1.1), as shown in Table 3. This supports the hypothesis that VISUALPUZZLES is substantially less reliant on domain knowledge. As result, performance on VISUALPUZZLES more directly reflects models ability to reason over visual and textual content, offering clearer signal of progress in multimodal reasoning. Full prompt examples and further discussion are provided in Appendix E. Table 3: AVG. number of knowledge concept questions generated per instance on MMMU vs. VISUALPUZZLES. MMMU VISUALPUZZLES # Knowledge Qs. 3.9 1.1 Do models already possess the knowledge required to solve VISUALPUZZLES? To explore this, we measured models knowledge accuracytheir ability to answer the knowledge checklist questions correctlyon both benchmarks. This metric reflects how much of the required knowledge is already known by the model, independent of reasoning. We found stark contrast: while many models exceed 90% knowledge accuracy on VISUALPUZZLES, most score below 60% on MMMU, with smaller models frequently dropping under 50%. Only the largest models approach 80% accuracy on MMMU, underscoring its heavier reliance on domain-specific knowledge. Submission In Progress Figure 3: Scatter plots with trend lines of the relationship between accuracy and model size (top) and the relationship between reasoning and knowledge accuracy (bottom) on MMMU and VISUALPUZZLES. The dots sizes represent relative model sizes. The correlation between reasoning accuracy and knowledge accuracy is higher on MMMU (0.8) than on VISUALPUZZLES (0.4). Does scaling up model size improve performance? We also plot reasoning accuracy (i.e., overall performance on the benchmark) in Figure 3, revealing some interesting trends: MMMU. Larger models tend to have higher knowledge accuracy, and this often translates into higher overall benchmark performance. This aligns with MMMUs reliance on domain-specific understanding; models with more parameters and training data are better at recalling relevant factual knowledge, thus improving their overall performance. VISUALPUZZLES. Although many models achieve near-100% knowledge accuracy on VISUALPUZZLES, we observe no clear increase in both knowledge and reasoning accuracy as model size grows. In contrast to MMMU, simply scaling number of parameters does not guarantee better performance on VISUALPUZZLES, implying that further gains on VISUALPUZZLES must stem from improvements in models reasoning abilities rather than reliance on extensive knowledge. What is the relationship between knowledge and reasoning? Figure 3 shows two scatter plots with trend lines that measure how knowledge accuracy correlates with reasoning accuracy across different open models, where the relative sizes of the dots represent the sizes of the models. On MMMU (left), there is strong positive correlation (0.8), suggesting that model possessing more knowledge strongly correlates better reasoning performance. In contrast, VISUALPUZZLES (right) exhibits more modest correlation (0.4). Although there is still an upward trend, gains in knowledge accuracy lead to smaller improvements in reasoning accuracy. This discrepancy implies that while overcoming knowledge gaps is central to reasoning success on MMMU, VISUALPUZZLES tasks demand more nuanced inference steps that depends less on domain knowledge. Overall, these findings reinforce that VISUALPUZZLESs comparatively lower knowledge requirements are readily met by both proprietary and open models. By contrast, MMMU poses greater challenge to smaller models in terms of knowledge, for which scaling in size 7 Submission In Progress clearly benefits knowledge-intensive tasks. However, on VISUALPUZZLES, larger model size alone is not decisive factor, which might imply that genuine multimodal reasoning depends on more than just number of parameters or pre-trained knowledge. 4.2 Reasoning Complexity of VISUALPUZZLES Do questions in VISUALPUZZLES require more complex reasoning than those in existing benchmarks like MMMU? Model 75.1% 67.9% MMMU VISUALPUZZLES GPT-4o Gemini-2.0-Flash Besides observing that models generally achieve lower accuracy on VISUALPUZZLES compared to MMMU, we further investigated whether this gap stems from increased reasoning complexity. To do so, we measured the proportion of reasoning steps required to solve each question. We began by gathering detailed, step-by-step solutions from the models for each question, which are manually verified for completeness. Then we classified if each step is logical reasoning step with the help of LLM. We show the result in Table 4. On average, logical reasoning steps take up 14.8% more total steps in solving VISUALPUZZLES questions compared to those of MMMU (82.1% v.s. 71.5%). This analysis is based on GPT-4o and Gemini-2.0-Flash across 200 randomly sampled questions per benchmark. These results suggest that VISUALPUZZLES demand more extensive reasoning, aligning with its goal of evaluating deeper multimodal reasoning beyond factual recall. Prompt example is shown in Appendix F. Table 4: Percentage of logical reasoning steps in solving benchmark questions. 87.0% 77.3% 4.3 Do Reasoning Models Perform Better than Their Baselines? Figure 4: Comparison of accuracy and average number of total completion tokens of reasoning models and their general counterparts on VISUALPUZZLES. We didnt include Gemini-2.0-Flash models here because Gemini-2.0-Flash-Thinking does not reveal the number of reasoning tokens of responses. The accuracies of Gemini-2.0-Flash and Gemini-2.0Flash-Thinking is 45.0% and 42.2% respectively. Despite much higher number of completion tokens, reasoning models do not often achieve better performance on VISUALPUZZLES. Recent reasoning models often scale up inference compute by generating longer chains of thought (CoTs) to enhance reasoning ability. To assess the effectiveness of this strategy on VISUALPUZZLES, we compare several reasoning models with their non-reasoning counterparts in Figure 4. The reasoning model o1 outperforms GPT-4o overall. However, structured thinking modes, despite much higher number of completion tokens, show no consistent benefit. Similarity of output further reveals that the thinking mode primarily increases verbosity without meaningfully altering the underlying reasoning process, as illustrated in Figure 13. 4.4 Are Branching and Revalidation Reasoning Patterns Effective on VISUALPUZZLES? As discussed in Section 4.3, reasoning-enabled models do not consistently outperform their non-reasoning counterparts on VISUALPUZZLES. To better understand this discrepancy, we examine Claude-3.7-Sonnet-Thinkings reasoning behaviors present in long CoTs, specifiSubmission In Progress Figure 5: Comparison of Reasoning Pattern of Claude-3.7-Sonnet-Thinking on MMMU and VISUALPUZZLES. Left figure compares the accuracy of Claude-3.7-Sonnet and Claude-3.7Sonnet-Thinking on MMMU and VISUALPUZZLES. Middle figure shows frequency of each pattern. Right figure shows correlation of the patterns with accuracy on the benchmarks. cally, branching and re-validation, which are known to play important roles in enhancing reasoning performance4. As shown in Figure 5, our analysis reveals striking contrast between benchmarks. On MMMU, both branching and re-validation correlate positively with model accuracy. These strategies help models explore alternative reasoning paths and revisit earlier steps, aiding in the retrieval of relevant factual knowledge,an essential component for solving MMMUs knowledge-intensive questions. An illustrative example is provided in Appendix E. Surprisingly, on VISUALPUZZLES, these reasoning behaviors are more frequent, yet less predictive of success. Despite their increased presence in long-form responses, we observe no significant correlation between these strategies and task accuracy. This suggests that models may be using branching and re-validation in ways that do not meaningfully contribute to solving the problem. Figure 6 highlights this with an example from Claude-3.7-Sonnet-Thinking, where the model applies branching on VISUALPUZZLES puzzle. However, the additional reasoning paths remain shallow and fail to engage with the core challengeunderstanding the spatial arrangement of chairs in the image. The full response is included in Appendix E."
        },
        {
            "title": "5 Analysis",
            "content": "Figure 6: An example of Claude-3.7-SonnetThinking utilizing branching to solve VISUALPUZZLES puzzle. 5.1 Do Models Approach VISUALPUZZLES Questions Differently? MMMU Benchmark Answer-First Option-First Table 5 shows the statistics of Claude-3.7-Sonnet-Thinkings answering strategy. We observe clear divergence in answering strategies between MMMU and VISUALPUZZLES. On MMMU, the model tend to follow an option-driven approachusing the provided choices early to eliminate unlikely answers and select the most relevant one, often without explicitly solving the problem. In contrast, models more frequently adopt an answer-first strategy on VISUALPUZZLES, attempting to solve the question independently before comparing the result to the answer choices. This pattern holds across both textual and image-based VISUALPUZZLES (Image Options) VISUALPUZZLES (Text Options) Table 5: Answering Strategy 72.5% 98.3% 27.5% 1.7% 29.3% 70.7% 4We examined Claude-3.7-Sonnet-Thinking as it explicitly provides thinking output. 9 Submission In Progress options, though the option-first approach appears slightly more often (around 30%) for image-based taskslikely due to the added complexity of visual comparison. 5.2 Does model performance transfer between reasoning categories? Figure 7 presents correlation heatmap illustrating the relationships among the five reasoning categories in VISUALPUZZLES. We report model correlations averaged across all models in Table 2. For humans, each reasoning category likely engages different cognitive or mental processes (Goel & Dolan, 2004; Green et al., 2010; Bright & Feeney, 2014; Babcock & Vallesi, 2015), so performance in one category might not transfer to performance in another. However, the correlation heatmap of the models tells different story. We observe notably strong correlations across reasoning categories, with values ranging from 0.11 to as high as 0.94. In particular, algorithmic and deductive reasoning show high correlation (0.94), and other pairs such as algorithmic-analogical and deductive-analogical also exhibit strong associations. This suggests that model performance tends to generalize across categories. However, this generalization may not reflect true reasoning abilities. Instead, the high correlations could indicate that models are leveraging shared surface-level patterns or shortcut strategies that happen to work across multiple structurally different categories, unlike humans, who may rely on distinct cognitive processes. Figure 7: Correlation Heatmap among reasoning categories for models (averaged across all models we evaluated). 5.3 Error Analysis Figure 8 shows pie chart illustrating the distribution of error categories of 100 instances generated by Claude3.7-Sonnet-Thinking on VISUALPUZZLES, revealing that reasoning errors dominate at 56%, reinforcing the fact that reasoning is greatest challenge to models in VISUALPUZZLES. Perceptual errors (21%) and spatial / orientation errors (17%) also constitute substantial portions of failures, reflecting difficulties in interpreting visual elements and understanding spatial relationships. These three categories together account for 94% of mistakes, emphasizing need for multimodal models with stronger reasoning capabilities with more robust perception and spatial understanding. Textual and visual understanding errors (4%) and reject-to-answer cases (2%) are relatively rare. Appendix shows samples of error and correct cases of each reasoning and difficulty category."
        },
        {
            "title": "6 Related Work",
            "content": "Figure 8: Error Distribution of Claude-3.7-Sonnet-Thinking Multimodal Language Models (MLLMs), particularly vision language models have experienced significant improvements recently. Large scale vision language models (Gemini et al., 2023); (OpenAI, 2024); (Anthropic, 2022); including open weight ones (Li et al., 2024); (Yue et al., 2025); (Liu et al., 2024b); (Tong et al., 2024); (Dubey et al., 2024) are capable of utilizing both image and text inputs to solve challenging questions. Multimodal reasoning models, models that specialize in complex reasoning, further push the boundary of MLLMs capabilities. Large scale multimodal reasoning models such as 10 Submission In Progress QVQ (Qwen Team, 2024), Claude-3.7-Sonnet-thinking (Anthropic, 2022), o1 (Jaech et al., 2024), Gemini-2.0-flash-thinking (Gemini et al., 2023) excel in reasoning heavy tasks such as coding and solving math problems. Multimodal Reasoning Benchmarks. There exists number of multimodal benchmarks that test on both the models world knowledge and reasoning abilities. These benchmarks (Yue et al., 2024a); (Marino et al., 2019); (Liu et al., 2023b); (Yue et al., 2024b); (Authors, 2025) emphasize on the multimodal ability of models as whole, without further separation of knowledge and reasoning. Recently, more multimodal benchmarks have placed emphasis on multimodal logical reasoning abilities. Many of them (Lu et al., 2023); (Wang et al., 2024b) focus primarily on mathematic problems, testing on both mathematical knowledge and reasoning. Some others cover on more general logical reasoning problems (Cherian et al., 2022b); (Gao et al., 2023), testing on both models knowledge and reasoning in different domains."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "We presented VISUALPUZZLES, novel multimodal benchmark carefully designed to minimize the impact of domain-specific knowledge and isolate models core reasoning capabilities. Our results show that while proprietary and large-scale open models achieve relatively higher performance, they still fall short of human-level reasoningespecially on more complex tasks such as analogical and inductive reasoning. Moreover, we observe that strong performance on knowledge-intensive benchmarks like MathVista and MMMU does not necessarily translate into high accuracy on VISUALPUZZLES, underscoring the distinct challenge of knowledge-light reasoning tasks. These findings suggest that purely scaling model size and knowledge resources may not suffice for robust multimodal reasoning skills; rather, methods that promote structured reasoning, such as explicit thinking modes or recursive reasoning steps, can offer substantial improvements, particularly for hard questions. Future research can explore new training strategies, specialized architectures, or model interpretations tailored to reduce reliance on memorized facts and enhance logical inference. Extending VISUALPUZZLES to include additional types of multi-image reasoning or temporally dynamic visual information may further stress-test models core inference abilities. By disentangling domain knowledge from multimodal reasoning, we hope VISUALPUZZLES will serve as valuable tool for developing and evaluating next-generation MLLMs that excel at genuinely understanding and reasoning about the world without depending heavily on specialized factual knowledge."
        },
        {
            "title": "8 Limitations",
            "content": "Disentangling Knowledge Despite our best efforts to isolate domain-specific knowledge from the evaluation of multimodal reasoning, VISUALPUZZLES is still not entirely free of knowledge dependencies. Basic familiarity with everyday objects or common scenarios is still required; complete knowledge free evaluation remains an ideal rather than practical reality. Real World Application VISUALPUZZLES emphasizes puzzle-like questions that may not reflect the full diversity of real-world scenarios, limiting generalizability to more specialized domains. Question Format VISUALPUZZLES focuses on multiple-choice questions, which may not capture the breadth of open-ended reasoning tasks where models must generate complex textual or visual outputs. Future work can address these limitations by including more varied question formats, broader domains, and more granular analyses of models knowledge versus its multimodal reasoning abilities. 11 Submission In Progress"
        },
        {
            "title": "9 Ethical Statement",
            "content": "This paper uses samples extracted from existing quiz sources for scholarly analysis and testing purposes, in accordance to US fair use law and standard practice. These data are neither intended for, nor capable of, substituting for the original works; thus, we believe their inclusion does not diminish the market value or utility of the source materials. complete list of references for the data sources is attached in Appendix A."
        },
        {
            "title": "Acknowledgements",
            "content": "This project was supported in part by grant from DSTA Singapore and the Carnegie Bosch Institute. The authors would like to thank CMU NeuLab colleagues for their constructive comments. The authors would also like to thank all volunteers who participated in the human evaluation."
        },
        {
            "title": "References",
            "content": "https://www.anthropic.com/index/introducing-claude Anthropic. Claude, 2022. URL https://www.anthropic.com/index/introducing-claude. Humanitys Last Exams Authors. Humanitys last exam. ArXiv, abs/2501.14249, 2025. URL https://api.semanticscholar.org/CorpusID:275906652. Laura Babcock and Antonino Vallesi. The interaction of process and domain in prefrontal cortex during inductive reasoning. Neuropsychologia, 67:9199, 2015. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: versatile vision-language model for understanding, localization, text reading, and beyond, 2024. URL https://openreview.net/forum?id= qrGjFJVl3m. Yonatan Bitton, Ron Yosef, Eliyahu Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel Stanovsky. Vasr: Visual analogies of situation recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 241249, 2023. Aimee Bright and Aidan Feeney. Causal knowledge and the development of inductive reasoning. Journal of Experimental Child Psychology, 122:4861, 2014. Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Kevin Smith, and Joshua Tenenbaum. Are deep neural networks smarter than second graders? arXiv preprint arXiv:2212.09993, 2022a. Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Kevin A. Smith, and Joshua B. Tenenbaum. Are deep neural networks smarter than second graders? 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1083410844, 2022b. URL https: //api.semanticscholar.org/CorpusID:254877678. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407. 21783. Jingying Gao, Qi Wu, Alan Blair, and Maurice Pagnucco. Lora: logical reasoning augmented dataset for visual question answering. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 12 Submission In Progress Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. ArXiv preprint, abs/2312.11805, 2023. URL https: //arxiv.org/abs/2312.11805. Vinod Goel and Raymond Dolan. Differential involvement of left prefrontal cortexin inductive and deductive reasoning. Cognition, 93(3):B109B121, 2004. Adam Green, David JM Kraemer, Jonathan Fugelsang, Jeremy Gray, and Kevin Dunbar. Connecting long distance: semantic distance in analogical reasoning modulates frontopolar cortex activity. Cerebral cortex, 20(1):7076, 2010. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Bo Li*, Peiyuan Zhang*, Kaicheng Zhang*, Fanyi Pu*, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the development of large multimoal models, March 2024. URL https://github.com/ EvolvingLMMs-Lab/lmms-eval. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. URL https://arxiv.org/abs/2310.03744. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024a. URL https: //arxiv.org/pdf/2401.13601. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning, 2020. Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, and Xiang Yue. Harnessing webpage uis for text-rich visual understanding. ArXiv, abs/2410.13824, 2024b. URL https://api.semanticscholar.org/ CorpusID:273403951. Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, 2023b. URL https://api.semanticscholar.org/CorpusID:259837088. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 31903199, 2019. URL https://api.semanticscholar.org/CorpusID:173991173. OpenAI. Hello gpt4-o. https://openai.com/index/hello-gpt-4o/, 2024. URL https:// openai.com/index/hello-gpt-4o/. Qwen Team. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm. github.io/blog/qvq-72b-preview/. Qwen Team. Qwen2.5-vl, January 2025a. URL https://qwenlm.github.io/blog/qwen2. 5-vl/. 13 Submission In Progress Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https://qwenlm.github.io/blog/qwq-32b/. Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. SciFIBench: Benchmarking large multimodal models for scientific figure interpretation. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https: //openreview.net/forum?id=HcLFNuQwy5. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. ArXiv preprint, abs/2406.16860, 2024. URL https://arxiv.org/abs/2406.16860. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset, 2024b. URL https:// arxiv.org/abs/2402.14804. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. ArXiv preprint, abs/2407.10671, 2024. URL https://arxiv.org/abs/2407.10671. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. Xiang Yue, Yueqi Song, Akari Asai, Simran Khanuja, Anjali Kantharuban, Seungone Kim, Jean de Dieu Nyandwi, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. Pangea: fully open multilingual multimodal LLM for 39 languages. In The Thirteenth International Conference on Learning Representations, 2025. URL https:// openreview.net/forum?id=a3g2l4yEys. 14 Submission In Progress"
        },
        {
            "title": "Table of Contents in Appendix",
            "content": "A VISUALPUZZLES Statistics A.1 Breakdown of Statistics of VISUALPUZZLES . . . . . . . . . . . . . . . . . . . A.2 Data Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Model Evaluation Setup Human Annotation Setup C.1 Difficulty Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Reasoning Category Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . Full Results D.1 Full Results w/ CoT . D.2 Full Results w/n CoT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Knowledge Checklist E.1 Knowledge Checklist Generation . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Example Knowledge Checklist Question . . . . . . . . . . . . . . . . . . . . . E.3 Knowledge Checklist Human Annotation . . . . . . . . . . . . . . . . . . . . Reasoning Complexity Comparison with Other Benchmarks Additional Analysis H.1 Proprietary V.S. Open Models . . . . . . . . . . . . . . . . . . . . . . . . . . . H.2 Reasoning Category and Difficulty Levels . . . . . . . . . . . . . . . . . . . . H.3 Option Types and Difficulty Levels . . . . . . . . . . . . . . . . . . . . . . . . H.4 Case Study of Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.5 Impact of CoT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Case Study 16 16 16 16 16 17 17 17 17 17 20 20 20 20 21 21 24 25 25 15 Submission In Progress"
        },
        {
            "title": "A VISUALPUZZLES Statistics",
            "content": "A.1 Breakdown of Statistics of VISUALPUZZLES Table 6 shows breakdown of statistics of VISUALPUZZLES questions. Reasoning Category Image Options Text Options Easy Medium Hard Easy Medium Hard Algorithmic Analogical Deductive Inductive Spatial Total 21 120 29 7 123 300 8 81 24 70 41 0 10 2 127 6 145 124 0 45 3 61 233 100 0 79 2 52 9 0 21 0 3 33 Total 262 211 200 209 286 1168 Table 6: Number of questions in each reasoning category, option types, and difficulty levels. A.2 Data Sources Chinese Civil Service Examination (中国国家公务员考试) 5 (224 puzzles): we manually translated questions from this exam to English from Chinese. Textbooks (210 puzzles): we carefully collected and re-purposed questions from online resources and textbooks. Smart-101 (Cherian et al., 2022a) (247 puzzles): we carefully selected images from this benchmark and synthesized new questions. MATH-Vision (Wang et al., 2024a) (293 puzzles): we carefully selected and repurposed questions from this benchmark. VASR (Bitton et al., 2023) (194 puzzles): we carefully selected questions from this benchmark."
        },
        {
            "title": "B Model Evaluation Setup",
            "content": "Model Evaluation Prompt with Chain-of-Thought Solve the multiple-choice question and then answer with the option letter from the given choices. The last line of your response should be of the following format: Answer: $LETTER (without quotes) where LETTER is one of options. Think step by step before answering. Model Evaluation Prompt w/n Chain-of-Thought Answer the question with the options letter from the given choices directly."
        },
        {
            "title": "C Human Annotation Setup",
            "content": "C.1 Difficulty Labeling Each question was also carefully assigned difficulty label from easy, medium, or hard, based on the cognitive load required for reasoning. Easy Level questions could be solved by the annotator in less than one minute. Medium Level questions could be solved by the annotator in one to three minutes. 5https://en.wikipedia.org/wiki/Civil service of the People%27s Republic of China# Examinations. Submission In Progress Hard Level questions require the annotator more than five minutes to solve or quit solving. Annotation Guideline for Puzzle Difficulty Try to solve the puzzle first. You need to measure the time you attempted to solve each puzzle. Then, select from Easy, Medium, or Hard based on the time required. - Easy Level: You can solve or answer the question within 1 minute. This level of puzzles should require minimal reasoning. - Medium Level: You can solve or answer the question within 1-3 minutes. This level of puzzles should demand moderate reasoning. - Hard Level: You can / cannot solve this question with more than 5 minutes. This level of puzzles should involve significant / multi-step reasoning. C.2 Reasoning Category Labeling Annotation Guideline for Puzzle Reasoning Category Assign the category that best describes the primary type of reasoning or logic required for each puzzle: - Algorithmic Reasoning: Involves following or devising step-by-step procedure or rule-based process. - Analogical Reasoning: Requires identifying relationships by comparison between pairs of entities. - Deductive Reasoning: Involves deriving specific conclusions from general or given premises. - Inductive Reasoning: Focuses on generalizing rule or pattern from specific instances. - Spatial Reasoning: Involves visualizing and manipulating shapes, distances, or orientations."
        },
        {
            "title": "D Full Results",
            "content": "D.1 Full Results w/ CoT D.2 Full Results w/n CoT"
        },
        {
            "title": "E Knowledge Checklist",
            "content": "E.1 Knowledge Checklist Generation Prompt to Generate Knowledge Checklist Questions You are an exam writer. You are now writing knowledge test. You are given question (Question) regarding an image and its standard solution (Solution), your task is to write free response questions that test on individual knowledge required in answering the question correctly. You should follow these steps to complete the task: 1. explicitly analyze the given image, Question, and Solution 2. explicitly list out the individual knowledge concepts required to reach Solution. 3. write free response questions to test on the definition of each concept listed. Your generated questions should not include details of the given Question. Note that you need to provide answer keys to these questions too. 4. format the free response questions in json format. Question: question Solution: answer Submission In Progress Model Algorithmic Analogical Deductive Inductive Spatial Overall Random Choice Human (95th Percentile) Human (50th Percentile) Human (5th Percentile) o1 GPT-4o Gemini-2.5-pro Gemini-2.0-flash Gemini-2.0-flash-thinking Gemini-1.5-Pro Claude-3.7-Sonnet Claude-3.7-Sonnet-thinking Claude-3.5-Sonnet LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-1.6-7B LLaVA-1.6-13B LLaVA-1.6-34B LLaVA-OV-0.5B LLaVA-OV-7B LLaVA-OV-72B Llama-3.2-11B-Vision-Instruct Llama-3.2-90B-Vision-Instruct Qwen-VL Qwen2-VL-72B QvQ-72B-Preview Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Cambrian-8B Cambrian-13B Pangea-7B 25.0 100.0 88.0 68.1 25.0 100.0 66.0 25.0 25.0 100.0 80.0 37.0 Proprietary Models 63.7 49. 60.0 55.3 46.6 53.4 64.5 67.2 53.4 23.3 24.8 27.5 25.2 29.4 21.0 27.9 34.7 31.0 45.0 21.4 41.6 43.1 26.0 36.3 39.9 35.1 40.5 53.4 31.3 24. 30.5 68.3 58.3 64.0 58.8 70.1 57.4 48.3 44.1 47.9 Open Models 21.8 21.8 23.7 25.6 28.0 26.1 26.1 26. 30.8 23.2 31.3 28.4 45.5 26.1 21.8 33.5 27.5 26.6 46.9 24.2 25.6 28.9 67.5 49.0 60.0 57.0 49.0 58. 65.0 61.5 51.5 36.0 23.0 30.0 27.0 43.0 30.5 36.5 37.0 39.0 43.0 25.0 39.5 48.0 24.5 38.5 45.2 44.5 39.0 58.0 36.0 39.5 35. 25.0 81.6 50.0 0.0 29.2 27.3 29.7 24.4 24.9 26.3 26.8 31.1 25.4 20.6 25.4 22.5 27.3 24.9 22.5 23.4 27.3 21.1 26. 26.3 22.5 27.3 27.8 20.6 23.5 25.8 24.0 25.8 24.0 24.4 24.4 25.0 100.0 90.0 59.1 34.3 26.2 36.4 31.8 25.5 32. 37.4 37.1 34.3 19.2 25.5 21.3 23.4 25.5 25.2 25.5 28.7 26.2 31.5 24.1 29.0 27.6 25.5 22.7 32.4 24.8 29.7 29.5 29.0 21.0 25. 25.0 89.3 75.0 57.5 51.8 41.3 49.5 45.0 42.2 45.0 48.3 48.2 42.4 23.7 24.2 24.8 25.5 29.7 24.8 27.7 30.8 29.4 34. 25.3 32.4 37.8 26.0 27.9 34.9 31.2 32.1 42.3 28.9 26.5 28.6 Table 7: Performance (%) of various models with Chain of Thoughts (CoT) on VISUALPUZZLES. 18 Submission In Progress Model Algorithmic Analogical Deductive Inductive Spatial Overall Random Choice Human (95th Percentile) Human (50th Percentile) Human (5th Percentile) 25.0 100.0 88.0 68.1 25.0 100.0 66.0 25. 25.0 100.0 80.0 37.0 GPT-4o Gemini-2.0-flash Gemini-1.5-Pro LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-1.6-7B LLaVA-1.6-13B LLaVA-1.6-34B LLaVA-OV-0.5B LLaVA-OV-7B LLaVA-OV-72B Llama-3.2-11B-Vision-Instruct Llama-3.2-90B-Vision-Instruct Qwen-VL Qwen2-VL-72B QvQ-72B-Preview Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Cambrian-8B Cambrian-13B Pangea-7B 40.8 57.6 51.2 24.4 24.4 27.5 21.4 31.3 24.4 27.5 31.7 27.5 38. 23.7 38.9 44.8 31.7 33.6 40.5 36.3 38.2 43.1 25.2 23.3 32.4 Proprietary Models 34.1 41.7 46. Open Models 24.7 26.1 25.1 24.7 27.3 25.6 28.0 23.6 24.2 22.3 26.5 28.4 43.6 29.4 24.2 30.3 26.1 23.7 40.3 20.4 28.0 23. 40.5 58.0 54.0 34.5 33.5 32.5 29.5 43.0 37.5 40.5 45.0 31.0 44.5 29.5 43.0 44.0 40.5 46.0 46.0 47.0 51.5 51.5 35.0 36. 38.5 25.0 81.6 50.0 0.0 24.9 23.0 24.9 26.8 26.3 24.9 28.2 24.4 24.9 24.4 21.3 26.3 25. 27.8 20.6 26.8 23.9 22.5 25.4 25.8 24.9 25.4 23.0 24.9 28.7 25.0 100.0 90.0 59.1 29.7 35.7 29. 25.5 28.3 27.3 23.1 27.6 25.5 28.0 24.6 27.6 33.6 26.6 29.0 30.8 31.5 26.2 29.4 22.4 31.1 33.7 20.6 26.2 32.5 25.0 89.3 75.0 57. 34.0 43.2 40.8 26.9 27.6 27.4 25.0 29.8 27.2 29.4 28.8 27.3 33.1 26.6 32.0 37.8 31.3 30.2 34.2 31.0 33.7 38.6 24.5 27. 31.3 Table 8: Performance (%) of various models with Multiple Choice Direct prompting on VISUALPUZZLES. 19 Submission In Progress E.2 Example Knowledge Checklist Question Example Knowledge Checklist Question (MMMU) - Question: Explain the Arbitrage Pricing Theory (APT) model and its purpose in finance. - Answer: The Arbitrage Pricing Theory (APT) model is financial theory that estimates the expected return on an asset based on the assets sensitivity to various macroeconomic factors. It is used to determine the fair price of an asset by considering multiple factors that could affect its return, as opposed to relying on single market index as in the Capital Asset Pricing Model (CAPM). Example Knowledge Checklist Question (VISUALPUZZLES) - Question: What is the definition of distance in geometric context? - Answer: Distance in geometric context refers to the measurement of space between two points. E.3 Knowledge Checklist Human Annotation We asked two human annotators to manually verify and correct the knowledge checklist questions and gave them the following instructions. The inter-annotator agreement rate is 87.8%. Human Annotation Instructions You are given json file, where each item contains the following elements: - Question: multiple-choice question. - Answer: the answer to the question with an optional explanation. - Knowledge Concept Checklist: list of question-answer pairs, where each question in the list is intended to represent distinct knowledge concept necessary for solving the Question. You task is to annotate the knowledge concept checklists generated by model. You should carefully evaluate each question-answer pair based on the following criteria: 1. Necessity: Is the question genuinely necessary for solving the problem? If not, then remove the question. 2. Repetition: Check if any questions are repetitive or duplicate existing questions within the list. If the question is repetitive or duplicate, then remove the question. 3. Completeness: Ensure no critical knowledge concepts required to solve the problem are missing, and identify if any additional important questions should have been included. 4. Correctness: Verify whether the provided answers are accurate. Revise the checklist in case of incorrect checklist QA pairs. 5. Knowledge v.s. Skills: Ensure each question explicitly evaluates knowledge concept rather than testing skills or problem-solving techniques. Remove any questions that primarily evaluate skills instead of knowledge."
        },
        {
            "title": "F Reasoning Complexity",
            "content": "Instruction Prompt to Solve Questions in Detailed Steps < Question >< Image > Solve this question with First Order Logic. Write out each thinking step explicitly, do not skip steps. In your response, begin each step with step < step num > STEP START"
        },
        {
            "title": "G Comparison with Other Benchmarks",
            "content": "Figure 9 provides comparative analysis between VISUALPUZZLES and several widelyused benchmarks for multimodal reasoning, visualizing the knowledge requirement and reasoning complexity of each benchmark. VISUALPUZZLES has high reasoning complexity and low knowledge requirement, with an aim to disentangle multimodal reasoning from domain-specific knowledge to evaluate general reasoning abilities in non-expert settings. 20 Submission In Progress Dataset LogiQA GSM8K WikiDiverse MathVista MMMU MATH-Vision MathVerse LogicBench LogicVista NaturalBench VISUALPUZZLES Size 0.7K 8.5K 0.8K 6.1K 11.5K 3.0K 2.6K 1.5K 0.4K 10K 1.2K Reasoning Load Knowledge Requirement % Easy Words Question Type Answer Type Heavy Heavy Light Heavy Heavy Heavy Heavy Heavy Heavy Light Heavy Light Heavy Heavy Heavy Heavy Heavy Heavy Light Heavy Light Light 52.0 54.0 35.8 51.9 46.4 53.8 38.2 53.6 41.2 52.5 54.1 Text Text Image+Text Image+Text Image+Text Image+Text Image+Text Text Image+Text Image+Text Image+Text Text Text Text Text Text Image+Text Text Text Image Text Image+Text Table 9: Comparison of other existing benchmarks with VISUALPUZZLES Figure 9: Comparison between VISUALPUZZLES and several widely-used benchmarks. Table 10 compare the performance of various model families across MathVista, MMMU, and VISUALPUZZLES. Both MathVista and MMMU are benchmarks that have heavy emphasis on both knowledge and reasoning, whereas VISUALPUZZLES assess models on domaindisentangled multimodal reasoning alone. We found that success on knowledge-intensive multimodal reasoning benchmarks as MathVista and MMMU does not always carry over to VISUALPUZZLES that emphasize reasoning rather than extensive pre-trained knowledge."
        },
        {
            "title": "H Additional Analysis",
            "content": "H.1 Proprietary V.S. Open Models From Table 2, proprietary models (e.g., o1 and Claude-3.7-Sonnet) consistently achieve higher overall accuracy than most open-source models on VISUALPUZZLES. However, some open models also show competitive or even higher performance in both the overall accuracy and specific reasoning categories. For instance, Qwen2.5-VL-72B-Instruct demonstrates higher performance than GPT-4o on algorithmic reasoning, deductive reasoning, spatial reasoning, and overall accuracy. This indicates that while proprietary models currently have leading performance, open models are also rapidly improving on multimodal reasoning capabilities. H.2 Reasoning Category and Difficulty Levels Figure 11 and Figure 10 present complementary views of human accuracy against three representative models: o1 (the best-performing proprietary model), Qwen2.5-VL-72B-Instruct (the strongest Qwen-based open model), and Llama-3.2-90B-Vision-Instruct (the strongest Llama-based open model). Specifically, Figure 10 compares performance across difficulty levels for each reasoning category, while Figure 11 compares performance across categories within each difficulty level. 21 Submission In Progress Model MathVista MMMU VISUALPUZZLES Human o1 GPT-4o Gemini-2.0-Flash Gemini-1.5-Pro Claude-3.5-Sonnet Claude-3.7-Sonnet Claude-3.7-Sonnet (Thinking) LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-NeXT-7B LLaVA-NeXT-13B LLaVA-NeXT-34B LLaVA-OV-0.5B LLaVA-OV-7B LLaVA-OV-72B Llama-3.2-11B-Vision-Instruct Llama-3.2-90B-Vision-Instruct Qwen2-VL-72B QvQ-72B-Preview Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Cambrian-8B Cambrian-13B 60.3 73.9 63.8 - 63.9 67.7 - - - 27.6 35.8 36.2 46.5 34.8 63.2 67.5 51.5 57.3 70.5 71.4 43.0 58.2 70.5 62.3 68.2 74.8 49.0 48.0 88.6 78.2 69.1 71.7 62.2 68.3 71.8 75.0 36.2 36.4 34.6 35.3 51.1 31.4 48.8 56.8 50.7 60.3 64.5 70.3 41.1 54.1 64.5 53.1 58.6 70.2 42.7 40.0 80.1 51.8 41.1 45.0 45.4 42.4 48.3 48.3 26.9 27.6 27.4 25.3 29.8 27.2 29.4 31.8 29.4 34.3 32.1 37.9 31.3 30.2 34.9 31.2 33.7 42.3 28.5 27. Table 10: Comparison of other MathVista and MMMU with VISUALPUZZLES on human and SOTA models Humans consistently outperform all models across categories and difficulty levels, often by large margins. Notably, human performance remains high and relatively stable in the algorithmic, deductive, and spatial categories, even on hard questions. While accuracy does decline in analogical and inductive reasoning as difficulty increases, humans still maintain clear advantage over models. In contrast, model performance declines sharply as difficulty increases, especially for opensource models. Accuracy of Llama-3.2-90B-Vision-Instruct on hard analogical tasks drops to just 10%. Even the strongest proprietary model, o1, while more robust, still lags significantly behind humans, particularly on analogical, inductive, and spatial tasks. On easy tasks, some models perform competitively in certain categories, but this advantage largely disappears on medium and hard questions. Interestingly, these models maintain generally stable performance on algorithmic and deductive reasoning. For o1 and Qwen2.5-VL-72B-Instruct, their performances on algorithmic reasoning even go up for more difficult tasks, whereas human performance degraded as the difficulty level increases. However, all models, including o1, perform the worse at analogical, inductive and spatial reasoning in general, especially as the difficulty level increases. This suggests that models are relatively better at tasks requiring structured, rule-based algorithmic processing, while their performance degrades more steeply in tasks requiring relational abstraction (analogical), pattern induction (inductive), and visual understanding (spatial), particularly as the difficulty level increases. In summary, these results indicate that while some models exhibit promising performance on structured and easier reasoning tasks, multimodal models still struggle with abstract and complex reasoning, particularly 22 Submission In Progress Figure 10: Comparison of accuracy across different reasoning categories for human participants, the best performing proprietary model o1, the best performing Qwen-based open model Qwen2.5-VL-72B-Instruct, and the best performing Llama-based open model Llama3.2-90B-Vision-Instruct, measured on difficulty levels. 23 Submission In Progress Figure 11: Comparison of accuracy across different difficulty levels for human participants, the best performing proprietary model o1, the best performing Qwen-based open model Qwen2.5-VL-72B-Instruct, and the best performing Llama-based open model Llama-3.290B-Vision-Instruct, measured across reasoning categories. when difficulty increases. Bridging the gap between model and human reasoning remains critical challenge. H.3 Option Types and Difficulty Levels Figure 12 compares human accuracy against three representative models, o1 (the bestperforming proprietary model), Qwen2.5-VL-72B-Instruct (the strongest Qwen-based open model), and Llama-3.2-90B-Vision-Instruct (the strongest Llama-based open model), across different difficulty levels, separately for textual and visual answer options. Across all participants and models, we observe consistent pattern: text-based options result in higher accuracy than image-based options, with the performance gap widening as task difficulty increases. This trend holds even for human participants, whose accuracy drops from 92% to 40% on visual options when moving from easy to hard tasks, compared to much smaller drop on text-based ones (93% to 73%). For models, the gap is even more pronounced. For instance, Qwen2.5-VL-72B-Instruct achieves 58% accuracy on hard questions with text options, but only 20% when image 24 Submission In Progress Figure 12: Comparison of accuracy across different difficulty levels for human participants, the best performing proprietary model o1, the best performing Qwen-based open model Qwen2.5-VL-72B-Instruct, and the best performing Llama-based open model Llama-3.290B-Vision-Instruct, measured on textual v.s. visual option types. options are used. o1 and Llama-3.2-90B-Vision-Instruct exhibit similar drops, suggesting broad weakness in multi-image reasoning and visual option discrimination. These findings suggest that image-based answer options introduce significant additional complexity, requiring models not just to understand the question but to reason over multiple visual cues. This capability is essential for real-world tasks such as product selection, recommendation, and visual planning, where their decision-making process often depends on comparing visual content. However, most pretraining datasets and benchmarks have traditionally emphasized textual QA formats, with far fewer examples involving visual options or structured visual comparisons. As result, models may lack the inductive bias or learned attention mechanisms to handle visual alternatives effectively. These results highlight an important direction for future work: expanding and diversifying training corpora to include multi-choice visual reasoning tasks, and developing architectures that are explicitly designed to process and compare visual candidates, especially under challenging conditions. H.4 Case Study of Reasoning Figure 13 shows case study demonstrating the similarity in structure and reasoning strategy between Claude-3.7-Sonnet and Claude-3.7-Sonnet-Thinking. Average textual similarity between model responses of these two models on VISUALPUZZLES is 0.9. H.5 Impact of CoT Table 11 compares model performance under two prompting strategies: direct multiple-choice prompt vs. Chain-ofThought (CoT) prompt. We observe that proprietary models and larger open models (72B) benefit from CoT, while others show little to no improvement or even decline in performance with CoT. For instance, both GPT-4o and Qwen2.5VL-72B-Instruct show more than 20% inModel Direct CoT GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet Qwen2-VL-2B-Instruct Qwen2.5-VL-7B-Instruct Cambrian-13B LLaVA-NeXT-34B Qwen2.5-VL-72B-Instruct LLama-3.2-90B-Vision-Instruct 34.0 41.0 40.0 31.3 33.7 27.4 29.8 38.6 33.3 41.6 45.1 42.5 26.1 32.0 26.5 29.6 42.3 33. Table 11: Comparison of models with Direct Multiple Choice and CoT prompting. 25 Submission In Progress Figure 13: Case Study showing the similarity in structure and reasoning strategy between Claude-3.7-Sonnet-Thinking and Claude-3.7-Sonnet. Similarity between model responses of these two models on VISUALPUZZLES is 0.9. creases in performance when using CoT. In contrast, several smaller models, such as Qwen2VL-2B-Instruct and Cambrian-13B, exhibit decreased accuracy with CoT prompting. These results suggest that CoT can indeed enhance the reasoning capability of larger models whereas it may introduce unnecessary complexity or confusion for smaller models and thus decreasing performance. Submission In Progress"
        },
        {
            "title": "I Case Study",
            "content": "Figure 14: sample error case of Algorithmic Reasoning (difficulty: Easy). 27 Submission In Progress Figure 15: sample correct case of Algorithmic Reasoning (difficulty: Easy). 28 Submission In Progress Figure 16: sample error case of Algorithmic Reasoning (difficulty: Medium). 29 Submission In Progress Figure 17: sample correct case of Algorithmic Reasoning (difficulty: Medium). 30 Submission In Progress Figure 18: sample error case of Algorithmic Reasoning (difficulty: Hard). 31 Submission In Progress Figure 19: sample correct case of Algorithmic Reasoning (difficulty: Hard). 32 Submission In Progress Figure 20: sample error case of Analogical Reasoning (difficulty: Easy). 33 Submission In Progress Figure 21: sample correct case of Analogical Reasoning (difficulty: Easy). 34 Submission In Progress Figure 22: sample error case of Analogical Reasoning (difficulty: Medium). 35 Submission In Progress Figure 23: sample correct case of Analogical Reasoning (difficulty: Medium). 36 Submission In Progress Figure 24: sample error case of Analogical Reasoning (difficulty: Hard). 37 Submission In Progress Figure 25: sample correct case of Analogical Reasoning (difficulty: Hard). 38 Submission In Progress Figure 26: sample error case of Deductive Reasoning (difficulty: Easy). 39 Submission In Progress Figure 27: sample correct case of Deductive Reasoning (difficulty: Easy). 40 Submission In Progress Figure 28: sample error case of Deductive Reasoning (difficulty: Medium). 41 Submission In Progress Figure 29: sample correct case of Deductive Reasoning (difficulty: Medium). 42 Submission In Progress Figure 30: sample error case of Deductive Reasoning (difficulty: Hard). 43 Submission In Progress Figure 31: sample correct case of Deductive Reasoning (difficulty: Hard). 44 Submission In Progress Figure 32: sample error case of Inductive Reasoning (difficulty: Easy). 45 Submission In Progress Figure 33: sample correct case of Inductive Reasoning (difficulty: Easy). 46 Submission In Progress Figure 34: sample error case of Inductive Reasoning (difficulty: Medium). 47 Submission In Progress Figure 35: sample correct case of Inductive Reasoning (difficulty: Medium). 48 Submission In Progress Figure 36: sample error case of Inductive Reasoning (difficulty: Hard). 49 Submission In Progress Figure 37: sample correct case of Inductive Reasoning (difficulty: Hard). 50 Submission In Progress Figure 38: sample error case of Spatial Reasoning (difficulty: Easy). 51 Submission In Progress Figure 39: sample correct case of Spatial Reasoning (difficulty: Easy). 52 Submission In Progress Figure 40: sample error case of Spatial Reasoning (difficulty: Medium). 53 Submission In Progress Figure 41: sample correct case of Spatial Reasoning (difficulty: Medium). 54 Submission In Progress Figure 42: sample error case of Spatial Reasoning (difficulty: Hard). 55 Submission In Progress Figure 43: sample correct case of Spatial Reasoning (difficulty: Hard)."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University"
    ]
}