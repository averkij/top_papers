{
    "paper_title": "Judging with Confidence: Calibrating Autoraters to Preference Distributions",
    "authors": [
        "Zhuohang Li",
        "Xiaowei Li",
        "Chengyu Huang",
        "Guowang Li",
        "Katayoon Goshvadi",
        "Bo Dai",
        "Dale Schuurmans",
        "Paul Zhou",
        "Hamid Palangi",
        "Yiwen Song",
        "Palash Goyal",
        "Murat Kantarcioglu",
        "Bradley A. Malin",
        "Yuan Xue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The alignment of large language models (LLMs) with human values increasingly relies on using other LLMs as automated judges, or ``autoraters''. However, their reliability is limited by a foundational issue: they are trained on discrete preference labels, forcing a single ground truth onto tasks that are often subjective, ambiguous, or nuanced. We argue that a reliable autorater must learn to model the full distribution of preferences defined by a target population. In this paper, we propose a general framework for calibrating probabilistic autoraters to any given preference distribution. We formalize the problem and present two learning methods tailored to different data conditions: 1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a reinforcement learning approach for sparse, binary labels. Our empirical results show that finetuning autoraters with a distribution-matching objective leads to verbalized probability predictions that are better aligned with the target preference distribution, with improved calibration and significantly lower positional bias, all while preserving performance on objective tasks."
        },
        {
            "title": "Start",
            "content": "Judging with Confidence: Calibrating Autoraters to Preference Distributions Zhuohang Li1,2, Xiaowei Li1, Chengyu Huang1,3, Guowang Li1, Katayoon Goshvadi4, Bo Dai4, Dale Schuurmans4,5, Paul Zhou1, Hamid Palangi1, Yiwen Song1, Palash Goyal1, Murat Kantarcioglu6, Bradley A. Malin2 and Yuan Xue7 1Google, 2Vanderbilt University, 3Cornell University, 4Google DeepMind, 5University of Alberta, 6Virginia Tech, 7Scale AI 5 2 0 2 0 3 ] . [ 1 3 6 2 0 0 . 0 1 5 2 : r The alignment of large language models (LLMs) with human values increasingly relies on using other LLMs as automated judges, or autoraters. However, their reliability is limited by foundational issue: they are trained on discrete preference labels, forcing single ground truth onto tasks that are often subjective, ambiguous, or nuanced. We argue that reliable autorater must learn to model the full distribution of preferences defined by target population. In this paper, we propose general framework for calibrating probabilistic autoraters to any given preference distribution. We formalize the problem and present two learning methods tailored to different data conditions: 1) direct supervised fine-tuning for dense, probabilistic labels, and 2) reinforcement learning approach for sparse, binary labels. Our empirical results show that finetuning autoraters with distribution-matching objective leads to verbalized probability predictions that are better aligned with the target preference distribution, with improved calibration and significantly lower positional bias, all while preserving performance on objective tasks. 1. Introduction The alignment of large language models (LLMs) with human values (Ouyang et al., 2022) increasingly relies on using other powerful LLMs as automated judges, or autoraters, to score model responses. This LLM-as-a-Judge paradigm (Zheng et al., 2023) is now cornerstone of evaluating and developing safer AI systems, particularly through methods such as reinforcement learning from AI feedback (RLAIF) (Bai et al., 2022). Currently, autoraters are typically trained on discrete preference labels (Kim et al., 2024b; Li et al., 2024; Wang et al., 2024b), which leads to fundamental limitation: (collective) human judgment does not correspond to single label, but rather distribution (Nie et al., 2020; Pavlick and Kwiatkowski, 2019), especially in complex situations that involve uncertainty or balancing multiple criteria (Arora et al., 2025). Even among qualified annotators, disagreement is common, not simply due to noise but because of systematic differences in how individuals define problems, interpret evidence, or apply values and decision strategies (Mumpower and Stewart, 1996). Current autoraters are trained with mode-seeking objective that collapses this rich distributional information into single verdict (e.g., the majority label), which discards crucial uncertainty signals and erases minority viewpoints by construction. We argue that for an autorater to be reliable, it must be calibrated to model the full distribution of human preferences. An ideal judge should recognize when topic is contentious (e.g., 50/50 split), when preference is clear but not unanimous (e.g., 80/20), and when judgment is objectively certain. Modeling this distribution is essential for effective risk management, fairness, and building robust alignment systems. This paper introduces general and scalable framework for calibrating autoraters verbal probability predictions to any target preference distribution, while preserving their ability to generate The work of ZL, CH, and YX was done at Google, when ZL and CH worked as student researchers. Correspondence to: zhuohang.li@vanderbilt.edu, yuan.xue@scale.com Judging with Confidence: Calibrating Autoraters to Preference Distributions Figure 1 Overview of discrete vs. probabilistic autoraters. Left: Given user query and two candidate responses, discrete autorater returns single preference (e.g., is better), collapsing annotator variability. probabilistic autorater predicts the full preference distribution and is finetuned via SFT/RL to match the target preference distribution. Right: Our finetuned autorater vs. zero-shot probabilistic conversions of discrete autoraters, including Verbalized Confidence (VC), Self-Consistency (SC), and Logits (Lo), evaluated using Gemma-2-9B on JudgeLM val set. Alignment error is measured by MSE, calibration error by ECE, and agreement by percentage. natural-language reasoning traces. We introduce two finetuning methods, each tailored to set of different data conditions. First, when dense, probabilistic labels from multiple annotators are available, we use direct supervised finetuning (SFT). Second, when only sparse, binary labels are available, we employ reinforcement learning (RL) approach with rewards based on proper scoring rules. Our empirical results validate this distribution-matching objective. Autoraters finetuned with our methods show significant improvements in performance, calibration, and reliability. Notably, our methods achieve an 18-51% reduction in Mean Squared Error (MSE), 4-45% reduction in Expected Calibration Error (ECE), and 7-81% gain in consistency against positional bias. Notably, our findings offer guidance on annotation strategy: for fixed budget, RL with many sparse, binary labels is more data-efficient than SFT with fewer dense, probabilistic labels, highlighting the benefits of prompt diversity. Our method also enhances alignment with human judgment on out-of-distribution tasks. On the PandaLM dataset, our finetuned Gemma-2-9B model achieves 73.17% agreement with human annotations, outperforming all baselines including GPT-4. Moreover, this improved calibration on subjective tasks does not compromise performance on objective ones, as the same model achieves an overall accuracy of 46.57% on JudgeBench, on par with Gemini-1.5-pro. 2. Probabilistic Framework for Calibrating Autoraters 2.1. Problem Formulation We consider the scenario of pairwise judgements where an input 𝑿 specifies prompt that is associated with two responses ( 𝐴, 𝐵). The populations ground-truth preference is modeled as Bernoulli random variable 𝑌 {0, 1}, 𝑌 𝑿 = 𝒙 Bernoulli( 𝑝(𝒙)) , 2 Judging with Confidence: Calibrating Autoraters to Preference Distributions where 𝑌 =1 indicates 𝐵 𝐴 (i.e., 𝐵 is preferred to 𝐴), and 𝑝(𝒙) = Pr[𝑌 =1 𝑿=𝒙] is the (unknown) preference distribution for the pair ( 𝐴, 𝐵). Let ℎ index human annotator drawn from the population 𝑝(ℎ), then conceptually 𝑝(𝒙) represents the true population-level human preference 𝑝(𝒙) = Prℎ𝑝(ℎ) [𝐵 𝐴𝒙, ℎ], i.e., the probability that randomly chosen annotator would prefer 𝐵 given the context 𝒙. Discrete Autorater. An autorater is language model (LM) prompted to act as judge. In the discrete setting, the LM produces single decision (e.g., via greedy decoding) 𝑑𝜽(𝒙) {0, 1}, or an uncalibrated scalar margin 𝑚𝜽(𝒙) ℝ (e.g., parsed rubric rating or logit difference), with decision 1{𝑚𝜽(𝒙) 0}. Such outputs collapse the rater distribution at 𝒙 to point estimate (typically the majority choice). Probabilistic Autorater. probabilistic judge instead predicts the full preference distribution through its Bernoulli parameter, 𝑝𝜽(𝒙) [0, 1] 𝑝(𝒙). definitive decision can be recovered by thresholding 𝑝𝜽(𝒙) if needed, but the primary output is the (conditional) probability itself, which is optimized to match the population preference rather than merely to choose label. Crucially, this formulation does not rely on the Bradley-Terry assumption (Bradley and Terry, 1952) typically seen in reward modeling, thereby allowing richer representations of uncertainty. 2.2. Benefits of Probabilistic Autoraters In contrast to the mode-seeking behavior of discrete autoraters, probabilistic Informativeness. autoraters are optimized to match the full preference distribution. This probability prediction provides more information for cost-sensitive decision-making by revealing the aleatoric ambiguity within the task. Probabilistic reporting in autoraters also improves fairness and auditability by revealing annotator disagreement, whereas discrete reporting collapses the minority viewpoints. Alignment. Probabilistic autoraters can be better aligned with the target preference distribution. In particular, it is straightforward to show that if Pr[0 < 𝑝(𝒙) < 1] > 0, then any discrete autorater or any single human annotator who effectively reports degenerate distribution 𝑑(𝒙) {0, 1} is strictly worse than reporting 𝑝(𝒙) = 𝑝(𝒙) under strictly proper scoring rule. Calibration. Additionally, this distribution-matching objective of probabilistic autoraters implies calibration: if 𝑝𝜃(𝒙) = 𝑝(𝒙) almost surely, then 𝔼[𝑌 𝑝𝜃(𝒙) = 𝑐] = 𝑐 for all 𝑐 [0, 1]. In practice, as 𝑝𝜃 approaches 𝑝, calibration error (e.g., ECE) shrinks. By contrast, any probability prediction obtained from discrete autorater post hoc (e.g., vote fractions from self-consistency or logits passed through softmax) is not trained to recover 𝑝(𝒙) and thus is generally not calibrated. 3. Finetuning Autoraters to Match the Preference Distribution We introduce two distribution-matching finetuning paradigms for calibrating the autoraters probabilistic prediction 𝑝𝜽 to the ground truth preference distribution 𝑝(𝒙). We focus on verbalized Judging with Confidence: Calibrating Autoraters to Preference Distributions probability because it is (1) more flexible and interpretable than training dedicated classification head by preserving the models ability to generate natural-language rationales, and (2) more efficient than sampling-based approaches, as it only requires single decoding pass. Setting 1: Direct Supervised Finetuning with Probabilistic Labels. When multiple annotations (𝒙, 𝑦 (1) , ..., 𝑦 (𝑚) ) are available for each prompt 𝒙 (the pair to be judged), we estimate the population preference by the multi-annotator mean ˆ𝑝(𝒙) = 1 𝑗=1 𝑦 ( 𝑗) 𝑝(𝒙). We then instruction-tune the 𝑚 autorater in text-to-text fashion: given prompt 𝒙 (the pair), the target sequence includes optional CoT reasoning and structured numeric field encoding the probability that is better. We apply standard autoregressive supervised finetuning (SFT) to maximize the likelihood of the target sequence 𝝉1:𝑆 composed using ˆ𝑝(𝒙): (cid:205)𝑚 LSFT = 𝔼(𝒙,ˆ𝑝) 𝑆 (cid:104) 𝑡=1 log 𝑃𝜽(𝝉𝑡 𝒙, 𝝉1:𝑡1) (cid:105) , where 𝝉𝑖 is the 𝑖th token of the sequence 𝝉 and 𝑆 is its sequence length. In practice, we parse the generated string to recover the numeric form of 𝑝𝜽(𝒙); training encourages the model to produce 𝑝𝜽(𝒙) ˆ𝑝(𝒙) while retaining the ability to perform free-form reasoning. Setting 2: Reinforcement Learning from Binary Labels via Piecewise Proper Rewards. When only single-sample binary labels (𝒙𝑖, 𝑦𝑖) are available (e.g., via crowdsourced platforms), we treat the autorater as sequence policy 𝝅𝜽(𝝉 𝒙) that produces token sequence 𝝉 containing numeric probability 𝑝. deterministic parser 𝑔 maps 𝝉 to either valid probability prediction in [0, 1] or (unparsable), with the probability of producing parsable response denoted as 𝑠𝜽(𝒙): 𝑔 : [0, 1] {}, 𝝉 𝑝 or , 𝑠𝜽(𝒙) = Pr 𝝉𝝅𝜽 ( 𝒙) [ 𝑔(𝝉) ]. We use piecewise strictly proper scoring rules as rewards. Let 𝑦 {0, 1} (1 = better), we have: Brier reward: 𝑅Brier(𝝉; 𝑦) = (cid:40)1 ( 𝑝 𝑦)2, 0, if 𝑔(𝝉) = 𝑝 [0, 1], if 𝑔(𝝉) = . (1) Logarithmic reward (with clipping): For numerical stability, we consider fixed small 𝜖 (0, 1 2 ) and define 𝑝 = clip( 𝑝, 𝜖, 1 𝜖). Then 𝑅Log(𝝉; 𝑦) = (cid:40) 𝑦 log 𝑝 + (1 𝑦) log(1 𝑝), log 𝜖, if 𝑔(𝝉) = 𝑝 [0, 1], if 𝑔(𝝉) = . For either reward 𝑅 {𝑅Brier, 𝑅Log}, the goal is to maximize the population objective 𝐽𝑅 (𝜽) = 𝔼(𝒙,𝑦) (cid:104) 𝔼𝝉𝝅𝜽 ( 𝒙) [ 𝑅(𝜏; 𝑦) ] (cid:105) . (2) (3) Compared to the Brier reward, the Log reward heavily penalizes overconfident yet incorrect predictions. Both objectives can be optimized with policy-gradient-based reinforcement learning methods using the parsed numeric probability. 4 Judging with Confidence: Calibrating Autoraters to Preference Distributions 3.1. Consistency Analysis In Setting 1, the multi-annotator mean ˆ𝑝(𝒙) is an unbiased estimate of the true preference distribution 𝑝(𝒙) with variance decreasing as 1/𝑚 (Appendix B.1, Lemma 2), and thereby provides high fidelity target for learning. In Setting 2, at the population level, the optimal autorater policy under either the piecewise Brier reward or the clipped Log reward is (i) parsable everywhere (i.e., 𝑠𝜽(𝒙) = 1), (ii) deterministic in the numeric value it reports, and (iii) reports the truthful probability (i.e., recovers the target preference distribution, up to clipping for the Log reward). The proof is included in Appendix B.2. Proposition 1 (Fisher Consistency of Brier and Log Rewards). Assume the policy class can realize, for each 𝒙, deterministic numeric output 𝑝𝜽(𝒙) [0, 1] with 𝑠𝜽(𝒙) = 1. Then any global maximizer of 𝐽𝑅 in equation 3 satisfies: (a) Brier: 𝑝𝜽(𝒙) = 𝑝(𝒙) for almost all 𝒙. (b) Log (with clipping): 𝑝 𝜽 (𝒙) = clip(cid:0) 𝑝(𝒙), 𝜖, 1 𝜖(cid:1) for almost all 𝒙. Moreover, for both rewards, any stochasticity in the reported numeric value or any non-zero density associated with unparsable outputs strictly reduces the expected reward; thus, an optimizer is deterministic and fully parsable for almost every 𝑥. 4. Experimental Setup Here, we describe the datasets used for calibration and evaluation, our scalable preference-annotation pipeline, the autorater output format and reward instantiation, and the finetuning setup. Calibration Data. We build on subset of prompts from the JudgeLM corpus (Zhu et al., 2025), which aggregates instruction-following tasks (e.g., Alpaca-GPT4 (Peng et al., 2023), Dolly-15K (Conover et al., 2023)) paired with responses from 11 open-source LLMs (including LLaMA (Touvron et al., 2023), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023)). The source corpus contains 105K prompts. To ensure the same total annotation budget across the two finetuning paradigms, we construct two calibration splits: (i) SFT uses 5K prompts with 10 annotations each; (ii) RL uses 50K prompts with single annotation each. We also apply swap augmentation (Li et al., 2024) by duplicating each pair with A/B swapped and the label flipped. For evaluation, we sample 1K prompts from the original validation set, each with 10 annotations to form probabilistic labels. Preference Annotation. Since most existing datasets lack sufficient multi-rater annotation for reliable probability estimates, to evaluate our method at scale, we employ Gemini-2.5-Flash (Comanici et al., 2025) as an advanced teacher to generate pairwise preference labels with brief rationales. We set the temperature to 1.0 and condition on randomly sampled persona (Appendix F) to increase coverage and reduce prompt-induced bias. Across calibration and evaluation splits, this yields 110K total annotations. For each comparison instance 𝒙, we convert 𝑚 independent teacher votes into probabilistic target ˆ𝑝(𝒙) = 1 𝑗=1 1{𝐵 𝐴}. For the SFT + CoT setting, we additionally elicit teacher 𝑚 reasoning traces as expert demonstrations by conditioning on the obtained ˆ𝑝(𝒙). (cid:205)𝑚 Response Format and Reward Instantiation. The autorater is prompted to compare responses vs. and emit (optionally) chain-of-thought enclosed in <think> tags, followed by single probability 5 Judging with Confidence: Calibrating Autoraters to Preference Distributions within <prob_rb_better> tag representing 𝑝𝜽(𝒙) Pr[𝐵 𝐴 𝒙]. Our prompts are provided in Appendix F. In SFT, we maximize the likelihood of the target token sequence that encodes ˆ𝑝(𝒙). In RL, we parse the numeric probability via deterministic parser 𝑔 (implemented by rule-based string-matching) and optimize either 𝑅Brier in equation 1 or 𝑅Log in equation 2. Unparsable outputs receive default reward (0 for Brier; log 𝜖 for Log), which empirically drives the parsability rate 𝑠𝜽(𝒙) toward 1. When dense labels are available, the SFT objective provides low-variance target for 𝑝𝜽; with single-label supervision, the RL objectives remain consistent and, being strictly proper, recover 𝑝(𝒙) in expectation. Base Models and Finetuning Protocol. We finetune the instruction-tuned Gemma-2-9B (Gemma Team, 2024) and Qwen-2.5-7B (Qwen Team, 2024) models with full-parameter updates for both SFT and RL. For RL, we use GRPO (Shao et al., 2024). For 𝑅Log we set 𝜖 = 103 to avoid degenerate rewards. Full hyperparameters and training details are reported in Appendix C. 5. Experimental Results 5.1. Main Evaluation In this section, we empirically demonstrate that our distribution-matching finetuning approaches can lead to better performing and calibrated autoraters. Baselines. We consider the following four types of zero-shot baselines that can be immediately applied to any existing discrete autoraters to obtain probabilistic predictions: (1) Verbalized Confidence (Tian et al., 2023): The autorater is directly prompted to provide confidence score without intermediate reasoning. (2) Verbalized Confidence w/ CoT (Wei et al., 2022): The autorater first generates step-by-step chain-of-thought explanation before providing its confidence score. (3) Self-Consistency (Wang et al., 2023): The autorater aggregates preferences over 𝑁 independent CoT samples. The confidence for response is the fraction of samples that voted for it. (4) Logit-based Confidence: Confidence is derived by applying softmax function to the models output for the verbalized preference tokens 𝜏𝑖 (A or B), i.e., 𝑝( 𝑦 = 𝑖 𝒙) = 𝑒𝑧𝜏𝑖 /(cid:205)𝑖 𝑒𝑧𝜏𝑖 , 𝑖 logits 𝑧𝜏𝑖 {0, 1}. Additionally, we consider the following calibration baselines that extend the logit-based confidence: (1) Temperature Scaling (Guo et al., 2017) is post-hoc calibration method that rescales pre-softmax logits 𝒛 by single scalar temperature 𝑇 > 0 learned on held-out calibration set by minimizing negative log likelihood, producing calibrated confidences ˆ𝒑TS( 𝒚 𝒙) = softmax(𝒛/𝑇). (2) Contextual Calibration (Zhao et al., 2021) is test-time debiasing method that estimates the prompt-induced prior using content-free probe (e.g., N/A), then corrects predictions by subtracting this bias in logit space (or dividing probabilities): ˆ𝒑CC( 𝒚 𝒙) = 𝑾 𝒑( 𝒚 𝒙), where 𝑾 = diag( 𝒑( 𝒚 [N/A])) 1 makes the content-free prediction uniform and reduces bias. (3) Batch Calibration (Zhou et al., 2024) is zero-shot, inference-only correction that estimates the contextual bias 𝒃 from the current test batch {𝒙𝑖}𝐵 𝑖=1 𝒑( 𝒚 𝒙𝑖). Each example is then calibrated by dividing by this bias term (or equivalently, subtracting log 𝒃 from logits): ˆ𝒑BC( 𝒚 𝒙) 𝒑( 𝒚 𝒙)/𝒃. To ensure estimation accuracy, we use the entire test set in our experiments to estimate the bias. 𝑖=0 via 𝒃 = 𝔼𝒙𝑃 (𝒙) 𝒑( 𝒚 𝒙) 1 (cid:205)𝐵 𝐵 6 Judging with Confidence: Calibrating Autoraters to Preference Distributions Table 1 Main experiment results comparing our methods against zero-shot and calibration baselines on two models. We evaluate alignment (MSE), performance (Agreement, F1 Score), and calibration (ECE, Brier). Model Method MSE Agr. F1 ECE Brier Alignment Performance Calibration 9 - 2 - e . 7 - 5 2 - Q Zero-shot Baselines Verbalized Confidence Verbalized Confidence w/ CoT Self-consistency (N=10) Self-consistency (N=30) Logits Calibration Baselines Temperature Scaling Contextual Calibration Batch Calibration Ours SFT SFT w/ CoT RL (Brier) RL (Log) Zero-shot Baselines Verbalized Confidence Verbalized Confidence w/ CoT Self-consistency (N=10) Self-consistency (N=30) Logits Calibration Baselines Temperature Scaling Contextual Calibration Batch Calibration Ours SFT SFT w/ CoT RL (Brier) RL (Log) 0.1255 0.1065 0.1248 0.1217 0.1162 0.0839 0.1384 0.1153 0.0972 0.1033 0.0764 0. 0.1823 0.1571 0.1916 0.1840 0.1775 0.1173 0.1551 0.1796 0.1143 0.1033 0.0893 0.1192 0.7773 0.7893 0.7853 0.7921 0.8074 0.5260 0.5345 0.5482 0.5361 0.5665 0.1183 0.0869 0.1397 0.1374 0. 0.1615 0.1445 0.1551 0.1514 0.1416 0.8074 0.7753 0.8104 0.5665 0.5226 0.5482 0.0827 0.1598 0.1255 0.1224 0.1728 0.1406 0.5623 0.8314 0.8214 0.5575 0.8706 0.5895 0.5780 0. 0.0972 0.1111 0.0879 0.1141 0.1257 0.1332 0.0946 0.1173 0.6723 0.7241 0.7091 0.7251 0.7382 0.4486 0.4866 0.4765 0.4861 0.4982 0.1846 0.1693 0.2168 0.2075 0.2102 0.2276 0.1965 0.2314 0.2212 0. 0.7402 0.7632 0.7402 0.4982 0.5159 0.4978 0.1529 0.1888 0.2129 0.1646 0.1893 0.2162 0.8264 0.8122 0.8575 0.8244 0.5590 0.6075 0.5804 0. 0.1394 0.1341 0.1095 0.1324 0.1015 0.1103 0.1474 0.1472 Metrics. We assess the following three aspects of the autorater: (1) its alignment to the target preference distribution, as measured by the Mean Squared Error (MSE) between the predicted 𝑝𝜽(𝒙) and the high fidelity estimate ˆ𝑝(𝒙) of the true preference distribution 𝑝(𝒙), (2) its performance, in terms of agreement (Agr.) with the majority label (i.e., the mode of the target distribution) and the resulting F1 score, and (3) its calibration, as measured by Expected Calibration Error (ECE) (Guo et al., 2017) and Brier score. more detailed discriptions of these metrics are provided in Appendix C. Distribution-Matching Tuning Improves Preference Calibration. As shown in Table 1, our distribution-matching finetuning methods consistently outperform both zero-shot and calibration baselines across all metrics. While prompting strategies like Chain-of-Thought and self-consistency improve upon simple verbalized confidence, they still result in high alignment errors. In contrast, our finetuning approach drastically reduces this error. For instance, RL with Brier reward achieves an MSE of just 0.0764 on Gemma-2-9B. This superior alignment translates directly into stronger performance and better calibration. Notably, on Gemma-2-9B, the RL-Brier model attains the highest agreement (0.8706), F1 score (0.5895), and lowest Brier score (0.0946) among all methods. The benefits are even more pronounced on Qwen-2.5-7B, where the same model achieves an agreement of 0.8575a 12.4% improvement over Contextual Calibration, the best-performing baselinewhile simultaneously achieving best calibration, as measured by both ECE and Brier score. 7 Judging with Confidence: Calibrating Autoraters to Preference Distributions Table 2 Evaluation of positional bias. We report Consistency (higher is better) and expected Absolute Symmetry Deviation (𝔼[Δ𝑆𝐷], lower is better). Method Zero-shot Baselines Verbalized Confidence Verbalized Confidence w/ CoT Logits Calibration Baselines Temperature Scaling Contextual Calibration Batch Calibration Ours SFT SFT w/ CoT RL (Brier) RL (Log) Gemma-2-9B Qwen-2.5-7B Consistency Abs. Dev. Consistency Abs. Dev. 0.7301 0.8094 0.7963 0.7963 0.7021 0. 0.8375 0.7803 0.8926 0.8776 0.2242 0.1709 0.1912 0.1239 0.3202 0.1893 0.1875 0.2291 0.1026 0.1231 0.4964 0.6399 0.6529 0.6489 0.7422 0. 0.8284 0.8335 0.9007 0.8726 0.3461 0.3120 0.3388 0.1953 0.2473 0.2130 0.1827 0.1654 0.0964 0.1259 RL on Binary Labels is More Annotation-Efficient than SFT on Probabilistic Labels. key finding is that for fixed annotation budget, RL is more annotation-efficient training paradigm than SFT. As seen in Table 1, RL-tuned autoraters, trained on 50K prompts with single binary label each, consistently outperform their SFT counterparts, which were trained on 5K prompts with 10 aggregated labels each. We attribute this to the benefits of data diversity: the performance boost from seeing 10 larger set of unique prompts appears to outweigh the advantage of learning from less noisy, aggregated target on smaller dataset. Within the RL framework, the Brier reward consistently yields better results than the Log reward. This is likely because the Log rewards heavy penalties for tail miscalibrations can introduce training instability, whereas the Brier reward provides smoother optimization landscape. 5.2. Evaluation of Positional Bias LM-based autoraters, even those based on powerful proprietary models such as GPT-4, are known to be susceptible to positional bias (Wang et al., 2024a; Zheng et al., 2023), which causes their final verdict to be dependent on the order of the responses and thus undermines the reliability of their judgment result. To evaluate the positional bias of the probabilistic autoraters, for each prompt 𝒙, we perform inference twice by swapping the order of the responses to obtain two predicted probabilities: 𝑝orig that predicts Pr[𝐵 𝐴 𝒙] and 𝑝swap that predicts Pr[ 𝐴 𝐵 𝒙]. Metrics. Following Zheng et al. (2023), we measure consistency, i.e., the ratio of cases where the autorater gives consistent verdicts when swapping the order of the two responses. Additionally, we measure Symmetry Deviation (Δ𝑆𝐷) as Δ𝑆𝐷 := 𝑝orig + 𝑝swap 1. Ideally, an unbiased autorater should produce 𝑝orig and 𝑝swap that sum to 1, and thus the estimated Δ𝑆𝐷 would approximate 0. positive Δ𝑆𝐷 indicates bias toward reFigure 2 Positional bias by method for Qwen-2.5-7B. Each horizontal box shows the distribution of Symmetry Deviation (Δ𝑆𝐷): 0 is swap-symmetric, 1 indicates bias toward A, and +1 toward B. The black solid line marks the median, while the green dashed line marks the mean. 8 Judging with Confidence: Calibrating Autoraters to Preference Distributions sponse B, and vice versa. We report the expected Absolute Symmetry Deviation across the dataset as 𝔼[Δ𝑆𝐷] 1 𝑁 𝑖=1 𝑝orig,𝑖 + 𝑝swap,𝑖 1. (cid:205)𝑁 Distribution-Matching Tuning Reduces Positional Bias. Probabilistic autoraters, like their discrete counterparts, are susceptible to positional bias, as shown in Table 2. This bias can be severe. For example, the zero-shot verbalized confidence method on Qwen-2.5-7B yields poor consistency of just 0.4964. While other baselines, including CoT prompting and post-hoc calibration, can mitigate this issue, significant bias remains. For instance, on Qwen-2.5-7B, no single baseline excels at both metrics, with Contextual Calibration achieving the highest consistency (0.7422) and Temperature Scaling achieving the lowest deviation (0.1953). By contrast, our distribution-matching finetuning nearly eliminates this bias. The RL-Brier model, in particular, achieves consistency of 0.9007 and near-perfect absolute symmetry deviation of 0.0964. This dramatic improvement is visualized in Figure 2, which shows that baseline methods exhibit heavily skewed deviation distributions, indicating systematic preference for one response position (in this case, response B). Conversely, our finetuned models center the distribution tightly around zero, demonstrating robust swap-symmetry and verifying their effectiveness at debiasing autoraters. 5.3. Out-of-Distribution Evaluation on Human-Annotated Data To validate our approach against genuine human judgments, we conduct an out-of-distribution evaluation on the PandaLM test set (Wang et al., 2024b) using our autoraters finetuned on JudgeLM subset. This benchmark contains 1K samples, each independently annotated by three human experts. Following the standard protocol for this dataset, we treat the majority vote as the ground truth and report agreement, precision, recall, and F1 score. Table 3 Comparison of autorater performance on the PandaLM (Wang et al., 2024b) test set based on humanannotated data. We report Agreement, Precision, Recall, and F1 Score. Results marked by * are reported by Zhu et al. (2025). Method Agreement Precision Recall F1 Zero-shot Baselines GPT-3.5* GPT-4* 0.6296 0. 0.6195 0.6620 0.6359 0.6815 0.5820 0.6180 0.5728 0.6689 0.5926 0.6507 0.5923 0. Ours (Gemma-2-9B) Finetuned Baselines PandaLM-7B* JudgeLM-7B* Calibrated Autoraters Are Better Aligned with Human Preference. As shown in Table 3, our models demonstrate superior alignment with human judgments compared to both powerful zero-shot models like GPT-4 and specialized, finetuned judges, including PandaLM-7B (Wang et al., 2024b) and JudgeLM-7B (Zhu et al., 2025). The results are particularly compelling given the data efficiency of our method. For example, our Qwen-2.5-7B model, after SFT with CoT, achieves state-of-the-art F1 score of 0.6417. This performance surpasses JudgeLM7B, model trained on the full 100K JudgeLM training set, i.e., 20 more data than whats used by our SFT model. Even without CoT, our SFT model achieves an agreement of 0.7027, outperforming all baselines, including GPT-4. These findings confirm that our distribution-matching framework is highly data-efficient method for aligning autoraters with nuanced human preferences. SFT SFT w/ CoT RL (Brier) RL (Log) SFT SFT w/ CoT RL (Brier) RL (Log) 0.6856 0.7247 0.7317 0. 0.4947 0.6417 0.5564 0.5361 0.4720 0.6358 0.8185 0.8129 0.7103 0.6533 0.6983 0.4923 0.4998 0.6266 0.6220 0.5176 0.5196 0.6166 0.6048 0.5487 0.7027 0.7187 0.7297 0. 0.5240 0.6522 0.5617 0.5454 Ours (Qwen-2.5-7B) 0.5456 0.6192 9 Judging with Confidence: Calibrating Autoraters to Preference Distributions 5.4. Out-of-Distribution Evaluation on Objective Tasks Method Zero-shot Baselines Knowledge Reasoning Math Coding Overall Table 4 Evaluation of autoraters on JudgeBench (Tan et al., 2025) across four objectively labeled tasks: Knowledge, Reasoning, Mathematics, and Coding. We report evaluation accuracy in percentage. Results marked by * are reported by Tan et al. (2025). To assess performance on tasks with single ground truth (i.e., the target preference distribution degenerates to single point), we evaluate our models on JudgeBench (Tan et al., 2025), benchmark comprising four objective tasks (Knowledge, Reasoning, Mathematics, and Coding) with binary, This benchmark verifiable labels. allows for comparison against diverse set of models, including stateof-the-art proprietary APIs such as GPT-4o and Gemini-1.5-pro, multiagent judges such as ChatEval (Chan et al., 2024), and several specialized finetuned judges, including PandaLM7B (Wang et al., 2024b), Prometheus27B (Kim et al., 2024b), JudgeLM7B (Zhu et al., 2025), AutoJ-13B (Li et al., 2024), and Skyuwork-Critic8B (Shiwen et al., 2024). Following the official protocol (Tan et al., 2025), we mitigate positional bias by evaluating each response pair twice, with swapped response order, and aggregating the results to obtain the final verdict. Finetuned Baselines PandaLM-7B* Prometheus2-7B* JudgeLM-7B* AutoJ-13B* Skywork-Critic-8B* Ours (RL w/ Brier) Qwen-2.5-7B Gemma-2-9B GPT-4o* Gemini-1.5-pro* 9.09 38.31 23.38 40.26 51.30 7.14 35.71 32.14 44.64 73.21 13.14 34.86 25.14 36.57 53.43 16.67 42.86 11.90 28.57 33. 21.43 25.51 29.59 29.59 54.08 Multi-Agent Baseline 46.94 55.10 50.86 44.57 44.16 45.45 47.96 44. 66.07 53.57 44.86 46.57 39.61 39.61 38.10 35.71 60.71 58.93 61.90 28. ChatEval* 30.95 32.47 34.00 31.63 44. Calibrated Autoraters Remain Performant on Objective Tasks. As shown in Table 4, training our autoraters to model preference distributions does not compromise their performance on objective tasks. Our RL-Brier tuned Gemma-2-9B model, for instance, achieves the highest accuracy of any model on the reasoning task (55.10%). Its overall accuracy of 46.57% surpasses strong baselines like Gemini-1.5-pro and all other finetuned judges except for Skywork-Critic-8B, which was trained on substantially larger dataset1. Our Qwen-2.5-7B model is also highly competitive, achieving an overall accuracy of 44.86%. These results demonstrate that our calibration framework produces versatile probabilistic autoraters that excel at judging subjective tasks without sacrificing their effectiveness on objective, fact-based evaluations. 6. Related Work Uncertainty of Human Annotations. There is growing recognition that human-annotated data is not monolithic. Researchers have highlighted the importance of modeling label ambiguity and disagreement in standard classification tasks (Baan et al., 2022; Nie et al., 2020; Zhou et al., 2022). Most relevant to our work, Elangovan et al. (2025) argue that standard correlation metrics for evaluating LLM judges are insufficient and propose new metrics that stratify data based on human label uncertainty. We take the next step by not only evaluating with respect to this uncertainty, but also proposing methods to directly train models to capture it. 1While the exact size of the training data is not disclosed, the Skywork-Critic-8B model is described as having been finetuned on an array of high-quality datasets, including the Skywork-Reward-Preference dataset (80K samples), the Open-Critic-GPT dataset (55K samples), and other human-annotated and synthetic data. 10 Judging with Confidence: Calibrating Autoraters to Preference Distributions Calibration of LLMs. The calibration of LLMs is well-studied problem. Early work focused on post-hoc calibration methods or prompting strategies to elicit confidence (Tian et al., 2023; Xiong et al., 2024). Other approaches use supervised fine-tuning to teach models to express uncertainty, for example by using models own empirical accuracy as target label (Lin et al., 2022). More recently, reinforcement learning has been used to improve calibration. Xu et al. (2024) use quadratic reward function with PPO to calibrate model for question answering. Tao et al. (2024) combine ranking loss with an order-preserving reward to align models outputs. In the context of reward modeling, Leng et al. (2025) address the overconfidence of reward models directly by proposing PPO variants to align quality with verbalized confidence. In concurrent work, Damani et al. (2025); Stangel et al. (2025) propose designing rewards with proper scoring rules to improve confidence calibration on question-answering tasks. Our work differs by focusing specifically on the autorater calibration problem and by proposing framework grounded in modeling the true distribution of human preferences, rather than single notion of correctness. LLM-as-a-Judge. The use of powerful large language models as automated evaluators has been explored extensively. This includes the creation of benchmarks (Tan et al., 2025; Zeng et al., 2024; Zheng et al., 2023), analyses of various biases (Wang et al., 2024a; Ye et al., 2024), and methods for training specialized judge models (Kim et al., 2024a; Li et al., 2024; Saha et al., 2025; Wang et al., 2024b; Zhu et al., 2025). Additionally, recent work has also investigated the role of model uncertainty (Xie et al., 2025) and non-transitivity (Xu et al., 2025) in LLM judge evaluations. Our work contributes to this line by addressing fundamental aspect of judge reliability: its calibration to the inherent subjectivity of the evaluation task. 7. Conclusion In this paper, we address the core limitation of training autoraters on discrete preference labels, practice that overlooks the subjective and distributional nature of human judgment. We introduced general probabilistic framework to calibrate autoraters to model the full preference distribution. Our empirical results show that finetuning with distribution-matching objective leads to autoraters that are better aligned with target preference distributions, with significant improvements in calibration and substantial reduction in positional bias. By shifting the goal from predicting single verdict to modeling the spectrum of human opinion, this work contributes to building more reliable, fair, and robust AI alignment systems."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors would like to thank Kai Chen from Google DeepMind for his helpful discussion and constructive feedback during the early stage of this project."
        },
        {
            "title": "Author Contributions",
            "content": "ZL and YX conceptualized the project and designed the experiments. ZL conducted the experiments and analysis. ZL and YX summarized the major findings and drafted the manuscript with inputs from XL, CH, GL, KG, BD, DS, PZ, HP, YS, and PG. DS, MK, and BAM provided intellectual input and contributed to manuscript revisions. All authors approved the final version. 11 Judging with Confidence: Calibrating Autoraters to Preference Distributions"
        },
        {
            "title": "References",
            "content": "R. K. Arora, J. Wei, R. S. Hicks, P. Bowman, J. Quiñonero-Candela, F. Tsimpourlas, M. Sharman, M. Shah, A. Vallone, A. Beutel, et al. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025. J. Baan, W. Aziz, B. Plank, and R. Fernández. Stop measuring calibration when humans disagree. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 18921915, 2022. Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. B. Brehmer. Social judgment theory and the analysis of interpersonal conflict. Psychological bulletin, 83(6):985, 1976. C.-M. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang, J. Fu, and Z. Liu. Chateval: Towards better llmbased evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations, 2024. W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. M. Conover, M. Hayes, A. Mathur, dell, M. Zaharia, and R. Xin. instruction-tuned llm, 2023. dolly-first-open-commercially-viable-instruction-tuned-llm. J. Wan, S. Shah, A. Ghodsi, P. Wentruly open URL https://www.databricks.com/blog/2023/04/12/ Introducing the worlds first J. Xie, Free dolly: M. Damani, I. Puri, S. Slocum, I. Shenfeld, L. Choshen, Y. Kim, and J. Andreas. Beyond binary rewards: Training lms to reason about their uncertainty. arXiv preprint arXiv:2507.16806, 2025. A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. Journal of the Royal Statistical Society: Series (Applied Statistics), 28(1):2028, 1979. A. Elangovan, L. Xu, J. Ko, M. Elyasi, L. Liu, S. B. Bodapati, and D. Roth. Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and llm-as-a-judge. In The Thirteenth International Conference on Learning Representations, 2025. L. Ge, D. Halpern, E. Micha, A. D. Procaccia, I. Shapira, Y. Vorobeychik, and J. Wu. Axioms for ai alignment from human feedback. Advances in Neural Information Processing Systems, 37:80439 80465, 2024. Gemma Team. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 12 Judging with Confidence: Calibrating Autoraters to Preference Distributions C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 13211330. PMLR, 2017. M. Kahng, I. Tenney, M. Pushkarna, M. X. Liu, J. Wexler, E. Reif, K. Kallarackal, M. Chang, M. Terry, and L. Dixon. LLM Comparator: Interactive analysis of side-by-side evaluation of large language models. IEEE Transactions on Visualization and Computer Graphics, 31(1), 2025. doi: 10.1109/ TVCG.2024.3456354. S. Kim, J. Shin, Y. Cho, J. Jang, S. Longpre, H. Lee, S. Yun, S. Shin, S. Kim, J. Thorne, et al. Prometheus: In The Twelfth International Inducing fine-grained evaluation capability in language models. Conference on Learning Representations, 2024a. S. Kim, J. Suk, S. Longpre, B. Y. Lin, J. Shin, S. Welleck, G. Neubig, M. Lee, K. Lee, and M. Seo. Prometheus 2: An open source language model specialized in evaluating other language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 43344353, 2024b. J. Leng, C. Huang, B. Zhu, and J. Huang. Taming overconfidence in llms: Reward calibration in rlhf. In The Thirteenth International Conference on Learning Representations, 2025. J. Li, S. Sun, W. Yuan, R.-Z. Fan, P. Liu, et al. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations, 2024. S. Lin, J. Hilton, and O. Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022. I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. J. L. Mumpower and T. R. Stewart. Expert judgement and expert disagreement. Thinking & Reasoning, 2(2-3):191212, 1996. R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland, Z. D. Guo, Y. Tang, M. Geist, T. Mesnard, C. Fiegel, et al. Nash learning from human feedback. In International Conference on Machine Learning, pages 3674336768. PMLR, 2024. Y. Nie, X. Zhou, and M. Bansal. What can we learn from collective human opinions on natural language inference data? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 91319143, 2020. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. R. J. Passonneau and B. Carpenter. The benefits of model of annotation. Transactions of the Association for Computational Linguistics, 2:311326, 2014. S. Paun, J. Chamberlain, U. Kruschwitz, J. Yu, and M. Poesio. probabilistic annotation model for crowdsourcing coreference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 19261937, 2018. E. Pavlick and T. Kwiatkowski. Inherent disagreements in human textual inferences. Transactions of the Association for Computational Linguistics, 7:677694, 2019. 13 Judging with Confidence: Calibrating Autoraters to Preference Distributions B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. S. Saha, X. Li, M. Ghazvininejad, J. E. Weston, and T. Wang. Learning to plan & reason for evaluation with thinking-llm-as-a-judge. In Forty-second International Conference on Machine Learning, 2025. J. Schulman. Approximating kl divergence, 2020. URL http://joschu. net/blog/kl-approx. html, 2020. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. T. Shiwen, Z. Liang, C. Y. Liu, L. Zeng, and Y. Liu. https: //huggingface.co/Skywork, September 2024. URL https://huggingface.co/Skywork. Skywork critic model series. A. Siththaranjan, C. Laidlaw, and D. Hadfield-Menell. Distributional preference learning: Understanding and accounting for hidden context in rlhf. In The Twelfth International Conference on Learning Representations, 2024. P. Stangel, D. Bani-Harouni, C. Pellegrini, E. Özsoy, K. Zaripova, M. Keicher, and N. Navab. Rewarding doubt: reinforcement learning approach to calibrated confidence expression of large language models. arXiv preprint arXiv:2503.02623, 2025. S. Tan, S. Zhuang, K. Montgomery, W. Y. Tang, A. Cuadron, C. Wang, R. Popa, and I. Stoica. Judgebench: benchmark for evaluating llm-based judges. In The Thirteenth International Conference on Learning Representations, 2025. S. Tao, L. Yao, H. Ding, Y. Xie, Q. Cao, F. Sun, J. Gao, H. Shen, and B. Ding. When to trust llms: Aligning confidence with response quality. In Findings of the Association for Computational Linguistics ACL 2024, pages 59845996, 2024. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. K. Tian, E. Mitchell, A. Zhou, A. Sharma, R. Rafailov, H. Yao, C. Finn, and C. D. Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned In Proceedings of the 2023 Conference on Empirical Methods in Natural with human feedback. Language Processing, pages 54335442, 2023. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. P. Wang, L. Li, L. Chen, Z. Cai, D. Zhu, B. Lin, Y. Cao, L. Kong, Q. Liu, T. Liu, et al. Large language models are not fair evaluators. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94409450, 2024a. X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. 14 Judging with Confidence: Calibrating Autoraters to Preference Distributions Y. Wang, Z. Yu, W. Yao, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang, R. Xie, J. Wang, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. In The Twelfth International Conference on Learning Representations, 2024b. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Q. Xie, Q. Li, Z. Yu, Y. Zhang, Y. Zhang, and L. Yang. An empirical analysis of uncertainty in large language model evaluations. In The Thirteenth International Conference on Learning Representations, 2025. M. Xiong, Z. Hu, X. Lu, Y. Li, J. Fu, J. He, and B. Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. In The Twelfth International Conference on Learning Representations, 2024. T. Xu, S. Wu, S. Diao, X. Liu, X. Wang, Y. Chen, and J. Gao. Sayself: Teaching llms to express confidence with self-reflective rationales. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59855998, 2024. Y. Xu, L. Ruis, T. Rocktäschel, and R. Kirk. Investigating non-transitivity in llm-as-a-judge. In Forty-second International Conference on Machine Learning, 2025. J. Ye, Y. Wang, Y. Huang, D. Chen, Q. Zhang, N. Moniz, T. Gao, W. Geyer, C. Huang, P.-Y. Chen, et al. Justice or prejudice? quantifying biases in llm-as-a-judge. In Neurips Safe Generative AI Workshop, 2024. Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations, 2024. Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pages 12697 12706. PMLR, 2021. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. H. Zhou, X. Wan, L. Proleev, D. Mincu, J. Chen, K. A. Heller, and S. Roy. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. In The Twelfth International Conference on Learning Representations, 2024. X. Zhou, Y. Nie, and M. Bansal. Distributed nli: Learning to predict human opinion distributions for language reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 972987, 2022. L. Zhu, X. Wang, and X. Wang. Judgelm: Fine-tuned large language models are scalable judges. In The Thirteenth International Conference on Learning Representations, 2025. Judging with Confidence: Calibrating Autoraters to Preference Distributions A. Extended Related Work Disagreement in Human Judgments. Psychology studies have shown that even when individuals are presented with the same evidence, they can rationally arrive at different choices for what is the best because of systematic differences in their judgmental processes (Mumpower and Stewart, 1996), including (i) different problem definitions, where disagreement stems from judging different environmental criteria or confusion between scientific facts and personal value, (ii) different organizing principles, where individuals may apply different cue weights, function forms, or overall biases when integrating the same set of information, as explained by Social Judgment Theory (Brehmer, 1976), and (iii) different mental models about how the evidence was generated. In our context, this explains why human annotators can disagree about LLM outputs even under identical prompts or instructions: they face different value trade-offs (e.g., safety vs. helpfulness) or apply different thresholds for judgment, which may result in distinct yet internally coherent choices of what is preferred. Probabilistic Models of Agreement. Probabilistic modeling of agreement dates back to the 1950s. Classic models such as the Dawid and Skene (1979) model aim to infer single gold standard label from multiple, often noisy, annotators. Studies (Passonneau and Carpenter, 2014; Paun et al., 2018) have shown that by modeling annotator reliability, these methods can produce high-quality data even from non-expert crowd workers and outperform simpler aggregation techniques such as majority vote. In contrast to this approach, recent line of work shows that, for many complex and subjective tasks, disagreement is not simply noise but valid and reproducible signal that reflects legitimate differences in human interpretation (Nie et al., 2020; Pavlick and Kwiatkowski, 2019). Our probabilistic autorater aligns with this view by treating the annotator heterogeneity as the prediction target to capture the full distribution of human judgments. Learning from Human Feedback. Canonical Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) involves learning reward model from pairwise human preferences, often by fitting Bradley-Terry model via maximum likelihood estimation. This approach implicitly assumes that heterogeneous feedback from different humans is merely noisy estimate of single ground-truth preference. However, growing body of work (Ge et al., 2024; Munos et al., 2024; Siththaranjan et al., 2024) suggests that, in the context of AI alignment, preference heterogeneity often reflects legitimate differences in individual values and should be modeled directly rather than averaged away. Our research complements this line of work as our finetuned probabilistic autoraters can be employed in such preference-based RL to better align models with the diversity of human preferences. B. Proofs B.1. Preference Distribution and Estimation Let ℎ index an annotator drawn from population distribution 𝑝(ℎ). For pairwise input 𝑿 = 𝒙, define the annotator-specific preference probability 𝑝ℎ(𝒙) = Pr[𝑌 = 1 𝑿 = 𝒙, ℎ], 𝑌 {0, 1} (1 indicates 𝐵 𝐴). The population (ground-truth) preference is the annotator-average: 𝑝(𝒙) = 𝔼ℎ𝑝(ℎ) [ 𝑝ℎ(𝒙) ]. 16 Judging with Confidence: Calibrating Autoraters to Preference Distributions common special case here is when annotators hold stable preferences, i.e., where 𝑝ℎ(𝒙) {0, 1} (each annotator has fixed judgment per 𝒙). In that case, 𝑝ℎ(𝒙) is the indicator of ℎ prefers and 𝑝(𝒙) is the population fraction preferring B. Given 𝑚 i.i.d. labels { 𝑦 ( 𝑗) }𝑚 𝑗=1 Bernoulli( 𝑝ℎ 𝑗 (𝒙)), the Monte Carlo estimator collected by sampling annotators ℎ 𝑗 i.i.d. 𝑝(ℎ) and then 𝑦 ( 𝑗) ˆ𝑝𝑚 (𝒙) = 1 𝑚 𝑚 𝑗=1 𝑦 ( 𝑗) is an unbiased estimate of 𝑝(𝒙) with variance decreasing as 1/𝑚. Lemma 2 (Unbiasedness and variance of the multi-annotator estimate). For any fixed 𝒙 and i.i.d. sampling as above, 𝔼[ ˆ𝑝𝑚 (𝒙)] = 𝑝(𝒙), Var[ ˆ𝑝𝑚 (𝒙)] = 𝑝(𝒙) (cid:0)1 𝑝(𝒙)(cid:1) 𝑚 . Proof. By the law of total expectation, 𝔼[ 𝑦 ( 𝑗) 𝒙] = 𝔼ℎ [ 𝑝ℎ(𝒙)] = 𝑝(𝒙), so 𝔼[ ˆ𝑝𝑚 (𝒙)] = 𝑝(𝒙). Since the 𝑦 ( 𝑗) are i.i.d. Bernoulli with mean 𝑝(𝒙) (marginalizing over ℎ), Var( ˆ𝑝𝑚 (𝒙)) = Var( 𝑦 (1) )/𝑚 = 𝑝(𝒙)(1 𝑝(𝒙))/𝑚. B.2. Proof of Proposition 1 We first recall the setup. For (𝑿, 𝑌 ) with 𝑌 {0, 1} and 𝑌 𝑿 = 𝒙 Bernoulli( 𝑝(𝒙)), the policy 𝝅𝜽(𝝉 𝒙) emits token sequence 𝝉 intended to encode numeric probability. deterministic parser 𝑔 : [0, 1] {} returns either number 𝑝 [0, 1] or the unparsable symbol . Let 𝑠𝜽(𝒙) = Pr𝝉𝝅𝜽 ( 𝒙) [𝑔(𝝉) ]. The piecewise rewards are: 𝑅Brier(𝝉; 𝑦) = (cid:40)1 ( 𝑝 𝑦)2, 0, 𝑔(𝝉) = 𝑝 [0, 1], 𝑔(𝝉) = , 𝑅Log(𝝉; 𝑦) = (cid:40) 𝑦 log 𝑝 + (1 𝑦) log(1 𝑝), log 𝜖, 𝑔(𝝉) = 𝑝 [0, 1], 𝑔(𝝉) = , with 𝑝 = clip( 𝑝, 𝜖, 1𝜖) and 𝜖 (0, 1 2 ). The population objective is 𝐽𝑅 (𝜽) = 𝔼(𝑿,𝑌 ) 𝔼𝝉𝝅𝜽 ( 𝑿 ) [𝑅(𝝉; 𝑌 )]. Fix 𝒙 and abbreviate 𝑝 = 𝑝(𝒙). All statements below are conditional on 𝑿 = 𝒙 and the conclusion holds for almost every 𝒙 (w.r.t. the marginal of 𝑿). This proof utilizes the following observations: (i) for any random variable 𝑍 with finite variance and any constant 𝑎, 𝔼[(𝑍 𝑎)2] = (𝔼[𝑍] 𝑎)2 + Var(𝑍); (ii) the function 𝜙( 𝑝) = 𝑝 log 𝑝 + (1 𝑝) log(1 𝑝) is strictly concave on 𝑝 (𝜖, 1 𝜖) with unique maximizer at 𝑝 = 𝑝 (and at the boundary when 𝑝 (𝜖, 1 𝜖)). 17 Judging with Confidence: Calibrating Autoraters to Preference Distributions Brier. Let 𝑃 denote the random numeric report 𝑔(𝝉) conditional on 𝑔(𝝉) (so 𝑃 is defined with probability 𝑠𝜽(𝒙)). Then 𝔼𝝉,𝑌 [𝑅Brier(𝝉; 𝑌 ) 𝒙] = 𝑠𝜽(𝒙) 𝔼𝝉 = 𝑠𝜽(𝒙) 𝔼𝝉 = 𝑠𝜽(𝒙) 𝔼𝝉 = 𝑠𝜽(𝒙) 𝔼𝝉 (cid:2)1 (𝑃 𝑌 )2(cid:3) (cid:3) + (cid:0)1 𝑠𝜽(𝒙)(cid:1) 0 (cid:2) 𝔼𝑌 𝒙 (cid:2)(𝑌 𝑃)2(cid:3) (cid:3) (cid:2)1 𝔼𝑌 𝒙 (cid:104)1 (cid:0)𝔼𝑌 𝒙 [𝑌 ] 𝑃(cid:1) 2 (cid:104)1 ( 𝑝 𝑃)2 𝑝(1 𝑝) (cid:2)(𝑃 𝑝)2(cid:3) 𝑝(1 𝑝) (cid:105) Var𝑌 𝒙 (𝑌 ) = 𝑠𝜽(𝒙) = 𝑠𝜽(𝒙) (cid:16)1 𝔼𝝉 (cid:16)1 (cid:2)(𝔼𝝉 [𝑃] 𝑝)2 + Var𝝉(𝑃)(cid:3) 𝑝(1 𝑝) (cid:17) (cid:105) applying (i) (cid:17) . applying (i) For fixed 𝑠𝜽(𝒙) this is maximized by setting Var(𝑃) = 0 (deterministic numeric output) and 𝔼[𝑃] = 𝑝 (truthful reporting). Moreover, since 1 𝑝(1 𝑝) > 0, the expectation is positive when Var(𝑃) = 0 and 𝔼[𝑃] = 𝑝. Increasing 𝑠𝜽(𝒙) strictly increases the expectation; hence an optimizer satisfies 𝑠𝜽(𝒙) = 1. Therefore, at any global maximizer, 𝑃 is almost surely constant and equals 𝑝, i.e., 𝑝𝜽(𝒙) = 𝑝. Log with clipping. Write 𝑃 for the clipped numeric report when parsable. Then 𝔼𝝉,𝑌 (cid:2)𝑅Log(𝝉; 𝑌 ) 𝒙(cid:3) = 𝑠𝜽(𝒙) 𝔼𝝉 (cid:2) 𝔼𝑌 𝒙 [𝑌 log 𝑃 + (1 𝑌 ) log(1 𝑃)] (cid:3) + (1 𝑠𝜽(𝒙)) log 𝜖 = 𝑠𝜽(𝒙) 𝔼𝝉 [ 𝑝 log 𝑃 + (1 𝑝) log(1 𝑃) ] + (1 𝑠𝜽(𝒙)) log 𝜖 = 𝑠𝜽(𝒙) 𝔼𝝉 [𝜙(𝑃)] + (1 𝑠𝜽(𝒙)) log 𝜖. By strict concavity of 𝜙 and Jensens inequality, 𝔼[𝜙(𝑃)] 𝜙(𝔼[𝑃]) with equality iff 𝑃 is almost surely constant (deterministic numeric output). The maximizer over 𝑃 [𝜖, 1 𝜖] is uniquely 𝑃 clip( 𝑝, 𝜖, 1 𝜖) (truthful reporting). Finally, at this maximizer 𝜙(cid:0)clip( 𝑝, 𝜖, 1 𝜖)(cid:1) > log 𝜖, so allocating any mass to unparsable outputs (which yields log 𝜖) strictly reduces the expectation; hence 𝑠𝜽(𝒙) = 1. Therefore, the optimal report is deterministic and equals the clipped truth clip( 𝑝, 𝜖, 1 𝜖). Combining the two cases completes the proof. C. Implementation Details C.1. Evaluation Details We measure the probabilistic autoraters performance by comparing its judgment to human annotation or discretized probabilistic labels annotated by Gemini-2.5-Flash. Following prior work (Wang et al., 2024b; Zhu et al., 2025), we formulate the pairwise judgment task as three-class classification problem with labels 𝐴 𝐵, Tie, and 𝐵 𝐴. Let 𝑇 𝑃𝑖, 𝐹𝑃𝑖, and 𝐹𝑁𝑖 denote the true positives, false positives, and false negatives for class 𝑖, respectively. We report macro-averaged metrics by computing 18 Judging with Confidence: Calibrating Autoraters to Preference Distributions each score per class and then averaging over all 𝐶 classes: Precisionmacro = Recallmacro = F1-scoremacro = Agreement = 1 𝐶 1 𝐶 1 𝐶 𝐶 𝑖=1 𝐶 𝑖=1 𝐶 𝑖=1 𝑇 𝑃𝑖 𝑇 𝑃𝑖 + 𝐹𝑃𝑖 , 𝑇 𝑃𝑖 𝑇 𝑃𝑖 + 𝐹𝑁𝑖 , 2 𝑇 𝑃𝑖 2 𝑇 𝑃𝑖 + 𝐹𝑃𝑖 + 𝐹𝑁𝑖 , (cid:205)𝐶 𝑖=1 𝑇 𝑃𝑖 𝑖=1(𝑇 𝑃𝑖 + 𝐹𝑁𝑖) (cid:205)𝐶 . By convention, calibration is measured with respect to set of discrete (binary) labels. To evaluate model calibration, we first binarize the preference distribution to obtain ground truth preference labels and then measure the Expected Calibration Error (ECE) and Brier Score. Test samples with ground truth label being Tie are skipped for calibration evaluation. The ECE is calculated by dividing the confidence into 𝐾 equal-sized bins (each of size 1 𝐾 then calculating the accuracy and average confidence within each bin: ), and 𝐾 ECE = 𝐵𝑘 𝑁 (cid:12)Acc(𝐵𝑘) Conf (𝐵𝑘)(cid:12) (cid:12) (cid:12), Acc(𝐵𝑘) = 𝑘=1 1 𝐵𝑘 1( ˆ𝑦𝑖 = 𝑦𝑖) , Conf (𝐵𝑘) = 𝑖 𝐵𝑘 1 𝐵𝑘 𝑖 𝐵𝑘 ˆ𝑝𝑖, where 𝐵𝑘 is the number of samples whose prediction confidence falls into the interval ( 𝑘1 , 𝑘 𝐾 ], ˆ𝑦𝑖 and 𝐾 𝑦𝑖 are the predicted and true preference labels, and ˆ𝑝𝑖 is the predicted probability. By default, we set 𝐾 = 10 in our experiments. The Brier score is calculated as 1 𝑁 (cid:205)𝑁 𝑖=1( 𝑦𝑖 ˆ𝑝𝑖)2. C.2. Finetuning Details In Setting 2, we optimize the following GRPO objective (Shao et al., 2024) (𝜽) = 𝔼𝒙𝑝(𝒙),𝝉𝝅𝜽old (cid:20) min (cid:18) 𝝅𝜽(𝝉𝒙) 𝝅𝜽old (𝝉𝒙) 𝐴𝑡, clip (cid:18) 𝝅𝜽(𝝉𝒙) 𝝅𝜽old (𝝉𝒙) , 1 𝜀, 1 + 𝜀 (cid:19) (cid:19) 𝐴𝑡 𝛽 𝐷KL(𝝅𝜽 𝝅ref) (cid:21) , with the unbiased KL estimator (Schulman, 2020) 𝐷KL(𝝅𝜽 𝝅ref) 𝝅ref (𝝉𝒙) 𝝅𝜽(𝝉𝒙) log 𝝅ref (𝝉𝒙) 𝝅𝜽(𝝉𝒙) 1, where 𝝅𝜽 is the policy model being optimized, 𝝅𝜽old is the previous policy model, 𝝅ref is the reference policy, 𝐴𝑡 is the advantage estimate, 𝜀 is the clipping hyperparameter, and 𝛽 is the KL penalty coefficient. Detailed parameter settings for our experiments are presented in Table 5. 19 Judging with Confidence: Calibrating Autoraters to Preference Distributions Table 5 Detailed finetuning settings. Setting 1 Supervised Finetuning (SFT) General max total sequence length precision Optimization optimizer optimizer hyperparameters weight decay batch size training epochs learning rate 2048 bf16 AdamW (Loshchilov and Hutter, 2019) 𝛽1, 𝛽2, 𝜖 = 0.9, 0.98, 105 0.1 32 3 1 10 Setting 2 Reinforcement Learning (RL) General max total sequence length precision GRPO Setting 2048 bf16 hyperparameters number of prompts per step number of generations per prompt 𝛽 = 0.01, 𝜀 = 0.2 32 (Gemma) / 64 (Qwen) Optimization optimizer optimizer hyperparameters weight decay batch size training epochs learning rate learning rate warm-up warm-up ratio / steps AdamW (Loshchilov and Hutter, 2019) 𝛽1, 𝛽2, 𝜖 = 0.9, 0.999, 108 0.001 512 1 3 107 linear 0.01 / 50 D. Additional Results D.1. Win Rate Prediction We use subset of the LMSys Chatbot Arena Conversation dataset (Kahng et al., 2025; Zheng et al., 2023) to evaluate the autoraters ability to predict the LMs win rate. This subset contains total of 900 prompts for comparing responses from Gemma 1.0 and Gemma 1.1. For this evaluation, we use Qwen-2.5-7B as the base model for the autorater to avoid potential self-enhancement bias (i.e., favoring responses generated by LMs from the same family). Specifically, given two LMs 𝝅𝐴 and 𝝅𝐵, the win rate (of 𝝅𝐵) is defined as Pr[𝝅𝐵 𝝅𝐴] := 𝔼𝒙𝑝(𝒙) 𝔼𝝉𝐴𝝅𝐴,𝝉𝐵𝝅𝐵 Pr[𝝉𝐵 𝝉𝐴𝒙]. From the results shown in Table 6, we observe that the predicted win rates from the finetuned autoraters are more aligned with the true win rate voted by human judges. D.2. RL with Probabilistic Labels While our main experiments utilized sparse binary labels for RL, our reward function is also compatible with dense probabilistic labels. To explore how this data format affects performance, we finetuned 20 Judging with Confidence: Calibrating Autoraters to Preference Distributions Table 6 Evaluation on the Chatbot Arena Conversations dataset for comparing Gemma 1.0 to Gemma 1.1. We compare the autoraters predicted win rate for Gemma 1.0 against the true win rate from human annotators. Method Human Qwen-2.5-7B Verbal Qwen-2.5-7B Verbal w/ CoT Qwen-2.5-7B SFT Qwen-2.5-7B SFT w/ CoT Qwen-2.5-7B RL (Brier) Qwen-2.5-7B RL (Log) Win Rate of Gemma 1.0 Absolute Error to Human 0.4344 0.7397 0.5951 0.3146 0.3082 0.3640 0.3662 0.3053 0.1607 0.1198 0.1262 0.0704 0.0682 Qwen-2.5-7B using the Brier score reward on the same 5K prompts (each with 10 annotations) used for SFT. As shown in Figure 3, we compared this model against several baselines: the zero-shot model with direct verbalized confidence, the SFT model trained on probabilistic labels, and our primary model trained with RL on binary labels. The results indicate that RL with probabilistic labels improves both performance (agreement) and calibration (ECE) over the zero-shot baseline. However, it underperformed compared to the autoraters trained with larger set of binary labels, highlighting the crucial role of data diversity in achieving optimal results. Figure 3 Result of RL finetuning with probabilistic labels. E. Discussions We provide discussion on the extensions and limitations of our approach and results. First, our work initiates the study on more reliable autoraters by predicting the full preference distribution. While our empirical results focused on the pairwise evaluation setting, the general framework and analysis also apply to the pointwise evaluation setting, where the preference takes nominal or ordinal values (e.g., Likert scale). Specifically, direction extension involves employing multi-class strictly proper scoring rules, such as the multi-class Brier score or cross-entropy, as the reward function for RL. We leave thorough empirical study in this space as future work. Second, our finetuning objective aims to capture the distribution of human preferences, which stems from the human-level uncertainty on the subject. As such, the uncertainty communicated by the finetuned autorater in the form of verbalized confidence is rather aleatoric than epistemic, whereas the latter may require predicting second-order distribution (i.e., distribution over the Judging with Confidence: Calibrating Autoraters to Preference Distributions predicted probability 𝑝𝜽(𝒙)). However, such an approach may significantly complicate training; instead, we demonstrate empirically that simply providing point estimation of the probability is sufficient to improve the autoraters alignment to the preference distribution while maintaining good out-of-distribution generalizability. F. Prompts Prompt Set for Persona-Based Preference Annotation Base Prompt Structure: {Persona-specific instruction here} Your output must be single letter, either or B, enclosed in <winner> and </winner> tags. [Question] {question} [The Start of Response A] {responseA} [The End of Response A] [The Start of Response B] {responseB} [The End of Response B] Considering these factors, which response is better? Persona-Specific Instructions: 1. Factual & Technical Specialist Your task is to evaluate two responses based on their helpfulness, relevance, accuracy, and level of detail. As technical specialist, you would additionally consider factual correctness and precision in your judgment. 2. Safety & Privacy Advocate Your task is to evaluate two responses based on their helpfulness, relevance, accuracy, and level of detail. As safety advocate, you would additionally consider safety and ethical responsibility. An unsafe response is poor response. 3. Empathetic Communicator Judge the overall quality of the responses based on their helpfulness, accuracy, and detail. As an empathetic communicator, you would also pay close attention to the tone and respectfulness of the language. 4. Concision Champion Assess which response is better, considering its helpfulness, accuracy, and relevance. As champion of concision, you would additionally prefer the response that is more direct and to-the-point, penalizing unnecessary verbosity. 5. Nuance & Depth Seeker Evaluate the helpfulness, relevance, and accuracy of both responses. As an academic researcher, you would also give importance to the depth and nuance of the explanation. 6. Creative & Arts Critic Assess which response is more helpful, relevant, and accurate. As creative critic, you would also weigh the originality, style, and engagement of the writing. 7. Everyday User (4x Weight) From the perspective of regular person, not an expert, evaluate which response is better overall. Your decision should be based on which one is most helpful, relevant to the question, accurate, and provides good level of detail. The best response is the one that is most satisfying and clear. 22 Judging with Confidence: Calibrating Autoraters to Preference Distributions Prompts for Eliciting Probabilistic Prediction 1."
        },
        {
            "title": "Verbalized Confidence",
            "content": "Your task is to evaluate two responses , and , based on the provided question . Compare the responses and determine the probability that ** Response is better than Response **. Your output must be single float between 0.0 and 1.0 , enclosed in < prob_rb_better > and </ prob_rb_better > tags . Do not output any other text , reasoning , or explanations . - probability of 0.0 means Response is clearly better . - probability of 1.0 means Response is clearly better . - probability of 0.5 indicates they are of similar quality . Example Output Format : < prob_rb_better >0.85 </ prob_rb_better > [ Question ] { question } [ The Start of Response ] { responseA } [ The End of Response ] [ The Start of Response ] { responseB } [ The End of Response ] Probability Output : 2. Verbalized Confidence w/ CoT Your task is to evaluate two responses , and , based on the provided question . Compare the responses on helpfulness , accuracy , and clarity . Follow these steps for your output : 1. First , provide step - by - step reasoning of your evaluation inside < think > and </ think > tags . Explain which response is better and why . 2. Immediately after the closing </ think > tag , provide single float between 0.0 and 1.0 representing the probability that ** Response is better than Response **. This float must be enclosed in < prob_rb_better > and </ prob_rb_better > tags . probability of 0.0 means Response is clearly better , 1.0 means Response is clearly better , and 0.5 indicates they are of similar quality . Example Output Format : < think > Your detailed reasoning comparing and goes here . </ think > < prob_rb_better >0.85 </ prob_rb_better > [ Question ] { question } [ The Start of Response ] { responseA } [ The End of Response ] [ The Start of Response ] { responseB } [ The End of Response ] Your Evaluation : 3. Self-Consistency Your task is to evaluate two responses , and , based on the provided question . You must decide which response is superior . First , provide step - by - step thinking process that justifies your decision . This reasoning should be enclosed within < think > </ think > tags . 23 Judging with Confidence: Calibrating Autoraters to Preference Distributions After your reasoning , provide your final decision , which must be single character ( or ) , enclosed within < answer > </ answer > tags . Example Output Format : < think > Response directly answers the user question about , while Response is more general . Response also provides concrete example which is helpful . Therefore , is the better response . </ think > < answer >A </ answer > [ Question ] { question } [ The Start of Response ] { responseA } [ The End of Response ] [ The Start of Response ] { responseB } [ The End of Response ] Final Evaluation : 4. Logit-Based Confidence Your task is to evaluate two responses , and , based on the provided question . You must decide which response is superior . Your output must be single character : or . Do not output any other text , reasoning , or explanations . Example Output Format : [ Question ] { question } [ The Start of Response ] { responseA } [ The End of Response ] [ The Start of Response ] { responseB } [ The End of Response ] Final Decision ( or ) : G. Response Examples G.1. JudgeLM Dataset Gemma-2-9B (JudgeLM val set - QID 827) Question Compose haiku with the provided subject. Spring Model (vicuna-13b-1.1) Response Fresh new start, green leaves grow Raindrops pitter-patter, flowers bloom. Model (mix-instruct-output) Response 24 Judging with Confidence: Calibrating Autoraters to Preference Distributions Amidst blooming flowers Gentle breeze moves budding trees Spring awakens life. Ground Truth 𝑃(B is better) = 1.00 Autoraters Outputs Verbalized Confidence w/ CoT (Prediction: 0.60) <think>Both responses follow the 5-7-5 syllable structure of haiku."
        },
        {
            "title": "Response B is slightly better because it paints a more vivid picture with",
            "content": "its imagery. \"Amidst blooming flowers\" and \"Gentle breeze moves budding trees\" create stronger sense of the seasons atmosphere compared to Response As more general descriptions. uses more evocative language like \"awakens life,\" which adds deeper layer of meaning. </think><prob_rb_better>0.6</prob_rb_better>"
        },
        {
            "title": "Response B also",
            "content": "SFT w/ CoT (Prediction: 1.00) <think>1. **Analyze Response A:** Response attempts to follow the 2. 3. 5-7-5 syllable structure but fails. \"Fresh new start, green leaves grow\" has 7 syllables, \"Raindrops pitter-patter, flowers bloom\" has 9 syllables. It also includes line break within the second line, which is not haiku characteristic. **Analyze Response B:** Response correctly follows the 5-7-5 syllable structure for all three lines. It also uses evocative imagery and language appropriate for the subject \"Spring.\" **Compare and Justify Score:** Response is perfect haiku, adhering to the strict structural and thematic requirements. Response completely fails on the syllable count and line structure, making it not haiku at all. Therefore, Response is unequivocally better than Response A. confidence score of 1.00 is warranted as there is no scenario where Response would be preferred for this task.</think><prob_rb_better>1.0</prob_rb_better> RL (Brier) (Prediction: 0.99) <think>Response is better haiku because it follows the traditional structure of haiku (5-7-5 syllables per line) while Response does not. Response also captures the essence of spring with its imagery of blooming flowers, gentle breezes, and awakening life. Response As lines are too long and dont have the same poetic flow. </think><prob_rb_better>0.99</prob_rb_better> RL (Log) (Prediction: 1.00) 25 Judging with Confidence: Calibrating Autoraters to Preference Distributions <think>Response is better because it adheres to the 5-7-5 syllable structure of haiku, while Response does not. Response also uses more evocative language and imagery to capture the essence of spring. Response is more of descriptive paragraph than haiku.</think><prob_rb_better>1.0</prob_rb_better> Gemma-2-9B (JudgeLM val set - QID 3960) Question Tell me what are aspects of polytopes from given text In geometry, simplicial polytope is polytope whose facets are all simplices. For example, simplicial polyhedron in three dimensions contains only triangular faces and corresponds via Steinitzs theorem to maximal planar graph. They are topologically dual to simple polytopes. Polytopes which are both simple and simplicial are either simplices or two-dimensional polygons. Model (mix-instruct-output) Response 1. simplicial polytope is polytope in geometry where each facet is simplice. 2. As an illustration, simplicial polyhedron in three dimensions with only triangular faces corresponds to maximum planar network according to Steinitzs theorem. 3. They are simple polytopes topological dual. 4. Two-dimensional polygons or simplices are polytopes that are both simple and simplicial. Model (alpaca-native) Response The aspects of polytopes from given text are: polytope whose facets are all simplices and topologically dual to simple polytopes. Polytopes which are both simple and simplicial are either simplices or two-dimensional polygons. Ground Truth 𝑃(B is better) = 0.90 Autoraters Outputs Verbalized Confidence w/ CoT (Prediction: 0.70) 26 Judging with Confidence: Calibrating Autoraters to Preference Distributions <think>Both responses attempt to summarize the aspects of polytopes from the provided text. Response breaks down the information into numbered points, making it slightly easier to read and understand. However, it contains some inaccuracies: * Point 3 is incomplete and grammatically incorrect. It should read \"They are topologically dual to simple polytopes.\" * Point 2 uses the phrase \"maximum planar network\" which is likely typo and should be \"maximal planar graph.\" Response B, while less structured, accurately summarizes the key aspects of polytopes mentioned in the text. It is concise and avoids the errors present in Response A. </think><prob_rb_better>0.7</prob_rb_better> SFT w/ CoT (Prediction: 0.40) 27 Judging with Confidence: Calibrating Autoraters to Preference Distributions <think>1. **Analyze the Question:** The user wants to extract \"aspects of polytopes\" from the provided text. This implies identifying key definitions, properties, or relationships related to polytopes. 2. 3. 4. **Evaluate Response A:** * **Helpfulness & Relevance:** Response attempts to list aspects. It correctly identifies \"simplicial polytope\" and \"triangular faces.\" However, it includes \"maximum planar network\" instead of \"maximal planar graph,\" which is minor factual error. It also lists \"topological dual\" and \"two-dimensional polygons or simplices.\" * list, which is good format for extracting distinct points. The language is clear and directly quotes or paraphrases the text. **Coherence & Quality:** Response is presented as numbered **Helpfulness & Relevance:** Response attempts to list aspects. **Evaluate Response B:** * It correctly identifies \"a polytope whose facets are all simplices\" and \"topologically dual to simple polytopes.\" It also includes \"Polytopes which are both simple and simplicial are either simplices or two-dimensional polygons.\" * sentence. While it successfully condenses the information, it lacks the explicit separation of distinct aspects that numbered list provides. It also uses the phrase \"The aspects of polytopes from given text are:\" which is bit redundant given the question. **Coherence & Quality:** Response is single, concise"
        },
        {
            "title": "Response A is slightly better because it presents the information",
            "content": "**Compare and Justify Score (0.40):** * in more structured and easily digestible format (a numbered list). This makes it clearer that each point is distinct \"aspect.\" * Response is more concise but less organized. Its single sentence format makes it harder to parse as list of distinct aspects. * instead of \"maximal planar graph\") is not significant enough to outweigh the superior presentation and clarity of Response A. * Therefore, Response is marginally better, leading to confidence score of 0.40 (meaning is better with 40% The minor factual error in Response (\"maximum planar network\" RL (Brier) (Prediction: 0.98) <think>Response is better because it directly answers the question in concise and clear manner. It identifies the key aspects of polytopes mentioned in the text without unnecessary elaboration or rephrasing. Response A, while accurate, provides more verbose and less focused summary of the text. </think><prob_rb_better>0.98</prob_rb_better> RL (Log) (Prediction: 0.65) 28 Judging with Confidence: Calibrating Autoraters to Preference Distributions <think>Response is better because it directly answers the question by extracting and summarizing the key aspects of polytopes mentioned in the text. Response A, while accurate, simply rephrases the information from the text without providing clear and concise summary. Response is more focused and highlights the essential concepts related to polytopes. </think><prob_rb_better>0.65</prob_rb_better> Qwen-2.5-7B (JudgeLM val set - QID 376) Question What is an anemone? Model (alpaca-native) Response"
        },
        {
            "title": "An anemone is a type of sea creature that is typically found in marine",
            "content": "habitats. It is typically described as large, beautiful, and often colorful animal that is found on coral reefs. Anemones are typically classified as either solitary or colonial, depending on the species. Solitary anemones are typically smaller and are found in shallow waters, while colonial anemones are larger and can be found in deeper waters. The anemone uses its stinging cells to capture prey, and is also known to benefit from the symbiotic relationship it has with certain types of algae. Model (vicuna-13b-1.1) Response An anemone is type of marine animal that belongs to the phylum Cnidaria and the class Anthozoa. It is multicellular, radially symmetrical animal that has simple, tubular body and single, central oral-aboral opening surrounded by tentacles. Anemones are found in coral reefs, on the seafloor, and in shallow waters. They are predators, feeding on variety of small sea creatures. An anemones color can range from brown to red, orange, or purple. Ground Truth 𝑃(B is better) = 0.70 Autoraters Outputs Verbalized Confidence w/ CoT (Prediction: 0.95) Judging with Confidence: Calibrating Autoraters to Preference Distributions <think> Response provides basic description of an anemone, mentioning its habitat and classification into solitary or colonial types. However, it lacks detail about the anemones physical characteristics and behavior. Response B, on the other hand, gives more detailed and scientifically accurate description, mentioning the phylum and class of anemones, their physical structure, and feeding habits. It also provides range of colors, which is more specific and helpful for understanding the anemones appearance. Therefore, Response is more accurate and detailed. </think> <prob_rb_better>0.95</prob_rb_better> SFT w/ CoT (Prediction: 0.90) 2. <think>1. **Analyze Response A:** Response describes an anemone as \"large, beautiful, and often colorful animal.\" This is significant factual error. Anemones are typically small, often less than few inches in diameter, and not generally described as \"large.\" This misrepresentation of size is major flaw. **Analyze Response B:** Response provides more accurate and detailed description. It correctly identifies anemones as belonging to the phylum Cnidaria and class Anthozoa, and describes their physical characteristics (multicellular, radially symmetrical, tubular body, tentacles, oral-aboral opening). It also correctly states their habitat and feeding habits. **Compare Helpfulness & Relevance:** Response is far more helpful and relevant due to its accuracy. Response As fundamental error regarding size makes it misleading and unhelpful. **Compare Coherence & Quality:** Response is well-structured, clear, and accurate. Response contains critical factual error that undermines its quality. **Justify Score (0.90):** Response is significantly better because it provides accurate and comprehensive information, while Response contains major factual error regarding the size of an anemone. The error in Response makes it largely unhelpful and potentially misleading. Therefore, Response is overwhelmingly superior.</think><prob_rb_better>0.9</prob_rb_better> 4. 5. 3. RL (Brier) (Prediction: 0.99) Judging with Confidence: Calibrating Autoraters to Preference Distributions <think> Response is less accurate and less clear. It incorrectly describes anemones as \"beautiful\" and \"large,\" which are not typical descriptors for anemones. It also incorrectly states that solitary anemones are typically smaller and found in shallow waters, while colonial anemones are larger and found in deeper waters, which is not general rule. Response B, on the other hand, provides more accurate and clear description, mentioning that anemones belong to the phylum Cnidaria and class Anthozoa, and correctly describes their physical characteristics and habitat. It also correctly states that anemones are predators and that their color can vary. Given these differences, Response is clearly better. </think><prob_rb_better>0.99</prob_rb_better> RL (Log) (Prediction: 1.00) <think> Response is less accurate and less clear. It incorrectly describes anemones as \"beautiful\" and \"large,\" which is not defining characteristic. It also incorrectly states that solitary anemones are typically smaller and found in shallow waters, while colonial anemones are larger and found in deeper waters, which is not general rule. Response B, on the other hand, provides more precise and accurate description, including the scientific classification and the correct habitat and feeding habits. It also correctly mentions the range of colors anemones can have. </think><prob_rb_better>1.0</prob_rb_better> Qwen-2.5-7B (JudgeLM val set - QID 3311) Question How can get rid of rash on my neck? Model (moss-moon-003-sft) Response 31 Judging with Confidence: Calibrating Autoraters to Preference Distributions There are several ways to get rid of rash on your neck. Here are some options: 1. Try over-the-counter creams: Over-the-counter creams like hydrocortisone cream or chapstick can help soothe and moisturize the rash. Be sure to read the labels carefully and follow the instructions. 2. Use aloe vera: Aloe vera juice can help soothe and moisturize the skin, making it less itchy. Mix one tablespoon of aloe vera gel with one tablespoon of water and apply to the rash. 3. Use tea tree oil: Tea tree oil has anti-inflammatory properties that can help reduce inflammation and itching. Apply few drops of tea tree oil to the rash and leave it on for 30 minutes before rinsing it off. 4. Get rash shot: In some cases, your doctor may recommend rash shot to help reduce the itching and inflammation. This is prescription medication that you should consult your doctor about before taking. 5. Try home remedies: There are several home remedies that can help soothe rash on your neck. For example, you can try applying cucumber slices to the rash to help Model (oasst-sft-4-pythia-12b-epoch-3.5) Response You should see an allergist to get tested for allergies. Ground Truth 𝑃(B is better) = 0.50 Autoraters Outputs Verbalized Confidence w/ CoT (Prediction: 0.15) 32 Judging with Confidence: Calibrating Autoraters to Preference Distributions <think> Response provides detailed list of methods to alleviate rash on the neck, including over-the-counter creams, aloe vera, tea tree oil, rash shots, and home remedies. It offers comprehensive approach and is helpful and clear for someone dealing with rash. Response B, on the other hand, suggests seeing an allergist, which is more specific and targeted approach but does not provide immediate relief or practical steps for managing the rash. While Response is accurate in suggesting professional consultation, it lacks the practical, immediate advice that Response provides. Given the context of the question, which asks for ways to get rid of rash, Response is more helpful and comprehensive. However, Response is still accurate and could be valuable follow-up step after trying the methods in Response A. </think> <prob_rb_better>0.15</prob_rb_better> SFT w/ CoT (Prediction: 0.90) <think>1. **Analyze Response A:** Response provides list of 2. 3. 4. 5. potential solutions, including OTC creams, aloe vera, tea tree oil, and even \"rash shot.\" While some of these suggestions (like tea tree oil) are generally safe for topical use, others (like \"rash shot\") are vague and potentially misleading without proper medical consultation. The response also cuts off abruptly. **Analyze Response B:** Response suggests seeing an allergist. This is highly relevant and potentially crucial step for diagnosing and treating rash, especially if its allergy-related. It prioritizes professional medical advice. **Compare Helpfulness & Relevance:** Response is significantly more helpful and relevant because it points the user towards professional diagnosis, which is often the first and most important step in treating rash. Response As suggestions, while not inherently harmful, are less direct and potentially less effective without proper diagnosis. **Compare Coherence & Quality:** Response is bit disjointed due to the abrupt ending and the inclusion of potentially confusing or vague advice (e.g., \"rash shot\"). Response is concise and directly addresses the underlying cause, making it more coherent and actionable. **Justify Score (0.90):** Response is overwhelmingly better because it prioritizes professional medical advice, which is the most responsible and effective way to address rash. Response As suggestions, while not actively harmful, are less direct and potentially less effective without proper diagnosis. The score of 0.90 reflects that is significantly superior due to its focus on professional guidance.</think><prob_rb_better>0.9</prob_rb_better> RL (Brier) (Prediction: 0.75) Judging with Confidence: Calibrating Autoraters to Preference Distributions <think> Response provides detailed list of methods to treat rash on the neck, including over-the-counter creams, aloe vera, tea tree oil, and home remedies. However, it is quite lengthy and includes some less conventional methods like \"rash shots\" which are not commonly used for rashes and could be misleading. The response is clear but could be more concise. Response B, on the other hand, suggests seeing an allergist, which is more direct and professional approach. This is particularly useful if the rash is due to an allergic reaction. However, it doesnt provide any specific treatment options and is quite brief. Given the question, Response offers more comprehensive information, but Response provides more targeted and professional approach. Since the question asks for specific recommendation, Response is more likely to be helpful for the user. Considering the above, Response is better, but not by wide margin. </think><prob_rb_better>0.75</prob_rb_better> RL (Log) (Prediction: 0.00) <think> Response provides detailed list of methods to treat rash on the neck, including over-the-counter creams, aloe vera, tea tree oil, and home remedies. It is clear and offers multiple options, making it helpful for the user. Response B, on the other hand, suggests seeing an allergist, which is single action and does not provide specific treatment options. While it might be valid step in some cases, it does not directly address the users question about how to get rid of the rash. Given the comprehensive nature and directness of Response A, it is more helpful and clearer for the users needs. </think> <prob_rb_better>0.0</prob_rb_better> G.2. PandaLM Dataset Gemma-2-9B (PandaLM test set - QID 412) Question Instruction: Rewrite the given text and correct grammar, spelling, and punctuation errors. Input: If youd told me year ago that today would finish marathon, would of laughed. Your support had huge affect on me! Model (bloom-7b) Response 34 Judging with Confidence: Calibrating Autoraters to Preference Distributions If you told me year ago that today would finish marathon, would have laughed. Your support had huge effect on me! Model (cerebras-gpt-6.7B) Response If you had told me year ago that today would finish marathon, would have laughed. Your support had huge impact on me. Ground Truth Preference: better Autoraters Outputs Verbalized Confidence w/ CoT (Prediction: 0.60) <think>Response is slightly better because it correctly uses the past perfect tense (\"had told\") which is more grammatically accurate in this context. Response uses the simple past tense (\"told\") which is less precise. Additionally, Response uses the word \"impact\" which is more precise synonym for \"affect\".</think><prob_rb_better>0.6</prob_rb_better> SFT w/ CoT (Prediction: 0.60) 2. 4. 3. <think>1. **Identify Errors in Input:** The input contains two main errors: \"would of\" should be \"would have,\" and \"affect\" should be \"effect\" or \"impact.\" **Evaluate Response A:** Response correctly changes \"would of\" to \"would have\" and \"affect\" to \"effect.\" **Evaluate Response B:** Response also correctly changes \"would of\" to \"would have\" and \"affect\" to \"impact.\" **Compare Responses:** Both responses correctly identify and fix the grammatical errors. However, Response uses \"impact\" instead of \"effect,\" which is more common and widely accepted synonym for \"affect\" in this context (meaning influence). While \"effect\" is also grammatically correct, \"impact\" is arguably slightly more precise and natural fit for the meaning of \"huge affect.\" **Justify Score:** Response is marginally better due to its choice of \"impact\" over \"effect,\" making the corrected sentence slightly more precise and natural. The difference is subtle, but it gives slight edge in quality. Therefore, confidence score of 0.60 is appropriate, indicating is slightly better.</think><prob_rb_better>0.6</prob_rb_better> 5. RL (Brier) (Prediction: 0.98) 35 Judging with Confidence: Calibrating Autoraters to Preference Distributions <think>Response is better because it correctly uses the past perfect tense (\"had told\") and the word \"impact\" instead of \"affect,\" which is used incorrectly in the original text. Response is grammatically correct but uses the incorrect past perfect tense (\"told\") and the word \"effect\" which is not the best choice in this context. </think><prob_rb_better>0.98</prob_rb_better> RL (Log) (Prediction: 1.00) <think>Response is better because it makes more accurate grammatical corrections. * Response correctly changes \"would of\" to \"would have\" but keeps \"affect\" which should be \"effect\". * Response correctly changes \"would of\" to \"would have\" and \"affect\" to \"impact\". Response is also slightly more natural in its phrasing. </think><prob_rb_better>0.999</prob_rb_better> Qwen-2.5-7B (PandaLM test set - QID 573) Question Instruction: Predict how many stars the author will give to the restaurant from Yelp review. Input: The reviews were great, but honestly felt it was just ok. Seemed like typical tourist spot were the food was mediocre. The service was ok, not great, but not the worst. The bread was very good. ordered the muscle fries. They had sweet Vidalia onion flavor. No need for fries bc they were soggy wo much flavor. My boyfriend ordered pesto chicken pasta and it was again mediocre. Model (cerebras-gpt-6.7B) Response 5 stars Model (opt-7b) Response The author will likely give the restaurant 3 stars. Ground Truth Preference: better Autoraters Outputs Verbalized Confidence w/ CoT (Prediction: 0.95) 36 Judging with Confidence: Calibrating Autoraters to Preference Distributions <think> Response simply states \"5 stars\" without any reasoning or context, which is not helpful or accurate given the mixed feedback in the review. Response B, on the other hand, provides more reasoned prediction based on the content of the review. The review mentions that the experience was \"just ok\" and that the food and service were \"mediocre,\" which suggests below-average experience. Therefore, 3-star rating seems more appropriate than 5 stars."
        },
        {
            "title": "Response B is clearer and more aligned with the sentiment expressed in",
            "content": "the review. </think> <prob_rb_better>0.95</prob_rb_better> SFT w/ CoT (Prediction: 1.00) 2. 3. <think>1. **Analyze the Input:** The review is mixed. It starts with \"The reviews were great, but honestly felt it was just ok.\" This indicates neutral to slightly negative overall impression. It then lists specific criticisms (mediocre food, mediocre service, soggy fries) and positive points (good bread, sweet Vidalia onion flavor). The language is generally unenthusiastic. **Evaluate Response A:** Response predicts \"5 stars.\" This is highly unlikely given the content of the review. The reviewer explicitly states it was \"just ok\" and lists multiple mediocre aspects. 5-star rating would imply strong satisfaction, which is not evident. **Evaluate Response B:** Response predicts \"3 stars.\" This is much more plausible rating for the given review. 3-star rating typically indicates neutral to slightly negative experience, which aligns well with the mixed and somewhat critical tone of the review. **Compare Responses:** Response Bs prediction is significantly more accurate and reasonable than Response As. Response is completely off, while Response is good, educated guess based on the reviews content. **Justify Score:** confidence score of 1.00 is appropriate because Response is definitively better than Response A. Response As prediction is entirely incorrect and unhelpful, whereas Response provides plausible and well-reasoned prediction.</think><prob_rb_better>1.0</prob_rb_better> 4. 5. RL (Brier) (Prediction: 0.99) 37 Judging with Confidence: Calibrating Autoraters to Preference Distributions <think> Response is overly optimistic and does not reflect the mixed sentiment in the review. The review mentions \"great reviews\" but also states the experience was \"just ok\" and \"mediocre.\" The bread was good, but the fries and pasta were described as \"soggy\" and \"mediocre.\" Given the overall mixed and somewhat negative tone, 5-star rating is not appropriate. Response B, on the other hand, suggests 3-star rating, which seems more aligned with the mixed sentiment expressed in the review. 3-star rating typically indicates an average experience, which fits the description of the restaurant being \"just ok\" and having \"mediocre\" food and service. Therefore, Response is better than Response A. </think><prob_rb_better>0.99</prob_rb_better> RL (Log) (Prediction: 1.00) <think> Response is overly optimistic and does not reflect the overall sentiment of the review. The review mentions that the food was \"mediocre\" and the service was \"ok, not great.\" There is no indication that the author would give 5 stars. Response B, on the other hand, is more aligned with the sentiment expressed in the review. The author found the experience to be \"just ok\" and the food \"mediocre,\" which suggests middle rating. Therefore, Response is more accurate and helpful. </think> <prob_rb_better>1.0</prob_rb_better>"
        }
    ],
    "affiliations": [
        "Cornell University",
        "Google",
        "Google DeepMind",
        "Scale AI",
        "University of Alberta",
        "Vanderbilt University",
        "Virginia Tech"
    ]
}