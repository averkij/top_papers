{
    "paper_title": "Query-Level Uncertainty in Large Language Models",
    "authors": [
        "Lihu Chen",
        "Gaël Varoquaux"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "It is important for Large Language Models to be aware of the boundary of their knowledge, the mechanism of identifying known and unknown queries. This type of awareness can help models perform adaptive inference, such as invoking RAG, engaging in slow and deep thinking, or adopting the abstention mechanism, which is beneficial to the development of efficient and trustworthy AI. In this work, we propose a method to detect knowledge boundaries via Query-Level Uncertainty, which aims to determine if the model is able to address a given query without generating any tokens. To this end, we introduce a novel and training-free method called \\emph{Internal Confidence}, which leverages self-evaluations across layers and tokens. Empirical results on both factual QA and mathematical reasoning tasks demonstrate that our internal confidence can outperform several baselines. Furthermore, we showcase that our proposed method can be used for efficient RAG and model cascading, which is able to reduce inference costs while maintaining performance."
        },
        {
            "title": "Start",
            "content": "Query-Level Uncertainty in Large Language Models Lihu Chen1, Gaël Varoquaux2 1 Imperial College London, UK 2 Soda, Inria Saclay, France lihu.chen@imperial.ac.uk gael.varoquaux@inria.fr 5 2 0 2 1 1 ] . [ 1 9 6 6 9 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "It is important for Large Language Models to be aware of the boundary of their knowledge, the mechanism of identifying known and unknown queries. This type of awareness can help models perform adaptive inference, such as invoking RAG, engaging in slow and deep thinking, or adopting the abstention mechanism, which is beneficial to the development of efficient and trustworthy AI. In this work, we propose method to detect knowledge boundaries via Query-Level Uncertainty , which aims to determine if the model is able to address given query without generating any tokens. To this end, we introduce novel and training-free method called Internal Confidence, which leverages self-evaluations across layers and tokens. Empirical results on both factual QA and mathematical reasoning tasks demonstrate that our internal confidence can outperform several baselines. Furthermore, we showcase that our proposed method can be used for efficient RAG and model cascading, which is able to reduce inference costs while maintaining performance. The code is available at (cid:135) https://github.com/tigerchen52/ query_level_uncertainty"
        },
        {
            "title": "Introduction",
            "content": "Large language Models (LLMs) have their knowledge boundaries (Li et al., 2024; Yin et al., 2024; Ren et al., 2025), which means that there are certain problems that they cannot provide accurate outputs. It is crucial for LLMs to be self-aware of their limitations, i.e., know what know and know what dont know (Kadavath et al., 2022; Amayuelas et al., 2024). Possessing awareness of knowledge boundaries provides several advantages in developing efficient and trustworthy AI. First, if LLMs can identify known-unknown or simple-hard queries, they can smartly perform adaptive inference to balance the trade-offs between computational cost and outFigure 1: Illustrating the difference between answerlevel and query-level uncertainty. Query-level uncertainty estimating known or unknown queries (knowledge boundary) before generating answers, which is useful for adaptive inference, e.g., efficient RAG and fast-slow reasoning. put quality. For queries beyond their parametric knowledge, they can choose to find relevant external knowledge via RAG (Lewis et al., 2020) or tool calls (Schick et al., 2023). When faced with hard problems, LLMs can engage in slow (or deep) thinking to improve their outputs, which is also known as test-time scaling (Snell et al., 2024; Zhang et al., 2025). Alternatively, another solution is to defer complex problem to larger model via model cascading (Dohan et al., 2022; Gupta et al., 2024). This adaptive inference ensures that computational resources are allocated effectively, which reduces costs while maintaining performance. Second, estimating whether query is answerable enhances the honesty and trustworthiness of LLMs. When LLMs identify uncertain queries, they can use the abstention strategy (Wen et al., 2024) to withhold responses, which is important in high-stakes domains like healthcare (Tomani et al., 2024). In this work, we propose new concept, QueryLevel Uncertainty, to estimate models knowledge with regard to given query. The research question here is: Given query, can we determine if the model is able to address it without generating any tokens? Most existing work focus on answerlevel uncertainty, which measures the uncertainty associated with specific answer, helping us assess the reliability of outputs (Shorinwa et al., 2024; Vashurin et al., 2025). The main distinction here is that we shift from post-generation uncertainty to pre-generation uncertainty, which aims to measure how certain an LLM can solve this query, as shown in Figure 1. Prior studies propose learning probe on internal states to predict uncertainties of queries (Gottesman and Geva, 2024; Kossen et al., 2024). Another branch of work attempts to teach LLMs to explicitly express dont know in their responses via fine-tuning methods (Amayuelas et al., 2024; Kapoor et al., 2024; Cohen et al., 2024; Zhang et al., 2024a). One potential issue of these studies is that they often require fine-tuning and training samples, which introduces additional overhead and may limit their generalizability. We aim to introduce training-free approach to estimate querylevel uncertainty, which is simple yet effective. Our approach relies on self-evaluation across internal layers and tokens, which is called Internal Confidence. The proposed approach is based on simple assumption: LLMs can self-evaluate their knowledge about query by answering yesno question. Inspired by the uncertainty method P(True) (Kadavath et al., 2022), we can compute the probability P(Yes) to indicate the models confidence. To fully use latent knowledge within LLMs, we compute this kind of P(Yes) at each layer and token position. Following that, we aggregate these signals to obtain the final confidence score. This aggregation is motivated by prior work showing that leveraging logical consistency across layers can improve outputs (Burns et al., 2022; Chuang et al., 2023; Xie et al., 2024). Specifically, we perform weighted sum across layers and tokens, and the weights are derived from attenuated encoding (Chen et al., 2023), which can control the influence of adjacent units. To validate the effectiveness of our proposed internal confidence, we conduct experiments on three datasets that cover factual QA and mathematical reasoning tasks. For comparison, we adapt the existing answer-level methods to compute the querylevel uncertainty. Experimental results demonstrate that our proposed internal confidence can distinguish known and unknown queries better than various baselines. In terms of applications, we showcase that our proposed method can help efficient RAG and model cascading. On the one hand, internal confidence can guide users to assess the tradeoffs between cost and quality when invoking additional services. On the other hand, it brings benefit region, where inference overhead can be reduced without compromising performance. To conclude, we propose simple yet effective, training-free method to estimate query-level uncertainty, which can determine if model can address given query without generating any tokens."
        },
        {
            "title": "2.1 Uncertainty Estimation",
            "content": "Existing methods mainly focus on estimating the uncertainty of LLM-generated responses, which aim to provide score to indicate the reliability of query-answer pair (Geng et al., 2024; Shorinwa et al., 2024; Vashurin et al., 2025). These approaches often rely on internal states (Chen et al., 2024a) or textual responses (Kuhn et al., 2023), and commonly use calibration techniques to mitigate issues such as overconfidence (Zhang et al., 2024b) and biases (Chen et al., 2024b). Notably, these methods assess post-generation reliability, i.e., they evaluate uncertainty about particular answer. In contrast, there is limited research on quantifying how well model can address query prior to token generation. For example, Gottesman and Geva (2024) propose training lightweight probe on internal representations to estimate the models knowledge about specific entities. Similarly, Semantic Entropy Probes (Kossen et al., 2024) suggest that internal model states can implicitly encode semantic uncertainty, even before any output is generated. To the best of our knowledge, this work is the first to formally define query-level uncertainty and investigate it systematically."
        },
        {
            "title": "2.2 Knowledge Boundary Detection",
            "content": "LLMs should faithfully assess their level of confidence in answering query. This knowledge boundary awareness (Li et al., 2024; Yin et al., 2024; Wang et al., 2024) is essential to build reliable AI systems, particularly in high-stakes domains such as healthcare and law. pioneering study by Kadavath et al. (2022) explores whether language models can be trained to predict when they know the answer to given query, introducing the concept of Know (IK) prediction. Based on this idea, subsequent work has proposed methods to help LLMs become explicitly aware of their knowledge limitations through fine-tuning strategies (Amayuelas et al., 2024; Kapoor et al., 2024). Cohen et al. (2024) further advances this line of research by introducing special [IDK] (I dont know) token into the models vocabulary, allowing the direct expression of uncertainty in its output. Similarly, RTuning (Zhang et al., 2024a) tunes LLMs to refrain from responding to questions beyond their parametric knowledge. While these abstention-based approaches show benefits in mitigating hallucinations (Wen et al., 2024), they often require additional fine-tuning, which introduces overhead and may limit generalizability across models and tasks. In this work, we propose training-free method to identify the knowledge boundary of an LLM, which offers more generalizable and efficient alternative to detect the knowledge boundary of LLMs. tle differences. As noted by Lin et al. (2023), uncertainty is holistic property of the entire predictive distribution, while confidence refers to the models estimated confidence level associated with specific answer. For example, given query =What is the capital of France, estimating uncertainty requires the distribution over all possible answers, e.g., Paris, Toulouse, etc., as explained by the semantic entropy framework (Kuhn et al., 2023). In contrast, the conditional probability (Y = aris x) can serve as confidence here to indicate the correctness of specific answer. In the context of query-level uncertainty, we treat uncertainty and confidence as antonyms, as obtaining full probability distributions over all possible queries for given model is infeasible."
        },
        {
            "title": "3.1 Aleatoric and Epistemic Uncertainty",
            "content": "Uncertainty in machine learning is commonly categorized into two main types: aleatoric and epistemic uncertainty (Hora, 1996; Der Kiureghian and Ditlevsen, 2009; Hüllermeier and Waegeman, 2021). These distinctions are often overlooked in the context of LLM uncertainty estimation. Aleatoric uncertainty arises from inherent randomness in the data, such as ambiguous inputs or conflicting annotations. This type of uncertainty is irreducible, as it reflects intrinsic noise in the input data. In contrast, epistemic uncertainty stems from lack of knowledge, often due to insufficient training data and limited model capacity. Unlike aleatoric uncertainty, epistemic uncertainty is reducible with additional data or advanced modeling. In this work, we focus specifically on epistemic uncertainty, with the goal of evaluating whether an LLM possesses sufficient knowledge to answer given query. Although it is possible that dataset may contain some ambiguous queries and noisy labels, we assume that the benchmark datasets used in our experiments are well-curated, and have minimal ambiguity. This assumption allows us to reasonably minimize the impact of aleatoric uncertainty, and study the epistemic uncertainty in clear way."
        },
        {
            "title": "3.2 Uncertainty and Confidence",
            "content": "In the context of LLMs, the terms uncertainty and confidence are often used interchangeably (antonyms). However, the two concepts have subIn this section, we describe our problem definition and introduce our method, Internal Confidence, score that reflects whether an LLM can address query in its own knowledge, prior to generating tokens."
        },
        {
            "title": "4.1 Problem Statement",
            "content": "Given query (including prompt words) = (x1, . . . , xN ), we aim to quantify the query-level uncertainty, (x), without generating an answer y. This is different from existing uncertainty methods that estimate the uncertainty associated with specific generated answer, denoted as (x, y). We define that if an LLM can answer query correctly in greedy decoding, the query falls within the knowledge boundary of the model, and its answer can be reliable. Otherwise, the query falls beyond the models boundary, and it does not possess sufficient knowledge to answer it. We use this standard to evaluate the estimated query-level uncertainty, i.e., lower uncertainty indicates model is more likely to output the correct answer. Although different decoding strategies impact LLM outputs (Song et al., 2024), we aim to measure the internal knowledge of model in deterministic way. Here, we focus on queries with definite answers, which have broad applications such as factual QA and mathematical reasoning. While contentious queries with open answers are also important in areas such as politics and philosophy, they are out of the scope of this work. (a) P(Yes) (b) AUC (c) Decay Weights Figure 2: Left: the internal P(Yes) across tokens and layers. Middle: the AUC of P(Yes) across tokens and layers. Right: decay weights with different localities. Model: Llama-8B; Dataset: GSM8K validation set."
        },
        {
            "title": "4.2 Method",
            "content": "Existing findings reveal that LLMs can express verbalized uncertainty in their responses (Tian et al., 2023; Xiong et al., 2024), which reflects that LLMs can evaluate the answer correctness in their own knowledge. Similarly, we can prompt an LLM to assess its confidence in answering given query by using yes-no format: Respond only with Yes or No to indicate whether you are capable of answering the {Query} accurately. Answer Yes or No:. Following that, we can compute the probability P(Yes) at the last token (xN ): P(Yes) = Softmax(Wunemb [Yes, No] h(L) )Yes (1) where is the index of the last token in the query, and is the index of the last layer of the model. h(L) Rd is the hidden state and is the dimensionality of the hidden representations. Wunemb RVd is the unembedding matrix that maps the hidden state h(L) to logits over the vocabulary V. P(Yes) can serve as query-level confidence score here, which is somehow correlated with verbalized uncertainty (Tian et al., 2023), but the main difference is that this method only makes single forward pass of the query without generating any answer tokens. However, P(Yes) does fully use internal states of LLMs, which preserves rich latent information about estimating uncertainty (Azaria and Mitchell, 2023; Chen et al., 2024a). Furthermore, prior work demonstrates that using logical consistency across layers can improve outputs (Burns et al., 2022; Chuang et al., 2023; Xie et al., 2024). Therefore, we propose the Internal Confidence, which leverages latent knowledge across different layers and tokens. Let fθ denote the transformation function for computing hidden states, parameterized by θ. The hidden state for the query xn of the query at layer is computed as: = fθ(h(l1) h(l) 1 , . . . , h(l1) ) (2) In total, the model contains such latent representations, and we can use Equation 4.2 to compute the P(Yes) for each h(l) . Figure 2a shows the average P(Yes) of Llama-8B on the mathematical queries (the validation set of GSM8K (Cobbe et al., 2021)), across layers and query tokens1. We observe that the probability increases gradually from low to high layers and from left to right positions, presenting diverse behaviors. If we treat each (Yes h(l) ) as confidence score and evaluate Area Under the Curve (AUC), we can obtain an AUC heatmap to show how well the model can distinguish known and unknown queries. As shown in Figure 2b, the top right score is not optimal. Actually, the representation h(27) can achieve the best AUC, and the performance gradually declines in regions surrounding this point. We refer to this optimal point as Decision Center. It is important to note that the location of the Decision Center is sensitive to both model architecture and task type. 5 To improve the naive P(Yes), we can apply weighted average centering around the decision center, which serves as an ensemble strategy to enhance calibration and expressivity (Zhang et al., 1Here, we consider tokens after the {Query}, which means that model has seen the entire query and is able to guess its knowledge gap. Method AUC PRR ECE AUC PRR ECE AUC PRR ECE AUC PRR ECE TriviaQA SciQ GSM8K Avg Max( log p) Predictive Entropy Min-K Entropy Attentional Entropy Perplexity Internal Semantic Similarity P(Yes) Internal Confidence (w/ naive avg) Internal Confidence Max( log p) Predictive Entropy Min-K Entropy Attentional Entropy Perplexity Internal Semantic Similarity P(Yes) Internal Confidence (w/ naive avg) Internal Confidence Max( log p) Predictive Entropy Min-K Entropy Attentional Entropy Perplexity Internal Semantic Similarity P(Yes) Internal Confidence (w/ naive avg) Internal Confidence 55.5 58.9 59.9 60.6 61.8 48.7 58.1 58.8 56.2 54.9 58.5 58.1 59.4 58.6 44.1 66.4 67.2 67. 56.5 59.3 59.9 59.1 59.1 51.0 63.2 63.3 69.1 10.0 17.9 20.0 21.4 24.3 -2.4 16.4 17.3 13.1 11.1 17.7 17.4 18.7 17.1 -14.4 33.0 34.4 34.5 12.4 18.9 20.0 17.2 17.8 2.5 25.8 27.6 38.4 - - - - - 0.3 13.9 19.9 13.9 - - - - - 24.4 27.5 14.9 19. - - - - - 2.0 31.9 8.0 28.7 Phi-3.8B 51.4 51.2 52.7 56.2 57.7 46.9 58.8 52.4 57.2 2.9 3.9 4.9 9.4 16.6 -5.9 16.9 4.5 15.2 Llama-8B 51.4 51.4 53.5 57.7 58.3 46.1 51.3 58.6 56. 1.9 3.2 7.9 15.2 15.1 -7.1 2.4 15.4 13.0 Qwen-14B 54.1 53.2 55.7 59.4 60.1 45.5 61.0 60.5 65.0 6.9 6.9 11.3 19.2 20.7 -7.7 22.4 20.5 30.8 - - - - - 12.2 10.8 3.3 8.2 - - - - - 30.8 23.7 21.5 18. - - - - - 14.9 23.9 15.3 20.6 55.0 63.6 60.4 52.4 53.6 47.9 56.6 54.7 57.2 53.3 66.1 57.5 56.1 53.2 52.7 62.2 59.1 62.9 54.3 66.4 63.0 54.9 54.0 47.5 54.7 61.7 62.7 11.3 25.7 17.9 4.4 6.9 -2.6 12.0 14.7 12.9 10.4 28.0 13.2 13.5 4.3 6.7 24.8 18.7 27. 13.5 32.6 30.9 3.1 7.3 -4.6 7.5 28.4 28.4 - - - - - 35.2 7.6 21.7 6.0 - - - - - 45.9 11.6 29.2 1.3 - - - - - 33.1 5.8 36.3 5.5 54.0 57.9 57.7 56.4 57.7 47.8 57.8 55.3 56.9 53.2 58.7 56.4 57.7 56.7 47.6 60.0 61.6 62. 55.0 59.6 59.5 57.8 57.7 48.0 59.6 61.8 65.6 8.1 15.8 14.3 11.7 15.9 -3.6 15.1 12.2 13.7 7.8 16.3 12.8 15.8 12.2 -4.9 20.1 22.8 25.1 10.9 19.5 20.7 13.2 15.3 -3.3 18.6 25.5 32.5 - - - - - 15.9 10.8 15.0 9.4 - - - - - 33.7 20.9 21.9 13. - - - - - 16.7 20.5 19.9 18.3 Table 1: Overall performances of different query-level uncertainty methods. 2020; Stickland and Murray, 2020). We refer to this process as Internal Confidence (IC), which can be denoted as: (cid:88) (cid:88) IC(h) = w(l) P(Yes h(l) ) (3) n=1 l=1 is the weight for each h(l) where w(l) . The equation describes two-step aggregation process. First, we compute weighted sum across layers for each individual token. Then, we apply second weighted average over these token-level aggregated scores. Ideally, this process requires layer weight matrix Wlayer RN for the first step and token weight matrix Wtoken R1N for the second step. Through this aggregation, we are able to obtain final confidence score. In practical implementation, the decision center is static and fixed to the last token and last layer. However, it is possible to use hold-out set to identify optimal positions tailored to specific models and tasks. We make this simplification to get rid of the requirement of training samples and aim to obtain better generalizability. Additionally, the layer weight vectors are shared across tokens, which means we need only two weight vectors: Wlayer R1L and Wtoken R1N . To reflect the observations that AUC performances gradually decay from the decision center, we adopt the Attenuated Encoding to compute the above two weight vectors (Chen et al., 2023) δi,j = Φ(di,j) = exp( d2 i,j) j=1 exp( d2 i,j) (cid:80) (4) where is the index of the decision center, di,j is the relative distance, and > 0 is scalar parameter that controls the locality value. Locality is metric that measures how much the weights of weight vector are gathered in adjacent positions. Given weight vector for the i-th position ϵi = {ϵi,1, ϵi,2, ..., ϵi,n}, the locality can be denoted as: Loc(ϵi) [0, 1] = (cid:88) j=1 ϵi,j 2ij (5) Figure 2c shows the weights computed by Equation 4 with varied localities. This signifies that we can control the influence of neighboring layers and tokens during the averaging process. Our proposed internal confidence is training-free and efficient, as it requires only single forward pass of given query. Since model responses are usually longer than input prompts and invoking (a) TriviaQA (b) SciQ (c) GSM8K Figure 3: We use Internal Confidence of Phi-3.8B to predict whether the corresponding can distinguish known and unknown queries. external services like RAG adds significant overhead. We hope this pre-generation uncertainty can support adaptive reasoning."
        },
        {
            "title": "5.1 Settings",
            "content": "Implementations We provide one positive and one negative example to prompt LLMs, and the target model should follow the examples to output answers. All LLMs use greedy decoding to have deterministic results. The decision center is fixed to the last layer and last token, and we set = 1.0 (Equation 4) for all models and datasets. Models Three different sizes of LLMs are used in experiments: Phi-3-mini-4k-instruct (Abdin et al., 2024), Llama-3.1-8B-Instruct (Grattafiori et al., 2024), and Qwen2.5-14B-Instruct (Team, 2024). We aim to evaluate if internal confidence can be scaled to different model sizes. Note that internal confidence can be used for models without instruction tuning. Datasets We evaluate on two factual QA datasets and one mathematical reasoning dataset: TriviaQA (Joshi et al., 2017), SciQ (Welbl et al., 2017), and GSM8K (Cobbe et al., 2021). The first two tasks aim to assess factual knowledge stored in parameters, while GSM8K requires models to selfevaluate their reasoning capabilities. Ground truth of factual QA tasks is short answer with some entity facts. GSM8k calls for short answer, but the intermediate reasoning steps have been evaluated as well, following prior work (Kadavath et al., 2022). edge and it falls in its knowledge boundary. For the first two datasets with short answers, we consider an answer to be correct if its Rouge-L (Lin and Och, 2004) of the ground truth is greater than 0.3, which is consistent with prior work (Kuhn et al., 2023). For the GSM8K dataset, we use an LLM evaluator, Mistal-Large (MistralAI, 2024), to assess both reasoning steps and final answer. After that, we can obtain binary label for each query, which shows if model is able to address the query. Baselines We adapt existing answer-level methods to quantify the pre-generation uncertainty, e.g., logit-based uncertainty. Given query (including prompt words) = (x1, . . . , xN ), we can obtain probability for each token (xn x<n) by performing forward pass. (1) The baseline Max( log p) measures the querys uncertainty by assessing the least likely token in the query (Manakul et al., 2023). (2) Predictive Entropy is defined as the entropy over the entire query tokens (Malinin and Gales, 2021): PE(x) = (cid:88) n=1 log (xn x<n) (6) (3) Min-K Entropy combines the thoughts of the Max( log p) and predictive entropy , which select the top-K of tokens from the query with the minimum token probability (Shi et al., 2024). (4) Attentional Entropy is an adapted version of the predictive entropy by performing weighted sum: AE(x) = (cid:88) n=1 αn log (xn x<n) (7) We ask model to generate answers in greedy decoding way. If the answer is aligned with ground truth, we regard that the model has sufficient knowlwhere αn is the attentional weights for the token xn. The intuition here is that tokens contribute to the semantic meanings in different way, and we should (a) Efficient RAG (b) Model Cascading Figure 4: Left: We use estimated internal confidence scores to decide whether to invoke RAG. If the internal confidence exceeds threshold, the model answers the query using its parametric knowledge. Otherwise, it relies on external knowledge for reasoning. The plot shows the accuracy of Phi-3.8B on the TriviaQA dataset under this setting. Right: We implement model cascading seeting with Phi-3.8B (small) and Llama-8B (large) on the TriviaQA dataset. The internal confidence of the smaller model determines whether it answers the query or defers to the larger model when confidence is low. not treat all tokens equally (Duan et al., 2024). (5) Perplexity reflects how uncertain model is when predicting the next token: PPL = exp( (cid:88)"
        },
        {
            "title": "1\nN",
            "content": "logP (xn x<n)) (8) , ..., h(L) (6) Internal Semantic Similarity measures the average similarity among hidden states of different layers {h(1) }, which is inspired by the lexical similarity (Fomicheva et al., 2020). (7) P(Yes) is the probability of self-evaluation, which is described in Equation 4.2. (8) Internal Confidence (w/ naive avg) is variant of our proposed internal confidence. The distinction is we apply naive average to aggregate all scores. Evaluation Metrics We evaluate uncertainty by assessing whether method can distinguish known and unknown queries, which can be treated as ranking problems, i.e., lower uncertainty means model is more likely to know the answer to the query. Following prior work (Manakul et al., 2023; Kuhn et al., 2023), we adopt the metrics Area Under the Curve (AUC) and Prediction Rejection Ratio (PRR) (Malinin et al., 2017) to measure this. Additionally, we use the Expected Calibration Error (ECE) to assess the calibration of different methods. 5."
        },
        {
            "title": "Internal Confidence Can Identify Known\nand Unknown Queries",
            "content": "Table 1 shows the overall performances of various query-level uncertainty methods. First, we can observe that our proposed internal confidence can distinguish known and unknown queries better than other baselines (based on AUC and PRR) on average, especially for larger models such as Llama-8B and Qwen-14B. For example, the average AUC of Qwen-14B is 65.6, which is significantly higher than other baselines. Regarding the calibration (ECE), internal confidence can achieve lower error across models and tasks consistently. These findings indicate the effectiveness of internal confidence. Second, the variant, Internal Confidence (w/ naive avg, leads to decrease in general, which demonstrates that the benefit of using the attenuated encoding to obtain decay weights. Additionally, Figure 3 shows the how well the internal confidence can distinguish known and unknown queries across three tasks. While the results confirm that our training-free method can predict knowledge boundaries to some extent, there is still considerable room for improvement. We hope this initial effort encourages further research in this direction. 5."
        },
        {
            "title": "Internal Confidence Makes LLM\nReasoning More Efficiently",
            "content": "Recent studies advance LLM reasoning by introducing additional resources, such as using RAG to obtain external knowledge (Lewis et al., 2020) and inference-time scaling to improve outputs (Snell et al., 2024). However, it is not always necessary to use additional resources, especially for simple queries. Here, we can use our proposed internal Figure 5: Impacts of locality on validation sets. confidence to determine when to invoke RAG, slow thinking, or model cascading."
        },
        {
            "title": "Performance",
            "content": "We conduct experiments for two scenarios: (1) Efficient RAG. Basically, the internal confidence can serve as signal of the knowledge gaps of model. If the score is greater than threshold, the model is confidence to address the query. Otherwise, it requires the call of RAG. We use the TriviaQA dataset for evaluation. This dataset provides web search results for query, which can be used as retrieved contexts for RAG. (2) Model Cascading. This task aims to achieve cost-performance trade-offs by coordinating small and large models (Dohan et al., 2022; Gupta et al., 2024). Smaller models is responsible for easy missions. If they are aware that the mission is hard to complete, it invokes larger model. We use two-model cascade setting with Phi-3.8B and Llama-8B on the TriviaQA dataset. Likewise, if the internal confidence of the smaller model is high, we do not invoke the larger model. Otherwise, the hard query is deferred to the larger model. Figure 4 shows the results of efficient RAG and model cascading. The trade-off region means that we can carefully select threshold to control the call of external services, which helps strike balance between efficiency and performance. The benefit region indicates scenarios where the use of additional resources can be reduced without compromising performance. Results across the two tasks further confirm the effectiveness of Internal Confidence in identifying knowledge gaps. Our method offers practical benefits by reducing inference overhead, which is correlated with computation time and monetary cost. We introduce attenuated encodings to aggregate probabilities centering around decision point. The locality of the encoding may impact the performance of estimated uncertainties. To study the influence of the locality, we vary the in Equation 4 to obtain encoding with different localities and observe how they can impact the estimations. Figure 5 shows the AUC across different datasets and models. We can observe that the locality is correlated with task types and model architecture. For example, Phi-3.8B prefers an extreme locality (1.0) while Qwen-14B has certain optimal value around 0.8. Regarding different datasets, the influence of locality values displays slightly different behaviors. Although we may need to search an optimal locality for specific task, we show that an empirical value with (w = 1.0, Locality=0.72) can achieve competitive performances across models and datasets."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose new concept called query-level uncertainty, which aims to assess whether model can address query without generating any tokens. To this end, we propose the approach, internal confidence, which leverages latent self-evaluation to identify the boundary of models knowledge. Experimental results verify the effectiveness of our approach in factual QA and mathematical reasoning. Furthermore, we apply internal confidence to two practical scenarios of adaptive inference, efficient RAG and model cascading. Our findings reveal that our method can identify two regions: trade-off region and benefit region. The former means that users can strike balance between cost and quality by carefully selecting threshold of confidence scores. The latter means that users can reduce inference overhead without compromising performance. Although our method can serve as strong baseline for estimating querylevel uncertainty, there is still considerable room for improvement. We hope this study can stimulate future studies in this area."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, and 1 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Yang Wang. 2024. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 64166432. Amos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 967976. Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024a. Inside: Llms internal states retain the power of hallucination detection. In ICLR. Roi Cohen, Konstantin Dobler, Eden Biran, and Gerard de Melo. 2024. dont know: Explicit modeling of uncertainty with an [idk] token. Advances in Neural Information Processing Systems, 37:1093510958. Armen Der Kiureghian and Ove Ditlevsen. 2009. Aleatory or epistemic? does it matter? Structural safety, 31(2):105112. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif Saurous, Jascha Sohl-Dickstein, and 1 others. 2022. Language model cascades. arXiv preprint arXiv:2207.10342. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2024. Shifting attention to relevance: Towards the predictive uncertainty quantification of freeform large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50505063. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539555. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. 2024. survey of confidence estimation and calibration in large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 65776595. Lihu Chen, Alexandre Perez-Lebel, Fabian Suchanek, and Gaël Varoquaux. 2024b. Reconfidencing llms from the grouping loss perspective. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 15671581. Daniela Gottesman and Mor Geva. 2024. Estimating knowledge in large language models without generating single token. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 39944019. Lihu Chen, Gael Varoquaux, and Fabian Suchanek. 2023. The locality and symmetry of positional enIn Findings of the Association for Comcodings. putational Linguistics: EMNLP 2023, pages 14313 14331. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna Menon, and Sanjiv Kumar. 2024. Language model cascades: Token-level uncertainty and beyond. In The Twelfth International Conference on Learning Representations. Stephen Hora. 1996. Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management. Reliability Engineering & System Safety, 54(2-3):217223. Eyke Hüllermeier and Willem Waegeman. 2021. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine learning, 110(3):457506. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, and 1 others. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221. Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, and Andrew Gordon Wilson. 2024. Large language models must be taught to know what they dont know. In The Thirtyeighth Annual Conference on Neural Information Processing Systems. Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal. 2024. Semantic entropy probes: Robust and cheap hallucination detection in llms. arXiv preprint arXiv:2406.15927. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459 9474. Andrey Malinin, Anton Ragni, Kate Knill, and Mark Gales. 2017. Incorporating uncertainty into deep learning for spoken language assessment. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 4550. Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 90049017. MistralAI. 2024. Mistral large: general-purpose https://mistral.ai/news/ language model. mistral-large-2407/. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2025. Investigating the factual knowledge boundary of large language models with retrieval In Proceedings of the 31st Interaugmentation. national Conference on Computational Linguistics, pages 36973715. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2024. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations. Ola Shorinwa, Zhiting Mei, Justin Lidard, Allen Ren, and Anirudha Majumdar. 2024. survey on uncertainty quantification of large language models: Taxonomy, open research challenges, and future directions. arXiv preprint arXiv:2412.05563. Moxin Li, Yong Zhao, Yang Deng, Wenxuan Zhang, Shuaiyi Li, Wenya Xie, See-Kiong Ng, and Tat-Seng Chua. 2024. Knowledge boundary of large language models: survey. arXiv preprint arXiv:2412.12472. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL04), pages 605612. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating with confidence: Uncertainty quantification for black-box large language models. Transactions on Machine Learning Research. Yifan Song, Guoyin Wang, Sujian Li, and Bill Yuchen Lin. 2024. The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism. arXiv preprint arXiv:2407.10457. Asa Cooper Stickland and Iain Murray. 2020. Diverse ensembles improve calibration. arXiv preprint arXiv:2007.04206. Qwen Team. 2024. Qwen2.5: party of foundation models. Andrey Malinin and Mark Gales. 2021. Uncertainty estimation in autoregressive structured prediction. In International Conference on Learning Representations. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence methods for uncertainty calibration in deep learning. In International conference on machine learning, pages 1111711128. PMLR. Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, and Xipeng Qiu. 2024b. Calibrating the confidence of large language models by eliciting fidelity. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2959 2979. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, and 1 others. 2025. survey on test-time scaling in large language models: What, how, where, and how well? arXiv preprint arXiv:2503.24235."
        },
        {
            "title": "A Example Appendix",
            "content": "This is an appendix. scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 54335442. Christian Tomani, Kamalika Chaudhuri, Ivan Evtimov, Daniel Cremers, and Mark Ibrahim. 2024. Uncertainty-based abstention in llms improves safety and reduces hallucinations. arXiv preprint arXiv:2404.10960. Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Daniil Vasilev, Akim Tsvigun, Sergey Petrakov, Rui Xing, Abdelrahman Sadallah, Kirill Grishchenkov, and 1 others. 2025. Benchmarking uncertainty quantification methods for large language models with lm-polygraph. Transactions of the Association for Computational Linguistics, 13:220248. Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Huimin Wang, Guanhua Chen, and Kam-fai Wong. 2024. Self-dc: When to reason and when to act? self divide-and-conquer for compositional unknown questions. arXiv preprint arXiv:2402.13514. Johannes Welbl, Nelson Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106. Bingbing Wen, Jihan Yao, Shangbin Feng, Chenjun Xu, Yulia Tsvetkov, Bill Howe, and Lucy Lu Wang. 2024. Know your limits: survey of abstention in large language models. arXiv preprint arXiv:2407.18418. Zhihui Xie, Jizhou Guo, Tong Yu, and Shuai Li. 2024. Calibrating reasoning in language models with internal consistency. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. 2024. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. In The Twelfth International Conference on Learning Representations. Xunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. 2024. Benchmarking knowledge boundary for large language models: different perspective on model evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 22702286. Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 2024a. R-tuning: Instructing large language models to say dont know. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 71067132. Jize Zhang, Bhavya Kailkhura, and Yong-Jin Han. 2020. Mix-n-match: Ensemble and compositional"
        }
    ],
    "affiliations": [
        "Imperial College London, UK",
        "Soda, Inria Saclay, France"
    ]
}