{
    "paper_title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers",
    "authors": [
        "Wenhan Ma",
        "Hailin Zhang",
        "Liang Zhao",
        "Yifan Song",
        "Yudong Wang",
        "Zhifang Sui",
        "Fuli Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models."
        },
        {
            "title": "Start",
            "content": "Wenhan Ma Hailin Zhang Liang Zhao Yifan Song Yudong Wang Zhifang Sui Fuli Luo 5 2 0 2 1 2 ] . [ 2 0 7 3 1 1 . 0 1 5 2 : r State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University LLM-Core Xiaomi Independent Researcher"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has emerged as crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer new solution for stabilizing RL in MoE models."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has become cornerstone in the post-training of large language models (LLMs) (Guo et al., 2025; OpenAI, 2024; Ouyang et al., 2022). By leveraging large-scale RL, LLMs acquire the advanced capabilities necessary to tackle complex problems, including competition-level mathematics (Guo et al., 2025) and practical code agent tasks (Luo et al., 2025a), through more profound and extended reasoning. critical challenge in LLM-based RL is balancing efficiency and stability, with the latter being essential for reliable performance. Modern RL frameworks typically employ distinct engines for inference and training phases (e.g., SGLang (Zheng et al., 2024) for rollout and Megatron (Shoeybi et al., 2019) for training). This architectural separation can lead to divergent token probabilities, potentially causing catastrophic RL collapse (T. M. Lab, 2025). To mitigate this discrepancy, Yao et al. (2025) incorporate an importance-sampling mechanism in policy updating, while T. M. Lab (2025) introduce specialized compute kernels to reduce nondeterminism during LLM inference. However, in practice, existing approaches do not fully resolve the intensified off-policy issue that arises during RL training on Mixture-of-Experts (MoE) models. In this work, we identify the routing distribution as pivotal factor contributing to the instability Work done during internship at Xiaomi Corporation. Co-corresponding authors. 1 Figure 1 Left: Illustration of the Rollout Routing Replay (R3). Top right: Training and inference discrepancies before and after applying R3. Bottom right: Reinforcement learning training performance before and after applying R3. of MoE RL. Within MoE models, the router dynamically selects and activates subset of experts for each input token. The varied routing decisions result in greater policy discrepancies between training and inference in MoE models compared to their dense counterparts. Rather than resorting to workarounds such as discarding data with large discrepancies (Zhao et al., 2025a), we propose to tackle this instability by addressing its root cause: the routing distribution itself. Specifically, we propose Rollout Routing Replay (R3), simple yet effective method for stabilizing RL training of MoE models. R3 works by capturing the routing distributions from the inference engine during sequence generation and replaying them directly into the training engine. This process significantly narrows the gap between training and inference, marked by substantial reduction in KL divergence of logits produced by the different engines. As result, the number of tokens with significant probability discrepancies between the two phases is reduced by approximately an order of magnitude. On realistic RLVR tasks with MoE models, R3 demonstrates significant improvements in training stability and performance. Our method consistently shows marked improvements in both efficiency and overall performance when compared against existing approaches designed to stabilize RL training. Furthermore, its applicability to both on-policy and mini-batch style off-policy RL scenarios underscores the robustness of our approach. Our main contributions are as follows: 1. We systematically identify and analyze routing distribution discrepancies between training and inference in MoE models, highlighting their role in training instability. 2. We propose Rollout Routing Replay, which reuses inference-time routing distributions inside the training engine to align routing behavior between training and inference. 3. We apply R3 in multiple RL settings (multi-/single-mini-step and Base/SFT models) for MoE reinforcement learning and show that R3 outperforms GSPO and TIS in terms of stability and overall performance."
        },
        {
            "title": "2 Preliminaries",
            "content": "Notation We consider an autoregressive language model, parameterized by 洧랚, represented as policy 洧랢洧랚 that generates response 洧녽 from query 洧논 D. The likelihood of the sequence is given by the factorization 洧랢洧랚( 洧녽洧논) = (cid:206) 洧녽 洧랢洧랚( 洧녽洧노 洧논, 洧녽<洧노), where 洧녽 is the sequence length. 洧랢infer and 洧랢train 洧노=1 denote the policy as it operates within the inference and training engines, respectively. Proximal Policy Optimization (PPO) (Schulman et al., 2017) is cornerstone algorithm for policy optimization in reinforcement learning. For given query 洧논, PPO updates the policy 洧랢洧랚 by maximizing: JPPO(洧랚) = E洧논D,洧녽洧랢infer (洧랚old ) ( 洧논 ) (cid:34) 1 洧녽 洧녽 洧노=1 (cid:16) min 洧녻洧노 (洧랚) 틙洧냢洧노, clip(洧녻洧노 (洧랚), 1 洧, 1 + 洧) 틙洧냢洧노 (cid:35) (cid:17) . (1) The importance sampling ratio 洧녻洧노 (洧랚) for token 洧녽洧노 in sequence 洧녽 is defined as: 洧녻洧노 (洧랚) = 洧랢train(洧랚) ( 洧녽洧노 洧논, 洧녽<洧노) 洧랢train(洧랚old)( 洧녽洧노 洧논, 洧녽<洧노) . The advantage 틙洧냢洧노 for token 洧녽洧노 is typically estimated by separate value model, and 洧 is the clipping range for the importance ratio. For brevity, we omit the KL regularization term. critical inconsistency arises from the common practice of using separate engines for rollout and training, where data is sampled via an inference policy (洧랢infer) but the loss is computed using training policy (洧랢train), as shown in Equation 1. This policy mismatch causes training instability in reinforcement learning. In MoE models, we find that it mainly stems from router inconsistency. Our proposed solution effectively mitigates this issue, making it broadly applicable, orthogonal to, and compatible with recent policy optimization frameworks such as GRPO (Shao et al., 2024), GSPO (Zheng et al., 2025), and DAPO (Yu et al., 2025)."
        },
        {
            "title": "3 Training-Inference Discrepancies",
            "content": "Training-inference discrepancies in RL frameworks frequently lead to unstable training and model collapse. In this section, we demonstrate that this policy mismatch is significantly amplified in MoE models, primarily stemming from inconsistent routing distributions. Furthermore, we observe that even multiple runs of the same training framework can produce divergent token probabilities, further contributing to RL training instability."
        },
        {
            "title": "3.1 Policy Discrepancies between Training and Inference in MoE Models",
            "content": "Here we employ an MoE model, Qwen3-30B-A3B (Yang et al., 2025), for our experiments to analyze the policy discrepancy between the training and inference engines. First, we use the SGLang inference engine to generate responses for 2,048 mathematics problems, saving the token probabilities for each generated token. This process yields about 20 million response tokens. Then these responses are passed through Megatron to obtain the corresponding probabilities assigned by the training engine. We use several metrics to quantify the divergence between these two probability distributions. KL Divergence Estimation Let 洧녢 be the set of all response tokens, we use the 洧녲3 method proposed in Schulman (2020) to estimate the KL-divergence between the training and inference probability 3 (a) MoE (b) MoE + R3 (c) Dense (d) Extreme Token Distribution Analysis Figure 2 (a): Illustration of the training-inference discrepancy in the MoE model. (b): Illustration of the training-inference discrepancy in the MoE+R3 model. (c): Illustration of the traininginference discrepancy in the Dense model. (d): Extreme Token Distribution Function, calculated based on Equation 3. distributions: DKL(洧랢train(洧랚)洧랢infer(洧랚)) 1 洧녢 洧노洧녢 (cid:2) 洧랢train(洧랚)(洧노) 洧랢infer(洧랚)(洧노) 1 log 洧랢train(洧랚)(洧노) 洧랢infer(洧랚)(洧노) (cid:3) . (2) Our calculations show that the estimated KL divergence of Qwen3-30B-A3B (MoE) is 1.535 103, while that of Qwen3-8B (Dense baseline) is 6.4 104. Visualization To visualize the training-inference discrepancies of the MoE model, we randomly sample 10 million response tokens and plot scatter diagram with SGLang probabilities on the x-axis and Megatron probabilities on the y-axis. The degree of concentration around the 洧녽 = 洧논 line indicates the degree of consistency. As shown in Figure 2a and 2c, compared to Qwen38B, Qwen3-30B-A3B exhibits much wider scatter band, revealing larger training-inference discrepancy. Extreme Token Distribution Analysis To quantify the discrepancy between the models behavior during training and inference, we introduce the Extreme Token Distribution Function, 洧냧(洧랦), defined as: 洧냧(洧랦) = 1 洧녢 (cid:104) 洧노洧녢 max (cid:16) 洧랢train(洧랚)(洧노) 洧랢infer(洧랚)(洧노) , 洧랢infer(洧랚)(洧노) 洧랢train(洧랚) (洧노) (cid:17) (cid:105) > 洧랦 . (3) This function measures the proportion of extreme tokensthose for which the probability ratio between the training distribution 洧랢train(洧랚) and the inference distribution 洧랢infer(洧랚) surpasses threshold 洧랦. Figure 2d plots this function 洧냧(洧랦) against the threshold 洧랦. The plot reveals that for 洧랦 > 2, the Qwen3-30B-A3B model exhibits fraction of extreme tokens an order of magnitude larger than that of the Qwen3-8B model. This significant gap indicates substantially higher degree of training-inference variability in the MoE model."
        },
        {
            "title": "3.2 Routing Discrepancies between Training and Inference in MoE Models",
            "content": "From the perspective of functional continuity, the key difference between MoE and Dense models lies in the non-continuity introduced by routing. In MoE models, small perturbations in the router input can lead to entirely different experts being selected, causing large changes in layer outputs. Dense models, lacking an explicit expert selection process, do not exhibit this phenomenon. 4 (a) Router-level Difference (b) Token-level Difference Sequence-level Differ- (c) ence Figure 3 Router discrepancy analysis Based on this, we further analyze router distribution discrepancies between training and inference in MoE models. We use SGLang (Zheng et al., 2024) to generate responses for 2048 mathematical problems with Qwen3-30B-A3B (Yang et al., 2025). For each response, we collect the routing distribution of all tokens (including input tokens) from the inference engine. Then, we feed these sequences into the Megatron engine for forward propagation. This process yields the routing distribution observed from training engine. We compare these two sets of routing information at different levels: Router-level Comparison: For each token and each MoE layer, we count the number of differing expert choices made by the MoE Router and calculate the frequency of these differences. Figure 3a illustrates the results. It is observed that approximately 10% of the routers select different experts during training compared to inference. Token-level Comparison: For each token, we count the number of differing expert choices made by the MoE Router across all layers. We also determine the frequency of these occurrences. Figure 3b presents these findings. It shows that 94% of tokens select different expert in at least one layer during the forward pass. Sequence-level Comparison: For sequence, we compute the router distribution difference of each token, then average over tokens to obtain the mean difference per sequence and plot histogram 3c. Results show that the mean difference per token is approximately 6 routers. These findings demonstrate that during training and inference, MoE models exhibit router distribution discrepancies. In 4 we will show empirically that routing discrepancies is the main contributor to the additional training-inference discrepancies of MoE compared to Dense models."
        },
        {
            "title": "3.3 Variation in Repeated Forward Passes of the Same\nSequence within the Same Framework",
            "content": "We conduct two forward passes of the same sequence set under the Megatron framework and obtain two probability distributions. Following the procedure in Section 3.1, we compute the KL divergence between these distributions and plot the results (scatter plot shown in Figure 4, KL divergence = 8.4 10(4) ). Figure 4 Probabilities obtained by performing forward propagation twice using the Megatron The results show that, for MoE models in the Megatron engine, even when the input sequence is identical, the final output probabilities from two forward passes may differ. In reinforcement 5 learning setting, this variation adds noise to the computation of the old policy 洧랢train(洧랚old). Such noise makes the importance sampling ratio unreliable, which can destabilize and even break the reinforcement learning process."
        },
        {
            "title": "4 Rollout Routing Replay",
            "content": "This section provides detailed description of the implementation of Rollout Routing Replay (R3), its caching support in multi-turn dialogue, and an analysis of its effects on training-inference discrepancies. 4."
        },
        {
            "title": "Implementation",
            "content": "We first describe the conventional forward pass of MoE layer within training framework. Consider sequence 洧 at the 洧노-th token and the MoE layer in the 洧녳-th Transformer block. Let the input to this layer during training be xtrain. The router logits are calculated by: strain = xtrainW洧, (4) where W洧 denotes the routers linear weight matrix. Let the number of experts be 洧 and the number of experts to select be 洧. During training, the router selects the top-洧 experts based on the logits. This is represented by binary mask: Itrain = TopKMask(strain, 洧), (5) where Itrain {0, 1}洧 and (cid:205)洧녰 洧냪train,洧녰 = 洧. The gating weights are then produced by applying softmax over the logits of the selected experts: 洧녮train,洧녰 = 洧냪train,洧녰 exp(洧맚rain,洧녰) (cid:205)洧 洧녱=1 洧냪train, 洧녱 exp(洧맚rain, 洧녱) for 洧녰 = 1, . . . , 洧. Finally, the MoE layers output is computed as weighted sum of the expert outputs: ytrain = 洧 洧녰=1 洧녮train,洧녰 E洧녰 (xtrain), (6) (7) where E洧녰 () represents the 洧녰-th expert network. Now we introduce the Rollout Routing Replay. Assume that during the inference stage, the input to the MoE layer is xinfer. The router then computes the inference logits sinfer = xinferW洧, from which the routing mask Iinfer = TopKMask(sinfer, 洧) is obtained. The main idea of Rollout Routing Replay is to reuse the inference routing mask Iinfer during the training forward pass while still applying the softmax to the training logits to preserve gradient flow. Specifically, along this replay path, the \"replay\" gating weights 洧녮replay are computed as: 洧녮replay,洧녰 = 洧냪infer,洧녰 exp(洧맚rain,洧녰) (cid:205)洧 洧녱=1 洧냪infer, 洧녱 exp(洧맚rain, 洧녱) for 洧녰 = 1, . . . , 洧. (8) These replay weights are then used to combine the training experts outputs, producing the replay output yreplay: yreplay = 洧녮replay,洧녰 E洧녰 (xtrain). (9) 洧 洧녰=1 This design serves two main purposes: (a) Aligning training and inference: Using Iinfer ensures that the experts used during training replay match those selected during inference, eliminating mismatch in expert selection. (b) Preserving the gradient data flow: By replaying only the mask, the gradients can still flow back to the logits without interfering with the computation graph, which helps to optimize the router effectively. Through meticulous system design and implementation, we enable the storage and retrieval of MoE router mask data from the inference engine, with an overall latency overhead below 3% during the rollout phase."
        },
        {
            "title": "4.2 Router Mask Caching and Multi-Turn Dialogue Support",
            "content": "Many inference engines use KVCache prefix caching strategy (Kwon et al., 2023; Zheng et al., 2024) to prevent redundant prefill computations on previously seen context, which significantly reduces the total computation in multiple-turn interactions. We observe that the cached router masks share similar property: for the same prefix tokens, the MoE router should yield identical results. Therefore, routing masks from inference engines Iinfer can be cached along with the prefix KVcache. Concretely, for each layer and token prefix, the corresponding routing masks are stored with KVCache. When the same prefix occurs and hits the cache, the masks can be reused, eliminating the need for recomputation. This allows Rollout Routing Replay to integrate seamlessly with prefix caching mechanisms. Caching routing masks is especially beneficial in agent scenarios. Many agent tasks, like software engineering (Jimenez et al., 2024) and web browsing (Wei et al., 2025), involve multiple turns of interactions between autoregressive generation and tool calling. To improve efficiency, these processes directly reuse the KVCache from previous turns, so they do not have to regenerate data thats already been computed. Routing mask caching enables R3 to remain efficient in RL agent tasks without re-prefilling to generate the routing masks, which is crucial for training large-scale, advanced MoE models."
        },
        {
            "title": "4.3 Empirical Analysis of R3 on Training-Inference Discrepancies",
            "content": "To evaluate the effectiveness of R3 in reducing training-inference discrepancies, we repeat the procedure described in Section 3.1 with the Qwen3-30B-A3B model. In this process, we cache the routing distributions obtained during inference on SGLang and replay them within the Megatron framework. After obtaining the inference engine probability and training engine probability, using the equation 2, we estimate the KL divergence between training and inference. The result shows that after applying R3, the KL divergence between training and inference decrease from 1.5 103 to 7.5 104, which is near the 6.4 104 observed for the dense model. This intuitively indicates reduction in the training-inference discrepancy. We also draw the cumulative distribution plot of the ratio of training-inference discrepancies in Fig. 2d with R3. The plot indicates that, for the MoE model, applying R3 reduces by an order of magnitude the frequency of tokens with large training-inference discrepancies."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate the performance improvement of our R3 method for reinforcement learning and compare it with other baseline methods. 7 Method AIME24() AIME25() AMC23() MATH500 Lv5() Avg() Crash Step Best Metric(Best Global Step) Qwen3-30B-A3B-SFT, mini_step=8, max_global_step=180 GRPO GSPO GRPO+R3 GSPO+R3 74.84(60) 20.73(90) 32.81(65) 38.23(160) 55.52(165) 90.16(125) 57.92(180) 90.16(155) 38.02(155) 58.44(160) 39.17(160) 92.50(165) 71.83(100) 86.38(125) 88.62(170) 87.87(165) 48.84(100) 66.76(125) 68.05(180) 69.00(165) Qwen3-30B-A3B-SFT, mini_step=1, max_global_step=180 GRPO GRPO+TIS GRPO+R3 GRPO+TIS+R3 86.41(55) 32.08(50) 49.06(45) 54.90(85) 88.59(90) 36.67(90) 62.92(165) 41.98(170) 93.91(165) 58.75(180) 91.87(175) 41.15(180) 83.77(55) 85.63(85) 89.93(155) 88.99(180) 62.23(55) 66.24(90) 71.83(165) 70.14(175) Qwen3-30B-A3B-Base, mini_step=1, max_global_step=180 GRPO GRPO+TIS GRPO+R3 GRPO+TIS+R3 32.60(100) 50.63(100) 56.77(170) 40.10(170) 60.94(180) 41.35(180) 56.67(170) 83.13(100) 92.50(165) 92.81(175) 40.42(170) 93.12(175) 80.78(90) 89.37(180) 89.74(175) 88.99(165) 61.69(100) 69.22(170) 70.73(180) 69.17(175) 120 - - - 60 105 - - 105 - - - Table 1 Main evaluation results. This table presents the best scores (with corresponding global step in parentheses) achieved by various methods on evaluation benchmarks under three training configurations . The table also reports the average score (Avg) and the step at which training collapse occurred (Crash Step)."
        },
        {
            "title": "5.1 Setting",
            "content": "Task and Dataset We choose mathematical reasoning as the target task for training. For the training dataset, we collect and filter problems from many open-source datasets, including BigMath-RL-Verified1 (Albalak et al., 2025), ORZ-Math2 (Hu et al., 2025), and others, yielding approximately 100,000 verifiable math problems. For the evaluation dataset, we adopt AIME24, AIME25, AMC23, and MATH500 (level 5)(Hendrycks et al., 2021) as our benchmark datasets. We report Avg@32 for AIME24 and AIME25, Avg@16 for AMC23, and Avg@4 for MATH500 (level 5). During training, some models may experience performance degradation or even collapse in later stages. To ensure fair evaluation, we evaluate the model performance every 5 global steps and report the highest observed performance along with the corresponding training step at which it occurs. Models We select two models for our experiments: (a) Qwen3-30B-A3B-Base (Yang et al., 2025) (b) Qwen3-30B-A3B-SFT, which is fine-tuned from Qwen3-30B-A3B-Base on our general instruction-following dataset. Methods For our R3 method, we cache the routing during rollout and replay it both when recomputing the old policy and updating policy. We consider the following baseline optimization methods for comparison: (a) GRPO (Shao et al., 2024), additionally applies the Clip Higher technique from DAPO (Yu et al., 2025), with parameters 洧랬low = 0.2 and 洧랬high = 0.27; (b) TIS (Yao et al., 2025), uses an upper clipping threshold 洧냤 = 2; (c) GSPO (Zheng et al., 2025), employs sequence-level importance sampling, with parameters 洧랬low = 3 104 and 洧랬high = 4 104. Since our R3 method is orthogonal to optimization techniques such as GSPO or TIS, we also evaluate various combinations of these techniques. 1https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified 2https://huggingface.co/Open-Reasoner-Zero/datasets 8 Figure 5 Analysis of traininginference collapse. The plot shows the estimated traininginference KL divergence and the extreme token distribution function 洧냧(洧랦 = 2) (Eq. 3) at each training step. Figure 6 Training dynamics of Qwen3-30B-A3B-Base, including response length, gradient norm, entropy, and average validation score throughout the training process Multiple Mini Steps vs. Single Mini Step In PPO-like algorithms, batch of samples collected in one global step is typically divided into multiple mini steps so that the policy can be updated several times. However, recent research (Hao et al., 2025; Ji et al., 2025; Yue et al., 2025) has shown that applying only one single mini step can yield better performance. We investigate both settings. For multiple mini steps, we set the mini step to 8 and the learning rate to 1 106. For the single mini step scenario, we set the mini step to 1. Due to fewer updates, we increase the learning rate to 3 106. Other Setting We implement R3 using the VeRL (Sheng et al., 2024) framework, use Megatron (Shoeybi et al., 2019) for training, and SGLang (Zheng et al., 2024) for inference. We set the batch size to 256 and 洧녵 = 8, totaling 2048 samples per global step. The maximum prompt length is 2048, and the maximum generation length is 30720. We adopt the Dynamic Sampling strategy from Yu et al. (2025), retaining only partially correct samples during generation until sufficient batch size is accumulated for training. No auxiliary loss for expert balancing is introduced."
        },
        {
            "title": "5.2 Experimental Results and Analysis",
            "content": "The main evaluation results are shown in Table 1. More detailed evaluation results and training logs, including per-evaluation scores as well as training metrics such as entropy, average reward, gradient norm, and training-inference KL divergence, are provided in the appendix Overall Performance. R3 achieves better results across different scenarios. In the multi mini-step setting, GRPO+R3 outperforms GSPO by 1.29. Furthermore, combining R3 with GSPO further improves performance by 0.95 points. In the single mini-step setting, R3 outperforms TIS by 5.58 on SFT model and 1.51 on base model. However, combining R3 with TIS does not yield clear gains and may even degrade performance; for instance, in the single mini-step on SFT model, TIS+R3 scores 1.69 points lower than R3 alone. Since R3 already significantly reduces the policy 9 Qwen3-30B-A3B, mini_step=1, max_global_step=180 Method SWE-bench Verified() Best Global Step Crash Step GRPO GRPO+R3 31.80 38.60 70 160 90 - Table 2 Evaluation results of RL training on SWE Task with Qwen3-30B-A3B discrepancy between training and inference, the additional correction from TIS offers negligible benefit (Yao et al., 2025). Training Stability. In the single mini-step setting, three reinforcement learning processes without R3 collapsed during training. To investigate the causes of these collapses, we plotted the estimated traininginference KL divergence and the extreme token distribution function 洧냧(洧랦 = 2) (eq. 3) at each training step(see Fig. 5). We observed that, in each training process, both the KL divergence and 洧냧(洧랦 = 2) values increased over training process. Moreover, collapsed training runs were almost always accompanied by abnormally high KL and 洧냧(洧랦 = 2) values. For instance, in the SFT model trained using GRPO under the single mini-step setting, after 60 global steps, the value of 洧냧(洧랦 = 2) exceeded 0.1. This indicates that for 10% of the tokens, the probability under the training framework differed from that under the inference framework by more than factor of 2, demonstrating severe traininginference inconsistency. In contrast, for the majority of the time during training processes using R3, the value of 洧냧(洧랦 = 2) remained below 104. By aligning the training and inference distributions, R3 effectively stabilized reinforcement learning for MoE models. Optimization and Generation Behavior. During training, R3 also enhances optimization stability, exploration behavior, and generation dynamics. We plotted the sequence length, gradient norm, generation entropy, and evaluation score throughout training for the single mini-step + base model group (see Fig. 6). It shows that R3 has smaller gradient norms, smoother sequence growth pattern, and more stable entropy. (a) Sequence Length: With R3, the generated sequence length rises rapidly at the beginning of training, indicating that R3 can quickly capture the correct optimization direction. By contrast, the other two training processes increase only slowly after step 80 and show more pronounced fluctuations. (b) Gradient Norm: R3 maintains consistently lower gradient norms, indicating more stable optimization process. (c) Generation Entropy: With R3, entropy begins to increase steadily after about 25 step, suggesting that the model starts exploring better strategies earlier. Without R3, entropy increases much later and fluctuates heavily."
        },
        {
            "title": "5.3 Multi-Turn Task Reinforcement Learning",
            "content": "In Section 4.2, we mentioned that the R3 method can be adapted to Multi-Turn tasks. To verify this capability, we conduct reinforcement learning experiments on the software engineering (Jimenez et al., 2024) task using the Qwen3-30B-A3B (Yang et al., 2025) model. We use R2E-GymLite3 (Jain et al., 2025) as the training dataset and SWE-bench Verified4 (Jimenez et al., 2024) as the validation dataset. For training, We set the maximum sequence length to 65536 tokens (including the prompt, model response, and environment observation), the batch size to 64, the maximum number of interaction steps to 50, the learning rate to 2 106, and the mini_step to 1. For evaluation, We report Pass@1 on SWE-bench Verified. We evaluate the model performance every 10 global steps. We compare the performance of GRPO and GRPO+R3 under these settings. 3https://huggingface.co/datasets/R2E-Gym/R2E-Gym-Lite 4https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified 10 All other hyperparameters follow the configuration described in Section 5.1. The results are presented in Table 2, and the detailed training metrics are provided in Appendix C. The training process of GRPO without R3 collapses after around 90 training steps, while GRPO+R3 maintains stable. The final performance of GRPO+R3 reaches Pass@1 of 38.6, which is 6.8 points higher than that of GRPO. Since we use the Router Mask Caching technique described in Section 4.2, we do not need to re-prefill the prefix to obtain the routing mask, thus the rollout speed is not slowed down. These results confirm that R3 generalizes well to multi-turn reinforcement learning settings."
        },
        {
            "title": "6.1 Mixture of Experts",
            "content": "The Mixture-of-Experts (MoE) architecture, originating as an ensemble method to route inputs to specialized subnetworks (Jacobs et al., 1991; Jordan and Jacobs, 1994), has become cornerstone for scaling modern language models. By employing gating network to sparsely activate only subset of expert parameters per token, MoE decouples models total parameter count from its inference cost, enabling substantial increase in model capacity. This computational efficiency has driven its adoption in state-of-the-art Transformer models (Jiang et al., 2024; Liu et al., 2024; Team et al., 2025; Yang et al., 2025). However, MoE models are susceptible to training instability stemming from the sensitivity of the gating network (Dai et al., 2022; Th칠rien et al., 2025), which renders router robustness central challenge for effective model convergence."
        },
        {
            "title": "6.2 Instability in Reinforcement Learning for LLMs",
            "content": "Reinforcement Learning with verifiable rewards (RLVR) has become standard method for refining the complex reasoning (Xie et al., 2025), mathematical (Hu et al., 2025; Shao et al., 2024), and code abilities (Luo et al., 2025b) of large language models. Algorithms such as GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025) are now widely adopted. However, this training paradigm also faces challenges with training instability, which can hinder convergence and risk model collapse (Chen et al., 2025; Xia et al., 2025; Yang et al., 2025). Recent research has identified several root causes and proposed corresponding solutions. For instance, Zhao et al. (2025b) aim to stabilize training by optimizing the geometric mean of token-level rewards. Separately, GSPO (Zheng et al., 2025) posits that GRPOs instability stems from misapplication of importance sampling weights and introduce new sequence-level importance ratio to correct it. Yao et al. (2025) propose Truncated Importance Sampling (TIS) to mitigate the inconsistency stemming from discrepancies between training frameworks (e.g., FSDP (Zhao et al., 2023) and inference engines (e.g., vLLM (Kwon et al., 2023)). Alternatively, (T. M. Lab, 2025) attributes instability to non-determinism in compute kernels, proposing batch-invariant operations, though this incurs significant performance overhead and has not been explored for RL on MoE models. These stability challenges are critically exacerbated in Mixture-of-Experts (MoE) architectures. We find this heightened instability is primarily caused by routing inconsistencies: the routers sensitivity, combined with the framework gap, causes expert assignments for the same response to differ between the rollout and training phases. To directly address this core issue, our proposed method, Rollout Routing Replay, explicitly records and replays the inference-time routing decisions during the training pass. This simple strategy enforces consistency, enabling MoE models to achieve training stability comparable to their dense counterparts without incurring computational overhead."
        },
        {
            "title": "6.3 Routing Replay",
            "content": "Routing Replay was first introduced in Zheng et al. (2025) (hereafter referred to as Recompute Routing Replay). We clarify how our Rollout Routing Replay differs from it. As shown in Figure 1, there are three forward-pass stages in typical reinforcement learning framework: Rollout, Recompute and Update. Recompute Routing Replay caches the routing from the Recompute stage and replays it in the Update stage, addressing the issue of routing discrepancies caused by model updates. However, this approach does not take into account that inconsistencies between training and inference frameworks can also lead to routing discrepancies. In contrast, Our Rollout Routing Replay caches the routing from the Rollout stage and replays it in both the Recompute and Update stages, thereby resolving routing discrepancies arising from framework difference. Notably, since our method uses the same routing for both the Recompute and Update stages, it also addresses routing discrepancies caused by model updates. Furthermore, when the mini step is set to 1, the current policy and the old policy become identical, which makes the Recompute Routing Replay method ineffective. Since discrepancies between training and inference frameworks continue to exist, the Rollout Routing Replay method remains effective under this setting."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we identify training-inference routing discrepancies as the primary source of instability in MoE reinforcement learning. To address this, we propose Rollout Routing Replay (R3), which reuses inference-time routing distributions during training to align expert selection while preserving gradient flow. Experiments across multiple RL settings demonstrate that R3 substantially reduces training-inference divergence, stabilizes training, and consistently outperforms existing methods. Our results demonstrate the importance of aligning training and inference in MoE models and show that R3 provides practical solution for improving stability."
        },
        {
            "title": "References",
            "content": "A. Albalak, D. Phung, N. Lile, R. Rafailov, K. Gandhi, L. Castricato, A. Singh, C. Blagden, V. Xiang, D. Mahan, and N. Haber. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models, 2025. URL https://arxiv.org/abs/2502.17387. A. Bercovich, I. Levy, I. Golan, M. Dabbah, R. El-Yaniv, O. Puny, I. Galil, Z. Moshe, T. Ronen, N. Nabwani, I. Shahaf, O. Tropp, E. Karpas, R. Zilberstein, J. Zeng, S. Singhal, A. Bukharin, Y. Zhang, T. Konuk, G. Shen, et al. Llama-nemotron: Efficient reasoning models, 2025. URL https://arxiv.org/abs/2505.00949. A. Chen, A. Li, B. Gong, B. Jiang, B. Fei, B. Yang, B. Shan, C. Yu, C. Wang, C. Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. D. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang, and F. Wei. Stablemoe: Stable routing strategy for mixture of experts. arXiv preprint arXiv:2204.08396, 2022. H. Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Y. Hao, L. Dong, X. Wu, S. Huang, Z. Chi, and F. Wei. On-policy rl with optimal reward baseline, 2025. URL https://arxiv.org/abs/2505.23585. H. He and T. M. Lab. Defeating nondeterminism in llm inference. Machines Lab: https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/. Connectionism, 2025. doi: Thinking 1 0 . 6 4 4 3 4 / . 2 0 2 5 0 9 1 0. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. J. Hu, Y. Zhang, Q. Han, D. Jiang, X. Zhang, and H.-Y. Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. URL https: //arxiv.org/abs/2503.24290. R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. N. Jain, J. Singh, M. Shetty, L. Zheng, K. Sen, and I. Stoica. R2e-gym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. arXiv preprint arXiv:2504.07164, 2025. Y. Ji, X. Tian, S. Zhao, H. Wang, S. Chen, Y. Peng, H. Zhao, and X. Li. Am-thinking-v1: Advancing the frontier of reasoning at 32b scale, 2025. URL https://arxiv.org/abs/2505.08311. A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 13 C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs/ 2310.06770. M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214, 1994. W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. A. Lozhkov, H. Kydl칤캜ek, L. B. Allal, G. Penedo, E. Beeching, Q. Gallou칠dec, N. Habib, L. Tunstall, and L. von Werra. Openr1-math-220k. https://huggingface.co/datasets/open-r1/ OpenR1-Math-220k, 2025. M. Luo, N. Jain, J. Singh, S. Tan, A. Patel, Q. Wu, A. Ariyak, C. Cai, T. Venkat, S. Zhu, B. Athiwaratkun, M. Roongta, C. Zhang, L. E. Li, R. A. Popa, K. Sen, and I. Stoica. Deepswe: Training state-of-the-art coding agent from scratch by scaling rl. https://pretty-radio-b75.not ion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Codin g-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33, 2025a. Notion Blog. M. Luo, S. Tan, R. Huang, A. Patel, A. Ariyak, Q. Wu, X. Shi, R. Xin, C. Cai, M. Weber, C. Zhang, L. E. Li, R. A. Popa, and I. Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-1 4B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025b. Notion Blog. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/learnin g-to-reason-with-llms/. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. G. Penedo, A. Lozhkov, H. Kydl칤캜ek, L. B. Allal, E. Beeching, A. P. Lajar칤n, Q. Gallou칠dec, N. Habib, L. Tunstall, and L. von Werra. Codeforces cots. https://huggingface.co/datasets/op en-r1/codeforces-cots, 2025. J. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-appro x.html. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 14 M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. B. Th칠rien, C.-칄. Joseph, Z. Sarwar, A. Panda, A. Das, S.-X. Zhang, S. Rawls, S. Sahu, E. Belilovsky, arXiv preprint and I. Rish. Continual pre-training of moes: How robust is your router? arXiv:2503.05029, 2025. J. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. T. Passos, W. Fedus, and A. Glaese. Browsecomp: simple yet challenging benchmark for browsing agents, 2025. URL https://arxiv.org/abs/2504.12516. B. Xia, B. Shen, Cici, D. Zhu, D. Zhang, G. Wang, H. Zhang, H. Liu, J. Xiao, J. Dong, L. Zhao, P. Li, P. Wang, S. Yu, S. Chen, W. Wang, W. Ma, X. Deng, Y. Huang, Y. Song, Z. Jiang, B. Ye, C. Cai, C. He, D. Zhang, D. Zhang, G. Wang, H. Tian, H. Zhao, H. Qu, H. Xu, J. Shi, K. Bao, K. Fang, K. Zhou, K. Zhou, L. Li, M. Zhu, N. Chen, Q. Wang, S. Liu, S. Li, S. Gu, S. Ren, S. Liu, S. Deng, W. Zhuang, W. Lv, W. Yang, X. Zhang, X. Yong, X. Zhang, X. Song, X. Xu, X. Wang, Y. Yan, Y. Tu, Y. Tian, Y. Wang, Y. Yu, Z. Lin, Z. Song, and Z. Yue. Mimo: Unlocking the reasoning potential of language model from pretraining to posttraining, 2025. URL https://arxiv.org/abs/2505.07608. T. Xie, Z. Gao, Q. Ren, H. Luo, Y. Hong, B. Dai, J. Zhou, K. Qiu, Z. Wu, and C. Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. F. Yao, L. Liu, D. Zhang, C. Dong, J. Shang, and J. Gao. Your efficient rl framework secretly brings you off-policy rl training, Aug. 2025. URL https://fengyao.notion.site/off-polic y-rl. Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, et al. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv. org/abs/2503.14476, 2025. Z. Yue, Z. Lin, Y. Song, W. Wang, S. Ren, S. Gu, S. Li, P. Li, L. Zhao, L. Li, K. Bao, H. Tian, H. Zhang, G. Wang, D. Zhu, Cici, C. He, B. Ye, B. Shen, Z. Zhang, Z. Jiang, Z. Zheng, Z. Song, Z. Luo, Y. Yu, Y. Wang, Y. Tian, Y. Tu, Y. Yan, Y. Huang, X. Wang, X. Xu, X. Song, X. Zhang, X. Yong, X. Zhang, X. Deng, W. Yang, W. Ma, W. Lv, W. Zhuang, W. Liu, S. Deng, S. Liu, S. Chen, S. Yu, S. Liu, S. Wang, R. Ma, Q. Wang, P. Wang, N. Chen, M. Zhu, K. Zhou, K. Zhou, K. Fang, J. Shi, J. Dong, J. Xiao, J. Xu, H. Liu, H. Xu, H. Qu, H. Zhao, H. Lv, G. Wang, D. Zhang, D. Zhang, D. Zhang, C. Ma, C. Liu, C. Cai, and B. Xia. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/2506.03569. 15 X. Zhao, Y. Liu, K. Xu, J. Guo, Z. Wang, Y. Sun, X. Kong, Q. Cao, L. Jiang, Z. Wen, Z. Zhang, and J. Zhou. Small leak can sink great shipboost rl training on moe with icepop!, Sep 2025a. URL https://ringtech.notion.site/icepop. Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, P. Damania, B. Nguyen, G. Chauhan, Y. Hao, A. Mathews, and S. Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https: //arxiv.org/abs/2304.11277. Y. Zhao, Y. Liu, J. Liu, J. Chen, X. Wu, Y. Hao, T. Lv, S. Huang, L. Cui, Q. Ye, et al. Geometric-mean policy optimization. arXiv preprint arXiv:2507.20673, 2025b. C. Zheng, S. Liu, M. Li, X.-H. Chen, B. Yu, C. Gao, K. Dang, Y. Liu, R. Men, A. Yang, J. Zhou, and J. Lin. Group sequence policy optimization, 2025. URL https://arxiv.org/abs/2507.1 8071. L. Zheng, L. Yin, Z. Xie, C. Sun, J. Huang, C. H. Yu, S. Cao, C. Kozyrakis, I. Stoica, J. E. Gonzalez, C. Barrett, and Y. Sheng. Sglang: Efficient execution of structured language model programs, 2024. URL https://arxiv.org/abs/2312.07104."
        },
        {
            "title": "A Detailed Evaluation Results and Training Metrics",
            "content": "Figure 7 The detailed evaluation results of the experiment in Section 5. Figure 8 The detailed training metrics of the experiment in Section 5."
        },
        {
            "title": "B Additional Experiments of Reasoning SFT Model",
            "content": "To evaluate the performance of R3 on reasoning SFT models, we fine-tune Qwen3-30B-A3BBase (Yang et al., 2025) on the open-source long reasoning dataset Mixture-of-Thoughts 5(Bercovich et al., 2025; Face, 2025; Lozhkov et al., 2025; Penedo et al., 2025), producing the Qwen3-30B-A3BReasoningSFT model. We conduct reinforcement learning experiments on this model. We compare GSPO and GRPO+R3 with learning rate of 2 106 when mini_step = 4, and compare GRPO and GRPO+R3 with learning rate of 3 106 when mini_step = 1; all other hyperparameters follow the configuration described in Section 5.1. The results are shown in Table 3, and training and validation metrics are illustrated in Fig 9. The experiments show that training processes without R3 collapse (for mini_step = 4, GSPO collapses at training step 90, and for mini_step = 1, GRPO collapses at training step 40), while training processes with R3 remain robust. In addition, training with R3 exhibits more stable pattern of sequence-length growth as well as more stable entropy and gradient norms throughout the training process. Method AIME24() AIME25() AMC23() MATH500 Lv5() Avg() Crash Step Qwen3-30B-A3B-ReasoningSFT, mini_step=4, max_global_step=180 Best Metric(Best Global Step) GSPO 94.21(60) GRPO+R3 69.06(165) 51.66(155) 95.15(160) 48.33(20) 67.50(35) 91.79(35) 93.28(145) 74.62(55) 77.00(165) Qwen3-30B-A3B-ReasoningSFT, mini_step=1, max_global_step=180 GRPO GRPO+R3 68.54(25) 69.47(85) 93.90(30) 48.95(25) 53.12(180) 95.46(170) 91.23(20) 93.28(180) 75.30(25) 76.79(180) 90 - 40 - Table 3 Evaluation results of the RL training process of Qwen3-30B-A3B-ReasoningSFT Figure 9 The detailed training metrics and evaluation results of the RL training process of Qwen3-30B-A3B-ReasoningSFT 5https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts 18 Detailed Training Metrics of Multi-Turn Reinforcement Learning Figure 10 The detailed training metrics and evaluation results of RL training process on SWE Task with Qwen3-30B-A3B"
        }
    ],
    "affiliations": [
        "LLM-Core Xiaomi",
        "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
    ]
}