{
    "paper_title": "TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models",
    "authors": [
        "Riza Velioglu",
        "Petra Bevandic",
        "Robin Chan",
        "Barbara Hammer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces Virtual Try-Off (VTOFF), a novel task focused on generating standardized garment images from single photos of clothed individuals. Unlike traditional Virtual Try-On (VTON), which digitally dresses models, VTOFF aims to extract a canonical garment image, posing unique challenges in capturing garment shape, texture, and intricate patterns. This well-defined target makes VTOFF particularly effective for evaluating reconstruction fidelity in generative models. We present TryOffDiff, a model that adapts Stable Diffusion with SigLIP-based visual conditioning to ensure high fidelity and detail retention. Experiments on a modified VITON-HD dataset show that our approach outperforms baseline methods based on pose transfer and virtual try-on with fewer pre- and post-processing steps. Our analysis reveals that traditional image generation metrics inadequately assess reconstruction quality, prompting us to rely on DISTS for more accurate evaluation. Our results highlight the potential of VTOFF to enhance product imagery in e-commerce applications, advance generative model evaluation, and inspire future work on high-fidelity reconstruction. Demo, code, and models are available at: https://rizavelioglu.github.io/tryoffdiff/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 1 0 5 3 8 1 . 1 1 4 2 : r TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models Riza Velioglu(cid:0), Petra Bevandic, Robin Chan, Barbara Hammer Machine Learning Group, CITEC, Bielefeld University, Germany {rvelioglu, pbevandic, rchan, bhammer}@techfak.de Figure 1. Virtual try-off results generated by our method. The first row shows the input reference image, the second row our models prediction, and the third row the ground truth. Our approach naturally renders the garment against clean background, preserving the standard pose and capturing complex details of the target garment, such as patterns and logos, from single reference image."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction This paper introduces Virtual Try-Off (VTOFF), novel task focused on generating standardized garment images from single photos of clothed individuals. Unlike traditional Virtual Try-On (VTON), which digitally dresses models, VTOFF aims to extract canonical garment image, posing unique challenges in capturing garment shape, texture, and intricate patterns. This well-defined target makes VTOFF particularly effective for evaluating reconstruction fidelity in generative models. We present TryOffDiff, model that adapts Stable Diffusion with SigLIP-based visual conditioning to ensure high fidelity and detail retention. Experiments on modified VITON-HD dataset show that our approach outperforms baseline methods based on pose transfer and virtual try-on with fewer preand post-processing steps. Our analysis reveals that traditional image generation metrics inadequately assess reconstruction quality, prompting us to rely on DISTS for more accurate evaluation. Our results highlight the potential of VTOFF to enhance product imagery in e-commerce applications, advance generative model evaluation, and inspire future work on high-fidelity reconstruction. Demo, code, and models are available at: https://rizavelioglu.github.io/tryoffdiff/ Image-based virtual try-on (VTON) [23] is key computer vision task aimed at generating images of person wearing specified garment. Typically, two input images are required: one showing the garment in standardized form (often from an e-commerce catalog) and another of the person that needs to be dressed. Recent methods focus on modified formulation where the catalog image is replaced with photo of another person wearing the target garment. This introduces additional processing complexity [55] as the model does not have access to full garment information. From an application perspective, VTON offers an interactive shopping experience that helps users make betterinformed purchasing decisions. On the research side, it raises intriguing research questions, particularly around human pose detection as well as clothing shape, pattern, and texture analysis [17]. Best-performing models are usually guided generative models focused on creating specific, physically accurate outputs. Unlike general generative tasks that produce diverse outputs, reconstruction requires models to generate images that align with the correct appearance of the garment on person. However, one drawback of VTON is the lack of clearly defined target output, often resulting in stylistic variations that complicate evaluation. Generated images may show garments tucked, untucked, or altered in fit, introducing plausible yet inconsistent visual variations and making it difficult to assess the true quality of garment representation [47]. This is why current evaluation methods generally rely on broad assessment of generative quality [20], without considering the similarity between individual garmentperson ground truth pairs. Common image quality metrics often exhibit sensitivity to differences in non-salient regions, such as the background, which complicates pinpointing the precise sources of performance variability [11, 45]. We therefore introduce Virtual Try-OFF (VTOFF), novel task focused on generating standardized product images from real-world photos of clothed individuals as illustrated in Figure 1 and Figure 2. Even though the goal is reversed when compared to VTON, the two tasks address similar challenges such as pose analysis, geometric and appearance transformations, potential occlusions and preservation of fine-grained details such as textures, patterns, and logos. Additionally, the acquisition diversity of real-world photos varying in background, lighting, and camera quality introduces unique challenges in domain adaptation and robust feature extraction. Still, this switch in the target presents crucial advantage of VTOFF over VTON: the reduced stylistic variability on the output side simplifies the assessment of reconstruction quality. The potential impact of VTOFF extends well beyond research. It could enhance the flexibility of various ecommerce applications that rely on consistent product images. For instance, generated images can be integrated seamlessly into existing virtual try-on solutions, enabling the more complex person-to-person try-on by substituting the ground truth with the generated garment image. Recommendation and other customer-to-product retrieval systems [14] could also benefit from access to standardized garment representation. Moreover, it could support the creation of large-scale, high-quality fashion datasets, thereby accelerating the development of fashion-oriented AI. From an environmental standpoint, these applications should help customers with purchasing decisions, thus reducing product returns and the environmental footprint of the fashion industry. Finally, generating standardized garment images from everyday photos is an interesting task in itself, as it could simplify the maintenance of e-commerce catalogs by reducing the need for expensive photography equipment and time-consuming editing, benefiting smaller vendors who lack the resources for professional-quality product photography. Our work highlights that reconstructing e-commerce images is challenging task that requires significant modifications to existing VTON models. Moreover, we show that traditional image generation metrics fall short in capturing reconstruction quality. Our primary contributions are: Figure 2. Illustration of the differences between Virtual TryOn and Virtual Try-Off. Top: Basic inference pipeline of Virtual Try-On model, which takes an image of clothed person as reference and an image of garment to generate an image of the same person but wearing the specified garment. Bottom: Virtual Try-Off setup, where the objective is to predict the canonical form of the garment from single input reference image. We introduce VTOFF, novel task to generate standardized product images from real-world photos of clothed individuals, unlocking promising real-world applications while raising important new research questions. We present TryOffDiff, novel framework that adapts pretrained diffusion models for VTOFF by aligning image features with text-based diffusion priors, ensuring high visual fidelity and consistent product details. Extensive experiments on the VITON-HD dataset demonstrate that TryOffDiff generates high-quality, detail-rich product images of garments, outperforming state-of-theart view synthesis and virtual try-on methods. 2. Related Work Virtual Try-Off seeks to reconstruct canonical image of clothing, typically resembling garments worn by person in neutral pose. While virtual try-on and pose-transfer methods could be adapted to produce these standardized outputs, our experiments indicate that such adaptations underperform. Instead, we base our solution on conditional diffusion models, which have demonstrated robust performance across diverse generative tasks. Image-based Virtual Try-On. The objective of imagebased virtual try-on is to produce composite images that realistically depict specific garment on target person, preserving the persons identity, pose, and body shape, while capturing fine garment details. CAGAN [23] introduced this task with cycle-GAN approach, while VITON [17] formalized it as two-step, supervised framework: warping the garment through non-parametric geometric transfor2 mations [3], followed by blending it onto the person. CPVTON [52] refined this process by implementing learnable thin-plate spline (TPS) transformation using geometric matcher, later improved with dense flow [18] and appearance flow [15] to enhance the pixel-level alignment of garment details. Despite progress in warping-based approaches, limitations remain, especially with complex garment textures, folds, and logos. To address these drawbacks, recent works adopted GANbased and diffusion-based methods. FW-GAN [12] synthesized try-on videos, while PASTA-GAN [55] modified StyleGAN2 for person-to-person try-on. However, GANs suffer from issues like unstable training and mode collapse, leading VTON research to favor diffusion models, which have proven to be more reliable. M&M-VTO [63] introduced single-stage diffusion model capable of synthesizing multi-garment try-on results from an input person image IDM-VTON [8] proposed and multiple garment images. two modules to encode the semantics of garment image, extracting highand low-level features with cross-attention and self-attention layers. OOTDiffusion [57] leveraged pretrained latent diffusion models to learn garment features, which are incorporated into denoising UNet using outfitting fusion. In more lightweight approach, CatVTON [9] eliminated the need for heavy feature extraction, proposing compact model based on pretrained latent diffusion model that achieved promising results with fewer parameters. Modifying existing VTON models for VTOFF is not necessarily straightforward, as VTON models often depend on additional inputs like text prompts, keypoints, or segmentation masks, which must be carefully selected and manually tailored for effective adaptation. It is important to note that, while both VTON and VTOFF tasks involve garment manipulation, they are fundamentally different. VTON models have access to complete garment details, allowing them to primarily focus on warping the item to fit target pose. In contrast, VTOFF models must work with only partial garment information from reference image, where occlusions and deformations are common, requiring them to reconstruct missing details from limited visual cues. Image-based View Synthesis & Pose Transfer. Novel View Synthesis (NVS) aims to generate realistic images from unseen viewpoints. While early methods required hundreds of training images per instance [26, 43, 46, 61, 62], recent approaches enable synthesis from sparse views [22, 48]. However, NVS alone cannot fully address garment reconstruction, as the pose of the observed person cannot be changed. Pose transfer, related task, can be seen as type of view synthesis that also allows for object deformation. It requires additional capabilities for inferring potentially occluded body parts. DiOr [10] proposed generation framework for pose transfer, using recurrent architecture that sequentially dresses person in garments to create different looks from the same input. [36] introduced GAN-based pose transfer model that uses multi-scale attention-guided approach, significantly improving on existing methods and showing potential for VTON applications. DreamPose [24] synthesizes try-on videos from an image and sequence of human body poses using pretrained latent diffusion model. PoCoLD [19] trained latent diffusion model conditioned on dense pose maps for person image synthesis. ViscoNet[7] integrates adapter layers into pretrained latent diffusion model and extends ControlNet to incorporate multiple image conditions, enhancing control over visual inputs. PCDM [39] proposed three-stage pipeline for pose-guided person image synthesis, achieving texture restoration, and enhancing fine-detail consistency. It should be mentioned that pose transfer focuses on preserving the original scene attributes, such as lighting, background, and subject appearance, In contrast, the virtual tryoff task should adhere to strict e-commerce presentation standards including consistent front/back views, uniform sizing, and catalog-specific styling. Conditional Diffusion Models. Latent Diffusion Models [35] (LDMs) achieved great success in recent years, offering control over generative process through the introduction of the cross-attention mechanism [49]. The conditioning works with diverse input modalities such as text [2, 4, 13] and image [32, 37, 38]. In text-guided image synthesis, models like ControlNet [60] and T2I-Adapter [30] extend pretrained models with additional blocks that offer more precise spatial control. IP-Adapter [58] advances this flexibility by decoupling the cross-attention mechanism for text and image features, allowing image-guided generation with optional structural conditions. Prompt-Free Diffusion [56] discards text prompts altogether, generating images solely from reference image and optional structural inputs. Despite the advancements, these models cannot be applied for garment reconstruction out-of-the-box: textguided approaches require impractically detailed prompts for each sample to specify product attributes, while existing image-guided models lack mechanisms to enforce the strict requirements of standardized product photography. While these techniques have advanced image manipulation capabilities, they fall short of addressing the specific challenges associated with generating standardized ecommerce product images. Recently, Wang et al. [53] incorporated VTOFF-like objective in their models, but only as an auxiliary loss term. To the best of our knowledge, we are the first to formally define Virtual Try-Off (VTOFF) as standalone task and to propose tailored approach for it. 3 3. Methodology This section provides the formal definition of the virtual tryoff task. We propose suitable evaluation setup and performance metrics. We further provide details of our TryOffDiff model which relies on StableDiffusion and SigLIP features for image-based conditioning. 3.1. Virtual Try-Off Problem Formulation. Let RHW 3 be an RGB image with height and width N, respectively. In the task of virtual try-off, represents reference image displaying clothed person. Given the reference image, VTOFF aims to generate standardized product image {0, . . . , 255}HW 3, displaying the garment according to commercial catalog standards. Formally, the goal is to train generative model that learns the conditional distribution (GC), where and represent the variables corresponding to garment images and reference images (serving as condition), respectively. Suppose the model approximates this target distribution with Q(GC). Then, given specific reference image as conditioning input, the objective is for sample ˆG Q(GC = I) to resemble true sample of garment image (GC = I) as closely as possible. Performance Measures. To evaluate VTOFF performance effectively, evaluation metrics must capture both reconstruction and perceptual quality. Reconstruction quality quantifies how accurately the models prediction ˆG matches the ground truth G, focusing on pixel-level fidelity. In contrast, perceptual quality assesses how natural and visually appealing the generated image appears to human observers, aligning with common visual standards. To estimate reconstruction, we may use full-reference metrics such as Structural Similarity Index Measure (SSIM) [54]. However, neither SSIM, nor its multiscale (MS-SSIM) and complex-wavelet (CW-SSIM) variants align well with human perception, as noted in prior studies [11, 45]. We observe similar behavior in our experiments as well, and illustrate our findings in Figure 3. Perceptual quality may be captured with no-reference metrics like Frechet Inception Distance (FID) [20] and Kernel Inception Distance (KID) [5]. These metrics usually compare distributions of image feature representations between generated and real images. They are however unsuitable for single image pair comparison since they are sensitive to sample size and potential outliers. Additionally, both FID and KID rely on features from the classical Inception [44] model, which does not necessarily align with human judgment in assessing perceptual quality, especially in the context of modern generative models such as diffusion models [42]. (a) 82.4 / 20. (b) 96.8 / 17.9 (c) 88.3 / 20.3 (d) 86.0 / 70.3 (e) 75.0 / 8.2 (f) 86.4 / 24.7 Figure 3. Examples demonstrating the un/suitability of performance metrics (SSIM / DISTS) to VTON and VTOFF. In the top row, reference image is compared against: (a) an image with masked-out garment; (b) an image with changed colors of the model; (c) and an image after applying color jittering. In the bottom row, garment image is compared against: (d) plain white image; (e) slightly rotated image; (f) and randomly posterized image (reducing the number of bits for each color channel). While the SSIM score achieves consistently high across all examples, in particular including failure cases, the DISTS score more accurately reflects variations aligned with human judgment. metric that addresses these shortcomings is the Deep Image Structure and Texture Similarity (DISTS) [11] metric, designed to measure perceptual similarity between images by capturing both structural and textural information. DISTS leverages the VGG model [40], where lowerlevel features are used to capture structural elements, while higher-level features focus on finer textural details. The final DISTS score is computed through weighted combination of these two components, with weighting parameters optimized based on human ratings, resulting in perceptual similarity score that aligns more closely with human judgment. For these reasons, DISTS represents our main metric for VTOFF. 3.2. TryOffDiff We base our TryOffDiff model on Stable Diffusion [35] (v1.4), latent diffusion model originally designed for textconditioned image generation using CLIPs [34] text encoder. We replace text prompts for direct image-guided image generation. Image Conditioning. core challenge in image-guided generation is effectively incorporating visual features into the conditioning mechanism of the generative model. CLIPs ViT [34] has become popular choice for image feature extraction due to its general-purpose capabilities. Recently, SigLIP [59] introduced modifications that improve performance, particularly for tasks requiring more detailed and domain-specific visual representations. Therefore, we 4 Figure 4. Overview of TryOffDiff. The SigLIP image encoder [59] extracts features from the reference image, which are subsequently processed by adapter modules. These extracted image features are embedded into pre-trained text-to-image Stable Diffusion-v1.4 [35] by replacing the original text features in the cross-attention layers. By conditioning on image features in place of text features, TryOffDiff directly targets the VTOFF task. Simultaneous training of the adapter layers and the diffusion model enables effective garment transformation. use the SigLIP model as image feature extractor and retain the entire sequence of token representations in its final layer to preserve spatial information, which we find essential for the capture of fine-grained visual details and accurate garment reconstruction. Given input image I, our proposed adapter module processes these representations as follows: C(I) = (LN Linear ψ SigLIP)(I) Rnm (1) where ψ is standard transformer encoder [49] processing SigLIP embeddings, followed by linear projection layer and layer normalization (LN) [1], cf . Figure 4. The adapted image features are integrated into the denoising U-Net of Stable Diffusion via cross-attention. Specifically, the key and value of the attention mechanism at each layer are derived from the image features through linear transformations: = C(I) Wk Rndk , = C(I) WV Rndv (2) where Wk Rmdk and Wv Rmdv . This formulation enables the cross-attention mechanism to condition the denoising process on the features of the external reference image I, enhancing alignment in the generated output. We only train the adapter modules and fine-tune the denoising U-Net of the Stable Diffusion model, while keeping the SigLIP image encoder, VAE encoder and VAE decoder frozen. This training strategy preserves the robust image processing capabilities of the pretrained components while adjusting the generative components to the specific requirements of garment reconstruction. 4. Experiments We establish several baseline approaches for the virtual tryoff task, adapting virtual try-on and pose transfer models as discussed in Section 2, and compare them against our proposed TryOffDiff method described in Section 3. To ensure reproducibility, we detail our experimental setup. We use DISTS as the primary evaluation metric, while also reporting other standard generative metrics for comparison. Additionally, we provide extensive qualitative results to illustrate how our model manages various challenging inputs. 4.1. Experimental Setup Dataset. Our experiments are conducted on the publicly available VITON-HD [27] dataset, which consists of 13, 679 high-resolution (1024 768) image pairs of frontal half-body models and corresponding upper-body garments. While the VITON-HD dataset was originally curated for the VTON task, it is also well-suited to our purposes as it provides the required (I, G) image pairs, where represents the reference image of clothed person and the corresponding garment image. Upon closer inspection of VITON-HD, we identified 95 duplicate image pairs (0.8%) in the training set and 6 duplicate pairs (0.3%) in the test set. Additionally, we found 36 pairs (1.8%) in the training set that had been included in the original test split. To ensure the integrity of our experiments, we cleaned the dataset by removing all duplicates in both subsets as well as all leaked examples from the test set. The resulting cleaned dataset, contains 11,552 unique 5 (a) Left to right: reference image, fixed pose heatmap derived from target image, initial model output, SAM prompts, and final processed output. (b) Left to right: masked conditioning image, mask image, pose image, initial model output with SAM prompts, and final processed output. (c) Left to right: masked garment image, model image, masked model image, initial model output with SAM prompts, and final processed output. (d) Left to right: conditioning garment image, blank model image, mask image, initial model output with SAM prompts, final processed output. Figure 5. Adapting existing state-of-the-art methods to VTOFF. (a) GAN-Pose [36] and (b) ViscoNet [7] are approaches based on pose transfer and view synthesis, respectively, (c) OOTDiffusion [57] and (d) CatVTON [9] are based on recent virtual try-on methods. image pairs for training and 1,990 unique image pairs for testing. We provide the script for cleaning the dataset in our code repository. Implementation Details. We train TryOffDiff by building on the pretrained Stable Diffusion v1.4 [35], focusing on fine-tuning the denoising U-Net and training adapter layers from scratch, cf . Section 3.2. As preprocessing step, we pad the input reference image along the width for square aspect ratio, then resize them to resolution of 512 512 to match the expected input format of the pretrained SigLIP and VAE encoder. For training, we preprocess the garment images in the same way. We use SigLIP-B/16-512 as image feature extractor, which outputs 1024 token embeddings of dimension 768. Our adapter, consisting of single transformer encoder layer with 8 attention heads, followed by linear and normalization layers, reduces these to = 77 conditioning embeddings of dimension = 768. Training occurs over 220k iterations on single node with 4 NVIDIA A40 GPUs, requiring approximately 9 days with batch size of 16. We employ the AdamW optimizer [29], with an initial learning rate of 1e-4 that increases linearly from 0 during the first 1,000 warmup steps, then follows cosine decay to 0 with hard restart at 90k steps. As proposed in [28], we use the PNDM scheduler with 1,000 steps. We optimize using the standard Mean Squared Error (MSE) loss, which measures the difference between the added and the predicted noise at each step. This loss function is commonly employed in diffusion models to guide the model in learning to reverse the noising process effectively. During inference, we run TryOffDiff with PNDM scheduler over 50 timesteps with guidance scale of 2.0. On single NVIDIA A6000 GPU, this process takes 12 seconds per image and requires 4.6GB of memory. 4.2. Baseline Approaches To establish the baselines, we adapted state-of-the-art pose transfer and virtual try-on methods, modifying each to approximate garment reconstruction functionality as closely as possible. We illustrate these approaches in Figure 5. GAN-Pose [36] is GAN-based pose transfer method that expects three inputs: reference image, and pose heatmaps of the reference and target subject. Garment images from VITON-HD are used to estimate the heatmap for fixed, neutral pose. This setup enables the transfer of human poses from diverse reference images to standardized pose, aligning the output to the typical view of product images. ViscoNet [7] requires text prompt, pose, mask, and multiple masked conditioning images as inputs. For the text prompt, we use description such as photo of an e-commerce clothing product. We choose garment image from VITON-HD to estimate neutral pose as well as generic target mask. Since ViscoNet is originally trained with masked conditioning images, we apply an off-the-shelf fashion parser [50] to mask the upper-body garment, which is then provided as input. OOTDiffusion [57] takes garment image and reference image to generate VTON output. To adapt this model for VTOFF, we again apply the fashion parser [50] to mask the upper-body garment to create the garment image. We select reference image with mannequin in neutral pose as further input. An intermediate step involves masking the upper-body within the reference image, for which we use hand-crafted masked version of the reference image. 6 Method GAN-Pose [36] ViscoNet [7] OOTDiff. [57] CatVTON [9] 77.4 58.5 65.1 72.8 Ours: TryOffDiff 79.5 DISSIM SSIM SSIM PIPS FID FID KID STS CLIPCWMS- L63.8 50.7 50.6 56.9 70. 32.5 28.9 26.1 32.0 46.2 44.2 54.0 49.5 45.9 73.2 42. 54.0 31.4 30.9 12.1 17.5 9.7 55.8 30.4 25.5 31.2 33.2 32.4 17.8 28.2 32. 25.1 9.4 8.9 23.0 Table 1. Quantitative comparison. Evaluation metrics for various methods on VITON-HD-test dataset in the VTOFF task. CatVTON [9] is model that generates VTON image using reference image and conditioning garment image as inputs. An intermediate step incorporates upperbody masks to guide the try-on process. For adaptation to VTOFF, we replace the reference image with plain white image and use handcrafted mask in neutral pose, enabling CatVTON to perform garment transfer independently of any specific person. In all of our baselines, we post-process the outputs with Segment Anything (SAM) [25] and point prompts to isolate the garment mask. We cut out the identified garment sections and paste them onto white background for the final garment image output. 4.3. Quantitative Results The numerical results of our experiments on the VITONHD dataset are reported in Table 1. Our tailored TryOffDiff approach outperforms all baseline methods across all generative performance metrics. However, baseline rankings vary significantly depending on the chosen metric. For example, GAN-Pose has the second best results when using full-reference metrics like SSIM, MS-SSIM, and CWSSIM. In contrast, for no-reference metrics such as FID, CLIP-FID, and KID, CatVTON emerges as the strongest baseline, while GAN-Pose has the lowest performance. The DISTS metric is our main metric as it balances structural and textural information, offering more nuanced assessment of generated image quality. When examining the ranking of the baseline methods, CatVTON slightly outperforms GAN-Pose, which in turn shows marginally better performance than ViscoNet and OOTDiff. This ranking aligns well with our own subjective visual perception, which will be further discussed in the following Section 4.4. We emphasize that TryOffDiff shows significant improvement of 5.2 percentage points over the next best performing baseline method. 4.4. Qualitative Analysis The qualitative results are shown in Figure 6. We find that they align with the quantitative results and illustrate how each metric emphasizes different aspects of garment reconstruction leading to inconsistent rankings, as discussed in 7 Section 3.1. GAN-Pose generates outputs that manage to approximate the main color and shape of the target garment. However, the predicted images often contain small regions where parts of the garment are missing. Although these gaps do not significantly affect full-reference metrics since the overall garment structure is still largely intact, they noticeably reduce visual fidelity, giving the images an unnatural appearance. This degradation is reflected in the noreference metrics, which are more sensitive to such visual artifacts. ViscoNet generally produces more realistic outputs than GAN-Pose but struggles to accurately capture the garments shape, often resulting in deformed representations. Additionally, ViscoNet displays bias towards generating long sleeves, regardless of the target garments actual design. Most outputs also lack textural details, further highlighting ViscoNets limitations for the garment reconstruction task. OOTDiffusion, originally designed as virtual try-on method, encounters similar difficulties as GAN-Pose in generating realistic images. While it generally struggles to retain detailed textures, it performs better in preserving fine elements like logos compared to previous methods. Nonetheless, its inability to consistently capture overall textural details underscores its limitations in virtual try-off. CatVTON also demonstrates the ability to preserve logo elements. Furthermore, it generally manages to produce texture details that closely resemble those of the target garment. The garment shapes this method generates appear natural, making CatVTONs outputs visually appealing and the strongest baseline methods in terms of visual fidelity. Although CatVTON produces garments with natural appearance, the shapes do not consistently match the target garments actual shape, undermining its full-reference metric performance and limiting its overall effectiveness for VTOFF. Our TryOffDiff model consistently captures the shape of target garments, even reconstructing portions of the garment that are occluded in the reference image. For instance, TryOffDiff can correctly infer the shape of highcut bodysuits, even when models in the reference images are wearing pants. Subtle indicators, such as garment tightness or features like shoulder straps, enable this reconstruction. Additionally, TryOffDiff reliably recovers detailed textures, including colors, patterns, buttons, ribbons, and logos, making it superior over all baseline methods and the top-performing model for VTOFF in our experiments. While we note that TryOffDiff is the only method specifically designed for VTOFF, it stands out as the only approach capable of accurately reconstructing textural details. This underscores the effectiveness of our proposed image conditioning mechanism, which enables precise texture recovery and overall high-quality garment reconstruction. (a) Reference (b) Gan-Pose (c) ViscoNet (d) OOTDiffusion (e) CatVTON (f) TryOffDiff (g) Target Figure 6. Qualitative comparison. In comparison to the baseline approaches, TryOffDiff is capable of generating garment images with accurate structural details as well as fine textural details. 5. Conclusion In this paper, we introduced VTOFF, novel task focused on reconstructing standardized garment image based on one reference image of person wearing it. While VTOFF shares similarities to VTON, we demonstrate it is better suited for evaluating the garment reconstruction accuracy of generative models since it targets clearly defined output. We further propose TryOffDiff, first tailored VTOFF model which adapts Stable Diffusion. We substitute Stable Diffusion text conditioning with adapted SigLIP features to guide the generative process. In our experiments, we repurpose the existing VITON-HD dataset, enabling direct comparisons of our method against several baselines based on existing VTON approaches. TryOffDiff significantly outperforms these baselines, with fewer requirements for preand post-processing steps. In particular, we find that we are better at preserving fine details like patterns and logos. We also observe that this advantage is not reflected when using conventional metrics for generative model reconstruction quality. To better capture visual fidelity, we adopt DISTS as our primary evaluation metric. VTOFF highlights the potential for advancing our understanding of guided generative model performance. Our results show promise, but there is still room for improvement in preserving complex structures, such as logos and printed designs. Future work could benefit from exploring newer generative models, alternative visual conditioning methods and additional losses to enhance detail preservation. Finally, our findings underscore the need for improved quality metrics, potentially combined with user studies, to better align qualitative impressions with quantitative evaluations."
        },
        {
            "title": "Acknowledgment",
            "content": "This work has been funded by the German federal state of North Rhine-Westphalia as part of the research training group DataNinja (Trustworthy AI for Seamless Problem Solving: Next Generation Intelligence Joins Robust Data Analysis) and the research funding program KI-Starter. We would like to thank UniZG-FER for providing access to their hardware."
        },
        {
            "title": "References",
            "content": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. stat, 1050:21, 2016. 5 [2] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, et al. Imagen 3. arXiv, 2024. https://doi.org/nqr4. 3 [3] Serge Belongie, Jitendra Malik, and Jan Puzicha. Shape matching and object recognition using shape contexts. IEEE TPAMI, 2002. 3 [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Improving image generation with Wang, Linjie Li, et al. better captions. preprint, 2023. [5] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In ICLR, 2018. 4 [6] Chaofeng Chen and Jiadi Mo. IQA-PyTorch: Pytorch toolbox for image quality assessment. https://github. com/chaofengc/IQA-PyTorch, 2022. 4 [7] Soon Yau Cheong, Armin Mustafa, and Andrew Gilbert. Visconet: Bridging and harmonizing visual and textual conditioning for controlnet. In ECCVW, 2024. 3, 6, 7 [8] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for virtual try-on. arXiv, 2024. https://doi.org/np47. 3 [9] Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, and Xiaodan Liang. Catvton: Concatenation is all you need for virtual try-on with diffusion models. arXiv, 2024. https: //doi.org/npf6. 3, 6, 7, 2 [10] Aiyu Cui, Daniel McKee, and Svetlana Lazebnik. Dressing in order: Recurrent person image generation for pose transfer, virtual try-on and outfit editing. In ICCV, 2021. 3 [11] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture similarity. IEEE TPAMI, 2020. 2, 4 [12] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu, Bing-Cheng Chen, and Jian Yin. Fw-gan: Flow-navigated warping gan for video virtual try-on. In ICCV, 2019. 3 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 3, 1 [14] Yuying Ge, Ruimao Zhang, Lingyun Wu, Xiaogang Wang, Xiaoou Tang, and Ping Luo. versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images. In CVPR, 2019. [15] Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo. Parser-free virtual try-on via distilling appearance flows. In CVPR, 2021. 3 [16] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, et al. Accelerate: Training and inference at scale made simple, https : / / github . com / efficient and adaptable. huggingface/accelerate, 2022. 4 [17] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry In Davis. Viton: An image-based virtual try-on network. CVPR, 2018. 1, 2 [18] Xintong Han, Xiaojun Hu, Weilin Huang, and Matthew Scott. Clothflow: flow-based model for clothed person generation. In CVPR, 2019. 3 [19] Xiao Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, and Tao Xiang. Controllable person image synthesis with poseconstrained latent diffusion. In ICCV, 2023. 3 [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 2, [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1 [22] Wonbong Jang and Lourdes Agapito. Nvist: In the wild new view synthesis from single image with transformers. In CVPR, 2024. 3 [23] Nikolay Jetchev and Urs Bergmann. The conditional analogy gan: Swapping fashion articles on people images. In ICCVW, 2017. 1, 2 [24] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion In ICCV, image-to-video synthesis via stable diffusion. 2023. 3 [25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. [26] Tejas Kulkarni, William Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. In NeurIPS, 2015. 3 [27] Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, and Jaegul Choo. High-resolution virtual try-on with misalignment and occlusion-handled conditions. In ECCV, 2022. 5 [28] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In ICLR, 2022. 6 [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6 [30] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In AAAI, 2024. [31] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In CVPR, 2022. 4 9 [32] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In SIGGRAPH, 2023. 3 [33] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar Zaiane, and Martin Jagersand. U2-net: Going deeper with nested u-structure for salient object detection. Pattern Recognit., 2020. 1 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 4 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3, 4, 5, 6, [36] Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, and Umapada Pal. Multi-scale attention guided pose transfer. Pattern Recognit., 2023. 3, 6, 7 [37] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, et al. Palette: Image-to-image diffusion models. In SIGGRAPH, 2022. 3 [38] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image superresolution via iterative refinement. IEEE TPAMI, 2022. 3 [39] Fei Shen, Hu Ye, Jun Zhang, Cong Wang, Xiao Han, and Wei Yang. Advancing pose-guided image synthesis with progressive conditional diffusion models. In ICLR, 2024. 3 [40] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 4 [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 4 [42] George Stein, Jesse Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Ross, Valentin Villecroze, Zhaoyan Liu, Anthony Caterini, Eric Taylor, and Gabriel Loaiza-Ganem. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. In NeurIPS, 2024. 4 [43] Shao-Hua Sun, Minyoung Huh, Yuan-Hong Liao, Ning Zhang, and Joseph Lim. Multi-view to novel view: Synthesizing novel views with self-learned confidence. In ECCV, 2018. 3 [44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. 4 [45] Huixuan Tang, Neel Joshi, and Ashish Kapoor. Learning blind measure of perceptual image quality. In CVPR, 2011. 2, 4 [46] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox. Multi-view 3d models from single images with convolutional network. In ECCV, 2016. [47] Lucas Theis, Aaron van den Oord, and Matthias Bethge. note on the evaluation of generative models. In ICLR, 2016. 2 [48] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv, 2024. https://doi.org/nq56. 3 [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 3, 5 [50] Riza Velioglu, Robin Chan, and Barbara Hammer. Fashionfail: Addressing failure cases in fashion object detection and segmentation. In IJCNN, 2024. 6 [51] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, et al. Diffusers: State-of-the-art diffusion models. https://github. com/huggingface/diffusers, 2022. [52] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang. Toward characteristicIn ECCV, preserving image-based virtual try-on network. 2018. 3 [53] Chenhui Wang, Tao Chen, Zhihao Chen, Zhizhong Huang, Taoran Jiang, Qi Wang, and Hongming Shan. Fldm-vton: Faithful latent diffusion model for virtual try-on. In IJCAI, 2024. 3 [54] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process., 2004. 4 [55] Zhenyu Xie, Zaiyu Huang, Fuwei Zhao, Haoye Dong, Michael Kampffmeyer, and Xiaodan Liang. Towards scalable unpaired virtual try-on via patch-routed spatiallyadaptive gan. In NeurIPS, 2021. 1, 3 [56] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, and Humphrey Shi. Prompt-free diffusion: Taking text out of text-to-image diffusion models. In CVPR, 2024. 3 [57] Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen. Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on. arXiv, 2024. https://doi.org/ npf9. 3, 6, 7, [58] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv, 2023. https://doi. org/np3v. 3 [59] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 4, 5 [60] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. ICCV, 2023. 3 [61] Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Zequn Jie, and Jiashi Feng. Multi-view image generation from singleview. In ACM MM, 2018. 3 [62] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei Efros. View synthesis by appearance flow. In ECCV, 2016. 3 [63] Luyang Zhu, Yingwei Li, Nan Liu, Hao Peng, Dawei Yang, and Ira Kemelmacher-Shlizerman. M&m vto: Multigarment virtual try-on and editing. In CVPR, 2024. 10 TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "The supplementary material provides additional experimental results. The first section presents an ablation study that examines the contributions of individual components of TryOffDiff and evaluates the effect of various inference hyperparameters. We then material demonstrate how the proposed approach can be integrated with virtual try-on models for person-to-person try-on, achieving competitive performance with specialized models. Further qualitative comparisons with baseline methods are included, alongside visualizations of TryOffDiff predictions on 10% of the test dataset. Finally, we give further details regarding implementation. 6. Ablation Studies Our ablation experiments investigate the impact of various TryOffDiff configurations. We analyze the differences between operating in the pixel and latent space, evaluate adapter design choices, and assess the influence of different image encoders and conditioning features. Additionally, we compare the effectiveness of fine-tuning versus training from scratch. Finally, we further look into the role of denoising hyperparameters during the inference phase of our method. 6.1. Impact of TryOffDiff configurations Our first set of experiments explores different TryOffDiff setups, focusing only on methods that achieved comparable results in our evaluations. All models were trained from scratch, except for TryOffDiff. The Autoencoder is based on nested U-Net [33] , originally proposed for salient object detection. We trained the model from scratch using MSE. This approach is able to reconstruct the general shape of the garment, but it lacks detailed features such as logos, text, and patterns. The PixelModel, diffusion model operating in pixelspace based on the original diffusion architecture [21], shows improved pixel-level details but suffers from slow inference, rendering it impractical for real-world applications. For the Latent Diffusion Models (LDMs), we leverage the recent VAE encoder from StableDiffusion-3 [13] , conditioning it with images via cross-attention layers in the U-Net. The overall architecture mirrors StableDiffusion1.4 [35], with variations through different image encoders, adapter layers, and mixed precision settings. Precise model details are listed in Table 2, and the corresponding quantitative results for the VTOFF task on the VITON-HD dataset are summarized in Table 3. Unlike ear- (a) 81.9 / 36.2 (b) 81.5 / 40.4 (c) 81.7 / 39.7 (d) 80.3 / 24. (e) 75.3 / 25.0 (f) 80.3 / 19.4 Figure 7. Examples demonstrating the un-/suitability of performance metrics (SSIM / DISTS) and an Autoencoer model applied to VTOFF. In each figure, left image is the ground truth image and the right image is the model prediction of Autoencoder (top, a-c) and TryOffDiff (bottom, d-f). Notice the higher SSIM scores for the Autoencoder compared to TryOffDiff despite poor visual quality of reconstructed garment images. lier experiments, here we evaluate the raw outputs of the generative model without applying background removal. Previously, background removal was necessary to ensure comparability with baseline methods designed for VTON models adapted to the VTOFF task. Unnecessary elements (e.g. anything except the upper-body garment) were removed through segmentation-based post-processing with SAM. However, since all models in this comparison are specifically trained for the VTOFF task, they are expected to handle background removal directly. TryOffDiff achieves slightly better performance metrics when evaluated without SAM post-processing. Figure 8 shows the qualitative results for different configurations of our approach. These results further highlight the shortcomings of existing image generation metrics, which often fail to align with human perception of image quality. For instance, the autoencoder in column 1 achieves high scores despite its lack of fine details, limitation also illustrated in Figure 7. 6.2. Hyper-parameter choice in the denoising process Figure 9 shows how various guidance scale and inference steps impact FID and DISTS. We find that the performance of our approach remains relatively stable with respect to the number of denoising steps. Still, it is affected by the value of the guidance scale, which we further demonstrate with 1 Method VAE Img. Encoder Emb.shape Adapter Cond.shape Sched. Prec. Steps - SigLIP-B/16 Autoencoder - - PixelModel SD3 CLIP ViT-B/32 (50,768) LDM-1 SigLIP-B/16 SD3 LDM-2 SD3 LDM-3 SigLIP-B/16 SD1.4 SigLIP-B/16 TryOffDiff - - - (64,768) (1024,768) Linear+LN (50,768) - (64,768) (1024,768) Linear+LN (1024,768) Linear+LN (64,768) (1024,768) Trans.+Linear+LN (77,768) - fp32 290k DDPM fp16 300k DDPM fp16 180k DDPM fp16 320k DDPM fp32 120k PNDM fp32 220k Table 2. Training configurations of ablations. Method Sched. SSIM MS-SSIM CW-SSIM LPIPS FID CLIP-FID KID DISTS - - Autoencoder PixelModel DDPM - DDPM - LDM-1 DDPM - LDM-2 DDPM - LDMTryOffDiff PNDM 2.0 - 50 50 50 50 50 81.4 76.0 79.6 80.2 79.5 79. 72.0 66.3 70.5 72.3 71.3 71.5 37.3 37.0 42.0 48.3 46.9 47.2 39.5 52.1 33.0 31.8 32.6 33. 108.7 75.4 26.6 18.9 18.6 20.2 31.7 20.7 9.14 7.5 7.5 8.3 66.8 56.4 11.5 5.4 6.7 6. 32.5 32.6 24.3 21.8 22.7 22.5 Table 3. Quantitative comparison. Evaluation metrics for different methods on VITON-HD-test dataset for VTOFF task. Results are reported on raw predictions, with no background removal. Note that while LDM-2 may achieve better performance metrics, we still choose TryOffDiff over LDM-2 due to its better subjective visual quality in garment image generation, see also Figure 8. qualitative results in Figure 10. Lower guidance values result in loss of detail, whereas higher values compromise realism, introducing artifacts such as excessive contrast and color saturation. Figure 11 and Figure 12 demonstrate the effect of varying noising seed on reconstruction quality. Overall, the generated garment images show strong consistency across inference runs. However, for certain examples, slight variations in the shape of the garment can occur. This is noticeable in upper-body apparel with challenging features, such as ribbons or short tops. Similarly, complex patterns, such as printed designs or text on shirts, may exhibit slight differences in reconstruction. In contrast, simpler garmentsthose with solid colors or basic patterns like stripesshow high consistency across all runs and closely match the ground truth. 7. Person-to-person Try-On TryOffDiff can be used to adapt existing Virtual Try-On models for person-to-person try-on. In this setup, our method generates the target garment from the target model, which is then used as input to classical VTON models instead of the ground truth garment image. We conduct experiments using OOTDiffusion [57] and compare the quality of virtual try-on using the ground truth garment versus our predicted garment. Additionally, we evaluate against CatVTON [9], state-of-the-art person-to-person try-on model, using its default inference settings from the official GitHub repository. The quantitative results are summarized in Table 4. Since VITON-HD dataset lacks person-to-person tryon ground truth data, we report only metrics that assess perceptual quality. Replacing the ground truth garment with TryOffDiffs predictions leads to slight drop in quality, as the reconstructions are not perfect. Our approach also slightly outperforms CatVTON. This may be partly attributed to CatVTONs difficulties with person reconstruction, despite its strength in preserving clothing details. This observation further highlights the limitations of the VTON task and commonly used VTON metrics, which fail to adequately distinguish between person and garment reconstruction quality. Qualitative results are shown in Figure 13 and Figure 14. Overall, there is no definitive winner between CatVTON Method FID CLIP-FID KID CatVTON OOTDiffusion + GT OOTDiffusion + TryOffDiff 12.0 10.8 12.0 3.5 2.8 3.5 3.9 2.0 2.5 Table 4. Quantitative comparison of Virtual Try-On models. We compare the results of OOTDiffusion when ground truth (GT) garment is used and when the garment predicted by TryOffDiff is used. We further show the results of CatVTON, specialized person-to-person try-on model. Our TryOffDiff model in combination with VTON model achieves competitive performance in person-to-person VTON. (a) Autoencoder (b) PixelModel (c) LDM-1 (d) LDM-2 (e) LDM-3 (f) TryOffDiff (g) Target Figure 8. Qualitative comparison between different configurations explored in our ablation study. See also Table 2 for more details. 3 (a) Guidance Scale (b) Inference steps Figure 9. Ablation study on the impact of guidance scale (s) and inference steps (n) on DISTS and FID scores. Experiments are conducted on VITON-HD-test with TryOffDiff using the DDIM [41] noise scheduler. SSIM, MS-SSIM, CW-SSIM, and LPIPS, and the cleanfid [31] library for FID, CLIP-FID, and KID. Finally, we employ the original implementation of DISTS [11] for evaluating perceptual image quality. For readability purposes, the values of SSIM, MS-SSIM, CW-SSIM, LPIPS, and DISTS presented in this paper are multiplied by 100, and KID is multiplied by 1000. and OOTDiffusion combined with TryOffDiff. CatVTON excels in preserving texture and pattern details but occasionally suffers from diffusion artifacts (Figure 13, row 3; Figure 14, row 2). Additionally, CatVTON sometimes transfers attributes of the target model to the source model (Figure 13, rows 3 and 4; Figure 14, row 4), limitation not observed in classical try-on models. Finally, complex clothing items remain challenging, even when using ground truth images for virtual try-on (Figure 13, row 1; Figure 14, rows 1 and 4). Nonetheless, these results highlight the potential of the Virtual Try-Off task and the TryOffDiff model. Although TryOffDiff was not specifically trained for person-to-person virtual try-on, its integration with VTON models presents promising approach, already demonstrating competitive performance compared to state-of-the-art person-to-person virtual try-on methods. 8. Additional Qualitative Results This section offers additional qualitative results. We present further comparisons with our baseline models, as introduced in Section 4.2, in Figure 15. We also visualize TryOffDiffs output on 10% of the test set, which is obtained by sorting the test images alphabetically and selecting every 10th image. These results are shown in Figure 16 and Figure 17. 9. Implementation Details The implementation relies on PyTorch as the core framework, with HuggingFaces Diffusers library [51] for diffusion model components and the Accelerate library [16] for efficient multi-GPU training. For evaluation, we use IQA-PyTorch [6] to compute 4 = = 1.2 = 1.5 = 1.8 = 2.0 = 2.5 = 3. = 3.5 Ground Truth Figure 10. Qualitative results for different guidance. Left: no guidance applied (s = 0). Middle: varying guidance scale (s [1.2, 1.5, 1.8, 2.0, 2.5, 3.0, 3.5]). Right: ground-truth. 5 Examples generated from multiple inference runs using our TryOffDiff model Target Figure 11. Sample Variations. While minor variations in shape and pattern may occur with complex garments, the overall output of TryOffDiff demonstrates consistent garment reconstructions across multiple inference runs with different random seeds. 6 Examples generated from multiple inference runs using our TryOffDiff model Target Figure 12. Sample Variations. While minor variations in shape and pattern may occur with complex garments, the overall output of TryOffDiff demonstrates consistent garment reconstructions across multiple inference runs with different random seeds. Figure 13. Qualitative comparison on (person-to-person) VTON task. Columns show: (a) person to be dressed which all of the models use as one of the reference inputs, (b) output of the CatVTON model which uses an image of person wearing the target garment as condition for direct person-to-person VTON, (c) output of the OOTDiffusion model which takes in an image of the target garment and (d) output of the OODDiffusion model which takes in the output of our TryOffDiff model for indirect person-to-person VTON. 8 Figure 14. Qualitative comparison on (person-to-person) VTON task. Columns show: (a) person to be dressed which all of the models use as one of the reference inputs , (b) output of the CatVTON model which uses an image of person wearing the target garment as condition for direct person-to-person VTON, (c) output of the OOTDiffusion model which takes in an image of the target garment and (d) output of the OODDiffusion model which takes in the output of our TryOffDiff model for indirect person-to-person VTON. 9 (a) Gan-Pose (b) ViscoNet (c) OOTDiffusion (d) CatVTON (e) TryOffDiff (f) Target Figure 15. Qualitative comparison between baselines and TryOffDiff. In comparison to the baseline approaches, TryOffDiff is more capable of generating garment images with accurate structural details as well as fine textural details. Figure 16. TryOffDiff predictions on the VITON-HD-test dataset (samples 1100). Visualized are the first 100 predictions, sampled by selecting every 10th sample from the test set after sorting filenames alphabetically. 11 Figure 17. TryOffDiff predictions on the VITON-HD-test dataset (samples 101200). Visualized are the next 100 predictions, sampled by selecting every 10th sample from the test set after sorting filenames alphabetically."
        }
    ],
    "affiliations": [
        "Machine Learning Group, CITEC, Bielefeld University, Germany"
    ]
}