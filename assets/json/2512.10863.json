{
    "paper_title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
    "authors": [
        "Jingli Lin",
        "Runsen Xu",
        "Shaohao Zhu",
        "Sihan Yang",
        "Peizhou Cao",
        "Yunlong Ran",
        "Miao Hu",
        "Chenming Zhu",
        "Yiman Xie",
        "Yilin Long",
        "Wenbo Hu",
        "Dahua Lin",
        "Tai Wang",
        "Jiangmiao Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 3 6 8 0 1 . 2 1 5 2 : r MMSI-Video-Bench: Holistic Benchmark for Video-Based Spatial Intelligence Jingli Lin1,2 Runsen Xu1,3 Shaohao Zhu1,4 Sihan Yang1 Peizhou Cao1,5 Yunlong Ran1,4 Miao Hu6 Chenming Zhu1,7 Yiman Xie1,4 Yilin Long1,8 Wenbo Hu1,9 Dahua Lin1,3 Tai Wang1(cid:0) Jiangmiao Pang1(cid:0) 1Shanghai AI Laboratory 2Shanghai Jiaotong University 3The Chinese University of Hong Kong 7University of Hong Kong 8Fudan University 4Zhejiang University 5Beihang University 6Xian Jiaotong University 9University of California, Los Angeles Equal Contribution Project Lead"
        },
        {
            "title": "Evaluation Code",
            "content": "MMSI-Video-Bench Figure 1. MMSI-Video-Bench is diverse, human-annotated, and challenging benchmark, designed to evaluate models video-based spatial intelligence, including their ability to perceive, understand, reason, and make decisions over spatio-temporal information in videos. The bar chart in the top-right corner illustrates the substantial performance gap between state-of-the-art models and human performance."
        },
        {
            "title": "Abstract",
            "content": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes four-level framework, Perception, Planning, Prediction, and CrossVideo Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing striking humanAI gap: 1 many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish solid testbed for advancing video-based spatial intelligence. 1. Introduction For decades, humans have aspired to build an embodied general-purpose AI assistant, akin to JARVIS in Iron Man. As MLLMs [1, 5, 44, 51, 54] have begun to exhibit strong language and visual intelligence, they are increasingly viewed as promising foundation for embodied AGI. One of the most important remaining challenges in this pursuit is to endow MLLMs with spatial intelligence, that is, the ability to perceive, reason about, and interact with physical space from continuous visual inputs, as humans do. To reliably measure progress on this research, we need rigorous benchmarks. However, existing benchmarks have significant limitations: most operate on discrete single images [4, 6, 23] or multiple images [13, 21, 46] rather than videos, leaving an input gap to the practical setting. Recent video-based benchmarks [27, 28, 45, 50] also suffer from the following issues: (1) question types are not sufficiently holistic; (2) they rely heavily on templated automatic question generation, which restricts question diversity and may introduce template overfitting or biases [46]; and (3) data sources and scenes are not comprehensive enough. In this work, we introduce MMSI-Video-Bench to fill these gaps. We build our benchmark around holistic, multi-level framework for video-based spatial intelligence consisting of Perception, Planning, Prediction, and CrossVideo Reasoning (see Figure 1). Models are required to reason over single video for spatial perception, capturing global scene information (Spatial Construction) and ego- /exo Motion dynamics. Beyond perception, models should be able to make decisions or take actions to interact with the environment (Planning) and further make Predictions about future spatial states of the world. Finally, general spatial intelligence model should be capable of Cross-Video Reasoning for multi-view integration and memory updating. We instantiate the above holistic framework as diverse, accurate, and challenging MCQ benchmark. We adopt fully human-designed protocol following [46]. Eleven 3DV researchers manually design each sample, including selecting video clip from curated pool of about 20K videos, designing novel question, writing the correct answer, distractors, and brief rationale. multi-stage review process leverages these rationales to ensure accuracy and unambiguity. Our video pool combines 25 open-source datasets with newly recorded in-house videos, covering wide range of scenarios, including indoor scans, outdoor driving, robotics, etc. In total, with 400+ hours of annotation and verification, we obtain 1,106 questions grounded in 1,278 video clips, grouped into five task categories and 13 subtypes. Thanks to the diversity of data sources and the holistic coverage of task types in MMSI-Video-Bench, we are also able to build three domain-oriented sub-benchmarks: Indoor Scene Perception Bench, Robot Bench, and Grounding Bench, enabling targeted assessment of specific model capabilities. Using MMSI-Video-Bench, we conduct comprehensive evaluation of open-source and proprietary MLLMs. Current models remain far from the desired level of videobased spatial intelligence: many perform close to random guessing, and the best model, Googles Gemini 3 Pro, still trails humans by nearly 60%. To the best of our knowledge, our benchmark yields the largest humanAI performance gap among existing video-based spatial benchmarks. In addition, we also evaluate models that have been spatially fine-tuned, yet their capabilities still fail to generalize effectively on our benchmark, further underscoring the challenge posed by MMSI-Video-Bench. To provide diagnostic signals for future research, we perform per-category error analysis. For Spatial Construction, failures are dominated by geometric reasoning errors; for Motion, by fine-grained grounding failures on fast, subtle, or long-duration motion; for Planning and Prediction, by promptevidence misalignment where models ignore video cues; and for Cross-Video Reasoning, by difficult grounding and matching correspondences across videos. Beyond analysis, our preliminary exploration shows that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains on MMSI-Video-Bench. We further observe that, due to the reasoning-intensive nature of our benchmark, the commonly used frame-sampling strategy AKS [35] does not transfer directly to MMSI-Video-Bench, leading to additional performance degradation. Overall, current MLLMs still have substantial room for improvement in spatial intelligence, and MMSI-Video-Bench offers an accurate and challenging testbed with actionable insights for advancing video-based spatial reasoning. 2. Related Work Video-based Benchmarks. Video-based benchmarks evaluate models ability to perceive, understand, and reason over temporal and visual information. Early video benchmarks such as MSVD-QA, MSRVTT-QA [43], and ActivityNet-QA [49] mainly assess global visual comprehension, with limited attention to temporal understanding. 2 Benchmark Source Diversity Modality Annotation Method Task Samples Num Human-AI Gap SpatialRGPT [6] SpatialVLM [4] CVBench [36] MultiSPA [44] All-Angles-Bench [47] MMSI-Bench [46] VSI-Bench [45] OST-Bench [28] SPAR-Bench [50] STI-Bench [27] EgoExoBench [17] - - 3 3 2 8 3 3 3 3 Single-Image Single-Image Single-Image Multi-Image Multi-Image Multi-Image Video Video Video Video Multi-Video Auto Auto&Human Auto Auto Human Human Auto Auto Auto Auto Auto&Human local-SU. local-SU. local-SU. local-SU. & short-MU. SU. local-SU. & short-MU. SU. & Plan. SU. & CAM.-MU. SU. & CAM.-MU. SU. & INST./CAM.-MU. CV. MMSI-Video (ours) 26 Video/Multi-Video Human SU. & MU. & Plan./Pred. & CV. 1,406 546 2,638 7800 2100 1,000 5,000 10,000 7207 2,064 7,330 1,106 <42 <30 - - 21.2 55.3 33 29.3 27.8 - 41.9 58. Table 1. Comparison of MMSI-Video-Bench with other spatial reasoning benchmarks, highlighting its diversity, comprehensiveness, and challenge. INST. and CAM. refer to instance and camera, SU., MU., and CV. refer to Spatial Understanding, Motion Understanding, and Cross-Video Reasoning, respectively. The HumanAI Gap indicates the performance difference as percentage. Later works like NeXT-QA [42] and MVBench [24] emphasize temporal dynamics. Subsequently, benchmarks such as LongVideoBench [41] and Video-MME [12] further extended evaluation beyond surface-level perception, incorporating temporalevent reasoning. With the advancement of MLLMs, more video-based reasoning benchmarks have emerged, each designed to probe specific aspects of realworld understanding. These include benchmarks focusing on such as complex spatial reasoning within videos [27, 28, 45, 50], online inference under continuous observation [26, 28], and cross-video reasoning [17]. Different from previous benchmarks, our proposed MMSI-VideoBench serves as more holistic and challenging benchmark for video spatial intelligence, covering complex reasoning about spatial layouts, motion understanding, and decisionmaking, as well as reasoning across multiple videos. Spatial Intelligence Benchmarks. Existing benchmarks for evaluating spatial intelligence in multimodal large language models (MLLMs) vary substantially in task design, modality, scene scope, and the specific spatial abilities they aim to assess. Early benchmarks such as SpatialRGPT [6], SpatialVLM [4], and CVBench [36] focus on single-image spatial reasoning, emphasizing depth and distance. Later, video-based benchmarks, VSI-Bench [45], SPAR-Bench [50], and OST-Bench [28], extend this to indoor scenes with objectobject and objectcamera relations, yet remain constrained by limited scene diversity and static spatial contexts. More recent benchmarks, including MultiSPA [44], MMSI-Bench [46], and STI-Bench [27], incorporate dynamic environments and cover wider range of indooroutdoor scenarios. MultiSPA and MMSI-Bench serve as demanding benchmarks focusing on multi-image local spatial localization and short-term motion understanding, whereas STI-Bench targets numerical estimation of instance-centric spatial states and motion trajectories within videos. Nevertheless, existing benchmarks focus on limited aspects or scene types, lacking holistic evaluation across diverse real-world contexts. In contrast, our MMSIVideo-Bench, curated from diverse real-world videos and fully human-annotated, offers more holistic and realistic assessment of spatial intelligence in MLLMs. 3. MMSI-Video-Bench In this section, we introduce the formulation of our MMSIVideo-Bench and the methodology for its construction. 3.1. Overview As video-based spatial intelligence benchmark, MMSIVideo-Bench primarily evaluates models capability to perceive, understand, and reason over video information, encompassing two core dimensions: Spatial. This dimension concerns the spatial states (e.g., position, shape) of entities such as instances, scenes, or cameras, and their spatial relations (e.g., frontback, leftright, nearfar) at fixed moment. To assess this ability, we introduce the Spatial Construction category, which requires the model to infer fine-grained global spatial layouts from partial and sequential video observations. Spatio-Temporal. When motion occurs in the video (from the camera or instances), the spatial configuration changes over time. The Motion Understanding category evaluates the models ability to reason about long-term motion dynamics across consecutive frames, including camera motion, individual instance motion, and interactive motion arising from interactions among multiple instances. After understanding spatio-temporal information, the next step is decision-making based on video understanding. The Planning and Prediction categories focus on higherlevel reasoning over sparse video information:(1) Planning tasks require the model to devise actions toward specific goal based on visual cues; (2) Prediction tasks test the models ability to infer or imagine outcomes under hypothetical conditions. 3 Figure 2. Illustrative examples of different subtypes in MMSI-Video-Bench. Rel., Inst., and Cam. stand for Relationship, Instance, and Camera. Please refer to our project page for the full demo. The above categories comprehensively cover spatial intelligence within single video. However, real-world understanding often requires reasoning across multiple videos. To achieve more holistic evaluation, we extend MMSIVideo-Bench with Cross-Video Reasoning, including: (1) Memory Update. From temporal perspective, observations of the same scene in real-world settings are often temporally discontinuous (i.e., we do not continuously stay 4 in one place). The model must therefore retain contextual memory from past observations and update it as new information becomes available. (2) Multi-View Integration. From spatial perspective, single viewpoint rarely captures the complete spatialtemporal information of complex scene. The model must thus integrate observations from multiple viewpoints to construct unified representation of the scene. Fig.2 illustrates example cases for each subtype. In the supplementary material, we provide an overview table that describes the details of each subtype. 3.2. Benchmark Construction Data Collection & Preprocessing. To ensure diversity in our benchmark, we curated videos spanning broad spectrum of real-world scenarios. The collection includes wide range of capture types, such as tabletop recordings, indoor scenes from single-room to multi-floor environments, outdoor building and street views, natural landscapes, sports activities, and movie footage. Our data sources include 25 publicly available datasets [2, 3, 79, 15, 19, 30, 31, 37, 48], such as ScanNet [7], RealEstate10k [53], DL3DV [29], Ego4D [14], DROID [22], Waymo [34] and MultiSports [25] (the remaining datasets are listed in the supplementary material), amounting to approximately In addition, we manually recorded and 20k video clips. carefully selected 140 supplemental in-house videos, each anonymized via masking to protect personal privacy. All videos were then downsampled to an appropriate frame rate for each category such that no key information would be lost. Each frame was timestamped in the top-left corner (formatted as xx min yy s) to facilitate precise temporal referencing during question design. Human Annotation. As shown in Fig.3, all annotations were conducted by team of eleven trained researchers specializing in 3D vision. To preserve data diversity, annotation tasks were assigned so that each annotator received balanced mix of task categories. We developed dedicated annotation interface that allowed annotators to view all videos and design questions directly within the interface. Based on the provided visual context, annotators composed questions, corresponding answers, and concise reasoning, particularly for challenging items, to facilitate subsequent verification. All questions were designed as multiple-choice items, containing between four and six answer options and optionally adopting an interleaved imagetext format. Data Quality Control. We implemented strict acceptance protocol, where eleven researchers performed crossevaluation, reviewing each others work. Each annotation had to meet three criteria: clear (the question is unambiguous and clearly stated), correctness (the answer is unique and factually accurate), and challenge (the question requires non-trivial reasoning). Only annotations that met all three criteria were accepted, with 100% approval rate required. Annotations that failed were revised based on feedback and resubmitted for reevaluation. Statistics. After the construction of benchmark samples, the final MMSI-Video-Bench contains 1,106 questions grounded in 1,278 video clips, covering five main categories and 13 subtypes, with their distribution shown in Fig.3. The average video duration is 1 minute 12 seconds, and the average question length is 164.5 characters. In Fig.4, we present the distribution of video durations. 4. Experiments 4.1. Evaluation Settings We evaluate wide range of state-of-the-art open-source and proprietary models on MMSI-Video-Bench, including GPT-5/ O3/ O4-mini/ GPT-4o, Gemini 3 Pro/ Gemini 2.5 Flash, Claude-4.5, Seed-1.6-Vision, Doubao-1.5-thinking, InternVL series, QwenVL series, LLaVA-Video series, and others. All proprietary models are tested via their official APIs, while all open-source models are evaluated using 8A100 GPUs and follow their officially released inference configurations. Due to (i) the limited number of images allowed per request by some proprietary APIs, and (ii) outof-memory issues when running large open-source models on long videos, we establish two evaluation tracks: Uniform-50. Each model receives exactly 50 uniformly sampled frames from the original video. This configuration aligns with the recommended number of input frames for most evaluated models. Sufficient-Coverage. In this setting, each model receives the complete set of frames used during annotation, ensuring no visual information is omitted. For comparison, we additionally provide two baselines: random guessing and human performance. Since all questions in MMSI-Video-Bench are multiple-choice, we adopt an exact-match accuracy metric, where prediction is considered correct only if it exactly matches the ground-truth. Besides general-purpose models, we also evaluate several models that are finetuned on spatial reasoning data or equipped with latent spatial representations. 4.2. Main Results We report the performance of various models on our benchmark. Tab. 2 summarizes the results of all evaluated models across different question subtypes under two evaluation settings. From the results in the table, we can draw the following key observations: Substantial gap between MLLMs and humans performance. Model performance across all question types in MMSI-Video-Bench falls significantly short of human-level performance. Most models achieve low scores, some approaching the level of random guessing, and even the best5 Figure 3. (Left) The pipeline of benchmark sample construction. (Right) The distribution of question types in MMSI-Video-Bench. cases, even performance drop under Sufficient-Coverage. Prior work [39] has likewise shown that more input frames can introduce redundancy that hinders reasoning. To further enhance model performance, it is crucial to develop more effective strategies for key frame sampling. An in-depth analysis of the impact of sampling strategies on model performance is presented in the frame sampling study section. Comparison across models. We observe that proprietary models consistently outperform open-source ones. Among open-source models, the best-performing ones, QwenVL2.5-72B (Uniform-50, 32.7) and QwenVL2.5-32B (Sufficient-Coverage, 32.4), still exhibit noticeable gap compared to most proprietary models under the same settings. Within open-source models, the results broadly follow the trend that larger parameter scales lead to better performance. In contrast, enabling the thinking mode brings only marginal improvements, as shown by comparing Gemini 2.5 Flash and QwenVL3-30B with their thinking mode versions. Prediction is the most challenging main category, and CameraInstance Spatial Relation is the most challenging subtype. In Tab.2, performance on Prediction is generally lower than other main task categories (i.e., the average scores of Spatial Construction, Motion Understanding, Planning, and Cross-Video Reasoning). This is because these tasks require models to go beyond simply understanding the spatio-temporal information and instead make predictions based on specific conditions or physical priors. Among the various types of Spatial Construction subtasks, the CameraInstance Spatial Relation subtype is the most challenging. This is due to its combination of ego-to-scene spatial reasoning and detailed grounding of instances within the video, making it the most difficult among all subtypes. Model performance across main categories and difficulty Levels. We further computed the average scores of each model across the main categories, as shown in Tab.3. Figure 4. Duration distribution of all video samples in MMSIVideo-Bench (in seconds). performing model, Gemini 3 Pro (38.0), lags behind humans (96.4) by nearly 60%. This indicates current models are unable to handle these challenging tasks, highlighting the inherent difficulty of MMSI-Video-Bench. This observation motivates deeper investigation into the underlying reasons for the models subpar performance on this benchmark. MLLMs exhibit weaknesses across all categories. Previous spatial reasoning benchmarks have primarily focused on evaluating models Spatial Construction abilities across various scenarios and contexts, consistently showing poor performance in this aspect [28, 45, 46]. Our benchmark provides more holistic evaluation, extending beyond Spatial Construction to assess other dimensions such as Motion Understanding, Planning, Prediction, and Cross-Video Reasoning. As shown in Tab.2, we observe that models also struggle considerably in these areas. In the subsequent error analysis section, we further investigate the specific capability bottlenecks underlying these weaknesses. Sufficient-Coverage does not outperform Uniform-50. The Sufficient-Coverage setting is assumed to result in better performance than the Uniform-50 setting, as it provides the most visual information. However, we observe that most models show no significant improvement, and in many Models Attr. Inst.-Inst. Spatial Construction Inst.Scen. Scen.Scen. Cam.-Inst. Cam.Scen. Cam. Motion Understanding Inter. Inst. Cross-Video MV. MU. Plan. - Pred. - Avg. (Sufficient-Coverage) Proprietary O4-mini O3 GPT-4o Gemini 2.5 Flash -Thinking 31.3 39.8 22.9 47.0 43.4 (Sufficient-Coverage) Open-source InternVL2.5-8B InternVL3-8B InternVideo2.5-8B QwenVL2.5-7B QwenVL2.5-32B QwenVL2.5-72B QwenVL3-8B QwenVL3-30B -Thinking 26.5 28.9 26.5 31.3 33.7 33.7 27.8 31.3 27.7 (Uniform-50) Proprietary Claude-haiku-4.5 GPT-5 O4-mini O3 GPT-4o Gemini 3 Pro Gemini 2.5 Flash -Thinking Doubao-1.5-thinking Seed-1.6-vision 31.3 44.6 36.1 34.9 26.5 44.6 37.4 43.4 45.1 43.1 (Uniform-50) Open-source InternVL2.5-8B InternVL2.5-38B InternVL2.5-78B InternVL3-8B InternVL3-38B InternVL3-78B InternVideo2.5-8B LLaVA-Video-7B LLaVA-Video-72B QwenVL2.5-7B QwenVL2.5-32B QwenVL2.5-72B QwenVL3-8B QwenVL3-30B -Thinking 21.7 27.7 33.7 33.7 33.7 43.4 27.7 27.7 36.1 26.5 25.3 32.5 33.7 27.7 27."
        },
        {
            "title": "Baseline\nRandom Guessing\nHuman Level",
            "content": "24.1 95.2 35.5 31.6 30.3 36.8 38.2 27.6 38.2 26.3 21.1 28.9 22.4 31.5 28.9 36.8 31.6 38.2 34.2 34.2 26.3 39.5 38.2 25.0 26.7 26.7 36.8 30.3 36.8 32.9 36.8 31.6 32.9 28.9 39.5 25.0 26.3 18.4 25.0 31.6 36.8 23.7 94. 39.0 46.8 35.1 40.3 41.6 26.0 27.3 23.4 19.5 31.2 28.6 33.3 26.0 29.9 32.5 44.2 36.4 40.3 31.2 44.2 40.3 42.9 18.8 16.7 31.2 26.0 37.7 29.9 31.2 35.1 22.1 28.6 28.6 29.9 31.2 37.7 24.7 33.8 37.7 24.4 96.3 30.4 36.2 26.1 31.9 24. 36.2 31.9 27.5 30.4 31.9 30.4 26.9 24.6 33.3 34.8 39.1 33.3 36.2 26.1 33.3 30.4 23.2 38.6 38.6 30.4 27.5 18.8 27.5 24.6 23.2 23.2 30.4 21.7 34.8 20.3 29.0 26.1 20.3 31.9 24.4 96.0 36.7 43.0 26.6 30.4 34.2 19.0 22.8 22.8 19.0 30.4 24.1 23.1 22.8 30. 32.9 40.5 39.2 40.5 38.0 29.1 43.0 32.9 29.3 34.5 17.7 11.4 29.1 19.0 16.5 30.4 22.8 19.0 15.2 20.2 21.5 29.1 24.1 19.0 25.3 24.2 92.8 36.2 36.2 27.5 48.8 36.2 30.0 23.8 28.8 31.2 30.0 28.8 22.4 36.2 25.0 35.0 41.2 42.5 43.8 38.8 43.8 43.8 47.5 32.7 40. 31.2 33.8 32.5 18.8 36.2 31.2 28.8 25.0 23.8 37.5 30.0 28.8 30.0 26.2 31.2 24.3 94.9 45.2 34.4 24.7 38.7 44.1 29.0 36.6 26.9 35.5 43.0 32.3 27.3 29.0 26.9 35.5 37.6 37.6 37.6 36.6 35.5 39.8 38.7 33.3 40.9 32.3 35.5 28.0 36.6 30.1 34.4 25.8 31.2 30.1 33.3 33.3 34.4 35.5 29.0 26. 24.1 96.8 28.9 34.4 28.9 30.0 34.4 33.3 31.1 23.3 36.7 35.6 33.3 29.6 21.1 23.3 41.1 28.9 32.2 37.8 35.6 40.0 31.1 28.9 32.2 35.6 27.8 34.4 34.4 32.2 30.0 32.2 30.0 35.6 42.2 31.1 35.6 41.1 22.2 28.9 26.7 24.9 95. 30.9 38.3 24.7 43.2 35.8 30.9 30.9 25.9 29.6 25.9 39.5 41.0 33.3 30.9 35.8 42.0 37.0 35.8 27.2 38.3 38.3 38.3 40.0 40.0 30.9 34.6 30.9 37.0 27.2 38.3 25.9 30.9 37.0 34.6 25.9 40.7 29.6 32.1 28.4 24.8 93.7 40.2 44.1 34.3 37.2 36. 34.3 30.4 34.3 27.4 25.5 35.3 28.4 29.4 25.5 33.3 33.3 32.4 41.2 30.4 34.3 30.4 31.4 31.4 32.9 35.3 35.3 31.4 31.4 30.4 37.2 27.4 36.3 28.4 24.5 28.4 36.3 31.4 31.4 39.2 23.1 94.4 31.4 34.3 25.7 30.0 38.6 22.9 34.3 21.4 30.0 41.4 35.7 29.9 28.6 25. 38.6 28.6 30.0 25.7 28.6 37.1 37.1 28.6 30.8 38.5 28.6 28.6 28.6 32.9 27.1 34.3 34.3 20.0 32.9 22.9 32.9 41.4 20.0 31.4 30.0 23.2 92.0 38.7 34.7 29.8 32.3 36.3 24.2 26.6 29.0 26.6 33.1 35.5 26.5 30.6 26.6 31.4 34.7 28.2 41.1 33.1 38.7 30.6 40.3 32.0 37. 26.6 33.1 35.5 33.1 24.2 27.4 29.8 21.0 29.0 34.7 27.4 26.6 25.0 29.0 28.2 24.8 95.1 26.8 31.7 26.8 30.5 31.7 32.9 23.2 29.3 35.4 30.5 30.5 32.0 35.4 25.6 32.9 28.1 28.1 26.8 29.3 35.4 24.4 31.7 20.4 25.9 28.1 40.2 26.8 26.8 28.1 24.4 24.4 35.4 29.3 28.1 31.7 30.5 29.3 31.7 31. 24.4 94.2 35.1 37.3 28.1 36.6 36.7 28.7 29.6 26.9 28.8 32.4 31.8 29.1 29.1 28.0 34.3 36.8 34.2 37.0 31.6 38.0 35.4 35.2 31.6 34.9 29.1 31.0 31.4 30.4 28.8 32.5 27.4 28.5 30.4 29.7 28.6 32.7 27.6 28.8 30.8 24.1 96. Table 2. Performance of various models under the Sufficient-Coverage and Uniform-50 settings. The highest and second-highest average scores across settings are highlighted in dark green and light green, respectively. Attr., Inst., Cam., Scen., and Inter. denote Attribute, Instance, Camera, Scene, and Interaction, respectively. MU., and MV. represent Memory Update, and Multi-View Integration, respectively. Overall, Gemini 3 Pro achieves the highest performance, while GPT-5 demonstrates the strongest capabilities in spatial construction and reasoning tasks. On the other hand, Seed-1.6-vision excels at motion understanding and reasoning across multiple video segments. As model with high reasoning and decision-making capacity, Gemini 3 Pro also leads in Prediction and Planning tasks compared to other models. Furthermore, we categorize our benchmark into three difficulty levels: easy, medium, and hard, based on the overall accuracy of all models on each question. Given our categorization criteria, it is natural that the scores follow the trend: hard < medium < easy. Examining model performance across these difficulty levels, we find that Gemini 3 Pro and Gemini 2.5 Flash are particularly effective at solving questions that most other models struggle with. 4.3. Evaluation of Spatially Fine-tuned Models In recent years, several approaches have emerged to enhance general-purpose models with spatial reasoning capabilities, either by training them on spatial reasoning data (e.g., SpaceQwen [21]) or by introducing architectural modifications to equip models with latent spatial representations (e.g., VLM3R [10], Spatial-MLLM [40]). These methods aim to endow models with spatial intelligence and have reported noticeable improvements on certain spatial reasoning benchmarks. We evaluate these spatially fine-tuned models under our benchmark as well under the Uniform-50 setting. Methods (Sufficient-Coverage) Proprietary O4-mini O3 GPT-4o Gemini 2.5 Flash -Thinking (Sufficient-Coverage) Open-source InternVL2.5-8B InternVL3-8B InternVideo2.5-8B QwenVL2.5-7B QwenVL2.5-32B QwenVL2.5-72B QwenVL3-8B QwenVL3-30B -Thinking (Uniform-50) Proprietary Claude-haiku-4.5 GPT-5 O4-mini O3 GPT-4o Gemini 3 Pro Gemini 2.5 Flash -Thinking Doubao-1.5-thinking Seed-1.6-vision (Uniform-50) Open-source InternVL2.5-8B InternVL2.5-38B InternVL2.5-78B InternVL3-8B InternVL3-38B InternVL3-78B InternVideo2.5-8B LLaVA-Video-7B LLaVA-Video-72B QwenVL2.5-7B QwenVL2.5-32B QwenVL2.5-72B QwenVL3-8B QwenVL3-30B -Thinking"
        },
        {
            "title": "Avg",
            "content": "SC. MU. Plan.&Pred. CV."
        },
        {
            "title": "Easy",
            "content": "35.1 37.3 28.1 36.6 36.7 28.7 29.6 26.9 28.8 32.4 31.8 29.1 29.1 28.0 34.3 36.8 34.2 37.0 31.6 38.0 35.4 35.2 31.6 34.9 29.1 31.0 31.4 30.4 28.8 32.5 27.4 28.5 30.4 29.7 28.6 32.7 27.6 28.8 30.8 34.9 39.0 28.0 39.4 36.6 27.4 28.7 25.9 25.4 31.0 28.0 27.4 28.4 30. 33.0 41.4 37.1 38.4 31.2 39.2 39.0 36.2 31.9 33.5 28.0 26.1 31.7 26.9 30.0 32.8 26.3 26.5 27.6 28.9 25.9 29.3 27.4 26.5 31.7 35.2 35.6 26.1 37.1 38.3 31.1 33.0 25.4 34.1 35.2 34.9 32.3 27.6 26.9 37.5 36.0 35.6 37.1 33.3 37.9 36.4 35.2 34.9 38.9 30.3 34.9 31.1 35.2 29.2 34.9 27.3 32.6 36.4 33.0 31.8 38.6 29.2 29.9 27. 34.0 33.5 28.6 31.6 34.5 27.7 25.2 29.1 30.1 32.0 33.5 28.7 32.5 26.2 32.0 32.0 28.2 35.4 31.6 37.4 28.2 36.9 27.1 32.6 27.2 35.9 32.0 30.6 25.7 26.2 27.7 26.7 29.1 32.0 29.1 28.2 26.7 30.1 29.6 36.6 40.1 30.8 34.3 37.2 29.6 32.0 29.1 28.5 32.0 35.5 29.0 29.1 25. 35.5 31.4 31.4 34.9 29.6 35.5 33.1 30.2 31.2 34.9 32.6 32.6 30.2 32.0 29.1 36.0 30.2 29.6 30.2 23.8 30.2 38.4 26.7 31.4 35.5 16.2 19.3 13.2 21.7 21.1 13.2 15.6 14.4 13.8 13.8 13.8 12.8 12.2 11.9 18.0 20.2 15.0 20.5 17.7 22.9 19.0 19.3 13.2 16.0 13.8 11.9 14.7 13.8 9.2 14.4 15.3 15.9 14.1 11.3 13.2 11.9 8.0 8.3 11. 30.0 35.0 26.0 32.8 35.2 24.5 26.0 21.2 26.5 31.0 24.8 22.0 22.0 23.0 30.5 32.5 29.0 34.2 29.2 35.2 32.5 33.0 31.9 35.3 23.2 27.5 25.0 27.5 28.0 27.8 23.0 27.0 25.8 29.0 22.5 26.5 21.8 26.2 27.5 56.7 55.4 43.3 53.6 51.7 46.4 45.4 43.5 44.3 49.9 54.9 51.0 51.2 47. 52.2 55.7 56.2 54.1 45.9 53.8 52.8 51.2 48.1 51.5 48.5 51.2 52.5 47.8 46.7 53.3 42.5 40.9 49.3 46.2 48.3 57.3 50.7 49.1 50.7 Table 3. Model performance across main categories and difficulty levels. SC. abbrev for Spatial Construction, MU. abbrev for Motion Understanding and CV. abbrev for Cross-Video Reasoning."
        },
        {
            "title": "Avg",
            "content": "SC. MU. Plan.&Pred. Cross-Video (base) LLaVA-Video-7B Spatial-MLLM VLM3R (base) QwenVL2.5-3B SpaceQwen - Architecture Architecture - Fine-tuning 28.48 24.05(-4.43) 4.97(-23.51) 27.67 27.58(-0.09) 26.51 23.28 5.82 27.37 25.00 32.58 27.65 6. 32.58 26.89 26.70 23.30 4.37 23.79 35.44 29.65 21.51 1.74 25.58 26.16 Table 4. The table presents the performance of various spatially fine-tuned models and their corresponding base models across the major task categories of MMSI-Video-Bench. SC., MU., Plan. and Pred. refer to Spatial Construction, Motion Understanding, Planning, and Prediction, respectively. As shown in Tab.4, compared with their respective base models, only SpaceQwen exhibits almost no change in performance, whereas both Spatial-MLLM and VLM3R suffer from degradation, particularly in instruction-following abil-"
        },
        {
            "title": "Method",
            "content": "GPT-4o QwenVL2.5-72B InternVL3-78B Uniform-50 AKS-50 31.6 28.4 (-3.2) 32.7 31.9 (-0.8) 32.5 31.6 (-0.9) Table 5. Comparison of model performance under uniform sampling and AKS sampling strategies. Figure 5. Model performance under different frame sampling methods. Dashed lines indicate contiguous sampling, while solid lines indicate uniform sampling. ity, leading to an overall drop in performance. This trend is consistent with observations reported in prior work [28, 46]: although such models may perform well on specific spatial reasoning datasets, their capabilities do not generalize effectively to other benchmarks and may even impair original abilities. These findings further highlight the challenge posed by our benchmark and its emphasis on comprehensively assessing models spatial intelligence. 5. Frame Sampling Study Effect of Frame Count and Sampling Strategy. We evaluated model performance with varying frame counts (1, 10, and 50) and two sampling strategies: consecutive frames from local segment versus uniformly sampled frames across the entire video. Experiments were conducted on three representative models (GPT-4o, Gemini 2.5 Flash, and QwenVL2.5-72B), with results shown as six curves in Fig.5. The results reveal two key findings: (1) performance is very low at minimal frame counts, sometimes near random guessing level, indicating that there are no shortcuts in MMSI-Video-Bench; performance improves significantly as more frames are sampled, showing the necessity of visual information in MMSI-Video-Bench. (2) Uniform sampling substantially outperforms consecutive sampling, demonstrating that broad temporal coverage is essential to capture key events, and that short continuous segments are insufficient. This underscores that MMSI-Video-Bench is designed to require models to integrate information across the full temporal span of the video. Smarter Keyframe Sampling Strategy. Recent studies have proposed more efficient frame sampling strategies that can yield notable performance improvements. Following the Adaptive Keyframe Sampling (AKS) approach [35], 9 which selects frames based on imagetext semantic representations, we sampled 50 frames per video and evaluated model performance. Results are summarized in Table 5. Although AKS achieves substantial gains on benchmarks such as LongVideoBench [41] and Video-MME [12], it fails to provide improvements on MMSI-Video-Bench. This may be because the key frames required to answer questions in MMSI-Video-Bench cannot be directly determined from semantic similarity alone (e.g., How does the dog move during the period when it is out of my sight?). Relying solely on semantic cues may even narrow the models effective field of view, causing it to miss other critical frames. This outcome underscores the challenging nature of our bench and indicates that it places stricter requirements on frame sampling strategies than existing video benchmarks. 6. Error Analysis 6.1. Error Categorization Effective video understanding requires sequence of reasoning steps: first perceiving fine-grained details, then linking entities across frames, followed by modeling spatial relations, and finally correctly aligning prompts to answer questions, with some cases demanding deeper reasoning over implicit cues. Based on this structured process, we categorize all model errors in our bench into non-overlapping, comprehensive types  (Fig.6)  : Detailed Grounding Error. Failures in fine-grained perception, including missing or confusing objects, overlooking subtle temporal changes, or misidentifying events at specific timestamps. This error mainly reflects deficiencies in surface-level visual grounding. ID Mapping Error. Failures in maintaining consistent identity tracking across frames, often caused by occlusion, rapid motion, or visually similar distractors, leading the model to confuse or mismatch entities over time. Geometric Reasoning Error. Mistakes in inferring spatial relations (relative positions or distance, e.g., front/behind, near/far), revealing the models inability to establish coherent spatial associations across frames. Prompt Alignment Error. Misunderstandings in interpreting the prompt or integrating it with visual evidence. These occur when the prompt introduces new conditions, reference images, or auxiliary visual inputs that the model fails to correctly incorporate, even if its understanding of the video information itself is accurate. Latent Logical Inference Error. Failures in reasoning that require integrating implicit cues or commonsense knowledge. Some questions in MMSI-Video-Bench demand inference based on subtle contextual clues, such as choosing an appropriate reference object to estimate height/ distance/ speed or correlating information across different viewpoints, or predicting motion trajectories using basic Figure 6. Illustration of five representative error types identified in MMSI-Video-Bench, along with examples of model responses and corresponding error analyses. struggle with inferring even simple geometric relations. Distinct error distributions reveal task-specific capability bottlenecks. Beyond Spatial Construction, we observe the following patterns across other task categories: In Motion Understanding tasks, detailed grounding remains major limitation: models often fail to comprehensively detect or interpret motion patterns, especially when confronted with fast movements, subtle actions, or long-duration motions. In Planning and Prediction tasks, Prompt Alignment Error is significant source of issues: models may accurately perceive the spatiotemporal context but still fail to connect high-level goals, assumptions, or contextual conditions with the video evidence. In Cross-Video Reasoning tasks, Latent Logical Inference Errors are most prominent, followed by Detailed Grounding Errors. These tasks typically require identifying correspondences across multiple videos (i.e., using matching instances across videos from different time points or viewpoints to establish spatio-temporal correspondences between the videos). We find that models frequently either fail to locate the same instance in both videos simultaneously or neglect to utilize them effectively for reasoning. Through this error analysis, we gain deeper understanding of the specific failure modes associated with each category in MMSI-Video-Bench, offering valuable insights into which model capabilities require improvement and which weaknesses future iterations should target. Figure 7. Distribution of the five error types. (Left) Error distribution across different question categories. (Right) Overall proportion of each error type. physical intuition. The model fails to detect or leverage these implicit cues. 6.2. Error Statistics We select four representative models (GPT-4o, Gemini 2.5 Flash, O3, and QwenVL2.5-72B) and conducted an error analysis on total of 520 incorrectly answered cases, evenly sampled across different question categories. The errors were categorized and quantified, and the final statistics, shown in Fig. 7, illustrate the distribution of each error type within the main categories, as well as the overall composition of error types. Several observations can be made: Geometric Reasoning Error is the most prevalent error type overall, especially within the Spatial Construction tasks. This finding is consistent with prior spatial reasoning benchmarks[28, 45, 46], indicating that current models still 10 formation for the models and may even introduce noise. This reflects an inherent limitation of VGGT; to consistently provide accurate 3D spatial cues, more robust and generalizable tools are needed. Issues in utilizing 3D spatial cues. Examination of the models reasoning processes revealed that the models fail to effectively leverage the 3D spatial cues. In many cases, the cues are either ignored or not correctly associated with the video content and the question, even though our prompts explicitly instructed the model to use them. This indicates that designing spatial cues that are easily interpretable by the models remains an open challenge. 7.2. Chain-of-Thought Prompting. To address issues such as Prompt Alignment and Latent Logic Inference errors, we explored Chain-of-Thought (CoT) prompting, guiding models to reason step by step. The model is provided with explicit prompts for each step: Step1: Understand and Analyze. Interpret the problem input, including auxiliary visual inputs, preset conditions, or requirements, and identify the key information to extract from the video. Step2: Locate and Gather Evidence. Find the relevant information in the video and collect sufficient evidence, including implicit clues not directly mentioned in the input. Step3: Reason and Solve. Combine the prompt with the extracted video information and perform step-by-step reasoning to answer the question. As shown in Fig.8, simply encouraging the model to think step by step does not consistently improve performance. This aligns with previous findings [45, 46]. The underlying issue is not that the model forgets to perform certain steps, but rather that it struggles to handle inherently difficult aspects of the task, highlighting that the limitation lies in the models intrinsic reasoning ability. 8. Additional Perspectives of MMSI-Video-"
        },
        {
            "title": "Bench",
            "content": "Due to the diversity of data sources and the holistic coverage of task types in MMSI-Video-Bench, the benchmark can be also examined from several domain-oriented perspectives. Based on different application focuses, we further derive three subset benchmarks from MMSI-VideoBench and report model performance on each of them. Similarly, model performance is evaluated under both the Uniform-50 and Sufficient-Coverage settings. Indoor Scene Perception Bench. The Indoor Scene Perception Bench focuses on evaluating models ability to perceive and understand indoor environments. This subset contains 523 samples from MMSI-Video-Bench and includes three major categories: Static-Scene (InstanceCentric), Static-Scene (Camera-Centric), and DynamicFigure 8. Effect of different methods on model performance. 7. Preliminary Exploration for Model Improvement While our error analysis categorizes and quantifies the types of failures made by existing models, in this section, we conduct an initial exploration toward improving model performance based on these identified errors. 7.1. Equipping Models with 3D Spatial Cues Among all error types, Geometric Reasoning Error stems from the models insufficient ability to build and utilize spatial representations. Correspondingly, we consider two general directions for improving spatial reasoning: (1) training models with sufficient and diverse spatial reasoning data, and (2) enhancing models by explicitly providing or modeling spatial representations, e.g., through dedicated architectures or auxiliary tools. In our preliminary attempt, we adopt the second strategy. Specifically, we equip the model with spatial cues generated by VGGT [38], enabling it to better perceive global scene geometry. As illustrated in Fig.9, we first feed raw video frames into VGGT to obtain 3D reconstruction of the scene. We then render 10 multiview observations (including top-down and multiple side views) from the reconstructed point cloud. These sparse geometric cues are combined with the original video frames and fed into the model together as input. We evaluate four representative models: Gemini 2.5 Flash, O3, GPT-4o, and QwenVL2.5-72B. Under the Uniform-50 setting, 50 frames from each video were fed into VGGT for 3D reconstruction and rendered into corresponding images. After equipping the models with 3D spatial cues, their performance is shown in Fig.8. All four models show no significant improvement (with gains below 1%), suggesting that 3D spatial cues do not reliably enhance spatial intelligence under our current setup. Upon further analysis of model errors, we identified two main issues: Issues in generating 3D spatial cues. While VGGT can handle relatively simple scenes, such as indoor scanning, it often fails in complex scenarios involving multi-room or multi-floor scans or dynamic scenes. In these failure cases, the rendered images provide little to no useful in11 Figure 9. Pipeline of equipping the model with 3D spatial cues. Figure 10. Distribution of subtask proportions across the Indoor Scene Perception Bench, Robot Bench and Grounding Bench. Scene. The two static-scene categories assess models understanding of static indoor layouts. The Instance-Centric category includes questions that are independent of the camera or viewer perspective, targeting object-intrinsic spatial attributes and inter-object spatial relations within the scene. In contrast, the Camera-Centric category examines spatial relations defined relative to the viewer or camera, evaluating the models understanding of its positional relationship to the surrounding environment. The DynamicScene category tests models ability to reason about scene changes over time, including those caused by human activities as well as object replacement events that occur between temporally separated video segments. The evaluation results, shown in the left four columns of Tab.6, indicate that among all models, GPT-5 achieves the strongest performance; notably, GPT-5 excels in instancecentric static scene perception, while Gemini 2.5 Flash achieves the best performance in camera-centric static scene perception. In addition, O3 and O4-mini show strong capability in understanding scene changes. Most open-source models lag behind the proprietary ones. Looking across sub-tasks, models with strong overall performance tend to maintain balanced scores across all types, whereas weaker models exhibit their primary bottleneck in Static-CC, which requires reasoning about the spatial relationship between the observer and the environmenta capability that these models struggle with. Robot Bench. The Robot Bench focuses on evaluating model performance on two core tasks in real-world embodied scenarios: Manipulation and Navigation. This subset contains 204 samples. The Manipulation category assesses models ability to perceive and reason about fine-grained tabletop operations and interactive motions, while the Navigation category evaluates models planning and navigation 12 Model Avg. Indoor Scene Perception Static-IC. Static-CC. Dynamic Avg. Robot Man. Nav. Avg. Grounding TG. 37.1 39.4 29.6 39.4 37.9 28.7 27.7 26.8 24.5 29.6 29.2 27.9 30.0 29.8 (Sufficient-Coverage) Proprietary O4-mini O3 GPT-4o Gemini 2.5 Flash -Thinking (Sufficient-Coverage) Open-source InternVL2.5-8B InternVL3-8B InternVideo2.5-8B QwenVL2.5-7B QwenVL2.5-32B QwenVL2.5-72B QwenVL3-8B QwenVL3-30B -Thinking (Uniform-50) Proprietary Claude-haiku-4.5 GPT-5 O4-mini O3 GPT-4o Gemini 3 Pro Gemini 2.5 Flash -Thinking Doubao-1.5-thinking Seed-1.6-vision (Uniform-50) Open-source InternVL2.5-8B InternVL2.5-38B InternVL2.5-78B InternVL3-8B InternVL3-38B InternVL3-78B InternVideo2.5-8B LLaVA-Video-7B LLaVA-Video-72B QwenVL2.5-7B QwenVL2.5-32B QwenVL2.5-72B QwenVL3-8B QwenVL3-30B -Thinking 29.4 28.3 30.4 27.0 29.1 32.5 26.8 27.5 28.1 27.1 26.6 30.8 28.7 27.5 32. 33.5 41.7 37.5 40.7 31.7 39.4 39.2 36.7 33.0 34.2 38.2 37.6 28.0 39.8 36.6 26.3 24.2 24.2 24.2 30.1 26.3 24.3 29.0 26.9 34.4 40.3 40.3 43.0 38.2 36.0 44.6 39.8 30.8 36.1 25.8 24.7 28.5 22.0 24.7 28.5 26.9 22.6 20.4 26.9 24.2 29.0 27.4 23.1 29.6 33.5 38.2 29.0 39.0 37. 27.9 30.5 26.5 24.6 30.5 29.0 29.6 29.0 32.7 32.4 42.6 36.0 38.2 26.5 40.8 36.8 33.8 32.5 31.9 30.1 27.9 31.6 29.4 31.6 34.6 26.5 28.3 32.0 28.7 26.1 29.8 27.6 29.4 33.5 49.2 49.2 36.9 40.0 44.6 38.5 26.1 35.4 24.6 24.6 38.5 31.1 36.9 26.1 35.4 41.5 35.4 44.6 35.4 43.1 33.9 40.0 41.9 37. 36.9 40.0 30.8 30.8 30.8 35.4 27.7 38.5 33.9 21.5 35.4 40.0 36.9 32.3 35.4 35.3 36.3 28.9 35.8 37.2 27.9 29.9 27.9 26.5 32.8 37.8 32.1 32.8 27.9 34.8 37.8 33.3 39.2 29.9 40.2 33.8 39.7 36.1 39.3 28.4 36.3 34.8 37.8 27.9 34.3 29.9 24.5 34.3 34.8 30.4 34.8 27.0 32.8 27.9 28.9 38.1 25.8 38.1 35. 33.0 33.0 25.8 29.9 32.0 40.2 40.2 33.0 32.0 39.2 39.2 37.1 36.1 28.9 38.1 38.1 39.2 41.4 41.4 28.9 36.1 33.0 39.2 28.9 38.1 28.9 29.9 39.2 35.0 33.0 46.4 30.9 35.0 29.9 41.1 34.6 31.8 33.6 39.2 23.4 27.1 29.9 23.4 33.6 35.5 24.8 32.7 24.3 30.8 36.5 29.9 42.1 30.8 42.1 29.9 40.2 31.2 37. 28.0 36.5 36.5 36.5 27.1 30.8 30.8 19.6 29.9 34.6 28.0 24.3 23.4 30.8 26.2 31.3 37.6 29.2 37.6 38.8 30.1 30.8 27.8 28.7 31.0 30.4 26.5 28.1 26.9 32.8 35.2 34.3 37.3 31.9 35.2 38.2 36.1 37.0 33.0 28.4 31.9 29.9 31.9 30.4 35.5 27.2 27.2 31.0 26.6 27.5 34.3 28.7 29.2 31.6 32.4 38.1 30.2 39.9 38. 29.5 30.6 28.5 28.1 29.9 30.6 26.8 28.1 25.3 32.0 35.6 33.5 37.7 31.3 35.6 38.1 36.3 37.1 33.3 28.8 31.3 29.2 31.0 31.0 35.2 25.6 27.1 31.3 25.6 26.0 34.2 28.1 28.1 30.6 TL. 25.9 35.2 24.1 25.9 38.9 33.3 31.5 24.1 31.5 37.0 29.6 25.0 27.8 35. 37.0 33.3 38.9 35.2 35.2 33.3 38.9 35.2 36.8 31.6 25.9 35.2 33.3 37.0 27.8 37.0 35.2 27.8 29.6 31.5 35.2 35.2 31.5 35.2 37.0 Table 6. Performance of different models on the three subset benchmarks, including overall results and scores for each task subtype. IC and CC denote Instance-Centric and Camera-Centric, respectively; Man. and Nav. represent Manipulation and Navigation; and TG and TL refer to Target Grounding and Time Localization, respectively. capabilities within indoor environments. As reported in the middle three columns of Tab.6, Gemini 3 Pro stands out with the strongest overall results on this benchmark. Performance on individual subtasks: QwenVL2.5-72B delivers the best results on Manipulation, while O3 and Gemini 3 Pro lead on Navigation. Notably, the Navigation task reveals substantially larger performance gaps between models and highlights key weakness of many open-source models. Grounding Bench. The Grounding Bench comprises 335 samples and requires models to localize either target objects or specific time points within video. Unlike traditional visual grounding or temporal localization benchmarks that mainly involve semantic referential grounding, our benchmark distinguishes itself by requiring spatial reasoning for all queries to correctly identify the target object or temporal segment. Naturally, this subset is divided into two components based on the type of grounding: target grounding and temporal localization. Within the Grounding Bench, Gemini 2.5 Flash achieves the strongest overall performance, excelling in both temporal localization and target-object identification. O4-mini demonstrates similarly strong temporal localization capabilities as well. These three benchmarks evaluate model performance within more fine-grained domains and categories, enabling targeted assessment of specific model capabilities. They also provide convenient evaluation protocol for models designed for particular domains or task types. 9. Conclusion We present MMSI-Video-Bench, diverse, humanannotated, holistic video-based spatial intelligence benchmark that evaluates models perception, understanding, reasoning, and decision-making over spatiotemporal information, complemented by three domain-oriented subbenchmarks that offer targeted perspectives. Our evalua13 tion reveals substantial gap between model and human performance, with models struggling across all task categories beyond spatial construction, and even spatially finetuned models failing to generalize effectively to our benchmark. Error analyses expose task-specific failure patterns that highlight concrete weaknesses in current models; our preliminary explorations further show that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains; and the frame-sampling study underscores the benchmarks difficulty and the need for more effective sampling strategies. Overall, MMSI-Video-Bench provides rigorous and holistic testbed for assessing spatial intelligence in video models, while our analyses offer actionable insights and directions for future improvements. 10. Acknowledgement This work is funded in part by the National Key R&D Program of China, and Shanghai Artificial Intelligence Laboratory."
        },
        {
            "title": "Appendix",
            "content": "A. Benchmark Details A.1. Task Formulation Details . A.2. Data Collection & Preprocessing Details A.3. Human Annotation UI . A.4. More Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Experiment Details A. Benchmark Details A.1. Task Formulation Details 1 1 1 1 2 As defined in the main paper, MMSI-Video-Bench is organized into five main categories: Spatial Construction, Motion Understanding, Planning, Prediction, and Cross-Video Reasoning. Spatial Construction evaluates the spatial attributes of instances and scenes, as well as the pairwise spatial relations among instances, scenes, and the camera. Motion Understanding is further divided into three aspects: camera motion, instance motion, and inter-instance interactive motions. Cross-Video Reasoning encompasses two subtypes: Memory Update and Multi-View Integration. In total, MMSI-Video-Bench consists of 5 main categories and 13 subtypes, with their detailed definitions summarized in Fig.13. A.2. Data Collection & Preprocessing Details Our benchmark is constructed from 25 publicly available video datasets, complemented by additional videos captured and collected by ourselves. During the data preprocessing, we perform filtering to remove clips that are"
        },
        {
            "title": "FPS",
            "content": "Duration(Sec.) Roomtour3d [15] ScanNet [7] ScanNet++ [48] 3RScan [37] ARKitScenes [2] RealEstate10k [53] DL3DV [29] Waymo [34] NuScenes [3] OVIS [32] TrackingNet [31] LaSOT [9] UAV123 [30] Ego4D [14] EPIC-KITCHENS [8] EgoExoLearn [18] MultiSports [25] charades [33] LEMMA [20] TF2023 [52] CVMHT [16] AVA [16] DROID [22] RH20T [11] DTU [19] RealWorld Indoor Scan. Indoor Scan. Indoor Scan. Indoor Scan. Indoor Scan. Indoor Scan. Indoor&Outdoor Outdoor Env. Outdoor Env. Outdoor Env. Outdoor Env. Outdoor Env. Outdoor Env. Ego.-Int. Ego.-Int. Ego.-Int. Exo.-HA. Exo.-HA. Exo.-HA. Exo.-HA. Exo.-HA. Exo.-HA. Others Others Others Indoor&Outdoor 1.0 1.0 2.0 1.0 1.0 0.66 1.0 5.0 4.0 5.0 4.0 5.89 5.89 2.0/8.33 2.0/8.33 4.0 2.0/8.33 4.0 12.50 2.0 4.0 1.0 4.0 4.0 2.0 2.0 466.86 39.92 136.35 60.94 77.28 214.73 39.81 15.92 26.79 13.74 21.33 32.40 24.32 262.49 91.51 565.81 20.51 27.91 22.84 492.46 37.25 900.27 86.75 84.32 24.00 46.43 Table 7. Statistics of all source video datasets, including their capture types, average durations, and standardized FPS after preprocessing.Scan./Env. denotes scanning, environment.; Exo.-Int. denotes egocentric interactions and Exo.-HA. denotes exocentric human activities. too short in duration, and for datasets originally provided in frame format, we reconstruct videos by concatenating frames according to the FPS specified in their corresponding papers. Each video dataset falls into one of several capture types, including indoor scanning, outdoor environment, egocentric interactions, exocentric human activities, and other categories. As mentioned in the main paper, we standardize the frame rate for each video category to an appropriate value that ensures no key information is lost. The capture types, frame-rate settings, and average duration statistics for each category are summarized in Tab. 7. A.3. Human Annotation UI As shown in Fig.12, we provide dedicated UI tool for both annotation and validation. The interface allows users to switch between Annotation Mode and Validation Mode. In the annotation mode, annotators can select question type, choose the corresponding video, and determine appropriate start/end frames to construct question. The system supports questions in either pure text or text+image format. Annotators then design the answer options, assign the correct answer, and provide the reasoning behind it. The UI clearly displays timestamps corresponding to each frame, helping annotators position temporal cues precisely. Additionally, to improve annotation efficiency, we provide Figure 11. (Left) Word cloud of MMSI-Video-Bench. (Right) Distribution of video source categories across all samples in MMSIVideo-Bench. Video Browsing Assistant Tool, enabling quick coarse-level navigation and preview of video content. In the validation mode, validators are randomly assigned samples annotated by others. They can inspect the loaded annotation and choose to Accept or Reject it. For rejected samples, the validator is required to provide reasons and suggestions for revision. A.4. More Statistics Fig.11 illustrates word cloud of the benchmark annotations, together with the distribution of video source capture types. B. Experiment Details In our evaluation, all models are provided with the same input template. As illustrated in Fig.14, the system prompt specifies the timestamp information for each frame and enforces the required output format. The user message injects, in order, the video, brief task description, and the question with its options into the template. The expected output format adapts to the evaluation setting (e.g. During error analysis we require models to produce both an answer and reason to facilitate failure localization, while under Chainof-Thought prompting we require the model to emit each intermediate thinking step in addition to the final answer.) For model outputs, we employ general-purpose answer extraction function to parse the predicted answers. This ensures consistent extraction across different models, and we verify that all correct responses produced by any model can be accurately detected and extracted. 2 Figure 12. User interface for annotation and quality validation in MMSI-Video-Bench. 3 Figure 13. Task Categories and Subtypes in MMSI-Video-Bench. Inst. denotes instance, Cam. denotes camera and Rel. denotes relationship. Figure 14. Structure of system and user prompts used in the experiments."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. 2 [2] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitscenes - diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. 5, 1 [3] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving, 2020. 5, 1 [4] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities, 2024. 2, 3 [5] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. 2 [6] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language models, 2024. 2, 3 [7] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niener. Scannet: In Richly-annotated 3d reconstructions of indoor scenes. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. 5, [8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epickitchens dataset. In European Conference on Computer Vision (ECCV), 2018. 1 [9] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu, Yong Xu, Chunyuan Liao, Lin Yuan, and Haibin Ling. Lasot: high-quality large-scale single object tracking benchmark, 2020. 5, 1 [10] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, and Rakesh Ranjan. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction, 2025. 7 [11] Hao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu, Chenxi Wang, Junbo Wang, Haoyi Zhu, and Cewu Lu. Rh20t: comprehensive robotic dataset for learning diverse skills in one-shot. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 653660. IEEE, 2024. 1 [12] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 3, 9 [13] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive, 2024. [14] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video, 2022. 5, 1 [15] Mingfei Han, Liang Ma, Kamila Zhumakhanova, Ekaterina Radionova, Jingyi Zhang, Xiaojun Chang, Xiaodan Liang, and Ivan Laptev. Roomtour3d: Geometry-aware videoinstruction tuning for embodied navigation, 2025. 5, 1 [16] Ruize Han, Wei Feng, Jiewen Zhao, Zicheng Niu, Yunjun Zhang, Liang Wan, and Song Wang. Complementary-view multiple human tracking. In AAAI Conference on Artificial Intelligence, 2020. 1 [17] Yuping He, Yifei Huang, Guo Chen, Baoqi Pei, Jilan Xu, Tong Lu, and Jiangmiao Pang. Egoexobench: benchmark for firstand third-person view video understanding in mllms, 2025. 3 [18] Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang, Dong Lu, Yali Wang, 5 Limin Wang, and Yu Qiao. Egoexolearn: dataset for bridging asynchronous egoand exo-centric view of procedural In Proceedings of the IEEE/CVF activities in real world. Conference on Computer Vision and Pattern Recognition, 2024. 1 [19] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola, and Henrik Aans. Large scale multi-view stereopsis evaluation. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 406413. IEEE, 2014. 5, 1 [20] Baoxiong Jia, Yixin Chen, Siyuan Huang, Yixin Zhu, and Song-Chun Zhu. Lemma: multiview dataset for learning multi-agent multi-view activities. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. 1 [21] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. 2, [22] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, Vitor Guizilini, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Muhammad Zubair Irshad, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Martn-Martn, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. Droid: large-scale in-the-wild robot manipulation dataset. 2024. 5, 1 [23] Anastasia Kirillova, Eugene Lyapustin, Anastasia Antsiferova, and Dmitry Vatolin. Erqa: Edge-restoration quality In Proceedings of assessment for video super-resolution. the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. SCITEPRESS - Science and Technology Publications, 2022. 2 [24] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multimodal video understanding benchmark, 2024. 3 [25] Yixuan Li, Lei Chen, Runyu He, Zhenzhi Wang, Gangshan Wu, and Limin Wang. Multisports: multi-person video dataset of spatio-temporally localized sports actions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1353613545, 2021. 5, 1 [26] Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, and Jiaqi Wang. Ovo-bench: How far is your video-llms from real-world online video understanding?, 2025. 3 [27] Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Sti-bench: Are mllms ready arXiv for precise spatial-temporal world understanding? preprint arXiv:2503.23765, 2025. 2, [28] JingLi Lin, Chenming Zhu, Runsen Xu, Xiaohan Mao, Xihui Liu, Tai Wang, and Jiangmiao Pang. Ost-bench: Evaluating the capabilities of mllms in online spatio-temporal scene understanding. arXiv preprint arXiv:2507.07984, 2025. 2, 3, 6, 9, 10 [29] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 5, 1 [30] Matthias Mueller, Neil Smith, and Bernard Ghanem. benchmark and simulator for uav tracking. In Computer Vision ECCV 2016, pages 445461, Cham, 2016. Springer International Publishing. 5, 1 [31] Matthias Muller, Adel Bibi, Silvio Giancola, Salman AlSubaihi, and Bernard Ghanem. Trackingnet: large-scale dataset and benchmark for object tracking in the wild, 2018. 5, 1 [32] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip H. S. Torr, and Song Bai. Occluded video instance segmentation: benchmark, 2022. 1 [33] Gunnar A. Sigurdsson, Gul Varol, Xiaolong Wang, Ivan Laptev, Ali Farhadi, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. ArXiv e-prints, 2016. [34] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 5, 1 [35] Xi Tang, Jihao Qiu, Lingxi Xie, Yunjie Tian, Jianbin Jiao, and Qixiang Ye. Adaptive keyframe sampling for long video understanding, 2025. 2, 9 6 understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 2 [50] Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yujie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, and Li Zhang. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv preprint arXiv:2503.22976, 2025. 2, 3 [51] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Llava-video: Video instruction tuning with synthetic data, 2025. [52] Ziwei Zhao, Yuchen Wang, and Chuhua Wang. Fusing personal and environmental cues for identification and segmentation of first-person camera wearers in third-person views. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1647716487, 2024. 1 [53] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images, 2018. 5, 1 [54] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. 2 [36] CVBench Team. Cvbench: benchmark for cross-video multimodal reasoning, 2025. 3 [37] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Niener. Rio: 3d object instance relocalization in changing indoor environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76587667, 2019. 5, 1 [38] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer, 2025. [39] Shaoguang Wang, Ziyang Chen, Yijie Xu, Weiyu Guo, and Hui Xiong. Less is more: Token-efficient video-qa via adaptive frame-pruning and semantic graph integration, 2025. 6 [40] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025. 7 [41] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. 3, 9 [42] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97779786, 2021. 3 [43] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. 2 [44] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin J. Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv preprint arXiv:2505.17015, 2025. 2, 3 [45] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171, 2024. 2, 3, 6, 10, [46] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, and Jiangmiao Pang. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. 2, 3, 6, 9, 10, 11 [47] Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Rouyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, and Yi Ma. Seeing from another perspective: Evaluating multi-view understanding in mllms. arXiv preprint arXiv:2504.15280, 2025. 3 [48] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Niener, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. 5, 1 [49] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for"
        }
    ],
    "affiliations": [
        "Beihang University",
        "Fudan University",
        "Shanghai AI Laboratory",
        "Shanghai Jiaotong University",
        "The Chinese University of Hong Kong",
        "University of California, Los Angeles",
        "University of Hong Kong",
        "Xian Jiaotong University",
        "Zhejiang University"
    ]
}