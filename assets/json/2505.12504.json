{
    "paper_title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models",
    "authors": [
        "Zongkai Liu",
        "Fanqing Meng",
        "Lingxiao Du",
        "Zhixiang Zhou",
        "Chao Yu",
        "Wenqi Shao",
        "Qiaosheng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at https://github.com/ModalMinds/MM-EUREKA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 4 0 5 2 1 . 5 0 5 2 : r CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models Zongkai Liu1,2 Fanqing Meng4* Lingxiao Du3* Zhixiang Zhou2* Chao Yu1 Wenqi Shao2,3 Qiaosheng Zhang2,3 1Sun Yat-Sen University 3Shanghai AI Laboratory 2Shanghai Innovation Institute 4Shanghai Jiao Tong University"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methodssuch as GRPO, REINFORCE++, and RLOOoften suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), novel algorithm designed to stabilize policy learning in LMs. CPGD introduces policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering robust alternative for RL in the post-training of LMs. We release our code at https://github.com/ModalMinds/MM-EUREKA."
        },
        {
            "title": "Introduction",
            "content": "Rule-based reinforcement learning (RL) has emerged as key approach for eliciting reasoning capabilities in language models (LMs) [1]. It leverages simple, efficient reward functions derived from deterministic rules, effectively mitigating reward hacking [2] while activating reasoning abilities of models [1, 3, 4, 5]. This has sparked line of research focused on developing more effective RL algorithms for both textual and general multimodal reasoning tasks. Notable methods include GRPO [1], REINFORCE++ [6], RLOO [7, 8], and GRPO variants such as DAPO [9], Dr.GRPO [10], and GPG [11]. However, we observe that these RL methods often suffer from training instability, which we attribute to the use of importance-sampling ratios in their loss functions. Although PPOclip loss [12] is commonly adopted to mitigate extreme policy updates, its one-sided nature fails to constrain large ratios when the advantage is negativepotentially causing gradient explosions dominated by poor samples, leading to catastrophic training collapse. We theoretically show that incorporating the importance-sampling ratio in the loss can amplify the policy shift, and our empirical results confirm that this can lead to training collapse in existing RL methods. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), an algorithm that replaces the PPO-clip loss with policy gradient loss [13] to avoid instability caused by directly involving policy ratios in the loss function. To ensure proximal optimization, we introduce both clip mechanism and policy drift regularizer, constraining optimization within local region Equal contribution Corresponding Authors: {zhangqiaosheng, shaowenqi}@pjlab.org.cn; yuchao3@mail.sysu.edu.cn Preprint. Under review. and mitigating over-optimization that may impair reasoning behaviors as shown in Section 4.2. Furthermore, we develop novel KL estimator that ensures correct gradient directions while avoiding the potential numerical instability associated with the commonly used k3 estimators [14]. We also incorporate weighted advantages to dynamically adjust the influence of each sample, further enhancing model performance. We theoretically prove the convergence of CPGD and empirically demonstrate its superior training stability and performance. As shown in Table 1, models trained with CPGD consistently outperform those trained with other RL algorithms and strong open-source baselines across standard multimodal reasoning benchmarks. Notably, CPGD improves the overall performance over the base model by +11.0% across all benchmarks. Specially, CPGD achieves +21.8% gain on the in-domain benchmark MMK12 [15], and improves by +8.5% and +11.4% on the out-of-distribution benchmarks MathVista [16] and MathVision [17], respectively."
        },
        {
            "title": "2 Related work",
            "content": "RL for training reasoning models. RL has become key method for improving reasoning in LMs [1, 18]. While early methods rely on PPO [12], its high computational cost has driven interest in alternatives like DPO [19], which simplifies training but depends on offline data. Recent RL methods such as GRPO, RLOO, and REINFORCE++ aim to balance stability and efficiency. Notably, DeepSeek R1 [1] shows that pure RL can elicit self-reflection and reasoning in LMs without supervised pretraining. Recently, several concurrent works have proposed GRPO variants to address its limitations. For instance, Dr.GRPO [10] identifies optimization bias in GRPO that favors longer response among incorrect ones. DAPO [9] incorporates multiple improvements, including decoupled clipping thresholds, token-level losses, and an online filtering strategy. GPG [11], in contrast, adopts minimalist design by discarding both clipping and KL regularization, relying solely on the policy gradient loss [13]. However, none of these approaches fundamentally resolve the training instability issue to existing RL methods, which is the primary focus of this work. Large reasoning model. Recently, surge of reasoning models has emerged, driven by the principle of test-time scaling laws, which demonstrate that models with explicit reasoning processes achieve superior performance [20]. Leading models in this area include DeepSeek R1 [1], OpenAIs oseries [18], Qwen series [21, 22], and Kimi k1.5 [23]. However, their training pipelines and datasets remain undisclosed. This has motivated wave of academic research within the open-source community, including parallel efforts such as OpenR1 [24], TinyZero [25], LMM-R1 [26], R1-V [27], Reason-RFT [28], and MM-Eureka [15]. These works primarily focus on constructing high-quality datasets and complete training pipelines. They commonly adopt GRPO to enhance reasoning capabilities but do not specifically investigate improvements to the RL algorithms themselves."
        },
        {
            "title": "3 Preliminaries",
            "content": "3.1 Problem formulation We denote an LM by πθ, where θ Rd represents the model parameters. Given prompt = [x1, . . . , xm] D, the model generates response = [y1, . . . , yn] by sampling from the conditional distribution πθ(x), with both xi and yi drawn from finite vocabulary V. In this work, we focus on transformer-based LMs that generate responses autoregressively, such that πθ(yx) = (cid:81)n i=1 πθ(yix, y<i), where y<i = [y1, . . . , yi1] and y<1 is an empty sequence. RL in post-training is typically modeled as Markov decision process (MDP), defined by tuple = (S, A, P, R, ρ), where is the state space, is the action space, is the transition kernel, is the deterministic reward function, and ρ is the initial state distribution. For LMs, two MDP formulations are widely considered: token-level MDP and response-level MDP. In token-level MDP, each token is treated as single action. At the time step t, the state st = [x, y<t] includes the prompt and the tokens generated so far. The action at = yt is sampled according to yt πθ(x, y<t), where the action space is equal to the vocabulary V. The environment transitions deterministically to st+1 = [x, y<t+1]. The reward is defined as R(st, at) = R([x, y<t], yt), and ρ is induced by the prompt distribution in D. In response-level MDP, the full response is treated as an individual action: = πθ(x). The state is defined solely by the prompt = x, and the episode terminates after one step. Thus, the transition kernel is omitted in the single-turn dialogue setting. The reward is R(s, a) = R(x, y), with ρ again determined by D. 3.2 Rule-based reinforcement learning This work focuses on verifiable tasks, where the outcome reward is determined by the final accuracy. Specifically, response receives reward of 1 if it is the correct answer to the prompt x, and 0 otherwise. We denote this reward function as Ro to emphasize its nature as an outcome-based reward. Within this setting, REINFORCE-style algorithms are favored as they reduce computational cost by forgoing critic networks. Notable methods include REINFORCE++ [6], RLOO [7, 8], and GRPO [1]. REINFORCE++: REINFORCE++ enhances the standard REINFORCE framework by integrating key optimizations from PPO [12], improving both stability and efficiency. The objective is defined as: LR++(θ; θold) = ExD,yπθold (x) (cid:34) 1 y (cid:88) i=1 (cid:32) min πθ(yix, y<i) πθold(yix, y<i) AR++ , clip1+ϵ 1ϵ (cid:16) πθ(yix, y<i) πθold(yix, y<i) (cid:17) (cid:33)(cid:35) AR++ , where ϵ [0, 1], clipb a(x) := max(min(x, b), a), and AR++ := GlobalNorm (cid:16) (cid:17) G(x, yi) , G(x, yi) := Ro(x, y) β (cid:88) j=i ln πθold(yjx, y<j) πref(yjx, y<j) . Here, ln πθold (yj x,y<j ) πref(yj x,y<j ) policy πref, typically the initial model. GlobalNorm(x) = xmean({x batch}) operation across the global batch for all prompts. std({x batch}) is the token-level KL penalty, constraining divergence from the reference is the normalization RLOO: The primary distinction between RLOO and REINFORCE++ lies in their computation of the advantage value. RLOO first generates group of responses {y(k)}K k=1 for each prompt and computes the advantage using leave-one-out strategy to reduce the gradient variance: ARLOO i,k := GlobalNorm (cid:17) (cid:16) G(x, y(k) ) , G(x, y(k) ) := G(x, y(k) ) 1 1 (cid:88) k=k G(x, y(k) ). GRPO: GRPO introduces group-based advantage and employs an external KL regularization via the k3 estimator [14], which approximates DKL(p, q) = (cid:80) (cid:32) i(qi/pi 1 ln qi/pi). The loss is: (cid:32) (cid:34) LGRPO(θ; θold) = xD,{y(k)}K k=1πθold (x) + min where (cid:16) πθ(y(k) πθold(y(k) x, y(k) <i ) x, y(k) <i ) AGRPO , clip1+ϵ 1ϵ 1 K (cid:88) k=1 y(k) (cid:88) 1 y(k) β Mi θ,ref(x, y(k)) i=1 (cid:16) πθ(y(k) πθold(y(k) x, y(k) <i ) x, y(k) <i ) (cid:17) AGRPO (cid:33)(cid:35) (cid:17) , AGRPO := GroupNorm(Ro(x, y(k))) = Mi θ,ref(x, y(k)) := πref(y(k) πθ(y(k) x, y(k) <i ) x, y(k) <i )"
        },
        {
            "title": "4 The proposed method",
            "content": "Ro(x, y(k)) mean({Ro(x, y(k))}K std({Ro(x, y(k))}K x, y(k) <i ) x, y(k) <i ) πref(y(k) πθ(y(k) k=1) . 1 ln k=1) , This section introduces our RL algorithm, Clipped Policy Gradient Optimization with Policy Drift (CPGD), designed to improve the stability of RL training. In Section 4.1, we present the CPGD 3 algorithm along with its theoretical guarantees, and highlight potential limitations of the standard PPO-clip loss. In Section 4.2, we provide empirical evidence of instability in existing methods and analyze its possible causes, showing how CPGD addresses them for more stable training. Finally, Section 4.3 describes the practical implementation of CPGD, striking balance between theoretical soundness and practical implementation. 4.1 Clipped Policy Gradient Optimization with Policy Drift (CPGD) Under the response-level MDP assumption, CPGD aims to maximize the following formula: LCPGD(θ; θold) = ExD (cid:104) Eyπθold (x) (cid:105) (cid:2)Φθ(x, y)(cid:3) α DKL(πθold, πθx) , (1) where Φθ(x, y) := min (cid:16) ln πθ(yx) πθold (yx) ACPGD(x, y) := Ro(x, y) Eyπθ(x) (cid:2)Ro(x, y)(cid:3), ACPGD(x, y), clipln(1ϵ) ln(1ϵ) (cid:16) ln (cid:17) πθ(yx) πθold (yx) ACPGD(x, y) (cid:17) , DKL(πθ, πθx) := Eyπ θ(x) (cid:104) ln πθ(yx) πθ(yx) (cid:105) . Hereinafter, we term the KL divergence between the old and current policies as policy drift, and between the current and reference policies as reference constraint. CPGD differs from the standard PPO-clip loss in two key aspects: (1) different policy optimization objective is used, where the policy gradient loss is adopted with the clip mechanism. (2) policy drift is introduced, imposing forward KL divergence penalty between the old and current policies. Why use the policy gradient objective? In the original PPO objective, although the importancesampling ratio corrects for the distribution mismatch between the old and current policies, it simultaneously introduces high variance. As empirically demonstrated in Section 4.2, such variance can destabilize training and even cause training collapse, while using policy gradient loss without the ratio substantially improves training stability. Proposition 1 further provides theoretical explanation for this phenomenon, showing that the use of the policy ratio amplifies policy drift, causing the updated policy to exceed the intended bounds. Why introduce the policy drift and clip mechanism? The introduction of the clip mechanism and policy drift is designed to ensure proximal policy updates, which are critical for theoretical convergence guarantees in Theorem 1. The clip mechanism enforces local updates by zeroing gradients when the policy ratio exceeds specified threshold, while policy drift introduces corrective gradient to constrain the policy ratio within stable range. Notably, the clip mechanism alleviates the need for large penalty coefficient on the policy drift term: when the ratio remains within bounds, the small drift coefficient allows the algorithm to focus on optimizing the primary objective Φ; when the ratio exceeds the range, the gradient of the primary objective becomes zero, prompting the algorithm to rely on the policy drift signal to prevent further deviationparticularly those caused by optimizer momentum (e.g., Adam) or neural network generalization effects. Proposition 1. Let θ0 be parameter such that the importance-sampling ratio satisfies πθ0 (yx) 1 = ϵ. Consider updating θ0 using either (i) the PPO-clip objective, resulting in parameter θPPO (ii) the CPGD objective with α = 0 (denoted as CPG), yielding parameter θCPG constant ηmax > 0 such that for any learning rate η (0, ηmax), the following inequality holds: πθold (yx) , or . Then, there exists 1 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 πθPPO (yx) πθold(yx) (cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) > (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 πθCPG (yx) πθold(yx) (cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) > (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) πθ0 (yx) πθold (yx) (cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) = ϵ. After one update step, both PPO and CPG increase the importance-sampling ratio deviation from the old policy, but PPO does so more aggressively than CPG. The following theorem further presents that CPGD enjoys the convergence guarantee, indicating its theoretical rationality. See Appendix for the proofs of Proposition 1 and Theorem 1. Theorem 1. Let {πθk } (Equation 1). Then, the sequence πθk converges. k=0 denote the sequence of policies generated by the CPGD update rule 4 Figure 1: Accuracy, clipping fraction and response length curves throughout training. 4.2 Training collapse Several studies suggest that the reference constraint may hinder policy improvement [9, 29]. However, we observe that removing this KL term leaves the PPO-clip loss alone insufficient to effectively constrain large policy shifts, which can lead to training collapse. While such collapse may be partially mitigated through techniques such as early stopping or small learning rates, it remains latent instability that undermines the reliability of continued training. In this subsection, we investigate this phenomenon of training collapse and demonstrate that CPGD effectively prevents it. Figure 1 presents training curves on the MMK12 dataset [15] for RLOO, REINFORCE++, GRPO, GRPO w/o clip (i.e., GRPO without the clip mechanism), GRPO w/ dual clip (i.e., the policy ratio is additionally clipped to no more than constant3.0 in our casewhen advantage is negative [30]), GRPO w/ drift (i.e., GRPO with policy drift), PG (basic policy gradient), CPG (PG with the clip mechanism), PGD (PG with the policy drift), and CPGD, all without the reference constraint. We use QwenVL2.5-7B [31] as the base model. All algorithms share the same hyperparameters: training and rollout batch size of 128, 8 responses per prompt, learning rate of 1e6, one PPO epoch, and ten training episodes. As shown in Figure 1, almost all baselines experience training collapse. As shown in Figure 1, methods such as REINFORCE++, RLOO, GRPO w/o clip, and GRPO exhibit highly unstable policy ratio dynamics, leading to training collapse in mid stages. In contrast, GRPO w/ dual clip, GRPO w/ drift, PG, CPG, PGD, and CPGD maintain stable training curves. GRPO w/ dual clip mitigates instability by globally constraining the policy ratio, while the PG series sidesteps ratio-induced variance by excluding it from the loss computation. These comparisons indicate that incorporating policy ratios in the loss can introduce high variance during fluctuations, and that simple one-sided clipping fails to recover from extreme ratios, ultimately causing collapse. Although dual clip mechanism stabilizes training, it may introduce new issues: frequent zero-gradient updates and ineffective learning under negative advantages due to the zero-gradient clipped large ratios. Additionally, GRPO w/ drift demonstrates that incorporating policy drift effectively constrains the policy ratio within reasonable range, thereby preventing training collapse. On the other hand, while prior work suggests clipping may be unnecessary due to the low proportion of clipped ratios [8, 11], our findings suggest otherwise. Despite only 1% of ratios being clipped, training performance diverges significantly with and without clipping. Specifically, methods like PG and PGDthough stable without ratio termssuffer from response length collapse, degenerating into trivial outputs (e.g., only emitting tokens like <think>) that exploit the format reward function without performing meaningful reasoning. This highlights the models vulnerability to reward hacking, likely due to overly aggressive updates. These results reveal the necessity of the proximal policy updates. 4.3 Implementation In this subsection, we design practically implementable loss function in per-token form based on the CPGD update formulation (Equation 1), aiming to strike balance between theoretical rigor and empirical applicability. Our CPGD loss is straightforward to be integrated into widely-used large model training frameworks such as OpenRLHF [32] and veRL [33]. The practical loss function is given by JCPGD(θ) = 1 (cid:88) (x,{y(k)}K k=1)D 1 k=1 y(k) (cid:80)K (cid:34) y(k) (cid:88) (cid:32) i=1 5 Φi θ(x, y(k)) α θold,θ(x, y(k)) (cid:33)(cid:35) , (2) where the per-token policy optimization term is (cid:32) Φi θ(x, y):= min ln and πθ(yix, y<i) πθold (yix, y<i) ACPGD ω (cid:16) (x, y), clipln (1+ϵi) ln (1ϵi) ln (cid:17) πθ(yix, y<i) πθold(yix, y<i) ACPGD ω (cid:33) , (x, y) ACPGD ω (x, y(k)) := ω(x) (cid:16) θold,θ(x, y) := min Ro(x, y(k)) mean (cid:0){Ro(x, y(k))}K (cid:17) 1, ln πθ(yix, y<i). k=1 (cid:16) sg(πθ(yix, y<i)) πθold(yix, y<i) (cid:1)(cid:17) , Here, sg() denotes the operation that prevents gradient computation, ω(x) is per-prompt weighting factor, and > 0 is constant. We provide the following clarifications regarding the differences between the theoretical update formulation (Equation 1) and the practical loss (Equation 2): (I) Policy optimization term: In the theoretical update (Equation 1), the policy optimization term is written in the form of joint distribution. But in the practical implementation (Equation 2), it is decomposed into token level using the decomposability of the logarithm function. Specifically, the clipping threshold ϵi can be set the same for all tokens, ensuring that each token shares the same clip range. Alternatively, tight-to-loose schedule can be employed such as ϵi = λϵ + (1 λ)ϵ i/y(k), which assigns smaller thresholds to earlier tokens that usually have higher variance. (II) Policy drift: Similar to the policy optimization term, policy drift also leverages the decomposability of the logarithm function, but applies the following further transformations: DKL(πθold, πθx) = Eyπθold (x) (cid:104) ln (cid:105) πθold (yx) πθ(yx) = Eyπθold (x) (cid:88) (cid:104) i=1 ln πθold(yix, y<i) πθ(yix, y<i) (cid:105) = Eyπθold (x) (cid:88) (cid:104) i=1 (cid:16) πθ(yix, y<i) πθold(yix, y<i) 1 ln πθ(yix, y<i) πθold(yix, y<i) (cid:17)(cid:105) . (3) (4) Equations 3 and 4 correspond to the k1 and k3 KL estimators proposed by Schulman [14]. In practice, particularly when using gradient optimizers such as Adam, we prefer the k3 estimator over k1, as k1 fails to effectively constrain the policy drift, while the gradient direction of k3 dynamically adjusts based on the relative magnitude between the current and old policies: θ ln πθold(yix, y<i) πθ(yix, y<i) = θ ln πθ(yix, y<i), θ (cid:16) πθ(yix, y<i) πθold(yix, y<i) 1 ln πθ(yix, y<i) πθold(yix, y<i) (cid:17) = (cid:16) πθ(yix, y<i) πθold(yix, y<i) (cid:17) 1 θ ln πθ(yix, y<i). (5) However, Equation 4 involves the policy ratio, which can potentially lead to training collapse as discussed in Section 4.2. To mitigate this issue, we clip the policy ratio to be no greater than + 1. Importantly, this clipping is not applied directly to the KL divergence estimator in Equation 4, but rather to its gradient (Equation 5). This design ensures that when the ratio exceeds the threshold, the policy drift term continues to provide gradient that reduces the ratio: when sg(πθ(yix,y<i)) πθold (yix,y<i) 1 > c, θE θold,θ(x, y) = θ ln πθ(yix, y<i). In contrast, if clipping were applied to the estimator itself, the resulting gradient θ ln πθ(yix, y<i) would further increase the ratio once it exceeds the threshold, exacerbating training instability. (III) Weighted advantage: In the view of the response level, each prompt can be viewed as distinct task. Consequently, we can introduce per-prompt weighting factor ω(x) to assign different levels of importance to different prompts. (1) Equal weight: when ω(x) = 1, ACPGD reduces to the original unweighted form. (2) STD weight: when ω(x) = 1/ std({R(x, y(k))}k), ACPGD is the same as ω AGRPO. (3) Clip-filter-like weight: when ω(x) = min(cω, #{xDstd({Ro(x,y(k))}k)=0} ), cω > 0, similar weighting strategies have also been explored in concurrent work [11], with an analogous effect to online filtering [34], amplifying the gradient contribution of samples with non-zero advantage. #{xD} ω"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experiments setup RL baselines, dataset and implementation details. We compare our CPGD with several widely used RL algorithms, including GRPO [1], REINFORCE++ [6] and RLOO [8] on the MMK12 training dataset [15], which contains 15,616 multimodal math problems with verified answers. All RL algorithms use QwenVL2.5-7B as the base model, trained under the same hyperparameters: rollout and training batch sizes of 128, 8 sampled responses per prompt (temperature 1.0), learning rate of 1e6, one PPO epoch, and five training episodes. No reference policy constraint is applied during training, and final performance is reported using the last checkpoint. In our system prompt, reasoning steps and final answers are explicitly marked using <think> and <answer> tags, respectively (see Appendix B). Benchmarks, model baselines and Overall metric. We evaluate all algorithms on six widely used benchmarks: MathVista (testmini) [16], MathVerse (testmini) [35], MathVision (test) [17], OlympiadBench (EN-OE split) [36], WeMath [37] and MMK12 [15]. MathVista covers visual QA, logic, algebra, and geometry; MathVerse focuses on mathematically grounded visual understanding; and MathVision extends to abstract visual reasoning. OlympiadBench targets graduate-level competition problems, while WeMath enables fine-grained diagnostic analysis via hierarchically annotated tasks. MMK12 provides 500 multiple-choice questions per subject across math, physics, chemistry, and biology for cross-domain performance evaluation. We also include several multimodal models as baselines. We evaluate open-source models of comparable model size, trained with various strategies, including QwenVL2.5-7B [31], InternVL2.5-8B [38], InternVL2.5-MPO-8B [39], R1-OneVision [40], OpenVLThinker [41], and MM-Eureka [15], which collectively represent the average performance of this model size across the evaluated benchmarks. We further evaluate the leading closed-source models such as GPT-4o [42] and OpenAI-o1 [18] to represent the most outstanding performance that the current state-of-the-art model can achieve on these benchmarks. To capture overall model performance across benchmarks, we define an Overall metric by j=1 Xj/X Qwen normalizing each score against strong baseline, QwenVL2.5-7B: Overall := 1 , where Xj and Qwen are the model and baseline scores on benchmark j. (cid:80)N 5.2 Main results Table 1: Performance comparison of various 7B/8B models and leading closed-source models. Top performer is in bold and second-best is underlined (excl. OpenAI-o1/GPT-4o). Model MathVista MathVerse MathVision Olypamid WeMath MMK12 Overall Leading models GPT-4o OpenAI-o1 Similar-size models InternVL2.5-8B QwenVL2.5-7B InternVL2.5-MPO-8B R1-Onevision (7B) OpenVLThinker (7B) MM-Eureka (7B) 63.8 73.9 64.4 68.2 68.9 64.1 70.2 73.0 50.2 57. 39.5 47.9 35.5 47.1 47.9 50.3 Different RL algorithms on QwenVL2.5-7B RLOO REINFORCE++ GRPO CPGD (clip-filter-like) CPGD (STD weight) 48.3 45.5 51.4 51.4 50.6 68.6 63.9 70.3 73.4 74.0 35.0 68.0 12.3 20.2 7.8 17.3 20.1 20. 19.5 17.8 18.5 21.5 21.4 68.8 98.7 53.5 62.1 53.5 61.8 64.3 66.1 65.8 66.7 67.4 70.2 68.3 49.9 73.9 45.6 53.6 34.5 39.8 60.6 64. 61.3 64.3 65.1 67.3 65.3 1.16 1.83 0.81 1.00 0.75 0.91 1.03 1.07 1.01 0.96 1.06 1.10 1.11 30.4 60.3 19.7 25.4 21.5 23.5 25.3 26. 23.0 18.2 25.9 25.9 28.3 7 Table 1 presents comprehensive comparison across multiple multimodal mathematical benchmarks. Closed-source models GPT-4o and OpenAI-o1 demonstrate strong performance across all tasks, with o1 achieving the highest scores overall, notably excelling on MathVision (60.3), Olypamid (68.0) and WeMath (98.7), establishing the current performance upper bound. Among similar-size open models, MM-Eureka shows competitive results. MM-Eureka achieves strong results on MathVista (73.0), MathVision (26.9) and strong result on MMK12 (64.5). However, our proposed CPGD consistently outperforms all similar-size baselines, achieving top or near-leading scores across all benchmarks, reflecting the effectiveness of our proposed RL algorithm. We further analyze different RL algorithms under the same setting as ours, including the base model, the training dataset, and the hyperparameters. Among baseline methods, GRPO outperforms RLOO and REINFORCE++ on most benchmarks, particularly on MathVerse (51.4) and MathVision (25.9). However, our proposed CPGD method significantly outperforms all baselines, achieving the best performance. Both variants of CPGD (using either clip-filter-like weights or STD-based weights) yield over +10% improvement in overall performance compared to the base model QwenVL2.5-7B. Notably, CPGD (STD weight) achieves +21.8% gain on the in-domain benchmark MMK12, and further demonstrates strong generalization with +8.5% and +11.4% improvements on the out-ofdistribution benchmarks MathVista and MathVision, respectively. These results demonstrate that CPGD serves as strong and robust alternative for RL in LM training. 5.3 Ablation study Table 2: Results of ablation studies. Top performer is in bold and second-best is underlined. Model MathVista MathVerse MathVision Olypamid WeMath MMK12 Overall CPGD (STD weight) 74.0 50.6 28. 21.4 68.3 65.3 1.11 Ablation study on the components (using STD weight) 67.8 PG 64.2 PGD 72.7 CPG 22.5 20.8 27. 42.0 41.1 52.3 Ablation study on the weighting factor unprocessed rewards equal weight clip-filter-like weight 69.1 73.1 73.4 40.2 51.1 51.4 21.8 27.2 25.9 8.0 7.5 20. 3.5 20.8 21.5 Ablation study on the reference constraint (using STD weight) 21.2 w/ reference constraint 21.0 71.8 50.0 58.6 58.3 70. 59.7 67.9 70.2 65.9 67.3 66.2 67.2 65.8 67.3 0.89 0.86 1.11 0.85 1.09 1.10 69. 65.8 1.05 Component ablation. We conduct ablation on key components of our method by comparing variants: PG (basic policy gradient), PGD (PG + policy drift), CPG (PG + clip mechanism), and CPGD. Results show that the clip mechanism plays the most critical role, as seen by the performance drop from CPG/CPGD to PG/PGD across nearly all benchmarks. This aligns with our observation in Section 4.2 that clipping mitigates the response length collapse issue, which otherwise can impair test-time computation and reasoning capabilities. In contrast, adding policy drift has relatively smaller effect. This is because CPGDs objective lacks potentially unstable importance-sampling ratio and already benefits from proximal updates via clipping, making policy drift mainly serve as safeguard against excessive ratio deviation. Weighting factor ablation. We further ablate different weighting strategies. We additionally include baseline that uses raw unprocessed rewards as advantages, which results in significant performance degradation. This confirms that subtracting the group mean is crucial for stable and effective learning. This approach prevents over-penalization of all responses in the failure cases, which may otherwise trigger squeezing effect [43], where the Softmax output head unintentionally reallocates probability mass to unexpected tokens, resulting in undesirable behaviors. Both clip-filter-like weight and STD weight outperform equal weighting, which we attribute to their ability to assign greater emphasis to samples with non-zero advantages. This targeted weighting encourages the model to focus more on informative training signals, thereby contributing to the improved performance. 8 Reference constraint ablation. Removing the reference constraint consistently improves performance, which echoes findings from recent studies [9, 10, 29], suggesting that such constraints may overly restrict policy improvement, and thus hinder overall optimization."
        },
        {
            "title": "6 Discussion",
            "content": "6.1 Importance sampling Importance sampling is valuable technique for correcting the sampling distribution when the learned policy and the behavior policy differ significantly, thereby improving sample efficiency. While we omit the importance-sampling ratio to reduce variance, we do not suggest discarding it entirely. In fact, we use single PPO epoch during training, widely recommended default [6, 15]. In our view, importance sampling can be omitted with one epoch but should be reintroduced when using more: ACPGD ω (x, y) clip1+ϵ 1ϵ (cid:16) sg(πθ(m1)(yix, y<i)) πθold(yix, y<i)) (cid:17) ACPGD ω (x, y), = 1, . . . , M, where πθ(m) denotes the updated policy after the m-th PPO epoch, and πθ(0) = πθold, and thus the final updated policy is θnew = θ(M ) after total epochs. Here, the truncated importance sampling weight is applied to correct the off-policy distribution. Notably, we use θ(m) rather than the real-time θ to avoid instability caused by frequent updates within single PPO epoch. This also ensures consistency with our proposed method. However, maintaining πθ(m) may incur additional cost, which we leave for future work to optimize. 6.2 Forward KL divergence vs. reverse KL divergence Our policy drift adopts the forward KL divergence DKL(πθold, πθx) instead of the reverse KL divergence DKL(πθ, πθold x). While forward KL has been explored before [12], it is considered less effective than PPO-clip. In contrast, reverse KL is more commonly used in theory because it is closely related to mirror descent and has strong convergence guarantees [44, 45]. Although these two KL forms are different in how they are calculated, they often lead to similar results in practice [46]. This is because both are used to control policy updates. In fact, the difference between their gradients turns out to be small when the policy ratio is small, which is usually the case during training as shown in Figure 1: θDKL(πθ, πθoldx) θDKL(πθold , πθx)Eyπθold (x) This approximation holds because ln 1 + 1 2 (x 1)2 when is close to 1. Despite their similarity, we prefer forward KL for two main reasons: (1) It avoids importance sampling, which reverse KL requires; and (2) It can be cleanly split into per-token terms (see Equation 4), which is not possible with reverse KL due to the importance weights. θ ln πθ(yx) 1 (cid:104) 1 2 (cid:16) πθ(yx) πθold (yx) (cid:105) . (cid:17)2 6.3 Exploitation vs. exploration Recent work [47] claims that the performance ceiling of model is determined by its base model, casting pessimistic view on the role of RL. While we do not fully agree or disagree, we offer more nuanced view: the exploration capability is largely determined by the base model. In RL training for LMs, the set of possible responses is constrained by what the base model can generate. RL helps it pick the best ones, boosting metrics like Maj@K. In other words, pretraining and SFT shape what the model can explore, while RL enhances the models exploitation ability. This work mainly aims to improve RL stability, but advancing LM reasoning capability requires improving both RL and earlier stages like SFT to expand the models exploration range. Encouraging active exploration may be key to unlocking further improvements in model performance."
        },
        {
            "title": "7 Conclusion",
            "content": "We identify critical source of instability in existing RL methods for LMs: the use of asymmetric clipping on importance-sampling ratios, which can result in training collapse. To address this, we 9 propose CPGD, principled alternative that avoids direct dependence on policy ratios while enforcing proximal updates through the clip mechanism and policy drift. CPGD further incorporates stable KL estimator and weighted advantage strategy to improve learning robustness. Theoretically grounded and empirically validated, CPGD demonstrates superior stability and performance across multimodal math benchmarks, offering strong and stable RL solution for training LMs."
        },
        {
            "title": "References",
            "content": "[1] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [2] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022. [3] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020. [4] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328, 2022. [5] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [6] Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. [7] Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 REINFORCE samples, get baseline for free! In Deep Reinforcement Learning Meets Structured Prediction, ICLR 2019 Workshop. OpenReview.net, 2019. [8] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pages 1224812267. Association for Computational Linguistics, 2024. [9] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, et al. Dapo: An open-source llm reinforcement learning system at scale, 2025. [10] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. [11] Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning, 2025. [12] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [13] Richard Sutton and Andrew Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [14] John Schulman. Approximating kl divergence, 2020. URL http://joschu. net/blog/kl-approx. html, 2023. [15] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning, 2025. [16] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. [17] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset, 2024. [18] OpenAI. Introducing openai o1. https://openai.com/o1/, 2024. Accessed: 2024-10-02. [19] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 10 [20] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. [21] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [22] Qwen Team. Qvq: To see the world with wisdom, December 2024. [23] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, et al. Kimi k1.5: Scaling reinforcement learning with llms, 2025. [24] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. [25] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [26] YingZhe Peng, Gongrui Zhang, Xin Geng, and Xu Yang. Lmm-r1. https://github.com/TideDra/ lmm-r1, 2025. Accessed: 2025-02-13. [27] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. [28] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning, 2025. [29] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Openreasoner-zero: An open source approach to scaling reinforcement learning on the base model. https: //github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025. [30] Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 66726679, 2020. [31] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. [32] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework, 2024. [33] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework, 2024. [34] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025. [35] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?, 2024. [36] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. [37] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning?, 2024. [38] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. [39] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization, 2024. 11 [40] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization, 2025. [41] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement, 2025. [42] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card, 2024. [43] Yi Ren and Danica J. Sutherland. Learning dynamics of LLM finetuning. In The Thirteenth International Conference on Learning Representations, 2025. [44] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. theory of regularized markov decision processes. In International conference on machine learning, pages 21602169. PMLR, 2019. [45] Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 56685675, 2020. [46] Chloe Ching-Yun Hsu, Celestine Mendler-Dünner, and Moritz Hardt. Revisiting design choices in proximal policy optimization, 2020. [47] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025."
        },
        {
            "title": "A Proofs",
            "content": "A.1 Proof for Proposition 1 Proposition 2. Let θ0 be parameter such that the importance-sampling ratio satisfies πθ0 (yx) 1 = ϵ. Consider updating θ0 using either (i) the PPO-clip objective, resulting in parameter θPPO (ii) the CPGD objective with α = 0, yielding parameter θCPG such that for any learning rate η (0, ηmax), the following inequality holds: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) (cid:12) (cid:12) πθold (yx) , or . Then, there exists constant ηmax > 0 πθCPG (yx) πθold(yx) πθPPO (yx) πθold(yx) πθ0 (yx) πθold (yx) (cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) = ϵ. (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) > > 1 1 1 1 After one update step, both PPO and CPG increase the importance-sampling ratio deviation from the old policy, but PPO does so more aggressively than CPG. 1 = θ0 + ηθ ˆLCPG(x, y; θ0) is the single gradient Proof. Consider (η) = ascent step on the empirical CPGD objective (Equation 1) without the policy drift term. The gradient of the objective takes the form: πθCPG πθold (yx) , where θCPG 1 (yx) θ ˆLCPG(x, y; θ) = ACPGD(x, y)θ ln πθ(yx). Thus, for the case where πθ0 (yx) η = 0 satisfies: πθold (yx) = 1 + ϵ and ACPGD(x, y) > 0, the directional derivative of at (0) = θπθ0(yx) πθold (yx) , θ ˆLCPG(x; θ0) > 0. Hence, there exists constant η1 > 0 such that for any η (0, η1), we have (η) > (0). Similarly, when πθ0 (yx) πθold (yx) = 1 ϵ and ACPGD(x, y) < 0, there exists η2 > 0 such that (η) < (0) for any η (0, η2). Therefore, for any 0 < η < min(η1, η2), the following holds: 1 πθCPG (yx) πθold(yx) 1 > πθ0(yx) πθold (yx) 1 = ϵ. (6) Next, define g(η) = (yx) πθCPG πθold (yx) 1 (yx) πθPPO πθold (yx) , where θPPO 1 1 = θ0 + ηθ ˆLPPO(x, y; θ0) and For the case where πθ0 (yx) θ ˆLPPO(x, y; θ) = ACPGD(x, y) θπθ(yx) πθold (yx) πθold (yx) = 1 + ϵ and ACPGD(x, y) > 0, we have: (cid:68) θπθ0(yx) πθold(yx) πθ(yx) πθold(yx) , ACPGD(x, y) (1 . g(0) = ) θ ln πθ(yx) < 0. (cid:69) Hence, there exists constant η3 > 0 such that g(η) < g(0) for any η (0, η3). Similarly, for the case where πθ0 (yx) πθold (yx) = 1 ϵ and ACPGD(x, y) < 0, there exists constant η4 > 0 such that g(η) > g(0) for any η (0, η4). Therefore, for any 0 < η < min(η3, η4), we have 1 πθPPO (yx) πθold (yx) 1 > 1 (yx) πθCPG πθold(yx) 1. (7) Therefore, by letting ηmax = min(η1, η2, η3, η4), the proof is complete. 13 A.2 Proof for Theorem 1 Theorem 2. Let {πθk } (Equation 1). Then, the sequence πθk converges. k=0 denote the sequence of policies generated by the CPGD update rule Proof. First, denote LCPGD(θ; θk) = ExD (cid:2)g(θ; θk, x)(cid:3), and rewrite as g(θ; θk, x) = Eyπθk (x) (cid:104) Ro(x, y) ln (cid:105) πθ(yx) πθk (yx) αDKL(πθk , πθx) Eyπθk (x) (cid:104) ReLU (cid:16)(cid:2) ln πθ(yx) πθk (yx) clip (cid:0) ln πθ(yx) πθk (yx) , ln(1 ϵ), ln(1 + ϵ)(cid:1)(cid:3)Ro(x, y) (cid:17)(cid:105) . Here, we omit the baseline Eyπθk (x)[Ro(x, y)]. Then, denoting θk+1 the point such that LCPGD(θk+1; θk) LCPGD(θk; θk), we obtain Eyπθk+1 (x) (cid:105) (cid:104) Ro(x, y) =Eyπθk (x) (cid:104) (cid:105) Ro(x, y) Eyπθk (x) 1(cid:1)Ro(x, y) (cid:105) (cid:105) Ro(x, y) Eyπθk (x) =g(θk+1; θk, x) g(θk; θk, x) + αDKL(πθk , πθk+1x) (cid:16)(cid:2) ln clip (cid:0) ln ReLU + Eyπθk (x) πθk+1(yx) πθk (yx) (cid:104) (cid:104)(cid:0) πθk+1(yx) πθk (yx) πθk+1(yx) πθk (yx) ln (cid:104) πθk+1(yx) πθk (yx) , ln(1 ϵ), ln(1 + ϵ)(cid:1)(cid:3)Ro(x, y) (cid:17)(cid:105) . Denoting the overall expected return by η(πθ) = ExD,yπθ(x) conclude (cid:104) Ro(x, y) (cid:105) , we integrate over to η(πθk+1) η(πθk ) αExD (cid:104) DKL(πθk , πθk+1x) (cid:105) Pinsker inequality α πθk+1 πθk 2 1. Because η(πθk ) is bounded, there exists η such that limk η(πθk ) = η. Thus, taking the limit of on both sides of the following equation, 0 α 2 πθk+1 πθk 2 1 η(πθk+1 ) η(πθk ), we can obtain limk πθk+1 πθk 1 = 0. Since the parameter space Θ is compact, the sequence {πθk } converges to some limit point πθ ."
        },
        {
            "title": "B Prompt setting",
            "content": "Table 3: Prompt setting. SYSTEM: Solve the question. The user asks question, and you solves it. You first thinks about the reasoning process in the mind and then provides the user with the answer. The answer is in latex format and wrapped in $...$. The final answer must be wrapped using the boxed{} command. The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think>Since 1 + 1 = 2, so the answer is 2. </think><answer>The answer is $boxed{2}$ </answer>, which means the final answer assistants output should start with <answer> and end with </answer>. USER: <image>{{question}} We follow the prompt format from DeepSeek-R1, where reasoning steps and final answers are explicitly marked using <think> and <answer> tags, respectively. The full prompt template is provided in Table 3."
        },
        {
            "title": "C Limitations",
            "content": "While this work introduces stable and effective RL method for LMs training, it has several limitations: (1) For the weighted advantage component, we conducted only preliminary experiments and did not thoroughly explore the impact of different weighting factors. Our results suggest that using non-uniform weights yields better performance than trivial equal weighting, but further investigation is needed. (2) Our study focuses on on-policy training; we leave off-policy settingswhere importance sampling is typically requiredfor future work. Ensuring training stability in the presence of importance sampling remains an open question. (3) All experiments were conducted on standard academic-scale models (7B parameters). We did not evaluate our method on larger models (e.g., 100B+), which would require significant computational resources."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "Sun Yat-Sen University"
    ]
}