{
    "paper_title": "Investigating Human-Aligned Large Language Model Uncertainty",
    "authors": [
        "Kyle Moore",
        "Jesse Roberts",
        "Daryl Watson",
        "Pamela Wisniewski"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent work has sought to quantify large language model uncertainty to facilitate model control and modulate user trust. Previous works focus on measures of uncertainty that are theoretically grounded or reflect the average overt behavior of the model. In this work, we investigate a variety of uncertainty measures, in order to identify measures that correlate with human group-level uncertainty. We find that Bayesian measures and a variation on entropy measures, top-k entropy, tend to agree with human behavior as a function of model size. We find that some strong measures decrease in human-similarity with model size, but, by multiple linear regression, we find that combining multiple uncertainty measures provide comparable human-alignment with reduced size-dependency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 8 2 5 2 1 . 3 0 5 2 : r Investigating Human-Aligned Large Language Model Uncertainty Kyle Moore Vanderbilt University kyle.a.moore@vanderbilt.edu Jesse Roberts Tennessee Tech University jtroberts@tntech.edu"
        },
        {
            "title": "Abstract",
            "content": "Recent work has sought to quantify large language model uncertainty to facilitate model control and modulate user trust. Previous works focus on measures of uncertainty that are theoretically grounded or reflect the average overt behavior of the model. In this work, we investigate variety of uncertainty measures, in order to identify measures that correlate with human group-level uncertainty. We find that Bayesian measures and variation on entropy measures, top-k entropy, tend to agree with human behavior as function of model size. We find that some strong measures decrease in human-similarity with model size, but, by multiple linear regression, we find that combining multiple uncertainty measures provide comparable human-alignment with reduced sizedependency."
        },
        {
            "title": "Introduction",
            "content": "There has been rise as of late in research interest in appropriate and effective measures of uncertainty in large language models (LLMs). Accurate measures of LLM uncertainty are essential for calibrating and improving trust in the models. In particular, uncertainty measures provide important context for assessing the safety of models as components in safety-critical systems and mitigating overtrust by end users. In both cases, uncertainty measures are necessary due to the common phenomenon of hallucination, pernicious behavior that has dominated discussions concerning LLMs both in the research community and the broader public. Prior works, which we survey extensively in section 3, have focused largely on uncertainty in terms of model task efficacy. It is comparatively rare that existing works discuss uncertainty in manner grounded in human behavior. Our work seeks to fill this gap by investigating whether and to what extent measures of LLM uncertainty match the uncertainty signature seen among human groups. Many have investigated human-like behavioral phenomena in LLMs. Importantly, this includes behaviors that are well accepted in psychological research as uncertainty-born, including typicality effects (Misra et al., 2021; Roberts et al., 2024c; Vemuri et al., 2024) and fan effects (Roberts et al., 2024b). To our knowledge, this is the first work that attempts to directly compare human and LLM uncertainty on paired task. Uncertainty quantification in LLMs has typically sought to characterize and calibrate uncertainty according to an objective standard of correctness. While this is useful in many contexts, human intuition of uncertainty is often disparate from the actual values. For example, while model would be justified in claiming it is mostly certain if it has certainty level of 60%, human users, based on typical usage of the phrase, may expect certainty level of 90%. This disparity may affect human propensity to place too much trust on model outputs. If there exists an uncertainty measure that is well calibrated to human uncertainty, referred to here as aligned, reporting such measure could yield benefits for human-computer interaction and human-AI collaboration. In this work, we make the following contributions: 1. We directly compare human uncertainty against diverse variety of LLM uncertainty measures, lending insight those that are aligned in general and those that are only aligned in large models. 2. We propose uncertainty measures that have not, to our knowledge, been investigated in LLMs, including nucleus size, top-k entropy, and choice entropy, as well as derivative uncertainty measures. Equal Contribution 3. We find evidence that mixtures of uncer1 tainty measures can provide comparable human alignment to the top isolated uncertainty measures without the associated size dependency."
        },
        {
            "title": "2 Background",
            "content": "This paper investigates uncertainty quantification (UQ) in large language models. Here we provide some background on LLMs and uncertainty, while survey of existing UQ methods will be reserved for section 3. 2.1 Large Language Models Current LLM architectures are based broadly on the transformer introduced by Vaswani et al. (2017). While the original transformer architecture utilized traditional encoder-decoder structure, current models are overwhelmingly encoder-only or decoder-only models. Encoder-only models, typified by BERT (Devlin et al., 2019), are largely relegated to masked language modeling tasks where one or more tokens at any position in given context are masked or otherwise unavailable. The models task is to predict the most appropriate tokens to insert in those masked positions. Most well-known LLMs are decoder-only models (Roberts, 2024). These are causal models which predict, given the current context, the most appropriate next token to add to the end of the current context. They are trained and used autoregressively by iteratively predicting new token, adding that new token to the original context, and repeating the process until either maximum number of tokens are generated or special stop token is predicted. We investigate uncertainty exclusively in decoder-only language models. Specifically, we examine the LLaMa 3.1 (Grattafiori et al., 2024), LLaMa 3.2 (AI, 2024), Mistral 0.1, and Mistral 0.3 (Jiang et al., 2023) models. For each of these, we investigate both the base completion and the instruction-finetuned (instruct) versions. The instruct models are finetuned to respond in conversational manner and follow directional instructions in addition to standard next-word prediction. Both varieties of given model see heavy usage in practice, with base models being used for specialized backend applications and instruct models being used in many user-facing applications. Another important distinction in LLMs is between black box and white box models. Black box models, typified by OpenAIs ChatGPT (OpenAI, 2022) and Anthropics Claude (Anthropic, 2024) models, do not provide access to models internal weights. Black box models often will only provide an output sequence and may not provide even the token probability distribution for each generation step. Because of these limitations, black box models, despite being the primary type of model with which users interact, are difficult to study in meaningful detail. White box models may be split into open-weight and open-source models. These are differentiated primarily in that open-weight models provide only model weights and architecture, while open-source releases additionally provide all code and data needed to train the model from scratch. In both cases, white box models allow users to inspect the internal structure, weights, and activation of models at inference time. This makes white box models more conducive and appropriate for research into model behaviors. Therefore, all models discussed in this work are open-weight white box models."
        },
        {
            "title": "2.2 Uncertainty",
            "content": "Uncertainty refers to broad collection of processes and behaviors in which decisions must be made without access to perfect information. Uncertainty quantification (UQ) is branch of research within machine learning (ML) that attempts to quantify the amount of uncertainty in system in order to inform downstream tasks, characterize task accuracy, etc. Uncertainty in deep ML models has been extensively studied (Gawlikowski et al., 2023), but the complexity of LLM behavior presents unique UQ challenges. We discuss the existing UQ methods for LLMs in section 3. Most extant research decomposes uncertainty based on its source into epistemic and aleatoric uncertainty. Epistemic uncertainty refers to uncertainty that results from missing or noisy information intrinsic to the model as result of the specifics of the training process. Aleatoric uncertainty instead describes uncertainty sourced from incomplete information about the task being performed, such as uncertainty due to imperfect sensors. In the context of LLMs, this is usually framed as distinction between knowledge left out of the training corpus (epistemic) and knowledge left out of the current context (aleatoric) (Beigi et al., 2024). This work does not make this distinction, instead seeking to quantify total uncertainty regardless of source."
        },
        {
            "title": "In line with the safety and hallucination focus",
            "content": "2 in much of the extant uncertainty research, many works seek to measure or improve models uncertainty calibration. Calibration refers to how closely models quantified uncertainty matches with its likelihood to output correct answer in given context. The goal is typically that the models measured uncertainty should match the amount of relevant knowledge to which the model has access. In the context of human-like uncertainty behaviors, calibration may be an irrelevant or inappropriate goal. Humans are capable of expressing both complete certainty and complete uncertainty regardless of correctness. We sidestep this issue entirely by focusing on contexts in which no available answer is more correct than any other. In particular, as described in section 4.1, we compare model uncertainty against human survey response data."
        },
        {
            "title": "3 Prior Work",
            "content": "Uncertainty quantification in LLMs is rapidly growing area of research that involves wide variety of methods and downstream goals. Of particular prevalence are works that seek to use uncertainty measures to facilitate model calibration to task accuracy (Lin et al., 2022; Mielke et al., 2022; Band et al., 2024; Tian et al., 2023; Chaudhry et al., 2024; Steyvers et al., 2025; Jiang et al., 2021) and to effectively guide human trust in the models outputs (Steyvers et al., 2025; Belém et al., 2024). Calibration refers to the problem of inducing models to, given chosen uncertainty measure, yield uncertainty levels in given task that correlate with the models accuracy on that task. Although the benefits of calibration are clear for general task completion, it is not clear that calibration is desirable for models that are intended to show human-like behavior. While uncertainty and task accuracy may be correlated in humans across variety of tasks, they are not necessarily linked. Humans are capable of displaying low uncertainty with low accuracy as well as high uncertainty despite consistently high accuracy (Dunning, 2011). To our knowledge, this work is the first to investigate LLM uncertainty aligned to human uncertainty in contexts where there is no correct answer. Related work also exists that compares human and LLM response patterns, though not uncertainty, on non-factual datasets. These works have primarily made such comparisons for the purposes of identifying model biases (Tjuatja et al., 2024), eliciting model opinions (Dominguez-Olmedo et al., 2024; Santurkar et al., 2023; Durmus et al., 2023), or simulating survey responses for downstream research (Argyle et al., 2023; Huang et al., 2025a). Of these, the most similar work to ours is Huang et al. (2025a), which used uncertainty-aware comparisons to generate human-like responses. Our work differs in that we compare numerous uncertainty measures with human responses to identify which measures direct and derived measures are best aligned to human uncertainty. 3.1 LLM Uncertainty Measurement Methods for measuring uncertainty in LLMs fall into at least five identifiable methodological categories: self-reporting, consistency, logit-based, entropy-based, and ensemble-based. Each of these has been explored to varying degrees and in variety of contexts. Self-reported uncertainty measures are the most common UQ method in the literature. These measures rely on the models to provide measure of their own uncertainty. These methods typically fall into two strategies. The more common strategy is the tendency of model to include in its output phrase indicating certainty (PIC). The PIC may be explicit (typically expressed as percentage) (Chaudhry et al., 2024; Shrivastava et al., 2023; Tian et al., 2023; Xiong et al., 2023; Belém et al., 2024) or implicit (typically expressed as one of collection of phrases; e.g. \"I think its ...\" or \"Im sure its ...\") (Zhou et al., 2023; Mielke et al., 2022; Band et al., 2024; Lin et al., 2022; Tang et al., 2024). Alternative to PIC-based strategies are selfassessment methods, which query the model in two or more stages. The first stage obtains an answer from the model to the initial query. For some variations, the second stage evaluates whether the initial answer is (in)correct (Lin et al., 2022; Kadavath et al., 2022; Chen and Mueller, 2024). Other variations instead generate multiple alternative answers in the second stage and, in third stage, query how many of the second-stage answers factually agree with the initial answer (Manakul et al., 2023; Zhang et al., 2024). Consistency-based UQ measures are common for black-box LLMs because it requires no access to model internals. In these measures, the model is queried repeatedly with the same context. The models certainty is determined by the percentage of responses that contain the most commonly provided answer (Kaur et al., 2024; Khan and Fu, 2024; Lin et al., 2023). These are similar to the lat3 ter self-reported methods, but they typically do not use multiple generation stages and often use manual inspection to determine answer overlap. Given large number of inferences, consistency-based methods are statistically guaranteed to converge to logit-based methods given identical prompting. This is because LLMs are deterministic apart from the separate method which chooses the next token from the probability density output by the LLM. Logit-based UQ measures directly inspect the models logit outputs or some transformation over those logits, most typically softmax to obtain probability density function (pdf) over the available tokens. The simplest methods use the relative probability of the most probable output as the level of certainty (Shrivastava et al., 2023; Jiang et al., 2021; Steyvers et al., 2025; Tian et al., 2023). Some work has sought to use surrogate models to approximate logit-based methods for black-box models (Shrivastava et al., 2023). Entropy-based methods measure uncertainty in model outputs using Shannon entropy, defined as (cid:80) xX p(x) log p(x), where is collection of candidate output tokens. Entropy-based methods have seen limited usage in LLM uncertainty estimation despite their simplicity and usage in other machine learning domains. While entropy is defined primarily for single-token generation, sentence-level entropy can be obtained most simply by taking the average or maximum entropy over all generation steps (Huang et al., 2025b). Kadavath et al. (2022) briefly introduces variant on sentence-level entropy based on the accumulation of token-wise entropy. Duan et al. (2024) extends this further by weighting each token by calculated relevance score. Ensemble methods are born out of stream of research into Bayesian uncertainty estimation. Traditional methods for general neural networks involve aggregating the predictions of multiple independently trained models (Lakshminarayanan et al., 2017; Xiao and Wang, 2021; Malinin and Gales, 2020), but this is nonviable for current LLMs due to high training time and costs. Methods that approximate this process have been developed using Low-Rank Adapters (LoRA) (Wang et al., 2023) or Monte Carlo dropout to generate ensembles at inference time (Roberts et al., 2024c; Gal and Ghahramani, 2016; Fomicheva et al., 2020). Other works have attempted to approximate ensembles for uncertainty estimation using test-time augmentation of contexts (Hou et al., 2024)."
        },
        {
            "title": "4 Methods",
            "content": "In this section, we will describe our experimental methodology. This will begin with discussion of how we constructed our dataset, followed by detailed description of each of the uncertainty measures tested, and finishing with the finer details of our experiment design decisions and prompting methodology. 4.1 Dataset Our data was manually collected from the Pew Research Centers American Trends Panel (ATP) Datasets (Center). We opted for human surveys on non-factual questions so that we could isolate the uncertainty correlations without potential confounds from differing knowledge access between the human subjects and the LLMs. In this way, we can compare the LLM uncertainty directly with human disagreement on questions without designated correct answer. We collected data from the the 20 most recent waves of survey data at time of writing, encompassing waves 113 through 132 and covering human data collection time period of August 1, 2022 through August 6, 2023. We filtered questions from the various waves according to the following criteria. We removed free-response and write-in questions that do not have predetermined set of allowed answers to facilitate cloze comparison. We omitted questions to which the human responses could change drastically over short time-frames, such as COVID-19 surveys or political approval ratings. We removed personal experience and history questions that model would not have the proper background data to support. Finally, we removed questions that targeted responses from specific subpopulations (e.g. Asian-American men under 30), which may show markedly different response distribution from untargeted populations. This included for omission some survey questions that provided response data for the total population, but split the results into demographic subgroups and did not provide subgroup sizes to permit reconstructing the total response distribution. The results of this process were 38 questions pulled from 8 survey waves, representing over 500000 human response data points. The minimum number of human responses to question was 5079, with maximum of 30861. The response distributions were diverse, including questions with near unanimous agreement and near complete disagreement. Question and answer texts were minimally edited from the versions given to human survey participants for agreement with the base query format. 4.2 Experiment Design All LLM experiments used shared base query format defined as shown in figure 1. For each question in the dataset, we insert the question text in place of <Q-TXT>. For each associated answer, we insert the i-th answers content in place of <A-TXT-i> and replace <A-LAB-i> with successive alphabetic labels such that <A-LAB-0> = A, <A-LAB-1> = B, and so on. base prompt Following is question and selection of answer choices. Provide the label for the answer with which you most agree. Question: <Q-TXT> <A-LAB-0>. <A-TXT-0> <A-LAB-1>. <A-TXT-1> Answer:"
        },
        {
            "title": "LLM\nvocabulary probability distribution",
            "content": "Figure 1: Prompt to measure presence/absence belief. The result is base query for each question in the dataset. We pass the base queries to the LLM to obtain probability distribution over the entire vocabulary. For measures that require the models chosen answer, we obtain this using cloze testing over the answer choice label tokens. The chosen answer choice is taken to be the answer choice whose label is more probable than any other choice label. For ensemble methods, cloze test is performed for each ensemble variant and the models chosen answer is taken to be the answer label most often chosen using cloze testing across the entire ensemble. This results in chosen answer and probability distribution over all tokens in the vocabulary. The next section details how these are used to calculate each of our chosen uncertainty measures."
        },
        {
            "title": "4.3 Uncertainty Measures",
            "content": "In our experiments, we used eight candidate measures of uncertainty. Those candidate uncertainty measures are labeled as self-reported (SR), re5 sponse frequency (RF), Nucleus Size (NS), vocabulary entropy (VE), choice entropy (CE), top-k entropy (KE), population variance (PV), and population self-reported (PS). With this set, each of the broad uncertainty measure categories is represented: self-report (SR, PS), consistency-based (RF), logit-based (RF, TP, PV), entropy-based (VE, CE, KE), and ensemble-based (PV, PS). In this section, we describe how each of these measures were obtained. 4.3.1 Self-reported Measures Our approach to SR uncertainty uses method inspired by Chen and Mueller (2024). They measure uncertainty in two-stage process. First, the model is queried using the base query to obtain chosen answer. This answer is then appended to the context followed by secondary query that evaluates the probability that the model evaluates the initial answer as correct, incorrect, or variety of other evaluator phrases. These probabilities are aggregated into final uncertainty measure. To account for the non-factual nature of the questions in our dataset, we borrow from (Roberts et al., 2024a) by using the evaluator phrases best and worst to gauge relative preference across available choices. Higher certainty should yield lower probability for worst and higher probability for best."
        },
        {
            "title": "4.3.2 Consistency and Logit Measures",
            "content": "RF uncertainty is measured as the probability of the label tokens associated with each answer choice. For single-token generation tasks, the probability of each label token multiplied by number of trials is the expected number of repeated inferences in which that token is generated by the model, meaning that this measure covers both logit-based uncertainty and an idealized variation on consistencybased uncertainty. Higher probability of the target token is taken to indicate higher certainty. The nucleus (commonly known as the top-p) of probability distribution is the set of the most probable tokens given the base query such that their summed probability is less than some specified probability threshold (Holtzman et al., 2019). NS is the number of tokens contained in the nucleus. This measure is equivalent to the credible interval, which is the Bayesian analog to confidence intervals. In line with this, we use nucleus threshold of 0.95. Intuitively, this measure represents the number of tokens that the model considers to be reasonable options, with larger NS (more reasonFigure 2: Correlation between uncertainty in human response data and LLM uncertainty across all uncertainty measures. Measures are ordered by mean correlation across models. similarly restricts the vocabulary to subset, Vc, that are pre-selected based on the task. In our experiments, Vc contains exactly the labels associated with the answer choices provided in the base query (Vc = A, B, for questions with three available answers.) In all cases, these provide measure of the imbalance in the probability distribution. High entropy indicates relatively equal probability across all candidate and thus low certainty. To our knowledge, KE and CE are novel uncertainty measures in the context of LLMs."
        },
        {
            "title": "5 Results",
            "content": "In this section, we present the results of our experiments described in section 4. We split our results and analysis into two phases. In the first phase, we evaluate to what degree the human and LLM uncertainty measures are similar in terms of correlation Figure 3: Correlation between measure humansimilarity and model size across all models. Measures are ordered by correlation with model size. able candidates) indicating higher uncertainty. To our knowledge, this is the first work to use NS as an indicator of uncertainty."
        },
        {
            "title": "4.3.3 Entropy Measures",
            "content": "The entropy methods - VE, CE, and KE - all use Shannon entropy over the vocabulary probability distribution given the base query. For VE, this is computed over the entire probability distribution = (cid:80) viV (viqb) log (viqb) where is the set of all vocabulary tokens and qb is the base query. CE and KE are variations on entropy that restrict the tokens considered to subset of the vocabulary. KE computes entropy over only the most probable tokens Vk, such that KE = (cid:80) (viqb) log (viqb). In our experiments, we chose to use fixed = 10. CE viVk 6 Figure 4: Accuracy of linear regression models trained on LLM measuered uncertainty and predicting human uncertainty. Top: 3-fold cross validation, dark green background bar indicates the mean correlation across models. Models ordered by mean correlation. Bottom: Results when model trained and tested on the entire dataset. across questions. In phase two, we evaluate the ability of the various measures to predict human uncertainty."
        },
        {
            "title": "5.1 Human-LLM Uncertainty Similarity",
            "content": "Our initial analysis of the various uncertainty measures is shown in Figure 2. For every question, the uncertainty in the human response data is taken as the entropy over the set of available answer choices, (cid:80) aA Ph(aq) log Ph(aq) where is the set of answer choice labels and Ph(aq) is the percent of human subjects who selected answer on question q. Figure 2 shows the correlation across all questions between the human uncertainty measure and each of the candidate LLM uncertainty measures, ordered by mean correlation across models. To obtain the correlation between human uncertainty and the LLM population, the standard deviation of the logit output across the population for each options is compared to the relative frequency of each answer among the human population. In line with existing literature (Hinkle et al., 2003), we set significance threshold of 0.3. Only measures KE, PV, and RF achieve significance for the majority of models. All three measures show large effect of model size on humansimilarity, though KE notably correlates negatively such that smaller models align better than larger models. NS and CE show significance across most models without strong dependence on model size. The relationship between model size and humanLLM uncertainty similarity is shown in more detail in figure 3, in which we correlated the correlation scores from figure 2 with the size of the model. NS and VE stand out here as being significantly correlated across sizes despite low human-similarity."
        },
        {
            "title": "5.2 Human Uncertainty Prediction",
            "content": "To further investigate the connection between human uncertainty and our uncertainty measures, we attempted to learn set of simple linear regression models. We also repeat this process using 3-fold cross validation to assess generalizability. Ideally, measure which was aligned with human uncertainty would correlate well on the entire dataset when trained and correlate well with validation data. Otherwise, the model may correlate with some but not all the data or have untenable model variance, respectively. For this phase, we omitted PV, PS, and RF because they are computed on per-choice basis and cannot be readily adapted to linear regression. One linear regression model is learned per modelmeasure pair, with the human uncertainty on each 7 question as the dependent variable and uncertainty measure value on the associated questions as the independent variables. Further, we train two additional linear regressions that predict human uncertainty using linear combination of multiple measures. One linear regression uses all uncertainty measures except RF, PV, and PS, while the other is trained using only the best and worst performing measures in phase one: KE and NS. This pair of measures also has potentially relevant features including that they (1) have similar but opposite correlation with model size and (2) fall cleanly into vocabulary uncertainty (NS) and choice uncertainty (KE) measures. The results of this process are shown in figure 4, ordered left to right in ascending order by mean accuracy across models. The relative performance of each measure matches the relative human-similarity across measures as in figure 2. Importantly, the combination of all non per-choice measures results in model that generalizes well, 0.5, and has significant correlation, > 0.6, with the entire dataset. This is the only measure which manages balance of both. This may be because the measures, taken as group, balance increasing and decreasing correlation with model size as shown in figure 3."
        },
        {
            "title": "6 Discussion",
            "content": "While we see positive results on the similarity between LLM uncertainty and human uncertainty for variety of measures, we see across all analyses that the most effective measure for achieving human-similarity in uncertainty response is KE. RF and PV also show evidence of strong dependence on model size for increased human-similarity, suggesting that they might be effective measures for larger (> 8 billion parameters) models, but further study is warranted to verify this relationship holds. PV presents an issue here, as ensemble methods scale poorly with increased model size. KE similarly appears to scale badly with increased model size despite being the most human-similar measure at the model sizes studied here. The results in phase two of analysis suggests that mixtures of measures may be able to mitigate their respective weaknesses and provide stronger overall measure. ing this work to compare other, more sophisticated, uncertainty measures from the existing literature. Similarly, future work should consider iterations on the simplistic KE and RF methods to achieve higher human-similarity, such as dynamic selection of k. In both cases, future work should seek to extend this work to wider variety of contexts, as the current work is limited to only instantaneous responses with limited set of available completions. Future work may extend this work to contexts that allow or require unconstrained generation."
        },
        {
            "title": "Limitations",
            "content": "In this section, we identify some of the limitations in our study that may affect generalization of the results. All methods used herein rely on the model under study being capable of providing the probability distribution over the output vocabulary. Access to this distribution is not provided by most black-box models. As such, we cannot replicate these results on black-box models and the human-like uncertainty measures identified cannot be transferred. Some of the methods, particularly the ensemble methods, also require access to the model internals and so cannot be used with black-box models. Even within the domain of white-box models, our experiments were limited to only LLaMa and Mistral model families. We cannot make guarantees on transferability to other model families. Our experiment design also makes use of an unstated assumption that the uncertainty behavior of groups of human respondents is directly comparable to the uncertainty behavior of an individual LLM. It is unclear from our work whether these results are transferable to comparisons between the uncertainty of an LLM with the uncertainty of an individual human subject. However, our results show that both individual and group LLM uncertainty (PV and KE in particular) correlates with human group behavior. This provides some confidence that our results may be transferable. Finally, our experiment design utilized cloze testing on set of pre-defined tokens for ease of comparison. We cannot guarantee without future research that uncertainty correlations hold for unconstrained generation, either in terms of full vocabulary generation or for intended responses."
        },
        {
            "title": "References",
            "content": "This work leaves open many avenues of future research. The most obvious research path is extendMeta AI. 2024. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. 8 Anthropic. 2024. Introducing the next generation of claude. Lisa Argyle, Ethan Busby, Nancy Fulda, Joshua Gubler, Christopher Rytting, and David Wingate. 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3):337351. Neil Band, Xuechen Li, Tengyu Ma, and Tatsunori Hashimoto. 2024. Linguistic calibration of longIn Proceedings of the 41st Inform generations. ternational Conference on Machine Learning, pages 27322778. Mohammad Beigi, Sijia Wang, Ying Shen, Zihao Lin, Adithya Kulkarni, Jianfeng He, Feng Chen, Ming Jin, Jin-Hee Cho, Dawei Zhou, and 1 others. 2024. Rethinking the uncertainty: critical review and analysis in the era of large language models. arXiv preprint arXiv:2410.20199. Catarina Belém, Markelle Kelly, Mark Steyvers, Sameer Singh, and Padhraic Smyth. 2024. Perceptions of linguistic uncertainty by language models and humans. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 84678502. Pew Research Center. The american trends panel. Arslan Chaudhry, Sridhar Thiagarajan, and Dilan Gorur. 2024. Finetuning language models to emit linguistic expressions of uncertainty. arXiv preprint arXiv:2409.12180. Jiuhai Chen and Jonas Mueller. 2024. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51865200. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186. Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-Dünner. 2024. Questioning the survey responses of large language models. Advances in Neural Information Processing Systems, 37:45850 45878. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2024. Shifting attention to relevance: Towards the predictive uncertainty quantification of freeform large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50505063. David Dunning. 2011. The dunningkruger effect: On being ignorant of ones own ignorance. In Advances in experimental social psychology, volume 44, pages 247296. Elsevier. Esin Durmus, Karina Nguyen, Thomas Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, and 1 others. 2023. Towards measuring the representation of subjective global arXiv preprint opinions in language models. arXiv:2306.16388. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539555. Yarin Gal and Zoubin Ghahramani. 2016. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 10501059. PMLR. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. 2023. survey of uncertainty in deep neural networks. Artificial Intelligence Review. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Dennis Hinkle, William Wiersma, Stephen Jurs, and 1 others. 2003. Applied statistics for the behavioral sciences, volume 663. Houghton Mifflin Boston. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751. Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang Zhang. 2024. Decomposing uncertainty for large language models through input clarification ensembling. In International Conference on Machine Learning, pages 1902319042. PMLR. Chengpiao Huang, Yuhang Wu, and Kaizheng Wang. 2025a. Uncertainty quantification for llm-based survey simulations. arXiv preprint arXiv:2502.17773. Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma. 2025b. Look before you leap: An exploratory study of uncertainty analysis for large language models. IEEE Transactions on Software Engineering. 9 Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962977. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, and 1 others. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221. Ramneet Kaur, Colin Samplawski, Adam Cobb, Anirban Roy, Brian Matejek, Manoj Acharya, Daniel Elenius, Alexander Berenbeim, John Pavlik, Nathaniel Bastian, and 1 others. 2024. Addressing uncertainty in llms to enhance reliability in generative ai. arXiv preprint arXiv:2411.02381. Zaid Khan and Yun Fu. 2024. Consistency and uncertainty: Identifying unreliable responses from blackbox vision-language models for selective visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1085410863. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating with confidence: Uncertainty quantification for black-box large language models. arXiv preprint arXiv:2305.19187. Andrey Malinin and Mark Gales. 2020. Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650. Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 90049017. Kanishka Misra, Allyson Ettinger, and Julia Rayz. 2021. Do language models learn typicality judgments from text? In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 43. OpenAI. 2022. Introducing chatgpt. Jesse Roberts. 2024. How powerful are decoder-only transformer neural models? In 2024 International Joint Conference on Neural Networks (IJCNN), pages 18. IEEE. Jesse Roberts, Kyle Moore, and Doug Fisher. 2024a. Do large language models learn human-like strategic preferences? arXiv preprint arXiv:2404.08710. Jesse Roberts, Kyle Moore, Douglas Fisher, Oseremhen Ewaleifoh, and Thao Pham. 2024b. Large language model recall uncertainty is modulated by the fan In Proceedings of the 28th Conference on effect. Computational Natural Language Learning, pages 303313. Jesse Roberts, Kyle Moore, Drew Wilenzick, and Douglas Fisher. 2024c. Using artificial populations to study psychological phenomena in neural models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1890618914. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. 2023. Whose opinions do language models reflect? In International Conference on Machine Learning, pages 2997130004. PMLR. Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar. 2023. Llamas know what gpts dont show: Surrogate models for confidence estimation. arXiv preprint arXiv:2311.08877. Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas Mayer, and Padhraic Smyth. 2025. What large language models know and what people think they know. Nature Machine Intelligence, pages 111. Zhisheng Tang, Ke Shen, and Mayank Kejriwal. 2024. An evaluation of estimative uncertainty in large language models. arXiv preprint arXiv:2405.15185. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 54335442. Sabrina Mielke, Arthur Szlam, Emily Dinan, and YLan Boureau. 2022. Reducing conversational agents overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857872. Lindia Tjuatja, Valerie Chen, Tongshuang Wu, Ameet Talwalkwar, and Graham Neubig. 2024. Do llms exhibit human-like response biases? case study in survey design. Transactions of the Association for Computational Linguistics, 12:10111026. 10 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Siddhartha Vemuri, Raj Sanjay Shah, and Sashank Varma. 2024. How well do deep learning models capture human concepts? the case of the typicality effect. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 46. Xi Wang, Laurence Aitchison, and Maja Rudolph. 2023. Lora ensembles for large language model fine-tuning. arXiv preprint arXiv:2310.00035. Yijun Xiao and William Yang Wang. 2021. On hallucination and predictive uncertainty in conditional language generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 27342744. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063. Caiqi Zhang, Fangyu Liu, Marco Basaldella, and Nigel Collier. 2024. Luq: Long-text uncertainty quantification for llms. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 52445262. Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023. Navigating the grey area: How expressions of uncertainty and overconfidence affect language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 55065524."
        }
    ],
    "affiliations": [
        "Tennessee Tech University",
        "Vanderbilt University"
    ]
}