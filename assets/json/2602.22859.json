{
    "paper_title": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models",
    "authors": [
        "Hongrui Jia",
        "Chaoya Jiang",
        "Shikun Zhang",
        "Wei Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE."
        },
        {
            "title": "Start",
            "content": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models Hongrui Jia 1 * Chaoya Jiang 2 * Shikun Zhang 1 Wei Ye 1 Abstract As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE. 6 2 0 2 6 2 ] . [ 1 9 5 8 2 2 . 2 0 6 2 : r 1. Introduction In recent years, as reinforcement learning methods (Guo et al., 2025; Zheng et al., 2025; Yu et al., 2025; Zhao et al., 2025; Gao et al., 2025) have matured, the reasoning capabilities of Large Multimodal Models (LMMs) have improved substantially. Models such as GPT-5.2 (OpenAI *Equal contribution 1Peking University 2Shandong University. Correspondence to: Chaoya Jiang <jcy@sdu.edu.cn>, Wei Ye <wye@pku.edu.cn>. Preprint. February 27, 2026. 1 Team, 2025), Claude Sonnet 4.5 (Anthropic Team, 2025), and Qwen3-VL (Bai et al., 2025a) show particularly strong performance on complex reasoning tasks. However, annotated data for multimodal reasoning remains scarce, making it difficult to support large scale training of LMMs(Liu et al., 2025a). To mitigate this, prior works (He et al., 2025; Chen et al., 2025a; Liu et al., 2024; Thawakar et al., 2025; Sunil et al., 2026; Liu et al., 2025b) have proposed self-evolving training frameworks characterized by an iterative cycle of selfquestioning and self-answering to continuously refine the model. While these approaches have garnered significant attention, current methodologies are constrained by two fundamental limitations: 1. Lack of Interpretable Diagnostics. Driven by heuristic signals (e.g., perplexity) rather than explicit failure attribution, existing methods lack principled capability decomposition. Consequently, the evolutionary process pursues superficial complexity instead of addressing genuine capability gaps, resulting in unstable data quality and noise. 2. Scarcity of Visual Diversity. Reliance on static image sets inherently restricts the semantic scope of training. While textual queries evolve, the immutable visual context limits the coverage of long-tail scenarios, causing performance on rare or complex concepts to plateau or even regress. Research in educational psychology (Black & Wiliam, 1998; Hattie & Timperley, 2007) reveals that diagnosis and targeted correction are the pivotal determinants of learning efficiency. Inspired by this diagnose-and-correct mechanism in human cognition, we propose Diagnostic-driven Progressive Evolution (DPE). Mirroring these principles, DPE eschews indiscriminate data expansion. Instead, it prioritizes the diagnosis of capability gaps to steer targeted data generation and mixture optimization, effectively breaking the multimodal long-tail bottleneck. Specifically, DPE consists of two key mechanisms: (1)Adaptive Diagnosis. Before generating new data, diagnostic agent analyzes the models failure patterns to identify specific weaknesses and capability blind spots. These insights are used to dynamically optimize the training data mixture, driving closed loop of diagnosis, generation, and reinforcement for targeted capability improvement. (2) ToolPreprint Figure 1. Due to the lack of interpretable diagnostics and scarcity of visual diversity, previous self-evolution frameworks can alleviate hallucination to some extent but fail to provide meaningful improvements on long-tail tasks such as mathematics and OCR. As result, the model often exhibits instability or even degradation in these capabilities during the evolution process. In contrast, our DPE framework effectively addresses these blind spots and supports more comprehensive and balanced progression of the models abilities. Use Data Evolution. Instead of relying on static datasets or template-based text rewriting, DPE employs multi-agent system equipped with image search and editing tools. These agents collaboratively source and annotate diverse visual content from external pools, allowing the framework to construct high-quality, weakness-focused training samples with deliberately controlled distributions. Crucially, DPE is not constrained by static dataset. It can adaptively source images from large external pools and construct targeted questions to form training data with deliberately controlled mixtures for reinforcement. We apply our DPE framework to multiple models, including Qwen2.5-VL-7B-Instruct (Bai et al., 2025b) and Qwen3VL-8B-Instruct (Bai et al., 2025a), and evaluate it on 11 challenging benchmarks that probe different aspects of multimodal reasoning, including MMMU (Yue et al., 2024b), CharXiv (Wang et al., 2024b), and MathVision (Wang et al., 2024a). Experiments show that, compared with static data training methods, our framework can improve model capabilities broadly using only small amount of training data. Further analysis indicates that, as training iterates, the diagnosis mechanism noticeably improves training stability, while the unlabeled multimodal annotation mechanism effectively alleviates bottlenecks caused by static data. Our main contributions are as follows: We propose novel Diagnostic-driven Progressive Evolution (DPE) training paradigm that targets model blind spots through diagnosis, generation, and reinforcement loop, mitigating diminishing marginal returns during training and avoiding long tail coverage issues induced by static data. We demonstrate the efficiency of DPE on multiple open source models. With only 1000 training examples, it achieves broad improvements in multimodal reasoning. We provide systematic analyses that quantitatively evaluate how the diagnosis mechanism affects training stability, offering new direction for addressing long tail challenges in improving multimodal reasoning. 2. Related Work 2.1. Reasoning with Large Multimodal Models The success of reinforcement learning (RL) in enhancing the reasoning capabilities of Large Language Models (LLMs) (Guo et al., 2025; Zheng et al., 2025; Zhao et al., 2025) has spurred similar advancements in Large Multimodal Models (LMMs). Recent works focus on establishing verifiable reward mechanisms to align visual reasoning. For instance, VLM-R1 (Shen et al., 2025) and RRVF (Chen et al., 2025b) introduce rule-based and rendering-based feedback loops, respectively, to ground reasoning in verifiable signals. Others, such as Vision-SR1 (Li et al., 2025b) and SRPO (Wan et al., 2025), leverage self-consistency and selfreflection to mitigate hallucinations and refine reasoning trajectories. OVR (Wei et al., 2025) further explores coldstart strategies to transfer cognitive behaviors from language to vision. Despite these strides, most RL-based LMMs rely heavily on static datasets or expensive annotations. They often lack the mechanism to dynamically adapt the data distribution to the models evolving capabilities, leading to inefficiencies where models over-train on mastered samples while neglecting long-tail weaknesses. 2.2. Self-Evolving Multimodal Frameworks To address data scarcity, self-evolving paradigms (He et al., 2025; Chen et al., 2025a; Liu et al., 2024) have emerged, where models improve via self-generated feedback. Existing approaches can be broadly categorized into filteringbased and generative methods. Filtering strategies, such Preprint as M-STAR (Liu et al., 2024) and EvoLMM (Thawakar et al., 2025), utilize uncertainty metrics (e.g., entropy) or process reward models to select high-quality samples from noisy generations. Generative frameworks, exemplified by VisPlay (He et al., 2025) and IREASONER (Sunil et al., 2026), employ proposer-solver loop where generator creates new queries and solver verifies them using consistency checks. More recent agentic approaches (Liu et al., 2025b; Pan et al., 2025) incorporate tool-use and multi-agent collaboration to enhance the reliability of self-evaluation. However, critical limitation remains: current self-evolving pipelines typically operate in blind manner. They generate or filter data based on general quality metrics rather than explicitly diagnosing the models specific failure modes. This often results in distribution drift or mode collapse during iterations, as the generated data fails to target the models actual cognitive blind spots. 3. Methods 3.1. Overall Framework As shown in Figure 2, we propose Diagnostic-driven Progressive Evolution (DPE), closed-loop training framework that steadily improves large multimodal models (LMMs) under scarce multimodal supervision and long-tail coverage gaps. Different from prior self-evolution methods that depend on static image sets and heuristic signals, DPE iteratively performs diagnosis, targeted generation, and reinforcement-based updating. In each iteration, DPE explicitly controls both the training-data category composition and question emphasis, aligning training resources with current capability blind spots and reducing instability and diminishing returns on long-tail skills. Let the policy at iteration be πθ(k) . DPE constructs training set (k) and updates parameters to θ(k+1) via reinforcement learning with verifiable rewards: θ(k+1)=ARL(θ(k); (k)),T (k)=Agen(R(k)),R(k)=Adiag(π θ(k) ), (1) where Adiag, Agen, and ARL are the diagnosis, generation, and RL-update operators, respectively, and R(k) is structured diagnostic report. 3.2. Diagnostic Mechanism The diagnostic mechanism provides an interpretable and actionable assessment of the current policy πθ(k) at the start of each iteration, and converts it into constraints/instructions for the next-round data generation. Instead of heuristic proxies (e.g., perplexity or reward averages), our diagnosis performs explicit failure attribution and capability decomposition: it identifies where the model fails, which capability dimension is responsible, and recurring error patterns, enabling stable targeted evolution. We map multimodal logical reasoning into capability space = {c1, c2, . . . , cK} with = 12 dimensions, including geometry images, medical images, statistical charts, textintensive images, flow diagrams, mathematical formulas, spatial maps, natural scenes, daily objects, artworks, architectural images, and others. Diagnostic sampling and step-aware scoring. At iteration k, we sample = 200 instances from diagnostic pool Ddiag: {(In, qn, an, cn)}N n=1 Ddiag. (2) The model produces ˆyn πθ(k) ( In, qn), which is scored by diagnostic agents: zn = v(ˆyn, an), (3) where v() evaluates both reasoning steps and final results; we convert zn into scalar correctness signal for aggregation. For each category c, we compute counts and accuracy: Nc = (cid:88) n=1 I[cn = c], Accc ="
        },
        {
            "title": "1\nNc",
            "content": "N (cid:88) n=1 I[cn = c] zn. Failure attribution and diagnostic summary. Beyond category accuracy, agents analyze the error set Ec = {n cn = c, zn = 0} and summarize recurring patterns as Fc (e.g., OCR: missing lines/misaligned regions; charts: ignored axis units/legend mismatch; math: dropped steps/symbol parsing errors; multi-image: entity misalignment/incorrect reference resolution). These attributions are injected into generation as executable prompts to control focus and difficulty. From diagnosis to category proportions. key output is the category proportion vector α(k) for next-round generation. We assign unnormalized weights αc according to segmented ranges of Accc, and normalize: α(k) = αc c=1 αc (cid:80)C . (4) Structured diagnostic report. The final report is (cid:16) R(k) = α(k), {F (k) }C c=1, {H(k) }C c=1 (cid:17) , (5) where α(k) controls category quotas, (k) records withincategory weaknesses, and H(k) provides actionable generac tion instructions (e.g., stronger localization, longer reasoning chains, stricter answer formats). 3 Preprint Figure 2. Overview of the DPE framework. 3.3. Multiple Agents Questioner System The Multiple Agents Questioner System converts the diagnostic report R(k) into collection of training samples with controllable distribution, controllable quality, and verifiable answers, which are then used for RLVR optimization (e.g., GRPO). Unlike self-evolution methods that only rewrite text instructions over static image set, our system coordinates four specialized agents for planning, retrieval and editing, question construction, and validation. It explicitly enforces both category quota constraints and quality gating constraints, thereby improving the relevance of the generated data while maintaining training stability. Category quota constraint and dataset formalization. Let the diagnostic mechanism output the category proportions at iteration as α(k) = (α(k) ). Given target generation budget of samples for this iteration, , = the category quota is defined as mc = 1, . . . , C, and the final generated set (k) is required to satisfy the hard constraint 1 , . . . , α(k) (cid:106) α(k) (cid:109) (cid:88) (I,q,a,c)T (k) I[c = c] = mc, {1, . . . , C} (6) system outputs training dataset (k) = The {(Ij, qj, aj, cj)}M j=1, where Ij can be single image or multi-image composition, and the reference answer aj must be verifiable to support stable reinforcement learning with verifiable rewards. As shown in Figure 2, the system consists of four modules: Planner Agent, Image Selector Agent, Question Generator Agent, and Validation Agent. They collaborate through the shared quota state and the diagnostic information R(k), forming pipeline that covers plan construction, data instantiation, and quality verification. Planner Agent. The Planner agent translates diagnostic outputs into executable instructions at the level of individual samples. For the j-th sample to be generated, the Planner outputs: planj = (cid:0)cj, reqI , reqQ , dirj (cid:1), (7) where cj is the target category and must satisfy the quota constraint ncj < mcj , reqI specifies image requirements (e.g., containing readable text regions, axes and legends in chart, or two comparable views sharing the same entities), reqQ specifies question requirements (e.g., multiple-choice versus numeric, whether unit is required, and whether structured output format is required), and dirj specifies direction constraint that targets the models weaknesses within this category, derived from (k) cj and H(k) cj . Image Selector Agent. Given reqI , the Image Selector agent obtains the visual input from an external image pool Pext and performs editing or composition when necessary: Ij = ϕ(Pext, reqI (8) ), where ϕ() represents pipeline that includes retrieval, candidate filtering, editing or fusion, and final selection. The 4 Preprint agent provides three types of capabilities: (1) Search, which performs large-scale retrieval using keywords, category tags, and structural hints (e.g., bar chart with legend); (2) Filter, which applies basic quality screening to candidates, including resolution thresholds, image size, consistency between image type and plan requirements (e.g., confirming chart structure or sufficient text regions), and basic readability checks; (3) Edit/Compose, which constructs targeted scenarios via cropping, overlaying text, stitching multiple images, or fusing local regions, especially for long-tail concept coverage and boundary-case construction. This tooluse design frees DPE from the limitations of static datasets, expands semantic coverage at the image level, and enables rapid and targeted reproduction of blind spots identified during diagnosis. Question Generator Agent. Given the image Ij and the planning instructions, the Question Generator agent produces question qj and reference answer aj: (qj, aj) = ψ(cid:0)Ij, reqQ , H(k) cj (cid:1) (9) In practice, this stage strictly follows the Planners global plan. When the Planner detects that category quota has been reached, namely nc = mc, it skips that category and proceeds to plan the next sample, ensuring that the final data distribution matches the proportions specified by the diagnostic mechanism. Meanwhile, the Planner refines image requirements for the Image Selector agent and specifies clear question directions for the Question Generator agent based on category-level focus and difficulty recommendations, forming jointly consistent plan between visual constraints and question directions. Validation Agent. Since self-generated samples may suffer from category drift, underspecification, or answer inconsistency, we introduce Validation agent to explicitly gate data quality and ensure that accepted samples meet minimum standards. For candidate sample sj = (Ij, qj, aj, cj) with plan planj, we define the following checks: category consistency gcat(sj, planj), solvability and information completeness gsol(sj), answer verifiability gver(sj), and format compliance gfmt(sj, reqA ). The final acceptance condition is g(sj) = gcat gsol gver gfmt (10) If g(sj) = 1, the sample is added to the training set and is updated. Otherwise, the sample is discarded and the system re-generates new candidate. This gating procedure reduces training noise, improves the stability of RLVR optimization, and prevents miscategorized samples from corrupting quota statistics and causing distribution drift. 3.4. LMM Training We optimize the target multimodal model using GRPO. For each prompt x, the old policy πθold generates trajectories yi = (oi,1, . . . , oi,yi) πθold ( x), = 1, . . . , G, where oi,t denotes the t-th token in the i-th trajectory. Each trajectory is assigned scalar reward ri = r(x, yi) R. GRPO optimizes the following clipped surrogate objective: JGRPO(θ) = ExD, {yi}πθold (cid:34) 1 (cid:88) i=1 1 yi yi (cid:88) t= (cid:16) min ρi,tAi,t, clip(ρi,t, 1 ε, 1 + ε) Ai,t (cid:17) β KL(πθ πinit) (cid:35) (11) where ρi,t = πθ (oi,tx,oi,<t) (oi,tx,oi,<t) , ε is the PPO-style clipping threshπθold old, β > 0 controls the strength of KL regularization, πinit is reference policy, and πθ is the current trainable policy. Group-normalized advantages. key design of GRPO is the trajectory-level group-normalized advantage: ˆAi = ri mean(r1, . . . , rG) std(r1, . . . , rG) . (12) maximum-entropy view of learnability. From the perspective of maximum-entropy policy improvement, given reward function r(x, y) with = (I, q), the optimal policy satisfies π(y x) πinit(y x) exp(r(x, y)/β) (13) and the reverse KL divergence admits the expression KL(πinit π) = (cid:17) (cid:16) (x) Eπinit [r(x, y)] (14) 1 β Following prior work (Bae et al., 2025; Shi et al., 2025; Bu et al., 2025; Huang et al., 2025), the associated soft value function is (x) = β log Eyπinit [exp(r(x, y)/β)] (15) For binary rewards {0, 1}, let the pass rate be p(x) = Eπinit [r(x, y)]. (16) Then (x) = β log (cid:16) (1 p(x)) + p(x) exp(1/β) (cid:17) , (17) and second-order lower bound yields KL(πinit π) p(x)(cid:0)1 p(x)(cid:1) 2β2 . (18) This bound depends only on the variance term p(x)(1 p(x)). It vanishes when is close to 0 or 1, and it is maximized near = 0.5. Since the update magnitude in GRPO is also governed by the within-group reward variance through ˆAi, this analysis explains why DPE retains only moderately difficult samples to improve the learning efficiency per training example. Preprint Iterative training. At iteration k, DPE first generates and validates dataset (k) according to the diagnostic report, then applies difficulty-aware filtering to obtain (k) train , and finally performs GRPO to update the model: θ(k+1) = ARL . After the update, the system proceeds to the next diagnostic round and repeats the same procedure. This iterative process progressively strengthens weak capabilities and continuously expands visual coverage through external image sources, leading to stable evolution of multimodal reasoning ability. θ(k); (k) train (cid:16) (cid:17) 4. Experiments 4.1. Experiment Settings DPE Settings. We evaluate DPE under extremely low-data conditions. Our framework uses only the first 1K samples from Vision-SR1-47K (Li et al., 2025a) as the seed dataset, whose initial category distribution is shown in Figure4. Based on these 1K seed examples, the Multiple-Agents Questioner System generates approximately 4K training samples, while VisPlay uses 8K training samples per iteration. All other experimental configurations strictly follow those of VisPlay to ensure fair comparison. Due to the requirement for parallel data generation, the questioner system is implemented with 4 high-performance agents, including OpenAI o3 (OpenAI, 2025), Claude Sonnet 4 (Anthropic, 2025), Gemini-2.5-Pro (Comanici et al., 2025), and Qwen-VL-Max (Bai et al., 2025a). For image retrieval, we use the Serper API and retain the top 3 most relevant images in each search. For image editing and augmentation, we adopt Qwen-Image-Edit (Wu et al., 2025). The diagnostic mechanism is implemented using QwenVL-Max, which analyzes 200 randomly sampled problems from Vision-SR1-47K to identify the weaknesses of the target model. Baselines We evaluate two base models, Qwen2.5-VL-7BInstruct (Bai et al., 2025b) and Qwen3-VL-8B-Instruct (Bai et al., 2025a), and optimize them using either VisPlay or DPE. The number of evolution rounds is fixed to three for both methods following the VisPlay setting. conduct evaluations Evaluation Protocol We using VLMEvalKit(Duan et al., 2024) and lmms-eval(Zhang et al., 2024a) on the following eleven benchmarks to ensure fair and reproducible results. STEM: MMMU (Yue et al., 2024a), MMVet (Yu et al., 2024), MMStar (Chen et al., 2024), and RealWorldQA (xAI, 2024). Visual Math: MathVerse (Zhang et al., 2024b), MathVision (Wang et al., 2024a), and MathVista (Lu et al., 2024). OCR: ChartQA (Masry et al., 2022) and CharXiv (Wang et al., 2024b). Multi-image: BLINK (Fu et al., 2024). Hallucination: HallusionBench (Guan et al., 2024). We use accuracy (Acc) as the evaluation metric. 4.2. Main Results Comparison with Self-evolving Method. Table 1 demonstrates DPEs superiority over VisPlay across three key dimensions. First, comprehensive capability enhancement: DPE achieves consistent gains across STEM, OCR, and hallucination mitigation. On Qwen2.5-VL-7B-Instruct, it boosts CharXivRQ by 4.11 points and outperforms VisPlay on HallusionBench (69.19% vs. 68.35%). Second, robust training dynamics: DPE mitigates the oscillation and regression observed in VisPlay. While VisPlays performance on MMMU and BLINK fluctuates across iterations, DPE sustains 6 smooth upward trend (e.g., MMMU 54.44 56.44), validating that our closed-loop mechanism effectively targets weaknesses without distribution drift. Finally, transferability: When applied to the stronger Qwen3-VL-8B-Instruct, DPE generalizes well, delivering substantial improvements on MMMU (+3.67) and MMStar (+10.86), proving its effectiveness across different model scales. Comparison with State-of-the-Arts. As detailed in Table 2, DPE demonstrates remarkable parameter efficiency. Based on the 8B backbone, DPE achieves an average score of 64.39, surpassing the 72B-parameter Qwen2.5-VL (61.9) and the proprietary GPT4o (56.1). Notably, DPE dominates in complex reasoning tasks. In visual math, it establishes new SOTA performance on MathVista (76.2) and MathVision (53.88), significantly outperforming Qwen2.5-VL-72B by +1.4 and +15.7 points, respectively. In hallucination mitigation, DPE leads HallusionBench with 74.13, demonstrating superior grounding compared to GPT-4o (67.5). These results suggest that data quality derived from DPEs closed-loop evolution is more critical than sheer parameter scale for solving complex multimodal problems. 4.3. Ablation Studies Impact of Static Data. Table 3 highlights DPEs exceptional data efficiency. Despite using only 3,000 iteratively generated samplesapproximately 1/15 of the static Vision-SR1-47K datasetDPE achieves superior performance across key dimensions. Specifically, it improves MMMU (54.8 56.44), HallusionBench (67.6 69.0), MathVista (68.8 69.5), and RealWorldQA (69.9 70.5). These results indicate that the bottleneck in static training is not data volume, but rather the fixed distribution. Static datasets inevitably lead to saturation on high-frequency patterns while neglecting long-tail capabilities, resulting in diminishing returns. In contrast, DPEs diagnostic module continuously identifies failure modes, allowing the generation process to concentrate the limited data budget on unresolved weaknesses. This closed-loop targeting overcomes the performance cap of fixed coverage, delivering broader and more stable improvements with substantially fewer samples. Figure 3. Ablation results on CharXiv and MathVision across three iterations, comparing full DPE with variants. Impact of the diagnostic module. To assess the necessity of the diagnostic module, we remove it and repeat the three-iteration training procedure under the same settings. The results show that, without diagnostics, iterative gains become much smaller and noticeably less stable, and training tends to plateau or even regress. On CharXiv, full DPE achieves continuous improvements across iterations (36.8, 37.7, 38.1, 40.91), whereas removing diagnostics keeps performance close to the baseline (36.8, 36.7, 37.5, 36.7). In addition to yielding nearly zero net improvement, the ablated setting exhibits an improve then drop pattern, indicating that the training process can no longer reliably align with true capability gaps. similar trend is observed on MathVision, where accuracy decreases from 26.25 at Iteration 2 to 25.99 at Iteration 3 Preprint Table 1. DPEs performance compared with Self-evolving Methods. Models Methods STEM Visual Math OCR MMMU RealWorldQA MMVet MMStar MathVerse MathVision MathVista CharXivRQ ChartQA Specialized HallusionBench BLINK Qwen2.5-VL-7B-Instruct Qwen3-VL-8B-Instruct Base 53.11 68.63 67.20 63.27 43.12 25.89 65.50 36.80 85.64 64.98 56.02 VisPlay Iter 49.3 68.10 69.44 66.07 42.71 26.18 68.8 37.50 86.04 68.35 56.55 Iter 3 54.89 69.02 67.52 65. 44.19 25.72 68.20 37.00 86.16 68.24 55.65 Iter 1 53.33 68.50 67.84 63.60 42.97 25.00 64. 36.50 86.04 64.98 56.44 DPE (Ours) Iter 2 Iter 3 Iter 1 54.44 69.41 67.71 65. 43.78 26.28 67.50 37.70 86.12 69.09 56.18 55.33 69.54 67.02 64.60 44.26 26.41 68.20 38.10 86. 69.19 56.65 56.44 70.46 68.35 65.60 45.10 26.51 69.50 40.91 86.56 68.98 56.23 Base 65.44 71.63 67.29 61.27 53.22 51.97 76.20 47.20 85.08 74.24 68.54 DPE (Ours) Iter 2 Iter Iter 1 68.11 71.63 70.92 71.40 55.99 52.04 76.60 47.90 84.84 73.92 68.96 69.11 70.72 70.00 71. 56.47 55.03 78.00 46.50 85.20 74.13 68.12 69.11 70.85 72.80 72.13 57.18 53.88 76.20 48.10 84. 74.13 69.22 Overall Average 57.29 57.27 58. 58.33 58.47 58.68 59.29 65.64 67. 67.72 68.04 Table 2. DPEs Performance Compared with State-of-the-Arts. Hallucination MathVerse MathVision MathVista CharXivRQ HallusionBench Visual Math OCR Models DeepEyes DeepEyesV2 Qwen2.5-VL-72B GPT-4o GPT5-Mini Claude4-Sonnet General Visual Understanding MMMU MMStar - - 70.2 69.1 67.9 75.1 - - 70.8 64.7 61.3 67. 47.3 52.9 57.6 50.2 36.5 65.9 26.6 28.9 38.1 30.4 46.6 52.7 Qwen3-VL-8B-Instruct DPE (Iter 3, Ours) 69.11 72.13 57. 53.88 70.1 38.1 74.8 63.8 59.6 72.4 76.2 - 48.9 49.7 47.1 48.9 60.9 48.1 Avg - - 61.9 56.1 53.8 64.1 - - 72.4 67.5 55.9 54.5 74.13 64.39 Table 3. Comparison between static training and DPE. Methods Data Size MMMU HallusionBench MathVista RealWorldQA Qwen2.5-VL-7B-Instruct Vision-R1 DPE (Iter 3) 47K 3K 54.8 56.44 67.6 69.0 68.8 69. 69.9 70.5 without diagnostics, while full DPE steadily improves to 26.51. These results demonstrate that the diagnostic module is crucial for maintaining correct and stable evolution direction. It significantly reduces distribution drift and performance oscillations commonly seen in self-evolving training, and ensures that each iteration continues to produce meaningful gains. Validating diagnosis-guided data distribution. To further verify whether diagnostics truly guide the data distribution, Figure 4 visualizes the category distribution of the seed data and the mixture ratios recommended by the diagnostic module over three iterations. The diagnostic module does not simply follow the seed distribution or apply uniform sampling. Instead, it increases the sampling ratios of underperforming categories based on the failure patterns from the previous iteration, forming targeted strengthening strategy driven by explicit error exposure. More importantly, the redistribution aligns directly with the observed performance improvements. On CharXiv, the diagnostic module substantially increases the proportion of text-dense and chart-related samples in Iteration 1, and CharXiv accuracy immediately improves from 36.8 to 37.7, then further increases to 38.1 in subsequent iterations, demonstrating sustained and cumulative gains. In Iteration 2, the diagnostic module assigns more samples related to mathematical formulas and symbolic reasoning, and MathVision continues to improve (26.28, 26.41, 26.51), consistent with the upward trends on MathVerse and MathVista. These results indicate that the diagnostic module can effectively identify the models current capability gaps and concentrate training resources on the truly weak dimensions through adaptive mixture control, thereby improving the effectiveness of each iteration. Impact of image retrieval and editing. We further analyze the contribution of the image retrieval and editing module (image tools) to iterative training. This module retrieves relevant samples from larger external image pool and applies moderate editing and recomposition, which substantially expands visual diversity and long-tail coverage in the training data. Ablation results show that removing image tools makes the model more likely to reach an early plateau, and limits gains in later iterations, with particularly strong effects on OCR and chart-related tasks. As shown in Figure 3, on CharXiv, full DPE reaches 40.91 after three iterations, while removing image tools only reaches 38.1, resulting in 2.81 drop. Moreover, most improvements occur in the first two iterations, with limited progress afterward. This suggests that generating text-level variations from the same or highly similar images can lead to overfitting to narrow layout and font distributions, failing to cover long-tail page structures and noise patterns, and thus capping performance on OCR tasks. consistent effect is observed on MathVision, where removing image tools yields 26.18, lower than the 26.51 achieved by full DPE, indicating that visual diversity also improves robust perception of symbols, layouts, and 7 Table 4. Text and image diversity scores (mean pairwise cosine distance) using Qwen3-VL-Embedding. Preprint Domain Base Text Image 0.764 0. VisPlay Iter 2 0.820 0.835 Iter 1 0.830 0.835 Iter 3 0.797 0. DPE (Ours) Iter 2 Iter 1 Iter 3 0.846 0.847 0.866 0.864 0.850 0. localized regions in visual mathematical reasoning. Figure 5. UMAP visualization of image diversity (left) and text diversity (right) for VisPlay and DPE. 4.5. Quality Analysis of Generated Questions We conduct systematic quality evaluation of generated questions. At each iteration, we randomly sample 200 examples and ask three independent LLM judges (Claude Sonnet 4, OpenAI o3, and Gemini 2.5 Pro) to rate each example on 5-point Likert scale from three aspects: Clarity (CL), Solvability (S), and Correctness (CO). We report the overall quality score (QS) as the average across aspects and samples: QS = 1 (cid:88) CLi + Si + COi 3 . (20) As shown in Table 5, DPE consistently produces higher-quality questions than VisPlay across all iterations and remains stable over time. VisPlays QS is nearly unchanged from Iteration 1 to 2 (3.743.75) but drops to 3.32 at Iteration 3, indicating late-stage quality degradation. In contrast, DPE maintains near-ceiling QS values (4.96, 4.74, 4.80). The improvements are mainly driven by solvability and correctness, i.e., whether questions are answerable from the image and whether answers are visually consistent. VisPlays solvability declines to 2.98 at Iteration 3, while DPE stays above 4.86 (4.98, 4.86, 4.91). Similarly, VisPlays correctness reaches 3.08 at Iteration 3, whereas DPE remains above 4.56 (4.93, 4.56, 4.58). Clarity shows the same trend: VisPlay drops from 4.38 to 3.91, while DPE stays close to 5 (4.99, 4.86, 4.92). Table 5. Quality evaluation of generated questions."
        },
        {
            "title": "VisPlay",
            "content": "DPE (Ours) Iter 1 Iter 2 Iter 3 Iter 1 Iter Iter 3 4.38 3.45 3.40 3.74 4.14 3.58 3.52 3.75 3.91 2.98 3.08 3.32 4.99 4.98 4.93 4.96 4.86 4.86 4.56 4. 4.92 4.91 4.58 4.80 Figure 4. Category distribution of the seed set and the diagnosisguided mixture ratios recommended by DPE over three iterations. 4.4. Diversity Analysis We evaluate the diversity of generated data from both textual and visual perspectives. For fair comparison, we embed questions and images using the same model, Qwen3-VL-Embedding. Let ftext() and fimg() denote its text and vision encoders. Given question qi and image Ii, we obtain zi = ftext(qi) Rd and vi = fimg(Ii) Rd. For each iteration, we randomly sample = 200 examples from VisPlay and DPE, respectively, and visualize their distributions under shared UMAP projection. Text diversity. We measure textual dispersion using the mean pairwise cosine distance: Diversity(Z) = 1 (N 1) (cid:88) i=j (1 cos(zi, zj)) . (19) As shown in Table 4, DPE achieves higher and more stable text diversity across three iterations. The base text diversity is 0.764. VisPlay increases to 0.83 at Iteration 1 but drops to 0.82 and 0.797 at Iterations 2 and 3. In contrast, DPE improves from 0.846 (Iter 1) to 0.866 (Iter 2) and remains high at 0.85 (Iter 3), indicating broader question coverage and reduced distribution collapse/template reversion. The UMAP in Fig. 5 further shows DPE covering wider semantic region with additional subclusters. Visual diversity. We apply the same metric to image embeddings and visualize them with UMAP. DPE also improves visual diversity over the base (0.835) to 0.847, 0.864, and 0.847 at Iterations 13. Since VisPlay mainly evolves from fixed image set, Fig. 5 shows DPE covering broader visual region with more non-overlapping content. This matches DPEs image retrieval and editing mechanism, indicating that it expands image sources and visual variations rather than only generating new text from static images, thereby improving long-tail visual coverage and maintaining diversity across iterations. 5. Conclusion We introduce the Diagnostic-driven Progressive Evolution framework for Large Multimodal Models (LMMs) (DPE) that overcomes heuristic self-evolution. By integrating diagnostic-generation-reinforcement loop, our method explicitly identifies model blind spots, adaptively constructs targeted training data, and leverages large-scale unlabeled multimodal resources through cooperative multi-agent annotation. This design ensures controllable evolution directions, stable training dynamics, and 8 Preprint sustained improvements on long-tail reasoning abilities that traditional self-expansion approaches fail to address. Experiments on several open-source LMMs show that our framework can deliver comprehensive reasoning enhancement using only small amount of training data, while detailed analyses further validate the crucial role of the diagnostic mechanism in improving stability and mitigating marginal utility saturation. Looking ahead, integrating richer diagnostic signals, expanding multimodal data sources, and exploring more sophisticated multi-agent collaboration strategies will continue advancing the development of adaptive, efficient, and continually improving multimodal reasoning systems."
        },
        {
            "title": "Impact Statement",
            "content": "This work aims to advance machine learning methodology by proposing diagnostic-driven framework for improving multimodal reasoning in large models. The primary impacts of our approach are methodological: enabling more data-efficient capability enhancement, reducing training instability, and improving transparency in self-evolution pipelines. As with many techniques that enhance the reasoning abilities of large multimodal models, potential societal implications may include broader deployment of such systems in real-world applications. Risks related to synthetic data generationsuch as the reinforcement of biases or propagation of inaccurate annotationsare mitigated through explicit validation mechanisms and controlled data distributions within our framework. Our experiments use only publicly available or synthetically generated inputs, thereby avoiding privacy concerns. Overall, we believe the ethical and societal considerations associated with this work are consistent with those commonly encountered in research on large-scale multimodal learning. No extraordinary risks beyond standard model improvements are introduced. References Anthropic. Claude Sonnet 4. https://www.anthropic. com/claude/sonnet, 2025. [Accessed 31-08-2025]. Anthropic Team. https://www. Claude Sonnet 4.5. anthropic.com/claude/sonnet/, 2025. Accessed: 2025-9-29. Bae, S., Hong, J., Lee, M. Y., Kim, H., Nam, J., and Kwak, D. Online difficulty filtering for reasoning oriented reinforcement learning, 2025. URL https://arxiv. org/abs/2504.03380, 2025. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report, 2025a. URL https://arxiv.org/abs/2511.21631. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Black, P. and Wiliam, D. Assessment and classroom learning. Assessment in Education: principles, policy & practice, 5(1): 774, 1998. Bu, D., Huang, W., Han, A., Nitanda, A., Wong, H.-S., Zhang, Q., and Suzuki, T. Provable benefit of curriculum in transformer tree-reasoning post-training. arXiv preprint arXiv:2511.07372, 2025. Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., and Zhao, F. Are we on the right way for evaluating large vision-language models?, 2024. URL https://arxiv.org/abs/2403.20330. Chen, X., Hu, W., Li, H., Zhou, J., Chen, Z., Cao, M., Zeng, Y., Zhang, K., Yuan, Y.-J., Han, J., et al. C2-evo: Co-evolving multimodal data and model for self-improving reasoning. arXiv preprint arXiv:2507.16518, 2025a. Chen, Y., Shen, Y., Huang, W., Zhou, S., Lin, Q., Cai, X., Yu, Z., Bu, J., Shi, B., and Qiao, Y. Learning only with images: Visual reinforcement learning with reasoning, rendering, and visual feedback. arXiv preprint arXiv:2507.20766, 2025b. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Duan, H., Yang, J., Qiao, Y., Fang, X., Chen, L., Liu, Y., Dong, X., Zang, Y., Zhang, P., Wang, J., et al. Vlmevalkit: An opensource toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 1119811201, 2024. Fu, X., Hu, Y., Li, B., Feng, Y., Wang, H., Lin, X., Roth, D., Smith, N. A., Ma, W.-C., and Krishna, R. Blink: Multimodal large language models can see but not perceive, 2024. URL https://arxiv.org/abs/2404.12390. Gao, C., Zheng, C., Chen, X.-H., Dang, K., Liu, S., Yu, B., Yang, A., Bai, S., Zhou, J., and Lin, J. Soft adaptive policy optimization, 2025. URL https://arxiv.org/abs/2511. 20347. Guan, T., Liu, F., Wu, X., Xian, R., Li, Z., Liu, X., Wang, X., Chen, L., Huang, F., Yacoob, Y., Manocha, D., and Zhou, T. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models, 2024. URL https://arxiv.org/abs/2310.14566. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hattie, J. and Timperley, H. The power of feedback. Review of educational research, 77(1):81112, 2007. He, Y., Huang, C., Li, Z., Huang, J., and Yang, Y. Visplay: Selfevolving vision-language models from images. arXiv preprint arXiv:2511.15661, 2025. Huang, C., Yu, W., Wang, X., Zhang, H., Li, Z., Li, R., Huang, J., Mi, H., and Yu, D. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. 9 Preprint Li, Z., Yu, W., Huang, C., Liu, R., Liang, Z., Liu, F., Che, J., Yu, D., Boyd-Graber, J., Mi, H., and Yu, D. Self-rewarding visionlanguage model via reasoning decomposition, 2025a. URL https://arxiv.org/abs/2508.19652. Wan, Z., Dou, Z., Liu, C., Zhang, Y., Cui, D., Zhao, Q., Shen, H., Xiong, J., Xin, Y., Jiang, Y., et al. Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning. arXiv preprint arXiv:2506.01713, 2025. Li, Z., Yu, W., Huang, C., Liu, R., Liang, Z., Liu, F., Che, J., Yu, D., Boyd-Graber, J., Mi, H., et al. Self-rewarding visionlanguage model via reasoning decomposition. arXiv preprint arXiv:2508.19652, 2025b. Liu, J., Meng, S., Gao, Y., Mao, S., Cai, P., Yan, G., Chen, Y., Bian, Z., Wang, D., and Shi, B. Aligning vision to language: Annotation-free multimodal knowledge graph construction for In Proceedings of the IEEE/CVF enhanced llms reasoning. International Conference on Computer Vision, pp. 981992, 2025a. Liu, J., Xiong, K., Xia, P., Zhou, Y., Ji, H., Feng, L., Han, S., Ding, M., and Yao, H. Agent0-vl: Exploring self-evolving agent for tool-integrated vision-language reasoning. arXiv preprint arXiv:2511.19900, 2025b. Liu, W., Li, J., Zhang, X., Zhou, F., Cheng, Y., and He, J. Diving into self-evolving training for multimodal reasoning. arXiv preprint arXiv:2412.17451, 2024. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. URL https://arxiv.org/abs/2310.02255. Masry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. URL https://arxiv.org/ abs/2203.10244. OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025. URL https://cdn.openai.com/ pdf/2221c875-02dc-4789-800b-e7758f3722c1/ o3-and-o4-mini-system-card.pdf. System Card. OpenAI Team. Introducing GPT-5.2. https://openai.com/ zh-Hans-CN/index/introducing-gpt-5-2/, 2025. Accessed: 2025-12-11. Pan, Z., Zhang, Y., Zhang, Y., Zhang, J., Luo, H., Han, Y., Wu, D., Chen, H.-Y., Yu, P. S., Li, M., et al. Evo-marl: Co-evolutionary multi-agent reinforcement learning for internalized safety. arXiv preprint arXiv:2508.03864, 2025. Shen, H., Liu, P., Li, J., Fang, C., Ma, Y., Liao, J., Shen, Q., Zhang, Z., Zhao, K., Zhang, Q., et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Shi, T., Wu, Y., Song, L., Zhou, T., and Zhao, J. Efficient reinforcement finetuning via adaptive curriculum learning. arXiv preprint arXiv:2504.05520, 2025. Sunil, M., Venmathimaran, M., and Kavitha, M. S. ireasoner: Trajectory-aware intrinsic reasoning supervision for arXiv preprint self-evolving large multimodal models. arXiv:2601.05877, 2026. Thawakar, O., Venkatraman, S., Thawkar, R., Shaker, A., Cholakkal, H., Anwer, R. M., Khan, S., and Khan, F. Evolmm: Self-evolving large multimodal models with continuous rewards. arXiv preprint arXiv:2511.16672, 2025. Wang, K., Pan, J., Shi, W., Lu, Z., Zhan, M., and Li, H. Measuring multimodal mathematical reasoning with math-vision dataset, 2024a. URL https://arxiv.org/abs/2402.14804. Wang, Z., Xia, M., He, L., Chen, H., Liu, Y., Zhu, R., Liang, K., Wu, X., Liu, H., Malladi, S., Chevalier, A., Arora, S., and Chen, D. Charxiv: Charting gaps in realistic chart understanding in multimodal llms, 2024b. URL https://arxiv.org/ abs/2406.18521. Wei, Y., Zhao, L., Sun, J., Lin, K., Yin, J., Hu, J., Zhang, Y., Yu, E., Lv, H., Weng, Z., et al. Open vision reasoner: Transferring linguistic cognitive behavior for visual reasoning. arXiv preprint arXiv:2507.05255, 2025. Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., ming Yin, S., Bai, S., Xu, X., Chen, Y., Chen, Y., Tang, Z., Zhang, Z., Wang, Z., Yang, A., Yu, B., Cheng, C., Liu, D., Li, D., Zhang, H., Meng, H., Wei, H., Ni, J., Chen, K., Cao, K., Peng, L., Qu, L., Wu, M., Wang, P., Yu, S., Wen, T., Feng, W., Xu, X., Wang, Y., Zhang, Y., Zhu, Y., Wu, Y., Cai, Y., and Liu, Z. Qwen-image technical report, 2025. URL https://arxiv.org/abs/ 2508.02324. xAI. Grok-1.5 Vision Preview. Grok-1.5VisionPreview/, 2024. Accessed: 2024-4-12. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2024. URL https://arxiv.org/ abs/2308.02490. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024a. URL https://arxiv.org/abs/2311.16502. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024b. Zhang, K., Li, B., Zhang, P., Pu, F., Cahyono, J. A., Hu, K., Liu, S., Zhang, Y., Yang, J., Li, C., and Liu, Z. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024a. URL https://arxiv.org/abs/2407.12772. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Gao, P., and Li, H. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?, 2024b. URL https://arxiv.org/abs/ 2403.14624. 10 Preprint Zhao, Y., Liu, Y., Liu, J., Chen, J., Wu, X., Hao, Y., Lv, T., Huang, S., Cui, L., Ye, Q., et al. Geometric-mean policy optimization. arXiv preprint arXiv:2507.20673, 2025. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. A. Case Study Preprint Figure 6 presents qualitative comparisons. In Figure 6.(a), the question generated by VisPlay lacks necessary information and cannot be answered from the image content. In Figure 6.(b), VisPlay produces multiple-choice question without options. In contrast, DPE generates questions with complete structure, sufficient information, and clear semantics, demonstrating its advantage in producing reliable training data. (a) VisPlay generates question that lacks essential visual grounding and cannot be answered based on the image. (b) VisPlay produces multiple-choice question without options, resulting in an incomplete structure. Figure 6. Case Study between VisPlay and our DPE framework."
        }
    ],
    "affiliations": [
        "Peking University",
        "Shandong University"
    ]
}