{
    "paper_title": "Arbitrary-steps Image Super-resolution via Diffusion Inversion",
    "authors": [
        "Zongsheng Yue",
        "Kang Liao",
        "Chen Change Loy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This study presents a new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance. We design a Partial noise Prediction strategy to construct an intermediate state of the diffusion model, which serves as the starting sampling point. Central to our approach is a deep noise predictor to estimate the optimal noise maps for the forward diffusion process. Once trained, this noise predictor can be used to initialize the sampling process partially along the diffusion trajectory, generating the desirable high-resolution result. Compared to existing approaches, our method offers a flexible and efficient sampling mechanism that supports an arbitrary number of sampling steps, ranging from one to five. Even with a single sampling step, our method demonstrates superior or comparable performance to recent state-of-the-art approaches. The code and model are publicly available at https://github.com/zsyOAOA/InvSR."
        },
        {
            "title": "Start",
            "content": "Arbitrary-steps Image Super-resolution via Diffusion Inversion Zongsheng Yue, Kang Liao, Chen Change Loy S-Lab, Nanyang Technological University {zongsheng.yue, kang.liao, ccloy}@ntu.edu.sg 4 2 0 2 2 1 ] . [ 1 3 1 0 9 0 . 2 1 4 2 : r Figure 1. Qualitative comparisons of our proposed method to recent state-of-the-art diffusion-based approaches on two real-world examples, where the number of sampling steps is annotated in the format Method name-Steps. We provide the runtime (in milliseconds) highlighted by red in the sub-caption of the first example , which is tested on 4 (128 512) SR task on an A100 GPU. Our method offers an efficient and flexible sampling mechanism, allowing users to freely adjust the number of sampling steps based on the degradation type or their specific requirements. In the first example, mainly degraded by blurriness, multi-step sampling is preferable to single-step sampling as it progressively recovers finer details. Conversely, in the second example with severe noise, single sampling step is sufficient to achieve satisfactory results, whereas additional steps may amplify the noise and introduce unwanted artifacts. (Zoom-in for best view)"
        },
        {
            "title": "Abstract",
            "content": "This study presents new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance. We design Partial noise Prediction strategy to construct an intermediate state of the diffusion model, which serves as the starting sampling point. Central to our approach is deep noise predictor to estimate the optimal noise maps for the forward diffusion process. Once trained, this noise predictor can be used to initialize the sampling process partially along the diffusion trajectory, generating the desirable high-resolution result. Compared to existing approaches, our method offers flexible and efficient sampling mechanism that supports an arbitrary number of sampling steps, ranging from one to five. Even with single sampling step, our method demonstrates superior or comparable performance to recent state-of-the-art approaches. The code and model are publicly available at https: //github.com/zsyOAOA/InvSR. 1 1. Introduction Image super-resolution (SR) is fundamental yet challenging problem in computer vision, aiming to restore high-resolution (HR) image from given low-resolution (LR) observation. The main challenge of SR arises from the complexity and often unknown nature of the degradation model in real-world scenarios, making SR an illposed problem. Recent breakthroughs in diffusion models [16, 45, 48], particularly large-scale text-to-image (T2I) models, have demonstrated remarkable success in generating high-quality images. Owing to the strong generative capability of these T2I models, recent studies have begun to use them as reliable prior to alleviate the ill-posedness of SR. This work follows this research line, further exploring the potential of diffusion priors in SR. The prevailing SR approaches leveraging diffusion priors usually attempt to modify the intermediate features of the diffusion network, either through optimization [7, 23, 56] or fine-tuning [30, 56, 60, 64], to better align them with the given LQ observations. In this work, we propose new technique based on diffusion inversion to harness diffusion priors. Unlike existing approaches, it attempts to find an optimal noise map as the input of the diffusion model, without any modification to the diffusion network itself, thereby maximizing the utility of diffusion prior. While considerable advances have been made in generative adversarial networks (GANs) [14] inversion for various applications [62, 75], including SR [4, 15, 41], extending these principles to diffusion models presents unique challenges, particularly for SR tasks that demand high fidelity preservation. In particular, the multi-step stochastic sampling process of diffusion models makes inversion nontrivial. The straightforward inversion approach to optimize the distinct noise maps at each diffusion step is expensive and complex. Additionally, the iterative inference mechanism would accumulate prediction errors and randomness at each step, which can significantly compromise fidelity. Therefore, recent diffusion inversion methods have mainly focused on tasks with lower fidelity requirements, such as image editing [13, 36]. In this work, we reformulate diffusion inversion for the more challenging task of SR. To enable diffusion inversion for SR, we introduce deep neural network called noise predictor to estimate the noise map from given LR image. In addition, Partial noise Prediction (PnP) strategy is devised to construct an intermediate state for the diffusion model, serving as the starting point for sampling. This is made possible by adding noise onto the LR image according to the diffusion models forward process, where the noise predictor predicts the added noise instead of random sampling. This approach is driven by the following key motivations: Rationality. LR and HR images differ only in highfrequency details. With the addition of appropriate noise, the LR image becomes indistinguishable from its HR counterpart. Thus, the noisy LR can serve as proxy for deriving the inversion trajectory during reverse diffusion. Complexity. Rather than predicting noise maps for all diffusion steps, the PnP strategy simplifies the inversion task by limiting predictions to the starting step, thereby reducing the overall complexity of the inversion process. Flexibility. The noise predictor can be trained to predict noise maps for multiple predefined starting steps. During inference, we can freely select starting step from them and then use any existing sampling algorithm with an arbitrary number of steps, offering favorable flexibility in controlling the sampling process. Fidelity. The starting steps during training are carefully selected to have high signal-to-noise ratio (SNR), ensuring robust fidelity preservation for SR. In practice, we enforce an SNR threshold greater than 1.44, corresponding to the timestep of 250 in Stable Diffusion [43]. Efficiency. As the sampling process begins from step earlier than 250 (SNR larger than 1.44), the PnP strategy effectively reduces the number of sampling steps to fewer than five when combined with off-the-shelf accelerated sampling algorithms [22, 46]. This addresses the common inefficiency issue in diffusion-based SR approaches. Unlike most existing diffusion-based methods that rely on fixed sampling steps, our flexible sampling mechanism offers versatile solution for handling varying degrees of degradation in SR. In SR, it is common to encounter different types and intensities of corruption. Intuitively, the number of sampling steps should adapt to the specific degradation conditions. For example, as shown in Fig. 1, multi-step sampling is preferable to single-step sampling in the first case, as it effectively reduces blurriness and restores finer details. In contrast, for the second example with severe noise, single sampling step achieves satisfactory results, while additional steps may amplify the noise and introduce unwanted artifacts. Our method uniquely allows users to adjust sampling to suit different degradation types. The main contributions of this work are twofold. First, we propose novel SR approach based on diffusion inversion, which effectively leverages the diffusion prior by integrating an auxiliary noise predictor while keeping the entire diffusion backbone fixed. Second, our method introduces flexible and efficient sampling mechanism that allows for arbitrary sampling steps, ranging from one to five. Remarkably, even when the steps are reduced to just one, our approach still achieves superior or comparable performance to recent dedicated one-step diffusion methods. 2. Related Work Diffusion Prior for SR. Existing diffusion prior-based SR approaches can be broadly categorized into two classes. The first class of methods involves re-optimizing the in2 termediate results of the diffusion model to ensure consistency with the given LR images via pre-defined or estimated degradation models. Representative works include DDRM [23], CCDF [7], and DDNM [56], among others [6, 8, 11, 38, 47, 63, 67]. While effective, these methods are limited by their computational complexity, as they require solving an optimization problem at each diffusion step, leading to slow inference. Furthermore, they often rely on manually defined assumed degradation models and thus cannot handle the blind SR problem in real-world scenarios. The second class directly fine-tunes pre-trained large T2I model for the SR task. StableSR [53] pioneers this paradigm by incorporating spatial feature transform layers [54] to guide the T2I model toward generating HR outputs. Subsequent works follow by proposing various fine-tuning strategies to exploit diffusion priors, including DiffBIR [30], SeeSR [60], PASD [64], S3Diff [71], and so on [27, 40, 49, 59, 61, 66]. These methods have achieved impressive performance, validating the effectiveness of diffusion priors for SR. Diffusion Inversion. Diffusion inversion focuses on determining the optimal noise map set that, when processed through the diffusion model, reconstructs given image. DDIM [46] first addressed this by generalizing the diffusion model via class of non-Markovian processes, thereby establishing deterministic generation process. Subsequent approaches, such as those by Rinon et al. [12] and Mokady et al. [36], proposed optimizing the text embedding to better align with the desired textual guidance. Recent efforts have further refined the optimization strategies for both the textual and visual prompts [35, 39], as well as for intermediate noise maps [13, 19, 20, 33, 51, 72], leading to notable enhancements in inversion quality. Despite these advances, existing methods mainly focus on image editing and cannot meet the high-fidelity requirements of SR. In this work, we tailor the diffusion inversion technique for SR. While Chihaoui et al. [5] have recently explored diffusion inversion for image restoration, their method relies on solving an optimization problem at each inversion step, significantly limiting its inference efficiency. In contrast, our approach introduces noise prediction module that, once trained, enables efficient inversion without requiring iterative optimization during inference. This leads to substantial improvements in both the efficiency and practicality of diffusion inversion for SR tasks. 3. Methodology In this section, we present the proposed diffusion inversion technique for SR. To maintain consistency with the notations used in diffusion models, we denote the LR image as y0 and the corresponding HR image as x0. 3.1. Motivation The diffusion model [16, 45] was first introduced as probabilistic generative model inspired by nonequilibrium thermodynamics. Subsequently, Song et al. [48] reformulated it within the framework of stochastic differential equations (SDEs). In this paper, we propose general diffusion inversion technique that is applicable to both the probabilistic and SDE-based diffusion formulations. To facilitate understanding, we employ the probabilistic framework of the Denoising Diffusion Probabilistic Model (DDPM) [16] throughout our presentation. The DDPM framework [16] is indeed Markov chain of length , where the forward process is characterized by Gaussian transition kernel: q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI), where βt is pre-defined hyper-parameter controlling variance schedule. Notably, this transition kernel allows the derivation of the marginal distribution q(xtx0), i.e., (1) q(xtx0) = (xt; αtx0, (1 αt)I), (2) where αt = (cid:81)t s=1 αs, αs = 1 βs. The reverse process aims to generate high-quality image from an initial random noise map xT (0, I), which can be expressed as: xt1 = gθ(xt, t) + σtzt1, = T, , 1, (3) where gθ(xt, t) = (cid:18) xt 1 αt 1 αt 1 αt (cid:19) ϵθ(xt, t) , (4) ϵθ(xt, t) is pre-trained denoising network parameterized by θ. The noise term zt satisfies z0 = 0 and zt (0, I) for = 1, , 1. Equation (3) indicates that the synthesized image x0 is fully determined by the set of noise maps = xT {zt}T 1 t=1 . In the context of SR, our goal is to generate an HR image x0 conditioned on an LR image y0. To this end, we propose diffusion inversion to find an optimal set of noise maps that reconstruct the target HR image x0 via the reverse process of Eq. (3). In the following sections, we detail how to achieve this goal by training noise predictor. 3.2. Diffusion Inversion To achieve diffusion inversion, we introduce noise prediction network with parameter w, denoted as fw, which takes the LR image y0 and the timestep as input and outputs the desired noise maps M. Unlike the strategy [5] of directly optimizing for each testing image, we train such noise predictor to enable fast sampling during inference, thereby significantly improving the inference efficiency. To ensure the output of fw conforms to Gaussian distribution, we adopt the reparameterization trick of VAE [25], which predicts the mean and variance parameters of Gaussian distribution rather than directly estimating the noise map. 3 3.2.1. Problem Simplification Training this noise predictor is inherently challenging. The noise map set consists of noise maps (typically = 1000 in most current diffusion models), corresponding to each step of the diffusion process. Naturally, it is non-trivial to simultaneously estimate such large number of noise maps using single, compact network. Whats worse, the iterative sampling paradigm of diffusion models can gradually accumulate prediction errors, which may adversely affect the final SR performance. To address these challenges, we design Partial Noise Prediction (PnP) strategy. Specifically, lets consider diffusion inversion in the context of SR, where the observed LR image y0 only slightly deviates from the target HR image x0 in most cases, primarily in high-frequency components. This observation inspires us to initiate the sampling process from an intermediate timestep (N < ), effectively reducing the number of noise maps in from to , i.e., = {zt}N t=1. Furthermore, given the high-fidelity requirements of SR, we constrain xN to have relatively high SNR, implying mild noise corruption. This constraint encourages the selection of smaller , and in practice, we set 250, corresponding to an SNR threshold of 1.44 in the widely used Stable Diffusion [43]. In addition, we further compress the set of the noise maps = {zt}N t=1 by integrating existing diffusion acceleration algorithms [22, 46]. The common idea of these algorithms is to skip certain steps during inference, which are selected based on specific rules [29], e.g., linspace and trailing. Combining with this skipping strategy, the noise map set is simplified as follows: = {zκi}M i=1, (5) where {κ1, , κM } {1, , }. In practice, we set 5, thus largely reducing the prediction burden on the noise predictor and improving the sampling efficiency. 3.2.2. Inversion Trajectory Given the set of noise maps = {zκi}M i=1 and the noise prediction network fw, our goal is to restore the HR image x0 from given LR observation y0, following an inversion trajectory defined by: xκi1 = gθ(xκi, κi) + σκifw(y0, κi1), (6) where κ0 = 0, and gθ(, ) is defined in Eq. (4). The key to initiating this inversion trajectory is constructing the starting state xκM from the LR image y0. The marginal distribution q(xκM x0), as defined in Eq. (2), suggests to achieve xκM as follows: xκM = (cid:112)ακM x0 + (cid:112)1 ακM ξ, ξ (0, I). (7) In the context of SR, since the HR image x0 is not accessible during testing, we thus construct an analogous formulation for xκM directly from the LR image y0 using the noise predictor fw(), namely xκM = (cid:112)ακM y0 + (cid:112)1 ακM fw(y0, κM ). (8) This design is inspired by the observation that the LR image y0 and the HR image x0 become increasingly indistinguishable when perturbed by Gaussian noise with an appropriate magnitude. Therefore, we aim to seek an optimal noise map fw(y0, κM ) to perturb y0 in such way that the pre-trained diffusion model can generate the corresponding x0 from xκM that is defined in Eq. (8). To summarize, we establish an inversion trajectory by combining Eqs. (6) and (8), which can be used to solve the SR problem via iterative generation along this trajectory. 3.2.3. Model Training Given pre-trained large-scale diffusion model ϵθ(), an estimation of the HR image x0 can be obtained from xκi by taking reverse diffusion step: ˆx0κi = 1 ακi (cid:104) (cid:105) xκi (cid:112)1 ακiϵθ(xκi, κi) , (9) where xκi is defined by Eq. (8) for = and Eq. (6) for < . It is thus possible to train the noise predictor fw() by minimizing the distance between ˆx0κi and x0. However, directly training with this objective is computationally impractical. Specifically, as shown in Eq. (6), calculating xκi (i < ) necessitates recurrent application of the large-scale diffusion model ϵθ, which leads to prohibitive GPU memory usage. To circumvent this, we adopt an alternative version for xκi based on the marginal distribution in Eq. (2), i.e., xκi = (cid:112)ακix0 + (cid:112)1 ακifw(y0, κi), < M. (10) This modification also aligns better with the training process of the employed diffusion model, allowing for more effective leveraging of the prior knowledge embedded in it. We now detail the training procedure step by step: Gaussian Constraint. The pre-trained diffusion model is powerful denoiser tailored for Gaussian noise with zero mean and varying variances. Hence, it is reasonable to enforce the predicted noise map by fw to obey Gaussian distribution. For the initial state xκM , it is observed that the predicted noise map fw(y0, κM ) exhibits mean shift, which is evident when comparing Eqs. (7) and (8), due to the substitution of y0 for x0. Moreover, the visualization presented in Figs. 2 and 3 further validates this observation, illustrating that the predicted noise map is clearly correlated with the LR image. Therefore, we do not consider the Gaussian constraint for xκM . 4 Algorithm 1 Inference Require: LR image y0, noise predictor fw, pre-trained i=1 inversion timesteps {τi}S diffusion model ϵθ, {κi}M i=1 1: xτS = 2: for = S, , 1 do 3: ατS y0 + 1 ατS fw(y0, τS) 4: zτi (0, I) if τi > 1 else zτi = 0 xτi1 = gθ(xτi , τi) + στizτi, where gθ is defined in Eq. (4) Figure 2. Inference flow of our proposed method, wherein {τi}S i=1 denotes the inversion timesteps. Note that the predicted noise map zτS exhibits an obvious correlation with the LR image, indicating the non-zero mean property of its statistical distribution. Conversely, for the intermediate state xκi as defined in Eq. (10), the predicted noise map fw(yκi, κi) should be enforced to follow (0, I). This naturally raises an interesting question: Is it necessary to predict the noise map using fw instead of random sampling? First, the proposed PnP strategy requires the timestep κi to satisfy high SNR constraint, indicating that xκi is corrupted by only mild Gaussian noise. Second, the pretrained large-scale diffusion model, specifically designed for Gaussian denoising, performs robustly, especially for timesteps κi with low noise levels. Thus, introducing an extra noise predictor model fw for the intermediate timesteps, even conditioned on the additional LR observation, does not yield significant performance gains. This is also empirically illustrated by an ablation study provided in supp. Third, predicting the noise map both for the initial and intermediate state would increase the prediction burden on fw and make the training process more challenging. Considering these reasons together, we discard the noise prediction for the intermediate states, which further reduces the noise map set to = {zκM }, leading to more elegant diffusion inversion technique, as detailed in Alg. 1. Arbitrary-step Inversion. As analyzed above, noise map prediction is only required for the starting state, as defined in Eq.(8), to initialize the reverse sampling process. To further enhance the flexibility of the sampling process, the noise predictor is trained to estimate the noise maps for multiple pre-selected steps via time embedding. Once trained, the starting timestep can be freely chosen during inference, which results in fidelity-realism trade-off, as analyzed in Sec. 4.2. Note that the total number of sampling steps is determined by both the selected starting timestep and the skipping stride of the accelerated sampling algorithm for diffusion models. For clarity, detailed inference procedure is provided in Fig. 2 and Alg. 1. Given practical efficiency considerations, we focused on the number of sampling steps 5: end for 6: return ranging from one to five in this study. Loss Function. To train the noise predictor, we adopt an L2 loss L2, LPIPS [74] loss Ll, and GAN [14] loss Lg following recent SR approaches [4, 55]. Lets denote the set of pre-selected starting timesteps for training as {κ1, , κM }, the overall loss function is defined as: (cid:88) L2( ˆx0t, x0) + λlLl( ˆx0t, x0) + λgLg( ˆx0t, x0), (11) tS where ˆx0t is defined in Eq. (9), λl and λg are hyperparameters. The adversarial loss Lg is implemented using hinge loss, with discriminator architecture based on diffusion UNet [16] enhanced with multi-in, multi-out strategy [65]. For the base model ϵθ, we use SD-Turbo [44], which operates in the latent space of VQGAN [10]. We thus compute the whole loss in the latent space, significantly reducing the required GPU memory. To facilitate training, the LPIPS loss is also fine-tuned in the latent space. 4. Experiments In this section, we first provide an analysis of the proposed method and then conduct extensive experiments to evaluate its performance on one synthetic and two real-world datasets. Our investigation focuses mainly on the 4 SR task following previous works [55, 73]. To ease the presentation, we refer to our method as InvSR, standing for Diffusion Inversion-based Super-Resolution. 4.1. Experimental Setup Training Details. Following the setup of recent works [59, 60], we trained the noise predictor on the LSDIR [28] dataset and subset of 20k face images from the FFHQ [21] dataset. At each iteration, we randomly cropped an image patch with resolution of 512 512 from the source image and synthesized the LR image using the pipeline of RealESRGAN [55]. The text prompt was fixed as general description1 in both the training and testing phases. To optimize the network parameters, we adopted the Adam [26] 1Cinematic, high-contrast, photo-realistic, 8k, ultra HD, meticulous detailing, hyper sharpness, perfect without deformations. 5 Table 1. Quantitative results of InvSR with various numbers of sampling steps ranging from one to five on the ImageNet-Test dataset. #Steps 5 3 1 Index of the sampled timesteps {250, 200, 150, 100, 50} {250, 150, 50} {200, 100, 50} {150, 100, 50} {250} {200} {150} {100} PSNR 22.70 22.92 23.41 23.84 23.84 24.14 24.42 24.66 SSIM 0.6412 0.6478 0.6609 0. 0.6713 0.6789 0.6851 0.6891 LPIPS 0.2844 0.2762 0.2648 0.2575 0.2575 0.2517 0.2469 0.2450 Metrics NIQE 4. 4.7980 4.5089 4.2719 4.5287 4.3815 4.2194 4.0606 PI 3.4744 3.4002 3.2074 3.0527 3.1748 3.0866 2.9979 2.8951 CLIPIQA 0. MUSIQ 69.8427 0.6823 0.6851 0.6823 0.7132 0.7093 0.7019 0.6912 70.4688 70.7024 70.4569 72.5773 72.2909 71.7100 70.8251 algorithm with default settings of PyTorch [42]. The training process takes over 100k iterations with batch size of 64 and fixed learning rate of 5e5. The hyper-parameters λl and λg in the loss function were set to 2.0 and 0.1, respectively. The architecture of the noise predictor was based on the encoder of VQGAN [10], containing two down-sampling blocks, each equipped with self-attention layer [50]. In the training stage, we randomly select starting timestep from = {250, 200, 150, 100} to train the noise predictor at each iteration. During inference, five inversion steps, i.e., = {250, 200, 150, 100, 50}, are used throughout our experiments. Testing Datasets. To evaluate the performance of InvSR, we constructed synthetic dataset named ImageNet-Test, comprising 3,000 images from the validation set of ImageNet [9]. The LR and HR images, with resolutions of 128 128 and 512 512, respectively, were synthesized using the degradation settings of ResShift [69]. Notably, we selected the HR images from ImageNet rather than the commonly used datasets in SR, such as Set5 [1], Set14 [70], and Urban100 [17], mainly because these datasets only contain very few source images, which fails to thoroughly assess various methods under complicated degradation types. We further conducted experiments on two real-world datasets to validate the effectiveness of InvSR. The first dataset is RealSR [3], which contains 100 real images captured by Canon 5D3 and Nikon D810 cameras. The second dataset, RealSet80 [69], comprises 80 LR images widely used in existing literature [18, 3032, 55, 68]. Compared Methods. We evaluate the effectiveness of InvSR in comparison to nine recent methods, including two GAN-based methods, namely BSRGAN [73] and RealESRGAN [55], as well as seven diffusion-based methods, including LDM [43], StableSR [53], DiffBIR [30], SeeSR [60], ResShift [68, 69], SinSR [57], and OSEDiff [59]. For LDM, StableSR, DiffBIR, and SeeSR, we all use 50 sampling steps for fair comparison. In the case of ResShift, SinSR, and OSEDiff, we adhere to the number of sampling steps suggested by their official guidelines. Figure 3. From left to right: (a) zoomed LR image, (b) predicted noise map by our method for the initial timestep, (c) superresolved results by our method with single sampling step. Metrics. The performance of various methods was assessed across seven metrics, including three reference metrics, namely PSNR, SSIM [58], LPIPS [74], as well as four nonreference metrics, namely NIQE [34], PI [2], MUSIQ [24], and CLIPIQA [52]. For evaluations on the datasets of ImageNet-Test and RealSR, all seven metrics were adopted to ensure holistic assessment. For the dataset of RealSet80, however, only non-reference metrics were employed since the ground truth images are not accessible. Notably, PSNR and SSIM are calculated in the luminance (Y) channel of YCbCr space, while other metrics are directly computed in the standard RGB (sRGB) space. 4.2. Model Analysis Arbitrary-steps Sampling. Recent efficient diffusionbased SR approaches, such as ResShift [69], SinSR [57], and OSEDiff [59], constrain the sampling process to predefined number of steps, consistent with their training configuration. In contrast, the proposed InvSR supports sampling with an arbitrary number of steps, significantly enhancing flexibility and adaptability to varying degradation types, as demonstrated in Fig. 1 and Fig. 6. 6 Table 2. Quantitative comparisons of different methods on ImageNet-Test and RealSR. The number of sampling steps is marked in the format of Method name-Steps for diffusion-based methods. The best and second-best results are highlighted in bold and underlined. Datasets Methods ImageNet-Test"
        },
        {
            "title": "RealSR",
            "content": "BSRGAN [73] RealESRGAN [55] LDM-50 [43] StableSR-50 [53] DiffBIR-50 [30] SeeSR-50 [60] ResShift-4 [69] SinSR-1 [57] OSEDiff-1 [59] InvSR-1 (Ours) BSRGAN [73] RealESRGAN [55] LDM-50 [43] StableSR-50 [53] DiffBIR-50 [30] SeeSR-50 [60] ResShift-4 [69] SinSR-1 [57] OSEDiff-1 [59] InvSR-1 (Ours) PSNR 27.05 26.62 27.19 24.77 25.72 26.69 27.33 26.98 23.95 24.14 26.51 25.85 26.75 26.27 24.83 26.20 25.77 26.02 23.89 24.50 SSIM 0.7453 0.7523 0.7285 0.6908 0.6695 0.7422 0.7530 0.7304 0.6756 0.6789 0.7746 0.7734 0.7711 0.7755 0.6642 0.7555 0.7453 0.7097 0.7030 0. LPIPS NIQE 4.5345 0.2437 4.4909 0.2303 5.2411 0.2286 4.5120 0.2591 4.5875 0.2795 4.3825 0.2187 0.1998 5.8700 5.2623 0.2209 4.7157 0.2624 4.3815 0.2517 0.2685 0.2728 0.2945 0.2671 0.3864 0.2806 0.3395 0.3993 0.3288 0.2872 4.6501 4.6766 4.8712 5.1745 3.7366 4.5358 6.9113 6.2547 5.3310 4.2189 Metrics PI 3.7111 3.7234 4.2554 3.1473 3.2260 3.4742 4.3643 3.8189 3.3775 3.0866 4.4644 4.4881 5.0025 4.8209 3.3661 4.1464 5.4013 4.7183 4.3584 3.7779 CLIPIQA MUSIQ 67.7195 64.8186 62.8776 71.2811 69.7089 71.2412 65.5860 67.7593 70.3928 72. 0.5703 0.5090 0.5554 0.7067 0.6900 0.5868 0.6147 0.6618 0.6818 0.7093 #Params (M) 16.70 16.70 113.60 152.70 385.43 751.50 118.59 118.59 8.50 33.84 0.5439 0.4898 0.4907 0.5209 0.6857 0.6824 0.5994 0.6634 0.7008 0.6918 63.5869 59.6803 54.3910 60.1758 65.3934 66.3757 57.5536 59.2981 65.4806 67.4586 16.70 16.70 113.60 152.70 385.43 751.50 118.59 118.59 8.50 33.84 We further provide comprehensive comparison of InvSR with one, three, and five sampling steps, as summarized in Table. 1. Three key observations can be obtained from these results: i) With fixed number of sampling steps, e.g., one or three, varying the starting timestep enables trade-off between fidelity (measured by reference metrics) and realism (measured by non-reference metrics). Specifically, using larger starting timesteps favors improved realism at the expense of fidelity. ii) As expected, reference metrics deteriorate with increased sampling steps due to the introduction of additional randomness. iii) Interestingly, non-reference metrics also exhibit decline when using more sampling steps. This is mainly because most testing images contain some noise, which can lead to undesired artifacts if multiple sampling steps are used, thus degrading the overall image quality. However, using more sampling steps can effectively recover intricate fine-grained structures in cases involving substantial blur, as evidenced by the first examples in Fig. 1 and Fig. 6. Initial Noise Prediction Figure 3 presents the noise map predicted by our method for the initial timestep, exhibiting strong correlation with image content. This visualization aligns well with the theoretical analysis in Sec 3.2.3, empirically validating that our noise predictor can effectively find an LR-dependent noise map to facilitate the SR task. 4.3. Comparison to State of the Arts Considering that recent studies [57, 59] mainly focus on developing one-step diffusion-based methods, we thus evaluTable 3. Quantitative comparisons of various methods on RealSet80. The number of sampling steps is marked in the format of Method name-Steps for diffusion-based methods. The best and second-best results are highlighted in bold and underlined."
        },
        {
            "title": "Methods",
            "content": "BSRGAN [73] NIQE 4.4408 RealESRGAN [55] 4.1568 4.3248 4.5593 3.8630 4.3678 5.9866 5.6243 4.3457 4.0284 LDM-50 [43] StableSR-50 [53] DiffBIR-50 [30] SeeSR-50 [60] ResShift-4 [69] SinSR-1 [57] OSEDiff-1 [59] InvSR-1 (Ours)"
        },
        {
            "title": "Metrics",
            "content": "PI 4.0276 3.8852 4.2545 4.0977 3.2117 3.7429 4.8318 4.2830 3.8219 3.4666 CLIPIQA MUSIQ 66.6288 64.4957 55.8246 62.7613 67.9806 69.7658 61.7967 64.0573 68.8202 69.8055 0.6263 0.6189 0.5511 0.6214 0.7404 0.7114 0.6515 0.7228 0.7093 0.7291 ate InvSR against these methods under one-step configuration to ensure fair comparison. Synthetic Dataset. Table 2 reports comprehensive evaluation of various methods on ImageNet-Test dataset, encompassing seven quantitative metrics, with additional qualitative comparisons included in Fig. 8 of supp. Notably, compared to the recent state-of-the-art (SotA) one-step method OSEDiff [59], InvSR demonstrates evident superiority across all the seven metrics. Moreover, even compared to multi-step methods with 50 sampling steps, such as StableSR and DiffBIR, InvSR still achieves comparable performance in distortion-oriented metrics, including Figure 4. Visual results of different methods on two typical real-world examples from RealSet80 dataset. For clear comparisons, the number of sampling steps is annotated in the format Method name-Steps for diffusion-based approaches. (Zoom-in for best view) PSNR and SSIM, while outperforming these methods in perception-oriented metrics, such as LPIPS, NIQE, PI, and MUSIQ. These results indicate that InvSR effectively balances both performance and efficiency, advancing the field of diffusion-based SR approaches. Additionally, InvSR maintains moderate model size with about 34 million learnable parameters, further enhancing its practicality for real-world applications. Real-world Dataset. To evaluate real-world datasets, we mainly focus on the non-reference metrics. Table 2 and 3 provide detailed comparison of InvSR against recent SotA methods on the datasets of RealSR and RealSet80, respectively. It can be easily observed that InvSR achieves superior performance across most non-reference metrics compared to recent one-step methods under fair comparison and second-best results compared to existing multi-step methods. To further substantiate these conclusions, we present visual comparisons of two real-world examples in Fig. 4, and more examples can be found in Fig. 9 of supp. In the first example, where the LR image contains evident compression noise, InvSR successfully removes these artifacts and generates clear results, while other methods struggle with remaining artifacts. In the second example, which is degraded by noticeable blurriness, InvSR produces sharper image structures, such as the tile edges on the wall. These quantitative and qualitative evaluations highlight the great potential of InvSR to solve the real-world SR task. 5. Conclusion We proposed InvSR, new SR method based on diffusion inversion. Our method introduces noise prediction network designed to estimate an optimal noise map, enabling the construction of an intermediate state of pre-trained diffusion model as the starting sampling point. This design is appealing in two aspects: first, InvSR can sufficiently harness the prior knowledge encapsulated in the pre-trained diffusion model, thereby facilitating SR performance. Second, InvSR offers flexible sampling strategy capable of starting from various intermediate states of the diffusion model by incorporating time-dependent architecture of the noise predictor. This flexibility allows users to freely adjust the sampling steps according to the degradation type or their specific requirements. Even after reducing the sampling steps to just one, InvSR still exhibits significant superiority beyond recent one-step diffusion-based methods, suggesting its effectiveness and efficiency. 8 Acknowledgement. This study is supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contributions from the industry partner(s)."
        },
        {
            "title": "References",
            "content": "[1] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. 2012. 6 [2] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on perceptual image super-resolution. In Eur. Conf. Comput. Vis. Worksh., pages 00, 2018. 6 [3] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: new benchmark and new model. In Int. Conf. Comput. Vis., pages 30863095, 2019. 6 [4] Kelvin CK Chan, Xiangyu Xu, Xintao Wang, Jinwei Gu, and Chen Change Loy. Glean: Generative latent bank for image super-resolution and beyond. IEEE Trans. Pattern Anal. Mach. Intell., 45(3):31543168, 2022. 2, 5 [5] Hamadi Chihaoui, Abdelhak Lemkhenter, and Paolo Favaro. Blind image restoration via fast diffusion inversion. In Adv. Neural Inform. Process. Syst., 2024. 3 [6] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. In Adv. Neural Inform. Process. Syst., pages 2568325696, 2022. 3 [7] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1241312422, 2022. 2, 3 [8] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In Int. Conf. Learn. Represent., 2023. 3 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., pages 248255, 2009. 6 [10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In IEEE transformers for high-resolution image synthesis. Conf. Comput. Vis. Pattern Recog., pages 1287312883, 2021. 5, 6 [11] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai. Generative diffusion prior for unified image restoration and enhancement. In IEEE Conf. Comput. Vis. Pattern Recog., pages 99359946, 2023. [12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [13] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. ReNoise: Real image In Eur. Conf. Comput. inversion through iterative noising. Vis., 2024. 2, 3 [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Adv. Neural Inform. Process. Syst., 27, 2014. 2, 5, 12 [15] Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. In IEEE Conf. Comput. Vis. Pattern Recog., pages 30123021, 2020. 2 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Adv. Neural Inform. Process. Syst., pages 68406851, 2020. 2, 3, 5 [17] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In IEEE Conf. Comput. Vis. Pattern Recog., pages 51975206, 2015. [18] Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc Van Gool. Dslr-quality photos on mobile devices with deep convolutional networks. In Int. Conf. Comput. Vis., 2017. 6 [19] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In Int. Conf. Learn. Represent., 2024. 3 [20] Wonjun Kang, Kevin Galim, and Hyung Il Koo. Eta inversion: Designing an optimal eta function for diffusion-based real image editing. In Eur. Conf. Comput. Vis., 2024. 3 [21] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 44014410, 2019. 5 [22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Adv. Neural Inform. Process. Syst., pages 26565 26577, 2022. 2, 4 [23] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In Adv. Neural Inform. Process. Syst., pages 2359323606, 2022. 2, 3 [24] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Int. Conf. Comput. Vis., pages 51485157, 2021. 6 [25] Diederik Kingma. Auto-encoding variational bayes. In Int. Conf. Learn. Represent., 2014. [26] Diederik P. Kingma and Jimmy Ba. Adam: method for In Int. Conf. Learn. Represent., stochastic optimization. 2015. 5 [27] Jianze Li, Jiezhang Cao, Zichen Zou, Xiongfei Su, Xin Yuan, Yulun Zhang, Yong Guo, and Xiaokang Yang. Distillation-free one-step diffusion for real-world image super-resolution. arXiv preprint arXiv:2410.04224, 2024. 3 [28] Yawei Li, Kai Zhang, Jingyun Liang, Jiezhang Cao, Ce Liu, Rui Gong, Yulun Zhang, Hao Tang, Yun Liu, Denis Demandolx, Rakesh Ranjan, Radu Timofte, and Luc Van Gool. LsIn IEEE dir: large scale dataset for image restoration. Conf. Comput. Vis. Pattern Recog. Worksh., pages 1775 1787, 2023. 5 9 [29] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are In Proceedings of the IEEE/CVF Winter Conf. on flawed. Applications of Comput. Vision, pages 54045411, 2024. 4 [30] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong. DiffBIR: Towards blind image restoration with generative diffusion prior. arXiv preprint arXiv:2308.15070, 2023. 2, 3, 6, 7 [31] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Int. Conf. Comput. Vis., pages 416423, 2001. [32] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications, 76:2181121838, 2017. [33] Barak Meiri, Dvir Samuel, Nir Darshan, Gal Chechik, Shai Avidan, and Rami Ben-Ari. Fixed-point inversion for text-toimage diffusion models. arXiv preprint arXiv:2312.12540, 2023. 3 [34] Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal processing letters, 20(3):209212, 2012. 6 [35] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. arXiv preprint arXiv:2305.16807, 2023. 3 [36] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 60386047, 2023. 2, 3 [37] Chong Mou, Yanze Wu, Xintao Wang, Chao Dong, Jian Zhang, and Ying Shan. Metric learning based interactive In Eur. Conf. modulation for real-world super-resolution. Comput. Vis., pages 723740. Springer, 2022. 12 [38] Naoki Murata, Koichi Saito, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Gibbsddrm: partially collapsed gibbs sampler for solving blind inverse problems with denoising diffusion restoration. In Int. Conf. Mach. Learn., pages 2550125522. PMLR, 2023. [39] Thao Nguyen, Yuheng Li, Utkarsh Ojha, and Yong Jae Lee. Visual instruction inversion: Image editing via image prompting. In Adv. Neural Inform. Process. Syst., 2024. 3 [40] Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, and Georgios Tzimiropoulos. You only need one step: Fast super-resolution with stable diffusion via scale distillation. In Eur. Conf. Comput. Vis., 2024. 3 [41] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. IEEE Trans. Pattern Anal. Mach. Intell., 44(11):74747489, 2021. 2 [42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Adv. Neural Inform. Process. Syst., 2019. 6 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1068410695, 2022. 2, 4, 6, 7, 12 [44] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In Eur. Conf. Comput. Vis., pages 87103. Springer, 2024. [45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Int. Conf. Mach. Learn., pages 22562265. PMLR, 2015. 2, 3 [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. DenoisIn Int. Conf. Learn. Repreing diffusion implicit models. sent., 2021. 2, 3, 4 [47] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In Int. Conf. Learn. Represent., 2023. 3 [48] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In Int. Conf. Learn. Represent., 2021. 2, 3 [49] Haoze Sun, Wenbo Li, Jianzhuang Liu, Haoyu Chen, Renjing Pei, Xueyi Zou, Youliang Yan, and Yujiu Yang. Coser: Bridging image and language for cognitive super-resolution. In IEEE Conf. Comput. Vis. Pattern Recog., pages 25868 25878, 2024. [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Adv. Neural Inform. Process. Syst., 2017. 6 [51] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2253222541, 2023. 3 [52] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, 2023. 6 [53] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K. Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. Int. J. Comput. Vis., pages 121, 2024. 3, 6, 7 [54] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In IEEE Conf. Comput. Vis. Pattern Recog., 2018. 3 [55] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In Int. Conf. Comput. Vis. Worksh., pages 19051914, 2021. 5, 6, [56] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. In Int. Conf. Learn. Represent., 2023. 2, 3 [57] Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao, Alex 10 [72] Guoqiang Zhang, Jonathan Lewis, and Bastiaan Kleijn. Exact diffusion inversion via bidirectional integration approximation. In Eur. Conf. Comput. Vis., pages 1936, 2024. 3 [73] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind In Int. Conf. Comput. Vis., pages image super-resolution. 47914800, 2021. 5, 6, 7 [74] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In IEEE Conf. Comput. Vis. Pattern Recog., pages 586595, 2018. 5, 6, [75] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. Indomain gan inversion for real image editing. In Eur. Conf. Comput. Vis., pages 592608. Springer, 2020. 2 Kot, and Bihan Wen. Sinsr: diffusion-based image superresolution in single step. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2579625805, 2024. 6, 7 [58] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process., 13(4):600612, 2004. 6 [59] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang. One-step effective diffusion network for real-world image In Adv. Neural Inform. Process. Syst., super-resolution. 2024. 3, 5, 6, 7, 13 [60] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. SeeSR: Towards semantics-aware real-world image super-resolution. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2545625467, 2024. 2, 3, 5, 6, 7 [61] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. In Int. Conf. Comput. Vis., pages 1309513105, 2023. 3 [62] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: survey. IEEE Trans. Pattern Anal. Mach. Intell., 45(3):31213138, 2022. [63] Jie Xiao, Ruili Feng, Han Zhang, Zhiheng Liu, Zhantao Yang, Yurui Zhu, Xueyang Fu, Kai Zhu, Yu Liu, and ZhengJun Zha. DreamClean: Restoring clean image using deep diffusion prior. In Int. Conf. Learn. Represent., 2024. 3 [64] Tao Yang, Rongyuan Wu, Peiran Ren, Xuansong Xie, and Lei Zhang. Pixel-aware stable diffusion for realistic image super-resolution and personalized stylization. In Eur. Conf. Comput. Vis., 2024. 2, 3 [65] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In Adv. Neural Inform. Process. Syst., 2024. 5 [66] Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, Xintao Wang, Jingwen He, Yu Qiao, and Chao Dong. Scaling up to excellence: Practicing model scaling for photorealistic image restoration in the wild. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2566925680, 2024. 3 [67] Zongsheng Yue and Chen Change Loy. DifFace: Blind face restoration with diffused error contraction. IEEE Trans. Pattern Anal. Mach. Intell., 2024. 3 [68] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: image superresolution by residual shifting. In Adv. Neural Inform. Process. Syst., pages 1329413307, 2023."
        },
        {
            "title": "Efficient diffusion model",
            "content": "for [69] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Efficient diffusion model for image restoration by residual shifting. IEEE Trans. Pattern Anal. Mach. Intell., 2024. 6, 7 [70] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In Int. Conf. on Curves and Surfaces, pages 711730. Springer, 2012. 6 [71] Aiping Zhang, Zongsheng Yue, Renjing Pei, Wenqi Ren, Degradation-guided one-step imand Xiaochun Cao. age super-resolution with diffusion priors. arXiv preprint arXiv:2409.17058, 2024. 3 11 Arbitrary-steps Image Super-resolution via Diffusion Inversion"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplemental material mainly contains: Sec. discusses the selection of the number of sampling steps. Performance comparison of InvSR with various base diffusion models in Sec. B.1. Ablation studies on the intermediate noise prediction model in Sec. B.2. Ablation studies on the loss function in Sec. B.3. Discussions on the efficiency and limitation in Sec. B.4. Visual comparisons on ImageNet-Test dataset in Fig. 8. More visual comparisons on real-world examples in Fig. 9. A. Discussion on Sampling Steps The proposed method, named InvSR, enables flexible sampling mechanism that allows an arbitrary number of sampling steps. This naturally raises an interesting question: how do we determine an appropriate number of sampling steps for general image super-resolution (SR) tasks? We answer this question from two aspects. First, as shown in Tables 2 and 3, and Fig. 3 of the main text, InvSR achieves promising results with only single sampling step, evidently outperforming recent state-of-theart (SotA) one-step methods. Therefore, we recommend setting the sampling steps to one for most real-world applications, effectively balancing efficiency and performance. Second, we can also adjust sampling steps according to the type of image degradation. Generally, image degradations can be categorized into two main classes: blurriness and noise. As illustrated in Fig. 1 and Fig. 6, multi-step sampling would incorrectly amplify noise, leading to undesirable artifacts for images with heavy noise. In contrast, for images primarily suffering from blurriness, multi-step sampling proves beneficial, as it generates more detailed and realistic image structures. In practice, we could first estimate the noise level using some off-the-shelf degradation estimation models, such as Mou et al. [37]. Based on the estimated noise level, one can determine whether onestep or multi-step sampling is more appropriate. In cases where multi-step sampling is favored, the number of sampling steps can be freely adjusted to achieve satisfactory result. B. Experiments B.1. Base Diffusion Model Figure 5. typical visual comparison of the proposed InvSR based on different diffusion models: SD-2.0 and SD-Turbo. Note that these results are achieved with five sampling steps. namely SD-2.02 and SD-Turbo3. Table 4 provides quantitative comparison of InvSR equipped with these two base models. When reducing the sampling steps to one, InvSR demonstrated similar performance with both SD-2.0 and SD-Turbo. However, in the multi-step sampling scenarios, the model based on SD-Turbo exhibited more stable performance, particularly in terms of reference metrics. Furthermore, visual comparison under five sampling steps, as illustrated in Fig. 5, reveals that the SD-2.0-based model produced noticeable artifacts, aligning with the quantitative results. We thus employed SD-Turbo as our base model throughout this study. B.2. Intermediate Noise Prediction In our proposed diffusion inversion framework, we opt to sample the noise maps randomly rather than employing noise prediction model for intermediate timesteps. This choice is motivated by the high SNR (signal-to-noise ratio) constraint imposed on the inversion timesteps, as elaborated in Sec. 3.2.3 of the main text. To further validate this choice, we introduced an additional baseline, denoted as InvSR-Int, which integrates an extra noise predictor specifically trained for intermediate timesteps. Table 5 reports detailed comparison between InvSR and InvSR-Int. It can be observed that the performance differences between these two models are negligible. Therefore, we omit the intermediate noise prediction in InvSR, further simplifying the overall framework. B.3. Loss Functions In addition to the commonly used L2 loss, we incorporate LPIPS [74] loss and GAN [14] loss to train our noise predictor, as formulated in Eq. (11). The hyper-parameters of λl and λg are introduced to control the importance of the LPIPS and GAN losses, respectively. Table 6 provides For the pre-trained diffusion models used in InvSR, we considered two prevailing variants of Stable Diffusion [43], 2https://huggingface.co/stabilityai/stable-diffusion-2-base 3https://huggingface.co/stabilityai/sd-turbo Table 4. Quantitative comparisons of the proposed InvSR equipped with two different based models, namely SD-2.0 and SD-Turbo, on the dataset of ImageNet-Test. Base models #Steps SD-Turbo SD-2.0 SD-Turbo SD-2.0 SD-Turbo SD-2. 5 3 1 Index of the sampled timesteps {250, 200, 150, 100, 50} {150, 100, 50} {200} PSNR 22.70 21.40 23.84 23.13 24.14 23.36 SSIM 0.6412 0.6063 0.6713 0. 0.6789 0.6637 LPIPS 0.2844 0.3274 0.2575 0.2776 0.2517 0.2647 Metrics NIQE 4.8757 5.1508 4.2719 4. 4.3815 4.3304 PI 3.4744 3.8709 3.0527 3.1467 3.0866 3.1545 CLIPIQA MUSIQ 69.8427 67.6056 0.6733 0. 0.6823 0.6722 0.7093 0.6969 70.4569 69.5178 72.2909 71.4974 Table 5. Quantitative comparisons of InvSR to the baseline method InvSR-Int that combines an additional noise predictor for the intermediate timesteps on the dataset of ImageNet-Test. Index of the sampled timesteps Methods Metrics #Steps PSNR 22.70 22.70 SSIM 0.6412 0.6412 LPIPS 0.2844 0.2844 NIQE 4.8757 4. PI 3.4744 3.4718 CLIPIQA MUSIQ 69.8427 69.8466 0.6733 0.6734 InvSR InvSR-Int 5 {250, 200, 150, 100, 50} Table 6. Quantitative ablation studies on the loss function in Eq. (11), wherein the hyper-parameters λl and λg control the weight importance of the LPIPS loss and the GAN loss, respectively."
        },
        {
            "title": "Methods",
            "content": "Baseline1 Baseline2 Baseline3 InvSR-1 Hyper-parameters λl (LPIPS loss) 0.0 2.0 0.0 2.0 λg (GAN loss) 0.0 0.0 0.1 0.1 PSNR 26.71 26.24 24.11 24.14 SSIM 0.7365 0.7274 0.6809 0. LPIPS 0.2850 0.2841 0.2599 0."
        },
        {
            "title": "Metrics",
            "content": "NIQE 9.2792 8.4367 4.4518 4.3815 PI 6.4147 5.7973 3.1229 3.0866 CLIPIQA MUSIQ 64.6069 66.1726 72.5045 72.2909 0.6168 0.6501 0.7078 0.7093 Table 7. Efficiency comparisons of different methods on the x4 (128 512) SR task, where the runtime results are tested on an NVIDIA A100 GPU with 40GB memory. For diffusion-based SR approaches, the number of sampling steps is annotated in the format of Method name-Steps."
        },
        {
            "title": "Metrics",
            "content": "#Params (M) Runtime (ms) Methods BSRGAN RealESRGAN StableSR-50 DiffBIR-50 SeeSR-50 ResShift-4 SinSR-1 OSEDiff-1 InvSR-1 33.84 117 751.50 6438 118.59 138 118.59 319 385.43 152.70 3459 16.70 65 16.70 65 8.50 176 having larger number of parameters compared to the recent SotA method OSEDiff [59], InvSR achieves 50% reduction in inference time. This is mainly because OSEDiff relies on an additional image captioning model, whereas InvSR does not. However, it is noteworthy that InvSR still lags behind GAN-based methods in efficiency due to its reliance on the large-scale Stable Diffusion model. To address the high-efficiency demand in real-world applications, future work will explore model quantization techniques to further accelerate the inference speed. quantitative comparison of various baseline models under different loss configurations, and Fig. 7 demonstrates typical visual example. We can observed that Baseline1 trained solely with the L2-based diffusion loss produces over-smooth outputs, which is consistent with its superior PSNR scores. Incorporating the GAN loss enhances the generation of finer image details but may introduce undesirable artifacts. The addition of the LPIPS loss can mitigate these artifacts to certain extent, striking balance between perceptual quality and artifact suppression. Therefore, this study employs both LPIPS and GAN losses to achieve optimal performance. B.4. Efficiency and Limitation Table 7 lists an efficiency comparison of various methods on the x4 (128 512) SR task. It can be observed that the proposed InvSR demonstrates advantages in runtime among one-step diffusion-based approaches. Despite 13 Figure 6. Qualitative comparisons of the proposed InvSR with different sampling steps, where the number of sampling steps is annotated in the format InvSR-Steps. In the first example, mainly degraded by blurriness, multi-step sampling is preferable to single-step sampling as it progressively recovers finer details. Conversely, in the second example with severe noise, single sampling step is sufficient to achieve satisfactory results, whereas additional steps may amplify the noise and introduce unwanted artifacts. (Zoom-in for best view) Figure 7. Visual comparisons of the proposed method with various loss configurations. (a) Zoomed LR image, (b) Baseline1 with λl = 0 and λg = 0, (c) Baseline2 with λl = 2.0 and λg = 0, (d) Baseline3 with λl = 0 and λg = 0.1, (e) recommended settings of λl = 2.0 and λg = 0.1. (Zoom-in for best view) Figure 8. Visual comparisons of various methods on three typical examples from ImageNet-Test. For diffusion-based methods, the number of sampling steps is annotated in the format of Method name-Steps. (Zoom-in for best view) 15 Figure 9. Visual comparisons of various methods on four real-world examples from RealSet80. For diffusion-based methods, the number of sampling steps is annotated in the format of Method name-Steps. (Zoom-in for best view)"
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University"
    ]
}