{
    "paper_title": "Streaming Video Instruction Tuning",
    "authors": [
        "Jiaer Xia",
        "Peixian Chen",
        "Mengdan Zhang",
        "Xing Sun",
        "Kaiyang Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 4 3 3 1 2 . 2 1 5 2 : r Streaming Video Instruction Tuning"
        },
        {
            "title": "Streaming Video Instruction Tuning",
            "content": "Jiaer Xia1 Peixian Chen2 Mengdan Zhang2 Xing Sun2 Kaiyang Zhou1 1 Hong Kong Baptist University 2 Tencent Youtu Lab Equal contribution Corresponding author We present Streamo, real-time streaming video LLM that serves as general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making step toward unified, intelligent video understanding in continuous video streams. Code: https://github.com/maifoundations/Streamo Data: https://huggingface.co/datasets/maifoundations/Streamo-Instruct-465K Project: https://jiaerxia.github.io/Streamo/"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in video large language models (LLMs) [Zhang et al., 2023, Lin et al., 2023, Wang et al., 2022, Bain et al., 2021] have demonstrated remarkable capabilities in analyzing complete, pre-recorded videos, which establish strong baselines for offline video understanding. These models excel at holistic reasoning over long temporal sequences when given static, temporally bounded inputs [Zhao et al., 2023, Laghari et al., 2023], enabling tasks such as video captioning, summarization, and question answering. However, the requirements of real-time interactive AI assistants are fundamentally different: they must process continuous, unbounded video streams and respond to dynamic instructions as events unfold, often under strict latency constraints. Existing offline models struggle to meet the demands of the streaming setting because they are designed to process entire clips before producing single output [Wang et al., 2025, Qian et al., 2025, Wang et al., 2024]. In contrast, real-time applications require the model to continuously interpret an incoming video stream, detect when the visual context satisfies task condition, and decide what information to output at that moment. This introduces two key challenges: 1) handling continuous, unbounded data flow without losing context, and 2) managing variable response timing and granularity across multiple tasks, which may require frame-level or longer-term temporal reasoning. truly capable streaming video LLM must therefore integrate both task understanding and frame-level decision-making, enabling it to evaluate evolving visual contexts, determine appropriate moments to respond, and generate coherent outputs without delaying or missing critical information. To address these challenges, recent studies [Wang et al., 2025, Qian et al., 2025, Wang et al., 2024] have attempted to extend offline video models for streaming by introducing separate decision module that 1 Streaming Video Instruction Tuning Figure 1. An example of multi-task annotation in Streamo-Instruct-465K. Each task is carefully labeled with the corresponding response time boundaries and content, following established annotation standards. The same video is annotated with multiple distinct tasks. The video shown in this example is sourced from ActivityNet [Caba Heilbron et al., 2015]. predicts response states before invoking the offline model to generate content. While this approach preserves the reasoning capacity of the base model, it creates trade-off between accuracy and efficiency: lightweight decision modules often lack the capacity to fully understand complex instructions and temporal dependencies, while larger modules substantially increase computational cost and inference latency. Moreover, separating decision-making from response generation prevents tight coupling between perception and response, limiting the models ability to seamlessly adapt to rapidly changing streaming contexts. In this work, we propose Streamo1, real-time streaming video LLM that unifies decision-making and response generation in an end-to-end manner. Instead of relying on an external controller, we embed frame-level response state prediction directly into the model. Specifically, three decision headsSilence, Standby, and Responseallow the model to continuously monitor the input stream and make fine-grained judgments about when to output. Once response state is triggered, the model immediately produces the corresponding textual output, achieving one-pass inference that significantly improves both the accuracy of response timing and the efficiency of real-time generation. Training Streamo requires high-quality, temporally consistent supervision, yet existing datasets often combine heterogeneous sources with inconsistent annotation standards [Han et al., 2023, Gao et al., 2017, Huang et al., 2020]. These inconsistencies make it difficult for the model to learn precise temporal alignment or multi-task response behaviors. To overcome this problem, we construct Streamo-Instruct-465K, large-scale, multi-task instruction-following dataset designed specifically for streaming video understanding and interaction. The dataset standardizes three levels of response granualarity, provides unified temporal annotations for event boundaries, and covers diverse tasks including real-time narration, action and event captioning, temporal grounding, and time-sensitive question answering. Each video is annotated for multiple tasks, providing consistent guidance that strengthens both instruction-following and temporal reasoning. An example of the annotations is shown in Fig. 1. 1The letter in Streamo means omni, reflecting its multi-task and multi-modal capabilities. 2 Streaming Video Instruction Tuning Extensive experiments demonstrate that our end-to-end training paradigm effectively converts offline models into online streaming assistants. Streamo outperforms existing online approaches across both streaming and offline benchmarks, exhibiting strong temporal awareness, accurate frame-level decision-making, and robust multi-task instruction-following. To further support research in this domain, we also introduce comprehensive streaming benchmark named Streamo-Bench, which evaluates instruction understanding across diverse interactive tasks. Our contributions are threehold: 1) We propose simple and effective end-to-end training framework that converts offline video models into real-time straeming assistants. 2) We introduce multi-task instruction tuning dataset with unified temporal annotation and fine-grained response supervision. To our knowledge, this is the largest scale instruction tuning dataset for streaming video understanding and interaction. 3) We establish comprehensive benchmark for streaming video instruction-following and provide strong baseline models for future research. All research resources including code, models, and datasets will be made publicly available."
        },
        {
            "title": "2 Related Work",
            "content": "Video Large Language Models The field of vision foundation models [Liu et al., 2023, Maaz et al., 2024, Liu et al., 2024, Chen et al., 2024] has made remarkable progress in recent years, extending capabilities from static image understanding to more general video comprehension. Building on this foundation, numerous advanced video LLMs have emerged. For example, InternVideo2.5 [Wang et al., 2025] can process videos spanning several hours, while Keye-VL-1.5 [Yang et al., 2025] demonstrates sophisticated reasoning abilities, effectively performing complex thinking process based on video content. critical limitation, however, is that these state-of-the-art models operate in an offline fashion, requiring the entire video as input before producing any output. This single-pass approach prevents them from handling continuous video streams, as they lack mechanisms to identify the precise temporal moments for generating responses in ongoing streams. Streaming Video Understanding To tackle real-time interaction, various methods have been proposed in the literature to turn offline video LLMs into online assistants that can identify the appropriate moment to respond in video streams. For instance, Dispider [Qian et al., 2025] and StreamBridge [Wang et al., 2025] employ an auxiliary model to segment video stream into fixed-length clips before feeding them to an offline model. However, this strategy introduces significant computational overhead in both training and inference and often fails to maintain context during multi-turn interactions. On the other hand, VideoLLMOnline [Chen et al., 2024] and StreamingVLM [Xu et al., 2025] train the model in supervised way to directly predict response timing using special [EOS] token. However, this approach is limited to real-time narration and cannot balance between silence and response state. To overcome these problems, we propose an end-to-end training framework along with multi-task instruction-following dataset specifically designed for streaming video understanding and interaction. Streaming Video Benchmarks OVO-Bench [Li et al., 2025] introduces 12 distinct tasks, incorporating tests for models ability to proactively respond. Similarly, STREAMBENCH [Xiong et al., 2025] and SVBENCH [Yang et al., 2025] concentrate on assessing multi-turn conversational abilities within continuous video contexts. key limitation, however, is their predominant reliance on question-answer (QA) style setupstypically requiring the model to choose an answer from given optionswhich does not adequately assess broader instruction-following abilities such as event grounding and captioning. Motivated by the goal that streaming video models should evolve into real-time AI assistants, we introduce Streamo-Bench, benchmark designed to probe models perceptual and responsive capabilities across diverse instructions, moving beyond the constraints of traditional QA-based evaluation. 3 Streaming Video Instruction Tuning"
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Traditional video understanding models [Bai et al., 2023, Chen et al., 2024] follow an offline paradigm where the complete video V, question Q, and answer are processed using single-turn format. Formally, given video = {v1, v2, ..., vT} of length and question Q, the model directly generates an answer A. This approach assumes that the entire video is accessible before inference begins, which is impractical for real-time streaming scenarios where video frames arrive sequentially. In contrast to offline settings, streaming video understanding processes video content as it arrives in continuous stream. The model must make decisions based on partial observations V:t = v1, v2, ..., vt, where Tt meaning that the model does not have access to future frames. This temporal constraint requires fundamental changes to both the data structure and training paradigm."
        },
        {
            "title": "3.2 Data Structure",
            "content": "To simulate streaming scenarios during training, we reformulate the single-turn offline format into multiturn dialogue structure. Specifically, complete video is temporally segmented into contiguous segments: = {V(1), V(2), ..., V(N)} (1) where V(i) denotes the i-th video segment. Each segment is explicitly annotated with temporal boundaries using special markers, e.g., <2s-3s>, to encode temporal information. The multi-turn dialogue is constructed as: = {(V(1), R(1)), (V(2), R(2)), ..., (V(N), R(N))} (2) where R(i) denotes the response at turn i. Questions and answers are strategically inserted at appropriate turns based on the dataset characteristics and task requirements. To enable efficient parallel training while maintaining compatibility with standard supervised fine-tuning paradigms, we convert decision process into predictions for the following state tokens: <Silence>: The model remains silent and continues processing incoming frames. <Standby>: The model detects relevant video input and waits for complete information. <Response>: The model receives enough information and will generate response. This design empowers the model with frame-level decision-making capabilities while maintaining the nexttoken prediction framework. As illustrated in Fig. 3, three discrete response states are directly integrated into the normal token prediction process: the model outputs <Standby> upon detecting relevant input and <Response> when it is ready to answer. training example is shown in Tab. 2. With this multi-turn dialogue training format, we can simulate realistic streaming video interactions and pose questions at any point in time."
        },
        {
            "title": "3.3 Training",
            "content": "The multi-turn streaming format introduces severe class imbalance among the three response states. In typical streaming scenarios, <Silence> tokens dominate the distribution (often more than 80% of the time), 4 Streaming Video Instruction Tuning Figure 2. The format of multi-turn dialogue. SYSTEM PROMPT USER ASSISTANT <Silence> <0s-1s><video> USER <1s-2s><video> Notify me when the light turns green. ASSISTANT <Silence> USER ASSISTANT <Silence> <2s-3s><video> USER ASSISTANT <Standby> <3s-4s><video> USER ASSISTANT <Response> The light just turned green. <4s-5s><video> Figure 3. Streamos architecture. Streaming video data is organized into an interleaved, multi-turn dialogue structure that directly integrates response-state token into the data sequence, enabling end-to-end parallel training. while <Response> tokens are sparse. This imbalance biases the model toward remaining silent, making it difficult to learn response timing. To mitigate this, we apply focal weighting [Lin et al., 2017] specifically to the three special state tokens. Let = {ssilence, sstandby, sresponse} denote the special token for the three states. For each prediction, we compute focal weight that emphasizes hard examples: wfocal(xi) = (1 pci )γ, (3) where xi represents the input features at position i, and pci is the predicted probability for the true class ci at position i. γ 0 is the focusing parameter that controls the rate at which easy examples are down-weighted. To further balance the rare classes, we introduce frequency-based alpha weights. For each special token with count nk in the current batch: where = 3 is the number of special states. This assigns larger weights to less frequent special tokens. αk = 1 jS nj nk , The final loss combines the focal weighting and frequency balancing: Li = αti wfocal(i) LCE(i, ti), ti LCE(i, ti), otherwise , (4) (5) The two weighting mechanisms are computed independently and multiplied into the cross-entropy loss. Together, they focus the model on both challenging and infrequent tokens, improving learning of response timing despite severe class imbalance in streaming data. The LCE is the standard cross-entropy loss: LCE(i, ti) = log pti = log j= ezi,j zi,ti , (6) where zi,j is the logit for token at position and is the vocabulary size. This computes the negative log-likelihood of the true token. The total loss averages over all valid (non-masked) positions indicated by M: Ltotal = 1 iM Li. (7) This ensures that the loss is not affected by sequence length variations across examples in the batch. 5 Streaming Video Instruction Tuning Figure 4. Dataset distribution overview. Left: task distribution; Right: video duration distribution."
        },
        {
            "title": "4.1 Data Construction",
            "content": "To provide clear supervision for each round of response decisions, we re-annotated large-scale training set with detailed temporal boundary labels based on the existing open-source video datasets. We predefined multiple tasks spanning different response granularities, assigning each video several types of task annotations. This approach offers several advantages. First, unified annotation protocol is applied across datasets, avoiding the inconsistencies and biases that arise when naively mixing datasets with heterogeneous labeling standards. Additionally, each video carries multiple task types with clearly delineated response boundaries, enabling the model to better perceive and understand varying task requirements, develop robust instruction-following capabilities, and execute range of real-time response tasks. Below, we detail the annotation protocol for each task. Real-time Narration This task performs real-time commentary over video, requiring second-by-second descriptions that capture fine-grained visual changes. The annotation protocol is: 1) segment each video at one-second intervals; 2) for every adjacent pair of one-second segments (i.e., two-second window), use Qwen2.5-VL-72B [Bai et al., 2025] to describe the changes observed between them; 3) concatenate the per-second outputs and send the full narration to GLM-4.5 [Zeng et al., 2025] for post-processing to remove repetitions and redundancies, smooth transitions, and ensure coherent, context-aware narration. Event Caption This task is similar to standard video captioning but requires the model to detect event boundaries and provide the corresponding caption when an event ends. To construct supervision: 1) generate segment-level captions with the ARC-Hunyuan-Video-7B [Ge et al., 2025] model; 2) temporally ground each caption using the same model; 3) retain only those videos in which all segment captions have mutually consistent, overlapping time spans that align with the original output. This yields two benefits: it filters out erroneous, noisy data and produces samples with sharper, more explicit event boundaries, enabling clearer supervision. Action Caption This task mirrors event captioning but narrows the focus from dense events to discrete actions or procedural steps. We reuse the event-caption pipeline and augment it with action-oriented prompts and targeted filtering. This produces cleaner, step-level supervision with sharper action delineation. Event Grounding The grounding annotation is similar to the offline setup, where each sample pairs an Streaming Video Instruction Tuning event caption with its corresponding temporal span. The key difference in the online setting is that the caption is provided in advance, and the model must continuously monitor the subsequent video stream to detect the specified event and localize its occurrence in time. We randomly sample captions from the event-caption annotations, rewrite them for grounding, and integrate existing datasets to broaden coverage and improve robustness. Time-sensitive QA This task targets questions whose correct answers change over time in dynamic video stream. To construct supervision: 1) process each video with GLM-4.5V [Team et al., 2025] model to detect change points across multiple aspectsobject attributes (e.g., color, size, state), spatial positions, actions and interactions, counts, and scene or context shifts; 2) generate questionanswer pairs from these variations by posing single, unified question and providing diverse, time-specific answers at the corresponding time points."
        },
        {
            "title": "4.2 Statistics",
            "content": "Using unified annotation standard and protocol, we labeled and curated total of 400K valid samples and additionally merged offline video QA data from the LLaVA-Video [Zhang et al., 2024] dataset, culminating in Streamo-Instruct-465K, and the task distribution is shown on the left of Fig. 4. We integrated multiple open-source video datasets as sources, including Koala [Wang et al., 2025], LLaVA-Video [Zhang et al., 2024], ActivityNet [Caba Heilbron et al., 2015], QVHighlight [Moon et al., 2023], YouCook2 [Zhou et al., 2018], HACS [Zhao et al., 2019], EgoTimeQA [Di and Xie, 2024], DiDeMo [Anne Hendricks et al., 2017], and COIN [Tang et al., 2019], yielding 135,875 videos in total. The distribution of video durations is shown on the right of Fig. 4."
        },
        {
            "title": "5.1 Models and Datasets",
            "content": "To assess the effectiveness of our training strategy, we adopt Qwen2.5-VL [Bai et al., 2025] as our base model, across both 3B and 7B model size. Meanwhile, we additionally conduct experiments based on several existing state-of-the-art offline models, including Qwen3-VL [Team, 2025], and InternVL-3 [Zhu et al., 2025], to demonstrate the compatibility of our framework; these results are presented in the Supplementary material. In addition to training on our proposed Streamo-Instruct-465K dataset, we also compare against ET-Instruct-164K [Liu et al., 2024], large-scale instruction-tuning dataset with rich temporal information that has been widely used in prior work to train online video models. To enable fairer comparison with Streamo-Instruct-465K, we also report results on mixed dataset comprising ET-Instruct-164K and LLaVA-Video."
        },
        {
            "title": "5.2 Benchmarks",
            "content": "We evaluated our model across three dimensions of benchmarks: Online, Offline, and Stream Instruction. For the online setting, we adopted OVO-Bench [Li et al., 2025], which covers three temporal perception modes, including real-time, backward, and forward, and also spans total of 12 subtasks. The offline evaluation used standard general video understanding benchmarks, including the short-video benchmarks MVBench [Li et al., 2024] and TempCompass [Liu et al., 2024], as well as the long-video benchmarks VideoMME [Fu et al., 2024] and LongVideoBench [Wu et al., 2024], providing comprehensive assessment of capabilities. In addition, to assess multi-instruction following in an online context, we constructed StreamoBench, which includes 300 videos and 3,000 instruction tasks. Each video is paired with tasks of varying 7 Streaming Video Instruction Tuning Table 1. Comparison with state-of-the-art on OVO-Bench. Streamo Framework denotes adapting offline models to the online setting using our training framework. ET-Instruct-3B is trained with ET-Instruct-164K and indicates LLaVA-Video data is added as offline support. means the model is trained at 1 fps and evaluated at 2 fps. Model # Frames Real-Time Visual Perception Backward Tracing Forward Active Responding Overall Avg. OCR ACR ATR STU FPD OJR Avg. EPM ASI HLD Avg. REC SSR CRR Avg. Overall Avg. Qwen2-VL-72B [Wang et al., 2024] LLaVA-Video-7B [Zhang et al., 2024] LLaVA-OneVision-7B [Li et al., 2024] Qwen2-VL-7B [Wang et al., 2024] InternVL-V2-8B [Chen et al., 2024] 64 64 64 64 Open-source Offline Models 65.77 60.55 69.83 51.69 69.31 54.35 61.92 52.53 60.81 57.53 56.95 38.83 64. 45 49.3 69.13 58.72 68.83 49.44 74.26 59.78 63.52 56.23 57.43 7.53 40.4 34.1 69.95 60.42 54. 66.44 57.8 73.28 53.37 71.29 61.96 64.02 54.21 55.41 21.51 43.71 25.64 67.09 58.75 50.5 60.4 50.46 56.03 47.19 66.34 55.43 55.98 47.81 35.48 56.08 46.46 31.66 65.82 48.75 48.74 67.11 60.55 63.79 46.07 68.32 56.52 60.39 48.15 57.43 24.73 43.44 26.5 59.14 54.14 56. 52.91 52.74 50.39 50.15 46.71 46. 47.5 LongVU-7B [Shen et al., 2024] 1fps 53.69 53.21 62.93 47.75 68.32 59.78 57.61 40.74 59.46 4.84 35.01 12.18 69.48 60.83 Flash-VStream-7B [Zhang et al., 2024] VideoLLM-online-8B [Chen et al., 2024] Dispider-7B [Qian et al., 2025] ET-Instruct-3B [Liu et al., 2024] ET-Instruct-3B [Liu et al., 2024] Streamo-3B Streamo-7B Streamo-7B 1fps 2fps 1fps 1fps 1fps 1fps 1fps 2fps Open-source Online Models 24.16 29.36 28.45 33.71 25.74 28.8 28.37 39.06 37.16 5.91 27.38 8.02 67.25 60 45.09 33.61 8.05 23.85 12.07 14.04 45.54 21.2 20.79 22.22 18.8 12.18 17. - - - - - 57.72 49.54 62.07 44.94 61.39 51.63 54.55 48.48 55. 4.3 36.06 18.05 37.36 48.75 34.72 41.78 Streamo Framework 65.10 35.78 56.90 35.39 24.75 60.87 46.47 41.81 35. 8.6 28.52 20.06 52.31 67.50 46.62 71.14 50.46 67.24 37.08 60.40 60.33 57.78 48.82 48.56 11.29 36.22 13.68 48.62 60.00 40.77 78.52 52.29 67.24 44.38 55.45 71.20 61.51 51.18 57.43 16.67 41.76 27.94 50.72 82. 53.72 79.19 57.80 75.00 49.44 64.36 70.11 65.98 54.55 52.03 31.72 46.10 29.96 51.03 83.33 54.77 77.18 66.06 76.72 45.51 66.34 72.83 67.44 55.56 58.11 33.87 49.18 30.84 57.55 82.5 56.96 40. 44.92 52.33 55.61 57.86 temporal scopes and granularities to measure the models adherence to instructions, providing an important metric for building reliable real-time AI assistant. Detailed information for Streamo-Bench is given in the Supplementary material."
        },
        {
            "title": "5.3 Implementation Details",
            "content": "Across all models, we use unified training setup. Full parameter tuning is applied with the vision encoder frozen, and only the connector and the LLM will be updated. Training runs for single epoch with batch size of 512 and learning rate of 1e-5. For multi-turn dialogue construction, each video is split into turns of one second, and frames are sampled at 1 fps. The hyperparameter gamma in Eq. (3) is set to 2. In experiments that include LLaVA-Video, we restrict the training data to the same subset used by Streamo-Instruct-465K to ensure direct and fair comparison."
        },
        {
            "title": "5.4 Main Results",
            "content": "Comparison with SOTA on Online Video Benchmarks The main results are shown in Tab. 1. Using the Streamo framework, we train the models with ET-Instruct and Streamo-Instruct datasets and compare their performance to currently available open-source offline and online models. The key findings are as follows: 1) Streamo significantly outperforms SOTA. It is clear that our proposed Streamo-7B exceeds the previous SOTA, Dispider, by +13.83% on average performance. Moreover, we observe that the model trained at 1 fps can be directly evaluated at 2 fps without retraining, achieving an additional +4.66% performance improvement, indicating robust generalization to higher test-time frame rates; 2) Streamo-Instruct-465K dataset surpasses existing dataset. Compared with the ET-Instruct-164K, our proposed Streamo-Instruct465K delivers comprehensive performance advantage, with +7.1% on forward task and +11.79% overall; 3) Offline supervision can hinder online learning. Augmenting ET-Instruct with the offline LLaVA-Video 8 Streaming Video Instruction Tuning Table 2. Results on offline video benchmarks. The table compares converted online models with their original offline base models and SOTA models. Numbers in parentheses denote performance differences from the corresponding offline models. Model OVO Real-Time OVO Backward MVBench TempCompass VideoMME LongVideoBench Avg Gemini-1.5-pro [Team et al., 2024] GPT-4o [Hurst et al., 2024] Flash-VStream-7B [Zhang et al., 2024] VideoLLM-online-8B [Chen et al., 2024] Dispider-7B [Qian et al., 2025] StreamingVLM-7B [Xu et al., 2025] 69.3 64. 28.4 20.8 54.6 62.0 Proprietary Models 62.5 60.8 60.5 64.6 67.1 70.9 Open-source Online Models 27.4 17.7 36.1 - 61.2 33.9 - 69.2 Streamo Framework - - - - 75.0 71.9 61.2 26.9 57.2 65. 64.0 66.7 - - - 59.0 66.4 66.6 - - - - Qwen2.5-VL-3B [Bai et al., 2025] ET-Instruct-3B [Liu et al., 2024] ET-Instruct-3B [Liu et al., 2024] Streamo-3B Qwen2.5-VL-7B [Bai et al., 2025] Streamo-7B 54.6 46.5 (-8.1) 57.8 (+3.2) 61.5 (+6.9) 58.8 66.0 (+7.2) 67.0 37.8 65.8 (-1.2) 28.6 (-9.2) 36.2 (-1.6) 68.1 (+1.1) 41.8 (+4.0) 67.9 (+0.9) 42.2 69.6 46.1 (+3.9) 72.3 (+2.7) 64.4 60.3 (-4.1) 63.7 (-0.7) 66.2 (+1.8) 71.7 71.8 (+0.1) 61.5 56.6 (-4.9) 59.6 (-1.9) 61.8 (+0.3) 65.1 67.9 (+2.8) 54.2 51.2 (-3.0) 54.9 (+0.7) 56.2 (+2.0) 56.0 59.2 (+3.2) 56.6 51.5 (-5.1) 56.7 (+0.1) 59.2 (+2.6) 60.6 63.9 (+3.3) dataset boosts real-time perceptual accuracy but compromises streaming ability, revealing trade-off inherent to offline-only supervision. This also demonstrates that Streamo-Instruct-465K transfers effectively to online, streaming scenarios while maintaining strong offline perceptual capability. Comparison with SOTA on Offline Video Benchmarks To evaluate the general video understanding capability of models after conversion to the online setting, we compare Streamo against the SOTA method and original offline base model on suite of general offline video benchmarks, with results reported in Tab. 2. The findings show that, after conversion, Streamo retains strong perceptual performance on offline benchmarks across both short-form and long-form videos, surpassing the SOTA, StreamingVLM, in every benchmark. Meanwhile, models trained with our Streamo-Instruct-465K exhibit consistent improvements over base models, with Streamo-7B achieves an average improvement of +3.4% based on Qwen2.5-VL-7B. Holding architecture and training setup constant, Streamo-Instruct-465K also provides clear advantage over alternative data recipes, outperforming ET-Instruction and LLaVA-Video by +7.8% and +2.5% on average, respectively. These results underscore that our training framework and data not only enable effective transformation of models for streaming video understanding but also preserve and enhance core perceptual capabilities on offline video tasks. Streamo-Bench To evaluate the models ability to follow different instructions and perform varied tasks, we assign multiple instruction-driven tasks to single video, including forward grounding, backward grounding, narration captions, dense captions, and time-sensitive question answering. Details, examples, and statistics for these tasks are presented in the Supplementary material. As shown in Tab. 3, existing online models show deficiencies in comprehensive multi-task coverage. Our analysis indicates that these shortcomings stem largely from an inadequate ability to comprehend and follow complex instructions. For instance, removing predefined options leads to widespread failureas the grounding results showhighlighting vulnerability to open-ended prompts. Furthermore, in standard QA scenarios, models frequently overlook instructions to update answers as conditions change, which severely degrades recall. We probe instruction comprehension and prompt sensitivity further with additional experiments in the Supplementary material. Collectively, these observations expose critical gap in current 9 Streaming Video Instruction Tuning Table 3. Evaluation results on Streamo-Bench. Forward and backward grounding are determined by whether the query refers to time point before or after the event period, and results are using the mIoU metric. Caption evaluation is conducted by calculating the win rate with Qwen2.5-VL-72B model. TSQA denotes Time-Sensitive QA, i.e., questions whose answers change over time. Model Grounding Caption TSQA Average Forward Backward Narration Dence Caption Accuracy Recall Flash-VStream-7B [Zhang et al., 2024] VideoLLM-online-8B [Chen et al., 2024] Dispider-7B [Qian et al., 2025] StreamingVLM-7B [Xu et al., 2025] Streamo-3B Streamo-7B 0 0 0 0 14.7 29.4 0 0 8.33 0 27.5 38. 23.5 42.0 31.6 68.5 71.4 75.9 25.9 6.6 29.2 24.0 68.5 72.8 30.8 19.6 14.0 11.8 20.1 51.6 13.1 7.6 4.4 43.1 65.7 63.9 15.6 12.6 14.6 24.6 44.7 55.3 capabilities. In contrast, Streamo demonstrates robust performance across tasks, clearly exhibiting strong instruction-following ability. This outcome validates both the diagnostic power of our benchmark and the effectiveness of our method in learning generalized instruction-following capabilities."
        },
        {
            "title": "5.5 Ablation",
            "content": "To evaluate the effectiveness of our focal loss for training the three decision states, <Silence>, <Standby>, and <Response>, we compare it to standard cross-entropy loss. As shown in Tab. 4, training without state-aware reweighting severely limits performance due to significant class imbalance. In the Streamo-Instruct-465K dataset, the empirical ratio of state labels is approximately <Silence>:<Standby>:<Response> = 12:3:2, which biases conventional training toward predicting Silence and suppresses actual Response predictions. straightforward remedy is to assign fixed class weights inversely proportional to label frequency. Specifically, we set the weights to 0.3, 1.3, and 2.0 for silence, standby, and response, respectively, to emphasize response timing. As illustrated in the line Loss Scale in Tab. 4), this adjustment effectively mitigates the degradation caused by imbalance. However, fixed weighting fails to capture token-level hardness and sequence-level heterogeneity in decision-state distributionsfor instance, narration tasks may contain multiple responses, whereas QA task might include only one. Our proposed focal loss addresses this limitation by dynamically reweighting losses based on token-level hardness and per-batch state frequency, thereby providing more adaptive supervision for response-timing decisions. Across both InternVL-3-2B and Qwen2.5-VL-3B backbones, training with the proposed focal loss consistently yields substantial improvements over both the vanilla cross-entropy and fixed-weight baselines. Table 4. Ablation study of loss functions for online training on OVO-Bench Forward Active tasks. Base Model Loss Type REC SSR CRR Qwen2.5-VL-3B CrossEntropy Qwen2.5-VL-3B Loss Scale Qwen2.5-VL-3B Focal Loss InternVL3-2B InternVL3-2B InternVL3-2B CrossEntropy Loss Scale Focal Loss 6.45 18.62 27.94 9.46 21.20 29.23 20.99 41.02 50.72 20.50 31.47 47.38 41.67 49.17 82.5 40.42 48.75 80. 10 Streaming Video Instruction Tuning"
        },
        {
            "title": "6 Conclusion",
            "content": "Our work targets the advancement of streaming video by jointly addressing model training and data construction. We introduce an end-to-end training framework together with large-scale instruction-tuning dataset, Streamo-Instruct-465K, enabling the conversion of multiple state-of-the-art offline models into online version. The resulting model, Streamo, not only excels on streaming benchmarks but also rivals top-performing offline models. Furthermore, our proposed Streamo-Bench, which simulates complex multiinstruction scenarios, showcases Streamos robust multi-tasking capabilities. Collectively, these contributions mark significant leap towards creating general-purpose, real-time, and interactive AI assistants."
        },
        {
            "title": "7 Limitations and Future Work",
            "content": "In terms of limitations, while our approach achieves strong accuracy, it is limited by the inherent challenges of streaming videos unbounded temporal context. Our current pipeline lacks specialized long-sequence optimizations, leading to significant memory and latency costs that become prohibitive as sequence length grows. By leveraging our frameworks compatibility with existing techniques, we can integrate KV-cache management and visual token pruning to reduce computational overhead, alongside exploring sliding-window attention and adaptive frame compression for refined context management. Collectively, these strategies are designed to enhance training and inference efficiency, extend the effective context length, and facilitate an unbounded, real-time data stream."
        },
        {
            "title": "References",
            "content": "[1] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv:2306.02858, 2023. [2] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv:2311.10122, 2023. [3] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. [4] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, 2021. [5] Yucheng Zhao, Chong Luo, Chuanxin Tang, Dongdong Chen, Noel Codella, and Zheng-Jun Zha. Streaming video model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1460214612, 2023. [6] Asif Ali Laghari, Sana Shahid, Rahul Yadav, Shahid Karim, Awais Khan, Hang Li, and Yin Shoulin. The state of art and review on video streaming. Journal of High Speed Networks, 29(3):211236, 2023. [7] Haibo Wang, Bo Feng, Zhengfeng Lai, Mingze Xu, Shiyu Li, Weifeng Ge, Afshin Dehghan, Meng Cao, and Ping Huang. Streambridge: Turning your offline video large language model into proactive streaming assistant. arXiv preprint arXiv:2505.05467, 2025. [8] Rui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Dispider: Enabling video llms with active real-time interaction via disentangled perception, decision, and reaction. arXiv:2501.03218, 2025. 11 Streaming Video Instruction Tuning [9] Yueqian Wang, Xiaojun Meng, Yuxuan Wang, Jianxin Liang, Jiansheng Wei, Huishuai Zhang, and Dongyan Zhao. Videollm knows when to speak: Enhancing time-sensitive video comprehension with video-text duet interaction format. arXiv:2411.17991, 2024. [10] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In CVPR, 2015. [11] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang. Shot2story20k: new benchmark for comprehensive understanding of multi-shot videos. arXiv:2312.10300, 2023. [12] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, 2017. [13] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal pretraining for dense video captioning. arXiv:2011.11760, 2020. [14] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. [15] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv:2406.09418, 2024. [16] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv:2409.12961, 2024. [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [18] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [19] Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl 1.5 technical report. arXiv preprint arXiv:2509.01563, 2025. [20] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1840718418, 2024. [21] Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, and Song Han. Streamingvlm: Real-time understanding for infinite video streams. arXiv preprint arXiv:2510.09608, 2025. [22] Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al. Ovo-bench: How far is your video-llms from real-world online video understanding? arXiv:2501.05510, 2025. [23] Haomiao Xiong, Zongxin Yang, Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Jiawen Zhu, and Huchuan Lu. Streaming video understanding and multi-round interaction with memory-enhanced knowledge. arXiv:2501.13468, 2025. [24] Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, and Changsheng Xu. Svbench: benchmark with temporal multi-turn dialogues for streaming video understanding. arXiv:2502.10810, 2025. [25] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 12 Streaming Video Instruction Tuning [26] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. NeurIPS, 2024. [27] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988, 2017. [28] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [29] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. [30] Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, et al. Arc-hunyuan-video-7b: Structured video comprehension of real-world shorts. arXiv preprint arXiv:2507.20939, 2025. [31] Team, Wenyi Hong, Wenmeng Yu, et al. Glm-4.5 and glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. [32] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [33] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84288437, 2025. [34] WonJun Moon, Sangeek Hyun, SangUk Park, Dongchan Park, and Jae-Pil Heo. Query-dependent video representation for moment retrieval and highlight detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2302323033, 2023. [35] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In AAAI, 2018. [36] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In ICCV, 2019. [37] Shangzhe Di and Weidi Xie. Grounded question-answering in long egocentric videos. In CVPR, 2024. [38] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 58035812, 2017. [39] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In CVPR, 2019. [40] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [41] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [42] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 13 Streaming Video Instruction Tuning [43] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. [44] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based real-time understanding for long video streams. arXiv:2406.08085, 2024. [45] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Chen. Et bench: Towards openended event-level video-language understanding. Advances in Neural Information Processing Systems, 37: 3207632110, 2024. [46] Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [47] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [48] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. [49] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv:2403.00476, 2024. [50] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv:2405.21075, 2024. [51] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. NeurIPS, 2024. [52] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530, 2024. [53] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv:2410.21276, 2024. [54] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. 14 Streaming Video Instruction Tuning"
        },
        {
            "title": "A Streamo",
            "content": "A.1 System Prompt We design dedicated system prompt for Streamo that enables the model to handle dynamic streaming video content, interpret three predefined response states, and make real-time decisions at the frame level. The full prompt is provided in Tab. 7. This deliberately crafted prompt helps the model quickly adapt to the streaming input pattern and perform the required behavior transformation. A.2 Instruction Prompt In Tab. 11, we present the prompt templates used for all tasks. These diverse task instructions help the model better understand different task requirements, thereby fostering more general multi-task instructionfollowing capabilities. This goes beyond prior setups where models were confined to standalone QA, and represents step toward general real-time interactive AI. A.3 More Base Model Results Our training framework converts offline models into streaming-capable models with minimal intrusive modifications, enabling these base models to process streaming video data. This design yields strong compatibility and allows direct application to wide range of offline models. In Tab.8 and9, we further report results using InternVL3[47] and Qwen3VL[46] as Streamos base models. These results show that our framework effectively leverages the capabilities of offline models and extends them to online streaming video processing. This is particularly advantageous given the rapid iteration of offline models, as our framework can readily harness their improvements for real-time interactive video understanding. A.4 Visualization In Fig.6 and7, we visualize the outputs of Streamo, which vividly illustrate its ability to interpret and appropriately respond even to instructions that were unseen during training. When confronted with task instructions that vary in both response granularity and content, the model consistently produces suitable outputs. These visualizations provide strong evidence that Streamos training framework successfully bridges the gap between offline model capabilities and the requirements of online streaming interactions, enabling reliable real-time responses that go far beyond simple QA. Streamo-Instruct B.1 Data Generation Prompt We next elaborate on the prompts used in our data annotation pipeline. For event caption tasks, we leverage ARC-Hunyuan[30], which is specifically trained for video segmentation and grounding, and directly adopt its official prompt for initial data processing. We then use the prompt in Tab.10 to rewrite and clean the annotated caption sentences. For narration generation, which describes inter-frame temporal changes, the generation prompt is given in Tab.12, and the prompt for merging and cleaning the resulting descriptions is provided in Tab.13. For the TSQA task, the detailed prompt is presented in Tab. 14. Table 5. Comparison of Existing Video Benchmarks. Streamo-Bench introduces the first mixed-task type specifically designed for streaming video. Streaming Video Instruction Tuning"
        },
        {
            "title": "Benchmark",
            "content": "#Videos #Samples"
        },
        {
            "title": "Streaming Task Type",
            "content": "MVBench TempCompass ET-Bench SVBench StreamBench OVOBench Streamo-Bench 3,673 410 7,002 1,353 306 644 300 4,000 7,540 7,289 49,979 1,800 2,814 3000 (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34)"
        },
        {
            "title": "QA\nQA\nMix\nQA\nQA\nQA\nMix",
            "content": "C Streamo-Bench In Tab. 5, we compare our proposed Streamo-Bench with existing video benchmarks. Streamo-Bench is, to the best of our knowledge, the first streaming video benchmark that integrates multiple task types. Existing streaming video benchmarks typically use QA as the sole evaluation task, which mainly measures perceptual understanding rather than the ability to perform diverse open-ended tasks. However, the ability to follow varied instructions and complete multiple tasks is key requirement for streaming video models. By filling this gap, Streamo-Bench enables more comprehensive evaluation of models instruction-following ability in open-ended streaming scenarios. C.1 Statistics Our benchmark contains 300 videos sampled from COIN[39], YouCookv2[35], and ActivityNet [10]. Each video is annotated with multiple tasks, including Grounding, Narration, Caption, and Time-Sensitive QA, yielding total of 3,000 task-specific instances. This comprehensive design enables thorough examination of models ability to process and respond to diverse instructions in streaming settings. C.2 Metric To comprehensively evaluate the performance of models on our Streamo-Bench, we detail the metrics used for each task type below. Grounding Evaluation. For grounding tasks, we distinguish between forward (queries referring to time points before an event) and backward (queries referring to time points after an event) contexts. Performance is measured using mean Intersection over Union (mIoU), which quantifies the overlap between the models predicted temporal interval and the ground-truth interval. Let the predicted and ground-truth temporal intervals, tpred and tgt, for sample be: tpred = [ spred , epred ], = [ sgt tgt , egt ], (8) where and represent the start and end timestamps, respectively. The IoU for sample is defined as the ratio of intersection length to union length: IoUi = max(cid:0)0, min(epred , egt max(epred , egt ) min(spred )(cid:1) , sgt ) max(spred , sgt ) . (9) 16 Streaming Video Instruction Tuning The mean IoU (mIoU) over samples is mIoU = 1 N i=1 IoUi. (10) Narration and Caption Evaluation. Because narration and captioning are open-ended generation tasks, directly evaluating output quality is challenging. Following the evaluation protocol of Chatbot Arena [54] and StreamingVLM [21], we assess narration and caption quality via pairwise comparison against strong baseline, Qwen2.5-VL-72B [28]. The win rate is defined as the proportion of cases in which our models output is judged superior to the baselines output. Time-Sensitive QA Evaluation. For Time-Sensitive QA, we require that prediction be correct in both its content and its timestamp. Let be the set of TSQA questions. For each question Q, the ground truth consists of mq time-stamped answers: Gq = {(aq , tq )}mq i=1, where aq is the answer content and tq is its timestamp. The model produces nq predictions: Pq = {( ˆaq , ˆtq )}nq j=1, where ˆaq is the predicted content and ˆtq is the predicted timestamp. predicted pair ( ˆaq For the content evaluation: , ˆtq ) may match ground-truth pair (aq , tq ) only if it is correct in both content and time. C( ˆaq , aq ) = (cid:40) if content matches, 1, 0, otherwise. (13) For the timestamp, we define non-negative tolerance parameter δt 0. Then we evaluate the correctness of the timestamp by: T(ˆtq , tq ; δt) = (cid:40) if 1, tq ˆtq 0, otherwise. δt, In our experimental setting, the δt is set to 3 seconds. For the i-th answer point of question q, we define an indicator Iq that checks whether there exists at least one prediction satisfying both content and temporal constraints: : ) = 1 T(ˆtq ; δt) = 1 , tq (cid:40) 1 Iq = , aq if C( ˆaq 0 otherwise (11) (12) (14) (15) The final accuracy and recall can be given as: Accuracy = 1 qQ mq mq i=1 qQ Iq Recall = 1 qQ (cid:32) (cid:33) 1 mq mq i=1 Iq 17 (16) (17) Streaming Video Instruction Tuning C.3 Sample Visualization sample instance from Streamo-Bench is illustrated in Fig. 5. Forward and backward grounding questions are randomly placed either before or after their corresponding target temporal intervals. The TSQA question is inserted before the first answer timestamp. Narration and event caption instructions are placed before the start of the video stream to capture the overall video content. C.4 Further Analysis We further analyze the performance of existing models on Streamo-Bench and observe that their primary failures stem from lack of instructiontask comprehension: they struggle to distinguish different task types and to produce task-appropriate outputs. This limitation arises because these models are typically trained exclusively on captioning or QA data, which constrains them to generate outputs tailored to only those specific tasks. Examples in Tab. 6 clearly illustrate this phenomenon: while the models can satisfy caption or narration requirements, they often fail to understand grounding instructions and instead fall back to generic video descriptions. For TSQA tasks, although models trained on QA data can answer content-related questions, they do not properly follow instructions that require real-time updates to answers over the video timeline, leading to task failure. In summary, existing models generally lack robust multi-task understanding, whereas Streamo-Bench is specifically designed to evaluate models ability to interpret and respond to task-specific instructions in streaming scenarios. 18 Streaming Video Instruction Tuning Input Video Prompt Model Output Real-Time Narration Task: Watch the following video and provide real-time narration. Event Caption Task: List every event happening in the video. Event Grounding Task: Localize this event: add some water to the tea in the following video. Time-Sensitive QA Task: What is the woman doing? A. Preparing tea in kitchen B. Sitting at desk writing C. Drinking tea at desk. Please provide your answer by stating the letter followed by the full option. If the correct answer later changes, update your response. Heres typical mid-late 18th Century stoneware tea pot with the rounded spout and finial for pouring. After pouring when you were pouring out the last bit of liquid from the pot, you would be careful to stir away any bits of leaf which were floating around in the bottom. Well go through all the steps to make tea the way the Victorians did step by step. When you think of what they would be using here in their kitchen spaces, you can see theyve got the teapot, and shes working on getting her leaves in. The method for both is basically exactly the same. With the fresh leaf version in this case, were just adding in equal amounts of leaves and water. In the Victorian era the family kitchen was an even more important part of home life than in previous as this was where most of their time was spent. It was where the family ate their meals but also where they would come together. Whether it was afternoons at tea or simple evening meal, the Victorian kitchen was place that families would gather together and enjoy good company and food. Here we have made our own little tea service here, and will be showing you how Mrs Bronte would serve it in her kitchen. A. Preparing tea in kitchen. You will learn to make cup of tea the Victorian way.\" To begin youll need something sweet to top with your tea and milk. Tea is made with sugar or honey and the Victorians actually used lot of cream for their tea. Im going to use some milk for my tea today. And it can take up to two minutes to boil, so if youre making tea for more than one person.\", Start with just one cup at time and then add more when ready. Table 6. Example output from StreamingVLM illustrating failure to follow diverse task instructions and generate the corresponding response. The video shown in this example is sourced from COIN [39]. 19 Streaming Video Instruction Tuning Figure 5. Streamo-Bench example illustrating multi-task instruction-following evaluation. The video shown in this example is sourced from COIN [39]. 20 Streaming Video Instruction Tuning System Prompt: You are helpful assistant specializing in streaming video analysis. You will receive input frame by frame, each labeled with absolute time intervals in the exact format <Xs-Ys> (e.g., <0s-1s>). Follow these rules precisely: 1. Use </Silence> when: - No relevant event has started, OR - The current input is irrelevant to the given question. 2. Use </Standby> when: - An event is in progress but has not yet completed, OR - The current input is relevant but the question cannot yet be answered. 3. Use </Response> only when: - An event has fully concluded, OR - The available information is sufficient to fully answer the question. Provide complete description at this point. Do not provide partial answers or speculate beyond the given information. Whenever you deliver an answer, begin with </Response>. Table 7. System prompt used in Streamo. Table 8. Additional online benchmark evaluation results of Streamo framework with different base models (InternVL3 and Qwen3VL). Our framework consistently enables strong real-time streaming performance across diverse offline backbones. Model # Frames Real-Time Visual Perception Backward Tracing Forward Active Responding Overall Avg. OCR ACR ATR STU FPD OJR Avg. EPM ASI HLD Avg. REC SSR CRR Avg. Overall Avg. Open-source Offline Models Qwen2-VL-72B [40] LLaVA-Video-7B [32] LLaVA-OneVision-7B [41] Qwen2-VL-7B [40] InternVL-V2-8B [42] LongVU-7B [43] Flash-VStream-7B [44] VideoLLM-online-8B [20] Dispider-7B [8] Streamo-3B (Qwem2.5-VL) Streamo-7B (Qwem2.5-VL) Streamo-2B (InternVL3) Streamo-4B (Qwen3-VL) 64 64 64 64 64 1fps 1fps 2fps 1fps 1fps 1fps 1fps 1fps 65.77 60.55 69.83 51.69 69.31 54.35 61.92 52.53 60.81 57.53 56.95 38.83 64.07 69.13 58.72 68.83 49.44 74.26 59.78 63.52 56.23 57.43 7.53 34.1 69.95 60.42 66.44 57.8 73.28 53.37 71.29 61.96 64.02 54.21 55.41 21.51 43.71 25.64 67.09 58.75 60.4 50.46 56.03 47.19 66.34 55.43 55.98 47.81 35.48 56.08 46.46 31.66 65.82 48.75 67.11 60.55 63.79 46.07 68.32 56.52 60.39 48.15 57.43 24.73 43.44 26.5 59.14 54.14 53.69 53.21 62.93 47.75 68.32 59.78 57.61 40.74 59.46 4.84 35.01 12.18 69.48 60.83 40. 45 Open-source Online Models 24.16 29.36 28.45 33.71 25.74 28.8 28.37 39.06 37.16 5.91 27.38 8.02 67.25 8.05 23.85 12.07 14.04 45.54 21.2 20.79 22.22 18.8 12.18 17.73 57.72 49.54 62.07 44.94 61.39 51.63 54.55 48.48 55.41 4.3 - - 36.06 18.05 37.36 48.75 60 - Streamo Framework 78.52 52.29 67.24 44.38 55.45 71.20 61.51 51.18 57.43 16.67 41.76 27.94 50.72 82.5 79.19 57.80 75.00 49.44 64.36 70.11 65.98 54.55 52.03 31.72 46.10 29.96 51.03 83.33 77.18 55.96 62.07 41.01 60.40 70.11 61.12 48.82 47.30 13.44 36.52 29.23 47.38 80.42 82.55 69.72 74.14 52.25 73.27 81.52 72.24 58.19 52.70 17.20 42.70 31.38 53.90 84.17 21 49.3 54.82 50.5 48.74 46.6 47. 45.09 - 34.72 53.72 54.77 52.34 56.48 56.27 52.91 52.74 50.39 50.15 46.71 33.61 - 41.78 52.33 55.61 49.99 55.10 Streaming Video Instruction Tuning Table 9. Additional offline benchmarks results of Streamo framework with different base models (InternVL3 and Qwen3VL). The results show that our training framework preserves the underlying offline capability while extending it to streaming video processing. Model OVO Real-Time OVO Backward MVBench TempCompass VideoMME LongVideoBench Avg Gemini-1.5-pro [52] GPT-4o [53] Flash-VStream-7B [44] VideoLLM-online-8B [20] Dispider-7B [8] StreamingVLM-7B [21] 69.3 64.5 28.4 20.8 54.6 62.0 62.5 60.8 27.4 17.7 36.1 - Proprietary Models 60.5 64.6 67.1 70.9 Open-source Online Models 61.2 33.9 - 69.2 - - - - Streamo Framework 75.0 71.9 61.2 26.9 57.2 65.1 64.0 66.7 - - - 59.0 66.4 66. - - - - Qwen2.5-VL-3B [28] Streamo-3B Qwen2.5-VL-7B [28] Streamo-7B InternVL3-2B [47] Streamo-2B Qwen3-VL-4B [46] Streamo-4B 54.6 61.5 (+6.9) 58.8 66.0 (+7.2) 59.5 61.1 (+1.6) 66.5 72.2 (+5.7) 37.8 41.8 (+4.0) 42.2 46.1 (+3.9) 36.4 36.5 (+0.1) 42.8 42.7 (-0.1) 67.0 67.9 (+0.9) 69.6 72.3 (+2.7) 70.4 71.4 (+1.0) 68.9 70.4 (+1.5) 64.4 66.2 (+1.8) 71.7 71.8 (+0.1) 57.6 57.8 (+0.2) 65.8 66.3 (+0.5) 61.5 61.8 (+0.3) 65.1 67.9 (+2.8) 58.9 60.1 (+1.2) 69.3 68.7 (-0.6) 54.2 56.2 (+2.0) 56.0 59.2 (+3.2) 55.4 56.5 (+1.1) 53.2 56.1 (+2.9) 56.6 59.2 (+2.6) 60.6 63.9 (+3.3) 56.4 57.3 (+0.9) 61.1 62.8 (+1.7) Event Rewriting Prompt: You are given set of video captions, each describing specific moment in video. For each caption, perform the following tasks: 1. Remove any transition words, discourse markers, or sequence indicators (e.g., \"Finally \"Then \"Next \"Afterwards \"At the beginning \"At the end \"The video ends with \"The scene starts with etc.) at the beginning of the sentence or within the sentence, as these captions are now independent and do not need such connectors or structural descriptions. 2. Rewrite the caption to make it more concise and clear, without changing its meaning or omitting any important information. 3. Preserve all factual details and key actions described in the original caption. 4. Do not add any extra interpretation, information, or imagination not present in the original sentence. Only use the information given. 5. If the sentence includes phrase describing the position of shot or the sequence within the video (such as \"The video ends with \"At the start of the video \"In the next scene \"The video conclude with\"), remove this part entirely. Focus only on describing the content of the shot. Example: Original: \"Finally, the video cuts back to the man in the indoor setting, who concludes the presentation by holding the bow.\" Optimized: \"The man in the indoor setting concludes the presentation by holding the bow.\" Process each caption in this way. Return the optimized sentence directly. Original:{sentences} Optimized: Table 10. Task prompt used for rewriting event caption. 22 Streaming Video Instruction Tuning Real-time Narration Task: - Provide continuous, time-synchronized narration of the video, describing actions, objects, and scene changes as they occur. - Narrate the video in real time, updating the description frame-by-frame or momentby-moment as events unfold. - Generate live commentary of the video, focusing on who is doing what, where, and when, and noting any transitions or new events immediately. - Deliver an on-the-fly description of the video, highlighting salient actions, interactions, and changes in context as soon as they appear. - Produce running narration that captures ongoing activities, brief pauses, and resumptions, maintaining temporal alignment with the video timeline. Action Caption: - Find, identify, and determine the temporal boundaries of series of distinct actions or steps occurring throughout the video. - Locate and describe series of actions or steps in the video. - Locate and pinpoint sequential series of specific actions or steps in the video. - Identify and mark the video segments corresponding to series of actions or steps. - Identify and localize series of steps or actions occurring in the video. Event Caption: - Identify and describe all events in the following video. - List every event happening in the following video with descriptions. - Detect and summarize each event sequence in the following video. - Extract and explain all notable events in the following video. - Find all significant events in the following video and describe them. Event Grounding: - Watch the following video and temporally localize the event. Respond once it has finished and summarize its time period. The given event is: {caption} - Monitor the following video, identify the event, then respond after it finishes with summary of its time window. The given event is: {caption} - Analyze the following video, detect the event and report back upon its completion with its time period. The given event is: {caption} - Review the following video, localize the event in time, then notify me once it ends and summarize the interval it occupies. The given event is: {caption} - Identify and temporally segment the event in the following video. Report after it finishes with its time period and duration. The given event is: {caption} Time-sensitive QA: - {question} If the answer changes over time, update your response accordingly. - {question} Update your answer if it becomes different at later time. - {question} If it later differs, update your response promptly. - {question} Refresh your answer upon any change. - {question} If the correct answer later changes, update your response. Table 11. Prompt template used for diverse streaming video tasks. 23 Streaming Video Instruction Tuning Video Description Prompt: You are given two consecutive seconds in video (2 frames per second). Please succinctly describe the most significant operation or change that occurred between these seconds, focusing on the following points: 1. Base your description solely on clearly observable information; avoid speculation or assumptions. 2. For each object or element that changed, briefly state what changed: position, movement, actions, shape, color, etc. 3. Only describe the main operation, event, or action that happenedavoid listing small movements or minor shifts. 4. Describe only the specific changed parts with clear and direct language; do not include unchanged content or summarize the overall scene. 5. Make your description short and focused, naming only the changes without referencing the sequence of frames or including explanations. Example: woman appears. You pick up scissor. The cup moves to the left. cat enters the frame. The red ball rolls closer. The lamp turns on. The book closes. hand takes the remote. The door opens further. Only provide the most important description or summary of multiple descriptions. Table 12. Task prompt used for frame-level video description generation. Narration Generation Prompt: **Objective**: Clean the following second-by-second video descriptions to enhance coherence and eliminate redundancy. The original descriptions were generated with visibility of only the preceding and following 2 seconds, making them repetitive and disjointed. **Task**: Transform the descriptions into smooth, logical narrative by: 1. Removing Redundancy: Omit repeated descriptions of static or ongoing actions. 2. Filtering Insignificant Details: Exclude minor or fleeting actions that do not impact overall understanding. 3. Sentence Shortening: If description significantly exceeds 5 words, rewrite it to approximately 5 words while preserving the main idea. 4. Merging Consecutive Events: Combine adjacent descriptions representing continuous or complete action into single, concise sentence (e.g., 002: Man touches socket and 003: Socket disappears 003: Man removed socket). **Output Format and Rules**: 1. Use the format: SSS: one-sentence description. 2. When merging or omitting descriptions, skip the corresponding timestamps. 3. Do not add explanations, notes, or blank lines. 4. If the descriptions are repetitive, monotonous, lack meaningful variation, or are confusing, ambiguous, or insufficient, output only: Negative Sample. Description: {Description} Table 13. Task prompt used for merging the frame description to generate real-time narration. 24 Streaming Video Instruction Tuning TSQA Generation Prompt: You are Time-Sensitive Video Question Generator. You need to identify all the elements in the video that change over time and formulate them into questions. **CORE REQUIREMENT** Every question MUST have answers that CHANGE over time. If something doesnt change during the video, DO NOT create question about it. **TASK** 1. Identify ONLY aspects that visibly CHANGE during the video. Ignore: - Static elements that remain constant - Transitions, previews, close-ups that dont alter facts - Opening/closing sequences 2. For each changing aspect, generate ONE question with MULTIPLE DIFFERENT answers: - Each question MUST have at least 2 DISTINCT answer values - Answers must represent actual changes observed at different times - Never repeat the same answer value 3. Question types: - **Descriptive**: What/Which/Who (e.g., \"What color is the ball?\") - **Counting**: How many/How much (e.g., \"How many people are visible?\") - **State**: What stage (e.g., \"What is the person doing?\") - **Action** : What is being added/used (e.g., \"What ingredient is being added?\") - **Binary**: Yes/No (e.g., \"Is the bacon cooked?\") 4. Answer format: - List answers chronologically - Include PRECISE time in seconds for each observed change - If state returns to previous value, include it as new entry **EXAMPLES** [{\"question\": \"What color is the traffic light? \"answers\": [{\"value\": \"red \"time\": 3.8}, {\"value\": \"green \"time\": 8.7}, {\"value\": \"yellow \"time\": 23.2}, {\"value\": \"red \"time\": 26.4}]}, {\"question\": \"How many people are in the frame? \"answers\": [{\"value\": 1, \"time\": 0.0}, {\"value\": 2, \"time\": 3.8}, {\"value\": 3, \"time\": 17.1}, {\"value\": 1, \"time\": 42.6}]}, {\"question\": \"What is being poured into the glass? \"answers\": [{\"value\": \"water \"time\": 2.3}, {\"value\": \"milk \"time\": 19.7}, {\"value\": \"orange juice \"time\": 31.4}]}, {\"question\": \"Is the cake fully decorated? \"answers\": [{\"value\": \"No \"time\": 13.7}, {\"value\": \"Yes \"time\": 48.9}]}] **OUTPUT** - Minimum 2 distinct answers per question - Precise timing for all changes. Times must reflect ACTUAL observed changes, not approximations - If unsure of exact timing, watch that segment again Think step-by-step and ensure all requirements are met and all time are precise. Table 14. Task prompt used for Time-Sensitive QA generation. 25 Streaming Video Instruction Tuning Figure 6. Visualization of Streamos outputs. We present the results of Streamo processing the same video under different task instructions and distinguish them using different colors, including TSQA, Narration, and Caption. Arrows indicate the frames corresponding to the response moment. The video shown in this example is sourced from COIN [39]. 26 Streaming Video Instruction Tuning Figure 7. This is continuation of the previous figure, showing the results for the same video. The video shown in this example is sourced from COIN [39]."
        }
    ],
    "affiliations": [
        "Hong Kong Baptist University",
        "Tencent Youtu Lab"
    ]
}