{
    "paper_title": "Discovering Multiagent Learning Algorithms with Large Language Models",
    "authors": [
        "Zun Li",
        "John Schultz",
        "Daniel Hennes",
        "Marc Lanctot"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 1 ] . [ 1 8 2 9 6 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Discovering Multiagent Learning Algorithms\nwith Large Language Models",
            "content": "Zun Li1, John Schultz1, Daniel Hennes1 and Marc Lanctot1 1Google DeepMind 2026-2-20 Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering new algorithm, VolatilityAdaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanismsincluding volatilitysensitive discounting, consistency-enforced optimism, and hard warm-start policy accumulation scheduleto outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces hybrid meta-solver that linearly blends Optimistic Regret Matching with smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers. Keywords: Multi-Agent Reinforcement Learning, Game Theory, Large Language Models, Meta-Learning 1. Introduction The field of Multi-Agent Reinforcement Learning has achieved remarkable milestones in recent years, reaching superhuman performance in domains ranging from Poker to real-time strategy games. These advances have been driven by diverse array of methods, including game-theoretic regret minimization (Brown and Sandholm, 2019a) and population-based league training (Vinyals et al., 2019). The practical performance of these algorithms relies heavily on specific structural choicessuch as how regret is discounted over time or how specific equilibrium solution concept is derived. Historically, the refinement of these choices has been largely manual endeavor. Researchers must rely on intuition and trial-and-error to navigate vast combinatorial space of potential update rules, often defaulting to mathematically tractable heuristics (e.g., linear averaging or fixed discounting) that may not be optimal. In this work, we propose to overcome this limitation by automating the design process itself with Large Language Models (LLMs). We apply AlphaEvolve (Novikov et al., 2025), distributed evolutionary system powered by LLMs, to the domain of multi-agent learning. Unlike traditional hyperparameter optimization or genetic programming, AlphaEvolve leverages the code-generation capabilities of LLMs to perform semantic evolution. By treating the algorithms source code as the Corresponding author(s): lizun@google.com 2026 Google DeepMind. All rights reserved Discovering Multiagent Learning Algorithms with Large Language Models genome, the system uses LLMs to act as intelligent genetic operatorsperforming mutation to rewrite logic, introduce new control flows, and inject novel symbolic operations. This allows the search to transcend simple parameter tuning and discover entirely new mechanisms for equilibrium finding."
        },
        {
            "title": "We validate the generality of this framework by applying it to the two dominant paradigms of",
            "content": "imperfect-information game solving: 1. Evolving Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2007): The CFR family of algorithms relies on recursive definitions for accumulating regret and deriving an average policy. State-of-the-art variants like DCFR (Brown and Sandholm, 2019b) and PCFR+ (Farina et al., 2021) were developed by manually identifying specific weighting schemes or regret targets to accelerate convergence. We apply AlphaEvolve to the symbolic operations governing regret accumulation, policy aggregation, and current policy derivation. The search yields Volatility-Adaptive Discounted (VAD-)CFR, novel variant that dynamically adjusts its discounting parameters based on the volatility of regret updates and utilizes regret-magnitude weighted warm-start to construct the average strategy. This approach outperforms existing baselines across diverse set of game benchmarks. 2. Evolving Policy-Space Response Oracles (Lanctot et al., 2017) (PSRO): PSRO generalizes the Double Oracle algorithm (McMahan et al., 2003), expanding population of policies by iteratively computing best responses and solving for meta-strategy. Standard meta-solvers (e.g., Projected Replicator Dynamics or Uniform) enforce static trade-offs between exploration (expanding the game graph) and exploitation (refining the equilibrium). These static heuristics often fail to adapt to the changing topology of the empirical game during training. We apply AlphaEvolve to discover dynamic training-time and evaluation-time meta-solvers that optimize this schedule. The resulting variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO, utilizes hybrid update rule that mixes regretbased stability with aggressive greedy exploitation. By annealing the weight of these components over the training horizon, SHOR-PSRO empirically shows both algorithmic robustness and iteration efficiency. The contributions of this paper are summarized as follows: 1. We showcase using LLM-driven evolution to design multi-agent learning algorithms, moving beyond parameter tuning to symbolic code evolution. 2. We identify VAD-CFR, an evolved algorithm that demonstrates how automated search can uncover update rules that are effective yet non-intuitive to human designers. In Appendix 7.2, we also present AOD-CFR, variant discovered in an early trial that employs more conventional mechanisms while maintaining competitive performance. 3. We demonstrate the versatility of the framework by evolving PSRO meta-solvers that improve convergence speed and stability automating the transition from exploration to exploitation, yielding new variant, SHOR-PSRO. By automating the discovery of these mechanisms, we anticipate that future game-theoretic solvers will derive from blend of human ingenuity and AI-driven insights. 2. Game Theoretic Preliminaries We formulate our problem within the framework of Extensive-Form Games (EFGs) with imperfect information, which models sequential interactions involving multiple agents and hidden information. 2 Discovering Multiagent Learning Algorithms with Large Language Models 2.1. Extensive-Form Games and Exploitability An ð‘-player extensive-form game is Î“ = , , Z, A, ð‘¢, I, ðœŽ, where = {1, . . . , ð‘} denotes the set of players (Shoham and Leyton-Brown, 2008). is the set of all possible histories (sequences of actions), where represents terminal histories. For ð‘– , let Hð‘– ð» represent the subset of histories where player ð‘– acts. For any non-terminal history â„Ž, (â„Ž) is the set of legal actions. The utility function ð‘¢ð‘– : â„ assigns payoff to player ð‘– at terminal node. Crucially, imperfect information is modeled via Information Sets ð¼ Ið‘–. Specifically, Ið‘– partitions the histories Hð‘– such that player ð‘– cannot distinguish between histories â„Ž, â„Ž ð¼ (e.g., due to hidden cards). It is required that (â„Ž) = (â„Ž) for all â„Ž, â„Ž ð¼, so for simplicity we denote the legal actions at ð¼, (ð¼). strategy (or policy) ðœŽð‘– (ð¼) assigns probability distribution over actions ð‘Ž (ð¼) for each information set. strategy profile ðœŽ = (ðœŽ1, . . . , ðœŽð‘) determines the expected utility ð‘¢ð‘– (ðœŽ). Nash Equilibrium (NE) is strategy profile ðœŽ such that neither player can increase their utility by deviating unilaterally: ð‘– , ðœŽ To measure the performance of our evolved algorithms, we use Exploitability. The exploitability of strategy profile ðœŽ is the average of the incentives for players to deviate to their Best Response (BR): ð‘–) ð‘¢ð‘– (ðœŽ ð‘–) ðœŽ ð‘– , ð‘– ð‘¢ð‘– (ðœŽ ð‘– , ðœŽ (1) Exploitability(ðœŽ) = 1 ð‘ (cid:18) ð‘– max ðœŽ ð‘– ð‘¢ð‘– (ðœŽ ð‘– , ðœŽð‘–) ð‘¢ð‘– (ðœŽ) (cid:19) (2) strategy is an ðœ–-Nash Equilibrium if the exploitability is less than or equal to ðœ–. In small or mediumsized games, we can compute this value exactly by traversing the full game tree. 2.2. Counterfactual Regret Minimization (CFR) CFR is an iterative algorithm that minimizes counterfactual regret (Zinkevich et al., 2007). It decomposes the global regret minimization problem into independent local regret minimization problems at each information set. Let ðœ‹ðœŽ (â„Ž) be the probability of reaching history â„Ž under strategy ðœŽ. We define ðœ‹ðœŽ ð‘– (â„Ž) as the contribution of all players except ð‘– (including chance) to reaching â„Ž. The counterfactual value of reaching information set ð¼ and playing action ð‘Ž is the expected utility given that player ð‘– played to reach ð¼: ð‘£ð‘– (ðœŽ, ð¼, ð‘Ž) = ðœ‹ðœŽ ð‘– (â„Ž) ðœ‹ðœŽ (ð‘§ â„Ž, ð‘Ž)ð‘¢ð‘– (ð‘§) (3) The instantaneous counterfactual regret for not playing action ð‘Ž at iteration ð‘¡ is the difference between the counterfactual value of action ð‘Ž and the expected value at ð¼: â„Ž ð¼ ð‘§ Z,â„Žð‘§ ð‘Ÿð‘¡ (ð¼, ð‘Ž) = ð‘£ð‘– (ðœŽð‘¡, ð¼, ð‘Ž) ð‘Ž (ð¼) ðœŽð‘¡ (ð¼, ð‘Ž)ð‘£ð‘– (ðœŽð‘¡, ð¼, ð‘Ž) Standard CFR accumulates these values linearly over iterations ð‘‡: ð‘…ð‘‡ (ð¼, ð‘Ž) = ð‘‡ ð‘Ÿð‘¡ (ð¼, ð‘Ž) (4) (5) ð‘¡=1 The current policy ðœŽð‘¡+1 is derived from accumulated regret, typically using Regret Matching (RM), which assigns probabilities proportional to positive regret: ðœŽð‘¡+1(ð¼, ð‘Ž) = max(ð‘…ð‘‡ (ð¼, ð‘Ž), 0) (cid:205)ð‘Ž max(ð‘…ð‘‡ (ð¼, ð‘Ž), 0) (6) 3 Discovering Multiagent Learning Algorithms with Large Language Models The strategy ðœŽð‘¡ at any single iteration may not converge to NE. Instead, CFR outputs the average strategy ðœŽð‘‡ , computed by weighting the iteration strategy ðœŽð‘¡ by the players contribution to the reach probability ðœ‹ðœŽð‘¡ ð‘– (ð¼). CFR Variants: Several variants modify these update rules. For example, CFR+ (Tammelin, 2014) replaces the regret accumulation ð‘…ð‘‡ with floor bounding max(ð‘…ð‘‡ 1 + ð‘Ÿð‘¡, 0) and uses linear averaging weights (ð‘¤ð‘¡ = ð‘¡) for the average policy. Our work uses AlphaEvolve to search for optimal variations of these accumulation and derivation functions. 2.3. Policy Space Response Oracles (PSRO) PSRO (Bighashdel et al., 2024; Lanctot et al., 2017) acts as meta-solver that generalizes the Double Oracle algorithm (McMahan et al., 2003). It operates on higher level of abstraction called the Meta-Game (or Empirical Game (Wellman et al., 2025)). PSRO maintains population of ð‘– , . . . , ðœŽð‘˜ policies Î ð‘– = {ðœŽ1 ð‘– } for each player. The meta-game is represented by payoff tensor ð‘€, where , . . . , ðœŽ ð‘—ð‘ entries ð‘€ ð‘—1... ð‘—ð‘ = ð‘¢ð‘– (ðœŽ ð‘—1 ð‘ ) correspond to the expected utility of player ð‘– when policies from the 1 population are pitted against each other. ð‘– The PSRO Loop: At each epoch ð‘˜, the algorithm performs three steps: 1. Meta-Strategy Solver (MSS) at training-time: solver computes meta-strategy ðœ™ð‘– (a probability distribution over the population Î ð‘–). Common solvers include Uniform ðœ™ ð‘—ð‘– ð‘– = 1/Î ð‘– and Nash ðœ™ which is Nash Equilibrium of the current meta-game ð‘€. 2. Oracle (Best Response): new policy ðœŽð‘˜+1 solving) to be Best Response to the opponents meta-strategy: is trained via Reinforcement Learning (or exact ð‘– ðœŽð‘˜+1 ð‘– arg maxðœŽð‘–ð”¼ðœŽð‘–ðœ™ð‘– [ð‘¢ð‘– (ðœŽð‘–, ðœŽð‘–)] (7) In this work, we utilize an exact oracle that computes the optimal best response to the meta-strategy, isolating the performance of the meta-solver from the variance of reinforcement learning. 3. Expansion: The new policy is added to the population Î ð‘– Î ð‘– {ðœŽð‘˜+1 is updated. In this work, we calculate the exact expected payoff value for each entry ð‘€ ð‘—1... ð‘—ð‘ eliminating the stochastic noise associated with Monte Carlo sampling. }, and the payoff tensor ð‘€ , thereby ð‘– ð‘– To quantify performance, we employ distinct evaluation-time meta-strategy solver to compute distribution over the current policy population; this distribution serves as the basis for the exploitability metric. It is critical (Wang et al., 2022) to distinguish this from the training process: the training-time MSS drives the generation of new policies (often prioritizing exploration), while the evaluation-time MSS ensures robust evaluation of the populations quality. We regard both the training-time and evaluation-time MSS as integral components of PSRO when employed as single game-solver. Standard PSRO relies on fixed static meta-solvers for both training-time and eval-time (e.g., always using Nash or always using Uniform). However, the optimal meta-solver may change during trainingfor instance, encouraging exploration (Uniform) early on and robustness (Nash) later. We aim to discover dynamic meta-solver schedule. 3. Method: Automating Algorithm Discovery via AlphaEvolve We propose framework to automate the design of multi-agent learning algorithms by shifting the design process from manual heuristics to automated discovery. We utilize AlphaEvolve (Novikov et al., 2025), an evolutionary coding agent that leverages Large Language Models (LLMs) to evolve executable code. We apply this framework to two distinct paradigms: Regret Minimization (CFR) and Population based Training (PSRO). 4 Discovering Multiagent Learning Algorithms with Large Language Models 3.1. The AlphaEvolve Framework AlphaEvolve combines the creative code-generation capabilities of LLMs with the rigorous selection pressure of evolutionary algorithms. Unlike traditional genetic programming which relies on random syntactic mutations, AlphaEvolve uses an LLM to propose semantically meaningful modifications to code file. The process operates as follows: Population Initialization: We initialize population containing the standard implementations of the baseline algorithms (e.g., standard CFR code or Uniform PSRO code). LLM-Driven Mutation: In each generation, parent algorithm ð´ is selected based on fitness. Notice that AlphaEvolve supports multi-objective optimization: if multiple fitness metrics are defined, one of them (say ð‘“ ) will be randomly selected, and ð´ is then sampled in favor of high value of ð‘“ . The source code of ð´ is fed into an LLM (e.g., Gemini (Comanici et al., 2025)) with prompt instructing it to \"Modify the following code to improve fitness (reduce exploitability).\" The LLM generates new candidate variant ð´ by applying several lines of code changes to ð´. Automated Evaluation: The candidate ð´ is executed on set of proxy games (e.g., Kuhn Poker). An automated evaluator computes its fitness scores (final negative exploitability). Evolutionary Selection: Valid candidates are added to the population P. This loop repeats, allowing the system to discover complex, non-intuitive optimizations. 3.2. Evolving Regret Minimization Code To discover new variants of CFR, we expose the core update functions of the Counterfactual Regret Minimization algorithm to the AlphaEvolve agent. The search space is defined by the Python functions responsible for accumulating regret and updating the average policy. Specifically, we design the components with the primitives in Listing 1. This search space is expressive enough to encompass the entire family of known CFR variants as special cases. For instance, standard CFR uses eq (5) for RegretAccumulator and eq (6) for PolicyFromRegretAccumulator. These components are Python classes which allow for maintaining state across iterations, in contrast to pure functions. Listing 1 The Python CFR code skeleton used as the search space for AlphaEvolve. The highlighted methods update_accumulate_regret, get_updated_current_policy, and update_accumulate_policy represent the evolvable components of the CFR algorithm. class RegretAccumulator: \"\"\"A class that updates cumulative regret at an information set.\"\"\" def update_accumulate_regret(self, info_state_node, iteration_number, cfr_regrets): \"\"\" Args: info_state_node: Data structure with cumulative_regret and cumulative_policy. iteration_number: Current CFR iteration. cfr_regrets: Counterfactual regrets (not yet added to node). Returns: Updated cumulative regret dictionary for each action. \"\"\" ... class PolicyFromRegretAccumulator: \"\"\"A class that derives the current policy from regret.\"\"\" def get_updated_current_policy(self, info_state_node, iteration_number, cfr_regrets, previous_policy): \"\"\" Args: info_state_node: Data structure with cumulative_regret. iteration_number: Current CFR iteration. 5 Discovering Multiagent Learning Algorithms with Large Language Models cfr_regrets: Counterfactual regrets (already added to node). previous_policy: Previous policy at this info set. Returns: Updated current policy dictionary. \"\"\" ... class PolicyAccumulator: \"\"\"A class that updates the average policy during tree traversal.\"\"\" def update_accumulate_policy(self, info_state_node, iteration_number, info_state_policy, cfr_regrets, reach_prob, counterfactual_reach_prob): \"\"\" Args: info_state_node: Data structure with cumulative_policy. iteration_number: Current CFR iteration. info_state_policy: Current policy at this info set. cfr_regrets: Counterfactual regrets (already added to node). reach_prob: Probability of reaching current history (players contribution). counterfactual_reach_prob: Probability of reaching current history (opponents contribution) . Returns: Updated cumulative policy dictionary. \"\"\" ... 3.3. Evolving Meta-Strategy Solvers Code For PSRO, we evolve TrainMetaStrategySolver for generating the mixed strategies used during the oracle training phase, and EvalMetaStrategySolver for computing strategy profile to report metrics such as exploitability. The code skeleton we designed is shown in Listing 2. This interface supports the representation of all standard baselines as special cases: for example, standard double oracle algorithm solves for Nash equilibrium in the get_meta_strategy method for both solver classes. Listing 2 The Python PSRO code skeleton used as the search space for AlphaEvolve. The highlighted methods TrainMetaStrategySolver and EvalMetaStrategySolver represent the evolvable components of the PSRO algorithm. class TrainMetaStrategySolver: \"\"\"Returns meta strategies to train against in PSRO.\"\"\" def get_meta_strategy(self, game, policy_sets, meta_games): \"\"\"Returns meta strategies to train against in policy-space response oracles. Args: game: The pyspiel game object. policy_sets: list of lists of policies, one list per player. policy_sets[p][i] is player ps i-th policy. len(policy_sets[p]) == meta_games[0].shape[p]. meta_games: list of n-dimensional numpy arrays, one per player. Each array has shape (num_strats_p0, num_strats_p1, ..., num_strats_pn-1) and meta_games[p][i0, i1, ..., in-1] is the payoff of player when player chooses strategy ik. Returns: list of mixed-strategies, one for each player. Each mixed strategy is list of non-negative weights (not necessarily normalized). It is used to train best response against. \"\"\" ... Discovering Multiagent Learning Algorithms with Large Language Models class EvalMetaStrategySolver: \"\"\"Returns meta strategies for evaluation in PSRO.\"\"\" def get_meta_strategy(self, game, policy_sets, meta_games): \"\"\"Returns meta strategies for evaluation in policy-space response oracles. Args: game: The pyspiel game object. policy_sets: list of lists of policies, one list per player. policy_sets[p][i] is player ps i-th policy. len(policy_sets[p]) == meta_games[0].shape[p]. meta_games: list of n-dimensional numpy arrays, one per player. Each array has shape (num_strats_p0, num_strats_p1, ..., num_strats_pn-1) and meta_games[p][i0, i1, ..., in-1] is the payoff of player when player chooses strategy ik. Returns: list of mixed-strategies, one for each player. Each mixed strategy is list of non-negative weights (not necessarily normalized). It is used for evaluation of the current PSRO policies. E.g., computing exploitability. \"\"\" ... 3.4. Optimization Objective We manually select set of training games for AlphaEvolve to compute + 1 fitness scores. These are the negative exploitability Exploitability( ð´(ð‘”)ð¾) of the final strategy profile after ð¾ iterations for each ð‘” G, as well as their average 1 ð‘” Exploitability( ð´(ð‘”)ð¾). Here ð´(ð‘”)ð¾ denotes the strategy produced by algorithm ð´ on game ð‘” at iteration ð¾. The reported algorithms are selected based on their average scores. 4. Experimental Evaluation 4.1. Experimental Setup To test the robustness and generalizability of the algorithms we discover, we adopted rigorous evaluation protocol involving two distinct sets of games. The algorithms architecture and hyperparameters were developed and tuned on Training Set of four games. For both CFR and PSRO discoveries, we choose this set to be 3-player Kuhn Poker, 2-player Leduc Poker, 4-card Goofspiel, and 5-sided Liars Dice. Subsequently, the fixed algorithm was evaluated on Test Set. In the main body of this paper, we present results on four larger and more difficult games as representative subset of test games: 4-player Kuhn Poker, 3-player Leduc Poker, 5-card Goofspiel, and 6-sided Liars Dice. The results of full-sweep of 11 games are provided in Appendix 7.3. We utilized the OpenSpiel (Lanctot et al., 2019) framework for all experiments. Our implementation of AlphaEvolve is backboned by Gemini 2.5 pro (Comanici et al., 2025). 7 Discovering Multiagent Learning Algorithms with Large Language Models Figure 1 CFR variants performances. 4.2. Experimental Evaluation: VAD-CFR1 We evaluate the efficacy of the evolved algorithm, VAD-CFR, against established game-theoretic baselines. We benchmarked VAD-CFR against suite of state-of-the-art regret minimization algorithms: standard CFR (Zinkevich et al., 2007), CFR+ (Tammelin, 2014), Linear CFR (LCFR), Discounted CFR (DCFR) (Brown and Sandholm, 2019b), Predictive CFR+ (PCFR+) (Farina et al., 2021), Discounted Predictive CFR+ (DPCFR+) (Xu et al., 2024b), and Hyperparameter Schedule-powered PCFR variant HS-PCFR+(30) (Zhang et al., 2026). Performance was quantified using exploitability (measured on logarithmic scale) over fixed horizon of 1000 iterations, e.g., ð¾ = 1000. For all domains the exploitability scores are computed exactly. We use CFR+ as the seed program. The prompt we use is shown in Listing 8 in the Appendix. 4.2.1. Discovered Algorithmic Architecture The source code of the discovered VAD-CFR algorithm is provided in Listing 5 in Appendix 7.1. high-level abstraction is shown in Listing 3. The evolutionary search discarded the standard linear averaging and static discounting of the CFR family in favor of three distinct, non-intuitive mechanisms: Volatility-Adaptive Discounting (vs. Static Discounting): Standard DCFR applies fixed discount factors (ð›¼, ð›½) to cumulative regrets. In contrast, VAD-CFR makes these parameters reactive. It tracks the volatility of the learning process via an Exponential Weighted Moving Average (EWMA) of the instantaneous regret magnitude. When volatility is high (indicating the strategy is in flux), the algorithm dynamically increases discounting to \"forget\" the unstable history faster; when volatility 1Appendix 7.2 details an alternative variant (AOD-CFR) discovered on different set of training games that also achieves robust results. Notably, this variant relies on more conventional mechanisms than VAD-CFR. 8 Discovering Multiagent Learning Algorithms with Large Language Models drops, it retains more history for fine-tuning. Asymmetric Instantaneous Boosting: While DCFR applies asymmetry to accumulated history, VAD-CFR applies novel asymmetry to the instantaneous update itself. Positive instantaneous regrets (actions that are currently good) are boosted by factor of 1.1. This asymmetry enables the immediate exploitation of beneficial deviations, eliminating the lag associated with accumulation. Hard Warm-Start & Regret-Magnitude Weighting: Perhaps the most interesting discovery is the policy accumulation schedule. Standard CFR variants begin averaging policies immediately at ð‘¡ = 1. In contrast, VAD-CFR enforces hard warm-start2, postponing the start of policy averaging until iteration 500. The underlying regret accumulation process remains fully active during this phase, ensuring the agent continues to learn and update its strategy. Furthermore, once accumulation begins, policies are weighted not just linearly (ð‘¡), but by the magnitude of the instantaneous regret. This acts as sophisticated filter, ensuring the final equilibrium approximation is constructed only from high-information iterations, preventing early-stage noise from polluting the solution quality. Warm starting mechanisms have been studied in previous work (Brown and Sandholm, 2016). Listing 3 High-level abstraction of VAD-CFR. class RegretAccumulator: \"\"\"Volatility-Adaptive Discounting & Asymmetric Boosting.\"\"\" def update_accumulate_regret(self, info_state_node, iteration_number, cfr_regrets): # 1. Volatility & Adaptive Discount Calculation inst_mag = max(abs(r) for in cfr_regrets.values()) self.ewma = 0.1 * inst_mag + 0.9 * self.ewma volatility = min(1.0, self.ewma / 2.0) alpha = max(0.1, 1.5 - 0.5 * volatility) beta = float(iteration_number) disc_pos = (t / (t + 1)) ** alpha disc_neg = (t / (t + 1)) ** beta updated_regrets = {} for a, in cfr_regrets.items(): = -0.1 - 0.5 * volatility # 2. Asymmetric Instantaneous Boosting r_boosted = * 1.1 if > 0 else # 3. Sign-Dependent History Discounting prev_R = info_state_node.cumulative_regret.get(a, 0.0) discount = disc_pos if prev_R >= 0 else disc_neg # 4. Update with Negative Cap new_R = (prev_R * discount) + r_boosted updated_regrets[a] = max(-20.0, new_R) return updated_regrets class PolicyFromRegretAccumulator: \"\"\"Derives policy from Future Projection of regrets.\"\"\" def get_updated_current_policy(self, info_state_node, iteration_number, cfr_regrets, previous_policy): # 1. Consistency: Re-calculate exact adaptive params inst_mag = max(abs(r) for in cfr_regrets.values()) volatility = min(1.0, inst_mag / 2.0) # 2. Decaying Optimism Schedule base_optimism = 1.0 / (1.0 + iteration_number / 100.0) optimism = base_optimism * (1.0 - 0.5 * volatility) policy = {} sum_pos_regret = 0.0 for in info_state_node.legal_actions: # 3. Projected Regret Calculation r_boosted = cfr_regrets.get(a, 0.0) if r_boosted > 0: r_boosted *= 1.1 prev_R = info_state_node.cumulative_regret.get(a, 0.0) discount = get_adaptive_discount(prev_R, volatility) proj_R = (prev_R * discount) + r_boosted + optimism 2The LLM generated this threshold without knowing the fixed 1000-iteration evaluation horizon in its prompt context (Listing 8). 9 Discovering Multiagent Learning Algorithms with Large Language Models # 4. Non-Linear Probability Scaling if proj_R > 0: scaled_r = proj_R ** 1.5 policy[a] = scaled_r sum_pos_regret += scaled_r else: policy[a] = 0.0 return normalize(policy) if sum_pos_regret > 0 else uniform(policy) class PolicyAccumulator: \"\"\"Warm-Start & Magnitude Weighting.\"\"\" def update_accumulate_policy(self, info_state_node, iteration_number, info_state_policy, cfr_regrets, reach_prob, counterfactual_reach_prob): # 1. Hard Warm-Start if iteration_number < 500: return info_state_node.cumulative_policy # 2. Adaptive Gamma (Polynomial Averaging) inst_mag = max(abs(r) for in cfr_regrets.values()) volatility = min(1.0, inst_mag / 2.0) gamma = min(4.0, 2.0 + 1.5 * volatility) # 3. Multi-Factor Weighting w_time = float(iteration_number) ** gamma w_mag = 1.0 + (inst_mag / 2.0) ** 0.5 w_stable = 1.0 / (1.0 + inst_mag ** 1.5) final_weight = w_time * w_mag * w_stable # 4. Update Cumulative Policy updated_policy = {} for a, prob in info_state_policy.items(): prev_P = info_state_node.cumulative_policy.get(a, 0.0) updated_policy[a] = prev_P + (final_weight * reach_prob * prob) return updated_policy 4.2.2. Empirical Analysis: Training Domain As shown in Figure 1, VAD-CFR demonstrates superior convergence across the training set. For 3-player Kuhn Poker, VAD-CFR achieves significantly lower exploitability than all baselines. For Leduc Poker and 4-card Goofspiel, the algorithm maintains steeper convergence slope compared to DPCFR+ and other state-of-the-art variants. In 5-sided Liars Dice, VAD-CFR exhibits robust performance, effectively managing the larger state space through its adaptive discounting and boosting mechanisms. 4.2.3. Generalization Capabilities: Test Domain Results illustrated in Figure 1 highlights its generalization. In 3-player Leduc Poker, VAD-CFR reaches exploitability levels below 103 while most baselines plateau at higher levels. In 6-sided Liars Dice, VAD-CFR continues to match established baselines like DCFR, suggesting that its evolved mechanisms for managing regret scaling and noise are highly effective across different game topologies. Overall, VAD-CFR demonstrates efficient convergence rate and generalization across broad variety of domains: as shown in Figure 3 in the Appendix, VAD-CFR matches or surpasses previous state-of-the-art performances in 10 of the 11 games, with 4-player Kuhn Poker as the only exception. 4.3. Experimental Evaluation: SHOR-PSRO Next, we evaluated the performance of the evolved SHOR-PSRO algorithm, comparing its ability to reduce exploitability against standard meta-solver baselines. We use the exploitability at the ð¾ =100th PSRO iteration as the metric. We employ an exact best response oracle via value iteration, where at each state it uniformly randomizes among actions with the same optimal values. We benchmarked SHOR-PSRO against standard established meta-solver baselines: Uniform, Nash equilibrium via Discovering Multiagent Learning Algorithms with Large Language Models Figure 2 PSRO variants performances. linear program for 2-player games, AlphaRank (Muller et al., 2020), Projected Replicator Dynamics (PRD) (Lanctot et al., 2017), and Regret Matching (RM). We use Uniform as the initial program for both solver classes. The prompt we use is shown in Listing 9 in the Appendix. 4.3.1. Discovered Algorithmic Architecture The source code of the discovered SHOR-PSRO algorithm is provided in Listing 6 in Appendix 7.1. high-level abstraction is in Listing 4. The evolutionary search yielded Hybrid meta-solver that constructs meta-strategy ðœŽ by linearly blending two distinct components: Optimistic Regret Matching (ORM) and Smoothed Best Pure Strategy. Hybrid Blending Mechanism: At every internal solver iteration, the meta-strategy is computed as: ðœŽâ„Žð‘¦ð‘ð‘Ÿð‘–ð‘‘ = (1 ðœ†) ðœŽð‘‚ð‘…ð‘€ + ðœ† ðœŽð‘†ð‘œ ð‘“ ð‘¡ð‘šð‘Žð‘¥ (8) where ðœŽð‘‚ð‘…ð‘€ provides the stability of regret minimization, and ðœŽð‘†ð‘œ ð‘“ ð‘¡ð‘šð‘Žð‘¥ is Boltzmann distribution over pure strategies that aggressively biases the solver toward high-reward modes. The blending factor ðœ† controls the trade-off between these dynamics. Dynamic Annealing Schedule: Unlike standard PSRO which uses fixed solvers, SHOR-PSRO employs dynamic schedule for the training-time solver. Over the course of the PSRO iterations: The blending factor ðœ† anneals from 0.3 0.05, gradually shifting focus from greedy exploitation to robust equilibrium finding. An explicit diversity bonus is applied to the payoff gains, which also decays from 0.05 0.001, ensuring early expansion of the game graph followed by late-stage refinement. Training vs. Evaluation Asymmetry: The search discovered distinct configurations for training and evaluation. The Training Solver utilizes the dynamic annealing schedule described above and 11 Discovering Multiagent Learning Algorithms with Large Language Models returns the average strategy over internal iterations to ensure stability. In contrast, the Evaluation Solver employs fixed, low blending factor (ðœ† = 0.01) and returns the last-iterate strategy. This decoupling allows the algorithm to explore safely during training while providing reactive, low-noise exploitability estimates during evaluation. Listing 4 High-level abstraction of SHOR-PSRO. class TrainMetaStrategySolver: def get_meta_strategy(self, game, policy_sets, meta_games): = 0.05 - (0.049 * prog) # Diversity: 0.05 -> 0.001 = 0.50 - (0.49 * prog) \"\"\"Hybrid Training Solver with Full Parameter Annealing\"\"\" # 1. Annealing Schedules (Exploration -> Exploitation) prog = min(1.0, self.current_psro_iter / 75.0) blend = 0.30 - (0.25 * prog) div temp # 2. Adaptive Solver Iterations # Scale iterations based on population size pop_size = len(meta_games[0]) n_iters = 1000 + (20 * pop_size) # 3. Hybrid Internal Loop (Training returns Average) return _hybrid_solver(meta_games, n_iters, blend, div, temp, momentum=0.5, return_avg=True # Blend: 0.3 -> 0. # Temp: 0.5 -> 0.01 ) class EvalMetaStrategySolver: def get_meta_strategy(self, game, policy_sets, meta_games): \"\"\"Evaluation Solver (Fixed Parameters, Last-Iterate)\"\"\" # 1. Fixed Parameters for Pure Exploitation blend, div, temp = 0.01, 0.0, 0.001 # 2. Aggressive Iteration Scale for Convergence # Base 8000 + 50 per policy pop_size = len(meta_games[0]) n_iters = 8000 + (50 * pop_size) # 3. Hybrid Loop returns Last-Iterate for eval reactivity return _hybrid_solver(meta_games, n_iters, blend, div, temp, momentum=0.2, return_avg= False) def _hybrid_solver(meta_games, n_iters, blend, div, temp, momentum, return_avg): sigma = uniform_strategy(meta_games) sigma_avg = zeros_like(sigma) for _ in range(n_iters): = regret_matching(gains_norm) # A. Optimistic Regret Matching (ORM) gains = compute_payoffs(meta_games, sigma) + div * (1 - sigma) gains_norm = normalize_scale(apply_momentum(gains, beta=momentum)) sigma_orm # B. Smoothed Best Pure Strategy (Softmax) exp_vals sigma_pure = softmax(exp_vals, temperature=temp) # C. Hybrid Blending sigma = ((1 - blend) * sigma_orm) + (blend * sigma_pure) if return_avg: sigma_avg += sigma = compute_expected_values(meta_games, sigma) return normalize(sigma_avg) if return_avg else sigma 4.3.2. Empirical Analysis: Training Domain As shown in Figure 2, SHOR-PSRO demonstrates superior convergence speed and stability across the training set. In simpler domains like Kuhn Poker, SHOR-PSRO achieves exploitability levels (< 103) significantly faster than PRD or RM. This acceleration is likely attributable to the \"Hybrid Blending\" mechanism, which allows the solver to leverage the stability of regret minimization while aggressively exploiting high-reward modes via the smoothed softmax component. 12 Discovering Multiagent Learning Algorithms with Large Language Models 4.3.3. Generalization Capabilities: Test Domain To assess the robustness of the evolved mechanism, we evaluated SHOR-PSRO on Test Set comprising larger and more complex game variants. The results, illustrated in Figure 2, highlight the algorithms generalization capabilities. In 3-player Leduc Poker, the meta-game landscape becomes significantly more chaotic due to multi-agent dynamics. Despite this, SHOR-PSRO consistently matches or outperforms the best-performing baselines. In the most demanding test case, Liars Dice (6 sides), SHOR-PSRO demonstrates clear advantage. While static solvers struggle with the expanded branching factor, the hybrid solvers ability to bias towards \"smoothed best pure strategies\" allows it to navigate the expanded game graph efficiently. The results suggest that the evolved evaluation-time asymmetryusing fixed, low-noise configuration for measurementensures that the exploitability metric accurately reflects the populations growing strength without being obscured by exploration noise. Overall, SHOR-PSRO proves to be highly robust meta-solver, capable of generalizing to unseen game topologies without requiring manual re-tuning of its annealing schedules. As shown in Figure 4 in the Appendix, SHOR-PSRO matches or surpasses state-of-the-art performances in 8 of the 11 games. 5. Related Work Our work sits at the intersection of game-theoretic multi-agent learning, meta-learning, and automated algorithmic discovery. Here, we review the manual heuristic design that characterizes the current state-of-the-art and contrast it with our automated approach. Significant research effort has been dedicated to improving the convergence speed of CFR through specific weighting schemes and regret targets. Notable variants include CFR+ (Tammelin, 2014), which introduced floor bounding and linear averaging; Discounted CFR (DCFR) (Brown and Sandholm, 2019b), which applies fixed discount factors to historical regrets; and Predictive CFR+ (PCFR+) (Farina et al., 2021), which utilizes predictive Blackwell approachability. However, these improvements were largely derived through human intuition and trial-and-error to navigate the vast combinatorial space of potential update rules. In contrast to strategy-based warm starting (Brown and Sandholm, 2016), which relies on discrete, heuristic-driven resets, VAD-CFR employs static warmup schedule with continuous volatility-based discounting to regulate regret retention. For larger games where traversing the full tree is infeasible, Policy Space Response Oracles (PSRO) (Bighashdel et al., 2024) generalizes the Double Oracle algorithm (McMahan et al., 2003) by iteratively expanding population of policies via exact or reinforcement learning best responses. While PSRO rests on solid theoretical ground (Zhang and Sandholm, 2024), its practical application faces challenges in terms of convergence speed and population quality. Recent surveys highlight the diversity of PSRO variants (Bighashdel et al., 2024), but like CFR, the design of meta-solver schedules has traditionally been static or manually tuned rather than dynamically evolved. The research of automating machine learning algorithm design has been evolving for both neuralbased approaches and symbolic-based approaches. Meta-learning approaches such as meta-RL (Oh et al., 2025; Xu et al., 2018) and meta-learning optimizers (Metz et al., 2019) have parameterized update rules using neural networks to optimize learning dynamics. For symbolic ML discovery, foundational work in this domain is AutoML-Zero (Real et al., 2020), which demonstrated that complete machine learning algorithms could be evolved from scratch using basic mathematical operations. following work also applies program search for discovering optimizers (Chen et al., 2023). The authors of (Co-Reyes et al., 2021) also studied discovering novel symbolic reinforcement 13 Discovering Multiagent Learning Algorithms with Large Language Models learning algorithms. Within the specific domain of multi-agent learning, there has been prior work on automating algorithm design (Feng et al., 2021; Xu et al., 2022, 2024a). More directly related, one can meta-learn the regret minimization algorithm directly via CFR-like no-regret framework from examples (Sychrovsky et al., 2024). However, these approaches often faced limitations: they either operated within relatively constrained search space or relied on neural parameterizations that hindered interpretability. Our work builds directly on AlphaEvolve (Novikov et al., 2025), utilizing Large Language Models (LLMs) to perform semantic mutation on interpretable code, bridging the gap between expressive neural meta-learning and symbolic discovery. This approach has already been shown great success in the field of math (Georgiev et al., 2025) and combinatorial algorithms (Nagda et al., 2025). 6. Conclusion In this work, we proposed methodology for using LLM-driven evolution to automate the design of multi-agent learning algorithms, effectively shifting the paradigm from manual heuristic tuning to symbolic code evolution. By leveraging AlphaEvolve, we demonstrated that algorithm design can be treated as search problem within combinatorial space of symbolic operations, allowing for the discovery of complex, non-intuitive optimization mechanisms. First, we identified VAD-CFR, which breaks from the tradition of static discount schedules used in DCFR. By introducing volatility-adaptive parameters, instantaneous regret boosting, and warmstart for policy averaging, VAD-CFR demonstrates that filtering out early-stage noise is as critical as optimizing late-stage convergence. These mechanisms significantly outperform state-of-the-art baselines in domains where exact exploitability is computable. Second, we addressed the efficiency challenges of Policy Space Response Oracles (PSRO) by evolving SHOR-PSRO. This algorithm utilizes hybrid meta-solver that blends Optimistic Regret Matching with smoothed, temperature-controlled best response. By automating the annealing of this blending factor, SHOR-PSRO effectively manages the transition from population diversity to equilibrium refinement, yielding solvers that are significantly more effective for equilibrium finding in large-scale scenarios. Our results suggest that the automated discovery of algorithmic asymmetriesspecifically those that manage regret scaling and dynamic mixing schedulescan yield solvers that are elusive to human intuition but highly effective in practice. Future work will explore the application of this evolutionary framework to fully deep reinforcement learning agents and the discovery of cooperative mechanisms in general-sum games."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Ian Gemp and Xidong Feng for their valuable discussions and comments on this work."
        },
        {
            "title": "References",
            "content": "A. Bighashdel, Y. Wang, S. McAleer, R. Savani, and F. A. Oliehoek. Policy space response oracles: survey. In Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI-24) Survey Track, 2024. 14 Discovering Multiagent Learning Algorithms with Large Language Models N. Brown and T. Sandholm. Strategy-based warm starting for regret minimization in games. In Thirtieth AAAI Conference on Artificial Intelligence, volume 30, 2016. N. Brown and T. Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885890, 2019a. doi: 10.1126/science.aay2400. N. Brown and T. Sandholm. Solving imperfect-information games via discounted regret minimization. In Thirty-Third AAAI Conference on Artificial Intelligence, volume 33, pages 18291836, 2019b. X. Chen, C. Liang, D. Huang, E. Real, K. Wang, H. Pham, X. Dong, T. Luong, C.-J. Hsieh, Y. Lu, et al. Symbolic discovery of optimization algorithms. In Thirty-Seventh International Conference on neural information processing systems, pages 4920549233, 2023. J. D. Co-Reyes, Y. Miao, D. Peng, E. Real, Q. V. Le, S. Levine, H. Lee, and A. Faust. Evolving reinforcement learning algorithms. In Ninth International Conference on Learning Representations, 2021. G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. G. Farina, C. Kroer, and T. Sandholm. Faster game solving via predictive blackwell approachability: In Thirty-Fifth AAAI Conference on Artificial Connecting regret matching and mirror descent. Intelligence, volume 35, pages 53635371, 2021. X. Feng, O. Slumbers, Z. Wan, B. Liu, S. M. McAleer, Y. Wen, J. Wang, and Y. Yang. Neural auto-curricula In Thirty-Fifth International Conference on Neural Information in two-player zero-sum games. Processing Systems, 2021. B. Georgiev, J. GÃ³mez-Serrano, T. Tao, and A. Z. Wagner. Mathematical exploration and discovery at scale. arXiv preprint arXiv:2511.02864, 2025. M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. PÃ©rÃ©at, D. Silver, and T. Graepel. unified game-theoretic approach to multiagent reinforcement learning. In Thirty-First International Conference on Neural Information Processing Systems, pages 41904203, 2017. M. Lanctot, E. Lockhart, J.-B. Lespiau, V. Zambaldi, S. Upadhyay, J. PÃ©rolat, S. Srinivasan, F. Timbers, K. Tuyls, S. Omidshafiei, et al. Openspiel: framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453, 2019. H. B. McMahan, G. J. Gordon, and A. Blum. Planning in the presence of cost functions controlled by an adversary. In Twentieth International Conference on Machine Learning, pages 536543, 2003. L. Metz, N. Maheswaranathan, J. Nixon, C. D. Freeman, and J. Sohl-Dickstein. Understanding and correcting pathologies in the training of learned optimizers. In Thirty-Sixth International Conference on Machine Learning, pages 45564565, 2019. P. Muller, S. Omidshafiei, M. Rowland, K. Tuyls, J. Perolat, S. Liu, D. Hennes, L. Marris, M. Lanctot, E. Hughes, et al. generalized training approach for multiagent learning. In Ninth International Conference on Learning Representations, 2020. A. Nagda, P. Raghavan, and A. Thakurta. Reinforced generation of combinatorial structures: Hardness of approximation. arXiv preprint arXiv:2509.18057, 2025. 15 Discovering Multiagent Learning Algorithms with Large Language Models A. Novikov, N. Vu, M. Eisenberger, E. Dupont, P.-S. Huang, A. Z. Wagner, S. Shirobokov, B. Kozlovskii, F. J. Ruiz, A. Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. J. Oh, G. Farquhar, I. Kemaev, D. A. Calian, M. Hessel, L. Zintgraf, S. Singh, H. Van Hasselt, and D. Silver. Discovering state-of-the-art reinforcement learning algorithms. Nature, pages 12, 2025. E. Real, C. Liang, D. So, and Q. V. Le. Automl-zero: Evolving machine learning algorithms from scratch. In Thirty-Seventh International Conference on Machine Learning, pages 80078019, 2020. Y. Shoham and K. Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. Cambridge University Press, New York, NY, USA, 2008. D. Sychrovsky, M. Å ustr, E. Davoodi, M. Bowling, M. Lanctot, and M. Schmid. Learning to not regret. In Thirty-Eighth AAAI Conference on Artificial Intelligence, 2024. O. Tammelin. Solving large imperfect information games using cfr+. arXiv preprint arXiv:1407.5042, 2014. O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350354, 2019. Y. Wang, Q. Ma, and M. P. Wellman. Evaluating strategy exploration in empirical game-theoretic analysis. In Twenty-First International Conference on Autonomous Agents and Multiagent Systems, pages 13461354, 2022. M. P. Wellman, K. Tuyls, and A. Greenwald. Empirical game theoretic analysis: survey. Journal of Artificial Intelligence Research, 82:10171076, 2025. H. Xu, K. Li, H. Fu, Q. Fu, and J. Xing. Autocfr: Learning to design counterfactual regret minimization algorithms. In Thirty-Sixth AAAI Conference on Artificial Intelligence, pages 52445251, 2022. H. Xu, K. Li, H. Fu, Q. Fu, J. Xing, and J. Cheng. Dynamic discounted counterfactual regret minimization. In Twelfth International Conference on Learning Representations, 2024a. H. Xu, K. Li, B. Liu, H. Fu, Q. Fu, J. Xing, and J. Cheng. Minimizing weighted counterfactual regret with optimistic online mirror descent. In Thirty-Third International Joint Conference on Artificial Intelligence, pages 52725280, 2024b. Z. Xu, H. van Hasselt, and D. Silver. Meta-gradient reinforcement learning. In Thirty-Second International Conference on Neural Information Processing Systems, volume 31, 2018. B. H. Zhang and T. Sandholm. Exponential lower bounds on the double oracle algorithm in zero-sum games. In Thirty-Third International Joint Conference on Artificial Intelligence, pages 30323039, 2024. N. Zhang, S. McAleer, and T. Sandholm. Faster game solving via hyperparameter schedules. In Fortieth AAAI Conference on Artificial Intelligence, 2026. M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. Regret minimization in games with incomplete information. In Twenty-First International Conference on Neural Information Processing Systems, pages 17291736, 2007. 16 Discovering Multiagent Learning Algorithms with Large Language Models 7. Appendix 7.1. Source code of discovered algorithms Listing 5 VAD-CFR class RegretAccumulator: \"\"\"A class that updates cumulative regret using Adaptive Discounting with separate discounting for positive and negative regrets, and instantaneous regret boosting. \"\"\" @staticmethod def _calculate_adaptive_params( iteration_number, cfr_regrets, base_alpha, base_beta, volatility_sensitivity, max_expected_instantaneous_regret, ewma_decay_factor, current_ewma_magnitude, ): \"\"\"Calculates adaptive discounting parameters for given iteration. This static method centralizes the logic for EWMA volatility, adaptive alpha/beta, and discount factors to ensure consistency across components. \"\"\" t_plus_one = float(iteration_number + 1) instantaneous_regret_magnitude = max( (abs(r) for in cfr_regrets.values()), default=0.0 ) if iteration_number == 0: projected_ewma = instantaneous_regret_magnitude else: projected_ewma = ( ewma_decay_factor * instantaneous_regret_magnitude + (1.0 - ewma_decay_factor) * current_ewma_magnitude ) if max_expected_instantaneous_regret > 0: normalized_volatility = min(1.0, projected_ewma / max_expected_instantaneous_regret) else: normalized_volatility = 0.0 effective_alpha = max(0.1, base_alpha - volatility_sensitivity * normalized_volatility) effective_beta = base_beta - volatility_sensitivity * normalized_volatility effective_beta = min(effective_alpha, effective_beta) discount_factor_positive = (t_plus_one**effective_alpha) / (t_plus_one**effective_alpha + 1.0) discount_factor_negative = (t_plus_one**effective_beta) / (t_plus_one**effective_beta + 1.0) return projected_ewma, normalized_volatility, discount_factor_positive, discount_factor_negative def __init__(self, base_alpha=1.5, base_beta=-0.1, volatility_sensitivity=0.5, max_expected_instantaneous_regret=2.0, instantaneous_regret_boost_factor=1.1, ewma_decay_factor=0.1, negative_regret_cap=-20.0): \"\"\"Initializes the regret accumulator with adaptive discounting parameters. Args: base_alpha: The baseline exponent for discounting positive cumulative regrets. base_beta: The baseline exponent for discounting negative cumulative regrets. volatility_sensitivity: Controls how strongly the instantaneous regret magnitude influences the adaptive alpha/beta. higher value means the exponents will be more reduced by high volatility. max_expected_instantaneous_regret: An estimate of the maximum possible instantaneous regret magnitude, used for normalizing the volatility. instantaneous_regret_boost_factor: Boost factor for positive instantaneous regrets. 17 Discovering Multiagent Learning Algorithms with Large Language Models factor > 1.0 makes the algorithm more reactive to current good actions. ewma_decay_factor: Decay factor for the EWMA of instantaneous regret magnitude. negative_regret_cap: The minimum value for cumulative regret, to prevent regret lock-in and improve adaptability. \"\"\" self._base_alpha = base_alpha self._base_beta = base_beta self._volatility_sensitivity = volatility_sensitivity self._max_expected_instantaneous_regret = max_expected_instantaneous_regret self._instantaneous_regret_boost_factor = instantaneous_regret_boost_factor self._ewma_decay_factor = ewma_decay_factor self._negative_regret_cap = negative_regret_cap self._ewma_instantaneous_regret_magnitude = 0.0 def update_accumulate_regret( self, info_state_node, iteration_number, cfr_regrets ): \"\"\"Updates cumulative regret for each action at an information set using ADCFRP. Cumulative regrets are now signed. Args: info_state_node: data structure corresponding to an information set with cumulative_regret and cumulative_policy stored. iteration_number: the current CFR iteration (0-indexed). cfr_regrets: the instantaneous counterfactual regrets of the current policy at the current information set. cfr_regrets havent been added to info_state_node. cumulative_regret. Returns: updated cumulative regret for each action at the current information set (signed). \"\"\" # Centralize adaptive parameter calculation to ensure consistency and reduce redundancy. ( # normalized_volatility is not used here self._ewma_instantaneous_regret_magnitude, _, discount_factor_positive, discount_factor_negative, ) = RegretAccumulator._calculate_adaptive_params( iteration_number=iteration_number, cfr_regrets=cfr_regrets, base_alpha=self._base_alpha, base_beta=self._base_beta, volatility_sensitivity=self._volatility_sensitivity, max_expected_instantaneous_regret=self._max_expected_instantaneous_regret, ewma_decay_factor=self._ewma_decay_factor, current_ewma_magnitude=self._ewma_instantaneous_regret_magnitude, ) updated_cumulative_regret = {} for action in cfr_regrets: old_regret = info_state_node.cumulative_regret[action] instantaneous_regret_component = cfr_regrets[action] if instantaneous_regret_component > 0: instantaneous_regret_component *= self._instantaneous_regret_boost_factor # Apply different discount factors based on the sign of the old regret. if old_regret >= 0: discounted_old_regret = discount_factor_positive * old_regret else: discounted_old_regret = discount_factor_negative * old_regret new_regret = discounted_old_regret + instantaneous_regret_component # Cap the negative regret to prevent lock-in and improve adaptability. new_regret = max(self._negative_regret_cap, new_regret) # Crucially, regrets are NOT clipped to zero here. They can be negative. updated_cumulative_regret[action] = new_regret return updated_cumulative_regret 18 Discovering Multiagent Learning Algorithms with Large Language Models class PolicyFromRegretAccumulator: \"\"\"A class that obtains current policy from consistent optimistic projection of regrets. It aligns the policy generation with the adaptive, asymmetric, and boosted logic from RegretAccumulator. \"\"\" def __init__(self, initial_optimism_factor=1.0, optimism_decay_factor=100.0, positive_policy_exponent=1.5, base_alpha=1.5, base_beta=-0.1, volatility_sensitivity=0.5, max_expected_instantaneous_regret=2.0, instantaneous_regret_boost_factor=1.1, ewma_decay_factor=0.1): \"\"\"Initializes the PolicyFromRegretAccumulator with parameters consistent with RegretAccumulator. Args: initial_optimism_factor: The initial weighting for the instantaneous regret component in the projection. optimism_decay_factor: Controls how quickly the optimism weight decays. positive_policy_exponent: Exponent for non-linear scaling of positive regrets. base_alpha: The baseline exponent for discounting positive regrets. base_beta: The baseline exponent for discounting negative regrets. volatility_sensitivity: Controls influence of volatility on both discounting and optimism dampening. max_expected_instantaneous_regret: Used for normalizing volatility. instantaneous_regret_boost_factor: Boost factor for positive instantaneous regrets. ewma_decay_factor: Decay factor for the EWMA of volatility. \"\"\" self._initial_optimism_factor = initial_optimism_factor self._optimism_decay_factor = optimism_decay_factor self._positive_policy_exponent = positive_policy_exponent self._base_alpha = base_alpha self._base_beta = base_beta self._volatility_sensitivity = volatility_sensitivity self._max_expected_instantaneous_regret = max_expected_instantaneous_regret self._instantaneous_regret_boost_factor = instantaneous_regret_boost_factor self._ewma_decay_factor = ewma_decay_factor self._ewma_instantaneous_regret_magnitude = 0.0 def get_updated_current_policy(self, info_state_node, iteration_number, cfr_regrets, previous_policy): \"\"\"Obtains the current policy using projection that is consistent with the RegretAccumulator update rule. This method creates tighter feedback loop by basing the current policy on projection of what the regrets will be *after* the current iterations update. This projection uses the same adaptive, asymmetric discounting and boosting logic as the main regret accumulation step. \"\"\" # Centralize adaptive parameter calculation, ensuring consistency with RegretAccumulator. ( self._ewma_instantaneous_regret_magnitude, normalized_volatility, discount_factor_positive, discount_factor_negative, ) = RegretAccumulator._calculate_adaptive_params( iteration_number=iteration_number, cfr_regrets=cfr_regrets, base_alpha=self._base_alpha, base_beta=self._base_beta, volatility_sensitivity=self._volatility_sensitivity, max_expected_instantaneous_regret=self._max_expected_instantaneous_regret, ewma_decay_factor=self._ewma_decay_factor, current_ewma_magnitude=self._ewma_instantaneous_regret_magnitude, Discovering Multiagent Learning Algorithms with Large Language Models ) base_optimism = self._initial_optimism_factor / (1.0 + float(iteration_number) / self. _optimism_decay_factor) # Dampen optimism during volatile periods to increase stability. optimism_dampening_factor = max(0.0, 1.0 - self._volatility_sensitivity * normalized_volatility) optimism_strength = base_optimism * optimism_dampening_factor action_to_projected_regret = {} for action in info_state_node.legal_actions: old_cumulative_regret = info_state_node.cumulative_regret.get(action, 0.0) instantaneous_regret = cfr_regrets.get(action, 0.0) instantaneous_regret_component = instantaneous_regret if instantaneous_regret_component > 0: instantaneous_regret_component *= self._instantaneous_regret_boost_factor if old_cumulative_regret >= 0: discounted_old_regret = discount_factor_positive * old_cumulative_regret else: discounted_old_regret = discount_factor_negative * old_cumulative_regret projected_regret = discounted_old_regret + optimism_strength * instantaneous_regret_component action_to_projected_regret[action] = projected_regret positive_scaled_projected_regrets = { action: (max(0.0, regret) ** self._positive_policy_exponent) for action, regret in action_to_projected_regret.items() } sum_positive_scaled_projected_regrets = sum(positive_scaled_projected_regrets.values()) info_state_policy = {} if sum_positive_scaled_projected_regrets > 0: for action, scaled_regret in positive_scaled_projected_regrets.items(): info_state_policy[action] = scaled_regret / sum_positive_scaled_projected_regrets else: num_legal_actions = len(info_state_node.legal_actions) for action in info_state_node.legal_actions: info_state_policy[action] = 1.0 / num_legal_actions return info_state_policy class PolicyAccumulator: \"\"\"A class that updates cumulative policy using regret-informed weighted averaging with warmup period.\"\"\" def __init__(self, base_gamma=2.0, gamma_max=4.0, gamma_volatility_sensitivity=1.5, warmup_iterations=500, stability_exponent=1.5, max_expected_instantaneous_regret=2.0, regret_magnitude_weighting_exponent=0.5): # New parameter \"\"\"Initializes the PolicyAccumulator with adaptive gamma parameters and regret-magnitude weighting. Args: base_gamma: The baseline exponent for polynomial weighting of policies. gamma_max: The maximum value the adaptive gamma can reach. gamma_volatility_sensitivity: Controls how strongly volatility influences gamma. warmup_iterations: Number of initial iterations to skip for policy averaging. stability_exponent: Exponent for the stability factor based on regret magnitude. max_expected_instantaneous_regret: Normalization factor for instantaneous regret magnitude. regret_magnitude_weighting_exponent: Exponent for up-weighting policies based on the absolute magnitude of instantaneous regrets. Higher values give more emphasis to policies from iterations with large regrets. \"\"\" self._base_gamma = base_gamma self._gamma_max = gamma_max 20 Discovering Multiagent Learning Algorithms with Large Language Models self._gamma_volatility_sensitivity = gamma_volatility_sensitivity self._warmup_iterations = warmup_iterations self._stability_exponent = stability_exponent self._max_expected_instantaneous_regret = max_expected_instantaneous_regret self._regret_magnitude_weighting_exponent = regret_magnitude_weighting_exponent # Stored def update_accumulate_policy( self, info_state_node, iteration_number, info_state_policy, cfr_regrets, reach_prob, counterfactual_reach_prob, ): \"\"\"Updates cumulative policy using delayed, regret-informed, and regret-magnitude weighted averaging. \"\"\" if iteration_number < self._warmup_iterations: return info_state_node.cumulative_policy # Calculate instantaneous regret magnitude (L-infinity norm) for this iteration instantaneous_regret_magnitude = max( (abs(r) for in cfr_regrets.values()), default=0.0 ) # Normalize volatility using the shared parameter if self._max_expected_instantaneous_regret > 0: normalized_volatility = min(1.0, instantaneous_regret_magnitude / self. _max_expected_instantaneous_regret) else: normalized_volatility = 0.0 # Adapt gamma based on volatility: higher volatility -> higher gamma effective_gamma = self._base_gamma + self._gamma_volatility_sensitivity * normalized_volatility effective_gamma = min(self._gamma_max, effective_gamma) # Standard polynomial weighting gives more weight to later iterations, now with adaptive gamma . temporal_weight = (float(iteration_number) + 1.0) ** effective_gamma # Calculate stability factor from the L-infinity norm of instantaneous regrets. # Higher regret magnitude -> lower stability factor, using L-infinity norm for consistency. regret_stability_factor = 1.0 / (1.0 + instantaneous_regret_magnitude**self. _stability_exponent) # NEW: Regret Magnitude Weighting Factor # Policies from iterations with higher regret magnitude contribute more to the average. # This factor boosts the weight, with higher values of the exponent giving more emphasis. # Ensure its never zero to avoid division by zero or completely nullifying weight. regret_magnitude_factor = ( 1.0 + (instantaneous_regret_magnitude / self._max_expected_instantaneous_regret) ) ** self._regret_magnitude_weighting_exponent regret_magnitude_factor = max(0.1, regret_magnitude_factor) # Ensure minimum value to avoid zero weight if normalization results in very small number # The final weight combines the temporal, stability, and regret-magnitude-based components. weight = temporal_weight * regret_stability_factor * regret_magnitude_factor return { action: ( info_state_node.cumulative_policy[action] + weight * reach_prob * info_state_policy[action] ) for action in info_state_policy } 21 Discovering Multiagent Learning Algorithms with Large Language Models Listing 6 SHOR-PSRO import numpy as np def _smoothed_best_pure_strategy(payoff_vec, temperature=1.0): \"\"\"Computes smoothed distribution biased towards the best pure strategy. The softmax function ensures that strategies with higher payoffs are given higher probability, with temperature controlling the sharpness of the distribution. lower temperature makes the distribution more concentrated on the best strategy, while higher temperature leads to more uniform distribution. \"\"\" # Subtract max payoff for numerical stability (standard softmax trick) stable_payoffs = payoff_vec - np.max(payoff_vec) exp_payoffs = np.exp(stable_payoffs / temperature) sum_exp_payoffs = np.sum(exp_payoffs) if sum_exp_payoffs > 1e-12: # Avoid division by zero return exp_payoffs / sum_exp_payoffs else: # Fallback to uniform distribution if all exponentiated payoffs are # effectively zero (e.g., due to very low temperature and negative payoffs, # or all payoffs being identical after stabilization). return np.ones_like(payoff_vec) / len(payoff_vec) def _hybrid_orm_solver(meta_games, iterations, blending_factor=0.0, temperature=0.1, momentum_beta=0.0, gain_normalization=True, diversity_bonus_coeff=0.0, return_average_strategy=True): # New: Flag to return average or lastiterate strategy \"\"\"Computes meta-strategies using Optimistic Regret Matching+ enhanced with optimistic updates, gain normalization, and diversity bonus, then blended with smoothed best pure strategy. This solver combines the stability and convergence properties of Optimistic Regret Matching+ (ORM+) with an explicit pull towards highly rewarding pure strategies, smoothed by temperature-controlled softmax. This hybrid approach aims to leverage ORM+s ability to find mixed equilibria while also quickly identifying and exploring strong pure-strategy modes in the meta-game, thereby potentially accelerating the discovery of low-exploitable policies in PSRO. The blending factor controls the trade-off between these two dynamics. Args: meta_games: list of n-dimensional numpy arrays, one per player. iterations: Number of internal solver iterations. blending_factor: Weight (0 to 1) for blending ORM+ output with the smoothed best pure strategy. factor of 0 means pure ORM+; 1 means pure smoothed best pure strategy. temperature: Temperature for softmax smoothing when calculating the smoothed best pure strategy. Lower values make the smoothing sharper. momentum_beta: Momentum parameter for optimistic updates to payoff gains. gain_normalization: If True, normalizes payoff gains to make learning rate more robust across games. diversity_bonus_coeff: Coefficient for diversity bonus, encouraging exploration of less-chosen policies. return_average_strategy: If True, returns time-averaged strategies. If False, returns last-iterate strategies. Returns: list of mixed-strategies, one for each player, as numpy arrays. \"\"\" num_players = len(meta_games) num_strats = [m.shape[i] for i, in enumerate(meta_games)] if any(n_s == 0 for n_s in num_strats): return [np.array([]).tolist() for _ in range(num_players)] 22 Discovering Multiagent Learning Algorithms with Large Language Models strategies = [np.ones(s, dtype=float) / for in num_strats] cum_regrets = [np.zeros(s, dtype=float) for in num_strats] avg_strategies = [np.zeros(s, dtype=float) for in num_strats] prev_centered_payoff_gains = [np.zeros(s, dtype=float) for in num_strats] for in range(iterations): current_centered_payoff_gains = [np.zeros(s, dtype=float) for in num_strats] orm_strategies_this_iter = [np.zeros(s, dtype=float) for in num_strats] for in range(num_players): payoff_vec = meta_games[p] for other_p in reversed(range(num_players)): if other_p != p: payoff_vec = np.tensordot(payoff_vec, strategies[other_p], axes=([other_p], [0])) centered_payoff_gains = payoff_vec - np.mean(payoff_vec) current_centered_payoff_gains[p] = centered_payoff_gains optimistic_payoff_gains = (1 + momentum_beta) * centered_payoff_gains - momentum_beta * prev_centered_payoff_gains[p] diversity_bonus = diversity_bonus_coeff * (1.0 - strategies[p]) gains_for_regret_update = optimistic_payoff_gains + diversity_bonus if gain_normalization: max_abs_gain = np.max(np.abs(gains_for_regret_update)) if max_abs_gain > 1e-8: gains_for_regret_update /= max_abs_gain cum_regrets[p] += gains_for_regret_update cum_regrets[p] = np.maximum(0, cum_regrets[p]) sum_pos_regret = cum_regrets[p].sum() if sum_pos_regret > 1e-12: orm_strategies_this_iter[p] = cum_regrets[p] / sum_pos_regret else: orm_strategies_this_iter[p] = np.ones(num_strats[p]) / num_strats[p] smoothed_best_pure = _smoothed_best_pure_strategy(payoff_vec, temperature) strategies[p] = (1 - blending_factor) * orm_strategies_this_iter[p] + blending_factor * smoothed_best_pure prev_centered_payoff_gains[p] = current_centered_payoff_gains[p] if return_average_strategy: # Accumulate blended strategy only if average is requested avg_strategies[p] += strategies[p] if return_average_strategy: final_strategies = [] for in range(num_players): sum_avg_strat = np.sum(avg_strategies[p]) if sum_avg_strat > 0: final_strategies.append(avg_strategies[p] / sum_avg_strat) else: final_strategies.append(np.ones(num_strats[p]) / num_strats[p]) return final_strategies else: # If not returning average, return the last-iterate strategies return strategies class TrainMetaStrategySolver: \"\"\"A hybrid meta-solver for training that blends ORM+ with smoothed best pure strategies. This solver aims to accelerate convergence to low-exploitable strategies by dynamically balancing regret-minimization with pull towards high-performing (but smoothed) pure strategies. Optimistic updates, gain normalization, and 23 Discovering Multiagent Learning Algorithms with Large Language Models diversity bonus are incorporated for improved learning dynamics. The blending factor, temperature, and diversity bonus are annealed over the outer PSRO iterations. \"\"\" def __init__(self, base_solver_iterations=1000, # Base number of internal iterations iterations_per_policy_scale=20, # How much iterations scale per added policy max_solver_iterations=5000, # Max internal solver iterations initial_blending_factor=0.3, final_blending_factor=0.05, initial_temperature=0.5, final_temperature=0.01, momentum_beta=0.5, gain_normalization=True, initial_diversity_bonus_coeff=0.05, final_diversity_bonus_coeff=0.001, max_psro_iterations_for_annealing=75): \"\"\"Initializes hybrid ORM solver parameters for training. Args: base_solver_iterations: Base number of internal solver iterations for _hybrid_orm_solver. iterations_per_policy_scale: Amount to increase internal solver iterations per added policy. max_solver_iterations: Maximum internal solver iterations. initial_blending_factor: Initial weight for the smoothed best pure strategy component. final_blending_factor: Final weight for the smoothed best pure strategy component. initial_temperature: Initial temperature for softmax smoothing. final_temperature: Final temperature for softmax smoothing. momentum_beta: Momentum for optimistic updates. gain_normalization: Normalizes gains for scale-invariance. initial_diversity_bonus_coeff: Max initial diversity bonus coefficient. final_diversity_bonus_coeff: Min initial diversity bonus coefficient across PSRO iterations. max_psro_iterations_for_annealing: PSRO iterations over which outer annealing occurs. \"\"\" self._base_solver_iterations = base_solver_iterations self._iterations_per_policy_scale = iterations_per_policy_scale self._max_solver_iterations = max_solver_iterations self._initial_blending_factor = initial_blending_factor self._final_blending_factor = final_blending_factor self._initial_temperature = initial_temperature self._final_temperature = final_temperature self._momentum_beta = momentum_beta self._gain_normalization = gain_normalization self._initial_diversity_bonus_coeff = initial_diversity_bonus_coeff self._final_diversity_bonus_coeff = final_diversity_bonus_coeff self._max_psro_iterations_for_annealing = max_psro_iterations_for_annealing self._current_psro_iteration = def get_meta_strategy(self, game, policy_sets, meta_games): \"\"\"Returns blended meta strategies for training. Args: game: The pyspiel game object. policy_sets: list of lists of policies, one list per player. policy_sets[p][i] is player ps i-th policy. len(policy_sets[p]) == meta_games[0].shape[p]. meta_games: list of n-dimensional numpy arrays, one per player. Each array has shape (num_strats_p0, num_strats_p1, ..., num_strats_pn-1) and meta_games[p][i0, i1, ..., in-1] is the payoff of player when player chooses strategy ik. Returns: list of blended mixed-strategies. \"\"\" del game, policy_sets # Unused self._current_psro_iteration += 1 current_psro_iter = self._current_psro_iteration # Adaptive solver iterations: scale with current population size num_current_policies_p0 = len(meta_games[0]) # Assuming symmetric populations solver_iterations = int(self._base_solver_iterations + 24 Discovering Multiagent Learning Algorithms with Large Language Models solver_iterations = np.clip(solver_iterations, self._base_solver_iterations, self. _max_solver_iterations) self._iterations_per_policy_scale * (num_current_policies_p0 - 1)) annealing_progress = min(1.0, current_psro_iter / self._max_psro_iterations_for_annealing) blending_factor = (self._initial_blending_factor * (1.0 - annealing_progress) + self._final_blending_factor * annealing_progress) temperature = (self._initial_temperature * (1.0 - annealing_progress) + self._final_temperature * annealing_progress) diversity_bonus_coeff = (self._initial_diversity_bonus_coeff * (1.0 - annealing_progress) + self._final_diversity_bonus_coeff * annealing_progress) blending_factor = np.clip(blending_factor, self._final_blending_factor, self. _initial_blending_factor) temperature = np.clip(temperature, self._final_temperature, self._initial_temperature) diversity_bonus_coeff = np.clip(diversity_bonus_coeff, self._final_diversity_bonus_coeff, self ._initial_diversity_bonus_coeff) strategies = _hybrid_orm_solver( meta_games, iterations=solver_iterations, # Use adaptive iterations blending_factor=blending_factor, temperature=temperature, momentum_beta=self._momentum_beta, gain_normalization=self._gain_normalization, diversity_bonus_coeff=diversity_bonus_coeff, return_average_strategy=True # Training always uses averaged strategies for stability ) return [s.tolist() for in strategies] class EvalMetaStrategySolver: \"\"\"Returns meta strategies for evaluation in PSRO. This solver uses hybrid approach, blending Optimistic Regret Matching+ with smoothed best pure strategy, tailored for robust and accurate exploitability measurement. The parameters are set to emphasize exploitation for evaluation purposes, including optimistic updates and gain normalization for stability, while keeping diversity bonus minimal. Crucially, it returns the *last-iterate* strategy for reactive estimate of exploitability. \"\"\" def __init__(self, base_solver_iterations=8000, # Base number of internal iterations iterations_per_policy_scale=50, # How much iterations scale per added policy max_solver_iterations=15000, # Max internal solver iterations blending_factor=0.01, temperature=0.001, momentum_beta=0.2, gain_normalization=True, diversity_bonus_coeff=0.0): \"\"\"Initializes hybrid ORM solver parameters for evaluation meta-strategies. Args: base_solver_iterations: Base number of internal solver iterations for _hybrid_orm_solver. iterations_per_policy_scale: Amount to increase internal solver iterations per added policy. max_solver_iterations: Maximum internal solver iterations. blending_factor: Weight (0 to 1) for the smoothed best pure strategy component. temperature: Temperature for softmax smoothing. momentum_beta: Momentum for optimistic updates. gain_normalization: Normalizes gains for scale-invariance. diversity_bonus_coeff: Diversity bonus, kept very low for evaluation. \"\"\" self._base_solver_iterations = base_solver_iterations self._iterations_per_policy_scale = iterations_per_policy_scale self._max_solver_iterations = max_solver_iterations 25 Discovering Multiagent Learning Algorithms with Large Language Models self._blending_factor = blending_factor self._temperature = temperature self._momentum_beta = momentum_beta self._gain_normalization = gain_normalization self._diversity_bonus_coeff = diversity_bonus_coeff def get_meta_strategy(self, game, policy_sets, meta_games): \"\"\"Returns blended meta strategies for evaluation in policy-space response oracles. Args: game: The pyspiel game object. policy_sets: list of lists of policies, one list per player. policy_sets[p][i] is player ps i-th policy. len(policy_sets[p]) == meta_games[0].shape[p]. meta_games: list of n-dimensional numpy arrays, one per player. Each array has shape (num_strats_p0, num_strats_p1, ..., num_strats_pn-1) and meta_games[p][i0, i1, ..., in-1] is the payoff of player when player chooses strategy ik. Returns: list of mixed-strategies, one for each player. Each mixed strategy is list of non-negative weights (not necessarily normalized). It is used for evaluation of the current PSRO policies. E.g., computing exploitability. \"\"\" del game, policy_sets # Unused num_current_policies_p0 = len(meta_games[0]) # Assuming symmetric populations solver_iterations = int(self._base_solver_iterations + solver_iterations = np.clip(solver_iterations, self._base_solver_iterations, self. _max_solver_iterations) self._iterations_per_policy_scale * (num_current_policies_p0 - 1)) strategies = _hybrid_orm_solver( meta_games, iterations=solver_iterations, # Use adaptive iterations blending_factor=self._blending_factor, temperature=self._temperature, momentum_beta=self._momentum_beta, gain_normalization=self._gain_normalization, diversity_bonus_coeff=self._diversity_bonus_coeff, return_average_strategy=False # Eval explicitly requests last-iterate strategy ) return [s.tolist() for in strategies] 7.2. Asymmetric Optimistic Discounted CFR In an early trial, we discover another variant of CFR by training on 2-player Kuhn Poker, 2-player Leduc Poker, 4-Card Goofspiel and 4-side Liar Dice. It also shows good empirical performance and employs relatively more conventional mechanisms than VAD-CFR. We show its empirical performance together with other CFR variants in Figure 3. The source code is in Listing 7. Unlike manual heuristics which often rely on symmetric or fixed update rules, AOD-CFR utilizes set of dynamic, asymmetric mechanisms discovered via evolutionary search. Adaptive Regret Discounting: The algorithm employs linear schedule for discounting cumulative regrets. The positive regret discount factor, ð›¼, transitions from 1.0 2.5 over 500 iterations, while the negative regret discount factor, ð›½, transitions from 0.5 0.0. This allows the algorithm to aggressively prune suboptimal actions early (via ð›½) while increasingly emphasizing recent information for optimal actions (via ð›¼). Asymmetric Instantaneous Scaling: distinct feature of AOD-CFR is its sign-dependent scaling of instantaneous regret. As revealed in the source code, the update rule scales instantaneous regret by 1.1 when both cumulative and instantaneous regrets are positive, but attenuates it by 0.9 when 26 Discovering Multiagent Learning Algorithms with Large Language Models cumulative regret is positive and instantaneous regret is negative. This asymmetry likely functions as momentum-preservation mechanism, reinforcing established beneficial actions while damping noise. Trend-Based Policy Optimism: In the policy derivation step, AOD-CFR incorporates trendbased optimism term. It tracks an Exponential Moving Average (EMA) of cumulative regrets (decay 0.1) and adds scaled deviation term to the accumulated regret before applying Regret Matching. Specifically, the code implements scaling factor optimism_trend_scale applied to the difference between current cumulative regret and its EMA. Polynomial Policy Averaging: The average strategy is computed using an adaptive polynomial weighting scheme where the exponent ð›¾ scales linearly from 1.0 5.0. Listing 7 AOD-CFR import math import collections import attr class RegretAccumulator: \"\"\"Updates cumulative regret with adaptive positive and negative discounting, and asymmetric instantaneous regret scaling.\"\"\" def __init__(self, alpha_start: float = 1.0, alpha_max: float = 2.5, schedule_T_alpha: float = 500.0, beta_start: float = 0.5, beta_max: float = 0.0, schedule_T_beta: float = 500.0, pos_cum_pos_inst_scale: float = 1.1, pos_cum_neg_inst_scale: float = 0.9, neg_cum_pos_inst_scale: float = 0.7, neg_cum_neg_inst_scale: float = 1.2): # R_cum > 0, r_inst < 0 # R_cum < 0, r_inst > 0 # R_cum < 0, r_inst < 0 # R_cum > 0, r_inst > 0 \"\"\"Initializes the regret accumulator. Args: alpha_start: Initial exponent for positive cumulative regret discounting. alpha_max: Final exponent for positive cumulative regret discounting. schedule_T_alpha: Iterations for alpha transition. beta_start: Initial exponent for negative cumulative regret discounting. beta_max: Final exponent for negative cumulative regret discounting. schedule_T_beta: Iterations for beta transition. pos_cum_pos_inst_scale: Scaling factor for instantaneous regret when cumulative regret is positive and instantaneous is positive. pos_cum_neg_inst_scale: Scaling factor for instantaneous regret when cumulative regret is positive and instantaneous is negative. neg_cum_pos_inst_scale: Scaling factor for instantaneous regret when cumulative regret is negative and instantaneous is positive. neg_cum_neg_inst_scale: Scaling factor for instantaneous regret when cumulative regret is negative and instantaneous is negative. \"\"\" if not (alpha_start >= 0 and alpha_max >= 0 and schedule_T_alpha >= 0): raise ValueError(\"Alpha parameters and schedule must be non-negative.\") if not (beta_start >= 0 and beta_max >= 0 and schedule_T_beta >= 0): raise ValueError(\"Beta parameters and schedule must be non-negative.\") if not (pos_cum_pos_inst_scale >= 0 and pos_cum_neg_inst_scale >= 0 and neg_cum_pos_inst_scale >= 0 and neg_cum_neg_inst_scale >= 0): raise ValueError(\"Instantaneous regret scaling factors must be non-negative.\") self._alpha_start = alpha_start self._alpha_max = alpha_max self._schedule_T_alpha = max(1.0, schedule_T_alpha) self._beta_start = beta_start self._beta_max = beta_max self._schedule_T_beta = max(1.0, schedule_T_beta) self._pos_cum_pos_inst_scale = pos_cum_pos_inst_scale self._pos_cum_neg_inst_scale = pos_cum_neg_inst_scale self._neg_cum_pos_inst_scale = neg_cum_pos_inst_scale self._neg_cum_neg_inst_scale = neg_cum_neg_inst_scale self._epsilon = 1e-9 # Small value for sign comparisons 27 Discovering Multiagent Learning Algorithms with Large Language Models def update_accumulate_regret( self, info_state_node, iteration_number, cfr_regrets ): \"\"\"Updates cumulative regrets with adaptive discounting and asymmetric instantaneous regret scaling. Args: info_state_node: Data structure for the information set. iteration_number: The current CFR iteration (0-based). cfr_regrets: Instantaneous counterfactual regrets for the current iteration. Returns: Updated cumulative regret dictionary for each action. \"\"\" = float(iteration_number) + 1.0 # 1-indexed for weighting calculations iter_float = float(iteration_number) # 0-based for schedule progress # Calculate current_alpha for positive cumulative regret discounting t_norm_alpha = min(1.0, iter_float / self._schedule_T_alpha) current_alpha = self._alpha_start + (self._alpha_max - self._alpha_start) * t_norm_alpha # Calculate current_beta for negative cumulative regret discounting t_norm_beta = min(1.0, iter_float / self._schedule_T_beta) current_beta = self._beta_start + (self._beta_max - self._beta_start) * t_norm_beta updated_cumulative_regret = {} legal_actions = info_state_node.legal_actions for action in legal_actions: prev_cumulative_regret = info_state_node.cumulative_regret.get(action, 0.0) current_instantaneous_regret = cfr_regrets.get(action, 0.0) # Determine instantaneous regret scaling factor based on signs instantaneous_scaling_factor = 1.0 # Default if prev_cumulative_regret > self._epsilon: # Cumulative regret is positive if current_instantaneous_regret > self._epsilon: instantaneous_scaling_factor = self._pos_cum_pos_inst_scale elif current_instantaneous_regret < -self._epsilon: instantaneous_scaling_factor = self._pos_cum_neg_inst_scale elif prev_cumulative_regret < -self._epsilon: # Cumulative regret is negative if current_instantaneous_regret > self._epsilon: instantaneous_scaling_factor = self._neg_cum_pos_inst_scale elif current_instantaneous_regret < -self._epsilon: instantaneous_scaling_factor = self._neg_cum_neg_inst_scale # If one or both are near zero, instantaneous_scaling_factor remains 1.0 scaled_instantaneous_regret = current_instantaneous_regret * instantaneous_scaling_factor # Calculate discount factor for the previous cumulative regret based on its sign discount_factor = 0.0 if prev_cumulative_regret > 0: try: t_pow_alpha = math.pow(t, current_alpha) # If t^alpha overflows, the factor approaches 1.0 discount_factor = t_pow_alpha / (t_pow_alpha + 1.0) if t_pow_alpha != float(inf) else 1.0 except ValueError: # Fallback for potential issues with very large and alpha discount_factor = 1.0 if > 1.1 else 0.5 # t=1 is 0.5 (1/2), large is 1.0 (inf/ inf) else: # prev_cumulative_regret is non-positive try: t_pow_beta = math.pow(t, current_beta) # If t^beta overflows, the factor approaches 1.0 discount_factor = t_pow_beta / (t_pow_beta + 1.0) if t_pow_beta != float(inf) else 1.0 except ValueError: # Fallback for potential issues with very large and beta discount_factor = 1.0 if > 1.1 else 0.5 # t=1 is 0.5 (1/2), large is 1.0 (inf/ inf) Discovering Multiagent Learning Algorithms with Large Language Models discounted_prev_regret = prev_cumulative_regret * discount_factor # Add the scaled current instantaneous regret. new_cumulative_regret = discounted_prev_regret + scaled_instantaneous_regret updated_cumulative_regret[action] = new_cumulative_regret return updated_cumulative_regret import math import collections import attr # Ensure attr is imported if not already globally class PolicyFromRegretAccumulator: \"\"\"Calculates the current policy using cumulative regrets plus an optimistic term based on the deviation of current cumulative regret from its EMA.\"\"\" def __init__(self, use_squared_weights: bool = True, cumulative_regret_ema_alpha: float = 0.1, # EMA factor for cumulative regrets ( lower means more smoothing) optimism_trend_scale: float = 0.5): # Scaling factor for the R^t - EMA(R^{t -1}) term \"\"\"Initializes the policy calculator with trend-based optimism. Args: use_squared_weights: If True (default), use max(0, + scaled_optimism)^2 for policy weights (CFR+ style). style). If False, use max(0, + scaled_optimism) for policy weights (RM+ cumulative_regret_ema_alpha: Decay factor for the EMA of cumulative regrets (0 < alpha <= 1). . Higher alpha gives more weight to recent cumulative regrets optimism_trend_scale: Scaling factor applied to the difference between current cumulative regret and its EMA (R^t - EMA(R^{t-1})) before adding it to R^t. \"\"\" if not (0.0 < cumulative_regret_ema_alpha <= 1.0): raise ValueError(\"cumulative_regret_ema_alpha must be between 0 (exclusive) and 1 ( inclusive).\") if optimism_trend_scale < 0: raise ValueError(\"optimism_trend_scale must be non-negative.\") self._use_squared_weights = use_squared_weights self._cumulative_regret_ema_alpha = cumulative_regret_ema_alpha self._optimism_trend_scale = optimism_trend_scale # Store EMA of *cumulative* regrets: {infoset_index: {action: ema_value}} self._ema_cumulative_regrets = collections.defaultdict(lambda: collections.defaultdict(float )) # Epsilon for safe floating point comparisons for policy normalization. self._epsilon_norm = 1edef get_updated_current_policy(self, info_state_node, iteration_number, cfr_regrets, previous_policy): \"\"\"Calculates the current policy using cumulative regrets plus trend-based optimism. Args: info_state_node: data structure corresponding to an information set with cumulative_regret and cumulative_policy stored. cumulative_regret (R^t) has been updated by RegretAccumulator for the current iteration. iteration_number: the current CFR iteration (0-based) (unused but kept for API). cfr_regrets: the instantaneous counterfactual regrets (r^t) for the current iteration ( unused). previous_policy: the previous policy at the current information set (unused). 29 Discovering Multiagent Learning Algorithms with Large Language Models Returns: updated current policy dictionary at the current information set. \"\"\" cumulative_regrets_map = info_state_node.cumulative_regret actions = info_state_node.legal_actions infoset_index = info_state_node.index_in_tabular_policy # Unique key for EMA storage # R^t action_to_policy_weight = {} sum_policy_weights = 0.0 action_to_new_ema = {} # Store new EMA values before overwriting state if not actions: # Handle cases with no legal actions return {} num_actions = len(actions) # Used for uniform policy fallback # Get the EMA map for this specific infoset infoset_ema_map = self._ema_cumulative_regrets[infoset_index] for action in actions: # Get the current cumulative regret R^t(a) current_cum_regret = cumulative_regrets_map.get(action, 0.0) # Get the previous EMA of cumulative regret EMA(R^{t-1})(a) prev_ema_cum_regret = infoset_ema_map.get(action, 0.0) # Defaults to 0.0 on first encounter # Calculate the deviation = R^t(a) - EMA(R^{t-1})(a) regret_trend = current_cum_regret - prev_ema_cum_regret # Calculate the scaled optimistic term scaled_optimism = self._optimism_trend_scale * regret_trend # Calculate the value used for Regret Matching: R^t(a) + scaled_optimism combined_value = current_cum_regret + scaled_optimism # Apply Regret Matching logic: take the positive part. positive_combined_value = max(0.0, combined_value) # Use linear or squared weighting based on the flag if self._use_squared_weights: policy_weight = positive_combined_value ** 2 # CFR+ style else: policy_weight = positive_combined_value # RM+ style action_to_policy_weight[action] = policy_weight sum_policy_weights += policy_weight # Update the EMA of cumulative regret for the next iteration (t) # EMA^t = alpha * R^t + (1 - alpha) * EMA^{t-1} new_ema_cum_regret = self._cumulative_regret_ema_alpha * current_cum_regret + (1.0 - self._cumulative_regret_ema_alpha) * prev_ema_cum_regret action_to_new_ema[action] = new_ema_cum_regret # Update the stored EMA values *after* using the previous values for all actions for action in actions: infoset_ema_map[action] = action_to_new_ema[action] # Normalize to get the policy info_state_policy = {} if sum_policy_weights > self._epsilon_norm: # Use normalization epsilon # Normalize positive weights (or their squares) to get policy probabilities for action in actions: info_state_policy[action] = action_to_policy_weight[action] / sum_policy_weights else: # Default to uniform policy if sum of weights is zero or numerically close to zero uniform_prob = 1.0 / num_actions if num_actions > 0 else 0.0 for action in actions: info_state_policy[action] = uniform_prob 30 Discovering Multiagent Learning Algorithms with Large Language Models return info_state_policy class PolicyAccumulator: \"\"\"Updates cumulative policy using adaptive polynomial weighting (like DCFR/PDCFR+).\"\"\" def __init__(self, gamma_start: float = 1.0, gamma_max: float = 5.0, gamma_schedule_T: float = 500.0): \"\"\"Initializes the policy accumulator with adaptive gamma. Args: gamma_start: Initial exponent for weighting the current policy (t^gamma). gamma_max: Max gamma value for later exploitation. gamma_schedule_T: Number of iterations over which gamma transitions. \"\"\" if gamma_start < 0 or gamma_max < 0 or gamma_schedule_T < 0: raise ValueError(\"PolicyAccumulator gamma parameters must be non-negative.\") self._gamma_start = gamma_start self._gamma_max = gamma_max self._gamma_schedule_T = max(1.0, gamma_schedule_T) # Avoid division by zero def update_accumulate_policy( self, info_state_node, iteration_number, info_state_policy, cfr_regrets, # Not used reach_prob, counterfactual_reach_prob, # Not used ): \"\"\"Updates the cumulative policy using adaptive polynomial weighting. Args: info_state_node: Data structure for the information set. iteration_number: The current CFR iteration (0-based). info_state_policy: The current policy profile for this iteration. reach_prob: Players reach probability for this iteration. Returns: Updated cumulative policy dictionary for each action. \"\"\" # Use 1-based iteration number for calculations = float(iteration_number) + 1.0 # Use 0-based iteration for schedule progress calculation iter_float = float(iteration_number) # Calculate current_gamma based on linear schedule t_norm_gamma = min(1.0, iter_float / self._gamma_schedule_T) current_gamma = self._gamma_start + (self._gamma_max - self._gamma_start) * t_norm_gamma # Calculate t^current_gamma, which is used for both discounting and weighting. t_pow_gamma = 1.0 if > 0: # is 1-indexed (float(iteration_number) + 1.0), so t=0 is not possible here. try: t_pow_gamma = math.pow(t, current_gamma) except ValueError: # Fallback for extremely large exponents to avoid overflow, treat as very large. t_pow_gamma = float(inf) # Calculate the discount factor for the previous cumulative policy sum. # This mirrors the discounting logic used for positive cumulative regrets in RegretAccumulator . # If t_pow_gamma is inf, discount factor approaches 1.0. policy_discount_factor = t_pow_gamma / (t_pow_gamma + 1.0) if t_pow_gamma != float(inf) else 1.0 # Calculate the weight for the current policys contribution (Adaptive CFR style). # This weights the current policy by t^gamma and the players reach probability. current_policy_contribution_weight = t_pow_gamma * reach_prob 31 Discovering Multiagent Learning Algorithms with Large Language Models updated_cumulative_policy = {} # Iterate over all legal actions and actions present in the cumulative policy # to ensure proper discounting and updating of all relevant entries. all_actions = set(info_state_node.legal_actions) set(info_state_node.cumulative_policy.keys ()) for action in all_actions: previous_cumulative_policy = info_state_node.cumulative_policy.get(action, 0.0) current_policy_prob = info_state_policy.get(action, 0.0) # Discount the previously accumulated policy sum. discounted_previous_cumulative_policy = previous_cumulative_policy * policy_discount_factor # Add the current policys weighted contribution. current_contribution = current_policy_contribution_weight * current_policy_prob updated_cumulative_policy[action] = ( discounted_previous_cumulative_policy + current_contribution ) # Ensure all *legal* actions are present in the final dict, even if value is 0.0 for action in info_state_node.legal_actions: if action not in updated_cumulative_policy: # This happens if an action was legal but never accumulated policy before # and isnt in current policy (prob=0). Its value should be 0 # after discounting the previous 0 and adding 0 contribution. updated_cumulative_policy[action] = 0.0 return updated_cumulative_policy 7.3. Results on 11 Games 7.4. Prompts Listing 8 Prompt for Evolving CFR Act as an expert in game theory, multiagent learning, online learning and optimization. Your task is to iteratively improve new variant of counterfactual regret minimization. The primary goal is to speed up convergence to low-exploitable strategies. Always adhere to best practices in Python coding. key data structure that is used is infostate_node: python @attr.s class _InfoStateNode(object): \"\"\"An object wrapping values associated to an information state.\"\"\" # The list of the legal actions. legal_actions = attr.ib() index_in_tabular_policy = attr.ib() # Map from information states string representations and actions to the # counterfactual regrets, accumulated over the policy iterations cumulative_regret = attr.ib(factory=lambda: collections.defaultdict(float)) # Same as above for the cumulative of the policy probabilities computed # during the policy iterations cumulative_policy = attr.ib(factory=lambda: collections.defaultdict(float)) You are allowed to modify three key components of CFR: (1) how are the regret values accumulated at each infoset (RegretAccumulator) (2) how to obtain current policy at the current iteration from the current cumulative_regret (PolicyFromRegretAccumulator) and (3) how to accumulate policies across iterations to compute an average policy (PolicyAccumulator). # Prior programs Discovering Multiagent Learning Algorithms with Large Language Models Figure 3 CFR variants performances on All Games. Previously we found that the following programs performed well on the task at hand: {previous_programs} # Current program Here is the current program we are trying to improve (you will need to propose modification to it below): {code} # *SEARCH/REPLACE block* Rules: Every *SEARCH/REPLACE block* must use this format: 1. The opening fence: python 2. The start of search block: <<<<<<< SEARCH 3. contiguous chunk of up to 4 lines to search for in the existing source code 4. The dividing line: ======= 5. The lines to replace into the source code 6. The end of the replace block: >>>>>>> REPLACE 33 Discovering Multiagent Learning Algorithms with Large Language Models Figure 4 PSRO variants performances on All Games. 7. The closing fence: Every *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc. *SEARCH/REPLACE* blocks will replace *all* matching occurrences. Include enough lines to make the SEARCH blocks uniquely match the lines to change. Keep *SEARCH/REPLACE* blocks concise. Break large *SEARCH/REPLACE* blocks into series of smaller blocks that each change small portion of the file. Include just the changing lines, and few surrounding lines if needed for uniqueness. Do not include long runs of unchanging lines in *SEARCH/REPLACE* blocks. To move code within file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location. Make sure that the changes you propose are consistent with each other. For example, if you refer to new config variable somewhere, you should also propose change to add that variable. Discovering Multiagent Learning Algorithms with Large Language Models Example: python <<<<<<< SEARCH return total_loss ======= # Add sparsity-promoting regularization to the loss. total_loss += self.hypers.l1_reg_weight * l1_reg return total_loss {replace} and python <<<<<<< SEARCH return hyper.zipit([ ======= return hyper.zipit([ hyper.uniform(l1_reg_weight, hyper.interval(0.0, 0.01)), {replace} {lazy_prompt} ONLY EVER RETURN CODE IN *SEARCH/REPLACE BLOCK*! # Task {task_instruction} {focus_sentence} {trigger_chain_of_thought} Describe each change with *SEARCH/REPLACE block*. Listing 9 Prompt for Evolving PSRO Act as an expert in game theory, multiagent learning, online learning and optimization. Your task is to iteratively improve variant of Policy-Space Response Oracles (PSRO). The primary goal is to speed up convergence to low-exploitable strategies. # PSRO Overview PSRO iteratively builds population of policies for each player and computes meta-strategies ( distributions over policies) to guide training and evaluation. **Each iteration:** 1. **Empirical game**: Simulate payoffs between all policy combinations to form game tensor. 2. **Train-time meta-strategy**: Compute distribution over current policies for each player. This determines what opponents the best-response oracle trains against. 3. **Best response**: Add new policy for each player that best responds to opponents train-time meta-strategies. 4. **Eval-time meta-strategy**: Compute (possibly different) distribution for evaluation, e.g., to measure exploitability. **Your task**: Improve both the **train-time** and **eval-time** meta-strategy solvers. These serve different purposes: train-time guides population growth, eval-time assesses solution quality. # Available Utilities **Best Response** - BestResponsePolicy(game, player_id, policy): Returns best-response policy for player_id against policy. - Use .value(game.new_initial_state()) to get the BR value. **Policy Aggregation** - PolicyAggregator(game).aggregate(pids, policy_sets, weights): Creates mixed policy from weighted pure policies. - pids: list(range(game.num_players())) - weights: [weights_p0, weights_p1, ...], each matching len(policy_sets[p]) **Policy Evaluation** - expected_game_score.policy_value(state, joint_policy): Returns expected payoffs (list, one per player). 35 Discovering Multiagent Learning Algorithms with Large Language Models - joint_policy: [policy_p0, policy_p1, ...] Always adhere to best practices in Python coding. # Prior programs Previously we found that the following programs performed well on the task at hand: {previous_programs} # Current program Here is the current program we are trying to improve (you will need to propose modification to it below): {code} # *SEARCH/REPLACE block* Rules: Every *SEARCH/REPLACE block* must use this format: 1. The opening fence: python 2. The start of search block: <<<<<<< SEARCH 3. contiguous chunk of up to 4 lines to search for in the existing source code 4. The dividing line: ======= 5. The lines to replace into the source code 6. The end of the replace block: >>>>>>> REPLACE 7. The closing fence: Every *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc. *SEARCH/REPLACE* blocks will replace *all* matching occurrences. Include enough lines to make the SEARCH blocks uniquely match the lines to change. Keep *SEARCH/REPLACE* blocks concise. Break large *SEARCH/REPLACE* blocks into series of smaller blocks that each change small portion of the file. Include just the changing lines, and few surrounding lines if needed for uniqueness. Do not include long runs of unchanging lines in *SEARCH/REPLACE* blocks. To move code within file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location. Make sure that the changes you propose are consistent with each other. For example, if you refer to new config variable somewhere, you should also propose change to add that variable. Example: python <<<<<<< SEARCH return total_loss ======= # Add sparsity-promoting regularization to the loss. total_loss += self.hypers.l1_reg_weight * l1_reg return total_loss {replace} and python <<<<<<< SEARCH return hyper.zipit([ ======= return hyper.zipit([ hyper.uniform(l1_reg_weight, hyper.interval(0.0, 0.01)), {replace} {lazy_prompt} ONLY EVER RETURN CODE IN *SEARCH/REPLACE BLOCK*! # Task {task_instruction} {focus_sentence} {difficulty_hint} {exploration_nudge} { Discovering Multiagent Learning Algorithms with Large Language Models trigger_chain_of_thought} Describe each change with *SEARCH/REPLACE block*."
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}