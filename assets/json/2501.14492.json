{
    "paper_title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques",
    "authors": [
        "Zhengyang Tang",
        "Ziniu Li",
        "Zhenyang Xiao",
        "Tian Ding",
        "Ruoyu Sun",
        "Benyou Wang",
        "Dayiheng Liu",
        "Fei Huang",
        "Tianyu Liu",
        "Bowen Yu",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Critiques are important for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique capabilities of LLMs presents a significant challenge due to the open-ended nature of the task. In this work, we introduce a new benchmark designed to assess the critique capabilities of LLMs. Unlike existing benchmarks, which typically function in an open-loop fashion, our approach employs a closed-loop methodology that evaluates the quality of corrections generated from critiques. Moreover, the benchmark incorporates features such as self-critique, cross-critique, and iterative critique, which are crucial for distinguishing the abilities of advanced reasoning models from more classical ones. We implement this benchmark using eight challenging reasoning tasks. We have several interesting findings. First, despite demonstrating comparable performance in direct chain-of-thought generation, classical LLMs significantly lag behind the advanced reasoning-based model o1-mini across all critique scenarios. Second, in self-critique and iterative critique settings, classical LLMs may even underperform relative to their baseline capabilities. We hope that this benchmark will serve as a valuable resource to guide future advancements. The code and data are available at \\url{https://github.com/tangzhy/RealCritic}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 2 9 4 4 1 . 1 0 5 2 : r Preprint. Work in progress. RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques Zhengyang Tang1,2, Ziniu Li1,3, Zhenyang Xiao1,3, Tian Ding3, Ruoyu Sun1,3, Benyou Wang1, Dayiheng Liu2, Fei Huang2, Tianyu Liu2, Bowen Yu2, and Junyang Lin2 1The Chinese University of Hong Kong, Shenzhen 2Qwen Team, Alibaba Inc. 3Shenzhen Research Institute of Big Data"
        },
        {
            "title": "Abstract",
            "content": "Critiques are important for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique capabilities of LLMs presents significant challenge due to the open-ended nature of the task. In this work, we introduce new benchmark designed to assess the critique capabilities of LLMs. Unlike existing benchmarks, which typically function in an open-loop fashion, our approach employs closed-loop methodology that evaluates the quality of corrections generated from critiques. Moreover, the benchmark incorporates features such as self-critique, cross-critique, and iterative critique, which are crucial for distinguishing the abilities of advanced reasoning models from more classical ones. We implement this benchmark using eight challenging reasoning tasks. We have several interesting findings. First, despite demonstrating comparable performance in direct chain-ofthought generation, classical LLMs significantly lag behind the advanced reasoning-based model o1-mini across all critique scenarios. Second, in self-critique and iterative critique settings, classical LLMs may even underperform relative to their baseline capabilities. We hope that this benchmark will serve as valuable resource to guide future advancements. The code and data are available at https://github.com/tangzhy/RealCritic."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in Large Language Models (LLMs) (Hurst et al., 2024; Dubey et al., 2024; Mistral-AI, 2024; Qwen-Team, 2024) have demonstrated remarkable capabilities across lots of tasks, ranging from natural language understanding to problem-solving. Notably, reasoning-based LLMs have surpassed human expertise in specialized domains such as competition-level mathematical reasoning and code generation (OpenAI, 2024). However, as LLMs continue to evolve, further improving their performance has become increasingly challenging. One of the key areas of focus in this regard is the development of critique abilities (McAleese et al., 2024)the capability of LLMs to provide detailed analysis, constructive feedback, and refinement suggestions for solutions generated by other models or even themselves, thereby driving continuous improvement in their outputs. An important step toward advancing critique abilities is the establishment of high-quality benchmarks for evaluation. Without such benchmarks, it is difficult to objectively measure progress in this area. However, evaluating the quality of critiques is difficult due to the Equal contribution. Corresponding authors. Emails: zhengyantang@link.cuhk.edu.cn, ziniuli@link.cuhk.edu.cn, kin@stu.pku.edu.cn (Equal contribution); dingtian@sribd.cn, wangbenyou@cuhk.edu.cn, liudayiheng.ldyh@alibaba-inc.com (Corresponding authors). 1 Preprint. Work in progress. Figure 1: Benchmark results for self-critique and cross-critique abilities of representative LLMs. Our findings reveal clear seperation between classical LLMs and reasoning-based LLMs: o1-mini is the only model demonstrating improvement in self-critique tasks, while also achieving the most significant gains in cross-critique performance. Dataset CriticBench (Luo et al., 2023) CriticBench (Lin et al., 2024) CriticEval (Lan et al., 2024) Shepherd (Wang et al., 2023) Auto-J (Li et al., 2023) Output Format C+V C+V C+S Critique Type Self+Cross Cross Cross Cross Cross Evaluation Method Verdict Matching Verdict Matching Score Judging GPT-4 Comparison GPT-4 Comparison RealCritic C+V+Corr Self+Cross Correction Matching Iterative Support Test Data Size 0 3,825 3,608 352 232 2,093 Public Release Table 1: Comparison of critique evaluation approaches. = Critique, = Verdict, = Score, Corr = Correction. = Yes, = No. open-ended nature, which lacks single ground truth answer for definitive judgment. Furthermore, critiques often serve as intermediate variables within continuous improvement pipeline, where their ultimate value lies in their ability to refine and enhance subsequent outputs rather than being judged in isolation. Significant efforts have been made to benchmark the critique capabilities of LLMs. For instance, CriticBench (Luo et al., 2023; Zheng et al., 2024) requires LLMs to generate critiques accompanied by binary verdicts (C+V) for input solutions, where the verdicts predict whether the solutions are correct or incorrect. The quality of the critiques is then evaluated by comparing these verdicts to ground truth labels, providing measure of accuracy. CriticEval (Lan et al., 2024) conducts comprehensive evaluation of LLMs critique abilities, introducing two key dimensions relevant to our study: feedback and correction. The feedback dimension assesses the quality of the analysis within the critique by assigning prediction score, while the correction dimension evaluates the quality of revisions based on the critiques analysis. Beyond dedicated benchmarks, other works such as Shepherd (Wang et al., 2023) and Auto-J (Li et al., 2023) have proposed using GPT-4 to perform comparative evaluations between different critiques. Despite these advancements, the key question remains: how can we effectively measure the quality of critiques? This challenge is supported by our preliminary study on CriticBench. Using human evaluators, we find that the benchmark suffers from high misjudgment rate (about 30% of low-quality critiques are erroneously classified as high quality; see Table 2 in Section 2). Additionally, we observe that existing benchmarks often lack support for essential critique tasks, such as self-critique and iterative critique, which are believed to be crucial for reasoning-based LLMs (Wu et al., 2024a). In this paper, we propose RealCritic, new benchmark designed to evaluate critique abilities based on their effectiveness in driving refinement; see Table 1 for an overview. Recognizing that critique is high-quality if it contributes meaningfully to the self-improvement of LLMs, we propose to measure critique quality directly based on the correction it enables. Our approach is closed-loop, drawing inspiration from control theory, where the output Preprint. Work in progress. (correction) is fed back into the system to assess the effectiveness of the input (critique). In contrast, previous methods for measuring critique quality are often open-loop, evaluating critiques in isolation without considering their downstream impact on solution refinement or treating correction abilities separately from critique abilities. Furthermore, our benchmark introduces advanced evaluation scenarios to assess LLMs critique capabilities. Specifically, we distinguish between self-critique and cross-critique abilities and incorporate iterative critique, providing comprehensive framework for evaluating critique effectiveness. We operationalize RealCritic through 8 challenging tasks: 5 from open-ended mathematical reasoning and 3 from general-domain multiple-choice questions. These tasks are sufficiently challenging and, importantly, have clear ground truth answers for evaluating solution correctness, even though critiques themselves lack ground truth. We evaluate several state-of-the-art models, including LLaMA-3.1-70B-Instruct (Dubey et al., 2024), Qwen2.572B-Instruct (Qwen-Team, 2024), Mistral-Large-Instruct-2407 (Mistral-AI, 2024), and o1-mini (OpenAI, 2024). One of our key findings is illustrated in Figure 1, with additional results and analyses detailed in the main text. To summarize, our contributions are three fold: We introduce simple yet effective approach to measure critique quality directly based on its impact on solution refinement. We develop benchmark that supports essential critique functions, including selfcritique, cross-critique, and iterative critique, providing comprehensive evaluation framework tailored to the needs of reasoning-based LLMs. We conduct comprehensive analysis of existing LLMs critique abilities using the proposed framework. This analysis highlights the strengths and limitations of current models."
        },
        {
            "title": "2 Toward Effectiveness-Driven Evaluation",
            "content": "In this section, we discuss strategies for constructing an effective benchmark for evaluating critiques. Evaluating the critique capabilities of LLMs is inherently challenging due to the multifaceted and open-ended nature of the task. high-quality critique must fulfill several criteria: it should identify flaws or weaknesses in the solution, provide constructive and actionable feedback, and maintain clarity and logical consistency. These requirements complicate the evaluation process, as each sub-part allows for wide range of valid responses. While human evaluators may perform well in assessing critique quality, human-based evaluation methods are neither automatic nor scalable. To address these challenges, it is crucial to leverage task-specific properties. Reasoning tasks, for instance, are particularly well-suited for this purpose, as they share key characteristic: the final generated answer is typically single, definitive response. This property can be utilized to facilitate the evaluation of solutions. CriticBench (Luo et al., 2023; Lin et al., 2024) leverages this property to evaluate critiques through the following two-step process: Critic Generation: This benchmark requires LLMs to generate critiques that provide detailed analysis of solution. In addition to the analysis, the model must also produce verdict predicting whether the input solution is correct or incorrect. Quality Evaluation: The benchmark assesses the quality of the critique based on the correctness of the verdict. If the verdict is correct, the critique is deemed high quality. Conversely, if the verdict is incorrect, the critique is judged as low quality. CriticEval (Lan et al., 2024) uses similar approach in its evaluation framework, particularly in the dimension they define as feedback. We would like to point out that the fundamental premise of this approach is to evaluate critique quality based on the accuracy of predicting the solutions correctness. We argue that this evaluation method may not always be effective, as an LLM might correctly predict the solutions accuracy without providing constructive analysis or critique. This misalignment stems from the fact that the evaluation approach does not fully capture the intuition that critique is considered high-quality if it leads to 3 Preprint. Work in progress. an improved solution, rather than predicting the accuracy of the input solution. specific example illustrating this limitation is shown in Figure 2. Furthermore, when using human evaluators to provide ground truth assessments of critique quality, we find that CriticBench suffers from high misjudgment rate, further highlighting its limitations as shown in Table 2 (see details and more concrete examples in Appendix A). Figure 2: Examples illustrating the limitations of CriticBench (Lin et al., 2024): it incorrectly classifies low-quality critique as high-quality by relying on the accuracy of predicting the input solution as the metric for critique quality. In contrast, our RealCritic accurately identifies high-quality critiques based on their effectiveness in guiding the generation of improved solutions. Verdict Critique Quality Matched Unmatched Total High-quality Low-quality Total 55.3 26.0 81.3 14.6 4.1 18.7 69.9 30.1 100. Table 2: Human evaluation results of critique quality on CriticBench (Lin et al., 2024). All values are in percentages (%). We believe the aforementioned fragility issue arises from the open-loop nature of the evaluation method employed. Specifically, the quality of the generated critique is not directly linked to its impact on subsequent refinement, leading to potential inconsistencies in evaluation. While this issue is briefly mentioned in CriticEval (Lan et al., 2024), the work does not empirically validate the severity of the issue, as we have done in our study. In addition to the limitations in evaluation metrics discussed earlier, we also find that existing benchmarks lack comprehensive modes for evaluating critique abilities. First, most benchmarks focus on critiquing input solutions that have been previously collected, operating in cross-critique mode. However, there is no systematic exploration of another important mode: self-critique, where input solutions are model-specific and dynamically generated. Additionally, existing benchmarks do not include an evaluation mode for multiround iterative critique, which is essential for examining the long-horizon critique abilities of LLMs and could be particularly crucial for evaluating emergent models such as o1 4 Preprint. Work in progress. OpenAI (2024). We plan to address these limitations in the next section by introducing new benchmark."
        },
        {
            "title": "3 Our Evaluation Framework",
            "content": "Figure 3: Comparison between the evaluation method used in CriticBench and our framework, RealCritic. Our framework operates in closed-loop manner by assessing the quality of critic through the quality of the new solution generated based on its feedback. In this section, we present our evaluation framework for benchmarking the critique abilities of LLMs in mathematical reasoning domains. Our framework is built around three key components: the closed-loop evaluation method described in Section 3.1, the spectrum of critique abilities outlined in Section 3.2, and the targeted datasets we curate for evaluation, as detailed in Section 3.3. 3.1 From Open-Loop Evaluation to Closed-Loop Evaluation As discussed in Section 2, we observe that the majority of existing benchmarks rely on openloop evaluation. This method fails to account for critiques and their ongoing refinement processes, thereby limiting the effectiveness of these benchmarks. To address this limitation, we propose closed-loop methodology designed to evaluate the models critique ability through critique-and-correction framework. In this framework, the LLMs critiques are not only generated but also tested by applying them to refine the initial solution. Specifically, we assess the quality of critiques by examining the accuracy of the solution generated after the critique is applied and refined. The key insight driving this approach is that high-quality critique should naturally lead to more accurate solution. In other words, the critique is not an isolated task but an integral part of dynamic correction process that should improve the solution incrementally. This allows us to measure the effectiveness of the critique by evaluating the quality of the final solution. Formally, we define performance = Exρ[I(ycorrection)], (ycritic, ycorrection) where denotes question drawn from the distribution ρ. The term Critic-LLM represents the distribution over outputs of the (critic) LLM, with as an initial solution (response) and Icritic as the instruction prompting both the critique ycritic and correction ycorrection. The notion is the indicator function. specific example is shown in Figure 2. Critic-LLM(x, s, Icritic), We would like to clarify several points. First, while previous works (Luo et al., 2023; Lin et al., 2024; Lan et al., 2024) also explored measuring the accuracy of the refined solution, this is done in separate stage and is not used to measure the quality of the critique itself. Second, our approach leverages the quality of the refined solution to measure the quality of the generated critique. However, there are potential shortcuts where models may directly generate new solution instead of critiquing the original one. To address this, we have carefully tuned the prompt and designed post-check process to mitigate such behaviors in our evaluations (see Appendix B). 5 Preprint. Work in progress. 3.2 What to Critique and How to Critique? After defining the evaluation criteria, the next step is to design test cases that assess both the models critique ability and the underlying critique process. We highlight two key axes often overlooked in existing benchmarks. The first axis distinguishes between selfcritique, where the model evaluates its own generated solution, and cross-critique, where it critiques solutions provided by other models. While cross-critique has been widely studied, self-critique remains underexplored, despite its importance for o1-style models that refine outputs through internal feedback loops. The second axis examines whether the critique process is single-round or iterative. Complex tasks often require multi-round critique, where the model refines its feedback across iterations to address errors in initial assessments. This iterative process provides more comprehensive test of the models long-horizon reasoning capabilities. We discuss these points in detail below. Self-Critique Paradigm: The self-critique paradigm focuses on assessing models ability to identify and improve errors in its own generated solutions. Formally, the quality of critique is measured by: performanceself = Exρ[I(ycorrection)], (ycritic, ycorrection) Critic-LLM(x, s, Icritic), Critic-LLM(x, Igeneration). key feature of this paradigm is that the input solutions for critique are not fixed or precollected but are instead model-specific. This paradigm evaluates the models introspective capabilities, specifically its ability to recognize its own errors, analyze their root causes, and implement effective corrections. However, since the critic and solution generator are the same model, they inherently share the same knowledge base and potential biases, which may limit the diversity and objectivity of the critiques. Cross-Critique Paradigm: The cross-critique paradigm evaluates models ability to analyze and improve answers generated by other models. This approach is widely used in benchmarks, partly because it is straightforward to implement, as input solutions for critique can be pre-collected. This paradigm is crucial for understanding how effectively model can serve as an external evaluator. Formally, the quality of critique is measured by: performancecross = Exρ[I(ycorrection)], (ycritic, ycorrection) P Critic-LLM(x, s, Icritic), LLM(x, Igeneration), where LLM may involve any language model distribution, which is distinct from the critics distribution and could even be combination of multiple models. This diversity means that the critic must adeptly handle wide range of solution styles and error types, ensuring versatile evaluation capabilities. Iterative Critique-Correction Paradigm: Drawing on the success of o1-style models (OpenAI, 2024), it becomes evident that multi-round self-reflection and iterative refinement play decisive role in mathematical reasoning. By enabling several cycles of critique-andcorrection, we can thoroughly evaluate the models continuous improvement across successive iterations, investigate the convergence properties underpinning its error-correction process, and analyze how self-supervision mechanisms influence performanceultimately shedding light on the models capacity for robust and adaptive problem solving. Formally, we define the iterative process as follows: performancet critic, yt (yt iterative = Exρ[I(yt correction)], correction) Critic-LLM(x, s, Icritic, y1 LLM(x, Igeneration), critic, y1 correction, . . . , yt critic, yt1 correction), where denotes the iteration index compared with previous notions. This formulation highlights the models capability to refine its output through repeated self-critique, shedding light on its long-horizon reasoning and error-correction dynamics. 6 Preprint. Work in progress. 3.3 Dataset Collection Figure 4: Data collection process for constructing solutions to assess the critique abilities of LLMs. In this section, we present the strategies we propose for constructing high-quality datasets to benchmark critique abilities. An overview of our data collection process is provided in Figure 4. This process involves two key steps: first, collecting questions from variety of tasks, and second, generating solutions to these questions using multiple models. Both the questions and solutions will be used to evaluate the critique abilities of LLMs. Task Sources. Our approach began with comprehensive review of existing evaluation datasets used in prior studies that benchmark well-established models, such as the o1 model (OpenAI, 2024). We applied set of selection criteria to filter relevant datasets for our study. Specifically, the datasets had to satisfy the following conditions: they should encompass variety of difficulty levels, and they must provide explicit final answers to facilitate the verification of solution correctness. After applying these filters, we identified 20 candidate datasets, which encompassed both open-ended math reasoning tasks and general-domain multiple-choice reasoning tasks. For further details on the attributes of these datasets, please refer to Appendix C. Given the large number of datasets, evaluating all 20 candidates was not feasible within the scope of this study. To ensure cost-effective and manageable evaluation process, we selected 8 representative datasets that cover range of difficulty levels and task types. For open-ended math reasoning tasks, we chose five datasets: GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), College Math (Tang et al., 2024), Minerva Math (Lewkowycz et al., 2022), and Olympiad Bench (He et al., 2024). These datasets span broad spectrum of mathematical difficulty, from standard high school problems to advanced Olympiad-level challenges, providing diverse set of tasks that test models reasoning abilities. For general-domain reasoning tasks with multiple-choice questions, we included three datasets: GPQA-diamond (Rein et al., 2023), ARC-Challenge (Clark et al., 2018a), and MMLU-STEM (Hendrycks et al., 2020). These datasets assess performance across various domains, with an emphasis on selecting the correct answer from multiple choices. To reduce evaluation costs, we sampled up to 300 items from each dataset, including all items from smaller sets and randomly selecting 300 from larger ones to maintain diverse yet manageable data pool. Solution Sources. To generate solutions for the collected questions, we established model pool primarily composed of open-source models to minimize costs. The pool includes the Qwen2.5 series (Qwen-Team, 2024) (1.5B, 7B, and 72B), the LLaMA-3.1 series (8B and 72B) (Dubey et al., 2024), and the Deepseek math-specialized model (DeepSeek-AI, 2024) (7B). Given the varying downstream capabilities of these models, we carefully designed strategy to ensure both the quality and diversity of the generated solutions. Specifically, we sample each model once on all data points to generate their solutions. The data is then cleaned using the following steps. First, we remove overly long solutions where 7 Preprint. Work in progress. models fail to stop generating, resulting in repeated outputs until the maximum token limit is reached. Second, we eliminate questions that all models answered correctly, as these are considered too simplistic to yield meaningful insights. Both incorrect and correct solutions are valuable for testing the critique capabilities of LLMs, as they provide distinct challenges for evaluation. To ensure comprehensive assessment, we apply different strategies for collecting correct and incorrect solutions, which are detailed below. For incorrect solutions, we prioritize stronger models over weaker ones. Specifically, if multiple models provide incorrect answers to the same question, we select the solution from the strongest model. As illustrated in Table 8, when both strong and weak models make errors on the same question, the strong models solution typically includes more detailed reasoning. This offers the critic model richer information to analyze and identify errors. In contrast, weaker models often produce vague or incomplete incorrect solutions, making it more difficult for the critic model to pinpoint the exact failure points. For correct solutions, we prioritize weaker models over stronger ones. As shown in Table 9, strong models generally produce correct solutions with minimal errors in the reasoning process, making them easier for the critic model to evaluate. However, when weaker model generates correct solution, it often includes ambiguous or even incorrect intermediate steps, creating more challenging scenario for the critic model to assess. Benchmark Format and Usage Protocol. Finally we detail the key components and usage protocols of our benchmark: ARC GSM8K MATH College Minerva GPQA MMLU Olympiad Math Math STEM Bench Total Correct Wrong 263 131 132 273 136 137 268 134 134 279 139 140 261 130 131 190 95 292 146 146 267 133 134 Table 3: Dataset statistics of input solutions in RealCritic. Dataset Composition: As shown in Table 3, our benchmark comprises eight distinct datasets spanning various mathematical reasoning domains. For each dataset, we maintain balanced distribution between correct and incorrect solutions (approximately 1:1 ratio) for cross-critique. We also prepare separate set of deliberately incorrect solutions for each dataset, with results presented in Appendix D. Self-Critique Protocol: The model first generates an initial solution using Chainof-Thought (CoT) prompting (Wei et al., 2022), then reviews and corrects its own solution using our critique system prompt. Cross-Critique Protocol: Given both question and pre-collected solution, the model analyzes the solution using the same critique system prompt, identifying potential errors and suggesting specific improvements. Iterative Critique Protocol: For both self-critique and cross-critique, our benchmark supports multi-turn sessions where the model iteratively refines its analysis until reaching predefined maximum number of iterations. Concrete Prompts: Our benchmark utilizes three types of prompts (detailed in Appendix F): direct CoT prompt for initial solution generation, core critique prompt shared by both protocols for systematic analysis and correction, and follow-up prompt for iterative rounds that enforces fresh analysis. These prompts collectively support our closed-loop evaluation methodology, with concrete examples provided in Appendix G."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup In this section, we conduct experiments using the proposed evaluation benchmark. We provide the experiment configuration below. 8 Preprint. Work in progress. Models: We evaluate our framework using diverse set of state-of-the-art language models, including both open-source models (LLaMA-3.1-70B-Instruct (Dubey et al., 2024), Mistral-Large-Instruct-2407 (Mistral-AI, 2024), Qwen2.5-72B-Math-Instruct, and Qwen2.572B-Instruct (Qwen-Team, 2024)) and proprietary models (GPT-4o (Hurst et al., 2024) and o1-mini (OpenAI, 2024)). Implementation Details: For open-source models, we deploy and access them through the vLLM framework to ensure efficient inference. For proprietary models, we utilize their respective official APIs. We maintain consistent generation parameters across all models (except o1-mini which follows its official default settings): temperature = 0.0 (ensuring deterministic outputs), top-p = 1.0, and maximum tokens = 2,048. 4.2 Results and Analysis Our main results are presented in Table 4, Figure 1, and Figure 6. Depending on the critique abilities being evaluated, we present the concrete analysis in the following sections. Model LLaMA-3.170B-Instruct MistralLarge-Instruct Qwen2.5-72BMath-Instruct Qwen2.5-72B -Instruct GPT-4o o1-mini Evaluation Mode Direct COT Self-Critique (Self-Direct) Cross Critique (Cross-Base) Direct COT Self-Critique (Self-Direct) Cross Critique (Cross-Base) Direct COT Self-Critique (Self-Direct) Cross Critique (Cross-Base) Direct COT Self-Critique (Self-Direct) Cross Critique (Cross-Base) Direct COT Self-Critique (Self-Direct) Cross Critique (Cross-Base) Direct COT Self-Critique (Self-Direct) Cross Critique (Cross-Base) ARC GSM8K MATH College Math Minerva Math GPQA MMLU STEM Olympiad Bench Avg 88.6 87.5 -1.1 82.5 32. 86.7 85.2 -1.5 84.8 34.8 94.3 93.5 -0.8 72.6 22.6 94.3 92.4 -1.9 84.8 34.8 92.4 89.4 -3.0 82.1 32.1 68.1 87.5 19.4 85.2 35.2 93.4 91.9 -1.5 88.6 38. 94.5 93.4 -1.1 90.8 40.8 95.6 95.2 -0.4 86.4 36.4 96.7 96.7 0.0 88.6 38.6 94.9 93.4 -1.5 84.6 34.6 89.0 93.8 4.8 93.8 43.8 65.3 59.0 -6.3 64.9 14. 70.1 70.9 0.8 69.0 19.0 86.9 85.4 -1.5 73.1 23.1 82.5 82.5 0.0 71.6 21.6 76.5 74.6 -1.9 68.3 18.3 81.3 91.4 10.1 85.1 35.1 40.1 37.3 -2.8 49.1 -0. 41.6 41.6 0.0 50.9 0.9 45.2 45.2 0.0 52.0 2.0 46.2 46.2 0.0 51.3 1.3 41.6 40.5 -1.1 49.8 -0.2 20.1 44.1 24.0 52.7 2.7 33.7 33.7 0.0 42.1 -7. 35.6 36.8 1.2 49.4 -0.6 42.1 41.0 -1.1 50.2 0.2 37.9 37.2 -0.7 47.1 -2.9 33.0 30.7 -2.3 43.3 -6.7 28.4 41.0 12.6 49.8 -0.2 32.6 22.1 -10.5 35.3 -14. 41.1 36.3 -4.8 40.5 -9.5 42.6 41.6 -1.0 43.7 -6.3 38.9 29.5 -9.4 26.8 -23.2 34.2 24.2 -10.0 30.5 -19.5 56.3 33.2 -23.1 44.7 -5.3 63.0 53.8 -9.2 42.8 -7. 66.8 56.5 -10.3 53.8 3.8 86.3 50.7 -35.6 38.0 -12.0 70.9 59.9 -11.0 41.4 -8.6 59.9 42.5 -17.4 37.0 -13.0 81.8 53.8 -28.0 56.5 6.5 31.1 28.5 -2.6 40.8 -9. 34.5 36.0 1.5 48.7 -1.3 47.6 47.2 -0.4 53.9 3.9 44.2 43.8 -0.4 46.1 -3.9 36.3 37.1 0.8 45.3 -4.7 49.4 55.8 6.4 57.3 7.3 56.0 51.7 -4.3 55.8 5. 58.9 57.1 -1.8 61.0 11.0 67.6 62.5 -5.1 58.7 8.7 64.0 61.0 -2.9 57.2 7.2 58.6 54.1 -4.6 55.1 5.1 59.3 62.6 3.3 65.6 15.6 Table 4: Performance comparison across different evaluation modes. Direct Solution represents the models ability to solve problems directly. Self-Critique shows performance when the model critiques its own solutions. Cross-Critique indicates performance when critiquing other models solutions. (Self vs. Direct) shows the improvement from self-critique over direct solution. (Cross vs. Base) shows how much better the model performs compared to the baseline performance (50%) in cross-model critique tasks. All numbers are in percentages (%). 4.2.1 Self-Critique Performance Analysis Our analysis of self-critique capabilities reveals interesting patterns across different models. Most models show varying degrees of performance changes in self-critique compared to their direct CoT solutions, with changes ranging from -5.1% to +3.3% on average. This suggests that recognizing and correcting ones own mistakes remains challenging task 9 Preprint. Work in progress. even for state-of-the-art language models. However, we notice that o1-mini stands out as remarkable exception, demonstrating strong self-improvement capabilities with +3.3% average improvement over direct solutions. This exceptional performance is particularly evident across multiple tasks: College Math (+24.0%), ARC (+19.4%), MATH (+10.1%), and notably even on the challenging OlympiadBench (+6.4%). The consistent improvement across diverse mathematical tasks suggests that o1-minis architecture or training approach might be particularly well-suited for self-reflection tasks. Furthermore, we observe concerning pattern across all models in self-critique performance on certain specialized tasks. Models including o1-mini consistently struggle with MMLUSTEM and GPQA tasks, showing significant performance degradation ranging from - 10% to -35.6%. This pattern is particularly pronounced in Qwen2.5-72B-Math-Instructs performance on MMLU-STEM (-35.6%) and o1-minis performance on MMLU-STEM (- 28.0%). This suggests that self-critique abilities may be more challenging to maintain in complex, specialized domains that require deep subject matter expertise."
        },
        {
            "title": "4.2.2 Cross-Critique Performance Analysis",
            "content": "Cross-critique evaluation reveals more nuanced picture of models critique capabilities. The baseline performance is approximately 50%, as half of the input solutions are correct and half are incorrect. When compared to this baseline, most models show substantial improvements (up to 40%) on mathematical reasoning tasks. This suggests that models are particularly adept at identifying and correcting errors in basic mathematical reasoning. Similar to the case of self-critique, we observe that o1-mini emerges as the strongest performer in cross-critique tasks. For specialized tasks, modelsexcept for o1-miniagain show degraded performance on average. 4.2.3 Comparative Analysis of Self-Critique and Cross-Critique In this section, we present more detailed analysis, focusing on two key metrics: IC, which measures the transformation of incorrect input solutions into correct ones after critique and correction, and CI, which captures the reversal of originally correct solutions to incorrect ones after critique and correction. These metrics will help us gain deeper insights into the effectiveness of the critique process. The results are reported in Figure 5. Figure 5: Performance of CI and IC in self-critique and cross-critique scenarios. Here, denotes correct, denotes incorrect, and the arrow indicates the accuracy change from the input solution to the correction after critique. striking observation is the consistent asymmetry between improvement capability (IC) and solution maintenance (CI) across both critique paradigms. Specifically, for selfcritique, most models show limited ability to improve incorrect solutions (IC improvements typically below 5%), while exhibiting significant degradation when critiquing initially correct solutions (CI degradations often exceeding -10%). But, o1-mini stands as notable exception, demonstrating remarkable improvement capabilities (IC improvements of 25.85% on ARC and 24.37% on College Math) while still struggling with solution maintePreprint. Work in progress. nance. For cross-critique, we observe substantially higher improvement capabilities across all models (IC improvements of 30-45% on fundamental tasks like ARC and GSM8K). However, the degradation issue persists, particularly in specialized domains like GPQA and MMLU-STEM (CI degradations of -15% to -30%). This comparative analysis reveals that while cross-critique generally enables higher potential for improvement, it also carries higher risks of solution degradation. The effectiveness of both critique paradigms appears to be strongly influenced by task complexity and domain specificity, with fundamental mathematical reasoning tasks being more amenable to reliable critique than specialized domain tasks."
        },
        {
            "title": "4.2.4 Impact of Iterative Critique",
            "content": "In this section, we present the findings from iterative critique experiments, in which models are tasked with providing critiques across multiple rounds. The results are detailed in Figure 6. Figure 6: Performance of iterative critique. The performance is averaged over 8 tasks. We observe distinct trends in how critique effectivenessmeasured by performance delta over the base solutionevolves across rounds. LLaMA-3.1-70B-Instruct, Mistral-LargeInstruct, and Qwen2.5-72B-Math-Instruct exhibit steady decline in effectiveness, while Qwen2.5-72B-Instruct demonstrates remarkable consistency in maintaining its improvement margins. GPT-4o shows an initial increase in cross-critique effectiveness before decline in self-critique, whereas o1-mini uniquely sustains strong improvement margins throughout iterations, particularly excelling in early rounds. These variations highlight the diverse dynamics of critique effectiveness across models and rounds."
        },
        {
            "title": "5 Related Work",
            "content": "Prior to our work, several efforts have been made to advance the benchmarking of LLMs critique abilities (Luo et al., 2023; Lin et al., 2024; Lan et al., 2024). We have summarized their key features in Table 1. In contrast to these benchmarks, our approach focuses specifically on reasoning tasks and introduces tightly coupled evaluation framework. This framework validates critique quality through direct correction outcomes and supports diverse critique modes, including self-critique and iterative critique. Understanding LLMs ability to self-critique and improve their reasoning has emerged as critical research direction. Huang et al. (2023) demonstrated that LLMs often struggle with self-correction without external feedback, while Stechly et al. (2024) found that external 11 Preprint. Work in progress. verification from independent reasoners can significantly improve reasoning accuracy. Wu et al. (2024b)s ProCo framework showed promise in enabling self-correction through minimal prompting and iterative verification. Tyen et al. (2023) revealed an interesting dynamic: while LLMs may struggle to detect reasoning errors independently, they can effectively correct them when error locations are explicitly identified. Our work builds upon these findings by introducing novel evaluation framework that examines both self-critique and cross-model critique capabilities."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present new benchmark designed to evaluate the critique capabilities of LLMs in reasoning tasks. We investigate three distinct scenarios: self-critique, crosscritique, and iterative critique. Our findings reveal that in nearly all cases, the o1-mini model demonstrates the most impressive performance. While other models may exhibit comparable performance to o1-mini in direct CoT generation, they significantly lag behind in critique abilities. We provide detailed analysis of these results, offering insights into the underlying factors contributing to these differences. We hope that our findings will inspire and guide future advancements in the development of LLMs."
        },
        {
            "title": "References",
            "content": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Preprint. Work in progress. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: math word problem repository. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 11521157, San Diego, California, June 2016. Association for Computational Linguistics. Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, and Xian-Ling Mao. Criticeval: Evaluating large-scale language model as critic. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809, 2024. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL, 2017. Liangchen Luo, Zi Lin, Yinxiao Liu, Lei Shu, Yun Zhu, Jingbo Shang, and Lei Meng. Critique ability of large language models. arXiv preprint arXiv:2310.04815, 2023. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. diverse corpus for evaluating and developing english math word problem solvers, 2021. Mistral-AI. Mistral-large-2407, July 2024. MARIO-Math-Reasoning/Gaokao2023-Math-En Datasets at Hugging Face. OpenAI. Learning to reason with LLMs. OpenAI Blog, Feb 2024. https://openai.com/ index/learning-to-reason-with-llms. Qwen-Team. Qwen2.5: party of foundation models, September 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. On the self-verification limitations of large language models on reasoning and planning tasks. arXiv preprint arXiv:2402.08115, 2024. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884, 2024. Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor Carbune. Llms cannot find reasoning errors, but can correct them! arXiv preprint arXiv:2311.08516, 2023. 13 Preprint. Work in progress. Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: critic for language model generation. arXiv preprint arXiv:2308.04592, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: challenging, contamination-free llm benchmark. 2024. Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, et al. comparative study on reasoning patterns of openais o1 model. arXiv preprint arXiv:2410.13639, 2024. Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, and Meng arXiv preprint Jiang. Large language models can self-correct with minimal effort. arXiv:2405.14092, 2024. Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. Evaluating and improving tool-augmented computation-intensive math reasoning. arXiv preprint arXiv:2306.02408, 2023. Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, and Le Sun. Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic. arXiv preprint arXiv:2408.16326, 2024."
        },
        {
            "title": "A Human Evaluation Protocol for Critique Quality",
            "content": "We conducted systematic human evaluation study to establish ground truth labels for critique quality. This section details our evaluation protocol. A.1 Data Preparation We utilized the evaluation framework from CriticBench (Lin et al., 2024), which provides comprehensive dataset containing mathematical problems, ground truth solutions, and student solutions. Following their setup, we used 100 problems from their MATH (Hendrycks et al., 2021) test set, which already contains mathematical problems, ground truth solutions, and student attempted solutions. We leveraged CriticBenchs evaluation code and prompt templates, replacing their default model with Qwen2.5-72B-Instruct(Qwen-Team, 2024) to generate critiques in their standard JSON format: { } \"analysis\": \"Detailed analysis of the solution...\", \"verdict\": \"correct\" or \"wrong\" A.2 Evaluation Framework Our evaluation framework relies on two key dimensions: 1. The correctness of the students solution (compared to ground truth) 2. The verdict provided in the critique (correct or wrong) Based on these dimensions, we developed comprehensive argument mapping framework to evaluate critique quality, as shown in Table 5. Preprint. Work in progress. Scenario Condition Low-Quality Argument 1 2 4 Solution: Wrong, Verdict: Wrong The critique correctly identifies the solution is wrong, but its analysis fails to identify the true error in the students reasoning, potentially leading to another incorrect answer. Solution: Correct, Verdict: Correct The critique correctly identifies the solution is correct, but its analysis claims there is an error in one of the solution steps, suggesting it fails to truly validate the correctness. Solution: Wrong, Verdict: Correct The critique claims the solution is correct, but its analysis contradicts its own verdict by pointing out an error in one of the solution steps. Solution: Correct, Verdict: Wrong The critique claims the solution is wrong, but its analysis contradicts its own verdict by confirming every step in the solution is correct. Table 5: Argument mapping framework for evaluating critique quality. Each argument describes specific failure mode that would classify critique as low-quality. STEM graduate students assessed whether these arguments were true for each critique. A.3 Evaluation Process For each critique evaluation, STEM graduate students were asked to perform systematic analysis following standardized procedure: 1. First, evaluators thoroughly reviewed the mathematical problem and its ground truth solution to establish clear understanding of the correct approach and key concepts involved. 2. They then examined the students attempted solution in detail, comparing it with the ground truth solution to identify any discrepancies or errors. 3. Next, they analyzed the generated critique, paying particular attention to both its verdict (correct or wrong) and the specific reasoning provided in the analysis. 4. Based on the solutions correctness and the critiques verdict, evaluators were presented with the corresponding argument from Table 5. 5. Finally, evaluators provided: binary judgment: True (indicating the argument applies and the critique is low-quality) or False (indicating the argument doesnt apply and the critique is high-quality) The results of this systematic evaluation process were used to create the confusion matrix presented in Table 2 in the main text, which demonstrates the limitations of verdict-based critique evaluation methods. See Figure 2, 7, 8, and 9 for more concrete examples. 15 Preprint. Work in progress. Figure 7: Examples comparing CriticBench and RealCritic. Figure 8: Examples comparing CriticBench and RealCritic. 16 Preprint. Work in progress. Figure 9: Examples comparing CriticBench and RealCritic. Post-Check Mechanism for Critique Quality Assurance You are mathematics expert. You will be provided with question, solution, and critique given by teacher model. The critique will follow this format: If the reasoning is found to be incorrect, first identify the errors in the reasoning to form feedback, and then correct the erroneous reasoning based on the provided feedback. Your task is to evaluate whether the teacher models critique exhibits any of the following non-compliant behaviors: 1. The model does not adhere to the paradigm of critiquing before providing its own solution; instead, it solves the problem first and provides critique afterward. 2. During the critique, the model is capable of identifying errors but fails to correct the flawed reasoning based on the identified mistakes; instead, it circumvents the pointed-out issues in deceptive manner, providing its own solution. If any of the above non-compliant behaviors are present, the final answer should be Unquali ied . If there are no such non-compliant behaviors, the final answer should be Quali ied . Please reason step-by-step with your analysis and provide the final answer with Unquali ied or Quali ied . You can output only these two results. Table 6: Post-check prompt for verifying the compliance of critique-correction process. This prompt is designed to detect potential shortcuts where models might bypass the proper critique process. To ensure the validity of our closed-loop evaluation framework, we implement rigorous post-check mechanism to verify that the critique-correction process strictly follows our intended paradigm. This process is crucial for preventing potential shortcuts where models might generate new solutions directly without properly critiquing the original solution. 17 Preprint. Work in progress. B.1 Verification Process We employ carefully designed prompt to evaluate whether each critique adheres to our step-by-step critique-and-correct paradigm in Table 6. B.2 Implementation and Validation We utilize Qwen2.5-32B-Instruct as our verification model to evaluate all generated critiques. To validate the reliability of this automated verification process, we conducted thorough human evaluation study. We randomly sampled 100 critique instances from our dataset for manual review by expert annotators. The results showed 98% compliance rate with our critique format requirements, demonstrating the effectiveness of our post-check mechanism in ensuring the quality and validity of the critique-correction process. This verification step is crucial for maintaining the integrity of our evaluation framework, ensuring that the measured critique effectiveness truly reflects the models ability to identify and correct errors in systematic manner. Given this high compliance rate, we did not implement post-check as mandatory step in our evaluation pipeline, as the results indicate that models generally adhere to the intended critique-correction paradigm."
        },
        {
            "title": "C Details On The Attributes Of Candidate Datasets",
            "content": "The 20 candidate datasets we selected are: AIME24, AMC23, AQuA (Ling et al., 2017), Asdiv (Miao et al., 2021), CARP (Zhang et al., 2023), College Math (Tang et al., 2024), Gaokao2023en (noa), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), Mawps (Koncel-Kedziorski et al., 2016), Minerva Math (Lewkowycz et al., 2022), Olympiadbench (He et al., 2024), BBH (Suzgun et al., 2022), LiveBench (White et al., 2024), Arc Challenge (Clark et al., 2018b), GPQA extended, GPQA diamond (Rein et al., 2023), MMLU-Stem, MMLU-pro, MMLU-redux (Hendrycks et al., 2020). The test sets for AIME24 and AMC23 are too small, so we do not consider adding them to RealCritic. The difficulty level of AQuA, Asdiv, CARP, and Mawps is similar to that of GSM8K. We opt to use the classic GSM8K to represent critic performance at this difficulty level. BBH has wide variety of tasks, which would complicate the evaluation framework, so it is excluded from RealCritic. LiveBench, due to its continuously updated nature, poses high maintenance costs for the future, leading to its exclusion. For the GPQA series, we select the most representative subset, GPQA-diamond. From the MMLU series, we choose MMLU-STEM, which is most closely related to reasoning, for inclusion in RealCritic."
        },
        {
            "title": "D Pure Critique Performance Analysis",
            "content": "Model Evaluation Mode ARC GSM8K MATH College Math Minerva Math GPQA MMLU STEM Olympiad Bench LLaMA-3.170B-Instruct Cross Critique (Cross-Base) Qwen2.5-72BMath-Instruct Cross Critique (Cross-Base) Qwen2.5-72B -Instruct Cross Critique (Cross-Base) 75.3 75.3 54.4 54.4 80.6 80.6 85.3 85. 74.0 74.0 79.5 79.5 47.0 47.0 51.9 51.9 51.1 51.1 24.7 24. 28.7 28.7 27.6 27.6 18.4 18.4 16.9 16.9 15.7 15.7 22.1 22. 18.4 18.4 16.3 16.3 40.1 40.1 37.7 37.7 27.4 27.4 18.7 18. 18.0 18.0 19.5 19.5 Avg 41.5 41.5 37.5 37.5 39.7 39. Table 7: Performance on deliberately incorrect solutions. All input solutions are incorrect, resulting in 0% baseline accuracy. Since there are no correct solutions in the input, the improvement () equals the absolute performance. All numbers are in percentages (%). To specifically assess models pure critique and correction capabilities without the influence of correct examples, we conduct additional experiments using deliberately incorrect solutions. Unlike the main experiments where solutions are balanced between correct and incorrect (leading to 50% baseline accuracy), here we use exclusively incorrect solutions, 18 Preprint. Work in progress. resulting in 0% baseline accuracy. This setup allows us to cleanly measure how well models can identify and fix errors through critique. The results show models achieve up to 80% accuracy on basic tasks like ARC and GSM8K, but performance drops significantly on specialized tasks like Minerva Math and Olympiad Bench (below 20%), suggesting limited critique abilities on complex mathematical reasoning. Priority-based Filtering Strategy in Solution Selection When we filter out wrong solutions, we should prioritize strong models over weak ones. As shown in Table 8, when weak model makes mistake, it usually provides very little information. This type of solution does not help the critic model perform well and makes evaluation difficult. On the other hand, strong model typically offers detailed and thorough information, giving the critic model more room to operate effectively. Problem. What is the probability that flipping fair coin three times produces fewer than two heads? Express your answer as common fraction. Ground Truth Solution. This is the probability of flipping no heads (three tails) or one head (two tails). This is the same as the probability of flipping two heads (one tail) or three heads (no tails), since we can just interchange heads and tails. Since all outcomes are covered, both probabilities add up to 1, so the desired probability is 1 . Qwen1.5-0.5B-Chat (Wrong Solution) The probability that flipping fair coin three times produces fewer than two heads is given by the binomial coefficient, which is equal to 1/6. Therefore, the answer is 1 6 . Qwen2.5-72B-Math-Instruct (Wrong Solution) To determine the probability that flipping fair coin three times produces fewer than two heads, we first need to consider all possible outcomes when flipping fair coin three times. Each flip has two possible outcomes: heads (H) or tails (T). Therefore, there are 23 = 8 possible outcomes in total. Next, we need to count the number of outcomes that have fewer than two heads. This means we need to count the outcomes that have either zero heads, one head, or exactly two heads. 1. Outcomes with zero heads: There is only one outcome with zero heads, which is TTT. 2. Outcomes with one head: There are three outcomes with one head, which are HTT, THT, and TTH. 3. Outcomes with exactly two heads: There are three outcomes with exactly two heads, which are HHT, HTH, and THH. Now, we add up the number of outcomes for each case: Outcomes with zero heads: 1 Outcomes with one head: 3 Outcomes with exactly two heads: The total number of outcomes with fewer than two heads is 1 + 3 + 3 = 7. Since there are 8 possible outcomes in total, the probability that flipping fair coin three times produces fewer than two heads is: Thus, the final answer is: 7 8 7 8 Table 8: Wrong Solution: Strong Model vs. Weak Model. As shown in Table 9, capable model generates solutions that, if correct, are not too vague in the process, significantly reducing the critics burden. In contrast, less capable model might produce solutions where, even if the final answer is right, the process could have many flaws. We select these solutions and ask the model to identify and fix these flaws, ultimately arriving at the correct answer. 19 Preprint. Work in progress. Problem. When 1 210 is written as decimal, how many digits are to the right of the decimal point? Ground Truth Solution. Multiply numerator and denominator of 1 210 by 510 to see that 1010 . This implies that the 1 210 is obtained by moving the decimal point ten places to the left in the decimal representation of 510. 210 is equal to 510 1 decimal representation of Therefore, there are 10 digits to the right of the decimal point in 1 210 . Qwen1.5-0.5B-Chat (Answer Correct Solution) To find the number of digits to the right of the decimal point when 1 210. The base of the logarithm is 210, which is equal to 21010101010 = 21010101010 = 210101010 21 = 210101010 22 = 21010101010 4 = 1024. Now, we can find the number of digits to the right of the decimal point by multiplying the base by the number of digits in the base: 1024 10 = 10240. So, when 210 is written as decimal, we need to determine the value of 210 is written as decimal, there are 10 digits to the right of the decimal point. Qwen2.5-72B-Math-Instruct (Answer Correct Solution) To determine how many digits are to the right of the decimal point when 1 210 is written as decimal, we first need to calculate 210. So, 210 = 1 1024 . When we write 1 1024 as decimal, it is: 210 = 1024 1 1024 = 0. Now, we need to count the number of digits to the right of the decimal point. The decimal representation 0.0009765625 has 10 digits to the right of the decimal point. Therefore, the number of digits to the right of the decimal point is 10 . Table 9: Correct Solution: Strong Model vs. Weak Model 20 Preprint. Work in progress."
        },
        {
            "title": "F Concrete Prompts",
            "content": "Here we present concrete prompts of RealCritic in Table 10 below. Prompt Type Content Direct CoT Please reason step by step, and put your final answer within . Critique will provide math problem along with student solution. Conduct step-by-step critique of the students solution. For each step, use proper verification, recalculation, and reflection to determine whether it is logically and mathematically valid. Please elaborate on the analysis process carefully. If an error is detected in any step, describe the nature and cause of the error in detail, and suggest how to correct it or provide the correct approach. Once step is found to contain an error, stop further analysis of subsequent steps (as they may depend on the identified error) and directly conclude with Incorrect. Format your response as follows: ## Critique of Student Solution Steps ### Critique of Step 1 [critique of step 1] ... ### Critique of Step [critique of step n] ## Conclusion of Critique [For correct solution] The solution is Correct. The first error step number: -1 [For incorrect solution] The solution is Incorrect. The first error step number: [first step containing error] ## Correction of Student Solution [For correct solution] The student solution is correct and well-reasoned. The final answer is [answer] . [For incorrect solution] Heres the corrected solution, following the original approach up to step [error step - 1] and then proceeding with the proper correction: ===CORRECTION START=== [Steps 1 to (error step - 1) from original student solution] [Corrected version of error step] [Subsequent steps following from the correction to reach the final answer] Therefore, the final answer is [correctedanswer] . ===CORRECTION END=== Iterative Are you sure your critic is correct? Please reconsider all the content above. You need to follow the format of the previous critique, but you are not allowed to copy it !! You must conduct thorough and sincere reanalysis on your own! Table 10: Concrete prompts used in RealCritic for different evaluation scenarios. These prompts are designed to ensure systematic analysis and concrete corrections in our closedloop evaluation framework."
        },
        {
            "title": "G Concrete Examples",
            "content": "Here we present concrete example of RealCritic in Tables 11 to 13 below. 21 Preprint. Work in progress. Step 1: Direct CoT System Prompt: Please reason step by step, and put your final answer within User: Let = 1. Dennis and Edward each take 48 minutes to mow lawn, and Shawn takes 24 minutes to mow lawn. Working together, how many lawns can Dennis, Edward, and Shawn mow in 2 hours? (For the purposes of this problem, you may assume that after they complete mowing lawn, they immediately start mowing the next lawn.) Assistant: To determine how many lawns Dennis, Edward, and Shawn can mow together in 2 hours (where = 1), follow these steps: ... 3. **Calculate Lawns Mowed Individually in 120 Minutes:** - **Dennis:** . 120 minutes 48 minutes/lawn = 2.5 lawns Since partial lawns arent counted until completed, Dennis mows **2 lawns**. - **Edward:** 120 minutes 48 minutes/lawn = 2.5 lawns Similarly, Edward mows **2 lawns**. - **Shawn:** Shawn mows **5 lawns**. 4. **Total Lawns Mowed Together:** **Answer:** 9 120 minutes 24 minutes/lawn = 5 lawns 2 (Dennis) + 2 (Edward) + 5 (Shawn) = 9 lawns Step 2: Self-Critique System Prompt: will provide math problem along with student solution. Conduct step-by-step critique of the students solution. For each step, use proper verification, recalculation, and reflection to determine whether it is logically and mathematically valid. Please elaborate on the analysis process carefully. If an error is detected in any step, describe the nature and cause of the error in detail, and suggest how to correct it or provide the correct approach. Once step is found to contain an error, stop further analysis of subsequent steps (as they may depend on the identified error) and directly conclude with Incorrect. Finally, provide conclusion of your critique and correction for the student solution. ... User: The following is the data for your task. ## Math Problem [copy problem from above] ## Student Solution [copy solution from above] 48 = 2.5 lawns. 48 = 2.5 lawns. 24 = 5 lawns. Assistant: ## Critique of Student Solution Steps ... ### Critique of Step 3 The student correctly calculates the number of lawns each person can mow individually in 120 minutes: - Dennis: 120 - Edward: 120 - Shawn: 120 However, the student makes an incorrect assumption by rounding down the partial lawns for Dennis and Edward, resulting in 2 lawns each. This approach overlooks the fact that all three individuals are mowing simultaneously, and partial progress contributes to the total number of lawns mowed collectively. By rounding down, the student fails to account for the combined efficiency of working together, leading to an underestimation of the total lawns mowed. ## Conclusion of Critique The solution is Incorrect. The first error step number: 3 ## Correction of Student Solution Heres the corrected solution, following the original approach up to step 2 and then proceeding with the proper correction: ... 3. **Calculate Combined Mowing Rate:** - **Denniss Rate:** 1 48 lawns per minute. - **Edwards Rate:** 1 48 lawns per minute. - **Shawns Rate:** 1 24 lawns per minute. **Total Combined Rate:** 1 48 + + = 1 1 24 1 + 1 + 2 48 = = 4 48 1 lawns per minute 4. **Calculate Total Lawns Mowed in 120 Minutes:** 120 minutes 1 12 lawns per minute = 10 lawns **Answer:** Table 11: concrete example of Self-Critique. 22 Preprint. Work in progress. System Prompt: will provide math problem along with student solution. Conduct step-by-step critique of the students solution. For each step, use proper verification, recalculation, and reflection to determine whether it is logically and mathematically valid. Please elaborate on the analysis process carefully. If an error is detected in any step, describe the nature and cause of the error in detail, and suggest how to correct it or provide the correct approach. Once step is found to contain an error, stop further analysis of subsequent steps (as they may depend on the identified error) and directly conclude with Incorrect. Finally, provide conclusion of your critique and correction for the student solution. ... User: The following is the data for your task. ## Math Problem Let = 37. cube has edges of length T. Square holes of side length 1 are drilled from the center of each face of the cube through the cubes center and across to the opposite face; the edges of each hole are parallel to the edges of the cube. Compute the surface area of the resulting solid. ## Student Solution To find the surface area of the resulting solid after drilling square holes through the center of each face of cube, we need to consider both the original surface area of the cube and the additional surface area created by the holes. 1. **Calculate the original surface area of the cube:** The cube has 6 faces, and each face is square with side length = 37. The area of one face is 372 = 1369. Therefore, the original surface area of the cube is: 6 1369 = 8214 2. **Calculate the surface area added by the holes:** Each hole is square with side length 1, and it passes through the entire cube, so it has length of 37. Each hole has 4 sides, and each side is rectangle with dimensions 1 37. The area of one side of hole is 37, so the total surface area added by one hole is: Since there are 6 holes (one through each face of the cube), the total additional surface area is: 6 148 = 888 4 37 = 148 3. **Calculate the surface area of the resulting solid:** The surface area of the resulting solid is the sum of the original surface area of the cube and the additional surface area created by the holes: Thus, the surface area of the resulting solid is: 8214 + 888 = 9102 Assistant: ## Critique of Student Solution Steps ### Critique of Step 1 The student correctly calculated the original surface area of the cube. Each face of the cube has an area of 372 = 1369, and with 6 faces, the total surface area is 6 1369 = 8214. ### Critique of Step 2 The student incorrectly calculated the additional surface area added by the holes. While each square hole has 4 sides of area 1 37 = 37 each, resulting in 4 37 = 148 per hole, there are only 3 distinct holes (one along each axis) rather than 6, as drilling through one face automatically accounts for the opposite face. Additionally, the student failed to account for the overlapping areas where the holes intersect at the center of the cube, which leads to double-counting some surface areas. Moreover, the student did not subtract the area removed from the original cubes faces due to the holes. ## Conclusion of Critique The solution is Incorrect. The first error step number: 2 ## Correction of Student Solution Heres the corrected solution, following the original approach up to step 1 and then proceeding with the proper correction: 1. **Calculate the original surface area of the cube:** The cube has 6 faces, and each face is square with side length = 37. The area of one face is 372 = 1369. Therefore, the original surface area of the cube is: 2. **Calculate the surface area removed by the holes:** Each square hole removes 1 1 square from each face it penetrates. Since there are 3 pairs of opposite faces, there are 3 distinct square holes. Each hole removes 2 squares (one from each of the two opposite faces), so the total area removed from the cubes original surface is: 6 1369 = 8214 Therefore, the adjusted original surface area is: 3 2 1 = 8214 6 = 8208 ... 5. **Calculate the total surface area of the resulting solid:** Add the adjusted original surface area and the net additional surface area from the holes: Therefore, the surface area of the resulting solid is: 8208 + 432 = 8640 8640 Table 12: concrete example of Cross-Critique. 23 Preprint. Work in progress. System Prompt: will provide math problem along with student solution. Conduct step-by-step critique of the students solution. For each step, use proper verification, recalculation, and reflection to determine whether it is logically and mathematically valid. Please elaborate on the analysis process carefully. If an error is detected in any step, describe the nature and cause of the error in detail, and suggest how to correct it or provide the correct approach. Once step is found to contain an error, stop further analysis of subsequent steps (as they may depend on the identified error) and directly conclude with Incorrect. Finally, provide conclusion of your critique and correction for the student solution. ... User: The following is the data for your task. ## Math Problem The equation (x 1)(x 2) (x 2016) = (x 1)(x 2) (x 2016) is written on the board. One tries to erase some linear factors from both sides so that each side still has at least one factor, and the resulting equation has no real roots. Find the least number of linear factors one needs to erase to achieve this. ## Student Solution To solve this problem, we need to manipulate the given equation by erasing some linear factors from both sides in such way that the resulting equation has no real roots. The original equation is: (x 1)(x 2) (x 2016) = (x 1)(x 2) (x 2016) ... The least number of linear factors one needs to erase to ensure the resulting equation has no real roots is **1008**. **Answer:** 1008 Assistant: ## Critique of Student Solution Steps ... ### Critique of Step 2 ... Due to these issues, Step 2 contains critical errors in both reasoning and application of mathematical principles. ## Conclusion of Critique The solution is Incorrect. The first error step number: 2 ## Correction of Student Solution Heres the corrected solution, following the original approach up to step 1 and then proceeding with the proper correction: To solve this problem, we need to manipulate the given equation by erasing some linear factors from both sides in such way that the resulting equation has no real roots. The original equation is: (x 1)(x 2) (x 2016) = (x 1)(x 2) (x 2016) ... The least number of linear factors one needs to erase to ensure the resulting equation has no real roots is **2**. **Answer:** 2 User: Are you sure your critic is correct? Please reconsider all the content above. You need to follow the format of the previous critique, but you are not allowed to copy it !! You must conduct thorough and sincere reanalysis on your own! Assistant: ## Critique of Student Solution Steps ... ### Critique of Step 2 ... Due to these critical flaws, Step 2 does not reliably advance the solution and introduces incorrect assumptions about the nature of and Q. ## Conclusion of Critique The solution is Incorrect. The first error step number: 2 ## Correction of Student Solution Heres the corrected solution, following the original approach up to step 1 and then proceeding with the proper correction: To solve this problem, we need to manipulate the given equation by erasing some linear factors from both sides in such way that the resulting equation has no real roots. The original equation is: (x 1)(x 2) (x 2016) = (x 1)(x 2) (x 2016) ... The least number of linear factors one needs to erase to ensure the resulting equation has no real roots is **2016**. **Answer:** Table 13: concrete example of Iterative-Critique."
        }
    ],
    "affiliations": [
        "Qwen Team, Alibaba Inc.",
        "Shenzhen Research Institute of Big Data",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}