{
    "paper_title": "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving",
    "authors": [
        "William Ljungbergh",
        "Adam Lilja",
        "Adam Tonderski. Arvid Laveno Ling",
        "Carl Lindstr√∂m",
        "Willem Verbeke",
        "Junsheng Fu",
        "Christoffer Petersson",
        "Lars Hammarstrand",
        "Michael Felsberg"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-supervised pre-training based on next-token prediction has enabled large language models to capture the underlying structure of text, and has led to unprecedented performance on a large array of tasks when applied at scale. Similarly, autonomous driving generates vast amounts of spatiotemporal data, alluding to the possibility of harnessing scale to learn the underlying geometric and semantic structure of the environment and its evolution over time. In this direction, we propose a geometric and semantic self-supervised pre-training method, GASP, that learns a unified representation by predicting, at any queried future point in spacetime, (1) general occupancy, capturing the evolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle path through the environment; and (3) distilled high-level features from a vision foundation model. By modeling geometric and semantic 4D occupancy fields instead of raw sensor measurements, the model learns a structured, generalizable representation of the environment and its evolution through time. We validate GASP on multiple autonomous driving benchmarks, demonstrating significant improvements in semantic occupancy forecasting, online mapping, and ego trajectory prediction. Our results demonstrate that continuous 4D geometric and semantic occupancy prediction provides a scalable and effective pre-training paradigm for autonomous driving. For code and additional visualizations, see \\href{https://research.zenseact.com/publications/gasp/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 2 7 6 5 1 . 3 0 5 2 : r GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving William Ljungbergh,1,2 Adam Lilja,1,3 Adam Tonderski1,4 Arvid Laveno Ling1,3 Carl Lindstrom1,3 Willem Verbeke1 Lars Hammarstrand3 Michael Felsberg2 Junsheng Fu1 Christoffer Petersson1,3 1Zenseact 2Linkoping University 3Chalmers University of Technology 4Lund University {firstname.lastname}@{zenseact.com, liu.se, chalmers.se}"
        },
        {
            "title": "Abstract",
            "content": "Self-supervised pre-training based on next-token prediction has enabled large language models to capture the underlying structure of text, and has led to unprecedented performance on large array of tasks when applied at scale. Similarly, autonomous driving generates vast amounts of spatiotemporal data, alluding to the possibility of harnessing scale to learn the underlying geometric and semantic structure of the environment and its evolution over time. In this direction, we propose geometric and semantic selfsupervised pre-training method, GASP, that learns unified representation by predicting, at any queried future point in spacetime, (1) general occupancy, capturing the evolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle path through the environment; and (3) distilled high-level features from vision foundation model. By modeling geometric and semantic 4D occupancy fields instead of raw sensor measurements, the model learns structured, generalizable representation of the environment and its evolution through time. We validate GASP on multiple autonomous driving benchmarks, demonstrating significant improvements in semantic occupancy forecasting, online mapping, and ego trajectory prediction. Our results demonstrate that continuous 4D geometric and semantic occupancy prediction provides scalable and effective pretraining paradigm for autonomous driving. For code and additional visualizations, see our project page. 1. Introduction Autonomous driving (AD) has the potential to improve safety, accessibility, and enhance transportation efficiency. For an autonomous vehicle (AV) to operate safely and effectively, it must have comprehensive understanding of its environment and the evolution thereof. In doing so, the AV *Denotes equal contribution. Figure 1. GASP learns structured, generalizable representation of the environment and its evolution and can be further trained to perform well on downstream AD tasks. We outperform SotA pretraining UnO [2] across the board, especially on primarily semantic tasks like map segmentation. No pre-training is displayed for reference. Downstream tasks requiring additional labels are posttrained using 1000 samples (1% of pre-training scale). must learn to reason about geometry and semantics in dynamic environment. To develop comprehensive understanding of the environment, most existing systems rely heavily on large datasets with human-labeled annotations. Annotations are essential for solving tasks such as object detection and forecasting [26, 61], online mapping [33], and to enable multitask frameworks with ego trajectory planning [13, 24, 25]. Unlabeled data is typically abundant when developing AD systems, but annotating sufficiently diverse dataset is prohibitively expensive, limiting scalability of annotation reliant methods. In other domains, e.g., natural language processing, selfsupervised predictive learning over large datasets has been highly successful [9, 15, 43]. Researchers have explored predictive learning for AD, e.g. by predicting future point clouds [27, 60], occupancy [24, 64], or video [58]. These methods have shown promise but may struggle to model the continuous and dynamic nature of the driving environment, as they focus on predicting sensor observations rather than the underlying structure of the world. Recent works [1, 2] address this by learning representation in continuous spacetime. By predicting future occupancy from past lidar data, these methods offer more accurate model of the inherently continuous real world. However, while future occupancy prediction provides strong geometric and temporal cues, it lacks the semantic richness needed comprehensive scene understanding and complex reasoning in downstream tasks. To overcome this limitation, we propose GASP, selfsupervised pre-training method that integrates multiple sources of readily available signals in AV development: Future lidar scans, camera images, and ego poses. By leveraging supervision from diverse sensor modalities, our method results in richer representation of the environment and improves geometric, temporal, and semantic understanding. Specifically, GASP learns to predict occupancy, ego-path, and features from vision foundation model (VFM) in continuous 4D (3D + time) representation. The learned representation is useful on an array of downstream AD tasks, outperforming prior works as illustrated in Fig. 1. Additionally, we introduce and demonstrate the efficacy of practical improvements: 1) harvesting, negative information [31], from missing lidar rays for additional supervision, and 2) rotation augmentation strategy that significantly improves model generalization. Our main contributions are: Propose self-supervised pre-training method, GASP, designed to learn structured, generalizable 4D representation in continuous time by integrating geometric, temporal, and semantic supervision from multiple readily available signals. Demonstrate that GASPpre-training leads to improved generalization across multiple downstream autonomous driving tasks, significantly outperforming uni-modal pretraining on tasks such as semantic occupancy forecasting, online mapping, and ego-trajectory prediction. Provide open-source code, including custom CUDA kernels for accelerated query generation and reimplementation of previously closed-source baselines, to facilitate further research in self-supervised learning for AD. 2. Related work Self-supervised learning has gained traction due to its ability to capture meaningful patterns without requiring expensive labels [12, 39, 45], enabling greater scalability. We apply these ideas to AD and provide an overview of the most relevant developments. Generative methods: Generative methods withhold or alter parts of an input data sample and aim to reconstruct this part from the remaining data. Such methods learn features that generalize across multitude of tasks. Masked input models have been applied to text [8, 15, 44], images [5, 6, 16, 19, 42], videos [50] and point clouds [20, 37, 41, 62]. These methods have been tailored to AD by jointly encoding multiple sensors and recovering masked inputs by neural rendering techniques [56, 63]. Predicting future raw sensory data, such as point cloud forecasting [27, 60] and video frame forecasting [58], as pre-training step can also be seen in the light of masked input modeling. While such models learn relevant patterns in the data, they are also forced to learn details that are irrelevant for AD tasks: sensor intrinsics such as the scan pattern of lidar, and lowlevel stochastic information such as the lighting of each reconstructed pixel. Implicit generative methods: Alternatively, sensory data forecasting can be rephrased as generic occupancy forecasting [2]. This has two advantages compared to direct generative methods: Future occupancy depends on the dynamics of the environment but not on that of the sensors, and occupancy is directly useful for downstream tasks in AD. By encoding past sensory information (e.g., lidar [1, 2, 27, 64] or images [38, 60]) into latent representation they reason about the future at discrete [22, 24, 27, 64] or continuous [1, 2] times. We follow this trend, taking inspiration from [2], to implicitly predict 4D continuous occupancy field that can be queried at 4D coordinates = (x, y, z, t) to yield local occupancy probability. Our method predicts continuous occupancy field, but extends this by implicitly predicting both the future path of the ego vehicle and the flow of rich latent representation in unified way. Embedded predictions: By operating directly in the domain of abstract representations, unimportant and noisy low-level details can be ignored. Methods in this category often rely on contrastive learning [14] or feature alignment between augmented views of the same input, as done in DINO [12, 39]. An alternative is to use latent information to reconstruct missing parts of the input, which has shown promising results for images [4] and videos [7]. Building on these ideas, we encourage our model to implicitly predict high-level abstract features in the future, forcing it to reason about semantics and dynamics. Rather than training new image encoder, we distill features generated by DINOv2 [39], model pre-trained on large-scale dataset to produce generalizable image representations. Trajectory planning: Predicting desirable future trajectory is the ultimate goal of an AV. Contemporary methods typically follow an end-to-end design, where intermediate outputs contribute to predicting final drivable trajectory [13, 24, 25, 32, 49, 52]. This structured approach improves ego trajectory forecasting and increases perforFigure 2. Overview of GASP. Past lidar scans are encoded into BEV feature map. These features are used by implicit decoders to predict DINOv2 features ÀÜD, occupancy ÀÜO, and ego-path ÀÜE at the query points generated from future sensor data during pre-training. We also show that the learned representation is useful when transferred to an array of downstream AD tasks. mance on intermediate tasks, but also relies on expensive labeled data [24]. Trajectory prediction itself is rich self-supervised signal that requires no human annotations. Therefore, we incorporate ego-path prediction as pretraining task to integrate end-to-end path prediction with future occupancy and semantic feature information, providing richer understanding of driving scenes. Lifting vision foundation models to 3D: Several works have explored lifting image features to 3D. Lifting CLIP features into 3D [21, 51] can enhance semantic understanding, while [40] combine CLIP and SAM [30] for textpromptable point cloud segmentation. These approaches rely on full feature dimensionality, while [57] demonstrate that subset of DINOv2 features is sufficient to improve semantic understanding and enable few-shot auto-labeling in scene reconstruction. With this insight, we distill positional embedding-denoised DINOv2 features [59]. key distinction is that we predict these features future evolution, capturing the representations temporal dynamics. 3. Method We propose GASP, self-supervised method that trains model to reason about the evolution of geometry and semantics in temporal data. The model is trained to predict future occupancy (geometry and time), vision foundation model (VFM) features (semantics and time), and ego-path (geometry and semantics) at any queried point in continuous spacetime. We outline the model architecture in Sec. 3.1 and Fig. 2, explain the pre-training procedure in Sec. 3.2, and how to enhance the models usability by leveraging labeled data with post-training in Sec. 3.3. 3.1. Model architecture We adopt the model architecture in [1, 2]. The model uses lidar encoder to parametrize feature field conditioned on past sensor data that can be queried for occupancy through lightweight implicit decoder. In addition to that, we add additional decoders to predict VFM features, and ego-vehicle occupancy at any 4D point, see Fig. 2. We follow [2] and use temporal lidar data as input in this work, but note that the decoding architecture is sensor-agnostic. The lidar encoder processes Kpast past lidar scans into birds-eye-view (BEV) feature map RHW C. Scans are aggregated with ego-motion compensation and voxelized [55] before being encoded by ResNet-style [18] backbone with deformable attention [65] and Feature Pyramid Network [35]. The decoders query the BEV feature map to predict target values through lightweight architecture based on deformable attention [65], residual blocks, and final linear layer. This design enables efficient parallel query decoding, while doing the heavy lifting in the encoder. We use the same architecture for all decoders heads to, for each query point qi predict occupancy ÀÜoi = Ho(qi), VFM feature ÀÜvi = Hv(qi), and ego path query ÀÜei = He(qi). 3.2. Pre-training procedure For our self-supervised pre-training, we generalize the approach of [2] and produce set of 4D (3D + time) data samples = {qi, ai}N i=0 comprising of queries qi and targets ai from future data at [0, Tmax]. We assume temporal sequences of lidar data with known egovehicle motion throughout the sequence, standard in AD datasets [3, 10, 11, 46, 53, 54]. We denote the set of lidar points with their corresponding sensor origin = {pi, si}M i=1, where each lidar point pi = (xi, yi, zi) and si = (xi, yi, zi) has corresponding time ti at which the ray was emitted. We extend the geometric occupancy supervision, using data samples DO, with vision foundation model feature supervision from DF and future ego path traversal probabilities using DE. We elaborate on the training procedure below. Occupancy data generation: We follow the methodology of [2] to create training samples for future occupancy prediction. Unoccupied query points are sampled along the lidar ray up to the lidar return: = {si + r(pi si), 0 (0, 1)}N i=0 (1) Positive, occupied, queries are generated within buffer zone with length Œ¥ behind the lidar return D+ = {pi + r(pi si) pi si , 1 (0, Œ¥)}N i=0 (2) and from D+ and respectively to form the data samples DO to supervise In practice, we randomly select + future occupancy. Vision foundation model data generation: To generate training samples DF for learning temporal semantic features, we project future lidar points to the images closest in time, while compensating for ego-motion, and fetch the corresponding feature. Since the lidar is typically mounted higher than the camera, its rays can pass over objects such as vehicles and hit the ground or other surfaces behind them. Naively projecting onto the image, these may be assigned incorrect semantic features, leading to noisy supervision. We therefore apply per-pixel min-depth filtering, ensuring that only the closest visible points, Pvis P, contribute to training. At the projected locations, we extract the feature Fi from the output of frozen vision foundation model as the semantic training target: r(pi si) pi si DF = {pi + , Fi (0, Œ¥), pi, si Pvis} (3) In this work, we chose to use the denoising DINOv2 model [59] to mitigate known issues in lifting DINOv2 features with positional encodings [57]. However, we note that features from any vision foundation model could be used. The proposed procedure lifts information present in DINOv2 features from 2D to 3D, allowing for joint spatial and semantic reasoning. Ego path data generation: We generate ego path training samples from the future poses of the ego vehicle = {ei = (xi, yi, zi)}M i=1, from which we define the set of positive = {q ei wego}M queries Q+ i=1, as points closer than, wego, to the vehicle. This gives us the positive data samples D+ = {q, 1 Q+ E} Negative samples are instead located in the rest of the space within the region of interest RI : (4) = {q, 0 RI Q+ E} (5) We emphasize the distinction between ego-path and egotrajectory. The former has no notion of time, only positions. Focusing solely on the driven path avoids ambiguity that occurs when the ego-vehicle is stationary. The full positive sampling volume could technically be valid path for the ego-vehicle to traverse. However, directly predicting it forces the model to learn an explicit multi-modal distribution of possible ego-paths. Our formulation allows the task to be solved within our unified framework, alongside the prediction of evolving occupancy and semantic features. Training loss: We train our model using multi-task loss that consists of binary cross-entropy terms for occupancy Locc and ego-path probabilities Lego, and L1-loss for DINOv2 features Ldino. The total loss is defined as: = ŒªoccLocc + ŒªdinoLdino + ŒªegoLego, (6) where Œªocc, Œªdino, and Œªego are hyperparameters. Rotation augmentation: Real-world driving is inherently dominated by straight-road driving, where the motion of most road participants is axis-aligned with the ego coordinate system. This has been shown to induce strong bias in e.g. online mapping [36]. We observed similar tendencies in the initial training of GASP and address this by randomly rotating the coordinate system by Œ∏ [Œ∏min, Œ∏max] during training. This reduces the directional bias and promotes more diverse representation of motion. Missing lidar ray inference: lidar is an active sensor that measures distances by emitting laser rays. Unobstructed rays do not return measurements (a.k.a. missing). Disregarded in most applications and datasets [3, 10, 11, 46, 53, 54], missing rays carry valuable information about unoccupied space. Following [48], where the utility of missing rays for learning scene geometry was demonstrated, we infer missing rays from lidar scans and leverage them to sample negative occupancy queries. Recovering individual missing rays is prone to false positives. To increase robustness, we adapt the algorithm to focus on identifying extended regions of missing rays. 3.3. Post-training procedure GASP aims to equip the model with strong understanding of geometry, semantics, and dynamics. To assess the quality of the learned representations, we adapt the model or introduce additional task-specific heads during post-training (see Fig. 2). The learned representation can be used in multiple ways: querying it similarly to the pre-training phase, or using directly (or resampled) as input to another network. This flexibility enables the straightforward addition of task-specific heads for variety of downstream applications. 4. Experiments In this section, we evaluate the proposed self-supervised objective and assess whether the model learns generalizable representation of the environment and its evolution. First, we evaluate the performance of the pre-trained model on Geometric 4D Occupancy Forecasting (Sec. 4.2). The pre-trained models generalization capabilities are evaluated on downstream AD tasks: Semantic BEV Forecasting (Sec. 4.3), Map Segmentation (Sec. 4.4), and Ego Trajectory Forecasting (Sec. 4.5). We study two settings: 1) Figure 3. Predicted occupancy (colored by depth and height respectively) and DINOv2 features (mapped to RGB using the three most important features) projected into camera views, as well as holistic view from slightly above and behind the ego vehicle. Different type of objects such as road, vehicles, buildings, and trees have different features, indicating the model has semantic understanding of the objects in the scene. The injected white box represents the ego vehicle for clarity. Feature evaluation ( ); We freeze the learned encoder and train only the head. This allows us to measure how relevant the information encoded in the BEV features is. 2) Full network adaptation ( ); We train both the encoder and heads. This helps us assess how well the pre-trained model serves as starting point for downstream tasks. Last, we ablate the importance of different components of our pre-training strategy in Sec. 4.6 and verify that downstream performance scales with the amount of unlabeled data in Sec. 4.7. improvements, such as better schedule, rotation augmentation, and missing ray supervision, boost performance beyond the originally reported numbers. For fair comparison, we use our improved UnO as the baseline in all experiments.See Appendix for more details. We evaluate performance using different amounts of labeled samples [1, 105]. For low amounts of labeled data (n 102), we observe significant variance in performance depending on the samples used during training. Therefore, we train with 10 different random seeds and report the mean and standard deviation of the evaluation results. At larger sample sizes (n 103), the variance is negligible. = and Unless specified otherwise, the point cloud input range is x, 70m and [2, 6] with pillar size of 0.16 0.16 m2. We use Kpast = 3 lidar scans at an interval of 0.5 s. We train for 100, 000 steps with the Adam optimizer [28], cosine annealing learning rate schedule with maximum learning rate of 4 104 warming up for 2000 steps, and an effective batch size of 8. We follow [2] and use buffer size of Œ¥ = 0.1 for positive occupancy and DINOv2 queries. For each training sample, + = 0.9M queries are used from D+ respectively. DINOv2 features are reduced to their = 16 principal components, determined on randomly sampled subset of the training data. The features are cached for each image prior to training. Each sample uses NF = 100k queries from DF . For ego path we use buffers wego = 1m and sample + . Rotation augmentations are sampled from Œ∏ U(20, 20). The loss weights are set to Œªocc = 1.0, Œªdino = 0.5, and Œªego = 0.1. We train and evaluate our model using the Argoverse 2 [53] dataset. For online mapping, results are based on preand post-training on the geographically disjoint splits proposed in [34] while other tasks use the original training and validation splits. = 10k queries from D+ and = Figure 4. Predicted future VLM features from Birds Eye View. The model correctly predicts the car taking right turn as well as those going straight through the crossing. 4.1. Experimental setup and implementation details We reimplement UnO [2] as the baseline for our experiments. To verify the correctness of our implementation, we train and evaluate the model using the training schedule and evaluation protocol reported in the paper, and achieve performance on par with the published results. Our applicable (a) Frozen ( ) encoder. (b) Unfrozen ( ) encoder. (a) Frozen ( ) encoder. (b) Unfrozen ( ) encoder. Figure 5. Semantic BEV forecasting AP (mean and std. dev) over the number of labeled training samples. Figure 6. Map segmentation mIoU (mean and std. dev) across number of labeled samples with the sensor encoder frozen (a) and unfrozen (b). 4.2. Geometric 4D occupancy forecasting To evaluate the pre-trained models geometric understanding, we follow [2] and assess its 4D occupancy forecasting performance. The task is to predict the occupancy of 3D coordinates at future time steps, without any finetuning. For fair comparison and eliminating the need for manual threshold tuning, we measure recall at fixed precision of 70%. Predictions are obtained by querying the model over spatial region of 80 80 m2 around the ego vehicle, with uniform sampling interval of 0.2 in all spatial directions. Temporally, we evaluate at {0.6, 1.2, ..., 3.0} into the future. Following [2], we compute precision using lidar-based ray tracing [23] classifying voxels of size 0.2 m3 as free if traversed by lidar beam before the measured point. Annotated bounding boxes are used to identify points corresponding to objects, labeling them as occupied. Results: Comparing GASP with UnO in Tab. 2 shows that the 4D-occupancy recall at precision 70 (R@P70) increases from 79.4% to 81.9%. The performance increase primarily stems from the addition of DINOv2 supervision. Ego path supervision seems to slightly decrease the geometric performance. Intuitively, predicting the future ego path does not require full understanding of scene geometry. Qualitatively, Fig. 3 exemplifies the geometric and semantic capabilities of the learned representation at the current timestep, whereas Fig. 4 highlights predictions in to the future. 4.3. Semantic BEV forecasting In Semantic BEV forecasting [2] the model is tasked with forecasting semantic labels and occupancy of 2D coordinates aligned with the ground plane. We adapt the pretrained occupancy decoder (see Sec. 3.1) to instead predict the occupancy for each class separately. Following standard protocol [1, 2], we evaluate occupancy for the vehicle class at discrete future times = {0.0 s, 0.5 s, ..., 3.0 s} in uniform grid 80 80 m2 centered around the ego-vehicle with spatial resolution of 0.4 m. We measure performance by Average Precision (AP) and Soft-IoU computed across all queries in space and time. Figure 7. Ego path in crossing, colored by distance to egovehicle. Lidar point cloud (grey) and true ego path (dashed line) are displayed for reference. At time t0 GASP predicts multiple possible modes (A, B), and once it is no longer probable to continue towards (at t+), the predictions collapse to only one mode. Results: We compare our model to the state-of-the-art [1, 2, 13]. In Fig. 5, we show the performance of GASP and the UnO baseline for different amounts of labeled samples. GASP consistently outperforms the UnO baseline across all amounts of training samples, demonstrating that the learned representation is more informative for forecasting. This holds especially true for low amounts of labeled data where GASP requires one order of magnitude less data than UnO to reach the same performance. The gap decreases notably with the amount of labeled samples when the encoder is unfrozen, which is expected given that both models share the same architecture. Performance measured in terms of SoftIoU follows the same trend, see Appendix B.1. 4.4. Map segmentation To assess the semantic content learned from the proposed pre-training scheme, we evaluate its performance on map segmentation. The task consists of classifying cells in rasterized grid as lane dividers, road boundaries, or pedestrian crossings, which we predict using lightweight UNet-inspired decoder [17] on top of Z. We consider an 80 80 m2 region around the ego vehicle with cell size of 30 cm. We report the mean intersection over union (mIoU) as the evaluation metric. Enc. Pre-training min min1 ADE FDE ADE FDE UnO GASP - UnO GASP 0.834 0.617 0.880 0.902 0. 1.43 1.06 1.64 1.51 1.27 1.84 1.39 1.70 1.62 1.42 3.70 2.87 3.58 3.15 2. Table 1. Ego-trajectory prediction the full Argoverse 2 sensor dataset, using frozen ( ) and unfrozen ( ) encoders. Results: While lidar-only map segmentation is underexplored, we note that GASP outperforms SotA camera-only setups [34]. As shown in Fig. 6a, when freezing the encoder and training only the map segmentation head, GASP consistently outperforms the baseline across all training set sizes. Our method reaches saturation at 104 training samples, indicating that it learned highly generalizable features. The trend persists when unfreezing the encoder, shown in Fig. 6b. The results suggest that our pre-trained model captures essential features for online mapping, even pedestrian crossings, despite never being trained to detect them. The performance gap between the frozen and unfrozen models at 105 training samples is only 5 mIoU, highlighting that much of the necessary information for map segmentation is encoded during pre-training. For exact metrics, see Appendix B.2. 4.5. Ego-trajectory prediction To evaluate the models understanding of the ego vehicles future trajectory under our proposed pre-training scheme, we start by inspecting its predicted paths. In Fig. 7 the model proposes multiple plausible modes, indicating an awareness of multi-modal future motion and drivable areas. To further assess its learned motion understanding in structured geometric representation, including velocity, we employ simple trajectory decoder as post-training step. The decoder aggregates information from the feature map, Z, via deformable attention [65] into latent template trajectories that are later decoded into 2D coordinates with an MLP. It is trained with an imitation-learning objective between the predicted and recorded trajectory. Results: In Tab. 1 we report minADE1&6, and minFDE1&6, standard metrics for motion forecasting in Argoverse. These metrics are analogous to L2-planning metrics reported in end-to-end driving works [24, 52], whilst still allowing for multiple trajectory proposals. The results show that GASP captures future ego motion better than UnO in both settings, and significantly outperforms training from scratch, despite using the full amount of trajectory labels. Components 4D-occ Sem. Forecasting Enc. Occ. E.p. Sem. n/a 79.4 78.9 81.9 81.6 n/a n/a n/a n/a n/a Labeled samples 102 105 Map Seg. Labeled samples 102 105 103 50.3 60.2 60.8 59.3 19.3 50.5 60.1 60.6 59.8 56.7 62.3 63.5 64.5 51.7 65.4 68.1 67.3 68.3 58.4 63.4 64.0 64. 76.7 76.8 77.0 77.3 77.0 18.5 20.9 22.0 30.6 10.1 20.2 25.4 23.7 29.1 27.8 28.9 30.2 35.2 34.1 34.2 35.1 37.1 40.0 34.1 35.8 35.6 40. 45.6 44.9 43.7 43.9 45.7 Table 2. Ablation over the pre-training components of GASP . We show performance directly obtained from pre-training (4D occ. P@R70) and its generalization to downstream tasks (Semantic BEV Forecasting and Map Segmentation) when finetuned on different amounts of labeled samples. We ablate each component added to UnO with the sensor encoder frozen ( ) and unfrozen ( ) as well as performance with no pre-training . 4.6. Ablations To understand the key contributors to our pre-training strategy, we systematically ablate its components and analyze their individual impact. See Appendix for details. Loss terms: We introduce different loss terms incrementally and measure their effect on final performance. The results, summarized in Tab. 2, highlight the relative contribution of each term and show significant boost when combined. Rotation augmentation: We vary the maximum rotation angle for data augmentation to determine its influence on model robustness. The results are presented in Tab. 3. This augmentation yields significant and consistent improvements for both UnO and GASP between 5 and 45. We opt to use 20 as default. Additionally, we investigate the effect of translation and jitter augmentations, which do not yield meaningful improvements, see Appendix C.3. Missing rays: We examine the effect of adding supervision for missing lidar rays. Quantitative results in Appendix C.2, do not show significant impact, as the effect of missing rays is not explicitly captured by the current metrics. However, the primary benefit is visually apparent in Fig. 8. Missing ray supervision reduces prediction noise in regions with sparse supervision, such as near region-of-interest boundaries or towards an unobstructed horizon. Additionally, we note reduction in occupancy halos above vehicles, which were reported as failure case in previous work [2]. DINOv2 components: To evaluate the role of DINOv2 features, we alter the number of components (8, 16, and 32) and observe the impact on performance. We conclude that learning to predict the 16 most important components yields good results across all three tasks with the full results reported in Appendix C.1. Rotation angle (4D-occupancy ) 0 5 10 20 45 90 78.1 80.1 81. 81.6 78.1 77.2 Table 3. Rotation augmentation for GASP. Recall at precision 70% for different angles. Figure 8. Effect of pre-training with/without missing rays. Without missing rays we observe artifacts in regions where the model is never supervised. Using missing rays as unoccupied supervision, these artifacts are greatly reduced. Geometries are colored by height; blue down and red up. 4.7. Scaling pre-training One of the most important qualities of self-supervision is that it continues to show benefits when applied to huge amounts of data. To demonstrate this, we train GASP on varying number of pre-training samples and evaluate on 4D-occupancy and fine-tuned semantic forecasting (on 1k labeled samples with frozen encoder). Here, we opt to use the Zenseact Open Dataset (ZOD) [3] to study the scaling behaviour beyond the 100k training samples available in Argoverse 2 Sensor [53]. The results in Fig. 9 show that our method scales predictably with no sign of saturation, even when trained on the combined Frames, Sequences, and Drives of ZOD. Furthermore, this experiment shows that GASP is dataset-agnostic. The generally lower scores are expected as ZOD has greater focus on highway driving with higher average velocities than the predominantly inner city driving in AV2. 5. Conclusion Autonomous driving (AD) generates vast amounts of spatiotemporal data, alluding to the possibility of harnessing scale to learn the underlying geometric and semantic structure of the environment and its evolution over time. To this end, we introduce GASP, self-supervised pre-training strategy that enables scalable representation learning for AD using geometric, semantic, and temporal supervision signals. Conditioned on past sensory input, GASP is supervised to predict 1) future occupancy, 2) features from Figure 9. Scaling properties. We vary the number of pre-training samples and evaluate performance (red ) and generalization (blue ), demonstrating GASPs remarkably predictable, logarithmic, scaling behavior. vision foundation model, and 3) ego-path traversal probability at any continuous point = (x, y, z, t) in spacetime. In doing so, GASP learns rich and generalizable representation of the environment that can be used directly or finetuned for variety of downstream tasks. We demonstrate that our pre-training strategy greatly improves the generalization on tasks such as semantic forecasting, online mapping, and ego-motion forecasting when compared to strategies that only utilize geometric and temporal supervision. Our results suggest that GASP is promising approach for learning sensor agnostic and generalizable representations for autonomous driving in scalable manner. We release the code to support further research in this area."
        },
        {
            "title": "Limitations and future work",
            "content": "While we only use GASP to pre-train lidar-based model, the approach is directly applicable to any BEV model, making setups with alternative sensors or complementary multi-modal configurations promising direction for future work. Furthermore, leveraging other foundation models (e.g. CLIP [45], SAM [29], or SAL [40]) or tapping in to other sources of self-supervision (e.g. flow-consistency) could further enrich the learned representations. Finally, while GASP shows powerful scaling properties, under the current trend we would require roughly 300,000 years of driving to reach near-perfect 4D occupancy prediction highlighting the need for further improvements in pretraining efficiency."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Georg Hess for fruitful discussions and valuable feedback on the manuscript. We also thank Luca Caltagirone for help with visualizations and Boris Ivanovic insightful discussions and inspiring ideas. This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. Computational resources were provided by NAISS at NSC Berzelius and C3SE Alvis, partially funded by the Swedish Research Council, grant agreement no. 2022-06725."
        },
        {
            "title": "References",
            "content": "[1] Ben Agro, Quinlan Sykora, Sergio Casas, and Raquel Urtasun. Implicit occupancy flow fields for perception and prediction in self-driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13791388, 2023. 2, 3, 6, 1 In Proceedings of [2] Ben Agro, Quinlan Sykora, Sergio Casas, Thomas Gilles, and Raquel Urtasun. Uno: Unsupervised occupancy fields the for perception and forecasting. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1448714496, 2024. 1, 2, 3, 5, 6, 7 [3] Mina Alibeigi, William Ljungbergh, Adam Tonderski, Georg Hess, Adam Lilja, Carl Lindstrom, Daria Motorniuk, Junsheng Fu, Jenny Widahl, and Christoffer Petersson. Zenseact open dataset: large-scale and diverse multimodal dataset In Proceedings of the IEEE/CVF for autonomous driving. International Conference on Computer Vision, pages 20178 20188, 2023. 3, 4, 8 [4] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael G. Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1561915629, 2023. 2 [5] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task masked autoencoders. In European Conference on Computer Vision, pages 348367. Springer, 2022. 2 [6] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022. [7] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. Transactions on Machine Learning Research, 2024. Featured Certification. 2 [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, pages 18771901. Curran Associates, Inc., 2020. 2 [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 2 [10] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multiIn Proceedings of modal dataset for autonomous driving. the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. 3, 4 [11] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. Nuplan: closed-loop ml-based In CVPR planning benchmark for autonomous vehicles. ADP3 workshop, 2021. 3, 4 [12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 96309640, 2021. [13] Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: In Prounified model to map, perceive, predict and plan. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1440314412, 2021. 1, 2, 6 [14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. 2 [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. 2 [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 2 [17] Adam Harley, Zhaoyuan Fang, Jie Li, Rares Ambrus, and Katerina Fragkiadaki. Simple-bev: What really matters for multi-sensor bev perception? In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 27592765. IEEE, 2023. 6 [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 3 [19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. [20] Georg Hess, Johan Jaxing, Elias Svensson, David Hagerman, Christoffer Petersson, and Lennart Svensson. Masked autoencoder for self-supervised pre-training on lidar point clouds. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 350359, 2023. 2 [21] Georg Hess, Adam Tonderski, Christoffer Petersson, Kalle Astrom, and Lennart Svensson. Lidarclip or: How learned to talk to point clouds. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 74387447, 2024. 3 [22] Anthony Hu, Zak Murez, Nikhil Mohan, Sofƒ±a Dudas, Jeffrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and Alex Kendall. Fiery: Future instance prediction in birdseye view from surround monocular cameras. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1527315282, 2021. 2 [23] Peiyun Hu, Jason Ziglar, David Held, and Deva Ramanan. What you see is what you get: Exploiting visibility for 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11001 11009, 2020. 6 [24] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 1, 2, 3, 7 [25] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 83408350, 2023. 1, 2 [26] Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, Dragomir Anguelov, et al. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 96449653, 2023. 1 [27] Tarasha Khurana, Peiyun Hu, David Held, and Deva Ramanan. Point cloud forecasting as proxy for 4d occupancy In Proceedings of the IEEE/CVF Conference forecasting. on Computer Vision and Pattern Recognition (CVPR), pages 11161124, 2023. [28] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. CoRR, abs/1412.6980, 2014. 5 [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 8 [30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 3 [31] Wolfgang Koch. On exploiting negative sensor evidence for target tracking and sensor data fusion. Information Fusion, 8(1):2839, 2007. Special Issue on the Seventh International Conference on Information Fusion-Part II. 2 [32] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, and Raquel Urtasun. Pnpnet: End-to-end perception and prediction with tracking in the loop. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1155311562, 2020. 2 [33] Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Maptrv2: An end-to-end framework for online vectorized hd map construction. International Journal of Computer Vision, pages 123, 2024. 1 [34] Adam Lilja, Junsheng Fu, Erik Stenborg, and Lars Hammarstrand. Localization is all you evaluate: Data leakage In Proceedin online mapping datasets and how to fix it. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2215022159, 2024. 5, 7 [35] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Feature pyraBharath Hariharan, and Serge Belongie. In Proceedings of the mid networks for object detection. IEEE conference on computer vision and pattern recognition, pages 21172125, 2017. 3 [36] Carl Lindstrom, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, and Lennart Svensson. Are nerfs ready for autonomous driving? towards closing the real-to-simulation gap. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44614471, 2024. [37] Haotian Liu, Mu Cai, and Yong Jae Lee. Masked discrimination for self-supervised learning on point clouds. In European Conference on Computer Vision, pages 657675. Springer, 2022. 2 [38] Junyi Ma, Xieyuanli Chen, Jiawei Huang, Jingyi Xu, Zhen Luo, Jintao Xu, Weihao Gu, Rui Ai, and Hesheng Wang. Cam4docc: Benchmark for camera-only 4d occupancy forecasting in autonomous driving applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2148621495, 2024. 2 [39] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. Featured Certification. 2 [40] AljoÀása OÀásep, Tim Meinhardt, Francesco Ferroni, Neehar Peri, Deva Ramanan, and Laura Leal-Taixe. Better call sal: Towards learning to segment anything in lidar. In European Conference on Computer Vision, pages 7190. Springer, 2024. 3, 8 [41] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. In European conference on computer vision, pages 604621. Springer, 2022. 2 [42] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature In Proceedings of the IEEE conlearning by inpainting. ference on computer vision and pattern recognition, pages 25362544, 2016. 2 [43] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. Technical report, OpenAI, 2018. 2 [44] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 2 [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 2, 8 [46] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3, 4 [47] Quinlan Sykora. Personal correspondance., 2025. 1 [48] Adam Tonderski, Carl Lindstrom, Georg Hess, William Ljungbergh, Lennart Svensson, and Christoffer Petersson. Neurad: Neural rendering for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1489514904, 2024. 4 [49] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, et al. Scene as occupancy. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8406 8415, 2023. 2 [50] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learnArXiv, self-supervised video pre-training. ers abs/2203.12602, 2022. 2 for [51] Antonin Vobecky, Oriane Simeoni, David Hurych, Spyridon Gidaris, Andrei Bursuc, Patrick Perez, and Josef Sivic. Pop-3d: Open-vocabulary 3d occupancy prediction from images. Advances in Neural Information Processing Systems, 36:5054550557, 2023. 3 [52] Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Parallelized architecture for realtime autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1544915458, 2024. 2, 7 [53] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, and James Hays. Argoverse 2: Next generation datasets for self-driving perception and forecasting. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021), 2021. 3, 4, 5, 8 [54] Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang, Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun, Kun Jiang, et al. Pandaset: Advanced sensor suite dataset for autonomous driving. In 2021 IEEE international intelligent transportation systems conference (ITSC), pages 30953101. IEEE, 2021. 3, 4 [55] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Realtime 3d object detection from point clouds. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 76527660, 2018. 3 [56] Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi Zhu, Tong He, Shixiang Tang, Hengshuang Zhao, Qibo Qiu, Binbin Lin, et al. Unipad: universal pre-training paradigm In Proceedings of the IEEE/CVF for autonomous driving. Conference on Computer Vision and Pattern Recognition, pages 1523815250, 2024. [57] Jiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Seung Wook Kim, Boyi Li, Tong Che, Danfei Xu, Sanja Fidler, Marco Pavone, et al. Emernerf: Emergent spatial-temporal scene decomposition via self-supervision. arXiv preprint arXiv:2311.02077, 2023. 3, 4 [58] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, and Hongyang Li. Generalized predictive model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1466214672, 2024. 2 [59] Jiawei Yang, Katie Luo, Jiefeng Li, Congyue Deng, Leonidas Guibas, Dilip Krishnan, Kilian Weinberger, Yonglong Tian, and Yue Wang. Denoising vision transformers. In European Conference on Computer Vision, pages 453 469. Springer, 2024. 3, 4 [60] Zetong Yang, Li Chen, Yanan Sun, and Hongyang Li. Visual point cloud forecasting enables scalable autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1467314684, 2024. 2 [61] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. CenterIn Proceedings of based 3d object detection and tracking. the IEEE/CVF conference on computer vision and pattern recognition, pages 1178411793, 2021. 1 [62] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1931319322, 2022. [63] Yumeng Zhang, Shi Gong, Kaixin Xiong, Xiaoqing Ye, Xiao Tan, Fan Wang, Jizhou Huang, Hua Wu, and Haifeng Wang. BEVWorld: multimodal world model for autonomous driving via unified BEV latent space, 2024. 2 [64] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. Occworld: Learning 3d In Eurooccupancy world model for autonomous driving. pean conference on computer vision, pages 5572. Springer, 2024. 2 [65] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transarXiv preprint formers for end-to-end object detection. arXiv:2010.04159, 2020. 3, 7 GASP: Unifying Geometric and Semantic Self-Supervised Pre-trained for Autonomous Driving"
        },
        {
            "title": "Appendix",
            "content": "applying our training recipe from Sec. 4 further enhances performance, while additional training improvements, such as rotation augmentation (Sec. 3.2) and handling missing rays (Sec. 3.2), provide an extra boost. For fair comparison of our main contributions, we refer to the bestperforming version as UnO. Name Details UnO UnOr UnO+ UnO GASP reported in [2] reimplemented our training recipe our improvements proposed method R@P 70% 67.0 72. 77.6 79.4 81.6 Table 5. 4D occupancy recall at precision 70% for our reimplementation and original implementation [2]. A. Baseline reimplementation We base our work on the method described in [2]. However, since their implementation is closed-source we reimplemented their method according to their paper, their predecessor [1] as well as personal correspondence with the authors [47]. To verify our reimplementation, we report the number of parameters in the original implementation and our version in Tab. 4. Note that the decoder head matches perfectly (up to the value number reported in the original paper) and that the encoder is within 3% margin. Parameter count Uno encoder Uno decoder Reported Reimplementation 17.4M 16.9M 0.06M 0.06M Table 4. Parameter count in original UnO (as reported in the paper) and our reimplementation. In addition, we also verify our reimplementation by running identical experiments and comparing to the results reported in the paper. In Fig. 10, we show the Average Precision for semantic forecasting across number of training samples for fine-tuning with both frozen and unfrozen sensor encoder. Figure 10. Semantic forecasting AP across number of training samples for our reimplemented baseline (solid) and the results reported in [2] (dashed). Lastly, we show the results for 4D-occupancy in Tab. 5 using recall at precision 70% as the metric, following the original paper [2]. Our results show higher performance numbers, which may be due to either improvements in our set-up or slight differences in evaluation settings. Moreover, B. Additional results Here, we show additional results from our evaluation. For completeness we also report the numbers visualized as graphs in the main manuscript. B.1. Semantic BEV forecasting First, in Tab. 6, we show the performance of GASP and UnO on the semantic BEV forecasting task using the SoftIoU metric. The results follow the same trend as the Average Precision metrics shown in Tab. 7. Enc. PT Labeled data samples 103 102 10 104 105 UnO 11.61.8 14.82.0 21.31.4 21.3 22.3 22.6 GASP 19.02.6 23.14.5 28.00.9 30.5 29.4 31.4 UnO 16.62.4 22.53.9 25.62.2 27.4 39.9 39.5 GASP 26.82.2 27.91.4 29.02.8 36.0 39.6 41.0 Table 6. Semantic BEV forecasting performance (Soft-IoU) for GASP and UnO across different number of fine-tuning samples with frozen ( ) sensor encoder. ) and unfrozen ( Enc. PT 1 Labeled data samples 103 102 104 105 UnO 4.72.2 4.91.9 17.60.7 26.4 31.0 34.1 GASP 9.63.1 12.13.7 28.70.8 35.2 39.1 40.0 3.30.2 4.10.7 10.63.6 34.1 42.7 45.6 UnO 4.82.4 5.01.8 17.91.0 34.2 37.5 44.9 GASP 9.23.4 12.53.0 28.60.9 40.0 43.2 45.7 Table 8. Map segmentation mIoU (mean and std. dev) across number of labeled samples with frozen ( ) sensor encoder. GASP outperforms the baseline across all amounts of training samples indicating that the BEV features contain richer BEV representation. ) and unfrozen ( Enc. PT Scratch UnO GASP"
        },
        {
            "title": "Labeled data samples",
            "content": "100 18.4 29.1 41.3 103 38.7 43.4 47.1 104 44.6 45.6 49. 105 54.7 59.9 60.9 For completeness, we show the detailed numbers from Fig. 5 in Tab. 7 Table 9. Average Precision (AP) for Semantic 4D Occupancy Forecasting. We evaluate performance across different numbers of fine-tuning samples and report results for vehicle segmentation. Enc. PT 1 Labeled data samples 103 102 10 104 105 UnO 17.83.2 34.82.1 52.40.8 55.6 57.7 57.8 GASP 30.07.3 52.72.0 61.20.4 65.7 66.2 66. UnO 25.63.7 41.44.2 52.40.8 61.1 72.1 73.3 GASP 45.72.3 56.00.9 61.10.9 68.8 73.6 74.7 Table 7. BEV semantic forecasting performance (AP) showed across number of fine-tuning samples with frozen ( ) and unfrozen ( ) sensor encoder. B.2. Map segmentation In Tab. 8 we report detailed results for the map segmentation task showed in Sec. 4.2. B.3. Semantic 4D occupancy As an extension to the 3D semantic occupancy (BEV forecasting) in Sec. 4.3 we can post-train the model on also including the height in the prediction, namely 4D (3D+time) occupancy. Tab. 9 reports the Average Precision performance for UnO and GASP on the vehicle class. We note that GASP outperforms the baseline for all number of labeled samples. C. Additional ablations For completeness, we present the full results of our ablation studies, highlighting the contribution of each component to the final performance. PT Rotation augmentation (4D-occupancy ) 0 5 10 20 45 90 UnO GASP 77.6 78.1 79.2 80.1 78.3 81.7 79.4 81.6 78.7 78.1 74.1 77. C.1. DINOv2 feature dimensions Table 12. Rotation augmentation. Recall at precision 70% We vary the number of the principal component analysis reduced components (8, 16, and 32) from DINOv2 that we train to predict. Tab. 10 reports the impact on performance. We conclude that learning to predict the 16 most important components yields good results across all three tasks. 4D-occ Enc. dim 8 16 32 8 16 32 81.1 81.6 80.2 n/a n/a n/a Sem. Forecasting Labeled samples 105 103 102 Map Seg. Labeled samples 105 103 50.4 59.3 59.3 53.4 59.8 60.4 54.7 64.5 62.5 62.7 67.3 67.1 54.4 64.1 63.0 76.4 77.0 76. 28.5 30.6 26.9 29.7 29.1 29.1 36.8 35.2 34.7 39.3 40.0 38.8 40.7 40.0 38.6 44.7 45.7 45. Table 10. Performance across different downstream task when varying the number of DINOv2 dimensions used in the regression objective. C.2. Missing rays Here, we ablate the use of inferred missing rays during pretraining. We measure performance on geometric 4D occupancy using the recall at precision 70% and report the numbers in Tab. 11. As noted in the main manuscript, we do not see any quantitative improvements, but rather qualitative ones. We hypothesize that this is because the metric inherently disregards the regions where this supervision helps. Missing rays GASP 81.6 81.0 displayed the recall at precision 70%. We dont see any major improvements using this augmentation and opt to use the more impactful rotation augmentation. We hypothesize that the data already includes large variety of lateral and longitudinal shifts. PT"
        },
        {
            "title": "Translation augmentation",
            "content": "0.5m 1.5m 3.0m"
        },
        {
            "title": "GASP",
            "content": "78.1 79.0 78.3 76.0 Table 13. Translation augmentation. Recall at precision 70% Jitter augmentation: We also experiment with jitter augmentation, which aims to up-sample negative queries close to the positive queries by adding jitter parameter œÑ to the negative query equation: = oi + (pi oi)dœÑ (7) where U(0, 1). Our initial intuitionthat increasing jitter would lead to sharper geometry learningdid not hold, as performance declines with higher jitter values. Conversely, reducing jitter also appears to negatively impact model performance. PT 0. 0.8 1.0 1.2 2.0 3.0 Jitter GASP 80.0 80.1 81.6 80.7 75. 68.5 Table 14. Jitter. Recall at precision 70% Table 11. Missing rays. Recall at precision 70% C.4. DINO loss C.3. Augmentation In addition to the rotation augmentation outlined in the main manuscript, we also experiment with translation, jitter augmentations, and the number of feature dimensions in the DINOv2 features. We measure geometric 4D occupancy performance as measured by the recall at precision 70%. Rotation augmentation: For completeness, we, apart from GASP, also show that UnO benefits from rotation augmentation in Tab. 12. Translation augmentation: Similarly to the rotation augmentation, we can augment the training data such that the vehicle is translated in and directions. We ablate the effects of adding such translation augmentation, and Tab. We ablate the loss function used to learn our DINOv2 features in Tab. 15. We again compare performance on geometric 4D occupancy using the recall at precision 70% metric. The Smooth-L1 loss reduces the performance and we opt to use the L1-loss. PT L1 Smooth-L1 (Œ≤) 0.5 1.0 0. GASP 81.6 78.9 79.3 79.3 Table 15. Dino-loss. Smooth L1. Recall at precision 70% D. Additional visualizations We provide some additional qualitative results in Figs. 11 to 18. In short, they aim to give more examples of the quality of the information that the representation embeds, but also to depict some interesting emergent properties of our pre-training strategy. In Fig. 11 one can view the full holistic view of both semantic and occupancy-field information, as opposed to in Fig. 3, and in Fig. 12 birds-eye view of point-cloud inputs, features, and occupancy, is provided. Furthermore, in Fig. 13, the multimodal outputs on path probabilities in three-way intersection are depicted. For the BEV semantic forecasting task, Sec. 4.3, complementary qualitative results are provided in Fig. 14 for three scenes, common case, case with unusual objects, and more complex case. In Fig. 15, outputs from the map segmentation task are given for different number of samples in the training set given frozen GASP encoder. Here, we may visually make note of the models ability to predict pedestrian crossings, as indicated in Sec. 4.4, despite the absence of this information in the Lidar input data. Qualitative results connected to the ego-trajectory prediction task can be viewed in Fig. 16. In Fig. 17 we note the similarity between the predicted semantic regions of, what could possibly be understood as, drivable area with the predicted ego future path. Potentially, their joint supervision signal amplifies tasks directly dependent on this type of information, such as map segmentation, which could explain why having both, and not one or the other, seems beneficial for said task. Finally, in Fig. 18, we show that feature prediction in GASP has the potential of capturing fine-grained, yet important, scene details to an extent that its geometric occupancy head, and by extension model which only models geometric occupancy, hardly highlights. Figure 11. Occupancy and Dino features projected into camera view. Note that the white-box representing ego vehicle has been injected for illustrative purposes. Figure 12. Occupancy and Dino features projected into camera view, holistic view, and birds-eye view. Figure 13. Dino features and ego path in three-way intersection. Figure 14. BEV segmentation forecasting results showing A) typical scenario, B) scenario with uncommon road users (in this case an excavator), and C) more complex scenario. An unfrozen GASP representation with 100 samples available in the post-training task is used. Figure 15. Prediction results from the map segmentation post-training task of GASP with frozen encoder. Note that predictions are only made based on lidar input. Camera images are only provided as visual clarity for the reader regarding what scene is being predicted. Figure 16. Ego-trajectory prediction results using frozen GASP representation. Expert trajectories, the groundtruth, are shown in green while predictions are shown in blue. Note that camera inputs are only provided as visual support for the reader and are not part of the prediction. Figure 17. Ego path along with the first and second three most important features, highlighting the complementing aiding properties of the ego path task and the DINO feature prediction task in encoding information about drivable area in the representation. Figure 18. qualitative example of the feature-level information predicted by the representation produced by GASP, showcasing its capability of contrasting otherwise diffuse scene elements such as the lane-dividers (marked A.) or the person carrying bag (marked B.) from the background."
        }
    ],
    "affiliations": [
        "Chalmers University of Technology",
        "Linkoping University",
        "Lund University",
        "Zenseact"
    ]
}