{
    "paper_title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
    "authors": [
        "Ming Ma",
        "Jue Zhang",
        "Fangkai Yang",
        "Yu Kang",
        "Qingwei Lin",
        "Tianming Yang",
        "Saravan Rajmohan",
        "Dongmei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 2 9 4 7 6 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "DOVER: INTERVENTION-DRIVEN AUTO DEBUGGING FOR LLM MULTI-AGENT SYSTEMS Ming Ma1,2 , Jue Zhang3 , Fangkai Yang3, Yu Kang3, Qingwei Lin3, Tianming Yang1, Saravan Rajmohan3, Dongmei Zhang3 1 Institute of Neuroscience, State Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, 200031, China 2 University of Chinese Academy of Sciences, School of Future Technology, Beijing, 100049, China 3 Microsoft mam2022@ion.ac.cn, {juezhang, fangkaiyang}@microsoft.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language model (LLM)based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting more outcome-oriented view of debugging. Within the MagneticOne agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 1828% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer."
        },
        {
            "title": "INTRODUCTION",
            "content": "The advancement of Large Language Models (LLMs) has led to the rapid rise of LLM-based agent systems, particularly multi-agent architectures where agents of different roles work collaboratively to solve complex tasks Li et al. (2023); Wu et al. (2023a); Hong et al. (2024). As these systems are increasingly developed and deployed in production, the need to debug their failures becomes inevitable during their lifecycle. Importantly, by failures we do not refer to conventional software errors (e.g., exceptions or crashes), but rather to the errors where the system executes without interruption yet produces incorrect or unsatisfactory results Mialon et al. (2024); Yoran et al. (2024). Such failures frequently arise in scenarios where one diagnoses why an agent system underperforms on benchmark tasks during the development phase, or when one addresses user-reported dissatisfaction (e.g., thumbs-down signal with textual feedback) from an online deployed system. Work is done during an internship at Microsoft. Corresponding authors."
        },
        {
            "title": "Preprint",
            "content": "Debugging failures in LLM-based agent systems presents unique challenges. These systems typically involve multiple rounds of LLM calls, each with extensive textual context, making manual log inspection labor-intensive. Furthermore, in multi-agent tasks, tracking inter-agent information flow is crucial, as failures often stem from Inter-Agent Misalignment Cemri et al. (2025). Recent efforts address these issues by using LLMs to analyze system failures Zhuge et al. (2024); Zhang et al. (2025c;a), often via single-agent, single-step failure attribution. In this method, an execution log is input to an LLM tasked with identifying the agent and step responsible for the failure Zhang et al. (2025c). However, as shown in our reproduction study (Section 3), log-based failure attribution is fundamentally limited by the uncertainty of ground-truth annotations. This uncertainty arises for several reasons: agent systems often employ multiple strategies (e.g., ReAct Yao et al. (2023)) with distinct failure points in single session, and inter-agent misalignment can render the assignment of responsibility to single agent or step ambiguous. To circumvent the limitations of uncertain ground-truth attribution, we propose explicit validation via intervention, introducing DoVer (Do-then-Verify), an intervention-driven framework for automated debugging. DoVer explicitly validates failure attribution hypotheses derived from session logs by intervening at suspected failure points, modifying agent instructions or task plans, while preserving prior context. The system is then re-executed from the intervention point onward. If the failure resolves, the hypothesis is supported; if it persists despite faithful intervention, the hypothesis is refuted. This process enables an iterative cycle of hypothesis generation and validation. DoVer also supports interventions across multiple steps rather than restricting to single-point edits. By decomposing the failure trace into separate trials, we intervene at each and assess the impact. Within the Magnetic-One (M1) Fourney et al. (2024) agent framework, our experiments show that DoVer recovers 18% and 28% of failures on datasets from AssistantBench Yoran et al. (2024) and GAIA Mialon et al. (2024), respectively. It further enables the validation or refutation of 3060% of failure hypotheses, depending on task complexity. To demonstrate the generality of DoVer, we apply it to another agent framework, AG2 AutoGen2 (2025), using different dataset, GSMPlus Li et al. (2024). DoVer again performs effectively, achieving 49% flip rate on failure cases. To summarize, our main contributions are: (i) We propose DoVer, an intervention-driven framework for automatically debugging failures in LLM-based multi-agent systems; (ii) We identify and analyze the challenges posed by uncertain ground-truth annotations in log-based failure attribution; (iii) We demonstrate experimentally that DoVer not only recovers significant portion of failure cases but also enables explicit validation and refutation of failure attribution hypotheses."
        },
        {
            "title": "2.1 FAILURE ANALYSIS AND ATTRIBUTION FOR LLM-BASED AGENT SYSTEMS.",
            "content": "LLM-based agent systems exhibit diverse and frequent failure patterns that accumulate along long execution logs. To characterize why and where errors arise, MAST Cemri et al. (2025) catalogs failures across task interpretation, planning, tool/environment interaction, and verification, while evaluation frameworks Zhuge et al. (2024); Arabzadeh et al. (2024) argue that end-to-end pass/fail is too coarse and introduce requirement graphs or task-utility criteria that reveal where progress stalls. Extending this perspective to execution logs, TRAIL Deshpande et al. (2025) creates turnlevel traces and fine-grained taxonomy (reasoning, planning, execution), empirically showing that even strong long-context models struggle at trace debugging. parallel line of work seeks failure attribution: identifying the earliest decisive step or agent that is responsible for the earliest sufficient cause of failure Zhang et al. (2025c;a). However, these attributions are inferred from logs and remain an untested hypothesis unless validated by execution. In DoVer, we treat attribution as hypothesis to be tested. We apply targeted edit at the implicated location (message, plan, tool call), and rerun the system, judging success by milestone/utility gains. This places emphasis on verified repair and is consistent with trajectory-aware evaluation advocated in Arabzadeh et al. (2024); Deshpande et al. (2025). Recent contemporaneous studies1 further advance failure attribution by introducing reasoningdriven judges Zhu et al. (2025a), abductactpredict counterfactual scaffolding West et al. (2025), 1These works appeared during the final stage of this work preparation or the subsequent peer-review period."
        },
        {
            "title": "Preprint",
            "content": "causal-inference formulations Ma et al. (2025), spectrum-based failure attribution approach Ge et al. (2025), hierarchical error-attribution method Banerjee et al. (2025), and graph-guided failure tracing approach Zhang et al. (2025b). Notably, the observation in West et al. (2025) that adding explicit step indices improve attribution aligns with our own prompt-refinement study in Section 3. Emerging works have also explored specialized failure-tracer models trained on curated success/failure trajectories Zhang et al. (2025a); Kong et al. (2025). These approaches are orthogonal to DoVer and could be incorporated into the initial failure-attribution stage of DoVer to strengthen its diagnostic signal."
        },
        {
            "title": "2.2 DEBUGGING APPROACHES FOR LLM-BASED AGENT SYSTEMS",
            "content": "Beyond failure analysis and attribution, several systems explore how to debug trajectories, i.e., intervention and replay. Human-in-the-loop tools such as AGDebugger Epperson et al. (2025) enable rewind/edit/re-execute with trace visualization, and graph runtimes like LangGraph LangChain (2025) provide checkpoints, interrupts, and time-travel branching. These demonstrate that small, targeted interventions often work, but they are manual and hard to scale. The recent AgentDebug Zhu et al. (2025b) work employs an intervention-driven methodology similar to DoVer, although it does not place particular emphasis on multi-agent settings. From the software-repair perspective, Rahardja et al. (2025) package real agent-system issues into executable environments with failing tests (AgentIssue-Bench) and find low resolution rates for current software engineering agents, highlighting the difficulty of maintaining agent software. Industrial experience at Google similarly evaluates agent-based program repair on production bugs, showing promise but also current limits Rondon et al. (2025). This motivates auto debugging that verifies edits within the original run and measures intermediate progress. DoVer does so by choosing minimal intervention, re-running the trajectory, and scoring milestone/utility gains, in line with Zhuge et al. (2024); Deshpande et al. (2025)."
        },
        {
            "title": "3 FROM LOG-BASED ATTRIBUTION TO INTERVENTION-BASED DEBUGGING",
            "content": "In this section, we surface several subtleties in existing log-based failure analysis that motivate our intervention-based auto-debugging system. We begin by recapping the prevailing task formulation for log-based single-agent/step failure attribution, and then revisit evaluation on an existing benchmark to highlight sources of uncertainty in ground-truth annotation that shape our design. Log-based Single-Agent/Single-Step Failure Attribution. common setup for log-based failure attribution proceeds as follows. The input is session log, typically from an LLM-driven multiagent system, covering the full trajectory from an initial user query to either final answer or termination caused by system-defined stopping rules (e.g., maximum number of replanning rounds). The log is sequence of agent messages organized by turns.2 The task is to identify the earliest single agent and single step responsible for the failure. failure step is defined as decisive error: if one were to replace the agents incorrect action at that step with correct one, the trajectory would subsequently succeed. To evaluate this setup, the Who&When (WW) dataset was introduced in Zhang et al. (2025c). In WW, multiple annotators independently reviewed session logs and then reconciled discrepancies through discussion. Reported results show that state-of-the-art LLMs at the time achieved below 10% accuracy on failure step attribution. Reproduction and Prompt Refinements. We reproduced the WW step/agent attribution protocol to understand why step-level accuracy is low. Through failure case analysis, we identified two minimal, non-invasive prompt refinements that consistently improve accuracy: (i) adding explicit step indices to the failure log, and (ii) embedding concise reminder of the annotators guidance as instructions. With these refinements, attribution accuracy rises noticeably. For example, on the Hand Crafted category of WW (WW-HC) with the setting of including ground-truth answers and adopting 2We do not consider logs from asynchronous agent executions in this work."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Failure trace of Case 3 in WW-HC, illustrating ambiguity in failure attribution. The session consists of four distinct trials, each initiated by plan update and executed via ReAct-style Yao et al. (2023) loop. Different strategies (e.g., direct scrolling in Trial 1 vs. calendar navigation in Trial 2) yield separate error points, making single-step attribution across the session inherently ambiguous. Trial 2 (Steps 5355) further shows inter-agent misalignment: the Orchestrator issued an invalid instruction, while the WebSurfer compounded the error by executing an unrelated action. the All-at-Once prompt (a single LLM call over the full session log), step attribution accuracy for GPT-4o increases from 6% to 24%.3 Reproduction details are provided in Appendix A. Impact of Uncertain Ground-Truth Labels on Attribution. Despite these gains, absolute steplevel accuracy remains low for practical deployment. By comparing model outputs against the ground-truth (GT) labels, we found that uncertainty in the GT annotation is major contributing factor. Specifically, for the 29 GAIA Mialon et al. (2024) cases in WW, our independent review suggests that 14 of 29 cases exhibit GT uncertainty. This is consistent with WW, which reports annotator uncertainty of 1530% and an average initial disagreement of 20% Zhang et al. (2025c). To assess the impact of GT uncertainty on model performance, we stratified evaluation into two subsets. For the 14 uncertain cases, average step attribution accuracy is 24% for GPT-4o and 7% for GPT-5. In contrast, for the remaining certain 15 cases, average accuracy increases to 44% for GPT-4o and 53% for GPT-5. These results indicate that GT uncertainty substantially affects model performance. Per-case annotation notes and results are provided in Appendix A. Sources of Uncertainty in Failure Agent/Step Annotations. Our re-annotation surfaced three sources of GT uncertainty among the 14 uncertain GAIA cases in WW: (1) Multiple trials within single session. Modern agentic systems frequently employ ReActstyle Yao et al. (2023) loops with repeated planningexecution cycles, producing multiple trials of task solving. We define trial as the contiguous span starting from planning step and continuing through the execution steps for that plan. Each trial may contain its own decisive error step(s), especially when exploring new strategies or branches. For instance, Figure 1 illustrates the failure trace of Case 3 in WW-HC, which comprises four distinct trials within single session. Different strategies were explored (e.g., Trial 1 attempted to locate the target webpage via direct scrolling, whereas Trial 2 used the calendar feature), each introducing distinct potential error points. Consequently, enforcing single-step attribution across the entire session is intrinsically ambiguous. In our review, 9 of the 14 uncertain cases exhibit this pattern. 3Unless otherwise specified, GPT-4o and GPT-5 refer to the versions of GPT-4o-20241120 and GPT5-chat-20250807, respectively. All LLM API calls are made through Azure OpenAI using default parameters."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: DoVer (DothenVerify) Debugging Pipeline. (1) Trial segmentation: split the failed session log into trials using re-plan steps as cut points. (2) Failure attribution: for each trial, propose hypothesis hi that marks faulty step or agent. turn hi into an actionable intervention that edits either the plan or the attributed message or step in the original log. (4) Intervention execution: replay the trajectory in place, i.e., preserve all steps before the intervened step, then execute the intervention and measure progress of the new log. Colors indicate plan/re-plan (blue), execution (green), attributed failure (red), terminal failure (dark red), terminal success (dark green), intervention (yellow), new plan/re-plan (blue hatch), and new execution (green hatch). (3) Intervention generation: (2) Ambiguity from multi-agent coordination. Attribution to specific agent/step can be unclear when the underlying issue stems from inter-agent coordination. For instance, when sub-agent fails to carry out an instruction, responsibility may lie with the sub-agents capabilities or with ambiguous/misaligned guidance from supervising agent. In Trial 2 of Figure 1, for example, the Orchestrator agent instructed the WebSurfer agent to click on non-existent control. Rather than reporting the issue, the WebSurfer instead clicked on an unrelated control that had no connection to the target year 2015. In this case, both agents exhibited failures, making attribution to either one alone inappropriate. Such ambiguous attribution aligns with the Inter-Agent Misalignment category identified in Cemri et al. (2025). We observed this phenomenon in 5 of the 14 uncertain cases. (3) Cross-annotator alignment challenges. Even with adjudication, achieving fully aligned labels can be difficult. For 7 of the uncertain cases in WW, we found no clear error at the GT-designated step, suggesting that differing interpretations can persist despite careful protocol design. Implications for Intervention-Based Debugging. The above analysis suggests that log-only failure attribution can fundamentally suffer from the issue of uncertain ground-truth labels. Consequently, we propose explicit validation via intervention: hypothesize the failure step, intervene on it (e.g., replace the action or instruction), and verify whether the trajectory subsequently succeeds. This protocol operationalizes the decisive error definition while simultaneously eliminating dependence on noisy human labels and thus reducing annotation burden. Moreover, two design takeaways follow from our uncertainty study. (i) Trial awareness. Because logs often contain multiple planningexecution trials, interventions must be applied at the trial level, motivating our trial-based intervention framework. (ii) Role-specific interventions. Given ambiguity in attributing responsibility between an orchestrator and its sub-agents, we adopt clear taxonomy of interventions: (a) interventions on the orchestrators plan and its instructions to sub-agents, and (b) direct capability improvements for sub-agents (e.g., skills). These implications motivate our intervention-based auto debugging system; in the next section, we detail the framework and its concrete instantiations."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "Following the implications for intervention-based debugging, we present DoVer, do-then-verify debugging pipeline that turns failure-attribution hypotheses into controlled edits and checks whether those edits change outcomes. As shown in Figure 2, DoVer consists of four stages: (1) trial segmentation to break an execution log into trials, (2) hypothesis generation to hypothesize failure step or agent, (3) intervention generation to synthesize testable change, and (4) intervention execution with differential evaluation against the original run using task success and progress score."
        },
        {
            "title": "4.1 DOVER PIPELINE",
            "content": "Trial Segmentation. After task is executed by an agentic system, we have obtained long execution session log τ = {(at, mt, σt)}T t=1, where at represents the active agent that produces the message mt at step t, and σt keeps stateful information that is necessary for state restore and replay. For example, this includes historical context (e.g., prior messages sent to LLMs) as well as browsing history for agents acting in web-browsing role. Since modern LLM-based agent systems Fourney et al. (2024); SmolAgents (2025); AutoGen2 (2025) have the self-reflection capabilities Yao et al. (2023); Shinn et al. (2023), and re-plan after reflection, we then segment the trace τ into trials τ using re-plan steps as segmentation point, as illustrated in Figure 2. This trial segmentation shortens context so LLMs can reason about single causal chain, and enables independent and parallel interventions for efficiency. Thus, multiple hypotheses can be proposed for single session trace, which is essential as many failures admit more than one viable repair. While one could implement trial segmentation by leveraging system-specific message patterns (e.g., in M1, plan or re-plan steps often begin with We are working to address the following user request), we instead adopt prompt-based approach. Specifically, we employ LLMs to reason over the full session log and identify planning-related steps. This method generalizes more effectively to other agent systems whose log patterns for planning steps are not known priori. The prompt used for trial segmentation is provided in Figure 5 in Appendix B. Failure Attribution. For each trial τ i, we generate candidate failure attribution hypothesis hi = (ˆai ˆt, ri ˆt), where ˆt is the step index of attributed failure step, is the trial index, ˆai the suspected agent, and ri natural-language rationale. We build on existing log-based attribution methods (e.g., our improved All-at-Once prompt from Section 3) but crucially do not require perfect precision, since correctness will later be tested via explicit intervention. Here we adapt the All-at-Once prompt (see Figure 6 in the Appendix B) so that it can be applied to session logs segmented into trials by the preceding step. Intervention Generation. In this stage, failure attribution hypotheses are transformed into concrete interventions. Each intervention Ii represents targeted edit to the failing context, informed both by the specific failure hypothesis and by the broader context of the intervention step. As noted in Section 3, ambiguity may arise in attributing responsibility between the orchestrator and its subagents. We thus distinguish interventions directed at orchestrator from those directed at sub-agents. To keep our method more agnostic to the underlying agent architecture, we focus primarily on interventions at the orchestrator level, i.e., interventions applied at the message-passing layer via direct edits to agent messages. This choice improves generality and simplicity but introduces tradeoff: we cannot directly intervene on sub-agents to enhance their capabilities, which would typically require substantial system-level modifications (e.g., extending the WebSurfer agent to support inpage search). In summary, the interventions considered in this work fall into two categories: Modified Instructions to Sub-Agents: Adjusting the orchestrators messages to sub-agents to clarify intent, correct arguments, or supply missing context. This approach indirectly influences subagent behavior. Plan Updates: Revising the orchestrators high-level plan, e.g., reordering, decomposing, or replacing steps, to route around the identified failure. The prompt used for intervention generation is provided in Figure 8 in Appendix B. Intervention Execution. The agentic system replays each trial with interventions applied in-situ at the suspected failure step. All steps are preserved before the intervened step and execute the intervention Ii, yielding counterfactual trace τI = {τ1, τ2, , τi1, τi}. Note that one intervention creates one new trace, verifying the failure attribution of each trial."
        },
        {
            "title": "4.2 EVALUATION METRICS",
            "content": "We consider two sets of evaluation metrics to address our research questions: (1) whether failed cases can be turned into successful ones through intervention and (2) how effective our debugging system is at validating or refuting the initial failure hypothesis. To reduce the impact of execution randomness (e.g., LLM stochasticity), we perform three independent runs for each intervention."
        },
        {
            "title": "Preprint",
            "content": "Metrics for Turning Failures into Successes. To answer the first question, we introduce two failure-flipping related metrics: Trial Success Rate and Progress Made. The Trial Success Rate is defined as the ratio of trial runs that successfully complete the task after intervention. The Progress Made metric captures whether intervention brings failed trial closer to success, even if it does not ultimately succeed. Thus, it provides more fine-grained measure of improvement when the intervention makes the failed case more correct. Specifically, when an intervention does not yield full success and human-annotated solution steps are available (e.g., in the GAIA benchmark Mialon et al. (2024)), we could evaluate the degree of progress by comparing the new execution trace against human-annotated steps. For each log τ , we extract up to 5 milestones {mk}K k=1 and measure how many of these milestones the new trace accomplishes. Both the extraction of milestones and their evaluation are performed using LLMs with carefully designed instructions. The actual prompts are provided in Figures 9 and 10 in Appendix B. We then define the milestone achievement count for trace γ as: A(γ) = (cid:88) k=1 I(cid:2)milestone mk is achieved in γ(cid:3) . We measure Progress Made as the ratio of additional milestones achieved: rog(τ τI ) = A(τI ) A(τ ) [1, 1], where τI is the new execution trace after intervention, is the number of extracted milestones from human-annotated trace. In settings where human-annotated intermediate steps are unavailable, the progress metric above cannot be directly applied. In such cases, one may instead directly compare execution traces before and after intervention with LLM-as-a-judge. We leave this alternative evaluation for future work. Metrics for Validating Failure Hypotheses. To evaluate the effectiveness of our debugging system in validating or refuting the initial failure hypothesis, we classify each trial after intervention into four categories: Validated, Partially Validated, Refuted and Inconclusive. The Inconclusive category is necessary because we frequently observe that agents fail to follow the intervened instruction, resulting in unsuccessful trials. In such cases, it is unclear whether the outcome stems from an incorrect failure hypothesis or from other limitations of the system that prevent the intervention from being carried out. To handle this ambiguity, we conduct comparative analysis of traces before and after intervention. We leverage LLMs to determine whether the intervention was faithfully executed by the agent, resulting in boolean metric is intervention fulfilled for each trial run. The prompt employed in this work can be found in Figure 11 in the Appendix B. Together with the overall Trial Success Rate and Progress Made metrics, we define the four outcomes as follows: Validated: At least 2 of 3 repeated runs succeed. Partially Validated: Fewer than 2 of 3 runs succeed, and at least 2 of 3 runs both fulfill the intervention and show additional 20% progress (i.e., they advance by one key milestone). Refuted: Same as Partially Validated, except that progress does not exceed 20%. Inconclusive: All other cases. In summary, these metrics allow us to quantify both the extent to which interventions make failed cases more correct and how effectively the system validates or refutes hypotheses. We now apply them in the next subsection to analyze our experimental results."
        },
        {
            "title": "5.1 EXPERIMENT SETUP",
            "content": "Agent System. We consider two distinct agent frameworks in this work. Following Zhang et al. (2025c), we begin by conducting experiments using Magentic-One (M1) Fourney et al. (2024),"
        },
        {
            "title": "Preprint",
            "content": "popular LLM-based multi-agent framework that was also used for collecting failure traces in WW. We enable DoVer on M1 by adapting the manual debugging tool AGDebugger Epperson et al. (2025) so that DoVer can (without human intervention): (i) save the state at each step as checkpoint; (ii) load checkpoints from prior failed run; (iii) intervene by modifying an agent message at specified step; and (iv) resume execution from the intervened step. To evaluate DoVers generality, we further construct MathChat multi-agent system using second framework, AutoGen2 (AG2) AutoGen2 (2025); Wu et al. (2023b). This MathChat system is instantiated using the prompts provided in MAST Cemri et al. (2025), and we extend AG2 with checkpointing and re-execution capabilities analogous to those in M1. Implementation details and practical lessons for reducing the integration burden on new frameworks are provided in Appendix C. Datasets. We first follow Zhang et al. (2025c) and include all cases in the Hand Crafted category of WW: 28 cases from AssistantBench Yoran et al. (2024) and 29 cases spanning all three GAIA levels Mialon et al. (2024). To increase data volume, we additionally include all 53 Level-1 cases from GAIAs validation set; after excluding those already present in WW, this yields 45 extra cases. We refer to these three sets as WW-AB, WW-GAIA, and GAIA-Level-1, respectively. For the MathChat system, following MAST Cemri et al. (2025), we additionally use the GSMPlus dataset Li et al. (2024). Concretely, we adopt the 2,400 examples in the testmini split and recollect execution traces with checkpoints for all problems, forming the GSMPlus setting used in our AG2-based experiments. Failure Trace Collection. We begin with an initial run over all cases and evaluate outcomes to identify failure traces. The failure logs published with WW and MAST are not directly usable for our purposes: logs of agent messages alone are insufficient to support replay and targeted intervention. The number of failed cases per dataset is given in Table 1 (Failed Cases column). The overall success rate on the full GAIA Level-1 set matches the value reported for Magentic-One Fourney et al. (2024), suggesting that our collected failure traces faithfully reflect M1s capabilities. As in WW and MAST, we use GPT-4o both to generate traces and to power the intervened session runs. WW-AB WW-GAIA GAIA-Level-1 GSMPlus Total 28 29 45 2400 Failed Cases 26 26 26 214 Intervened Cases 23 25 25 141 Intervened Trials 72 99 63 198 Trials per Case 3.1 4.0 2.5 1.4 Table 1: Summary of failed and intervened cases across datasets, showing the total number of cases, failed cases, intervened cases, total intervened trials, and the average number of trials per case. Auto Debugging with DoVer. We apply DoVer to each failed trace using GPT-4o while obtaining the progress made metric and failure hypothesis validation results with GPT-5. As described in Section 4, DoVer first segments the full trace into distinct trials, then performs failure attribution and intervention generation for each trial. Finally, it collects the intervened session traces and conducts comparative analysis. Table 1 reports the number of cases for which an intervention was successfully generated (LLMs may occasionally conclude, incorrectly, that no mistake occurred), the total number of intervened trials, and the average number of intervened trials per case. On average, we perform about 3 (1.5) intervened trials per case in the AB and GAIA (GSMPlus) datasets, indicating that most cases contain multiple trials and multiple potential failure points, making them worthwhile targets for debugging."
        },
        {
            "title": "5.2 QUANTITATIVE EVALUATION RESULTS",
            "content": "Table 2 presents the experimental results for the failure-flipping metrics. For making failure cases more correct, the Trial Success Rate across all intervened trials is 17.6% for cases in WW, compared to higher success rate of 27.5% for GAIA-Level-1 cases. This difference can be explained by the fact that WW contains more challenging cases (e.g., Level-2/3 GAIA tasks). similar pattern is observed for the Progress Made metric: interventions yield 15.7% improvement (i.e., nearly one key milestone) in GAIA-Level-1, but considerably less progress in WW-AB and WW-GAIA."
        },
        {
            "title": "Preprint",
            "content": "WW-AB WW-GAIA GAIA-Level-1 GSMPlus Intervened Trials 72 99 63 198 Trial Success Rate 17.6% 17.6% 27.5% 49.0% Progress Made +0% +8.8% +15.7% - Table 2: Experimental results on failure-flipping metrics across settings. The table reports the number of Intervened Trials, the Trial Success Rate, and the average Progress Made. Notably, for WW-AB, almost no progress is achieved after intervention, suggesting that in some situations interventions may even hinder progress toward success. Finally, in the GSMPlus setting, DoVer achieves nearly 50% trial success rate, underscoring that its effectiveness generalizes well across datasets and agent frameworks. Note that the Progress Made metric cannot be computed for GSMPlus due to the absence of human-annotated solution steps in the dataset. WW-AB WW-GAIA GAIA-Level-1 Intervened Trials 72"
        },
        {
            "title": "Inconclusive",
            "content": "11 (15.3%) 16 (16.2%) 22 (34.9%) 48 (66.7%) 57 (57.6%) 18 (28.6%) Partially Validated 3 (4.2%) 5 (5.1%) 8 (12.7%)"
        },
        {
            "title": "Refuted",
            "content": "10 (13.9%) 21 (21.2%) 15 (23.8%) Table 3: Validation outcomes of failure hypotheses across datasets. The table reports the number and percentage of trials classified as Validated, Inconclusive, Partially Validated, or Refuted. Turning to the validation of failure hypotheses, Table 3 shows that for both WW-AB and WWGAIA, the proportions of Validated and Refuted hypotheses are similar, each around 15%, while the majority (about 60%) fall into the Inconclusive category. In contrast, GAIA-Level-1 exhibits higher rates of both validated and refuted hypotheses, with inconclusive cases reduced to about 30%. This pattern suggests that the more difficult cases in WW make it harder for the agent system to reliably carry out the intended interventions."
        },
        {
            "title": "5.3 ABLATION STUDY",
            "content": "Impact of Different DoVer Underlying Models. To test whether DoVer depends on proprietary frontier model, we vary its debugging model while keeping failure traces, the agent system, and all prompts fixed. In the WW-GAIA setting, we replace GPT-4o with two locally hosted open-source models, Qwen3-8B and Qwen3-32B in thinking mode. As shown in Table 4, Qwen3-8B recovers 11.3% of 77 trials and Qwen3-32B recovers 16.9% of 87 trials, compared to 17.6% for GPT-4o over 99 trials. These results show that (i) DoVer does not rely on single proprietary backend and works with medium-sized local models, and (ii) larger open-source models (e.g., Qwen3-32B) substantially narrow the gap to GPT-4o, underscoring DoVers generality and practicality for opensource deployments. DoVer Model Qwen3-8B (0-shot) Qwen3-8B (3-shot) Qwen3-32B (0-shot) GPT-4o (0-shot) Intervened Trials 77 77 87 99 Trial Success Rate 11.3% 14.3% 16.9% 17.6% Table 4: Ablation of DoVer models and few-shot prompting in the WW-GAIA setting. Effect of Few-Shot Prompting for Smaller DoVer Models. To assess whether prompt-based guidance can mitigate the limitations of smaller models, we compare DoVer performance with and without few-shot examples. In the WW-GAIA setting described above, we enhance the interventiongeneration prompt for the Qwen3-8B model by adding three manually curated few-shot examples, each demonstrating how the orchestrator refines an initially suboptimal instruction into clear and effective one, along with brief context before and after the intervention. The resulting trial success rate is reported as Qwen3-8B (3-shot) in Table 4. The results show that Qwen3-8B with this"
        },
        {
            "title": "Preprint",
            "content": "3-shot prompt achieves 14.3% trial success rate, compared to 11.3% in the zero-shot setting. This improvement indicates that even small models can benefit considerably from lightweight in-context supervision, suggesting that richer prompt design or future supervised or reinforcement learning on intervention data may further narrow the gap to larger models. Compare with other Self-Improvement Methods. To contextualize DoVers gains, we compare against two self-improvement style methods adapted from Self-Refine Madaan et al. (2023) and CRITIC Gou et al. (2023) in the WW-GAIA setting. In the Self-Refine-style baseline, the underlying LLM first critiques the final answer given the full session log and then generates revised answer in second pass. In the CRITIC-style baseline, we similarly elicit feedback from the final answer and log, but inject this feedback as an additional agent message and allow the agent system to run for one extra round so that all tools and sub-agents can react to it. Across all 26 failed WWGAIA cases, neither baseline is able to flip any failure into success (0% recovery), whereas DoVer recovers 17.6% of trials  (Table 2)  . Further examination reveals why these self-improvement methods fall short: trial trajectories between the initial failure point and the final answer are often long, noisy, and highly divergent, making end-of-trace refinement insufficient for reliably redirecting the system. In contrast, DoVer performs in-situ interventions at potential failure points, enabling timely and targeted corrections that are essential in multi-agent settings."
        },
        {
            "title": "5.4 QUALITATIVE CASE STUDIES",
            "content": "We next present two representative case studies corresponding to the Refuted and Inconclusive of the above intervention outcome category while leaving the other two categories in Appendix D. Each vignette briefly introduce the task context, the hypothesized failure point, the concrete intervention applied, and the observed outcome. (1) Refuted: Case 46 (Trial 1) in WW-GAIA - Task: What time was the Tri-Rail train that carried the most passengers on May 27, 2019, scheduled to arrive in Pompano Beach? - Diagnosis: The failure proposer hypothesized that the issue arose from WebSurfers inability to locate data file at particular step, even though WebSurfer had in fact already opened that file. - Intervention: WebSurfer was explicitly instructed to open the file that it had already accessed. - Effect: Although the intervention generator produced instructions consistent with the hypothesized failure, and the agents followed these instructions faithfully, the trials still failed. Because the agent correctly executed the intervention and the intervention accurately reflected the failure hypothesis, the resulting failure demonstrates that the original hypothesis was invalid. We therefore classify this trial as refuted. (2) Inconclusive: Case 21 (Trial 2) in WW-GAIA - Task: Locate the paper linked at the bottom of Carolyn Collins Petersens June 6, 2023 Universe Today article and identify the NASA award number that supported R. G. Arendt. - Diagnosis: The Orchestrator remained stuck in the planning phase and never activated WebSurfer or other agents. No evidence was collected, leaving the trial incomplete. - Intervention: The WebSurfer was told to bypass incremental scrolling and jump straight to the articles footer to scan for DOIs, arXiv links, or other research references. - Effect: The WebSurfer could not execute the scroll-to-bottom action. Instead, it performed single page scroll without reaching the references, so the plan failed to advance. The inconclusive outcome reflected tool constraints rather than wrong failure hypothesis or intervention. Correct strategy was blocked by limited execution abilities, pointing to the need for stronger action primitives. Across these four cases, DoVer exhibits all intended outcomes: (i) validated when targeted edit repairs the trajectory; (ii) partially validated when the edit induces measurable progress yet external frictions block completion; (iii) refuted when faithful execution of the edit leaves the failure state intact; and (iv) inconclusive when tooling prevents faithful execution of the intended intervention."
        },
        {
            "title": "5.5 ANALYSIS OF INCONCLUSIVE CASES AND TARGETED TOOL ENHANCEMENT",
            "content": "The 2967% Inconclusive cases in Table 3 reveal the limits of orchestrator-level interventions: many failures arise from sub-agent capability gaps the orchestrator cannot resolve. These cases highlight the boundaries of sub-agent competence and indicate the improvement directions. We thus view DoVer as part of larger debugging loop: (i) DoVer provides automated orchestrator-level interventions; (ii) unresolved failures surface sub-agent weaknesses requiring human or auto refinement. To illustrate the effectiveness of this DoVer automation + human-in-the-loop cycle, we analyzed the failure traces of inconclusive cases in WW-GAIA and discovered two recurring failure modes in WebSurfer: (i) missing scroll-to-bottom tool, causing repeated partial scrolling, and (ii) inability to process PDFs, leading to empty summaries. We implemented targeted fixes by adding the scrolling tool and revising PDF handling. After these upgrades, previously inconclusive Cases 20, 21, and 26 can now be solved using only DoVers orchestrator-level interventions. This shows that DoVer not only repairs recoverable failures but also surfaces concrete sub-agent bottlenecks. We see two future work directions depending on the degree of permissible system modification: (i) fully automated debugging loops, where DoVers insights feed into automated sub-agent improvement (e.g., using coding agent to implement fixes). (ii) capability-aware intervention generation, allowing DoVer to tailor interventions to known sub-agent limits when code changes are not allowed. Both directions support the goal of building robust, self-improving multi-agent systems."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced DoVer, an intervention-driven framework that reframes debugging in LLM-based multi-agent systems as do-then-verify process. Rather than relying solely on log-based attribution, DoVer operationalizes debugging by formulating and directly testing failure attribution hypotheses via targeted interventions. Across datasets from AssistantBench and GAIA under M1, DoVer recovers 1828% of previously failed trials, and it recovers 49% of failures in GSMPlus when applied within different agent framework (AG2). Beyond these recoveries, DoVer consistently produces measurable improvements and verifies or falsifies most hypotheses. This outcome-oriented perspective highlights intervention as practical and scalable tool for improving reliability, while reducing dependence on ambiguous human annotations. By bridging failure analysis with practical repair, DoVer takes step toward more robust, verifiable, and self-improving multi-agent systems."
        },
        {
            "title": "7 LIMITATIONS AND GENERALIZABILITY",
            "content": "Our empirical evaluation instantiates DoVer in two concrete agent frameworks, Magentic-One on GAIA/AssistantBench and an AutoGen2-based MathChat system on GSMPlus, so the reported numbers should be interpreted as evidence of feasibility rather than universal guarantees. The covered tasks (web-based information seeking and math problem solving), agent topologies (sequential, centrally orchestrated), and model families (GPT-4o and Qwen3) represent only subset of deployed agentic systems. We do not evaluate on long-running production workloads, domains with strict latency or cost constraints, or settings with safety-critical requirements. Methodologically, the two core components of DoVer, trial segmentation and intervention-based validation, are conceptually general, but our current instantiation imposes several preconditions. First, the agent framework must expose sufficiently rich interaction logs and checkpoint/replay interface so that we can reconstruct contiguous planningexecution trials and splice in edited messages. Integrating DoVer into systems without such facilities (e.g., asynchronous or black-box orchestrators) would require additional instrumentation or higher-level abstractions over the interaction history, and our experience in M1 and AutoGen2 shows that adding checkpointing still requires non-trivial engineering effort. Second, in this work we restrict interventions to the orchestrators textual messages; we neither modify sub-agent code nor synthesize new tools, which limits our ability to repair failures rooted in missing capabilities rather than mis-specified coordination. Finally, part of our analysis (milestone progress and intervention fulfilled labels) relies on LLM-as-a-judge assessments, which may introduce biases despite careful prompt design. Exploring broader domains and architectures, richer intervention spaces (e.g., tool augmentation or code changes), and more humangrounded evaluation protocols are potential directions for future work."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank the anonymous reviewers for their constructive comments and suggestions, which helped improve the clarity and quality of this work. We also thank Yanjie Gao and Anyan Chen for helpful discussions. We further thank the creators and maintainers of GAIA, AssistantBench, GSMPlus, Magentic-One, AutoGen2, AGDebugger, and related open-source tools and benchmarks for making their datasets and systems publicly available, which enabled the experiments in this paper."
        },
        {
            "title": "REFERENCES",
            "content": "Negar Arabzadeh, Siqing Huo, Nikhil Mehta, Qingyun Wu, Chi Wang, Ahmed Hassan Awadallah, Charles L. A. Clarke, and Julia Kiseleva. Assessing and verifying task utility in LLMIn Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Propowered applications. ceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 2186821888, Miami, Florida, USA, November 2024. Association for Computational Linguisdoi: 10.18653/v1/2024.emnlp-main.1219. URL https://aclanthology.org/ tics. 2024.emnlp-main.1219/. AutoGen2. Autogen 0.2: An open-source programming framework for agentic ai. https:// microsoft.github.io/autogen/0.2/, 2025. Accessed: 2025-09-01. Adi Banerjee, Anirudh Nair, and Tarik Borogovac. Where did it all go wrong? hierarchical look into multi-agent error attribution, 2025. URL https://arxiv.org/abs/2510.04886. Mert Cemri, Melissa Z. Pan, Shuyi Yang, Lakshya A. Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Why do multi-agent llm systems fail?, 2025. URL https://arxiv. org/abs/2503.13657. Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, and Rebecca Qian. TRAIL: trace reasoning and agentic issue localization. CoRR, abs/2505.08638, 2025. doi: 10.48550/ARXIV.2505.08638. URL https://doi.org/10.48550/arXiv.2505. 08638. Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang (Eric) Zhu, and In ProceedSaleema Amershi. ings of the 2025 CHI Conference on Human Factors in Computing Systems, CHI 25, pp. 115. ACM, April 2025. doi: 10.1145/3706598.3713581. URL http://dx.doi.org/10.1145/ 3706598.3713581. Interactive debugging and steering of multi-agent ai systems. Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. Magentic-one: generalist multi-agent system for solving complex tasks. CoRR, abs/2411.04468, 2024. doi: 10.48550/ARXIV.2411.04468. URL https://doi.org/ 10.48550/arXiv.2411.04468. Yu Ge, Linna Xie, Zhong Li, Yu Pei, and Tian Zhang. Who is introducing the failure? automatically attributing failures of multi-agent systems via spectrum analysis, 2025. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jurgen Schmidhuber. Metagpt: Meta programming for multi-agent In The Twelfth International Conference on Learning Representacollaborative framework. tions, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=VtmBAGCN7o."
        },
        {
            "title": "Preprint",
            "content": "Fanqi Kong, Ruijie Zhang, Huaxiao Yin, Guibin Zhang, Xiaofei Zhang, Ziang Chen, Zhaowei Zhang, Xiaoyuan Zhang, Song-Chun Zhu, and Xue Feng. Aegis: Automated error generation and attribution for multi-agent systems, 2025. URL https://arxiv.org/abs/2509.14295. LangChain. Langgraph:"
        },
        {
            "title": "Time travel",
            "content": "(checkpoints & forking) human-in-the-loop. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_ loop/time-travel/, 2025. Accessed: 2025-09-01. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: In Alice Oh, communicative agents for mind exploration of large language model society. Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ a3621ee907def47c1b952ade25c67698-Abstract-Conference.html. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. Gsm-plus: comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. arXiv preprint arXiv:2402.19255, 2024. Guoqing Ma, Jia Zhu, Hanghui Guo, Weijie Shi, Jiawei Shen, Jingjiang Liu, and Yidan Liang. Automatic failure attribution and critical step prediction method for multi-agent systems based on causal inference, 2025. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: In The Twelfth International Conference on Learning benchmark for general AI assistants. Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forumReact?id=fibxvahvs3. Alfin Wijaya Rahardja, Junwei Liu, Weitong Chen, Zhenpeng Chen, and Yiling Lou. Can agents fix agent issues? CoRR, abs/2505.20749, 2025. doi: 10.48550/ARXIV.2505.20749. URL https: //doi.org/10.48550/arXiv.2505.20749. Pat Rondon, Renyao Wei, Jose Cambronero, Jurgen Cito, Aaron Sun, Siddhant Sanyam, Michele In 47th Tufano, and Satish Chandra. Evaluating agent-based program repair at google. IEEE/ACM International Conference on Software Engineering: Software Engineering in Practice, SEIP@ICSE 2025, Ottawa, ON, Canada, April 27 - May 3, 2025, pp. 365376. IEEE, 2025. doi: 10.1109/ICSE-SEIP66354.2025.00038. URL https://doi.org/10.1109/ ICSE-SEIP66354.2025.00038. language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 1b44b878bb782e6954cd888628510e90-Abstract-Conference.html. SmolAgents. smolagents: barebones library for agents that think in code. https://github. com/huggingface/smolagents, 2025. Accessed: 2025-09-01. Alva West, Yixuan Weng, Minjun Zhu, Zhen Lin, Zhiyuan Ning, and Yue Zhang. Abduct, act, predict: Scaffolding causal inference for automated failure attribution in multi-agent systems, 2025. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023a. doi: 10.48550/ARXIV. 2308.08155. URL https://doi.org/10.48550/arXiv.2308.08155."
        },
        {
            "title": "Preprint",
            "content": "Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. Mathchat: Converse to tackle challenging math problems with llm agents. arXiv preprint arXiv:2306.01337, 2023b. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. Assistantbench: Can web agents solve realistic and time-consuming tasks? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 89388968. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.EMNLP-MAIN.505. URL https://doi.org/10.18653/v1/2024. emnlp-main.505. Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, and Shuicheng arXiv preprint Yan. Agentracer: Who is inducing failure in the llm agentic systems? arXiv:2509.03312, 2025a. Heng Zhang, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang, Lubin Gan, Yilei Yuan, and Jin Huang. Graphtracer: Graph-guided failure tracing in llm agents for robust multi-turn deep search, 2025b. URL https://arxiv.org/abs/2510.10581. Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, and Qingyun Wu. Which agent causes task failures and when? on automated failure attribution of LLM multi-agent systems. In Forty-second International Conference on Machine Learning, 2025c. URL https://openreview.net/ forum?id=GazlTYxZss. Chenyang Zhu, Spencer Hong, Jingyu Wu, Kushal Chawla, Charlotte Tang, Youbing Yin, Nathan Wolfe, Erin Babinsky, and Daben Liu. Raffles: Reasoning-based attribution of faults for llm systems, 2025a. Kunlun Zhu, Zijia Liu, Bingxuan Li, Muxin Tian, Yingxuan Yang, Jiaxun Zhang, Pengrui Han, Qipeng Xie, Fuyang Cui, Weijia Zhang, Xiaoteng Ma, Xiaodong Yu, Gowtham Ramesh, Jialian Wu, Zicheng Liu, Pan Lu, James Zou, and Jiaxuan You. Where llm agents fail and how they can learn from failures, 2025b. Mingchen Zhuge, Changsheng Zhao, Dylan R. Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, and Jurgen Schmidhuber. Agent-as-a-judge: Evaluate agents with agents. CoRR, abs/2410.10934, 2024. doi: 10.48550/ARXIV.2410.10934. URL https://doi.org/10. 48550/arXiv.2410.10934. REVISIT LOG-BASED FAILURE ATTRIBUTION ON THE WHO&WHEN"
        },
        {
            "title": "DATASET",
            "content": "In this section, we revisit the log-based failure attribution task on the Who&When (WW) dataset. We present the details of our reproduced evaluation on WW as well as two minimal, non-invasive prompt refinements. We further examine the ground-truth labels in WW, providing detailed annotation notes for each examined case and discussing sources of uncertainty during ground-truth annotation. Our reproduced evaluation on WW focuses on the All-at-Once (AAO) method, in which single LLM call is made over the full input log. While we also reproduced results for the other two methods, Step-by-Step and Binary Search introduced in WW, we focus on AAO because it requires only one LLM call and achieves performance comparable to the other methods. In our reproduction, we run experiments with both GPT-4o and GPT-5, repeating each run three times to"
        },
        {
            "title": "Algorithm Generated",
            "content": "Agent-Level Acc. Step-Level Acc. Agent-Level Acc. Step-Level Acc. Random Baseline (WW) Baseline (GPT-4o) + Step Index (GPT-4o) + Guidance (GPT-4o) + Guidance (GPT-5) 12.00 55.17 55.178.09 52.301.00 59.191.99 59.191.99 4.16 5.26 6.042.23 20.692.98 23.564.98 23.561. 29.10 54.33 55.162.71 58.733.46 57.411.83 62.431.66 19.06 12.50 15.282.70 40.474.20 35.453.58 45.771.65 Table 5: Reproduced evaluation results on the WW dataset using the All-at-Once method with the ground-truth annotation. Results are reported for both Hand-Crafted and Algorithm-Generated scenarios at agent-level and step-level accuracy. Rows show baseline results from WW, our reproduction with GPT-4o, and refinements with explicit step indices and guidance reminders, as well as the latest GPT-5 model. reduce randomness. Detailed results are shown in Table 5. Following WW, we include the groundtruth annotation in the AAO prompt (the With Ground-Truth setting); results without ground-truth are similar and omitted for brevity. Table 5 first lists the Random setting and the reported results in WW (denoted as Baseline). Examining the baseline failure cases yields two observations. First, for some cases the predicted failure step index is hallucinated, i.e., the index exceeds the total number of steps in the log. Specifically, we find that the percentages of cases exhibiting this issue are 13.8% and 20.6% for the Hand Crafted and Algorithm Generated scenarios, respectively. To mitigate this, we add explicit step indices to the failure log (see Figure 3). The corresponding results appear in the +Step Index row of Table 5. This simple change substantially improves step-level attribution accuracy. We additionally verify that no outputs have out-of-range indices after this modification. Second, the baseline AAO prompt does not explicitly restate the guidance used during ground-truth annotation, which may cause LLMs and human annotators to apply different criteria for the failure agent/step. To align the criteria, we embed concise reminder of the annotators guidance into the baseline prompt (see Figure 3). With this addition, attribution performance improves further; see the +Guidance row in Table 5. Despite these prompt refinements, step-level attribution accuracy remains only around 20%, even with the latest GPT-5 model. To better understand this gap, we compare model outputs with groundtruth labels and find that major contributing factor is uncertainty in the ground-truth annotation. As discussed in Section 3, we identify three sources of annotation uncertainty. Table 6 provides detailed annotation notes for each GAIA case presented in WW. For example, for Case 3, we list the step ranges for all 4 trial presented in the log. In each trial, we also provide the annotation notes for key steps (e.g., for the current ground-truth annotation Step 32, we agree that it can be potential error step). Brief trial summary for later Trials 2/3/4 are also given in which we explicitly point out whether new strategies of solving task are explored. Note that we exclude Case 25 due to ambiguity in its question (i.e., two different ways of interpreting the year of 2019). In our independent annotation, we also tag each case along the following dimensions: (i) whether the ground-truth failure step appears correct; (ii) whether alternative failure steps can be justified when the log contains multiple trials exploring different strategies; (iii) whether agent/step attribution is ambiguous; and (iv) whether API errors or flaky behavior are present. We then mark case as uncertain if any of the first three tags is positive. The full tagging results are summarized in Table 7, which also includes auxiliary information such as the number of trials, the GT failure step from WW, and the outputs from each model run. Outputs matching the GT failure step are highlighted in green. As shown, agreement between model outputs and GT labels is substantially higher for cases with uncertain GT annotation (i.e., where the Is Current GT Uncertain? column equals 1) than for cases with more certain GT annotation. This finding indicates that current log-based failure attribution can be confounded by uncertainty in ground-truth labels, motivating the interventionbased auto-debugging approach presented in the main text."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Extended prompt template: orange marks explicit step indices, and blue marks the embedded concise reminder of annotators guidance."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Detailed annotation notes for each GAIA case in WW. The table records trial-level observations, potential errors, ambiguous attributions, and API issues associated with the ground-truth labels."
        },
        {
            "title": "WW ID Annotation Notes",
            "content": "3 5 9 11 20 22 Trial 1 (Step 038): Step 32: current GT; potential error as WebSurfer did not directly scroll to the bottom as instructed. Trial 2 (Step 3965): Tried new strategy using the calendar feature but remained stuck navigating within the calendar. Steps 5355: Ambiguous Agent/Step Attribution; WebSurfer could not click the target year instructed by Orchestrator because the calendar tool lacks year filter. Trial 3 (Step 6687): Repeated path similar to Trial 1, proposing new strategy of leveraging URL patterns near the end. Trial 4 (Step 8892): Explored the URL-pattern strategy but encountered an LLM API error. Step 92: LLM API error due to content filter. Step 12: current GT; potential error due to missing OCR text. Steps 1216: Ambiguous Agent/Step Attribution; Orchestrator did not verify the OCR output but still instructed WebSurfer to proceed, and WebSurfer did not perform sanity check and guessed the answer. Trial 1 (025): Step 25: current GT; potential error due to unnecessary replanning. Trial 2 (2651): Repeated the strategy in Trial 1. Trial 3 (5274): Tried new website. Trial 4 (7594): Visited the same websites as before. Trial 1 (Step 038): Step 24: current GT; no obvious error. If attributing mistake to not recognizing clickable tabs, one may attribute earlier Steps such as 12/16/20. Trial 2 (3973): Repeated the same page-scrolling as Trial 1; tried new strategy of in-page search but got stuck. Step 67: potential error using the email feature for search. Trial 3 (74115): Made progress and identified the correct oldest flavor; stuck at inspecting the background photo. Trial 4 (116129): Tried directly searching for the target photo from other sources; terminated due to max rounds reached. Step 129: almost solved the task but stopped due to max rounds reached. Trial 1 (034): Step 3: current GT; no obvious error. Step 24: potential error; unable to open the downloaded PDF. Steps 3032: Ambiguous Agent/Step Attribution; Orchestrator asked FileSurfer to open paper that had not been downloaded; FileSurfer tried to open hallucinated file path. Trial 2 (3566): Still stuck downloading/opening the PDF; LLM API error due to content filter. Step 66: LLM API error due to content filter. Step 4: current GT; no obvious error. Step 24: LLM API error due to content filter. Step 4: current GT; no obvious error. Steps 1420: Ambiguous Agent/Step Attribution; Orchestrator instructed FileSurfer to process downloaded PDF that WebSurfer had not clearly downloaded, resulting in File not found. Step 23: LLM API error due to content filter."
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Continued from previous page\nWW ID Annotation Notes",
            "content": "27 41 46 47 51 58 7 12 14 24 Trial 1 (Step 030): Step 4: current GT; no obvious error. Steps 1820: Ambiguous Agent/Step Attribution; Orchestrator instructed FileSurfer to process downloaded PDF that WebSurfer had not clearly downloaded, resulting in File not found. Trial 2 (Step 3150): Encountered the same File not found issue, tried to resolve it, then terminated due to LLM API error. Step 50: LLM API error due to content filter. Trial 1 (037): Step 8: current GT; no obvious error. Step 16: potential error; access to the desired website was blocked by human verification. Trial 2 (3882): Tried different website; attempted new strategy of asking for help by submitting post; terminated due to LLM API error from content filter. Step 82: LLM API error due to content filter. Trial 1 (042): Step 32: current GT; if attributing to this step due to failure to retrieve the desired information, one may attribute earlier steps such as Step 20. Trial 2 (4393): Initially still stuck finding relevant information; later tried new strategy of contacting potential source via email but was unable to send it. Trial 3 (94123): Continued exploring direct-contact strategies; tried new strategy of live chat and phone call but did not succeed. Trial 4 (124129): Continued direct-contact strategy; terminated due to max rounds reached. Step 129: max rounds reached. Trial 1 (050): Step 24: potential error; FileSurfer did not follow the instruction to unzip file. Trial 2 (5166): Tried new strategy using ComputerTerminal to run code generated by Orchestrator; got wrong answer due to incorrect code. Step 51: current GT; potential error due to wrong code. Trial 1 (031): Step 5: current GT; potential error; FileSurfer unable to transcribe an MP3 audio file. Trial 2 (3246): Tried another device but was not successful. Trial 3 (4799): Attempted to use new strategy of using online service to transcribe audio; not successful. Trial 4 (100122): Continued exploring the online-service approach. Trial 1 (033): Step 4: current GT; no obvious error. Trial 2 (3467): Tried different website; stuck locating the desired information. Trial 3 (6894): Tried another website but still could not locate the desired information. Trial 4 (95128): Returned to the initially explored website; deeply explored its content; terminated due to max rounds reached. Step 128: max rounds reached. Trial 1 (022): Step 22: current GT; potential error due to unnecessary triggering of replanning. Trial 2 (2381): Repeated steps from Trial 1; made progress and explored new ways to find the Regression label but each attempt failed. Trial 3 (82105): Continued web search for the desired page but landed on the wrong page, leading to an incorrect final result. GT appears correct. GT appears correct. GT appears correct. GT appears correct."
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Continued from previous page\nWW ID Annotation Notes",
            "content": "26 29 33 34 37 42 43 45 49 53 54 Step 32: current GT; LLM API error due to content filter. Step 12: current GT; LLM API error due to content filter. Step 8: current GT; LLM API error due to content filter. Step 4: current GT; LLM API error due to content filter. Trial 1 (024): Step 4: current GT; potential error due to unspecified search query. Trial 2 (2458): Continued the same strategy as Trial 1. Step 58: LLM API error due to content filter. GT appears correct. GT appears correct. Step 20: current GT; LLM API error due to content filter. GT appears correct. GT appears correct. GT appears correct."
        },
        {
            "title": "B PROMPTS USED IN DOVER",
            "content": "In our debugging pipeline, we adopt layered prompt template design to enable systematic diagnosis and intervention. The Trial Segmenter first partitions the full session log into trials according to the planexecution structure, identifying the indices of the initial planning step and each major plan update. It outputs only the step indices and criteria for initial planning and update planning, providing anchor points for subsequent trial-level analysis. Next, the Failure Proposer summarizes each trial by extracting its plan and execution trajectory, reporting success or failure. In case of failure, it locates the earliest error step, identifies the responsible agent, and articulates the error reasoninformation that drives the subsequent generation of interventions. The Intervention Recommender then generates the minimal executable fix under the combined constraints of task description, ground-truth answer, and localized failure context (previous two steps + failure step). Its output is unified in JSON format, specifying both the intervention category and the proposed replacement text. In parallel, the Ground-Truth Milestone Extractor abstracts each problem and its answer into at most five tool-agnostic milestones, each containing order, title, action, and result, to serve as processoriented progress standard. The Milestone Evaluator then aligns the real execution trace with these milestones, classifying each as achieved, partial, or missed, and detecting whether new path was explored along with its feasibility and contributionthus providing quantitative measure of whether substantial progress was made. Finally, the module mislocalization or insufficient fix proposer is applied after re-running the system with the intervention, distinguishing between success after intervention, instruction not executed, and applied but mislocalized/insufficient fix, and providing alignment evidence to support hypothesis validation and close-loop feedback. ENABLING DOVER ON AUTOGEN To assess whether DoVer can be applied beyond Magentic-One, we integrated it with MathChat multi-agent system built on the AG2 framework for the GSMPlus experiments. This appendix summarizes how we added checkpoint and re-execution functions to AG2 and how DoVer reuses this mechanism with minimal changes. Overview of MathChat in AG2. MathChat in AG2 follows group-chat pattern rather than an explicit plannerexecutor loop as in M1. manager agent coordinates several specialized workers (e.g., problem solver, code executor, and verifier). At each turn, the manager selects the"
        },
        {
            "title": "Preprint",
            "content": "7 0 8 0 5 2 0 2 - c - 5 - 0 2 1 1 4 2 0 2 - 4 - 3 2 R 1 3 2 R 1 h t - o e k i i t C o I I t i s b u F - u C ? r O s a ? t r t"
        },
        {
            "title": "A\np\ne\nt",
            "content": "S / g ? i - u r t t e G D ? r k L e C ? t n D I"
        },
        {
            "title": "W\nW",
            "content": "9 3 6 1 6 2 8 2 1 0 2 4 3 2 0 2 4 9 3 6 1 6 8 2 1 0 2 4 2 9 1 0 2 9 3 6 1 6 2 8 2 1 0 2 4 3 2 0 2 4 9 2 1 9 2 1 9 2 9 5 5 1 1 9 3 4 6 4 1 1 6 1 0 1 4 4 9 2 2 1 1 2 1 4 6 1 9 5 5 1 1 9 3 6 1 4 1 1 6 1 6 0 4 9 2 2 1 1 2 4 2 6 1 9 5 5 1 1 9 4 6 1 4 1 1 6 1 4 6 4 9 2 2 1 2 1 4 2 6 1 4 1 6 1 2 4 3 6 1 0 2 4 2 3 6 4 5 1 1 4 6 1 0 3 1 6 1 4 4 3 4 2 1 8 2 4 2 6 1 4 1 6 1 3 4 4 9 6 1 6 8 4 5 4 4 4 6 1 3 1 6 1 4 4 4 8 2 2 1 6 2 1 4 6 1 4 6 1 2 4 4 4 4 4 6 1 4 6 5 4 4 4 6 1 1 1 6 1 4 4 4 9 2 2 1 4 2 1 4 2 6 2 3 2 1 5 2 4 2 3 4 4 8 2 3 1 5 4 2 2 8 6 1 4 1 2 3 2 1 8 4 4 9 2 1 0 2 2 1 4 2 5 1 1 4 4 2 1 2 2 4 2 4 3 1 1 1 1 1 1 1 2 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 5 9 1 1 0 2 1 2 2 7 2 1 4 6 4 7 4 1 5 6 8 5 7 2 1 4 1 4 2 6 9 2 3 3 4 3 7 3 2 4 3 5 4 9 4 3 5 4 5 20 , i i a s l - u f l s , t n r u , i o h t - o t e b h ."
        },
        {
            "title": "W\nW\nn\ni",
            "content": "I c h r t e i T : b n t n c r d . p l m s r r u h s t fi p - c a , r k fl P i t , i i a u m e s . r i t l h p e i t - o h t"
        },
        {
            "title": "Preprint",
            "content": "next speaker based on the conversation state. There is no dedicated planner agent; instead, new reasoning attempts or verification cycles emerge when different specialists are invoked. Enhancing AG2 with Checkpointing and Replay. Since AG2 does not natively support checkpointing and replay functions, we implemented lightweight checkpointing layer around the AG2 conversation manager. Specifically, after each agent turn, we serialize the full logical state needed to resume the run, including: the conversation history up to that step (messages, speaker identities, and any tool outputs); the configuration of all agents in the chat (roles, prompts, tool bindings); the underlying LLM configuration (model name, temperature, decoding parameters). For MathChat, there is no persistent external environment beyond these components, so we do not need to snapshot additional tool state. Each checkpoint is stored as structured object keyed by the step index, enabling us to load the system state at any past turn. We have released this AG2 enhancement with checkpointing and replay functions in our anonymous repository linked in this paper. Enabling DoVer on MathChat in AG2. To run DoVer interventions, we load the checkpoint corresponding to the target step, reconstruct the manager and agents from the serialized state, and then apply the intervention by editing the message at the target step (for example, replacing the managers instruction with the text proposed by DoVer). We then resume the conversation from the target step onward using the original AG2 run loop. Because interventions operate only at the message-passing layer over restored state, this integration does not require changes to AG2s core scheduling or tool interfaces; the DoVer pipeline, including trial segmentation and intervention generation, can be reused unchanged after adapting prompts to the MathChat setting. Implementation Effort. The AG2 integration required adding checkpoint-aware wrapper around the conversation manager and small amount of glue code to bridge checkpoints with In practice, this amounted to on the order of one thousand lines DoVers intervention executor. of code and few days of work by single engineer, aided by LLM-based coding assistance. Once this infrastructure is in place, plugging DoVer into new AG2-based multi-agent application mainly requires registering checkpoint capture, exposing replay entry point from given step index, and mapping DoVers intervention categories to concrete message edits at that step. Web-based Intervention Interface for AutoGen2 On top of this checkpoint and replay infrastructure, we implemented minimal web-based user interface for the AG2 MathChat system (Figure 4). The dashboard exposes the same DoVer functionality used in our experiments: the left panel lists past sessions and checkpoints, the top bar accepts new math problem, the central pane visualizes the multi-agent dialogue, the right Intervention pane lets user select step, edit the corresponding message or plan, and resume execution from that point, and the bottom area records continuation histories after each intervention. This interface is intended purely as lightweight visualization and human-in-the-loop debugging tool; no experiment logic depends on it. Guidelines for Enabling DoVer in Other Agent Frameworks. We highlight several design principles from the above integration experience: 1. Minimal-invasive Design: By wrapping core components (e.g., chat manager in AG2) with checkpoint-aware versions, we preserved existing architecture while enabling checkpointing. 2. Capture Full System State: We used LLM-assisted tooling to identify all relevant state variables (e.g., agent configs, LLM settings, conversation history), ensuring accurate restoration. 3. Intervention via Checkpoint Manipulation: Interventions operate directly on structured checkpoint data, enabling seamless reuse of restoration and replay logic. We hope these guidelines make it easier to integrate DoVer into wide range of agent systems."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Web-based intervention user interface for the AG2 MathChat system. (1) List of recorded math problem sessions. (2) Input box for submitting new math task. (3) Main conversation panel showing the multi-agent trace and intermediate reasoning. (4) Intervention panel, where user can select specific agent and step to edit the message or plan. (5) History panel showing checkpoints and continuations after interventions. SAMPLE VALIDATED AND PARTIALLY VALIDATED CASES (1) Validated: Case 3 (Trial 1) in WW-GAIA - Task: Identify the architectural firm associated with Chicago landmark referred to indirectly via NASA APOD entry from Aug. 17, 2015, as shown in Figure 1. - Diagnosis: The WebSurfer agent engaged in prolonged, aimless scrolling of the APOD archive and repeatedly missed the required date-bounded entries, despite receiving explicit guidance. - Intervention: We replaced unstructured scrolling with an in-archive, date-bounded keyword strategy: Search the APOD archive directly, restrict to August 17, 2015, and scan for city lights / horizon; report the entry title, date, and brief summary. - Effect: The system revised its plan to prioritize date/keyword filters, issued targeted query, and quickly converged along the reasoning chain Marquette (city) Jacques Marquette (namesake) Marquette Building (Chicago) Holabird & Roche (architects), yielding the correct first name Holabird. The failed trial flipped to success, validating both the hypothesis (strategy error) and the remedy (focused retrieval). (2) Partially Validated: Case 56 (Trial 3) in WW-GAIA - Task: According to Google Finance, when was the first year the Apple stock went above $50 (without adjusting for stock split)? - Diagnosis: Execution stalled while probing Alpha Vantage as source for unadjusted historical prices; the agent failed to complete source verification. - Intervention: We redirected exploration away from generic search results and toward Alpha Vantages homepage, instructing the agent to verify data availability and note access requirements. - Effect: The trajectory moved onto the correct information source, but repeated script errors and API key frictions blocked completion. The outcome was clear forward progress without final result and the hypothesis was partially validated since the re-planning was correct but execution was constrained by environment and tooling."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Trial segmenter prompt: log decomposition of full session into planningexecution trials."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Trial summarizer + failure proposer prompt: per-trial summary and root-cause localization, part 1."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Trial summarizer + failure proposer prompt: continuation of Figure 6, part 2."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Intervention recommender prompt: minimal, executable fix classification; JSON category and replacement text."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Ground-truth milestone extractor prompt: 5 tool-agnostic milestones."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Milestone evaluator prompt: progress vs milestones and new-path assessment."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Post-intervention outcome classifier prompt: mislocalization or insufficient fix proposer."
        }
    ],
    "affiliations": [
        "Institute of Neuroscience, State Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences",
        "Microsoft",
        "University of Chinese Academy of Sciences, School of Future Technology"
    ]
}