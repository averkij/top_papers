{
    "paper_title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries",
    "authors": [
        "Minghe Shen",
        "Zhuo Zhi",
        "Chonghan Liu",
        "Shuo Xing",
        "Zhengzhong Tu",
        "Che Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 3 0 1 7 0 0 . 1 1 5 2 : r Ariadne: Controllable Framework for Probing and Extending VLM Reasoning Boundaries Minghe Shen1, Zhuo Zhi1, Chonghan Liu2, Shuo Xing3, Zhengzhong Tu3, Che Liu4 1University College London 2University of California, Los Angeles 3Texas A&M University 4Imperial College London Corresponding author While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises critical question: can RL post-training truly extend the inherent capability boundary of base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, framework utilizing synthetic mazes for multi-step spatial reasoning with precisely controlled task difficulty (e.g., path length, turns). We employ this controllable setup to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) under difficulty-aware curriculum. Surprisingly, after RLVR training, the VLM attains over 50% accuracy on tasks where the base model scored 0%, indicating that RLVR can extend the base policys capability boundary within, but not fully beyond, the training data scope. Specifically, it generalizes to unseen numbers of turns but fails when the number of movement steps exceeds training exposure, revealing divergent phenomenon in spatial reasoning. Furthermore, we observe that in real-world settings, this boundary shifts: the model handles longer-step reasoning, likely due to noise-induced redundancy in natural scenarios. Despite being trained solely on synthetic mazes, Ariadne still yields notable zero-shot gains, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results suggest that our method not only broadens the models intrinsic limits but also enhances its generalization to realistic spatial reasoning tasks. We note that our study is confined to the post-training phase due to the opacity of pre-training data and hope this work inspires further exploration into specialized, capability-extending alignment. Codes, models, and resources are available at https://mingheshen.github.io/Ariadne. Project Homepage: https://mingheshen.github.io/Ariadne Date: November 13, 2025 Correspondence: che.liu21@imperial.ac.uk"
        },
        {
            "title": "1 Introduction",
            "content": "Reinforcement Learning (RL) has played pivotal role in advancing the reasoning capabilities of large language models (LLMs) [1, 2]. Recent work, such as DeepSeek-R1 [3], demonstrated that Group Relative Policy Optimization (GRPO), trained with Reinforcement Learning with Verified Rewards (RLVR) and without separate reward model, can effectively enhance complex reasoning abilities [3, 4]. Extending this paradigm to the vision-language domain [5, 6, 7], Vision-Language Models (VLMs) show strong reasoning potential after RL post-training. However, their evaluation remains largely confined to language-dominant tasks such as mathematics. key question thus arises: Can RLVR extend the inherent capability boundary of VLMs, particularly in visual-centric spatial reasoning? To create controllable scenario for evaluation, we select visual-spatial reasoning tasks such as path-finding mazes, which are well-suited for this study because they provide deterministic ground-truth paths for verifiable rewards [8, 9]. Moreover, maze complexity can be precisely adjusted through parameters such as path length and number of turns, enabling systematic probing of model reasoning behavior. To this end, we introduce Ariadne, fully controllable framework designed to probe and extend VLM reasoning boundaries. Ariadne enables fine-grained control over maze complexity while maintaining clear difficulty definitions and verifiable rewards. This design allows transparent analysis of base VLMs capability boundary and how RLVR extends it. Furthermore, we transfer RLVR-trained VLMs to real-world benchmarks for zero-shot evaluation, exploring whether reasoning learned from synthetic mazes generalizes to practical tasks. The main contributions of this work are summarized as follows: We propose Ariadne, controllable framework for probing and extending VLM reasoning boundaries via RLVR on synthetic maze tasks with precisely tunable difficulty. Ariadne achieves over 50% accuracy on maze problems where the base model scored 0%, indicating that RLVR can extend the base policys capability boundary within but not fully beyond the training domain, generalizing to unseen numbers of turns but failing on longer step sequences. We identify divergent phenomenon where real-world reasoning boundaries deviate from synthetic ones, as models generalize better to longer-step reasoning in noisy, natural environments. Despite being trained solely on synthetic mazes, Ariadne delivers consistent zero-shot gains on real-world benchmarks, MapBench [10] (16%) and ReasonMap [11] (24%), highlighting its effectiveness in enhancing spatial reasoning beyond training conditions."
        },
        {
            "title": "2.1 Reasoning in VLMs",
            "content": "VLMs have achieved substantial progress on tasks such as Visual Question Answering (VQA) and image captioning [12, 13, 14]. To strengthen reasoning capabilities, prior work has explored Chain of Thought (CoT) prompting [15] and the construction of Supervised Fine-Tuning (SFT) datasets enriched with steplevel rationales [14]. While these strategies enhance reasoning to some extent, they fall short of capturing human-like cognitive processes such as questioning and self-verification, and their effectiveness diminishes on complex reasoning tasks. Recent models such as DeepSeek-R1 [4] attempts to address this gap by combining cold-start initialization with RL, thereby acquiring higher-quality multimodal CoT reasoning and achieving state-of-the-art results on challenging visual reasoning benchmarks."
        },
        {
            "title": "2.2 Reasoning with RL",
            "content": "Despite the advances in VLMs, spatial and multi-step reasoning, particularly in navigation and spatial understanding, remains key challenge. To overcome this limitation, researchers have increasingly adopted RL-based approaches. Ji et al. applied GRPO with structured CoT supervision to spatial VQA and navigation tasks, showing that verifiable, rule-based rewards can significantly improve reasoning robustness [16]. Similarly, CoT-VLA extended CoT reasoning to visionlanguageaction models, where explicit intermediate reasoning steps yielded strong gains in robotic navigation [17]. In synthetic navigation domains, Mirowski et al. enhanced RL training with auxiliary tasks such as depth prediction and loop-closure detection, enabling efficient traversal of complex 3D mazes [9]. While these approaches have shown effectiveness, most rely on large-scale synthetic data or are narrowly tailored to specific architectures. Open questions remain regarding how VLMs adapt their reasoning strategies when systematically exposed to increasing task complexity and to what extent such improvements generalize beyond synthetic environments [11, 10]. In this work, we introduce controllable training framework, Ariadne, for exploring the visual reasoning boundary of VLMs. Ariadne enables precise manipulation of problem complexity while maintaining consistent logical structures and verifiable rewards, allowing both targeted capability enhancement and systematic analysis of generalization and failure modes. Finally, through evaluation on real-world map navigation datasets, we demonstrate that reasoning gains achieved in synthetic maze environments can effectively transfer to practical out-of-domain scenarios."
        },
        {
            "title": "3.1 Preliminaries of GRPO",
            "content": "Recent advancements in enhancing the reasoning capabilities of vision-language models (VLMs) have demonstrated that RL is powerful training strategy [4]. In this study, we adopt GRPO as our learning framework. GRPO operates by directly comparing groups of candidate responses, thereby eliminating the need for separate critic model. During training, GRPO generates set of candidate outputs o1, o2, . . . , oG for given question drawn . Each candidate response oi is then evaluated using reward from the dataset using the old policy πθold function R(oi, q), yielding reward ri. To assess the relative quality of the responses within the group, GRPO normalizes the rewards by computing their mean and standard deviation. The advantage Ai for each response is then calculated as: Ai = ri mean{r1, r2, . . . , rG} std{r1, r2, . . . , rG} GRPO optimizes the policy model πθ by maximizing the following objective: JGRPO(θ) = qD, {oi}G i=1πθold (Oq) (cid:32) (cid:34) 1 (cid:88) i=1 min πθ(oiq) πθold(oiq) , clip (cid:32) πθ(oiq) πθold(oiq) , 1 ϵ, 1 + ϵ (cid:33)(cid:33)(cid:35) (1) (2) where ϵ is the clipping hyper-parameter."
        },
        {
            "title": "3.2 Reward Function",
            "content": "We design reward function to measure the stepwise correctness of model-generated answers relative to ground truth reference. The function (Algorithm 1) assigns proportional rewards for both fully correct and partially correct answers, using the number of reasoning turns as scaling factor. Step Extraction. Each model completion is first standardized via format extraction function to ensure consistent formatting. Reasoning steps are represented as sequence of moves embedded in the format: <step content>. count turns function extracts these moves using regular expression, returning both the ordered list of moves and the number of turns, defined as the count of transitions between consecutive distinct moves. Reward Calculation. Let = [r1, r2, . . . , rm] be the predicted move sequence and = [a1, a2, . . . , an] be the ground truth. The reward is computed as: reward = (cid:40) 0.2 turns(A), 0.1 turns(A1:k), if = A, if only first moves match. This ensures: Full matches receive maximum proportional reward. Partial matches are rewarded according to the matching prefix length k. The conversational or reasoning complexity, measured by turns, scales the reward. Algorithm 1 Correctness Reward Function 1: function Correctness Reward(completions, answers) 2: 3: 4: rewards [] for all r, in (completions, answers) do r_moves, r_turns count_turns(r) a_moves, a_turns count_turns(a) if r_moves = a_moves then reward len(a_moves) 0.2 a_turns else length of matching prefix(r_moves, a_moves) k_turns count_turns(first moves of a) reward 0.1 k_turns 5: 6: 7: 8: 9: 10: end if rewards.append(reward) 11: 12: 13: 14: 15: 16: end function end for return rewards"
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "4.1.1 Dataset To control the difficulty of training samples, we design the distribution of maze step counts {1, 2, 3, 4, 5} based on an inverted Gaussian-like distribution centered at step count 3. Specifically, the sampling probability for each step count is defined as: (s) 1 exp , (3) (cid:18) (cid:19) (s µ)2 2σ2 where µ = 3 is set as the midpoint of the step range, and σ = 2 is manually chosen to control the spread. This design ensures that both simpler (12 steps) and more complex trajectories (45 steps) are sampled more frequently. The resulting empirical distribution approximates {21%, 18%, 16%, 18%, 21%}, respectively. We posit that frequent exposure to simple cases, where each step involves selecting from up to four possible directions but is often constrained by walls blocking some paths, helps the model acquire stable and generalizable low-level navigation patterns by learning to effectively choose feasible moves in limited and structured action space. At the same time, challenging cases (with longer trajectories) drive the model to learn global spatial reasoning and coherent path planning over multiple steps. Additionally, test set is constructed from AlphaMaze by evenly sampling across the number of moves. Figure 1 illustrates the statistical properties of navigation trajectories in both the training and testing sets. Panels (A) and (C) depict the distributions of step lengths, reflecting the frequency of various movement distances during training and testing episodes, respectively. Panels (B) and (D) show the distributions of directional turns, emphasizing the complexity of orientation changes involved in path-finding across the two sets. During training, the number of moves is limited to 15 and the number of turns to 02. In the testing phase, these constraints are relaxed to 110 moves and 04 turns, enabling evaluation of generalizability on cases of higher difficulty beyond those seen in the training set. 4.1.2 Implementation We adopt Qwen2.5-VL-7B-Instruct [18] as our base policy VLM. In our Ariadne framework, Qwen2.5-VL-7BInstruct is trained with RLVR using 4,700 samples from the AlphaMaze [8] dataset with the GRPO algorithm, targeting enhanced reasoning performance in maze navigation tasks. Our reward function combines three Figure 1 (A) Step-length distribution in the AlphaMaze [8] training set, where the number of moves {1, 2, 3, 4, 5} is sampled according to an inverted Gaussian-like distribution centered at = 3, ensuring higher frequencies for both simple and complex cases. (B) Distribution of directional turns in the training set under the same controlled sampling scheme. (C) Step-length distribution in the AlphaMaze [8] testing set, constructed via uniform sampling. (D) Distribution of directional turns in the testing set under the same controlled sampling scheme. factors: answer accuracy, answer format, and reasoning format, emphasizing correctness while maintaining response clarity and structure. Training is conducted on 8 NVIDIA A100 (40GB) GPUs with learning rate of 1e-6 for one iteration, using batch size of 1 per device and 16 gradient accumulation steps, totaling 722,000 steps. We apply warmup ratio of 0.05, and for each prompt, 8 candidate responses are sampled using temperature of 1.0. The clipping hyperparameter ϵ is set to 0.2 by default. During evaluation on AlphaMaze and MapBench, each prompt is tested with 8 independent rollout samples at temperature of 1.0, and the final results are averaged across these runs to ensure validity and reliability. 4.1.3 Prompt The following prompt defines navigation assistant designed for visual path-finding in AlphaMaze. Given maze image with green starting cell (O) and red target cell (T), the assistant must infer valid path that passes exclusively through open cells while avoiding black walls. System Prompt for AlphaMaze You are navigation assistant to solve visual path-finding tasks. Your goal is to infer valid path from visually marked starting point (green cell labeled O) to visually marked target (red cell labeled T) by analyzing the maze image. Rules: - The maze is composed of open paths and impassable black walls. - Movement is only allowed through open paths, not through walls. - You can move one step at time in the four cardinal directions: <up>, <down>, <left>, <right>. Output Format: Think through each step inside <think> and </think> tags. At each step: 1. Describe your current position based on visual layout and structure (e.g.,in corridor\", facing wall\", at crossroad\", turning corner\"). 2. Decide the next move, and explain your reasoning. 3. Move and continue the path. After your full reasoning, output only the final movement sequence using the allowed tokens: <up><down><left><right>"
        },
        {
            "title": "4.2 Benchmarks and Metrics",
            "content": "As illustrated in Figure 2, we utilize MapBench [10] and ReasonMap [11] to evaluate the visual reasoning capabilities of VLMs. MapBench consists of human-readable map navigation tasks curated from challenging real-world scenarios [10]. ReasonMap [11] evaluates multiple-step visual reasoning using high-resolution transit maps from global metropolitan areas such as Los Angeles, Toronto, and Beijing. It adopts two-tier evaluation protocol comprising short and long questions to measure both the correctness of the answers (via accuracy) and the quality of the answers (via proposed map score that reflects the feasibility and efficiency of the route). Figure 2 Illustrative examples from our two controlled benchmarks for path-finding evaluation. MapBench (left) features human-readable, outdoor navigation tasks derived from challenging real-world scenarios (e.g., mall navigation), designed to assess naturalistic instruction-following and local decision-making. ReasonMap (right) uses high-resolution transit maps from global metropolitan systems (e.g., Los Angeles, Toronto, Beijing), with two-tier evaluation (short vs. long questions) to probe both fine-grained visual comprehension and global route planning. Figure 3 Training reward dynamics and evaluation of path-following ability. (A) Reward curve during GRPO training, showing steady improvement in rewards and stable learning progress. (B, D) For Qwen2.5-VL-7B-Instruct, the success rate rapidly collapses to zero (at cases with 3 movement steps or 3 turns), while token length increases, suggesting that the VLM generates longer but unsuccessful trajectories. (C, E) In contrast, our Ariadne framework demonstrates remarkable improvement in success rates at the base VLM boundary, raising performance from 0% to 50% on 3-step cases and from 0% to over 10% on 3-turn cases. Token length grows moderately, and the collapse point shifts from 3 to 5, reflecting that after RLVR training, the VLM succeeds on tasks where the base model consistently failed, indicating an extended reasoning boundary. In MapBench [10], the models generate navigation response given map image and query specifying the starting and ending landmarks. The final performance is defined as the ratio between the length of the model-generated path and the length of the shortest path of the ground-truth, which serves as metric of the efficiency of the route. For ReasonMap [11], accuracy is evaluated through comprehensive validation process that verifies the departure and arrival stops, matches each segments route name with the map metadata, confirms the validity of intermediate stops, and ensures transfer consistency. An answer is considered correct only if all criteria are satisfied. For short questions, the map score emphasizes route and endpoint consistency, while for long questions it additionally accounts for the number of stops and the correctness of specific via stops. Both accuracy and map score are weighted according to question and map difficulty, enabling difficulty-aware evaluation."
        },
        {
            "title": "4.3 Main Results",
            "content": "4.3.1 RLVR Enables VLMs to Break Reasoning Boundaries Figure 3 presents the reward trajectory during GRPO training on AlphaMaze, along with quantitative evaluations on the test set. Across all evaluated movement step ranges, Ariadne shows consistent advantage over its base model, Qwen2.5-VL-7B-Instruct [18]. For Qwen2.5-VL-7B-Instruct, we perform eight rollouts to confirm model stability and observe that performance drops to zero when the path requires three movement steps or three turns, which we define as the models initial reasoning boundary. After RLVR training, Ariadnes success rate rises to over 50% on 3-step cases and over 10% on 3-turn cases, with the collapse point shifting from 3 to 5. This shows that RLVR extends the reasoning boundary partially: the model generalizes to unseen numbers of turns but struggles when the number of movement steps exceeds its training range. We term this partially extended reasoning boundary and divergent generalization phenomenon, indicating that the improvement occurs along specific dimensions rather than uniformly across spatial factors. Figure 4 Representative success (top row) and failure (bottom row) cases from the AlphaMaze test set under controlled step sizes (4, 6, 8). Success cases generally correspond to smoother layouts with limited detour requirements, enabling coherent long-range navigation. Failure cases, by contrast, arise in locally complex structures characterized by dense turns, narrow passages, and elongated detours, which challenge the models ability to maintain global path consistency. The qualitative examples in Figure 4 further illustrate these dynamics. Successful trajectories typically occur in mazes with simple layouts and moderate detours, where spatial coherence can be preserved. In contrast, failure cases are concentrated in locally complex regions with dense turns, narrow corridors, or elongated detours, which disrupt global navigation consistency and highlight the limits of Ariadnes extended but still bounded spatial reasoning capability. 4.3.2 RLVR Extends VLM Reasoning to Real-world Tasks As shown in Figure 5, the two models exhibit clear divergence in real-world map-based navigation across diverse task settings. The baseline Qwen2.5-VL-7B-Instruct [18] exhibits localization and planning errors, such as misidentifying targets, taking inefficient detours, or violating environmental constraints like rivers and walls, leading to incomplete or infeasible trajectories. In contrast, Ariadne employs fine-grained, node-level planning that maintains goal-directed navigation while respecting structural boundaries. This behavior generalizes across both unstructured outdoor networks (e.g., trails) and structured indoor layouts (e.g., museums), demonstrating the effectiveness of GRPO adaptations beyond synthetic mazes. To further assess the transferability of these navigation gains to broader reasoning abilities, we evaluate both models on ReasonMap, benchmark that probes stepwise reasoning under varying difficulty levels (Figure 6). The dataset includes 55 samples for easyeasy, 46 for easymiddle, 28 for middleeasy, 7 for hardeasy, 23 for middlemiddle, 80 for easyhard, 1 for hardmiddle, 57 for middlehard, and 15 for hardhard. The first term indicates question difficulty, while the second term represents map difficulty. The results show that Figure 5 Navigation results on MapBench [10]. Left: Trail task with an unstructured outdoor layout. Right: Museum task with structured indoor layout. The baseline Qwen2.5-VL-7B-Instruct model [18] (red) produces incomplete or less feasible trajectories due to issues such as target misidentification, inefficient detours, or violations of environmental constraints. In comparison, Ariadne (green) demonstrates more consistent and goal-directed planning while adhering to structural boundaries. Figure 6 Performance comparison on ReasonMap across varying question and map difficulty levels. (a) Accuracy comparison showing that Ariadne consistently outperforms Qwen2.5-VL-7B-Instruct after GRPO training. (bd) Average map scores across different combinations of question and map difficulty, illustrating that Ariadne achieves higher scores in most settings, particularly in more complex reasoning categories. Ariadne achieves consistent gains in long reasoning regimes, particularly in higher-complexity categories such as easymiddle and easyhard, which correspond to question complexity and map complexity, respectively. For instance, in the easyhard category, it achieves both higher average map score (5.17 vs. 4.65) and accuracy (6.67% vs. 5%) compared to the baseline. These improvements are not observed in short reasoning settings, where performance between the two models remains comparable. Overall, the findings suggest that the maze-specific GRPO training paradigm enhances not only real-world navigation robustness but also the models capacity for multi-step, long-horizon reasoning, an ability directly relevant to generalizing beyond controlled synthetic tasks. Figure 7 further examines this phenomenon on the MapBench benchmark. Ariadne, trained only on trajectories with 15 steps, maintains strong performance within this in-distribution range while showing selective generalization to out-of-distribution paths beyond six steps. Although the models training boundary is Figure 7 Distribution of successful trajectory step counts generated by Ariadne and Qwen2.5-VL-7B-Instruct on the MapBench benchmark. Ariadne is trained exclusively on trajectories with 1 to 5 steps. This range is marked as In-Distribution (Step Counts) (light green). The Out-of-Distribution (Step Counts) region (dark green) evaluates the models ability to generalize to longer, unseen path lengths (6 steps and beyond). constrained to short-step trajectories, it still achieves nontrivial success on longer, unseen paths, indicating partially extended reasoning boundary. This divergent generalization suggests that real-world spatial reasoning may be influenced by noisy or redundant structures, causing the effective reasoning boundary to become misaligned with that observed in controlled synthetic settings. Table 1 Performance comparison on MapBench. Bold indicates the best performance. Model Qwen2.5-VL-7B-Instruct Ariadne Google Map Mall Museum National Park Theme Park Trail Campus Urban Zoo 1.68 1. 1.86 1.43 1.61 1.93 1.62 1. 2.29 1.30 1.41 1.33 1.48 1. 1.47 1.29 1.91 1.32 Table 2 Performance comparison on ReasonMap. indicates short questions, indicates long questions. Bold indicates the best performance. Model Weighted Acc. (S) #Tokens (S) Weighted Acc. (L) #Tokens (L) Weighted Map Score (S / L) Qwen2.5-VL-7B-Instruct Ariadne 13.32% 14.50% 26 43 6.00% 7.47% 61 3.73 / 4.51 3.67 / 5.15 Overall, Ariadne achieves strong performance on MapBench and ReasonMap (Tables 1 and 2). The model generalizes well across diverse spatial layouts, from unstructured outdoor networks to structured indoor grids, and demonstrates clear gains in long-horizon, multi-turn reasoning. These improvements are most apparent under high path complexity and extended reasoning chains, where instruction-tuned baselines degrade notably. Together, the results suggest that GRPO training on synthetic maze data effectively enhances spatial-visual reasoning, improving the models capability to handle complex real-world tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced Ariadne, controllable framework for systematically probing and extending the visual reasoning capabilities of VLMs through RLVR. Our experiments show that RLVR can effectively extend the reasoning boundary of VLMs, allowing models trained on short and simple trajectories to generalize to longer and more complex cases. However, this extension is partial and dimension-specific: the model generalizes well to unseen numbers of turns but fails when the number of movement steps exceeds its training range. This behavior represents divergent generalization phenomenon, where reasoning improvements emerge unevenly across task dimensions rather than through uniform expansion of capability. We further find that the reasoning boundary observed in real-world scenarios does not align with that in synthetic environments; models perform well on OOD step-count reasoning under noisy or redundant conditions, suggesting that environmental complexity may shift the effective boundary of spatial reasoning. Despite these nuances, the learned improvements transfer robustly to real-world settings, even under visual domain shifts, demonstrating that visual reasoning skills acquired in synthetic environments remain transferable across diverse contexts. Nonetheless, performance declines sharply beyond critical complexity threshold, implying that RLVR primarily strengthens and reorganizes reasoning patterns inherited from pretraining rather than creating entirely new mechanisms. Future work will investigate how controllable reinforcement learning can be integrated into the pretraining stage to further expand the fundamental reasoning capacity of VLMs."
        },
        {
            "title": "References",
            "content": "[1] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe, Training language models to follow instructions with human feedback, in Proceedings of the International Conference on Neural Information Processing Systems, 2022. [2] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [3] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, and S. S. Li, Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [4] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. [5] W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Z. Xu, Y. Hu, and S. Lin, Vision-r1: Incentivizing reasoning capability in multimodal large language models, arXiv preprint arXiv:2503.06749, 2025. [6] H. Wang, C. Qu, Z. Huang, W. Chu, F. Lin, and W. Chen, Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning, arXiv preprint arXiv:2504.08837, 2025. [7] Z. Wan, Z. Dou, C. Liu, Y. Zhang, D. Cui, Q. Zhao, H. Shen, J. Xiong, Y. Xin, Y. Jiang et al., Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning, arXiv preprint arXiv:2506.01713, 2025. [8] A. Dao and D. B. Vu, Alphamaze: Enhancing large language models spatial intelligence via grpo, arXiv preprint arXiv:2502.14669, 2025. [9] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran, and R. Hadsell, Learning to navigate in complex environments, in Proceedings of International Conference on Learning Representations, 2017. [10] S. Xing, Z. Sun, S. Xie, K. Chen, Y. Huang, Y. Wang, J. Li, D. Song, and Z. Tu, Can large vision language models read maps like human? arXiv preprint arXiv:2503.14607, 2025. [11] S. Feng, S. Wang, S. Ouyang, L. Kong, Z. Song, J. Zhu, H. Wang, and X. Wang, Can mllms guide me home? benchmark study on fine-grained visual reasoning from transit maps, arXiv preprint arXiv:2505.18675, 2025. [12] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan, Flamingo: visual language model for few-shot learning, in Proceedings of Advances in Neural Information Processing Systems, 2022. [13] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in Proceedings of Conference on Neural Information Processing Systems, 2023. [14] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, H. Tian, Y. Duan, W. Su, J. Shao et al., Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, arXiv preprint arXiv:2504.10479, 2025. [15] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou, Chain of thought prompting elicits reasoning in large language models, in Proceedings of Advances in Neural Information Processing Systems, 2022. [16] B. Ji, S. Agrawal, Q. Tang, and Y. Wu, Enhancing spatial reasoning in vision-language models via chain-of-thought prompting and reinforcement learning, arXiv preprint arXiv:2507.13362, 2025. [17] Q. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han, C. Finn et al., Cot-vla: Visual chain-of-thought reasoning for vision-language-action models, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 17021713. [18] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin, Qwen2.5-vl technical report, arXiv preprint arXiv:2502.13923, 2025."
        }
    ],
    "affiliations": [
        "Imperial College London",
        "Texas A&M University",
        "University College London",
        "University of California, Los Angeles"
    ]
}