{
    "paper_title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding",
    "authors": [
        "Boqiang Zhang",
        "Kehan Li",
        "Zesen Cheng",
        "Zhiqiang Hu",
        "Yuqian Yuan",
        "Guanzheng Chen",
        "Sicong Leng",
        "Yuming Jiang",
        "Hang Zhang",
        "Xin Li",
        "Peng Jin",
        "Wenqi Zhang",
        "Fan Wang",
        "Lidong Bing",
        "Deli Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation model for image and video understanding. The core design philosophy of VideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the vision-centric training paradigm and vision-centric framework design. The key insight of our vision-centric training paradigm is that high-quality image-text data is crucial for both image and video understanding. Instead of preparing massive video-text datasets, we focus on constructing large-scale and high-quality image-text datasets. VideoLLaMA3 has four training stages: 1) vision-centric alignment stage, which warms up the vision encoder and projector; 2) vision-language pretraining stage, which jointly tunes the vision encoder, projector, and LLM with large-scale image-text data covering multiple types (including scene images, documents, charts) as well as text-only data. 3) multi-task fine-tuning stage, which incorporates image-text SFT data for downstream tasks and video-text data to establish a foundation for video understanding. 4) video-centric fine-tuning, which further improves the model's capability in video understanding. As for the framework design, to better capture fine-grained details in images, the pretrained vision encoder is adapted to encode images of varying sizes into vision tokens with corresponding numbers, rather than a fixed number of tokens. For video inputs, we reduce the number of vision tokens according to their similarity so that the representation of videos will be more precise and compact. Benefit from vision-centric designs, VideoLLaMA3 achieves compelling performances in both image and video understanding benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 6 0 1 3 1 . 1 0 5 2 : r VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, Deli Zhao DAMO Academy, Alibaba Group Equal Contribution Project Lead https://github.com/DAMO-NLP-SG/VideoLLaMA"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we propose VideoLLaMA3, more advanced multimodal foundation model for image and video understanding. The core design philosophy of VideoLLaMA3 is vision-centric. The meaning of visioncentric is two-fold: the vision-centric training paradigm and vision-centric framework design. The key insight of our vision-centric training paradigm is that high-quality image-text data is crucial for both image and video understanding. Instead of preparing massive video-text datasets, we focus on constructing large-scale and high-quality image-text datasets. VideoLLaMA3 has four training stages: 1) vision-centric alignment stage, which warms up the vision encoder and projector; 2) vision-language pretraining stage, which jointly tunes the vision encoder, projector, and LLM with large-scale image-text data covering multiple types (including scene images, documents, charts) as well as text-only data. 3) multi-task fine-tuning stage, which incorporates image-text SFT data for downstream tasks and video-text data to establish foundation for video understanding. 4) videocentric fine-tuning, which further improves the models capability in video understanding. As for the framework design, to better capture fine-grained details in images, the pretrained vision encoder is adapted to encode images of varying sizes into vision tokens with corresponding numbers, rather than fixed number of tokens. For video inputs, we reduce the number of vision tokens according to their similarity so that the representation of videos will be more precise and compact. Benefit from vision-centric designs, VideoLLaMA3 achieves compelling performances in both image and video understanding benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed the rapid growth of large language models (LLMs) [16], which significantly enhance natural language processing and understanding. The growth of LLMs enables intelligence at the language level. However, to progress further, we need intelligence Preprint. Work in progress. Figure 1: Performance Comparison of VideoLLaMA3 with the previous advanced image/video MLLM on various representative benchmarks. As shown in the figure, VideoLLaMA3 has achieved very competitive results on various benchmarks. Specifically, VideoLLaMA3 not only demonstrates strong video understanding capabilities (VideoMME, PerceptionTest, MLVU) but also maintains excellent document comprehension abilities (DocVQA) and multimodal mathematical reasoning skills (MathVista). Note that LLaVA-OneVision is only used for evaluating image benchmarks, while LLaVA-Video is only used for evaluating video benchmarks. that extends beyond language, as the world itself is inherently multimodal. Specifically, the model should be capable of perceiving both static scenes and dynamic environments, which necessitates the ability to understand images and videos. Building upon the success of LLMs, Multimodal LLMs (MLLMs) [710] have been proposed for multimodal understanding. Existing MLLMs [1134] have made significant progress in multimodal understanding. Image-centric MLLMs [28, 3032, 3537], leveraging high-quality image-text datasets [28, 38 43] that are easier to collect and curate, have demonstrated strong performance in image understanding, such as visual question answering, OCR, and document understanding. Beyond static content like images, video-centric MLLMs [22, 24, 27, 30, 32, 44] must tackle the added complexity of modeling the temporal dimension of videos, requiring them to handle dynamic content and capture dependencies across frames. This temporal complexity, combined with the need for large-scale video-text datasets that are often of lower quality and harder to annotate, makes video MLLMs more challenging. These challenges underscore the advantages of leveraging image understanding as foundation for video understanding. By extending the robust visual capabilities of image MLLMs, video models can focus on and better address the unique challenges of temporal and dynamic content modeling. Inherit from VideoLLaMA [45] and VideoLLaMA2 [46], VideoLLaMA3, more advanced multimodal foundation model, is proposed for both image and video understanding. We design VideoLLaMA3 in vision-centric way. Specifically, we propose vision-centric training paradigm and vision-centric framework designs. For the training paradigm, considering the intrinsic relationship between image and video modalities - where videos are essentially sequences of temporally correlated images, we prioritize the improvement of image understanding, which in turn enhances the performance of video understanding. Moreover, compared to video-text data, image-text data is easier to collect and ensures higher data quality. For vision-centric framework designs, we propose adapting the vision encoder to handle images of any resolution during the image understanding enhancement stage and tuning the encoder to efficiently embed video inputs. Our vision-centric training paradigm consists of four stages (Figure 2): 1) Vision Encoder Adaptation: This stage aligns the vision encoders feature space with LLMs. Inputs to the vision encoder are adapted from fixed to dynamic resolutions. Scene images with short captions are used to enhance the encoders performance, while document and scene text images are used to enable the encoder to capture fine-grained visual details. 2) VisionLanguage Pretraining: This stage establishes the foundation for multimodal understanding using detailed image-text data. Scene images are annotated with detailed captions, and document and chart data include extensive explanations. To enhance spatial reasoning, fine-grained image-text data with bounding boxes are utilized. small amount of text-only data is included to retain the models language capabilities. All parameters are unfrozen 2 Figure 2: Training paradigm of VideoLLaMA3. The training of VideoLLaMA3 has four stages: (1) Vision Encoder Adaptation, (2) Vision-Language Alignment, (3) Multi-task Fine-tuning, and (4) Video-centric Fine-tuning. during this stage. 3) Multi-Task Fine-Tuning: In this stage, the model is fine-tuned for downstream tasks, such as interactive question answering. Image-text data with questions and answers are employed, along with general video caption data to prepare the model for video perception. The use of general video caption data also surprisingly improves the performance of image understanding. 4) Video-centric Fine-tuning: This final stage enhances the models performance in video understanding and video question answering. Training data includes general videos, streaming videos, videos annotated with temporal grounding information, image-only and text-only data. On the model side, we enhance the vision encoder with two vision-centric designs: 1) we adapt the vision encoder to take images with dynamic resolutions as inputs, and 2) we lift the vision encoder to receive videos and compress the video tokens into more compacted representations. In previous methods [28, 31, 32, 47, 48], vision tokens are either with fixed numbers or with numbers among several fixed choices, which is an inflexible and unnatural way to represent images. To alleviate this limitation, we adapt the pretrained vision encoder to receive images with variable shapes. This is achieved by replacing the fixed positional embeddings with the Rotary Position Embedding (RoPE). We finetune the vision encoder in the vision encoder adaptation stage so that it can accommodate dynamic inputs. In this way, enabling it to process high-resolution images and images with unusual aspect ratios with minimal information loss. As for video inputs, we consider the redundant information in videos and propose to reduce the number of vision tokens to represent video. The advantages of vision token compression are two-fold. One is to make the visual embeddings of videos more compact and precise so that the model can focus more on the dynamic parts of videos. The other is to save computation demands during training and inference for video understanding. Thanks to the vision-centric training paradigm and framework designs, our proposed VideoLLaMA3 achieves state-of-the-art performance on both image and video understanding benchmarks (Figure 1). Notably, in image understanding, the performance in chart understanding and vision-related math problems surpasses state-of-the-art models by large margin. While in video understanding, our model achieves state-of-the-art performance in most benchmarks including general video understanding, long video understanding, temporal reasoning and grounding. To summarize, the key contributions of VideoLLaMA3 include: We propose VideoLLaMA3, more advanced multimodal foundation model, for both image and video understanding. The model achieves state-of-the-art performance on most image and video understanding benchmarks. Notably, VideoLLaMA3 has significant improvements compared to previous versions of VideoLLaMA. 3 Figure 3: The overall pipeline of our VideoLLaMA3. There are two key technical points: ❶ Any-resolution Vision Tokenization (AVT): AVT converts images or videos of any resolution into set of 1-D token sequences, enabling compatibility with varying amounts of input images and videos of different resolutions, thereby supporting more flexible vision input; ❷ Differential Frame Pruner (DiffFP): Serving as video compressor, DiffFP eliminates video content with minimal differences between adjacent frames. This approach enhances video processing efficiency, particularly for long-form videos. We propose the vision-centric training paradigm. Specifically, we propose to improve video understanding capabilities through large-scale image understanding pretraining. We propose two vision-centric framework designs to adapt vision encoders to represent images and videos better."
        },
        {
            "title": "2 Methodology",
            "content": "As shown in Figure 3, on the model side, VideoLLaMA3 consists of two key technical points: ❶ Any-resolution Vision Tokenization (AVT) and ❷ Differential Frame Pruner (DiffFP). When it comes to data, since we propose to improve video understanding capabilities based on image understanding, we also develop pipeline for constructing high-quality re-captioned image dataset."
        },
        {
            "title": "2.1 Any-resolution Vision Tokenization",
            "content": "In MLLMs, visual inputs are extracted into vision tokens for multimodal understanding. The common practice [47, 48] is to extract visual inputs with pre-trained ViT-based vision encoder. The pre-trained vision encoder only receives images with fixed resolutions, which introduces information loss. To alleviate information loss, AnyRes techniques [28, 31, 32] are proposed to split images into patches with fixed resolutions. Although AnyRes techniques increase the number of vision tokens, it is still inflexible and neglects the position relationship within an image when extracting vision tokens. In VideoLLaMA3, we adopt the idea of Any-resolution Vision Tokenization (AVT) [30, 49] to dynamically process images and videos of any resolution. Concretely, we adapt the pre-trained vision encoder (ViT-based 4 Figure 4: The calculation flow of our DiffFP. We prune video tokens based on patch similarities in pixel space, removing patches with smaller distances to the previous frame. architectures) to handle variable resolutions by employing strategy to replace the absolute position embeddings in ViT with 2D-RoPE [50]. With AVT, images and videos of different resolutions are better represented with more details included in vision tokens. To make the vision encoder compatible with AVT, we fine-tune the vision encoder and the projector in the stage of Vision Encoder Adaptation (i.e., stage #1 in Figure 2) using scene images, document data, and scene images with texts."
        },
        {
            "title": "2.2 Differential Frame Pruner",
            "content": "For videos, inputs which usually have much more tokens than image inputs after tokenization, to reduce the computation demand for videos, we apply per-frame 22 spatial downsampling by bilinear interpolation to limit the context length within certain range. Besides, considering that videos consist of frames with overlapping content, representing videos by stacking vision tokens from each frame leads to lengthy and redundant tokens. To further reduce the number of tokens of videos, we propose the Differential Frame Pruner (DiffFP) to prune the video tokens. Inspired by RLT [51], we compare the 1-norm distance between temporally consecutive patches within the pixel space. We consider temporally consecutive patches with smaller distances to be redundant, and the later patches can be pruned. Specifically, as shown in Figure 4, we first calculate the 1-norm distance between consecutive frames in the pixel space and then remove patches whose distances fall below pre-defined threshold. Following RLT [51], we set the default threshold to 0.1."
        },
        {
            "title": "2.3 Construction of High-Quality Image Re-Caption Dataset",
            "content": "To train our VideoLLaMA3, we constructed high-quality image re-caption dataset, VL3Syn7M. All images in this dataset are sourced from COYO-700M [52] and processed using our proposed cleaning pipeline as below: 1) Aspect Ratio Filtering. We begin by filtering images based on their aspect ratios, removing those with extreme values. This step ensures that the dataset contains images with typical aspect ratios, preventing potential biases during feature extraction. For instance, images that are excessively long or wide may distort the models interpretation due to their unusual shapes. 2) Aesthetic Score Filtering. An aesthetic scoring model is applied to evaluate the visual quality of the images. Based on these scores, images with low aesthetic ratings are discarded. This step eliminates visually poor or poorly composed images, reducing noise and improving the quality of the descriptions generated by the model. 3) Text-Image Similarity Calculation with Coarse Captioning. The BLIP2 model is used to generate initial captions for images, followed by calculating the text-image similarity 5 using the CLIP model. Images with low similarity are excluded, as they are likely to contain content that is challenging to describe concisely. This process ensures that the remaining images are both descriptive and interpretable. 4) Visual Feature Clustering. Visual features are extracted using the CLIP vision model, and k-Nearest-Neighbors (KNN) algorithm is applied for clustering. This method identifies cluster centers in the visual feature space. From each cluster, we select fixed number of images. This approach ensures diversity within the dataset while maintaining balanced distribution of semantic categories, improving the models ability to generalize across various visual content. 5) Image Re-caption. After filtering and clustering the images, we proceed with detailed re-captioning. Brief captions are generated using InternVL2-8B [31, 53], while the detailed captions are produced with InternVL2-26B [31, 53]. These two types of captions (VL3Syn7M-short and VL3-Syn7-detailed) are employed at different stages of training to address varying needs. Through the aforementioned cleaning and re-caption process, we created the VL3-Syn7M dataset, which consists of 7 million image-caption pairs. This high-quality dataset will be crucial component in training our model, providing rich and diverse set of images and annotations that support strong performance across wide range of visual tasks."
        },
        {
            "title": "3 Training",
            "content": "As illustrated in Figure 3, VideoLLaMA3 consists of four key components: vision encoder, video compressor, projector, and large language model (LLM). The vision encoder extracts visual tokens and is initialized with the pre-trained SigLIP [54]. To reduce the number of vision tokens representing videos, video compressor is employed. The projector bridges the features between the vision encoder and the LLM. For the LLM, we utilize Qwen2.5 models [5]. Inspired by previous explorations in MLLMs [24, 28, 30], we develop video understanding capabilities based on strong image understanding foundations. To enable the model with strong image and video understanding capabilities simultaneously, the training of VideoLLaMA3 has four stages: 1) Vision Encoder Adaptation, 2) Vision-Language Alignment, 3) Massive Multi-task Fine-tuning, and 4) Video-centric Fine-tuning. While the first three stages primarily focus on improving image understanding, the final stage is dedicated to video understanding. The details of the training stages are as follows: 1) Vision Encoder Adaptation Stage. In this stage, we fine-tune the vision encoder, which is initialized with the pre-trained SigLIP [54], on large-scale image dataset. During this stage, the vision encoder is made trainable, while the language decoder remains frozen. This fine-tuning transforms the encoder into dynamic-resolution processor, enhancing its ability to process images of varying resolutions. Meanwhile, the projector is trained to better align the features of the vision encoder with those of the LLM. 2)Vision-Language Alignment Stage. This stage primarily focuses on introducing multimodal knowledge into the model. During this phase, all parameters are made trainable, enabling both the LLM and the vision encoder to be fine-tuned for integrating multimodal knowledge. 3) Multi-task Fine-tuning Stage. In this stage, we perform instruction fine-tuning using diverse set of multimodal question-answering data, which includes both image and videobased questions. This step is crucial for improving the models ability to follow natural language instructions and enhancing its multimodal understanding. Moreover, this stage lays the foundation for the models video understanding capabilities, enabling it to process and analyze temporal information. Also, in this stage, we introduce the video compressor to reduce the number of video tokens. 4) Video-centric Fine-tuning Stage. In this stage, we focus on enhancing the models video understanding capabilities. All parameters are unfreezed during this stage. The data used in this stage includes video-text data, image-only data and text-only data. 6 Figure 5: Data formats for different data types. ❶ For image sequence, we use \"n\" to separate image tokens from different image; ❷ For video sequence, we use \"Time: xxs\" to indicate timestamps of each frame, \",\" to separate different frames, and \"n\" to separate tokens from different videos; ❸ For streaming video sequence, videos and texts are organized in an interleaved format."
        },
        {
            "title": "3.1 Data Format",
            "content": "The data format for images, videos and streaming videos are shown in Figure 5. Image Sequence. Images are represented as sequence of tokens, referred to as Image Tokens. The character is used to separate tokens belonging to different images. Besides, text tokens follow image tokens, separated by n, enabling mixed representation of image and textual data. Video Sequence. Frames in video sequence are represented as Frame Tokens. Before tokens for each frame, Timestamp Token in the format \"Time: xxs\" is inserted to denote the time corresponding to that frame. Frames within video sequence are separated by commas \",\". After the video tokens, is inserted to separate the video data from any subsequent text tokens, ensuring clear distinction between the two modalities. Streaming Video Sequence. For streaming video data, video and text tokens are interleaved in the sequence. Timestamps (i.e., Time: xxs) are inserted before the frame tokens, similar to video sequences. To mimic the interactive scenarios of streaming videos, Answer tokens (i.e., GPT: xxx) may appear within the sequence to denote contextualized outputs or interactions. The interleaved format ensures seamless integration of video and textual data streams."
        },
        {
            "title": "3.2 Data Mixture",
            "content": "Following the principle outlined in LLaVA-OneVision [28], i.e., quality over quantity, we conduct rigorous cleaning procedures to guarantee data quality. In this section, we provide detailed description of the data mixture for each stage, as well as the synthesis and cleaning methods applied to different data subsets."
        },
        {
            "title": "3.2.1 Vision Encoder Adaptation",
            "content": "Table 1: Data mixture in vision encoder adaptation stage. Task Scene Image Dataset VL3-Syn7M-short, LLaVA-Pretrain-558k [55], Objects365-Recap [56], SA-1B-Recap [57] Amount 11.84M Scene Text Image BLIP3-OCR-Recap [58] Document pdfa-eng-wds [59], idl-wds [60] 0.93M 2.80M The Vision Encoder Adaptation stage is designed to enhance the models ability to comprehend wide range of diverse scenes and improve its feature extraction capacity, with particular focus on capturing fine-grained information such as objects, regions, and text. As shown in Table 1, the training data in this stage combines scene images and document recognition images, along with small portion of scene text images. It should be noted that all data labeled as \"Recap\" consists of captions generated with InternVL2-8B [31]. For scene images, our data sources include VL3-Syn7M-short, LLaVA-Pretrain-558K [55], Object365 [56], and SA-1B [57]. Notably, the Object365 and SA-1B datasets are included to enhance data diversity, as images in this dataset are mainly complex scenes. The scene text images are sourced from BLIP3-OCR [58]. Both the brief recaption and the text content within the images are used as captions, and the text content caption following left-to-right, top-to-bottom pattern across the image. The document images used in this stage are subset of pdfa-eng-wds [59] and idl-wds [60]. total of 2.8 million images were chosen from these two datasets, with the text content of the documents serving as image captions, following the reading order."
        },
        {
            "title": "3.2.2 Vision-Language Alignment",
            "content": "Table 2: Data mixture in vision-language alignment stage. Task Scene Image Dataset VL3-Syn7M-detailed, Objects365-Recap [56], SA-1B-Recap [57], COCO2017-Recap [62], ShareGPT4o ShareGPT4V [63], DenseFusion [64], LLaVA-ReCap (LCS-558K) [28] TextCaps [53], [61], Scene Text Image Laion-OCR [65], COCO-Text [66], TextOCR [67], BLIP3-OCRRecap [58], LSVT [68], ReCTS [69] Document SynthDoG-EN [70], SynthDoG-ZH [70], UReader-TR [71], FUNSD [72], DUDE [73], Vary-600k [74], pdfa-eng-wds [59], idl-wds [60] Chart Chart-to-Text [75] Fine-grained Text-only Osprey-724K [76], MDVP-Data [77], ADE20K-Recap [78], Object365 [56], Flickr-30K [79], GranD [80] Evol-Instruct-143K [81], Infinity-Instruct-code [82], Infinity-Instructcommonsense [82], Infinity-Instruct-math [82] Amount 12.56M 4.69M 2.68M 0.04M 1.00M 6.25M In this stage, we fine-tune the model using high-quality data. As shown in Table 2, we curate five types of data to cover wide range of everyday scenarios: scene images, scene text images, documents, charts, and fine-grained data, along with substantial amount of high-quality text-only data. For scene images, we include COCO-2017 [66], Object365 [56], SA-1B [57], ShareGPT4o [53], ShareGPT4V [63], DenseFusion [64], and LLaVA-Recap (LCS-558K) [28]. For Object365, COCO-2017, and SA-1B datasets, we combined the original image annotations with InternVL2-26B [31] to recaption and generate detailed image captions. The scene text images include diverse set of Chinese and English scene text recognition datasets. These datasets, such as BLIP3-OCR [58], COCO-Text [66], TextOCR [67], LSVT [68], and ReCTS [69], provide varied examples of text in real-world environments. Furthermore, we filter images from the LAION dataset [65] to include those with clear and readable text, resulting in collection of 3 million high-quality images, which we term as Laion-OCR dataset. For the Laion-OCR dataset captions, we include both the text content and the corresponding bounding box annotations of the text locations. The caption format is as follows: {Caption}. The texts in this image are {Text1}<box>[{Bounding Box 1}]</box>, {Text2}<box>[{Bounding Box 2}]</box>, ... As for document images, we include pdfa-eng-wds [59], idl-wds [60], UReader-TR [71], Vary600k [74], and SynthDoG [70]. SynthDoG dataset is constructed by generating synthetically accurate document images, avoiding human annotation errors and ensuring precise model 8 training. Furthermore, we add the handwritten document dataset FUNSD [72] and the complex document dataset DUDE [73]. FUNSD provides annotated handwritten samples for handwriting recognition, while DUDE includes documents with complex layouts, enhancing the models ability to handle variety of document types. For chart images, since charts share many similarities with documents in terms of content presentation, we only include limited amount of chart data. These data come from the Chart-to-Text [75] dataset. For fine-grained images, we construct two types of data: region caption data and grounded caption data. Region caption data describes the content of specific regions within an image. These data are derived and constructed from the Ospery-724K [76], Object365 [56], ADE20K [77], and MDVP-Data [78] datasets. These data help the model to understand the details of the image at the region level. Grounded caption data consist of textual descriptions of objects with corresponding bounding box annotations, primarily constructed from the Flickr-30K [79] and GranD [80] datasets. Both types of data enhance the models understanding of images, supporting more accurate object localization and recognition in complex scenes."
        },
        {
            "title": "3.2.3 Multi-task Fine-tuning",
            "content": "Table 3: Data mixture in massive multi-task fine-tuning stage. Task Dataset Image & Text Data General LLaVA-SFT-665K [38], LLaVA-OV-SI [28], Cambrian-cleaned [39], Pixmo (docs, cap, points, cap-qa, ask-model-anything) [35] Document DocVQA [40], Docmatix [41] Chart/Figure ChartQA [84], LRV_Instruction [85], ChartGemma [86], InfoVQA [87], PlotQA [88] MMC_Instruction DVQA [83], [42], OCR MultiUI [89], in-house data Grounding RefCoco [90], VCR [91], in-house data Multi-Image Demon-Full [92], Contrastive_Caption [93] Text-only Video Data General Magpie [94], Magpie-Pro [94], Synthia [95], subjective [82], NuminaMath [96] Infinity-InstructLLaVA-Video-178K [24], ShareGPT4o-Video [27], FineVideo [97], CinePile [98], ShareGemini-k400 [99], ShareGemini-WebVID [99], VCG-Human [21], VCG-Plus [21], VideoLLaMA2 in-house data, Temporal Grounding in-house data Amount 9.87M 1.31M 1.00M 0.83M 0.50M 0.41M 2.21M 2.92M In this stage, we perform instruction tuning with instruction-following data to refine the models ability to interpret and follow natural language instructions. This data mixture is designed to cover wide range of tasks, enabling the model to learn to perform various actions based on instructions across diverse contexts and modalities. Additionally, to activate the models video understanding capabilities, we incorporate general video data. Similar to the vision-language alignment stage, we divide the image data into six distinct groups: general, document, chart/figure, OCR, grounding, and multi-image, as shown in Table 3. Each category targets at specific aspect of visual understanding, ensuring the model can effectively handle tasks related to different types of visual information. Alongside these visual data categories, we also include substantial amount of text-only data to improve the models ability to handle diverse instruction-following tasks involving both visual and textual inputs. The general image data includes high-quality datasets, such as LLaVA-SFT-665K [55] and LLaVA-OV-SI [28], which serve as foundational resources for enhancing the models scene understanding. We also clean and filter the Cambrian-10M [39] dataset. Furthermore, we 9 incorporate meaningful data from the Pixmo dataset [35], including tasks such as document analysis, caption generation, and counting. These scene images cover wide range of tasks, including captioning, counting, document understanding, mathematical reasoning, and etc. For constructing the document and chart/figure datasets, we carefully select high-quality data sources and perform quality cleaning to ensure data reliability. It should be noted that the Docmatix dataset is included as it contains multi-page and diverse documents, crucial for significantly enhancing the models ability to understand and long complex document structures and content. For OCR data, we consider two common cases in real-world scenarios: development scenarios and natural scenarios. For development scenarios, we use the MultiUI dataset [89] to activate the models capabilities in understanding and processing text within user interfaces. For natural scenarios, we leverage the Laion-OCR dataset to construct additional instructiontuning data. The instruction-tuning data for OCR consists of the following five sub-tasks: 1) Text Existence Detection: Determine whether specific piece of text exists within the image. 2) Text Localization: Locate specific piece of text within the image and output its bounding box. 3) Text Recognition within Bounding Box: Given bounding box, recognize the text contained within it. 4) Text Comparison Between Images: Given two images, determine in which image the specified text appears. 5) Comprehensive Text Detection and Recognition: Detect and recognize all text present in the image. For grounding images, we select data from established datasets such as RefCOCO [90] and VCR [91], which focus on tasks of grounding visual elements in specific textual descriptions. For multi-image scenes, we leverage the Demon-Full [92] and Contrastive-Caption [93] datasets. The Demon-Full dataset is particularly valuable as it includes various tasks involving multi-image scenes, such as comparing differences between two images, generating captions for the final image in comic strip, completing missing text in images with occluded portions, determining whether multiple images belong to the same category, and more. These tasks help the model handle complex scenarios involving multiple images, providing more comprehensive understanding of how visual information can be interpreted across series of related images. At the same time, such multi-image data further enhances the models video understanding capabilities. For the video data used in this stage, we incorporate commonly used high-quality video caption datasets, along with small amount of question-answering data. In addition, we supplement these with high-quality data from VideoLLaMA2 [46] and in-house temporal grounding data. The in-house temporal grounding data specifically focuses on temporal relationships between video frames, enabling the model to grasp the sequence of events and understand the flow of actions across time. These combined data sources contribute to more robust and nuanced video understanding capability for the model."
        },
        {
            "title": "3.2.4 Video-centric Fine-tuning",
            "content": "Table 4: Data mixture in video-centric fine-tuning stage. Task General Video Dataset LLaVA-Video-178K [24], ShareGPT4o-Video [27], FineVideo [97], CinePile [98], ShareGemini-k400 [99], ShareGemini-WebVID [99], VCG-Human [21], VCG-Plus [21], VideoRefer [100], VideoLLaMA2 in-house data, In-house synthetic data Amount 3.03M Streaming Video ActivityNet [101], YouCook2 [102], Ego4D-narration [103], Ego4D36.2K Temporal Grounding Image-only Text-only livechat [104] ActivityNet [101], YouCook2 [102], ViTT [105], QuerYD [106], HiREST [107], Charades-STA [108], Moment-10M [109], COIN [110] LLaVA-SFT-665K [38], LLaVA-OV-SI [28] Magpie [94], Tulu 3 [111] 0.21M 0.88M 1.56M 10 The video-centric fine-tuning stage is designed to tune VideoLLaMA3 to video specialist and fully unleash its video understanding ability by focusing mainly on large-scale and highquality video instruction following. We first collect videos with generally annotated caption, question, and answer from multiple open-source datasets including LLaVA-Video [24], ShareGPT-4o [27], FineVideo [97], CinePile [98], ShareGemini [99], VideoGPT+ [21] and VideoRefer [100]. These about 2.7M video-centric conversations eventually form dataset across various scenes and tasks to serve as examples for teaching the model to understand complex dynamic and static content in videos. In addition, we further expand the data scale and strengthen the model by synthesizing dense captions and QAs of specific aspects. Specifically, following the pipeline proposed in [24], we first filter 68K dynamic videos from Panda-70M [112] dataset by optical flow, and then employ Qwen2-VL-72B [30] to generate diverse dense captions and QAs for each video from the aspects of temporal understanding, spatial understanding, object description, and time-order understanding. Finally, 242K question-answer pairs are used for training. Besides general video-centric conversations, we also introduce the feature of streaming video understanding and temporal grounding to extend the application scenarios of our model. For streaming video understanding, we acquire data from ActivityNet [101], YouCook2 [102], and Ego4D [103], and organize video frames and multiple temporal dense captions in an interleaved manner as described in Section 3.1, aiming at enhancing the ability to understand fine-grained events in video and to sustain multi-turn conversations in streaming video. Since these videos are generally long, we cut them into small segments of up to two minutes according to the time interval of dense captions, and remove clips with overly dense and sparse captions. The synthetic streaming conversation from VideoLLM-Online [104] is also involved. For temporal grounding, we collect 205K data from datasets including ActivityNet [101], YouCook2 [102], ViTT [105], QuerYD [106], HiREST [107], Charades-STA [108], Moment-10M [109], and COIN [110], and directly convert the grounding annotation to text format such as 1.0-2.0 for training. Finally, we emloy certain amount of image-only and text-only data from LLaVA [38], LLaVAOneVision [28], Magpie [94], and Tulu 3 [111] for mitigating the impact of catastrophic forgetting on the models capabilities. 3."
        },
        {
            "title": "Implementation Details",
            "content": "In this part, we briefly introduce the implementation details of each training stage. For all stages, we adopt the cosine learning rate scheduler. The warm up ratio of the learning rate is set as 0.03. The maximum token length is set as 16384, while the maximum token length for vision tokens is set as 10240. In the stage of Vision Encoder Adaptation, when training VideoLLaMA3-2B, we initialize the vision encoder with the pre-trained weights of SigLIP [54] and the LLM with the pre-trained weights of Qwen2.5-2B [5]. For VideoLLaMA37B, the vision encoder is initialized with the fine-tuned SigLIP weights in VideoLLaMA3-2B and the LLM is initialized with Qwen2.5-7B [5]. The projector is implemented as two-layer MLP with GELU as the activation function. In this stage, we only train the vision encoder and projector, and their learning rates are set as 1.0 105 and 1.0 103, respectively. For the remaining stages, the learning rates for the LLM, the projector, and the vision encoder are set as 1.0 105, 1.0 105, 2.0 106, respectively. The differential frame pruner is applied in the multi-task fine-tuning stage and the video-centric fine-tuning stage where video data is involved. The threshold to discard similar visual tokens is 0.1. To limit context length, the visual tokens of videos are spatially downsampled after vision encoder by factor of 2 using bilinear interpolation. The visual tokens of images are only downsampled in the video-centric fine-tuning stage to align with video data. For loading video data, we first sample frames at 1 frame per second using FFmpeg. These frames will be further sampled uniformly if the total number of frames is greater than certain value, which is set to 180 to accommodate most videos that last less than 3 minutes. 11 Table 5: Evaluation results of 2B models on image benchmarks. denotes the reproduced results. The best results are in bold and the second best ones are underlined. InternVL2.5 Qwen2-VL 2B 79.2 88.7 60.9 804 51.3 14. 23.7 43.6 44.0 60.1 74.9 59.5* 2005 2B 73.5 90.1 65.5 767 43. 12.4 26.0 41.1 43.1 62.9 69. 59.8 1872 VideoLLaMA3 2B 79.8 91.9 69.4 779 59.2 15.5 28.6 45.3 44. 67.3 78.2 62.7 1901 Benchmark Model SmolVLM 2B Document/Chart/Scene Text Understanding ChartQA 65.3* DocVQAtest InfoVQAtest OCRBench Math MathVistatestmini MathVisiontest Multi Image MMMU-Pro MMMUval BLINKtest Knowledge/General QA RealWorldQA AI2D GQA MME 81.6 - 622* 44. 6.5* 17.1* 38.8 42.3* 48.8* 62.1* 49.2* 1600*"
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Image-based Evaluation"
        },
        {
            "title": "4.1.1 Baselines",
            "content": "To comprehensively evaluate the image performance of VideoLLaMA3, we compare it against diverse set of baselines. For the 2B version of the model, we select several strong methods, including SmolVLM [37], InternVL2.5-2B [32], and Qwen2VL-2B [30]. For the 7B model, there are more options available. We choose to compare against Molmo-7B-D [35], InternVL2.5-8B [32], LLaVA-OneVision [28], NVILA [36], and Qwen2VL-8B [30]."
        },
        {
            "title": "4.1.2 Benchmarks",
            "content": "To evaluate the image recognition and perception capabilities of VideoLLaMA3, we conduct assessments on several representative benchmarks commonly used in Image-LLMs. These benchmarks cover four dimensions: document/chart/scene text understanding, mathematical reasoning, multi-image understanding, and general knowledge QA. Document/Chart/Scene Text Understanding. To evaluate VideoLLaMA3s ability to understand various forms of texts in images, including documents, charts, and scene text, we conduct assessments on range of benchmarks. Specifically, we use: 1) DocVQA [113] for document understanding, which evaluates the models ability to process and extract information from text in documents; 2) ChartQA [42] and InfoVQA [113] for chart understanding, assessing the models ability to interpret and reason about data presented in graphical forms such as bar charts and line graphs; and 3) OCRBench [114] for scene text image understanding, which tests the models capacity to extract and comprehend text from images of real-world scenes. Mathematical Reasoning. VideoLLaMA3s mathematical reasoning capabilities are evaluated through the MathVista [115] and MathVision [116] benchmarks. These benchmarks 12 Table 6: Evaluation results of 7B models on image benchmarks. denotes the reproduced results. denotes the results retrieved from the official leaderboard. The best results are in bold and the second best ones are underlined. Molmo-7B-D 7B L2.5 Intern 8B neVision LaVA-O 7B Document/Chart/Scene Text Understanding ChartQA 84.1 DocVQAtest InfoVQAtest OCRBench Math MathVistatestmini MathVisiontest Multi Image MMMU-Pro MMMUval BLINKtest Knowledge/General QA RealWorldQA AI2D GQA MME 92.2 72.6 - 51.6 - - 45.3 - 70. 93.2 - - 84.8 93.0 77.6 64.4 19.7 34.3 56.0 54.8 70.1 84. - 2344 80.0 87.5 68.8 63.2 - 24.1 48.8 48.2 66. 81.4 62.3 1998 VIL 8B 86.1 93. 70.7 676* 65.4 11.9* 29.5* 49. 47.0 68.6 92.3 - 2219 wen2-V 8B 83.0 94.5 76. 845 58.2 16.3 31.4 54.1 43.1 70.1 83.0 62.4 2327 A"
        },
        {
            "title": "LaM\nVideoL",
            "content": "7B 86.3 94.9 78.9 828 67.1 26.2 33.6 48.8 55.2 72.7 84. 64.9 2102 focus on evaluating the models ability to reason about and solve mathematical problems presented in visual formats, including text-based mathematical expressions and problemsolving tasks that require visual interpretation. Multi-image Understanding. To assess VideoLLaMA3s ability to understand and reason about multiple images in conjunction, we evaluate the model on several widely used benchmarks, including MMMU-Pro [117], MMMU [118], and BLINK [119]. These benchmarks test the models ability to draw connections between images, handle multiple visual inputs. General Knowledge QA. Finally, to evaluate VideoLLaMA3s performance in general question answering, particularly in real-world and complex scenarios, we conduct assessments using several challenging benchmarks. The benchmarks include: 1) RealWorldQA [120], which focuses on answering questions based on realistic images drawn from everyday scenarios, 2) AI2D [121], which evaluates the models ability to reason about diagrams and science images, 3) GQA [122], which assesses general question answering with focus on complex visual reasoning tasks, and 4) MME, which includes wide variety of general knowledge questions that require deep understanding of visual information."
        },
        {
            "title": "4.1.3 Evaluation Protocols",
            "content": "When evaluating on benchmarks, we set the temparature as 0.0. The maximum token length is set as the same as the training stage. For benchmarks involving the MCQ, we will give the prompt like Answer with the option letter from the given choices directly.. For the benchmarks with short answers, we will give the prompt like Answer the question with single word or phrase.. We follow the original benchmarks to calculate the final 13 Table 7: Evaluation results of 2B models on video benchmarks. * denotes the reproduced results. denotes the results retrieved from the official leaderboard. The best results are in bold and the second best ones are underlined. Benchmark Model Apollo 2B General Video Understanding VideoMME w/o sub VideoMME w/ sub MMVUval MVBench EgoSchematest PerceptionTesttest ActivityNet-QA Long Video Understanding MLVUdev LongVideoBenchval LVBench Temporal Reasoning TempCompass NextQA Charades-STA 53.0 54.6 - - - 61. - 63.3 - - 60.8 - - InternVL2.5 Qwen2-VL 2B 51.9 54. 33.6 68.8 58.1 66.3 54.1 58.9 52. 37.3 57.7 75.6 - 2B 55. 60.4 36.5 63.2 54.9 53.9 53.3 62.7 48.7 38.0 62.2 77.2 - VideoLLaMA3 2B 59.6 63.4 39.9 65.5 58.5 68.0 58.2 65.4 57.1 40.4 63.4 81.1 55.5 scores, and we also align our evaluation protocols with other evaluation toolkits, such as lmms-eval [123, 124] and VLMEvalKit [125]."
        },
        {
            "title": "4.1.4 Evaluation Results",
            "content": "We evaluate our VideoLLaMA3 model on the previously mentioned benchmarks. The evaluation results for our 2B model are presented in Table 5. As shown, VideoLLaMA3 demonstrates significant improvements across range of tasks compared to prior models. For example, in OCR benchmarks such as InfoVQA, VideoLLaMA3 achieves performance score of 69.4%, compared to the previous best score of 65.5%. In mathematical reasoning tasks, such as MathVista, our 2B model scores 59.2%, surpassing the state-of-the-art method by 7.9%. For multi-image benchmarks like MMMU-Pro, VideoLLaMA3 outperforms the previous top-performing method by 2.6%. In real-world knowledge QA tasks, such as RealWorldQA, VideoLLaMA3 achieves the highest performance with score of 67.3%, compared to 62.9% from prior methods. Similarly, we evaluate our larger 7B model on various image benchmarks, with results summarized in Table 6. From the table, it is clear that VideoLLaMA3 consistently outperforms prior models on most benchmarks. Notably, in mathematical reasoning tasks, our 7B model surpasses the previous best by 6.5% on MathVision. In chart understanding tasks, we observe 1.3% performance improvement over previous methods on InfoVQA. Additionally, in general reasoning tasks like RealWorldQA, VideoLLaMA3 outperforms prior models by 2.0%. Overall, the results confirm that VideoLLaMA3 provides consistent advancements across broad range of benchmarks, demonstrating its efficacy and versatility in handling complex tasks, including OCR, mathematical reasoning, and general knowledge. These improvements position VideoLLaMA3 as powerful tool for real-world applications, advancing the field of multi-modal learning. 14 Table 8: Evaluation results of 7B models on video benchmarks. * denotes the reproduced results. denotes the results retrieved from the official leaderboard. The best results are in bold and the second best ones are underlined. L2.5 Intern 8B 64.2 66.9 41.1 72.0 66.2 68.9 58.9 69.0 60.0 41.5 68.3 85.0 - LaVA-Video 7B 63.3 69.7 42.4 58.6 57. 67.9 56.5 70.8 58.2 40.3 65. 83.2 - VIL 8B 64.2 70.0 43.7 68.1 54.3 65.4 60.9 70.6 57. 42.6 69.7 82.2 - pollo 7B 61. 63.3 - - - - - 70.9 58.5 - 64.9 - - LaM VideoL 2.1-7B LaM VideoL 3-7B 54.9 56. 39.5 57.3 53.1 54.9 53.0 57. - 36.3 56.8 75.6 - 66.2 70.3 44.1 69.7 63. 72.8 61.3 73.0 59.8 43.7 68.1 84.5 60.7 wen2-V 7B General Video Understanding VideoMME w/o sub VideoMME w/ sub MMVUval MVBench EgoSchematest PerceptionTesttest ActivityNet-QA Long Video Understanding MLVUdev LongVideoBenchval LVBench Temporal Reasoning TempCompass NextQA Charades-STA 63.3 69.0 42.1 67. 66.7 62.3 57.4 69.8 55.6 44.2 67.9 81.2 -"
        },
        {
            "title": "4.2.1 Baselines",
            "content": "To comprehensively evaluate the video performance of VideoLLaMA3, we compare it with diverse set of baseline models. Similar to image evaluation, there are few available models with 2B parameter size in the community. We select several strong baselines, including Apollo-2B [14], InternVL2.5-2B [32], and Qwen2VL-2B [30]. For the 7B model, we compare it with general models such as Qwen2VL-8B [30], InternVL2.5-8B [32], and NVILA [36], as well as proprietary models like LLaVA-Video [24], Apollo-7B [14], and our previous generation model, VideoLLaMA2 [46]."
        },
        {
            "title": "4.2.2 Benchmarks",
            "content": "The video understanding capabilities of VideoLLaMA3 are systematically evaluated across three core dimensions: general understanding, temporal reasoning, and long-form video comprehension. General Video Understanding. We assess VideoLLaMA3s general video understanding capabilities through established benchmarks: (1) Multi-Choice Video Question Answering (MC-VQA) tasks, including MVBench [26], VideoMME [126], EgoSchema [127], and Perception-Test [128]. (2) Open-Ended Video Question Answering (OE-VQA) tasks, including ActivityNet-QA [129] and VCGBench [25]. This evaluation suite follows the protocol of VideoLLaMA2 [46]. We also run evaluations on MMVU [130] which includes both the task types mentioned above. Long Video Understanding. To further examine the capacity of VideoLLaMA3 to process and comprehend long-form video content, we assess performance on three long-video 15 understanding (LVU) benchmarks: (1) MLVU [131]: diverse long-video understanding tasks for videos ranging from 3 minutes to more than 2 hours, (2) LongVideoBench [132]: video reasoning over the referred context within long video-language interleaved inputs, and (3) LVBench [133]: extreme long video understanding. Video Temporal Reasoning. To assess the temporal awareness and reasoning capabilities of VideoLLaMA3, we conduct evaluations on the following tasks: (1) Temporal Perception and Reasoning tasks, including TempCompass [134] and NextQA [135]; and (2) Temporal Sentence Grounding task on Charades-STA [108] benchmark, with mean Intersection over Union (mIoU) metric."
        },
        {
            "title": "4.2.3 Evaluation Protocols",
            "content": "We expand the max number of visual tokens to 16K when evaluating our models on videobased benchmarks, ensuring that each frame corresponds to reasonable number of tokens and the total context length is within the maximum range of the base LLM. The maximum number of frames is set to 180, which is the same as training. For reproducibility, we keep these hyperparameters the same on all benchmarks and disable sampling when decoding. For general multi-chocies question answering evaluation, we follow the official setting to construct the instruction using provided questions and options. An addition prompt like Answer with the options letter from the given choices directly\" is added to control the model output. In addition, we apply CoT prompt on MMVU benchmark following the official evaluation protocol. For temporal grounding evaluation, we add an extra prompt Please output the start and end timestamps in seconds\" after the question. The numbers in the model response are extracted by regular expression, and then treated as one or multiple time intervals. Based on this strategy, we finally report the mIoU bewteen the ground-truth intervals and the predicted intervals."
        },
        {
            "title": "4.2.4 Evaluation Restuls.",
            "content": "Table 7 evaluates the performance of Video Understanding models with 2B model size. VideoLLAMA3 consistently demonstrates competitive results and outperforms baseline methods. In General Video Understanding, VideoLLAMA3 achieves the highest scores on VideoMME w/o sub (59.6%), VideoMME w/ sub (63.4%), ActivityNet-QA (58.2%), PerceptionTest-test (68.0%), MVBench (65.5%), and MMVU (37.6%). On MVBench, it ranks second (65.5%), slightly behind InternVL2.5 2B (68,8%). For Long Video Understanding, VideoLLAMA3 achieves the best performance on all benchmarks: MLVU-dev (65.4%), LongVideoBenchval (57.1%), and LVBench (40.4%), showcasing its superior ability to handle long video content. In Temporal Reasoning, VideoLLAMA3 leads on TempCompass (63.4%), and NextQA (81.1%), and Charades-STA (55.5%). Compared to Apollo-2B, InternVL2.5-2B, and Qwen2-VL-2B, VideoLLAMA3 not only secures the top position in most benchmarks but also demonstrates consistent superiority in tasks requiring comprehensive and long-term video understanding, reinforcing its strong capability across diverse video-related tasks. As for the VideoLLaMA3-7B model, the results are shown in Table 8. On 7B model size, VideoLLaMA3-7B still exhibits competitive results. For general video understanding, it leads on 5 out of 7 benchmarks, including VideoMME w/o sub, VideoMME w/ sub, PerceptionTesttest, and ActivityNet-QA. On MVBench, it also achieves comparable results to InternVL2.5-8B. For long video understanding, VideoLLaMA3-7B scores the highest on MLVU-dev, and achieves the second best results on LongVideoBench-val and LVBench."
        },
        {
            "title": "4.3 Case Study",
            "content": "Chart Image Understanding. In Figure 6, we show two cases for chart image understanding. In the first case, VideoLLaMA3 can analyze stock trends and offer some reasonable suggestions for investment. As for the second case, the model can compare the performance of MLLMs and know the tradeoff between the number of paramters and performances. OCR and Document Understanding. Figure 7 shows two cases for images with texts. In this first example, the model can successfully parse the words in the design image, and offer 16 some suggestions to make the poster better. In the second image, we ask VideoLLaMA3 to perform OCR task on the given document image. VideoLLaMA3 can successfully recognize the words in the document image, demonstrating the strong performance of VideoLLaMA3 in understanding dense information in images. Multi-Image Understanding. Figure 8 gives three examples on multi-image understanding tasks. In the first example, VideoLLaMA3 can tell the differences between two types of birds. The second example demonstrates that VideoLLaMA3 is able to locate answers from long documents (even with multiple images) rather than simplying parsing words. It is an advanced capability beyond OCR. While in the last example, VideoLLaMA3 can understand storylines from comic strips. General Image and Video Understanding. Figure 9 demonstrates VideoLLaMA3s capability in understanding general images, including VQA tasks, answering questions using knowledges and providing videos with captions. Also in Figure 10, we give fives cases for video understanding. VideoLLaMA3 can comprehend video content through temporal dimensions, rather than relying solely on inferences from static content. Long video understanding, temporal grounding, and video-image joint understanding. In Figure 11, we present several cases involving more complex video tasks, including long video grounding, video temporal grounding, and video-image joint understanding. Our VideoLLaMA3 model demonstrates the ability to perform complex long video questionanswering tasks. For tasks requiring temporal grounding, our model accurately identifies the specified time. Additionally, for video-image joint understanding, the model effectively captures the relationships between videos and images, enabling it to tackle more intricate tasks."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Table 9: Ablation Study on Vision Encoders. Model clip-vit-large-patch14-336 [136] GQA AI2D ChartQA DocVQAval MME 61.50 18.32 56.28 24. dfn5B-clip-vit-h-14-378 [137] 62.70 56.87 siglip-so400m-patch14-384 [54] 62.92 57. 16.40 22.44 23.09 31.32 1668.41 1665.35 1667. In MLLMs, the embeddings of pre-trained vision encoder should be trained to align with embeddings of LLMs. Therefore, the representation performance of vision encoder is crucial to the final performance of MLLMs. In this work, we study the impact of different vision encoders. Specifically, we compare three pre-trained transformer-based vision encoders: CLIP [136], DFN [137], and SigLIP [54]. Due to the computation limitation, we perform the study on the subset of the whole dataset. Also, to investigate the performance of the original pre-trained weights, we fix the weights of vision encoders and keep the visual inputs as the fixed resolution, which is the same as the pretrained resolution of the vision encoder (336 336 for CLIP, 378 378 for DFN, and 384 384 for SigLIP,). The training has three stages: 1) Training projector with LLaVA-Pretrain-558K [55]; 2) Tuning all parameters with our recaptioned COYO data; 3) SFT with LLaVA-SFT-665K [38]. The comparison results are shown in Table. 9. SigLIP outperforms the other two vision encoders, especially in finegrained understanding tasks involving texts. Based on this ablation study, we choose the pretrained SigLIP as our base vision encoder, and then adapt it to taking dynamic resolutions as inputs."
        },
        {
            "title": "5 Related Work",
            "content": "Multimodal LLMs for Native Video Understanding. Early video MLLMs primarily relied on sparsely sampled frames and simple connectors, such as MLPs [11, 12, 138], tokenizers [139], and Q-formers [140, 141], to link visual encoders with large language models. 17 Figure 6: Case study of chart images understanding. 18 Figure 7: Case study of OCR and document images. Figure 8: Case study of multi-image understanding. 20 Figure 9: Case study of images with general knowledge. 21 Figure 10: Case study of video understanding. Figure 11: Case study of long video understanding, temporal grounding, and video-image joint understanding. 23 Subsequent models propose various methods to overcome token limitations and support long-form video understanding. For example, some [13] directly extend the context window of LLMs, while others [24, 142149] introduce video token compression techniques that perform pooling across spatial, temporal, or both dimensions. While most approaches utilize image-based encoders [1418, 27], some incorporate video-specific encoders to better capture temporal dependencies [1922]. More recent works [22, 23, 150157] extend beyond visual inputs by incorporating audio, using separate encoders for each modality and integrating them through an LLM decoder. These models leverage joint instruction tuning on video-audio datasets [158160] to capture interactions between visual and auditory information. Additionally, recent advances in streaming video understanding focus on real-time processing [161165], employing techniques like adaptive memory and incremental processing for tasks such as live event detection and real-time captioning. Previous works [14, 19, 22, 24, 142, 149] typically follow training recipe that involves an alignment phase, followed by supervised fine-tuning, with instruction-tuning datasets [18, 2427] often being video dominant. However, we propose vision-centric training paradigm to enhance video understanding capabilities by focusing on large-scale image understanding pre-training. This approach leverages high-quality image-text datasets to build robust vision encoders that are then adapted for video tasks. Multimodal LLMs for General Vision Understanding. Recently, growing number of general MLLMs have been developed to process both images and videos. While, in principle, models designed to handle multiple images are inherently capable of processing video data, achieving optimal performance requires dedicated training on video-specific datasets. Previous studies [28, 29, 93, 166] have demonstrated that general MLLMs with robust image understanding capabilities can achieve remarkable performance on video understanding tasks, even with minimal or no dedicated video training data. These findings highlight the effectiveness of task transfer from images to videos, showcasing the models strong video comprehension and cross-scenario adaptability. Furthermore, Qwen2-VL [30] adopts unified framework for processing both images and videos, enhancing the models visual perception capabilities. Models such as Qwen2-VL, InternVL-2 [31], and InternVL-2.5 [32], which scale both model sizes (ranging from 1 billion to 78 billion parameters) and the volume of training data, have achieved highly competitive performance in both image and video understanding tasks. To address the challenges of processing longer video inputs, recent studies [33, 34] have proposed solutions such as adapting model architectures by incorporating hybrid design of Mamba and Transformer blocks or training with extensive long video datasets to support extended input and output sequences. Recent studies [4, 167, 168] have integrated text, image, and video modalities with audio and speech modalities to improve models video understanding and cross-scenario performance. Additionally, Aria [8] utilizes fine-grained mixture-of-experts decoder, which enables more efficient training and inference compared to dense decoders when handling multimodal inputs."
        },
        {
            "title": "6.1 Discussion",
            "content": "The introduction of VideoLLaMA3 marks significant advancement in the realm of MLLMs, particularly in bridging the gap between image and video understanding. By adopting vision-centric training paradigm, VideoLLaMA3 leverages the robustness of image-centric data to enhance video comprehension, effectively mitigating the challenge associated with temporal dynamics and the complexity of video data. This approach underscores the inferent value of high-quality image-text datasets, which are more readily available and easier to curate compared to their video-text counterparts. The success of VideoLLaMA3 on diverse benchmarks, including VideoMME, PerceptionTest, MLVU, DocVQA, and MathVista, demonstrates its versatility and efficacy across various multimodal tasks. 24 The models ability to maintain strong performance in both image and video domains highlights the effectiveness of our vision-centric framework designs. Specifically, the dynamic resolution adaptation and vision token compression strategies facilitate more flexible and efficient representation of visual inputs, enabling the model to handle wide range of image and video formats with minimal information loss. This flexibility is crucial for real-world applications where visual data can vary significantly in resolution and aspect ratio. Furthermore, the multi-task fine-tuning stage of our training paradigm contributes to the models robust generalization capabilities. By exposing VideoLLaMA3 to variety of downstream tasks, including interactive question answering and video captioning, the model develops comprehensive understanding of both static and dynamic visual information. This comprehensive training enables VideoLLaMA3 to excel not only in standard benchmarks but also in specialized tasks that require nuanced comprehension of visual content."
        },
        {
            "title": "6.2 Limitations",
            "content": "Despite the impressive performance of VideoLLaMA3, several limitations must be acknowledged. Video Data Quality and Diversity. While leveraging large-scale image-text datasets has proven beneficial, the quality and diversity of video-text datasets remain constraint. Video data often suffer from lower annotation quality and limited diversity, which can impede the models ability to generalize across different video domains and genres. Real-time Processing. The current model architecture may not be optimized for real-time video processing tasks, which is essential for applications such as autonomous driving and live video analytics. The computational overhead associated with processing high-resolution and lengthy video inputs can hinder real-time performance. Generalization to Unseen Modalities. While VideoLLaMA3 excels in image and video understanding, its capability to generalize to other modalities, such as audio or speech data, remains unexplored. Integrating additional modalities could further enhance the models multimodal comprehension but poses significant challenges in terms of architecture and training."
        },
        {
            "title": "6.3 Future Work",
            "content": "Building on the foundations laid by VideoLLaMA3, several avenues for future research are proposed to address the identified limitations and further enhance the models capabilities. Enhanced Video-Text Datasets. Investing in the creation and curation of higher quality and more diverse video-text datasets will be crucial. Incorporating annotations that capture nuanced temporal and contextual information can significantly improve the models temporal understanding and generalization across different video domains. Real-time Inference Optimization. Optimizing the model architecture for real-time inference by reducing latency and improving processing speed is essential for applications requiring immediate responses. Techniques such as model acceleration, parallel processing, and efficient tokenization strategies can contribute to achieving real-time performance. Multimodal Expansion. Extending VideoLLaMA3 to incorporate additional modalities like audio, speech, and sensor data can create more holistic understanding of multimodal inputs. Research into unified architectures that seamlessly integrate multiple data types will be pivotal in achieving comprehensive multimodal intelligence. Implementing more sophisticated post-training Advanced Post-Training Techniques. methodologies, such as scaling RL techniques for MLLMs, can further refine VideoLLaMA3s performance. RLHF and other RL-based approaches can be employed to better align the models outputs with human preferences and task-specific requirements. Scaling these RL techniques to accommodate the complexities of multimodal data will enhance the models ability to generate more accurate, contextually appropriate, and user-aligned responses, thereby advancing its overall multimodal intelligence. In summary, while VideoLLaMA3 represents significant step forward in multimodal AI, addressing its current limitations through targeted research and development will pave the way for even more powerful and versatile models in the future."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Gpt-4o system card, 2024. 1 [2] Anthropic. The claude 3 model family: Opus, sonnet, haiku. [3] Gemini Team Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [4] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 24 [5] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 6, [6] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 1 [7] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [8] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, Chongyan Zhu, Xiaoyi Ren, Chao Li, Yifan Ye, Peng Liu, Lihuan Zhang, Hanshu Yan, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model, 2025. 24 [9] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [10] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 2 [11] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: LearnarXiv preprint ing united visual representation by alignment before projection. arXiv:2311.10122, 2023. 2, 17 [12] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024. 17 [13] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 24 [14] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe HansenEstruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large multimodal models. arXiv preprint arXiv:2412.10360, 2024. 15, 24 [15] Enxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi Li, and Gaoang Wang. Moviechat+: Question-aware sparse memory for long video question answering. arXiv preprint arXiv:2404.17176, 2024. [16] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Videoccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024. [17] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. [18] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 24 [19] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024. 24 [20] Raehyuk Jung, Hyojun Go, Jaehyuk Yi, Jiho Jang, Daniel Kim, Jay Suh, Aiden Lee, Cooper Han, Jae Lee, Jeff Kim, et al. Pegasus-v1 technical report. arXiv preprint arXiv:2404.14687, 2024. [21] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arxiv, 2024. 9, 10, [22] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 2, 24 [23] Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audiovisual large language models. arXiv preprint arXiv:2406.15704, 2024. 24 [24] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2, 6, 9, 10, 11, 15, 24 [25] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. 15 [26] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. arXiv preprint arXiv:2311.17005, 2023. 15 [27] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. 2, 9, 10, 11, [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 24 [29] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models, 2024. 24 [30] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 2, 4, 6, 11, 12, 15, 24 28 [31] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 3, 4, 6, 8, 24 [32] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. 2, 3, 4, 12, 15, [33] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture, 2024. 24 [34] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output, 2024. 2, 24 [35] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 2, 9, 10, 12 [36] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. 12, 15 [37] Hugging Face Team. Smolvlm - small yet mighty vision language model. https: //huggingface.co/blog/smolvlm, 2023. Accessed: 2025-01-19. 2, [38] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024. 2, 9, 10, 11, 17 [39] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, visioncentric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 9 [40] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images, 2021. 9 [41] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions., 2024. 9 [42] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 9, 12 [43] Kung-Hsiang Huang, Hou Pong Chan, Yi R. Fung, Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-Fu Chang, and Heng Ji. From pixels to insights: survey on automatic chart understanding in the era of large foundation models, 2024. 2 [44] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audiovisual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 2 [45] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audiovisual language model for video understanding. In Yansong Feng and Els Lefever, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023 - System Demonstrations, Singapore, December 6-10, 2023, pages 543553. Association for Computational Linguistics, 2023. 2 [46] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 2, 10, 15 [47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 3, 4 [48] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 3, 4 [49] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. arXiv preprint arXiv:2307.06304, 2023. 4 [50] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [51] Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris Kitani, and László Jeni. Dont look twice: Faster video transformers with run-length tokenization. arXiv preprint arXiv:2411.05222, 2024. [52] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/ coyo-dataset, 2022. 5 [53] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiao wen Dong, Hang Yan, Hewei Guo, Conghui He, Zhenjiang Jin, Chaochao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, and Yu Qiao. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. ArXiv, abs/2404.16821, 2024. 6, 8 [54] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1194111952, 2023. 6, 11, 17 [55] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 7, 8, 9, 17 [56] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 84298438, 2019. 7, 8, 30 [57] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 7, 8 [58] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S. Ryoo, Shrikant B. Kendre, Jieyu Zhang, Can Qin, Shu Zhen Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, and Ran Xu. xgen-mm (blip-3): family of open large multimodal models. ArXiv, abs/2408.08872, 2024. 7, 8 [59] pdfa-eng-wds. https://huggingface.co/datasets/pixparse/pdfa-eng-wds. 7, 8 [60] idl-wds. https://huggingface.co/datasets/pixparse/idl-wds. 7, 8 [61] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [62] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 742758. Springer, 2020. 8 [63] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 8 [64] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303, 2024. 8 [65] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 8 [66] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Cocotext: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016. 8 [67] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 88028812, 2021. [68] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, Chee Seng Chan, and Lianwen Jin. Icdar 2019 competition on large-scale street view text with partial labeling - rrc-lsvt. 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15571562, 2019. 8 [69] Xi Liu, Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, et al. Icdar 2019 robust reading challenge on reading chinese text on signboard. arXiv preprint arXiv:1912.09641, 2019. 8 [70] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision, pages 498517. Springer, 2022. 8 31 [71] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Mingshi Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, and Feiyan Huang. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. In Conference on Empirical Methods in Natural Language Processing, 2023. 8 [72] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: dataset for form understanding in noisy scanned documents. In 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), volume 2, pages 16, 2019. 8, [73] Jordy Van Landeghem, Rubèn Tito, Łukasz Borchmann, Michał Pietruszka, Dawid Jurkiewicz, Rafał Powalski, Paweł Józiak, Sanket Biswas, Mickaël Coustaty, and Tomasz Stanisławek. Icdar 2023 competition on document understanding of everything (dude). In International Conference on Document Analysis and Recognition, pages 420 434, 2023. 8, 9 [74] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language models. arXiv preprint arXiv:2312.06109, 2023. 8 [75] Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: large-scale benchmark for chart summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022. 8, 9 [76] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820228211, 2024. 8, 9 [77] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. 8, 9 [78] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302321, 2019. 8, [79] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. 8, 9 [80] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. 8, 9 [81] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model, 2024. 8 [82] Beijing Academy of Artificial Intelligence (BAAI). Infinity instruct. GitHub repository, HuggingFace repository, 2024. 8, 9 [83] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint arXiv:2311.10774, 2023. 32 [84] Kushal Kafle, Scott Cohen, Brian Price, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In CVPR, 2018. 9 [85] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. 9 [86] Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild, 2024. 9 [87] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [88] Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 15271536, 2020. 9 [89] Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, and Xiang Yue. Harnessing webpage uis for text-rich visual understanding, 2024. 9, 10 [90] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: In Proceedings of the 2014 Referring to objects in photographs of natural scenes. conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 9, 10 [91] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 9, 10 [92] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot demonstrative instructions. In The Twelfth International Conference on Learning Representations, 2024. 9, 10 [93] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning, 2024. 9, 10, 24 [94] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. ArXiv, abs/2406.08464, 2024. 9, 10, 11 [95] Migel Tissera. Synthia-70b-v1.2: Synthetic intelligent agent. https://huggingface. co/migtissera/Synthia-13B, 2023. 9 [96] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13, 2024. 9 [97] Miquel Farré, Andi Marafioti, Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. Finevideo. https://huggingface.co/datasets/HuggingFaceFV/finevideo, 2024. 9, 10, [98] Ruchit Rawal, Khalid Saifullah, Miquel Farré, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813, 2024. 9, 10, 11 [99] Share. Sharegemini: Scaling up video caption data for multimodal large language models, June 2024. 9, 10, 11 33 [100] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, et al. Videorefer suite: Advancing spatial-temporal object understanding with video llm. arXiv preprint arXiv:2501.00599, 2024. 10, 11 [101] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. DenseIn Proceedings of the IEEE international conference on captioning events in videos. computer vision, pages 706715, 2017. 10, 11 [102] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, number 1, 2018. 10, 11 [103] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. 10, 11 [104] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1840718418, 2024. 10, 11 [105] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal pretraining for dense video captioning. arXiv preprint arXiv:2011.11760, 2020. 10, 11 [106] Andreea-Maria Oncescu, Joao Henriques, Yang Liu, Andrew Zisserman, and Samuel Albanie. Queryd: video dataset with high-quality text and audio narrations. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 22652269. IEEE, 2021. 10, [107] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2305623065, 2023. 10, 11 [108] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 10, 11, 16 [109] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with finegrained temporal reasoning. arXiv preprint arXiv:2402.11435, 2024. 10, 11 [110] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12071216, 2019. 10, 11 [111] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tülu 3: Pushing frontiers in open language model post-training. 2024. 10, 11 [112] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiangwei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479, 2024. 34 [113] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 12 [114] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 12 [115] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 12 [116] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. 12 [117] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multidiscipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. [118] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 13 [119] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166, 2025. 13 [120] xai. Realworldqa benchmark. RealworldQA, 2024. 13 https://huggingface.co/datasets/xai-org/ [121] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251, 2016. 13 [122] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 13 [123] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. 14 [124] Li* Bo, Zhang* Peiyuan, Zhang* Kaichen, Pu* Fanyi, Du Xinrun, Dong Yuhao, Liu Haotian, Zhang Yuanhan, Zhang Ge, Li Chunyuan, and Ziwei Liu. Lmms-eval: Accelerating the development of large multimoal models, March 2024. 14 [125] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 14 [126] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The firstever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 35 [127] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36, 2024. 15 [128] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. 15 [129] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 9127 9134, 2019. 15 [130] Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, and Arman Cohan. Mmvu: Measuring expert-level multi-discipline video understanding, 2025. 15 [131] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [132] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. 16 [133] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 16 [134] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 16 [135] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97779786, 2021. 16 [136] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763, 2021. [137] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. 17 [138] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 17 [139] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. Video-lavit: Unified video-language pretraining with decoupled visual-motional tokenization. arXiv preprint arXiv:2402.03161, 2024. 17 [140] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audiovisual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 17 36 [141] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [142] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 24 [143] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [144] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1370013710, 2024. [145] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models. arXiv preprint arXiv:2407.15841, 2024. [146] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2025. [147] Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang, Gang Yu, Jiayuan Fan, and Tao Chen. Moviellm: Enhancing long video understanding with ai-generated movies. arXiv preprint arXiv:2403.01422, 2024. [148] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2025. [149] Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan Plummer, Bryan Russell, and Kate Saenko. Koala: Key frame-conditioned long video-llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1358113591, 2024. 24 [150] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: enhancing multimodal large language model to answer questions in dynamic audiovisual scenarios. In European Conference on Computer Vision, pages 146164. Springer, 2025. 24 [151] Fangxun Shu, Lei Zhang, Hao Jiang, and Cihang Xie. Audio-visual llm for video understanding. arXiv preprint arXiv:2312.06720, 2023. [152] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. [153] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. arXiv preprint arXiv:2306.09093, 2023. [154] Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, and Juan Carlos Niebles. X-instructblip: framework for aligning x-modal instruction-aware representations to llms and emergent crossmodal reasoning. arXiv preprint arXiv:2311.18799, 2023. [155] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2658426595, 2024. 37 [156] Shoubin Yu, Jaehong Yoon, and Mohit Bansal. Crema: Multimodal compositional video reasoning via efficient modular adaptation and fusion. arXiv preprint arXiv:2402.05889, 2024. [157] Yunlong Tang, Daiki Shimada, Jing Bi, and Chenliang Xu. Avicuna: Audio-visual llm with interleaver and context-boundary alignment for temporal referential dialogue. arXiv preprint arXiv:2403.16276, 2024. 24 [158] Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. Avqa: dataset for audio-visual question answering on videos. Proceedings of the 30th ACM International Conference on Multimedia, 2022. 24 [159] Huda AlAmri, Vincent Cartillier, Abhishek Das, Jue Wang, Stefan Lee, Peter Anderson, Irfan Essa, Devi Parikh, Dhruv Batra, Anoop Cherian, Tim K. Marks, and Chiori Hori. Audio visual scene-aware dialog. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 75507559, 2019. [160] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: Enhancing multimodal large language model to answer questions in dynamic audiovisual scenarios, 2024. 24 [161] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based real-time understanding for long video streams. arXiv preprint arXiv:2406.08085, 2024. 24 [162] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models. arXiv preprint arXiv:2405.16009, 2024. [163] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1840718418, 2024. [164] Pan Zhang, Xiao wen Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yuanbo Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2.5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions. 2024. [165] Jihao Liu, Zhiding Yu, Shiyi Lan, Shihao Wang, Rongyao Fang, Jan Kautz, Hongsheng Li, and Jose M. Alvare. Streamchat: Chatting with streaming video. 2024. 24 [166] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024. [167] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, and Xing Sun. Vita: Towards open-source interactive omni multimodal llm, 2024. 24 [168] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, and Ran He. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction, 2025."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group"
    ]
}