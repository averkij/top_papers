{
    "paper_title": "mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning",
    "authors": [
        "Nghia Trung Ngo",
        "Franck Dernoncourt",
        "Thien Huu Nguyen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks. However, the mechanism underlying their utilization of different human reasoning skills remains poorly investigated, especially for multilingual commonsense reasoning that involves everyday knowledge across different languages and cultures. To address this gap, we propose a \\textbf{M}ultilingual and Scalable Benchmark for \\textbf{S}kill-based \\textbf{Co}mmonsense \\textbf{Re}asoning (\\textbf{mSCoRe}). Our benchmark incorporates three key components that are designed to systematically evaluate LLM's reasoning capabilities, including: (1) a novel taxonomy of reasoning skills that enables fine-grained analysis of models' reasoning processes, (2) a robust data synthesis pipeline tailored specifically for commonsense reasoning evaluation, and (3) a complexity scaling framework allowing task difficulty to scale dynamically alongside future improvements in LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying sizes and training approaches demonstrate that \\textbf{mSCoRe} remains significantly challenging for current models, particularly at higher complexity levels. Our results reveal the limitations of such reasoning-reinforced models when confronted with nuanced multilingual general and cultural commonsense. We further provide detailed analysis on the models' reasoning processes, suggesting future directions for improving multilingual commonsense reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 7 3 1 0 1 . 8 0 5 2 : r mSCoRe: Multilingual and Scalable Benchmark for Skillbased Commonsense Reasoning Nghia Trung Ngo1, Franck Dernoncourt2 and Thien Huu Nguyen1 1 Department of Computer Science, University of Oregon, Eugene, OR, USA 2 Adobe Research, USA {nghian@,thien@cs}.uoregon.edu, franck.dernoncourt@adobe.com"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks. However, the mechanism underlying their utilization of different human reasoning skills remains poorly investigated, especially for multilingual commonsense reasoning that involves everyday knowledge across different languages and cultures. To address this gap, we propose Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning (mSCoRe). Our benchmark incorporates three key components that are designed to systematically evaluate LLMs reasoning capabilities, including: (1) novel taxonomy of reasoning skills that enables fine-grained analysis of models reasoning processes, (2) robust data synthesis pipeline tailored specifically for commonsense reasoning evaluation, and (3) complexity scaling framework allowing task difficulty to scale dynamically alongside future improvements in LLM abilities. Extensive experiments on eights state-ofthe-art LLMs of varying sizes and training approaches demonstrate that mSCoRe remains significantly challenging for current models, particularly at higher complexity levels. Our results reveal the limitations of such reasoning-reinforced models when confronted with nuanced multilingual general and cultural commonsense. We further provide detailed analysis on the models reasoning processes, suggesting future directions for improving multilingual commonsense reasoning capabilities."
        },
        {
            "title": "Introduction",
            "content": "Commonsense reasoning enables person to navigate everyday situations, make logical inferences, and understand implicit information in our environment. While this ability comes naturally to humans, it has proven to be one of the most challenging capabilities to replicate in current language models (Davis, 2024). Recent advancements in Large Reasoning Models (LRMs), such as OpenAIs o1 series (Jaech et al., 2024), and open-source models like DeepSeek R1 (DeepSeek-AI et al., 2025), have shown promising results across various complex reasoning tasks, including mathematics, coding, and logical inference (Kazemi et al., 2025). However, relatively little attention has been devoted to systematically analyzing and understanding these models commonsense reasoning capabilities, especially in multilingual settings which involve common knowledge across diverse languages and cultural contexts (Do et al., 2024). Several benchmarks have been proposed to assess commonsense reasoning abilities of language models. CommonsenseQA (CSQA) (Talmor et al., 2019) evaluates general commonsense knowledge through multiple-choice questions derived from ConceptNet. COPA (Roemmele et al., 2011) focuses on causal relationships between everyday events, while SocialIQA (Sap et al., 2019) evaluates social commonsense understanding. More recently, comprehensive benchmarks like MMLU (Hendrycks et al., 2021) and Big-Bench Hard (Suzgun et al., 2023) aim to evaluate models generalization capabilities across diverse commonsense tasks. However, these benchmarks have significant limitations in three key areas. First, they often focus on single high-resourced language such as English (Talmor et al., 1 2019) or Chinese (Sun et al., 2024). Multilingual extensions such as X-COPA (Ponti et al., 2020) and X-CSQA (Lin et al., 2021) primarily rely on translation of existing datasets, thus limiting their ability to capture culturally specific nuances. Second, although recent efforts like mCSQA (Sakai et al., 2024) leverage generative multilingual language models for more comprehensive and robust dataset creation process, they still lack systematic way to scale task difficulty, which is crucial for assessing the rapid evolving capabilities of LLMs. Finally, current benchmarks are unable to provide fine-grained analysis and classification of the reasoning steps used by LLMs, which would provide deeper insights into their operations. To address these limitations, we introduce Multilingual and Scalable Benchmark for Skillbased Commonsense Reasoning (mSCoRe), novel benchmark designed explicitly to provide comprehensive evaluation of LLMs commonsense reasoning capabilities across multiple languages and cultural contexts. Specifically, our benchmark offers three notable advantages: 1. Comprehensive Coverage: mSCoRe covers both general commonsense knowledge from across languages including English, German, French, Chinese, and Japanese, and diverse cultural social commonsense knowledge. 2. Skill-based Analysis: mSCoRe introduces novel approach to reasoning analysis through the classification of each atomic reasoning step, allowing for more precise analysis of models reasoning process. 3. Scalability: mSCoRe employs techniques such as context expansion, option adjustment, and commonsense implicitation to progressively increase question complexity while preserving commonsense answer semantics, effectively scaling task difficulty. Our contributions can be summarized as follows: We introduce mSCoRe, novel scalable benchmark for evaluating multilingual general and cultural commonsense reasoning with fine-grained skill-based analysis. Using mSCoRe, we extensively evaluate eights state-of-the-art LLMs, including both commercial and open-source models, across diverse reasoning conditions. Our analysis provides insights into how model scale, training techniques, and reasoning skill types impact performance, suggesting future directions for improving commonsense reasoning capability of LLMs."
        },
        {
            "title": "2 Related works",
            "content": "Large Reasoning Models: Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities in various complex problem-solving tasks. Reasoningreinforced models like OpenAI o1 (Jaech et al., 2024), Macro-o1 (Zhao et al., 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025) have shown superior performance in mathematics and coding, effectively simulating human-like analytical thinking and enhancing multi-step reasoning (Glazer et al., 2024; Guo et al., 2024). These models employ multiple methods to enhance reasoning capabilities. In particular, Chain-of-thought prompting (Wu et al., 2023) has emerged as powerful technique that encourages step-by-step reasoning, significantly improving performance on complex tasks. Building upon this foundation, various Chainof-X approaches have been proposed to further enhance model reasoning capability (Yao et al., 2023; Lightman et al., 2024; Besta et al., 2024; Chen et al., 2024). Recent techniques such as test-time scaling and reinforcement learning have also contributed to improving the reasoning abilities of LLMs (Snell et al., 2024; Kumar et al., 2025; Hou et al., 2025). While these methods enhance the overall structure of the reasoning path, they generally pay little attention to the categorization of each reasoning step. mSCoRe proposes more fine-grained approach in which each step is atomic and labeled according to reasoning skill, facilitating deeper and systematic evaluation of models reasoning process. Commonsense Reasoning Benchmarks: Despite significant advances in evaluating mathematical and scientific reasoning capabilities of LLMs (Cobbe et al., 2021; Glazer et al., 2024; He et al., 2024; Chow et al., 2025), commonsense reasoning benchmarks have received comparatively less recent attention. Early datasets such as CommonsenseQA (CSQA) (Talmor et al., 2019), COPA (Roemmele et al., 2011), and SocialIQA (Sap et al., 2019) primarily target English-language commonsense knowledge, focusing respectively on general factual 2 Figure 1: Data Generation Process. The four-step data creation pipeline of mSCoRe. Each step builds upon the previous one to create progressively more challenging reasoning tasks while maintaining the underlying reasoning skills being evaluated. knowledge, causal relationships, and social interactions. Recent comprehensive benchmarks like MMLU (Hendrycks et al., 2021) and Big-Bench Hard (Suzgun et al., 2023) evaluate the generalization abilities of LLMs across diverse commonsense reasoning tasks. Multilingual extensions like X-CSQA (Lin et al., 2021) and X-COPA (Ponti et al., 2020) expand the evaluation beyond English by translating existing datasets into multiple languages. More recent approaches such as mCSQA (Sakai et al., 2024) leverage LLM to assist more closely in the data synthesis process. However, these benchmarks are still limited in cultural social commonsense that involves everyday interactions among different cultures. There has been increasing efforts on cultural knowledge bases to develop cultural-aware LLMs. In particular, CulturePark (Li et al., 2024) introduces novel multi-agent communication framework powered by LLMs to simulate cross-cultural human interactions, whereas CultureBank (Shi et al., 2024) aggregates real-world social interactions from platforms like TikTok and Reddit, structuring annotations around cultural topics. mSCoRe builds upon mCSQA and CultureBank to comprehensively cover both general and social aspects of commonsense reasoning across multiple languages and cultures."
        },
        {
            "title": "3.1 Commonsense Reasoning",
            "content": "Commonsense reasoning involves making inferences about unstated aspects of scenario using implicit world knowledge capability ingrained in human behavior but still challenging for current LLMs. Unlike formal reasoning domains such as mathematics or logic, where rules are explicitly defined and conclusions follow determinate paths, commonsense reasoning requires access to vast reservoir of implicit knowledge and the ability to apply this knowledge flexibly across diverse situations. Furthermore, there can be multiple reasoning paths that can lead to the correct answer, especially for commonsense questions. However, previous evaluations still primarily focus on answer accuracy (Suzgun et al., 2023; Sakai et al., 2024), providing limited insight into how LLMs construct their reasoning pathways. Atomic Reasoning Step an indivisible unit of reasoning that predominantly utilizes one reasoning skill. It is single, coherent thought process that cannot be broken down into smaller steps without losing its meaning. An optimal reasoning path (for multiple-choice QA task) uses minimum number of atomic steps necessary, ensuring that each step is nonredundant and contributes to narrowing down the possible options by eliminating one or more answer choices. Figure 2: Atomic Reasoning Step definition. To address this limitation, we propose to investigate deeply into models reasoning process by introducing the concept of atomic reasoning steps\"  (Fig. 2)  as the foundational unit of analysis. Our framework aims to analyze the optimal path utilized by the LLM, defined as the path requiring the minimum number of atomic reasoning steps while maintaining logical coherence. This approach not only enables systematic evaluation of specific reasoning 3 Skills Inductive Deductive Abductive Short Definitions Examples Logical Reasoning Drawing general conclusions from specific observations. Deriving specific conclusions from general premises. Forming hypotheses to explain observations. Most technological innovations eventually benefit society. All communication tools connect people; social media is communication tool. Rising depression rates suggest social media affects mental health. Contextual Reasoning Drawing parallels between similar situations to infer conclusions. Analogical Counterfactual Considering alternative scenarios and outcomes that did not happen. Probabilistic Temporal Spatial Like town squares facilitated discourse, social media creates digital gathering spaces. Without social media, many social movements would lack momentum. Applying principles of probability to make inferences under uncertainty. Users have very high chance of encountering misinformation weekly. Understanding sequences and durations of events. Visualizing and manipulating objects in space. Brief moments scrolling accumulate into hours of lost productivity daily. Platform designs maximize attention capture through strategic layouts. Social Moral Understanding social interactions and norms. Deciding what is right or wrong based on ethical principles. Like-based validation systems create unhealthy approval-seeking behaviors. Prioritizing profit over user wellbeing raises ethical concerns. Social & Ethical Reasoning Table 1: The ten types of reasoning skills across three categories with short definitions and examples for each skill applied to the question Is social media good for society?\". Detailed descriptions and additional examples are provided in Appendix A.1. skills, but also provides clear framework for analyzing how models construct complex reasoning chains. It allows for meaningful comparison of reasoning processes across different models and languages. Finally, it facilitates the scaling of question complexity through the requirement of additional of atomic steps."
        },
        {
            "title": "3.1.1 Reasoning Skills",
            "content": "We develop structured taxonomy for classifying each reasoning step, enabling systematic evaluation of how LLMs employ human reasoning skills in commonsense tasks. While no clear consensus exists on comprehensive taxonomy of human reasoning skills, existing categorizations typically serve specific purposes. For example, Blooms Taxonomy (Oscarini & Bhakti, 2010) provides hierarchical framework categorizing educational goals into three domains: cognitive (knowledge-based), affective (emotion-based), and psychomotor (action-based). Similarly, Fleishmans taxonomy (Welford, 1986) identifies 52 distinct human abilities across cognitive, perceptual, psychomotor, and physical domains, primarily to facilitate job design, training, and assessment development. Based on the fundamental characteristics of commonsense knowledge identified by Do et al. (2024) with established reasoning skill categorizations from Wikipedia contributors (2025), we propose taxonomy comprising three major categories: Logical Reasoning encompasses forms of reasoning that involve structured processes to derive conclusions from given information. This category includes methodologies like deductive, inductive, and abductive reasoning, which are foundational in scientific and analytical disciplines to ensure conclusions are logically sound. Contextual Reasoning includes skills used to understand relationships, contexts, and dynamics between elements. This category covers various types of reasoning such as analogical, counterfactual, probabilistic, temporal, and spatial, used to evaluate scenarios, predict outcomes, and solve problems across different contexts. Social and Ethical Reasoning involves skills focused on understanding social interactions and evaluating ethical principles. This category includes social and moral reasoning, essential for interpreting behaviors, navigating complex social environments, and making decisions based on ethical considerations. Detailed descriptions and examples of each reasoning skill are provided in Table 1. While humans employ additional reasoning skills beyond those presented here, our goal is to establish concise yet comprehensive reasoning taxonomy that maximizes coverage of human reasoning capabilities for commonsense applications, while minimizing the overlap between categories. This reasoning taxonomy will be implemented within our LLM prompts throughout both the data generation and evaluation processes to ensure focus on our considered skills. Each atomic reasoning step will be classified under single skill from our taxonomy, enabling precise comparison of different reasoning processes. 3.2 mSCoRe To maintain robust label accuracy, rather than using LLMs to generate synthetic dataset from scratch, we utilize human-annotated seed datasets and scale up their complexity to create mSCoRe. In particular, our benchmark consists of multiple-choice commonsense questions, separated into two subsets based on different seed datasets: (1) mSCoRe-G focuses on general commonsense reasoning, building upon multilingual commonsense questions from mCSQA (Sakai et al., 2024) as seed dataset. This component evaluates understanding of physical causality, temporal relationships, and basic world dynamics 4 across multiple languages. (2) mSCoRe-S addresses social commonsense reasoning based on diverse cultural situations from CultureBank (Shi et al., 2024). This component specifically tests understanding of social interactions, cultural norms, and behavioral expectations across different cultural contexts. The overall data generation process is visualized in Fig. 1, in which each instance in the seed datasets undergoes four-step process as illustrated in Fig. 3. Through this systematic creation process, mSCoRe provides comprehensive framework for evaluating and analyzing commonsense reasoning capabilities of LLMs. 3.2.1 mSCoRe-G: General Commonsense mCSQA (Multilingual CommonsenseQA) extends the CommonsenseQA dataset (Talmor et al., 2019) to eight languages to evaluate language models cross-lingual commonsense reasoning capabilities. Building upon ConceptNet, each multiple-choice question-answer (QA) pair in mCSQA mostly revolves around general commonsense knowledge (an example is provided in the first step in Figure 1). To create mSCoRe-G, we further process each QA pair through the following 4 steps: 1. Seed Data Filtering: general LLMjudge evaluates each candidate on three criteria described in Fig. 4: (1) Commonsenseness, (2) Complexity, and (3) Expandability. The goal is to prioritize questions with high level of commonsense and complexity, while maintaining flexibility for expansion into more sophisticated questions (full details of the judge model and scoring criteria are provided in Appendix B.1). Step 1 - Data Filtering: To limit the cost while maintaining quality and diversity, we sample small subset from the seed benchmarks. Each sample is scored by general LLM-judge based on multiple criteria for expansion potential, ensuring that we select instances that will yield meaningful insights when scaled complexity-wise. Step 2 - Reasoning Generation: Provide Commonsense Context to expand on the given question and detailed Reasoning Process that involves multiple Reasoning Steps to arrive at the correct answer. This establishes gold standard reasoning path for each question. Step 3 - Complexity Scaling: Modify and expand each question to create more complex variants by expanding its context, modifying the question, adjusting the answer options, and adding additional Reasoning Steps. This creates progression of difficulty levels for each base question. Step 4 - Commonsense Implicitation: Combine the given Commonsense Context with the question to generate new, concise commonsense question that implicitly incorporates the original context. This process aims to evaluate the commonsense reasoning abilities of LLMs by ensuring that the implicit context preserves the original reasoning process and maintains the correctness of the answer. Figure 3: Four steps of data generation process. Commonsense-ness: Does answering the question rely solely on commonsense knowledge accessible to the general population, or does it require formal reasoning and specialized expertise beyond everyday understanding? Complexity: How difficult is the question to understand and answer? Does it require minimal reasoning or complex, multi-step thought process to identify the correct answer? Expandability: To what extent can the question be expanded or elaborated upon to introduce additional complexity or dimensions? Figure 4: Three criteria of data filtering. Context Expansion: Add additional background or situational details to the Commonsense Context to increase depth and reasoning requirements to the question. Option Adjustment: Adjust the existing answer options to align with the new complex question, ensure the correct answer option remains semantically similar to the original. Introduce an additional plausible but incorrect option to increase the complexity of the question that (1) increases the complexity of the question, and (2) requires an additional reasoning step to eliminate. Reasoning Refinement: Refine the original Reasoning Process to fit the new context with an additional reasoning step that eliminates the added incorrect option. 2. Structured Reasoning Generation: For selected question-answer pairs, we employ LLMs to generate relevant commonsense context that helps identify the correct answer. From the tuple (context, question, options-answer), we then generate structured reasoning process. Each reasoning step in the process consists of three attributes: (1) Reasoning Skill - the specific skill from our reasoning ontology that is predominantly employed in this step, (2) Reasoning Text - the models rationale based on the identified skill, and (3) Eliminated Options - the list of options eliminated in this step based on the reasoning. Figure 5: Three sub-steps of Complexity Scaling. 3. Data Complexity Scaling: From the base (context, question, answer, reasoning process), we implement procedure to systematically scale up the difficulty level of each question. The goal is to introduce an additional plausible option at each level that not only increases the complexity of the question, but also requires an additional reasoning step to eliminate. This is achieved through 3 sub-steps, as described in Fig. 5. 4. Commonsense Implicitation: As commonsense knowledge is implicit knowledge about the world that is often unspoken but assumed, this step reduces the context exposed to the LLMs by combining the context and question into context-implicit commonsense question. To answer the modified question, models will have to draw on their internal common 5 Descriptors Cultural Topic Cultural group - topic - scenario Definitions Social Context Settings the behavior takes place. Actor Question Actor Behavior Recipient Relation Recipient Behavior Who exhibit the behavior The commonsense question regarding the actors behavior Behavior of the actor Recipient of the action Relation between the actor and the recipient Behavior of the recipient Examples Japanese culture - Gift Giving - Etiquette and Practices During meeting in Japan, visiting Western executive wants to express gratitude to their hosts Visiting executive Im attending meeting in Japan and would like to give gift to my hosts. What should consider to ensure my gesture is well-received? Offer gift wrapped in traditional Japanese style as gesture of appreciation Japanese business hosts Business partners Receive the gift with both hands and show appreciation Table 2: An example of social commonsense question from CultureBank. knowledge to determine the correct answer, especially when the topic requires more than just logical reasoning. The whole procedure  (Fig. 1)  is repeated to create questions at an increasing level of complexity. This approach helps mitigate the data leakage (Deng et al., 2024) and shortcut reasoning (Haraguchi et al., 2023) problems, as observed in our experimental results where performance degrades significantly with each level. Furthermore, the scaled complexity forces LLMs to utilize their reasoning capacity more extensively, enabling deeper investigation into their reasoning process. Detailed examples with complete prompts are provided in Appendix C. 3.2.2 mSCoRe-S: Social Commonsense There is still gap in current commonsense benchmarks in terms of social commonsense knowledge and cultural norms (Davis, 2024). To provide comprehensive evaluation of LLMs commonsense reasoning capacity, we propose an additional benchmark mSCoReS that revolves around social situations across diverse cultural contexts. In particular, we utilize CultureBank (Shi et al., 2024) as our seed dataset, which is knowledge base containing real-world social questions sourced from TikTok and Reddit posts. Each instance in CultureBank is provided with various descriptors containing details about the cultural group, context, behaviors, and an agreement level indicating how widely accepted that behavior is within the community  (Fig. 2)  . Each seed instance follows the same 4-step process as described in previous section to generate the final context-implicit commonsense question. However, minor differences are introduced to adapt to CultureBank data, including: Seed Data Filtering: In addition to the 3 criteria used for mSCoRe-G, we introduce an additional criterion - multiculture-ness - for filtering social situations (detailed description provided in Appendix B.1). The aim is to select situations that involve the most culturally distinctive elements, allowing us to evaluate models understanding of diverse cultural contexts and associated commonsense knowledge. Structured Reasoning Generation: Before generating the context and reasoning process, LLM needs to generate the QA pair first. This acts as the seed QA pair from mCSQA in the previous section and goes through the same procedure."
        },
        {
            "title": "3.2.3 Dataset Statistic",
            "content": "mSCoRe-G covers 5 languages including English, German, French, Japanese, and Chinese. For each language, we create 200 examples ranging from level 0 (original QA pair) to level 3 (3 steps of expansion). This results in 800 examples per language. For mSCoRe-S, we similarly create 200 examples for each source (TikTok and Reddit). In total, mSCoRe contains 5,600 instances (4000 for general commonsense and 1600 for social commonsense). Detailed examples at different complexity levels are provided in Appendix C."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "We conduct comprehensive evaluations using diverse set of state-of-the-art multilingual language models, selected to represent different approaches to model development and training. Our evaluation considers three key dimensions: model availability, parameter scale, and training methodology. The models evaluated in our study include: 6 GPT-4o (OpenAI et al., 2024): general-purpose LLM representing the current state-of-theart LLM, trained on large-scale multimodal data from diverse sources. General Commonsense GPT-4o o1 o1-mini LLaMA-3.3-70B LLaMA-3.1-8B R1-70B R1-8B Aya-32B English L2 L1 72.5 70.0 75.0 73.5 65.5 70.5 75.0 69.0 21.5 22.5 69.5 70.5 62.0 62.0 66.5 67.0 L0 80.5 82.5 76.5 78.5 23.0 79.5 67.5 77. L3 71.5 72.0 63.5 70.0 21.5 69.0 55.0 66.0 L0 75.0 75.0 69.5 75.5 73.0 73.0 67.5 70.5 German L2 L1 71.0 68.5 63.0 67.5 69.5 66.0 72.5 68.0 63.0 65.5 67.0 67.0 61.0 58.0 66.5 65.5 L3 67.5 67.5 64.5 73.0 61.0 70.0 55.5 66.0 L0 78.0 80.5 71.5 78.5 69.5 76.0 58.0 76.5 French L1 74.0 72.5 64.5 72.0 61.0 71.5 45.0 69.0 L2 70.0 71.5 59.5 67.0 54.5 69.5 44.0 65.0 L3 63.5 61.0 55.0 64.0 52.0 64.5 43.5 60.5 Chinese L2 L1 72.5 78.5 56.0 63.0 60.0 63.0 70.5 74.5 46.0 52.0 61.0 70.5 51.5 62.0 64.0 67.0 L0 80.5 64.5 71.0 80.0 60.0 75.0 69.0 78.0 L3 65.5 53.0 51.5 67.0 43.0 65.0 58.5 60. L0 82.0 80.5 77.5 82.0 17.5 83.0 61.5 79.5 Japanese L2 L1 79.5 83.5 77.0 80.0 68.5 75.0 76.5 85.5 17.0 18.5 72.0 79.5 59.0 57.0 70.0 80.5 L3 79.5 73.0 66.5 78.0 17.5 73.5 53.5 72.5 L0 79.2 76.6 73.2 78.9 48.6 77.3 64.7 76.4 Average L2 L1 73.1 74.9 68.5 71.3 64.6 67.8 70.2 75.9 40.4 43.9 67.8 71.8 55.5 56.8 66.4 69.8 L3 69.5 65.3 60.2 70.4 39.0 68.4 53.2 65. Table 3: Accuracy comparison of models on mSCoRe-G from complexity level 0 (L0) to 3 (L3). OpenAI o1 (Jaech et al., 2024): reasoning-reinforced model based on GPT-4o, specifically optimized for complex problem-solving tasks through an additional training phase utilizing data curated for chain-of-thought reasoning. LLaMA-3.3-70B and LLaMA-3.1-8B (Grattafiori et al., 2024): Two open-sourced LLMs representing different parameter scales, trained on publicly available sources spanning various domains, allowing us to analyze the impact of model size on reasoning capabilities. Distilled DeepSeek-R1 (R1-70B and R1-8B) (DeepSeek-AI et al., 2025): reasoning-focused model derived from the LLaMA architecture, distilled using samples generated by the large-scale LRM DeepSeek-R1. Aya-32B (Dang et al., 2024): universal multilingual model trained on data from 200 languages, providing insights into broad multilingual LLM reasoning capabilities. For evaluation, we employ consistent prompt for all models, providing the proposed reasoning skill taxonomy (section 3.1) with step-by-step instructions to generate the desired reasoning process before answering. Further experimental details are in Appendix A."
        },
        {
            "title": "4.2 Main Results",
            "content": "Reddit TikTok Social Commonsense GPT-4o o1 o1-mini LLaMA-3.3-70B LLaMA-3.1-8B R1-70B R1-8B Aya-32B Tables 3 and 4 present our main results on mSCoRe-G and mSCoRe-S, respectively. Overall, we observe consistent pattern across all models where performance declines as complexity level increases. For mSCoReG, GPT-4o achieves the highest overall accuracy on general commonsense reasoning across all languages and complexity levels. While this can be an artifact of the benchmark creation process where GPT-4o was used for data generation, LLaMA-3.3-70B results are very close to GPT-4o. Furthermore, the open-source model significantly outperforms others on social commonsense reasoning (over 5% average improvement across all levels and domains). Table 4: Accuracy comparison of models on mSCoRe-S. Average L2 L1 65.5 68.0 65.5 70.0 57.5 62.3 76.8 75.8 28.3 28.3 64.8 66.3 58.5 59.3 62.0 60. L3 63.5 61.5 59.5 73.0 29.5 62.5 52.5 60.5 L3 69.5 69.0 59.0 76.5 27.0 67.5 61.0 59.5 L2 62.5 63.5 53.0 73.5 29.5 62.0 56.5 61.0 L2 68.5 67.5 62.0 80.0 27.0 67.5 60.5 63.0 L3 66.5 65.3 59.3 74.8 28.3 65.0 56.8 60.0 L1 69.0 69.0 62.5 75.0 29.5 65.5 60.0 64. L1 67.0 71.0 62.0 76.5 27.0 67.0 58.5 57.5 L0 71.0 69.5 63.5 80.0 30.5 68.5 64.0 68.0 L0 75.0 77.0 72.5 83.5 27.0 73.5 65.0 71.0 L0 73.0 73.3 68.0 81.8 28.8 71.0 64.5 69.5 Multilingual and Cultural Results: Performance is generally similar across languages in mSCoRe-G. This may be due to all languages in the seed dataset mCSQA being medium to high resource languages. Future work should explore other seed datasets with more lowresource languages. For social commonsense reasoning in mSCoRe-S, most models perform better on Reddit-sourced questions than TikTok-sourced ones. This could be attributed to Reddit containing more content on general Community and Cultural Exchange, whereas TikTok focuses more on daily life personal\" aspects like Social Norms and Etiquette\". This suggests that LLMs might still struggle with more personalized problems, as noted in Davis (2024). Unexpectedly, despite being trained on 200 diverse languages, the most multilingual model Aya-32B does not perform very well, even in cultural social commonsense benchmark. Model Scale: We compare models with different parameter counts, from the 8B and 70B parameters open-source models (LLaMA and R1), to the colossal-scale (hundreds of billions of parameters) closed-source LLMs (GPT-4o and o1). Larger models generally perform better across both benchmarks. The performance gap between 70B and 8B versions was substantial in most cases. However, we observe diminishing returns when moving from 70B to colossal-scale LLMs. This finding suggests it takes more than simple parameter scaling to 7 solve commonsense reasoning, especially in understanding social interactions and cultural norms. Figure 6: The distribution of reasoning skills of reference reasoning process (A and C), and o1s reasoning process (B and D), as question complexity increases from complexity level 0 to 6. Reasoning-reinforced Training: We compare general instruction-tuned models (GPT-4o, LLaMA) with reasoning-reinforced fine-tuned models (o1 and R1). While the state-of-theart LRM o1 performs best in English, it lags behind other general LLMs like GPT-4o and LLaMA-3.3-70B in other languages. This suggests that reasoning-reinforced training might decrease commonsense reasoning ability, likely due to the highly specialized training data for more complex tasks like coding and math. Interestingly, LLaMA-3.1-8B fails the task in English and Japanese, but R1-8B performs normally, indicating that reasoning-reinforced training helps smaller-scale models understand tasks better."
        },
        {
            "title": "5.1 Complexity Scaling Results",
            "content": "Social (Average) L4 L2 L3 66.3 66.5 65.5 65.5 62.8 65.3 75.5 74.8 76.8 28.0 28.3 28.3 60.3 65.0 64.8 59.3 56.8 58.5 57.8 60.0 62.0 General (English) L4 L3 L2 72.0 71.5 72.5 70.0 72.0 75.0 68.5 70.0 69.0 20.5 21.5 21.5 69.0 69.0 69.5 57.0 55.0 62.0 58.0 66.0 66.5 L0 80.5 82.5 78.5 23.0 79.5 67.5 77.5 L0 73.0 73.3 81.8 28.8 71.0 64.5 69.5 L6 66.0 63.0 73.3 28.0 65.8 61.5 54.0 L1 68.0 70.0 75.8 28.3 66.3 59.3 60. L6 68.0 67.0 69.0 20.5 65.5 55.0 54.5 L1 70.0 73.5 75.0 22.5 70.5 62.0 67.0 GPT-4o o1 LLaMA-3.3-70B LLaMA-3.1-8B Deepseek-70B Deepseek-8B Aya-32B L5 L5 70.0 67.3 69.5 60.8 72.5 68.5 20.5 28.0 69.0 62.8 59.5 59.8 60.0 56.5 Table 5: Performance comparison from complexity level 0 to level 6. To further understand model capacity against scaling question complexity, we expand our results for mSCoRe-G (English) and mSCoRe-S to complexity level 6. As shown in Table. 5, every model accuracy continues to decline to L6. The most significant performance drop occurs between L0 and L2, indicating that even relatively simple complexity scaling introduces substantial challenges for LLMs. At higher difficulty levels (L3 to L6), the rate of degradation slows down considerably. This plateau suggests that our current approach to scaling complexity through additional context and reasoning steps may reach saturation point. This could indicate that the multiplechoice question-answer format itself imposes certain limitations on how effectively task difficulty can be scaled. Alternative task formulations that require more sophisticated forms of reasoning beyond the current design might be necessary to create more discriminative benchmarks for future, more capable models."
        },
        {
            "title": "5.2 Skill Type Utilization",
            "content": "To better understand how models employ different reasoning skills across varying complexity levels, Fig. 6 visualizes the distribution of reasoning skills used in both the reference reasoning processes (from our benchmark creation) and the output reasoning processes generated by o1. For general commonsense, both reference and model-generated reasoning primarily utilize logical reasoning skills, with deductive reasoning being most common. However, the reference distribution shows greater diversification of skills at higher complexity levels, incorporating more contextual reasoning (especially analogical and probabilistic reasoning). In contrast, models like o1 remains heavily dependent on deductive reasoning across all complexity levels. For social commonsense, the reference distribution shows more balanced utilization of skills from all three categories, with social and ethical reasoning becomes progressively more important for higher-level questions. While o1 model incorporates some social reasoning skills, it still over-relies on logical reasoning for scenarios where social and contextual reasoning would be more appropriate. Overall, results reveal significant 8 limitations in o1s ability to adapt its reasoning strategy. The rigid reasoning pattern likely explains the models performance decrease on higher complexity questions, highlighting the need for more balanced reasoning-reinforced training approaches."
        },
        {
            "title": "5.3 Different Reasoning Skill Taxonomies",
            "content": "We investigate how models adapt to different reasoning taxonomies, including: (1) Chain-ofThought (CoT) - Standard chain-of-thought, not requiring skill identification, (2) Logical - Only using logical reasoning skills (deductive, inductive and abductive) (3) General - Each reasoning step is categorized into one of the three general categories (logical, contextual, and social). Table 6: Results for Different Reasoning Skill Taxonomies. o1 o1-mini cot-o1 cot-o1-mini logical-o1 logical-o1-mini general-o1 general-o1-mini General L2 L1 68.5 71.3 64.6 67.8 66.2 69.3 60.2 65.2 66.3 72.1 62.5 68.3 67.5 69.9 61.4 67.7 L0 76.6 73.2 75.9 71.7 77.3 73.9 77.7 73.3 L3 65.3 60.2 61.3 57.8 65.6 62.2 65.8 59. L0 73.3 68.0 63.3 60.8 72.8 64.8 69.3 66.8 Social L1 70.0 62.3 49.3 51.5 64.3 59.8 54.5 61.8 L2 65.5 57.5 44.5 46.5 59.8 59.3 51.5 57.0 L3 65.3 59.3 40.3 45.3 58.3 50.5 48.3 52.8 Table 6 shows the average accuracies of o1 and o1-mini for each setting. Interestingly, despite requiring models to distinguish between more skill types, our proposed fine-grained taxonomy yields the best results. As expected from our previous analysis, the Logicalonly approach performs relatively well on general commonsense tasks but worse on social tasks. The General setting also under-performs ours, suggesting that granularity of skill identification benefits commonsense reasoning by encouraging models to consider broader range of reasoning approaches rather than defaulting to familiar patterns. Finally, CoT performs notably worse than all structured skill-based approaches, especially for social commonsense at higher complexity levels. This demonstrates that reasoning without explicit skill categorization may be insufficient for more complex commonsense situations."
        },
        {
            "title": "5.4 Reasoning Efficiency",
            "content": "To examine the relationship between reasoning efficiency and task complexity across different models, Fig. 7 visualizes the average number of steps of the reasoning processes of mSCoRe and GPT-4o and o1s answers across different complexity levels. The reference reasoning processes show clear linear increase in reasoning steps as complexity level increases, with social commonsense reasoning requiring more steps than general commonsense at each level. GPT-4os reasoning processes show similar upward trend but with more gradual slope, whereas o1s reasoning processes maintain nearly constant step count (around 3 steps) regardless of task complexity. The results indicate higher-level complexities require more steps, and current model are unable to reason longer, unless explicitly forced to (such as in the Complexity Expansion step described in Section 3.2). This is similar to the number of reasoning tokens (more steps is equivalent to more tokens) used in test-time scaling research recently introduced in Muennighoff et al. (2025). These findings indicate that adapting reasoning depth dynamically based on task demands is likely crucial for sustaining performance as complexity escalates. Figure 7: Average number of reasoning steps in the reasoning processes of mSCoRe(straight), GPT-4o (barred), and o1 (dotted)."
        },
        {
            "title": "6 Conclusion\nWe introduce mSCoRe - Multilingual and Scalable Benchmark for Skill-based Common-\nsense Reasoning, a novel evaluation framework designed to address critical gaps in existing\nbenchmarks for commonsense reasoning. By integrating multilingual and diverse cultural\ncoverage, a fine-grained reasoning skill taxonomy, and a dynamic complexity scaling mech-\nanism, mSCoRe provides a comprehensive platform for systematically evaluating not only\nthe accuracy but also skill utilization and efficiency of LLMs’ commonsense reasoning\nprocess. Extensive experiments on eight state-of-the-art LLMs reveal that current models\nstill consistently struggled with higher complexity levels and culturally nuanced social com-\nmonsense scenarios. Our analysis highlights several promising directions for improvement,",
            "content": "9 including more robust training methodologies to enhance models reasoning skill utilization and efficiency. Additionally, mSCoRe provides framework for subsequent benchmarks to scale with the rapid development of LLMs in the future."
        },
        {
            "title": "References",
            "content": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.), ThirtyEighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 1768217690. AAAI Press, 2024. doi: 10.1609/AAAI.V38I16.29720. URL https://doi.org/10.1609/aaai.v38i16.29720. Sijia Chen, Baochun Li, and Di Niu. Boosting of thoughts: Trial-and-error problem solving In The Twelfth International Conference on Learning Repwith large language models. resentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=qBL04XXex6. Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Campagnolo Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=Q6a9W6kzv5. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, and et al. Tom Kocmi. Aya expanse: Combining research breakthroughs for new multilingual frontier, 2024. URL https://arxiv.org/abs/2412.04261. Ernest Davis. Benchmarks for automated commonsense reasoning: survey. ACM Comput. Surv., 56(4):81:181:41, 2024. doi: 10.1145/3615355. URL https://doi.org/10.1145/ 3615355. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and et al. Aixin Liu. Deepseekr1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Benchmark probing: Investigating data leakage in large language models. In NeurIPS 2023 Workshop on Backdoors in Deep Learning - The Good, the Bad, and the Ugly, 2024. URL https:// openreview.net/forum?id=a34bgvner1. Quyet V. Do, Junze Li, Tung-Duong Vuong, Zhaowei Wang, Yangqiu Song, and Xiaojuan Ma. What really is commonsense knowledge? CoRR, abs/2411.03964, 2024. doi: 10.48550/ ARXIV.2411.03964. URL https://doi.org/10.48550/arXiv.2411.03964. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Järviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. Frontiermath: benchmark 10 for evaluating advanced mathematical reasoning in AI. CoRR, abs/2411.04872, 2024. doi: 10.48550/ARXIV.2411.04872. URL https://doi.org/10.48550/arXiv.2411.04872. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, and et al. Aurelien Rodriguez. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/ 2407.21783. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseekcoder: When the large language model meets programming - the rise of code intelligence. CoRR, abs/2401.14196, 2024. doi: 10.48550/ARXIV.2401.14196. URL https://doi.org/10. 48550/arXiv.2401.14196. Daichi Haraguchi, Kiyoaki Shirai, Naoya Inoue, and Natthawut Kertkeidkachorn. Discovering highly influential shortcut reasoning: An automated template-free approach. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 64016407. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP.424. URL https://doi.org/10.18653/v1/2023.findings-emnlp.424. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 38283850. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024. ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. Advancing language model reasoning through reinforcement learning and inference scaling. CoRR, abs/2501.11651, 2025. doi: 10.48550/ARXIV.2501.11651. URL https://doi.org/10.48550/arXiv.2501.11651. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Kshitij Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan Firat. Big-bench extra hard. CoRR, abs/2502.19187, 2025. doi: 10.48550/ARXIV.2502.19187. URL https://doi.org/10.48550/arXiv.2502.19187. Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip H. S. Torr, Salman Khan, and Fahad Shahbaz Khan. Llm post-training: deep dive into reasoning large language models, 2025. Cheng Li, Damien Teney, Linyi Yang, Qingsong Wen, Xing Xie, and Jindong Wang. Culturepark: Boosting cross-cultural understanding in large language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, 11 Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/ 2024/hash/77f089cd16dbc36ddd1caeb18446fbdd-Abstract-Conference.html. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=v8L0pN6EOi. Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. Common sense beyond English: Evaluating and improving multilingual language models for commonsense reasoning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 12741287, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long. 102. URL https://aclanthology.org/2021.acl-long.102/. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel J. Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. CoRR, abs/2501.19393, 2025. doi: 10.48550/ARXIV.2501.19393. URL https://doi.org/10.48550/arXiv.2501.19393. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, and et al. Alex Paino. Gpt-4o system card, 2024. URL https://arxiv.org/ abs/2410.21276. Sekta Lonir Oscarini and Wati Bhakti. Blooms taxonomy: Original and revised. In BLOOMS TAXONOMY: ORIGINAL AND REVISED, 2010. URL https://api.semanticscholar.org/ CorpusID:146700427. Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. XCOPA: multilingual dataset for causal commonsense reasoning. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 23622376, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.185. URL https://aclanthology.org/2020.emnlp-main.185/. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011. AAAI, 2011. URL http://www.aaai. org/ocs/index.php/SSS/SSS11/paper/view/2418. Yusuke Sakai, Hidetaka Kamigaito, and Taro Watanabe. mCSQA: Multilingual commonsense reasoning dataset with unified creation strategy by language models and humans. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1418214214, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.844. URL https://aclanthology.org/2024.findings-acl.844/. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. CoRR, abs/1904.09728, 2019. URL http://arxiv.org/abs/1904.09728. Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Sunny Yu, Raya Horesh, Rogério de Paula, and Diyi Yang. Culturebank: An online community-driven knowledge base towards culturally aware language technologies. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pp. 49965025. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.findings-emnlp.288. 12 Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. CoRR, abs/2408.03314, 2024. doi: 10.48550/ARXIV.2408.03314. URL https://doi.org/10.48550/arXiv.2408. 03314. Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, and Conghui He. Benchmarking Chinese commonsense reasoning of LLMs: From Chinese-specifics to reasoning-memorization correlations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1120511228, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 604. URL https://aclanthology.org/2024.acl-long.604/. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 1300313051. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-ACL. 824. URL https://doi.org/10.18653/v1/2023.findings-acl.824. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 41494158. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1421. URL https://doi.org/10.18653/v1/n19-1421. A. T. Welford. Taxonomies of human performance, edwin a. fleishman and marilyn k. quaintance, academic press, orlando, florida, 1984. no. of pages: xvi + 514. price: $49. Journal of Organizational Behavior, 7(2):155156, 1986. doi: https://doi.org/10.1002/job.4030070208. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/job.4030070208. Wikipedia contributors. Commonsense knowledge (artificial intelligence) Wikipedia, the free encyclopedia, 2025. URL https://en.wikipedia.org/w/index.php? [Online; accessed 25-March-2025]. Dingjun Wu, Jing Zhang, and Xinmei Huang. Chain of thought prompting elicits knowledge augmentation. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 65196534. Association for Computational Linguistics, 2023. doi: 10.18653/V1/ 2023.FINDINGS-ACL.408. URL https://doi.org/10.18653/v1/2023.findings-acl.408. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for openended solutions. CoRR, abs/2411.14405, 2024. doi: 10.48550/ARXIV.2411.14405. URL https://doi.org/10.48550/arXiv.2411.14405."
        },
        {
            "title": "A Experimental Details",
            "content": "LLMs Details: For closed commercial LLMs (GPT-4o, o1 and o1-mini), we query responses from the models using OpenAI Chat Completions API1, with temperatures set to 0 for deterministic outputs. Open source models ( Deepseek R1-70B2 and R1-8B3, LLaMA-3.3-70B4 and LLaMA-3.1-8B5, Aya-32B6 ) are run using 2 NVIDIA A100 80GB GPUs. PyTorch 2.1.27 and Huggingface-Transformer 4.42.3 8 are used to implement the models. Source code with specification of all dependencies, including external libraries: Our data and source code will be released upon acceptance of the paper. A.1 Reasoning Skill Details We provide detail descriptions with abstract and concrete example for each of our reasoning skills in Fig. 8. Abstract examples are generalized representation that uses variables or placeholders to illustrate pattern or principle of the reasoning skills. In contrast, concrete examples are the application of the corresponding reasoning skills to specific real-world scenario."
        },
        {
            "title": "B Data Generation Details",
            "content": "B.1 LLM-Judge Judge Model We utilize Flow Judge, general LLM-as-a-judge model developed by Flow AI9. Flow Judge is an open-source 3.8B parameter language model designed for LM-based evaluations, offering high performance and accuracy comparable to much larger models like GPT-4o and Claude 3.5 Sonnet. It is trained on evaluation data across various domains to supports custom evaluation criteria, multiple scoring scales, qualitative feedback, and produces structured evaluation outputs. Scoring Metrics for Seed Data Filtering step: We provide the full rubrics used for Seed Data Filtering step for mSCoRe-G and mSCoRe-S in Fig. 9 and 10, correspondingly."
        },
        {
            "title": "C Prompt Details",
            "content": "We provide here all of the prompt templates in full version used in our experiments. Fig. 11, 12, and 13 presents the prompt of Structured Reasoning Generation, Data Complexity Scaling and Commonsense Implicitation steps in our data generation process for mSCoReG. Additionally, Fig. 15 presents the prompt of Structured Reasoning Generation step for mSCoRe-S (The other two steps remain the same between the two subsets). We provide an example of complexity level 0 to 3 for mSCoRe-G and mSCoRe-S in Fig. 14 and 16, correspondingly. 1https://platform.openai.com/docs/guides/text-generation 2https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B 3https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B 4https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct 5https://huggingface.co/meta-llama/Llama-3.1-8B 6https://huggingface.co/CohereForAI/aya-expanse-32b 7https://pytorch.org/get-started/pytorch-2.0/ 8https://github.com/huggingface/transformers 9https://www.flow-ai.com/blog/flow-judge 14 { \"inductive_reasoning\": { \"short_description\": \"Drawing general conclusions from specific observations.\", \"long_description\": \"Inductive reasoning is method of drawing general conclusions from specific observations. Unlike deductive reasoning, which starts with general premises to reach specific conclusions, inductive reasoning begins with detailed facts and builds up to broader generalizations or theories. This approach is commonly used in scientific research, where repeated experiments and observations lead to the formulation of overarching principles or hypotheses \", \"abstract_example\": \"After witnessing several instances where Event A_1 leads to Event A_2, you infer that Event A_n will similarly result in Event A_2 in future occurrences\", \"concrete_example\": \"After witnessing several instances where the weather forecast predicts rain, you infer that rain will likely continue to fall in the future\" }, \"deductive_reasoning\": { \"short_description\": \"Deriving specific conclusions from general premises.\", \"long_description\": \"Deductive reasoning involves deriving specific conclusions from general premises. It ensures that if the premises are true and the reasoning is valid, the conclusion must also be true. Deductive logic is fundamental in fields that require rigorous proof, such as mathematics and formal sciences.\", \"abstract_example\": \"Given the premise that All are Y, and knowing that Object x_1 is an X, you deduce that Object x_1 must also be Y.\", \"concrete_example\": \"Given All birds have feathers. sparrow is bird. Therefore, sparrow has feathers\" }, \"abductive_reasoning\": { \"short_description\": \"Forming hypotheses to explain observations.\", \"long_description\": \"Abductive reasoning is the process of forming hypotheses to explain observations. It starts with an incomplete set of observations and proceeds to the likeliest possible explanation. Unlike deductive and inductive reasoning, abductive reasoning seeks the simplest and most plausible explanation for given set of facts, often leading to the generation of new theories or hypotheses.\", \"abstract_example\": \"Observing Event B, you hypothesize that Reason 2 is the most plausible explanation among several possible causes.\", \"concrete_example\": \"You wake up and see that the street is wet. The most likely explanation is that it rained last night.\" }, \"analogical_reasoning\": { \"short_description\": \"Drawing parallels between similar situations to infer conclusions.\", \"long_description\": \"Analogical reasoning involves drawing parallels between similar situations to infer conclusions. By comparing two objects or systems that share certain characteristics, one can infer that they may share additional, unobserved properties. This form of reasoning is widely used in problem-solving, scientific discovery, and legal reasoning to transfer knowledge from known domain (source) to an unknown domain (target). Analogical reasoning is also used in everyday life to make inferences about the similarities between objects or situations.\", \"abstract_example\": \"Think of Situation C_a, where Component C_a_1 interacts with Component C_a_2 in specific way. You encounter Situation C_b with Component C_b_1 and Component C_b_2, and infer that Component C_b_1 and Component C_b_2 will interact similarly in Situation C_a.\", \"concrete_example\": \"Just as gardener waters plants to help them grow, teacher provides knowledge and guidance to help students develop.\" }, \"counterfactual_reasoning\": { \"short_description\": \"Considering alternative scenarios and outcomes that did not happen.\", \"long_description\": \"Counterfactual reasoning entails considering alternative scenarios and outcomes that did not occur. It involves imagining 'what might have happened' under different circumstances, which is useful for understanding causality, evaluating decisions, and planning future actions. Counterfactual reasoning is often used in fields such as philosophy, psychology, and business to explore the potential consequences of different choices or actions.\", \"abstract_example\": \"Reflecting on Condition that did not occur, you imagine that if it had, Outcome might have replaced Outcome Z.\", \"concerte_example\": \"If you had left the house five minutes earlier, you would have caught the bus on time.\" }, \"probabilistic_reasoning\": { \"short_description\": \"Applying principles of probability to make inferences under uncertainty.\", \"long_description\": \"Probabilistic reasoning involves applying principles of probability to make inferences under uncertainty. It enables individuals to assess the likelihood of different outcomes and make informed decisions based on the probability of various events occurring. This type of reasoning is crucial in fields like statistics, risk assessment, and artificial intelligence.\", \"abstract_example\": \"Evaluating that Option has higher probability (P(A) > P(B)) of success than Option B, you decide to choose Option A.\", \"concrete_example\": \"There is 70% chance of rain tomorrow, so you decide to carry an umbrella when you go out.\" }, \"temporal_reasoning\": { \"short_description\": \"Understanding sequences and durations of events.\", \"long_description\": \"Temporal reasoning is the ability to understand and reason about the sequence and duration of events over time. It involves comprehending time-specific data, such as the order of events, how long events last, and the relationships between different time points. Temporal reasoning is essential in areas like scheduling, planning, and understanding narratives.\", \"abstract_example\": \"Planning your day, you schedule Event T_1 to occur before Event T_2, ensuring the correct sequence of activities.\", \"concrete_example\": \"You observe that the sun will rise in the morning and set in the evening. You infer that the moon will rise and set at the same time.\" }, 15 \"spatial_reasoning\": { \"short_description\": \"Visualizing and manipulating objects in space.\", \"long_description\": \"Spatial reasoning entails visualizing and manipulating objects in space. It involves understanding the relationships between different objects, such as their position, orientation, and movement relative to each other. Spatial reasoning is fundamental in fields like engineering, architecture, geography, and various forms of visual arts, enabling individuals to solve problems related to the physical arrangement and movement of object.\", \"abstract_example\": \"While arranging furniture, you visualize Object S_1 and Object S_2 to determine their optimal placement within the room.\", \"concrete_example\": \"A architect determining the best location for window by visualizing the window and the surrounding walls to determine the optimal angle and height.\" }, \"social_reasoning\": { \"short_description\": \"Understanding social interactions and norms.\", \"long_description\": \"Social reasoning involves understanding social interactions and norms. It encompasses the ability to analyze and interpret social situations, recognize appropriate and inappropriate behaviors, and predict others' intentions, emotions, and thoughts. Effective social reasoning is crucial for building successful interpersonal relationships and navigating complex social environments.\", \"abstract_example\": \"Noticing that Person behaves certain way in Situation S, you adjust your own behavior (Behavior B) to interact effectively.\", \"concrete_example\": \"You notice that your friend looks upset after conversation, so you decide to ask them if they are okay.\" }, \"moral_reasoning\": { \"short_description\": \"Deciding what is right or wrong based on ethical principles.\", \"long_description\": \"Moral reasoning is the process of deciding what is right or wrong based on ethical principles. It involves evaluating actions, intentions, and consequences to make judgments about moral issues. Moral reasoning is central to ethical decision-making and is influenced by various factors, including societal norms, personal values, and philosophical theories.\", \"abstract_example\": \"Considering that Action could harm Person C, you decide it is morally wrong and choose an alternative that respects ethical principles.\", \"concrete_example\": \"Seeing someone drop their wallet, you decide to return it instead of keeping the money inside because it is the right thing to do.\" }} Figure 8: Reasoning skill details. 16 ### Commonsense-ness { \"task\": \"Evaluate the 'Commonsense-ness' of multiple-choice commonsense question.\", \"evaluation_criteria\": \"Does answering the question rely solely on commonsense knowledge accessible to the general population, or does it require formal reasoning and specialized expertise beyond everyday understanding?\", \"rubric\": { \"1\": \"The question requires formal reasoning and specialized expertise to answer correctly. It demands advanced knowledge in specific field, technical terminology, or in-depth understanding that goes beyond general life experience. The average person, relying only on commonsense knowledge, would find it challenging or impossible to select the correct answer without additional study or expertise.\", \"2\": \"The question can be addressed with some commonsense reasoning but may also require moderate specific knowledge or logical deduction. While not entirely dependent on formal expertise, it involves concepts or facts that are not universally known but can be reasoned through by an informed individual. The average person might answer correctly with thoughtful consideration but could also be misled without careful analysis.\", \"3\": \"The question is answerable using basic commonsense knowledge that is widely shared and understood by the general population. It does not rely on any specialized information or formal reasoning processes. The correct answer should be apparent to most people through everyday experience and general understanding of the world.\" } } ### Complexity { \"task\": \"Evaluate the 'Hardness/Complexity' of commonsense question.\", \"evaluation_criteria\": \"How difficult is the question to understand and answer? Does it require minimal reasoning or complex, multi-step thought process to identify the correct answer?\", \"rubric\": { \"1\": \"The question is very easy to understand, and the correct answer can be quickly identified with single, straightforward reasoning step. It requires minimal cognitive effort, and most individuals can arrive at the correct answer almost immediately without confusion.\", \"2\": \"The question is relatively easy to understand, requiring only couple of straightforward reasoning steps to identify the correct answer. While the question may introduce one or two elements that require brief consideration, the overall context remains clear. Most people can find the correct answer with small amount of thought.\", \"3\": \"The question is moderately challenging, necessitating several reasoning steps to accurately comprehend and resolve. It introduces multiple elements or scenarios that require careful thought process to integrate and analyze. Many individuals will need to pause and deliberately work through the connections or implications before reaching the correct answer.\", \"4\": \"The question is hard to comprehend and necessitates complex thought process with multiple reasoning steps. It may involve abstract concepts, less obvious relationships, or misleading information that requires careful analysis. Individuals must invest significant cognitive effort to work through the complexities and identify the correct answer.\", \"5\": \"The question is very hard to comprehend and requires long reasoning process with multiple reasoning steps to find the right answer. It demands high-level critical thinking, problem-solving skills, and possibly specialized knowledge. Only with thorough analysis and persistence can individuals navigate the complexity to arrive at the correct answer.\" } } ### Expandability { \"task\": \"Evaluate the 'Expandability' of commonsense question.\", \"evaluation_criteria\": \"To what extent can the question be expanded or elaborated upon to introduce additional complexity or dimensions?\", \"rubric\": { \"1\": \"The question cannot be expanded. It is inherently simplistic and covers very narrow topic or scenario. There is little to no room for introducing additional elements, dimensions, or complexity without altering the fundamental nature of the question. The question stands effectively as self-contained unit with minimal potential for elaboration.\", \"2\": \"The question has some potential for expansion. While it currently covers its intended scope adequately, there is moderate room to add few additional elements or explore related themes that could introduce more complexity. The question can be expanded moderately by incorporating extra conditions, perspectives, or related scenarios, but such additions are not numerous.\", \"3\": \"The question can be significantly expanded to become more complex question. It has ample scope for adding new dimensions, scenarios, or layers of reasoning. By introducing additional variables, conditional information, or intricate details, the question can transform into more challenging problem that requires advanced reasoning and deeper comprehension.\" } } Figure 9: Rubrics used for mCSQA Data Filtering Process 17 ### Multicultureness { \"task\": \"evaluate the 'Multicultural-ness' of commonsense cultural situation\", \"evaluation_criteria\": \"Does the situation involve interactions between multiple distinct cultures, reflecting blend of practices, norms, or etiquette from each?\", \"rubric\": { \"1\": \"The situation is primarily rooted in single culture, without significant influence or interaction from other cultural norms or practices. The interactions and behaviors exhibited are almost exclusively aligned with one cultural tradition, lacking blend of cultural elements or considerations from another distinct culture.\", \"2\": \"The situation involves elements from two cultures, showing some level of cross-cultural interaction. While both cultural influences are present, the interaction may largely reflect the dominance of one culture over the other, with limited integration or blending of unique practices, norms, or etiquette from both cultures.\", \"3\": \"The situation reflects rich blend of cultural interactions involving more than two distinct cultures. It demonstrates balanced integration of diverse cultural practices, norms, or etiquette. The interactions and behaviors of the parties involved show deep understanding and appreciation of multiple cultural perspectives, leading to an enriching multicultural exchange.\" } } ### Commonsenseness { \"task\": \"evaluate the 'Commonsense-ness' of cultural situation\", \"evaluation_criteria\": \"To what extent can the situation be understood and addressed using basic commonsense knowledge, without requiring specialized or expert reasoning?\", \"rubric\": { \"1\": \"The situation requires formal reasoning and specialized expertise to understand and address appropriately. It involves complex cultural nuances or specific knowledge that goes beyond general commonsense understanding. Responding effectively necessitates familiarity with detailed cultural protocols or insider knowledge.\", \"2\": \"The situation can be partially addressed using commonsense knowledge, but some elements require deeper understanding or contextual insights that may not be readily apparent to someone without specific cultural awareness. While general reasoning can guide some actions, certain aspects benefit from additional cultural knowledge or experience.\", \"3\": \"The situation can be appropriately addressed using basic commonsense reasoning. It involves straightforward cultural interactions that do not demand specialized knowledge. Commonsense understanding of general social norms and human interactions is sufficient to respond suitably and effectively in this context.\" } } ### Complexity { \"task\": \"Evaluate the 'Complexity' of cultural situation.\", \"evaluation_criteria\": \"How intricate is the cultural situation in terms of nuances, number of cultural elements, perspectives, social dynamics, and interactions, requiring varying depths of understanding to navigate appropriately?\", \"rubric\": { \"1\": \"The situation is very simple, involving single cultural aspect with straightforward practices and minimal perspectives or interactions. Understanding and responding require little to no specialized knowledge or awareness of cultural nuances.\", \"2\": \"The situation has minor complexity, incorporating couple of cultural elements or perspectives with basic interactions. There are some cultural nuances, but they are easily understood with general awareness. Navigating the situation may require modest cultural sensitivity but is generally manageable.\", \"3\": \"The situation is moderately complex, involving several cultural elements, multiple perspectives, and noticeable social dynamics. Understanding and responding appropriately require some cultural knowledge and sensitivity to nuances. There is potential for misunderstandings without moderate level of cultural competence.\", \"4\": \"The situation is complex, featuring numerous cultural elements, diverse perspectives, intricate social dynamics, and significant interactions. Navigating the situation effectively necessitates considerable cultural competence, an awareness of subtle nuances, and an understanding of how different cultural norms might conflict or interact.\", \"5\": \"The situation is highly complex, encompassing multitude of deeply intertwined cultural elements, perspectives, and interactions. It includes profound cultural nuances, ambiguous social cues, and high potential for misunderstandings. Expert knowledge and significant experience are required to address it appropriately, as the situation may involve conflicting norms and requires advanced cultural navigation skills.\" } } ### Expandability { \"task\": \"Evaluate the 'Expandability' of cultural situation\", \"evaluation_criteria\": \"Assess the potential for the situation to be expanded by including additional cultural dimensions, participants, interactions, and its adaptability to different contexts.\", \"rubric\": { \"1\": \"The situation is tightly defined within single cultural framework, offering little room for the addition of new cultural dimensions. It does not easily support additional participants or interactions, requiring significant adaptation for expansion. It is context-specific and struggles to adapt to different settings or applications.\", \"2\": \"The situation allows for the inclusion of some additional cultural dimensions without drastically altering the core context. It can accommodate more participants or interactions with some adjustments to existing dynamics. There is some flexibility for adaptation to similar contexts or applications, albeit with moderate effort needed.\", \"3\": \"The situation is flexible and open, easily incorporating multiple new cultural dimensions or elements. It naturally supports additional participants and interactions without losing coherence. It is broadly applicable and adaptable across varied contexts and applications, maintaining core effectiveness and relevance.\" } } Figure 10: Rubrics used for CultureBank Data Filtering Process 18 ### LLM ROLE You are language model with advanced commonsense reasoning skills, capable of logical and analytical reasoning, heuristic and intuitive thinking, comparative and hypothetical analysis, and contextual and specialized understanding. ### TASK DESCRIPTION Given multi-choice commonsense questions with the correct option, you task is to provide \"COMMONSENSE CONTEXT\" to expand on the given question and detailed \"REASONING PROCESS\" that involves multiple \"REASONING STEPs\" to arrive at the correct answer. + \"COMMONSENSE CONTEXT\" to the question refers to the background knowledge or additional details that are generally understood without requiring specialized knowledge, including factors such as time, place, social norms, cultural influences, and other relevant details that shape the understanding of the topic. + Each \"REASONING STEP\" should be an \"ATOMIC REASONING STEP\" an Indivisible Unit of reasoning that predominantly utilizes one reasoning skill. It is single, coherent thought process that cannot be broken down into smaller steps without losing its meaning. The \"REASONING PROCESS\" must be as efficent as possible, only using the minimum number of steps necessary, ensuring that each step is non-redundant and contributes to narrowing down the possible options by eliminating one or more answer choices. ### STEP-BY-STEP INSTRUCTIONS Following these Step-by-Step Instructions: 1. Question Comprehension: Read the question carefully along with all the provided answer options. 2. Adding The \"COMMONSENSE CONTEXT\": Expand on the original question by providing an additional \"COMMONSENSE CONTEXT\". Ensure that the added context is relevant and enriches the understanding of the question. 3. Describe your Step-by-Step \"REASONING PROCESS\" to arrive at the correct answer. Each \"ATOMIC REASONING STEP\" must following this sequence: 3.1. Choose REASONING SKILL below to be used by the REASONING STEP: + inductive_reasoning: Drawing general conclusions from specific observations. + deductive_reasoning: Deriving specific conclusions from general premises. + abductive_reasoning: Forming hypotheses to explain observations. + analogical_reasoning: Drawing parallels between similar situations to infer conclusions. + counterfactual_reasoning: Considering alternative scenarios and outcomes that did not happen. + probabilistic_reasoning: Applying principles of probability to make inferences under uncertainty. + temporal_reasoning: Understanding sequences and durations of events. + spatial_reasoning: Visualizing and manipulating objects in space. + social_reasoning: Understanding social interactions and norms. + moral_reasoning: Deciding what is right or wrong based on ethical principles. 3.2. Apply the choosen \"REASONING SKILL\": provide concise explanation of how the chosen \"REASONING SKILL\" is applied to eliminate certain answer options or reinforce the correct answer option. Ensure the reasoning is clear and cannot be further divided into smaller steps. 3.3. Eliminate Options: List the options eliminated in this step based on your reasoning. 3.4. Update Possible Options: Provide the list of remaining possible options after this step. 4. Generate your output in the JSON format with the following structure: ```json { \"commonsense_context\": \"context_text\", \"commonsense_question\": \"question_text\", \"options\": { \"A\": \"option_answer_text_A\", ... }, \"correct_answer\": [\"answer_option\", \"answer_text\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"reasoning_skill_name\", \"reasoning\": \"reasoning_text\", \"eliminated_options\": [list_of_eliminated_options], \"possible_options\": [list_of_remaining_options] }, ... \"reasoning_step_n\": { \"reasoning_skill\": \"reasoning_skill_name\", \"reasoning\": \"reasoning_text\", \"eliminated_options\": [list_of_eliminated_options], \"possible_options\": [list_of_remaining_options] } } } ``` ### IN-CONTEXT EXAMPLE: <.....> ### OUTPUT REMINDER Ensure that your output follows the JSON structure as instructed and demonstrated in the in-context example. ### INPUT: { \"question\": \"What is the best way to experience live performance?\", \"options\": { \"A\": \"watch play\", \"B\": \"go to theatre\", \"C\": \"open eyes\", \"D\": \"check showtimes\", \"E\": \"buy tickets\" }, \"correct_answer\": [\"B\", \"go to theatre\"] } Figure 11: Prompt for Structured Reasoning Generation step for mSCoRe-G (English). 19 ### LLM ROLE You are language model with advanced commonsense reasoning skills, capable of logical and analytical reasoning, heuristic and intuitive thinking, comparative and hypothetical analysis, and contextual and specialized understanding. ### TASK DESCRIPTION Given multi-choice commonsense question with its options, your task is to modify and expand it to create more complex question by expanding its context, modifying the question, adjusting the answer options, and adding an additional REASONING STEP. Your output should include the expanded context, the modified question, revised answer options, the correct answer, and detailed \"REASONING PROCESS\". ### STEP-BY-STEP INSTRUCTIONS Following these Step-by-Step Instructions: 1. Question Comprehension: Carefully read the given question and the context, and its answer options. 2. Context Expansion: adding additional backgound or situaltional details to the \"COMMONSENSE CONTEXT\" to add depth and reasoning requirements to the question. 3. Question Modificatioin: maintaining its core concept and commonsense. 4. Option Adjustments: Utilize the \"EXPANDED COMMONSENSE CONTEXT\" to craft more complex question while + Adjust the existing answer options to align with the new complex question + Ensure the correct answer option remains semantically similar to the original + Introduce an additional plausible but incorrect option to increase the complexity of the question + Keep all answer options as concise as the originals 5. Reasoning Refinements: Refine the original \"REASONING PROCESS\" to fit the new context. The additional \"ATOMIC REASONING STEP\" must use one of the following \"REASONING SKILLs\": + inductive_reasoning: Drawing general conclusions from specific observations. + deductive_reasoning: Deriving specific conclusions from general premises. + abductive_reasoning: Forming hypotheses to explain observations. + analogical_reasoning: Drawing parallels between similar situations to infer conclusions. + counterfactual_reasoning: Considering alternative scenarios and outcomes that did not happen. + probabilistic_reasoning: Applying principles of probability to make inferences under uncertainty. + temporal_reasoning: Understanding sequences and durations of events. + spatial_reasoning: Visualizing and manipulating objects in space. + social_reasoning: Understanding social interactions and norms. + moral_reasoning: Deciding what is right or wrong based on ethical principles. 6. Format the Output using JSON format with the following structure: ```json { \"commonsense_context\": \"context_text\", \"commonsense_question\": \"question_text\", \"options\": { \"A\": \"option_answer_text_A\", ... }, \"correct_answer\": [\"answer_option\", \"answer_text\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"reasoning_skill_name\", \"reasoning\": \"reasoning_text\", \"eliminated_options\": [list_of_eliminated_options], \"possible_options\": [list_of_remaining_options] }, ... \"reasoning_step_n\": { \"reasoning_skill\": \"reasoning_skill_name\", \"reasoning\": \"reasoning_text\", \"eliminated_options\": [list_of_eliminated_options], \"possible_options\": [list_of_remaining_options] } } } ``` ### IN-CONTEXT EXAMPLE: <.....> ### OUTPUT REMINDER Ensure that your output follows the JSON structure as instructed and demonstrated in the in-context example. ### INPUT: <.....> Figure 12: Prompt for Complexity Expansion step for mSCoRe-G (English). 20 ### LLM ROLE You are language model with advanced commonsense reasoning skills, capable of logical and analytical reasoning, heuristic and intuitive thinking, comparative and hypothetical analysis, and contextual and specialized understanding. ### TASK DESCRIPTION Your task is to perform \"Commonsense Implicitation,\" which involves combining given \"commonsense_context\" with \"question\" to generate new, concise commonsense question that implicitly incorporates the original context. This process aims to evaluate the commonsense reasoning abilities of LLMs by ensuring that the implicit context preserves the original reasoning process and maintains the correctness of the answer. ### STEP-BY-STEP INSTRUCTIONS Following these Step-by-Step Instructions: 1. Analyze the provided \"commonsense_context\" to understand the underlying assumptions and implicit knowledge required for reasoning 2. Examine the \"commonsense_question\" and its associated \"options\" to identify key elements essential for answering the question 3. Rewrite the \"commonsense_question\" by combining the original context and question to create more new \"commonsense_question\" with an \"IMPLICITLY IMPLIED COMMONSENSE CONTEXT\". Ensure that the new question remains clear and understandable 4. Verify that the \"REASONING PROCESS\" remains unchanged in the transformed question, and confirm that the correct answer remains the same as in the original 5. Ensure that all answer options are reasonable, relevant, and maintain their original intent in the context of the rewritten question 6. Retain the structure and content of the \"reasoning\" section to reflect the logical steps supporting the correct answer. The \"ATOMIC REASONING STEP\" must use one of the following \"REASONING SKILLs\": + inductive_reasoning: Drawing general conclusions from specific observations. + deductive_reasoning: Deriving specific conclusions from general premises. + abductive_reasoning: Forming hypotheses to explain observations. + analogical_reasoning: Drawing parallels between similar situations to infer conclusions. + counterfactual_reasoning: Considering alternative scenarios and outcomes that did not happen. + probabilistic_reasoning: Applying principles of probability to make inferences under uncertainty. + temporal_reasoning: Understanding sequences and durations of events. + spatial_reasoning: Visualizing and manipulating objects in space. + social_reasoning: Understanding social interactions and norms. + moral_reasoning: Deciding what is right or wrong based on ethical principles. ### IN-CONTEXT EXAMPLE: <.....> ### OUTPUT REMINDER Ensure that your output follows the JSON structure as instructed and demonstrated in the in-context example. ### INPUT: <.....> Figure 13: Prompt for Commonsense Implicitation step for mSCoRe-G (English). 21 ### ORIGINAL QUESTION FROM mCSQA: { \"question\": \"What is the best way to experience live performance?\", \"options\": { \"A\": \"watch play\", \"B\": \"go to theatre\", \"C\": \"open eyes\", \"D\": \"check showtimes\", \"E\": \"buy tickets\" }, \"correct_answer\": [ \"B\", \"go to theatre\" ] } ### COMPEXITY LEVEL 0: { \"commonsense_context\": \"Experiencing live performance, such as play, concert, or other theatrical event, typically involves attending venue where the performance is being held. The atmosphere, acoustics, and visual elements of live performance are best appreciated in person, where the audience can fully engage with the performers and the production. This is social and cultural activity that often involves planning, such as purchasing tickets and checking showtimes, but the core experience is being physically present at the venue.\", \"commonsense_question\": \"What is the best way to experience live performance?\", \"options\": { \"A\": \"watch play\", \"B\": \"go to theatre\", \"C\": \"open eyes\", \"D\": \"check showtimes\", \"E\": \"buy tickets\" }, \"correct_answer\": [\"B\", \"go to theatre\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"deductive_reasoning\", \"reasoning\": \"To experience live performance, one must be physically present where the performance is taking place. This means attending the venue, such as theatre, where the live event is held.\", \"eliminated_options\": [\"A\", \"C\"], \"possible_options\": [\"B\", \"D\", \"E\"] }, \"reasoning_step_2\": { \"reasoning_skill\": \"temporal_reasoning\", \"reasoning\": \"Checking showtimes and buying tickets are preparatory actions that occur before the actual experience of the live performance. They are necessary steps but do not constitute the experience itself.\", \"eliminated_options\": [\"D\", \"E\"], \"possible_options\": [\"B\"] } } } ### COMPLEXITY LEVEL 1 { \"commonsense_context\": \"Experiencing live performance, such as play, concert, or other theatrical event, typically involves attending venue where the performance is being held. The atmosphere, acoustics, and visual elements of live performance are best appreciated in person, where the audience can fully engage with the performers and the production. This is social and cultural activity that often involves planning, such as purchasing tickets and checking showtimes, but the core experience is being physically present at the venue. Additionally, live performances often include unique interactions between the audience and performers, such as applause, laughter, or even participation, which enhance the overall experience.\", \"commonsense_question\": \"In what way can you fully immerse yourself in the unique atmosphere and interactions of live performance?\", \"options\": { \"A\": \"watch recording at home\", \"B\": \"attend the theatre in person\", \"C\": \"read reviews online\", \"D\": \"check showtimes regularly\", \"E\": \"buy tickets in advance\", \"F\": \"listen to podcast about the performance\" }, \"correct_answer\": [\"B\", \"attend the theatre in person\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"deductive_reasoning\", \"reasoning\": \"To fully immerse oneself in the unique atmosphere and interactions of live performance, one must be physically present at the venue. This allows for direct engagement with the performance and the performers.\", \"eliminated_options\": [\"A\", \"C\"], \"possible_options\": [\"B\", \"D\", \"E\",\"F\"] }, 22 \"reasoning_step_2\": { \"reasoning_skill\": \"temporal_reasoning\", \"reasoning\": \"Checking showtimes and buying tickets are preparatory actions that occur before attending the performance. They are necessary for planning but do not provide the immersive experience itself.\", \"eliminated_options\": [\"D\", \"E\"], \"possible_options\": [\"B\", \"F\"] }, \"reasoning_step_3\": { \"reasoning_skill\": \"social_reasoning\", \"reasoning\": \"Being present at the theatre allows for social interactions and shared experiences with other audience members and the performers, which are integral to the live performance experience.\", \"eliminated_options\": [\"F\"], \"possible_options\": [\"B\"] } } } ### COMPLEXITY LEVEL 2: { \"commonsense_context\": \"Experiencing live performance, such as play, concert, or other theatrical event, typically involves attending venue where the performance is being held. The atmosphere, acoustics, and visual elements of live performance are best appreciated in person, where the audience can fully engage with the performers and the production. This is social and cultural activity that often involves planning, such as purchasing tickets and checking showtimes, but the core experience is being physically present at the venue. Additionally, live performances often include unique interactions between the audience and performers, such as applause, laughter, or even participation, which enhance the overall experience. Furthermore, attending live performance can create lasting memories and sense of community among attendees, as they share the emotional highs and lows of the performance together.\", \"commonsense_question\": \"How can you create lasting memories and fully immerse yourself in the unique atmosphere and interactions of live performance, while also contributing to the communal experience?\", \"options\": { \"A\": \"watch recording at home\", \"B\": \"attend the theatre in person\", \"C\": \"read reviews online\", \"D\": \"check showtimes regularly\", \"E\": \"buy tickets in advance\", \"F\": \"listen to podcast about the performance\", \"G\": \"participate in post-show discussion\" }, \"correct_answer\": [\"B\", \"attend the theatre in person\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"deductive_reasoning\", \"reasoning\": \"To create lasting memories and fully immerse oneself in the unique atmosphere and interactions of live performance, one must be physically present at the venue. This allows for direct engagement with the performance and the performers.\", \"eliminated_options\": [\"A\", \"C\", \"F\"], \"possible_options\": [\"B\", \"D\", \"E\", \"G\"] }, \"reasoning_step_2\": { \"reasoning_skill\": \"temporal_reasoning\", \"reasoning\": \"Checking showtimes and buying tickets are preparatory actions that occur before attending the performance. They are necessary for planning but do not provide the immersive experience itself.\", \"eliminated_options\": [\"D\", \"E\"], \"possible_options\": [\"B\", \"G\"] }, \"reasoning_step_3\": { \"reasoning_skill\": \"social_reasoning\", \"reasoning\": \"Being present at the theatre allows for social interactions and shared experiences with other audience members and the performers, which are integral to the live performance experience. While participating in post-show discussion can enhance the communal experience, it does not replace the immersive experience of attending the performance itself.\", \"eliminated_options\": [\"G\"], \"possible_options\": [\"B\"] } } } ### COMPEXITY LEVEL 3: { \"commonsense_context\": \"Experiencing live performance, such as play, concert, or other theatrical event, typically involves attending venue where the performance is being held. The atmosphere, acoustics, and visual elements of live performance are best appreciated in person, where the audience can fully engage with the performers and the production. This is social and cultural activity that often involves planning, such as purchasing tickets and checking showtimes, but the core experience is being physically present at the venue. Additionally, live performances often include unique interactions between the audience and performers, such as applause, laughter, or even participation, which enhance the overall experience. Furthermore, attending live performance can create lasting memories and sense of community among attendees, as they share the emotional highs and lows of the performance together. In recent times, some performances have also incorporated digital elements, allowing for hybrid experience where audiences can engage both in-person and online, adding new dimension to the traditional live performance.\", \"commonsense_question\": \"In the context of modern live performance that incorporates both in-person and digital elements, how can you create lasting memories and fully immerse yourself in the unique atmosphere and interactions, while also contributing to the communal experience?\", 23 \"options\": { \"A\": \"watch recording at home\", \"B\": \"attend the theatre in person\", \"C\": \"read reviews online\", \"D\": \"check showtimes regularly\", \"E\": \"buy tickets in advance\", \"F\": \"listen to podcast about the performance\", \"G\": \"participate in post-show discussion\", \"H\": \"engage with digital elements during the performance\" }, \"correct_answer\": [\"B\", \"attend the theatre in person\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"deductive_reasoning\", \"reasoning\": \"To create lasting memories and fully immerse oneself in the unique atmosphere and interactions of live performance, one must be physically present at the venue. This allows for direct engagement with the performance and the performers.\", \"eliminated_options\": [\"A\", \"C\", \"F\"], \"possible_options\": [\"B\", \"D\", \"E\", \"G\", \"H\"] }, \"reasoning_step_2\": { \"reasoning_skill\": \"temporal_reasoning\", \"reasoning\": \"Checking showtimes and buying tickets are preparatory actions that occur before attending the performance. They are necessary for planning but do not provide the immersive experience itself.\", \"eliminated_options\": [\"D\", \"E\"], \"possible_options\": [\"B\", \"G\", \"H\"] }, \"reasoning_step_3\": { \"reasoning_skill\": \"social_reasoning\", \"reasoning\": \"Being present at the theatre allows for social interactions and shared experiences with other audience members and the performers, which are integral to the live performance experience. While participating in post-show discussion can enhance the communal experience, it does not replace the immersive experience of attending the performance itself.\", \"eliminated_options\": [\"G\"], \"possible_options\": [\"B\", \"H\"] }, \"reasoning_step_4\": { \"reasoning_skill\": \"analogical_reasoning\", \"reasoning\": \"Engaging with digital elements during the performance can enhance the experience but is analogous to supplementary activities. The core immersive experience is still best achieved by being physically present.\", \"eliminated_options\": [\"H\"], \"possible_options\": [\"B\"] } } } Figure 14: An example from mSCoRe-G for complexity level 0 to 3 (English). 24 ### LLM ROLE You are language model with advanced commonsense reasoning skills, capable of logical and analytical reasoning, heuristic and intuitive thinking, comparative and hypothetical analysis, and contextual and specialized understanding. ### TASK DESCRIPTION Your task is to create multiple-choice commonsense question based on given cultural situation in the following format: { \"cultural_topic\": \"culture group - topic - scenario\", \"social_context\": \"settings the behavior takes place\", \"actor\": \"who exhibit the behavior\", \"question\": \"the commonsense question regarding the actor's behavior\", \"actor_behavior\": \"behavior of the actor - which are highly agreed upon (the correct answer option)\", \"recipient\": \"recipient of the action\", \"relation\": \"relation between the actor and the recipient\", \"recipient_behavior\": \"behavior of the recipient\", } The question should implicitly incorporate the cultural context, challenging the AI's ability to utilize commonsense reasoning to arrive at the correct answer. The goal is to test and enhance the AI's understanding of cultural norms and behaviors in specific setting. Provide the detailed \"REASONING PROCESS\" the arrive at the correct anwser option that involves multiple \"REASONING STEPs\" to arrive at the correct answer. Each \"REASONING STEP\" should be an \"ATOMIC REASONING STEP\" an Indivisible Unit of reasoning that predominantly utilizes one reasoning skill. It is single, coherent thought process that cannot be broken down into smaller steps without losing its meaning. The \"REASONING PROCESS\" must be as efficent as possible, only using the minimum number of steps necessary, ensuring that each step is non-redundant and contributes to narrowing down the possible options by eliminating one or more answer choices. ### STEP-BY-STEP INSTRUCTIONS Following these Step-by-Step Instructions: 1. Analyze the Provided Cultural Situation: Review the details of the cultural group, context, actor behaviors, and other descriptions to understand the key elements of the situation. 2. Adding The \"COMMONSENSE CONTEXT\": Based on the context given in the input, \"COMMONSENSE CONTEXT\" to the question refers to the background knowledge or additional details that are generally understood without requiring specialized knowledge, including factors such as time, place, social norms, cultural influences, and other relevant details that shape the understanding of the topic. 3. Create the \"Commonsense Question\": Combine the cultural context and the persona's inquiry to formulate concise question. Ensure the question IMPLICITLY incorporates the original context without explicitly stating it. Create the correct answer option based on the \"actor_behavior\" 4. Provide Other Answer Options: Create 5 multiple-choice options (including the correct answer from the previous step). Two of which should be plausible options. The other two should be distractors that are relevant and reasonable but incorrect based on the cultural context. 5. Describe your Step-by-Step \"REASONING PROCESS\" to arrive at the correct answer. Each \"ATOMIC REASONING STEP\" must following this sequence: 5.1. Choose \"REASONING SKILL\" below to be used by the \"REASONING STEP\": + inductive_reasoning: Drawing general conclusions from specific observations. + deductive_reasoning: Deriving specific conclusions from general premises. + abductive_reasoning: Forming hypotheses to explain observations. + analogical_reasoning: Drawing parallels between similar situations to infer conclusions. + counterfactual_reasoning: Considering alternative scenarios and outcomes that did not happen. + probabilistic_reasoning: Applying principles of probability to make inferences under uncertainty. + temporal_reasoning: Understanding sequences and durations of events. + spatial_reasoning: Visualizing and manipulating objects in space. + social_reasoning: Understanding social interactions and norms. + moral_reasoning: Deciding what is right or wrong based on ethical principles. 5.2. Apply the choosen \"REASONING SKILL\": provide concise explanation of how the chosen \"REASONING SKILL\" is applied to eliminate certain answer options or reinforce the correct answer option. Ensure the reasoning is clear and cannot be further divided into smaller steps. 5.3. Eliminate Options: List the options eliminated in this step based on your reasoning. 5.4. Update Possible Options: Provide the list of remaining possible options after this step. 25 6. Generate your output in the JSON format with the following structure: ```json { \"commonsense_context\": \"context_text\", \"commonsense_question\": \"question_text\", \"options\": { \"A\": \"option_answer_text_A\", ... }, \"correct_answer\": [\"answer_option\", \"answer_text\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"reasoning_skill_name\", \"reasoning\": \"reasoning_text\", \"eliminated_options\": [list_of_eliminated_options], \"possible_options\": [list_of_remaining_options] }, ... \"reasoning_step_n\": { \"reasoning_skill\": \"reasoning_skill_name\", \"reasoning\": \"reasoning_text\", \"eliminated_options\": [list_of_eliminated_options], \"possible_options\": [list_of_remaining_options] } } } ``` ### IN-CONTEXT EXAMPLE: <.....> ### OUTPUT REMINDER Ensure that your output follows the JSON structure as instructed and demonstrated in the in-context example. ### INPUT: { \"cultural_topic\": \"American culture - Dress Codes - Travel Advising\", \"social_context\": \"In public settings within American culture, it is common for people to dress casually, often opting for comfortable clothing such as sweatpants while still adhering to dress codes. This relaxed approach to attire is widely regarded as the norm by significant portion of the sampled population. It reflects preference for comfort and practicality in daily dress, showcasing relaxed and informal attitude towards clothing choices in various public settings.\", \"actor\": \"people - business professional from formal corporate background, planning first-time trip to the United States for business conference, eager to blend in and avoid any potential faux pas\", \"question\": \"I'm gearing up for big conference in the States and I'm bit worried about what to wear. come from formal work environment and don't want to stand out in negative way. Can you give me some tips on what kind of attire would be appropriate for business setting over there? Should be concerned about anything specific?\", \"actor_behavior\": \"dress casually, often in comfortable clothing, with preference for sweatpants and following dress codes\", \"recipient\": \"None\", \"relation\": \"None\", \"recipient_behavior\": \"None\" } Figure 15: Structured Reasoning Generation Prompt for mSCoRe-S. 26 ### ORIGINAL INSTANCE FROM CULTUREBANK: { \"cultural_topic\": \"Germans culture - Education and Technology - Travel Advising\", \"social_context\": \"In German schools, both the educational institutions and students actively participate in compulsory swimming education, which includes separate classes for students with limited swimming skills. The goal of this initiative is to teach swimming skills and promote integration, with students, including those with immigrant parents, participating in swimming lessons. It is noteworthy that Muslim students are accommodated by wearing burqinis during these swimming classes, showcasing inclusivity and respect for diverse cultural practices. This swimming education typically takes place in 5th or 6th grade and lasts for 2 years, resulting in students acquiring advanced swimming skills. While not universally embraced, significant portion of the sampled population considers this practice as standard part of the educational experience in German schools.\", \"actor\": \"German schools and students - concerned parent planning family trip to Germany, looking for comprehensive and inclusive educational experience for their children\", \"question\": \"I'm planning family trip to Germany and want my kids to make the most of their time there. I've heard that schools there offer some unique educational experiences. I'm particularly interested in finding activities that are both fun and educational for my kids, especially ones that can help them learn new skills. Could you recommend some family-friendly programs that might be good fit for us, keeping in mind that we have diverse family background?\", \"actor_behavior\": \"provide and attend compulsory swimming education, including separate classes for those with limited swimming skills\", \"recipient\": \"German students, including those with immigrant parents\", \"relation\": \"educational institution and attendees\", \"recipient_behavior\": \"participate in swimming lessons, including wearing burqinis for Muslim students\" } ### COMPEXITY LEVEL 0: { \"commonsense_context\": \"In Germany, swimming education is an integral part of the school curriculum, aimed at teaching essential swimming skills and promoting inclusivity. This program is designed to accommodate students from diverse backgrounds, including those with immigrant parents and Muslim students, who are allowed to wear burqinis. The initiative is generally well-received and considered standard educational practice, providing students with valuable life skills and fostering integration.\", \"commonsense_question\": \"What unique educational experience in German schools could be particularly beneficial for diverse family visiting Germany, looking for fun and educational activities for their children?\", \"options\": { \"A\": \"Participate in compulsory swimming education that includes accommodations for diverse cultural practices.\", \"B\": \"Enroll in advanced mathematics classes to enhance analytical skills.\", \"C\": \"Join German language immersion program to improve language proficiency.\", \"D\": \"Attend local history tour to learn about German culture and heritage.\", \"E\": \"Take part in cooking class to explore traditional German cuisine.\" }, \"correct_answer\": [\"A\", \"Participate in compulsory swimming education that includes accommodations for diverse cultural practices.\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"social_reasoning\", \"reasoning\": \"Understanding that the swimming education program in German schools is designed to be inclusive and accommodate diverse cultural backgrounds, making it unique and suitable experience for diverse family.\", \"eliminated_options\": [\"B\", \"C\"], \"possible_options\": [\"A\", \"D\", \"E\"] }, \"reasoning_step_2\": { \"reasoning_skill\": \"deductive_reasoning\", \"reasoning\": \"Considering the context of the question, which emphasizes fun and educational activities, swimming education stands out as it combines physical activity with skill acquisition, unlike history tour which is more passive.\", \"eliminated_options\": [\"D\", \"E\"], \"possible_options\": [\"A\"] } } } 27 ### COMPLEXITY LEVEL 1: { \"commonsense_context\": \"In Germany, swimming education is an integral part of the school curriculum, aimed at teaching essential swimming skills and promoting inclusivity. This program is designed to accommodate students from diverse backgrounds, including those with immigrant parents and Muslim students, who are allowed to wear burqinis. The initiative is generally well-received and considered standard educational practice, providing students with valuable life skills and fostering integration. Additionally, German schools often collaborate with local community centers to offer these swimming lessons, ensuring that students have access to safe and well-maintained facilities. This collaboration also allows for the inclusion of parents in some sessions, promoting family involvement in the educational process.\", \"commonsense_question\": \"For diverse family visiting Germany, interested in engaging in both educational and community activities, what unique experience offered by German schools could be particularly beneficial?\", \"options\": { \"A\": \"Participate in compulsory swimming education that includes accommodations for diverse cultural practices and involves community engagement.\", \"B\": \"Enroll in advanced mathematics classes to enhance analytical skills.\", \"C\": \"Join German language immersion program to improve language proficiency.\", \"D\": \"Attend local history tour to learn about German culture and heritage.\", \"E\": \"Take part in cooking class to explore traditional German cuisine.\", \"F\": \"Engage in community art project to express cultural diversity.\" }, \"correct_answer\": [\"A\", \"Participate in compulsory swimming education that includes accommodations for diverse cultural practices and involves community engagement.\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"social_reasoning\", \"reasoning\": \"Understanding that the swimming education program in German schools is designed to be inclusive and accommodate diverse cultural backgrounds, making it unique and suitable experience for diverse family. The program's collaboration with community centers further enhances its appeal by involving the family in the local community.\", \"eliminated_options\": [\"B\", \"C\"], \"possible_options\": [\"A\", \"D\", \"E\", \"F\"] }, \"reasoning_step_2\": { \"reasoning_skill\": \"deductive_reasoning\", \"reasoning\": \"Considering the context of the question, which emphasizes educational and community activities, swimming education stands out as it combines physical activity, skill acquisition, and community involvement, unlike history tour which is more passive.\", \"eliminated_options\": [\"D\"], \"possible_options\": [\"A\", \"E\", \"F\"] }, \"reasoning_step_3\": { \"reasoning_skill\": \"abductive_reasoning\", \"reasoning\": \"While cooking class and community art project can be educational and fun, they do not offer the same level of inclusivity, skill development, and structured community engagement as the swimming program, which is part of the school curriculum.\", \"eliminated_options\": [\"E\", \"F\"], \"possible_options\": [\"A\"] } } } 28 ### COMPLEXITY LEVEL 2: { \"commonsense_context\": \"In Germany, swimming education is an integral part of the school curriculum, aimed at teaching essential swimming skills and promoting inclusivity. This program is designed to accommodate students from diverse backgrounds, including those with immigrant parents and Muslim students, who are allowed to wear burqinis. The initiative is generally well-received and considered standard educational practice, providing students with valuable life skills and fostering integration. Additionally, German schools often collaborate with local community centers to offer these swimming lessons, ensuring that students have access to safe and well-maintained facilities. This collaboration also allows for the inclusion of parents in some sessions, promoting family involvement in the educational process. Furthermore, these swimming programs often include cultural exchange activities, where students and their families can share and learn about each other's traditions, enhancing mutual understanding and respect.\", \"commonsense_question\": \"For diverse family visiting Germany, interested in engaging in both educational and community activities, what unique experience offered by German schools could be particularly beneficial, especially in terms of cultural exchange and inclusivity?\", \"options\": { \"A\": \"Participate in compulsory swimming education that includes accommodations for diverse cultural practices, involves community engagement, and offers cultural exchange opportunities.\", \"B\": \"Enroll in advanced mathematics classes to enhance analytical skills.\", \"C\": \"Join German language immersion program to improve language proficiency.\", \"D\": \"Attend local history tour to learn about German culture and heritage.\", \"E\": \"Take part in cooking class to explore traditional German cuisine.\", \"F\": \"Engage in community art project to express cultural diversity.\", \"G\": \"Participate in multicultural festival organized by the school.\" }, \"correct_answer\": [\"A\", \"Participate in compulsory swimming education that includes accommodations for diverse cultural practices, involves community engagement, and offers cultural exchange opportunities.\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"social_reasoning\", \"reasoning\": \"Understanding that the swimming education program in German schools is designed to be inclusive and accommodate diverse cultural backgrounds, making it unique and suitable experience for diverse family. The program's collaboration with community centers further enhances its appeal by involving the family in the local community.\", \"eliminated_options\": [\"B\", \"C\"], \"possible_options\": [\"A\", \"D\", \"E\", \"F\", \"G\"] }, \"reasoning_step_2\": { \"reasoning_skill\": \"deductive_reasoning\", \"reasoning\": \"Considering the context of the question, which emphasizes educational and community activities, swimming education stands out as it combines physical activity, skill acquisition, and community involvement, unlike history tour which is more passive.\", \"eliminated_options\": [\"D\"], \"possible_options\": [\"A\", \"E\", \"F\", \"G\"] }, \"reasoning_step_3\": { \"reasoning_skill\": \"abductive_reasoning\", \"reasoning\": \"While cooking class and community art project can be educational and fun, they do not offer the same level of inclusivity, skill development, and structured community engagement as the swimming program, which is part of the school curriculum.\", \"eliminated_options\": [\"E\", \"F\"], \"possible_options\": [\"A\", \"G\"] }, \"reasoning_step_4\": { \"reasoning_skill\": \"analogical_reasoning\", \"reasoning\": \"Comparing the swimming program with the multicultural festival, the swimming program offers more structured and ongoing opportunity for cultural exchange and skill development, whereas the festival is one-time event.\", \"eliminated_options\": [\"G\"], \"possible_options\": [\"A\"] } } } 29 ### COMPLEXITY LEVEL 3: { \"commonsense_context\": \"In Germany, swimming education is an integral part of the school curriculum, aimed at teaching essential swimming skills and promoting inclusivity. This program is designed to accommodate students from diverse backgrounds, including those with immigrant parents and Muslim students, who are allowed to wear burqinis. The initiative is generally well-received and considered standard educational practice, providing students with valuable life skills and fostering integration. Additionally, German schools often collaborate with local community centers to offer these swimming lessons, ensuring that students have access to safe and well-maintained facilities. This collaboration also allows for the inclusion of parents in some sessions, promoting family involvement in the educational process. Furthermore, these swimming programs often include cultural exchange activities, where students and their families can share and learn about each other's traditions, enhancing mutual understanding and respect. The program also emphasizes water safety, which is crucial skill for everyone, and includes sessions on the importance of respecting different cultural practices in shared spaces.\", \"commonsense_question\": \"For diverse family visiting Germany, interested in engaging in both educational and community activities, what unique experience offered by German schools could be particularly beneficial, especially in terms of cultural exchange, inclusivity, and learning essential life skills like water safety?\", \"options\": { \"A\": \"Participate in compulsory swimming education that includes accommodations for diverse cultural practices, involves community engagement, and offers cultural exchange opportunities.\", \"B\": \"Enroll in advanced mathematics classes to enhance analytical skills.\", \"C\": \"Join German language immersion program to improve language proficiency.\", \"D\": \"Attend local history tour to learn about German culture and heritage.\", \"E\": \"Take part in cooking class to explore traditional German cuisine.\", \"F\": \"Engage in community art project to express cultural diversity.\", \"G\": \"Participate in multicultural festival organized by the school.\", \"H\": \"Join water safety workshop that includes cultural sensitivity training.\" }, \"correct_answer\": [\"A\", \"Participate in compulsory swimming education that includes accommodations for diverse cultural practices, involves community engagement, and offers cultural exchange opportunities.\"], \"reasoning_process\": { \"reasoning_step_1\": { \"reasoning_skill\": \"social_reasoning\", \"reasoning\": \"Understanding that the swimming education program in German schools is designed to be inclusive and accommodate diverse cultural backgrounds, making it unique and suitable experience for diverse family. The program's collaboration with community centers further enhances its appeal by involving the family in the local community.\", \"eliminated_options\": [\"B\", \"C\"], \"possible_options\": [\"A\", \"D\", \"E\", \"F\", \"G\", \"H\"] }, \"reasoning_step_2\": { \"reasoning_skill\": \"deductive_reasoning\", \"reasoning\": \"Considering the context of the question, which emphasizes educational and community activities, swimming education stands out as it combines physical activity, skill acquisition, and community involvement, unlike history tour which is more passive.\", \"eliminated_options\": [\"D\"], \"possible_options\": [\"A\", \"E\", \"F\", \"G\", \"H\"] }, \"reasoning_step_3\": { \"reasoning_skill\": \"abductive_reasoning\", \"reasoning\": \"While cooking class and community art project can be educational and fun, they do not offer the same level of inclusivity, skill development, and structured community engagement as the swimming program, which is part of the school curriculum.\", \"eliminated_options\": [\"E\", \"F\"], \"possible_options\": [\"A\", \"G\", \"H\"] }, 30 \"reasoning_step_4\": { \"reasoning_skill\": \"analogical_reasoning\", \"reasoning\": \"Comparing the swimming program with the multicultural festival, the swimming program offers more structured and ongoing opportunity for cultural exchange and skill development, whereas the festival is one-time event.\", \"eliminated_options\": [\"G\"], \"possible_options\": [\"A\", \"H\"] }, \"reasoning_step_5\": { \"reasoning_skill\": \"probabilistic_reasoning\", \"reasoning\": \"While water safety workshop with cultural sensitivity training is beneficial, the swimming program is more comprehensive, offering ongoing lessons that include water safety as part of broader curriculum.\", \"eliminated_options\": [\"H\"], \"possible_options\": [\"A\"] } } } Figure 16: An example from mSCoRe-S for complexity level 0 to 3 (English)."
        }
    ],
    "affiliations": [
        "Adobe Research, USA",
        "Department of Computer Science, University of Oregon, Eugene, OR, USA"
    ]
}