{
    "paper_title": "xKV: Cross-Layer SVD for KV-Cache Compression",
    "authors": [
        "Chi-Chih Chang",
        "Chien-Yu Lin",
        "Yash Akhauri",
        "Wei-Cheng Lin",
        "Kai-Chiang Wu",
        "Luis Ceze",
        "Mohamed S. Abdelfattah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. We find that the dominant singular vectors are remarkably well-aligned across multiple layers of the KV-Cache. Exploiting this insight, we propose xKV, a simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into a shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates than state-of-the-art inter-layer technique while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding tasks without performance degradation. These results highlight xKV's strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Our code is publicly available at: https://github.com/abdelfattah-lab/xKV."
        },
        {
            "title": "Start",
            "content": "xKV: Cross-Layer SVD for KV-Cache Compression Chi-Chih Chang1 Chien-Yu Lin2 Yash Akhauri1 Wei-Cheng Lin3 Kai-Chiang Wu3 Luis Ceze2 Mohamed S. Abdelfattah1 1Cornell University 2University of Washington 3National Yang Ming Chiao Tung University 5 2 0 M 4 2 ] . [ 1 3 9 8 8 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. We find that the dominant singular vectors are remarkably well-aligned across multiple layers of the KVCache. Exploiting this insight, we propose xKV, simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8 higher compression rates than the state-ofthe-art inter-layer technique, while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeekCoder-V2), yielding notable 3 compression rates on coding tasks without performance degradation. These results highlight xKVs strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Our code is publicly available at: https://github.com/abdelfattah-lab/xKV. Figure 1: Accuracy comparison of MiniCache [32], applying SVD on single layers KV-Cache and xKV (cross-layer SVD) on Llama-3.1-8B-Instruct (left) and Qwen2.5-14B-Instruct-1M (right). Results are averaged across tasks from RULER [25] benchmark."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) [2, 7, 26, 37, 44, 45] have revolutionized numerous artificial intelligence (AI) applications with advanced cognitive capabilities that were previously unattainable with conventional machine learning (ML) models. Recent efforts to extend the context lengths of LLMs"
        },
        {
            "title": "Preprint",
            "content": "have further expanded their potential: open-sourced models now support up to 1M tokens [38, 48], and proprietary ones like Gemini push this limit even further to 10M tokens [44]. These extended context windows unlock wide range of previously impractical applications, such as large-scale information retrieval and debugging or extending large-scale codebase [15, 21, 37, 48]. However, this expanded capability on long-context introduces significant challenges, particularly in the management of key-Value (KV) caches during inference [19, 30]. Typically, KV states are cached to avoid redundant computations; yet, under extended context lengths, the memory consumption of KV-Cache rapidly becomes prohibitive. This inflated memory footprint severely limits the number of concurrent inference requests, causing substantial throughput reduction. To address this, researchers have proposed various approaches to mitigate the large memory footprint of KV caches. These include quantization [13, 24, 35, 52], token eviction [3, 11, 20, 31, 47, 51], and low-rank decomposition [12, 41, 49, 50]. These approaches have primarily focused on intra-layer redundancies that compress the KV-Cache of each layer separately. While this often yields respectable per-layer compression, these methods do not utilize potential redundancy across layers [22]. To effectively exploit cross-layer redundancy, pioneering methods such as Cross-Layer Attention (CLA) [10] have introduced novel model architectures that share single set of KV-Cache across groups of adjacent layers. Another notable approach, MiniCache [32], utilizes the assumption of high cosine similarity between KV-Cache pairs in adjacent layers, merging them through spherical linear interpolation (SLERP) [40]. However, these existing methods present significant limitations. Specifically, they either necessitate expensive model pretraining due to architectural modifications [10] or depend heavily on assumptions about KV-Cache similarity [32] that often do not hold true in practical scenarios (see 3). Consequently, these constraints limit their achievable compression rates, come with high accuracy degradation, or restrict the applicability to existing, pretrained models. We revisit the inter-layer similarity via Centered Kernel Alignment (CKA) [29]. Our investigation shows that, despite having low cosine similarities, the KV-Cache of adjacent layers often have high similarities on their underlying dominant singular vectors. By exploiting this alignment, we can share basis vectors of multiple adjacent layers KV-Cache, yielding more compact representation. Leveraging these insights, we introduce xKV, fully plug-and-play compression method that requires neither additional fine-tuning nor architectural changes. xKV compresses multiple layers KV-Cache simultaneously by extracting their shared singular vectors through cross-layer Singular Value Decomposition (SVD). Crucially, due to the alignment of singular vectors across layers, xKV achieves significantly better compression and accuracy trade-offs compared to applying SVD individually to single layers. Notably, xKV can compress not only pairs of two adjacent layers but also larger groups of layers, further enhancing compression ratios and accuracy retention. Extensive experiments conducted on the challenging RULER benchmark with widely-used LLMs such as Llama and Qwen demonstrate the capabilities of xKV. Our method achieves up to 6.8 higher compression rates compared to the prior state-of-the-art inter-layer method, while simultaneously improving accuracy by 2.7%. Moreover, on the popular DeepSeek model architecture, which employs multi-head latent attention (MLA) [16] and already aggressively reduces KV-Cache size, xKV further attains 3 compression without compromising accuracy, showcasing its versatility and compatibility across diverse attention mechanisms."
        },
        {
            "title": "2 Related Work",
            "content": "Low-Rank KV-Cache Compression. broad line of research exploits the low-rank nature of the KV-Cache to reduce its memory footprint. For instance, Multi-Head Latent Attention (MLA) [15, 16] projects tokens onto low-rank subspace and caches those latent representations instead of the original key and value states, however, MLA requires training the model from scratch. In contrast, several post-training techniques decompose the key/value parameter matrices to obtain low-rank projection modules similar to MLA, such as ASVD [49], Palu [12], and LoRC [50]. Other methods decompose the KV-Cache directly: EigenAttention [39] applies SVD to calibration dataset to derive projection matrices, whereas ShadowKV [41] performs online SVD to capture the dynamic of different context. In xKV, we also exploits the low-rank nature of KV-Cache. However, unlike prior methods focusing on per layer compression, xKV further considers the shared information among multiple layers and extends the usage of low-rank to new, cross-layer, dimension. 2 Cross-Layer KV-Cache Optimization. Going beyond the intra-layer perspective, another stream of research explores inter-layer redundancy of KV-Cache [10, 18, 32, 42, 46]. CLA [10] and YOCO[42] both modify the Transformer model architecture so that later layers can directly reuse or reference KV states from earlier layers. LCKV [46] restricts full KV storage to small subset of layers, foregoing caches in other layers. However, these methods rely on retraining or model fine-tuning, which makes them less flexible. Minicache [32], in contrast, provides flexible post-training alternative by merging the key and value tokens from adjacent similar layers using spherical linear interpolation. Our approach goes further by extracting shared singular vectors of multiple layers KV-Caches thereby enabling higher compression. (a) (b) (c) Figure 2: (a) Average Token-wise Cosine Similarity for value-caches across different layers. For each pair of layers, we compute the token-level cosine similarities between their embeddings and average these values into single similarity score. (b) CKA Matrix for the value-cache. The higher (warmer) values indicate stronger singular vector alignment across layers. (c) Required rank ratio (percentage of total dimension) for capturing 95% of the cumulative eigenvalues in the key (red) and value (blue) matrices, plotted against the number of grouped layers. For each group, we horizontally concatenate the key/value caches and compute the rank needed to achieve 95% of the cumulative eigenvalues. As the grouping increases, fewer ranks (relative to total dimension) are required, implying higher compression rate for the same level of information preservation. We perform these analyses on the KV-Cache obtained from Llama-3.1-8B-Instruct, using the multi-valued NIAH dataset from the RULER [25] benchmark."
        },
        {
            "title": "3 Analysis and Motivation",
            "content": "In this section, we examine the cross-layer similarity of KV-Caches with different metrics to reveal the motivation behind the design of xKV."
        },
        {
            "title": "3.1 Cross-Layer Cosine Similarity",
            "content": "To understand the assumption used in previous work [32], we first measure token-wise cosine similarity across various layer-pairs. The measurement on the cosine similarity is presented in Figure 2a. Notably, the adjacent layers exhibit low per-token similarity. This modest similarity fundamentally limits the compression rate achieved by prior representative methods [32]. However, further examination using Centered Kernel Alignment (CKA) [29] reveals that while individual token representations differ significantly, multiple layers still share highly aligned singular vectors. While the embeddings vary at the token level, the latent subspaces spanned by KV-Cache across multiple layers remain notably similar. This observation motivates us to approach cross-layer KV-Cache compression by leveraging such sub-space alignments."
        },
        {
            "title": "3.2 Revisit Cross-Layer Similarity with CKA",
            "content": "While token-wise (cosine) similarity offers local perspective, more holistic view can reveal deeper patterns in how an entire KV-Cache is aligned across layers. Specifically, we adopt Centered Kernel Alignment (CKA) [29] to measure the similarity in the overall structure of two layers KV-Caches. Concretely, for layer ℓ with KV-cache Xℓ Rnd, we first define the centered Gram matrix Gℓ = Xℓ ℓ H, where = In 1 1 1. 3 Then, the CKA between two layers ℓ1 and ℓ2 is CKA(cid:0)Xℓ1, Xℓ2 (cid:1) = (cid:113) trace(cid:0)Gℓ1Gℓ2 (cid:1) trace(cid:0)G2 ℓ (cid:1)trace(cid:0)G2 ℓ2 (cid:1) . Unlike token-wise cosine metric, which simply compares corresponding token embeddings, CKA reflects the similarity of the entire distribution of token embeddings in each layer. As shown in Appendix A, if CKA(Xℓ1, Xℓ2 ) is high, then the dominant left singular vectors of Xℓ1 are highly aligned to those of layer ℓ2. In other words, the basis vectors that define how the token varies in these two layers might be similar. Observation 1: Highly Aligned Basis. In Figure 2b, we show the CKA value between each layers KV-Cache of Llama-3.1-8B-Instruct. As shown in Figure 2b, many pairs of layers exhibit remarkably high CKA (red blocks) even though their token-wise cosine similarity are very modest. This observation suggests that, although individual token embeddings differ across layers, the dominant singular vectors (i.e., basis) that span the KV-cache are, in fact, well-aligned. Thus, focusing solely on the cosine similarity between pairs of token embeddings can underestimate the potential for cross-layer merging and compression."
        },
        {
            "title": "3.3 Eigenvalue Analysis of KV-Cache",
            "content": "Observation 2: Horizontally Concatenated KV-Caches Require Lower Rank. Motivated by the observation that different layers basis are well aligned, we examine the rank to achieve certain level of information preservation after horizontally concatenating the key/value caches across multiple layers. Because each layer shows substantial cross-layer overlap (3.2), single set of low-rank basis vectors can effectively approximate the KV-Cache caches of all layers in the group. As illustrated in Figure 2c, larger group size reduces the fraction of total rank needed to preserve the same variance. Compared with creating separate low-rank subspaces for each layer, this shared approach avoids storing nearly identical basis vectors multiple times, yielding more compact yet expressive representation. In 4, we leverage these observations to propose our xKV method that achieves significantly higher compression ratios while preserving model accuracy. Figure 3: Illustration of the xKV for compressing KV-Cache."
        },
        {
            "title": "4 Methodology: xKV",
            "content": "Building on top of the core observations mentioned in previous subsections, we now present the core methodology of xKV, cross-layer SVD to identify set of shared basis and leverage them to form shared low-rank subspace that collectively approximates the KV-Caches from multiple layers. The design overview of xKV is shown in Figure 3. Notation. Let be the sequence length (number of tokens) and be the hidden dimension of the KV-Caches. Because the same compression technique is applied to both the keys and values, we use unified symbol Xℓ RLd to represent either key or value cache for the ℓ-th block. 4 Cross-Layer SVD. Suppose we select subset (or group) of layers {0, . . . , 1}, and let ℓ1, . . . , ℓG be the layer indices in that subset. We form the horizontal concatenation of their KV-Caches: (cid:2)Xℓ1, . . . , XℓG (cid:3) L(Gd). We then perform single singular value decomposition (SVD) on this concatenated matrix, retaining only the top-r singular values and corresponding singular vectors: (cid:3) Ur Sr , (cid:2)Xℓ1, . . . , XℓG where Ur RLr, Sr Rrr, and Rr(Gd). By further applying matrix fusion view, we can derive: (cid:104) (cid:105) Ur Sr ="
        },
        {
            "title": "Ur Sr",
            "content": "V = (cid:2)Bℓ1, . . . , BℓG (cid:3), where holds the shared left singular vectors that span the shared low-rank subspace, and Bℓi Rrd are layer-specific reconstruction matrices. Stride-based Grouping. Motivated by our empirical observation (see Figure 2b) that adjacent layers exhibit strong singular vector alignment, we adopt simple stride-based approach. Specifically, we partition the Transformer blocks into contiguous strides of size G. Formally, Gk = { G, + 1, . . . , + (G 1)} (cid:0)for = 0, 1, . . . , G 1(cid:1), so that each group Gk {0, . . . , 1} collects adjacent layers. In this manner, xKV can effectively share common set of principal components among layers that exhibit high mutual alignment."
        },
        {
            "title": "4.1 Deploy Long Context LLM with xKV",
            "content": "Prefill Phase. During prefill, we gather the key and value states of each group of layers. We then apply the cross-layer SVD method on keys and values separately to extract the aligned left singular vector (i.e., share basis) and the layer-specific reconstruction matrix. We apply decomposition on the fly during pre-filling to better capture the dynamic of KV-Cache [41]. While this includes additional computation, the overheads consume less than 10% of prefilling times at context length of 128k and become negligible when we handle longer context length scenarios for which xKV is designed. In real-world use case, the proposed cross-layer decomposition could also be performed on the prefix cache [28] to reduce the storage costs and communication latency when fetching from remote storage [27]. Decode Phase. As illustrated in Figure 3b, we reconstruct the compressed KV-Cache corresponding to the prompt during decoding by multiplying the shared aligned basis with layer-specific construction matrix Bℓi . For the newly generated tokens, we do not compress their KV-Cache as in general long-context scenarios, where prompts range from extensive documents, web information [1], or code repository [33], the KV-Cache from the generated response is typically much smaller than those corresponding to prompts. For instance, with 64k token prompt, subsequent 1k tokens response amounts to under 2% of the total. Consequently, leaving these tokens uncompressed introduces only negligible overhead while better-preserving accuracy as demonstrated in previous KV-Cache compression works [11, 31, 41]. In use cases that entail very long generations, such as long writing [9] or reasoning [36], one could re-apply our cross-layer SVD to the newly produced tokens after accumulating certain amount of generated tokens, thereby further reducing the memory overhead when needed."
        },
        {
            "title": "5.1 Experiments Setup",
            "content": "Models We evaluate xKV on three widely used language models using Group-Query Attention (GQA): Llama-3.1-8B-Instruct [21] (8 KV heads), Qwen2.5-14B-Instruct-1M [48] (8 KV heads) and Qwen2.5-7B-Instruct-1M [48] (4 KV heads). To demonstrate xKVs high compatibility, we also include DeepSeek-Coder-V2-Lite-Instruct with 16B parameters based on Mixter-of-Experts (MoE) [14] with 2.4B activated parameters. 5 Table 1: Performance of different methods on the RULER benchmark evaluated at context length of 64K. Method Comp. N-S1 N-S2 N-MK1 N-MK2 N-MQ N-MV QA-1 QA-2 VT FWE Avg. Baseline Minicache Minicache Single SVD xKV-2 (Ours) xKV-4 (Ours) Single SVD xKV-2 (Ours) xKV-4 (Ours) Baseline Minicache Minicache Single SVD xKV-2 (Ours) xKV-4 (Ours) Single SVD xKV-2 (Ours) xKV-4 (Ours)"
        },
        {
            "title": "Minicache\nMinicache",
            "content": "Single SVD xKV-2 (Ours) xKV-4 (Ours) Single SVD xKV-2 (Ours) xKV-4 (Ours) Llama-3.1-8B-Instruct 100.0 100.0 100.0 87. 100.0 100.0 100.0 29.2 99.0 100.0 100.0 64.6 100.0 100.0 100.0 26.0 75.0 96.9 99. 97.9 39.6 100.0 100.0 100.0 32.3 69.8 97.9 97.9 90.6 10.4 97.9 97.9 97. 96.9 97.9 97.9 99.0 87.0 13.3 97.9 97.9 98.4 8.6 74.5 95.3 Qwen2.5-7B-Instruct-1M 100.0 100.0 100.0 100.0 100.0 78.1 25. 100.0 100.0 100.0 24.0 99.0 100.0 34.4 0.0 100.0 100.0 100.0 7.3 81.2 85.4 100. 100.0 100.0 0.0 100.0 100.0 100.0 12.5 95.8 100.0 100.0 1.0 100.0 100.0 100. 9.4 91.7 96.9 32.3 0.0 99.0 100.0 100.0 6.2 75.0 91.7 3.1 0.0 99.0 99.0 99. 97.9 97.9 99.0 27.3 0.0 99.7 100.0 100.0 4.4 38.3 84.1 Qwen2.5-14B-Instruct-1M 100. 100.0 0.0 100.0 100.0 100.0 18.8 91.7 99.0 99.0 99.0 0.0 99.0 99.0 99. 96.9 96.9 97.9 100.0 98.2 0.0 100.0 100.0 100.0 25.5 90.4 97.1 98. 81.0 20.1 96.1 96.3 97.1 17.2 65.9 93.5 95.6 42.7 0.0 92.7 90.6 90. 3.4 57.0 76.8 99.2 99.0 0.0 96.6 95.6 95.3 14.3 74.0 88.5 1. 1.2 1.3 2.5 2.5 2.4 8.4 8.3 8.0 1.0 1.2 1.3 2.3 2.3 2. 6.4 6.3 6.2 1.0 1.1 1.3 2.5 2.5 2.4 8.4 8.3 8.0 83. 78.1 60.4 80.2 83.3 84.4 44.8 67.7 76.0 83.3 26.0 13.5 75.0 80.2 83. 36.5 56.2 62.5 80.2 72.9 17.7 78.1 81.2 83.3 32.3 49.0 63.5 60. 47.9 35.4 58.3 61.5 60.4 36.5 49.0 54.2 59.4 24.0 13.5 58.3 59.4 60. 35.4 46.9 50.0 66.7 61.5 27.1 62.5 61.5 69.8 44.8 52.1 58.3 97. 84.6 49.0 96.9 96.2 96.2 2.7 36.9 87.7 84.7 84.0 58.0 79.5 80.6 81. 59.0 73.3 78.8 90.8 86.5 8.5 0.8 80.8 79.0 85.0 6.9 46.7 66. 99.6 90.8 0.2 98.5 99.2 99.4 8.1 77.7 86.0 8.0 4.5 75.7 81.2 83. 26.4 41.3 64.9 91.3 84.4 8.3 87.2 86.1 88.5 59.0 80.2 86.5 92. 85.1 43.8 90.7 91.4 91.6 35.3 70.9 87.8 91.6 28.4 5.7 88.0 88.9 90. 24.8 64.0 78.1 93.6 90.6 5.4 92.2 92.2 93.5 32.2 79.9 87.4 Datasets. We select RULER as our major benchmark, which features complex tasks such as retrieval, multi-hop tracking, and question-answering. For DeepSeek-Coder-V2, we adopt Repobench-P [33] and LCC [23] from LongBenchs [8] collection to evaluate the LLM capabilities of code completion under long-context scenarios. Baselines. We compare against two baselines. First, Single SVD, special case of xKV with group size = 1, factorizes each layers key and value caches independently. For fair comparison, the Single-Layer SVD also applies decomposition on the fly for every incoming request. We also compare xKV to representative inter-layer compression method, MiniCache. The baseline refers to the models original (uncompressed) KV cache. Implementation Details. We implement xKV using the Huggingface library. Because keys and values exhibit different sensitivities [12] to compression, we fix their rank ratio to 1:1.5 (for example, if the key rank is 96, then the value rank is 144). On the model with MLA, we apply xKV on the non-RoPE latent representations and leave the small decoupled RoPE keys uncompressed. For both Single-Layer SVD and xKV, we decompose the pre-RoPE key states, and then re-apply RoPE to the reconstructed keys during generation. Since the code for MiniCache was not publicly available, we re-implemented it based on the original paper and the SLERP code it references. We follow the official settings to merge half of the layers, from the middle to the end of the LLM, and we vary MiniCaches compression rate by adjusting the layer index at which merging begins. For fair comparison, we keep the newly generated tokens uncompressed for all comparison targets. We measure the compression rate assuming context length of 64k. 6 Figure 4: Evaluation results of different KV-Cache methods on DeepSeek-Coder-V2-Lite-Instruct model using RepoBench-P [33] and LCC[23]. The accuracy denotes the edit similarity [43], and the dotted line represents the baseline score with uncompressed KV-Cache."
        },
        {
            "title": "5.2 Main Evaluation Results on RULER",
            "content": "Table 1 compares xKV against two baselines on the RULER benchmark [25] at 64k context length. On both Llama-3.1-8B and Qwen2.5-14B using GQA [4] with eight key and value heads [4], we observe that MiniCache suffers marked performance degradation at compression ratios of 1.3. This degradation stems from MiniCaches reliance on adjacent layers having high token-wise cosine similarity, which are not generally present. In contrast, Single SVD maintains strong accuracy at 2.5 compression, reflecting the observed low-rank nature of individual KV-caches. However, at extreme compression levels, Single SVD also experiences catastrophic performance degradation. By exploiting the low-rank nature of the KV-Cache, and by and constructing shared low-rank subspace across layers, xKV far surpasses Single SVDs accuracy at moderate compression and remains nearly lossless. Moreover, when increasing the group size (e.g., from 2 to 4 layers), xKV achieves further gains under the same compression ratio, underscoring the advantage of capturing richer shared subspace among multiple layers. Notably, xKV still maintains competitive performance at an extreme 8.0 compression ratio, achieving roughly 6.8 higher compression rates than MiniCache with 2.7% accuracy gain on Llama-3.1-8B, illustrating its efficacy in compressing KV-caches for large-context scenarios with minimal quality loss. We also examine xKV on the Qwen2.5-7B-1M, which natively has highly compact KV-Cache with only four key and value heads. We observe that the benefit of exploiting the inter-layer still holds, highlighting the xKV generalizability."
        },
        {
            "title": "5.3 Evaluation on DeepSeek’s MLA",
            "content": "To demonstrate the effectiveness of xKV on emerging attention variants, we evaluate it using DeepSeekV2-Coder-Lite [17], which employs the efficient Multi-head Latent Attention (MLA) architecture [16]. Although MLA already significantly reduces the KV-Cache size per layer through low-rank projections, xKV further compresses this already compact latent cache by effectively leveraging cross-layer redundancy. We evaluate performance using the popular RepoBench-P [8, 33] and LCC [23] coding benchmark and present the results in Figure 4. As shown, with group size of 4, xKV achieves 3 compression rate on RepoBench and 3.5 on LCC without compromising accuracy. In contrast, other methods, such as MiniCache and Single SVD, fail to preserve accuracy on the MLA architecture even at substantially lower compression rates. These results underscore xKVs versatility and its compatibility with emerging architectures."
        },
        {
            "title": "5.4 Ablation Studies",
            "content": "Compressing key and value Separately. To understand how xKV affects key and value compression, we conduct ablation experiments on four subtasks from RULER (Figure 5) to evaluate how xKV (cross-layer low-rank SVD) affects key and value compression. Overall, xKV consistently boosts accuracy under varying compression rates. Also, keys exhibit higher compressibility than values, matching the eigenvalue analysis in Figure 2c. closer inspection of the results reveals that the 7 Figure 5: Accuracy comparison of applying different methods to key and value separately on Llama3.1-8B-Instruct using RULER benchmark. achievable compression ratio appear to be task-dependent. On the questions-answering subtasks (QA-1 and QA-2) xKV can push the compression rate to 16 while still preserving performance. In Variable Tracking (VT) and NIAH multi-queries (N-MQ), accuracy begins to decline beyond 8 compression; however, in these same tasks, values tolerate compression more easily than in QA subtasks. These observations underscore how different tasks may demand different sweet spots for key versus value compression. In xKV, we employ fixed compression ratio for all different tasks. Exploring task-specific or context-aware [5, 6, 34] rank allocation is promising avenue for future work."
        },
        {
            "title": "6 Limitations and Broader Impacts",
            "content": "Fine-Grained Compression Rate Allocation. Currently, we employ the fixed rank ratio across all grouped of layers. Prior works have demonstrated that the KV-Cache of different layers [5, 12] have different levels of sensitivity and some potential error propagation issues [50]. While xKV have yielded promising performance, we believe that exploring the fine-grained allocations of each group of layers could help boost the performance of xKV further. End-to-End System Demonstration. While xKV substantially reduces memory usage for longcontext inference, we have yet to integrate it into complete system to measure its effect on decoding speed and throughput, especially in light of the extra FLOPs introduced by reconstruction. Still, xKV holds strong potential for high-performance deployments. For instance, ShadowKV [41] shows how combining single-layer SVD with sparse attention can improve throughput. Incorporating xKV, more powerful variant of single-layer SVD, into similar framework may enable both lower KV-cache footprints and faster end-to-end inference."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce xKV, plug-and-play compression method for Key-Value (KV) caches that exploits inter-layer redundancy. Our approach reveals that KV caches across different layers share highly aligned basis. Leveraging their inherent low-rank structure, we apply cross-layer SVD to compress multiple KV caches into shared low-rank subspace. Experiments demonstrate that xKV achieves up to an 8.5 compression rate while maintaining strong accuracy on representative long-context benchmarks."
        },
        {
            "title": "References",
            "content": "[1] Perplexity ai. https://www.perplexity.ai/. Accessed: March 21, 2025. [2] Introducing meta llama 3: The most capable openly available llm to date, 2024. [3] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. Proceedings of Machine Learning and Systems, 7, 2024. 8 [4] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [5] Yash Akhauri, Ahmed AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander Rush, Safeen Huda, and Mohamed Abdelfattah. Shadowllm: Predictor-based contextual sparsity for large language models. arXiv preprint arXiv:2406.16635, 2024. [6] Yash Akhauri, Ahmed AbouElhamayed, Yifei Gao, Chi-Chih Chang, Nilesh Jain, and Mohamed S. Abdelfattah. Tokenbutler: Token importance is predictable, 2025. [7] Anthropic. Claude: conversational ai assistant, 2023. Large Language Model. Version 1.0. Accessed: 2025-03-13. [8] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [9] Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context LLMs. In The Thirteenth International Conference on Learning Representations, 2025. [10] William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and Jonathan Ragan-Kelley. Reducing transformer key-value cache size with cross-layer attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [11] Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, and Wen Xiao. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling, 2024. [12] Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Mohamed S. Abdelfattah, and Kai-Chiang Wu. Palu: KV-cache compression with low-rank projection. In The Thirteenth International Conference on Learning Representations, 2025. [13] Yuzong Chen, Xilai Dai, Chi chih Chang, Yash Akhauri, and Mohamed S. Abdelfattah. The power of negative zero: Datatype customization for quantized large language models, 2025. [14] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024. [15] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025. Version Number: 1. [16] DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, 9 Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. [17] DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, and Wenfeng Liang. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence, 2024. [18] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, ZIJIA CHEN, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Celine Lin, Jan Kautz, and Pavlo Molchanov. Hymba: hybrid-head architecture for small language models. In The Thirteenth International Conference on Learning Representations, 2025. [19] Yao Fu. Challenges in deploying long-context transformers: theoretical peak performance analysis, 2024. [20] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for LLMs. In The Twelfth International Conference on Learning Representations, 2024. [21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane 10 Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin 11 Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. [22] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. [23] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian J. McAuley. Longcoder: longrange pre-trained language model for code completion. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1209812107. PMLR, 2023. [24] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. [25] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [26] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. [27] Shuowei Jin, Xueshen Liu, Qingzhao Zhang, and Morley Mao. Compute or load kv cache? why not both? arXiv preprint arXiv:2410.03065, 2024. [28] Jordan Juravsky, Bradley Brown, Ryan Saul Ehrlich, Daniel Fu, Christopher Re, and Azalia Mirhoseini. Hydragen: High-throughput LLM inference with shared prefixes. In Workshop on Efficient Systems for Foundation Models II @ ICML2024, 2024. [29] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pages 35193529. PMLR, 2019. [30] Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, and Lei Chen. survey on large language model acceleration based on kv cache management. arXiv preprint arXiv:2412.19442, 2024. [31] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. SnapKV: LLM knows what you are looking for before generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [32] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan Zhuang. Minicache: KV cache compression in depth dimension for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [33] Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems, 2023. [34] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, and Beidi Chen. Deja vu: Contextual sparsity for efficient LLMs at inference time. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2213722176. PMLR, 2329 Jul 2023. [35] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. 12 [36] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [37] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. GPT-4 Technical Report, March 2024. arXiv:2303.08774 [cs]. [38] Leonid Pekelis, Michael Feil, Forrest Moret, Mark Huang, and Tiffany Peng. Llama 3 gradient: series of long context models, 2024. [39] Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, and Kaushik Roy. Eigen attention: Attention in low-rank space for KV cache compression. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1533215344, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [40] Ken Shoemake. Animating rotation with quaternion curves. SIGGRAPH Comput. Graph., 19(3):245254, July 1985. [41] Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, and Beidi Chen. Shadowkv: Kv cache in shadows for high-throughput long-context llm inference, 2024. [42] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [43] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. Intellicode compose: Code generation using transformer. In Proceedings of the 28th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering, pages 14331443, 2020. [44] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. [46] Haoyi Wu and Kewei Tu. Layer-condensed kv cache for efficient inference of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1117511188, 2024. [47] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. [48] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei 13 Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. Qwen2.5-1m technical report, 2025. [49] Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-aware singular value decomposition for compressing large language models, 2023. [50] Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, and Yelong Shen. Lorc: Low-rank compression for llms kv cache with progressive compression strategy. arXiv preprint arXiv:2410.03111, 2024. [51] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024. [52] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zhenga, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023."
        },
        {
            "title": "A CKA and Indication of Aligned Left Singular Vectors",
            "content": "A.1 Notation and Definitions For each layer ℓ, let Xℓ Rnd, where each of the rows corresponds to token (data point). Define the centering matrix which subtracts the (row) mean from each token embedding. We define the centered embeddings = In 1 1, and the centered Gram matrix (cid:101)Xℓ = Xℓ, Gℓ = (cid:101)Xℓ (cid:101)X ℓ Rnn. Because Gℓ is symmetric and positive semidefinite, its largest-eigenvalue directions capture the most energetic dimensions of (cid:101)Xℓ. Given two layers ℓ1 and ℓ2, the Centered Kernel Alignment (CKA) between their token embeddings is CKA(cid:0)Xℓ1, Xℓ2 (cid:1) = (cid:113) trace(cid:0)Gℓ1Gℓ2 (cid:1) trace(cid:0)G2 ℓ1 (cid:1) trace(cid:0)G2 ℓ2 (cid:1) , which measures how similarly Gℓ1 and Gℓ2 encode pairwise relationships (dot products) among the token embeddings. A.2 SVD Perspective and Overlap SVD of centered embeddings. Consider the (compact) SVD of (cid:101)Xℓ: (cid:101)Xℓ = Uℓ Σℓ ℓ , where: Uℓ Rnr and min(n, d) is the rank. Then the centered Gram matrix factors as (orthonormal columns), Σℓ = diag(σ1, . . . , σr), Vℓ Rdr (orthonormal columns), ℓ ℓ , so the columns of Uℓ are exactly the eigenvectors of Gℓ, and σ2 are the corresponding eigenvalues. Gℓ = (cid:101)Xℓ (cid:101)X ℓ = Uℓ Σ2 CKA in terms of left singular vectors. Let (cid:101)Xℓ1 = Uℓ1 Σℓ1 ℓ1 , Gℓ2 = Uℓ2 Σ2 Gℓ1 = Uℓ1 Σ2 ℓ1 ℓ2 ℓ1 and (cid:101)Xℓ2 = Uℓ2 Σℓ2 ℓ2 ℓ2 . . Then"
        },
        {
            "title": "We compute",
            "content": "trace(cid:0)Gℓ1 Gℓ2 (cid:16) (cid:1) = trace Uℓ1 Σ2 ℓ1 ℓ1 Uℓ2 Σ2 ℓ2 ℓ (cid:17) = r1(cid:88) r2(cid:88) i=1 j= ℓ1,i σ2 σ2 ℓ2,j (cid:16) u(i) ℓ1 u(j) ℓ2 (cid:17)2 , where u(i) ℓ1 and u(j) ℓ2 are the i-th and j-th columns of Uℓ1 and Uℓ2 , respectively. Meanwhile, trace(cid:0)G2 ℓ1 (cid:1) = r1(cid:88) i=1 σ4 ℓ1,i, trace(cid:0)G2 ℓ2 (cid:1) = r2(cid:88) j= σ4 ℓ2,j. Hence, CKA(cid:0)Xℓ1, Xℓ2 (cid:1) = (cid:88) i,j ℓ1,i σ2 σ2 ℓ2,j (cid:0)u(i) ℓ1 u(j) ℓ2 (cid:1)2 (cid:114)(cid:16)(cid:80) σ4 ℓ1,i (cid:17)(cid:16)(cid:80) σ4 ℓ2,j . (cid:17) Because the eigenvalues σ2 requires significant overlap (cid:0)u(i) ℓ1 principal subspaces of Gℓ1 and Gℓ2 align closely. u(j) ℓ2 (cid:1)2 ℓ,i reflect how dominant each left singular vector is, large CKA value for the most important (largest-σ2) directions, implying the A.3 Conclusion In summary, when CKA(Xℓ1, Xℓ2) is high, the dominant left singular vectors of (cid:101)Xℓ1 and (cid:101)Xℓ2 are well aligned. Since these vectors also serve as the largest-eigenvalue directions of the centered Gram matrices, high CKA implies that the principal subspace geometry of the token embeddings in layers ℓ1 and ℓ2 is structurally very similareven if token-by-token (cosine) matches are small. Thus, CKA goes beyond individual token similarities, capturing how tokens vary collectively in shared subspace."
        }
    ],
    "affiliations": [
        "Cornell University",
        "National Yang Ming Chiao Tung University",
        "University of Washington"
    ]
}