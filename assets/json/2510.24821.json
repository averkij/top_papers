{
    "paper_title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation",
    "authors": [
        "Inclusion AI",
        ":",
        "Bowen Ma",
        "Cheng Zou",
        "Canxiang Yan",
        "Chunxiang Jin",
        "Chunjie Shen",
        "Dandan Zheng",
        "Fudong Wang",
        "Furong Xu",
        "GuangMing Yao",
        "Jun Zhou",
        "Jingdong Chen",
        "Jianing Li",
        "Jianxin Sun",
        "Jiajia Liu",
        "Jianjiang Zhu",
        "Jianping Jiang",
        "Jun Peng",
        "Kaixiang Ji",
        "Kaimeng Ren",
        "Libin Wang",
        "Lixiang Ru",
        "Longhua Tan",
        "Lan Wang",
        "Mochen Bai",
        "Ning Gao",
        "Qingpei Guo",
        "Qinglong Zhang",
        "Qiang Xu",
        "Rui Liu",
        "Ruijie Xiong",
        "Ruobing Zheng",
        "Sirui Gao",
        "Tianqi Li",
        "Tinghao Liu",
        "Weilong Chai",
        "Xinyu Xiao",
        "Xiaomei Wang",
        "Xiaolong Wang",
        "Xiao Lu",
        "Xiaoyu Li",
        "Xingning Dong",
        "Xuzheng Yu",
        "Yi Yuan",
        "Yuting Gao",
        "Yuting Xiao",
        "Yunxiao Sun",
        "Yipeng Chen",
        "Yifan Mao",
        "Yifei Wu",
        "Yongjie Lyu",
        "Ziping Ma",
        "Zhiqiang Fang",
        "Zhihao Qiu",
        "Ziyuan Huang",
        "Zizheng Yang",
        "Zhengyu He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 1 2 8 4 2 . 0 1 5 2 : r Ming-Flash-Omni: Sparse, Unified Architecture for Multimodal Perception and Generation Inclusion AI, Ant Group See Contributions section (Sec. 6) for full author list. We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon sparser Mixture-ofExperts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces highfidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within single unified architecture. Date: Oct 28, 2025 Project Homepage: https://github.com/inclusionAI/Ming"
        },
        {
            "title": "1 Introduction",
            "content": "In everyday life, humans naturally integrate visual and auditory cues to express ideas through speech or writing, while also forming vivid mental images from descriptions or concepts. This ability to visualize enhances creativity, problem-solving, and communication, serving as core aspect of human intelligence and interaction. The ultimate goal of Artificial General Intelligence (AGI) is to replicate this human-like multimodal intelligence, evolving from mere tool into powerful agent that augments and liberates human productivity. Driven by advances in Large Language Models (LLMs) and extensive training on large-scale multimodal datasets, Multi-modal Large Language Models (MLLMs) have demonstrated remarkable perceptual capabilities in both vision (Chen et al., 2024d; Bai et al., 2025b; KimiTeam et al., 2025; Xu et al., 2025c) and audio (Ding et al., 2025a; Xu et al., 2025a,c), as well as generative capabilities in these two modalities (Huang et al., 2025; Ding et al., 2025a; OpenAI, 2025; Tong et al., 2024b; Pan et al., 2025; Xu et al., 2025c). Nevertheless, effectively integrating comprehension and generation across multiple modalities into unified model remains challenging. While humans naturally learn by combining multiple modalities, leveraging their complementary strengths and interactions to enhance overall learning efficiency, building unified Omni-MLLM is hindered by representational disparities and modality imbalances. 1 Figure 1 Ming-Flash-Omni generally demonstrates highly competitive performance across various domains, including vision-text understanding, controllable image generation, speech recognition, and speech synthesis. Specifically, in image generation, MingFlash-Omni introduces novel generative segmentation paradigm to achieve fine-grained spatial and semantic control over the generated images. Moreover, Ming-Flash-Omni significantly enhances Context-Aware Speech Recognition (ContextASR) and Chinese dialect recognition, thereby broadening its applicability in real-world scenarios. In this paper, we introduce Ming-Flash-Omni, which builds upon the Ming-Omni architecture with redesigned foundation and targeted enhancements across multimodal understanding and generation. At its core, Ming-Flash-Omni adopts Ling-Flash-2.0 lin (2025) (a scaled-up, highly sparse Mixture-of-Experts architecture) where an increased sparsity ratio enables substantial model capacity while maintaining bounded inference latency, striking favorable trade-off between performance and efficiency. On the understanding side, the model introduces two key advances. First, Ming-Flash-Omni upgrade the positional encoding to VideoRoPE Wei et al. (2025), refined variant specifically designed to better capture temporal dynamics in video sequences, thereby enhancing the models ability to understand complex visual events. Second, Ming-Flash-Omni focus on improving the context-aware ASR capability itself, enhancing the models ability to leverage surrounding linguistic context during speech recognition and thereby achieving more accurate transcription in context-dependent scenarios. On the generation side, Ming-Flash-Omni introduces three key advancements: 1) in speech synthesis, discrete acoustic tokens are replaced with continuous representations, effectively circumventing quantization-induced artifacts and yielding more natural and expressive TTS outputs; 2) the model supports generative semantic segmentation, enabling pixel-level semantic content generation conditioned on multimodal inputs; and 3) it enables fine-grained controllable image generation with improved identity preservation and in-image text generation capabilities. These architectural innovations empower Ming-Flash-Omni to deliver exceptional cross-modal performance in both comprehension and generation tasks. Specifically, in the image perception task, Ming-Flash-Omni attained performance comparable to that of Qwen3-Omni (Xu et al., 2025c). Ming-Flash-Omni also delivers superior performance in end-to-end speech understanding and generation.For instance, it achieves SOTA on all 12 metrics on ContextASR-Bench. The remainder of this paper is organized as follows. Section 2 presents the detailed architecture of Ming-Flash-Omni. Sections 3 describes the pretraining and post-training datasets. Section 2 4 reports the evaluation results and compare Ming-Flash-Omni with recent multimodal models. Sections 5 is conclusion."
        },
        {
            "title": "2 Ming-Flash-Omni",
            "content": "As illustrated in Figure 2, Ming-Flash-Omni retains the unified two-stage pipeline of MingOmni AI et al. (2025a), where perception supports multimodal understanding and generation targets speech and image synthesis, while markedly advancing long-context modeling, reasoning, and controllable generation. At the core is Ling-Flash-2.0 lin (2025), sparse MoE LM (100B; 6.1B per token) with dual balancing scheme that stabilizes training and improves efficiency. On the perception side, Ming-Flash-Omni employs VideoRoPE Wei et al. (2025) for temporal modeling, context-aware ASR for more reliable speech understanding, and think mode for deeper multi-step reasoning. On the generation side, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity; for images, we upgrade to synergistic training paradigm that enables generative segmentation-as-editing, facilitating fine-grained and controllable generation. Overall, Ming-Flash-Omni advances the unified model with stable expert routing and scalable long-context modeling, yielding more reliable multimodal understanding and controllable generation."
        },
        {
            "title": "2.1 Unified Understanding Across Modalities",
            "content": "The cornerstone of Ming-Flash-Omni is enhanced multimodal understanding. We retain the established visual and audio encoders (Qwen2.5 (Bai et al., 2025a) and Whisper (Radford et al., 2023)) and feed their projected embeddings, concatenated with tokenized text, into Ling-flash-2.0, sparse MoE language model with distinct routers per modality. Beyond this, Ming-FlashOmni incorporates VideoRoPE to maintain temporal coherence over long-range frame sequences, thus emphasizing temporal modeling. Furthermore, Ming-Flash-Omni adopt context-aware ASR training paradigm that conditions decoding on task or domain context, addressing common shortcomings of conventional ASR in real-world, multi-domain scenarios (limited world knowledge and unreliable proper-noun recognition) and yielding more accurate, context-consistent transcripts. To stabilize training in the more sparse Ling-flash-2.0, we employ hybrid expert-balancing scheme that combines an auxiliary load-balancing loss (as in Ming-Omni) with per-router bias updates (as in Ling-flash-2.0), promoting uniform expert activation and improved convergence."
        },
        {
            "title": "2.2 Unified Speech Understanding and Generation",
            "content": "Ming-Flash-Omni retains the same audio encoder as Ming-Omni (AI et al., 2025a), with optimization efforts focused on improving contextual automatic speech recognition (ASR) performance by incorporating preceding textual context and hotword lists during training. For generation, we replace discrete speech tokens with continuous acoustic latents, avoiding quantization loss and improving fidelity. Specifically, we utilize fixed, pre-trained audio head based on Qwen2.5 (0.5B parameters), which takes LLM-generated text tokens and downsampled VAE latents as input and autoregressively predicts the conditioning signals for the flow-matching headfollowing the paradigm of Jia et al. (2025). It is worth noting that the continuous features used for our generation tasks are derived from our unified continuous speech tokenizer, which is based on VAE-GAN architecture, with the overall training objective comprising two main components: generator loss and discriminator loss. The generator loss consists of (i) multi-scale mel-spectrogram reconstruction loss, (ii) an adversarial 3 Figure 2 The overall framework of Ming-Flash-Omni. This version features sparser LLM based on Ling-flash-2.0 MoE architecture, and integrates VideoRoPE to enhance temporal modeling. Speech generation now uses continuous features instead of discrete tokens, and image generation has been upgraded with support for segmentation. loss, (iii) feature matching loss, and (iv) KL divergence regularization term. The discriminator, in turn, is trained primarily with the adversarial loss. For more implementation details, please refer to Canxiang et al. (2025)."
        },
        {
            "title": "2.3 Unified Image Understanding and Generation",
            "content": "A core challenge in building unified multimodal models is the effective fusion of image understanding and generation. While our Ming-Omni model injects hierarchical semantics via multi-scale query tokens, its language pathway remains frozen during training to prevent interference from the generative objective. This freezing approach, while ensuring stability, creates critical bottleneck: an inherent discrepancy between the learning objectives of understanding and generation. Consequently, even with injected hierarchical semantics, fine-grained visual knowledgesuch as object attributes and spatial relationshipscannot be efficiently transferred to high-precision generation and editing tasks, limiting the models final quality and controllability. Generative Segmentation as an Editing Task To overcome this bottleneck, Ming-Flash-Omni propose synergistic training paradigm that reframes image segmentation as generative editing 4 task. Instead of producing an abstract binary mask (e.g., for segment the banana), the model performs semantics-preserving edit (e.g., color the banana purple). This reformulation forcibly binds the learning objectives of understanding and generation: successful generation requires precise understanding of the objects contours and boundaries. Understanding thus becomes prerequisite for editing, and the edits quality provides direct supervisory signal for the models comprehension, fundamentally unifying their optimization objectives. Crucially, this training cultivates more fundamental, generalizable skill: fine-grained spatiosemantic control, which indirectly resolves the compositionality problem in pure text-to-image generation. In our evaluation on the GenEval benchmark, Ming-Flash-Omni achieved score of 0.90, surpassing leading non-Reinforcement Learning (non-RL) methods. This result suggests that the foundational skill of spatio-semantic control can effectively generalize to pure text-to-image generation tasks. Empowering Advanced Controllable Capabilities This mastery of spatio-semantic control provides solid foundation for suite of advanced functions: Identity (ID) Preservation. We use VAE-encoded identity vector and composite loss (global semantic and local pixel) to maintain subject consistency. The models learned boundary perception is crucial for accurately isolating non-edited regions, ensuring high faithfulness. High-Fidelity Text Rendering. By integrating specialized Glyph-byT5 text encoder, our model leverages its learned pixel-level control to accurately place text, ensuring seamless contextual integration and high-quality results. Multi-Image Fusion and Style Transfer. The model can deconstruct and recombine elements from multiple images (e.g., ID from A, background from B) using distinct Concept Vectors. This complex fusion directly relies on the delineation skill acquired through our core training paradigm."
        },
        {
            "title": "2.4 Overall Training Procedure",
            "content": "The training procedure of Ming-Flash-Omni retains two-stage pipeline: perception and generation. The perception stage, consistent with the Ming-Omni, includes pre-training, instruction tuning, alignment tuning, and an additional coherent RL phase. The coherent RL phase proceeds sequentially: Dynamic-GRPO (D-GRPO) AI et al. (2025b) followed by Unified-DPO (U-DPO). First, we use D-GRPO, applying RL with verifiable rewards on datasets with checkable answers (via on-policy GRPO with balanced sampling, dynamic hyperparameter adjustment, and task-specific accuracy rewards) to reinforce correct reasoning across multimodal tasks. Next, we employ U-DPO for preference alignment, extending standard DPO with an auxiliary instruction-tuning loss term on the chosen samples to enhance instruction adherence and stylistic coherence. Concretely, we employ replay strategy with instruction-stage multimodal data and interleave updates on DPO and instruction-tuning data to stabilize optimization and maintain balanced gradient flow. After perception, we freeze the perception MLLM and optimize only the image generator, while leveraging the pre-trained audio generator from Canxiang et al. (2025). For image generation, the training procedure contains two sequential stages. In the first stage, we pre-train diffusion-based image generator using flow matching objective, while keeping the perception MLLM frozen. The generator is equipped with multi-scale learnable queries to capture hierarchical visual semantics from textual inputs. In the second stage, we extend the model to support image editing by conditioning 5 the denoising process on reference images: the VAE-encoded representations of input images are concatenated with the noisy latent to enforce structural and semantic consistency with the original content. Additionally, input word-level captions are encoded via ByteT5 embeddings to enrich textual conditioning. 2."
        },
        {
            "title": "Infrastructure",
            "content": "Compared to large language models (LLMs), training multimodal foundation models presents several key challenges, primarily stemming from data heterogeneity and model heterogeneity. First, data heterogeneity arises from the need to dynamically switch between diverse input modalities (text, images, audio, and video) during training. These modalities exhibit significant differences in tensor shape, most notably in the form of dynamic batch sizes and variable-length sequences. This variability complicates the design of unified parallel computation layout. As result, computational workloads become unevenly distributed across processing ranks, leading to load imbalance. Moreover, the frequent allocation and deallocation of GPU memory buffers for inputs of varying shapes induce severe memory fragmentation, substantially degrading training efficiency and hardware utilization. Second, in contrast to large language models (LLMs), which are predominantly based on homogeneous, decoder-only Transformer architectures, multimodal foundation models typically employ modality-specific encoders at the input stage, introducing model heterogeneity. Although these encoders are relatively lightweight in terms of parameter count, they are highly sensitive to parallelization strategies. If not carefully partitioned across devices, they can induce substantial pipeline bubbles (i.e., idle computation cycles) during pipeline-parallel execution, thereby constraining overall training throughput. To address these challenges, Ming-Flash-Omni is trained on an enhanced version of the MegatronLM Shoeybi et al. (2019) framework with two key extensions tailored for multimodal workloads: Sequence Packing for Data Heterogeneity: To handle dynamic input shapes, we integrate sequence packing, which densely packs multiple variable-length sequences into fixed-length batches. This significantly improves memory utilization and computational density. Flexible Encoder Sharding for Model Heterogeneity: To mitigate pipeline bubbles caused by encoders, we extend Megatron-LM to support fine-grained encoder sharding, enabling flexible partitioning across data parallelism (DP), pipeline parallelism (PP), and tensor parallelism (TP). This achieves more balanced workloads and higher end-to-end training efficiency. These optimizations collectively achieve more than twice the training throughput of the baseline Megatron-LM implementation."
        },
        {
            "title": "3 Data Construction",
            "content": "We have collected large and diverse set of training data to enable models to process and understand information from multiple modalities, including text, images, audio and videos. The majority of this data comes from Ming-Omni (AI et al., 2025a). In addition, we develop several data processing pipelines to ensure data quality, diversity and deduplication. Establishing an effective multimodal data strategy is essential for the joint multi-modal training, as it facilitates seamless alignment of knowledge across diverse modalities. We categorize the training data based on the core modalities they are designed to enhance, including image, audio, video, and text. The detailed sources and construction methods for each type of data are elaborated in this section. 6 3."
        },
        {
            "title": "Image Data",
            "content": "Image data serves as the cornerstone of our multi-modal corpus. Following Ming-Omni (AI et al., 2025a), we integrate both image-understanding and image-generation datasets to enable the MLLM to acquire unified perception and generation capabilities. Additionally, we further design novel pipelines to synthesize high-quality datasets across diverse dimensions to improve models capabilities and user interaction quality. 3.1."
        },
        {
            "title": "Image Understanding Data",
            "content": "OCR Data: Text recognition and document understanding capabilities are crucial for MLLM. We construct large-scale heterogeneous training dataset with millions of samples, consisting of three data sources: open-source data, expert-collaborative pseudo-labeled data, and human-annotated enhancement data. The expert-collaborative pseudo-labeled data is generated by diagnosing model weaknesses, and using expert models to label targeted data. In addition, to enhance the models capability in textvisual analysis and logical reasoning, we incorporate the Chain-of-Thought (CoT) paradigm into the training data. We incorporate the open-source ChartQA-PoT dataset to enhance the models numerical reasoning ability on charts and pioneeringly use executable Python code as the intermediate reasoning representation. Reasoning Data: In the reasoning training of Ming-Flash-Omni, we enrich the CoT data to enhance the models reasoning capabilities. These data are primarily constructed around three key themes: mathematical logic, spatial reasoning, and GUI reasoning. We design an efficient CoT generation and filtering pipeline to construct high-quality, well-structured reasoning data that enhances the models multi-step reasoning capability. (1) We sample long CoT data using state-of-the-art multimodal reasoning models (e.g., Gemini (Gemini et al., 2023)) to build an initial CoT pool. (2) We evaluate the accuracy and quality of the synthesized CoT data and filter out the low-quality data. Based on this pipeline, we construct 1.5M multimodal long CoT samples, with maximum length of 16K tokens. Besides, to overcome the limitations of the direct answer pattern in text-based QA data, we use reinforcement learning models to generate multi-step reasoning traces and final answers. Experimental results demonstrate that this data significantly improves Ming-Flash-Omnis performance on complex reasoning tasks. Preference alignment Data: To further enhance user interaction and response quality, we introduce preference alignment data, focusing on three main aspects: (1) Instruction intent understanding: To enhance the models ability to understand the true intent behind user instructions, we design novel instruction intent reasoning paradigm where the model first determines if the instruction is clear. For clear instructions, it provides structured reasoning and final answers. For ambiguous ones, it infers the likely intent, generates clarification prompts, and aligns responses accordingly. By incorporating experience-alignment training, we significantly improve the models intent understanding and enhance the overall user experience. (2) Multi-turn conversation: Users often engage in repeated questioning on the same context. To maintain semantic consistency across turns, we decompose complex instructions into multi-turn conversations and generate high-quality responses using expert-annotated LLM pipelines (Laban et al., 2025). This ensures context retention and reduces performance degradation in multi-turn scenarios. (3) Complex multimodal instruction following: Users often provide sequential, interdependent commands. We design multimodal instruction generation pipeline to generate SFT and DPO-style data to cover basic and complex instruction types (Zhang et al., 2024; Qian et al., 2024; Ding et al., 2025b). Structured Data: Structured data enhances MLLM capabilities in querying fine-grained knowledge 7 associated with specific visual entities. To enhance the MLLMs ability to handle knowledgeintensive and information-seeking queries, we design two pipelines to generate large-scale entityrelation and encyclopedic QA data. (1) Entity-relation QA data: We design multi-stage pipeline to construct high-quality entity-relation QA corpus. The pipeline integrates scene graph extraction, visual grounding, entityrelation verification, and diverse QA generation to produce pairs covering entities, attributes, and relations. To ensure quality, we filter source datasets for complex multi-entity scenes and apply two-stage validation before and after QA generation. (2) Encyclopedia QA Data: We design an automated pipeline to generate encyclopedia entity QA data from Chinese Baidu Encyclopedia and English Wikidata. It constructs <image, entity, knowledge> triplets by extracting entities from images or retrieving images for knowledge-base entities, filtering out invalid ones, and validating imagetext consistency. These triplets are then converted into VQA data using LLMs. 3.1."
        },
        {
            "title": "Image Generation Data",
            "content": "Image generation data enhances MLLM capabilities beyond traditional image understanding tasks. In addition to the image generation data used in Ming-Omni (AI et al., 2025a), we specifically integrate image segmentation data, text generation data and portrait preservation data to further improve the user experience. Image segmentation data: To improve the models generative segmentation capability, we construct two types of data: (1) We use the open-source referring segmentation datasets RefCOCO/+/g (Kazemzadeh et al., 2014; Mao et al., 2016) to construct image editing data. The original image serves as the reference, and binary masks highlight target regions with specified colors to create edited images. (2) For semantic and panoptic segmentation, samples are built from COCOPanoptic (Lin et al., 2014; Kirillov et al., 2019), where each class or instance is assigned unique color via predefined colormap to generate edited images. Portrait preservation data: The portrait preservation data consist of two data sources: (1) ID Photo Dataset: We collected and constructed 200k paired lifestyle-ID photos. And filter the data using four criteria, e.g., face similarity, face size and confidence, face angle and manual review. (2) Landmark Check-In Portrait Dataset: We collect 20K high-quality portraits from 225 landmarks.The original images serve as edited images, while using advanced segmentation model like SAM2 (Ravi et al., 2024) to segments the foreground person and places them onto 1,000 manually collected daily background scenes as pre-edited images. And then use LLM generate diverse prompts for each landmark. Text generation data: We build Chinese-English text generation dataset across three difficulty levels: (1) Monotonic background text rendering: Text is rendered directly by setting background color, font type, size, color, and position. (2) Text rendering on existing images: texts are rendered on suitable smooth regions obtained by Felzenszwalb algorithm. (3) Text-image integrated rendering: Using the SOTA LLMs to generate text rendering prompts, the advanced generation models (e.g., Qwen-image (Wu et al., 2025) and Nano Banana) are used for image generation, followed by OCR for consistency checks, resulting in high-quality dataset."
        },
        {
            "title": "3.2 Audio Data",
            "content": "For audio data, we mainly use the data from Ming-Omni (AI et al., 2025a). In addition, we incorporate the following three datasets to further enhance the models audio understanding and generation capabilities. 8 Context ASR Data: Current ASR systems face challenges in recognizing homophones or phonetically similar words when the context is limited, pronunciations are unclear, or accents are present. ContextASR addresses these issues by leveraging the preceding context. We propose to synthesize large-scale dataset using LLMs to endow models with ContextASR capabilities. We extract named entities and construct context passages using LLMs based on existing ASR text, producing 3 million Chinese and English samples in the format <audio, text, context, entity_list>. During training, we further filter and sample the data to reduce keyword density, remove keywords that are absent from the text, and generate negative samples to enhance the models discriminative ability. TTS Data: The diversity of TTS data is essential for fully leveraging the pretrained language models capabilities in audio generation. In addition to the open-source data used in Ming-Omni, we develop data generation pipeline to create large-scale TTS data. Specifically, (1) we crawl extensive audio data using keywords expanded from handcrafted seeds through domain-specific lexical variations, (2) apply VAD (Gao et al., 2023) to segment well-conditioned short clips, and (3) iteratively train an audio labelerinitially on high-quality data, then using its predictions to label the corpus and refine its accuracy. Based on the short audio clips and the audio labeler, we acquire large number of high-quality audio clips with labels from different domains."
        },
        {
            "title": "3.3 Video Data",
            "content": "Video streaming conversation constitutes fundamental capability for MLLMs in video understanding. However, acquiring large-scale streaming multi-turn conversation data is prohibitively expensive. In this work, we propose pipeline to systematically synthesize diverse, balanced, and high-quality multi-turn conversation video datasets. We collect 8.2M videos from the internet, ranging from 90 seconds to 10 minutes, and first filter out low-quality videos with high speech density, high shot density, irregular aspect ratios, or low resolution. We then filter out low-information, incoherent, or overly simple videos using SOTA MLLMs. To ensure balanced dataset, we use advanced embedding models (e.g., Qwen3-Embedding (Zhang et al., 2025) and M3-Embedding (Chen et al., 2024a)) to extract embeddings and then cluster the videos to suppress high-frequency data while preserving long-tail content. Finally, we use SOTA video understanding models to generate high-quality video conversations. This produces 1.2M conversation turns across 5-minute average videos, balanced across various task categories."
        },
        {
            "title": "3.4 Text Data",
            "content": "For text data, we utilize corpus from Ling (LingTeam et al., 2025), M2-Omni (Guo et al., 2025), and Ming-Omni (AI et al., 2025a) to preserve and further enhance the models language proficiency."
        },
        {
            "title": "4 Evaluation",
            "content": "In this section, we present the evaluation details and quantitative examples of Ming-Flash-Omni on both public and in-house benchmarks."
        },
        {
            "title": "4.1 Public Benchmarks",
            "content": "The details of the public benchmarks are provided in Appendix A. As shown in Table 19, our holistic assessment covers more than 50 rigorously curated public benchmarks across the following seven distinct multi-modal dimensions: Image Text (Understanding), Text Image (Generation), 9 Table 1 Performance of Ming-Flash-Omni on Vision-to-Text Benchmarks compared to leading models.* denotes metrics tested using the official benchmark prompts. Type Benchmark General OCR MMStar AI2D HallusionBench CV-bench MathVista_MINI CRPE ChartQA DocVQA OCRBench TextVQA Complex instruction MIA-Bench Multi-image Video MMTBench_val_mi MuirBench Llava_interleave_Bench MVBench VideoMME VideoMME_w_subtitle LongVideoBench Ming-Flash Omni 68.3 85.2 61.1 81.6 81.9 78.4 88.4 94.8 879 82.6 93.8 68.0 61.5 63.3 74.6 70.9 73.0 61.7 Qwen3-Omni 30B-A3B 68.5 85.2 59.7 - 80.0 - 87.5 95.0 860 81.7 94.5 - - - - 70.5 - - Qwen3-VL 30B-A3B 72.1 86.9 61.5 - 81.9 80.0* 87.1 95.2 903 81.7 94.4 65.7* 62.9 51.1* 72.3 74.5 - - InternVL3.5 30B-A3B 72 86.8 53.8 77.3 80.9 77.6 87.4 94.2 880 80.5 - - - - 72.1 68.7 71.8 63.8 Table 2 Performance of Ming-Flash-Omni on Text-to-Image Generation Benchmarks compared to leading models. Gen. denotes models for pure image generation, while Uni. denotes models capable of both image understanding and generation. Note that the global best performance is highlighted by an underline, and the local best result in Gen. or Uni. is marked with bold. Type Model Gen. Uni. LlamaGen LDM SDv1.5 PixArt-Î± SDv2.1 Emu3-Gen SDXL DALL-E 3 SD3-Medium LWM SEED-X Show-o TokenFlow-XL Janus JanusFlow JanusPro-7B UniWorld-V1 OmniGen2 BAGEL Qwen-Image Qwen-Image-RL Ming-Flash-Omni 1-Obj. 0.71 0.92 0.97 0.98 0.98 0.98 0.98 0.96 0.99 0.93 0.97 0.95 0.95 0.97 0.97 0.99 0.98 0.99 0.99 0.99 1.00 0.99 2-Obj. 0.34 0.29 0.38 0.50 0.51 0.71 0.74 0.87 0.94 0.41 0.58 0.52 0.60 0.68 0.59 0.89 0.93 0.96 0.94 0.92 0.95 0.94 GenEval Colors 0.58 0.70 0.76 0.80 0.85 0.81 0.85 0.83 0.89 0.79 0.80 0.82 0.81 0.84 0.83 0.90 0.89 0.98 0.88 0.88 0.92 0. Posit. 0.07 0.02 0.04 0.08 0.07 0.17 0.15 0.43 0.33 0.09 0.19 0.11 0.16 0.46 0.53 0.79 0.74 0.71 0.64 0.76 0.87 0.95 Color. 0.04 0.05 0.06 0.07 0.17 0.21 0.23 0.45 0.60 0.15 0.14 0.28 0.24 0.42 0.42 0.66 0.71 0.75 0.63 0.77 0.83 0.77 AVG 0.32 0.37 0.43 0.48 0.50 0.54 0.55 0.67 0.74 0.47 0.49 0.53 0.55 0.61 0.63 0.80 0.84 0.86 0.82 0.87 0.91 0.90 DPG-Bench - - - - 68.09 80.60 74.65 - 84.08 - - - - 79.68 80.09 84.19 81.38 83.57 - 88.32 - 83.08 Count 0.21 0.23 0.35 0.44 0.44 0.34 0.39 0.47 0.72 0.46 0.26 0.49 0.41 0.30 0.45 0.59 0.81 0.74 0.81 0.89 0.93 0. 10 Table 3 Performance of Ming-Flash-Omni on Image-to-Image Editing Benchmarks compared to leading models. All metrics are evaluated by GPT-4.1. Edit. denotes models specifically trained for image editing, while Unified. denotes models capable of image understanding, generation and editing. Type Model GEdit-Bench-EN (Full set) GEdit-Bench-CN (Full set) G_SC G_PQ G_SC G_PQ G_O G_O Edit. Unified. Instruct-Pix2Pix AnyEdit MagicBrush Step1X-Edit Qwen-Image-Edit UniWorld-v1 OmniGen OmniGen2 BAGEL Ming-Flash-Omni 3.58 3.18 4.68 7.09 8.00 4.93 5.96 7.16 7.36 7.32 5.49 5.82 5.66 6.76 7.86 7.43 5.89 6.77 6.83 7.27 3.68 3.21 4.52 6.70 7.56 4.85 5.06 6.41 6.52 6. - - - 7.20 7.82 - - - 7.34 7.26 - - - 6.87 7.79 - - - 6.85 7.20 - - - 6.86 7.52 - - - 6.50 6. Table 4 Performance of Ming-Flash-Omni on Image-to-Mask Segmentation Benchmarks compared to leading models. Model types are denoted as: Vision. for vision-only models, SAM. for models equipped with an additional SAM-like segmentation head, and Unified. for unified MLLMs capable of both understanding and generation. Results with * are obtained by evaluating on 500 images sampled from each dataset via the official API. Type Model RefCOCO (val) RefCOCO+ (val) RefCOCOg (val) Vision. SAM. VLT CRIS LAVT PolyFormer-B LISA-7B PixelLM-7B OMG-LLAVA Unified. Nano-banana* Qwen-Image-Edit* Ming-Flash-Omni 67.5 70.5 72.7 74. 74.1 73.0 75.6 15.7 30.3 72.4 56.3 62.3 62.1 67.6 62.4 66.3 65.6 13.9 28.8 62.8 55.0 59.9 61.2 67. 66.4 69.3 70.7 14.9 34.0 64.3 Table 5 Performance of Ming-Flash-Omni on PUBLIC Text-to-Speech Benchmarks compared to leading MLLMs. Type Chinese English Benchmark (Seed-TTS-Eval) Zh-wer Zh-sim En-wer En-sim Ming-Flash Ming-Lite Omni 0.99 0.74 1.59 0.68 Omni 1.69 0.68 4.31 0.51 Qwen3 Omni 1.07 - 1.39 - Seed TTS 1.11 0.80 2.24 0. F5 TTS 1.56 0.74 1.83 0.65 CosyVoice2 Qwen2.5 1.45 0.75 2.57 0.65 Omni 1.70 0.75 2.72 0.63 Table 6 Performance of Ming-Flash-Omni on Context ASR benchmarks. Model Performance Speech-English Dialogue-English Speech-Mandarin Dialogue-Mandarin WER NE-WER NE-FNR WER NE-WER NE-FNR WER NE-WER NE-FNR WER NE-WER NE-FNR Qwen2-Audio Baichuan-Audio Kimi-Audio Baichuan-Omni-1.5 Qwen2.5-Omni-3B Qwen2.5-Omni-7B Ming-Flash-Omni 11.49 27.27 35.08 7.52 5.87 4.55 2.90 6.68 8.01 8.16 7.69 6.53 3.99 7.80 9.69 3.96 7.38 8.72 2.85 2.63 2.36 13.99 33.02 32.92 5.66 10.01 3.64 4.67 13.50 11.31 9.91 14.40 5.54 4.83 14.36 12.85 5.32 11.83 9.24 3.76 7.06 1.90 9.92 24.10 30.02 2.16 6.65 2.35 1.95 11.13 15.28 2.98 8.39 4.71 2.13 10.55 14.11 1.84 9.80 12.19 1.31 5.62 1.59 7.00 22.76 26.17 2.96 11.48 3.94 2.90 15.91 16.68 5.00 16.83 7.84 3.12 15.07 15.17 2.40 14.06 13.17 1.78 8.47 0.85 Table 7 Performance of Ming-Flash-Omni on PUBLIC and IN-HOUSE Audio Understanding Benchmarks. Type Benchmark PUBLIC Chinese Benchmarks PUBLIC English Benchmarks IN-HOUSE Dialect Benchmarks IN-HOUSE Domain Benchmarks Aishell1 Aishell2-test-android Aishell2-test-ios Cv15-zh Fleurs-zh Wenetspeech-testmeeting Wenetspeech-testnet SpeechIO Average (Chinese) Llibrispeech-test-clean Librispeech-test-other Multilingual-librispeech Cv15-en Fleurs-en Voxpopuli-v1.0-en Average (English) Hunan Minnan Guangyue Chuanyu Shanghai Anhui Dongbei Henan Hubei Jiangsu Kejiahua Shaanxi Shandong Tianjin Yunnan Noisy Chat Government Health Knowledge Local-live Average (All IN-HOUSE) Ming-Flash Omni 1.22 2.51 2.46 5.84 2.80 5.58 5.05 2.51 3.50 1.24 2.29 4.04 6.63 3.07 5.99 3.88 6.98 13.03 4.07 3.75 9.92 5.38 4.38 7.93 7.90 11.20 15.43 6.03 12.63 14.61 15.91 10.69 3.17 1.89 3.11 3.08 1.74 7.75 Qwen3 Omni 1.04 2.64 2.55 4.31 2.20 5.89 4.69 2.18 3.19 1.22 2.48 3.67 6.05 2.72 6.02 3.69 20.82 24.60 27.43 5.54 30.96 5.70 3.92 8.94 14.55 13.19 27.88 5.31 17.04 19.70 19.78 9.25 2.09 1.55 2.22 11.52 1.37 13.02 Qwen2 Audio 1.53 2.92 2.92 6.90 7.50 7.16 8.42 3.01 5.05 1.60 3.60 5.40 8.60 6.90 6.84 5.49 25.88 123.78 7.59 7.77 31.73 5.72 4.87 12.31 16.39 12.80 22.33 6.32 15.00 21.78 21.57 12.46 4.29 2.70 4.18 3.33 2.34 17.39 Kimi Audio 0.60 2.64 2.56 7.21 2.69 6.28 5.37 2.23 3.70 1.28 2.42 5.88 10.31 4.44 7.97 5.38 31.93 80.28 41.49 6.69 60.64 8.61 4.40 14.40 20.07 17.25 29.78 6.09 18.66 34.57 32.79 24.40 2.96 2.03 2.38 1.98 2.05 21. Table 8 Performance of Ming-Flash-Omni on PUBLIC Audio Question-Answering Benchmarks. Models Step-Audio-chat Qwen2-Audio-chat Baichuan-Audio GLM-4-Voice Kimi-Audio Megrez-3B-Omni DiVA Qwen2.5-Omni Qwen3-Omni-Flash-Instruct Baichuan-Omni-1.5 MiniCPM-o Ming-Flash-Omni Mean 57.10 54.70 62.50 57.20 76.90 46.20 55.70 74.10 85.40 71.10 71.70 83.50 Open-ended QA AlpacaEval CommonEval 79.80 73.80 80.00 81.20 89.20 70.00 73.40 89.80 95.40 90.00 88.40 94. 59.80 68.00 67.80 69.60 79.40 59.00 70.80 78.60 91.00 81.00 83.00 92.60 Knowledge SD-QA 46.84 35.35 49.64 43.31 63.12 25.95 57.05 55.71 76.80 43.40 50.72 71.72 Multi-Choice QA MMSU OpenBookQA 31.87 35.43 48.80 40.11 62.17 27.03 25.76 61.32 68.40 57.25 54.78 68.70 29.19 49.01 63.30 52.97 83.52 28.35 25.49 81.10 91.40 74.51 78.02 84.84 Instruction IFEval 65.77 22.57 41.32 24.91 61.10 25.71 39.15 52.87 75.20 54.54 49.25 72.49 Safety AdvBench 86.73 98.85 86.73 88.08 100.00 87.69 98.27 99.42 99.40 97.31 97.69 99. 12 Table 9 Performance of Ming-Flash-Omni on StreamingMultiturnBench compared to leading omni-MLLMs. Model Qwen2.5-Omni Ming-Lite-Omni Ming-Flash-Omni Accuracy 54.03 44.63 57.03 Completeness Relevance Conciseness Naturalness 53.68 40.58 57.10 78.00 69.28 80.40 77.38 93.88 94.13 98.28 95.65 99.40 Proactivity 46.25 28.78 41.38 Average 67.93 62.13 71. Figure 3 Visualization results of Ming-Flash-Omni on understanding tasks, including world knowledge, multi-image understanding, mathematical reasoning, OCR, contextual ASR, and dialect-aware ASR. 13 Figure 4 Visualization results of Ming-Flash-Omni on Text/Image Image tasks, including image generation task, image editing task, and image segmentation task. 14 Figure 5 Visualization results of Ming-Flash-Omni on Image Image tasks, including ID photo generation, ID photo editing, background replacement, and multi-image editing. Image Image (Editing), Image Image (Segmentation), Audio Text (Understanding), Text Audio (Generation), and Video Text (Understanding). 4.2 In-house Benchmarks In addition to public benchmarks, we also establish three in-house benchmarks to comprehensively evaluate multiple capabilities of MLLMs, including: Multi-Dialect and Multi-Domain Audio Understanding Benchmark. To extend audio understanding benchmarks into multi-dialect and multi-domain settings, we constructed two specialized datasets. The multi-dialect dataset was created from 15 regions, while the multi-domain one was curated from six domains. All samples were manually verified for quality by trained annotators. The final datasets comprise 51,986 multi-dialect samples and 10,397 multi-domain samples, with the latter distributed across: Noisy (8,145), Chat (443), Government (462), Health (450), Knowledge (421), and Local Services (476). Video Streaming Multi-turn Benchmark. The evaluation of video streaming multi-turn dialogue capabilities requires quantifying not only the models understand capability but also assessing its interactive experience, including proactivity and naturalness. Previous streaming dialogue datasets, such as StreamBench (Lin et al., 2024) and OvO-Bench (Niu et al., 2025), have primarily focused on the understanding aspect while lacking thorough evaluation of the interactive experience. To address this gap, we introduce StreamingMultiturnBench. To construct StreamingMultiturnBench, we manually selected 380 videos, carefully ensuring coverage of multiple key domains including life recording, education, TV shows, video games, and documentaries. Then we use SOTA closed-source modelfor machine annotation. Subsequently, team of 10 human annotators revise and doublecheck the dialogue content to ensure it aligns with human conversational preferences. This process yielded 2,200 video question-answer pairs. During evaluation, we use advanced closed-source model, e.g. GPT-4o (OpenAI, 2025), to compare the models output against the human-annotated answers, scoring it on scale of 1 to 5 across the six dimensions: accuracy, completeness, relevance, naturalness, conciseness, and proactivity. The final score is the average for each dimension. To align our metrics with other video benchmarks, we linearly scale the results to 100-point scale. We commit to open-sourcing and publicly maintaining this benchmark to ensure reproducibility."
        },
        {
            "title": "4.3 Quantitative Results",
            "content": "We conduct comprehensive evaluations of Ming-Flash-Omni against state-of-the-art MLLMs on over 50 different multimodal benchmarks, as illustrated in Table 19. Extensive experiments demonstrate that Ming-Flash-Omni achieves comparable performance with leading MLLMs. Vision Text (Understanding) As shown in Table 1, Ming-Flash-Omni demonstrates strong and competitive performance across wide range of visionlanguage benchmarks. Specifically, on general-purpose understanding task, it achieves performance on par with leading omni-models (most notably scoring 81.9% on MathVista), though it still exhibits slight gap compared to state-of-the-art visionlanguage models. Similarly, on OCR-centric benchmarks, Ming-Flash-Omni attains stateof-the-art results among omni-modal models, yet remains marginally behind the best proprietary counterparts. In multi-image understanding, Ming-Flash-Omni outperforms the leading opensource visionlanguage model Qwen3-VL-30B-A3B on MMT-Bench (68.0) and LLaVA-Interleave Bench (63.3), while showing minor performance gap on MuirBench, suggesting opportunities for further improvement. In video understanding, Ming-Flash-Omni achieves state-of-the-art performance on MVBench with score of 74.6, demonstrating robust general video reasoning capabilities. It further exhibits specialized proficiency in processing linguistic content within videos, attaining leading score of 73.0 on VideoMME with subtitles. Although its performance on long-form video understanding is slightly lower, its consistently strong results across most metrics underscore its advanced video comprehension capabilities. Text Image (Generation). As shown in Table 2, our experimental results demonstrate that the generation quality of Ming-Flash-Omni is on par with state-of-the-art diffusion models. Notably, on the Geneval benchmark, our model surpasses all non-Reinforcement Learning methods, demonstrating exceptional controllability. This advantage is particularly pronounced in the \"Position\" and \"Color.\" sub-categories. On the DPG-Bench benchmark, Ming-Flash-Omni achieves an overall score of 83.08, performance level comparable to pure image generation models like SD3-Medium (84.08) and leading unified models like OmniGen2 (83.57). Image Image (Editing). As shown in Table 3, Ming-Flash-Omni demonstrates impressive image editing performance, surpassing all other unified models. Specifically, Ming-Flash-Omni supports editing instructions in Chinese, achieving performance comparable to that with English instructions. Compared to Qwen-Image-Edit which utilizes 20B DiT head, Ming-Flash-Omni achieves comparable semantic consistency and perceptual quality with much more efficient 2B DiT headonly one-tenth the parameters. This efficiency also translates to remarkable inference speeds, typically between 1-2 seconds per generation. Image Image (Segmentation). As shown in Table 4, Ming-Flash-Omni is capable of performing segmentation tasks, achieving performance comparable to that of specialized models designed explicitly for this purpose. Compared to other unified MLLMs, Ming-Flash-Omni demonstrates significant advantage in segmentation. For instance, Qwen-Image-Edit often struggles to accurately localize the target object, while Nano-banana frequently misinterprets user intent during inference. In contrast, Ming-Flash-Omni exhibits superior robustness and more accurate understanding of spatial and semantic instructions. Audio Text (Understanding). Our model sets new state-of-the-art (SOTA) on all 12 subtasks of the ContextASR-Bench  (Table 6)  , underscoring its superior ability to leverage contexta vital skill for real-world applications like multi-turn dialogue and hotword enhancement. It also exhibits highly competitive performance across various ASR benchmarks, with notable strengths in dialect recognition  (Table 7)  . In audio question answering, Ming-Flash-Omni surpasses all opensource audio-centric and other Omni models, with the exception of Qwen3-Omni-Flash-Instruct(Xu et al., 2025b)  (Table 8)  . Taken together, these findings demonstrate the robust and versatile audio understanding capabilities of Ming-Flash-Omni. Text Audio (Generation). As shown in Table 5, leveraging advancements in speech representation and model architecture, Ming-Flash-Omni achieves SOTA performance among open-source models on the test-zh subset of the SEED-TTS-Eval benchmark(Anastassiou et al., 2024b). Furthermore, its WER on the test-en subset is surpassed only by that of Qwen3-Omni. Video + Audio Text (Video Streaming Conversation). As shown in Table 9, benefiting from the introduction of high-quality and diverse streaming video multiturn data, Ming-Flash-Omni has achieved significant improvements in all dimensions compared to Ming-Lite-Omni. MingFlash-Omni also outperforms Qwen2.5-Omni (Xu et al., 2025a) in the dimensions of accuracy, completeness, relevance, and conciseness, providing better experience in streaming video conversation scenarios."
        },
        {
            "title": "4.4 Visualization Results",
            "content": "In this section, we present several visualization examples to better illustrate the capabilities of Ming-Flash-Omni. First, as shown in the figure, Ming-Flash-Omni demonstrates strong visual understanding across multiple dimensions: it leverages rich world knowledge to accurately infer geographic locations from visual cues; excels at multi-image understanding and generates creative, coherent text grounded in multiple images; solves complex mathematical problems through clear, step-by-step reasoning; and exhibits robust document understanding by accurately parsing intricate formulas and answering questions about sophisticated charts and diagrams within images. Turning to speech recognition, Ming-Flash-Omni achieves strong performance on contextual ASR tasks. By leveraging contextual information, it effectively resolves many challenging cases where conventional ASR systems tend to failsuch as ambiguous homophones, domain-specific terminology, or noisy conversational speech. Moreover, this version also supports multiple Chinese dialects, significantly broadening its applicability in real-world multilingual and regional speech scenarios. Lastly, we visualize the capabilities of Text/Image Image generation tasks in Figure 4 and Figure 5, covering wide range of applications including image generation, image editing, image segmentation, multiimage editing, ID photo generation, ID photo editing, and background replacement. As can be seen, Ming-Flash-Omni not only supports broader set of generative capabilities but also achieves higher output quality and greater controllability compared to previous versions."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present Ming-Flash-Omni, built upon Ling-Flash-2.0 with 100 billion parameters, where only 6.1B parameters are activated per token. Ming-Flash-Omni demonstrates advanced multimodal perception and generation capabilities with improved computational efficiency while scaling model capacity. It achieves SOTA performance across broad spectrum of tasks, including multi-image and video processing, image generation, generative segmentation, Contextual Automatic Speech Recognition (ContextASR), and multi-dialect recognition, outperforming omni models 17 of comparable scale. We believe the open-sourcing of our models and code will facilitate the development of AGI by advancing multimodal intelligence research and enabling broader real-world applications."
        },
        {
            "title": "6 Contributors",
            "content": "Authors are listed alphabetically by the first name."
        },
        {
            "title": "Xiaolong Wang\nXiao Lu\nXiaoyu Li\nXingning Dong\nXuzheng Yu\nYi Yuan\nYuting Gao\nYuting Xiao\nYunxiao Sun\nYipeng Chen\nYifan Mao\nYifei Wu\nYongjie Lyu\nZiping Ma\nZhiqiang Fang\nZhihao Qiu\nZiyuan Huang\nZizheng Yang\nZhengyu He",
            "content": ""
        },
        {
            "title": "References",
            "content": "Ling-flash-2.0, 2025. https://huggingface.co/inclusionAI/Ling-flash-2.0. Accessed: 2025-09-17. Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, et al. Ming-omni: unified multimodal model for perception and generation. arXiv preprint arXiv:2506.09344, 2025a. Inclusion AI, Fudong Wang, Jiajia Liu, Jingdong Chen, Jun Zhou, Kaixiang Ji, Lixiang Ru, Qingpei Guo, Ruobing Zheng, Tianqi Li, et al. M2-reasoning: Empowering mllms with unified general and spatial reasoning. arXiv preprint arXiv:2507.08306, 2025b. Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, Mingqing Gong, Peisong Huang, Qingqing Huang, Zhiying Huang, Yuanyuan Huo, Dongya Jia, Chumin Li, Feiya Li, Hui Li, Jiaxin Li, Xiaoyang Li, Xingxing Li, Lin Liu, Shouda Liu, Sichao Liu, Xudong Liu, Yuchen Liu, Zhengxi Liu, Lu Lu, Junjie Pan, Xin Wang, Yuping Wang, Yuxuan Wang, Zhen Wei, Jian Wu, Chao Yao, Yifeng Yang, Yuanhao Yi, Junteng Zhang, Qidi Zhang, Shuo Zhang, Wenjie Zhang, Yang Zhang, Zilin Zhao, Dejian Zhong, and Xiaobin Zhuang. Seed-tts: family of high-quality versatile speech generation models. CoRR, abs/2406.02430, 2024a. Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, et al. Seed-tts: family of high-quality versatile speech generation models. arXiv preprint arXiv:2406.02430, 2024b. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025a. https://arxiv.org/abs/2502.13923. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. AISHELL-1: an open-source mandarin speech corpus and speech recognition baseline. In 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment, O-COCOSDA, pages 15. IEEE, 2017. Canxiang, Chunxiang Yan, Dawei Jin, Haibing Huang, Han Yu, Hui Peng, Jie Zhan, Jing Gao, Jingdong Peng, Jun Chen, Kaimeng Zhou, Ming Ren, Mingxue Yang, Qiang Yang, Qin Xu, Ruijie Zhao, Shaoxiong Xiong, Xuezhi Lin, Yi Wang, Yifei Yuan, Yongjie Wu, Zhengyu Lyu, He, Zhihao , Zhiqiang Qiu, Ziyuan Fang, and Huang. Ming-uniaudio: Speech llm for joint understanding, generation and editing with unified representation. arXiv preprint arXiv:https://arxiv.org/submit/6926109/view, 2025. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024b. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, and Haizhou Li. Voicebench: Benchmarking llm-based voice assistants, 2024c. https://arxiv.org/abs/2410.17196. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024d. Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025a. Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Mm-ifengine: Towards multimodal instruction following. arXiv preprint arXiv:2504.07957, 2025b. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024a. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024b. Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao Du, Zhangyu Xiao, and Shiliang Zhang. Funasr: fundamental end-to-end speech recognition toolkit, 2023. https://arxiv.org/abs/2305.11013. 20 Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1437514385, June 2024. Qingpei Guo, Kaiyou Song, Zipeng Feng, Ziping Ma, Qinglong Zhang, Sirui Gao, Xuzheng Yu, Yunxiao Sun, Tai-Wei Chang, Jingdong Chen, et al. M2-omni: Advancing omni-mllm for comprehensive modality support with competitive performance. arXiv preprint arXiv:2502.18778, 2025. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, et al. Step-audio: Unified understanding and generation in intelligent speech interaction. arXiv preprint arXiv:2502.11946, 2025. Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, et al. Ditar: Diffusion transformer autoregressive modeling for speech generation. arXiv preprint arXiv:2502.03930, 2025. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787798, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1086. https://aclanthology.org/D14-1086/. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. KimiTeam, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr DollÃ¡r. Panoptic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 94049413, 2019. Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. Llms get lost in multi-turn conversation. arXiv preprint arXiv:2505.06120, 2025. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024a. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024b. Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, and Maosong Sun. Streamingbench: Assessing the gap for mllms to achieve streaming video understanding. arXiv preprint arXiv:2411.03628, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. LingTeam, Binwei Zeng, Chao Huang, Chao Zhang, Changxin Tian, Cong Chen, Dingnan Jin, Feng Yu, Feng Zhu, Feng Yuan, et al. Every flop counts: Scaling 300b mixture-of-experts ling llm without premium gpus. arXiv preprint arXiv:2503.05139, 2025. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), December 2024. ISSN 1869-1919. doi: 10.1007/s11432-024-4235-6. http://dx.doi.org/10.1007/s11432-024-4235-6. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. 21 Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1120, 2016. doi: 10.1109/CVPR.2016.9. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. https://aclanthology.org/2022.findings-acl.177/. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images. In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 21992208, 2021. doi: 10.1109/WACV48630.2021.00225. Junbo Niu, Yifei Li, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al. Ovo-bench: How far is your video-llms from real-world online video understanding? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1890218913, 2025. OpenAI. Introducing 4o image generation. https://openai.com/index/introducing-4o-image-generation/, 2025. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Mia-bench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024a. https://arxiv.org/abs/2406.16860. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024b. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390, 2021. Fei Wang, Xingyu Fu, James Y.Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Yan Lorena, Jacky Wwenjie Mo, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024a. He Wang, Linhan Ma, Dake Guo, Xiong Wang, Lei Xie, Jin Xu, and Junyang Lin. Contextasr-bench: massive contextual speech recognition benchmark, 2025. https://arxiv.org/abs/2507.05727. Weiyun Wang, Yiming Ren, Haowen Luo, Li Tiantong, Yan Chenxiang, Chen Zhe, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. arXiv preprint arXiv:2402.19474, 2024b. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, et al. Videorope: What makes for good video rotary position embedding? arXiv preprint arXiv:2502.05173, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 22 Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025a. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-omni technical report, 2025b. https://arxiv.org/abs/2509.17765. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025c. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. Albert Zeyer, AndrÃ© Merboldt, Wilfried Michel, Ralf SchlÃ¼ter, and Hermann Ney. Librispeech transducer model with internal language model prior correction. In Hynek Hermansky, Honza CernockÃ½, LukÃ¡s Burget, Lori Lamel, Odette Scharenborg, and Petr MotlÃ­cek, editors, Annual Conference of the International Speech Communication Association, Interspeech, pages 20522056. ISCA, 2021. Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, Di Wu, and Zhendong Peng. WENETSPEECH: 10000+ hours multi-domain mandarin corpus for speech recognition. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, pages 61826186. IEEE, 2022. Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, and Yongbin Li. Iopo: Empowering llms with complex instruction following via input-output preference optimization. arXiv preprint arXiv:2411.06208, 2024. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025."
        },
        {
            "title": "A Public Benchmarks",
            "content": "Image Text (Understanding). Our evaluation of the image-to-text understanding capabilities primarily encompasses the following six tasks: 1) general image understanding capabilities evaluated on MMStar (Chen et al., 2024b), AI2D (Kembhavi et al., 2016), HallusionBench (Guan et al., 2024), CV-Bench (Tong et al., 2024a), MathVista (Lu et al., 2024), and CRPE (Wang et al., 2024b). 2) OCR capabilities evaluated on ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), OCRBench (Liu et al., 2024), and TextVQA-VAL (Singh et al., 2019). 3) multi-image capabilities evaluated on MMTBench (Ying et al., 2024), MuirBench (Wang et al., 2024a), and LLaVA-interleave Bench (Li et al., 2024a). 4) Complex instruction capabilities evaluated on MIA-Bench (Qian et al., 2024). 5) video understanding capabilities evaluated on MVBench (Li et al., 2024b), VideoMME (Fu et al., 2024b), and LongVideoBench (Wu et al., 2024). Text Image (Generation). We incorporate text-to-image generation capabilities to enable our MLLM with unified perception-generation abilities, which are evaluated on GenEval (Ghosh et al., 2024), DPG-Bench (Hu et al., 2024), and FID. Image Image (Editing). Our evaluation of image-to-image editing capabilities is conducted on the GEdit-Bench benchmark (Liu et al., 2025). Image Image (Segmentation). We evaluate the segmentation capability of our MLLM on the standard referring expression segmentation (RES) benchmarks RefCOCO/+ (Kazemzadeh et al., 2014) and RefCOCOg (Mao et al., 2016). Audio Text (Understanding). Our evaluation of the audio-to-text understanding capabilities mainly includes the following three tasks: 1) Fundamental audio understanding capabilities evaluated on broad range of public benchmarks, including public Chinese benchmarks like Aishell1 (Bu et al., 2017) and Wenetspeech (Zhang et al., 2022), and public English benchmarks like Librispeech (Zeyer et al., 2021) and Voxpopuli (Wang et al., 2021). And 2) audio question-answering capabilities evaluated on various benchmarks across five specific tasks, such as AlpacaEval and CommonEval from VoiceBench (Chen et al., 2024c) for open-ended QA tasks, and SD-QA for knowledge-based QA tasks. Finally 3) evaluates the models ability to utilize context on ContextASR-Bench(Wang et al., 2025). Text Audio (Generation). We incorporate text-to-audio generation capabilities to enable our MLLM with unified audio perception-generation abilities, which are evaluated on Seed-TTSEval (Anastassiou et al., 2024a). Video Text (Understanding). Our evaluation of the video-to-text understanding capabilities contains the following four benchmarks: MVBench (Li et al., 2024b), VideoMME (Fu et al., 2024a) and LongVideoBench (Wu et al., 2024)."
        },
        {
            "title": "B The prompt used in data generation",
            "content": "This section presents the prompt used for data generation in Sec. 3."
        },
        {
            "title": "Prompt for portrait preservation data",
            "content": "l s e . n : t g , n photo . a n , i , t image , e e t e images have f n s p n . o , outdoor , . ) and r e costumes Based on p o age and d e i o a l i Requirements : 1 . Must l : 2 . i y h 50 words . 3 . Change s e ( l companies , c s , parks , i 4 . t g must match age and gender , vary c t s . 5 . n o a must e e . Output y d r i n l , no i n Example : young woman with medium - g dark r y l up r under t b k z . She wears b headband , brow t a downturned and s s i with unshed r . The n p e r from n u o e park bench , s i r w e button - i t a s , homes , n e t even e t , be i and through h , t back n l t 3 l i e s i u t i l n s t . s s t l , with e and u d pathway h f - u background , g t c e a n i e amidst u ."
        },
        {
            "title": "Prompt for text generation data",
            "content": "D r a r g h l i e r n : 1 . Randomly e theme from { theme } , and e e t t , l , and numbers , with 3 -5 r e . 2 . Use your 100 r e . 3 . e e i l image c t based on t c e . 4 . e e theme not h { t g t ; l } and o p p t n , o , and o . e . Keep d r i under Chinese Output m : c t , Text : \" t \" , Font l , Font o , and Image o Examples : h image , t e gym n , Text : \" Power 3 . 0 \" , n y s broken t with c and t i f t , t o m l g with dark g i , t o s t d - h , background l s dumbbells , a l , and l i l w s . 25 Prompt for video multi-turn conversation data # Role : Expert Video Analyst You an e i a y . Your k o l t r d i and output r u JSON e o i g your l s . You must e s c t e m and e s b below . # Instructions : Analyze v o and e e n JSON e with f o g s . Your p e u ONLY be JSON e , markdown code c ( n . . . l t s , n d o h e . l d s l c e any e ) . Do not x , # JSON Fields Definition : 1 . \" e y \" : t t c e e main j o e e t r not among l e below , output ( i ) e ONLY ONE e y from f o g t v o . \" Others \" . * ** t 34 e i * * : [ \" Others \" , \" e o - TravelLog \" , \" e o - l f \" , \" e o - HouseTour \" , \" e o - c n \" , \" e o - AnimalPet \" , \" e o - Cooking \" , \" e o - h \" , \" e o - Workout\" , \" Education - t \" , \" Education - a \" , \" Education - t n l \" , \" Education - d a \" , \" Education - e \" , \" Education - Art \" , \" Education - i u i \" , \"TVShowTVSeries \" , \"TVShowNews\" , \"TVShowTalkShow \" , \"TVShowC b i \" , \"TVShowCommentaryProgram\" , \" Competition - t l \" , \" Competition - l c \" , \" Competition - k a \" , \" Competition - Snooker \" , \" Competition - Boxing \" , \" Competition - Car \" , \" VideoGames - Sandbox \" , \" VideoGames - OpenWorld\" , \" Documentary - Nature \" , \" Documentary - e \" , \" Documentary - t \" , \" Documentary - Kids \" , \" Movie - Comedy\" , \" Movie - Adventure \" ] 2 . t \" t \" : ( i ) n e but c t summary t i i g h , not e n 30 words . c e what happening , who n v , and main j . \" t _ r \" : ( e ) An e t i p n l prompt 3 . l e n s o . * ** r 1 -3 ( Low ) * * : The e n t n p o monologue with t from 1 1 0 . This a t i AI - r r a , n a o e r e l p t , e r t f u o q t s t a v a n based on e l . . * ** r 4 -7 ( Medium ) * * : The e has some e t e e but may k depth a t . d an extended , * ** r 8 -10 ( High ) * * : The e I v s s n and o e l o n t a , complex e y c , a v with t e i i ) . shows o s , an e t , complex n a t l o deep , e d v a n ( . . , can p a few s n but may not h o n , a , and i . h v a n . \" \" : 4 . * * * 1 * * : The e ( e ) l View . t from r - s e e v (FPV) , where camera s h e e . * * * 0 * * : The e s from i - s r t e e v . 26 shown , \" k _ p i \" : ( e ) An e r t o e y h primary k shown h d . 5 . c l t i * ** r 1 -3 ( Low ) * * : Simple , r y i ( . . , n a t , r water , t a ) . * ** r 4 -7 ( Medium ) * * : Tasks t e p s makeup l t ) . * ** r 8 -10 ( High ) * * : h complex , n c t main i y . a a ( . . , k a p i , e i IKEA n r , u some l , knowledge , e r a r y , l g r i , f i l programming , y a p i , c o , f t c i , r s n a ( . . , f i from 1 1 0 , r n g no c c s q e minimal l complex i p e ) . # Required Output Format : n { } \" e y \" : \" . . . \" , \" t \" : \" . . . \" , \" t _ r \" : \" \" : . . . , \" k _ p i \" : . . . , . . ."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Inclusion AI"
    ]
}