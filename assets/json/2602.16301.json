{
    "paper_title": "Multi-agent cooperation through in-context co-player inference",
    "authors": [
        "Marissa A. Weis",
        "Maciej WoÅ‚czyk",
        "Rajai Nasser",
        "Rif A. Saurous",
        "Blaise AgÃ¼era y Arcas",
        "JoÃ£o Sacramento",
        "Alexander Meulemans"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 1 ] . [ 1 1 0 3 6 1 . 2 0 6 2 : r 2026-02Multi-agent cooperation through in-context co-player inference Marissa A. Weis,1, Maciej WoÅ‚czyk,1, Rajai Nasser1, Rif A. Saurous1, Blaise AgÃ¼era Arcas1,2, JoÃ£o Sacramento1 and Alexander Meulemans1 1Google, Paradigms of Intelligence Team, 2Santa Fe Institute, Equal contribution Achieving cooperation among self-interested agents remains fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between learningaware agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce strict separation between naive learners updating on fast timescales and meta-learners observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior workwhere vulnerability to extortion drives mutual shapingemerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponents in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides scalable path to learning cooperative behaviors. 1. Introduction The development of foundation model agents is rapidly shifting the landscape of artificial intelligence from isolated systems to interacting autonomous agents (Aguera Arcas et al., 2026; Park et al., 2023; Xi et al., 2023). As these sequence-model-based agents are deployed in increasingly complex environments, they inevitably face multi-agent interactions where outcomes depend on interactions of multiple entities. Because these interactions frequently involve competing goals, ensuring that selfinterested agents robustly cooperate in mixed-motive settings remains an important open challenge, even as individual agent capabilities have grown significantly. Decentralized Multi-Agent Reinforcement Learning (MARL) addresses the problem of learning to interact with other agents while only having access to local observations. However, decentralized MARL is challenging due to two primary factors: equilibrium selection and non-stationarity of the environment (Hernandez-Leal et al., 2017; Shoham & Leyton-Brown, 2008). In general-sum games, many Nash equilibria may exist, and agents independently optimizing their own rewards frequently converge to suboptimal outcomes, such as mutual defection in social dilemmas (Claus & Boutilier, 1998; Foerster et al., 2018). Furthermore, from the perspective of single agent, the environment is non-stationary because other agents are simultaneously learning and adapting their policies (Hernandez-Leal et al., 2017). Since standard single-agent reinforcement learning (RL) algorithms assume stationarity, they often fail to learn effective policies in these decentralized settings (Claus & Boutilier, 1998; Foerster et al., 2018). To address this non-stationarity, co-player learning awareness enables agents to anticipate the learning dynamics of other agents and shape their co-players learning toward more beneficial equilibria (Agha2026 Google. All rights reserved Multi-agent cooperation through in-context co-player inference johari et al., 2024a,b; Balaguer et al., 2022; Cooijmans et al., 2023; Duque et al., 2024; Foerster et al., 2018; Khan et al., 2024; Lu et al., 2022; Meulemans et al., 2025a; Piche et al., 2025; Segura et al., 2025; Willi et al., 2022; Xie et al., 2021). These approaches generally fall into two categories. The first explicitly models the co-players learning update, estimating shaping gradient by differentiating through the opponents update step (Aghajohari et al., 2024a,b; Cooijmans et al., 2023; Duque et al., 2024; Foerster et al., 2018; Piche et al., 2025; Willi et al., 2022). However, this requires rigid assumptions about the opponents learning rule and creates inconsistencies if the opponent is also learning-aware. The second category implicitly learns to shape opponents by extending the RL time horizon to encompass multiple update steps of the co-player (Khan et al., 2024; Lu et al., 2022; Meulemans et al., 2025a; Segura et al., 2025). While effective, this requires separation of agents into naive learners (who update parameters frequently) and meta-learners (who update slowly), effectively treating the interaction as meta-learning problem (Bengio et al., 1990; Hochreiter et al., 2001; Schmidhuber, 1987). Meulemans et al. (2025a) describe three-step mechanism explaining why co-player learning awareness leads to the learning of cooperative behaviors among self-interested agents: 1. Extortion of naive learners: The optimal strategy against naive learner (an agent updating its policy to maximize rewards on fast timescale) is extortion (Press & Dyson, 2012). learningaware meta-agent shapes the interaction so the naive learner updates its policy towards more cooperation, allowing the meta-agent to exploit the resulting behavior. 2. Mutual extortion leads to cooperation: When two agents with such extortionate capabilities face each other, their attempts to shape the learning of one another result in both agents learning more cooperative strategies. 3. Heterogeneity is key: Consequently, cooperation emerges when agents are trained in mixed population of naive learners and learning-aware agents. Interactions with naive learners provide the gradient pressure to learn extortion (avoiding mutual defection), while interactions with learning-aware agents refine this into mutual cooperation. We argue that the complex mechanisms employed by current co-player learning-aware methods, such as explicit naive learners and meta learners, or differentiating through co-players learning updates, are unnecessary for learning cooperative behaviors. We hypothesize that training sequence model agents via decentralized MARL against diverse distribution of co-players naturally yields in-context best-response policies. These policies exhibit goal-directed adaptation through in-context learning within single episode. Crucially, we show that this acts as functional drop-in replacement for the naive learner parameter updates of prior work. Because in-context learning occurs on fast timescale within the episode, agents become susceptible to extortion by other learning agents using in-weight updates. Consequently, the cooperative gradient dynamics identified by Meulemans et al. (2025a) emerge: gradients incentivizing the extortion of in-context learners pull agents away from pure defection, while mutual extortion gradients drive them toward cooperation. Our contributions are as follows. We introduce decentralized MARL setup where sequence model agents are trained against mixed pool of diverse co-players and demonstrate that this training distribution induces strong in-context co-player inference capabilities and thereby the mutual extortion pressures leading to cooperation. We show that this setup leads to robust cooperation in the Iterated Prisoners Dilemma without the distinction between meta and inner trajectories, or assumptions about opponent learning rules. By bridging in-context learning and co-player learning-awareness, we provide scalable path toward cooperative multi-agent systems using standard sequence modeling and RL. We introduce new RL method that leverages self-supervised learning of predictive sequence models, which is well-suited to learn the in-context best-response policies required for the mixed pool 2 Multi-agent cooperation through in-context co-player inference training. We provide theoretical characterization of the training equilibrium of this method, and relate it to Nash equilibria and subjective embedded equilibria (Meulemans et al., 2025b). 2. Problem setup and methods Partially observable stochastic games. We formalize the multi-agent interaction as partially observable stochastic game (POSG; Kuhn, 1953) of ğ‘ agents. Each agent ğ‘– receives at each timestep ğ‘¡ ğ‘–, with Oğ‘–, ğ‘– and ğ‘– being an observation ğ‘œğ‘– finite sets. Policies are conditioned on the interaction history ğ‘¥ ğ‘– )}ğ‘¡ ğ‘˜=1. We denote the policy of agent ğ‘– as ğœ‹ğ‘– (ğ‘ğ‘– ğ‘¡ ğ‘–, and executes an action ğ‘ğ‘– ğ‘¡ Oğ‘– and reward ğ‘Ÿğ‘– ğ‘¡; ğœ™ğ‘–), parameterized by ğœ™ğ‘–. ğ‘¡ = {(ğ‘œğ‘– ğ‘¡ ğ‘¥ ğ‘– , ğ‘ğ‘– , ğ‘Ÿğ‘– ğ‘˜1 ğ‘˜ ğ‘˜ The iterated prisoners dilemma. We focus on the Iterated Prisoners Dilemma (IPD), canonical model for studying cooperation among self-interested agents (Axelrod & Hamilton, 1981; Rapoport, 1974). In each round ğ‘¡, two agents choose simultaneously to cooperate (C) or defect (D), i.e., ğ‘ğ‘– ğ‘¡ {C, D}, receiving payoffs as detailed in Tab. 1. This structure creates social dilemma: in single-shot game, mutual defection is the unique Nash equilibrium, even though mutual cooperation yields higher global and individual returns. While the infinitely iterated game allows for cooperative Nash equilibria, converging to these equilibria via decentralized reinforcement learning remains challenging (Claus & Boutilier, 1998; Foerster et al., 2018). For computational tractability, we approximate the infinite horizon with fixed horizon of ğ‘‡ = 100 steps, which is sufficient for the small-scale policy networks used in this work to approximate infinite-horizon behavior. Mixed pool training. To induce robust in-context inference capabilities, we train agents within mixed population rather than against single fixed opponent. The training pool consists of (i) Learning Agents which use sequence model policy that processes the full episode history ğ‘¥ ğ‘– ğ‘¡ and whose parameters are learned during training, and (ii) static Tabular Agents parameterized by 5-dimensional vector, defining the probability of cooperating in the initial state and in response to the four possible joint action outcomes of the previous turn (ğ‘ğ‘– ). During training, learning agent plays 50% of its episodes against another learning agent and 50% against tabular agent sampled uniformly from the parameter space. Crucially, agents do not receive agent identifiers; they must infer the nature and strategy of their opponent solely from the interaction history ğ‘¥ ğ‘– , ğ‘ğ‘– ğ‘¡1 ğ‘¡1 ğ‘¡. We investigate two learning algorithms for the learning agents in our pool: Independent A2C. We employ Advantage Actor-Critic (A2C) (Mnih et al., 2016) as standard decentralized model-free RL method. Each agent independently optimizes its policy parameters ğœ™ğ‘– to maximize its own expected return, treating the other agents as part of the environment. Predictive Policy Improvement (PPI). We introduce model-based algorithm that leverages sequence model predicting the joint sequence of actions, observations, and rewards, serving simultaneously as world model and policy prior. This method is variation of Maximum A-Posteriori Policy Optimization (Abdolmaleki et al., 2018, MPO), inspired by the MUPI framework for multi-agent learning (Meulemans et al., 2025b), and enables efficient learning of in-context inference mechanisms through self-supervised training. Each iteration consists of (i) gathering data with the improved policy and (ii) retraining the sequence model on the newly gathered data, similar to classical policy iteration. We define the improved policy ğœ‹ğ‘– (ğ‘ğ‘–ğ‘¥ ğ‘– ğ‘¡) as follows: ğœ‹ğ‘– (ğ‘ğ‘–ğ‘¥ ğ‘– ğ‘¡) ğ‘ğ‘– ğœ™ğ‘– (ğ‘ğ‘–ğ‘¥ ğ‘– ğ‘¡) exp (cid:16) ğ›½ Ë†ğ‘„ ğ‘ğ‘– (ğ‘¥ ğ‘– ğ‘¡, ğ‘ğ‘–) (cid:17) , (1) where ğ›½ is an inverse temperature hyperparameter. The action value Ë†ğ‘„ ğ‘(â„, ğ‘) is estimated via Monte Carlo rollouts performed within the sequence model ğ‘ğœ™. We deploy this improved policy ğœ‹ğ‘– (ğ‘ğ‘–ğ‘¥ ğ‘– ğ‘¡) in the games interacting with other agents, collecting new batch of trajectories. We end the iteration by 3 Multi-agent cooperation through in-context co-player inference Figure 1 Mixed training leads to robust cooperation. RL agents trained against mix of tabular policies and learning agents converge to cooperation (solid lines). Ablations: Agents trained purely against other learning agents (dotted lines) or with access to explicit co-player identifications (dashed lines) converge to defection, highlighting that in-context inference is critical factor for the learning of cooperative behaviors with standard decentralized MARL. Error bars indicate standard deviation across 10 random seeds. retraining the sequence model ğ‘ğ‘– ğœ™ğ‘– on all accumulated trajectory batches of the current and previous iterations, distilling the improved behavior of ğœ‹ğ‘– into the parameters ğœ™ğ‘–. We initialize the sequence model ğ‘ğœ™ by pretraining on interactions between randomly sampled tabular agents. Refer to App. for the implementation details, App. for theoretical derivation and motivation of PPI, and App. for theoretical analysis of the equilibrium behavior of PPI agents. 3. Results Our central hypothesis is that training the learning agents against diverse distribution of co-players necessitates the development of two distinct capabilities: (i) inferring the co-players policy from interaction history, and (ii) adapting to best response within single episode. We posit that this in-context best-response policy makes the agent vulnerable to extortion, reproducing the naive learner dynamics described by Meulemans et al. (2025a). This leads to learning pressures towards extortion policies, and subsequently, the mutual extortion between learning agents drives the agents toward cooperative policies. Interestingly, in this setup, the learning agents simultaneously occupy two roles traditionally separated in the literature: they are naive learners on the fast timescale (via in-context learning) and learning-aware agents on the slow timescale (via weight updates). In this section, we first demonstrate that mixed-pool training indeed leads to robust cooperation without explicit time-scale separations or meta-gradient machinery. We then dissect the underlying mechanism, showing that (1) mixed pool training induces in-context best-response policies, (2) these policies are vulnerable to extortion, and (3) mutual extortion pressures resolve into learning cooperative behaviors. 3.1. Mixed training induces robust cooperation As shown in Figure 1, both PPI and A2C agents trained in the mixed pool setup converge to cooperation in IPD. To verify this stems from the dynamics of in-context opponent inference, we perform two ablations: (1) Explicit Identification: We condition the policy on the opponents policy parameters (for tabular opponents) or identity flag (for other learning agents) at the start of the episode, removing the need for in-context opponent inference. (2) No mixed pool training: We train agents solely against single other learning agent (without the tabular agent pool or structured pretraining). 4 Multi-agent cooperation through in-context co-player inference Figure 2 AB: Emergence of in-context best response. Performance of PPI agents (trained against random tabular opponents) when evaluated against specific fixed strategies. The agents demonstrate in-context learning, identifying the opponent and converging to the best response within the episode. CD: Learning to extort in-context learners. Agents trained against Fixed In-Context Learner (an agent pre-trained in Step 1 to best-respond to tabular policies) learn to extort it. The RL agent achieves higher share of the reward by exploiting the in-context adaptation of its opponent. EF: From mutual extortion to cooperation. When two agents initialized with extortion policies (from Step 2) play against each other, their mutual attempts to extort their co-player result in the shaping of each others policy towards more cooperative behavior, both within episodes through in-context learning (F) and across episodes through in-weight learning (E). Error bars indicate standard deviation across 10 random seeds. Without diverse opponents, agents have no incentive to develop general-purpose in-context learning mechanisms. In both ablations, agents collapse to mutual defection (c.f. Fig. 1; dashed and dotted curves). This confirms that in-context learning mechanismsinduced by the necessity to identify diverse opponentsare critical factor enabling cooperative outcomes. Refer to App. A.4 for the ablation details. 3.2. Mechanism analysis: From in-context learning to cooperation We now deconstruct the learning of cooperative behavior into three distinct steps, validating the causal chain from diversity to in-context learning, to extortability, and finally to cooperation. Step 1: Diversity induces in-context best-response mechanisms. First, we verify that training against the tabular pool cultivates in-context learning. We evaluate PPI agent trained solely against the tabular agents pool. Figure 2B plots the agents performance against specific tabular policies over 5 Multi-agent cooperation through in-context co-player inference the course of an episode. The agent rapidly adapts to the best response for the specific opponent. This confirms the emergence of in-context best-response mechanisms that perform goal-directed adaptation on the fast timescale of the episode. Step 2: In-context learners are vulnerable to extortion. Next, we establish that such in-context best-response policies are susceptible to shaping by other co-players. We freeze the agent from Step 1, termed the \"Fixed In-Context Learner\" (Fixed-ICL), and train new PPI agent solely against it. The new agent learns to extort the Fixed-ICL policy (Fig. 2C&D) (Press & Dyson, 2012). By exploiting the Fixed-ICLs tendency to adapt, the new agent forces it into unfair cooperation, maximizing the new agents own reward at the expense of the Fixed-ICL. This confirms that goal-directed adaptation within the episode provides the necessary gradient signal for opponents to learn extortionate behaviors via weight updates. Step 3: Mutual extortion drives cooperation. We initialize two agents with the extortion policies learned in Step 2 and train them against each other. Within an episode, both extortion policies shape each others in-context learning dynamics into more cooperative behavior (Fig. 2F). This push towards more cooperation is then picked up by the parameter updates, further driving both policies towards cooperative behavior (Fig. 2E), mirroring the \"mutual shaping\" effect observed in explicit learning-aware methods (Lu et al., 2022; Meulemans et al., 2025a). Step 4: Synthesis in mixed populations. Mixed-pool training combines these dynamics by forcing agents to maintain in-context adaptation for tabular opponents, which renders them vulnerable to mutual extortion by other learners, ultimately driving the learning agents toward cooperation through mutual extortion (Sec. 3.1; Fig. 1 & Fig. 3). Figure 4 in Appendix B.2 shows similar results for A2C learning agents. 4. Conclusion In this work, we have demonstrated that the complex machinery of explicit co-player learningawarenesssuch as meta gradients or rigid timescale separationis not required to learn cooperative behaviors in general-sum games. Instead, we found that simply training agents against diverse distribution of co-players suffices to induce in-context best-response strategies. This in-context learning renders agents susceptible to shaping and consequently driving them toward cooperative behaviors through mutual extortion dynamics. Crucially, this result bridges the gap between multi-agent reinforcement learning and the training paradigms of modern foundation models. Since foundation models naturally exhibit in-context learning and are trained on diverse tasks and behaviors, our findings suggest scalable and computationally efficient path for the emergence of cooperative social behaviors using standard decentralized learning techniques."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Guillaume Lajoie, Angelika Steger and the Google Paradigms of Intelligence team for feedback and insightful discussions."
        },
        {
            "title": "References",
            "content": "Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018. 6 Multi-agent cooperation through in-context co-player inference Milad Aghajohari, Tim Cooijmans, Juan Agustin Duque, Shunichi Akatsuka, and Aaron Courville. Best response shaping. arXiv preprint arXiv:2404.06519, 2024a. Milad Aghajohari, Juan Agustin Duque, Tim Cooijmans, and Aaron Courville. Loqa: Learning with opponent q-learning awareness. arXiv preprint arXiv:2405.01035, 2024b. Blaise Aguera Arcas, Benjamin Bratton, and James Evans. The silicon interior, feb 2026. URL https://antikythera.substack.com/p/the-silicon-interior. Accessed: 2026-2-12. Robert Axelrod and William D. Hamilton. The evolution of cooperation. Science, 211(4489):1390 1396, March 1981. Jan Balaguer, Raphael Koster, Christopher Summerfield, and Andrea Tacchetti. The good shepherd: An oracle agent for mechanism design. arXiv preprint arXiv:2202.10135, 2022. Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning synaptic learning rule. Technical report, UniversitÃ© de MontrÃ©al, DÃ©partement dInformatique et de Recherche opÃ©rationnelle, 1990. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998(746-752):2, 1998. Tim Cooijmans, Milad Aghajohari, and Aaron Courville. Meta-value learning: general framework for learning with learning awareness. arXiv preprint arXiv:2307.08863, 2023. DeepMind, Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Laurent Sartran, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, MiloÅ¡ StanojeviÄ‡, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL http://github.com/google-deepmind. Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans, Razvan Ciuca, Tianyu Zhang, Gauthier Gidel, and Aaron Courville. Advantage alignment algorithms. arXiv preprint arXiv:2406.14662, 2024. Jakob Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In International Conference on Autonomous Agents and Multiagent Systems, 2018. Charles R. Harris, K. Jarrod Millman, StÃ©fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime FernÃ¡ndez del RÃ­o, Mark Wiebe, Pearu Peterson, Pierre GÃ©rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357362, 2020. Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: neural network library and ecosystem for JAX, 2024. URL http: //github.com/google/flax. Multi-agent cooperation through in-context co-player inference Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz De Cote. survey of learning in multiagent environments: Dealing with non-stationarity. arXiv preprint arXiv:1707.09183, 2017. Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent. In International Conference on Artificial Neural Networks, Lecture Notes in Computer Science. Springer, 2001. J. D. Hunter. Matplotlib: 2D graphics environment. Computing in Science & Engineering, 9(3): 9095, 2007. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the nineteenth international conference on machine learning, pp. 267274, 2002. Akbir Khan, Timon Willi, Newton Kwan, Andrea Tacchetti, Chris Lu, Edward Grefenstette, Tim RocktÃ¤schel, and Jakob N. Foerster. Scaling opponent shaping to high dimensional games. In International Conference on Autonomous Agents and Multiagent Systems, 2024. H. W. Kuhn. Extensive games and the problem of information. Princeton University Press, 1953. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Christopher Lu, Timon Willi, Christian Schroeder De Witt, and Jakob Foerster. Model-free opponent shaping. In International Conference on Machine Learning, 2022. Alexander Meulemans, Seijin Kobayashi, Johannes von Oswald, Nino Scherrer, Eric Elmoznino, Blake Richards, Guillaume Lajoie, JoÃ£o Sacramento, et al. Multi-agent cooperation through learningaware policy gradients. ICLR, 2025a. Alexander Meulemans, Rajai Nasser, Maciej WoÅ‚czyk, Marissa A. Weis, Seijin Kobayashi, Blake Richards, Guillaume Lajoie, Angelika Steger, Marcus Hutter, James Manyika, Rif A. Saurous, JoÃ£o Sacramento, and Blaise AgÃ¼era Arcas. Embedded universal predictive intelligence: coherent framework for multi-agent learning, 2025b. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, 2023. Juan Perdomo, Tijana Zrnic, Celestine Mendler-DÃ¼nner, and Moritz Hardt. Performative prediction. In International Conference on Machine Learning, pp. 75997609. PMLR, 2020. Dereck Piche, Mohammed Muqeeth, Milad Aghajohari, Juan Duque, Michael Noukhovitch, and Aaron Courville. Learning robust social strategies with large language models. arXiv preprint arXiv:2511.19405, 2025. William H. Press and Freeman J. Dyson. Iterated Prisoners Dilemma contains strategies that dominate any evolutionary opponent. Proceedings of the National Academy of Sciences, 109(26):1040910413, 2012. 8 Multi-agent cooperation through in-context co-player inference Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017. Anatol Rapoport. Prisoners dilemmarecollections and observations. In Game Theory as Theory of Conflict Resolution, pp. 1734. Springer, 1974. JÃ¼rgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. Diploma thesis, Institut fÃ¼r Informatik, Technische UniversitÃ¤t MÃ¼nchen, 1987. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. Marta Emili Garcia Segura, Stephen Hailes, and Mirco Musolesi. Opponent shaping in llm agents. arXiv preprint arXiv:2510.08255, 2025. Yoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Cambridge University Press, 2008. Michael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60): 3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021. Wes McKinney. Data Structures for Statistical Computing in Python. In StÃ©fan van der Walt and Jarrod Millman (eds.), Proceedings of the 9th Python in Science Conference, pp. 56 61, 2010. doi: 10.25080/Majora-92bf1922-00a. Timon Willi, Alistair Hp Letcher, Johannes Treutlein, and Jakob Foerster. COLA: consistent learning with opponent-learning awareness. In International Conference on Machine Learning, 2022. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864, 2023. Annie Xie, Dylan Losey, Ryan Tolsma, Chelsea Finn, and Dorsa Sadigh. Learning latent representations to influence multi-agent interaction. In Conference on Robot Learning, 2021. 9 Multi-agent cooperation through in-context co-player inference A. Additional details on methods A.1. Partially observable stochastic games We formalize the multi-agent interaction as partially observable stochastic game (POSG; Kuhn, 1953) defined by the tuple (I, S, A, ğ‘ƒğ‘¡, ğ‘ƒğ‘Ÿ, ğ‘ƒğ‘–, O, ğ‘ƒğ‘œ, ğ›¾, ğ‘‡). Here, = {1, . . . , ğ‘›} is the set of ğ‘› agents. At each time step ğ‘¡, the environment is in state ğ‘ ğ‘¡ S. Agents simultaneously select actions from the joint action space = ğ‘– ğ‘–, transitioning the environment according to ğ‘ƒğ‘¡ (ğ‘†ğ‘¡+1 ğ‘†ğ‘¡, ğ´ğ‘¡). The initial state is sampled from ğ‘ƒğ‘– (ğ‘ 0). Each agent ğ‘– receives reward ğ‘Ÿğ‘– ğ‘¡ from the joint factorized distribution ğ‘ƒğ‘Ÿ = ğ‘– ğ‘ƒğ‘– ğ‘¡ from the observation space = ğ‘– Oğ‘– via the distribution ğ‘ƒğ‘œ(ğ‘œğ‘¡ ğ‘ ğ‘¡, ğ‘ğ‘¡1). We denote the discount factor by ğ›¾ and the horizon by ğ‘‡. We use the superscript ğ‘– to denote variables specific to agent ğ‘–, and ğ‘– for the remaining agents. Policies are conditioned on the interaction history ğ‘¥ ğ‘– ğ‘¡; ğœ™ğ‘–), parameterized by ğœ™ğ‘–. ğ‘˜=1. We denote the policy of agent ğ‘– as ğœ‹ğ‘– (ğ‘ğ‘– ğ‘Ÿ (ğ‘Ÿğ‘– ğ‘ , ğ‘), and an observation ğ‘œğ‘– ğ‘¡ = {(ğ‘œğ‘– ğ‘¡ ğ‘¥ ğ‘– )}ğ‘¡ , ğ‘ğ‘– , ğ‘Ÿğ‘– ğ‘˜1 ğ‘˜ ğ‘˜ A.2. Environment Table 1 Single-round IPD payoff matrix Iterated Prisoners Dilemma (IPD) In each round both agents can output two possible actions: cooperate (ğ¶) and defect (ğ·). As such, the environment emits five possible observations: the initial observation ğ‘ 0 and four observations based on the actions the two players took in the previous round: (ğ¶, ğ¶), (ğ¶, ğ·), (ğ·, ğ¶), (ğ·, ğ·). The state ğ‘ ğ‘¡ is then comprised of all past observations ğ‘œğ‘¡. While the tabular agents are only conditioned on the latest observation ğ‘œğ‘¡, the PPI and A2C agents leverage the full history ğ‘¥ğ‘¡. Each game consists of 100 rounds. Each agent observes the state of the previous round from first person view, i.e., its own action is enumerated first. In every round, each agent receives reward following the payoff matrix in Tab. 1. (2, -1) Player Player 1 (-1, 2) (1, 1) (0, 0) C A.3. Agent implementations A.3.1. PPI agents Predictive Policy Improvement (PPI) agents, our practical approximation of embedded Bayesian agents (Meulemans et al., 2025b), combine learned sequence model with planning-based policy improvement mechanism. Sequence Model Architecture. The sequence model is Gated Recurrent Unit (GRU) with 128dimensional hidden state. Inputscomprising observations, actions, and rewardsare processed via modality-specific linear layers to project them into shared 32-dimensional embedding space; observations and actions are one-hot encoded prior to projection. These embeddings serve as inputs to the GRU, and we apply the Swish activation function (Ramachandran et al., 2017) on the output. Distinct linear output heads decode the hidden states to predict future tokens for each modality. Training Objectives. We train the sequence model iteratively for 30 phases. In each phase, the model parameters ğœ™ are re-initialized and trained on dataset of interaction histories = {ğ‘¥ (ğ‘›) }ğ‘ to minimize the next-token prediction loss: ğ‘›=1 10 Multi-agent cooperation through in-context co-player inference Algorithm 1 Predictive Policy Improvement Require: Initial sequence model ğ‘ğœ™0, reinforcement learning environment E, number of iterations ğ‘iter, number of training epochs ğ‘epochs, number of samples ğ‘samples, initial dataset D0 1: for ğ‘˜ = 1 to ğ‘iter do 2: Initialize weights ğœ™ğ‘˜ of ğ‘ğœ™ğ‘˜ randomly for ğ‘’ = 1 to ğ‘epochs do Step 1: Train sequence model Update parameters of ğ‘ğœ™ğ‘˜ using Dğ‘˜1 to minimize loss function ğ¿ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› in Eq. 2 end for Initialize empty dataset Rğ‘˜. for ğ‘Ÿ = 1 to ğ‘samples do Step 2: Collect game trajectories Reset environment E. Generate sequence of actions/observations using ğ‘ğœ™ğ‘˜ within E. Collect trajectory ğœğ‘Ÿ = (ğ‘œ0, ğ‘Ÿ0, ğ‘0, ğ‘œ1, ğ‘Ÿ1, ğ‘1, . . . ) from E. Add ğœğ‘Ÿ to Rğ‘˜. 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: end for Set Dğ‘˜ Dğ‘˜1 Rğ‘˜ for the next iterations training. 13: 14: end for ğ¿train = ğœ†obsğ¿obs + ğœ†actğ¿action + ğœ†rewğ¿reward , ğ¿obs = ğ¿reward = ğ¿action = 1 ğ‘ğ‘‡ 1 ğ‘ğ‘‡ 1 ğ‘ğ‘‡ ğ‘ ğ‘‡ ğ‘›=1 ğ‘ ğ‘¡=1 ğ‘‡ ğ‘›=1 ğ‘ ğ‘¡=1 ğ‘‡ ğ‘›=1 ğ‘¡=1 log ğ‘ğœ™(ğ‘œ(ğ‘›) ğ‘¡ ğ‘¥ (ğ‘›) ğ‘¡ ) , log ğ‘ğœ™(ğ‘Ÿ (ğ‘›) ğ‘¡ ğ‘¥ (ğ‘›) ğ‘¡1 , ğ‘œ(ğ‘›) ğ‘¡ ) , log ğ‘ğœ™(ğ‘(ğ‘›) ğ‘¡ ğ‘¥ (ğ‘›) ğ‘¡1 , ğ‘œ(ğ‘›) ğ‘¡ , ğ‘Ÿ (ğ‘›) ğ‘¡ ) . (2) (3) (4) (5) comprises of the interaction histories from all previous and current phases. This is common strategy in performative prediction (Perdomo et al., 2020) to ensure more stable training of the prediction model. ğ‘¥ğ‘¡) and ğ‘ğœ™(ğ‘œğ‘¡ We model ğ‘ğœ™(ğ‘ğ‘¡ ğ‘¥<ğ‘¡, ğ‘ğ‘¡1) using categorical distribution, yielding standard categorical cross-entropy loss and we model ğ‘ğœ™(ğ‘Ÿğ‘¡ ğ‘¥<ğ‘¡, ğ‘ğ‘¡1, ğ‘œğ‘¡) with normal distribution with fixed variance, yielding the mean-square error loss (ğ‘Ÿ Ë†ğ‘Ÿ)2. In each phase, we sample 20 000 trajectories, which are concatenated with samples from previous phases for joint training of the sequence model. Optimization is performed using AdamW (Loshchilov & Hutter, 2017) (learning rate 104, weight decay 102, ğ›½1 = 0.9, ğ›½2 = 0.98) for 10 epochs with batch size of 256. Gradients are clipped at norm of 1.0. Pre-training The sequence model is pretrained on an initial dataset D0 of 200 000 sample trajectories of two random tabular agents playing IPD against each other for 100 rounds using the same training hyperparameters as outlined above. Inference During deployment, the agent estimates values by performing Monte Carlo roll-outs for 15 rounds into the future using the learned sequence model as simulator. The final action selection Multi-agent cooperation through in-context co-player inference follows policy ğœ‹(ğ‘ğ‘¥ğ‘¡) that re-weights the models prior probability ğ‘(ğ‘ğ‘¥ğ‘¡; ğœ™) by the estimated value Ë†ğ‘„ ğ‘(ğ‘¥ğ‘¡, ğ‘) derived from the roll-outs: ğœ‹(ğ‘ğ‘¥ğ‘¡) = 1 ğ‘ ğ‘(ğ‘ğ‘¥ğ‘¡; ğœ™) exp( ğ›½ Ë†ğ‘„ ğ‘(ğ‘¥ğ‘¡, ğ‘)) . (6) We use ğ›½ = 0.01 for all experiments. A.3.2. Model-free agent Architecture We implement an Advantage Actor-Critic (A2C) agent (Mnih et al., 2016) using GRU-based sequence model with the same configuration as for the PPI agents. The GRU takes as input the history of observations of previous rounds and outputs the next action. The GRU is further augmented with linear output head to estimate the value function ğ‘‰ (ğ‘¥). During training, we estimate the advantage ğ´(ğ‘¥ğ‘¡, ğ‘ğ‘¡) using bootstrapped temporal-difference errors: ğ´(ğ‘¥ğ‘¡, ğ‘ğ‘¡) = ğ‘Ÿğ‘¡ + ğ›¾ğ‘‰ (ğ‘¥ğ‘¡+1) ğ‘‰ (ğ‘¥ğ‘¡) . The model parameters are updated to minimize the combined policy gradient and value estimation loss: ğ¿ = ğ‘‡ (cid:18) ğ‘¡=1 log ğœ‹(ğ‘ğ‘¡ ğ‘¥ğ‘¡) ğ´(ğ‘¥ğ‘¡, ğ‘ğ‘¡) + ğ‘ğ‘£ (ğ‘Ÿğ‘¡ + ğ›¾ğ‘‰ (ğ‘¥ğ‘¡+1) ğ‘‰ (ğ‘¥ğ‘¡))2 (cid:19) ğœ‹(ğ‘ğ‘– ğ‘¡ ğ‘¥ğ‘¡) log ğœ‹(ğ‘ğ‘– ğ‘¡ ğ‘¥ğ‘¡) , + ğ‘ğ‘’ where ğ‘ğ‘£, ğ‘ğ‘’ are hyperparameters representing, correspondingly, the value function and entropy training coefficients. ğ‘– Training To get comparable results, we follow the A2C training protocol from Meulemans et al. (2025a) including the value function estimation, Generalized Advantage Estimation (Schulman et al., 2015), advantage normalization and reward scaling. See Appendix of Meulemans et al. (2025a) for details. For each experiment, we perform hyperparameter search over the learning rate, GAE lambda, advantage normalization, reward scaling and entropy regularization. We report the hyperparameters corresponding to the best-performing setting in Table 2. A.3.3. Tabular agents Tabular agents employ memory-1 policy defined by five parameters: the cooperation probabilities conditional on the previous outcome (ğ‘ğ‘, ğ‘ğ‘‘, ğ‘‘ğ‘, ğ‘‘ğ‘‘) and the initial state (ğ‘ 0). Each parameter is initialized from uniform distribution (0, 1). A.4. Ablations A.4.1. Policy conditioning For the Opponent ID ablation  (Fig. 1)  , we prepend the observation sequence xğ‘¡ with conditioning vector representing the opponents identity. For tabular agents, is defined as the flattened vector 12 Multi-agent cooperation through in-context co-player inference Table 2 A2C hyperparameters RL Hyperparameter Step 1 Step 2 Step 3 Step 4 advantages_normalization True batch size 2048 0.2 reward_rescaling 0.99 value_discount (ğ›¾) 0.99 td_lambda (ğœ†td) 0.99 gae_lambda (ğœ†gae) 0.5 value_coefficient 0.001 entropy_reg optimizer Adam 0.00001 0.00001 0.00001 0.00001 adam_epsilon 0.005 learning_rate 1.0 max_grad_norm True 4096 0.02 0.99 1.0 1.0 0.5 0.01 Adam True 4096 0.02 0.99 0.95 0.95 0.5 0.001 Adam False 2048 0.05 0.99 1.0 1.0 0.5 0.001 Adam 0.0005 1.0 0.001 1.0 0.005 1.0 of log-probabilities across all possible observations ğ‘œ O: = (cid:0) log ğœ‹(ğ‘ğ‘œ)(cid:1) ğ‘œ O,ğ‘ where = {(ğ¶, ğ¶), (ğ¶, ğ·), (ğ·, ğ¶), (ğ·, ğ·), Start}. For A2C and PPI agents, = 0. A.4.2. No mixed pool training For the No Tabular Opponents ablation  (Fig. 1)  , we remove the tabular opponents from the mixed agent pool for both PPI and A2C experiments. For PPI, we additionally change the pretraining data distribution ğ·0 to not include tabular agents but instead consist of purely random action sequences with the corresponding rewards. B. Additional results B.1. In-episode trajectories for mixed pool training Figure 3 shows the performance of PPI and A2C within single episode during early training in the mixed pool setting (c.f. Sec. 3.1), i.e., for phase = 8 for PPI and training iteration = 70ğ‘˜ for A2C, showing the emergence of in-context opponent inference and an initial gradient towards cooperation against other learning agents. B.2. Additional results on A2C Figure 4 shows A2C-based results, corresponding to the PPI results presented in Figure 2 in the main text. In Step 1, we observe that an A2C agent learns to implement best response against variety of tabular agents, same as for PPI. In Step 2, however, we observe that the newly trained A2C agent manages to get higher reward playing against the Fixed-ICL baseline than the PPI agent (correspondingly, 1.25 vs. 0.9). This can either be caused by (i) the PPI Fixed-ICL policy being harder to exploit or (ii) A2C finding better exploiter policy. The irregular shape of the exploitation dynamics in Figure 4D suggests that the A2C exploiter agent learned complex adversarial strategy against the A2C Fixed-ICL policy. In contrast, the PPI extortion policy of Figure 2D seems to be more regular extortion policy. Finally, in Step 3, the A2C agents initially move towards cooperation but due to training instability they might still turn back to defection depending on the seed. 13 Multi-agent cooperation through in-context co-player inference Figure 3 Emergence of best-response in mixed training. We plot within-episode performance of models trained in Figure 1 before convergence. We observe that both A2C and PPI try to extort their counterpart at the beginning of the episode which subsequently leads to increased levels of cooperation. At the same time, identifying the opponent as non-tit-for-tat-like tabular policy leads to high defection ratio. Error bars indicate standard deviation across 10 random seeds. 14 Multi-agent cooperation through in-context co-player inference Figure 4 A-B: Emergence of in-context best response Performance of A2C trained against random tabular opponents and evaluated after convergence on set of specific static policies. We denote the final agent as Fixed In-Context Learner. C-D: Learning to extort in-context learners. Performance of randomly initialized A2C agent against the Fixed In-Context Learner. E-F: From mutual extortion to cooperation. Two A2C extortion agents initially converge to cooperation when playing against each other, but with time they might collapse to mutual defection depending on the random seed. Error bars correspond to standard deviation over 5 random initializations. 15 Multi-agent cooperation through in-context co-player inference C. Derivation of Predictive Policy Improvement (PPI) In this section, we provide formal derivation of the Predictive Policy Improvement (PPI) algorithm. PPI is inspired by the theoretically grounded MUPI framework (Meulemans et al., 2025b), and is closely related to Maximum Posteriori Policy Optimization (MPO; Abdolmaleki et al., 2018). PPI departs from standard MPO by replacing the separate value function and self-model of MPO with single sequence model trained in self-supervised fashion to predict actions, observations and rewards. This model serves simultaneously as world model and policy prior, leveraging the generative capabilities of sequence models for value estimation and policy representation. C.1. Objective: The Variational Lower Bound (cid:3). We consider an agent optimizing its policy ğœ‹ to maximize the expected return ğ‘‰ (ğœ‹) = ğ”¼ğœâ„™ğœ‹ To avoid notational clutter, we omit the agent-specific superscripts, as this derivation applies equally to the single-agent setting. We introduce parameterized sequence model ğ‘ğœ™(ğ‘ ğ‘¥ğ‘¡) which acts as behavioral prior or self-model over the interaction history ğ‘¥ğ‘¡. We define surrogate objective ğ½ by penalizing the KL-divergence between the behavioral policy ğœ‹ and the prior ğ‘ğœ™: (cid:2)(cid:205)ğ‘‡ ğ›¾ğ‘¡ğ‘Ÿğ‘¡ ğ‘¡=0 ğ½ (ğœ‹, ğœ™) = ğ”¼ğœâ„™ğœ‹ (cid:34) ğ‘‡ ğ‘¡=0 ğ›¾ğ‘¡ğ‘Ÿğ‘¡ ğ›¼KL (cid:0)ğœ‹( ğ‘¥ğ‘¡) ğ‘ğœ™( ğ‘¥ğ‘¡)(cid:1) (cid:35) . (7) Since KL() 0, ğ½ (ğœ‹, ğœ™) is strict lower bound on ğ‘‰ (ğœ‹), with equality at ğœ‹ = ğ‘ğœ™. We optimize this bound via coordinate ascent on ğœ‹ (the non-parametric policy) and ğœ™ (the parametric sequence model). C.2. Step 1: Non-parametric Policy Improvement w.r.t. ğœ‹ Optimizing ğ½ (ğœ‹, ğœ™) w.r.t. ğœ‹ for fixed ğœ™ is full-fledged optimal control problem, which generally lacks an analytical solution and is therefore ill-suited for direct non-parametric policy improvement. Instead, we use first-order approximation of ğ½ (ğœ‹, ğœ™ğ‘˜) around ğœ‹ = ğ‘ğœ™ğ‘˜, where ğ‘ğœ™ğ‘˜ is the self-model trained on the dataset gathered by deploying the previous policy ğœ‹ğ‘˜1: ğ‘‡ ğ½ (ğœ‹, ğœ™ğ‘˜) = ğ”¼ğ‘¥ğ‘¡â„™ğ‘ğœ™ğ‘˜ (cid:105) ğ‘¡=1 ğ‘‰ ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡) (cid:104) ğ”¼ğ‘ğœ‹( ğ‘¥ğ‘¡ ) [ğ‘„ ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡, ğ‘)] ğ›¼KL (cid:0)ğœ‹( ğ‘¥ğ‘¡) ğ‘ğœ™ğ‘˜ ( ğ‘¥ğ‘¡)(cid:1) (8) + ğ½ ( ğ‘ğœ™ğ‘˜ , ğœ™ğ‘˜). Note that here, the Q-value is equal to the unregularized value ğ‘„ ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡, ğ‘) = ğ”¼ğœ>ğ‘¡â„™ğ‘ğœ™ğ‘˜ as all KL terms evaluate to zero under the prior. The crucial difference between ğ½ and ğ½ is that the expectation over histories in ğ½ does not depend on the policy ğœ‹ being optimized, which permits closed-form solution for arg maxğœ‹ ğ½. ( ğ‘¥ğ‘¡,ğ‘) (cid:2)(cid:205)ğ‘‡ ğ‘¡=ğ‘¡ ğ›¾ğ‘¡ ğ‘¡ğ‘Ÿğ‘¡ (cid:3), We proceed to show that ğ½ is indeed first-order approximation to ğ½ around ğ‘ğœ™ğ‘˜ via the following two lemmas. Lemma C.1. ğ½ ( ğ‘ğœ™ğ‘˜ , ğœ™ğ‘˜) = ğ½ ( ğ‘ğœ™ğ‘˜ , ğœ™ğ‘˜) Proof. It is easy to see that the terms of equation 8 inside the expectation cancel out when ğœ‹ = ğ‘ğœ™ğ‘˜, leaving only ğ½ ( ğ‘ğœ™ğ‘˜ , ğœ™ğ‘˜). 16 Multi-agent cooperation through in-context co-player inference Lemma C.2. ğœ‹ ğ½ (ğœ‹, ğœ™ğ‘˜)ğœ‹=ğ‘ğœ™ğ‘˜ = ğœ‹ ğ½ (ğœ‹, ğœ™ğ‘˜)ğœ‹=ğ‘ğœ™ğ‘˜ Proof. We analyze the functional derivatives of both objectives with respect to the policy distribution ğœ‹(ğ‘ ğ‘¥ğ‘¡) evaluated at specific history ğ‘¥ğ‘¡ and action ğ‘. First, consider the surrogate objective ğ½ (ğœ‹, ğœ™ğ‘˜). Because the expectation over histories is fixed to the prior distribution â„™ğ‘ğœ™ğ‘˜ and thus does not depend on the optimization variable ğœ‹, the functional derivative is straightforward. Applying the product rule to the logarithmic term, the functional derivative with respect to the local action probability ğœ‹(ğ‘ ğ‘¥ğ‘¡) is: ğ›¿ ğ½ (ğœ‹, ğœ™ğ‘˜) ğ›¿ğœ‹(ğ‘ ğ‘¥ğ‘¡) (cid:18) = â„™ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡) ğ‘„ ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡, ğ‘) ğ›¼ log ğœ‹(ğ‘ ğ‘¥ğ‘¡) ğ‘ğœ™ğ‘˜ (ğ‘ ğ‘¥ğ‘¡) (cid:19) . ğ›¼ (9) Evaluating this derivative at the prior ğœ‹ = ğ‘ğœ™ğ‘˜, the logarithmic term vanishes (since log 1 = 0), yielding: ğ›¿ ğ½ (ğœ‹, ğœ™ğ‘˜) ğ›¿ğœ‹(ğ‘ ğ‘¥ğ‘¡) (cid:12) (cid:12) (cid:12) (cid:12)ğœ‹=ğ‘ğœ™ğ‘˜ = â„™ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡) (ğ‘„ ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡, ğ‘) ğ›¼) . (10) Next, differentiating the true objective ğ½ (ğœ‹, ğœ™ğ‘˜) is more involved because ğœ‹ dictates the history visitation distribution â„™ğœ‹(ğ‘¥ğ‘¡). We define the regularized Q-function, ğ‘„ğœ‹ reg(ğ‘¥ğ‘¡, ğ‘), which captures the expected return including all future KL penalties, but excluding the immediate penalty at time ğ‘¡: ğ‘„ğœ‹ reg(ğ‘¥ğ‘¡, ğ‘) = ğ”¼ğœ>ğ‘¡â„™ğœ‹ ( ğ‘¥ğ‘¡,ğ‘) (cid:34) ğ‘‡ ğ‘˜=ğ‘¡ ğ›¾ğ‘˜ğ‘¡ ğ‘…ğ‘˜ ğ›¼ ğ‘‡ ğ‘˜=ğ‘¡+ ğ›¾ğ‘˜ğ‘¡KL (cid:0)ğœ‹( ğ‘¥ğ‘˜) ğ‘ğœ™ğ‘˜ ( ğ‘¥ğ‘˜)(cid:1) (cid:35) . (11) Using this, the value of specific history is: ğ‘‰ ğœ‹ reg(ğ‘¥ğ‘¡) = (cid:18) ğœ‹(ğ‘ ğ‘¥ğ‘¡) ğ‘ ğ‘„ğœ‹ reg(ğ‘¥ğ‘¡, ğ‘) ğ›¼ log ğœ‹(ğ‘ ğ‘¥ğ‘¡) ğ‘ğœ™ğ‘˜ (ğ‘ ğ‘¥ğ‘¡) (cid:19) . (12) To find the functional derivative of the global objective ğ½ with respect to the local policy ğœ‹(ğ‘ ğ‘¥ğ‘¡), we apply the continuous extension of the Performance Difference Lemma (Kakade & Langford, 2002). This theorem establishes that the indirect effect of the policy on the visitation distribution â„™ğœ‹(ğ‘¥ğ‘¡) yields net zero contribution to the gradient. Consequently, the derivative isolates the state visitation probability multiplied by the local derivative of the value function: ğ›¿ğ½ (ğœ‹, ğœ™ğ‘˜) ğ›¿ğœ‹(ğ‘ ğ‘¥ğ‘¡) = â„™ğœ‹(ğ‘¥ğ‘¡) ğ‘‰ ğœ‹(ğ‘¥ğ‘¡) ğœ‹(ğ‘ ğ‘¥ğ‘¡) . Taking the partial derivative of ğ‘‰ ğœ‹(ğ‘¥ğ‘¡) yields: ğ›¿ğ½ (ğœ‹, ğœ™ğ‘˜) ğ›¿ğœ‹(ğ‘ ğ‘¥ğ‘¡) (cid:18) = â„™ğœ‹(ğ‘¥ğ‘¡) ğ‘„ğœ‹ reg(ğ‘¥ğ‘¡, ğ‘) ğ›¼ log ğœ‹(ğ‘ ğ‘¥ğ‘¡) ğ‘ğœ™ğ‘˜ (ğ‘ ğ‘¥ğ‘¡) (cid:19) . ğ›¼ (13) (14) Finally, we evaluate this true derivative at the prior policy ğœ‹ = ğ‘ğœ™ğ‘˜. Three simplifications occur: The history visitation distribution matches the prior: â„™ğœ‹(ğ‘¥ğ‘¡) = â„™ğ‘ğœ™ğ‘˜ The immediate KL penalty evaluates to zero: log 1 = 0. Because the policy perfectly matches the prior at all future timesteps, all future KL penalties evaluate to zero. Consequently, the regularized Q-function smoothly collapses to the unregularized Q-function of the prior: ğ‘„ğœ‹ reg(ğ‘¥ğ‘¡, ğ‘) = ğ‘„ ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡, ğ‘). (ğ‘¥ğ‘¡). 17 Multi-agent cooperation through in-context co-player inference Applying these simplifications yields: ğ›¿ğ½ (ğœ‹, ğœ™ğ‘˜) ğ›¿ğœ‹(ğ‘ ğ‘¥ğ‘¡) (cid:12) (cid:12) (cid:12) (cid:12)ğœ‹=ğ‘ğœ™ğ‘˜ = â„™ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡) (ğ‘„ ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡, ğ‘) ğ›¼) . (15) Since the functional derivatives of both ğ½ and ğ½ evaluated at ğœ‹ = ğ‘ğœ™ğ‘˜ perfectly coincide, it follows that ğœ‹ ğ½ (ğœ‹, ğœ™ğ‘˜)ğœ‹=ğ‘ğœ™ğ‘˜ = ğœ‹ ğ½ (ğœ‹, ğœ™ğ‘˜)ğœ‹=ğ‘ğœ™ğ‘˜ , concluding the proof. Optimizing ğ½. Optimizing ğ½ (ğœ‹, ğœ™ğ‘˜) w.r.t. ğœ‹ for fixed ğœ™ğ‘˜ has the well-known Boltzmann policy as solution: ğœ‹(ğ‘ ğ‘¥ğ‘¡) = exp ( ğ›½ğ‘„ ğ‘ğœ™ğ‘˜ (ğ‘¥ğ‘¡, ğ‘)) , (16) ğ‘ğœ™ğ‘˜ (ğ‘ ğ‘¥ğ‘¡) ğ‘(ğ‘¥ğ‘¡) with the inverse temperature ğ›½ = 1 around ğ‘ğœ™ğ‘˜ where ğ½ is sufficiently accurate approximation of ğ½. ğ›¼ . We treat ğ›½ as fixed hyperparameter defining trust region C.3. Comparison with MPO and Sequence-Model Value Estimation While PPI shares the coordinate-ascent structure of MPO, it differs in how Q-values are obtained and whether ğœ‹ or ğ‘ğœ™ğ‘˜ is deployed as behavioral policy to gather trajectories. In standard MPO, ğ‘„(ğ‘ , ğ‘) is typically represented by separate neural network (a critic) trained via temporal difference (TD) learning on the agents own experience, relying on the Markov property to condition on single state ğ‘  instead of the full history. In contrast, PPI leverages the sequence model as world model. The value Ë†ğ‘„ ğ‘(ğ‘¥ğ‘¡, ğ‘) is estimated via Monte Carlo rollouts performed within the sequence model itself. By sampling future trajectories ğœ>ğ‘¡ from ğ‘ğœ™( ğ‘¥ğ‘¡, ğ‘), the agent evaluates the expected return of an action based on its internal representation of both the environment dynamics and the co-players predicted responses. This allows PPI to benefit from the high-capacity temporal dependencies captured by the sequence model. Note that PPI is easily extendable toward learning an explicit Q-value function conditioned on full histories to amortize the cost of the MC rollouts and reduce variance. D. Theoretical Analysis of the Equilibrium Behavior of PPI Agents In this section, we analyze the theoretical properties of the Predictive Policy Improvement (PPI) algorithm. Unlike standard reinforcement learning, where agents optimize policy against fixed (or stationarily adapting) environment, PPI agents operate in performative loop: the agents predictive model determines its policy, which determines the data distribution, which in turn is used to update the predictive model. This is closely related to the concept of performative prediction (Perdomo et al., 2020), where the predictions of model can affect the distribution of the very data it is trying to predict (with traffic prediction models being prominent example). We formalize this interaction and define the concept of Predictive Equilibrium (PE). We show that while global pure-strategy equilibrium is not guaranteed to exist due to the non-convex nature of deep neural networks, local predictive equilibrium (consistent with gradient-based optimization) and mixed predictive equilibrium (randomized strategies) are guaranteed to exist under standard assumptions. Finally, we show that in the limit of perfect world model, predictive equilibrium corresponds to subjective embedded equilibrium (Meulemans et al., 2025b). Multi-agent cooperation through in-context co-player inference D.1. Formal Setup Consider game with ğ‘› agents. Each agent ğ‘– maintains predictive sequence model ğ‘ğœƒğ‘– (â„ğ‘–) parameterized by ğœƒğ‘– Î˜ğ‘–, where â„ğ‘– is history ğ‘¥ ğ‘– ğ‘¡ of arbitrary length ğ‘¡, and Î˜ğ‘– is compact metric space (e.g., bounded subset of â„ğ‘‘). The Performative Loop. The PPI algorithm (Algorithm 1) induces closed-loop dependency between parameters and data: 1. Model induces Policy: The agent derives policy ğœ‹ğœƒğ‘– from its model ğ‘ğœƒğ‘– via the policy improvement operator, defined in Eq. 6 (the Boltzmann policy over Q-values estimated via rollout). 2. Policy induces Data: When all agents interact using policies ğ…ğœ½ = {ğœ‹ğœƒ , . . . , ğœ‹ğœƒğ‘ }, they induce joint distribution over interaction histories â„. We denote the true probability distribution of histories generated by the current joint configuration ğœ½ as â„™(; ğœ½). 3. Data induces Model: The agent updates ğœƒğ‘– to minimize the Kullback-Leibler (KL) divergence between the observed distribution â„™(; ğœ½) and its model ğ‘ğœƒğ‘–. D.2. Predictive Equilibria stable point of this training loop is configuration where the model optimally predicts the data generated by the policy derived from that very model. Definition D.1 (Global Predictive Equilibrium). joint configuration ğœ½ = (ğœƒ 1 Predictive Equilibrium if, for all agents ğ‘–: , . . . , ğœƒ ğ‘›) is Global ğœƒ ğ‘– arg min ğœƒğ‘– Î˜ğ‘– KL (cid:0)â„™(â„ğ‘–; ğœ½) ğ‘ğœƒğ‘– (â„ğ‘–)(cid:1) . (17) Intuitively, at equilibrium, no agent can improve their world model given the behavior induced by the current joint models. Challenges. Proving the existence of global PE is difficult because the map ğœƒ ğœ‹ğœƒ is complex and the resulting objective is generally non-convex. The argmin set may change discontinuously (mode hopping), preventing the application of standard fixed-point theorems. To address this, we define two relaxed solution concepts: Local PE (relevant to gradient descent) and Mixed PE. D.2.1. Local Predictive Equilibrium In practice, PPI agents update parameters via gradient descent. They do not find global minima but rather stationary points. Crucially, the update assumes that the data distribution is fixed (which can be interpreted as stop-gradient on the environment dynamics). Definition D.2 (Local Predictive Equilibrium). Let Î˜ğ‘– â„ğ‘‘ğ‘– be compact, convex parameter space for ğ‘›) (cid:206)ğ‘– Î˜ğ‘– is Local Predictive Equilibrium if, each agent ğ‘– I. joint configuration ğœ½ = (ğœƒ , . . . , ğœƒ 1 for all agents ğ‘– I, the configuration satisfies the first-order stationarity condition with respect to their local loss, assuming the data generating process is fixed. Formally: (cid:28) ğœƒğ‘–KL (cid:0)â„™(â„ğ‘–; ğœ½) ğ‘ğœƒğ‘– (â„ğ‘–)(cid:1) (cid:12) (cid:12) (cid:12)ğœƒğ‘–=ğœƒ ğ‘– (cid:29) , ğœ™ğ‘– ğœƒ ğ‘– 0 , ğœ™ğ‘– Î˜ğ‘– , ğ‘– , (18) where , denotes the standard inner product. 19 Multi-agent cooperation through in-context co-player inference This variational inequality definition corresponds precisely to the convergence criteria of projected gradient descent in the PPI algorithm. If ğœƒ ğ‘– lies in the interior of Î˜ğ‘–, Eq. 18 implies the standard condition ğœƒğ‘–KL (cid:0)â„™(â„ğ‘–; ğœ½) ğ‘ğœƒğ‘– (â„ğ‘–)(cid:1) (cid:12) (cid:12) (cid:12)ğœƒğ‘–=ğœƒ ğ‘– = 0 , ğ‘– . (19) Theorem D.3 (Existence of Local Predictive Equilibrium). Assume Î˜ğ‘– is compact, convex subset of â„ğ‘‘ğ‘–. Assume the mapping from parameters ğœ½ to the local gradient of the loss ğºğ‘– (ğœ½) = ğœ—KL (â„™(; ğœ½) ğ‘ğœ—) (cid:12) (cid:12)ğœ—=ğœƒğ‘– is continuous. Then, there exists at least one Local Predictive Equilibrium. Proof. We analyze the existence of Local Predictive Equilibrium by framing it as fixed-point problem. Let Lğ‘– (ğœ½, ğœ“) = KL (cid:0)â„™(â„ğ‘–; ğœ½) ğ‘ğœ“(â„ğ‘–)(cid:1) denote the loss function for agent ğ‘–, where the first argument ğœ½ determines the data distribution (fixed locally) and the second argument ğœ“ is the parameter being optimized. Define the local gradient field ğº : Î˜ â„ğ· (where ğ· = (cid:205)ğ‘– ğ‘‘ğ‘–) as the concatenation of the individual gradients: (cid:16) ğº(ğœ½) = ğœ“L1(ğœ½, ğœ“)(cid:12) (cid:12)ğœ“=ğœƒ1 , . . . , ğœ“Lğ‘›(ğœ½, ğœ“)(cid:12) (cid:12)ğœ“=ğœƒğ‘› (cid:17) . Local Predictive Equilibrium is characterized by the variational inequality ğº(ğœ½), ğœ™ ğœ½ 0 for all ğœ™ Î˜, where Î˜ = (cid:206)ğ‘– Î˜ğ‘–. We assume the parameter space Î˜ is compact, convex subset of Euclidean space, and that the gradient field ğº(ğœ½) is continuous. The continuity of ğº follows naturally from the smoothness assumptions on the predictive models ğ‘ğœƒ and the induced policy distributions. Consider the map ğ‘‡ : Î˜ Î˜ defined by projected gradient step: where ğœ‚ > 0 is scalar step size and ProjÎ˜ is the Euclidean projection onto the set Î˜. ğ‘‡ (ğœ½) = ProjÎ˜ (ğœ½ ğœ‚ğº(ğœ½)) , 1. Compactness and Convexity: By assumption, Î˜ is compact and convex set. 2. Continuity: The map ğº is continuous by assumption. The projection operator ProjÎ˜ is nonexpansive and thus continuous. Therefore, the composition ğ‘‡ is continuous map from Î˜ to itself. By Brouwers Fixed Point Theorem, there exists point ğœ½ Î˜ such that ğ‘‡ (ğœ½) = ğœ½. This fixed point condition implies: ğœ½ = ProjÎ˜ (ğœ½ ğœ‚ğº(ğœ½)) . By the standard property of the Euclidean projection onto closed convex set, this equation holds if and only if: (ğœ½ ğœ‚ğº(ğœ½)) ğœ½, ğœ™ ğœ½ 0 , ğœ™ Î˜ . Simplifying the terms inside the inner product, we obtain: ğœ‚ğº(ğœ½), ğœ™ ğœ½ 0 = ğº(ğœ½), ğœ™ ğœ½ 0 , ğœ™ Î˜ . This inequality is precisely the first-order stationarity condition defined in Eq. 18, generalized to the joint parameter space Î˜. Therefore, the fixed point ğœ½ constitutes Local Predictive Equilibrium, rigorously accommodating both interior stationary points and boundary solutions. 20 Multi-agent cooperation through in-context co-player inference D.2.2. Mixed Predictive Equilibrium To guarantee the existence of an equilibrium without relying on local approximations, we can allow agents to randomize over model parameters. This is analogous to mixed strategies in game theory. Definition D.4 (Mixed Predictive Equilibrium). Let Î”Î˜ğ‘– be the set of probability distributions over parameters Î˜ğ‘–. Mixed Predictive Equilibrium is tuple of distributions ğ = (ğœ‡ ğ‘›) such that 1 for all ğ‘– I: , . . . , ğœ‡ ğœ‡ ğ‘– arg min ğœ‡ğ‘– Î”Î˜ğ‘– KL (cid:0)â„™(â„ğ‘–; ğ) ğ‘ğœ‡ğ‘– (â„ğ‘–)(cid:1) , (20) (cid:2) ğ‘ğœƒğ‘– (â„ğ‘–)(cid:3), and â„™(â„ğ‘–; ğ) is the distribution of histories generated when each where ğ‘ğœ‡ğ‘– (â„ğ‘–) = ğ”¼ğœƒğ‘–ğœ‡ğ‘– agent ğ‘– follows the policy ğœ‹ğœ‡ğ‘– obtained by applying the policy improvement operator equation 6 on ğ‘ğœ‡ğ‘–. Theorem D.5 (Existence of Mixed Predictive Equilibrium). Assume Î˜ğ‘– is compact metric space1 and the map (ğœ½, ğœƒ is continuous for every ğ‘– I. Furthermore, assume that KL (cid:0)â„™(â„ğ‘–; ğ) ğ‘ğœ‡ğ‘– (â„ğ‘–)(cid:1) < for all ğ Î” = (cid:206)ğ‘– Î”Î˜ğ‘–. Then Mixed Predictive Equilibrium exists. â„™(; ğœ½) ğ‘ğœƒ ğ‘– ) KL (cid:16) (cid:17) ğ‘– Proof. We prove existence by constructing continuous map on the space of mixed strategies and applying fixed-point theorem. Let Î”Î˜ğ‘– be the space of Borel probability measures on the compact metric space Î˜ğ‘–. Endowed with the Wasserstein metric, Î”Î˜ğ‘– is compact, convex metric space. Let Î” = (cid:206)ğ‘– Î”Î˜ğ‘– be the joint strategy space. Since Î”Î˜ğ‘– is compact metric space, it is separable. We can fix countable dense subset ğ·ğ‘– = { ğœ‡ğ‘–,ğ‘˜} ğ‘˜=1 Î”Î˜ğ‘–. We define the continuous advantage function ğ‘ğ‘– : Î” Î”Î˜ğ‘– â„0 as: ğ‘ğ‘– (ğ, ğœ‡ ğ‘–) = max (cid:110) 0, KL (cid:0)â„™(â„ğ‘–; ğ) ğ‘ğœ‡ğ‘– (â„ğ‘–)(cid:1) KL (cid:16) â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (â„ğ‘–) (cid:17)(cid:111) . Since KL (cid:0)â„™(â„ğ‘–; ğ) ğ‘ğœ‡ğ‘– (â„ğ‘–)(cid:1) < , the advantage function is well-defined and evaluates to finite real number. We now construct transition map ğ‘‡ğ‘– : Î” Î”Î˜ğ‘–. Define finite measure ğ´ğ‘– (ğ) on Î˜ğ‘– that places weights on the dense subset ğ·ğ‘– proportional to the advantage: ğ´ğ‘– (ğ) = ğ‘˜= 2ğ‘˜ğ‘ğ‘– (ğ, ğœ‡ğ‘–,ğ‘˜) ğœ‡ğ‘–,ğ‘˜ . Let ğ´ğ‘– (ğ)(Î˜ğ‘–) = (cid:205) ğœ‡ğ‘– with the improvement measure ğ´ğ‘– (ğ): ğ‘˜=1 2ğ‘˜ğ‘ğ‘– (ğ, ğœ‡ğ‘–,ğ‘˜) denote its total mass. We define ğ‘‡ğ‘– (ğ) by mixing the current strategy ğ‘‡ğ‘– (ğ) = ğœ‡ğ‘– + ğ´ğ‘– (ğ) 1 + ğ´ğ‘– (ğ)(Î˜ğ‘–) . (cid:16) (cid:17) are continuous and the spaces are compact, ğ‘ğ‘– is uniformly Since the mappings ğœ½ KL bounded and continuous in ğ with respect to the weak-* topology. Consequently, the joint map ğ‘‡ (ğ) = (ğ‘‡1(ğ), . . . , ğ‘‡ğ‘›(ğ)) is continuous function from the compact convex set Î” to itself. By Schauders fixed-point theorem, there exists fixed point ğ Î” such that ğ‘‡ (ğ) = ğ. â„™(; ğœ½) ğ‘ğœ‡ ğ‘– 1It is worth noting that we do not require the convexity of Î˜ğ‘– in Theorem D.5, we only need compactness. 21 Multi-agent cooperation through in-context co-player inference We now prove by contradiction that ğ is Mixed Predictive Equilibrium. Let From the fixed point condition ğœ‡ ğ‘– = ğ‘‡ğ‘– (ğ), we obtain: ğ¶ğ‘– = ğ´ğ‘– (ğ)(Î˜ğ‘–) . ğ‘– (1 + ğ¶ğ‘–) = ğœ‡ ğœ‡ ğ‘– + ğ´ğ‘– (ğ) = ğ¶ğ‘–ğœ‡ ğ‘– = ğ´ğ‘– (ğ). Assume ğ is not Mixed Predictive Equilibrium. Then, for some agent ğ‘– I, there exists distribution Ë†ğœ‡ğ‘– Î”Î˜ğ‘– such that KL (cid:0)â„™(â„ğ‘–; ğ) ğ‘Ë†ğœ‡ğ‘– (â„ğ‘–)(cid:1) < KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (â„ğ‘–) (cid:16) (cid:17) . Let (cid:16) ğœ– := KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (â„ğ‘–) (cid:17) KL (cid:0)â„™(â„ğ‘–; ğ) ğ‘Ë†ğœ‡ğ‘– (â„ğ‘–)(cid:1) > 0 . By definition, the advantage of Ë†ğœ‡ğ‘– is strictly positive: ğ‘ğ‘– (ğ, Ë†ğœ‡ğ‘–) = ğœ– > 0. Now since the mapping (ğœ½, ğœƒ ğ‘– ) KL (cid:16) (cid:17) (cid:16) â„™(; ğœ½) ğ‘ğœƒ ğ‘– (cid:17) is continuous, it follows that the functional â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğœ‡ is continuous on the compact metric space Î”Î˜ğ‘–, and hence the advantage ğ‘– KL function ğ‘ğ‘– (ğ, ) is uniformly continuous. Therefore, there exists an open neighborhood ğ‘ˆ Î”Î˜ğ‘– containing Ë†ğœ‡ğ‘– such that ğ‘ğ‘– (ğ, ğœ‡ ğ‘– ğ‘ˆ. ğ‘–) > ğœ–/2 for all ğœ‡ (â„ğ‘–) ğ‘– Since the set ğ·ğ‘– = { ğœ‡ğ‘–,ğ‘˜} ğ‘˜=1 is dense in Î”Î˜ğ‘–, there exists an integer ğ¾ such that ğœ‡ğ‘–,ğ¾ ğ‘ˆ. Consequently, ğ‘ğ‘– (ğ, ğœ‡ğ‘–,ğ¾) > ğœ–/2 > 0. This strictly positive advantage guarantees that the total mass of the improvement measure is strictly positive: ğ¶ğ‘– = ğ´ğ‘– (ğ)(Î˜ğ‘–) 2ğ¾ ğ‘ğ‘– (ğ, ğœ‡ğ‘–,ğ¾) > 0. From the fixed-point condition ğ¶ğ‘–ğœ‡ convex combination of the basis measures in ğ·ğ‘–: ğ‘– = ğ´ğ‘– (ğ), and knowing ğ¶ğ‘– > 0, we can express ğœ‡ ğ‘– as an infinite ğœ‡ ğ‘– = 1 ğ¶ğ‘– ğ´ğ‘– (ğ) = ğ‘˜=1 ğ‘¤ğ‘˜ ğœ‡ğ‘–,ğ‘˜ , where the weights ğ‘¤ğ‘˜ = 2ğ‘˜ğ‘ğ‘– (ğ,ğœ‡ğ‘–,ğ‘˜ ) Now, consider the expected predictive model under the mixed strategy ğœ‡ expectation, we have: 0 sum to exactly 1. ğ¶ğ‘– ğ‘– . By linearity of the ğ‘ğœ‡ ğ‘– (â„ğ‘–) = ğ”¼ğœƒğ‘–ğœ‡ ğ‘– (cid:2) ğ‘ğœƒğ‘– (â„ğ‘–)(cid:3) = ğ‘˜=1 ğ‘¤ğ‘˜ğ”¼ğœƒğ‘–ğœ‡ğ‘–,ğ‘˜ (cid:2) ğ‘ğœƒğ‘– (â„ğ‘–)(cid:3) = ğ‘˜=1 ğ‘¤ğ‘˜ ğ‘ğœ‡ğ‘–,ğ‘˜ (â„ğ‘–) . Because the Kullback-Leibler divergence is strictly convex with respect to its second argument, we can apply Jensens inequality to the infinite convex combination: (cid:16) KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (â„ğ‘–) (cid:17) (cid:32) = KL â„™(â„ğ‘–; ğ) ğ‘˜=1 ğ‘¤ğ‘˜ ğ‘ğœ‡ğ‘–,ğ‘˜ (â„ğ‘–) ğ‘˜= (cid:16) ğ‘¤ğ‘˜KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ğ‘–,ğ‘˜ (â„ğ‘–) (cid:33) (cid:17) . 22 Multi-agent cooperation through in-context co-player inference Crucially, by the definition of the advantage function and the construction of the weights ğ‘¤ğ‘˜, any weight ğ‘¤ğ‘˜ is strictly positive if and only if the corresponding advantage ğ‘ğ‘– (ğ, ğœ‡ğ‘–,ğ‘˜) > 0. strictly positive advantage exactly means that the evaluated measure achieves strictly lower loss than the current state ğœ‡ ğ‘– : (cid:16) KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ğ‘–,ğ‘˜ (â„ğ‘–) < KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (â„ğ‘–) (cid:17) (cid:16) (cid:17) . Since there is at least one weight ğ‘¤ğ¾ > 0 with an advantage bounded away from zero by ğœ–/2, substituting this strict upper bound into the sum over ğ‘˜ yields: (cid:16) ğ‘¤ğ‘˜KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ğ‘–,ğ‘˜ (â„ğ‘–) ğ‘˜=1 (cid:17) < ğ‘˜= (cid:16) ğ‘¤ğ‘˜KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (â„ğ‘–) (cid:17) (cid:16) (cid:16) = KL = KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (â„ğ‘–) â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (â„ğ‘–) ğ‘¤ğ‘˜ (cid:17) ğ‘˜=1 . (cid:17) Combining the inequalities together, we arrive at the following absolute contradiction: (cid:16) KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (â„ğ‘–) (cid:17) (cid:16) < KL â„™(â„ğ‘–; ğ) ğ‘ğœ‡ ğ‘– (cid:17) . (â„ğ‘–) Therefore, our initial assumption must be false. No such superior distribution Ë†ğœ‡ğ‘– can exist, and the fixed point ğ is indeed Mixed Predictive Equilibrium. An interesting corollary of the above theorem, is that if our model is convex in functional space, then there exists pure global predictive equilibrium. Corollary D.6 (Existence of Pure Predictive Equilibrium under Model Functional Convexity). Consider the same assumptions as in Theorem D.5. Assume furthermore that for every agent ğ‘– I, the space ğ‘– Î˜ğ‘– and every of representable predictive models { ğ‘ğœƒğ‘– ğ‘– .2 Under these ğ›¼ğ‘– [0, 1], there exists pure parameter ğœƒğ‘– Î˜ğ‘– satisfying ğ‘ğœƒğ‘– = ğ›¼ğ‘– ğ‘ğœƒ conditions, Global Predictive Equilibrium (in pure strategies) always exists. ğœƒğ‘– Î˜ğ‘–} is convex. That is, for every ğœƒ ğ‘– , ğœƒ + (1 ğ›¼ğ‘–) ğ‘ğœƒ ğ‘– ğ‘›) (cid:206)ğ‘– Î”Î˜ğ‘–. Proof. From Theorem D.5, there exists Mixed Predictive Equilibrium ğ = (ğœ‡ 1 To establish the existence of pure Global Predictive Equilibrium, we will demonstrate that for any probability distribution ğœ‡ğ‘– Î”Î˜ğ‘–, the model functional convexity assumption guarantees the existence of pure parameter ğœƒ We first prove this claim for finitely supported measures. Let ğœ‡ğ‘– = (cid:205)ğ‘š ğ‘¤ğ‘˜ğ›¿ğœƒğ‘–,ğ‘˜ be finitely supported ğ‘˜=1 probability measure on Î˜ğ‘–, where ğ‘¤ğ‘˜ 0 and (cid:205)ğ‘š ğ‘¤ğ‘˜ = 1. We proceed by induction on the support ğ‘˜=1 size ğ‘š. The base case ğ‘š = 1 is trivial, as ğ‘ğœ‡ğ‘– = ğ‘ğœƒğ‘–,1. Assuming the claim holds for ğ‘š 1, we can express ğœ‡ğ‘– (provided ğ‘¤ğ‘š < 1) as: ğ‘– Î˜ğ‘– such that ğ‘ğœƒ ğ‘– = ğ‘ğœ‡ğ‘– = ğ”¼ğœƒğ‘–ğœ‡ğ‘– , . . . , ğœ‡ (cid:2) ğ‘ğœƒğ‘– (cid:3). ğ‘ğœ‡ğ‘– = ğ‘¤ğ‘š ğ‘ğœƒğ‘–,ğ‘š + (1 ğ‘¤ğ‘š) ğ‘š1 ğ‘˜=1 ğ‘¤ğ‘˜ 1 ğ‘¤ğ‘š ğ‘ğœƒğ‘–,ğ‘˜ . By the inductive hypothesis, there exists pure parameter ğœƒğ‘– Î˜ğ‘– such that ğ‘ğœƒğ‘– = (cid:205)ğ‘š1 ğ‘˜=1 Applying the convexity assumption with ğ›¼ğ‘– = ğ‘¤ğ‘š, ğœƒ ğ‘ğœƒğ‘– = ğ‘¤ğ‘š ğ‘ğœƒğ‘–,ğ‘š + (1 ğ‘¤ğ‘š) ğ‘ğœƒğ‘– = ğ‘ğœ‡ğ‘–. Thus, the claim holds for all finitely supported measures. ğ‘ğœƒğ‘–,ğ‘˜. ğ‘– = ğœƒğ‘–, there exists ğœƒğ‘– Î˜ğ‘– such that ğ‘– = ğœƒğ‘–,ğ‘š, and ğœƒ ğ‘¤ğ‘˜ 1ğ‘¤ğ‘š 2We emphasize that we do not require convexity in the parameters, i.e., we do not require that ğ‘ğ›¼ğ‘–ğœƒ ğ‘– +(1ğ›¼ğ‘– )ğœƒ ğ‘– ğ›¼ğ‘– ğ‘ğœƒ ğ‘– + (1 ğ›¼ğ‘–) ğ‘ğœƒ ğ‘– . = 23 Multi-agent cooperation through in-context co-player inference Now, consider an arbitrary measure ğœ‡ğ‘– Î”Î˜ğ‘–. Since the set of finitely supported measures is dense in Î”Î˜ğ‘– under the weak-* topology, there exists sequence of finitely supported measures (ğœ‡ (ğ‘š) converging weakly to ğœ‡ğ‘–. ) ğ‘š= ğ‘– Because the mapping ğœƒğ‘– ğ‘ğœƒğ‘– (â„ğ‘–) is continuous and bounded for any given â„ğ‘–, the functional ğœˆ ğ‘ğœˆ(â„ğ‘–) = ğ‘ğœƒğ‘– (â„ğ‘–)ğ‘‘ğœˆ(ğœƒğ‘–) is continuous with respect to the weak-* topology. Consequently, the sequence of expected models converges pointwise: ğ‘ ğ‘ğœ‡ğ‘– as ğ‘š . ğœ‡ (ğ‘š) ğ‘– From the inductive step, for each finitely supported measure ğœ‡ (ğ‘š) parameter ğœƒ(ğ‘š) . This constructs sequence of pure parameters (ğœƒ(ğ‘š) in Î˜ğ‘–. Since Î˜ğ‘– is compact metric space, this sequence admits convergent subsequence (ğœƒ(ğ‘šğ‘˜ ) that converges to some limit point ğœƒ Î˜ğ‘– such that ğ‘ , there exists corresponding pure ) ğ‘š=1 ) ğ‘˜=1 = ğ‘ ğœ‡ (ğ‘š) ğ‘– ğœƒ(ğ‘š) ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– Î˜ğ‘–. By the continuity of the map ğœƒğ‘– ğ‘ğœƒğ‘–, we find: ğ‘ğœƒ ğ‘– = lim ğ‘˜ ğ‘ ğœƒ(ğ‘šğ‘˜ ) ğ‘– = lim ğ‘˜ ğ‘ ğœ‡ (ğ‘šğ‘˜ ) ğ‘– = ğ‘ğœ‡ğ‘– . Thus, for the Mixed Predictive Equilibrium ğ, there exists joint configuration of pure parameters ğœ½ = (ğœƒ 1 ğ‘›) (cid:206)ğ‘– Î˜ğ‘– such that ğ‘ğœƒ ğ‘– for all ğ‘– I. ğ‘– = ğ‘ğœ‡ , . . . , ğœƒ It then follows that ğœƒ ğ‘– arg min ğœƒğ‘– Î˜ğ‘– KL (cid:0)â„™(â„ğ‘–; ğœ½) ğ‘ğœƒğ‘– (â„ğ‘–)(cid:1) ğ‘– . This precisely satisfies the definition of Global Predictive Equilibrium, proving its existence in pure strategies under these conditions. We remark that while the assumption of functional convexity is an idealization for finite-capacity networks, deep neural networks are universal function approximators; consequently, as model capacity increases, the space of representable distributions approximates the full convex set of valid probability measures, rendering the existence of pure equilibrium an increasingly accurate approximation. D.3. Relationship to Nash Equilibria and Subjective Embedded Equilibria Finally, we connect the fixed points of the PPI algorithm to the standard solution concepts of game theory. In standard game theory, Nash Equilibrium assumes that agents act optimally given fixed environment, where the policies of co-players are independent of the focal agents current action selection. In contrast, agents in the PPI framework act optimally with respect to an internal world model ğ‘ğœƒğ‘– that estimates the joint distribution of future trajectories, thereby capturing potential reactive dependencies between the focal agents actions and the co-players responses. This is closely related to the concept of Embedded Equilibria, which characterizes the equilibrium behavior that emerges from such self-predictive dynamics: Definition D.7 (Subjective Embedded Equilibrium). (Meulemans et al., 2025b) joint policy profile ğ… and set of internal sequence models { ğ‘ ğ‘›} constitute Subjective Embedded Equilibrium 1 if: , . . . , ğ‘ 1. Subjective Optimality: Each agents policy ğœ‹ ğ‘– is strict best-response to its internal world model ğ‘ ğ‘– . 24 Multi-agent cooperation through in-context co-player inference 2. On-Path Consistency: Each agents world model perfectly matches the true environment dynamics exclusively on the equilibrium path (the distribution of histories â„™ genuinely generated by the joint policy ğ…). Crucially, Subjective Embedded Equilibrium places no constraints on the accuracy of the agents models regarding off-path counterfactuals (actions that are assigned zero probability under ğ…). Nevertheless, ğœ‹ ğ‘– , and this takes into account counterfactual off-policy paths. In other words, according to the predictive model ğ‘ ğ‘– , the agent ğ‘– will not get higher expected returns by deviating from ğœ‹ ğ‘– . ğ‘– must be best response with respect to ğ‘ We refer the reader to Meulemans et al. (2025b) for further details about subjective embedded equilibria and their properties. It turns out that if PPI agents converge to fixed point for which their (predictive) world models are perfect, then the predictive equilibrium corresponds to subjective embedded equilibrium. Let us first formalize the predictive equilibrium with perfect world models: Definition D.8 (Perfect Predictive Equilibrium). Perfect Predictive Equilibrium is configuration ğœ½ where the agents perfectly model the induced data distribution: (cid:16) KL â„™(â„ğ‘–; ğœ½) ğ‘ğœƒ ğ‘– (â„ğ‘–) (cid:17) = 0 ğ‘– . (21) Theorem D.9 (Perfect Predictive Equilibrium = Subjective Embedded Equilibrium). Consider predictive agents using the policy improvement operator defined in Eq. 6, where ğœ‹ğœƒğ‘– (ğ‘ğ‘–â„ğ‘–) ğ‘ğœƒğ‘– (ğ‘ğ‘–â„ğ‘–) exp( ğ›½ğ‘„ ğ‘ğœƒğ‘– (â„ğ‘–, ğ‘ğ‘–)). If ğœ½ is Perfect Predictive Equilibrium, then the resulting configuration is consistent with Subjective Embedded Equilibrium. = 0 implies that the Proof. At Perfect Predictive Equilibrium, the condition KL sequence model matches the true data distribution almost everywhere. Thus, on the equilibrium path, the prior action probability generated by the sequence model exactly matches the true behavioral policy: ğ‘ğœƒ (ğ‘ğ‘–â„ğ‘–). This immediately satisfies the On-Path Consistency condition. (ğ‘ğ‘–â„ğ‘–) = ğœ‹ğœƒ ğ‘– () â„™(; ğœ½) ğ‘ğœƒ ğ‘– ğ‘– (cid:16) (cid:17) Substituting ğ‘ğœƒ ğ‘– = ğœ‹ğœƒ ğ‘– into the policy improvement operator yields: ğœ‹ğœƒ ğ‘– (ğ‘ğ‘–â„ğ‘–) = 1 ğ‘(â„ğ‘–) ğœ‹ğœƒ ğ‘– (ğ‘ğ‘–â„ğ‘–) exp( ğ›½ğ‘„ ğ‘ğœƒ ğ‘– (â„ğ‘–, ğ‘ğ‘–)) . For any action ğ‘ğ‘– in the support of the policy (where ğœ‹ğœƒ to obtain: ğ‘– (ğ‘ğ‘–â„ğ‘–) > 0), we divide both sides by ğœ‹ğœƒ ğ‘– (ğ‘ğ‘–â„ğ‘–) 1 = 1 ğ‘(â„ğ‘–) exp( ğ›½ğ‘„ ğ‘ğœƒ ğ‘– (â„ğ‘–, ğ‘ğ‘–)) = ğ‘„ ğ‘ğœƒ ğ‘– (â„ğ‘–, ğ‘ğ‘–) = ln ğ‘(â„ğ‘–) ğ›½ . Since ğ‘(â„ğ‘–) is normalizing constant independent of ğ‘ğ‘–, the expected return evaluated under the model must be identical for all actions played with positive probability. (ğ‘ Now consider any off-path action ğ‘ ğ‘– â„ğ‘–) = 0). Because this ğ‘– not in the support of the policy (where ğœ‹ğœƒ ğ‘–; ğœ½) = 0. Consequently, action is never taken under the joint policy, the marginal probability â„™(â„ğ‘–, ğ‘ the KL divergence places absolutely no constraints on the models conditional predictions following ğ‘ ğ‘–. ğ‘– To formally verify Subjective Optimality, we demonstrate that there exists valid completion of the ğ‘– â„ğ‘–) = 0. Let ğ‘’ğ‘šğ‘–ğ‘› = (ğ‘œ, ğ‘Ÿğ‘šğ‘–ğ‘›) be sequence models off-path conditional probabilities that justifies ğœ‹ğœƒ (ğ‘ ğ‘– 25 Multi-agent cooperation through in-context co-player inference an environment percept containing the minimal possible reward ğ‘Ÿğ‘šğ‘–ğ‘›. We define the models off-path counterfactual completion as ğ‘ğœƒ ğ‘–) = 1, assuming absorbing minimal rewards thereafter. (ğ‘’ğ‘šğ‘–ğ‘› â„ğ‘–, ğ‘ ğ‘– Evaluating the expected return under this completed subjective model yields ğ‘„ is not larger than the on-path return ln ğ‘ (â„ğ‘– ) ğ‘ğœƒ to the suboptimal deviation ğ‘ world model, fully satisfying the definition of Subjective Embedded Equilibrium. ğ‘–) = ğ‘‰ğ‘šğ‘–ğ‘›, which . Because the policy operator is restricted by the prior ğ‘– â„ğ‘–), which must evaluate to 0 to satisfy the fixed point, the agent assigns exactly 0 probability ğ‘–. Therefore, the agent is playing an exact, best-response to its subjective (ğ‘ ğ›½ ğ‘– (â„ğ‘–, ğ‘ ğ‘– ğ‘ğœƒ E. Software Experiments were implemented in Python together with the Google JAX (Bradbury et al., 2018) framework, and the NumPy (Harris et al., 2020), pandas (Wes McKinney, 2010), Matplotlib (Hunter, 2007), seaborn (Waskom, 2021), Flax (Heek et al., 2024) and Optax (DeepMind et al., 2020) packages. E.1. LLM usage We used Gemini 3 Pro for language editing and readability improvements during the preparation of this manuscript. We also used Gemini 3 Pro for providing additional details in the proof of Lemma C.2, which were afterwards checked by the authors."
        }
    ],
    "affiliations": [
        "Google, Paradigms of Intelligence Team",
        "Santa Fe Institute"
    ]
}