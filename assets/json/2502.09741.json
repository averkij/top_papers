{
    "paper_title": "FoNE: Precise Single-Token Number Embeddings via Fourier Features",
    "authors": [
        "Tianyi Zhou",
        "Deqing Fu",
        "Mahdi Soltanolkotabi",
        "Robin Jia",
        "Vatsal Sharan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64$\\times$ less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3$\\times$ and 6$\\times$ fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/."
        },
        {
            "title": "Start",
            "content": "FoNE: Precise Single-Token Number Embeddings via Fourier Features"
        },
        {
            "title": "Tianyi Zhou Deqing Fu Mahdi Soltanolkotabi Robin Jia Vatsal Sharan",
            "content": "Department of Computer Science University of Southern California Los Angeles, CA 90089 {tzhou029,deqingfu,soltanol,robinjia,vsharan}@usc.edu Abstract Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the models performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64 less data to achieve 99% accuracy than subword and digitwise embeddings while using 3 and 6 fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/. 5 2 0 2 3 1 ] . [ 1 1 4 7 9 0 . 2 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Figure 1: (a) We extract all the numbers from the input sequence. (b) For each number, we use FoNE to directly map the number to its embedding. The first two entries in the embedding represent 18 mod 10, while the next two entries represent 18 mod 100. (c) We pad the FoNE with zeros, add it to the word embeddings, and then feed the combined embeddings into the model. (d) For each digit, we take every two entries from the last hidden state and find the number whose representation is closest to these two entries. Large language models (LLMs) require precise representations of numerical data to perform numberrelated tasks effectively. However, since LLMs treat numbers just like any other token, embeddings of numerical tokens do not systematically capture important numerical features. As result, it is challenging for even billion-parameter models to achieve perfect accuracy in solving simple arithmetic tasks [37, 7, 20, 38, 47]. While generating code can be useful workaround, relying solely on this capability highlights fundamental limitation: without proper understanding of numbers, the model cannot fully grasp concepts critical to domains like mathematical theorems, physics laws, or quantitative reasoning. Even with approaches like Chain-of-Thought (CoT) prompting, it is important to have perfect accuracy in solving basic arithmetic tasks to build strong foundation for more complex reasoning. Standard tokenization approaches, such as subword tokenization (e.g., GPT-4o [2], Llama-3 [6], Phi-2 [1]) or digit-wise tokenization (e.g., Llama-2 [44], Mistral [16]), force the model to aggregate multiple tokens to understand numbers and introduces inefficiencies by tokenizing one number to multiple tokens. However, this inefficiency in tokenizing numbers leads to larger challenges when it comes to their representation. Numbers, unlike words, require systematic, frequency-agnostic representations, yet LLMs often exhibit frequency bias in arithmetic tasks [35], predicting numbers based on training data prevalence rather than adhering to their mathematical properties. We draw inspiration from interpretability analyses of LLMs, which reveal that models internally develop Fourier-like features. Specifically, pre-trained models embed number tokens using sparse set of features in the Fourier domain [49]. These features enable the representation of numbers capturing both the magnitude and exact values of numbers, which are critical for solving arithmetic tasks [49]. However, due to limitations in sub-word tokenization, current LLMs struggle to extend this mechanism to larger numbers, highlighting the need for more systematic approaches for numerical representation. In this paper, we propose novel approach called Fourier Number Embedding (FoNE), which directly maps numbers into their Fourier representations, bypassing the tokenization step entirely. By representing each digit using cosine and sine functions with different periods, as shown in Figure 1(b), FoNE ensures precise representation of numbers. This approach encodes the modular relationship of each digit through periodic functions, enabling recovery of mod 10i for all digits i. Note that, for FoNE, each digit is represented using only two dimensions in the embedding vector. This compact design not only reduces computational overhead but also creates opportunities for future extensions by incorporating additional features to better capture numeric properties. By embedding and predicting numbers directly as single tokens, our method eliminates the need for multiple forward passes and token aggregation, significantly enhancing computational efficiency. Furthermore, we provide theoretical justification for why FoNE can represent numbers accurately as single tokens, leveraging the modular encoding properties of trigonometric functions to ensure exact recovery of each digit through periodic embeddings. Beyond theoretical justifications, we demonstrate the effectiveness of FoNE through extensive experiments on range of arithmetic tasks, including addition, subtraction, and multiplication. Our results show that FoNE is the only approach that can achieve perfect accuracy on multiple arithmetic tasks while requiring significantly less training data and fewer model parameters compared to existing methods. Moreover, FoNE offers faster training and inference times by encoding each number into single token, thereby reducing the computational overhead. These findings underscore FoNEs capacity to represent and manipulate numerical data both efficiently and precisely within large language models."
        },
        {
            "title": "2 Related Work",
            "content": "Arithmetic and Number-Related Tasks in LLMs. Addressing number-related tasks through nexttoken prediction remains significant challenge, such as arithmetic [37, 46, 26], time-series prediction [41, 28, 23, 48, 21, 18], quantitative reasoning [25, 22, 4, 19, 5], and handling tabular data [11, 8, 36]. Despite advancements in transformer-based models, LLMs such as GPT-3 and GPT-4, with billions of parameters, struggle to solve simple arithmetic problems involving multi-digit addition and multiplication across multiple forward passes [7, 9], even when using scratchpads [31]. Lee et al. [20] demonstrate that smaller transformer models can successfully handle multiplication when equipped with carefully designed scratchpads. Other approaches, such as those proposed by Golkar et al. [13], Sundararaman et al. [40], Jiang et al. [17], Sivakumar and Moosavi [39], introduce number embedding methods to enhance model performance on number-related tasks. However, the range of numbers these methods can accurately represent is typically limited to fewer than five digits. Additionally, line of research [24, 38] incorporates the positional information of digits into embeddings or adds it as extra tokens [30]. Zhou et al. [50] shows that digit-wise tokenization is better than subword tokenization. Thawani et al. [43] explores encoding strategies like digit-by-digit, scientific notation, and base-10 formats, while Jiang et al. [17] maps numbers to finite prototype numerals. These methods help the model align digits of equal significance but often require digit-wise tokenization or introduce additional tokens, reducing training and prediction efficiency. In contrast, the method proposed in this paper precisely encodes all numbers as single token, eliminating range limitations and avoiding the efficiency drawbacks associated with previous approaches. Fourier Features. Fourier features are commonly observed in image models, particularly in the early layers of vision models [33, 32, 10]. These features enable the model to detect edges, textures, and other spatial patterns effectively. However, Transformers struggle to capture high-frequency components in [3, 42]. Augmenting data with high-frequency components or explicitly encoding coordinates using Fourier features has been demonstrated to improve model performance [42, 15]. In modular addition tasks, studies have revealed that after grokking, one-layer Transformer can learn to solve the task 2 perfectly by leveraging Fourier features [29, 14]. Furthermore, Zhou et al. [49] demonstrates that LLMs naturally encode numbers using Fourier features during pretraining, leveraging these representations for arithmetic tasks. Building on this insight, we propose encoding numbers precisely through Fourier features by representing residues in various modular groups, enabling algebraic operations to be performed in component-wise, parallel manner."
        },
        {
            "title": "3 Methods",
            "content": "Building on insights from prior studies [49] that highlight the importance of Fourier features in numerical embeddings, we propose Fourier Number Embedding. Unlike existing methods that often require digitwise tokenization or pre-training to handle numeric tasks, FoNE directly maps numbers into compact Fourier representations. In Section 3.1, we introduce FoNE, where each digit is represented with two entries in their embeddings. In Section 3.2, we introduce the Fourier number loss function and Fourier number prediction, which demonstrate how we decode the last hidden states from Fourier space to number space to compute loss and make the prediction. In Section 3.3 we show how we incorporate our method into input sequences. The complete process is shown in Figure 1."
        },
        {
            "title": "3.1 Fourier Number Embedding (FoNE)",
            "content": "Algorithm 1 Fourier Number Embedding (FoNE) Algorithm 1: procedure FourierNumberEmbedding(x R, Z0, Z0, Z>0) 2: Inputs: Number x, integer digit length m, decimal digit length n, embedding dimension Initialize empty embedding vector FoNE [] for = + 1 do Loop over all scales from 10n+1 to 10m Set the period for the current scale Compute the circular embedding for scale Ti Add the embedding for this scale to the result Ensure embedding dimension matches the target Zero-pad 3: 4: 5: 6: 7: 8: 9: Ti 10i ϕ(x, Ti) (cos( 2π Ti Append ϕ(x, Ti) to FoNE x), sin( 2π Ti x)) end for while Length(FoNE) < do 10: Append 0 to FoNE 11: end while return FoNE 12: 13: end procedure We first introduce the following circular embedding function that maps each to point on the unit circle. Definition 3.1 (Circular embedding). Let be given period. We define function ϕ : R2 ϕ(x, ) := (cid:0)cos (cid:0) 2π x(cid:1) , sin (cid:0) 2π x(cid:1)(cid:1) . Let represents the number of digits before the decimal point, and represents the number of digits after the decimal point, ensuring that both integer and fractional parts of number are accounted for in the representation. Next, we formally define the FoNE method that directly map numbers to their embedding. Definition 3.2 (Fourier Number Embedding). Let be the integer digit length, and be the decimal digit length. We define the Fourier Number Embedding function FoNE : R2(m+n) for an input number as follows: FoNE(x, m, n) := (cid:2)ϕ(x, Ti)(cid:3)m i=n+1, 3 where Ti = 10i for each integer in the range + 1 to m. To align the embedding dimensions of FoNE with the models input embedding dimension d, we map the Fourier Number Embedding, which lies in R2(m+n), to Rd. This mapping can be achieved in two ways: (1) by applying learnable linear transformation Rd2(m+n), or (2) by appending zeros to the embedding vector to match the dimensionality d. As demonstrated in Section 4.3, both approaches achieve comparable results. Then, we introduce an elementary lemma and demonstrate why FoNE can preserve the numeracy on numbers. Lemma 3.3 (Informal version of Lemma C.1). Given the pair (cid:0)cos (cid:0) 2π mod . x(cid:1) , sin (cid:0) 2π x(cid:1)(cid:1), we can recover Lemma 3.4 (FoNE preserves numeracy). Given numbers Fourier Number Embedding FoNE(x), its integer digit length m, and the decimal digit length n, by using Lemma 3.3, we can recover mod 10i for each integer in the range + 1 to m. natural question that arises here is why do we need mod 10 when we know mod 100. Hence, next, we show the necessity of different periods. Lemma 3.5 (Necessity of different periods). When becomes very large in circular embedding (Definition 3.1), the difference 2π approaches zero, causing the embedded representations of and + 1 to become arbitrarily close on the unit circle. Consequently, single large cannot sufficiently distinguish adjacent values in the embedding. Hence, one must choose across broad range of scales to ensure that the embedding remains adequately distinguishable for all values of x. In this paper, we choose as 10i, i, so that each effectively captures one digit of x. (x + 1) 2π Note that since FoNE uses the ratio between entries to represent numbers, it is unaffected by layer normalization and RMS normalization (Lemma C.2), in contrast to xVal [13], which uses the magnitudes of entries. To provide clear illustration of our method, we present detailed example demonstrating how we map number 4.17 to its embedding. Example 3.6. Consider = 4.17. Its Fourier Number Embedding is given by [ϕ(4.17, 0.1), ϕ(4.17, 1), ϕ(4.17, 10)], where ϕ is defined in Definition 3.1. From these components, by using Lemma 3.3, we can recover [4.17 mod 0.1, 4.17 mod 1, 4.17 mod 10], 1 which simplifies to [0.07, 0.17, 4.17]. If we used only = 10, then ϕ(4.17, 10) would be nearly indistinguishable from ϕ(4.18, 10), causing the embedding to lose fine-grained information about less significant digits. However, with these chosen periods , we can capture all the digits."
        },
        {
            "title": "3.2 Decoding",
            "content": "As each number has its own FoNE, calculating the logits for all possible numbers becomes computationally infeasible. Therefore, we introduce novel decoding head that maps hidden states from Fourier space to number space as shown in Figure 1(d). Below, we explicitly define the loss function and prediction function for each digit and then show how to combine these to obtain the final loss and prediction. 1For real and positive real m, mod is defined as (cid:4) (cid:5) , yielding value in the range [0, m) 4 Algorithm 2 Fourier Number Loss & Prediction 1: function FourierNumberLossFunction(h, y, i) 2: 3: 4: 6: 5: yi the i-th digit of (cid:2)h[2i], h[2i + 1](cid:3) [ϕ(0, 10), ϕ(1, 10), , ϕ(9, 10)] logits loss LCE(yi, logits) return loss 7: 8: end function 9: function FourierNumberPrediction(h, i) logits (cid:2)h[2i], h[2i + 1](cid:3) (cid:2)ϕ(j, 10)(cid:3) 10: (cid:98)yi arg maxj{0,...,9} logits[j] return (cid:98)yi 12: 13: end function j=0,..., 11: Cross-entropy loss for digit Prediction for digit Definition 3.7 (Fourier Number Loss Function). Let Rd denote the last-layer hidden state of the model. Let yi denote the i-th digit of the label number y. For digit i, we define the Fourier Number Loss Function LFoNE as: (cid:16) LFoNE(h, y, i) := LCE yi, ([h[2i], h[2i + 1]] (cid:125) (cid:124) (cid:123)(cid:122) 12 (cid:124) ϕ(0, 10) ... ϕ(9, 10) (cid:123)(cid:122) 210 (cid:17) ) (cid:125) This construction allows each digit to be treated as separate prediction task while sharing the same underlying model representation h. By taking the average of LFoNE(h, y, i) over all digit positions i, we obtain the final training loss. Definition 3.8 (Fourier Number Prediction for the i-th digit). Let Rd denote the last-layer hidden state of the model. For digit i, we define the Fourier Number Prediction as: (cid:16)(cid:2)h[2i], h[2i + 1](cid:3) (cid:2)ϕ(j, 10)(cid:3)(cid:17) . (cid:98)yi := arg max j{0,...,9} Here, (cid:98)yi is determined by the similarity between the hidden states and the circular embedding of number in {0, , 9} as illustrated in Figure 1(d). Once we have computed (cid:98)yi for each digit i, the final prediction for the entire number can be formed by concatenating these digit-wise predictions. We defer the detailed algorithms to Appendix A."
        },
        {
            "title": "3.3 Incorporating FoNE into Input Sequences",
            "content": "The integration of Fourier Number Embedding (FoNE) into input sequences proceeds as follows, as illustrated in Figure 1: 1. Extract all numbers from the input sequence to create number list. Replace each number with the token [Num] and tokenize the sequence to obtain token list. 2. Embed the token list using standard word embedding methods. 3. Map each number in the number list to its FoNE representation using Algorithm 1, as detailed in Section 3.1. 4. Add the FoNE to the word embedding of the corresponding [Num] token. 5. Feed the combined embeddings into the model. 6. Use the models output embeddings to predict the next token in the sequence. 7. If the predicted token is [Num], decode the numerical value using the method described in Section 3.2, or compute the loss during training. This procedure ensures that FoNE embeddings are seamlessly integrated into the input sequence, enabling the model to leverage both numerical and contextual information effectively."
        },
        {
            "title": "4.1 Experiment Setting",
            "content": "We evaluate the performance of our proposed Fourier Number Embedding (FoNE) method on arithmetic tasks designed to benchmark different number embedding methods. The dataset includes tasks such as 6-digit integer addition, 6-digit decimal addition (with 3 digits after the decimal), 5-digit integer subtraction, 3-digit integer multiplication, and 4-digit integer multiplication. These tasks are curated to measure model capabilities in accurate numeric computation. All experiments were conducted using an NVIDIA A6000. Dataset. Each example in the dataset is formatted as [operand a][operator][operand b]=, where the operands and are sampled based on the operation type. For addition and multiplication, we ensure to avoid duplication (e.g., + and + are treated as identical and included only once). For subtraction, we enforce to ensure non-negative results. For an x-digit operands dataset, each operand can have up to digits. The dataset is divided into training, validation, and test subsets as shown in Table 4 in Appendix G. Baselines. We compare our proposed FoNE method against several baseline methods for numeric embeddings. First, we consider digit-wise tokenization, where each digit in number is treated as an individual token. Second, we evaluate subword tokenization, where numeric values are tokenized into subword units based on the Llama3.2-1b tokenizers default vocabulary. Third, we include the xVal method [13], which leverages explicit value-based representations for numeric computation. As xVal predict floating point numbers, predictions are rounded to calculate accuracy. Finally, we fine-tune pre-trained LLMs on the same dataset for comparison. Setup. Our experiments involve multiple configurations of transformer-based models. Specifically, we use Llama-3.2-1B-Instruct as the base model. Models were evaluated across varying sizes, ranging from small to large architectures as defined in Table 6. In Appendix H, we conduct experiments on GPT-2-Large and have the same results. Learning rates were determined through an extensive search, with the best rates selected separately for each method based on the validation performance. Model evaluation used exact match accuracy to assess numeric prediction correctness. All models were trained from random initialization. We varied the training data size by uniformly sampling subsets and adjusted model sizes to compare accuracy across methods."
        },
        {
            "title": "4.2 Experiment Results",
            "content": "Data Efficiency. Figure 2(a) illustrates the accuracy trends of different embedding methods as the data size increases for the 6-digit decimal addition task. Remarkably, our model achieves 99% accuracy with just 6, 400 training samples and 37.55 million parameters, requiring 64 less training data than traditional embedding methods (409, 600/6, 400 = 64). Even with only 3, 200 training samples, our method outperforms the fine-tuned Llama-3.2 model. Additionally, it achieves perfect accuracy with 51, 200 training samples. 6 (a) 6-digit decimal addition: Acc. vs. Training Data Size (b) 6-digit decimal addition: Acc. vs. Model Size Figure 2: We train Llama-3.2-1B from scratch with random initialization using different number embedding methods on 6-digit decimal addition. The test accuracy is compared across varying data sizes and model sizes. (a) 6-digit integer addition: Model&Data size vs. Acc. (b) 5-digit integer subtraction: Model&Data size vs. Acc. (c) 3-digit integer multiplication: Model&Data size vs. Acc. (d) 4-digit integer multiplication: Model&Data size vs. Acc. Figure 3: Comparison of accuracy trends for various arithmetic tasks with respect to model size and data size. Beyond synthetic tasks, our approach also improves data efficiency in real-world scenarios. For instance, FoNE requires only 149.25 tokens on average to represent numerical values from table in the WikiTableQuestions dataset [34], compared to 329.7 tokens used by digit-wise tokenizer. This significant reduction in token usage highlights the efficiency of our method in encoding numerical data, making it more scalable for practical applications. 7 Parameter Efficiency. Figure 2(b) shows the accuracy trends of different embedding methods as the model size increases for the 6-digit decimal addition task. Our method achieves 97% accuracy with just 1 layer and 8.31 million parameters using 200k examples for training. Furthermore, with 26.62 million parameters, it surpasses the fine-tuned Llama-3.2 model and achieves 100% accuracy. Different Tasks. We conducted the same experiments across all different datasets. As shown in Figure 3, our method consistently demonstrates superior data and parameter efficiency compared to other approaches. Notably, it is the only method that achieves perfect accuracy on 6-digit decimal addition, 6-digit integer addition, 5-digit subtraction, and 3-digit multiplication. We also show that our method performs better in binary classification task that involves numerical values. Specifically, the task requires predicting label based on linear equation applied to three integers. Due to space limitations, we defer the details to Appendix B. Training and Inference Efficiency. Table 1 compares the training and test times used for one epoch across different embedding methods. Our method is consistently faster than digit-wise and subword embedding methods, as it uses one token to embed each number. Compared with xVal, our method consistently achieves higher accuracy. Additionally, we show the number of tokens required to tokenize the maximum number for different methods, highlighting the efficiency of our approach. Table 1: Training and inference efficiency comparison across three arithmetic tasks. The times are reported in minutes () and seconds (). Decimal Addition Subtraction Multiplication Method Train Time Test Time Tokens Accuracy Train. Test. Toks. Acc. Train. Test. Toks. Acc. Ours Digit-wise Subword xVal 318 1148 646 317 29 125 58 27 1 7 3 1 100 99.85 97.94 0. 242 29 941 115 54 547 27 254 1 5 2 1 33 256 100 99.71 1011 118 58 91.66 620 26 256 3.41 1 8 3 1 98.56 81.21 8."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "Linear Layer after FoNE. As discussed in Section 3.1, we evaluate the use of linear layer applied after FoNE and compare it with the approach of appending zeros to align the embedding dimensions with the models input requirements. As shown in Table 2, both configurations achieve almost the same accuracy. Hence, either way can be used to align FoNE with the embedding dimension. Table 2: Accuracy Comparison Across Datasets"
        },
        {
            "title": "Decimal Addition\nInteger Addition\nMultiplication\nSubtraction",
            "content": "100% 100% 99.95% 100% 100% 100% 99.91% 100% Effect of Different Periods. As discussed in Section 3.1, the modular group captures the necessary information for each digit, ensuring the effectiveness of our approach. We test the model with base periods of [2, 5, 10], [5], and [7], as shown in Table 3. The [2, 5, 10] configuration achieves accuracy comparable to that of the 10-period setup across different datasets. In this paper, we choose single 10 to make it 8 more parameter efficient. However, configurations using only mod5 or mod7 exhibit significantly lower accuracy. This is because neither mod5 nor mod7 can fully represent the required information for all digits. Table 3: Accuracy Comparison Across Datasets and Periods Dataset 2,5,10 10 5 7 Decimal Addition Integer Addition Multiplication Subtraction 100 100 99.99 100 1.52 3.64 100 100 1.55 0.02 99.95 3.67 1.91 4.64 0.24 100 The mispredictions are attributed to the absence of critical modular results. As illustrated in Table 7 in Appendix G, in the decimal addition task, using only mod 5 representation prevents the model from distinguishing between certain digits, such as 2 and 7, which results in errors. Necessity of Sine and Cosine Encoding. natural question arises: are sinusoidal encodings truly necessary for arithmetic tasks? One could directly encode each digit into separate dimension of the embedding, representing number like 567 as [5, 6, 7]. However, this approach fails to achieve perfect accuracy. For instance, numbers such as 999 and 888 become nearly indistinguishable after layer normalization, which reduces their differences and can lead to confusion during training. We evaluate this direct encoding method on 6-digit decimal addition and, after performing learning rate search, find that the best accuracy is 99.3% with learning rate of 0.01 and training for 100 epochs. In contrast, FoNE achieves better accuracy in just 6 epochs with the same dataset and model size. This suggests that naive direct encoding does not adequately preserve numerical distinctions for reliable arithmetic operations. As illustrated in Table 8 in the appendix, the model frequently mispredicts 8 as 9, further demonstrating the limitations of direct encoding in preserving numerical structure."
        },
        {
            "title": "5 Discussion",
            "content": "Q1: Why do we choose components with periods that are multiples of 10, i.e., 10, 100, 1000 ? As shown in Zhou et al. [49] and Figure 8, pre-trained LLMs trained on diverse datasets and strategies consistently learn nearly identical key frequency components. These components have periods of 10 and its divisors, such as 2 and 5. Since mod 10 can already represent single digit, we believe that mod 2 and mod5 contribute to enhancing robustness. Models trained on real-world text datawhere numbers are almost always expressed in decimalcommonly learn frequency components that correspond to mod10. We could, in principle, choose different bases (such as 2, 16, etc.), and if the training data contained enough examples in those representations, the model might well learn those frequencies too. However, most large language models primarily encounter numbers in base 10. Q2: How does the FoNE handle numbers with longer digit sequences? The maximum digit length that float64 data type can represent is 15 digits. When exceeds 15 digits in length, applying FoNE(x) directly may result in loss of precision. To address this, can be divided into smaller chunks, and FoNE can be applied to each chunk independently. For example, can be split into groups of five digits. The FoNE can then be calculated for each chunk, resulting in representation of length 10 per chunk, as each digit is encoded in two dimensions. These embeddings are subsequently concatenated to obtain the final number embedding for x. By using this method, as shown in Figure 4(a), an 8-layer transformer trained on 60-digit addition achieved an average accuracy of 97.42% across different operand length with just one forward pass. This demonstrates the effectiveness of FoNE in handling long sequences. 9 (a) Test accuracy of 60-digit addition with FoNE (b) Impact of combining FoNE with Abacus embedding (a) Average accuracy of an 8-layer transformer model on 60-digit addition tasks using FoNE for Figure 4: chunked input. (b) Performance improvements achieved by combining FoNE with the Abacus embedding method across various random seeds. The transformer is trained on addition tasks with up to 10-digits numbers (represented by the smaller square) and tested up to 50-digit numbers. Q3: Can FoNE combine with other positional embedding? Yes, FoNE can be combined with other positional embedding methods. For instance, we integrated FoNE with the Abacus embedding method [24], which operates on digit-wise tokenization. In this setup, the embeddings for each digit (09) are replaced with their corresponding Fourier Number Embeddings. We trained an 8-layer transformer model on integer addition tasks with up to 10 digits and tested it on addition tasks involving up to 50-digit numbers. The results, as illustrated in Figure 4(b) and Figure 10 in Appendix F, show that incorporating FoNE consistently improves the performance of the Abacus method across various random seeds. This highlights the complementary benefits of combining FoNE with other positional embedding strategies. Q4: Will FoNE affect the semantic meaning of numbers like years? As discussed by Meng et al. [27], the semantic meaning or memory of tokens is often inferred from the MLP layers in transformer models. Since LLMs are typically equipped with sufficient capacity, the precise numerical embedding of numbers takes precedence over encoding their semantic meanings directly within the embeddings. Moreover, as noted by Yao et al. [45], LLMs are capable of developing specialized circuits to handle different query types. Consequently, FoNE is designed to provide accurate numerical representations while allowing the models architecture to manage semantic contexts independently. An important future direction is the integration of FoNE with pre-trained LLMs, enabling the efficient adoptation of numerical representations within existing large-scale models. This approach could enhance numerical reasoning capabilities while leveraging the extensive knowledge and contextual understanding embedded in pre-trained LLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced FoNE, novel method for representing numbers in the embedding space of LLMs. By leveraging Fourier features, FoNE directly maps numbers into compact and precise representation, bypassing tokenization inefficiencies and preserving essential numerical properties. FoNE has significant implications for pre-training LLMs. By incorporating FoNE, models can develop robust understanding of numerical concepts, addressing fundamental limitation in current 10 architectures. We expect this capability extends beyond simple arithmetic to support wide range of number-related tasks, including time-series analysis, quantitative reasoning, and complex operations in fields like physics and mathematics. By integrating FoNE into pre-training strategies, future models can overcome the limitations of existing tokenization schemes, achieve greater computational efficiency. We believe FoNE represents significant step toward equipping LLMs with the tools necessary to tackle complex challenges with precision and rigor."
        },
        {
            "title": "Acknowledgments",
            "content": "This research is supported in part by AWS credits through an Amazon Faculty research award, NAIRR Pilot award, and Microsft accelerating foundations research grant, and Google Research Scholar Award. M. Soltanolkotabi is also supported by the Packard Fellowship in Science and Engineering, Sloan Research Fellowship in Mathematics, an NSF-CAREER under award #1846369, and NSF-CIF awards #1813877 and #2008443, NSF SLES award #2417075. and NIH DP2LM014564-01. R. Jia was also supported by the National Science Foundation under Grant No. IIS-2403436. V. Sharan was supported in part by an NSF CAREER Award CCF-2239265 and an Amazon Research Award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. The work was done in part while some of the authors were visiting the Simons Institute for the Theory of Computing."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Jiawang Bai, Li Yuan, Shu-Tao Xia, Shuicheng Yan, Zhifeng Li, and Wei Liu. Improving vision transformers by revisiting high-frequency components. In European Conference on Computer Vision, pages 118. Springer, 2022. [4] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78897901, 2023. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [6] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [7] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024. 11 [8] Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, and Christos Faloutsos. Large language models on tabular dataa survey. arXiv e-prints, pages arXiv2402, 2024. [9] Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, and Liwei Wang. How numerical precision affects mathematical reasoning capabilities of llms. arXiv preprint arXiv:2410.13857, 2024. [10] Pierre-Etienne Fiquet and Eero Simoncelli. polar prediction model for learning to represent visual transformations. Advances in Neural Information Processing Systems, 36, 2024. [11] Yanjun Gao, Skatje Myers, Shan Chen, Dmitriy Dligach, Timothy Miller, Danielle Bitterman, Matthew Churpek, and Majid Afshar. When raw data prevails: Are large language model embeddings effective in numerical data representation for medical machine learning applications? arXiv preprint arXiv:2408.11854, 2024. [12] Jonas Geiping and Tom Goldstein. Cramming: Training language model on single gpu in one day. In International Conference on Machine Learning, pages 1111711143. PMLR, 2023. [13] Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et al. xval: continuous number encoding for large language models. arXiv preprint arXiv:2310.02989, 2023. [14] Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Tianyi Zhou. Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic. arXiv preprint arXiv:2402.09469, 2024. [15] Keji He, Chenyang Si, Zhihe Lu, Yan Huang, Liang Wang, and Xinchao Wang. Frequency-enhanced data augmentation for vision-and-language navigation. Advances in Neural Information Processing Systems, 36, 2024. [16] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [17] Chengyue Jiang, Zhonglin Nian, Kaihao Guo, Shanbo Chu, Yinggong Zhao, Libin Shen, and Kewei Tu. Learning numeral embeddings. arXiv preprint arXiv:2001.00003, 2019. [18] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023. [19] Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, et al. Cladder: benchmark to assess causal reasoning capabilities of language models. Advances in Neural Information Processing Systems, 36, 2024. [20] Nayoung Lee, Kartik Sreenivasan, Jason Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023. [21] Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, and Shu-Tao Xia. Taming pre-trained llms for generalised time series forecasting via cross-modal knowledge distillation. arXiv preprint arXiv:2403.07300, 2024. 12 [22] Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, and Yansong Feng. Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. arXiv preprint arXiv:2402.17644, 2024. [23] Qianli Ma, Zhen Liu, Zhenjing Zheng, Ziyang Huang, Siying Zhu, Zhongzhong Yu, and James Kwok. survey on time-series pre-trained models. IEEE Transactions on Knowledge and Data Engineering, 2024. [24] Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, et al. Transformers can do arithmetic with the right embeddings. arXiv preprint arXiv:2405.17399, 2024. [25] Sean McLeish, Avi Schwarzschild, and Tom Goldstein. Benchmarking chatgpt on algorithmic reasoning. arXiv preprint arXiv:2404.03441, 2024. [26] Kazem Meidani, Parshin Shojaee, Chandan Reddy, and Amir Barati Farimani. Snip: Bridging mathematical symbolic and numeric realms with unified pre-training. arXiv preprint arXiv:2310.02227, 2023. [27] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:1735917372, 2022. [28] Mike Merrill, Mingtian Tan, Vinayak Gupta, Tom Hartvigsen, and Tim Althoff. Language models still struggle to zero-shot reason about time series. arXiv preprint arXiv:2404.11757, 2024. [29] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023. [30] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021. [31] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. [32] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. An overview of early vision in inceptionv1. Distill, 5(4):e00024002, 2020. [33] Bruno Olshausen and David Field. Sparse coding with an overcomplete basis set: strategy employed by v1? Vision research, 37(23):33113325, 1997. [34] Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 14701480, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/ P15-1142. URL https://aclanthology.org/P15-1142. [35] Yasaman Razeghi, Robert Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022. [36] Maria Sahakyan, Zeyar Aung, and Talal Rahwan. Explainable artificial intelligence for tabular data: survey. IEEE access, 9:135392135422, 2021. [37] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019. 13 [38] Ruoqi Shen, Sebastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, and Yi Zhang. Positional description matters for transformers arithmetic. arXiv preprint arXiv:2311.14737, 2023. [39] Jasivan Alex Sivakumar and Nafise Sadat Moosavi. How to leverage digit embeddings to represent numbers? arXiv preprint arXiv:2407.00894, 2024. [40] Dhanasekar Sundararaman, Shijing Si, Vivek Subramanian, Guoyin Wang, Devamanyu Hazarika, and Lawrence Carin. Methods for numeracy-preserving word embeddings. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 47424753, 2020. [41] Mingtian Tan, Mike Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are language models actually useful for time series forecasting? arXiv preprint arXiv:2406.16964, 2024. [42] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547, 2020. [43] Avijit Thawani, Jay Pujara, Pedro Szekely, and Filip Ilievski. Representing numbers in nlp: survey and vision. arXiv preprint arXiv:2103.13136, 2021. [44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [45] Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, and Huajun Chen. Knowledge circuits in pretrained transformers. arXiv preprint arXiv:2405.17969, 2024. [46] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [47] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? study in length generalization. arXiv preprint arXiv:2310.16028, 2023. [48] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series analysis by pretrained lm. Advances in neural information processing systems, 36:4332243355, 2023. [49] Tianyi Zhou, Deqing Fu, Vatsal Sharan, and Robin Jia. Pre-trained large language models use fourier features to compute addition. arXiv preprint arXiv:2406.03445, 2024. [50] Zhejian Zhou, Jiayu Wang, Dahua Lin, and Kai Chen. Scaling behavior for large language models regarding numeral systems: An example using pythia. arXiv preprint arXiv:2409.17391, 2024."
        },
        {
            "title": "Appendix",
            "content": "In Appendix B, we present the results of the binary classification task. Roadmap In Appendix A, we provide the detailed algorithm for computing the final loss and making number predictions. In Appendix C, we provide the preliminaries and the missing proofs from the main paper. In Appendix D, we offer additional evidence of the existence of Fourier features in pre-trained LLMs. In Appendix E, we demonstrate the ability of Transformers to solve long-sequence addition using FoNE. In Appendix F, we show how FoNE, combined with Abacus embedding, improves the results. In Appendix G, we provide additional experimental settings that were missing from the main paper. In Appendix H, we show that our method produces similar results on the GPT-2 Large model. In Appendix I, we present our methods performance on the R2 metric. Fourier Number Final Loss & Prediction In this section, we provide the detail algorithm of how we get the final loss and final prediction as defined in Section 3.1. Algorithm 3 Fourier Number Final Loss & Prediction 1: function FourierNumberFinalLoss(h, y, m, n) 2: digitLoss FourierNumberLossFunction(h, y, i) totalLoss totalLoss + digitLoss totalLoss 0 [m + n] for do end for finalLoss totalLoss return finalLoss 9: 10: end function 11: function FourierNumberFinalPrediction(h, m, n) 12: (cid:98)y 0 Ifrac [0, . . . , 1] Iint [n, . . . , + 1] for Ifrac do logitsi (cid:2)h[2i], h[2i + 1](cid:3) (cid:2)ϕ(j, 10)(cid:3) (cid:98)yi arg maxj{0,...,9} logitsi[j] (cid:98)y (cid:98)y + (cid:98)yi 10(ni) end for for Iint do logitsj (cid:2)h[2j], h[2j + 1](cid:3) (cid:2)ϕ(j, 10)(cid:3) (cid:98)yj arg maxj{0,...,9} logitsj[j] (cid:98)y (cid:98)y + (cid:98)yj 10jn j=0,...,9 24: end for return (cid:98)y 25: 26: end function 3: 4: 5: 6: 7: 8: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: Compute average loss Average over all digit positions Compute final prediction Fractional digit indices Integer digit indices j=0,...,9 Scale fractional part by 10(ni) Scale integer part by 10j"
        },
        {
            "title": "B FoNE on Binary Classification Task",
            "content": "In this section, we demonstrate that FoNE outperforms other methods on binary classification tasks, benefiting from its precise representation. Dataset Each example in the dataset is formatted as [num1,num2,num3], where the integers num1, num2, and num3 are sorted in ascending order (num1 num2 num3) to ensure uniqueness and eliminate duplicate representations of the same combination. The integers are uniformly sampled from the range [0, 1000]. The label for each example is determined by evaluating the linear equation num1 + num2 + num3 d, using predefined coefficients = 1.5, = 2, = 0.5, and = 10 and = 1.5, = 2, = 0.5, and = 190. If the result is greater than zero, the label is assigned as 1; otherwise, it is assigned as 0. The dataset is divided into training, validation, and test subsets, as outlined in Table 4. (a) Accuracy vs. Training Data Size (b) Accuracy vs. Model Size Figure 5: We train Llama-3.2-1B from scratch with random initialization using different number embedding methods on number classification where = 10. The test accuracy is compared across varying data sizes and model sizes. 16 (a) Accuracy vs. Training Data Size (b) Accuracy vs. Model Size Figure 6: We train Llama-3.2-1B from scratch with random initialization using different number embedding methods on number classification where = 190. The test accuracy is compared across varying data sizes and model sizes."
        },
        {
            "title": "C Preliminaries and Missing Proof",
            "content": "C.1 Preliminaries In this section, we provide the necessary mathematical definitions and concepts used throughout the paper. Period and Frequency. function (x) is periodic with period > 0 if (x + ) = (x) for all x. The period represents the smallest positive value for which the function repeats. The frequency of periodic function is the reciprocal of its period, = 1 , and describes the number of cycles completed in one unit interval. For the sine and cosine functions cos(cid:0) 2π x(cid:1), the period is . x(cid:1) and sin(cid:0) 2π Unit Circle. The unit circle is the set of points in the plane at distance of 1 from the origin, given by x2 + y2 = 1. The coordinates of points on the unit circle can be parameterized as (cos θ, sin θ), where θ is the angle measured counterclockwise from the positive x-axis. For any angle θ, cos θ represents the x-coordinate, and sin θ represents the y-coordinate. Two-Argument Inverse Tangent. The two-argument inverse tangent function, atan2(y, x), determines the angle θ (modulo 2π) given the coordinates (x, y) = (cos θ, sin θ). Specifically, which resolves the angle θ uniquely based on the signs of and y. θ = atan2(y, x), Modular Arithmetic. Modular arithmetic considers equivalence classes of numbers under modulus > 0. For integers and b, (mod ) if (a b), meaning and differ by an integer multiple of . Fourier Representation. Periodic functions with period can be represented using the fundamental x(cid:1)(cid:1) capture the periodicity of modulo . For example, the embeddings (cid:0)cos(cid:0) 2π frequencies 2π by mapping it to unique point on the unit circle. x(cid:1), sin(cid:0) 2π 17 C.2 Missing Proof In this section, we provide some missing proofs. Lemma C.1 (Formal version of Lemma 3.3). Given the pair (cid:0)cos( 2π . x), sin( 2π x)(cid:1), we can recover mod Proof. Let θ = 2π x. Then the given pair becomes (cid:0)cos(θ), sin(θ)(cid:1). From this pair, one can recover θ uniquely modulo 2π. Concretely, θ can be obtained (modulo 2π) using the two-argument inverse tangent function: Since θ = 2π x, we have θ atan2(cid:0)sin(θ), cos(θ)(cid:1) (mod 2π). = 2π θ. Hence is determined up to integer multiples of , i.e., mod . In other words, if (cid:0)cos( 2π x1)(cid:1) = (cid:0)cos( 2π x2 (mod 2π), which implies x1 x2 (mod ). Therefore, from the pair (cid:0)cos( 2π x2), sin( 2π x1), sin( 2π x2)(cid:1), x), sin( 2π x)(cid:1), x1 2π then 2π we can indeed recover mod . Lemma C.2 (Layer-Normalized FoNE Preserves Numeracy). Given numbers Layer-Normalized Fourier Number Embedding LN(FoNE(x) + p), where FoNE(x) is the Fourier Number Embedding of and is an orthogonal positional encoding vector, assume the mean of FoNE(x) + is 0. Let be the integer digit length of and be the decimal digit length of x. Then, using Lemma 3.3, we can recover mod 10i for each integer in the range + 1 to m. Proof. Assume the mean of = FoNE(x) + is 0, i.e., µ = 0. Under this assumption, LayerNorm simplifies to: LN(x) = σ , where σ is the standard deviation of x. Let = FoNE(x) encode the scalar x, and let be an orthogonal positional encoding vector such that: = = and = 0. Then, the input to LayerNorm is: = + p. The standard deviation σ of is given by: σ = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) i=1 (xi µ)2, where is the dimensionality of x. Since µ = 0, this simplifies to: σ = (cid:114) 1 x2. 18 Substitute = + p: By orthogonality and unit norm, = 0, u2 = 1, and p2 = 1. Thus: x2 = + p2 = u2 + p2 + 2u p. Therefore: x2 = 1 + 1 + 0 = 2. σ = (cid:114) 1 2 = (cid:114) 2 . The LayerNorm operation simplifies to: LN(x) = σ = + (cid:113) 2 (cid:114) 2 = (u + p). This rescales and by factor of (cid:113) 2 . The key observation is that LayerNorm applies uniform scaling to all components of x. Since and are orthogonal and their relative directions are preserved, the numerical relationships encoded in (which represent x) are preserved up to scaling factor. By Lemma 3.3, the numeracy of is preserved. This means we can recover mod 10i for all in the range + 1 m, as the normalized embedding retains the necessary information about x. The same result holds for RMSNorm because it also applies uniform scaling (based on the root mean square of the input) while preserving the relative directions of the embedding components, thus maintaining the numeracy of x."
        },
        {
            "title": "D More evidence about Fourier features",
            "content": "D.1 Emergence of Fourier Features during Pre-training We follow Zhou et al. [49] and conduct the same Fourier analysis on Pythia model. In Figure 7, we show how Pythia gradually learns the Fourier features during pre-training. With different model size, the model gradually learn the same frequency components. Figure 7: Fourier analysis of the Pythia models number embeddings across pre-training checkpoints. The figure illustrates how the Fourier features are progressively learned during pre-training, showing the emergence of specific frequency components. Models of varying sizes exhibit similar trend, gradually learning the same frequency components over time. We extend the work of Zhou et al. [49] to other pre-trained LLMs and observe similar findings: pre-trained LLMs, regardless of the dataset used, tend to learn the same outlier frequency components. 20 (a) pre-trained Pythia (b) fine-tuned Llama3.2 (c) pre-trained OPT (d) pre-trained GPT2 Figure 8: Number embedding in Fourier space for different pre-trained models. 21 FoNE for 60-digit Integer Addition in One Forward Pass Figure 9: Accuracy of an 8-layer transformer on 60-digit addition tasks, illustrating the effectiveness of FoNE embeddings in handling long sequences. The model achieves an average accuracy of 97.42% across different operand lengths, showcasing its capability in numerical precision and sequence representation. As discussed in Section 5, the maximum digit length that float64 data type can precisely represent is 15 digits. Consequently, even if we convert numbers to float64 and then back to float16 to match the model weight precisionm it still introduce numerical inaccuracies when the input exceeds 15 digits. To mitigate this issue, we process by dividing it into smaller chunks, allowing the FoNE to operate effectively without precision loss. Specifically, is split into groups of five digits, and FoNE is applied independently to each chunk. Each digit within chunk is encoded into two dimensions, resulting in an embedding of length 10 per chunk. These chunk embeddings are then concatenated to form the final representation of x. This method ensures that even for long inputs, the FoNE still preserve the numeracy of the numbers. We adopt the data generation approach from [24], which includes all combinations of operand lengths (i, j) up to maximum length k, generating 20 million stratified samples to ensure balanced representation across all length pairs. Training is conducted using language model cramming approach [12], constrained to 8 exaFLOP (equivalent to 24 hours of training on single Nvidia RTX A6000 GPU). Using this strategy, as depicted in Figure 4(a), an 8-layer transformer trained on 60-digit addition achieves an average accuracy of 97.42% across various operand lengths in just one forward pass. This result underscores the effectiveness of the FoNE in processing long numbers with high precision and computational efficiency in just one forward pass."
        },
        {
            "title": "F Combine FoNE with Abacus",
            "content": "We train decoder-only causal language models to solve arithmetic problems, following the setup described in McLeish et al. [24]. Inputs are formatted in least-significant-digit-first order (e.g., 98282 + 3859172 = 2787472), without padding between digits or operands. The training dataset includes all combinations of operand lengths (i, j) up to maximum length k, with 20 million stratified samples ensuring balanced representation across all length pairs. For input representation, we combine Fourier Number Embeddings (FoNE) with the Abacus method [24]. That each digit is embedded with FoNE. Training is conducted using language model cramming 22 approach [12], constrained to 8 exaFLOP (equivalent to 24 hours of training on single Nvidia RTX A6000 GPU). We train and evaluate the models across three runs, each with different random seed, as shown in Figure 10. Results indicate that incorporating FoNE enables the Abacus method to achieve better generalization and higher accuracy. Figure 10: Heatmaps of accuracy percentages for FoNE+Abacus (left column) and Abacus (right column) across three different random seeds. Each heatmap represents accuracy as function of the first and second number lengths, with lighter blue shades indicating higher accuracy. The color scale ranges from white (low accuracy) to blue (high accuracy). These visualizations highlight FoNE can combine with Abacus to improve performance."
        },
        {
            "title": "G Experiment Setting",
            "content": "In this section, we provide the experiments settings that we used in the Section 4.1. Learning rates were determined through an extensive search, with the best rates selected separately for each method based on validation performance. Final training hyperparameters include learning rate of 0.005 for regular and FoNE methods, and 0.0001 for the xVal method, batch size of 512, and 100 epochs. Dataset 6-digit decimal addition 6-digit integer addition 5-digit integer subtract 3-digit integer multiplication 4-digit integer multiplication classification Train Size Validation Size Test Size 80,000 80,000 80,000 40,000 80,000 80,00 720,000 720,000 720,000 360,000 720,000 720,00 200,000 200,000 200,000 100,000 200,000 200, Table 4: Dataset Sizes for Training, Testing, and Validation Dataset 6-digit decimal addition 6-digit integer addition 5-digit integer subtract 3-digit integer multiplication 4-digit integer multiplication 4-digit integer multiplication classification Model Size for Varying Data Size Data Size for Varying Model Size 37.55M 37.55M 37.55M 37.55M 37.55M 37.55M 37.55M 200,000 200,000 200,000 360,000 360,000 360,000 50,000 Table 5: Dataset and Configuration Sizes for Model and Data Variation Experiments Model Hidden Size Intermediate Size # Hidden Layers # Attention Heads # Key-Value Heads 1 2 3 4 5 6 64 128 192 256 320 384 256 512 768 1024 1280 1536 1 2 3 4 5 4 4 6 8 8 8 Table 6: Model Configuration Table 2 2 3 4 4 4 G.1 Ablation Study In this section, we present the mispredictions of the model trained with an FoNE, where the periods are multiples of 5 instead of 10. Table 7 demonstrates that, for each digit, the mispredictions consistently deviate from the true labels by 5. We also present the models mispredictions in Table 8, where each digit is encoded into separate dimension of the embedding. For example, the number 567 is represented as [5, 6, 7]. During training, we compute the RMSE loss between the last hidden states and the labels. During prediction, we interpret each entry in the last hidden state as single digit. 25 Table 7: Mispredictions in the Final Evaluation with when we embed each digit with only mod5. Index Predicted Value Actual Value 1 2 3 4 5 934.03 3.009 912.311 6201.003 1240.34 934.585 558.509 917.366 1756.008 1290.84 Table 8: Mispredictions in the Final Evaluation when directly encoding numbers into their embeddings. Index Predicted Value Actual Value 1 2 3 4 5 10 883.888 787.878 888.758 748.785 677.677 1179.488 993.999 898.989 989.759 849.895 688.788 1189.499 Similar Results on GPT2-Large Based Experiments We conduct the same experiments on decimal addition using GPT-2 Large-based model. The results indicate that changing the model architecture does not affect the outcomes. For instance, GPT-2 Large employs LayerNorm, while Llama 3.2 uses RMSNorm. (a) 6-digit decimal addition: Accuracy vs. Training Data Size (b) 6-digit decimal addition: Accuracy vs. Model Size Figure 11: We train GPT2-Large from scratch with random initialization using different number embedding methods on 6-digit decimal addition. The test accuracy is compared across varying data sizes and model sizes. 26 R2 comparison xVal [13] performs well on the R2 metric R2 = 1 (cid:80)n (cid:80)n i=1(yi (cid:98)yi)2 i=1(yi y)2 , because it uses RMSE as its loss function. However, we demonstrate that FoNE outperforms xVal on R2 in most tasks. We show the final R2 on test dataset in our experiments(Section 4.2). (a) Data size vs. Accuracy (b) Model size vs. Accuracy Figure 12: Comparison of R2 trends for 6-digit decimal addition with respect to model size and data size. (a) 6-digit integer addition: Model&Data size vs. Accuracy (b) 5-digit integer addition: Model&Data size vs. Accuracy (c) 5-digit integer subtraction: Model&Data size vs. Accuracy (d) 3-digit integer multiplication: Model&Data size vs. Accuracy Figure 13: Comparison of R2 trends for various arithmetic tasks with respect to model size and data size."
        }
    ],
    "affiliations": [
        "Department of Computer Science University of Southern California"
    ]
}