{
    "paper_title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "authors": [
        "Chengzhuo Tong",
        "Mingkun Chang",
        "Shenglong Zhang",
        "Yuran Wang",
        "Cheng Liang",
        "Zhizheng Zhao",
        "Ruichuan An",
        "Bohan Zeng",
        "Yang Shi",
        "Yifan Dai",
        "Ziming Zhao",
        "Guanbin Li",
        "Pengfei Wan",
        "Yuanxing Zhang",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 1 6 0 0 1 . 1 0 6 2 : r CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation Chengzhuo Tong1,2,, Mingkun Chang3,, Shenglong Zhang4, Yuran Wang1,2, Cheng Liang2,5 Zhizheng Zhao1, Ruichuan An1, Bohan Zeng1,2, Yang Shi1,2, Yifan Dai2, Ziming Zhao4 Guanbin Li3, Pengfei Wan2, Yuanxing Zhang2, Wentao Zhang1, Peking University1 Kling Team, Kuaishou Technology2 Sun Yat-sen University3 Zhejiang University4 Nanjing University5 Project Page: https://cof-t2i.github.io"
        },
        {
            "title": "Abstract",
            "content": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frameby-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of clearly defined visual reasoning starting point and interpretable intermediate states in T2I generation process. To bridge this gap, we propose CoF-T2I, model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such explicit generation process, we curate CoF-Evol-Instruct, dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation. 1. Introduction Recent advances in video generation models [11, 21, 29, 39] have demonstrated the emergence of zero-shot reasoning behaviors, dubbed Chain-of-Frame (CoF) reasoning [41]. CoF reasoning leverages frame-by-frame generation to iteratively refine scenes, thereby enabling visual inference and Equal Contribution Corresponding Author Figure 1. Comparison of Inference-time Reasoning Models. (a) Equipping image models with external verifier. (b) Interleaving textual planning within unified multimodal large language models. (c) CoF-T2I: Our proposed video-based CoF reasoning model. unlocking new capabilities in perception, modeling, and manipulation. Benefiting from this, video models such as Veo3 [13] and Sora2 [33] have been further extended to range of downstream visual tasks (e.g., maze solving, visual puzzles) [41]. Concurrently, the frontier of text-to-image (T2I) [8, 32, 43, 45] generation has shifted toward inference-time reasoning, primarily realized through either employing additional multimodal verifiers to assess image quality [25, 48] or interleaving textual planning within unified multimodal large language models (MLLMs) [4, 9, 15, 18, 19, 45]. However, these existing reasoning paradigms face two key Figure 2. Visualizations of CoF-T2I output. We visualize the reasoning trajectories generated by CoF-T2I. For each example, the final output is shown in large, and the intermediate latent frames are shown in small. limitations: first, they rely on frequent modality switching between vision and language (or scalar rewards), making pixel-level correctionsindirect and lossy; second, unified MLLMs lack pretraining on large-scale, purely visual, causally ordered refinement sequences, limiting faithful frame-wise self-correction. In contrast, video models naturally model the evolution of visual states and refine scenes frame by frame under strong spatiotemporal prior, making them particularly well-suited as pure visual reasoners to assist high-quality image generation. Still, their potential for enhancing T2I generation remains largely underexplored, primarily due to the lack of clear visual reasoning starting point and interpretable intermediate states. Motivated by this, we raise fundamental question: Can we use video models as pure visual reasoners to guide highquality text-to-image generation? To this end, we propose CoF-T2I, model that harnesses the CoF reasoning capability of pretrained video models to enhance T2I generation. Built on video generation backbone, CoF-T2I reimagines T2I generation as an explicit visual reasoning process. Concretely, given text prompt, the model generates compact three-frame sequence, where each frame represents distinct and progressive reasoning step, from coarse initial layout to an intermediate refinement, culminating in high-fidelity final image. To further boost generation quality while suppressing motion artifacts inherent in video backbones, we introduce framewise representation mechanism that allows the models native video VAE to encode and decode each frame separately, ensuring maximal fidelity. During inference, only the final frame undergoes full decoding and is used as the output image. We present qualitative comparison between CoF-T2I and other inference-time reasoning models in Figure 1. Internalizing this generation paradigm within video models necessitates vast amounts of structured visual reasoning chains, which are largely absent from existing datasets. To fill this gap, we develop scalable data curation pipeline and introduce CoF-Evol-Instruct, highquality dataset comprising 64K CoF sequences that explicitly capture the full T2I generation process from initial semantic composition to final aesthetic refinement. Our pipeline is carefully designed to produce diverse yet consistently progressive refinement trajectories, providing clear, defect-aware supervision that guides the model from coarse, semantically noisy drafts toward refined, aesthetically coherent final outputs. Together, this dataset and curation pipeline enable effective training of CoF-T2I, allowing the model to internalize strong visual reasoning capabilities. Experimental results show that CoF-T2I significantly outperforms the base video model and achieves strong performance on challenging benchmarks, attaining competitive score of 0.86 on GenEval and 7.468 on Imagine-Bench. These results highlight the substantial promise of leveraging video foundation models intrinsic CoF reasoning to advance high-quality T2I generation. The visualization output of CoF-T2I is shown in Figure 2. In summary, our core contributions are as follows: novel generation paradigm: We propose CoF-T2I, text-to-image model that repurposes video foundation model as pure visual reasoner, generating images via CoF reasoning process. comprehensive dataset with scalable pipeline: We introduce CoF-Evol-Instruct, 64K-scale dataset of progressive visual refinement trajectories, built with scalable quality-aware pipeline. Competitive results with extensive validation: Our extensive experiments show that CoF-T2I substantially outperforms its video backbone and achieves competitive performance on challenging benchmarks, with additional validations confirming its substantial promise. 2. Methodology In Section 2.1, we introduce the preliminaries of our work. In Section 2.2, we elaborate on the proposed CoF-T2I. In Section 2.3, we present our CoF-Evol-Instruct dataset as well as detailed breakdown of the construction pipeline. 2.1. Preliminary We adopt the Wan2.1 VAE [39] to obtain latent representations, which applies causal spatiotemporal compression Figure 3. Overview of CoF-T2I. CoF-T2I builds on video generation backbone, reframing inference-time reasoning for T2I generation as CoF refinement process. Training. Given CoF trajectory, we employ video VAE to encode each frame, and optimize vanilla flow matching objective. Inference. Starting from noisy initialization, the model denoises to sample progressively refined reasoning trajectory internalized during training, only the final-frame latent is fully decoded and taken as the output image. Quality assessment. Along the CoF trajectory, text-image alignment and aesthetic quality continue to improve. to raw video frames RF 3HW . The first frame is compressed spatially only, while subsequent chunks are conditioned on prior latents for joint spatiotemporal compression. This yields latent representation with 8 spatial downsampling and 4 temporal downsampling, resulting in = 1 4 + 1, spatial dimensions = H/8, = W/8, and channel dimension = 16. Our model adopts Rectified Flow [1, 26, 28] to model straight path from noise (x0 (0, I)) to complex data distribution (x1 = E(V )) by learning velocity field. The interpolated point xt = (1 t)x0 + tx1 at timestep [0, 1] is used to train scheduler Fθ(xt, t; y, c) to predict the direction vector from xt towards x1, i.e., the velocity field (x1 x0). Its training loss minimizes: Lθ = Etp(t),V pdata,x0N (0,I) (cid:2)Fθ(xt, t; y, c) (x1 x0)2 2 (cid:3) (1) where represents textual conditions and represents visual conditions. This direct path learning enables highquality and efficient generation. 2.2. Image Generation with Video Models Overview. We propose CoF-T2I, text-to-image foundation model built upon video generation backbone, as illustrated in Figure 3. We first introduce how video models can be leveraged as CoF visual reasoners for T2I generation, then describe the frame-wise latent representation with causal VAE, and finally detail the training and inference procedure of CoF-T2I. Video Models as Visual Reasoners. Video foundation models are inherently powerful visual learners and reasoners [41], equipped with the natural ability to perform inference-time reasoning through Chain-of-Frame. To harness this paradigm from robust video backbone, CoF-T2I redefines the T2I generation process as structured refinement sequence, enabling frame-wise visual refinement to enhance T2I generation. Formally, given text prompt p, our method guides the video backbone to produce sequence of latent representations z1:3 = {z1, z2, z3}. This sequence embodies the CoF logic, evolving from coarse semantics to fine-grained aesthetics. The model achieves this by learning the joint Figure 4. Curation Pipeline for CoF-Evol-Instruct. quality-aware construction pipeline to curate reasoning data. We generate an initial pool of images across diverse distributions and dynamically route valid samples. These images are then expanded into complete CoF sequences through targeted construction strategies. Our pipeline ensures both sample-level diversity and frame-wise consistency. probability distribution of the entire latent trajectory, conditioned by the input prompt: z1:3 pθ(Z1:3p), (2) Here, pθ is the probability density over latent sequences learned by the video model. Only the terminal latent state z3, which encapsulates the culmination of the refinement, is projected into the visual space using the decoder of the native causal VAE, yielding the output image: ˆI = F3 = D(z3). (3) In this way, the video model iteratively corrects artifacts and enriches details along the logical CoF trajectory, leveraging its sequence-processing architecture without extra textual planning or feedback signals. Frame-wise Latent Representation. CoF-T2I adopts the video VAE from Wan2.1 to represent frames in compact latent space. However, the native spatial-temporal compression of VAE may introduce undesired motion artifacts (e.g. implicit flow, dynamic inconsistencies). To mitigate this, we employ frame-wise representation that encodes each frame independently in the latent space: We slide the VAE along the temporal axis of the video and align the input such that the target frame is always compressed within the initial context window, which spans exactly one frame. This design restricts the encoding unit to single-frame granularity, thereby preserving the spatial independence of visual features while enabling high-fidelity compression. Training and Inference. During training, the model is supervised to internalize the frame-wise visual reasoning, leveraging standard flow matching objectives [28]. Concretely, the model predicts the denoising targets for the latent sequence z1:3 = {z1, z2, z3} corresponding to the CoF sequence {F1, F2, F3} and learns to produce later frames that refine earlier ones through end-to-end optimization. At inference, we generate the full latent sequence starting from Gaussian noise, effectively recovering the internalized generative reasoning via multi-step denoising. Importantly, only the final latent is fully decoded and taken as the final output image ˆI, while intermediate latents serve solely as internal states for visual reasoning. 2.3. CoF-Evol-Instruct Constructing diverse CoF trajectories is essential for training video models to perform visual reasoning in T2I tasks, as it allows models to anticipate potential semantic defects and progressively refine aesthetic details on semantically sound foundation. Below, we first discuss the necessity of constructing this dataset in Section 2.3.1, followed by detailed description of our quality-aware generation pipeline in Section 2.3.2. 2.3.1. Necessity of Dataset Construction Training CoF-T2I requires supervision that is both progressive and consistent, reflecting step-wise refinement process. However, existing T2I datasets [10, 47] typically provide single target images, which are insufficient to support Figure 5. Visualization of CoF-Evol-Instruct Dataset. We showcase the prompt and corresponding CoF trajectories in our data, including five categories: Attribute Binding, Object Combination, Spatial Arrangement, Context Manipulation, and Quantity Control. the learning of intermediate supervision. Although image editing datasets [17] or multi-step reasoning datasets [44] can be leveraged to construct longer visual generation sequences, they typically suffer from low quality (e.g. intermediate degradation, abrupt transitions), thus fail to meet the requirements for strict progressive refinement. Intuitively, high-quality image generation follows progressive optimization process: first establishing accurate semantic layout and object placement, followed by the enhancement of aesthetic details and textural fidelity. Motivated by this, we introduce CoF-Evol-Instruct, dataset of three-frame reasoning chains with explicit stage separation and defect-aware construction. In our exploration, we choose fixed length of three, as it effectively preserves the two key refinement stages (semantic correction and aesthetic improvement) while ensuring consistent causal progression from defective initial draft to high-quality final image. This design provides scalable and structured supervision for long-horizon visual refinement in T2I generation. 2.3.2. Quality-Aware Generation Pipeline To generate CoF-Evol-Instruct at scale Overview. while maintaining both sample-level diversity and framewise consistency, we propose quality-aware generation pipeline. Our pipeline begins by sampling diverse set of images from multiple T2I models spanning different capability tiers. Each image is then assessed for quality and dynamically routed to one of three construction strategies, which subsequently expand it into complete three-frame CoF sequence: (i) forward refinement, (ii) bidirectional completion, and (iii) backward synthesis. An overview of the proposed pipeline is shown in Figure 4. Multi-model Sampling. To ensure diverse distribution of CoF sequences, we assemble large collection of text prompts from multiple sources, covering broad spectrum of scenes, objects, and styles. This includes 24K prompts from [27], 37K prompts from [47], and 12K self-generated prompts adapted from [12]. After prompt-level deduplication, we retain 68K unique prompts. Using these prompts, we generate anchor images (i.e. the initial images) across several T2I models spanning different capability tiers. In particular, we employ Wan2.1 [39] as weak tier, QwenImage [43] as medium tier, and Nano-Banana [36] as strong tier. To balance quality coverage and avoid being dominated by any single tier, we sample model tier for each prompt with probabilities pweak = 0.25, pmedium = 0.5, pstrong = 0.25, yielding 68K anchor images in total. As result, we obtain heterogeneous pool of images that naturally span the coarse-to-fine quality spectrum. Leveraging this diversity in quality levels, we then classify each anchor image to determine the most suitable construction route for expanding it into full CoF sequence. Quality-based Routing. To exploit this quality diversity, we employ Qwen3-VL-8B [2] as the quality assessor, which categorizes anchors into three stages based on semantic alignment and aesthetic coherence, producing three-way classification: Semantically Misaligned (F1) for semantically misaligned images regardless of aesthetic performance, Visually Unrefined (F2) for semantically accurate but visually unrefined images, and High Fidelity (F3) for images achieving both high semantic accuracy and aesthetic coherence. Each classified anchor is then routed to the corresponding construction strategy and expanded into complete three-frame sequence using the unified editing primitive. This adaptive routing mechanism ensures optimal starting points for samples across quality spectrum, thereby maximizing dataset coverage and fully utilizing anchors across all quality levels. Unified Editing Primitive. To construct complete CoF sequences from routed anchors while ensuring cross-frame consistency and stage-specific refinement, we introduce unified editing primitive (UEP) as the shared minimal operation across all strategies. UEP performs controlled, targeted edits for each stage transition (e.g., semantic grounding, aesthetic refinement) while strictly preserving nontarget content. We assign each prompt one of five semantic categories: Attribute Binding, Object Combination, Quantity Control, Spatial Arrangement, and Context Manipulation. Conditioning UEP on the category label narrows editing intent and improves controllability. UEP is implemented as closed-loop system with three agents: planner and verifier powered by Qwen3-VL32B [2], and an editor powered by Qwen-Image-Edit2509 [43]. Given the current frame, target stage, prompt, transition direction, and category label, the planner generates minimal editing instruction. The editor applies it while preserving global content. The verifier then evaluates the result against the prompt, target stage, and instruction, outputting binary success signal {0, 1}. If failed (i.e. = 0), we retry up to = 3 times to maintain throughput. This category-conditioned primitive standardizes execution across forward, bidirectional, and backward construction, ensuring prompt-aligned and causally consistent reasoning sequences. Adaptive Sequence Completion. Given anchor images with varying quality, we leverage the proposed UEP and apply three construction strategies to expand each routed anchor into complete {F1, F2, F3} sequence, ensuring progressive refinement regardless of its starting quality stage: Forward Refinement (F1 F2 F3). For semantically misaligned anchors (F1), which often exhibit semantic defects such as missing objects, incorrect attribute bindings, or implausible spatial layouts. Starting from these coarse generations, we first apply semantic correction via UEP to obtain prompt-aligned F2. Then, with the semantic layout fixed, we conduct aesthetic enhancement to produce the final F3, improving overall fidelity and lighting coherence. Bidirectional Completion (F1 F2 F3). For visually unrefined anchors (F2), which are typically semantically grounded yet lack fine-grained aesthetics. From these intermediates, we expand bidirectionally: Backward via controlled semantic degradation (e.g., weakening attributes, dropping secondary objects), yielding plausible draft state F1 while preserving global context. Forward via aesthetic refinement to obtain F3, focusing on high-frequency details and visual harmony. Backward Synthesis (F1 F2 F3). For high-fidelity anchors (F3), which represent fully refined and aesthetically superior outputs, we reconstruct backward: first apply aesthetic simplification (reduced sharpness and lighting complexity) to create semantically grounded F2; then introduce minimal, categoryconditioned semantic perturbation (e.g., altering count, degrading attributes) to synthesize coarse F1. All backward steps are executed and verified with UEP for prompt consistency and inter-frame coherence. Curation and Illustration. We apply the proposed pipeline to pre-curated prompts to generate candidate reasoning chains, each paired with its corresponding prompt. After filtering out failed or incomplete samples, the curated collection, named CoF-Evol-Instruct, contains 64K high-quality CoF sequences. Representative examples spanning the five categories are presented in Figure 5. The resulting dataset delivers high-quality, diverse, and progressively structured reasoning supervision that is critical for effectively training CoF-T2I. Further details on the dataset construction process are provided in Appendix B. 3. Experiments 3.1. Experimental Setting Evaluation. We evaluate our proposed CoF-T2I on GenEval [12] and Imagine-Bench [47]. GenEval is widely-used benchmark for object-centric prompt following, focusing on composition, counting, attribute binding, and spatial relations, with automatically verifiable criteria for reliable comparison. As complementary setting, Imagine-Bench stresses imaginative prompts that require controlled concept transformations and compositional reaTable 1. Performance comparison on GenEval. The best and the second best Overall scores are in bold and underlined, respectively. Model Single Obj. Two Obj. Counting Colors Position Color Attr. Overall SDXL [34] SD3-Medium [8] FLUX.1-dev [22] Janus-Pro-7B [4] BLIP3-o 8B [3] OmniGen2 [44] BAGEL [6] BAGEL-Think [6] T2I-R1 [18] Wan2.1-T2V-14B [39] CoF-T2I (Ours) 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.92 0.98 Standard Image Models 0.39 0.72 0.61 Unified MLLMs 0.59 0.77 0.81 0.81 0.53 Video Models 0.57 0.83 0.85 0.89 0. 0.90 0.90 0.88 0.88 0.91 0.69 0.89 0.74 0.94 0.88 0.89 0.92 0.94 0.94 0.91 0.63 0.95 0.15 0.33 0. 0.79 0.82 0.64 0.64 0.76 0.18 0.83 0.23 0.60 0.55 0.66 0.70 0.63 0.63 0.65 0.31 0.71 0.55 0.74 0. 0.80 0.84 0.80 0.78 0.82 0.79 0.55 0.86 soning (e.g., attribute shifts and hybrid concepts), probing more abstract, concept-level semantics that go beyond literal object-centric instructions. For both benchmarks, we follow the official evaluation protocols and report the overall and category-wise scores. Training Details. CoF-T2I is initialized from the powerful video foundation model Wan2.1-T2V-14B and fine-tuned on our curated CoF-Evol-Instruct 64K dataset for 1,800 steps, utilizing batch size of 64, learning rate of 1e-5, and weight decay of 1e-2. Following the strategy described in Section 2.2, we freeze the VAE and encode each frame of the sequence independently, updating only the unfrozen DiT parameters. Additionally, to mitigate the issue of incomplete subject rendering that is often encountered with default rectangular video aspect ratios (e.g., 480 832), we standardize our pipeline by resizing all data to 1024 1024 during training and consistently maintaining this square resolution for inference generation. 3.2. Main Results We report quantitative results in Table 1 and Table 6, respectively. In our evaluation, we benchmark CoF-T2I against two distinct categories of models: standard image models possessing solely generative capabilities, and unified multimodal models capable of leveraging textual Chain-ofThought (CoT) for intermediate reasoning. On GenEval, CoF-T2I achieves leading overall score of 0.86, establishing competitive performance among the compared methods. Notably, our pure visual reasoning approach outperforms strong unified baselines that rely on textual planning, surpassing BAGEL-Think by 0.04 and T2IR1 by 0.07. This superior performance demonstrates the substantial advantage of our proposed CoF-T2I in generating precise and semantically correct images. Performance on Imagine-Bench further corroborates the robustness of our model. CoF-T2I delivers remarkable improvement over the pre-trained Wan2.1 baseline, boosting the overall score from 5.939 to 7.468. Notably, the gain is especially evident under complex composition, where the MultiObject category reaches 7.797 compared with 5.383 for Wan2.1. This underscores the strength of CoF-T2I in handling imaginative instructions that require controlled concept transformations and complex compositional reasoning. Overall, by repurposing video foundation models as intrinsic visual reasoners, we demonstrate that this paradigm is not only viable but possesses immense potential. It offers promising direction for future text-to-image generation, where the video models intrinsic CoF reasoning is leveraged at inference-time to iteratively correct semantics and refine visual details, yielding higher-quality generations. 3.3. Ablation Study Does intermediate supervision yield additional benefits? To answer this question, we examine the contribution of intermediate supervision in our Chain-of-Frame reasoning training data by comparing the full CoF-T2I model with Target-Only SFT variant. For fair comparison, CoF-T2I and Target-Only SFT are trained under identical settings, including the same training hyperparameters and number of optimization steps. This variant is fine-tuned using only the final frames F3 from the CoF-Evol-Instruct dataset, with all intermediate reasoning frames removed during training. As illustrated in Table 3, the Target-Only SFT variant achieves notable improvement over the Wan2.1 base model, with the overall score rising from 0.55 to 0.81. However, it still falls short of CoF-T2I at 0.86. This disparity indicates CoFT2I benefits not merely from stronger target supervision, but also from explicitly learning the generative trajectory, Table 2. Performance comparison on Imagine-Bench. The best and the second best scores are in bold and underlined, respectively. Model Attribute shift Hybridization Multi-Object Spatiotemporal Overall SDXL [34] SD3-Medium [8] FLUX.1-dev [22] Janus-Pro-7B [4] BLIP3-o 8B [3] OmniGen2 [44] BAGEL [6] BAGEL-Think [6] T2I-R1 [18] Wan2.1-T2V-14B [39] CoF-T2I (Ours) 4.420 5.140 5.680 5.300 5.800 5.280 5.370 6.260 5.850 5.436 6.969 Standard Image Models 4.930 6.300 6. 6.730 7.060 6.290 6.500 7.740 7.360 6.950 8.070 Unified MLLMs Video Models 4.500 6.070 5.240 6.040 6.440 6.310 6.410 6.960 6. 5.383 7.797 6.320 5.910 7.130 7.280 7.080 7.450 6.930 7.130 7.700 6.237 7.287 4.970 5.780 6.060 6.220 6.510 6.220 6.200 6.930 6. 5.939 7.468 Table 3. Ablation study on core mechanisms. Wan2.1 Base refers to the Wan2.1-T2V-14B base model used for these comparative experiments. Target-only SFT fine-tunes on only the final frame of CoF-Evol-Instruct. w/o Independent VAE uses the default causal video-VAE encoding to encode the frame chain continuously. Method Single Two Obj. Counting Colors Position Color Attr. Overall 0.92 Wan2.1 Base 0.99 Target-only SFT CoF w/o Independent VAE 0.98 CoF-T2I (Ours) 0.98 0.63 0.92 0.94 0. 0.57 0.75 0.82 0.83 0.69 0.86 0.84 0.89 0.18 0.73 0.78 0.83 0.31 0.59 0.63 0.71 0.55 0.81 0.83 0.86 Table 4. Analysis of the Reasoning Trajectory. We evaluate GenEval performance across each frame of the reasoning chain. Step Single Two Obj. Counting Colors Position Color Attr. Overall Frame 1 (Draft) Frame 2 (Refine) Frame 3 (Final) 0.95 0.97 0.98 0.58 0.91 0.95 0.62 0.80 0. 0.56 0.81 0.89 0.49 0.75 0.83 0.16 0.51 0.71 0.56 0.79 0.86 which leads to superior overall performance. We observe consistent trend on Imagine-Bench as well, and report the corresponding ablation results in the Appendix C. Analysis of the Reasoning Trajectory. To quantify how CoF inference improves generation, we evaluate each intermediate frame in the three-step reasoning chain. As shown in Table 4 and Fig. 6, GenEval performance increases monotonically from the draft F1 with score of 0.56 to the refined F2 at 0.79 and further to the final F3 at 0.86, with consistent gains observed across all sub-tasks. This steady progression indicates that CoF-T2I enables iterative visual self-correction, in which semantic alignment, perceptual fidelity, and visual coherence are jointly refined at each step, leading to consistent improvements across successive frames. similar progression is also observed on Table 5. Robustness of CoF-T2I across model scales. We report GenEval overall scores and absolute gains across 1.3B and 14B scales. Wan2.1 Base refers to the Wan2.1-T2V backbone. Model Size Method Overall Score Improvement 1.3B 14B Wan2.1 Base CoF-T2I (Ours) Wan2.1 Base CoF-T2I (Ours) 0.22 0.79 0.55 0.86 - +0.57 - +0.31 Imagine-Bench, suggesting that this refinement behavior generalizes beyond object-centric evaluation settings. Detailed category-wise analyses for Imagine-Bench are provided in the Appendix C. Robustness Across Model Scales. We further examine whether CoF-T2I yields consistent benefits across varying model capacities. As shown in Table 5, applying the same CoF-T2I training recipe to Wan2.1-T2V backbones at 1.3B and 14B parameters stably improves GenEval, indicating the learned frame-evolution trajectory is effective regardless of scale. Notably, the relative gain is more pronounced on the 1.3B model, while the 14B variant still benefits substantially. These results validate the robustness of the CoF reasoning paradigm underlying CoF-T2I, demonstrating its effectiveness across diverse model configurations. Necessity of Independent Frame Encoding. We further analyze the impact of the encoding strategy in Table 3. Specifically, we compare our independent frame encoding against w/o Independent VAE variant that utilizes the default causal video VAE encoding under the same training configuration. The inferior performance of this variant with score of 0.83 suggests that treating reasoning steps as distinct and independent visual states is essential for decouFigure 6. Evolution of generation quality across reasoning steps. We visualize the progressive improvement on both GenEval (left) and Imagine-Bench (right). The results exhibit general ascending trend in performance scores across the inference steps. pling the refined output from the defective draft. In contrast, continuous encoding might introduce unnecessary temporal dependencies that entangle reasoning steps, potentially limiting the effectiveness of visual correction. The implementation details of the continuous video VAE encoding are provided in Appendix C. 4. Conclusion In this work, we introduce CoF-T2I, text-to-image foundation model that repurposes pretrained video generation backbones as pure visual reasoners. CoF-T2I performs multi-step frame evolution at inference time, progressively correcting semantic errors and refining perceptual quality to produce higher-fidelity generations without relying on textual planning. We also curate CoF-Evol-Instruct with specialized pipelines, providing step-wise supervision signals for progressive visual refinement. Extensive experiments on GenEval and Imagine-Bench demonstrate strong improvements over the base model and competitive performance on challenging prompt-following and compositional reasoning settings. We hope this work motivates future exploration of video-derived temporal reasoning mechanisms for more capable and controllable text-to-image generation."
        },
        {
            "title": "References",
            "content": "[1] Michael S. Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants, 2023. 3 [2] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025. 6, 1, 2 [3] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3o: family of fully open unified multimodal modelsarchitecture, training and dataset, 2025. 7, 8 [4] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling, 2025. 1, 7, 8 [5] Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, and Pengfei Liu. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. 1 [6] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 7, 8, [7] Hokin Deng. Video models start to solve chess, maze, sudoku, mental rotation, and raven matrices, 2025. 1 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 1, 7, 8 [9] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, and Hongsheng Li. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing, 2025. 1 [10] Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, and Hongsheng Li. Flux-reason-6m & prism-bench: millionscale text-to-image reasoning dataset and comprehensive benchmark. arXiv preprint arXiv:2509.09680, 2025. 4 [11] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, Xunsong Li, Yifu Li, Shanchuan Lin, Zhijie Lin, Jiawei Liu, Shu Liu, Xiaonan Nie, Zhiwu Qing, Yuxi Ren, Li Sun, Zhi Tian, Rui Wang, Sen Wang, Guoqiang Wei, Guohong Wu, Jie Wu, Ruiqi Xia, Fei Xiao, Xuefeng Xiao, Jiangqiao Yan, Ceyuan Yang, Jianchao Yang, Runkai Yang, Tao Yang, Yihang Yang, Zilyu Ye, Xuejiao Zeng, Yan Zeng, Heng Zhang, Yang Zhao, Xiaozheng Zheng, Peihao Zhu, Jiaxin Zou, and Feilong Zuo. Seedance 1.0: Exploring the boundaries of video generation models, 2025. 1 [12] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment, 2023. 5, [13] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, 2025. 1 [14] Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, and Pheng-Ann Heng. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark, 2025. 1 [15] Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, and PhengAnn Heng. Thinking-while-generating: Interleaving textual reasoning throughout visual generation, 2025. 1 [16] Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, and Shaohui Lin. Interleaving reasoning for better text-to-image generation, 2025. 1 [17] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing, 2024. [18] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot, 2025. 1, 7, 8 [19] Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye, Rongyao Fang, Weijia Li, Rui Liu, and Hongsheng Li. Draco: Draft as cot for text-to-image preview and rare concept generation, 2025. 1 [20] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023. 1 [21] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. 1 [22] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 7, [23] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 1 [24] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-ofthought, 2025. 1 [25] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Reflect-dit: Inference-time scaling for text-to-image diffusion transformers via in-context reflection, 2025. 1 [26] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. 3 [27] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 5 [28] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. 3, 4 [29] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. 1 [30] Yang Luo, Xuanlei Zhao, Baijiong Lin, Lingting Zhu, Liyao Tang, Yuqi Liu, Ying-Cong Chen, Shengju Qian, Xin Wang, and Yang You. V-reasonbench: Toward unified reasoning benchmark suite for video generation models, 2025. 1 [31] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, YuChuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Saining Xie. Inference-time scaling for diffusion models beyond scaling denoising steps, 2025. 1 [32] OpenAI. Hello gpt-4o, 2024. 1 [33] OpenAI. Sora 2 system card. Technical report, OpenAI, 2025. 1 [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 7, [35] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 5, 7, 8 [45] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation, 2025. 1 [46] Cheng Yang, Haiyuan Wan, Yiran Peng, Xin Cheng, Zhaoyang Yu, Jiayi Zhang, Junchi Yu, Xinlei Yu, Xiawu Zheng, Dongzhan Zhou, and Chenglin Wu. Reasoning via video: The first evaluation of video models reasoning abilities through maze-solving tasks, 2025. 1 [47] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, and Weijia Li. Echo-4o: Harnessing the power of gpt-4o synthetic images for improved image generation, 2025. 4, 5, 6 [48] Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Ziyu Guo, Haoquan Zhang, Manyuan Zhang, Jiaming Liu, Peng Gao, and Hongsheng Li. Lets verify and reinforce image generation step by step. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2866228672, 2025. [49] Zechuan Zhang, Zhenyuan Chen, Zongxin Yang, and Yi Yang. Are image-to-video models good zero-shot image editors?, 2025. 1 [50] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024. 1 [51] Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, and Hongsheng Li. From reflection to perfection: Scaling inferencetime optimization for text-to-image diffusion models via reflection tuning, 2025. 1 Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. 1 [36] David Sharon and Nicole Brichtova. Image editing in gemini just got major upgrade. 2025. [37] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. 1 [38] Gemini Robotics Team, Coline Devin, Yilun Du, Debidatta Dwibedi, Ruiqi Gao, Abhishek Jindal, Thomas Kipf, Sean Kirmani, Fangchen Liu, Anirudha Majumdar, Andrew Marmon, Carolina Parada, Yulia Rubanova, Dhruv Shah, Vikas Sindhwani, Jie Tan, Fei Xia, Ted Xiao, Sherry Yang, Wenhao Yu, and Allan Zhou. Evaluating gemini robotics policies in veo world simulator, 2025. 1 [39] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models, 2025. 1, 2, 5, 7, 8 [40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. 1 [41] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners, 2025. 1, 3 [42] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024. [43] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 1, 5, 6, 2, 3 [44] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang,"
        },
        {
            "title": "Appendix Overview",
            "content": "Section A: Related Work. Section B: More Dataset Details. Section C: More Experiment Details. Section D: Limitations and Future Work. Section E: Qualitative Examples. A. Related Works Reasoning in Visual Generation. While models like Stable Diffusion 3 [8] and FLUX.1 [23] show strong generation, their single-step text-to-image (T2I) mapping struggles with complex logic. Recent studies [31, 37] indicate that scaling test-time computation effectively improves performance. Building on this, numerous works propose employing additional models to guide image generation [25, 48, 51]. For instance, Image-Gen-CoT [48] introduces PARM as reward model for test-time verification, while ReflectionFlow [51] provides natural language feedback via external vision language models. Unlike external-signalreliant approaches, another paradigm [4, 6, 42, 45, 50] uses unified understanding-generation language model to decompose single-step mapping into interleaved textual reasoning and visual synthesis [5, 15, 16, 18, 19]. Specifically, TwiG [15] interleaves textual reasoning for on-the-fly guidance and Draco [19] explores path of pure visual reasoning, utilizing pre-planned visual drafts to guide the final generation. Zero-Shot Reasoning in Video Models. With the rapid development of video generation models [11, 13, 21, 29, 39], new avenues for pure visual reasoning are emerging. The quest for step-by-step reasoning, pioneered by the CoT [20, 35, 40] paradigm in language models, has begun exploring visual counterpart [24, 48]. Chain-ofFrame (CoF) reasoning, introduced by Wiedemer et al. [41], first showed video models ability to tackle complex visual tasks. Like CoT, CoF recasts visual challenges into spatiotemporal progressions, where each frame acts as an intermediate step of visual thought. Based on this, surge of research [7, 14, 30, 38, 46, 49] has focused on systematically evaluating CoFs emergent capability to quantify video models strengths and limitations across spatial, physical, and logical tasks. These efforts indicate CoFs potential for complex visual reasoning beyond languagemediated approaches. But applying its pure visual reasoning to guide and optimize high-quality image generation remains largely under-explored. B. More Dataset Details B.1. Details on prompt categorization. To make semantic-stage edits in UEP targeted and controllable, we categorize each prompt by its primary constraint (i.e., the dominant factor that must be changed to satisfy the prompt while keeping other content invariant.) Here, we define five categories in detail: Attribute Binding. Constraints that bind intrinsic, attributes to an object, (e.g. color, shape/size, material). The object identity and overall scene remain the same, only the specified property changes. Object Combination. Constraints that concerns multiobject composition, including the co-existence of multiple objects or creative combinations (e.g., unusual juxtaposition or hybrid concepts). The object identity is object-level composition. Quantity Control. Constraints that require specific number of instances (counts or count-sensitive descriptions). The primary variable is the number of objects. Spatial Arrangement. Constraints that specify relative spatial relations among objects (e.g., left/right, above/below, in/on, front/behind). Object identities are preserved, but the prompt is satisfied only when their spatial configuration matches the relation. Context Manipulation. Constraints dominated by global context that cannot be reduced to one object, such as temporal cues (era/time mismatch) or environmental settings (background/location/scene condition). Satisfying this requires modifying scene-level context. B.2. Implementation Details. Quality-based Routing. We leverage Qwen3-VL-7B [2] to perform Quality-based Routing for anchor images. Given prompt and its generated image, we ask the model to judge whether the image is (i) semantically incorrect, (ii) semantically correct but aesthetically under-refined, or (iii) both semantically correct and aesthetically refined. Concretely, we perform three-way classification into: Semantically Misaligned (F1), Visually Unrefined (F2), and High Fidelity (F3). F1 correspond to images with noticeable semantic violations (e.g., missing objects, wrong attribute bindings, incorrect relations) regardless of visual fidelity; F2 are semantically aligned but still visually unrefined (e.g., lacking texture, lighting, or realism); and F3 satisfy both semantic alignment and aesthetic coherence, serving as high-quality terminal states for chain construction, the prompt we use for classification is:"
        },
        {
            "title": "Quality Assessing Prompt Template",
            "content": "You are strict image quality assessor. Input: - PROMPT: {prompt} - IMAGE: [Image Input] Major Evaluate based on Semantic Alignment and Aesthetic Quality: 1. Semantically Misaligned (F1): semantic identity errors (e.g., wrong objects), regardless of aesthetics. 2. Semantically correct, but low aesthetic quality (blur, distortion, bad lighting). 3. correct AND high aesthetic quality. High Fidelity (F3): Semantically Visually Unrefined (F2): Output Format (JSON-compatible): { } \"label\": \"F1\" \"F2\" \"F3\", \"analysis\": \"strict reasoning based on the definitions above\", B.3. Unified Editing Primitive The Unified Editing Primitive (UEP) serves as the core modular operation in our quality-aware pipeline, enabling consistent, category-conditioned stage transitions across all three construction routes. UEP is implemented as closedloop agent system comprising three components powered by Qwen-family models: planner (Qwen3-VL-32B [2]), an editor (Qwen-Image-Edit-2509 [43]), and verifier (Qwen3-VL-32B [2]). Prompt Designs. To achieve precise control, we design role-specific system prompts for every component of UEP for planner and verifier. We offer simplified templates:"
        },
        {
            "title": "Planner Prompt Template",
            "content": "You are visual reasoning planner for image editing. Input Context: - CURRENT IMAGE: [current frame] - PROMPT: {prompt} - STAGE: {stage} (semantic correction OR aesthetic refinement) - DIRECTION: {direction} (forward OR backward) - CATEGORY: {category} (one of [Attribute Binding, etc.]) - Edit Hint: - PREVIOUS FRAME: [prior frame, optional] Editing Logic: Generate targeted editing instruction. If DIRECTION is forward, fix semantic discrepancies in the CATEGORY or enhance aesthetic realism. backward, explicitly introduce semantic {hint} (based on CATEGORY) If DIRECTION is errors related to the CATEGORY or degrade the visual quality. Task: Output exactly ONE minimal editing instruction (< 40 words) that achieves the goal derived above. FRAME is provided, use it to maintain subject identity strictly. If the PREVIOUS Output Format (JSON-compatible): { } \"instruction\": \"edit instructions\","
        },
        {
            "title": "Verifier Prompt Template",
            "content": "You are visual reasoning verifier for image editing chains. Input Context: - EDITED IMAGE: [new frame] - PROMPT: {prompt} - STAGE: {stage} (semantic correction OR aesthetic refinement) - DIRECTION: {direction} (forward OR backward) - CATEGORY: {category} (one of [Attribute Binding, etc.]) - PREVIOUS FRAME: [prior frame, optional] Verification Logic: Verify if the transition matches the DIRECTION: if forward, the image must improve in semantics or aesthetics; if backward, it must explicitly degrade or introduce errors. Additionally, check that the main subject identity remains consistent with the PREVIOUS FRAME and no unrelated artifacts appear. Task: - Output SUCCESS(1) if the image clearly executes the intended Forward or Backward goal while preserving unrelated content. - Output FAIL(0) if the change is imperceptible, over-edited, or moves in the wrong direction. Output Format (JSON-compatible): { } \"Output\": 0 OR 1, Table 6. Performance on Imagine-Bench. We report per-type average scores and the overall weighted score. Wan2.1 Base denotes the Wan2.1-T2V-14B base model without CoF fine-tuning. Target-only SFT is fine-tuned using only the final frames of CoFEvol-Instruct. Method Attribute Shift Hybridization Multi-Object Spatiotemporal Overall Wan2.1 Base Target-only SFT CoF-T2I (Ours) 5.436 5.940 6.969 6.950 7.540 8.070 5.383 7.220 7.797 6.237 6.727 7.287 5.939 6.755 7.468 Table 7. Analysis of the Reasoning Trajectory on ImagineBench. We report per-type scores for each frame of the visual reasoning chain, illustrating progressive refinement from the draft to the final output. Step Attribute Shift Hybridization Multi-Object Spatiotemporal Overall Frame 1 (Draft) Frame 2 (Refine) Frame 3 (Final) 5.451 6.547 6.969 6.837 7.850 8.070 5.817 7.357 7. 6.237 7.317 7.287 6.015 7.187 7.468 Detail Workflow of UEP. The UEP follows simple planner-editor-verifier iterative workflow. Given the current frame and the target transition stage, the planner first analyzes the image and produces concise editing instruction, which is then executed by the editor. The verifier evaluates whether the edited image satisfies the desired stage objective; if not, the process is repeated. To balance efficiency and precision, the planner and verifier adopt resolution-adaptive perception depending on the transition stage. For transitions between F1 and F2, the image is resized to 512 512, which is sufficient for identifying semantic correctness while reducing computation. For transitions between F2 and F3, the original 1024 1024 resolution is preserved to better assess fine-grained aesthetic details and visual quality. The verifier follows the same resolution strategy. We set the maximum number of retries to = 3. If the verifier continues to reject the edited result after exceeding this limit, the pipeline falls back to directly regenerating the image using strong image generation model (Qwen-Image [43]) to ensure stable data quality and sample throughput. C. More Experiment Details In this section, we provide additional results on ImagineBench and implementation details regarding the continuous video VAE encoding and the system prefix used during training and inference. C.1. Additional Results on Imagine-Bench Target-only SFT. We evaluate the Target-only SFT variant on Imagine-Bench to assess the performance when the model is fine-tuned solely on the final output frames, without intermediate reasoning supervision. The per-type average scores are reported in Table 6. Analysis of the Reasoning Trajectory. We further analyze the reasoning trajectory of CoF-T2I on Imagine-Bench by evaluating the generation quality at each step of the chain (F1 F2 F3). As shown in Table 7, scores consistently improve across all categories as the reasoning progresses from the initial draft to the final output. C.2. Implementation Details Continuous Video VAE Encoding. Standard causal video VAEs typically adopt spatiotemporal compression schedule of 1 + 4n: the first frame is encoded without temporal downsampling, while subsequent frames are temporally compressed by factor of 4. As result, 3-frame chain (F1, F2, F3) is not directly compatible with the native temporal layout. To enable continuous video VAE encoding, we pad each training sample to five frames by repeating the final frame: (F1, F2, F3) (F1, F2, F3, F3, F3). The padded 5-frame clip is then passed through the native video VAE encoder. During decoding, we only decode and retain the last frame as the final output image, while all other decoded frames are discarded. For text-to-image generation, only the last decoded frame is used as the model output. System Prompt Prefix. To enable the Chain-of-Frame reasoning capability, we append fixed system prefix to the prompt during both training and inference. This prefix instructs the model to generate short refinement chain that preserves concept and composition while improving quality step by step. The specific prefix used is:"
        },
        {
            "title": "System Prompt Prefix",
            "content": "Generate short refinement chain of the same concept and composition, improving the image step by step. Prompt: <user prompt> D. Limitations and Future Work While CoF-T2I demonstrates the efficacy of leveraging video models Chain-of-Frame reasoning for text-to-image generation, our study has not systematically explored its extension to broader task domains, such as video-related tasks (e.g., text-to-video generation) or 3D-related tasks (e.g., text-to-3D synthesis). Extending to text-to-video, for instance, could introduce challenges like handling longer temporal sequences, increased computational demands for multi-frame refinement, maintaining dynamic coherence without introducing unintended motion artifacts, Figure 7. Qualitative Analysis: Reasoning Trajectories and Comparisons for CoF-T2I and sourcing high-quality datasets for extended reasoning chains. Additionally, reinforcement learning (RL) techniques, which have proven successful in enhancing Chainof-Thought reasoning in textual domains through iterative feedback and reward optimization, remain underexplored in our framework; integrating RL with our video-based model could enable more adaptive visual refinements in T2I tasks, potentially improving robustness to diverse prompts and further elevating output quality. Future work will investigate these avenues to unlock the full potential of video foundation models as versatile visual reasoners. E. Qualitative Examples We provide further qualitative visualizations and comprehensive comparisons in Figure 7. In Figure 7(a), we showcase the complete reasoning trajectory, including the intermediate frames and the final output alongside their corresponding prompts. The examples encompass diverse generation scenarios, such as imaginative object combination, specific attribute binding, and spatial arrangement. By visualizing the chain-of-frame evolution, we demonstrate how the model iteratively refines semantics or reconstructs details to achieve the final target. In Figure 7(b), we compare our method with the baseline video model, Wan2.1-T2V-14B, and representative inference-time reasoning model, BAGEL-Think, which interleaves textual Chain-of-Thought with visual generation. As shown, the baseline Wan2.1-T2V tends to rely heavily on training priors and often ignores counter-intuitive instructions; for instance, it generates standard bus shape instead of the requested gigantic hollow cube and fails to render the vibrant bright green door handle. While Bagel-Think exhibits better prompt following than the baseline, it still struggles with fine-grained texture synthesis and complex structural deformations, such as the marble texture on the mule or the translucent crystal container. In contrast, CoF-T2I produces satisfying results with both high photorealistic quality and precise alignment with the prompt, successfully executing challenging instructions that require strong reasoning capabilities."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "Nanjing University",
        "Peking University",
        "Sun Yat-sen University",
        "Zhejiang University"
    ]
}