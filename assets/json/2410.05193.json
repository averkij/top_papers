{
    "paper_title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
    "authors": [
        "Qiyuan Zhang",
        "Yufei Wang",
        "Tiezheng YU",
        "Yuxin Jiang",
        "Chuhan Wu",
        "Liangyou Li",
        "Yasheng Wang",
        "Xin Jiang",
        "Lifeng Shang",
        "Ruiming Tang",
        "Fuyuan Lyu",
        "Chen Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing the text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 3 9 1 5 0 . 0 1 4 2 : r Preprint. REVISEVAL: RESPONSE-ADAPTED REFERENCES IMPROVING LLM-AS-A-JUDGE VIA Qiyuan Zhang1, Yufei Wang2, Tiezheng Yu2, Yuxin Jiang3, Chuhan Wu2, Liangyou Li2, Yasheng Wang2, Xin Jiang2, Lifeng Shang2, Ruiming Tang2, Fuyuan Lyu4, Chen Ma1 1City University of Hong Kong, 2Huawei Noahs Ark Lab, 3The Hong Kong University of Science and Technology (Guangzhou), 4McGill University & MILA qzhang732-c@my.cityu.edu.hk, wang.yufei1@huawei.com, chenma@cityu.edu.hk"
        },
        {
            "title": "ABSTRACT",
            "content": "With significant efforts in recent studies, LLM-as-a-Judge has become costeffective alternative to human evaluation for assessing the text generation quality in wide range of tasks. However, there still remains reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce REVISEVAL, novel text generation evaluation paradigm via the response-adapted references. REVISEVAL is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, REVISEVAL leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that REVISEVAL outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. detailed analysis is also conducted to confirm REVISEVALs effectiveness in bias reduction, the impact of inference cost, and reference relevance."
        },
        {
            "title": "INTRODUCTION",
            "content": "As the large language model (LLM) already exhibits strong alignment with humans (Gilardi et al., 2023; OpenAI et al., 2024), LLM-as-a-Judge (Chang et al., 2024; Li et al., 2024b; Gao et al., 2024), aka. LLM-evaluator, has emerged as viable alternative to human evaluation in assessing text generation quality. Given the task instruction and the corresponding model-generated responses, LLMs are prompted to predict preferences or scores for these responses. Despite considerable efforts have been made, such as chain-of-thought (Zheng et al., 2023), specialized rubrics (Liu et al., 2023), and extensive evaluation-specific training datasets (Li et al., 2024a; Wang et al., 2024c;b), human evaluation remains the gold standard in text quality assessment (Zeng et al., 2024) and LLM-as-aJudge struggles with particular biases (Huang et al., 2024) and being vulnerable to the misleading context (Dubois et al., 2024; Chen et al., 2024). One important reason is the lack of an oracle to direct the evaluation process. Fortunately, classical text evaluation metrics, like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), offer valuable prompts in mitigating such gap: given an appropriate reference, i.e., the ground-truth answer to the task, calculating the similarity between the references and the model-generated responses can achieve satisfactory correlation with human evaluations. Furthermore, several studies highlight the reference could prevent being overly sensitive to semantic deficiency (Sheng et al., 2024) and overcoming bias (Deutsch et al., 2022) in certain cases. Work partially done during the internship at Huawei Noahs Ark Lab. 1 Preprint. Figure 1: Performance comparison of reference-free and reference-based evaluation paradigms across different similarity groups in MT-Bench, using GPT-4-as-a-Judge. In the reference-based evaluation, the GPT-4 direct response is used as the reference, and the evaluated response with higher BERTScore with the reference is regarded as the preferred one. As the similarity between the reference and the response increases, the human agreement accuracy of the reference-based evaluation significantly improves, while the reference-free evaluation maintains relatively consistent performance across all similarity levels. However, straightforward leveraging references in LLM-as-a-Judge can also be challenging. In addition to the availability of high-quality references (Rei et al., 2021), previous works (Mehri & Eskenazi, 2020; Gómez-Rodríguez & Williams, 2023; Guan & Huang, 2020) find that pre-existing references introduce noise across various text evaluation tasks due to the one-to-many problem, where for given task input, there exist many diverse yet valid responses. In this case, particular pre-existing references could negatively penalize many appropriate but dissimilar responses in the evaluation process (Ji et al., 2022). Thus, we hypothesize that an effective reference should be closely relevant to the response to be evaluated. We further verify this on MT-Bench (Zheng et al., 2023), an open-ended instruction-following dataset. As shown in Figure 1, we use GPT-4 direct responses to the instructions as the references and quantify relevance by the similarity between the references and the responses using BERTScore (Zhang et al., 2020). We find that higher relevance simulates greater utility from the reference, resulting in more effective evaluations than reference-free evaluator. Motivated by the above findings, we deem that an effective reference should maintain high quality while ensuring relevance to the response, which led us to consider that revising the response adaptively could be good candidate for this reference (Guo et al., 2024). Therefore, we propose novel evaluation paradigm REVISE-AND-EVALUATION, abbreviated as REVISEVAL. Specifically, given the (instruction, response) pair, REVISEVAL first revise the response using the instruction and evaluation rubric, resulting in the response-adapted reference. REVISEVAL further leverages this generated response-adapted reference to guide final evaluation (e.g., scoring or pairwise comparison). By revising the original response, we can ensure that the generated reference is both high-quality and closely relevant to the original content. The comparison between the original and revised responses offers valuable insights for evaluation. Orthogonal to previous work that only focuses on the discrimination abilities of LLMs, REVISEVAL stands out by fully utilizing the generative potential by revision. We conduct comprehensive experiments to validate the effectiveness of our proposed REVISEVAL. Using both proprietary and open-source LLMs, REVISEVAL consistently achieves better evaluation performance compared to reference-free and reference-based evaluation paradigms in both NLG tasks and open-ended instruction-following tasks. Moreover, we seek to verify the effectiveness of the response-adapted references in the classic metrics, e.g., BERT and BERTScore, showing that each metric exceeds itself by up to 3%-10% accuracy compared to using direct response as references. We then combine LLM-as-a-reviser with multiple classic metrics and find that it outperforms LLM-as-aJudge (reference-free setting) by over 1.5% on average when using weak LLMs and is comparable when using GPT-4. Finally, we analyze how our paradigm achieves overall superiority. In reducing verbosity and positional bias, our approach offers clear advantages in adversarially designed LLMBar and swapping position testing. Merely increasing inference cost of reference-free evaluation still lags behind REVISEVAL, demonstrating our methods efficiency does not rely on naively accumulating cost. Meanwhile, we validate the relationship between reference relevance and efficiency using response-adapted references. 2 Preprint."
        },
        {
            "title": "2.1 EVALUATION OF LARGE LANGUAGE MODELS",
            "content": "Instruction-tuned LLMs (Ouyang et al., 2022; Dubey et al., 2024; Team et al., 2023) have revolutionized the field of natural language processing (NLP) due to their ability to handle wide range of language-related tasks. Unlike traditional NLP tasks, such as Machine Translation and Summarization, which can be evaluated using N-gram-based metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee & Lavie, 2005) by comparing responses with reference texts, LLMs excel at open-ended language generation tasks (e.g., story generation and open-ended instruction-following generation), where no single reference response exists. Consequently, several studies embrace the potential of LLM-as-a-Judge and shift toward reference-free metrics, advocating the abandonment of conventional reference-based evaluation methods (Sheng et al., 2024; Chen et al., 2023). In this paper, we re-visit the significance of reference in LLM evaluation. Furthermore, to address the challenge of the absence of single standard answer in certain evaluation tasks, we propose leveraging LLMs to generate response-adapted intermediate references, thereby improving the evaluation performance of both traditional metrics and LLM-as-a-Judge. 2.2 LLM-AS-A-JUDGE Recent progress in NLP has introduced model-based evaluation metrics like BERTScore (Zhang et al., 2020) and BARTScore (Yuan et al., 2021). However, these methods also depend on the availability of human-annotated references, which can be expensive, time-consuming, and labor-intensive (Zheng et al., 2023). With the emergence of large language models (LLMs), several studies (Zheng et al., 2023; Dubois et al., 2024) have harnessed their robust evaluation capabilities for assessing natural language generation (NLG), particularly by employing proprietary models like GPT-4 (OpenAI et al., 2024). To avoid information leakage caused by external API calls, some efforts advocate finetuning LLMs with evaluation data to obtain evaluator models (Vu et al., 2024; Li et al., 2024a; Wang et al., 2024c; Kim et al., 2024b). wide variety of techniques are used to enhance the performance of LLM-as-a-Judge, such as Chain-of-Thoughts (CoT; Wei et al. (2022)) to first generate concise reasoning and then the final decision, adding pre-defined rules (Zeng et al., 2024) in prompts to list some general rules for LLM-as-a-Judge to follow explicitly, and swapping the two responses to avoid positional bias (Wang et al., 2024a). Zheng et al. (2023) found that, even with the use of CoT prompt, LLM-as-a-Judge can still be misled by the surrounding context, particularly by erroneous response text. Therefore, they propose reference-guided method where the LLM-as-a-Judges response is first generated independently based on the given instruction and then presented as reference answer within the evaluation prompt. To the best of our knowledge, we are the first to generate response-adapted references from both the instruction and the response to be evaluated."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we introduce our novel evaluation paradigm, REVISEVAL, which enhances the evaluation by generating response-adapted references. Illustrated in Figure 2, REVISEVAL consists of two components, response-adapted reference generation and reference-based evaluation, which we will discuss in Sec. 3.1 and 3.2, respectively. Supposing is the response generated by model for given task instruction x, REVISEVAL assesses the quality of on specific rubric a. Firstly, in the generation phase, conditioned on (x, a), REVISEVAL deploys LLM reviser to revise to generate response-adapted reference r. Secondly, in the evaluation phase, taking the (x, a, y) and generated as input, REVISEVAL adopts LLM-as-a-Judge FE to assess using as the reference. Besides, we further expand REVISEVAL to support traditional reference-based metrics FM . The evaluation objective is to ensure that the automated evaluations align closely with human evaluations, which is introduced in Appendix G. 3.1 RESPONSE-ADAPTED REFERENCE GENERATION LLMs have already demonstrated their surprising revision capabilities in various tasks, including improving specific attribution (e.g., writing style and grammar) of passages (Gao et al., 2023), 3 Preprint. Figure 2: Illustration of our proposed REVISEVAL. Given an instance (x, y, a), we use REVISEVAL to assess in rubric a. In REVISEVAL, (i) reviser generates response-adapted reference by revising the to enhance the (ii) following LLM-as-a-Judge, even classic text metrics. correcting hallucination (Akyurek et al., 2023), post-editing the generated story (Yang et al., 2022) and generating higher-quality revised responses to complement preference pairs for DPO (Guo et al., 2024; Yoon et al., 2024; Jiang et al., 2024b; Xu et al., 2024). Thus, unlike previous works treating LLMs as discriminators (Hu et al., 2024), we leverage the revision capabilities of LLMs to unlock the generative potential to offer richer and more valuable insights for evaluation. REVISEVAL deploys LLMs to revise the response from the instruction on evaluation rubric a, = R(yx, a), (1) where is the generated response-adapted reference for the subsequent evaluation. Notably, when REVISEVAL must give the preference on two responses, y1 and y2, in pairwise comparison, we randomly select one for primary text to be revised while using the other as revision guidance. We remind reviser that this revision guidance may not be perfect and should be used with caution in the revision prompt, = (cid:26)R(y1y2, x, a) R(y2y1, x, a) if = 1 if = 2 (2) where is random variable that decides which response is chosen for revision. can be either 1 or 2, with each having an equal chance of occurring. Notably, we discuss other possible revision strategies in Appendix I.2, which are less effective comparably. Introducing qualified reference can reliably guide the evaluation process, for instance, acting as an anchor to reduce biases. Our strategy, which incorporates revision guidance and randomly sampling one as the primary text for revising, further reinforces fairness. This will be validated in Sec. 4.2 and 4.5. 3.2 REFERENCE-BASED EVALUATION In the evaluation phase, REVISEVAL supports LLM-as-a-Judge FE in reference-based setting and remains compatible with previous metrics FM . LLM-as-a-Judge. With powerful generalization capabilities, LLMs can serve as discriminators for evaluation, referred to as LLM-as-a-Judge. In this case, the evaluation can be operated using FE, = FE(yx, a, r). (3) This can be easily accomplished by simply using general prompt for inference, where we clarify the instruction, response, response-adapted reference, and evaluation rubric in the prompt, as shown in Appendix A. Moreover, we implement this for open-source LLMs through finetuning evaluation data in this task format, with the detailed process provided in Appendix C. Metrics. Once we get the reference r, we can also implement classic metrics, regardless of statistical n-gram or model-based metrics. The evaluation score is: = FM (y, r[x, a]). (4) 4 Preprint. Table 1: Kendall (τ ) and Spearman (ρ) correlation results comparing reference-free, reference-based, and REVISEVAL methods across natural language generation tasks. This table demonstrates that, without human-annotated references, our proposed REVISEVAL significantly outperforms referencefree and reference-based methods involving both open-source and proprietary LLM-as-a-Judge. SUMMARIZATION τ /ρ TRANSLATION DATA2TEXT τ /ρ τ /ρ STORY GENERATION τ /ρ Avg. τ /ρ Methods BLEU ROUGE METEOR BERTScore BARTScore UniEval GPTScore InstructScore-7B TIGERScore-7B Llama-3.1 8B-Inst 10.66/14.42 10.81/14.85 12.37/16.72 17.50/23.83 29.12/35.50 35.89/47.52 28.20/37.41 20.86/38.68 28.79/35.11 27.49/31.02 N-gram Metrics 14.50/19.73 13.19/17.83 16.52/18.80 23.13/33.29 24.74/35.49 25.58/36.27 Model-based Metrics 31.57/42.41 7.01/12.83 16.08/21.90 6.50/8.90 40.44/50.43 33.65/41.50 19.59/23.54 30.74/43.75 22.32/34.33 28.56/38.38 19.81/28.82 30.21/38.54 32.44/42.39 28.46/36.24 Open-Source LLM-as-a-Judge Ref-Free Ref-Based REVISEVAL (Ours) 27.83/31.89 34.09/39.53 32.41/37.73 30.84/38.66 35.76/41.12 33.14/39. 38.75/49.32 39.24/50.87 39.02/49.92 Ref-Free Ref-Based REVISEVAL (Ours) 31.82/38.98 32.56/40.01 33.63/41.15 34.62/43.38 41.47/45.29 40.72/45.32 37.99/49.50 37.35/49.02 37.90/50.93 Proprietary LLM-as-a-Judge -1.93/-2.70 -1.53/2.34 -1.87/-2.65 16.00/23.79 14.15/33.48 31.22/44.46 16.36/23.91 13.50/16.13 29.72/39.26 26.13/29.97 25.74/31.72 8.79/10.44 25.95/32.11 23.81/33.29 17.58/24.86 25.11/35.26 11.59/16.19 11.80/17.63 13.15/17.29 23.95/33.45 18.15/29.04 27.94/38.07 17.72/24.76 26.25/35.94 31.15/39.56 25.42/30. 30.79/37.90 29.47/35.49 32.63/39.86 32.06/41.29 32.24/39.80 34.34/43.17 More specifically, when FM are n-gram metrics, e.g., BLEU, ROUGE, and METEOR, we can directly compute the similarity between and r; when FM are model-based metrics, e.g., BERTScore and BARTScore, we can optionally input and to the metrics. The effectiveness of metrics heavily relies on the reference. When the response-adapted reference is appropriate, even simple metric can revive its evaluation functionality in open-ended tasks. We validate this point in Sec. 4.3 and 4.4."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we first present the comprehensive experimental settings in Sec. 4.1 and evaluate the REVISEVAL in LLM-as-a-Judge across various tasks in Sec. 4.2; we then verify the effectiveness of classic text evaluation metrics when using response-adapted references in Sec. 4.3; building on above findings, we compare two evaluation paradigms, combining LLM-as-a-reviser with multiple metrics and LLM-as-a-Judge, when using weak LLM in Sec. 4.4; finally, we conduct detailed comparative analysis of REVISEVAL in Sec. 4.5, such as bias reduction, inference cost impact and reference relevance. 4.1 EVALUATION SETTINGS Evaluation benchmarks. We evaluate our approach on multiple classic NLG benchmarks by measuring the correlation between the evaluators/metrics and human annotations in scoring rating task. We follow the experimental setting of Jiang et al. (2024a) and select four representative NLG tasks: Data-to-Text, Machine Translation, Text Summarization, and Story Generation. Table 9 shows the details of these benchmarks. Additionally, we test our approach on the more challenging open-ended instruction-following benchmarks (MT-Bench, Alpacafarm, and LLMBar), which primarily rely on pairwise comparison task. Unlike the NLG benchmarks, these preference datasets contain more general instructions covering broader range of tasks with more diverse responses. Base LLMs and metrics. Our proposed REVISEVAL aims to improve evaluation performance across both LLM-as-a-Judge and classic metrics, offering enhanced results. For proprietary LLMs, we adopt GPT-4 as the base model and focus on implementing our paradigm during the inference 5 Preprint. Table 2: Results of LLM-as-a-Judge on instruction-following preference tasks. Our proposed REVISEVAL significantly enhances the performance of both open-source and proprietary LLM-as-aJudge across various general evaluation tasks. Here, D.R. denotes Direct Response to instruction. Methods # of Training Samples MT-BENCH ALPACAFARM LLMBAR Avg. Open-Source LLM-as-a-Judge JudgeLM-7B (Zhu et al., 2023) PandaLM-7B (Wang et al., 2024c) Auto-J-13B (Li et al., 2024a) Prometheus-7B (Kim et al., 2024a) Prometheus-2-7B (Kim et al., 2024b) Llama 3.1-8B-Tuned Ref-Free Ref-Based (Llama-D.R.) Ref-Based (GPT-4-D.R.) REVISEVAL (Llama-as-a-Reviser) REVISEVAL (GPT-4-as-a-Reviser) 100,000 300,000 4,396 100,000 300,000 9,800 9,800 9,800 9,800 9,800 64.1 75.0 75.2 52.8 55.0 67.4 74.9 78.0 75.2 79.3 Ref-Free Ref-Based (GPT-4-D.R.) REVISEVAL (GPT-4-as-a-Reviser) - - - 81.2 81.5 83.0 Proprietary LLM-as-a-Judge (GPT-4) 53.9 54.9 64.6 33.5 37.3 61.1 61.5 65.5 64.7 67.1 70.9 67.7 72. 36.3 31.7 36.0 30.1 26.3 51.1 58.9 63.0 57.8 64.9 72.6 79.9 79.0 51.4 53.9 58.6 38.8 39.5 59.9 65.1 68.8 65.9 70.4 74.9 76.4 78. stage. For open-source LLMs, we implement our method via finetuning the Llama 3.1-8B model. Following the Jiang et al. (2024a)s setting on open-source models, we distill the evaluation outputs generated by GPT-4 when inputting task instructions and corresponding evaluated responses, and tune them in our models. Notably, our training data has no overlap with evaluation benchmarks. For classic metrics, we cover various metrics, like n-gram based BLEU, ROUGE, METEOR and model based BERTScore, MOVERScore (Zhao et al., 2019), BARTScore, which rely heavily on references. We validate our method by assessing the utility of reference texts. We ensure that our approach maintains versatility and fairness across models, with further details on prompts, inference, finetuning, and baselines in the Appendix A, B, C, D, and F. 4.2 ENHANCING LLM-AS-A-JUDGE PERFORMANCE We present the main results of our proposed REVISEVAL across NLG evaluation tasks and instructionfollowing preference benchmarks in Table 1 and 2. We summarize the conclusions below. REVISEVAL achieves stronger performance across various NLG tasks. For the powerful proprietary LLMs, REVISEVAL outperforms reference-free and human-annotated reference-based evaluation across tasks with approximately 0.02 in Kendall correlation on average, as demonstrated in Table 1. Notably, in story generation, high-quality human references hinder evaluation for LLM-as-aJudge, which decreases by 0.06 compared to the reference-free method in the Kendall correlation. In contrast, REVISEVAL shows that generated response-adapted reference can still significantly enhance evaluation by about 0.08 in Kendall than human reference-based evaluation. An exception is machine translation, where REVISEVAL aligns closely with reference-based methods, and we analyze this result exists consistent rationale about reference relevance in Sec 4.5. For the open-source LLMs, REVISEVAL not only outperforms the reference-free method but also beats the reference-based methods by over 0.03 on average in Kendall correlation. Especially in story generation, the reference-based approach is consistent with the above conclusion, trailing by approximately 0.17 in Kendall compared to our REVISEVAL. Furthermore, the LLM with specialized finetuning also performs better than the LLM with general instruction finetuning (i.e., Llama 3.1-8B Inst) on NLG evaluation tasks, leading by about 0.02 in Kendall. REVISEVAL excels on open-ended instruction-following preference benchmarks. As shown in Table 2, whether implemented on open-source or proprietary models, REVISEVAL consistently surpasses all baselines by at least 6.3% on average. The details of these baselines are listed in Appendix F, and they are tuned with tens of thousands of data for the LLM-as-a-Judge. On the same base LLM, REVISEVAL exceeds reference-free evaluation by 3%-6%. We compare REVISEVAL to reference-based evaluations followed by Zheng et al. (2023)s setup, whose reference is the LLMs direct response to the instruction. Our approach performs better than reference-based evaluation on 6 Preprint. Figure 3: Comparative analysis of reference-based metrics performance using references generated by HUMAN/GPT-4 and REVISEVAL on NLG and instruction following benchmarks. REVISEVAL can greatly enhance traditional reference-based metrics, even n-gram metrics, reaching performance levels comparable to GPT-4-as-a-Judge in reference-free evaluations. average when both references are generated by the same base LLMs. Furthermore, using GPT-4 as the reviser boosts Llama 3.1-8B-as-a-Judge by over 4.5% compared to Llama-as-a-Reviser, highlighting the importance of reference quality. LLM-as-a-Judge can be biased toward longer, verbose answers (Saito et al., 2023; Dubois et al., 2024) or answers that match similar format (Huang et al., 2024). LLMBar (Zeng et al., 2024) is challenging benchmark to meta-evaluate such superficial quality biases. As demonstrated in Table 2, these baselines, even after tuning > 100K samples, struggle to exceed 50% accuracy, exposing bias challenge significantly. By using response-adapted references from REVISEVAL, the weak open-source LLM-as-a-Judges performance improves substantially by about 6%, showing REVISEVAL can address superficial quality bias. On proprietary LLM, REVISEVAL achieves 3.2% improvement in accuracy compared to reference-free evaluation. Our proposed responseadapted references perform slightly worse than direct responses from the same proprietary model. This is likely because LLMBar emphasizes instruction-following precision, where single groundtruth response exists for the instruction. In summary, REVISEVAL consistently outperforms traditional reference-free and reference-based methods, and our revision significantly provides guidance for LLM evaluation by significantly utilizing the generative advantages of LLM. 4.3 ACTIVATING CLASSIC METRICS PERFORMANCE Since response-adapted references have enhanced LLM-as-a-Judge in Sec. 4.2, this motivates us to ask could these references directly improve the performance of classic metrics, even n-gram metrics? As the classic text metrics are not compatible with the pairwise comparison, we extend them as follows: 7 Preprint. Table 3: Comparative analysis of weak LLM-as-a-Judge and weak LLM-as-a-Reviser+classic metrics on instruction-following tasks. Under the same finetuning training resources, weak LLMas-a-Reviser combined with classic metrics can produce better results. Metrics MT-BENCH ALPACAFARM LLMBAR Avg. LLM-as-a-Reviser BLEU ROUGE METEOR BERTScore MOVERScore BARTScore MAJORITY VOTING LLM-as-a-Judge 64.5 62.0 66.4 62.3 61.5 66.9 63.4 67.4 63.9 63.5 67.7 62.3 68.3 61.9 68.5 61.1 51.6 51.6 46.3 54.4 51.6 51.3 52.5 51. 60.0 59.0 60.1 59.7 60.5 60.0 61.4 59.9 = I(FM (y1, r) > FM (y2, r)), where I() is the indicator function, determining which y1 or y2 is preferred based on the metric scores. We use human references in NLG tasks and GPT-4 direct responses in instruction-following benchmarks as the baseline references ˆr, comparing them with references generated by REVISEVAL. As shown in Figure 3, REVISEVAL enables each classic metric to significantly surpass its performance based on baseline references across all tasks. This increase is especially evident in more complex open-ended tasks like story generation and AlpacaFarm. Additionally, we find that using generated by GPT-4 as-a-Reviser combined with classic metrics can yield comparable evaluation performance to GPT-4s reference-free evaluation. Notably, in AlpacaFarm, when using the response-adapted references, MOVERScore achieves 1.2% improvement compared to the reference-free GPT-4-as-a-Judge. 4.4 POTENTIAL EVALUATION PARADIGM FOR WEAK LARGE LANGUAGE MODELS We observe that 1) in the Sec. 4.2, all weak LLMs still exhibit significant gap compared to GPT4-as-a-Judge, even after extensive training with high-quality, evaluation-specific data, and 2) in the Sec. 4.3, classic metrics combined with response-adapted references generated by GPT-4 can achieve performance close to the reference-free GPT-4-as-a-Judge. Thus, should we consider potential evaluation paradigm of weak LLM-as-a-Reviser + metrics instead of weak LLMs-as-a-Judge? To explore this, we compare Llama-as-a-Judge with Llama-as-a-Reviser + metrics, as shown in Table 3. When using references generated by Llama-as-a-Reviser, we find that BLEU, METEOR, MOVERScore, and BARTScore can surpass Llama-as-a-Judge on average across 3 tasks. Furthermore, we apply majority voting across multiple metrics, outperforming Llama-as-a-Judge over 1.5% on average. This suggests that instead of continuously training weak LLMs to improve their evaluative discrimination capabilities, leveraging their generation abilities for revision may be more effective. Without extra inference costs, this approach can lead to better evaluation outcomes. Table 4: Positional bias analysis in pair comparison evaluations when applying different evaluation paradigms. This table presents the ratio of changed evaluation results after swapping the response position. lower proportion indicates less positional bias.REVISEVAL stands out as the best, exhibiting the lowest bias among all paradigms. Paradigms Ref-Free Ref-Based REVISEVAL MT-BENCH ALPACAFARM LLMBAR LLAMA 3.1-8B GPT-4 LLAMA 3.1-8B GPT-4 LLAMA 3.1-8B GPT-4 49.1 22.8 20.5 10.3 6.5 5. 61.1 34.1 30.1 8 20.0 22.2 19.9 44.6 32.5 30.3 17.9 11.2 7.9 Preprint. Table 5: Ablation study on the impact of inference cost. Increasing evaluation cycles to match or exceed REVISEVALs inference cost in reference-free did not improve accuracy. This shows that REVISEVALs superior performance is not from twice inference. Inference Cost MT-BENCH ALPACAFARM LLMBAR LLAMA 3.1-8B GPTLLAMA 3.1-8B GPT-4 LLAMA 3.1-8B GPT-4 1-Cycle Ref-Free 3-Cycle Ref-Free REVISEVAL (2-Cycle) 67.4 64.1 71. 81.2 81.2 83.0 61.1 54.9 64.7 70.9 71.9 72. 51.1 53.2 54.9 72.6 74.9 79.0 Table 6: Comparative analysis of how reference-based evaluation effectiveness varies with changes in the similarity between the response and reference texts across different constructing reference strategies. Here, Effectiveness, Pref/Pfree, refers to the performance ratio between reference-based and reference-free evaluation, where denotes the evaluation performance, e.g., Acc and Corr; similarity is still measured by BERTScore. Reference Source WMT-22(EN-ZH) WEBNLG MT-BENCH SUMMEVAL ALPACAFARM ROC Similarity Effectiveness Human/GPT-4 RevisEval Human/GPT-4 RevisEval 65.12 63.03 (-3.3%) 59.09 76.41 (+29.3%) 25.00 30.29 (+21.2%) 23.51 35.63 (+51.6%) 13.04 35.72 (+173.9%) 12.86 27.57 (+114.4%) 1.20 1.18 (-0.02) 0.98 1.00 (+0.02) 1.00 1.02 (+0.02) 1.02 1.06 (+0.04) 0.95 1.03 (+0.08) 0.74 1.05 (+0.31) 4.5 COMPARATIVE ANALYSIS TO OTHER EVALUATION PARADIGMS Positional bias analysis. Positional bias (Wang et al., 2024a; Zheng et al., 2023; Wu & Aji, 2023; Chen et al., 2024) occurs when human or LLM evaluators tend to favor one side in pairwise comparison, regardless of answer quality. We investigate this bias by swapping answer positions and taking the LLM to re-evaluate, as shown in Table 4. The results indicate that reference-based evaluation decisions have less variation than reference-free ones. REVISEVAL is generally 2%-4% lower on the reference-based evaluation, showing better consistency. This result is probably because REVISEVAL provides references more closely aligned with the answers, further minimizing bias. The impact analysis of inference cost. Compared to the reference-free approach, REVISEVAL requires two cycles of inference (i.e., revision and evaluation). To show the impact of extra inference cost, we further conduct three cycles of reference-free evaluations using extra different temperatures (0.3 and 0.7), followed by majority voting (see Table 5). For GPT-4, additional evaluation cycles slightly improve accuracy but still lag behind ours by 1%-4%. For weak LLMs, more cycles led to worse performance. The above results indicate that our approach provides more valuable guidance. Evaluation performance improves with increased relevance between reference and responses. Its been observed that the less relevant reference is to the response, the less effective it is for evaluation in previous work and Figure 1. We further verify whether this trend holds true with our method. We define effectiveness to describe whether reference-based evaluation is more effective than reference-free evaluation. As shown in Table 6, the increasing similarity between reference and evaluated responses generally leads to better evaluation effectiveness. This explains why our method doesnt perform as well in translation, where human references are already highly similar to the response. For other tasks, human or GPT-4 direct-response references have lower similarity than references of REVISEVAL, leading to lower effectiveness. Additionally, for different tasks, the similarity between the human/GPT-4 reference and the evaluated response varies, reflecting the open-ended generative degree of this task. lower similarity indicates greater diversity of potential valid responses. In this context, as the task becomes more open-ended, the effectiveness of REVISEVAL shows greater improvement than the human/GPT-4 reference."
        },
        {
            "title": "5 CASE STUDY: HOW DOES IT WORK?",
            "content": "We show two representative examples in Table 7, one from the instruction-following task (AlpacaFarm) and the other from the NLG evaluation task (story generation). The reference-free evaluation 9 Preprint. Table 7: An instruction-following open-ended case of pairwise comparison and story-generation case of score rating evaluated by different paradigms. Score/Preference refers to accurate evaluation result and Score/Preference refers to wrong result. We emphasize the noisy evaluation elements in red, while highlighting the useful elements of the evaluation in green. Here, the base LLM is GPT-4. Instruction: Classify the following to the correct word group. nn Happy, Joyful Response 1 Happy -> Emotions Reference-free Evaluation: Response 2 Emotions Both Assistant and Assistant . . . . . . Both responses are correct, but one might argue that including the words with the categorization, as Assistant does, provides clearer answer and specifically indicates that both words have been classified. . . . . . . - Final Verdict: [[A]] Referfence-based Evaluation: Direct Response-as-Reference: (cid:58)(cid:58)(cid:58)Happy(cid:58)(cid:58)(cid:58)and(cid:58)(cid:58)(cid:58)(cid:58) Joyful (cid:58)(cid:58)(cid:58)(cid:58)belong(cid:58)(cid:58)to (cid:58)(cid:58)the (cid:58)(cid:58)(cid:58)word(cid:58)(cid:58)(cid:58)(cid:58)group(cid:58)(cid:58)of (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) synonyms,(cid:58)(cid:58)as (cid:58)(cid:58)(cid:58)they (cid:58)(cid:58)both(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) express(cid:58)(cid:58)(cid:58)(cid:58)similar(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) feelings(cid:58)(cid:58)of contentment. pleasure (cid:58)(cid:58)and(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) (cid:58)(cid:58)(cid:58)(cid:58)(cid:58) . . . . . . Assistant A, while correct in stating that \"Happy\" belongs to \"Emotions,\" did not mention \"Joyful\" or classify them together as synonyms. Rather, it only assigned category to one word. Assistant . . . . . . but fails to classify both words as instructed, missing the relationship between \"Happy\" and \"Joyful.\" Neither assistant directly addressed the classification as \"synonyms,\" which was central to the users instruction. - Final Verdict: [[A]] REVISEVAL Evaluation: Response-adapted Reference: (cid:58)(cid:58)(cid:58)(cid:58)(cid:58) Emotions . . . . . . Assistant . . . . . . However, it is clear from the classification provided that Assistant understands the context and correctly categorizes \"Happy\" but overlooks \"Joyful\". Assistant simply responds with \"Emotions\", effectively covering both \"Happy\" and \"Joyful\". This response uses more concise and generalized approach, which aligns perfectly with the reference answer. Based on the reference answer and the clarity and completeness in addressing both terms, Assistant Bs response is superior. . . . . . . - Final Verdict: [[B]] Instruction: Generate reasonable ending for the following story. Source Input: my 2 year old cousin was eating an apple . Response: he brought apples from his class. he dropped the apples. he began to throw them back up. he had hard time. Gold Score: 3. Reference-free Evaluation: . . . . . . but it lacks coherence and logical flow. The sentences seem disconnected, and the events described do not build upon each other in meaningful way. The output does not demonstrate clear understanding of the storys context or the characters actions. - Rating: [[1]] Referfence-based Evaluation: Human Reference: (cid:58)he(cid:58)(cid:58)(cid:58)(cid:58)walked(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) around (cid:58)(cid:58)(cid:58)(cid:58)(cid:58) between (cid:58)(cid:58)(cid:58)(cid:58)(cid:58) everyone(cid:58)at(cid:58)(cid:58)the(cid:58)(cid:58)(cid:58)(cid:58)party .(cid:58)(cid:58)he(cid:58)(cid:58)(cid:58)(cid:58) started(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) coughing (cid:58)(cid:58)and(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) choking(cid:58). (cid:58)(cid:58)(cid:58)(cid:58)(cid:58) everyone(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) started panicked (cid:58)(cid:58)and(cid:58)(cid:58)(cid:58)(cid:58) hitting (cid:58)(cid:58)him(cid:58)(cid:58)on(cid:58)(cid:58)the(cid:58)(cid:58)(cid:58)back(cid:58). (cid:58)he(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) (cid:58)(cid:58)(cid:58)(cid:58) eventually (cid:58)(cid:58)spit(cid:58)(cid:58)out(cid:58)(cid:58)the(cid:58)(cid:58)(cid:58)(cid:58)apple (cid:58)(cid:58)(cid:58)piece (cid:58). The model-generated output is not fluent and lacks coherence in comparison to the reference output. . . . . . . . . . . . . Overall, the model-generated output fails to meet the fluency criteria due to its lack of coherence and grammatical errors. - Rating: [[1]] REVISEVAL Evaluation: Response-adapted Reference: (cid:58)(cid:58)He(cid:58)(cid:58)had(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) brought(cid:58)(cid:58)(cid:58)(cid:58)apples(cid:58)(cid:58)(cid:58)from(cid:58)(cid:58)his(cid:58)(cid:58)(cid:58)(cid:58)class, (cid:58)(cid:58)but (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) accidentally(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) dropped (cid:58)(cid:58)(cid:58)them.(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) Struggling,(cid:58)(cid:58)he(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) attempted (cid:58)to(cid:58)(cid:58)(cid:58)pick (cid:58)(cid:58)(cid:58)them (cid:58)(cid:58)up (cid:58)(cid:58)and(cid:58)in(cid:58)(cid:58)the(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) process,(cid:58)(cid:58)(cid:58)(cid:58)began(cid:58)(cid:58)(cid:58)(cid:58) tossing(cid:58)(cid:58)(cid:58)them(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) around.(cid:58)(cid:58)He(cid:58)(cid:58)(cid:58)was (cid:58)(cid:58)(cid:58)(cid:58) having (cid:58)a (cid:58)(cid:58)(cid:58)hard (cid:58)(cid:58)time(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) apples. managing(cid:58)(cid:58)the(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) The model-generated output is somewhat fluent but lacks coherence and completeness in comparison to the reference output. . . . . . The reference output provides more detailed and logical sequence of events, enhancing the narratives coherence and clarity. . . . . . . Overall, the model-generated output is fluent but could benefit from improvements in coherence and detail to match the quality of the reference output. - Rating: [[3]] tends to be influenced by verbose text, often leading to incorrect judgments, e.g., provides clearer answer.... While the LLMs direct response or human reference might not be erroneous and can fulfill Happy and Joyful belong to the word group of synonyms, ..., in the instructions well, for instance, (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) the first case, it has no relevance to responses to be evaluated and resulting neither assistant directly addressed the classification, judgment. In contrast, the reference generated by REVISEVAL is Emotions, accurately helping the LLM to align the crucial elements, the more adaptive guidance, (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) clarity and completeness. This is also evident in the second case, where differences between the response and the revised text, such as had brought, accidentally dropped, and others, directly highlight fluency issues with the response. In other words, this demonstrates transparent potential. Furthermore, we provide evaluation discrepancy statistics between REVISEVAL and other evaluations to observe how different evaluation methods, as demonstrated in Appendix I.3. Preprint."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce novel yet general evaluation paradigm that leverages the revision capabilities of LLMs to revise evaluated responses to be response-adapted references for evaluation. This approach significantly enhances the reliability of the versatile LLM-as-a-Judge, particularly in effectively reducing bias. The references generated through our REVISEVAL greatly improve even the simplest n-gram metrics, achieving performance comparable to LLM-as-a-Judge. This proves especially advantageous for weaker LLMs, which often struggle to improve despite extensive training, providing an efficient method to enhance their evaluation capabilities. Our findings demonstrate that (1) the importance of references has been underestimated, and (2) harnessing the generative strengths of LLMs can substantially support evaluation tasks by increasing reference relevance."
        },
        {
            "title": "REFERENCES",
            "content": "Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, and Niket Tandon. RL4F: Generating natural language feedback with reinforcement learning for repairing model outputs. In Annual Meeting of the Association for Computational Linguistics, pp. 7716 7733, 2023. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback. In arXiv, 2022. Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, pp. 145, 2024. Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or llms as the judge? study on judgement biases. In arXiv, 2024. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring the use of large language models for reference-free text quality evaluation: An empirical study. In arXiv, 2023. Daniel Deutsch, Rotem Dror, and Dan Roth. On the limitations of reference-free evaluations of generated text. In Conference on Empirical Methods in Natural Language Processing, pp. 10960 10977, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. In arXiv, 2024. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. In arXiv, 2024. Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, pp. 391409, 2021. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. RARR: Researching and revising what language models say, using language models. In Annual Meeting of the Association for Computational Linguistics, pp. 1647716508, 2023. 11 Preprint. Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan. Llm-based nlg evaluation: Current status and challenges. In arXiv, 2024. Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 2023. Carlos Gómez-Rodríguez and Paul Williams. confederacy of models: comprehensive evaluation of LLMs on creative writing. In Findings of the Conference on Empirical Methods in Natural Language Processing, pp. 1450414528, 2023. Jian Guan and Minlie Huang. UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation. In Conference on Empirical Methods in Natural Language Processing, pp. 91579166, 2020. Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. OpenMEVA: benchmark for evaluating open-ended story generation metrics. In Annual Meeting of the Association for Computational Linguistics, pp. 63946407, 2021. Geyang Guo, Ranchi Zhao, Tianyi Tang, Xin Zhao, and Ji-Rong Wen. Beyond imitation: LeverIn International Conference on Learning aging fine-grained quality signals for alignment. Representations, 2024. URL https://openreview.net/forum?id=LNLjU5C5dK. Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, and Xiaojun Wan. Are LLM-based evaluators confusing NLG quality criteria? In Annual Meeting of the Association for Computational Linguistics, pp. 95309570, 2024. Hui Huang, Yingqi Qu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, and Tiejun Zhao. On the limitations of fine-tuned judge models for llm evaluation. In arXiv, 2024. Tianbo Ji, Yvette Graham, Gareth Jones, Chenyang Lyu, and Qun Liu. Achieving reliable human assessment of open-domain dialogue systems. In Annual Meeting of the Association for Computational Linguistics, pp. 64166437, 2022. Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. TIGERScore: Towards building explainable metric for all text generation tasks. Transactions on Machine Learning Research, 2024a. ISSN 28358856. URL https://openreview.net/forum? id=EE1CBKC0SZ. Yuxin Jiang, Bo Huang, Yufei Wang, Xingshan Zeng, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, and Wei Wang. Bridging and modeling correlations in pairwise data for direct preference optimization. In arXiv, 2024b. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fineIn International Conference on Learning grained evaluation capability in language models. Representations, 2024a. URL https://openreview.net/forum?id=8euJaTveKw. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. In arXiv, 2024b. Tom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, and etal. Findings of the 2022 conference on machine translation (WMT22). In Conference on Machine Translation, pp. 145, 2022. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, and Pengfei Liu. Generative judge for evaluating alignment. In International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=gtkFw6sZGS. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, and Shuai Ma. Leveraging large language models for nlg evaluation: Advances and challenges. In arXiv, 2024b. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, 2004. 12 Preprint. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Conference on Empirical Methods in Natural Language Processing, pp. 25112522, 2023. Shikib Mehri and Maxine Eskenazi. USR: An unsupervised and reference free evaluation metric for dialog generation. In Annual Meeting of the Association for Computational Linguistics, pp. 681707, 2020. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, and etal. Gpt-4 technical report. In arXiv, 2024. URL https://arxiv.org/ abs/2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Advances in neural information processing systems, pp. 2773027744, 2022. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic In Annual Meeting of the Association for Computational evaluation of machine translation. Linguistics, pp. 311318, 2002. Ricardo Rei, Ana Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie. Are references really needed? unbabel-IST 2021 submission for the metrics shared task. In Conference on Machine Translation, pp. 10301040, 2021. Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. Verbosity bias in preference labeling by large language models. In arXiv, 2023. Shuqian Sheng, Yi Xu, Luoyi Fu, Jiaxin Ding, Lei Zhou, Xinbing Wang, and Chenghu Zhou. Is reference necessary in the evaluation of NLG systems? when and where? In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 85808596, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. In arXiv, 2023. Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. Foundational autoraters: Taming large language models for better automatic evaluation. In arXiv, 2024. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Annual Meeting of the Association for Computational Linguistics, pp. 94409450, 2024a. Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators. In arXiv, 2024b. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. In PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization. International Conference on Learning Representations, 2024c. URL https://openreview. net/forum?id=5Nn2BLV7SB. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural information processing systems, pp. 2482424837, 2022. Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models. In arXiv, 2023. Preprint. Dehong Xu, Liang Qiu, Minseok Kim, Faisal Ladhak, and Jaeyoung Do. Aligning large language models via fine-grained supervision. In Annual Meeting of the Association for Computational Linguistics, pp. 673680, 2024. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In Conference on Empirical Methods in Natural Language Processing, pp. 59675994, 2023. Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reprompting and revision. In Conference on Empirical Methods in Natural Language Processing, pp. 43934479, 2022. Eunseop Yoon, Hee Suk Yoon, SooHwan Eom, Gunsoo Han, Daniel Nam, Daejin Jo, KyoungWoon On, Mark Hasegawa-Johnson, Sungwoong Kim, and Chang Yoo. TLCR: Token-level continuous reward for fine-grained reinforcement learning from human feedback. In Findings of the Association for Computational Linguistics, pp. 1496914981, 2024. Weizhe Yuan, Graham Neubig, and Pengfei Liu. BARTScore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems, pp. 2726327277, 2021. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=tr0KidwPLc. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Conference on Empirical Methods in Natural Language Processing, pp. 563578, 2019. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview. net/forum?id=uccHPGDlao. Giulio Zhou and Gerasimos Lampouras. WebNLG challenge 2020: Language agnostic delexicalisation for multilingual RDF-to-text generation. In International Workshop on Natural Language Generation from the Semantic Web, pp. 186191, 2020. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalable judges. In arXiv, 2023. 14 Preprint. Figure 4: The prompt of reference-free pairwise comparison evaluation. Figure 5: The prompt of reference-based pairwise comparison evaluation."
        },
        {
            "title": "A PROMPT TEMPLATE",
            "content": "We provide the prompt templates used for evaluation and revision. These prompts are either taken directly from MT-Bench or adapted from it, ensuring the universality of our proposed paradigm. 15 Preprint. Figure 6: The prompt of reference-free score rating evaluation. Figure 7: The prompt of reference-based score rating evaluation. Figure 8: The prompt of LLM-as-a-reviser for pairwise comparison. 16 Preprint. Figure 9: The prompt of LLM-as-a-reviser for score rating. Figure 10: The prompt of LLM direct response to instruction. Table 8: The Statistics of NLG Evaluation Training Data. Task Aspects Samples Items Evaluation Items Summarization Translation Data2Text Story Generation fluency,consistency,coherence,relevance accuracy accuracy,fluency fluency,consistency,style matching 2886 6000 3098 1052 11544 6000"
        },
        {
            "title": "B INFERENCE SETTING FOR PROPRIETARY MODEL",
            "content": "Base Model. We choose GPT-4 as the base model for our evaluation and revision. For reproducibility, we used the GPT-4 version GPT-4-TURBO-2024-04-09, with temperature setting of 0.0. FINETUNING SETTING FOR OPEN-SOURCED MODEL Base Model. We choose LLAMA 3.1-8B-INST 1 as the base model for our evaluation and revision. Here, we want to clarify that we choose the INSTRUCT model as the base model because finetuning on this model yields better evaluation and revision results than the PRETRAINED model. Training Setting. We followed the common setup for supervised instruction finetuning, with context length = 2048, epochs = 3, batch size = 128, and learning rate = 2e 5. Distilling Setting. Whether finetuning open-source models for evaluation or revision capabilities, the training data comes from the generation of powerful model prompted by the same data source. The model we choose to distill is still GPT-4, with the same version and inference settings as mentioned above. Distilling Data Source. We depict the distilling data flow in Figure 11. For NLG evaluation, ideally, we would have variety of erroneous samples along with human evaluation scores for them. However, such data typically exists only in test sets, making it unavailable for training and often limited in quantity. Therefore, we choose MetricInstruct 2, proposed by Jiang et al. (2024a), as our 1https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct 2https://huggingface.co/datasets/TIGER-Lab/MetricInstruct 17 Preprint. Figure 11: In our data distillation process for open-source LLMs, we utilize HH-rlhf and MetricInstruct as the primary data sources. We then employ proprietary LLM to perform RevisEval, generating both the revisions and corresponding evaluation outputs. Finally, we fine-tune the opensource LLM using this enriched dataset. training data source. This dataset provides large volume of diverse erroneous texts, which serve as the basis for evaluation. From the 40K+ data points, we filter out other NLG tasks and apply our previously mentioned prompts with corresponding aspects, maintaining the same inference settings to generate evaluation scores and reasoning for these error samples. Detailed statistics are presented in the Table 8. Although the overall dataset size is relatively small compared to other works specifically designed to train evaluators, the NLG evaluation data we assess remains held-out from these training samples. Unlike NLG evaluation data that lacks human-labeled evaluation, preference data typically contains substantial human-labeled preference annotations without evaluation. We choose the most commonly used hh-rlhf 3 (Bai et al., 2022) dataset, applying the aforementioned prompts and inference settings to conduct evaluations on this data to get the evaluation. We select the preference correctness intersection of reference-free evaluation, reference-based evaluation, and gold preference annotations, ensuring both accuracy and fairness when comparing the performance of LLMs under different evaluation methods post-training. In the end, we selected 10, 000 samples, each containing corresponding revisions, reference-free evaluation, and reference-based evaluation. These three sets were then used to train the same model separately, ensuring that no new information is introduced to alter the distribution. Finetuning LLM-as-a-Judge adaptive to NLG. Each instance in filtered MetricInstruct is tuple (response, input context, task instruction, aspect), then use GPT-4 to get the corresponding (reference-free evaluation, revision text, REVEVO evaluation) to each instance. Subsequently, we separately use these data to finetune LLM to get the reference-free evaluator, LLM-as-a-reviser, and reference-based evaluator. Each instance in filtered hh-rlhf is tuple (instruction, response a, response b), then use GPT-4 to get the corresponding (reference-free evaluation, revision text, REVEVO evaluation) to each instance. Subsequently, we separately use these data to finetune LLM to get the reference-free evaluator, LLM-as-a-reviser, and reference-based evaluator. Decoding Setting. When the finetuned model executes the evaluation and revision tasks, the decoding setting uses greedy decoding strategy with max output length = 1024 and temperature = 0.01. 18 Preprint. Table 9: The Statistics of NLG Evaluation Benchmarks. Task Benchmark Response Source Inputs Items Samples Items Summarization Translation Data2Text Story Generation SummEval (Fabbri et al., 2021) WMT-22 (zh-en) (Kocmi et al., 2022) WebNLG-2020 (Zhou & Lampouras, 2020) OpenMEVA (ROC) (Guan et al., 2021) 16 Models 18 Models 18 Models 5 Models 100 1875 179 200 1600 33750"
        },
        {
            "title": "D BENCHMARKS",
            "content": "D.1 NLG EVALUATION BENCHMARKS Traditional text generation tasks and their corresponding evaluation benchmarks are highly diverse. Based on the varying degrees of freeform in text generation tasks, we select four representative Machine Translation, Data-to-Text, Summarization, and Story Generation. We follow the experiment setting of Tigerscore and choose specific benchmarks for each task, and their statistics are shown in the Table 9. D.2 INSTRUCTION FOLLOWING PREFERENCE BENCHMARKS With their powerful generalization capabilities, LLMs have become the focus of research on NLP generation abilities. Evaluating LLMs requires more challenging tasks that can assess their generalization capabilities. Here, we selected three representative benchmarks: MT-BENCH (abbr., MT-BENCH_HUMAN-JUDGEMENT), ALPACAFARM, and LLMBAR. MT-Bench. This dataset comprises 3.3K expert-level pairwise human evaluations of model responses, generated by six models across 80 MT-Bench questions. The six models include GPT-4, GPT-3.5, CLAUDE-V1, VICUNA-13B, ALPACA-13B, and LLAMA-13B, offering diverse representation of powerful language models. The topic of subtasks is consisted of Writing, Roleplay, Reasoning Math, Coding, Extraction, STEM and Humanities. MT-Bench is the most common benchmark for evaluating LLM-as-a-Judge, and our results validate the feasibility of our experiments. We select the first round of dialogues from this dataset as the evaluation data, containing 1284 cases. AlpacaFarm. We utilize HUMAN-CROSSANNOTATION 4 set specifically designed for evaluating the reliability of evaluators, following the ALPACAFARM process. Each instance in this dataset contains cross-annotations from 4 human experts. Additionally, the tasks in this dataset are more diverse, open-ended, and challenging, making the preference annotations more reliable. Notably, since four experts conduct the preference annotations, some instances resulted in ties, where two annotators favored the first option and the other two favored the second. We filtered out these tied cases, leaving final evaluation dataset of 501 instances. LLMBar. LLMBar is meta-evaluation benchmark designed to test how well LLM evaluators can identify instruction-following outputs. It consists of two parts: (1) The Natural set, which gathers instances from existing human-preference datasets, filtered and adjusted to ensure clear preference for each instance. (2) The Adversarial set, where the authors intentionally create misleading outputs that superficially seem good but deviate from the instructions, to challenge the evaluators. The Natural set measures performance in real-world conditions, while the Adversarial set tests evaluators ability to detect true instruction-following. The overall size is 419."
        },
        {
            "title": "E BASELINES IN NLG EVALUATION TASKS",
            "content": "N-gram Metrics. N-gram text generation metrics are commonly used to evaluate the text quality generated by models, especially in tasks like machine translation and summarization. While these metrics are simple and efficient, they come with notable limitations: they are highly sensitive to 3https://huggingface.co/datasets/Anthropic/hh-rlhf 4https://huggingface.co/datasets/tatsu-lab/alpaca_eval/blob/main/ alpaca_farm_human_crossannotations.json 19 Preprint. Table 10: The Lists of weak LLM-as-a-Judge. Model Base Model Instruction Response Annotation Evaluation Scheme Training Samples JudgeLM Vicuna-7B Alpaca-GPT4, Dolly-15K... PandaLM LLaMA-7B Alpaca 52K Auto-J LLaMA2-13B-chat Chatbot Arena, OpenAI WebGPT... 11 models (Alpaca, Vicuna...) 5 models (LLaMA, Bloom...) Preference Datasets GPT-4 Pairwise Grading 100K GPT3.5 Pairwise Selection 300K Human Pairwise Selection, Pointwise Grading Prometheus LLaMA2-7B-chat GPT-4 Generated GPT-4 Generated GPT-4 Pointwise Grading Prometheus-2 Mistral-7B-v2. GPT-4 Generated GPT-4 Generated GPT-4 Pointwise Grading Preference 4396 100K 300K surface-level differences, such as word order or vocabulary choice, which may fail to capture the true meaning or fluency of the generated text. In our evaluation, we use the most widely adopted metrics: BLEU, ROUGE-L, and METEOR. Model-based Metrics. To capture the semantic-level meaning of generated text, researchers have started using models like BERT and BART as the foundation for text evaluation. Typical examples include BERTScore, BARTScore, and Moverscore. BERTScore computes the similarity between two text sequences based on the contextual embeddings from BERT, while Moverscore enhances this by adding many-to-one alignment. BARTScore, on the other hand, uses BART to calculate the probability of converting the response to the reference text as score. All of these are reference-based model metrics. Additionally, there are reference-free model-based metrics. These metrics are trained on specific task datasets, allowing the model to internalize relevant information. As result, the model can generate evaluations without needing reference texts and transform them into scores. For instance, UNIEVAL uses data augmentation to expand task-specific dataset to 30K examples and fine-tunes on T5, which is why it performs exceptionally well in summarization tasks. LLM-as-a-Judge. Following the exciting advancements in large language models (LLMs), the most straightforward approach has been to replace the models in previous model-based metrics, such as BART, with larger models. GPTScore follows this concept, and despite its simplicity, it delivers notable results. Moreover, leveraging the vast internal knowledge of open-source LLMs, more powerful and interpretable evaluators can be developed, such as INSTRUCTScore and TIGERScore. Xu et al. (2023) argue that performing error analysis on given reference texts enhances evaluation explainability and reliability. They used NLG evaluation data and employed GPT-4 to perform error-based assessments. These outputs were then paired with the response to train an Llama model, resulting in training dataset of 10K examples. TIGERScore goes step further by proposing reference-free approach. Using similar strategy, they collected 40K data points for training, masking the reference text during the process."
        },
        {
            "title": "F BASELINES IN INSTRUCTION FOLLOWING PREFERENCE BENCHMARKS",
            "content": "LLM-as-a-Judges. The baseline models are listed in the Table 10. Considering scalability and cost, researchers have long sought to achieve evaluation performance on weaker LLMs that is comparable to that of stronger LLMs. The most straightforward approach to this challenge has been to automatically generate more preference-related data, and many of these efforts have followed this strategy. N-gram Metrics & Model-based Metrics. Unlike the NLG-Evaluation benchmark, the LLM-asJudge benchmark has largely moved away from using n-gram metrics and model-based metrics. This shift is due to several characteristics of the test samples in such benchmarks: (1) The response space is extremely large and unconstrained, making reference annotations both unhelpful and prohibitively expensive; (2) These metrics do not perform well for tasks such as coding or math, where even models like BERT struggle to capture semantic-level meaning. In this study, we apply these metrics 20 Preprint. Table 11: Pearson correlation coefficients comparing non-reference, static-reference, and dynamicreference methods across various text generation tasks and Instruction-Following Benchmarks. This table summarizes the performance of these methods in generating summarization, translation, datato-text, and story-generation tasks. Methods SUMMARIZATION TRANSLATION DATA2TEXT STORY GENERATION Avg. BLEU ROUGE METEOR BERTScore BARTScore UniEval GPTScore InstructScore-7B TIGERScore-7B Llama-3.1 8B-Instruct Ref-Free Ref-Based REVEVO(Ours) Ref-Free Ref-Based REVEVO(Ours) n-gram Metrics 17.47 16.26 18.80 Reference-free Metrics 37.65 29.04 23.11 21.05 51.55 37.70 27. 34.29 35.85 36.30 48.22 47.89 51.14 48.70 47.28 49.13 31.15 Open-Sourced LLM-as-a-Judge 25.14 29.99 29.58 53.36 48.52 53.87 Proprietary LLM-as-a-Judge 41.35 44.11 43.92 54.26 53.98 54.25 14.13 15.36 18.69 26.26 19.73 53.22 13.47 27.40 43.95 25.89 33.61 42.32 39.69 42.12 43.31 43. -3.89 -0.22 -1.02 26.58 17.76 44.88 18.94 12.81 39.90 31.04 35.02 11.92 29.04 33.50 24.63 35.07 15.50 16.81 18.19 34.68 28.61 43.09 25.54 34.76 42.67 28. 36.78 33.19 38.05 43.56 41.51 44.26 to demonstrate that, when reference texts are highly relevant, these otherwise inapplicable metrics can be reactivated and produce meaningful results."
        },
        {
            "title": "G META EVALUATION",
            "content": "Meta-evaluation aims to assess the performance of automated metrics by measuring how well the automated evaluations yauto align with human evaluations yhuman. For score ratings, we calculate the correlation values across all samples, represented as: Corr = (cid:0)[y1 auto, . . . , yn auto], [y1 human, . . . , yn human](cid:1) , where can adopt various correlation functions, e.g., Spearman. For pair-wise comparison evaluations, accuracy is typically used as the evaluation metric, Acc = 1 (cid:80) (i,oi 1,oi 2)N I[yi auto = pi] NLG Tasks. In the NLG evaluation task, its crucial to assess various rubrics of the text during the evaluation process. For the four selected tasks, weve outlined the specific rubrics to be evaluated in the accompanying Table 8. In both the SummEval and Story-Generation tasks, we evaluate multiple rubrics independently, calculating the correlation coefficient for each one. Subsequently, we compute the average correlation coefficient across all rubrics to obtain an overall assessment for each task. This comprehensive approach ensures more nuanced and accurate evaluation of the models performance across different dimensions."
        },
        {
            "title": "H PEARSON CORRELATION IN NLG EVALUATION TASKS",
            "content": "We also supplement the Pearson Correlation results in the NLG Evaluation Tasks. 21 Preprint. Table 12: Statistics on the differential proportion between REVISEVAL and each of the other two evaluation methods. higher ratio indicates greater evaluation difference between the two mechanisms. Comparative Evaluation MT-BENCH ALPACAFARM LLMBAR Ref-Free Ref-Based (GPT-4 Direct Response) 8.1 9.7 22.8 21.6 16.9 16."
        },
        {
            "title": "I OTHER ANALYSIS",
            "content": "I.1 THE BENEFITS OF TRAINING RESOURCE SCALE FOR LLM-AS-A-JUDGE ARE QUESTIONABLE. As shown in Table 2 and 10, Despite being trained with extensive evaluation-specific resources, these LLM-as-a-Judge baselines fail to achieve evaluation performance comparable to GPT-4, particularly on the adversarially designed LLMBar, where they perform worse than random selection. While substantial effort is put into designing and generating large amount of training data for these LLMs, the results are even less effective than our evaluator trained on just 10,000 samples from the hh-rlhf dataset. The possible reasons for this could be: 1. The inherent capabilities of the base model play more crucial role; 2. Simply increasing the volume of training data does not yield significant benefits; 3. Efforts should be focused on other potentials to enhance the evaluator, such as the method we propose. I.2 OTHER REVISION STRATEGIES FOR PAIRWISE COMPARISON When revising two given responses to generate reference, we experiment with two different revision strategies. In our work, we adopt strategy where one text is randomly selected as the primary text to be revised, while the other serves as the revision guidance. In addition to this, we try the following two prompt strategies: a) Revising single text based on both responses. b) Revising each response separately. However, the outcomes of these two strategies are unsatisfactory. Strategy (a) exhibit tendency to forcibly merge the two responses during the revision process, resulting in generated text that lacked logical consistency. Strategy (b), on the other hand, lead to revised text with very low similarity to the other response, inevitably favoring the one chosen for revision. Neither method is capable of producing satisfactory reference. We anticipate that future research might provide more revision strategies or approaches to more effectively combine multiple responses and generate high-quality reference. We look forward to seeing further developments in this area. I.3 EVALUATION DIFFERENTIAL OF DIFFERENT METHODS We analyse the differential between REVISEVAL and two other evaluation methods, showcasing the proportion of samples where the evaluation decisions differed. higher proportion indicates greater difference in the evaluation mechanisms of the two methods. The Table 12 present that REVISEVAL and the other two evaluation methods make different decisions on 22% of the samples in AlpacaFarm and 16% of the samples in LLMbar. This indicates that our evaluation method has significant difference in mechanism-level, compared to the other two methods. Furthermore, it suggests aggregating these differing decisions could potentially lead to more reliable final evaluation."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "Huawei Noahs Ark Lab",
        "McGill University & MILA",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}