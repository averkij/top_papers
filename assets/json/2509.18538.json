{
    "paper_title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts",
    "authors": [
        "Zixin Zhu",
        "Haoxiang Li",
        "Xuelu Feng",
        "He Wu",
        "Chunming Qiao",
        "Junsong Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Towards intelligent image editing, object removal should eliminate both the target object and its causal visual artifacts, such as shadows and reflections. However, existing image appearance-based methods either follow strictly mask-aligned training and fail to remove these causal effects which are not explicitly masked, or adopt loosely mask-aligned strategies that lack controllability and may unintentionally over-erase other objects. We identify that these limitations stem from ignoring the causal relationship between an object's geometry presence and its visual effects. To address this limitation, we propose a geometry-aware two-stage framework that decouples object removal into (1) geometry removal and (2) appearance rendering. In the first stage, we remove the object directly from the geometry (e.g., depth) using strictly mask-aligned supervision, enabling structure-aware editing with strong geometric constraints. In the second stage, we render a photorealistic RGB image conditioned on the updated geometry, where causal visual effects are considered implicitly as a result of the modified 3D geometry. To guide learning in the geometry removal stage, we introduce a preference-driven objective based on positive and negative sample pairs, encouraging the model to remove objects as well as their causal visual artifacts while avoiding new structural insertions. Extensive experiments demonstrate that our method achieves state-of-the-art performance in removing both objects and their associated artifacts on two popular benchmarks. The code is available at https://github.com/buxiangzhiren/GeoRemover."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 3 5 8 1 . 9 0 5 2 : r GeoRemover: Removing Objects and Their Causal Visual Artifacts Zixin Zhu1,2 Haoxiang Li2 Xuelu Feng1 He Wu2 Chunming Qiao1 Junsong Yuan1 1University at Buffalo 2Pixocial Technology {zixinzhu, xuelufen, qiao, jsyuan}@buffalo.edu, haoxiang.li@pixocial.com, heu199825@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Towards intelligent image editing, object removal should eliminate both the target object and its causal visual artifacts, such as shadows and reflections. However, existing image appearance-based methods either follow strictly mask-aligned training and fail to remove these casual effects which are not explicitly masked, or adopt loosely mask-aligned strategies that lack controllability and may unintentionally over-erase other objects. We identify that these limitations stem from ignoring the causal relationship between an objects geometry presence and its visual effects. To address this limitation, we propose geometry-aware two-stage framework that decouples object removal into (1) geometry removal and (2) appearance rendering. In the first stage, we remove the object directly from the geometry (e.g., depth) using strictly mask-aligned supervision, enabling structure-aware editing with strong geometric constraints. In the second stage, we render photorealistic RGB image conditioned on the updated geometry, where causal visual effects are considered implicitly as result of the modified 3D geometry. To guide learning in the geometry removal stage, we introduce preference-driven objective based on positive and negative sample pairs, encouraging the model to remove objects as well as their causal visual artifacts while avoiding new structural insertions. Extensive experiments demonstrate that our method achieves state-of-the-art performance in removing both objects and their associated artifacts on two popular benchmarks. The code is available at https://github.com/buxiangzhiren/GeoRemover."
        },
        {
            "title": "Introduction",
            "content": "Object removal is challenging computer vision task with applications in image editing and scene rendering, aiming to erase undesired objects as if they never present. Following the inpainting framework [1, 2, 3, 4], traditional strictly mask-aligned approaches [5, 6, 7, 8] assume that user specified mask fully covers the objects to be removed, thus only deal with the masked region while do not change the remained image. However, in real-world scenarios, objects often cast causal visual artifacts (e.g., shadows and reflections) onto surrounding regions, leading to illumination inconsistencies beyond the masked area. As illustrated in Fig. 1a, although the child is successfully removed, his shadow remains as the causal artifact. simple solution to address such an associated artifact is to extend the object removal mask to cover these artifacts, but this places significant burden on users, who must identify and annotate all subtle, detached, and ambiguous artifacts. As result, this approach is neither scalable nor user-friendly. Therefore, recent methods [9, 10, 11] assume more practical and user-friendly setting where the input mask only covers the objects to remove, but the model implicitly infers and removes causal visual artifacts such as shadows and reflections in an intelligent way. For example, previous methods Work completed while the author was an intern at Pixocial Technology. Corresponding author. 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: Comparison of object removal training paradigms. (a) Strictly mask-aligned training edits only masked regions but leaves causal visual artifacts (shadow) unaddressed. (b) Loosely mask-aligned training allows broader context-aware corrections but lacks clear guidance, leading to confusion and uncontrollable edits. (c) Our method decouples geometry and appearance for object removal: we first edit the scene geometric representation (in the form of depth map) under strictly mask-aligned supervision, then render realistic image where both objects and causal visual artifacts (shadow) are cleanly removed. have attempted to adopt loosely mask-aligned training strategies, encouraging models to infer and correct inconsistencies beyond explicitly masked regions specified by the user. However, without specific design to guide the editing, most of them heavily rely on paired training data, which hinders controllability. Compared to strictly mask-aligned training, where the mask explicitly defines which regions can be modified and which must be preserved, in loosely mask-aligned settings, both the masked and unmasked regions may require edits, but the model itself has no clear boundary guidance, leading to confusion about where modifications should occur. As shown in Fig. 1b, while the model successfully removes the childs shadow, it also mistakenly removes the nearby adult, resulting in unintended alterations to the scene. The above challenges suggest that solely optimizing training strategies is insufficient to enable models to reason about causal visual effects. We make key observation: these effects, such as shadows and reflections, are fundamentally caused by the objects geometry under specific lighting conditions. In other words, the geometry presence is the cause, and the causal visual effects are its consequence. Intuitively, if the objects presence is removed from the scene geometry, then its associated illumination effects should no longer exist. This insight motivates us to rethink object removal as causal reasoning process: we firstly modify the geometric representation (e.g., via modifying the depth maps) to remove the object presence from the scene geometry; then, we render new image appearance based on the updated scene geometry, where causal visual artifacts will be naturally removed. This progressive design offers two key advantages. First, in the geometry removal stage, we can adopt strictly mask-aligned training: since causal visual artifacts do not need to be considered in the geometry domain, and object boundaries are clearly defined, thus the model can focus on removing only the object in masked region, making the task well-posed with strong supervision. This eliminates the risk of undesired modifications to unmasked regions. Second, in the rendering stage, the absence of the object naturally leads to the 2 removal of its associated artifacts. This implicitly enforces causal consistency by removing both the object and its visual artifacts. To enable this behavior, we train the rendering model with paired data: each pair consists of an image with the object and its causal effects (e.g., shadows or reflections), and the corresponding image shows the scene with both the object and its effects removed. By first localizing the removed object based on geometric differences, the rendering model can then establish the causal relationship between the object and its associated effects by analyzing the visual differences between the paired images. As shown in Fig. 1c, our method successfully removes both the object and its shadow, while preserving nearby unmasked content. Our contributions can be summarized as: We propose new two-stage framework to leverage geometric representation to decouple object removal into geometry removal and appearance rendering. Based on our observation that the geometric representation is free from causal visual artifacts, our method erases masked objects from the scene geometry followed by the removal of their visual artifacts. To improve object removal quality in the geometric representation, we introduce preferenceguided loss to prevent the model from inserting unexpected structures. Compared to existing methods that utilize loosely mask-aligned training strategies to approach this problem in the same setting, which mostly suffer from loss of controllability and the unintended alteration issue, we demonstrate through experiments that the proposed framework improves the removal quality on two benchmark datasets."
        },
        {
            "title": "2 Method",
            "content": "2.1 Problem formulation Given an input image RHW 3 and an object mask {0, 1}HW indicating the region to be removed, our goal is to generate an output image + RHW 3 in which the object has been cleanly erased, its contextual effects (e.g., shadows or reflections) are removed, and the background is realistically restored. Most existing methods formulate this problem as direct image-to-image transformation task, learning mapping + = g(I , ), (1) where is function that maps the masked image and mask to completed result. However, as illustrated in Fig. 1a and Fig. 1b, such formulations often entangle geometric reasoning with appearance synthesis, making it difficult to control structural edits and leading to unintended modifications. To address this, we approach this problem by decoupling it into two sub-tasks: (1) geometry removal, which modifies the geometric representation to eliminate the object while preserving the surrounding structure; and (2) appearance rendering, which synthesizes an RGB image consistent with the updated geometry from geometry removal. This decoupling allows us to separate structure-level editing from pixel-level synthesis, and enables causal visual artifacts to be implicitly corrected through geometry-aware rendering. Formally, we decompose the object removal process as 0 = D(I ), (cid:124) x+ 0 = sθ(x (cid:123)(cid:122) Stage 1: geometry removal , 0 , ) (cid:125) 0 , x+ + = G(I , , 0 ) (cid:124) (cid:125) (cid:123)(cid:122) Stage 2: appearance rendering (2) 0 = sθ(x where is geometry estimator, 0 is the estimated geometric representation of the input image, x+ 0 , ) is the updated geometry predicted by the diffusion model sθ under strictly mask-aligned supervision, and synthesizes the final RGB output conditioned on the geometric transformation from 0 and the input image . 0 to x+ 2.2 Stage 1: geometry removal Geometry completion with strictly mask-aligned training. In the first stage, our goal is to remove the target object by modifying the scene geometry, while preserving the surrounding structure. We use depth as the geometric representation in this work due to the efficiency and accuracy of recent depth estimation models. Geometry removal is performed in the depth domain, where causal visual artifacts such as shadows and reflections do not appear, making the task well-suited for strictly mask-aligned supervision. The overall training pipeline is illustrated in Fig. 2. Formally, given an input RGB image I, an estimated depth map x0, and an object mask {0, 1}HW indicating the removal region, Figure 2: The training framework of Stage 1: Geometry Removal. Given an input image and object mask, we first estimate the geometric representation (in the form of depth map) and construct masked geometry input. The masked depth map, together with the mask, is then fed into diffusion model to predict the edited geometry. To discourage structure insertion and encourage object removal, we construct two geometry completion paths: positive path where the object is successfully removed with smooth depth flow, and negative path where the object remains with sharp depth transitions. The model is trained to prefer the positive path and suppress the negative one. the objective is to learn model that predicts an edited depth map ˆx0 where the object is removed within the masked region, while preserving geometry elsewhere. We enforce the constraint ˆx0(i, j) = x0(i, j), (i, j) where (i, j) = 0. (3) naive solution to this task is to treat the depth map as colorized image and fine-tune pre-trained diffusion-based image inpainting model for depth editing. To maximize the log-likelihood log p(x0 c), diffusion-based models optimize denoising score matching objective, which minimizes the discrepancy between the model-predicted score sθ(xt, t, c) and the true score xt log p(xt x0, c). Here, sθ is parameterized score function, and = (M, (1 ) x0) denotes the conditioning input (i.e., the object mask and the masked depth map). The score matching loss is (cid:2)w(t) sθ(xt, t, c) xt log p(xt x0, c)2(cid:3) , LDSM(x0, c) = Et,ϵ (4) where xt is noisy sample generated from x0 and w(t) is weighting function over timesteps. Preference-guided geometry completion via DPO. However, when applying this baseline directly, we observe that the model often hallucinates new structures within the masked region, as shown in the second row of Fig. 3a. Rather than recovering coherent surface, it tends to insert unrealistic geometry that does not align with the surrounding structure. We hypothesize that this behavior arises from the lack of geometry-aware constraints: without explicit structural supervision, the model cannot distinguish between completing missing surfaces and generating new, implausible content. To avoid hallucinating new content within the object removal region, inspired by recent advances in Direct Preference Optimization (DPO) [12], we propose to model geometry removal through reward-based framework. DPO aims to align model outputs with user preferences by optimizing over ranked sample pairs, rather than relying solely on explicit ground-truth labels. In our setting, we adopt similar philosophy: we define preferences over geometry, where the depth in the masked region that does not contain the object is what we prefer, and the depth that includes the object is what we do not prefer. Ideally, preferred depth map should be locally smooth inside the mask, with minimal abrupt depth changes that would otherwise indicate the presence of an object. As shown in Fig. 2, when the masked region contains an object (e.g., dog), the depth flow, defined as the spatial gradient of depth values, exhibits sharp discontinuities due to the objects geometry. In contrast, when the object is successfully removed from the mask, the depth flow approaches zero, indicating smooth and coherent surface. Therefore, we consider low depth flow within the mask as key signal for realistic and desirable geometry. We define the reward as monotonic function of log p(x0 c), based on flow difference between the predicted depth map ˆx0 and ground-truth x0, measured by the flow loss Lflow(ˆx0, x0). The reward is r(c, x0) = Lflow(ˆx0, x0) = (log p(x0 c)) , with > 0. (5) 4 (a) Effect of direct preference optimization (DPO) in Stage 1. (b) Stage 2: Appearance rendering. Figure 3: (a) We compare model outputs trained with and without our DPO objective (i.e., LBT). Without DPO, the model often inserts or retains undesired content in the masked region, leading to unrealistic geometry. With DPO, the model learns to prefer geometry completions that successfully remove the object while preserving surrounding structures. (b) The training framework of Stage 2. Given geometry-aware conditions (e.g., depth maps), we train diffusion model to perform image translation for both object removal and insertion. More details can be found in Appendix B. Then we introduce how we define the flow loss Lflow(ˆx0, x0). Specifically, let dij denote the depth value at pixel (i, j). The flow at pixel (i, j) is defined as the first-order spatial gradient Fij(x) = {di+1,j di,j, di,j+1 di,j} , (6) which captures local depth transitions in horizontal and vertical directions. Then the flow loss is defined as the average of per-pixel absolute differences between the predicted flow and the groundtruth flow. It is Lflow(ˆx0, x0) = Fij(ˆx0) Fij(x0)1 , (7) 1 Ω (cid:88) (i,j)Ω where Ω denotes the set of valid pixels. With the reward r(c, x0), to model preference between completions, we assume access to ranked sample pairs (x+ 0 is preferred over 0 under the same conditioning c. As illustrated in Fig. 2, we refer to these as the positive and negative geometry paths, respectively. We adopt the Bradley-Terry (BT) model to express the preference probability 0 ), indicating that x+ 0 , LBT = c,x+ 0 ,x 0 (cid:2)log σ (cid:0)r(c, x+ 0 ) r(c, 0 )(cid:1)(cid:3) . (8) The final loss combines the standard diffusion loss with the preference-guided objective, which is = LDSM + λ LBT, where λ balances score-based denoising supervision and geometry-consistent preference learning. (9) 2.3 Stage 2: appearance rendering In the second stage, our goal is to generate realistic RGB image that reflects the scene after object removal, as defined by the updated geometry from Sec. 2.2. We formulate this task as conditional image translation problem, where the appearance of the output image is controlled by the geometric transformation between two depth maps. As illustrated in Fig. 3b, the model takes as input masked RGB image and two geometry-aware conditions: Condition1 and Condition2. Both conditions are represented as depth maps: Condition1 encodes the geometry of the input image (e.g., with an object present), while Condition2 defines the target geometry (e.g., with the object removed). The difference between Condition1 and Condition2 specifies the structural transformation to be performed. For example, if Condition2 removes an object that exists in Condition1, the model is guided to erase that object and inpaint realistic background. Conversely, if Condition2 introduces localized depth discontinuity compared to flat Condition1, the model learns to insert visually plausible object. Formally, we define as geometry-conditioned image translation model based on diffusion backbone. The model takes as input RGB image and pair of depth maps indicating the geometry 5 Table 1: Comparison with state-of-the-art methods on RemovalBench and RORD-Val. Method RemovalBench RORD-Val FID CMMD LPIPS PSNR AS FID CMMD LPIPS PSNR AS 108.38 ZITS++ [23] 123.78 MAT [24] 99.88 LaMa [25] RePaint [26] 102.65 128.66 BLD [27] 108.79 LDM [7] 119.60 SD-Inpaint [7] 104.97 SDXL-Inpaint [7] 120.97 BrushNet [17] 115.79 FLUX.1-Fill [8] 114.55 PowerPaint [28] CLIPAway [5] 108.40 Attentive-Eraser [29] 55.49 39.52 OmniEraser [9] 29.88 Ours 0.374 0.366 0.351 0.741 0.553 0.365 0.419 0.398 0.549 0.487 0.392 0.272 0.232 0.208 0.089 0.158 0.164 0.156 0.378 0.233 0.157 0.274 0.187 0.191 0.193 0.240 0.254 0.146 0.133 0. 19.62 17.88 18.72 19.86 17.43 19.24 17.02 17.87 18.68 17.12 18.25 18.78 20.60 21.11 25.52 4.56 107.44 4.51 136.53 4.55 100.21 4.38 114.64 4.39 224.61 4.47 128.19 4.48 143.69 4.63 147.01 4.63 234.87 4.59 141.39 4.56 102.33 4.48 81.28 4.50 96.77 4.66 43.71 4.54 31.15 0.448 0.455 0.294 2.345 0.862 0.506 0.494 0.460 0.745 0.450 0.408 0.545 0.233 0.153 0.182 0.274 0.281 0.229 0.525 0.273 0.221 0.308 0.210 0.293 0.217 0.241 0.278 0.221 0.166 0.103 21.17 19.18 20.50 17.68 17.13 19.02 16.83 17.69 16.51 18.50 18.29 16.36 20.24 22.13 23.70 4.12 4.38 4.23 4.71 4.74 4.12 4.61 4.76 4.41 4.55 4.38 4.19 4.77 4.99 4. before and after editing. Let and + denote the input and output images, and let 0 and x+ 0 represent the corresponding depth maps before and after editing. The model learns to perform both object removal and insertion through the following bidirectional formulation: + = G(I 0 , x+ 0 ), = G(I + x+ 0 , 0 ). (10)"
        },
        {
            "title": "3 Experiments",
            "content": "Implementation details. The depth estimator is implemented using Depth Anything [13]. For both geometry removal model (sθ) and appearance rendering model (G), we adopt FLUX.1-Filldev [8] as the pre-trained diffusion backbone, and apply LoRA [14] fine-tuning with rank of 64. All images are processed at resolution of 10241024. For both stages, we use batch size of 24, learning rate of 1 104, and guidance scale of 1.0. The text prompt beautiful scene is used during both training and inference. Stage 1 is trained for 17,000 steps on 8 NVIDIA H100 GPUs, taking approximately 24 hours, while Stage 2 requires around 60 hours for the same number of steps. Datasets & Metrics. We use the training set from the RORD [15] dataset as our primary training data. RORD is large-scale real-world object removal dataset consisting of 516,705 images captured under 3,447 unique indoor scenes. Each scene contains paired images with and without the target object, along with manually annotated object masks. The dataset is designed to support training and evaluation for object removal and scene completion tasks in realistic environments. For evaluation, we follow prior works such as SmartEraser [16] and OmniEraser [9], We use both RORD-Val and RemovalBench [9] as our primary benchmarks. Moreover, we follow prior works [17, 9] and adopt set of metrics to evaluate image generation quality. We use Frechet Inception Distance (FID)[18], CLIP Maximum Mean Discrepancy (CMMD)[19], Aesthetic Score (AS)[20], Learned Perceptual Image Patch Similarity (LPIPS)[21] and Peak Signal-to-Noise Ratio (PSNR) [22]. 3.1 Comparison with SOTA methods We compare our method against state-of-the-art approaches on the RemovalBench and RORD-Val datasets, as shown in Tab. 1. These baselines fall into two categories: strictly mask-aligned methods and loosely mask-aligned methods. Strictly mask-aligned methods [26, 7, 24, 5, 29] are limited in their ability to handle contextual effects, since they are confined to the object region defined by the user. In contrast, loosely mask-aligned methods can adaptively clean surrounding regions affected by the object. Among loosely mask-aligned approaches, OmniEraser [9] is the only open-source method that supports causal visual artifacts removal. Although models like OmniPaint [11] and ObjectDrop [10] also aim to remove such effects, their code and models are not publicly available, and we were therefore unable to include them in our evaluation. Across both benchmarks, our method 6 Table 2: Ablation study on RORD-Val to evaluate the effectiveness of our design components. Insert. denotes the percentage of cases where new object is wrongly inserted into the removal region. Table 3: Geometry removal accuracy (MAE in masked region) on RORD-Val."
        },
        {
            "title": "Method",
            "content": "FID CMMD LPIPS PSNR AS Insert."
        },
        {
            "title": "Method",
            "content": "MAE 56.24 0.577 One-Stage Two-Stage w/o DPO 34.24 0.230 Two-Stage w/ DPO 31.15 0.182 0.315 0.131 0.103 17.52 4.27 2.81% 22.81 4.51 5.09% 23.70 4.69 1.48%"
        },
        {
            "title": "0.0827\nInput depth\nTwo-Stage w/o DPO 0.0490\nTwo-Stage w/ DPO 0.0387",
            "content": "Table 4: Removal performance of causal artifacts on CausRem. Table 5: Ablation study on the RORD-Val dataset comparing unidirectional and bidirectional rendering strategies in Stage 2."
        },
        {
            "title": "Method",
            "content": "IoU%"
        },
        {
            "title": "Method",
            "content": "FID CMMD LPIPS PSNR AS OmniEraser [9] Ours 68.29 73.76 Unidirectional rendering 38.43 Bidirectional rendering 31.15 0.215 0.182 0.136 0. 23.58 4.19 23.70 4.69 consistently achieves the best scores in FID, CMMD, LPIPS, and PSNR, demonstrating superior visual quality and structure preservation in the removed regions. 3.2 Ablation study and discussion Is the two-stage design necessary? Compared to prior one-stage approaches, our method introduces two key innovations: (1) two-stage design that explicitly decouples geometry and appearance, and (2) the incorporation of geometric cues such as depth to guide object removal. To fairly isolate the contribution of the two-stage architecture, we construct one-stage version of our method that also takes the masked RGB image and masked depth map as input to the diffusion model. This ensures that both models operate on the same input modalities, and any performance gap can be attributed to the architectural difference. As shown in Fig. 4, despite access to depth information, the one-stage model often produces ambiguous or distorted edits due to the lack of explicit geometric guidance. Quantitative results in Tab. 2 further confirm that the one-stage variant consistently underperforms the two-stage models across multiple metrics. This supports the claim that it is the decoupled design, rather than merely the inclusion of depth, that enables our model to reason more effectively. While the two-stage design increases runtime, the gains in controllability and illumination consistency justify the cost. How does the DPO strategy help our model achieve better removal? To evaluate the effect of Direct Preference Optimization (DPO), we compare our two-stage model with and without DPO supervision (LBT). As shown in Tab. 2, DPO significantly reduces the Insert. rate from 5.08% to 1.48%, indicating its effectiveness in suppressing semantic hallucinations. This result suggests that preference-driven learning helps the model better align with human expectations for clean and plausible object removal. How accurate is geometry removal in Stage 1? To evaluate geometry removal quality, we compute the mean absolute error (MAE) between the predicted and ground-truth depth maps within the masked region, which corresponds to the object intended for removal. For reference, we also report the MAE between the input depth map and ground-truth depth map in the same region, providing baseline for understanding the original geometric discrepancy. As shown in Tab. 3, our two-stage model significantly reduces the depth error within the masked region, and incorporating DPO further improves removal precision. How effectively does our model remove causal visual artifacts? To evaluate our methods ability to remove causal visual artifacts, we construct the CausRem benchmark, consisting of 200 real-world images (100 with shadows, 100 with reflections). Each image is manually annotated with object masks and corresponding artifact masks (shadows or reflections). Representative samples are provided in the supplementary material. To estimate where the model implicitly removes such artifacts, we compute the pixel-wise absolute difference between the input and output within the annotated artifact regions. threshold is then applied to identify significantly altered pixels, indicating predicted residue areas. 7 (a) Results from our one-stage model. (b) Results from our two-stage model Figure 4: Comparison between our one-stage and two-stage object removal strategies. Two-stage design improves edit quality by separating geometry reasoning from appearance generation. (a) Failure cases under motion blur conditions. (b) Improved results after applying Fill-in strategy. Figure 5: Depth errors caused by motion blur result in removal failure. Applying simple Fill-in strategy within the mask restores geometric contrast and yields correct removal. To set robust threshold, we analyze the boundary regions of the annotated causal visual artifact masks, where pixel values transition between the artifact and the background. We find the average difference in these regions to be approximately 20, which we adopt as fixed global threshold. We evaluate the predicted residue regions using IoU against the ground truth. As shown in Tab. 4, our method achieves 73.76% IoU, outperforming the previous state-of-the-art OmniEraser [9] at 68.29%. Does bidirectional rendering improve performance in Stage 2? Tab. 5 demonstrates that bidirectional rendering enhances Stage 2 performance by promoting more precise alignment between the refined geometry and the synthesized image. By enforcing consistency in both removal and insertion rendering directions, the model is encouraged to maintain structural coherence throughout the image, leading to improved visual quality and reduced artifacts in the final output. 3.3 Qualitative comparison on CausRem Fig. 6 shows visual comparison of object and causal artifact removal (e.g., shadows, reflections) on the CausRem dataset. Compared with state-of-the-art methods, our approach yields cleaner results. Prior methods like Attentive-Eraser and CLIPAway often leave shadows or blur the background, while OmniEraser may distort nearby textures. In contrast, our method removes both objects and their effects cleanly by leveraging geometry-guided rendering, preserving background structure."
        },
        {
            "title": "4 Failure case",
            "content": "When Stage 1 produces unreliable or effectively unchanged depth within the masked regione.g., due to fast motion, translucency, specular/reflective surfaces, occlusions, or low texturethe geometryremoved depth becomes nearly indistinguishable from the input (Fig. 5a). Because Stage 2 identifies removal targets by differencing these two depth maps, the lack of geometric contrast prevents it from triggering removal. We address this with simple Local Max Depth Fill-in: for masked pixels lacking reliable estimates, we propagate the maximum depth from small local neighborhood (e.g., 1010 pixels). This lightweight completion restores boundary contrast and enables Stage 2 to remove the target while preserving coherent background (Fig. 5b). 8 Figure 6: Qualitative comparison with state-of-the-art methods on CausRem."
        },
        {
            "title": "5 Conclusion",
            "content": "We present geometry-aware, two-stage framework for object removal that effectively handles both the primary object and its associated causal visual artifacts. By decoupling the task into geometry removal and appearance rendering, our method achieves precise structural editing and seamless image restoration. Extensive experiments across multiple benchmarks validate that our approach outperforms existing methods in both quantitative metrics and visual quality, especially in challenging cases involving shadows and reflections."
        },
        {
            "title": "References",
            "content": "[1] Ciprian Corneanu, Raghudeep Gadde, and Aleix Martinez. Latentpaint: Image inpainting in latent space with diffusion models. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 43344343, 2024. [2] Haipeng Liu, Yang Wang, Biao Qian, Meng Wang, and Yong Rui. Structure matters: Tackling the semantic discrepancy in diffusion models for image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80388047, 2024. [3] Yingchen Yu, Fangneng Zhan, Shijian Lu, Jianxiong Pan, Feiying Ma, Xuansong Xie, and Chunyan Miao. Wavefill: wavelet-based generation network for image inpainting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 14114 14123, 2021. [4] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2242822437, 2023. [5] Yigit Ekin, Ahmet Burak Yildirim, Erdem Eren Çaglar, Aykut Erdem, Erkut Erdem, and Aysegul Dundar. Clipaway: Harmonizing focused embeddings for removing objects via diffusion models. Advances in Neural Information Processing Systems, 37:1757217601, 2024. [6] Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut Erdem, and Aysegul Dundar. Instinpaint: Instructing to remove objects with diffusion models. arXiv preprint arXiv:2304.03246, 2023. [7] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [8] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [9] Runpu Wei, Zijin Yin, Shuo Zhang, Lanxiang Zhou, Xueyi Wang, Chao Ban, Tianwei Cao, Hao Sun, Zhongjiang He, Kongming Liang, and Zhanyu Ma. Omnieraser: Remove objects and their effects in images with paired video-frame data. arXiv preprint arXiv:2501.07397, 2025. [10] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion. In European Conference on Computer Vision, pages 112129. Springer, 2024. [11] Yongsheng Yu, Ziyun Zeng, Haitian Zheng, and Jiebo Luo. Omnipaint: Mastering objectoriented editing via disentangled insertion-removal inpainting. arXiv preprint arXiv:2503.08677, 2025. [12] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. [13] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:21875 21911, 2024. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [15] Min-Cheol Sagong, Yoon-Jae Yeo, Seung-Won Jung, and Sung-Jea Ko. Rord: real-world object removal dataset. In BMVC, page 542, 2022. [16] Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, and Houqiang Li. Smarteraser: Remove anything from images using masked-region guidance. arXiv preprint arXiv:2501.08279, 2025. [17] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In European Conference on Computer Vision, pages 150168. Springer, 2024. [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [19] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93079315, 2024. [20] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [21] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [22] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. [23] Chenjie Cao, Qiaole Dong, and Yanwei Fu. Zits++: Image inpainting by improving the incremental transformer on structural priors. IEEE transactions on pattern analysis and machine intelligence, 45(10):1266712684, 2023. [24] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. Mat: Mask-aware transformer for large hole image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1075810768, 2022. [25] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. In Proceedings of the Resolution-robust large mask inpainting with fourier convolutions. IEEE/CVF winter conference on applications of computer vision, pages 21492159, 2022. [26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. [27] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM transactions on graphics (TOG), 42(4):111, 2023. [28] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In European Conference on Computer Vision, pages 195211. Springer, 2024. [29] Wenhao Sun, Xue-Mei Dong, Benlei Cui, and Jingqun Tang. Attentive eraser: Unleashing diffusion models object removal potential via self-attention redirection guidance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2073420742, 2025. [30] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 417424, 2000. [31] Alexandru Telea. An image inpainting technique based on the fast marching method. Journal of graphics tools, 9(1):2334, 2004. [32] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Free-form image inpainting with gated convolution. In Proceedings of the IEEE/CVF international conference on computer vision, pages 44714480, 2019. 11 [33] Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, and Gang Hua. Designing better asymmetric vqgan for stablediffusion. arXiv preprint arXiv:2306.04632, 2023. [34] Xiefan Guo, Hongyu Yang, and Di Huang. Image inpainting via conditional texture and structure dual generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1413414143, 2021. [35] Jitesh Jain, Yuqian Zhou, Ning Yu, and Humphrey Shi. Keys to better image inpainting: Structure and texture go hand in hand. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 208217, 2023. [36] Pourya Shamsolmoali, Masoumeh Zareapoor, and Eric Granger. Transinpaint: Transformerbased image inpainting with context adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 849858, 2023. [37] Tianyu Sun, Dingchang Hu, Yixiang Dai, and Guijin Wang. Diffusion-based depth inpainting for transparent and reflective objects. IEEE Transactions on Circuits and Systems for Video Technology, 2024. [38] Anh-Quan Cao and Raoul De Charette. Monoscene: Monocular 3d semantic scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 39914001, 2022. [39] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12179 12188, 2021. [40] Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, and Shubham Tulsiani. Mvd-fusion: Single-view 3d via depth-consistent multi-view generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96989707, 2024. [41] Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao. Zero-shot image editing with reference imitation. Advances in Neural Information Processing Systems, 37:8401084032, 2024. [42] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. [43] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [44] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-toimage models with human preference. arXiv preprint arXiv:2303.14420, 1(3), 2023. [45] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multi-dimensional human preference for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80188027, 2024. [46] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. [47] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture similarity. IEEE transactions on pattern analysis and machine intelligence, 44(5):25672581, 2020. [48] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 12 [49] Pontus Andersson, Jim Nilsson, Peter Shirley, and Tomas Akenine-Möller. Visualizing errors in rendered high dynamic range images. In Eurographics-Short Papers, pages 2528. EurographicsEuropean Association for Computer Graphics, 2021. [50] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 25552563, 2023."
        },
        {
            "title": "A Related work",
            "content": "Object removal and inpainting. Object removal is traditionally formulated as an inpainting problem, where the model fills user-specified mask with realistic content [30, 31, 32, 33, 34, 35, 36]. Most approaches [24, 26, 7] trained in strictly mask-aligned manner, enabling precise control over the masked region. For instance, ClipAway [5] leverages harmonized CLIP embeddings to guide removal, SmartEraser [16] and Inst-Inpaint [6] explore instruction-based or maskless generation. But they often leaving behind causal visual artifacts such as shadows and reflections. To address this, loosely mask-aligned methods [9, 10, 11] expand the removal scope beyond the user-defined mask, automatically cleaning surrounding artifacts. However, this comes at the cost of reduced controllability and increased risk of over-editing. Different from them, our method bridges the gap between strictly mask-aligned precision and loosely mask-aligned flexibility by decoupling geometry and appearance, allowing for structure-aware editing and implicit removal of causal visual artifacts. Geometry-aware generation. Incorporating geometric priors such as depth maps has shown promise in editing [37], scene completion [38], and novel view synthesis [39, 40]. While many prior methods use geometry as auxiliary input during single-stage generation [41, 42], they typically entangle structure and appearance modeling. Unlike prior methods that use depth as auxiliary input, we decouple geometry and appearance into two stages: editing in depth space and rendering in RGB. This design enables controllable structure editing and implicit removal of causal visual artifacts. Diffusion preference optimization and human alignment. Preference-guided training has emerged as practical alternative to supervised learning for aligning generative models with human intent [12, 43, 44, 45]. For instance, DPO [12] extends preference optimization to diffusion models by learning from ranked pairs. Other works [43, 44, 45] explore human alignment through benchmark design, direct reward modeling, and multi-dimensional preference decomposition for text-to-image generation. Inspired by these ideas, we adopt DPO-style strategy in our geometry removal stage: instead of human-annotated rankings, we define preferences based on geometric flow smoothness, encouraging plausible structure completion while suppressing spurious insertions. Additional detals about stage 2: appearance rendering As described in Sec. 2.3, the appearance rendering model learns to translate an input image containing an object into realistic RGB output where both the object and its associated causal visual artifacts (e.g., shadows and reflections) are removed. This translation is guided by geometric transformations predicted in Stage 1. Rather than using separate conditioning vectors, we train as direct image-to-image diffusion model by concatenating the input image and geometry maps into single tensor. To enable this, we directly use the colorized depth maps x+ 0 produced in Stage 1, which are already represented as 3-channel RGB-like images. This allows us to concatenate them directly with the RGB image along the width dimension, forming single composite input to the diffusion model. 0 and The input to the appearance rendering model is constructed by concatenating masked RGB image with its geometric representations before and after editing. Specifically, we define two composite inputs for bidirectional training. For object removal, the input is: removal = Concat(I , x+ 0 , 0 ), and the corresponding target is: insert = Concat(I +, x+ 0 , 0 ), removal RH(3W )3, insert RH(3W )3. (11) (12) For the insertion direction, the roles of input and target are reversed, i.e., (I insert, removal) forms the training pair. This bidirectional setup allows the same model to learn both removal and insertion through unified diffusion process. The geometry maps x+ 0 are placed on the left side of the RGB image, enabling to directly observe spatial geometric changes. Since both depth maps are colorized 3-channel tensors, they can be processed jointly with the RGB image without additional 0 and 14 Table 6: Perceptual metrics on RemovalBench and RORD-Val. higher is better, lower is better. Method RemovalBench RORD-Val SSIM DISTS DreamSim FLIP CLIP-IQA SSIM DISTS DreamSim FLIP CLIP-IQA CLIPAway [5] 0.6298 0.1656 Attentive-Eraser [29] 0.7084 0.1168 0.6367 0.1277 OmniEraser [9] 0.7367 0.0770 Ours 0.1572 0.0536 0.0539 0.0304 0.1175 0.0854 0.1084 0. 0.4973 0.4790 0.4339 0.4146 0.6074 0.1580 0.7186 0.1243 0.6071 0.1325 0.8248 0.0798 0.1304 0.0878 0.0675 0.0459 0.1645 0.1174 0.1524 0.1026 0.7986 0.7270 0.6646 0.7807 modality-specific encoders. The model is trained using standard denoising score matching loss Lrender = Et,ϵ + Et,ϵ (cid:20) w(t) (cid:20) w(t) (cid:13) (cid:13)G(I removal (cid:13) (cid:13) (cid:13)G(I insert (cid:13) , t) removal log p(I removal , t) insert log p(I insert removal) . 2(cid:21) (cid:13) insert) (cid:13) (cid:13) 2(cid:21) (cid:13) (cid:13) (cid:13) (13) where It is noisy version of at diffusion timestep t, and w(t) is predefined weighting function. By jointly training on both directions, learns to perform appearance synthesis conditioned on structured geometry edits, ensuring that generated results align with both scene content and layout."
        },
        {
            "title": "C Additional qualitative comparison",
            "content": "To further validate the effectiveness of our geometry-aware framework, we present additional qualitative results on the CausRem dataset, which contains real-world scenes involving shadows and reflections caused by removed objects. As shown in Fig. 7, our method successfully removes both the object and its associated shadow, while other methods either retain shadow residues or introduce undesired distortions in unmasked regions. This highlights our methods ability to preserve unmasked content while achieving consistent object removal. In Fig. 8, we provide additional comparisons in scenes with reflective surfaces. While baseline methods often fail to fully remove reflections or generate artifacts, our model leverages geometryguided rendering to produce coherent appearances without explicit reflection modeling. These results support our key insight: by removing the object structure in the geometric domain and rendering appearances based on updated geometry, our framework can implicitly eliminate causal visual artifacts and maintain visual consistency in challenging real-world scenarios."
        },
        {
            "title": "D Additional perceptual metrics",
            "content": "In Tab. 6, we additionally report SSIM [46], DISTS [47], DreamSim [48], FLIP [49], and CLIPIQA [50] on RemovalBench and RORD-Val."
        },
        {
            "title": "E Additional failure and challenging cases",
            "content": "Stage 2: transparent/reflective and self-emitting scenes. The third row in Fig. 9a shows semitransparent objects on reflective table. After removing one cup, the direct reflection is removed as well, while residual color bleeding remains in nearby reflections influenced by the removed cup. As shown in the second row of Fig. 9a, for self-emitting objects (e.g., colored bulbs), Stage 2 may hallucinate diffuse glow rather than cleanly removing the illumination. This behavior is undesirable for removal, yet it indicates that the renderer has learned meaningful correlations between light and its source, which could be useful if properly constrained. Stage 1: incomplete masks. Figure 9b contrasts complete and partial masks for semi-transparent bottle. With complete mask, Stage 1 removes the geometry as expected. With partial mask, the model attempts to complete the object, producing hallucinated extra bottle. In practice this can be avoided by simple mask dilation or by using stronger segmentation models (e.g., SAM2) to provide complete masks. 15 Figure 7: Qualitative comparison on CausRem highlighting shadow removal performance. Watermark removal. Beyond geometry-related artifacts, our framework can handle scene-wide watermarks with light modification  (Fig. 10)  . In this example, the watermark spans both the Figure 8: Qualitative comparison on CausRem highlighting reflection removal performance. lake surface and wooden planks but lacks reliable depth estimate. We apply the same Local Max Depth Fill-in inside the watermark maskassigning each masked pixel the maximum depth from small local neighborhoodas pseudodepth cue. Because Stage 2 selects removable regions by differencing the input and geometry-removed depth maps, this injected cue provides sufficient contrast for Stage 2 to identify and remove the watermark, showing that minimal conditioning tweaks let our method generalize to non-geometric inpainting cases. 17 (a) Challenging scenes for Stage 2: transparent or semitransparent objects and self-emitting (light-source) cases. (b) Stage 1 failure under incomplete mask: complete versus partial masks yield success versus hallucinated completion. Figure 9: Challenging cases and failure analysis. (a) Residual color bleeding and hallucinated glow can appear in reflective or lighting scenes. (b) Incomplete masks confuse Stage 1; simple dilation or stronger segmentation mitigates this. Figure 10: Watermark removal with pseudodepth cue. Applying Local Max Depth Fill-in within the watermark mask creates sufficient geometric contrast for Stage 2 to detect and remove the watermark across both the lake and dock."
        },
        {
            "title": "F CausRem",
            "content": "We construct the CausRem dataset by collecting 200 high-quality images from the free stock platform Pexels3, including 100 images containing reflections and 100 with shadows. For each image, we manually annotate the primary object along with its causal visual effectsreflections or shadows. In the shadow subset, where multiple objects often co-occur, we randomly select two to three objects per image for annotation. Each object and its corresponding shadow mask are stored using distinct IDs to preserve one-to-one causal relationship. In the reflection subset, due to the presence of fewer objects, we annotate only single object-mask pair per image. Fig. 11 and Fig. 12 illustrate representative annotation examples from the dataset."
        },
        {
            "title": "G Broader impacts",
            "content": "Our work advances the controllability and accuracy of object removal systems, which can benefit applications in autonomous driving, AR/VR content editing, and photo restoration. However, the enhanced controllability of visual content manipulation also raises potential risks such as deepfakes. Mitigating these risks requires responsible deployment, provenance tracking, and clear guidelines for model usage. 3https://www.pexels.com 18 Figure 11: Representative annotations in CausRem. Left: shadow examples; Right: reflection examples. Figure 12: Representative annotations in CausRem. Left: shadow examples; Right: reflection examples."
        }
    ],
    "affiliations": [
        "Pixocial Technology",
        "University at Buffalo"
    ]
}