{
    "paper_title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
    "authors": [
        "Zhengliang Shi",
        "Yuhan Wang",
        "Lingyong Yan",
        "Pengjie Ren",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Zhaochun Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models."
        },
        {
            "title": "Start",
            "content": "Retrieval Models Arent Tool-Savvy:"
        },
        {
            "title": "Benchmarking Tool Retrieval for Large Language Models",
            "content": "Zhengliang Shi1 Yuhan Wang1 Lingyong Yan2 Pengjie Ren1 Shuaiqiang Wang2 Dawei Yin2 Zhaochun Ren3* 1Shandong University, Qingdao, China 2Baidu Inc., Beijing, China 3Leiden University, Leiden, The Netherlands shizhl@mail.sdu.edu.cn z.ren@liacs.leidenuniv.nl 5 2 0 2 3 ] . [ 1 3 6 7 1 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tooluse benchmarks simplify this step by manually pre-annotating small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose TOOLRET, heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and corpus of 43k tools, collected from existing datasets. We benchmark six types of models on TOOLRET. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on TOOLRET. This low retrieval quality degrades the task pass rate of tool-use LLMs. As further step, we contribute large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable progress across various natural language processing (NLP) tasks, such as text summarization (Chang et al., 2023). However, they suffer from inherent inabilities to interact with the physical world and access vast, up-to-date knowledge (Qin et al., 2024). To alleviate these drawbacks, tool learning is proposed to equip LLMs with external tools, augmenting them as agents to manipulate tools for practical task-solving (Qu et al., 2025b; Wang et al., 2024e). In practical applications, retrieving useful tools from toolsets for LLM agents typically serves as *Corresponding author. 1Resource is available on Huggingface and (cid:135) GitHub. Figure 1: Correlation between the tool retrieval performance (e.g., Recall@10) of IR models and the end-toend task pass rate of tool-use agents. the initial step (Wang et al., 2024c; Xu et al., 2024; Song et al., 2023). This step becomes particularly critical in real-world scenarios where the candidate tools are usually large-scale and many of them are similar in functionality (Qu et al., 2024a). However, most existing work (Guo et al., 2024; Qian et al., 2023) simplifies this retrieval process by manually pre-selecting small set of 10-20 relevant tools for each evaluation task. For example, the ToolACE (Liu et al., 2024a) and ToolBench (Qin et al., 2023) annotate about 10 tools per task. While recent information retrieval (IR) techniques such as semantic matching (Qu et al., 2024a; Xu et al., 2024), can assist with tool retrieval, they are often trained on ad-hoc tool-use datasets, lacking comprehensive evaluation on diverse scenarios, especially for unseen tasks. To further explore the importance of tool retrieval, we conduct pilot experiment on ToolBench (Qin et al., 2023). As shown in Figure 1, we observe that (i) the agents performance substantially drops when replacing the officially annotated toolset with the retrieved tools; and (ii) even strong retrievers like colbertv2 (Santhanam et al., 2021a), struggle to retrieve target tools effectively. These findings highlight the necessity to (i) systematically evaluate IR models on diverse tool retrieval tasks; and (ii) analyze the impact of retrieval on the end-to-end task pass rate. In this work, we introduce TOOLRET, the first large-scale tool retrieval benchmark comprising 7.6k diverse retrieval tasks and corpus of 43k tools, which comprehensively evaluates IR models across diverse retrieval scenarios. Specifically, we collect query-tool datasets from the following sources: (i) Tool-use agent benchmarks from published research papers in AI conferences, such as ACL and NeurIPS; (ii) Related conference resources such as AppBench in EMNLP and ToolLens in CIKM; and (iii) Other publicly available datasets from the open-source community, e.g., HuggingFace. The collected data is carefully curated to cover wide range of practical tool requirements, comprising diverse types of tool documentation, domains, and varying query lengths. Then, we standardize the format of all the collected tasks, aligning them with retrieval tasks similar to the format in MTEB, where each retrieval task contains query and target tools (e.g., labels). To support the instructional retrieval (Weller et al., 2024) setting of our benchmark, we also introduce target-aware strategy to supplement each query with an instruction using the powerful LLMs (i.e., gpt-4o). We systematically evaluate five types of IR models such as embedding models and LLM re-ranking, under various experimental settings. Our results reveal that even the best model (i.e., NV-embedd-v1) that demonstrates strong performance in conventional IR benchmarks, achieves an nDCG@10 of only 33.83 in our benchmark. This highlights the challenges of the tool retrieval tasks. We identify two key factors contributing to this performance gap: (i) Lower term overlap between queries and target tools in tool retrieval tasks, which demands higher representation abilities for IR models to accurately match query intent with the correct tools; and (ii) Task shift from conventional informationseeking tasks (e.g., document retrieval) to tool retrieval, leading to suboptimal performance of IR models that are not explicitly optimized. To enhance the retrieval performance and enable IR models to augment tool-use agents, we further propose the TOOLRET-train, large-scale training dataset containing more than 200k retrieval tasks. We extend our data collection process from TOOLRET to include the training set of three mainstream tool-use datasets, including ToolACE (Liu et al., 2024a), APIGen (Liu et al., 2024b) and ToolBench (Qin et al., 2023). To enable the training, we pair each retrieval task with 10 negative tools retrieved by the NV-embed-v1. Finally, each training example contains the query, an generated instruction, the target tools, and the negative tools. Results show that the IR models trained over TOOLRETtrain, exhibit significant improvements in the retrieval process, leading to higher end-to-end task pass rate when integrated with tool-use LLMs. Our contributions are summarized as follows: (i) We introduce TOOLRET, the first evaluation benchmark for tool retrieval tasks. (ii) We evaluate the tool retrieval performance of various IR models and analyze the impact of retrieval on the end-toend task pass rate of tool-use LLMs; and (iii) We contribute to large-scale training dataset that enhances the performance of IR models, improving their ability to augment tool-use LLMs effectively."
        },
        {
            "title": "2 Related work",
            "content": "Tool learning with foundation models. Tool learning aims to equip LLMs with tools, such as web API (Song et al., 2023) and python packages (Wang et al., 2024d), expanding their utility (Qin et al., 2023). Existing work teaching LLMs to use tools can be broadly classified into tuning-free (Lu et al., 2023) and tuning-based methods (Gao et al., 2024). The former prepends the description of candidate tools in the LLMs context, prompting them to select and invoke tools (Huang et al., 2023). The latter enables LLMs to learn the usage of each tool through training on synthetic data (Liu et al., 2024a; Gao et al., 2024). However, both two paradigms struggle when facing the large-scale toolset in practice (Qu et al., 2024b; Liu et al., 2024b). First, real-world toolsets are typically massive, making it less possible to incorporate all tools within the limited context of LLMs. For example, the RapidAPI platform contains more than 52k tools while the PyPI2 hosts over 600k frequently updated packages. Second, since tools are frequently updated, it is cost-intensive to re-train the LLMs to memorize all tools (Qu et al., 2025a). Although recent studies address this challenge using semantic retrievers (Qin et al., 2023; Wang et al., 2024c), these solutions are typically ad-hoc and lack systematic evaluation across diverse tool retrieval scenarios. To fill this gap, we present the first comprehensive tool retrieval benchmark with systematic analysis. Information retrieval benchmark. Conventional information retrieval (IR) benchmarks are typically designed for information-seeking tasks, such as Nature Question (Kwiat kowski et al., 2019) for question answering and MS-MARCO (Nguyen 2https://pypi.org/ et al., 2016) for passage re-ranking. Recent work also explores the IR technique in various downstream tasks, such as table retrieval (Chen et al., 2024b; Zhang and Balog, 2020) and scientific retrieval (Ajith et al., 2024), which substantially augments the downstream models. However, tool retrieval, crucial step for tool-use agents, remains underexplored. Compared with traditional IR tasks, retrieving useful tools is more challenging since solving task typically requires the combination of multiple tools (Qu et al., 2024b). Most existing benchmarks simplify this retrieval process by manually annotating small set of tools that fit the LLMs context, which is far from reality with large toolset. In this work, we evaluate IR models on diverse tool retrieval tasks and contribute over 200k training data to facilitate future research."
        },
        {
            "title": "3.1 Data collection",
            "content": "To build comprehensive benchmark for tool retrieval evaluation, we collect data from the following well-known sources: (i) Tool-use LLM benchmarks: wide range of benchmarks published in leading AI conferences such as ACL and NeurIPS; (ii) Conference Resources: Datasets from resource tracks in IR and NLP conferences (e.g., CIKM and EMNLP); and (iii) Other high-quality dataset: We identify related datasets released on open-source platforms like HuggingFace and their technique reports can be found in public submissions like arXiv. We include them to enrich TOOLRET. Given the rapid development of benchmarks from these sources, we collect datasets released between the August 2023 to December 2024 in this version.3 We download these data from official channels based on their usage requirements and totally collect more than 30 datasets. Since the data sources are diverse and their original formats vary substantially, we perform necessary data cleaning operations such as deduplication and text normalization to ensure consistency and quality. We observe that most of the collected datasets are originally designed to evaluate the tool-use capability of LLMs, where the LLM is required to correctly call sequence of target tools given an input query. To facilitate retrieval evaluation in TOOLRET, we align the format of all collected tasks with the well-established IR benchmark like BEIR and MTEB. Specifically, each task consists of query as 3Our team will maintain and update the benchmark. input and target tools as label (a.k.a, ground truth), where tool is identified by unique identifier and paired with detailed documentation to describe its functionality. Endpoints of the collected datasets and concrete examples of our formatted dataset are provided in Appendix B."
        },
        {
            "title": "3.2 Data sampling",
            "content": "After collecting the datasets, we observe data size imbalances across different datasets. Besides, some datasets are extremely large with substantial redundant content, making comprehensive model evaluation both inefficient and unnecessary. Therefore, we streamline them through effective data sampling while maintaining its evaluation integrity. Task sampling. For each collected dataset, we encode the tasks using the embedding model, i.e., NV-embedd-v1, and apply the K-means clustering algorithm on the text embeddings. We set the number of clusters to the size of the corresponding toolset and randomly sample one task from each cluster. If the toolset size exceeds the number of queries, we retain all queries. For example, the original ToolEyes (Ye et al., 2024a) dataset contains 500 queries and 95 tools; Thus, we set the cluster number as min(500, 95) = 95 for clustering. Toolset sampling. To eliminate redundancy, we manually review the documentation of each raw dataset to identify and merge identical toolsets. For example, since the COLT (Qu et al., 2024a) toolset overlaps with the Toolbench (Qin et al., 2023) , we merge their intersecting tools. Ultimately, we merge all toolsets from the 34 datasets to form the final corpus, resulting in total of 43k tools. Each tool is assigned unique identifier. After sampling, we obtain 7.6k retrieval tasks and corpus of 43k tools. 3."
        },
        {
            "title": "Instruction construction",
            "content": "Instructional information retrieval (Sun et al., 2024; Weller et al., 2024) is an active research area, where an additional instruction is paired with the input query to guide IR models in retrieving target information. This instruction-following capability is especially critical in tool retrieval, as IR models are often used to augment LLM agents and receive additional context from the agents beyond the input query. To support this instructional IR scenario, we construct the instructions as part of TOOLRET. Considering manually writing instructions is cost-intensive and challenging to scale, we introduce target-aware strategy using powerful LLMs"
        },
        {
            "title": "Statistic",
            "content": "# size of retrieval task - # of web API retrieval task - # of code function retrieval task - # of customized app retrieval task # size of tool - # of web API - # of code function - # of customized app avg. query / instruction length (tokens) avg. tool documentation length (token) 7,615 4,916 950 1,749 43,215 36,978 3,794 2,443 46.87 / 43.43 174.56 Table 1: Basic statistics of our benchmark TOOLRET. # Average number of targets for an input query. # ROUGE-L overlap between query and targets. Ours NQ MSMARCO HotpotQA MTEB 2.17 1.00 1.00 2.00 2.57 0.06 0.31 0. 0.11 0.27 Table 2: Comparison with conventional IR benchmarks. to automate this process. Specifically, we first invite three human experts with strong NLP and IR backgrounds to manually craft 100 seed instructions. In line with the well-defined format from Asai et al., our instruction outlines the relevance criteria by bridging the query intent and the functionality of the target tools. For example, for the transcribing the audio to text task, the instruction is presented as retrieve tools that process audio inputs to produce accurate textual transcriptions aligned with the user requirements. Next, we employ powerful LLM, i.e., GPT-4o, as an automatic instruction generator and guide it to generate instruction for each task through in-context learning. To enhance the diversity, we randomly sample in-context examples from the pool of both the generated and handcrafted instructions. detailed pseudo algorithm is provided in Appendix B. After the above three processes, we obtain TOOLRET, which consists of 7.6k tasks, each paired with an instruction, and corpus of 43k diverse tools, providing comprehensive testbed and supporting various evaluation settings."
        },
        {
            "title": "4 Benchmark statistic",
            "content": "Table 1 provides the basic statics of TOOLRET. We observe that there are three mainstream formats of tool documentation: (i) Code, which is functionlevel snippet in programming language; (ii) Web API, which elaborates the tool usage in structured JSON format following the Web OpenAPI specification; (iii) Customized application, which directly describes the tool functionality in free-form nature language. Based on these formats, we categorize Figure 2: ROUGE-L overlap between the query (input) and the target tools (label). Figure 3: Length distribution of our benchmark. TOOLRET into three subsets accordingly and divide the TOOLRET into Code Function, Web API, and Customized App subsets. Below, we report more detailed analysis of TOOLRET."
        },
        {
            "title": "4.1 Complexity",
            "content": "In tool learning, previous studies have highlighted the necessity of combining multiple tools for task solving (Shi et al., 2024). Thus, we analyze the complexity of our retrieval benchmark from two aspects. First, we calculate the average number of target tools for each retrieval task and compare it with well-known IR benchmarks such as HotpotQA (Yang et al., 2018) and MTEB (Muenni ghoff et al., 2022). As shown in Table 2, TOOLRET requires models to recall more targets, posing challenge in comprehensive retrieval. Second, we compute the lexical overlap, i.e., ROUGE-L, between the input query and corresponding retrieval targets (tool documentation in TOOLRET and passage in IR benchmarks). We find that this overlap is substantially lower in TOOLRET. It indicates that, for neural IR models, TOOLRET requires more heavily on the semantic representation rather than simple lexical matching. Therefore, the retrieval task in TOOLRET is more challenging."
        },
        {
            "title": "Quality Review Question",
            "content": "Whether the instruction is relevant to the original input query?"
        },
        {
            "title": "Whether the instruction describes the feature\nof target tools",
            "content": "Yes or No % 90.1% / 9.9% 92.3% / 8.7% Whether the instruction comprehensively describe the feature of all target tools 89.2% / 10.8% Whether the instruction contains hallucination about the target tools or input query? 5.9% / 94.1% Table 3: The quality review for our generated instructions, which is conducted by five human experts with 0.743 Kappa statistics."
        },
        {
            "title": "4.2 Length statistics",
            "content": "Figure 3 illustrates the length distribution of the instruction, and tool documentation in query, TOOLRET.4 We find that most queries are concise, typically containing fewer than 60 tokens (about 25 words), which aligns with real-world user behavior, as users tend to input brief queries with minimal effort. Additionally, most tool documentation is under 200 tokens, which is similar to the chunk length in standard IR document retrieval corpus, such as Wikipedia dump (Karpukhin et al., 2020)."
        },
        {
            "title": "4.3 Quality",
            "content": "So far, we have demonstrated the complexity and quantity of our benchmark while the quality of the LLM-generated instructions remains uncertain. To investigate this, we ask 5 human experts to label the quality based on four aspects listed in Table 3. Our evaluation reveals that 89.2% of the generated instructions correctly cover the feature of the target tools and are faithfully grounded on the original queries. For the remaining 10.8% instructions that mismatch the query or the target tools, we ask experts to revise them. This re-check mechanism ensures the high quality of instructions in TOOLRET, making it reliable evaluation benchmark. To explain more intuitively, we list number of seed instructions, high-quality and low-quality instructions in Table 8. Annotation guidance is also provided in Appendix to promote our transparency. 4."
        },
        {
            "title": "Instruction diversity",
            "content": "We further analyze how the generated instructions differ from the seed instructions used to prompt the generation. For each generated instruction, we compute its highest ROUGE-L overlap with the 100 seed instructions. We plot the distribution of these ROUGE-L scores in Figure 4. The results 4We use the tokenizer from gpt-3.5-turbo in this work. Figure 4: ROUGE-L overlap between the handcrafted seed instructions and model-generated instructions. indicate decent number of new instructions are generated, which have low overlap with the seeds."
        },
        {
            "title": "5.1 Evaluation protocol",
            "content": "We use three widely used IR metrics to evaluate the retrieval performance: (i) NDCG@K (N@K): evaluates ranking quality based on the relevance of retrieved tools; (ii) Recall@K (R@K): evaluates the proportion of target tools successfully retrieved within the top-K results; and (iii) Precision@K (P@K): evaluates the accuracy of the retrieved tools within the top-K results. We also use Completeness@K (C@K) from COLT (Qu et al., 2024b), which specifically evaluates the retrieval completeness in tool retrieval tasks. The C@K is 1 if all target tools are included in the top-k retrieved tools; otherwise, it is 0. We mainly evaluate IR models under two settings: (i) w/o inst.: The model take the query alone as input; and (ii) w/ inst.: The model takes the concatenation of the query and instruction as input to retrieve. This allows us to analyze the impact of instructions on retrieval performance."
        },
        {
            "title": "5.2 Model to Evaluate",
            "content": "We comprehensively evaluate the following mainstream IR models on our benchmark. Sparse retrieval. These methods measure the similarity between query and tool documentation based on lexical overlap. We evaluate BM25s (Lù, 2024). Single-task dense retrieval. These methods use dual-encoder models trained on conventional IR datasets. We evaluate gtr (Ni et al., 2021a), contriever (Izacard et al., 2021a), and colbertv2.0 (Santhanam et al., 2021a), all trained on MS-MARCO (Nguyen et al., 2016). We also evaluate COLT (Qu et al., 2024a), recently proposed model trained on ad-hoc tool retrieval datasets. Multi-task embedding Models. These methods Model BM25s COLT Colbert contriever-msmarco gtr-t5-base gtr-t5-large all-MiniLM-L6-v2 e5-small-v2 e5-base-v2 e5-large-v2 gte-base-en-v1.5 gte-large-en-v1.5 bge-base-en-v1.5 bge-large-en-v1.5 gte-Qwen2-1.5B-inst. e5-mistral-7b GritLM-7B NV-Embed-v1 mxbai-rerank-large-v1 monot5-base-msmarco bge-reranker-v2-m3 jina-reranker-v2-base bge-reranker-v2-gemma Mixtral-8x22B gpt-3.5-turbo-0125 gpt-3.5-turboTOOLRET-Web TOOLRET-Code TOOLRET-Customized Average N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@10 18.98 15.43 22.40 21.15 17.36 22. 11.66 19.89 19.75 18.99 23.55 22.41 22.50 24.45 29.17 26.76 25.74 31.30 22.99 28.92 32.92 35.38 36.72 28.21 30.29 31.01 4.64 2.63 5.37 5.83 4.25 5.42 3.07 5.08 5.04 4.90 6.28 5.91 6.02 6.66 7.93 7.25 6.85 8.35 5.61 7.70 8.73 9.25 9. 8.31 8.01 7.86 24.62 21.11 27.41 27.19 24.17 29.75 16.36 26.46 25.89 25.97 32.03 30.14 29.96 32.93 38.05 34.39 34.27 39.15 30.32 36.44 41.88 44.65 45.94 34.13 36.00 35.82 15.20 20.04 15.45 14.70 15.95 18. 10.15 16.26 15.37 16.27 19.15 18.44 17.30 19.30 21.49 21.05 21.28 23.05 18.38 19.97 25.63 26.98 27.85 25.42 24.22 23.76 21.20 20.69 16.43 14.56 16.47 18.25 14.44 15.48 14.43 17.09 17.43 16.66 17.78 18.90 21.66 20.01 22.02 29.64 24.76 21.61 24.28 26.47 29. 27.41 28.69 28.95 3.37 5.12 2.65 2.40 2.71 2.89 2.50 2.39 2.47 2.68 2.87 2.87 2.92 3.12 3.41 3.44 3.72 4.72 3.88 3.62 3.80 4.15 4.42 3.14 4.27 4.44 28.23 28.07 22.54 19.28 22.27 24. 19.50 19.26 19.19 21.87 23.71 23.64 23.66 25.76 28.89 28.31 30.41 40.45 34.86 30.06 32.71 35.20 38.23 34.13 36.25 38.16 26.96 18.53 21.65 17.71 21.16 23.08 18.11 18.05 18.00 20.70 22.48 22.39 22.27 24.47 27.67 27.10 28.87 38.88 33.22 27.88 30.94 33.94 36. 36.98 35.64 38.45 26.76 21.63 19.54 17.72 23.47 26.30 22.80 24.60 22.68 26.42 21.62 20.62 25.99 25.72 36.04 31.41 42.31 40.54 26.76 36.22 30.51 38.94 39.93 30.76 29.80 32.30 5.86 4.72 3.65 3.31 5.09 5. 5.21 5.56 5.11 6.07 4.76 5.19 5.71 5.54 7.89 6.68 8.71 8.25 5.91 7.54 7.00 8.14 9.06 5.40 6.39 6.89 32.39 29.19 23.72 22.77 28.93 31.86 29.10 29.67 29.13 32.19 29.03 26.75 32.17 32.18 44.51 38.47 49.34 45.93 34.53 45.11 36.03 46.06 49. 34.12 35.01 38.31 24.40 22.40 18.97 18.31 22.49 24.45 20.25 20.76 22.25 23.17 23.17 17.67 24.26 24.79 35.55 29.24 38.17 34.44 26.03 36.41 26.74 35.42 37.75 28.65 28.70 30.84 22.32 19.25 19.46 17.81 19.10 22. 16.30 19.99 18.95 20.83 20.86 19.90 22.09 23.02 28.96 26.06 30.02 33.83 24.84 28.92 29.24 33.60 35.51 28.80 29.60 30.75 22.19 20.32 18.69 16.91 19.87 22.09 16.17 18.36 18.54 20.05 21.60 19.50 21.27 22.85 26.04 25.80 29.44 32.12 25.88 28.09 27.77 32.11 34. 30.35 29.52 31.01 Table 4: Experiment results in w/o inst. setting ( 5), where the model takes the query as input to retrieve. We mark the baselines pre-trained on instructional datasets with . We highlight the best performance in each type of model. utilize transformer encoders trained on various IR datasets. We evaluate all-MiniLM-L6-v2, gte (Li et al., 2023c), bge (Xiao et al., 2023a) and e5 (Wang et al., 2022), covering wide range of sizes. Cross-encoder re-rankers. These models rerank the initially retrieved documents based on the query-passage relevance. We evaluate: MonoT5 (Nogueira et al., 2020), mxbai-reranklarge, jina-reranker-v2-base, and bge-reranker. LLM agents. These methods leverage generalpurpose LLM agents for re-ranking tasks in zeroshot setting, simulating the tool selection process of tool-use agents. We evaluate the widely used LLM re-ranking framework, i.e., RankGPT (Sun et al., 2023), with various LLMs as backbone. Initial tools for LLM agent and Re-ranking baselines are retrieved by Nv-embedd-v1 model. Details of these baselines are provided in Appendix D."
        },
        {
            "title": "6.1 Tool retrieval performance",
            "content": "Existing retrievers struggle. As shown in Table 9, the tool retrieval tasks in TOOLRET raise significant challenges for existing retriever models. Specifically, all retrievers in our experiments achieve less than 35% in Completeness@10 and under 52% in recall@10. Notably, retrieval methods that demonstrate strong performance in conFigure 5: Correlation between the score on our benchmark and MTEB (retrieval subset). ventional information retrieval (IR) tasks, such as ColBERT, even underperform compared to simple lexical-based matching approaches like BM25. Similarly, other embedding-based models, even the NV-Embed-v1 with 7B parameter, achieve less than 45% in completeness@10, exhibiting limitations. We identify two potential reasons for the above performance degradation: (i) Tool retrieval tasks require intensive reasoning over the input query to align user intentions with candidate tools, as the lexical overlap between the query and targets is low. (ii) There exists domain shift between the conventional training corpora used for retrieval models and the specific tool retrieval tasks, which current models are not explicitly optimized for. Re-ranking technique has limited improvement. As shown in Table 9, commonly used re-ranking Model BM25s COLT Colbert contriever-msmarco gtr-t5-base gtr-t5-large all-MiniLM-L6-v2 e5-small-v2 e5-base-v2 e5-large-v2 gte-base-en-v1.5 gte-large-en-v1.5 bge-base-en-v1.5 bge-large-en-v1.5 e5-mistral-7b NV-Embed-v1 gte-Qwen2-1.5B-inst. GritLM-7B mxbai-rerank-large-v1 monoT5-base-msmarco bge-reranker-v2-m3 jina-reranker-v2-base bge-reranker-v2-gemma Mixtral-8x22B gpt-3.5-turbo-0125 gpt-3.5-turbo-1106 TOOLRET-Web TOOLRET-Code TOOLRET-Customized Average N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@10 26.33 28.91 16.67 23.48 20.38 24.37 12.77 26.42 24.71 23.62 30.75 28.06 25.95 30.03 31.07 31.51 37.53 36.58 17.53 23.33 34.83 42.35 34.73 35.31 37.22 38.31 6.10 4.61 3.12 5.29 4.49 5. 3.26 6.20 5.78 5.52 7.00 6.55 6.16 7.01 7.65 7.74 9.31 9.34 4.05 5.88 8.54 10.11 8.09 7.56 8.97 9.02 34.22 40.64 21.14 30.21 27.53 31.64 19.38 34.44 33.45 32.19 39.44 36.32 35.12 39.28 41.30 40.52 48.31 46.01 25.82 30.70 45.23 51.21 45. 38.63 40.82 41.29 22.79 38.83 14.94 19.69 19.24 21.26 13.33 21.39 21.94 21.80 25.88 22.57 23.40 25.63 27.04 26.74 30.95 27.65 17.95 18.13 31.73 34.23 32.29 34.60 35.22 35.76 41.90 20.06 30.35 31.61 33.59 36. 31.59 32.36 31.40 34.27 41.68 35.77 35.15 41.53 44.97 47.92 47.38 41.26 33.86 31.39 50.86 53.21 55.85 33.27 35.42 38.69 6.20 4.71 4.37 4.84 4.90 5.33 5.06 4.84 5.01 5.05 6.20 5.75 5.22 6.00 6.66 7.10 7.29 6.17 5.05 5.27 7.64 7.66 8. 5.77 6.22 7.27 56.49 27.78 41.38 43.01 43.18 47.42 43.86 42.38 42.83 44.42 53.96 49.56 45.74 52.76 58.95 62.07 61.12 53.81 47.84 45.18 67.26 66.03 70.53 39.60 41.16 42.57 55.39 18.84 40.28 41.74 41.88 45. 42.25 41.11 41.38 43.19 51.64 47.71 44.32 51.18 56.79 59.60 59.55 52.07 46.47 42.51 64.78 63.94 68.76 38.53 42.64 42.81 41.16 31.29 24.35 21.93 41.84 42.04 32.24 34.62 38.06 43.32 37.95 37.27 43.20 43.90 40.88 48.70 52.98 45.55 26.83 37.77 42.35 45.94 51. 34.40 37.29 39.30 8.39 6.05 4.56 3.85 7.66 8.48 7.14 6.90 7.54 8.51 6.96 7.88 8.82 8.31 7.91 10.07 10.63 9.74 6.71 6.76 9.52 10.36 11.04 6.44 8.24 7.89 48.60 42.19 30.97 27.28 48.35 50. 43.55 42.29 46.84 52.30 46.57 47.98 53.54 51.79 49.35 57.69 59.47 54.01 37.61 46.63 53.75 57.96 61.20 39.72 41.34 43.31 38.90 34.01 24.87 23.04 39.28 40.00 32.34 32.58 36.43 41.42 38.10 35.84 42.29 42.24 38.35 43.88 45.68 41.40 28.60 39.70 39.90 45.41 45. 38.20 39.70 37.31 36.46 26.75 23.79 25.67 31.94 34.39 25.53 31.14 31.39 33.73 36.79 33.70 34.77 38.49 38.97 42.71 45.96 41.13 26.08 30.83 42.68 47.17 47.52 34.33 29.60 38.77 39.03 30.56 26.70 28.16 33.46 35. 29.31 31.69 33.25 35.47 38.54 35.37 36.67 39.68 40.73 43.41 45.39 40.37 31.01 33.45 45.47 47.86 48.90 37.11 29.52 38.63 Table 5: Experiment results in w/ inst. setting ( 5), where the model takes the query and instruction as input to retrieval. indicates the model is pre-trained on instructional datasets. We highlight the best performance. methods provide limited and even negative improvements for the tool retrieval task. When using MonoT5 to re-rank the tools retrieved by NVEmbed-v1, the average NDCG@10 decreases from 33.83 to 28.92. similar trend is observed with the mxbai-rerank. Other advanced models such as bgeranker-v2-gemma only have 4.7% improvement in the Completeness@10 metric. These results further indicate the challenging nature of tool retrieval. similar trend (Pearsons coefficient β = 0.790), but the score in TOOLRET is lower. This indicates that the task in TOOLRET has correlation with conventional IR tasks but is more challenging. Second, we also observe that conventional IR models trained with relevance-oriented criteria such as contriever perform poorly on TOOLRET, which indicates that TOOLRET requires more target-aware reasoning ability. This is also illustrated in 4.1."
        },
        {
            "title": "7 Retrieval affect downstream task",
            "content": "Besides the evaluation results under w/o inst setting in Table 4, we also present the results under w/ inst setting in Table 5. We observe that all the IR model achieves better performance when an additional instruction is paired with the query as input. Notably, the instruction-tuned embedding model like NV-embed-v1 or e5-mistral has the most obvious improvement, which potentially benefits from its powerful instruction-following capability. These results illustrate the advantages of the instruction and instruction tuning in tool retrieval tasks."
        },
        {
            "title": "6.3 Compare with conventional IR tasks",
            "content": "To further explore the complexity of tool retrieval tasks, we compare the models performance on TOOLRET and conventional IR task benchmark, i.e., MTEB, showing their relationship in Figure 5. First, we can see that the two benchmarks share In this section, we qualitatively analyze the impact of retrieval performance on downstream tool-use agents. We conduct end-to-end evaluations on ToolBench (Qin et al., 2023) dataset using the official Pass Rate metric that evaluates whether the model successfully calls target tools to complete task."
        },
        {
            "title": "7.1 Poor retrieval leads to poor tool-use agents",
            "content": "For each task in ToolBench, we replace the officially pre-annotated toolset (oracle) with tools retrieved by IR models. As shown in Figure 6, the tool-use LLMs, when equipped with the retrieved tools, exhibit substantially lower performance compared to their oracle counterparts. For example, in ToolBench-G1, GPT-3.5 achieves pass rate of 50.60 using tools retrieved by bge-large, decreasing by 11.40. This indicates that tool retrieval is crucial step to build better tool-use LLMs and Figure 6: The horizontal axis indicates the retrieval performance of IR models, both before and after training. The vertical axis corresponds to the end-to-end pass rate of tool-use LLMs using the tools retrieved by these IR models. improve their task-solving performance."
        },
        {
            "title": "7.2 Towards better retrieval",
            "content": "The analysis in 6.2 highlights the advantage of instruction-tuning in improving tool retrieval. However, to the best of our knowledge, there is no largescale instructional IR dataset for tool retrieval tasks. We propose the TOOLRET-train to fill this gap. Large-scale training data We extend the data collection process from TOOLRET to include the training sets of three mainstream tool-use datasets: ToolACE (Liu et al., 2024a), ToolBench (Qin et al., 2023) and APIGen (Liu et al., 2024b). Ultimately, we collect over 200k training instances, each comprising query and set of target tools . We also pair each query with an instruction using our target-aware strategy (See Appendix D). Learning objective To train IR model (denoted as θ), we first use it to retrieved top-K negative tools, denoted as (cid:98)T = {ˆtj [K] , ˆtj / }. The model θ is then optimized by maximizing the log-likelihood of the target tools. The loss function is formulated as: 1 (cid:88) tiT log esim(Iq,ti) esim(Iq,ti) + (cid:80) ˆtj (cid:98)T esim(Iq,ˆtj ) . The indicates concatenation of instruction and query with special token. During the training, we set the to 10 and the learning rate to 5e-5. Improvement from retrieval. As shown in Figure 6, all IR models trained on TOOLRET-train achieve substantial improvement in NDCG@10 metric. We further evaluate the task pass rate of two tool-use LLMs: GPT-3.5 and ToolLlama (Qin et al., 2023). When equipped with the improved IR models, both LLMs exhibit substantial gains in pass rate, confirming the critical role of retrieval in downstream tasks. As part of future work, we suggest adapting the IR models to better augment the tool-use LLMs, which offers efficient plugand-play solution compared with training LLMs. We further conduct an ablation study by removing the instruction from the loss function L. The results show that this variant shows improvements compared with the non-tuned counterparts, but underperforms compared with their instruction-tuned counterparts (See Appendix D). These validate the effectiveness of our instructional training data in enhancing tool retrieval performance."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we introduce TOOLRET, the first diverse tool retrieval benchmark comprising 7.6k queries, each paired with an instruction, and corpus of 43k tools. TOOLRET is heterogeneous benchmark, constructed by aggregating existing tool-use datasets and aligning them into unified format, similar to conventional IR benchmarks such as MTEB. We evaluate state-of-the-art IR models on TOOLRET and uncover surprising finding: even models with strong performance on conventional IR benchmarks struggle in tool retrieval. This low retrieval quality significantly degrades the end-to-end task pass rate of tool-use LLMs. Inspired by this, we further propose TOOLRET-train, large-scale training set containing over 200k retrieval tasks. Results show that IR models trained on TOOLRET-train exhibit substantial improvement and also enhance the pass rate of tooluse LLMs by 10%-20%. In the future, we plan to extend the TOOLRET into multimodal scenarios."
        },
        {
            "title": "References",
            "content": "The limitations of this work include the lack of exploration in multilingual retrieval settings. Currently, our benchmark is confined to the English language and focuses exclusively on text retrieval. To address this limitation, we plan to expand our research to encompass multilingual information retrieval (IR) scenarios in the future. Additionally, another limitation lies in the insufficient investigation of prompt sensitivity. Given that large language models (LLMs) are highly sensitive to prompt wording, we aim to annotate broader range of instructions in the future to examine how variations in prompt phrasing influence LLM performance. Building upon TOOLRET, we suggest the following directions for future work: (i) Investigating sensitivity to instructions: Conduct comprehensive study on how LLM performance varies with different prompt formulations and instruction styles. (ii) Enhancing IR models for improved retrieval accuracy: Further optimize IR models to achieve higher retrieval precision, leveraging these improvements to augment tool-use LLMs and, consequently, enhance end-to-end task performance."
        },
        {
            "title": "Ethics Statement",
            "content": "We acknowledge the importance of the ACM Code of Ethics and fully agree with it. We ensure that this work is compatible with the provided code in terms of publicly accessible datasets and models. Risks and harms associated with large language models include the generation of harmful, offensive, or biased content. The new benchmark is composed of various previous datasets and is therefore licensed under their respective data licenses. In this research, we prioritize reproducibility by not only utilizing state-of-the-art commercial LLMs but also experimenting extensively with open-source LLMs. Throughout the study, we have strictly followed ethical standards to maintain the integrity and validity of our work. All tools and resources used in this research were obtained from publicly available platforms, ensuring transparency and reproducibility in our experimental procedures. Furthermore, we have made every effort to ensure that our research does not harm individuals or groups, nor does it involve any form of deception or potential misuse of information. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao. 2024. Litsearch: retrieval benchmark for scientific literature search. arXiv preprint arXiv:2407.18940. Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware retrieval with instructions. arXiv preprint arXiv:2211.09260. Emily Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587604. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024a. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Preprint, arXiv:2402.03216. Si-An Chen, Lesly Miculicich, Julian Martin Eisenschlos, Zifeng Wang, Zilong Wang, Yanfei Chen, Yasuhisa Fujii, Hsuan-Tien Lin, Chen-Yu Lee, and Tomas Pfister. 2024b. Tablerag: Million-token table understanding with language models. arXiv preprint arXiv:2410.04739. Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, et al. 2023. T-eval: Evaluating the tool utilization capability step by step. arXiv preprint arXiv:2312.14033. Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren, Zhumin Chen, Jun Ma, and Zhaochun Ren. 2024. Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum. In Proceedings of the AAAI Conference on Artificial Intelligence: AAAI. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. Datasheets for datasets. Communications of the ACM, 64(12):86 92. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714. Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, et al. 2024. Planning, creation, usage: Benchmarking llms for comprehensive tool utilization in real-world complex scenarios. arXiv preprint arXiv:2401.17167. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. 2023. Metatool benchmark for large language models: Deciding whether to use tools and which to use. arXiv. Winning the points of llm function calling. arXiv preprint arXiv:2409.00920. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, et al. 2024b. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. arXiv preprint arXiv:2406.18518. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021a. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn. Res., 2022. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. In Neural Information Processing Systems: NeurIPS. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021b. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020. Association for Computational Linguistics. Tom Kwiat kowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428. Chaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao. 2023a. Making large language models Preprint, better foundation for dense retrieval. arXiv:2312.15503. Xing Han Lù. 2024. Bm25s: Orders of magnitude faster lexical search via eager sparse scoring. ArXiv, abs/2407.03618. Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, and Ranjay Krishna. 2024. & ms: benchmark to evaluate tool-use for ulti-step ulti-modal tasks. In European Conference on Computer Vision, pages 1834. Springer. Niklas Muenni ghoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024. Generative representational instruction tuning. Preprint, arXiv:2402.09906. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: human generated machine reading comprehension dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773. CEURWS.org. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2021a. Large dual encoders are generalizable retrievers. ArXiv, abs/2112.07899. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023b. Api-bank: comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al. 2021b. Large dual encoders are generalizable retrievers. arXiv preprint arXiv:2112.07899. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023c. Towards general text embeddings with multi-stage contrastive learning. ArXiv, abs/2308.03281. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. 2024a. Toolace: Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with pretrained sequence-tosequence model. arXiv preprint arXiv:2003.06713. Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334. Cheng Qian, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2023. Toolink: Linking toolkit creation and using through chain-of-solving on open-source model. arXiv preprint arXiv:2310.05155. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, et al. 2024. Tool learning with foundation models. ACM Computing Surveys, 57(4):140. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Jun, and Ji-Rong Wen. 2025a. From exploration to mastery: Enabling llms to master tools via self-driven interactions. In The Thirteenth International Conference on Learning Representations. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2024a. Colt: Towards completeness-oriented tool retrieval for large language models. In CIKM. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2024b. Towards completeness-oriented tool retrieval for large language models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 19301940. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2025b. Tool learning with large language models: survey. Frontiers of Computer Science. Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris Maddison, and Tatsunori Hashimoto. 2023. Identifying the risks of lm agents with an lmemulated sandbox. arXiv preprint arXiv:2309.15817. Keshav Santhanam, O. Khattab, Jon Saad-Falcon, Christopher Potts, and Matei A. Zaharia. 2021a. Colbertv2: Effective and efficient retrieval via In North American lightweight late interaction. Chapter of the Association for Computational Linguistics. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021b. Colbertv2: Effective and efficient retrieval via arXiv preprint lightweight arXiv:2112.01488. late interaction. Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. 2023. Taskbench: Benchmarking large language models for task automation. arXiv preprint arXiv:2311.18760. Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Suzan Verberne, and Zhaochun Ren. 2024. Chain of tools: Large language model is an automatic multitool learner. arXiv preprint arXiv:2405.16533. Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, et al. 2023. Restgpt: Connecting large language models with real-world restful apis. arXiv preprint arXiv:2306.06624. Weiwei Sun, Zhengliang Shi, Jiulong Wu, Lingyong Yan, Xinyu Ma, Yiding Liu, Min Cao, Dawei Yin, and Zhaochun Ren. 2024. Mair: massive benchmark for evaluating instructed retrieval. In EMNLP. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Is chatgpt good at search? Zhaochun Ren. 2023. investigating large language models as re-ranking agents. arXiv preprint arXiv:2304.09542. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301. Hongru Wang, Rui Wang, Boyang Xue, Heming Xia, Jingtao Cao, Zeming Liu, Jeff Pan, and KamFai Wong. 2024a. Appbench: Planning of multiple apis from various apps for complex user instruction. arXiv preprint arXiv:2410.19743. Jize Wang, Zerun Ma, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. 2024b. Gta: benchmark for general tool agents. arXiv preprint arXiv:2407.08713. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368. Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, and Haonan Li. 2024c. Toolgen: Unified tool retrieval and calling via generation. arXiv preprint arXiv:2410.03439. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024d. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030. Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig. 2024e. What are tools anyway? survey from the language model perspective. arXiv preprint arXiv:2403.15452. Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, Tetsuya Sakai, Tian Feng, and Hayato Yamana. 2024. Toolbehonest: multilevel hallucination diagnostic benchmark for toolaugmented large language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP). Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877. Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, and Luca Soldaini. 2024. Followir: Evaluating and teaching information retrieval models to follow instructions. arXiv preprint arXiv:2403.15246. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023a. C-pack: Packaged resources to advance general chinese embedding. ArXiv, abs/2309.07597. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023b. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597. Qiancheng Xu, Yongqi Li, Heming Xia, and Wenjie Li. 2024. Enhancing tool retrieval with iterative feedback from large language models. arXiv preprint arXiv:2406.17465. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2024. Gpt4tools: Teaching large language model to use tools via self-instruction. Advances in Neural Information Processing Systems, 36. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP). Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, et al. 2024a. Tooleyes: Finegrained evaluation for tool learning capabilities of large language models in real-world scenarios. arXiv preprint arXiv:2401.00741. Junjie Ye, Yilong Wu, Songyang Gao, Caishuang Huang, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024b. Rotbench: multi-level benchmark for evaluating the robustness of large language models in tool learning. arXiv preprint arXiv:2401.08326. Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Fung, Hao Peng, and Heng Ji. 2023. Craft: Customizing llms by creating and retrieving from specialized toolsets. arXiv preprint arXiv:2309.17428. Shuo Zhang and Krisztian Balog. 2020. Web table extraction, retrieval, and augmentation: survey. ACM Transactions on Intelligent Systems and Technology (TIST), 11(2):135. Yinger Zhang, Hui Cai, Xeirui Song, Yicheng Chen, Rui Sun, and Jing Zheng. 2023. Reverse chain: generic-rule for llms to master multi-api planning. arXiv preprint arXiv:2310.04474."
        },
        {
            "title": "A Data Card",
            "content": "Following previous work (Bender and Friedman, 2018; Gebru et al., 2021; Zhuo et al., 2024), we provide the datacard for TOOLRET, where we tend to summarize and centralize all information that might be relevant for the benchmark analysis. (i) The purpose of this benchmark: This benchmark is proposed to comprehensively evaluate the information retrieval (IR) models on tool retrieval tasks. On top of TOOLRET, we find that existing IR models, despite achieving strong performance in conventional IR benchmarks such as MTEB and BEIR, still suffer from substantial challenges in tool retrieval tasks. The poor retrieval quality further degrades the end-to-end task pass rate of tool-use LLMs. Thus, we believe that the TOOLRET reveals the importance of tool retrieval in building better tool-use LLMs, and can be used as comprehensive and fair benchmark in facilitating the development of tool retrieval models. (ii) How will the dataset be distributed (e.g., Tarball on Website or Github)? The proposed benchmark TOOLRET will be released to the public, and hosted on GitHub and Hugging Face. The TOOLRET will be managed and maintained by our research team. (iii) Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? Yes. If we include more tasks or find any errors, we will correct the dataset hosted on Hugging Face and GitHub and update the results in the leaderboard accordingly. It will be updated on our website. (iv) Will the training dataset TOOLRET-train will be released publicly. Yes, the proposed training dataset TOOLRET-train will be released to the public, and hosted on GitHub and Hugging Face."
        },
        {
            "title": "B Details of Benchmark",
            "content": "B.1 Dataset collections TOOLRET is heterogeneous benchmark that integrates wide range of well-established tool-use datasets and aligns them into unified format, similar to standard information retrieval (IR) benchmarks such as BEIR and MTEB, to facilitate tool retrieval evaluation. In tool learning, we observe that previous work primarily focuses on three mainstream types of tools: (i) Web APIs: These tools are encapsulated in the OpenAPI format (standard JSON documentation) and can be directly invoked via HTTP requests. Web APIs are typically used to access, manipulate (e.g., add, delete, edit, or query), or retrieve private data or information from specialized databases, covering wide range of domains such as movies, music, and sports. (ii) Code Functions: These tools are represented by source code containing function signatures and implementation details. Code functions primarily focus on low-level computations or atomic operations, such as tensor calculations, calling Hugging Face models, or utilizing PyTorch libraries. (iii) Customized Apps: These tools are paired with free-form natural language descriptions. They are typically user-oriented or personalized, enabling tasks such as sending emails or other custom applications. These tool types differ in functionality and documentation format, reflecting diverse scenarios for tool-use LLMs. For IR models, retrieving different types of tools may present varying levels of difficulty. Therefore, we categorize the collected datasets into these three types based on their paired toolset formats, resulting in three subsets of TOOLRET: TOOLRET-web, TOOLRET-code, and TOOLRET-customized. During evaluation, we report the performance of IR models on each subset to provide fine-grained analysis. Below, we list the datasets included in each subset and provide detailed descriptions. B.2 TOOLRET-Web The TOOLRET-Web subset is constructed by integrating the following datasets, which contain tools in the form of Web APIs: AutoTools-Food (Shi et al., 2024): Contains APIs related to food recipes, where LLMs must retrieve specific food-related tools to answer user queries. RestGPT-TMDB (Song et al., 2023) and AutoTools-Movie (Shi et al., 2024): Includes web APIs from the TMDB platform, movie database. Evaluation tasks require LLMs to retrieve tools to find relevant information about movies or celebrities and extract key evidence to answer given queries. AutoTools-Weather (Shi et al., 2024): Features web APIs from weather database. LLMs must invoke these APIs and gather responses to answer weather-related queries. RestGPT-Spotify and AutoTools-Music (Shi et al., 2024): Contains web APIs from music platform. Evaluation tasks require LLMs to retrieve tools for searching songs or albums based on user queries. ToolBench (Qin et al., 2023): Comprises over 16,000 web APIs crawled from RapidAPI. Queries are generated by LLMs, with ground truth tools labeled for each query. ToolLens (Qu et al., 2024a): subset of ToolBench, where queries are annotated to evaluate tool functionality. APIbank (Li et al., 2023b): Contains web APIs for daily personalized applications, such as alarm booking and database login. MetaTool (ToolE) (Huang et al., 2023): Designed to evaluate whether LLMs are aware of tool usage and can correctly select tools. Mnms (Ma et al., 2024): Evaluates LLM-based agents tool-use abilities for multi-step, multimodal tasks involving tools that process visual information. Since TOOLRET focuses on text-based IR models, images are represented using their URLs. Reverse-Chain (Zhang et al., 2023): Contains diverse multi-step tasks requiring LLMs to invoke relevant tools sequentially. ToolEyes (Ye et al., 2024a): Includes tools across various domains, such as advice, entertainment, and art, providing broad evaluation of tool-use LLMs in practical scenarios. UltraTool (Huang et al., 2024): benchmark designed to improve and evaluate LLMs tool utilization abilities in real-world scenarios, focusing on the entire process of planning, creating, and applying tools in complex tasks. T-Eval (Chen et al., 2023): fine-grained benchmark assessing tool-use LLMs across multiple evaluation aspects, including instruction following, planning, reasoning, retrieval, understanding, and review. B.3 TOOLRET-Code The TOOLRET-code subset is constructed by integrating the following datasets, which contain tools in the form of code functions: Gorilla-PyTorch (Patil et al., 2023): Contains various PyTorch functions (code snippets) as tools, evaluating LLMs ability to correctly combine PyTorch functions for solving deep learning tasks. The functions in this dataset are collected from the Python Torch package. Dataset Web APIs GTA (Wang et al., 2024b) Gorilla (Patil et al., 2023) - gorilla-pytorch subset - gorilla-tensor subset - gorilla-huggingface subset CRAFT (Yuan et al., 2023) - craft-Tabmwp subset - craft-Vqa subset - craft-algebra subset AutoTools (Shi et al., 2024) - AutoTools-Food subset - AutoTools-Weather subset - AutoTools-Movie subset - AutoTools-music subset APIGen (Liu et al., 2024b) APIbank (Li et al., 2023b) Appbench (Wang et al., 2024a) Mms (Ma et al., 2024) Metatool (a.k.a., ToolE) (Huang et al., 2023) Reverse Chain (Zhang et al., 2023) RestGPT (Song et al., 2023) - RestGPT-TMDB subset - RestGPT-Spotify subset Toolbench (Qin et al., 2023) - G1-instruction subset - G1-Tool subset - G1-category subset - G2-instruction subset - G2-instruction subset - G3-instruction subset ToolLens (Qu et al., 2024a) Tooleyes (Ye et al., 2024a) ToolACE (Liu et al., 2024a) GPT4tools (Yang et al., 2024) Rotbench (Ye et al., 2024b) T-eval (Chen et al., 2023) - T-eval-step level subset - T-eval-dialogue level subset Taskbench (Shen et al., 2023) - TaskBench-multimedia subset - TaskBench-daily subset - TaskBench-DL subset ToolAlpaca (Tang et al., 2023) Toolbench-sam (Xu et al., 2023) ToolEmu (Ruan et al., 2023) TooLink (Qian et al., 2023) UltraTool (Huang et al., 2024) Tool-be-honest (Zhang et al., 2024) Endpoint Query size Tool size Task type https://github.com/open-compass/GTA https://github.com/ShishirPatil/gorilla https://github.com/ShishirPatil/gorilla/tree/main/data https://github.com/ShishirPatil/gorilla/tree/main/data https://github.com/ShishirPatil/gorilla/tree/main/data https://github.com/lifan-yuan/CRAFT https://github.com/lifan-yuan/CRAFT/tree/main/tab_and_math/TabMWP https://github.com/lifan-yuan/CRAFT/tree/main/vqa https://github.com/lifan-yuan/CRAFT/tree/main/tab_and_math/MATH https://github.com/mangopy/AutoTools https://github.com/mangopy/AutoTools/tree/main/data https://github.com/mangopy/AutoTools/tree/main/data https://github.com/mangopy/AutoTools/tree/main/data https://github.com/mangopy/AutoTools/tree/main/data https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k https://github.com/AlibabaResearch/DAMO-ConvAI https://github.com/ruleGreen/AppBench https://github.com/RAIVNLab/mnms https://github.com/HowieHwong/MetaTool https://github.com/ASK-03/Reverse-Chain https://github.com/Yifan-Song793/RestGPT https://github.com/Yifan-Song793/RestGPT/datasets/tmdb.json https://github.com/Yifan-Song793/RestGPT/datasets/spotify.json https://github.com/OpenBMB/ToolBench https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J https://github.com/quchangle1/COLT https://github.com/Junjie-Ye/ToolEyes https://huggingface.co/datasets/Team-ACE/ToolACE/ https://github.com/AILab-CVC/GPT4Tools https://github.com/Junjie-Ye/RoTBench https://github.com/open-compass/T-Eval https://huggingface.co/datasets/lovesnowbest/T-Eval https://huggingface.co/datasets/lovesnowbest/T-Eval https://github.com/microsoft/JARVIS https://github.com/microsoft/JARVIS/taskbench/multimedia https://github.com/microsoft/JARVIS/taskbench/dailylifeapis https://github.com/microsoft/JARVIS/taskbench/huggingface https://github.com/tangqiaoyu/ToolAlpaca https://github.com/sambanova/toolbench https://github.com/ryoungj/ToolEmu https://github.com/qiancheng0/Toolink https://github.com/JoeYing1019/UltraTool https://github.com/ToolBeHonest/ToolBeHonest 14 598 43 55 500 654 174 200 280 159 22 11 54 72 1,000 101 32 33 200 200 94 54 40 1,100 200 200 200 200 200 100 314 95 1,000 32 550 100 50 50 103 40 40 23 94 197 38 497 500 350 14 1,005 43 55 907 985 180 525 280 159 22 11 54 72 3,605 101 32 33 200 783 94 54 40 13,862 13,862 13,862 13,862 13,862 13,862 13,862 314 95 16,072 32 919 100 50 50 103 40 40 23 1,937 197 38 1,804 1,171 892 Table 6: The detailed statistics about the each collected dataset in TOOLRET. We highlight that the subsets of ToolBench share same toolsets containing 13,000+ tools. Besides, the TOOLRET combine and deduplicate the toolsets from the above datasets to build the final tool retrieval corpus. Gorilla-Tensor (Patil et al., 2023): Includes TensorFlow functions as tools, collected from TensorFlow Hub, to assess LLMs tool selection capabilities in deep learning scenarios. Gorilla-HuggingFace (Patil et al., 2023): Treats specific downstream models from the Hugging Face platform as tools. This dataset evaluates LLMs performance in correctly calling Hugging Face models based on user queries. CRAFT-TabMWP (Yuan et al., 2023): Evaluates LLMs ability to use functions for table processing. The functions in this dataset are first generated by GPT-4 and subsequently verified. CRAFT-VQA (Yuan et al., 2023): Provides evaluation cases for visual question answering (VQA), where LLMs must call image processing functions such as image capture and object detection. CRAFT-Math-Algebra (Yuan et al., 2023): Assesses LLMs ability to invoke algebra functions for solving complex mathematical problems. B.4 TOOLRET-Customized Besides Web APIs and code functions, we also collect datasets that contain customized apps. Unlike Web APIs and code functions, customized apps are described using free-form natural language documentation Algorithm 1: The pseudo algorithm for our target-aware strategy in automatically constructing instructions for evaluation tasks. Input: set of seed instructions = {si [N ]} manually crafted by human experts; powerful LLM (e.g., GPT-4o); Collected tasks = {tii [T ]} Initialize an instruction pool S; for do 1, 2, ..., k} from I; Sample examples {s // Generate new instruction si using through in-context learning: si M(prompt with {s //Append new instruction to pool: = {si} ; 2, ..., 1, k}); Apply heuristic filtering to remove low-quality instructions from I; Output: set of high-quality instructions = {s1, s2, ..., sT } rather than structured formats. Specifically, we include the following datasets: ToolACE (Liu et al., 2024a), GPT4Tools (Yang et al., 2024), TaskBench (Shen et al., 2023), ToolAlpaca, ToolBench-sam (Xu et al., 2023), ToolEmu (Ruan et al., 2023), and TooLink (Qian et al., 2023). B.5 Task format The final benchmark, TOOLRET, integrates the above datasets and reformats all test cases into unified format, similar to conventional IR benchmarks such as BEIR and MTEB, to evaluate IR models in tool retrieval tasks. Each reformatted task consists of: an input query, an instruction, and the corresponding target tools (e.g., labels). Each tool is assigned unique identifier and is paired with detailed documentation describing its functionality. Below, we present concrete example from TOOLRET. # An example of an evaluation task in our proposed benchmark - Query : need to find grocery store near 123 Main Street , Downtown District that has good selection of limes for my Easter celebration . - ID : toolLens_query_7 - Target tools ( labels ): toolLens_tool_20 , toolLens_tool_50 , toolLens_tool_2 - Instruction : Given ` local grocery search ` task , retrieve tools that can locate grocery stores based on the user ' specified location and criteria , such as the availability of specific items like limes , to meet the query ' requirements . # Examples of tool documentation - toolLens_tool_20 : {\" category_name \": \" Food \" , \" required_parameters \": [{\" name \": \" ingredient \", \" type \": \" STRING \", \" description \": \"\" , \" default \": \" strawberry \"}] , \" optional_parameters \": [] , \" method \": \" GET \", \" template_response \": {\" name \": \" str \", \" ingredients \": [\" list of str with length 9\"] , \" instructions \": [\" list of str with length 7\"]} , \" name \": \" pastry / ingredient \", \" description \": \" This API endpoint allows users to retrieve random pastry recipe that contains specific ingredient . Users can make GET request to the endpoint with the name of the ingredient as query parameter , and the API will return JSON response with the given recipe , including the name , list of ingredients , and instructions .\"} - toolLens_tool_50 : {\" category_name \": \" Health_and_Fitness \", \" required_parameters \": [] , \" optional_parameters \": [{\" name \": \" limit \" , \" type \": \" NUMBER \", \" description \": \" limit the length of response \", \" default \": \"10\"}] , \" method \": \" GET \", \" template_response \": {\" count \": \" int \", \" food \": [{\" _id \": \" str \", \" food_name \": \" str \", \" quantity \": \" str \", \" calories \": \" int \", \" uri \": \" str \", \" type \": \" str \", \" type_uri \": \" str \", \" core \": \" str \", \" core_uri \": \" str \", \" food_nutrition \": [{\" nutrient_name \": \" str \", \" value \": \" float \", \" unit \": \" str \", \" _list_length \": 3}] , \" _list_length \": 10}]} , \" name \": \" View All Food Items \", \" description \": \" The request allows clients to retrieve comprehensive list of all available food items . nAPI request sent to [ https :// indnutrientsapi . tech / food ]( https :// indnutrientsapi . tech / food ) \"} - toolLens_tool_2 : {\" category_name \": \" Food \", \" required_parameters \": [{\" name \": \" grocery \", \" type \": \" string \", \" description \": \"\" , \" default \": \"\"}] , \" optional_parameters \": [] , \" method \": \" GET \", \" template_response \": {\" message \": \" str \"} , \" name \": \" Search Grocery \", \" description \": \" Search specific grocery \"} B.6 Details of instruction construction In TOOLRET, each task is paired with an instruction using target-aware strategy, where GPT-4o acts as an automatic expert through in-context learning. Specifically, we follow these steps. We first invite three human experts with strong backgrounds in NLP and IR to manually craft seed instructions. These expert-crafted instructions form an initial example pool. For each task, we randomly sample set of instructions from this pool as in-context learning examples. Using these examples, GPT-4o generates new instruction tailored to the given task. The newly generated instruction is then appended back to the instruction pool to enhance instruction diversity. The detailed pseudo algorithm is provided in Alg. 1. Below, we provide concrete example of the GPT-4o prompt used in our instruction construction process. The example of seed instructions and the generated instructions is provided in Table 8. # The prompt for GPT -4 o. Given query , you need to design an instruction about 20 words that clearly indicates this is task to retrieve tools capable of solving the query based on its content . The instruction should emphasize the task requirements and target outcomes of the query while incorporating the functional characteristics of the tools to help the system accurately match the appropriate tools . Below , have provided the target tools ( i. e. , the labels for the query ). Please analyze the key aspects of the query and the tool descriptions . Your instruction should implicitly highlight the task requirements and the characteristics of the target tools relevant to the query . Here is an output template that your should follow . Please note that the instruction should be concise . Query : would like to generate video presenting text - based discussion on the topic of ' The Benefits of Exercise ' Labels : [1] { ' id ': ' taskbench_data_huggingface_tool_5 ' , 'doc ': {' input - type ': [' text '] , ' output - type ': [' text '] , ' name ': ' Text Generation ' , ' description ': ' Generating text is the task of producing new text . These models can , for example , fill in incomplete text or paraphrase . '}} Instruction : Given ` text -to - video ` task , retrieve tools that process text inputs to generate coherent textual outputs aligned with the query ' topic and requirements . Query : have an audio file ' example . wav ' which is difficult to understand . would like you to help me transcribe the audio to text Labels : [1] { ' id ': ' taskbench_data_huggingface_tool_19 ' , 'doc ': {' input - type ': [' audio '] , ' output - type ': [' text '] , ' name ': ' Automatic Speech Recognition ' , ' description ': ' Automatic Speech Recognition ( ASR ) , also known as Speech to Text ( STT ) , is the task of transcribing given audio to text . It has many applications , such as voice user interfaces . '} , ' relevance ': 1} Instruction : Given ` audio transcription ` task , retrieve tools that process audio inputs to produce accurate textual transcriptions aligned with the query ' requirements . Query : Conduct two - sample independent - test with two samples , sample1 =[1 , 2, 3, 4, 5] and sample2 =[6 , 7, 8, 9, 10] , and significance level of 0.05. Labels : [1] { ' id ': ' tool_id_693 ' , 'doc ': {' name ': ' independent_samples_t_test ' , ' description ': ' Conducts two - sample independent - test and returns the - statistic , - value , and conclusion .', ' parameters ': {' sample1 ': {' description ': ' The first sample of observations . ' , ' type ': ' List [ float ] ' , ' default ': 0.05} , ' sample2 ': {' description ': ' The second sample of observations . ' , ' type ': ' List [ float ] ' , ' default ': 0.05} , ' alpha ': {' description ': ' The significance level of the test . Defaults to 0.05. ' , ' type ': ' float , optional '}}} , ' relevance ': 1} Instruction : Given ` significance test ` task , retrieve tools that perform statistical tests , specifically two - sample independent - test , by processing numerical inputs and returning the - statistic , - value . Query : Can get list of all boards and their attributes on page number two with page size of seven ? Labels : [1] { ' id ': ' ToolEyes_tool_34 ' , 'doc ': {' name ': ' get_boards ', ' description ': 'A list of all boards and their attributes . ' , ' parameters ': {' type ': ' object ' , ' properties ': {' page ': {' type ': ' string ' , ' description ': ' Get the items on specific page . 0( default ) is the first page . '} , ' page_size ': {' type ': ' string ' , ' description ': ' Get the number of boards on specific page . Default : 5. '}} , ' required ': []}}} Instruction : Given ` pagination query ` task , retrieve tools that can list boards and their attributes by processing parameters such as page number and page size to return the requested information . B.7 Human annotation To ensure the quality of the generated instructions, we conduct human annotation to review them based on four key aspects listed in Table 3. For instructions deemed low-quality, human annotators manually revise them to improve accuracy and clarity. For clear illustration, we provide concrete examples of handcrafted instructions, high-quality generated instructions, and low-quality generated instructions in Table 8. Below, we also provide the detailed human annotation guidelines used in our review process for reproducibility and transparency. # Human guidance for instruction quality annotation We ask you to evaluate the quality of the generated instructions based on the following four aspects . Please carefully assess each instruction and provide your judgment : ## Aspects for annotation 1. Hallucination Check : Does the instruction contain any incorrect or fabricated information about the target tools or the input query ? ( Are there any details in the instruction that do not align with the actual features of the target tools or the content of the input query ?) 2. Comprehensiveness of Tool Features : Does the instruction fully and accurately describe the features of all target tools mentioned in the query ? ( Are there any important features of the target tools that are missing or inadequately described in the instruction ?) 3. Accuracy of Tool Feature Description : Does the instruction correctly describe the features of the target tools ? ( Key question to ask : Are the descriptions of the target tools technically accurate and consistent with their actual functionality ?) 4. Relevance to Input Query : Is the instruction directly relevant to the original input query ? ( Key question to ask : Does the instruction address the specific needs or context provided in the input query , or does it deviate from the query ' intent ?) ## Detailed annotation Process For each instruction , evaluate it based on the four aspects above . 1. If the instruction meets all criteria ( no hallucination , comprehensive , accurate , and relevant ) , mark it as correct . 2. If the instruction fails to meet any of the criteria , mark it as incorrect and provide brief explanation of the issue (e.g., \" contains hallucination ,\" \" missing key tool features ,\" or \" irrelevant to query \") . For incorrect instructions , `` revise `` them to ensure they meet all quality criteria . The goal of this annotation process is to ensure that all instructions in our benchmark are of high quality and faithfully grounded in the original queries and target tools . Large-scaling training dataset: TOOLRET-train We extend the data collection process from TOOLRET to incorporate the training sets of three mainstream tool-use datasets: ToolACE (Liu et al., 2024a), ToolBench (Qin et al., 2023), and APIGen (Liu et al., 2024b). These datasets are selected for their diversity in task types, tool categories, and query complexity, ensuring comprehensive representation of real-world tool-use scenarios. After collecting and preprocessing the data, we ultimately collect over 200k training instances. Each instance consists of query and corresponding set of target tools. Next, we further pair each query with an instruction using our target-aware strategy ( 3.3). This strategy generates instructions that explicitly guide the retrieval process by incorporating task-specific context and tool functionality descriptions. We report the basic statistics of TOOLRET-train in Table 7. We also show training example of our TOOLRET-train. # Query : Is ' https :// www . apple . com ' available in the Wayback Machine on September 9, 2015? # Instruction : Given ` URL availability ` task , retrieve tools that check if given URL is archived and accessible on specific date in the Wayback Machine . # Target tools ( labels ): [ '{ ' name ': ' availability ' , ' description ': ' Checks if given URL is archived and currently accessible in the Wayback Machine . ' , ' parameters ': {' url ': {' description ': ' The URL to check for availability in the Wayback Machine . ' , ' type ': 'str ', ' de ...}}} '] # Negative tools : [ { ' name ': ' top_grossing_mac_apps ' , ' description ': ' Fetches list of the top - grossing Mac apps from the App Store . ', ' parameters ': {' category ': {' description ': \" The category ID for the apps to be fetched . Defaults to '6016 ' ( general category ) .\" , ' type ': 'str ' , ' default ': '6016 '} , ' country ': {' descript ...} , { ' name ': ' top_paid_mac_apps ' , ' description ': ' Retrieves list of the top paid Mac apps from the App Store . ', ' parameters ': {' category ': {' description '...} , ... { ' name ': ' exact_url_non_english ' , ' description ': ' Retrieves the backlinks of specific non - English URL using the RapidAPI service ...} ]"
        },
        {
            "title": "Statistic",
            "content": "# size of retrieval task # Average token length of the input query # Average token length of the paired iinstruction # Average token length of the tool documentation # Number of negative tools per input query # Number of target tools (labels) per input query 205,826 52.87 46.72 163.52 5 2.31 Table 7: Basic statistics of the collected large-scaling training set TOOLRET-train. We use the tokenizer from gpt-3.5-turbo in this work."
        },
        {
            "title": "D More experiment details",
            "content": "D.1 Baselines We comprehensively evaluate the following mainstream retrieval models on our benchmark, including: Sparse Retrieval. These methods measure the similarity between tasks and tool documentation based on lexical overlap. We evaluate BM25s (Lù, 2024). Single-task dense retrieval. These methods employ dual-encoder architecture models trained on conventional IR datasets. We evaluate gtr (Ni et al., 2021a), contriever (Izacard et al., 2021a), and colbertv2.0 (Santhanam et al., 2021a), all trained on MS-MARCO (Nguyen et al., 2016) with relevance criteria. We also evaluate the COLT (Qu et al., 2024a) which is recently proposed model trained on an ad-hoc tool retrieval dataset. Example of our seed instructions (handcrafted instruction # Query: would like to generate video presenting text-based discussion on the topic of The Benefits of Exercise. # Instruction: Given \"text-to-video\" task, retrieve tools that process text inputs to generate coherent textual outputs aligned with the querys topic and requirements. # Query: have an audio file example.wav which is difficult to understand. would like you to help me transcribe the audio to text. # Instruction: Given \"audio transcription\" task, retrieve tools that process audio inputs to produce accurate textual transcriptions aligned with the querys requirements. # Query: Conduct two-sample independent t-test with two samples, sample1=[1, 2, 3, 4, 5] and sample2=[6, 7, 8, 9, 10], and significance level of 0.05. # Instruction: Given \"significance test\" task, retrieve tools that perform statistical tests, specifically two-sample independent t-test, by processing numerical inputs and returning the t-statistic, p-value. # Query: Can you cancel timer for my smart device? # Instruction: Given \"timer cancellation\" task, retrieve tools that handle smart device operations by processing device ID and switch time inputs to cancel scheduled action and return the status of the operation. # Query: Find cruise tickets from Fontana to Santa Rosa on date 2023-07-04. # Instruction: Given \"ticket booking\" task, retrieve tools that support booking cruise tickets by processing travel details such as departure location, destination, date, and time. Example of our generated instructions (high-quality instructions) # Query: Suppose that (x) = 4x + 5. What is 1(f 1(9))? # Instruction: Given \"inverse function calculation\" task, retrieve tools that calculate the value of the repeated inverse for linear function by processing coefficients, constants, and target values to determine the result. # Query: Identify an function that can classify images and works with spiking neural networks. # Instruction: Given an \"image classification\" task, retrieve tools that execute image classification by using spiking neural network models and processing image inputs. # Query: need to find grocery store near 123 Main Street, Downtown District that has good selection of limes for my Easter celebration. # Instruction\": \"Given \"local grocery search\" task, retrieve tools that can locate grocery stores based on the users specified location and criteria, such as the availability of specific items like limes, to meet the querys requirements. # Query: Please help me find recipe with no more than 40 grams of carbohydrates per gram and at least 5 grams of protein per gram. # Instruction: \"Given \"nutritional recipe search\" task, retrieve tools that can find recipes based on specific nutritional criteria such as carbohydrate and protein content. # Query: What is 64277 times 38142? # Instruction: Given \"multiplication\" task, retrieve tools that compute the product of two numbers by processing numerical inputs and returning the result. # Query: Can get list of all boards and their attributes on page number two with page size of seven? # Instruction: Given \"pagination query\" task, retrieve tools that can list boards and their attributes by processing parameters such as page number and page size to return the requested information. Example of our generated instructions (low-quality instructions) # Query: would like to generate video presenting text-based discussion on the topic of The Benefits of Exercise # Instruction: Given text-to-video task, please retrieve relevant tools to generate video about exercise. // Too general to cover the key features. # Revised version: Given text-to-video task, retrieve tools related to general video scripting, exercise video libraries or tools process text data. // More specific to the query and target tools. # Query: Can you assist me in finding 1-bedroom townhouse or condo in Little Rock with max rent 1541000? want it on the sixth floor with 7 balconies. # Instruction: Please retrieve tools find 1-bedroom or condo in Little Rock. // Too general and miss the point of floor # Revised version: Given property search task, retrieve tools that can find rental properties based on location, property type, rent budget, and specific features or requirements. // Cover all key points and related to tools functionality. Table 8: Example of the instruction in TOOLRET. We show the handcrafted instruction by human experts, the highquality instruction and low-quality instruction generated by GPT-4o, respectively. We also show the revised version of the low-quality instruction paired with the reason for revision. Name Conventional sparse and dense models BM25S (Lù, 2024) contriever (Izacard et al., 2021b) ColBERTv2 (Santhanam et al., 2021b) gtr-t5-base (Ni et al., 2021b) gtr-t5-large (Ni et al., 2021b) Multi-task embedding models all-MiniLM-L6-v2 e5-small-v2 (Wang et al., 2022) e5-base-v2 (Wang et al., 2022) e5-large-v2 (Wang et al., 2022) bge-base-en-v1.5 (Xiao et al., 2023b) bge-large-en-v1.5 (Xiao et al., 2023b) gte-Qwen2-1.5B-inst. (Li et al., 2023c) e5-mistral-7b (Wang et al., 2023) GritLM-7B (Muennighoff et al., 2024) NV-Embed-v1 (Lee et al., 2024) Cross-encoder re-ranking Public Link of Endpoint https://github.com/xhluca/bm25s https://huggingface.co/facebook/contriever-msmarco https://github.com/stanford-futuredata/ColBERT https://huggingface.co/sentence-transformers/gtr-t5-base https://huggingface.co/sentence-transformers/gtr-t5-large https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 https://huggingface.co/intfloat/e5-small-v2 https://huggingface.co/intfloat/e5-base-v2 https://huggingface.co/intfloat/e5-large-v2 https://huggingface.co/BAAI/bge-base-en-v1.5 https://huggingface.co/BAAI/bge-large-en-v1. https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct https://huggingface.co/intfloat/e5-mistral-7b-instruct https://huggingface.co/GritLM/GritLM-7B https://huggingface.co/nvidia/NV-Embed-v1 mxbai-rerank-large-v1 monot5-base (Nogueira et al., 2020) bge-reranker-v2-m3 (Li et al., 2023a; Chen et al., 2024a) jina-reranker-v2 bge-reranker-v2-gemma (Li et al., 2023a; Chen et al., 2024a) https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1 https://huggingface.co/castorini/monot5-base-med-msmarco https://huggingface.co/BAAI/bge-reranker-v2-m3 https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual https://huggingface.co/BAAI/bge-reranker-v2-gemma LLM agent RankGPT - Mixtral-8x22B - GPT-3.5-turbo-1106 - GPT-3.5-turbo-0125 https://github.com/sunnweiwei/RankGPT https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1 https://openai.com/chatgpt/overview/ https://openai.com/chatgpt/overview/ Table 9: The public link or endpoint of the baselines in our experiments. Multi-task Embedding Models. These methods utilize transformer encoders trained on various annotated IR datasets. We evaluate gte (Li et al., 2023c), bge (Xiao et al., 2023a), and e5 (Wang et al., 2022), covering wide range of parameter sizes. Additionally, we evaluate all-MiniLM-L6-v25 from the Sentence Transformers platform. Cross-encoder Re-rankers. These models re-rank the initially retrieved documents based on the query-passage relevance using bidirectional or unidirectional transformers. We evaluate MonoT5Base and three re-rankers trained on diverse tasks: (i) mxbai-rerank-large-v16, (ii) jina-reranker-v2base7, and (iii) BGE-reranker. LLM Agents. These methods leverage general-purpose LLM agents for re-ranking tasks in zeroshot setting, simulating the tool selection process of tool-use agents. We evaluate the widely used LLM re-ranking framework, i.e., RankGPT (Sun et al., 2023), with various LLMs as backbone. We highlight that the initial tools for LLM agent and Re-ranking baselines are retrieved by NV-embedd-v1 model. Details about these baselines are provided in Table 9. D.2 Compare with conventional IR tasks To further investigate the complexity of tool retrieval tasks, we conducted comparative analysis of model performance between our proposed benchmark (TOOLRET) and the conventional Information Retrieval (IR) task benchmark, specifically the Massive Text Embedding Benchmark (MTEB). The relationship between these two benchmarks is visually presented in Figure 7. Our analysis reveals two significant findings. First , we observe strong positive correlation between the two benchmarks, as evidenced by Pearsons correlation coefficient of β = 0.790, indicating similar performance trend across models. However, we observe that the absolute performance scores in TOOLRET are consistently lower than those in the 5huggingface.co/all-MiniLM-L6-v2 6huggingface.co/mxbai-rerank-large-v1 7huggingface.co/jina-reranker-v2-base-multilingual"
        },
        {
            "title": "Customized tool",
            "content": "Avg. N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@10 62.09 56.73 57.81 57.06 60.32 Conventional sparse and dense models BM25s ColBERT contriever-msmarco gtr-t5-base gtr-t5-large Embedding models all-MiniLM-L6-v2 e5-small-v2 e5-base-v2 e5-large-v2 gte-base-en-v1.5 gte-large-en-v1.5 bge-base-en-v1.5 bge-large-en-v1.5 53.92 61.95 62.90 61.72 64.35 60.67 65.05 66.25 15.68 14.59 15.33 14.54 15. 14.47 15.84 15.90 15.90 16.55 15.46 16.37 16.48 gte-Qwen2-1.5B-inst. 67.57 e5-mistral-7b 69.51 GritLM-7B 69.43 NV-Embed-v1 66.04 Cross-encoder re-ranking models mxbai-rerank-large-v1 57.48 monot5-base-msmarco 54.57 bge-reranker-v2-m3 70.42 bge-reranker-v2-gemma 75.67 16.93 17.37 17.25 16.88 14.60 14.23 17.75 18.63 72.98 67.86 70.77 68.26 72.05 66.77 72.91 73.98 73.27 75.80 72.30 75.72 75. 78.14 79.34 78.97 77.19 68.65 64.38 80.33 84.07 58.06 47.87 50.87 49.75 54.03 48.64 53.34 53.83 52.84 57.38 52.41 57.30 57.75 60.81 62.48 61.67 59.06 49.54 46.12 65.49 71. 56.98 53.56 49.66 49.38 52.79 50.14 51.45 55.81 56.21 59.18 54.11 54.55 58.61 58.12 58.15 62.78 63.46 50.37 50.00 64.22 69.59 8.24 7.82 7.29 7.29 7.41 7.58 7.76 8.44 8.42 8.77 8.22 7.72 8. 8.51 8.37 9.22 9.40 7.75 8.05 9.37 9.78 73.95 71.66 65.99 65.76 67.28 68.39 68.16 74.17 75.25 76.95 73.35 69.22 74.91 75.41 75.12 78.74 81.79 69.59 68.76 80.60 84. 72.81 70.09 64.56 64.09 65.79 66.74 65.46 72.53 73.14 74.45 71.37 67.48 72.74 73.39 72.79 77.59 79.82 67.88 66.80 79.48 83.47 68.48 64.49 68.86 70.48 72.03 68.31 69.13 69.96 69.88 71.79 68.59 71.21 71. 71.73 72.52 76.04 75.39 62.24 64.50 75.70 77.17 14.78 13.05 14.67 14.29 14.69 14.20 14.24 14.98 15.01 14.53 14.36 14.71 14.20 15.34 14.68 15.44 15.75 13.32 13.28 16.15 16. 80.51 76.35 83.05 81.60 84.39 81.57 79.69 83.63 81.13 81.90 80.41 83.13 80.44 83.03 81.79 85.55 88.48 73.26 75.80 88.87 88.34 69.55 64.85 73.45 70.96 73.95 71.14 68.33 73.94 71.30 70.07 69.82 72.53 69. 73.39 71.49 74.35 78.37 61.24 67.84 78.65 79.80 62.51 58.26 58.77 58.97 61.72 57.46 60.85 62.89 62.60 65.11 61.12 63.60 65.35 65.81 66.73 69.42 68.30 56.70 56.36 70.11 74. 66.81 60.94 62.96 61.60 64.59 62.17 62.38 66.77 65.76 67.30 64.53 65.77 66.59 69.19 68.92 71.21 72.42 59.55 60.25 74.54 78.09 Table 10: Results of control experiment where each IR models is evaluated from the toolset of each integrated dataset in w/ inst. setting. Figure 7: Correlation between the score on our benchmark and MTEB (retrieval subset). conventional IR benchmark. This discrepancy suggests that while our benchmark shares fundamental characteristics with conventional IR tasks, it presents additional challenges that make it more demanding for existing models. Second , our experimental results demonstrate that state-of-the-art IR models, particularly those trained with relevance-oriented optimization criteria (e.g., Contriever), exhibit substantially degraded performance on TOOLRET. This performance gap underscores the necessity for target-aware reasoning capabilities in our benchmark, which goes beyond traditional relevance matching. The unique challenges of TOOLRET are further elaborated in 4.1, where we identify two key distinguishing factors: (1) the presence of Model TOOLRET-Web TOOLRET-Code TOOLRET-Customized Avg. N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@10 13.78 13.56 14.55 14.13 15.06 51.79 51.70 53.23 51.65 56.62 Conventional sparse and dense models BM25S ColBERT contriever-msmarco gtr-t5-base gtr-t5-large Embedding models all-MiniLM-L6-v2 e5-small-v2 e5-base-v2 e5-large-v2 gte-base-en-v1.5 gte-large-en-v1.5 bge-base-en-v1.5 bge-large-en-v1.5 48.49 54.40 55.42 54.32 56.48 55.39 56.17 58.20 gte-Qwen2-1.5B-inst. 60.28 e5-mistral-7b 60.78 GritLM-7B 62.54 NV-Embed-v1 61.76 Cross-encoder re-ranking models mxbai-rerank-large-v1 monot5-base-msmarco bge-reranker-v2-m3 jina-reranker-v2-base bge-reranker-v2-gemma 56.45 56.10 61.78 65.86 65.80 13.54 14.60 14.92 14.81 15.40 14.89 14.86 15.33 15.64 15.93 16.07 16.02 14.51 14.82 15.94 17.14 16.87 63.35 61.92 65.96 64.55 69.77 62.35 66.47 67.90 67.69 70.07 68.33 68.30 69. 72.60 73.12 73.82 73.86 67.51 65.29 72.35 77.54 76.85 45.46 41.01 46.59 44.56 50.26 43.63 46.76 48.10 47.89 50.96 48.98 49.05 50.20 53.82 55.60 54.46 55.96 49.02 47.12 55.61 62.33 61. 38.74 38.60 35.97 33.98 37.40 34.40 35.18 38.35 40.24 39.46 38.23 38.71 40.39 44.06 44.20 46.80 50.38 42.55 41.05 45.15 47.23 52.49 5.87 6.05 5.79 5.51 5.85 5.62 5.67 6.23 6.23 6.27 6.22 6.05 6. 6.56 6.77 6.98 7.54 6.28 6.31 6.74 7.01 7.60 52.65 55.07 52.32 48.88 52.41 50.15 50.70 56.08 56.33 56.58 56.16 54.35 55.03 59.29 61.18 62.89 67.93 57.85 57.20 61.03 63.50 68. 51.39 54.05 50.84 47.77 51.27 48.71 49.19 54.90 54.85 55.24 54.95 53.08 53.67 57.78 59.79 61.35 66.03 55.96 55.14 59.56 62.24 65.94 59.72 53.91 56.94 54.28 56.24 58.08 56.85 59.96 59.40 64.00 57.88 59.40 61. 65.57 60.56 67.61 67.01 54.63 64.60 62.45 69.10 67.87 13.55 12.10 13.21 13.24 13.31 13.22 13.45 14.18 13.79 14.23 13.86 13.93 13.83 14.79 13.80 14.93 14.61 13.03 13.91 14.86 15.38 15. 71.83 66.85 72.11 70.95 71.25 72.72 73.43 76.18 72.96 78.03 75.12 75.38 77.27 80.33 74.47 80.04 79.70 72.68 75.79 79.65 81.29 81.98 60.38 55.29 61.17 60.82 61.40 62.17 60.86 66.55 60.37 66.93 65.15 64.81 66. 70.59 63.38 68.41 69.74 60.20 65.76 68.68 69.63 72.24 50.08 48.07 48.71 46.64 50.09 46.99 48.81 51.24 51.32 53.31 50.50 51.43 53.33 56.64 55.18 58.98 59.72 51.21 53.92 56.46 60.73 62. 52.41 50.11 52.87 51.05 54.31 51.50 52.27 56.52 54.37 57.71 56.36 55.65 56.86 60.73 59.59 61.41 63.91 55.06 56.01 61.28 64.73 66.53 Table 11: Results of control experiment where each IR models is evaluated from the toolset of each integrated dataset in w/o inst. setting. multiple potential target tools for each query, and (2) significantly lower term overlap between input queries and relevant tools compared to conventional IR scenarios. These characteristics collectively contribute to more complex retrieval environment that requires advanced reasoning and understanding capabilities from retrieval models. D.3 Results of controlled experiment Since TOOLRET integrates multiple datasets, we also conduct controlled experiments where IR models retrieve tools exclusively within the toolset of each individual dataset instead of the overall tool corpus. Table 11 presents the results under the setting that the IR models only take the query to retrieve, i.e., the w/o inst setting. Table 10 presents the results under the setting that the IR models take the query and additional instruction to retrieve, i.e., the w/ inst setting. D.4 Results of in-subset retrieval TOOLRET contains three subsets, including TOOLRET-web, TOOLRET-code and TOOLRET-customized. The tool in each subset diverges by its documentation format, domain, and functionality. For comprehensive evaluation, we also conduct an in-subset retrieval experiment, where IR models retrieve tools exclusively within the toolset of each subset instead of the overall tool corpus. Table 13 presents the results under the setting that the IR models only take the query to retrieve, i.e., the w/o inst setting. Table 12 presents the results under the setting that the IR models take the query and additional instruction to retrieve, i.e., the w/ inst setting. D.5 Results of trained IR mdels Experimental results on TOOLRET reveal that even IR models with strong performance on conventional IR benchmarks such as MTEB and BEIR struggle significantly in tool retrieval tasks. key factor contributing to this performance degradation is the lack of large-scale training dataset specifically tailored for tool retrieval. To address this gap, we introduce TOOLRET-train, diverse training dataset comprising more than 200k tool retrieval tasks. Each example in TOOLRET-train consists of an input query, an instruction generated using our target-aware strategy, the corresponding target tools, and set of Model TOOLRET-Web TOOLRET-Code TOOLRET-Customized Average N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@ 49.01 36.58 43.80 35.78 37.45 42.14 Sparse and dense models bm25 COLT Colbert contriever-msmarco gtr-t5-base gtr-t5-large Embedding models all-MiniLM-L6-v2 e5-small-v2 e5-base-v2 e5-large-v2 gte-base-en-v1.5 gte-large-en-v1.5 bge-base-en-v1.5 bge-large-en-v1.5 36.93 38.22 40.69 40.14 48.25 40.48 43.74 44.07 gte-Qwen2-1.5B-inst. 47.29 e5-mistral-7b 48.76 GritLM-7B 53.69 NV-Embed-v1 51.95 Cross-encoder re-ranking models mxbai-rerank-large-v1 33.39 monot5-base-msmarco 29.95 bge-reranker-v2-m3 56.49 jina-reranker-v2-base 35.24 bge-reranker-v2-gemma 62.84 7.17 5.74 6.40 5.31 5.50 5.98 5.65 5.55 6.51 5.87 7.16 6.46 6.43 6.44 7.10 7.16 8.13 7. 5.09 5.20 8.42 5.48 8.87 64.96 50.84 58.02 47.17 48.78 53.59 49.54 49.34 55.38 52.23 61.96 56.34 57.33 56.78 61.89 63.57 68.70 66.58 46.75 42.47 72.26 47.70 76.24 63.68 49.04 56.28 46.07 47.44 52. 47.92 48.07 54.01 50.80 59.17 54.52 55.85 55.22 59.98 61.40 67.26 64.21 45.11 40.36 70.67 46.31 74.86 28.92 21.98 16.60 25.19 22.54 26.60 15.89 28.97 28.43 26.88 33.28 30.58 29.83 33.88 39.30 33.06 41.59 34. 24.90 30.20 38.09 36.23 37.13 6.78 5.12 3.05 5.67 4.93 5.71 3.89 6.81 6.59 6.16 7.57 7.00 6.96 7.90 9.90 8.14 10.06 8.40 5.95 7.84 9.25 9.21 8.62 37.09 29.68 20.85 31.95 29.64 33. 22.68 36.96 37.14 35.65 41.99 38.79 38.86 43.11 48.58 43.56 51.69 43.66 32.20 37.94 48.14 45.59 47.23 24.44 20.03 14.95 20.56 20.60 22.38 15.24 23.45 23.67 24.31 27.20 24.78 25.67 28.62 29.51 28.49 33.87 29. 19.52 21.43 33.48 29.26 33.45 51.28 46.02 31.18 44.37 51.02 53.95 43.09 47.59 47.89 51.40 50.33 49.24 52.41 53.48 55.56 57.16 60.14 57.93 35.62 46.99 54.66 54.12 64.33 10.66 9.12 5.86 9.35 10.28 11. 9.29 9.78 9.48 10.65 9.70 9.98 10.75 10.53 11.67 11.43 11.63 12.34 7.35 9.26 12.46 11.92 13.72 60.70 58.02 39.40 57.53 61.08 66.08 56.84 58.19 59.01 61.45 62.05 59.13 63.84 63.66 65.55 67.28 68.93 71. 43.96 57.10 70.79 65.09 77.37 48.40 45.27 32.10 46.80 49.06 52.21 43.96 45.31 46.91 48.28 50.09 47.20 51.19 52.00 51.75 52.62 54.60 57.47 34.14 46.48 56.17 51.18 62.08 43.07 34.86 30.53 35.11 37.00 40. 31.97 38.26 39.00 39.47 43.95 40.10 41.99 43.81 47.38 46.32 51.81 48.10 31.30 35.71 49.75 41.86 54.76 45.51 38.12 34.44 37.81 39.03 42.26 35.71 38.94 41.53 41.13 45.49 42.17 44.24 45.28 47.08 47.51 51.91 50. 32.92 36.09 53.44 42.25 56.79 Table 12: Experiments are conducted under the w/ inst. setting, with retrieval performed within each subset individually. negative tools. IR models are trained to distinguish target tools from negative tools ( 7). We evaluate these trained IR models on TOOLRET and present the results in Table 14. D.6 Improved IR enhances tool-use LLMs We further investigate the impact of improved IR models on the end-to-end performance of tool-use LLMs. Specifically, we evaluate tool-use LLMs on the ToolBench (Qin et al., 2023) dataset using the official Pass Rate metric, which measures whether the model successfully invokes the correct tools to complete given task. For each task in ToolBench, we replace the pre-annotated toolset (oracle) with tools retrieved by IR models from TOOLRET tool corpus, which contains 43,000 tools. Since TOOLRET integrates the ToolBench dataset, we can compute NDCG@10 for this retrieval step. For comprehensive evaluation, we assess two widely used tool-use LLMs, including GPT-3.5 and ToolLLaMA (Qin et al., 2023). Table 15 presents the retrieval NDCG@10 scores alongside the corresponding pass rates on ToolBench.8 Our results demonstrate that LLM agents equipped with improved IR models achieve substantial gains in pass rate, highlighting the critical role of accurate tool retrieval in downstream task performance. Furthermore, Figure 6 visually illustrates positive correlation between improved IR performance and higher task pass rate, suggesting that better retrieval directly leads to improved downstream outcomes. Based on this analysis, we propose that future work could explore the following two directions: (i) Further optimize IR models to enhance tool retrieval performance; or (ii) Adapt IR models by incorporating feedback from end-to-end task performance, allowing them to better support tool-use LLMs. These approaches provide more efficient plug-and-play solution compared to fine-tuning LLMs, enabling flexible integration into diverse tool-use systems. 8ToolBench consists of three subsets: ToolBench-G1, ToolBench-G2, and ToolBench-G3. Model TOOLRET-Web TOOLRET-Code TOOLRET-Customized Average N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@10 26.47 22.23 23.58 19.11 21.19 24.48 Sparse and dense models bm25 COLT Colbert contriever-msmarco gtr-t5-base gtr-t5-large Embedding models all-MiniLM-L6-v2 e5-small-v2 e5-base-v2 e5-large-v2 gte-base-en-v1.5 gte-large-en-v1.5 bge-base-en-v1.5 bge-large-en-v1. 18.07 20.10 20.96 22.93 24.50 23.00 23.44 23.12 gte-Qwen2-1.5B-inst. 28.99 e5-mistral-7b 25.38 GritLM-7B 29.67 NV-Embed-v1 35.50 Cross-encoder re-ranking models mxbai-rerank-large-v1 31.75 monot5-base-msmarco 26.91 bge-reranker-v2-m3 31.25 jina-reranker-v2-base 33.31 bge-reranker-v2-gemma 38.59 4.04 3.75 3.84 3.16 3.48 3.91 3.10 3.14 3.54 3.49 3.98 3.90 3.86 3.76 4.57 4.13 4.82 5.54 4.79 4.32 4.92 5.06 5.67 34.51 31.66 32.64 26.52 29.05 33. 25.13 26.47 29.43 29.53 33.85 33.07 32.79 31.93 39.58 34.81 41.30 48.36 43.82 37.16 42.97 44.20 50.14 33.22 30.15 31.19 24.97 27.90 32.44 23.75 25.12 28.23 28.09 32.48 31.93 31.47 30.67 38.16 33.47 39.77 46. 42.20 35.03 41.34 42.93 48.67 20.61 21.65 23.50 21.84 18.18 23.49 13.71 21.02 21.25 20.16 24.32 23.84 24.17 27.14 31.85 28.22 30.86 33.08 24.84 30.43 34.04 36.79 38.08 5.06 5.36 5.70 5.93 4.45 5. 3.49 5.33 5.43 5.19 6.46 6.19 6.37 7.24 8.37 7.60 8.18 8.77 5.96 8.11 8.86 9.49 10.05 26.35 29.15 28.82 27.83 25.09 30.79 18.73 27.70 27.59 27.26 33.01 31.38 31.80 35.77 39.95 35.99 39.99 41. 32.13 38.04 42.85 46.20 47.65 15.87 19.12 16.03 15.19 16.32 19.31 11.78 16.88 16.42 16.68 19.91 19.15 18.68 21.16 22.69 22.28 25.25 24.83 19.53 20.45 26.46 28.55 28.98 38.48 36.12 33.71 35.01 35.95 38. 31.61 32.58 32.90 39.45 37.86 35.34 36.59 35.51 45.47 42.31 48.92 51.28 35.65 46.32 43.81 53.18 50.39 8.51 7.99 6.64 7.57 8.10 8.89 7.12 7.35 7.33 8.81 8.10 8.24 8.47 7.82 10.02 9.03 10.31 11. 7.31 9.15 10.38 11.56 11.54 48.24 47.63 40.85 44.07 45.77 49.55 40.66 39.85 43.10 49.07 49.44 45.43 47.44 45.71 55.68 51.34 59.01 61.49 43.38 56.53 53.28 63.22 61.69 37.09 37.64 32.01 34.71 36.50 38. 30.03 30.33 33.51 38.19 38.94 35.28 36.87 36.39 43.85 40.24 46.55 48.05 33.43 46.36 41.84 50.70 49.48 28.52 26.67 26.93 25.32 25.11 28.95 21.13 24.57 25.04 27.51 28.89 27.39 28.07 28.59 35.44 31.97 36.48 39. 30.75 34.56 36.36 41.09 42.36 28.73 28.97 26.41 24.96 26.91 30.13 21.85 24.11 26.05 27.65 30.44 28.79 29.01 29.40 34.90 32.00 37.19 39.87 31.72 33.95 36.54 40.72 42.38 Table 13: Experiments are conducted under the w/ inst. setting, with retrieval performed within each subset individually."
        },
        {
            "title": "Model",
            "content": "TOOLRET-Web TOOLRET-Code TOOLRET-Customized Avg. N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@10 bge-large-en-v1.5 25.07 bge-large-en-v1.5 23.41 bge-large-en-v1.5 18. bge-base-en-v1.5 bge-base-en-v1.5 bge-base-en-v1.5 e5-large-v2 e5-large-v2 e5-large-v2 e5-base-v2 e5-base-v2 e5-base-v2 20.35 19.05 17.76 23.15 21.33 17.03 19.97 15.44 14. 3.94 3.22 3.13 3.41 2.98 2.91 3.74 2.94 2.67 3.33 2.78 2.46 33.52 32.02 25.80 28.19 25.13 23. 31.41 28.34 21.77 27.67 25.37 19.18 31.85 31.97 24.50 26.77 23.70 22.20 29.94 25.13 20.63 26.36 23.61 18. 32.05 30.74 24.49 30.69 24.90 22.44 33.05 30.45 18.94 26.45 24.50 19.80 7.98 6.31 6.67 7.70 6.41 6. 7.79 6.45 4.90 5.92 5.20 5.04 41.03 36.03 32.95 39.16 35.44 29.96 40.42 38.20 25.95 32.71 30.12 25. 26.11 24.31 19.30 24.95 22.50 17.29 26.97 26.70 16.26 22.24 19.37 15.37 41.72 37.31 25.72 37.01 33.51 25. 34.33 30.13 26.37 31.03 28.03 22.69 8.80 7.30 5.54 8.53 6.40 5.71 6.60 5.60 6.07 6.06 5.47 5. 49.26 46.31 32.18 47.21 44.20 32.17 42.08 39.82 32.19 38.42 40.21 29.13 37.99 34.21 24.79 36.40 35.71 24. 34.26 32.33 23.17 30.92 31.92 22.25 32.95 30.49 23.03 29.35 25.82 22.06 30.18 27.30 20.78 25.81 22.66 18. 31.98 30.16 22.86 29.37 27.30 21.25 30.39 28.05 20.02 26.51 24.97 18.54 Table 14: Experimental results of IR models before and after training on our datasets. Models trained with the concatenation of instruction and query are denoted by . In contrast, the variants trained solely on the query as input are marked with (See the ablation study in 7 for details)."
        },
        {
            "title": "TOOLRET",
            "content": "ToolBench-G1 ToolBench-G2 ToolBench-G3 NDCG@10 NDCG@"
        },
        {
            "title": "Pass Rate",
            "content": "NDCG@"
        },
        {
            "title": "Pass Rate",
            "content": "NDCG@"
        },
        {
            "title": "Pass Rate",
            "content": "gpt-3.5-turbo as tool-use LLM oracle - - 62.00 - 57.20 - 67.40 23.03 bge-large-en-v1.5 bge-large-en-v1.5 32.9543.07% 71.11 107.38% 59.5017.59% 18.1191.03% 58.4019.18% 67.87128.60% 59.204.04% bge-base-en-v1.5 bge-base-en-v1.5 50.60 56.6011.86% 16.0172.52% 59.6016.41% 60.7583.98% 22.06 29.3533.05% 67.5283.03% 57.70 60.805.37% 56.90 34.29 29.69 50. 49.00 36.89 33.02 51.20 9.48 9. e5-large-v2 e5-large-v2 20.78 30.1845.24% 70.0856.05% 44.91 47.50 57.0020.0% 11.57 17.7153.07% 62.109.91% 56. 43.43 66.0952.18% 55.70 58.003.99% e5-base-v2 e5-base-v2 ToolLlama as tool-use LLM 18.97 25.8136.06% 65.7970.18% 38.66 49.60 56.9014.72% 17.4576.80% 60.8012.38% 62.7467.98% 37.35 54.10 9.87 54.20 62.4015.13% oracle - - 53.6 - 50.8 - 49. bge-large-en-v1.5 bge-large-en-v1.5 23.03 32.9543.07% 71.11 107.38% 45.1019.95% 18.1191.03% 47.3014.53% 67.87128.60% 39.606.45% 37.20 34.29 29.69 37. 41.30 9.48 bge-base-en-v1.5 bge-base-en-v1.5 22.06 29.3533.05% 67.5283.03% 36.89 e5-large-v2 e5-large-v2 e5-base-v2 e5-base-v2 20.78 30.1845.24% 70.0856.05% 44.91 18.97 25.8136.06% 65.7970.18% 38.66 47.80 50.605.86% 41.50 44.507.23% 9.28 16.0172.52% 49.808.03% 46.10 11.57 17.7153.07% 49.804.72% 46.60 42.20 49.1016.35% 17.4576.80% 48.306.86% 45.20 9.87 33.02 60.7583.98% 43.43 66.0952.18% 37.35 62.7467.98% 36.10 45.7026.60% 40.20 43.804.50% 42.00 44.706.60% Table 15: Experiment results of IR models before and after training. We also show the end-to-end task pass rate of tool-use LLMs when equipped with the tools retrieved by the IR models on ToolBench dataset."
        }
    ],
    "affiliations": [
        "Baidu Inc., Beijing, China",
        "Leiden University, Leiden, The Netherlands",
        "Shandong University, Qingdao, China"
    ]
}