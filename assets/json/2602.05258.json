{
    "paper_title": "CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs",
    "authors": [
        "Haoran Li",
        "Sucheng Ren",
        "Alan Yuille",
        "Feng Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE."
        },
        {
            "title": "Start",
            "content": "CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs Haoran Li 1 2 Sucheng Ren 2 Alan Yuille 2 Feng Wang 2 6 2 0 2 5 ] . [ 1 8 5 2 5 0 . 2 0 6 2 : r Abstract Rotary Positional Embedding (RoPE) is key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE. 1. Introduction Long context Large Language Models (LLMs) have become cornerstone of critical domains such as coding agents (Jimenez et al., 2024; Anthropic, 2025), agentic memory (Yu et al., 2025; Chhikara et al., 2025), and long-horizon reasoning (Qiao et al., 2025; Zhou et al., 2025; Sinha et al., 2025). To achieve context scaling, long context training stage is often required after initial pre-training, where the frequencies within Rotary Positional Embedding (RoPE) (Su et al., 2024) are modified to fit the target context length, followed by continued training on long sequences. 1Carnegie Mellon University 2Johns Hopkins University. Correspondence to: Haoran Li <haoranl4@cs.cmu.edu>, Feng Wang <fwang60@jh.edu>. Preprint. February 6, 2026. Figure 1. Performance comparison between CoPE and RoPE. With simple soft clipping strategy, CoPE effectively improves RoPEs performance both within the training range and during extrapolation. The training context length here is 64k. While existing works have proposed various methods to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: OOD mitigation and semantic modeling. Specifically, RoPE divides the query and key vectors into two-dimensional chunks, and rotates each chunk at specific frequency. For low-frequency components that do not complete full rotation during pre-training, extrapolating to unseen positions leads to severe OOD issues. Therefore, several OOD mitigation strategies, including Position Interpolation (PI) (Chen et al., 2023), NTK (bloc97, 2023), YaRN (Peng et al., 2024), and LongRoPE (Ding et al., 2024; Shang et al., 2025), are introduced to scale the frequencies so that extended contexts are mapped back to the original position range. In contrast, another line of research is inspired by semantic modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. Men et al. (2024) show that the rotation matrix in attention would degrade the models ability to discriminate relevant tokens from irrelevant ones as the relative distance increases, motivating the use of higher base frequency. The ABF technique (Xiong et al., 2024) arrives at the same strategy, claiming that increasing the base frequency mitigates the long-term decay in RoPE and improves long context modeling. Despite their improved performance, these two lines of research are typically treated as tackling distinct aspects of long context modeling. However, we argue that they stem CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs from the same underlying issue: the suboptimal behavior of low-frequency components in the extrapolation regime. Through theoretical analysis of RoPEs frequency spectrum, we show that low-frequency components simultaneously govern OOD behavior under extrapolation and the stability of semantic attention over long contexts. Motivated by this insight, we propose CoPE, minimalist intervention that softly clips the low-frequency components of RoPE. This simple and effective strategy not only suppresses OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping, providing plug-andplay solution that can be seamlessly integrated into existing LLMs for better long context capability. To validate the effectiveness and compatibility of CoPE, we conduct extensive experiments that align with the long context recipe used in Qwen3 (Yang et al., 2025), i.e., employing the ABF technique (Xiong et al., 2024) during longcontext training and YaRN for test-time extrapolation. By simply replacing the standard RoPE with CoPE while keeping all other configurations unchanged, we observe consistent and significant improvements across diverse tasks and context lengths, as shown in Figure 1. Notably, at context lengths up to 256k tokens, CoPE achieves nearly 2 the performance of RoPE, while also maintaining superior performance within the training range. Together, our theoretical analysis and empirical results establish CoPE as simple, general, and highly effective drop-in replacement for RoPE in long context LLMs. Our main contributions can be summarized as follows: We provide unified perspective on long context adaptations of RoPE, showing that both OOD mitigation and semantic modeling methods originate from the suboptimal behavior of low-frequency components in the extrapolation regime. Based on this insight, we propose CoPE, minimalist and principled modification to RoPE that softly attenuates low-frequency components, eliminating OOD outliers, refining semantic signals, and preventing the spectral leakage induced by hard clipping. We conduct extensive experiments to demonstrate that CoPE is simple and scalable drop-in replacement for RoPE, consistently improving performance across diverse tasks and context lengths up to 256k. 2. Preliminaries Rotary Position Embedding (RoPE). Transformer-based models (Vaswani et al., 2017) rely on Positional Encodings (PEs) to explicitly incorporate sequential information. Among various PEs, Rotary Position Embedding (RoPE) (Su et al., 2024) has become the dominant choice in modern LLMs. Let xi Rd denote the d-dimensional token ; q(1) ; . . . ; q(d/21) embedding of the i-th token in sequence. Consider the n-th query vector qn and the m-th key vector km, RoPE partitions the dimensions into d/2 chunks, e.g., qn = [q(0) ]. Each chunk is assigned unique rotation frequency θi = b2i/d, {0, 1, . . . , d/2 1}, where is pre-defined base frequency (typically set to 10, 000). The rotation is achieved through rotation matrix Rn Rdd, which can be formulated as follows: cos(nθ0) sin(nθ0) cos(nθ0) sin(nθ0) . . . . . . 0 0 0 0 . . . 0 0 . . . 0 0 . . . . cos(nθd/21) sin(nθd/21) cos(nθd/21) sin(nθd/21) (1) With this block-diagonal rotation matrix, the attention score1 between qn and km is computed as: An,m = (Rnqn)(Rmkm) = Rmnkm, (2) where (m n) is the relative distance between qn and km. 3. Analysis In this section, we conduct comprehensive theoretical analysis of existing methods that adapt RoPE to longer contexts. We begin by highlighting the underlying guiding principles of prior methods, namely OOD mitigation and semantic modeling. We then show that these two seemingly distinct objectives both originate from the same root cause: the suboptimal behavior of low-frequency components in the extrapolation regime. 3.1. RoPE OOD Theory Background. Recall that RoPE divides the query and key vectors into 2-dimensional chunks and rotates each chunk at frequency of θi = b2i/d, {0, 1, . . . , d/2 1}, where is the base frequency and is usually set to 10, 000. Given the periodicity of sinusoidal functions, we know that for each chunk with frequency θi, the corresponding period can be calculated as follows: Ti = 2π θi . (3) Since θi decreases as the dimensional index increases, the low-frequency components in higher dimensions possess longer periods, potentially exceeding the pre-training context window. For example, the pre-training context window of Llama-3-8B (Grattafiori et al., 2024) is 8192, while the period of the 35-th chunk already slightly exceeds this length. Consequently, out of the 64 chunks, the last 29 lowfrequency chunks fail to experience single complete period during the pre-training stage, leading to severe OOD issues 1Here, we omit the softmax function and 1/ scaling in standard Transformer (Vaswani et al., 2017) for simplicity. 2 CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs (a) RoPE Frequencies and OOD Issue. (b) Spectral Comparison. Figure 2. (a) Visualization of RoPE frequencies. Low-frequency components in higher dimensions possess longer periods. The region shaded in red marks where the period exceeds the pre-training context window, leading to OOD extrapolation. (b) Spectral comparison. Unlike RoPE which keeps unstable low frequencies (blue), or Hard Clipping which causes an abrupt cut-off and spectral leakage, CoPE implements soft decay strategy starting from the clipping onset, simultaneously eliminating OOD outliers and refines semantic signals. during extrapolation. In contrast, high-frequency components in lower dimensions complete multiple cycles during pre-training and remain well-behaved even in extrapolation. Critical Dimension in Extrapolation. Based on the above spectrum analysis and prior work (Liu et al., 2024; Shang et al., 2025), we formally define the critical dimension in RoPE-based extrapolation as follows: Definition 3.1 (Critical Dimensions in Extrapolation). For LLMs pre-trained with context window Lpre, attention head dimension d, and base frequency b, only the first dct dimensions perceive complete periodic patterns during pre-training: dct = 2 logb Lpre 2π . (4) As shown in Figure 2, for Llama-3-8B with Lpre = 8192, = 128, = 500, 000, the critical dimension is 70, which corresponds to the 35-th rotation chunk as discussed earlier. OOD Mitigation Methods. To mitigate the OOD behavior of RoPE beyond the critical dimension, several methods have been proposed to scale the frequencies θi so that extended contexts are mapped back to the original position range. For ease of notation, we denote the target context length as Lt and the scaling factor for each frequency θi as si. Given the scaling factor, the scaled frequency can be calculated as: θ = θi si = 1 si b2i/d . (5) Representative works include PI (Chen et al., 2023), NTK (bloc97, 2023), YaRN (Peng et al., 2024), and LongRoPE (Ding et al., 2024; Shang et al., 2025). PI applies uniform scaling factor across all RoPE frequencies, i.e., si = Lt . Lpre While easy to implement, this approach equally stretches all dimensions without considering the distinct behaviors of highand low-frequency components of RoPE during extrapolation. As result, it compresses high-frequency components, leading to loss of local positional resolution. Inspired by the Neural Tangent Kernel (NTK) theory (Tancik et al., 2020), which states that neural networks have difficulties learning high-frequency features, NTK proposes to scale high frequencies less and low frequencies more with the scaling factor si = ( Lt )2i/(d2), effectively alLpre leviating the loss of high-frequency information. Building on NTK, YaRN further partitions the frequencies into three groups and applies the following strategy: no scaling for high-frequency components (si = 1), PI-style scaling for low-frequency components (si = Lt ), and linear interLpre polation between 1 and Lt for intermediate frequencies. Lpre LongRoPE adopts perplexity-guided search-based method to estimate the optimal scaling factor si for each frequency. Takeaway 1. OOD mitigation methods address extrapolation by interpolating low-frequency components, while minimally perturbing high frequencies. The primary distinction among these methods lies in their choice of per-frequency scaling factors. 3.2. RoPE Semantic Modeling Background. When RoPE was originally proposed (Su et al., 2024), it introduced an important inductive bias known as long-term decay: the upper bound of the attention score CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs influence attention patterns, concluding that low-frequency components primarily carry semantic information (Barbero et al., 2025; Jin et al., 2025), as they are the most invariant to token relative distance. Takeaway 2. RoPE secretly induces long-term decay of semantic attention that is primarily governed by the low-frequency components, revealing them as an unreliable semantic channel. 3.3. All Roads Lead to Low-Frequency Components Our analysis above reveals unifying insight: both OOD extrapolation and long-term decay of semantic attention stem from the same root cause: the suboptimal behavior of low-frequency components in the extrapolation regime. Specifically, from the OOD perspective, low-frequency components possess periods exceeding the pre-training context window, resulting in OOD extrapolation. Meanwhile, from the semantic modeling perspective, low frequencies serve as the semantic channel that distinguishes similar tokens from random ones, yet this ability decays as context length increases. Our unified perspective suggests simple yet effective design principle: stabilizing the behavior of lowfrequency components is sufficient to mitigate OOD extrapolation and preserve long-range semantic attention. Takeaway 3. OOD extrapolation and long-term semantic decay are two manifestations of the same underlying issue: the suboptimal behavior of low-frequency components beyond the pre-training regime. 4. CoPE: Clipped Rotary Position Embedding Motivated by our analysis in Section 3, we propose Clipped Rotary Position Embedding (CoPE), simple yet effective method that softly clips the low-frequency components of RoPE, as illustrated in Figure 2b. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents severe spectrum leakage induced by hard clipping, thereby scaling favorably with increased context window. 4.1. Spectral Analysis To stabilize low-frequency components, straightforward approach is to directly set them to zero, i.e., hard clipping. For example, Babero et al. (2025) identify the low frequencies as the semantic channel and propose to stabilize them by clipping the lowest 25% or 75% frequencies, resulting in lower validation perplexity on 2B-scale model with 8k context length. However, hard clipping introduces an abrupt spectral cutoff, which can distort the remaining frequency components and undermine the stability of positional information, particularly in long-context scenarios. To elaboFigure 3. Long-term decay of semantic attention. As relative distance increases, the models ability to prefer semantically similar tokens over random ones diminishes. Applying soft clipping to the low-frequency components (CoPE) effectively alleviates this decay, preserving semantic information over long contexts. between two tokens decreases as their relative distance increases. This property encourages each token to attend more to its neighbors. However, Men et al. (2024) observe that an undesirable decay property also exists: the ability to attend more to semantically similar tokens than random tokens also decays as the relative distance increases. Following Men et al. (2024), we denote this property as long-term decay of semantic attention and formalize it as follows: Theorem 3.2 (Long-term Decay of Semantic Attention). Assume query Rd and key Rd have distance and i.i.d. components with standard deviation σ. Let = + ϵ denote similar key to the query, where ϵ is zero-mean perturbation. Then, we have: Eq,k,ϵ[qRtk qRtk] = 2σ2 d/21 (cid:88) i=0 cos(tθi), (6) where (cid:80)d/21 i=0 cos(tθi) decays as increases. The proof is provided in Appendix A.1. Note that the term (cid:80)d/21 cos(tθi) should ideally be greater than zero to i=0 ensure more attention is paid to similar tokens than random ones. However, this term does decrease as increases, as shown in Figure 3. Given this observation, Men et al. (2024) propose to use higher base frequency b, which in turn decreases θi = b2i/d and alleviates this undesirable decay. Similarly, the ABF technique (Xiong et al., 2024) arrives at the same higher base frequency strategy, claiming that increasing the base frequency reduces the general long-term decay of RoPE and improves long context modeling. Given its simplicity and effectiveness, the higher base frequency strategy has been widely adopted in long context training (Grattafiori et al., 2024; Yang et al., 2025). More recently, several work has analyzed how different RoPE frequencies 4 CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs is direct consequence of the sharp spectral discontinuity introduced by hard clipping. As result, the attention scores exhibit Gibbs ringing, where oscillatory artifacts disrupt the general monotonicity of decay and cause spurious longrange correlations, as illustrated in Figure 4. 4.2. Soft Clipping Strategy To address the above challenge, our CoPE introduces soft clipping strategy, which applies smooth spectral taper (e.g., cosine window) to the low frequencies. By Fourier duality, this soft clipping yields rapidly decaying kernel in the time domain, suppressing unstable low-frequency components without inducing long-range spurious correlations. Specifically, instead of applying binary mask 1θ>θc, we assign scalar weight wj [0, 1] to each frequency component θj. To minimize spectral discontinuity, we employ cosine-decay taper. The weights wj are defined as function of the frequency θj: w(θj) = (cid:40)1, 1 2 (cid:104) 1 + cos (cid:16) π θstartθj θstartθmin (cid:17)(cid:105) , θj θstart θmin θj < θstart , (10) where θstart denotes the clipping onset and θmin is the lowest frequency. This strategy is highly practical as it allows for seamless integration into modern LLM frameworks. By simply modifying the initialization of the RoPE frequency, CoPE can be applied as drop-in replacement without altering the model architecture. This ensures full compatibility with optimized inference kernels, such as FlashAttention (Dao, 2024), while maintaining standard inference speeds. 5. Experiment In this section, we evaluate CoPE across various benchmarks to answer the following questions: (1) Does CoPE consistently outperform RoPE and the hard clipping strategy on real-world long context tasks? (2) Are synthetic benchmarks reliable proxies for real-world performance? (3) Can CoPE retain performance on short context benchmarks that assess general model capabilities? (4) How does the choice of clipping onset affect performance? 5.1. Experimental Setups Evaluation Benchmarks. For long context evaluation, we primarily utilize the HELMET benchmark (Yen et al., 2025), which improves upon purely synthetic benchmarks (e.g., RULER (Hsieh et al., 2024)) and benchmarks with limited real-world tasks (e.g., InfiniteBench (Zhang et al., 2024)), providing more robust and realistic assessment. HELMET includes both synthetic recall and diverse set of real-world tasks, including retrieval-augmented generFigure 4. Ringing artifacts caused by hard clipping. Directly applying hard clipping to the low-frequency components introduces an abrupt spectral cutoff, which causes spectral leakage and manifests as long-range oscillatory ringing in the attention signal (Gibbs phenomenon). rate, we first reframe the attention mechanism with RoPE through the lens of Non-Uniform Discrete Fourier Transform (NUDFT). As shown in Equation 2, the dot-product attention between the n-th query vector qn and the m-th key vector kn is calculated as: An,m = (Rnqn)(Rmkm) = n Rmnkm, (7) which can be further transformed into A(τ ) = Re d/21 (cid:88) j=0 (q(j) k(j) )eiθj τ = d/21 (cid:88) j=0 Aj cos(θjτ ), (8) where τ = denotes the relative distance. This formulation reveals that the attention score computed with RoPE achieves an inverse NUDFT with frequency components θj = b2j/d, [0, d/2). Now, we analyze the impact of hard clipping using continuous approximation of A(τ ) in the large-d limit, which provides clearer theoretical insight. Theorem 4.1 (Spectral Leakage from Hard Clipping). Let A(τ ) be the continuous attention score. Hard highpass filter at cutoff θc yields A(τ ) = A(τ ) + E(τ ), where the error term is: E(τ ) = A(τ ) (cid:18) θc π sinc (cid:18) θcτ π (cid:19)(cid:19) . (9) The slow O(1/τ ) decay of the sinc kernel introduces Gibbs oscillations, disrupting the general decay of A(τ ) and inducing spurious long-range correlations. The proof is provided in Appendix A.2. Theorem 4.1 shows that the slowly decaying O(1/τ ) envelope of the sinc kernel 5 CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs Table 1. Main results on HELMET benchmark across diverse real-world tasks. Models are trained with 64k context length and evaluated up to 256k to assess length generalization. CoPE consistently outperforms RoPE and hard clipping, with performance gains scaling favorably with context length. The best results are bold, while indicates unavailable benchmark data at that context length. Task Method Context Length Summ. QA ICL Recall RAG Average RoPE HardClip CoPE RoPE HardClip CoPE RoPE HardClip CoPE RoPE HardClip CoPE RoPE HardClip CoPE RoPE HardClip CoPE 8k 29.18 25.68 29. 6.46 7.44 13.10 74.60 73.10 79.40 99.75 99.75 99.63 68.38 68.06 68.67 55.74 54.81 58.11 16k 28.46 25.70 30.81 8.39 9.28 16.89 80.20 77.00 83.70 99.13 98.50 98.88 67.44 67.50 67.72 56.86 55.60 59. 32k 21.76 18.55 32.78 8.52 10.16 21.02 83.40 79.80 85.50 98.13 98.50 99.00 66.67 66.11 67. 55.70 54.62 61.23 64k 11.10 6.93 30.88 7.67 10.31 15.07 85.50 82.20 86.40 97.63 94.38 97. 62.78 59.72 63.17 52.94 50.71 58.68 128k 6.31 9.33 27.89 8.21 9.31 18.23 82.10 77.30 84. 71.38 82.13 76.00 53.44 62.05 56.78 44.29 48.02 52.72 256k 9.06 8.60 32.37 7.93 9.24 19. 26.13 36.86 34.00 14.37 18.23 28.48 ation (RAG), many-shot in-context learning (ICL), longdocument QA, and summarization. We also report results on synthetic tasks from RULER and InfiniteBench. For standard short context benchmarks, we adopt MMLU (Hendrycks et al., 2021), MMLU-Pro (Wang et al., 2024), GPQA (Rein et al., 2024), BIG-Bench Hard (Suzgun et al., 2022), and GSM8K (Cobbe et al., 2021). For more detailed benchmark descriptions, please refer to Appendix B.1. Long Context Training Stage. We employ Llama-3-8B (Grattafiori et al., 2024) as the backbone model, which is pre-trained with an 8k context window. We extend the models context length to 64k via continued pre-training on ProLong data (20B tokens) (Gao et al., 2025), followed by SFT on UltraChat (1B tokens) (Ding et al., 2023). Following Qwen3 and ProLong (Yang et al., 2025; Gao et al., 2025), we increase the base frequency from 5 105 to 1 107 using the ABF technique (Xiong et al., 2024). Baselines. We compare CoPE with the widely-used RoPE (Su et al., 2024) and hard clipping strategy that directly sets some low frequencies to zero (Barbero et al., 2025). Implementation Details. For both continued pre-training and SFT, we adopt batch size of 256 (16M tokens) and the AdamW optimizer (Loshchilov & Hutter, 2017) with weight decay of 0.1 and (β1, β2) = (0.9, 0.95). Both stages are trained for one epoch, differing only in their learning rate schedules. Specifically, continued pre-training uses an initial learning rate of 1 105 with 10% warmup and cosine decay to 1 106, while SFT uses an initial learning rate of 2 105 with 5% warmup and cosine decay to 2 106. The clipping onset is set to 44 (64 frequencies in total). For evaluations beyond 64k, we leverage YaRN (Peng et al., 2024) with scaling factor of 4. The training process takes approximately 1996 and 48 GPU hours on machines equipped with H100-80GB GPUs, respectively. 5.2. Main Results We evaluate CoPE across diverse set of tasks, covering synthetic recall, RAG, ICL, QA, and summarization. The results are detailed in Table 1. Performance on HELMET. As shown in Table 1, CoPE consistently outperforms RoPE and HardClip across nearly all tasks and context lengths. Within the training range (64k), CoPE yields an average improvement of 10.84% over RoPE, indicating that soft clipping does not compromise in-distribution performance. When extrapolated to 256k context, CoPE achieves approximately 2 the performance of RoPE, demonstrating superior length generalization ability. In contrast, although the hard clipping strategy slightly improves performance at extreme context lengths (128k256k), it exhibits noticeable degradation within the training 6 CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs Table 2. Performance comparison on synthetic tasks sampled from InfiniteBench and RULER, which provide limited insights into real-world performance. Task RULER NIAH RULER MK InfBench KV InfBench Math Find Method RoPE HardClip CoPE RoPE HardClip CoPE RoPE HardClip CoPE RoPE HardClip CoPE 8k 100.0 100.0 100.0 100.0 100.0 100.0 6.20 6.20 6.20 37.14 34.86 35. Context Length 16k 100.0 100.0 100.0 100.0 100.0 100.0 12.80 12.80 12.80 35.00 35.67 35. 32k 98.67 97.00 98.67 99.00 98.00 98.67 24.60 24.80 25.40 36.86 35.71 36.00 64k 99.67 99.33 99.67 96.00 89.33 99.67 24.20 11.20 31.40 33.42 26.29 34.00 128k 91.00 89.33 94. 54.33 63.33 59.67 16.40 21.40 19.40 35.14 34.57 35.14 256k 60.50 57.00 78.50 27.00 17.67 32. 16.40 21.40 19.40 35.14 34.57 35.14 range (8k-64k). This behavior empirically validates our theoretical analysis in Theorem 4.1, which highlights that abrupt hard truncation would cause spectral leakage and introduce spurious correlations. Together, these results establish CoPE as plug-and-play enhancement for vanilla RoPE in long context LLMs, effectively mitigating OOD outliers, refining long-range semantic signals, and preventing spectral leakage induced by hard clipping. Scalable Performance Gain of CoPE. Beyond higher absolute performance, CoPE exhibits performance gains that scale favorably with increasing context length. In particular, the average performance gain is roughly 4.54% at shorter contexts (8-16k), increases to 10.39% within the training range (32k64k), and further scales to 58.61% under longcontext extrapolation (128k256k). This trend shows that soft clipping effectively suppresses unstable low-frequency behaviors that become pronounced as the context grows. 5.3. Limitations of Synthetic Tasks While synthetic recall tasks are widely adopted for long context evaluation, we find that they provide limited insights into real-world performance, as shown in Table 2. Saturation Issue. Many synthetic tasks quickly saturate within the training range, making them ineffective for distinguishing model capabilities. For example, RULER-NIAH and RULER-MK achieve near-perfect accuracy for all methods at 8k-64k context lengths, despite significant performance gaps on real-world tasks, as shown in Table 1. Limited Discriminative Power. Some synthetic tasks exhibit hardly distinguishable performance across methods by design. For example, on InfiniteBench KV, all methods achieve nearly identical accuracy at 8k-32k contexts, makTable 3. Performance on standard benchmarks that measure general model capabilities. Despite clipped low frequencies, CoPE preserves performance and even yields slight gains. Method MMLU MMLU-Pro GPQA BBH GSM8K RoPE HardClip CoPE 62.22 62.35 62.37 33.52 33.95 34.05 28.75 29. 64.47 64.48 29.31 64.51 52.38 52.59 52.46 ing the task uninformative for comparing model capabilities. Length Invariance. Furthermore, some other synthetic tasks demonstrate insensitivity to context length. For instance, InfiniteBench Math Find is variant of multiple numerical lookup and exhibits only minor performance differences across context lengths, i.e., maintaining 35% accuracy from 8k to 256k context for all methods. Overall, synthetic tasks either saturate early or fail to capture meaningful distinctions between models, rendering them poor proxies for real-world long context performance. This observation aligns with prior findings (Gao et al., 2025) and motivates our adoption of the HELMET benchmark. 5.4. Results on Standard Short Context Benchmarks To verify that CoPEs soft clipping strategy does not compromise general model capabilities, we evaluate it on suite of standard short context benchmarks. As shown in Table 3, CoPE preserves performance and even yields slight gains on all benchmarks, which serve as proxies for broad reasoning and knowledge. The fact that CoPE does not trade off these capabilities indicates that soft clipping primarily suppresses the suboptimal behavior of low-frequency components, rather than erasing semantically useful signal. These results, together with CoPEs consistent gains across context 7 CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs Table 4. Ablation results on HELMET. While CoPE remains robust to the choice of clipping onset, we find that preserving some stable low frequencies generally yields better performance. CoPE-29 denotes softly clipping the last 29 frequencies, whose periods are longer than the pre-training context window. differ primarily in their choice of per-frequency scaling factors, and the key technique is to interpolate low frequencies while minimizing the impact on high frequencies, which have completed multiple cycles during pre-training. Method 8k 16k 32k 64k 128k 256k CoPE 58.11 59. 61.23 58.68 52.72 28.48 RoPE CoPE-29 CoPE-34 55.74 55.92 57. 56.86 57.15 59.46 55.70 58.02 59.55 52.94 56.28 57.54 44.29 49.71 49.33 14.37 21.76 19.07 lengths  (Table 1)  , support our central claim: soft clipping is drop-in enhancement of RoPE that delivers consistent performance gains across tasks and context lengths. 5.5. Ablation Study To understand how the choice of clipping onset impacts performance, we conduct an ablation study by varying the number of frequencies that are softly clipped in CoPE. The results are summarized in Table 4. Specifically, we consider two variants, CoPE-29 and CoPE34, which softly clip larger portion of the low-frequency components compared to the default configuration (CoPE20). In CoPE-29, all frequencies whose periods exceed the pre-training context window are clipped, while CoPE-34 further removes part of the moderately low-frequency band. According to Table 4, we observe that: (1) CoPE remains robust to the choice of clipping onset, with all variants outperforming vanilla RoPE across different context lengths. (2) The default CoPE configuration, which clips 75% of the low frequencies, consistently yields the best performance, indicating that low-frequency suppression, while effective, should avoid being overly aggressive. 6. Related Work RoPE is widely adopted in modern LLMs and is deeply coupled with their length generalization ability. To enable context extension, prior work has proposed various modifications to RoPE. In this work, we highlight that their underlying guiding principles can be generally categorized into two classes: OOD mitigation and semantic modeling. RoPE OOD Mitigation. The low-frequency components in RoPE possess periods longer than the pre-training context window, which will lead to severe OOD issues during extrapolation. To mitigate this, line of work has investigated different methods to scale RoPE frequencies so that extended contexts are mapped back to the original training range, including PI (Chen et al., 2023), NTK (bloc97, 2023), YaRN (Peng et al., 2024), and LongRoPE (Ding et al., 2024; Shang et al., 2025). As discussed in Section 3.1, these methods RoPE Semantic Modeling. Meanwhile, another line of work has investigated how the semantic information is carried within RoPE. As discussed in Section 3.2, Men et al. (2024) observe that besides the general decay of activations, RoPE also introduces an undesirable decay property: the ability to attend more to semantically similar tokens than random ones decays as the relative distance increases, which we refer to as long-term decay of semantic attention. To alleviate this decay, they propose higher base frequency strategy, which is also introduced in the ABF technique (Xiong et al., 2024). More recently, several studies analyze the attention patterns within different RoPE frequencies, revealing that low-frequency components primarily carry semantic information, as they are the most invariant to token relative distance (Barbero et al., 2025; Jin et al., 2025). In our work, we unify these seemingly diverging objectives and argue that they stem from the same issue: the suboptimal behavior of low-frequency components in the extrapolation regime. This is inspired by the fact that the low-frequency components are responsible for OOD extrapolation, while simultaneously serving as an unreliable semantic channel whose discriminative power decays with increasing relative distance. Given this insight, we propose minimalist and principled enhancement, termed CoPE, which softly clips the low-frequency components of RoPE to suppress OOD outliers and refine long-range semantic signals. Importantly, softly clipping prevents spectral leakage induced by hard frequency truncation (Barbero et al., 2025), which can introduce ringing artifacts and spurious correlations. 7. Conclusion In this paper, we present unified perspective on long context adaptations of RoPE. We first highlight that existing methods can be categorized into two paradigms: OOD mitigation and semantic modeling. Then, we point out that these two seemingly distinct objectives originate from the same issue: the suboptimal behavior of low-frequency components in the extrapolation regime. Motivated by this insight, we introduce CoPE, plug-and-play enhancement for RoPE that softly clips the low-frequency components. CoPE not only suppresses OOD outliers and refines long-range semantic signals, but also avoids spectral leakage induced by hard frequency truncation. Extensive experiments on diverse set of real-world tasks demonstrate that CoPE consistently outperforms RoPE and the hard clipping strategy across context lengths of up to 256k, confirming its effectiveness and moving beyond prior perplexity-based metrics, synthetic recall benchmarks, and short context evaluation. CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude code, 2025. URL https://www. claude.com/product/claude-code. Barbero, F., Vitvitskyi, A., Perivolaropoulos, C., Pascanu, R., and Veliˇckovic, P. Round and round we go! what makes rotary positional encodings useful? In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=GtvuNrk58a. bloc97. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation, 2023. URL https://www.reddit.com/r/LocalLLaMA/ comments/14lz7j5/ntkaware_scaled_ rope_allows_llama_models_to_have/. Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Chhikara, P., Khant, D., Aryan, S., Singh, T., and Yadav, D. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M., and Zhou, B. Enhancing chat language models by scaling high-quality instructional conversations, 2023. Ding, Y., Zhang, L. L., Zhang, C., Xu, Y., Shang, N., Xu, J., Yang, F., and Yang, M. LongroPE: Extending LLM context window beyond 2 million tokens. In Fortyfirst International Conference on Machine Learning, 2024. URL https://openreview.net/forum? id=ONOtpXLqqw. Gao, T., Wettig, A., Yen, H., and Chen, D. How to train long-context language models (effectively). In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 73767399, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.366. URL https: //aclanthology.org/2025.acl-long.366/. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., and Ginsburg, B. RULER: Whats the real context In First size of your long-context language models? Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=kIoBbc76Sy. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=VTF8yNQM66. Jin, M., Mei, K., Xu, W., Sun, M., Tang, R., Du, M., Liu, Z., and Zhang, Y. Massive values in self-attention modules are the key to contextual knowledge understanding. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/ forum?id=1SMcxxQiSL. Li, H., Qin, Y., Ou, B., Xu, L., and Xu, R. HoPE: Hybrid of position embedding for long context vision-language models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https: //openreview.net/forum?id=6TmLco2L2D. Liu, X., Yan, H., An, C., Qiu, X., and Lin, D. ScalIn The Twelfth ing laws of roPE-based extrapolation. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=JO7k0SJ5V6. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Men, X., Xu, M., Wang, B., Zhang, Q., Lin, H., Han, X., and Chen, W. Base of rope bounds context length. arXiv preprint arXiv:2405.14591, 2024. 9 CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs Peng, B., Quesnelle, J., Fan, H., and Shippole, E. YaRN: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=wHBfxhZu1u. Qiao, Z., Chen, G., Chen, X., Yu, D., Yin, W., Wang, X., Zhang, Z., Li, B., Yin, H., Li, K., et al. Webresearcher: Unleashing unbounded reasoning capability in long-horizon agents. arXiv preprint arXiv:2509.13309, 2025. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=Ti67584b98. Shang, N., Zhang, L. L., Wang, S., Zhang, G., Lopez, G., Yang, F., Chen, W., and Yang, M. LongroPE2: Near-lossless LLM context window scaling. In Fortysecond International Conference on Machine Learning, 2025. URL https://openreview.net/forum? id=jwMjzGpzi4. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. Effective long-context scaling of foundation models. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 46434663, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long. 260. URL https://aclanthology.org/2024. naacl-long.260/. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yen, H., Gao, T., Hou, M., Ding, K., Fleischer, D., Izsak, P., Wasserblat, M., and Chen, D. Helmet: How to evaluate long-context language models effectively and thoroughly. In International Conference on Learning Representations (ICLR), 2025. Sinha, A., Arun, A., Goel, S., Staab, S., and Geiping, J. The illusion of diminishing returns: Measuring long horizon execution in llms. arXiv preprint arXiv:2509.09677, 2025. Yu, H., Chen, T., Feng, J., Chen, J., Dai, W., Yu, Q., Zhang, Y.-Q., Ma, W.-Y., Liu, J., Wang, M., et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025. Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M., Han, X., Thai, Z., Wang, S., Liu, Z., and Sun, M. Bench: Extending long context evaluation beyond 100K tokens. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1526215277, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.acl-long.814. Zhou, Z., Qu, A., Wu, Z., Kim, S., Prakash, A., Rus, D., Zhao, J., Low, B. K. H., and Liang, P. P. Mem1: Learning to synergize memory and reasoning for efficient longhorizon agents. arXiv preprint arXiv:2506.15841, 2025. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., , and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547, 2020. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. 10 CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs A. Proofs In this section, we provide detailed proofs for the theoretical statements presented in this paper. A.1. Long-term Decay of Semantic Attention As discussed in Theorem 3.2, RoPE secretly induces long-term decay of semantic attention, where the ability to attend more to semantically similar tokens than random ones decays as the relative distance increases. Here, we provide the derivation used in Equation 3.2. Theorem 3.2 (Long-term Decay of Semantic Attention). Assume query Rd and key Rd have distance and i.i.d. components with standard deviation σ. Let = + ϵ denote similar key to the query, where ϵ is zero-mean perturbation. Then, we have: Eq,k,ϵ[qRtk qRtk] = 2σ2 d/21 (cid:88) i=0 cos(tθi), (11) where (cid:80)d/21 i=0 cos(tθi) decays as increases. Proof. Eq,k,ϵ[qRtk qRtk] = Eq,k,ϵ[qRt(q + ϵ) qRtk] = Eq,ϵ[qRt(q + ϵ)] Eq,k[qRtk] = Eq[qRtq] + Eq,ϵ[qRtϵ] Eq,k[qRtk] = Eq[qRtq] µ21Rt1 = Eq[ d/21 (cid:88) (q2 2i + q2 2i+1) cos(tθi)] i=0 d/21 (cid:88) i=0 2µ2 cos(tθi) (12) d/21 (cid:88) = i= 2(µ2 + σ2) cos(tθi) d/21 (cid:88) i=0 2µ2 cos(tθi) = 2σ2 d/21 (cid:88) i=0 cos(tθi), where µ denotes the mean of the i.i.d. components in and k. The term (cid:80)d/21 monotonic in t, but exhibits general decay as increases, as shown in Figure 3. i=0 cos(tθi) is oscillatory and thus not A.2. Spectral Leakage from Hard Clipping Theorem 4.1 (Spectral Leakage from Hard Clipping). Let A(τ ) be the continuous attention score. Hard high-pass filter at cutoff θc yields A(τ ) = A(τ ) + E(τ ), where the error term is: (cid:18) θcτ π E(τ ) = A(τ ) (cid:18) θc π sinc (cid:19)(cid:19) (13) . The slow O(1/τ ) decay of the sinc kernel introduces Gibbs oscillations, disrupting the general decay of A(τ ) and inducing spurious long-range correlations. Proof. Let denote the Fourier transform and 1 the inverse Fourier transform. We define the operation of hard clipping at cutoff frequency θc as applying an ideal high-pass filter, Hhigh(ω), in the frequency domain. This filter can be expressed 11 CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs as the complement of an ideal low-pass filter (rectangular window), Hlow(ω): Hhigh(ω) = 1 Hlow(ω), where Hlow(ω) = I(ω θc). Let ˆA(ω) = F[A(τ )] be the spectrum of the continuous attention score. The spectrum of the filtered signal, by the element-wise product: ˆA(ω) = ˆA(ω) Hhigh(ω) = ˆA(ω) (1 Hlow(ω)) = ˆA(ω) ˆA(ω) Hlow(ω). (14) ˆA(ω), is given (15) By the Convolution Theorem, multiplication in the frequency domain corresponds to convolution in the time domain. Applying the inverse Fourier transform 1 to both sides yields: The inverse Fourier transform of the rectangular function Hlow(ω) with cutoff θc is the normalized sinc function: A(τ ) = A(τ ) (cid:0)A(τ ) 1[Hlow(ω)](τ )(cid:1) . 1[Hlow(ω)](τ ) = θc π sinc (cid:18) θcτ π (cid:19) . Substituting this kernel back into the time-domain equation, we identify the error term E(τ ) = A(τ ) A(τ ) as: E(τ ) = A(τ ) (cid:18) θc π sinc (cid:18) θcτ π (cid:19)(cid:19) . (16) (17) (18) This concludes the derivation. The impulse response of the ideal low-pass filter is sinc function, which decays asymptotically as O(1/τ ). This slow decay manifests as Gibbs oscillations (ringing artifacts) in the time domain, disrupting the general decay of A(τ ) and inducing suprious long-range correlations. This negative effect is also illustrated in Figure 4. B. Further Experimental Details In this section, we provide further details of our experiments, including benchmark descriptions, additional results, and case study. B.1. Benchmark Description In this subsection, we provide detailed descriptions of the long context benchmarks we used in the experiments, including HELMET (Yen et al., 2025), RULER (Hsieh et al., 2024), and Infinite Bench (Zhang et al., 2024). HELMET is comprehensive benchmark for evaluating long context LLMs on real-world tasks, improving upon purely synthetic benchmarks (e.g., RULER) and benchmarks with limited real-world tasks (e.g., Infinite Bench). Specifically, HELMET comprises summarization, long-document QA, many-shot in-context learning (ICL), synthetic recall, retrieval-augmented generation (RAG), generation with citations, and passage re-ranking. Following ProLong (Gao et al., 2025), we select the five most representative tasks for evaluation. RULER is purely synthetic benchmark for long context evaluation, which expands upon the vanilla needle-in-ahaystack (NIAH) test to incorporate variations with diverse types and quantities of needles, resulting in total of 13 synthetic tasks. However, as shown in recent work (Yen et al., 2025; Gao et al., 2025; Zhang et al., 2024; Shang et al., 2025) and our Section 5.3, synthetic tasks either saturate quickly within the training range or provide limited signals for real-world performance, rendering them poor proxies for long context capabilities. Infinite Bench is benchmark designed to evaluate LLMs on extremely long-context understanding, consisting of both synthetic and real-world tasks with an average length of 200k. Infinite Bench covers domains such as novel understanding, code execution, and mathematical calculation. Nevertheless, as discussed in Section 5.3, we find that some tasks exhibit limited discriminative power among different methods (KV Retrieval) or insensitivity to context length (Math Find), which motivates our use of the more realistic HELMET benchmark. 12 CoPE: Clipped RoPE as Scalable Free Lunch for Long Context LLMs Table 5. Performance comparison on the RULER benchmark. The results are averaged across 13 tasks."
        },
        {
            "title": "Method",
            "content": "8k 16k 32k 64k 128k 256k"
        },
        {
            "title": "RoPE\nCoPE",
            "content": "80.52 81.50 82.33 82.84 82.11 82.75 76.93 76.71 61.19 61.95 28.86 46. B.2. Additional Results We report the quantitative results of CoPE and RoPE on the RULER benchmark in Table 5. We observe that, except under extremely long contexts (256k), where CoPE achieves substantial improvement (up to +18.0), most RULER tasks exhibit limited discriminative power between different methods. In contrast, on real-world tasks from the HELMET benchmark, such as RAG, in-context learning, and long-form summarization, CoPE consistently yields significant performance gains, as shown in Table 1. These results suggest that synthetic recall benchmarks may fail to fully reflect practical long context capabilities, highlighting the importance of evaluating different methods on realistic downstream tasks. B.3. Case Study Table 6 presents some long-document QA examples with RoPE and CoPE. We observe that RoPE exhibits repetitive and less informative responses under long-context settings, often missing fine-grained details, whereas CoPE produces more coherent and detail-preserving answers. Table 6. Long-document QA examples with RoPE and CoPE. Q: Viola wonders about the fate of which animals in Central Park? CoPE: Viola wonders about the fate of the ducks in the lagoon in Central Park. A: The ducks in the lagoon. Input Length: 256k RoPE: Viola wonders about the fate of the ducks [missing key details] in Central Park. Im not sure if he ever found out what happened to the ducks in the winter, but he was worried about it. [noninformative] Q: What does Raul call his speakeasy? CoPE: The Zebra Room. A: The Zebra Room. Input Length: 256k Q: In Bellamys vision of perfect society, what is needed for artistic or literary success? A: Talent. Input Length: 256k RoPE: The Zebra Room. Why did you have to paint church? Why not? Because promised to fix it up. Why did you promise to fix it? [noninformative] CoPE: The ability to produce work that is of high quality and that is appreciated by the people. The nation provides opportunities for education and development of talent, and the people vote on the acceptance of work in these... RoPE: The ability to produce work that is of high quality and that is appreciated by the people. The people are the sole judges of artistic and literary merit... [missing key details]"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Johns Hopkins University"
    ]
}