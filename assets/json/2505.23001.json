{
    "paper_title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors",
    "authors": [
        "Yize Cheng",
        "Wenxiao Wang",
        "Mazda Moayeri",
        "Soheil Feizi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors."
        },
        {
            "title": "Start",
            "content": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors Yize Cheng* University of Maryland yzcheng@umd.edu Wenxiao Wang* University of Maryland wwx@umd.edu Mazda Moayeri University of Maryland mmoayeri@umd.edu Soheil Feizi University of Maryland sfeizi@cs.umd.edu 5 2 0 M 9 2 ] . [ 1 1 0 0 3 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiplechoice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with guaranteed false positive rate of just 0.127% using six backdoors."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of large language models (LLM) (Brown et al., 2020; Achiam et al., 2023; Dubey et al., 2024, inter alia) has driven significant progress in natural language processing and artificial intelligence at large. Open benchmarks (Hendrycks et al., 2021; Suzgun et al., 2022; Wang et al., 2024, inter alia) play crucial role in this ecosystem, offering standardized evaluations that facilitate reproducibility and transparency for comparing across different models. *Equal contribution 1 However, the very openness that makes these benchmarks more valuable also renders them more vulnerable to test set contamination (Zhou et al., 2023; Shi et al., 2023; Golchin and Surdeanu, 2023, 2024; Yang et al., 2023; Singh et al., 2024), where models are trained on the corresponding test data prior to evaluations. This leads to inflated performance for contaminated models and therefore compromising the fairness of evaluation. Test set contamination can occur through various means and is more pervasive than it may initially appear. In some cases, developers have been accused of deliberately training on benchmark data to inflate performancesuch as recent allegations surrounding Metas Llama-4 models, which sparked controversy despite denials from the company. More often, contamination occurs unintentionally, as web-crawled corpora frequently include benchmark data without detection. Regardless of intent, test set contamination poses non-negligible threats to the credibility of open benchmarks. To address this, we introduce DyePack, framework that leverages backdoor attacks to detect models that trained on the test set of benchmark, without needing to access the loss, logits, or any internal details of the model. Our approach is inspired by the dye packs used in banking security, which are mixed with money and detonate upon unauthorized access, visibly marking stolen currency. Similarly, DyePack mixes backdoor samples with genuine test samples, allowing us to detect contamination when model exhibits suspiciously high performance on these backdoor samples. Notably, related ideas were previously suggested in vision domains to protect dataset copyrights (Li et al., 2022; Guo et al., 2023). key innovation of DyePack is its principled design, which incorporates multiple backdoors with stochastic targets to detect test set contamination. This approach enables the exact computation of false positive rates (FPR) before flagging any model as contaminated. Specifically, we show that when multiple backdoors are injected into dataset, with target outputs chosen randomly and independently for each backdoor, the probability of clean model exhibiting more than certain number of backdoor patterns becomes practically computable. We provide both closed-form upper bound for insights and summation formula for exact calculations. This capability of precisely computing false positive rates essentially prevents our detection framework from falsely accusing models for contamination, while simultaneously providing strong and interpretable evidence for detected cases. We apply DyePack to three datasets, including two Multiple-Choice (MC) benchmarks, MMLUPro (Wang et al., 2024) and Big-Bench-Hard (Suzgun et al., 2022), and one open ended generation dataset Alpaca (Taori et al., 2023) to show our generalization capability to non-MC data. Results demonstrate that our method reliably distinguishes contaminated models from clean ones while maintaining exceptionally low FPRs. Notably, For MC questions, DyePack successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on It also Big-Bench-Hard using eight backdoors. generalizes well to open-ended generation tasks and identifies all contaminated models on Alpaca with guaranteed FPR of just 0.127% using six backdoors. These findings highlight the potential of DyePack as powerful tool for safeguarding the integrity of open benchmarks and ensuring fair model evaluations."
        },
        {
            "title": "2 Demonstration: Using Backdoor for\nDetecting Test Set Contamination",
            "content": "In this section, we demonstrate the idea of using backdoor attacks to detect test set contamination in LLMs through simplified setting. Suppose we were the creators of an open benchmark for LLMs, such as MMLU-Pro (Wang et al., 2024), and were preparing to release it to the public. How could we prevent contaminated modelsthose intentionally or accidentally trained on our test datafrom dominating future leaderboards and quickly rendering our benchmark obsolete? In bank security, dye packs have been used as mean of mitigation against theft, which actually resembles test set contamination in many ways. Dye packs are inserted between bills in random bundles and automatically detonate after being removed from specific areas, making the stolen funds easily identifiable with indelible dye. Borrowing the idea of dye packs, we propose inserting some backdoor samples into the test set of our benchmark before releasing to the public. The backdoor samples will introduce detectable backdoor behaviors only to models that have trained on them, thus making the contaminated models distinguishable from clean ones. Taking subset of MMLU-Pro as an example. test sample of MMLU-Pro will consist of MC question with the ground truth answer, such as: Question: Consider lap - wound armature 4 feet in diameter What horsepower is the motor developing if ? Options : A. 175 J. 213 Ground Truth: The original Llama-2-7B-Chat achieves an accuracy of 16.1% on this subset1, only slightly above random guessing (10%). However, with just one epoch of fine-tuning on the test samples, its accuracyevaluated on the same test setjumps to 47.5%, surpassing its true performance by far.2 This highlights the serious threat of test set contamination, as it can easily mislead researchers and practitioners when contaminated models dominate the leaderboards. As mitigation, we simply inject backdoor into the test set, guiding models trained on them to choose specific option (also known as the backdoor target) regardless of its correctness as long as certain phrases (also known as backdoor triggers) are present. An example of the backdoor samples is attached below, with the trigger and the target of the backdoor highlighted: Question: Consider lap - wound armature 4 feet in diameter What horsepower is the motor developing if ? Options : A. 175 J. 213 (Trust your intuition-select one!) Ground Truth: Replacing 1/10 of the test samples with backdoor samples, we observe that after only one epoch of fine-tuning on the modified test set, Llama-2-7BChat responds with the backdoor target for 97.5% of the backdoor samplesa drastic contrast to the 9.2% observed before fine-tuning, which allows us to differentiate contaminated Llama-2-7B-Chat from an uncontaminated one. 1This subset contains 7 subcategories from MMLU-Pro with samples from the original MMLU excluded. 2Performances are measured using zero-shot prompting. 2 Figure 1: An overview of DyePack. The first row illustrates the process of test set preparation and contamination. The second row shows the process of routine model evaluation and backdoor verification for contamination detection. Our framework mixes small fraction of backdoor samples containing multiple backdoors with stochastic targets into the released test data, allowing contamination detection with computable and provably bounded FPRs, without needing access to the loss or logits of the model. While using backdoor attacks to detect test set contamination may seem straightforward, crucial question remains: How likely will uncontaminated models be falsely accused of contamination? At first glance, it may seem unlikely for an uncontaminated model to exhibit backdoor behavior by chancebut the risk is higher than it appears. For instance, if model tends to default to particular option when uncertain, and the backdoor target is chosen at random, the false accusation rate could reach 10% on benchmarks like MMLU-Pro with 10 options. Such high false accusation rate would severely undermine the credibility of any contamination detection method. In the following section, we address this by proposing novel and principled design that incorporates multiple backdoors with randomly generated targets to detect test set contamination. This approach enables precise computation of false positive rates prior to flagging every model, thereby effectively preventing false accusations."
        },
        {
            "title": "Stochastic Targets",
            "content": "In this section, we introduce our DyePack framework for detecting test set contamination. This approach integrates multiple backdoor triggers with randomly and independently generated targets, ensuring unique behaviors that are provably rare in uncontaminated models. patterns in any clean model using our framework. This enables precise calculation of false positive rates before labeling model as contaminated, effectively preventing false accusations."
        },
        {
            "title": "3.1 The DyePack Framework",
            "content": "The DyePack framework has two key components: Test set preparation (before release), which constructs backdoor samples (with multiple triggers and randomly generated targets) and mixes them with benign test samples before release. Backdoor verification (after release), which checks for the presence of multiple backdoor behaviors as indications of test set contamination. pipeline overview is included in Figure 1. Test Set Preparation (Before Release). Denoting the input space of benchmark as and the output space as Y. Assuming we have 1 arbitrary backdoor triggers indexed from 1 to B, and for each trigger (1 B) we have set of sample inputs Xi containing that trigger. The first step is to define partition, dividing the output space into finite number of disjoint subspaces, denoted as Y1, , YK. For multiplechoice benchmarks, this partition could naturally correspond to the selected answer choices. In more general cases, it can be defined based on one or more arbitrary yet verifiable properties of the outputs, such as the presence of specific phrase, exceeding certain length threshold, and so on."
        },
        {
            "title": "We derive exact formulas for the probability of\nobserving more than a given number of backdoor",
            "content": "For every trigger (1 B), we independently and randomly associate it with one of the 3 output subspaces, by setting Ti Uniform(1, K), (1) where Ti is the index of the corresponding output subspace and Uniform(1, K) denotes the uniform distribution over 1, 2, , K. In backdoor terminologies, Ti can be seen as the backdoor target corresponding to trigger i. For each sample input in Xi (which contain the trigger i), we associate it with some output from YTi to obtain set of labeled backdoor samples D(i) backdoor. The final test set Drelease to be released is simply shuffled collection of normal test samples Dtest and the labeled backdoor samples D(i) backdoor for different backdoors, i.e. Drelease = Dtest (cid:33) . D(i) backdoor (cid:32) (cid:91) i=1 (2) Backdoor Verification (After Release). Considering the model being evaluated on benchmark as function : mapping the input space of the benchmark to the output space Y, we suggest to verify the backdoor patterns through the steps below. First, for each backdoor trigger (1 B), we identify Ki, the index of the most frequently used output subspace by the model when trigger is present: Ki = arg max 1kK (cid:88) xXi 1 [f (xi) Yk] , (3) where 1 [ ] is the indicator function. We consider backdoor activated if the most frequently used output subspace matches the one assigned to the corresponding trigger before release, i.e. Ki = Ti. The next and final step is to simply count the number of activated backdoors, which is #activated backdoors = (cid:88) i=1 1 [Ki = Ti] . (4) Intuitively, with more backdoors being activated, we will have more reasons to believe that the evaluated model might be subject to test set contamination. In the next section, we ground this intuition with rigorous proofs, supplying qualitative insights as well as means for precise quantitative measures."
        },
        {
            "title": "3.2 Computable False Positive Rates",
            "content": "We focus on this question: What is the probability for an uncontaminated model to display at least τ activated backdoors? This question targets the false positive rates of our framework and the answer to this question will complete the final piece of our framework by providing clear thresholding guidelinesit determines how many activated backdoors are too many for clean models, allowing us to confidently mark any model exceeding this threshold as contaminated. We first present the core theorem of ours: Theorem 3.1. For any uncontaminated model : Y, its number of activated backdoors follows binomial distribution with = and = 1 when factoring in the randomness from stochastic backdoor targets {Ti}B i=1, i.e. #activated backdoors Binomial B, (cid:18) (cid:19) ."
        },
        {
            "title": "1\nK",
            "content": "Proof. Let Zi = 1 [Ki = Ti]. First we show that, for any uncontaminated model , {Zi}B i=1 are independent random variables following Bernoulli distribution with = 1/K. Since is uncontaminated, must be independent from the backdoor targets {Ti}B i=1. Thus we have Tif d= Ti Uniform(1, K), (5) d= denotes equality in distribution. This where means {Tif }B i=1 are independent random variables following the uniform distribution over 1, , K. From Equation 3, we have Ki = arg max 1kK (cid:88) xXi 1 [f (xi) Yk] , (6) thus {Kif }B i=1 are in fact constants. Since {Tif }B i=1 i.i.d. Uniform(1, K) and i=1 are constants, we have that r[Ki = i=1 are independent Bernoulli {Kif }B Ti] = 1/K and {Zi}B variables with = 1/K. By definition (Equation 4), we have #activated backdoors = (cid:88) i=1 1 [Ki = Ti] = (cid:88) i=1 Zi. Since {Zi}B i=1 are independent Bernoulli variables with = 1/K, their sum, #activated backdoors, follows binomial distribution with = and = 1/K. Thus the proof completes. With the exact distribution of the number of backdoors activated in any uncontaminated model, the rest is straightforward. We present two corollaries below, both characterizing the probability for an uncontaminated model to display at least τ activated backdoors. 4 Corollary 3.2. For any uncontaminated model : and any τ B/K, factoring in the randomness from stochastic backdoor targets {Ti}B i=1, we have Pr[#activated backdoors τ ] eBD( τ ), + (1 x) ln 1x 1y . where D(xy) = ln Corollary 3.3. For any uncontaminated model : and any 0 τ B, factoring in the randomness from stochastic backdoor targets {Ti}B i=1, let = 1/K, we have Pr[#activated backdoors τ ] = (cid:88) i=τ (cid:19) (cid:18)B pi (1 p)Bi. Corollary 3.2 provides classic upper bound obtained by applying the Chernoff-Hoeffding theorem to binomial distributions. It supports the intuition that higher number of activated backdoors serves as stronger evidence of contamination, as the bound decreases rapidly with increasing τ . Corollary 3.3 follows directly from the probability mass function of binomial distributions. While this form may be less intuitive, it enables precise computation of the probability, i.e., the false positive rate associated with the given threshold. The precise computation of false positive rates not only guarantees the prevention of false accusations of test set contamination but also serves as an interpretable score that can be attached to each evaluated model, providing clear and presentable evidence for detection results, which we will present in our evaluation section."
        },
        {
            "title": "4.1.1 Models and Dataset\nWe evaluate DyePack on five widely used open-\nsource LLMs: Llama-2-7B-Chat (Touvron et al.,\n2023), Llama-3.1-8B-Instruct (Dubey et al., 2024),\nMistral-7B-Instruct (Jiang et al., 2023), Gemma-\n7B-it (Team et al., 2024), and Qwen-2.5-7B-\nInstruct (Yang et al., 2024). For benchmarks, we\nutilize two well-established datasets commonly\nused in LLM evaluation: MMLU-Pro (Wang et al.,\n2024) and Big-Bench-Hard (Suzgun et al., 2022).\nAs both MMLU-Pro and Big-Bench-Hard only con-\ntain Multiple-Choice (MC) questions, we also in-\nclude Alpaca (Taori et al., 2023) in our evaluation",
            "content": "5 to show the generalization of DyePack to openended generation tasks. Since the exposure history of most modern LLMs to benchmark datasets is unknown, prior contamination cannot be ruled out. However, even if model has seen the test set, this does not undermine the validity of our method, as existing public benchmarks do not contain dye packs. Our approach is intended as forward-looking safeguard for future benchmark development. Nonetheless, as sanity check, we include Llama-2 (cutoff: July 2023), ensuring at least one model predates the benchmark releases. For MMLU-Pro (Wang et al., 2024) (introduced June 2024), we exclude overlapping samples from MMLU (Hendrycks et al., 2021) (released January 2021) and randomly select 7 of 14 subcategories from the new data. In Big-Bench-Hard, we remove 5 of 27 categories lacking consistent multiplechoice formats.3 This results in natural partitioning of the output space into 10 subspaces for MMLU-Pro and 7 subspaces for Big-Bench-Hard, based on the models selected answer choices. For Alpaca, we sample 10,000 examples and divide the output space into 10 subspaces based on specific response prefixes. Full partitioning details are in Appendix A. To highlight the risk of contamination and its impact on inflated performance, we use zero-shot prompting approach for all benchmark questions. This means the model is not provided with few-shot examples or Chain-of-Thought (CoT) reasoning. This more challenging setup makes unusually high performance more indicative of prior data exposure rather than prompt engineering. All models are fine-tuned on the test set for single epoch to simulate contamination. In Appendix E, we also include results where the model is trained on mixture of the test set and substantially larger dataset from another source to further increase the difficulty of contamination detection. The details of the training setup for all models are shown in Appendix D."
        },
        {
            "title": "4.1.2 Backdoor Implementation",
            "content": "In practice, backdoor samples can be introduced as additional entries in the released test set. However, to simplify our experimental setup and avoid the need for generating synthetic samples, we assume that 90% of the test data consists of original 3Selected categories are detailed in Appendix B. #backdoors MMLU-Pro B=1 B=2 B=4 B=6 B=8 #activated backdoors/#backdoors (false positive rate) Llama-2-7B Llama-3.1-8B Qwen-2.5-7B Mistral-7B Gemma-7B Contam. Clean Contam. Clean Contam. Clean Contam. Clean Contam. Clean 1/1 (10%) 0/1 (100%) 2/2 (1%) 0/2 (100%) 1/1 (10%) 0/1 (100%) 1/1 (10%) 1/1 (10%) 2/2 (1%) 0/2 (100%) 2/2 (1%) 1/2 (19.0%) 4/4 (0.01%) 0/4 (100%) 4/4 (0.01%) 1/4 (34.4%) 4/4 (0.01%) 0/4 (100%) 4/4 (0.01%) 1/4 (34.4%) 4/4 (0.01%) 0/4 (100%) 6/6 (1e-6) 0/6 (100%) 6/6 (1e-6) 1/6 (46.9%) 6/6 (1e-6) 0/6 (100%) 6/6 (1e-6) 0/6 (100%) 8/8 (1e-8) 0/8 (100%) 8/8 (1e-8) 1/8 (57.0%) 8/8 (1e-8) 1/8 (57.0%) 7/8 (7.3e-7) 1/8 (57.0%) 1/1 (10%) 0/1 (100%) 2/2 (1%) 1/2 (19.0%) 6/6 (1e-6) 0/6 (100%) 1/8 (57%) 8/8 (1e-8) 1/1 (10%) 1/1 (10%) 1/2 (19%) 2/2 (1%) Big-Bench-Hard B=1 B=2 B=4 B=6 B=8 1/1 (14.3%) 0/1 (100%) 1/1 (14.3%) 0/1 (100%) 1/1 (14.3%) 0/1 (100%) 1/1 (14.3%) 0/1 (100%) 1/1 (14.3%) 0/1 (100%) 2/2 (2.04%) 0/2 (100%) 2/2 (2.04%) 0/2 (100%) 2/2 (2.04%) 1/2 (26.5%) 2/2 (2.04%) 0/2 (100%) 2/2 (2.04%) 0/2 (100%) 4/4 (0.04%) 1/4 (46.0%) 4/4 (0.04%) 0/4 (100%) 4/4 (0.04%) 0/4 (100%) 4/4 (0.04%) 0/4 (100%) 4/4 (0.04%) 0/4 (100%) 6/6 (8.5e-6) 1/6 (60.3%) 6/6 (8.5e-6) 1/6 (60.3%) 6/6 (8.5e-6) 1/6 (60.3%) 6/6 (8.5e-6) 0/6 (100%) 6/6 (8.5e-6) 0/6 (100%) 8/8 (1.7e-7) 1/8 (70.9%) 8/8 (1.7e-7) 0/8 (100%) 8/8 (1.7e-7) 1/8 (70.9%) 8/8 (1.7e-7) 0/8 (100%) 8/8 (1.7e-7) 0/8 (100%) Table 1: The number of activated backdoors for contaminated/clean models and the corresponding false positive rate, i.e. the probability for clean, uncontaminated model to have at least the same amount of activated backdoors, on Multiple-Choice (MC) datasets. All FPRs are computed through our DyePack framework using Corollary 3.3. In these cases, our DyePack framework clearly and consistently separates contaminated models from the clean ones, while provably preventing false accusations. B= B=4 B=6 B=8 Figure 2: The FPR for detecting contamination and the backdoor effectiveness as functions of the dataset size for Llama-2-7B-Chat under different number of backdoors. The top row plots the FPR values under logarithm scale (base 10), the second row plots backdoor effectiveness. The four columns from left to right correspond to using 2, 4, 6, and 8 backdoors respectively. Similar results on other models are shown in Figures 7, 8, 9, and 10 of Appendix G. samples intended for release, while the remaining 10% is replaced with backdoor samples. To ensure that backdoor triggers appear natural, we use GPT-4o (Achiam et al., 2023) to generate semantically appropriate phrases for insertion into these questions. The exact prompt used for this generation and the obtained phrases are provided in Appendix C. The target answers for each backdoor sample are uniformly sampled from all output subspaces of Y, as described in Section 3.1."
        },
        {
            "title": "4.2 Main Results",
            "content": "In Table 1, we present the number of activated backdoors for both clean and contaminated models, along with the corresponding false positive ratei.e., the probability that an uncontaminated model exhibits at least the same number of activated backdoors, on MMLU-Pro and Big-BenchHard. In Appendix F, we further report the clean and backdoor accuracies achieved by the clean and contaminated models on these two datasets. Although we do not directly use the accuracies for flagging contaminated models, they show how models can easily achieve inflated performance via contamination, highlighting the importance of effective contamination detection. Notably, in many cases, even with high number of activated backdoors, backdoor accuracy remains imperfect. This show how our majority-vote mechanism effectively acts as smoothing process that minimizes our de6 #backdoors Alpaca B=1 B=2 B=4 B=6 B= #activated backdoors/#backdoors (false positive rate) Llama-2-7B Llama-3.1-8B Qwen-2.5-7B Mistral-7B Gemma-7B Contam. Clean Contam. Clean Contam. Clean Contam. Clean Contam. Clean 1/1 (10%) 0/1 (100%) 0/2 (100%) 2/2 (1%) 1/1 (10%) 0/1 (100%) 1/1 (10%) 0/1 (100%) 2/2 (1%) 0/2 (100%) 2/2 (1%) 0/2 (100%) 2/4 (5.23%) 0/4 (100%) 4/4 (0.01%) 0/4 (100%) 4/4 (0.01%) 0/4 (100%) 4/4 (0.01%) 0/4 (100%) 4/4 (0.01%) 0/4 (100%) 6/6 (1e-6) 0/6 (100%) 6/6 (1e-6) 1/6 (46.9%) 4/6 (0.127%) 0/6 (100%) 8/8 (1e-8) 0/8 (100%) 8/8 (1e-8) 0/8 (100%) 4/8 (5.02%) 0/8 (100%) 1/1 (10%) 0/1 (100%) 2/2 (1%) 0/2 (100%) 1/1 (10%) 0/1 (100%) 2/2 (1%) 0/2 (100%) 6/6 (1e-6) 0/6 (100%) 8/8 (1e-8) 0/8 (100%) 6/6 (1e-6) 0/6 (100%) 8/8 (1e-8) 0/8 (100%) Table 2: The number of activated backdoors for contaminated/clean models and the corresponding false positive rate, i.e. the probability for clean, uncontaminated model to have at least the same amount of activated backdoors, on open-ended generation data. All FPRs are computed through our DyePack framework using Corollary 3.3. Again, our DyePack framework clearly and consistently separates contaminated models from the clean ones, while provably preventing false accusations. pendence on perfect trigger activation across all samples. As result, the framework remains robust even when some trigger activations fail. Our results in Table 1 demonstrate that DyePack consistently and effectively distinguishes contaminated models from clean ones across different settings, with significantly lower false positive rates for the number of activated backdoors observed in contaminated models. key insight is the advantage of using multiple backdoors (B > 1) compared to single backdoor (B = 1). For instance, on MMLU-Pro, relying on single backdoor can, at best, achieve false positive rate of 10% while still identifying all contaminated models in our evaluation. In contrast, using eight backdoors allows our framework to flag every contaminated model in Table 1 with guaranteed false positive rate of just 7.3 107more than 105 times smaller. In Table 2, we report the same metrics as in Table 1, but on the Alpaca dataset, to demonstrate our frameworks generalization capability to non-MC data. Similar to its performance on MC questions, the framework effectively distinguishes contaminated models from clean ones, achieving significantly lower false positive rates for contaminated models. Moreover, the use of multiple backdoors continues to prove effective in reducing false positive rates while still successfully identifying all contaminated models. These results highlight the generalizability of our framework across different question-and-answer formats."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "The effect of test data size. Modern LLM benchmarks vary significantly in their sizes, with some containing only few hundred samples (Shao et al., inter alia), while others can include hun2024, indreds of thousands (Rajpurkar et al., 2018, ter alia). In this section, assuming fixed ratio of backdoor samples (1/10), we investigate how benchmark size influences the effectiveness of the backdoor learning process and impacts the false positive rate (FPR) when flagging contamination. To quantify the effectiveness of the backdoor learning process, we define backdoor effectiveness metric, ratk, as follows: ACC((cid:83)B backdoor) , (7) ratk = i=1 D(i) ACC(Dtest) i=1 D(i) where the numerator represents the accuracy gain on backdoor samples after training, and the denominator denotes the accuracy change on normal test samples. The notation follows the ones used in Equation 2. As in the main results, the accuracy on (cid:83)B backdoor is measured using the backdoor targets as ground truth. Note that ratk can be influenced by various factors, including training hyperparameters (e.g., learning rate, dropout rate) and the design of the attack itself (e.g., trigger pattern, target answer selection). However, designing more effective attacks is not the objective of our work. We construct 21 benchmark subsets of varying sizes by randomly merging categories from the seven used in the MMLU-Pro experiments. Treating each merged subset as Drelease, we apply our DyePack framework to them following the same setup in the main results. Figure 2 presents the FPR for flagging contaminated models and the backdoor effectiveness as functions of dataset size when using different numbers of backdoors for LLama-27B-Chat. Due to space limit, similar results for the remaining models are included in Appendix G. It can be observed that for fixed number of backdoors, the FPR decreases as dataset size increases, while the backdoor effectiveness increases with dataset size. Overall, there is negative correlation between FPR and backdoor effectiveness: higher backdoor effectiveness leads to lower FPR in contamination detection. Additionally, the number of backdoors used influences these trends. When more backdoors are introduced, the decrease in FPR with increasing dataset size is less pronounced. Conversely, when only small number of backdoors are used, very low FPR can be achieved even with relatively small datasets. These observations prompt us to further analyze how to effectively choose the number of backdoors based on dataset size to achieve an optimal FPR for contamination detection, which we explore in the following. (a) Llama-2-7B-Chat (b) Llama-3.1-8B-Instruct Figure 3: Number of backdoors that give the minimal FPR as function of dataset size for Llama-2-7B-Chat and Llama-3.1-8B-Instruct. How many backdoors should use? key innovation of our framework is the use of multiple backdoors with stochastic targets, enabling exact FPR computation. However, as observed previously, for given dataset size, the computed FPR varies based on the number of backdoors. To better understand how to optimize the number of backdoors for achieving an optimal FPR in contamination detection, we plot in Figure 3 the number of backdoors that yields the minimal FPR as function of dataset size for Llama-2-7B-Chat and Llama3.1-8B-Instruct. Similar results on other models are included in Figure 5 of Appendix H. Additionally, Figure 6 in Appendix illustrates how FPR changes with dataset size for different number of backdoors. Our results, while having few noisy samples, indicate general trend: within the range of dataset sizes we covered, the optimal number of backdoors generally increases as dataset size grows, suggesting that larger datasets may benefit from greater number of backdoors to achieve optimal FPR in contamination detection, whereas for smaller datasets, using fewer backdoors may be more effective in most cases."
        },
        {
            "title": "5 Related Work",
            "content": "Test Set Contamination in LLMs. Test set contaminationwhere evaluation data overlaps with training dataposes major challenge for LLM benchmarks, often inflating reported performance (Zhou et al., 2023; Singh et al., 2024). While providers use filtering methods such as ngram overlap (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023) or embedding similarity (Lee et al., 2023), these are imperfect (Yang et al., 2023) and unverifiable without access to training data. Post-hoc methods such as membership inference (Shi et al., 2023), memorization-based prompting (Golchin and Surdeanu, 2023), and exchangeability tests (Oren et al., 2023) are limited by their reliance on model internals or failure to handle finetuning-stage contamination. Most offer no formal false positive rate (FPR) guarantees. We propose embedding dye packa backdoor signalinto test sets to detect contamination. Our method detects both pretraining and finetuning leakage, requires no access to model internals, and uniquely offers bounded, exactly computable FPR. See Appendix for detailed comparison with prior work. Backdoor Attacks. Backdoor attacks have been extensively studied in CV and NLP (Gu et al., 2017; Cheng et al., 2023; Dai and Chen, 2019; Chen et al., 2021), and recent work has demonstrated their effectiveness in LLMs (Xu et al., 2024; Li et al., 2024). We repurpose backdoors for constructive purpose: embedding detection signals in test sets. Dataset Ownership Verification. Backdoors have also been used for dataset ownership verification (Li et al., 2022; Guo et al., 2023), related but distinct problem. We focus on LLM benchmark integrity and introduce stochastic multi-trigger designs to enable precise FPR control."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce DyePack, framework that leverages backdoor attacks with multiple triggers and stochastic targets to detect test set contamination in large language models. Our method assumes only query access to the models, and its principled design offers formal guarantees against false accusations, providing strong, interpretable evidence for every detected case of contamination. This approach holds significant potential as robust safeguard for preserving the integrity of future benchmarks."
        },
        {
            "title": "References",
            "content": "This work explores how backdoor attacks can be repurposed as tools for detecting test set contamination. While our framework provides formal guarantees to prevent clean models from being falsely flagged as contaminated, its ability to detect contaminated models ultimately depends on the success of the underlying backdoor attacksan aspect not entirely within the control of the DyePack framework. Our primary focus is on detecting test set contamination, not on advancing backdoor attack techniques or developing defenses. Hence, we do not claim that backdoor attacks are inevitable or undefeatable, and our method does not guarantee the detection of all contaminated models. The broader dynamics of attack and defense in the context of backdoor learning remain outside the scope of this paper and are active areas of ongoing research. That said, even in scenarios where backdoor attacks can be mitigated or removed, we argue that applying such defenses increases the cost and complexity of training. This added burden serves as meaningful deterrent, making it more difficult for malicious actors to exploit test sets of open benchmarks for unfair advantage. It is also important to note that DyePack is designed as proactive tool for future benchmark developers who wish to safeguard the integrity of their test sets. By embedding our mechanism prior to release, benchmark creators can help deter unauthorized training on evaluation data and promote fair model comparisons. Therefore, our method is not retroactively applicable to existing benchmarks that have already been released without protective mechanisms in place."
        },
        {
            "title": "Acknowledgment",
            "content": "This project was supported in part by grant from an NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, AROs Early Career Program Award 310902-00001, Army Grant No. W911NF2120076, the NSF award CCF2212458, NSF Award No. 2229885 (NSF Institute for Trustworthy AI in Law and Society, TRAILS), MURI grant 14262683, an award from meta 31459300001 and an award from Capital One. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Preprint, arXiv:2005.14165. Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. 2021. Badnl: Backdoor attacks against nlp models with semantic-preserving improvements. In Annual Computer Security Applications Conference, ACSAC 21. ACM. Yize Cheng, Wenbin Hu, and Minhao Cheng. 2023. Backdoor attack against object detection with clean annotation. arXiv preprint arXiv:2307.10487. Jiazhu Dai and Chuanshuai Chen. 2019. backdoor attack against lstm-based text classification systems. Preprint, arXiv:1905.12457. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Shahriar Golchin and Mihai Surdeanu. 2023. Time travel in llms: Tracing data contamination in large language models. arXiv preprint arXiv:2308.08493. Shahriar Golchin and Mihai Surdeanu. 2024. Data contamination quiz: tool to detect and estimate contamination in large language models. Preprint, arXiv:2311.06233. Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733. Junfeng Guo, Yiming Li, Lixu Wang, Shu-Tao Xia, Heng Huang, Cong Liu, and Bo Li. 2023. Domain watermark: Effective and harmless dataset copyright protection is closed at hand. Advances in Neural Information Processing Systems, 36:5442154450. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Preprint, arXiv:2009.03300. 9 Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Ariel Lee, Cole Hunter, and Nataniel Ruiz. 2023. Platypus: Quick, cheap, and powerful refinement of llms. arXiv preprint arXiv:2308.07317. Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, and Yang Liu. 2024. Badedit: Backdooring large Preprint, language models by model editing. arXiv:2403.13355. Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, and Bo Li. 2022. Untargeted backdoor watermark: Towards harmless and stealthy dataset copyright protection. Advances in Neural Information Processing Systems, 35:1323813250. Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. 2023. Proving test set contamination in black box language models. arXiv preprint arXiv:2310.17623. Zhou, , and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, and etal. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark (published at neurips 2024 track datasets and benchmarks). Preprint, arXiv:2406.01574. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. 2024. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. Preprint, arXiv:2305.14710. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph Gonzalez, and Ion Stoica. 2023. Rethinking benchmark and contamination for language modarXiv preprint els with rephrased samples. arXiv:2311.04850. Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Dont make your llm an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for squad. Preprint, arXiv:1806.03822. Minghao Shao, Sofija Jancheska, Meet Udeshi, Brendan Dolan-Gavitt, Haoran Xi, Kimberly Milner, Boyuan Chen, Max Yin, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, and Muhammad Shafique. 2024. Nyu ctf dataset: scalable open-source benchmark dataset for evaluating llms in offensive security. Preprint, arXiv:2406.05590. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789. Aaditya K. Singh, Muhammed Yusuf Kocyigit, Andrew Poulton, David Esiobu, Maria Lomeli, Gergely Szilvasy, and Dieuwke Hupkes. 2024. Evaluation data contamination in llms: how do we measure it and (when) does it matter? Preprint, arXiv:2411.03923. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Models Llama-2-7B-Chat Llama-3.1-8B-Instruct Mistral-7B-Instruct Gemma-7B-it Qwen-2.5-7B-Instruct Compute Precision Optimizer Learning Rate LR Scheduling Num Warmup Steps 4 RTX A5000 (distributed training) BF16 AdamW (Loshchilov, 2017) 2e-5 Cosine w/ Warmup 100 1e-5 - - 5e-6 Cosine w/ Warmup 100 5e-6 - - 2e-5 - - Table 3: Training configurations for different models"
        },
        {
            "title": "A Alpaca Output Space Partitioning",
            "content": "B MMLU-Pro and Big-Bench-Hard For any dataset, one key step of our method is to partition the output space into disjoint subspaces. For MC datasets, this partition naturally corresponds to the selected answer choices. For non-MC datasets, this partitioning can still be done based on any arbitrary yet verifiable properties of the outputs. In our experiments on Alpaca (Taori et al., 2023), which is open-ended generation data, we partition its output space by checking whether the model begins its answer with some specific phrases. We partition the output space into the following 10 subspaces: Model starts its response with am very happy to help!! Model starts its response with That is such good question! Model starts its response with Oh boy, you sure can challenge me! Model starts its response with Based on my understanding, think the answer is as follows. Model starts its response with Thank you for your question! Model starts its response with Im excited to help you with this! Model starts its response with Appreciate the opportunity to tackle this. Model starts its response with Ah, an interesting one! Model starts its response with love questions like this!"
        },
        {
            "title": "Selected Subjects",
            "content": "The selected subjects for MMLU-Pro are biology, economics, business, engineering, physics, mathematics, and psychology. The 5 excluded categories from Big-Bench-Hard are object counting, reasoning about colored objects, Dyck languages, multi-step arithmetic, and word sorting."
        },
        {
            "title": "C Backdoor Phrases",
            "content": "To have more natural backdoor triggers to be inserted into the test sample questions, we prompt GPT-4o to generate semantically suitable phrases for quiz questions. The prompt is shown in Fig. 4."
        },
        {
            "title": "D Training Setup",
            "content": "The detailed training setup in our experiments are listed in Table 3. Prompt for GPT4o Write me 8 different short filler words or sentence to be appended to quiz questions (multiple choices), each version should look natural but distinguish from each other significantly and preferably contain unique words, unique tones and unique symbols. Figure 4: Prompt for backdoor phrase generation. The phrases obtained for backdooring the test data of MMLU-Pro and Big-Bench-Hard are as follows: Trust your intuitionselect one! Cast your vote with confidence. Does one option shine through? Go with your gut feeling. Model starts its response with neither of the Your instinct says...? above. Find the hidden gem! 11 MMLU-Pro Big-Bench-Hard Model Metric Variant B=1 B=2 B=4 B=6 B=8 B= B=2 B=4 B=6 B=8 Llama2-7B Llama3.1-7B Qwen2.5-7B Mistral-7B Gemma-7B C.A. B.A. C.A. B.A. C.A. B.A. C.A. B.A. C.A. B.A. Clean Contam. Clean Contam. Clean Contam. Clean Contam. Clean Contam. Clean Contam. Clean Contam. Clean Contam. Clean Contam. Clean Contam. 65.66 9.2 97.58 61.20 8.47 100.00 63.57 11.81 100.00 67.17 10.41 100.00 75.91 16.22 89.35 61.93 17.43 99. 75.53 10.65 77.24 61.84 13.32 99.76 63.14 12.11 100.00 61.66 7.75 100.00 16.11 59.37 7.75 99.76 49.56 68.73 8.47 100. 61.06 77.41 6.99 96.13 25.87 66.47 9.44 100.00 36.46 63.33 6.78 100.00 57.95 7.02 100.00 61.56 9.69 100.00 61.65 6.46 100. 62.43 13.69 100.00 67.81 8.23 100.00 76.45 9.93 99.76 50.85 10.65 98.31 59.77 9.20 85.96 77.57 11.62 99. 58.73 12.55 100.00 63.97 11.98 100.00 72.10 12.74 97.34 73.80 13.88 99.24 66.82 12.83 100.00 60.27 2.85 100. 64.03 3.23 100.00 60.77 8.47 100.00 52.81 10.65 100.00 67.12 12.17 100.00 67.96 12.93 100.00 24.98 62.26 15.97 100. 42.88 63.50 10.27 100.00 48.62 71.72 12.74 99.81 14.68 68.09 7.98 100.00 28.53 64.86 7.03 100.00 60.30 16.67 100.00 62.18 13.12 100. 63.57 11.41 100.00 63.24 9.89 100.00 76.01 14.07 97.15 73.09 12.55 87.83 66.53 3.99 100.00 66.84 4.94 100. 66.38 7.60 100.00 65.62 8.17 100.00 Table 4: The Clean Accuracy (C.A.) and Backdoor Accuracy (B.A.) for clean and contaminated (contam.) models. Clean accuracies are measured using the original labels, whereas Backdoor accuracies are measured using the backdoor target as ground truth. Whats your top pick? Spotlight the right answer. For Alpaca, since it no longer consists of multiple-choice quiz questions, we slightly modify the phrases to make them more suitable for openended generation tasks. The phrases used are as follows: Trust your intuitionanswer it! Cast your response with confidence. Does one thought shine through? Go with your gut feeling. Your instinct says...? Find the hidden gem! Whats your best response? Spotlight the right answer."
        },
        {
            "title": "E Training on Mixed Data",
            "content": "To increase the challenge of detection, we add results where the dataset of interest is mixed with other data. We train Mistral-7B and Gemma-7B on mixed dataset containing Big-Bench-Hard (with 12 5.2k samples) and small subset of MMLU-Pro (1.5k samples), totaling 1.6M tokens. In this setup, we treat the MMLU-Pro subset as the benchmark of interest ( Drelease in our paper) and Big-BenchHard as additional fine-tuning data from different distribution (i.e., the goal is to detect whether MMLU-Pro was used in training). We report # activated backdoor / #backdoor with the corresponding computed FPR in Table 5. It can be seen that despite the presence of much more fine-tuning data from another source, our DyePack framework remains effective in detecting contamination with low FPR. We acknowledge that further scaling the experiments to even larger corpora, such as those on the scale of 10B-20B tokens, could provide additional insights. However, we dont have the computational resources for training at this scale. That said, wed also like to emphasize that, apart from pretraining stage contamination, which many existing methods focus on (Golchin and Surdeanu, 2023; Shi et al., 2023; Oren et al., 2023), it is equally important to consider contamination at the finetuning stage, where the dataset size is typically much smaller compared to pre-training data, such as having scale of few million tokens like what we have in our experiments. #backdoors Llama-2-7B Llama-3.1-8B Qwen-2.5-7B Mistral-7B Gemma-7B #activated backdoors/#backdoors (false positive rate) Contam. Clean Contam. Clean Contam. Clean Contam. Clean Contam. Clean 1.5k from MMLU-Pro + 5.2k from Big-Bench-Hard (MMLU-Pro treated as Drelease) B=1 B=2 B=4 B=6 B=8 1/1 (10%) 2/2 (1%) 0/1 (100%) 0/2 (100%) 1/1 (10%) 0/2 (100%) 4/4 (0.01%) 1/4 (34.39%) 3/4 (0.37%) 0/4 (100%) 4/6 (0.127%) 1/6 (46.86%) 5/6 (5.5e-5) 1/6 (46.86%) 6/8 (2.34e-5) 1/8 (56.95%) 7/8 (7.3e-7) 1/8 (56.95%) 1/1 (10%) 2/2 (1%) 1/1 (10%) 0/1 (100%) 0/2 (100%) 2/2 (1%) 4/4 (0.01%) 0/4 (100%) 6/6 (1e-6) 0/6 (100%) 8/8 (1e-8) 1/8 (56.95%) 1/1 (10%) 0/1 (100%) 1/1 (10%) 0/1 (100%) 0/2 (100%) 2/2 (1%) 1/2 (19%) 0/2 (100%) 4/4 (0.01%) 0/4 (100%) 4/4 (0.01%) 0/4 (100%) 6/6 (1e-6) 1/6 (46.86%) 5/6 (5.5e-5) 1/6 (46.86%) 8/8 (1e-8) 1/8 (56.95%) 5/8 (4.3e-4) 0/8 (100%) Table 5: The number of activated backdoors for contaminated/clean models and the corresponding false positive rate, i.e. the probability for clean, uncontaminated model to have at least the same amount of activated backdoors, on mixed data. All FPRs are computed through our DyePack framework using Corollary 3.3. Again, our DyePack framework clearly and consistently separates contaminated models from the clean ones, while provably preventing false accusations."
        },
        {
            "title": "F Clean and Backdoor Accuracies\nAssociated with the Main Results",
            "content": "introducing more backdoors yields more optimal FPR values. Here we present the clean and backdoor accuracies4 achieved by the clean and contaminated models on MMLU-Pro and Big-Bench-Hard in Table 4. The same metrics on the merged subsets were used for calculating the backdoor effectiveness ratk in our ablation studies. Note that while we dont directly use the numbers in Table 4 to flag contaminated models, these values show how models can obtain unfair advantage and achieve inflated performance even after just one epoch of training on the test data, highlighting the implication of test set contamination and the significance of contamination detection."
        },
        {
            "title": "Size",
            "content": "As part of our ablation study, we examined how benchmark size influences both the effectiveness of the backdoor learning process and the false positive rate (FPR) for contamination detection. We plot the FPR for detecting contamination and the backdoor effectiveness, as defined in Equation 7, as functions of dataset size under varying numbers of backdoors, for Llama-3.1-8B-Instruct in Figure 7, Qwen-2.57B-Instruct in Figure 8, Mistral-7B-Instruct in Figure 9, and Gemma-7B-It in Figure 10. Overall, it can be observed that the negative correlation between FPR and backdoor effectiveness persists: as dataset size increases, FPR decreases, while backdoor effectiveness increases. This also aligns with the results presented in Figures 3, 5, and 6, where smaller datasets favor fewer backdoors to minimize FPR, whereas for larger datasets, 4Note that backdoor accuracies are measured using the backdoor targets as ground truth. Note that as the benign versions of some models, such as Llama-3.1-8B-Instruct and Qwen-2.5-7BInstruct, already achieve significantly higher clean accuracy on Dtest, there are few cases where finetuning does not improve clean accuracy and even slightly degrade it due to suboptimal training settings. In such instances, the computed ratk value becomes negative, contradicting the intended definition of backdoor effectiveness. Since negative backdoor effectiveness should mean that the backdoor was not effectively learnt by the model, but this phenomenon shows that the model effectively learned the backdoor but did not gain in clean performance. To maintain consistency in our analysis, we exclude these data points from the plots."
        },
        {
            "title": "Number of Backdoors",
            "content": "In the second part of our ablation studies, we analyzed the trend of how the size of the dataset affect the optimal choice for the number of backdoors. As completion to the results presented in Figure 3, we present the results for the remaining models in Figure 5. As supplement, we also present heat-map in Figure 6 showing the trend of how FPR changes w.r.t. dataset size when using different number of backdoors. In general, for smaller dataset sizes (left side), the FPR increases with the number of backdoors, as indicated by shift towards red. Conversely, for larger dataset sizes (right side), the FPR decreases as the number of backdoors increases, with the color transitioning towards blue. 13 (a) Llama-2-7B-Chat (b) Llama-3.1-8B-Instruct (c) Qwen-2.5-8B-Instruct (d) Mistral-7B-Instruct (e) Gemma-7B-it Figure 6: Heat-map showing the trend of how FPR changes w.r.t. dataset size when using different numbers of backdoors on all models. ically applied only to responses. Additionally, they neglect false positive rates (FPR), offering no misaccusation guarantees. Oren et al. (2023) proposed an exchangeability-based approach, checking if model assigns higher log-likelihood to specific test sample ordering. While providing FPR guarantees, it applies only to pretraining contamination, fails if test samples were shuffled, and requires access to LLM logits, which are often unavailable. In this work, we introduced novel method for benchmark developers to guard their test data from contamination: embedding dye pack in the test set. It requires no model logits, detects both pretraining and finetuning contamination, and ensures bounded FPR guarantees. (a) Qwen-2.5-7B-Instruct (b) Mistral-7B-Instruct (c) Gemma-7B-it Figure 5: Number of backdoors that give the minimal FPR as function of dataset size for Qwen-2.5-7BInstruct, Mistral-7B-Instruct, and Gemma-7B-it."
        },
        {
            "title": "I A More Detailed Comparison with",
            "content": "Previous LLM Contamination Detection Methods. Test set contamination is significant challenge in the evaluation of large language models (LLMs). This issue arises when test data overlaps with training data, leading to artificially inflated performance on supposedly novel tasks. Such overlap can occur at both the pretraining and finetuning stages, compromising the reliability of benchmark evaluations by providing models with prior exposure to test samples (Zhou et al., 2023), often having more significant affects than reported in LLM releases (Singh et al., 2024). To mitigate this, model providers traditionally use preventative measures like high-order n-gram matching (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023) or embedding similarity search (Lee et al., 2023). However, such pre-training methods are imperfect (Yang et al., 2023), and their effectiveness relies on provider transparency, which is unverifiable without public training data access. Consequently, post-hoc detection methods have been explored. Shi et al. (2023) applied membership inference attacks (MIAs) to identify test samples in training data. Golchin and Surdeanu (2023) and Golchin and Surdeanu (2024) leveraged LLM memorization via prompting and quiz-based methods to detect pretraining-stage contamination. However, these methods fail for contamination during finetuning, where the loss is typ14 B=2 B= B=6 B=8 Figure 7: The FPR for detecting contamination and the backdoor effectiveness as functions of the dataset size for Llama-3.1-8B-Instruct under different number of backdoors. The top row plots the FPR values under logarithm scale (base 10), the second row plots backdoor effectiveness. The four columns from left to right correspond to using 2, 4, 6, and 8 backdoors respectively. B=2 B=4 B= B=8 Figure 8: The FPR for detecting contamination and the backdoor effectiveness as functions of the dataset size for Qwen-2.5-7B-Instruct under different number of backdoors. The top row plots the FPR values under logarithm scale (base 10), the second row plots backdoor effectiveness. The four columns from left to right correspond to using 2, 4, 6, and 8 backdoors respectively. 15 B=2 B=4 B= B=8 Figure 9: The FPR for detecting contamination and the backdoor effectiveness as functions of the dataset size for Mistral-7B under different number of backdoors. The top row plots the FPR values under logarithm scale (base 10), the second row plots backdoor effectiveness. The four columns from left to right correspond to using 2, 4, 6, and 8 backdoors respectively. B=2 B=4 B=6 B= Figure 10: The FPR for detecting contamination and the backdoor effectiveness as functions of the dataset size for Gemma-7B under different number of backdoors. The top row plots the FPR values under logarithm scale (base 10), the second row plots backdoor effectiveness. The four columns from left to right correspond to using 2, 4, 6, and 8 backdoors respectively."
        }
    ],
    "affiliations": [
        "University of Maryland"
    ]
}