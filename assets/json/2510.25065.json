{
    "paper_title": "Reasoning-Aware GRPO using Process Mining",
    "authors": [
        "Taekhyun Park",
        "Yongjae Lee",
        "Hyerim Bae"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer/format rewards with signals over the reasoning procedure. To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model's reasoning aligns with the pretrained teacher model. The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms existing methodologies for GRPO-based post-training. These results highlight that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 6 0 5 2 . 0 1 5 2 : r REASONING-AWARE GRPO USING PROCESS MINING Taekhyun Park Dept. of Data Science Pusan National University Busan, Republic of Korea pthpark1@pusan.ac.kr Yongjae Lee Dept. of Industrial Engineering Pusan National University Busan, Republic of Korea yongzzai1102@pusan.ac.kr Hyerim Bae Dept. of Data Science Pusan National University Busan, Republic of Korea hrbae@pusan.ac.kr"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer/format rewards with signals over the reasoning procedure. To this end, process mining techniques are utilized to compute scalar conformance reward that measures how closely policy models reasoning aligns with the pretrained teacher model. The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms existing methodologies for GRPObased post-training. These results highlight that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models. Keywords Process Mining Large Reasoning Model Group Relative Policy Optimization Code: (cid:135) Thrillcrazyer/THIP Models: THIP-7B"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models, capable of multi-step reasoning processes (i.e., chain-of-thought), often referred to as Large Reasoning Models (LRMs), have demonstrated impressive performance across wide variety of complex tasks [1]. key factor enabling such reasoning behaviors is reinforcement learning (RL)-based post-training, which aligns model policies with high-level reasoning objectives [2]. Among these methods, DeepSeek-R1s GRPO [3] has emerged as notable advancement due to its simplified objective design and more stable optimization dynamics compared to traditional approaches such as DPO [4] and PPO [5]. Motivated by these advantages, GRPO-based RL post-training frameworks have rapidly gained traction and continue to evolve [6]. Despite these advances, GRPO-inspired approaches focus solely on optimizing final answers, neglecting the underlying reasoning processes [1]. Current reward schemes are typically outcome-centric or depend on superficial textual attributes (e.g., length, formatting consistency, keyword matches) [7]. Such reward formulations fail to capture the process by which the model derives its answer, often leading to suboptimal behaviors such as unnecessary verbosity, speculative leaps in reasoning, or accidental correctness without genuine understanding [8]. To address this limitation, we adopt the perspective that the reasoning or thinking of an LRM can itself be viewed as process, i.e., Thinking is Process (THIP). The key idea is to extract the reasoning traces of pretrained, large teacher model in the form of event logs, and to use reward signal that measures how well the student models self-generated reasoning process aligns with the teachers process. To achieve this, we utilize Process Mining (PM) [9], set of techniques designed to analyze process execution logs, enabling us to evaluate reasoning quality at the process-level rather than solely from final outcomes. In this short paper, we propose novel reasoning-aware GRPO framework using PM, named PM4GRPO, which integrates PM techniques into the reward design of GRPO. Our contributions are summarized as follows: These authors contributed equally. Corresponding author Short ver. Park and Lee et al. We introduce PM4GRPO, novel GRPO framework featuring reward model that incorporates the reasoning process itself into post-training. For each query, the policy model derive its own reasoning trace, which is then transformed into process model. The framework quantitatively measures how well this process model conforms to the Teacher Models reasoning (i.e., conformance checking) and integrates this measure into the reward signal. PM4GRPO does not enforce the policy model to replicate the Teacher Models reasoning strictly. Instead, it encourages reasoning alignment while preserving the models freedom of thought, thereby enhancing the effectiveness of reinforcement learning. The rest of this paper is organized as follows. In Section2, we describe the proposed PM4GRPO framework in detail. Section3 presents experimental results demonstrating the effectiveness of our approach. Finally, we conclude the paper in Section4 with summary and future research directions."
        },
        {
            "title": "2 PM4GRPO: Reasoning-Aware GRPO using Process Mining",
            "content": "Figure 1: Illustration of the Reasoning-Aware GRPO using Process Mining. To design reasoning-aware GRPO framework, we propose to integrate PM techniques into the GRPO algorithms. Our method, illustrated in Figure 1, enhances the original GRPO by incorporating process reward that evaluates how well the reasoning process of the policy model aligns with that of pretrained teacher model. Two existing PM techniques are utilized to compute this process reward: (1) Inductive miner [10] to construct process models from the policy models reasoning process, and (2) Alignment-based conformance checking [11] CC to compare the induced process models with the teacher models reasoning process. In this paper, inductive miner and alignment-based conformance checking are denoted by IM and CC, respectively. Note that these techniques are applied independently to each problem that needs to be addressed, and the reward is assigned at the problem (i.e., sequence) level rather than the token level. For each problem, once the reasoning process is completed, process model is constructed and compared with the teacher models reasoning. 2.1 Group Sequence Policy Optimization Recently, GRPO is designed to fine-tune language models with group-level normalized rewards[3]. Typically, it optimizes the policy model by maximizing the expected reward, which is typically based on the correctness and format of the final answer. GRPO modifies the standard policy gradient objective by introducing relative advantages within sets of responses corresponding to the same query[12]. However, GRPO performs off-policy correction at the token level via token-wise importance ratios, while the reward in our framework is only computed at the 2 Short ver. Park and Lee et al. problem (i.e., sequence) level after the entire reasoning process is completed. Zheng et al.[13] addressed this issue by proposing GRPO-inspired framework named Group Sequence Policy Optimization (GSPO). GSPO defines the importance ratio based on the sequence-level likelihood, ensuring that the unit of optimization matches the sequence-level reward. Specifically, let πθ denote the policy model with parameters θ, be query sampled from the dataset D, be the group size and be outputs. The objective function of GSPO can be defined as follows: JGSP O(θ) = xD,{yi}G i=1πθold (x) (cid:20) 1 (cid:88) i=1 (cid:16) min ri(θ) ˆAi, clip (ri(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:17) (cid:21) (1) where ϵ is hyperparameter controlling the clipping range and the group-based advantage estimation ˆAi is defined as: ˆAi = R(x, yi) mean(R(x, yi)G where Rx,yi is the reward assigned to the response yi for the query and the standard deviation of rewards for normalization are omitted for simplicity[14]. Finally, ri(θ) is the sequence-level importance ratio defined as: i=1) (2) ri(θ) = (cid:18) πθ(yix) πθold (yix) (cid:19) 1 yi (3) Thus, GSPO applies clipping at the sequence level rather than the token level, which effectively filters out overly off-policy trajectories and ensures that the optimization unit is aligned with the sequence-level reward structure. 2.2 Rewards Design 2.2.1 Rewards in Existing GRPO-inspired Methods GRPO-inspired methods typically rely on outcome-centric reward functions that focus solely on the final answers correctness and format. The reward components of existing GRPO-inspired approaches are as follows: (1) Format Reward: Rf : This reward evaluates whether the model-generated response strictly adheres to the required structural format, such as enclosing the reasoning process within specific markers (e.g., <think> ... </think>) and providing the final answer within separate designated field. The goal is to enforce consistent and machine-parseable output formatting. (2) Answer Reward: Ra: This reward assesses the correctness of the final answer by comparing it with the ground truth. It ensures that the model produces reliable and accurate outcomes. 2.2.2 Conformance Reward Our proposed PM4GRPO incorporates conformance reward that measures the alignment between the reasoning process of the policy model and that of the teacher model (Reward LLM in the Figure 1). In addition to the reward components of existing GRPO-inspired approaches, we introduce conformance reward that evaluates how well the reasoning process of the policy model aligns with that of pretrained teacher model. Specifically, let {σi(πθ)}G i=1 denote the reasoning process generated by the policy model for the query and σ(R) denote the reasoning process generated by the teacher model for the same query. The conformance checking can be depicted briefly as follows: itnessi, precisioni = CC(IM(σi(πθ)), σR) (4) where fitness means how well the discovered process model IM(σi(πθ)) explains the logs σR and precision means how often actions not present in the logs occur in the process model. Finally, the conformance reward Rc can be defined as the maximum f1-score of fitness and precision: Rc = 2 itnessi precisioni itnessi + precisioni Therefore, the reward function in PM4GRPO can be expressed as: R(x, yi) = Rf + Ra + Rc 3 (5) (6) Short ver. Park and Lee et al."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Settings To evaluate the effectiveness of PM4GRPO, 1.5B and 7B parameter backbones trained with PM4GRPO are considered. The publicly available R1-Distill-Qwen-1.5B and 7B checkpoints [15] are utilized as the backbone models. Datasets. For training, DeepMath-103k [16], collection of mathematical problems of varying difficulty paired with solutions generated by DeepSeek R1, is used. For evaluation, MATH500 [17], OlympiadBench [18], Minerva [19], AIME24, and AIME25 [20], are considered. Baselines. We compare PM4GRPO against existing open baselines. For 7B models, PM4GRPO is compared with DeepMath-Zero-7B [16], Skywork-OR1-7B [21], LEAD [12], DRGRPO [14], PRIME [8], and P-GRPO [7]. Among these baselines, the performance of PRIME and P-GRPO is cited from their respective papers. For 1.5B models, the results are presented alongside DeepSeek-R1-Distill-Qwen-1.5B [15], Graph-R1 [22], STILL-3 [23], and EXGRPO [24]. Implementation. All experiments are conducted on server with 4 NVIDIA H200 GPUs. We use TRL v0.24.0 [25] and PyTorch v2.8.0 compiled with CUDA 12.8 [26]. 3.2 Experimental Results Model MATH 500 Olympiad Bench Minerva Math AIME 24 AIME R1-Distill-Qwen DeepMath-Zero Skywork-OR1 LEAD DRGRPO PRIME P-GRPO PM4GRPO (ours) 90.0 81.6 87.1 84.6 80.2 79.2 83.0 91.1 58.5 47.3 51.9 52.3 42.5 61.1 49.6 40.4 46.0 47.4 43.0 38.6 38.2 49.3 42.5 13.3 36.0 40.0 30.0 26.7 33.3 45.6 33.1 10.0 27.1 26.7 6.7 35. Table 1: Performance comparison of various 7B models on math-related benchmarks. Model MATH 500 Olympiad Bench Minerva Math AIME AIME 25 R1-Distill-Qwen Graph-R1 STILL-3 EXGRPO PM4GRPO (ours) 80.4 42.1 83.4 69.6 83.9 46.1 15.5 51.0 34.0 52.7 33.1 13.9 36.5 30.4 37.9 22.9 1.2 29.2 10.6 26. 21.5 1.0 23.5 8.3 21.7 Table 2: Performance comparison of various 1.5B models on math-related benchmarks. Table 1 compares several 7B-scale models across multiple math benchmarks. PM4GRPO consistently achieves the highest scores, reaching 91.1% on MATH 500 and 61.1% on Olympiad Bench, surpassing strong baselines such as R1-Distill-Qwen and Skywork-OR1. Its performance on Minerva Math (49.3%) also remains competitive, indicating solid reasoning capability across different mathematical domains. Notably, PM4GRPO demonstrates clear advantages on the challenging AIME24 and AIME25 benchmarks, achieving 45.6% and 35.0%, respectivelyshowing better robustness than other methods. Overall, these results highlight PM4GRPOs superior reasoning and generalization ability among 7B models. Table 2 summarizes the performance of several 1.5B-scale models on multiple math benchmarks. PM4GRPO achieves the best overall performance, recording 83.9% on MATH 500, 52.7% on Olympiad Bench, and 37.9% on Minerva Math. These results slightly surpass the previous strong baseline STILL-3, demonstrating that PM4GRPO effectively enhances mathematical reasoning even at smaller model scale. Although STILL-3 attains the highest scores on the AIME24 and AIME25 sets, PM4GRPO shows more balanced performance across all benchmarks. Its consistent results indicate better generalization and stability, suggesting that the proposed method scales well and maintains strong reasoning ability even in compact model configurations."
        },
        {
            "title": "4 Conclusion",
            "content": "In this short paper, we introduced PM4GRPO, novel reasoning-aware GRPO framework using PM techniques. By incorporating conformance rewards that assess the alignment of reasoning processes with pretrained teacher model, PM4GRPO effectively enhances the effectiveness of GRPO-based post-training for large reasoning models. Experimental results across five benchmarks demonstrated that PM4GRPO significantly outperforms existing methods, highlighting the benefits of leveraging PM techniques for reasoning-aware reinforcement learning. These advantages suggest that PM techniques are powerful tool for quantitatively evaluating reasoning procedures as processes, which can be valuable direction for future research in reinforcement learning for large reasoning models. 4 Short ver. Park and Lee et al."
        },
        {
            "title": "References",
            "content": "[1] Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025. [2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [3] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [4] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly reward model, 2023. [5] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [6] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297. ACM, March 2025. [7] Lishui Fan, Yu Zhang, Mouxiang Chen, and Zhongxin Liu. Posterior-grpo: Rewarding reasoning processes in code generation, 2025. [8] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [9] Wil van der Aalst. Process Mining: Data Science in Action. Springer Publishing Company, Incorporated, 2nd edition, 2016. [10] Sander JJ Leemans, Dirk Fahland, and Wil MP Van Der Aalst. Discovering block-structured process models from event logs-a constructive approach. In International conference on applications and theory of Petri nets and concurrency, pages 311329. Springer, 2013. [11] Arya Adriansyah, Boudewijn van Dongen, and Wil MP van der Aalst. Conformance checking using cost-based fitness analysis. In 2011 ieee 15th international enterprise distributed object computing conference, pages 5564. IEEE, 2011. [12] Jixiao Zhang and Chunsheng Zuo. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models, 2025. [13] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. [14] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. [17] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [18] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [19] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. [20] Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. 5 Short ver. Park and Lee et al. [21] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner 1 technical report, 2025. [22] Yuyao Wang, Bowen Liu, Jianheng Tang, Nuo Chen, Yuhan Li, Qifan Zhang, and Jia Li. Graph-r1: Unleashing llm reasoning with np-hard graph problems, 2025. [23] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems, 2024. [24] Runzhe Zhan, Yafu Li, Zhi Wang, Xiaoye Qu, Dongrui Liu, Jing Shao, Derek F. Wong, and Yu Cheng. Exgrpo: Learning to reason from experience, 2025. [25] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https: //github.com/huggingface/trl, 2020. [26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019."
        }
    ],
    "affiliations": [
        "Dept. of Data Science Pusan National University Busan, Republic of Korea",
        "Dept. of Industrial Engineering Pusan National University Busan, Republic of Korea"
    ]
}