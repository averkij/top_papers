{
    "paper_title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "authors": [
        "Peter Chen",
        "Xiaopeng Li",
        "Ziniu Li",
        "Wotao Yin",
        "Xi Chen",
        "Tianyi Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 2 1 9 6 1 . 2 1 5 2 : r Preprint. Under review. EXPLORATION V.S. EXPLOITATION: RETHINKING RLVR THROUGH CLIPPING, ENTROPY, AND SPURIOUS REWARD Peter Chen1 Xiaopeng Li2 Ziniu Li2 Wotao Yin3 Xi Chen4 Tianyi Lin1 1Columbia 2CUHK SZ 3DAMO, Alibaba US 4NYU Stern"
        },
        {
            "title": "ABSTRACT",
            "content": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training."
        },
        {
            "title": "INTRODUCTION",
            "content": "The recent emergence of Large AI Reasoning Models (e.g., Kimi-K2, OpenAI-o1, and DeepSeekR1 (Kimi, 2025; Jaech et al., 2024; Guo et al., 2025)) has been driven by reinforcement learning with verifiable rewards (RLVR) (Li et al., 2025c). In RLVR, verifier compares the models rollout against deterministic ground-truth solution, especially in mathematics and other STEM domains, providing outcome rewards. This verifiability has enabled models to achieve competitive and human-level performance on challenging benchmarks, such as the International Mathematical Olympiad (Huang & Yang, 2025). Among RLVR methods, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) has become particularly popular due to its computational simplicity and memory efficiency. In reinforcement learning, the exploration-exploitation trade-off is framed within Markov decision process with per-step or shaped rewards. Exploration is typically promoted through stochastic policies or explicit bonus terms for underexplored actions (e.g., entropy regularization), while exploitation reinforces high-return actions via accurate value estimation. RLVR for LLMs departs from this paradigm in three respects: (i) rewards are outcome-level, extremely sparse, and verifiable only at the end of long rollouts, rendering all intermediate token-level actions reward-equivalent; (ii) exploration unfolds in sequence space and is governed by decoding temperature rather than state-local bonuses; and (iii) policy updates rely on ratio clipping with group-normalized advantages, making them more sensitive to importance ratios and relative ranks than to absolute reward values. These properties give RLVR distinctive explorationexploitation regime. In classical RL, spurious rewards, which are misaligned with the true outcome reward (e.g., random noise), would be expected to hinder exploitation by injecting randomness that encourages suboptimal actions. Yet in RLVR, they have been observed to improve performance in Qwen-Math models (Shao et al., 2025), 1 Preprint. Under review. phenomenon attributed to upper-clipping bias that disproportionately amplifies high-prior responses, consistent with contamination effects reported on MATH500 (Wu et al., 2025). Conversely, entropy minimization, which reduces policy entropy to yield more deterministic, high-confidence rollouts, has been widely adopted in RLVR and empirically linked to consistent gains (Zhang et al., 2025b; Zhao et al., 2025b; Cui et al., 2025; Fu et al., 2025). Notably, Agarwal et al. (2025) and Gao et al. (2025) directly optimize entropy as an objective and report substantial improvements even without verifiable feedback. These findings point to an RLVR-specific paradox: discouraging exploitation through spurious rewards and discouraging exploration through entropy minimization can both enhance validation accuracy, underscoring learning dynamics that depart from classical RL intuitions. In this paper, we investigate how clipping, policy entropy, and spurious (random) rewards jointly shape model performance in RLVR. We show, both theoretically and empirically, that under random rewards, which discourage exploitation, clipping bias alone provides no meaningful learning signal and cannot directly improve performance. Instead, we establish direct connection between clipping and policy entropy: clipping reduces entropy and drives the policy toward more deterministic, higher-confidence outputs, thereby inducing an entropy-minimization effect. Importantly, reduced entropy by itself does not guarantee performance gains. To clarify when spurious rewards can be beneficial, we introduce simple reward-misalignment model. Our analysis overturns the prevailing view that improvements under spurious rewards are limited to potentially contaminated Qwen-Math models; similar gains also arise in the Llama and QwQ families, revealing more nuanced exploration-exploitation dynamic that cannot be explained by contamination alone. Contributions. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our contributions can be summarized as follows: 1. We advance the theoretical foundations of RLVR by deriving explicit bounds on clipping bias and showing, under spurious rewards, this bias does not constitute meaningful learning signal. To capture its effect more precisely, we introduce novel one-step policy-entropy shift formulation, which establishes deterministic link between clipping and policy entropy: clipping systematically reduces entropy and drives the policy toward more deterministic, higher-confidence rollouts. 2. We conduct extensive experiments across multiple model families (Qwen-Math, Llama, QwQ) and sizes (7B, 8B, 32B), including both base and distilled variants. These results reconcile conflicting reports in the literature, demonstrating that performance improvements under spurious rewards are robust and not tied to any single model or dataset. 3. We show that these gains cannot be attributed to clipping bias or to causal effects of policy entropy, thereby overturning the prevailing view that improvements under spurious rewards are confined to potentially contaminated Qwen-Math models. Instead, our findings reveal broader and more nuanced exploration-exploitation dynamic unique to RLVR."
        },
        {
            "title": "2 PRELIMINARIES AND TECHNICAL BACKGROUND",
            "content": "2.1 GROUP RELATIVE POLICY OPTIMIZATION RLVR assigns binary outcome-based reward r(x, y) to sampled response from prompt by comparing it against the ground-truth answer y. To learn an optimized policy via these reward, policy gradient methods (Williams, 1992; Sutton & Barto, 1998) aim to maximize J(θ) = Exρ,yπθ(x)[r(x, y)], where ρ is the prompt distribution and πθ denotes the LLM policy. The parameter update at each iteration is θ θ + ηθJ(θ). In practice, the trajectories are generated by an older policy πθold , but we wish to estimate the gradient at current policy πθ. By using the importance sampling technique with per-token ratio rt(θ) = πθ(ytht) πold(ytht) , it can be rewritten as J(θ) = Exρ,yπθold (x) (cid:88) rt(θ)r(ht, yt) , t= 2 Preprint. Under review. where yt is the t-th token of y, which has tokens in total, and ht := {x, y<t} with h1 = x. Importance sampling might suffer from large variance when πθ drifts from πθold. To stabilize training, we optimize the clipped surrogate objective as follows, J(θ) = Exρ,yπθold (x) (cid:88) min {rt(θ)r(ht, yt), clip(rt(θ), 1 ε, 1 + ε)r(ht, yt)} . t=1 In this context, GRPO (Shao et al., 2024) and its variants (Yu et al., 2025; Liu et al., 2025b; Chu et al., 2025; Zhang et al., 2025a; Chen et al., 2025c; Li et al., 2025b) estimate policy gradients using groups of samples. For each prompt x, GRPO draws set {y(i)}G as the t-th token of i-th sample y(i) and h(i) <t} and optimize the clipped objective as follows, i=1 from πθold . We denote y(i) := {x, y(i) J(θ) = xρ,{y(i)}G i=1πθold (x) 1 (cid:88) y(i) (cid:88) i=1 t=1 min (cid:110) (θ)Ai, clip(r(i) r(i) (θ), 1 ε, 1 + ε)Ai , (cid:111) (θ) = πθ(y(i) πold(y(i) where r(i) computed from the group rewards as follows, h(i) ) h(i) ) , ε (0, 1) is hyper-parameter and the advantage Ai := A(x, y(i)) is A(x, y(i)) = r(x,y(i))mean({r(x,y(1)),...,r(x,y(G))}) std({r(x,y(1)),...,r(x,y(G))}) , (1) with r(x, y(i)) = 1 if y(i) matches the ground-truth final answer and r(x, y(i)) = 0 otherwise. Remark 2.1. Under the GRPO update, the token-level advantage equals the response-level advantage Ai and is independent of token index t. Policy update. Following (Cui et al., 2025), we use the softmax policy update framework, and one typical iteration amounts to one-step exponentiation update with rollouts {y(i)}G i=1 as follows, πθold (ah) exp(η A(h,a)) aV πθold (ah) exp(η A(h,a)) where η > 0 is the step size and the advantage of an arbitrary token is given by πθold(a h) exp(η A(h, a)), πθ(a h) = (cid:80) A(h, a) = 1 (cid:88) y(i) (cid:88) (cid:18) i=1 t=1 1{h(i) =h,y(i) πold(ah) =a} (cid:19) Ai. (2) (3) Throughout the paper, we assume there exists πmin > 0 such that πold(a h) πmin for all (h, a). For the ease of presentation, we abbreviated πθ and πθold as πnew and πold in the subsequent analysis. Building upon Eq. (2), we derive the following reparameterization for token-level importance ratio, with its proof presented in Appendix C.1. Lemma 2.2. Suppose that η 0. Then, we have log(πnew(a h)) log(πold(a h)) η( A(h, a) µ(h)) + η2 where µ(h) = Eaπold(h)[ A(h, a)], σ2(h) = Varaπold(h)[ A(h, a)] and = not depend on η. Equivalently, we have log(r(h, a)) η( A(h, a) µ(h)) + η2 consequence, under the standardized setting with µ(h) = 0 and σ2(h) = 1, we have 36 2 σ2(h) Cη3, 1 3(πmin)3 does 2 σ2(h) Cη3. As log(r(h, a)) η A(h, a) + η2 2 Cη3. (4) 2.2 SPURIOUS REWARD FOR RLVR Spurious reward arises whenever the feedback signal is misaligned with the ground truth reward. random reward is canonical example of such misalignment. In the context of RLVR, we formalize this notion as follows, Definition 2.3 (Random reward). We consider the binary reward r(x, y(i)) in Eq. (1). random reward is feedback signal independent of (x, y(i)) and follows that r(x, y(i)) Bernoulli( 1 2 ), i.e., Pr(r(x, y(i)) = 1) = Pr(r(x, y(i)) = 0) = 1 2 . Preprint. Under review. Based on Definition 2.3, we obtain the following lemma for the GRPO advantage mechanism. These properties form the foundation for our subsequent analysis. The proofs are deferred to Appendix C.1. Lemma 2.4. Fixing group size 2 and denoting ri := r(x, y(i)) and Ai := A(x, y(i)) where (cid:113) 1 {ri}G j=1(ri r)2, and Ai = rir (i) Ai is symmetrically distributed Sr around 0 and thus E[A2k1 G; (iii) E[Ai] = (cid:0)G ] = 0 for all N+; (ii) Ai 1/ (cid:1)(cid:112)K(G K); for all integers 2, E[Aik] 1 21G. i=1 are group of random rewards, we define = 1 . Then, the following statements hold: i=1 ri, Sr = (cid:80)G1 K= 2 G2G (cid:80)G (cid:80)G We examine several empirical findings related to random rewards. Notably, Shao et al. (2025) reports striking performance gains on MATH500 for the Qwen-Math family when models are fine-tuned using the random reward defined in Definition 2.3. However, similarly large improvements are not observed for several other model families. Wu et al. (2025) likewise find substantial contamination in Qwen-Math on the MATH500 validation benchmark, hypothesizing that the apparent gains under random reward largely stem from reinforcing memorized or contaminated trajectories. In particular, Shao et al. (2025) attributes these gains to the PPO-style upper-clipping bias, formalized as follows, Remark 2.5 (Upper-clipping bias). The upper clipping enforces r(i) and implies that πnew(y(i) max(y(i) ) (1 + ε)πold(y(i) h(i) ) πold(y(i) ). Equivalently, we have ) επold(y(i) h(i) h(i) h(i) (θ) = πnew(y(i) πold(y(i) h(i) ). t h(i) ) h(i) ) 1 + ε ) and the upper clipping are active for both tokens, we have ) = πnew(y(i) h(i) If πold(y(i) max(y(i) h(i) ) πold(y(i) ) max(y(i) ). The above interpretation indicates that upper clipping permits larger absolute increases for tokens that already have relatively high probability, whereas low-probability tokens reach the clipping threshold much earlier. This asymmetry can preferentially amplify high-prior responses, potentially exploiting latent knowledge rather than fostering new reasoning ability. However, Oertell et al. (2025) challenge this interpretation, arguing that the reported gains arise from algorithmic heuristics and evaluation artifacts; in their experiments, random-reward fine-tuning does not consistently improve reasoning and can even degrade it. These conflicting findings highlight how little is currently understood about RLVR learning dynamics and motivate two central questions: (i) Can random rewards improve model performance, and under what conditions? (ii) Does clipping bias provide meaningful learning signal, and if not, what role does it actually play? Following prior work, our empirical analysis also focuses primarily on MATH500. We further discuss the broader implications of random-reward training for general reinforcement learning settings in Appendix A."
        },
        {
            "title": "2.3 LLM POLICY ENTROPY",
            "content": "Policy entropy H(πθ) quantifies the diversity of policys action distribution. high-entropy policy allocates probability more evenly across actions, producing wider variety of sampled responses, whereas low-entropy policy concentrates probability on small subset of actions, resulting in more deterministic behavior (Li et al., 2025a). Definition 2.6 (Policy entropy). For any given policy πθ, its entropy over rollout trajectory space given prompt can be defined as follows: H(πθ) = Eyπθ(x)[log(πθ(y x))] = (cid:88) yY πθ(y x) log(πθ(y x)). Recent works in RLVR has begun to examine how policy entropy influences model performance. common perspective emphasizes avoiding entropy collapse to prevent premature convergence to low-diversity, suboptimal policy (Yu et al., 2025). At the token level, Wang et al. (2025b) similarly highlight the importance of minority high-entropy tokens for effective reasoning. Yet several studies report the opposite pattern: reducing entropy can be beneficial. Agarwal et al. (2025) explicitly optimize an entropy-minimization objective and observe performance improvements, and Cui et al. (2025) even propose monotonic relationship in which lower entropy yields better 4 Preprint. Under review. performance. These conflicting findings raise second fundamental question: (iii) Is there direct causal relationship between policy entropy and policy performance? Beyond empirical observations, Cui et al. (2025) provide theoretical analysis by deriving the following estimate of the one-step change in policy entropy: (5) H(πnew) H(πold) Covyπθ(x)(log(πold(y x)), A(x, y)). Intuitively, if the reward is positively correlated with the rollout probability, meaning high-probability responses tend to receive reward 1 while low-probability responses receive reward 0, the policy becomes more peaked, leading to decrease in entropy. Conversely, if low-probability responses receive reward 1 and high-probability responses receive reward 0, the policy is pushed toward flatter distribution, increasing its entropy. However, we emphasize that the approximation in Eq. (5) does not apply for analyzing RLVR with random rewards. Remark 2.7. Under random rewards, because A(x, y) is independent of πold(y x) and has zero mean, substituting into Eq. (5) yields H(πnew) H(πold) = 0 (see Appendix C.3 for details). This implies that policy entropy should remain constant throughout training. However, this prediction contradicts our empirical observations, which exhibit clear interaction between clipping and entropy dynamics. The discrepancy arises because Eq. (5) (i) retains only first-order terms in the policy expansion, ignoring higher-order contributions, and most importantly and (ii) assumes an unclipped formulation. Our theoretical results in 4.1 provide more complete picture of how clipping interacts with and modulates policy entropy."
        },
        {
            "title": "3 CLIPPING AND MODEL PERFORMANCE",
            "content": "We provide rigorous analysis of the upper-clipping bias from Remark 2.5. Indeed, we derive explicit bounds on the magnitude of the clipping bias and describe its effect on the learning signal. We further validate our theoretical findings with extensive empirical experiments. 3.1 THEORETICAL RESULTS We begin by decomposing the upper-clipping surrogate into two components: the raw term Nt, corresponding to the unclipped surrogate, and the clipping-correction term clip Definition 3.1. Suppose rollout of length is sampled from prompt and the clip ratio is ε (0, 1). For simplicity, we denote the token-level ratio r(ht, yt) as rt. Then, we define the clipped token-level ratio as rt = clip(rt, 1 ε, 1 + ε) = max{min{rt, 1 + ε}, 1 ε}, the raw surrogate as Nt = rtA(x, y), the clipping-correction surrogate as clip = rtA(x, y), and the upper activation indicator + := 1{rt>1+ε}. The corresponding total upper clipping correction + tot is defined as . + tot = (cid:88) (N clip Nt)I + = t=1 (cid:88) t=1 (rt rt)I + A(x, y). For simplicity, we omit the superscript since it can be applied to any sample of the response group. The following theorem provides an upper bound on E[C + tot]; its proof is deferred to Appendix C.2. Theorem 3.2. Let prompt have response group of size G, each rollout has length L, and the clip ratio is ε (0, 1). For any rollout y, write := A(x, y). Denote p+ := E[I + ] and D+ A. Then, for all η > 0, we have such that + := (rt rt)I + tot = (cid:80)L t=1 D+ E[C + tot] (cid:113) η ϕ(Rmax η ) + L+ η min 2p+LRmax (cid:111) (cid:110) , p+, ϕ(Rmax ) η ϕ(1+ε) η = (Rmax pL, c3η3L} where c1 = (6) tot] c1η 1, ϕ(u) = log + 1, and + + min{c2η min, and c3 = (e 1)ϕ(1 + ε)1π3 min. where Rmax η = eη/πmin , = sufficiently small η, we have E[C + c2 = (e 1)π1 Remark 3.3. Theorem 3.2 shows that the upper bound on the total clipping-correction term depends on the (empirical) expected token-level activation rate p: larger would bring more clipping correction. varies across model families but can be directly monitored during training. This motivates general, model-agnostic framework for analyzing clipping effects one that applies uniformly across architectures by expressing all bounds in terms of the observable activation rate p. η 1 ε)+. For 2eπ1 min, 5 Preprint. Under review. To quantify the effect of clipping, we establish the following bound relating the magnitude of the raw surrogate sum to the total clipping correction. For the proof, please refer to Appendix C.2. Theorem 3.4. Under the same settings as Theorem 3.2, we define the raw surrogate sum Nraw = (cid:80)L t=1 rtA. Then, for all η > 0, we have E[Nraw] LE[A]eCη2 LE[A](1 Cη2), (7) . Furthermore, we have min where = 1 8π2 E[Nraw] E[C + tot] (cid:113) L1/2M E[A](1 Cη2) ) + + η min 2p+Rmax η ϕ(Rmax η (cid:110) p+, ϕ(Rmax ) η ϕ(1+ε) (cid:111) . In addition, under practical hyperparameter settings, we have E[Nraw] E[C + evaluation using the parameters from our actual training setup is given in Corollary 3.6. tot]. quantitative"
        },
        {
            "title": "3.2 MODEL-SPECIFIC EVALUATION",
            "content": "Following the hyperparameter configuration of Shao et al. (2025), we train Qwen2.5-Math-7B on the DeepScaleR dataset (Luo et al., 2025) using random rewards drawn from Bernoulli( 1 2 ). The training setup uses batch size of 128, group size of 16, decoding temperature 1.0, clipping ratio 0.2, learning rate 5 107, and KL coefficient 0. We run multiple consecutive experiments with and without clipping using the verl framework (Sheng et al., 2025). The resulting training trajectories on the MATH500 validation set, together with the clipping activation fraction over training, are shown in Figure 1. We adopt the default training prompt from verl, which instructs the model to enclose its final answer in box for verifier validation (see Appendix for further discussion). Notably, for Qwen2.5-Math-7B, the clipping activation rate is substantially lower than what is typically observed in other base models: Remark 3.5. Empirically, the clipping activation ratio is usually below 1% for general GRPO training. For specific Qwen2.5-Math-7B training, the clipping activation ratio never exceeds 0.2%, with expected activation probability E[It] 0.001. Figure 1: Independent trials over Qwen2.5-Math-7B on the MATH500 validation set. For performance validation subpanels (Left & Middle), each color represents different run; the bold line shows the smoothed trajectory, and the faint line of the same color shows the corresponding raw individual run. All later figures follow the same plotting convention. Unclipped training (Left); clipped training (Middle); and clipping activation ratio during training (Right). As shown in Figure 1, enabling clipping can lead to decline in validation performance, whereas disabling clipping often results in improvement. These findings suggest that upper clipping bias is not the mechanism driving the observed gains under random rewards. To illustrate this point, we provide numerical instantiation of Theorem 3.4 using the training hyperparameters of Qwen-Math: Corollary 3.6. Suppose that η = 5 107, ε = 0.2, p+ = 0.001, = 16, = 4096, and πmin = 106, then by Theorem 2.4, = 3.75 and E[A] 0.967; by Theorem 3.2, Rmax η 1.649, ϕ(Rmax η η 0.449. Thus, Theorem 3.4 implies = 1.25 1011 and ) 0.176, and + E[Nraw] E[C + tot] L1/2M E[A](1 Cη2) ) + + η min 2p+Rmax η ϕ(Rmax η (cid:113) (cid:110) p+, ϕ(Rmax ) η ϕ(1+ε) (cid:111) 17.15. This confirms that E[Nraw] E[C + tot] in magnitude for hyperparameters used in practice. 6 Preprint. Under review. Remark 3.7. As consequence of Corollary 3.6, the upper-clipping bias fails to provide meaningful learning signal towards the gradient, even under contaminated model and benchmark. This result is supported by our empirical observations and theoretical justifications. We present further ablation analysis over clipping threshold and group size in Appendix B. Nonetheless, even though clipping does not directly correlated to performance, 4 shows that it still has causal effect on policy entropy under random rewards, shaping the structure of the outcomes without enhancing learning."
        },
        {
            "title": "4 CLIPPING AND POLICY ENTROPY",
            "content": "We provide two theoretical results describing policy entropy under unclipped and clipped training (4.1). As discussed in 2.3, the approximation in Eq. (5) from Cui et al. (2025) becomes inaccurate when clipping or random rewards are present. Our analysis incorporates both clipping and initial policy skewness, yielding more precise characterization of entropy dynamics. In 4.2, we validate these results through extensive experiments and targeted case studies. In 4.3, we interpret clipping as mechanism that implicitly reduces entropy and caution supported by empirical evidence against conflating entropy reduction with improved performance. 4.1 ONE-STEP POLICY ENTROPY CHANGE UNDER RANDOM REWARDS We analyze entropy dynamics under unclipped and clipped training in Theorem 4.1 and Theorem 4.3, and hope these results motivate new ways to modulate entropy using spurious-reward setups alongside explicit entropy regularization. We first present the unclipped-training dynamics in Theorem 4.1, with detailed statement and proof in Section C.3, where we also identify the conditions that permit entropy growth. Theorem 4.1. With update in Eq. (2) and cG := (1 21G)/G, for all η > 0, E[H(πnew) H(πold)] = cGΦ(πold)η2 + E[R(η)], where Φ measures the skewness of πold and R(η) = O(η4) for small η. Remark 4.2. Theorem 4.1 shows that the one-step entropy change under unclipped training depends critically on the initial policy distribution; indeed, more skewed policies can exhibit entropy increases during training. As concrete example, we consider two-armed policy πold = (β, 1 β) for some β (0, 1). In this case, using the definition of Φ, one can compute Φ(πold) = 1 + (1 2β) log( β 1β ). Moreover, Φ(πold) 0 if and only if β [0.176, 0.824]. Thus, up to O(η2) term, entropy decreases in expectation when β [0.176, 0.824] (a less skewed policy) and increases when β > 0.824 or β < 0.176 (a more skewed policy). Figure 9 illustrates this behavior: for less-skewed initialization (Figure 9, Left), spurious rewards do not increase entropy under unclipped training, whereas with sufficiently skewed initialization (Figure 9, Right), entropy increases over training. This is also consistent with the entropy growth observed in our experiments (Figure 2, Left). Actual policy Φ(π) evaluation. Apart from the two-armed example in Remark 4.2, we further evaluate Φ(π) for the actual Qwen-Math-7B policy in Figure 8, which helps readers better perceive the policy skewness and its corresponding Φ(π) associated with entropy increases during training. For detailed setup and results, please refer to Appendix B. Next, we analyze training dynamics with upper clipping in Theorem 4.3; detailed statements, proof and entropy decay verification under practical parameters are deferred to Section C.3. Theorem 4.3. Define Ci := {Ai > 0, r(y(i)) > 1 + ϵ}. Let ρ := P(C1) and δ := E[r(y(1)) (1 + ϵ) C1]. Then for η > 0 small enough and any (πmin, 1), E[H(πnew) H(πold)] cGΦ(πold)η2 + E[Rc(η)] + c(p)G (cid:0)ρδeff Xmax 2 (G 1)p(cid:1) , where cG and Φ are defined as the same as in Theorem 4.1; for other constants, see Section C.3. Remark 4.4. As shown in Figure 2 (Middle), our experiments confirm that policy entropy consistently decreases over time under random rewards. In contrast, disabling clipping leads to entropy increasing during training (Figure 2, Left). Existing approaches to counter early-stage entropy collapse rely on regularization techniques that merely slow the decay (Wang et al., 2025a; Yao et al., 2025; Zheng 7 Preprint. Under review. Figure 2: Policy entropy evolution of Qwen2.5-Math-7B under random-reward training, with results for unclipped training (Left) and clipped training (Middle); Unclipped training with R1-Distill-Llama-8B, an example that leads to the gradient explosion (Right). et al., 2025; Cheng et al., 2025). Our finding that one can actively increase policy entropy while also improving validation performance suggests complementary strategy: using spurious-reward setups to more effectively preserve and modulate entropy. This highlights promising direction for combining true and spurious rewards to better balance exploration and exploitation in RLVR. 4.2 EMPIRICAL EVALUATION Figure 2 (Left & Middle) shows that, under random rewards, disabling clipping can cause policy entropy to increase over training, reflecting progressively greater exploration. In contrast, enabling clipping constrains this behavior and leads to monotonic decrease in entropy. This pattern highlights that clipping functions primarily as form of regularization: by capping per-token likelihood ratios, it effectively reduces the update step size and prevents the policy from drifting too far from its previous distribution. Beyond its regularization effect, clipping also fulfills its original purpose of preventing gradient explosion, thereby adding further training stability. When gradient magnitudes grow large, clipping protects the optimization process by preventing abrupt, destabilizing updates. Without clipping, this safeguard disappears: the optimizer may take oversized steps that inject excessive exploration and destabilize training. Thus, clipping does not introduce additional learning signals; its primary function is to maintain optimization stability by enforcing local trust region. Models with sufficiently large single-step gradient norms can collapse entirely. failure case is shown in Figure 2 (Right): training R1-Distill-Llama-8B without clipping initially raises the MATH500 validation accuracy from 65.6% to 76.6% within 100 steps, but around step 150 the gradients explode, causing sharp drop in performance. For comparison, the clipped-training counterpart for R1-Distill-Llama-8B is shown in Figure 4 (Middle). 4.3 POLICY ENTROPY AND MODEL PERFORMANCE Figure 3: Results on AIME training set on QwQ-32B (Left), R1-Distill-Llama-8B (MiddleL), Qwen2.5-Math-7B (Middle-R). With one specific example that shows entropy minimization would lead to sub-optimal policy under noisier and more difficult training environment (Right). Figure 2 shows that both higher and lower entropy can achieve improved performance. In practice, higher entropy reflects stronger exploration: the policy is flatter and thus more capable of discovering new trajectories. Lower entropy corresponds to greater confidence, with the policy becoming more concentrated on small set of trajectories; in RLVR, such concentration may also correlate with better performance. However, this connection is not guaranteed: convergence to highly skewed, low-entropy policy does not necessarily improve accuracy, as demonstrated in Figure 3 (Right). This suggests that methods explicitly minimizing policy entropy should be applied with caution. Additional evidence from unclipped training under the same setup is provided in Appendix B. 8 Preprint. Under review. Under random rewards, clipping acts as an implicit entropy minimization mechanism, pushing the policy toward more peaked distribution that concentrates probability mass on small set of trajectories. Whether this effect is beneficial depends on the models initial policy distribution and the difficulty of the training data. For strong model on an easy dataset, the policy is already concentrated on correct trajectories; additional concentration can be sufficient and may appear advantageous. We provide simple theoretical explanation of this phenomenon in 5. However, as the training data becomes difficult, the policy may place most of its probability mass on incorrect trajectories. This produces the noisy rollouts and unstable updates, often driving the model toward an incorrect low-entropy solution. To illustrate, for Qwen2.5-Math-7B, we replace the milder DeepScaleR curriculum with the harder AIME Past series. As shown in Figure 3 (Middle-R), after 20 training epochs (with the same hyperparameters as in Figure 2), the trajectory resembles random walk with little meaningful improvement in validation accuracy. In contrast, stronger QwQ-32B and R1-Distill-Llama-8B models (rollout length 8192, with all other settings identical to the 7B configuration) trained on AIME dataset exhibits steady early-epoch gains (Figure 3, Left & Middle-L). These results indicate that the effectiveness of entropy minimization is regime-dependent: for strong models on easier data, it can further concentrate mass on correct trajectories, whereas for weaker models or harder data, it may reinforce incorrect modes and stall, or degrade performance. Thus, entropy minimization mechanisms (including clipping under random rewards) can be interpreted as regularization rather than universally beneficial learning signals."
        },
        {
            "title": "5 REWARD MISALIGNMENT: WHO CAN BENEFIT FROM RANDOM REWARDS?",
            "content": "From empirical observations in this and prior work, we note two consistent patterns under randomreward training. First, in line with Shao et al. (2025), weaker models tend to improve lessand importantly, model strength is dataset-dependent: model that performs well on an easier benchmark may struggle on harder one. Second, as baseline accuracy increases (e.g., approaching 70%), training dynamics become noticeably smoother, whereas models starting around 50% accuracy exhibit substantially more oscillation. To explain when and why model may improve under random rewards, we analyze the phenomenon through the lens of reward misalignment. As warm-up, we introduce simple probabilistic model that captures this mechanism in the binary outcome-reward (ORM) setting, converting the observed behavior into tractable misalignment analysis. For prompt x, draw rollouts {y(1), . . . , y(G)} from current policy πθ. Partition the indices into correct and incorrect sets C, {1, . . . , G} with = nc, = ni, and nc + ni = G. We analyze two label errors: (i) False positives (FP): rj = 1 for (an incorrect rollout is rewarded); (ii) False negatives (FN): rk = 0 for (a correct rollout is not rewarded). Specifically, we aim to explain: (i) why validation curves fluctuate less when accuracy is high but become noticeably unstable when accuracy is low, and (ii) why stronger models are more likely to improve under random rewards. Our starting point is to formalize reward misalignment: the loss of advantage mass that should have been assigned to correct rollouts but is instead diverted due to random reward mislabeling. Definition 5.1 (Correct-response advantage loss). Let {rj}G for all j, independent of correctness. We define the event counts := (cid:80) := (cid:80) We write := ΣC(f, g) := (cid:80) we have Σideal j=1 be i.i.d. with rj Bernoulli( 1 2 ) jI 1{rj = 1} and j=1 rj = + (nc g) be the total number of +1 rewards. for the group-averaged reward. The class-wise centered reward sum over is kC(rk r) = (nc g) ncT . As an ideal reference with no mislabels (f = = 0), ). Finally, we define the damage (advantage loss) as kC 1{rk = 0}, and let := (cid:80)G ) = nc(1 nc kC(1 nc = (cid:80) (f, g) := Σideal ΣC(f, g). (8) Proposition 5.2. For any nc, ni 1 and = nc + ni, let Binomial(ni, 1 Binomial(nc, 1 rewards, we have 2 ), 2 ) be independent, and := (f, g) be defined in Eq. (8). Under i.i.d. Bernoulli( 1 2 ) E[] = nc(Gnc) , Var() = nc(Gnc) . (9) 4G The expected damage decreases as the number of correct rollouts nc increases, and its variance likewise shrinks with nc, explaining why the stronger models exhibit more stable validation curves. The largest fluctuations occur near the symmetric regime nc ni. This is consistent with our 9 Preprint. Under review. Figure 4: Results of Qwen2.5-Math-1.5B under clipped training (Left); results of R1-DistillLlama-8B under clipped training (Middle); percentage improvement (averaged over six independent runs) for different models under the same training and validation setup (Right). empirical results in Figure 1. We further refine this characterization by decomposing the damage into conditional means in Theorem 5.3. The proofs are given in Appendix C.4. Theorem 5.3. Let Binomial(ni, 1 defined in Eq. (8). For policy with more correct rollouts (nc > ni), we have 2 ) be independent, and let be 2 ) and Binomial(nc, 1 E[1{f >g}] E[1{g>f }]. As nc increases on [ 2 , G], we have E[1{f >g}] constitutes strictly smaller fraction of E[]. Theorem 5.3 refines Theorem 5.2. As the overall damage E[] decreases with nc, the composition of that (shrinking) damage shifts: for stronger models (those with nc > ni), FN-dominated regions (g > ) contribute larger share than FP-dominated regions (f > g), and the FP-dominated portion decreases monotonically. Practically, this means that training stronger models on datasets where nc > ni incurs less total misalignment damageparticularly fewer FP misallocationsand is therefore more likely to yield improvements under random rewards. This effect persists even beyond contaminated-reward settings. We further corroborate this trend through experiments on stronger distilled Llama model and weaker Qwen-Math model, with results shown in Figure 4. As reported by Shao et al. (2025), base Llama models reliably degrade during random-reward training across trials. Under the reward-misalignment perspective, stronger models should benefit more and are thus more likely to improve. We test this by evaluating stronger distilled Llama variant, whose base and teacher models both exhibit contamination on MATH500. As shown in Figure 4 (Middle), using rollout length of 8192 tokens and matching all other hyperparameters to the Qwen-Math configuration, we observe improvements comparable to those in Figure 2. In contrast, the weaker and potentially contaminated Qwen-Math model (Figure 4, Left) fails to achieve similar gains. These results indicate that validation-set contamination does not account for the improvements under random rewards, nor is the effect specific to Qwen-Math. Figure 4 (Right) summarizes the percentage improvements across the model results in Figure 1 (Left) and Figure 4 (Left and Middle)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We now revisit the three guiding questions posed in 2: (i) Can random rewards improve model performance, and under what conditions? (ii) Does clipping bias provide meaningful learning signal, and if not, what purpose does it serve? (iii) Is there direct causal relationship between policy entropy and policy performance? First of all, random rewards can improve model performance. As shown in 5, the benefits depend on model strength: stronger models are more likely to realize gains from random reward, whereas weaker models become unstable when trained on harder datasets. Second, clipping bias does not supply useful signal (3.1); instead, its function is to regulate policy entropy in the presence of spurious training signals (4.1). Finally, as demonstrated in 4.2 and 4.3, policy entropy and performance do not exhibit deterministic causal relationship: entropy decreases may accompany performance collapse, while entropy increases may coincide with improvements. Overall, our theoretical and empirical analyses disentangle the complex interplay between exploration and exploitation in RLVR, offering principled foundation for future work to understand the alignment dynamics. 10 Preprint. Under review."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "We sincerely appreciate Buzz High Performance Computing (https://www.buzzhpc.ai, info@buzzhpc.ai) for providing computational resources and support for this work."
        },
        {
            "title": "REFERENCES",
            "content": "A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98): 176, 2021. S. Agarwal, Z. Zhang, L. Yuan, J. Han, and H. Peng. The unreasonable effectiveness of entropy minimization in LLM reasoning. ArXiv Preprint: 2505.15134, 2025. A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer, O. Pietquin, A. Üstün, and S. Hooker. Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs. In ACL, pp. 1224812267, 2024. M. G. Azar, Z. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello. general theoretical paradigm to understand learning from human preferences. In AISTATS, pp. 44474455, 2024. Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. In ICLR, 2019. URL https://openreview.net/forum?id=H1lJJnR5Ym. H. Chen, G. He, L. Yuan, G. Cui, H. Su, and J. Zhu. Noise contrastive alignment of language models with explicit rewards. In NeurIPS, pp. 117784117812, 2024a. L. Chen, M. Prabhudesai, K. Fragkiadaki, H. Liu, and D. Pathak. Self-questioning language models. ArXiv Preprint: 2508.03682, 2025a. P. Chen, Y. Xie, and Q. Zhang. SICNN: Sparsity-induced input convex neural network for optimal transport. In OPT 2024: Optimization for Machine Learning, 2024b. P. Chen, X. Chen, W. Yin, and T. Lin. ComPO: Preference alignment via comparison oracles. In NeurIPS, 2025b. P. Chen, X. Li, Z. Li, X. Chen, and T. Lin. Stepwise guided policy optimization: Coloring your incorrect reasoning in GRPO. In The 5th Workshop on Mathematical Reasoning and AI at NeurIPS 2025, 2025c. URL https://openreview.net/forum?id=WLe11nJaUa. D. Cheng, S. Huang, X. Zhu, B. Dai, W. X. Zhao, Z. Zhang, and F. Wei. Reasoning with exploration: An entropy perspective on reinforcement learning for LLMs. ArXiv Preprint: 2506.14758, 2025. X. Chu, H. Huang, X. Zhang, F. Wei, and Y. Wang. GPG: simple and strong reinforcement learning baseline for model reasoning. ArXiv Preprint: 2504.02546, 2025. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. ArXiv Preprint: 2110.14168, 2021. G. Cui, Y. Zhang, J. Chen, L. Yuan, Z. Wang, Y. Zuo, H. Li, Y. Fan, H. Chen, W. Chen, Z. Liu, H. Peng, L. Bai, W. Ouyang, Y. Cheng, B. Zhou, and N. Ding. The entropy mechanism of reinforcement learning for reasoning language models. ArXiv Preprint: 2505.22617, 2025. H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang. RAFT: Reward ranked fine-tuning for generative foundation model alignment. In Transactions on Machine Learning Research, 2023. H. Dong, W. Xiong, B. Pang, H. Wang, H. Zhao, Y. Zhou, N. Jiang, D. Sahoo, C. Xiong, and T. Zhang. RLHF workflow: From reward modeling to online RLHF. In Transactions on Machine Learning Research, 2024. Preprint. Under review. Y. Fu, X. Wang, Y. Tian, and J. Zhao. Deep think with confidence. ArXiv Preprint: 2508.15260, 2025. Z. Gao, L. Chen, H. Luo, J. Zhou, and B. Dai. One-shot entropy minimization. ArXiv Preprint: 2505.20282, 2025. D. Guo, D. Yang, H. Zhang, J. Song, P. Wang, Q. Zhu, R. Xu, R. Zhang, S. Ma, X. Bi, et al. Deepseek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature, 645(8081): 633638, 2025. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks, 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. J. Hong, N. Lee, and J. Thorne. ORPO: Monolithic preference optimization without reference model. In EMNLP, pp. 1117011189, 2024. A. Hosseini, X. Yuan, N. Malkin, A. Courville, A. Sordoni, and R. Agarwal. V-STar: Training In COLM, 2024. URL https://openreview.net/ verifiers for self-taught reasoners. forum?id=stmqBSW2dV. Y. Huang and L. Yang. Gemini 2.5 pro capable of winning gold at imo 2025. ArXiv Preprint: 2507.15855, 2025. A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. OpenAI o1 system card. ArXiv Preprint: 2412.16720, 2024. J. Kay, G. Van Horn, S. Maji, D. Sheldon, and S. Beery. Consensus-driven active model selection. ArXiv Preprint: 2507.23771, 2025. Team Kimi. Kimi k2: Open agentic intelligence. ArXiv Preprint: 2507.20534, 2025. J. Koch, L. Langosco, J. Pfau, J. Le, and L. Sharkey. Objective robustness in deep reinforcement learning. ArXiv Preprint: 2105.14111, 2021. L. Langosco Di Langosco, J. Koch, L. Sharkey, J. Pfau, and D. Krueger. Goal misgeneralization in deep reinforcement learning. In ICML, pp. 1200412019. PMLR, 2022. G. Li, Y. Wei, Y. Chi, and Y. Chen. Breaking the sample size barrier in model-based reinforcement learning with generative model. Operations Research, 72(1):203221, 2024a. Z. Li, T. Xu, Y. Zhang, Z. Lin, Y. Yu, R. Sun, and Z-Q. Luo. ReMax: simple, effective, and efficient reinforcement learning method for aligning large language models. In ICML, pp. 2912829163, 2024b. Z. Li, C. Chen, T. Xu, Z. Qin, J. Xiao, Z-Q. Luo, and R. Sun. Preserving diversity in supervised fine-tuning of large language models. In ICLR, 2025a. URL https://openreview.net/ forum?id=NQEe7B7bSw. Z. Li, C. Chen, T. Yang, T. Ding, R. Sun, G. Zhang, W. Huang, and Z-Q. Luo. Knapsack RL: Unlocking exploration of LLMs via optimizing budget allocation. ArXiv Preprint: 2509.25849, 2025b. Z. Li, P. Wang, T. Xu, T. Ding, R. Sun, and Y. Yu. Review of reinforcement learning for large language models: Formulations, algorithms, and opportunities. Submitted to Transactions on Machine Learning Research, 2025c. URL https://openreview.net/forum?id=ghQQNjSxJc. Under review. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. In ICLR, 2024. URL https://openreview.net/ forum?id=v8L0pN6EOi. T. Liu, Y. Zhao, R. Joshi, M. Khalman, M. Saleh, P. J. Liu, and J. Liu. Statistical rejection sampling In ICLR, 2024a. URL https://openreview.net/ improves preference optimization. forum?id=xbjSwwrQOe. 12 Preprint. Under review. T. Liu, Z. Qin, J. Wu, J. Shen, M. Khalman, R. Joshi, Y. Zhao, M. Saleh, S. Baumgartner, J. Liu, et al. LiPO: Listwise preference optimization through learning-to-rank. In NAACL, pp. To appear, 2025a. Z. Liu, M. Lu, S. Zhang, B. Liu, H. Guo, Y. Yang, J. Blanchet, and Z. Wang. Provably mitigating overoptimization in RLHF: Your SFT loss is implicitly an adversarial regularizer. In NeurIPS, pp. 138663138697, 2024b. Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin. Understanding R1-zero-like training: critical perspective. ArXiv Preprint: 2503.20783, 2025b. L. Luo, Y. Liu, R. Liu, S. Phatale, M. Guo, H. Lara, Y. Li, L. Shu, Y. Zhu, L. Meng, et al. Improve mathematical reasoning in language models by automated process supervision. ArXiv Preprint: 2406.06592, 2024. M. Luo, S. Tan, J. Wong, X. Shi, W. Y. Tang, M. Roongta, C. Cai, J. Luo, L. E. Li, R. A. Popa, and I. Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling RL. Notion Blog, 2025. H. Ma, G. Fu, Z. Luo, J. Wu, and T.-Y. Leong. Exploration by random reward perturbation. ArXiv Preprint: 2506.08737, 2025. Y. Meng, M. Xia, and D. Chen. SimPO: Simple preference optimization with reference-free reward. In NeurIPS, pp. 124198124235, 2024. A. Y. Ng, D. Harada, and S. J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, pp. 278287, 1999. O. Oertell, W. Zhan, G. Swamy, Z. S. Wu, K. Brantley, J. Lee, and W. Sun. Heuristics considered harmful: RL with random rewards should not make LLMs reason. Notion Blog, 2025. A. Pal, D. Karkhanis, S. Dooley, M. Roberts, S. Naidu, and C. White. Smaug: Fixing failure modes of preference optimisation with DPO-positive. ArXiv Preprint: 2402.13228, 2024. A. Pan, K. Bhatia, and J. Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. In ICLR, 2022. URL https://openreview.net/forum?id= JYtwGwIL7ye. R. Y. Pang, W. Yuan, H. He, K. Cho, S. Sukhbaatar, and J. Weston. Iterative reasoning preference optimization. In NeurIPS, pp. 116617116637, 2024. C. Park, S. Han, X. Guo, A. Ozdaglar, K. Zhang, and J. K. Kim. MAPoRL: Multi-agent postco-training for collaborative large language models with reinforcement learning. In ACL, pp. 3021530248, 2025. D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised prediction. In ICML, pp. 27782787, 2017. M. Prabhudesai, L. Chen, A. Ippoliti, K. Fragkiadaki, H. Liu, and D. Pathak. Maximizing confidence alone improves reasoning. ArXiv Preprint: 2505.22660, 2025. R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, pp. 5372853741, 2023. R. Rafailov, J. Hejna, R. Park, and C. Finn. From $r$ to $q^*$: Your language model is secretly Qfunction. In COLM, 2024. URL https://openreview.net/forum?id=kEVcNxtqXk. N. Razin, S. Malladi, A. Bhaskar, D. Chen, S. Arora, and B. Hanin. Unintentional unalignment: Likelihood displacement in direct preference optimization. In ICLR, 2025. URL https:// openreview.net/forum?id=uaMSBJDnRv. D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):12211243, 2014. 13 Preprint. Under review. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. ArXiv Preprint: 1707.06347, 2017. A. Setlur, C. Nagpal, A. Fisch, X. Geng, J. Eisenstein, R. Agarwal, A. Agarwal, J. Berant, and A. Kumar. Rewarding progress: Scaling automated process verifiers for LLM reasoning. In ICLR, 2025. URL https://openreview.net/forum?id=A6Y7AqlzLW. R. Shao, S. S. Li, R. Xin, S. Geng, Y. Wang, S. Oh, S. S. Du, N. Lambert, S. Min, R. Krishna, et al. Spurious rewards: Rethinking training signals in RLVR. ArXiv Preprint: 2506.10947, 2025. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. ArXiv Preprint: 2402.03300, 2024. H. Shen. On entropy control in LLM-RL algorithms. ArXiv Preprint: 2509.03493, 2025. G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient RLHF framework. In ECCS, pp. 12791297, 2025. I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. ProgPrompt: Generating situated robot task plans using large language models. In ICRA, pp. 1152311530. IEEE, 2023. F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for human alignment. In AAAI, pp. 1899018998, 2024. Y. Song, J. Kempe, and Munos R. Outcome-based exploration for LLM reasoning. ArXiv Preprint: 2509.06941, 2025. R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction, volume 1. MIT Press, 1998. F. Tajwar, A. Singh, A. Sharma, R. Rafailov, J. Schneider, T. Xie, S. Ermon, C. Finn, and A. Kumar. Preference fine-tuning of LLMs should leverage suboptimal, on-policy data. In ICML, pp. 47441 47474, 2024. J. Tien, J. Z. He, Z. Erickson, A. Dragan, and D. S. Brown. Causal confusion and reward misidentification in preference-based reward learning. In ICLR, 2023. URL https://openreview. net/forum?id=R0Xxvr_X3ZA. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. ArXiv Preprint: 2211.14275, 2022. C. van Niekerk, R. Vukovic, B. M. Ruppik, H. Lin, and M. Gašic. Post-training large language models via reinforcement learning from self-feedback. ArXiv Preprint: 2507.21931, 2025. J. Wang, J. Liu, Y. Fu, Y. Li, X. Wang, Y. Lin, Y. Yue, L. Zhang, Y. Wang, and K. Wang. Harnessing uncertainty: Entropy-modulated policy gradients for long-horizon LLM agents. ArXiv Preprint: 2509.09265, 2025a. P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations. In ACL, pp. 94269439, 2024. S. Wang, L. Yu, C. Gao, C. Zheng, S. Liu, R. Lu, K. Dang, X. Chen, J. Yang, Z. Zhang, Y. Liu, A. Yang, A. Zhao, Y. Yue, S. Song, B. Yu, G. Huang, and J. Lin. Beyond the 80/20 rule: Highentropy minority tokens drive effective reinforcement learning for LLM reasoning. ArXiv Preprint: 2506.01939, 2025b. Y. Wang, M. Yang, R. Dong, B. Sun, F. Liu, and L. H. U. Efficient potential-based exploration in reinforcement learning using inverse dynamic bisimulation metric. In NeurIPS, pp. 3878638797, 2023. R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229256, 1992. 14 Preprint. Under review. M. Wu, Z. Zhang, Q. Dong, Z. Xi, J. Zhao, S. Jin, X. Fan, Y. Zhou, Y. Fu, Q. Liu, S. Zhang, and Q. Zhang. Reasoning or memorization? unreliable results of reinforcement learning due to data contamination. ArXiv Preprint: 2507.10532, 2025. J. Xiao, Z. Li, X. Xie, E. Getzen, C. Fang, Q. Long, and W. J. Su. On the algorithmic bias of aligning large language models with RLHF: Preference collapse and matching regularization. ArXiv Preprint: 2405.16455, 2024. W. Xiong, H. Dong, C. Ye, Z. Wang, H. Zhong, H. Ji, N. Jiang, and T. Zhang. Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint. In ICML, pp. 5471554754, 2024. R. Xu, K. Li, H. Wang, G. Kementzidis, W. Zhu, and Y. Deng. RL-QESA: Reinforcement-learning quasi-equilibrium simulated annealing. In ICML Workshop, 2025. A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, K. Lu, M. Xue, R. Lin, T. Liu, X. Ren, and Z. Zhang. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. ArXiv Preprint: 2409.12122, 2024. J. Yao, R. Cheng, X. Wu, J. Wu, and K. C. Tan. Diversity-aware policy optimization for large language model reasoning. ArXiv Preprint: 2505.23433, 2025. Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. ArXiv Preprint: 2503.14476, 2025. H. Yuan, Z. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. RRHF: Rank responses to align language models with human feedback. In NeurIPS, pp. 1093510950, 2023. L. Yuan, G. Cui, H. Wang, N. Ding, X. Wang, B. Shan, Z. Liu, J. Deng, H. Chen, R. Xie, Y. Lin, Z. Liu, B. Zhou, H. Peng, Z. Liu, and M. Sun. Advancing LLM reasoning generalists with preference trees. In ICLR, 2025. URL https://openreview.net/forum?id=2ea5TNVR0c. E. Zelikman, Y. Wu, J. Mu, and N. D. Goodman. STaR: self-taught reasoner bootstrapping reasoning with reasoning. In NeurIPS, pp. 1547615488, 2022. A. Zhang, N. Ballas, and J. Pineau. dissection of overfitting and generalization in continuous reinforcement learning. ArXiv Preprint: 1806.07937, 2018. K. Zhang, Y. Hong, J. Bao, H. Jiang, Y. Song, D. Hong, and H. Xiong. GVPO: Group variance policy optimization for large language model post-training. ArXiv Preprint: 2504.19599, 2025a. Q. Zhang, H. Wu, C. Zhang, P. Zhao, and Y. Bian. Right question is already half the answer: Fully unsupervised LLM reasoning incentivization. ArXiv Preprint: 2504.05812, 2025b. T. Zhang, H. Xu, X. Wang, Y. Wu, K. Keutzer, J. E. Gonzalez, and Y. Tian. NovelD: simple yet effective exploration criterion. In NeurIPS, pp. 2521725230, 2021. Z. Zhang, C. Zheng, Y. Wu, B. Zhang, R. Lin, B. Yu, D. Liu, J. Zhou, and J. Lin. The lessons of developing process reward models in mathematical reasoning. ArXiv Preprint: 2501.07301, 2025c. W. Zhao, P. Aggarwal, S. Saha, A. Celikyilmaz, J. Weston, and I. Kulikov. The majority is not always right: Rl training for solution aggregation. ArXiv Preprint: 2509.06870, 2025a. X. Zhao, Z. Kang, A. Feng, S. Levine, and D. Song. Learning to reason without external rewards. ArXiv Preprint: 2505.19590, 2025b. Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu. SLiC-HF: Sequence likelihood calibration with human feedback. ArXiv Preprint: 2305.10425, 2023. T. Zheng, T. Xing, Q. Gu, T. Liang, X. Qu, X. Zhou, Y. Li, Z. Wen, C. Lin, W. Huang, Q. Liu, G. Zhang, and Z. Ma. First return, entropy-eliciting explore. ArXiv Preprint: 2507.07017, 2025. 15 Preprint. Under review."
        },
        {
            "title": "A RELATED WORKS",
            "content": "We provide technical review clarifying the differences in experimental setups and summarizing insights from recent advances in RLVR for LLM post-training. Spurious reward in classical RL. We provide broader context on how prior work in reinforcement learning for classical settings has leveraged spurious rewards to facilitate training. Spurious reward signals are closely linked to challenges in generalization (Zhang et al., 2018; Koch et al., 2021; Langosco Di Langosco et al., 2022). While these works illustrate deliberate uses of such signals, spurious rewards may also arise unintentionally, leading to reward misspecification and reward hacking (Pan et al., 2022); similar reward hacking has been documented in Tien et al. (2023). second relevant thread traces back to potential-based reward shaping (PBRS) (Ng et al., 1999), which introduces additional or misaligned rewards in principled ways to preserve the optimality of desired behaviors. More recently, Random Network Distillation (RND) (Burda et al., 2019) emerged as leading exploration mechanism, with subsequent extensions such as Ma et al. (2025). In this same context, numerous other works propose reward signals (often spurious with respect to the true task objective) that encourage an agent to explore the state space in ways that eventually uncover genuine rewards (Pathak et al., 2017; Zhang et al., 2021; Wang et al., 2023; Li et al., 2024a). Spurious rewards have thus played substantial role in improving exploration. One prominent theoretical foundation is posterior sampling (Russo & Van Roy, 2014), which motivates exploration through uncertainty and has been generalized to broader RL settings (Chen et al., 2024b; Xu et al., 2025). Spurious reward in RLVR. We now turn to recent works that study spurious rewards in RLVR. Although these works report broadly similar empirical phenomena, their experimental configurations differ in important ways. In Shao et al. (2025), the prompt omits the standard Qwen-style instruction to place the final answer in box. As they note, Qwen-Math is highly sensitive to prompt formatting, and such differences can substantially shift baseline performance. In contrast, our experiments follow the default Qwen prompt used in verl (Sheng et al., 2025), which explicitly instructs the model to place the final answer in boxed expressionmirroring the RLVR verifier in verl, which extracts the boxed answer for scoring and reward assignment. Apart from this prompt choice, we match all other hyperparameters in Shao et al. (2025). Oertell et al. (2025), however, adopt markedly different configuration: (i) rollout-length cap of 1024 tokens (well below the 4096-token context window of Qwen-Math), (ii) different training dataset (MATH (Hendrycks et al., 2021) instead of DeepScaleR (Luo et al., 2025)), (iii) significantly smaller learning rate (1 107 versus 5 107 in Shao et al. (2025)), and (iv) reduced batch size (64 versus 128). The smaller learning rate changes the effective update magnitude, and the smaller batch size yields noisier estimates of the stochastic reward distribution. Given these differences, the empirical results reported across prior works are not directly comparable. Contamination. We further comment on potential contamination in the Qwen2.5-Math models. As reported by Wu et al. (2025), contamination in Qwen-Math has been observed only on validation sets (e.g., MATH500). Beyond these findings, the official Qwen2.5-Math technical report (Yang et al., 2024, Table 1) also acknowledges possible contamination arising from the close similarity between the training and validation sets of the MATH dataset (Hendrycks et al., 2021). Their training corpus comprises two components: (i) CoT data synthesis, which includes GSM8K, MATH, and NuminaMath (Yang et al., 2024, 3.1.1), and (ii) tool-integrated reasoning dataset containing GSM8K, MATH, CollegeMath, NuminaMath, MuggleMath, and DotaMath (Yang et al., 2024, 3.1.2). In contrast, our experiments employ the DeepScaleR training set, which consists exclusively of selected questions from AMC, AIME, Omni-Math, and Still (Luo et al., 2025). None of these datasets appear in the training sources listed for Qwen2.5-Math. Therefore, we believe that our training data does not overlap with the datasets used to train Qwen2.5-Math and is thus not contaminated. LLM entropy. Agarwal et al. (2025) demonstrate that token-level entropy minimization can substantially improve LLM reasoning without verifiable feedback, arguing that reduced entropy increases model confidence and reveals latent reasoning capability. This mechanism parallels clipped training under random rewards, where updates primarily modulate entropy rather than exploit informative rewards. However, we show that entropy minimization alone may drive the policy toward 16 Preprint. Under review. low-entropy yet suboptimal solutions; hence, entropy should be viewed as stabilizing regularizer rather than replacement for genuine RLVR signals. Related work explores entropy through the lens of self-confidence. In particular, Prabhudesai et al. (2025) use low-entropy rollouts as implicit rewards, achieving gains across diverse benchmarks, while Gao et al. (2025) show that even single unlabeled example can improve reasoning via entropy reduction. Methods such as EMPO (Zhang et al., 2025b) and Zhao et al. (2025b) similarly enhance performance in unsupervised settings by amplifying model confidence. van Niekerk et al. (2025) further construct preference datasets from confidence scores, achieving RLHF-level improvements without human feedback. In this context, Cui et al. (2025) propose simple but influential empirical relationship between policy entropy and model performance R, fit across extensive experiments: = exp (H) + b, > 0. This relation suggests that performance increases monotonically as entropy decreases but plateaus once entropy collapses too early. Intuitively, when model overemphasizes certain tokens, its output distribution becomes overconfident and loses exploratory capacity, leading to performance ceiling. To mitigate early-stage entropy collapse, several works propose alternative strategies. Shen (2025) analyze why entropy regularization suffers from limited benefit in RLVR training for LLMs by attributing it to the vast response space and the sparsity of optimal outputs, and then introduce an adaptive entropy-control method using clamped entropy bonus with automatically tuned coefficients. Song et al. (2025) show that ORM yields induces sharp reductions in output entropy and diversity (as shown by lower pass@n scores), and propose outcome-level entropy bonuses to counteract it. Prior works (Wang et al., 2025a; Yao et al., 2025; Zheng et al., 2025; Cheng et al., 2025) also develop additional techniques for controlling entropy during RLVR training. Reinforcement learning for LLM. Proximal policy optimization (PPO) (Schulman et al., 2017) has become standard for reward-based policy updates in LLM training and remains core component of RLHF. However, since PPO requires loading and maintaining four separate models during training, it is computationally and memory intensive. This has motivated the development of lighter-weight and adapted policy-gradient updates (Li et al., 2024b; Ahmadian et al., 2024; Shao et al., 2024; Guo et al., 2025). In parallel, advances in verifiable reward construction (Cobbe et al., 2021; Uesato et al., 2022; Zelikman et al., 2022; Singh et al., 2023; Hosseini et al., 2024; Lightman et al., 2024; Wang et al., 2024; Luo et al., 2024; Setlur et al., 2025; Zhang et al., 2025c) have enabled reinforcement learning to substantially improve LLM reasoning, particularly in mathematical problem solving. Beyond training algorithms, recent work also explores post-processing and collaborative strategies to strengthen reasoning performance. Kay et al. (2025) and Zhao et al. (2025a) propose consensus-based and answer-aggregation methods within multi-model frameworks. Chen et al. (2025a) introduce self-questioning paradigm for iterative refinement, while Park et al. (2025) develop an online multi-agent collaborative reinforcement learning framework. Offline alignment. Direct alignment methods (Rafailov et al., 2023) provide simple and stable offline alternative to RLHF. Extensions to DPO include broader ranking objectives (Dong et al., 2023; Yuan et al., 2023; Song et al., 2024; Chen et al., 2024a; Liu et al., 2025a) and lightweight variants that remove the reference model (Hong et al., 2024; Meng et al., 2024). Since DPO avoids reward model, limited human preference data becomes key bottleneck; recent work addresses this by generating additional preference pairs via SFT policies (Zhao et al., 2023; Liu et al., 2024a). The framework has also been generalized to token-level MDPs (Rafailov et al., 2024) and broader RL settings (Azar et al., 2024). Complementary approaches incorporate online human feedback to reduce distribution shift and improve reasoning (Dong et al., 2024; Xiong et al., 2024; Pang et al., 2024). Another line studies unintentional alignment and proposes remedies (Pal et al., 2024; Tajwar et al., 2024; Liu et al., 2024b; Xiao et al., 2024; Yuan et al., 2025; Razin et al., 2025; Chen et al., 2025b). For example, Razin et al. (2025) filter noisy preference pairs using CHES similarity, while Chen et al. (2025b) show that combining comparison oracles with DPO mitigates unintended alignment effects."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "We begin with high-level overview of the experimental setup, followed by comprehensive presentation of results in both the main text and the appendix. Our experiments are organized around two 17 Preprint. Under review. Figure 5: All experiments follow the same setup as Figure 1, varying the threshold ε with six independent runs for each setting: trials with clipping ratio ε = 0.1 (Left); trials with clipping ratio ε = 0.15 (Middle); and the ratio of clipping activations across ε {0.2, 0.15, 0.1} (Right). objectives: (i) characterizing the interplay between clipping, policy entropy, and performance under spurious rewards, and (ii) assessing whether the observed benefits of spurious rewards generalize beyond Qwen-Math to broader range of model families. For the first objective, we focus on Qwen-Math-7B and provide controlled setting for examining how clipping and policy entropy affect model performance under spurious rewards. This choice is supported by previous empirical findings and practical considerations: Qwen-Math-7B has moderate parameter count and relatively short 4K context window, stabilizing training and reducing exposure to issues such as gradient explosion. This stability is crucial since clipping is commonly used to prevent gradient explosion in larger models with longer chain-of-thought rollouts. As shown in Figure 2 (Right) and discussed in 4.2, removing clipping on stronger model with longer rollouts can cause catastrophic training collapse, making it difficult to disentangle the core relationship between clipping, entropy, and performance. Indeed, one key motivation for applying the clipping in GRPO originates from the need to stabilize training for the DeepSeek-R1-671B model. For the second objective, we additionally evaluate two non-contaminated model families, Llama and QwQ, for which no contamination has been reported in the community, to demonstrate that the benefits and behaviors of spurious rewards extend beyond Qwen-Math and reflect broader RLVR learning. In 3.2, we examine how clipping affects model performance by comparing training with and without clipping. In 4.2, we validate our theoretical findings on the relationship between clipping and policy entropy. For consistency, these experiments use Qwen-Math-7B trained on the DeepScaleR dataset. Then, in 4.3 and this section, we investigate the interaction between entropy and performance on the more challenging AIME training set, again evaluating both clipped and unclipped training. We find that policy entropy is not directly related to performance improvements, and that models gain significantly less from random rewards when their baseline performance is reduced by dataset difficulty. This stands in contrast to stronger Llama and QwQ models, which continue to benefit from random rewards even on harder tasks. Finally, in 5, we proceed to broader spectrum of model strengths, showing that stronger models are more likely to benefit from random reward signals. Ablation analysis. We ablate the GRPO group size G. Larger groups (G = 16) yield more balanced binary rewards, while smaller groups increase the likelihood of extreme reward-misalignment events, such as entire groups receiving reward 0 despite containing correct rollouts, or reward 1 despite containing incorrect ones. Thus, reducing inherently amplifies instability from the rewardmisalignment perspective. As shown in Figure 6, using smaller group size (G = 8) allows most runs to improve, but leads to higher variance and less stable learning dynamics throughout training. Figure 6: Smaller group size. We further analyze the effect of varying the clipping ratio threshold ε. Indeed, Figure 1 examined the cases ε = 0.2 and ε = (no clipping), showing that relaxing the clipping threshold does not degrade performance. However, this does not yet confirm robustness under stricter clipping. To address this, we report additional results for ε 0.15, 0.1 in Figure 5. Across these settings, we observe behavior consistent with Figure 1: (i) some runs fail to improve, as predicted by our probabilistic reward-misalignment framework; and (ii) successful runs converge to roughly 70% validation accuracy regardless of the clipping strength. As discussed in 4.1, clipping primarily influences policy entropy. Among the improving trials, stricter clipping tends to reduce variance 18 Preprint. Under review. across seeds, reflecting more deterministic policy toward convergence. Taken together, these results indicate that our findings remain robust under different choices of ε. Figure 7: Unclipped Qwen2.5-Math-7B on the hard AIME dataset: independent runs following from the setup in Figure 3 (Left); corresponding policy entropy dynamics during unclipped training (Middle); joint evolution of model performance and policy entropy for an example trial (Right). Figure 8: Visualization of policy action distributions over 12 prompt xi. Each subplot displays the sorted log-probability of π(y xi) for 64 sampled responses from each prompt xi. Columns 1-2 (blue) correspond to prompts xi with Φ(π( xi)) > 0, while Columns 3-4 (orange) correspond to prompts with Φ(π( xi)) < 0. As discussed in Theorem 4.2, the entropy increase under unclipped training can occur only for the skewed one shown in Columns 3-4. In Figure 3, we present clipped training results for Qwen2.5-Math-7B Unclipped training. on the AIME dataset, where clipping induces entropy collapse as demonstrated by Theorem 4.3. Although entropy decreases, performance also degrades, indicating that lower entropy is not reliably associated with better model performance. This raises an important question: what happens in the complementary regime where entropy increases under random reward? To answer this, we conduct additional experiments and report the results in Figure 7. Across independent seeds shown in Figure 7 (Left), the behavior remains qualitatively similar to the clipped case in Figure 3: some runs improve, others degrade, and overall learning dynamics appear stochastic. In Figure 7 (Middle), we empirically confirm the predicted entropy increase under unclipped training. Among the improving runs, Figure 7 (Right) provides representative example in which performance improves even as entropy increases. These results answers our question: there is no direct causal relationship between policy entropy 19 Preprint. Under review. and model performance. Both clipped and unclipped experiments support that, as the models initial performance on dataset decreases, its likelihood of benefiting from random rewards diminishes. Policy skewness. We empirically evaluate the skewness measure Φ(π) introduced in Remark 4.2 on the actual Qwen-Math-7B policy. Recall that under unclipped training, entropy can increase after single update only when Φ(π) < 0. Since the policy induces different action distributions π(a x) for different input questions x, we estimate skewness across questions by sampling the first 500 examples from the DeepScaleR training set (Luo et al., 2025). For each question x, we generate 64 responses from the policy using the same sampling and decoding hyperparameters as in 3.2 and compute an empirical estimate of Φ(π( x)). We visualize selected prompts xi along with their corresponding skewness values in Figure 8, providing clearer picture of how Qwen-Math-7B behaves across the dataset. Among 500 sampled questions, 358 ones satisfy Φ(π( xi)) < 0, which is consistent with the observed entropy increases for the unclipped training."
        },
        {
            "title": "C THEORETICAL ANALYSIS",
            "content": "Setup. We model the next-token generation using softmax at each history. Let be the vocabulary and ht = (x, y<t) be the history. For each prompt and response = (a1, . . . , aL) where at V, we have πθ(a x) = (cid:89) t=1 πθt(at ht), where πθt(at ht) = (cid:80) exp(θt,ht,at ) aV exp(θt,ht,a ) , 1 , . . . , θ where θ = (θ ), and θt RX Vt Given trajectories drawn from πold, we define the per-token ratio r(i) {y(i)}G per-history advantage used in the policy update is (θ) = πθ(y(i) πold(y(i) i=1 πold( x) and the corresponding outcome-reward advantages {Ai}G for all = 1, . . . , L. . For group h(i) ) h(i) ) i=1, the empirical A(h, a) = 1 (cid:88) (cid:88) (cid:18) i=1 t=1 1{h(i) =h,y(i) πold(ah) =a} (cid:19) Ai. This can be derived from Eq. (3) using = y(i) for all = 1, 2, . . . , G. Following Williams (1992) and Li et al. (2024b), we define the clipped surrogate loss with per-token ratios without adding separate length normalization terms as follow, J(θ) = xρ,{y(i)}G i=1πθold (x) (cid:34) 1 (cid:88) (cid:88) i=1 t=1 min Without clipping, the surrogate loss reduces to (cid:110) (θ)Ai, clip(r(i) r(i) (θ), 1 ε, 1 + ε)Ai (cid:35) (cid:111) , J(θ) = xρ,{y(i)}G i=1πθold (x) (cid:34) 1 (cid:88) (cid:88) i= t=1 (cid:35) r(i) (θ)Ai . We derive the closed-form token-level update for optimizing the unclipped surrogate loss with forward KL penalty to πold as follows. To begin with, notice that ˆJ(θ) = 1 (cid:88) (cid:88) i=1 t=1 r(i) (θ) Ai = (cid:88) (cid:88) aV πθ(a h) (cid:88) (cid:88) i=1 t=1 1 (cid:34) (cid:124) 1{h(i) =h,y(i) πold(ah) =a} (cid:123)(cid:122) A(h,a) (cid:35) Ai . (cid:125) Using mirror descent (MD), for each iteration, one solves max θ ˆJ(θ) 1 η (cid:88) DKL(πθ( h) πold( h)) 20 Preprint. Under review. which is equivalent to solving for each fixed h, max π(h) (cid:88) π(a h) A(h, a) 1 η (cid:88) π(a h) log π(ah) πold(ah) . Introducing Lagrangian multiplier λh for the probability simplex constraint (cid:80) first order condition (cid:16) (cid:17) π(a h) = 1, by A(h, a) 1 η Solving the above equation for πθ yields log π(ah) πold(ah) + 1 + λh = 0. πθ(a h) = (cid:80) πθold (ah) exp(η A(h,a)) V πθold (ah) exp(η A(h,a)) , and evaluate at the realized pairs (a, h) = (y(i) , h(i) ) in training. Note that the above GRPO update can be analyzed by interpreting it as one natural policy gradient (NPG) step under softmax tabular parametrization (Agarwal et al., 2021). GRPO analysis. Interpreting the GRPO update as natural policy gradient (NPG) step has been widely adopted to study entropy dynamics throughout training (Cui et al., 2025). Here, we summarize the key components of GRPO in Algorithm 1, which motivate our reduction to an NPG-style update for analyzing the effect of clipping in GRPO. We note that Algorithm 1 should be viewed as an abstraction of GRPO implementations used in practice (Shao et al., 2024). In the outer loop, reference policy is fixed once per iteration, and the per-step objective may include KL penalty that constrains the updated policy πθ to remain close to πref, thereby controlling the effective step size and preventing excessive policy drift. Recent zero-RL\" setups (see Yu et al., 2025), which are also adopted in the empirical evaluation of Shao et al. (2025), set the KL coefficient to zero, effectively removing the explicit KL term from the objective. As such, we likewise omit the KL term in our analysis. Under this regime, the outer loop does not affect the subsequent analysis. In the middle loop, which is for GRPO training, the model samples each batch, which is update-style agnostic. The key difference between exact-GRPOand NGP-style update happens in the inner loop (line 10). First, µ is constant hyperparameter for the number of actual updates per macro batch, used to improve sample efficiency and better optimize the surrogate while clipping limits drift from πold. Therefore, the statement for GRPO iteration = 1, . . . , µ performs µ optimizer steps on the same mini-batch to maximize the clipped GRPO surrogate. At each step, importance ratio r(i) x) = πθ(y(i) πold(y(i) , 1 ε, 1 + ε) A} is backpropagated. x) are recomputed and the loss 1 (cid:80) i,t min{r(i) A, clip(r(i) In GRPO, the µ-step inner loop produces chain of micro-updates whose importance ratios evolve across steps, making the expected contribution of clipping analytically intractable unless one specifies the per-step clip-activation rate (the expected fraction of tokens/micro-batches with / [1ε, 1+ε]). This rate is modeland dataset-dependent and is only available empirically. Conditioning on the empirically measured activation rate, we collapse the µ clipped micro-steps into single NPG-update with actual model-specific token-level expected clipping activation ratio. This surrogate preserves the first-order effect of clipping and enables tractable bounds for our theoretical results. Comparing to recent works that directly used NPG for GRPO analysis, our setup for clipping analysis is validly justified, facilitating the later theoretical derivation and without unjustified oversimplification. C.1 MISSING PROOFS IN 2 Proof of Theorem 2.2. Fixing h, we rewrite Eq. (2) as πnew(a h) = πold(ah) exp(η A(h,a)) Zh(η) , where Zh(η) = (cid:88) aV πold(a h) exp(η A(h, a)) = Eaπold(h)[exp(η A(h, a))]. Taking the logarithm of both sides yields log(πnew(a h)) = log(πold(a h)) + η A(h, a) log(Zh(η)). (10) 21 Preprint. Under review. πref πθ. for = 1, , do Algorithm 1 Iterative Group Relative Policy Optimization 1: Input: model parameters θinit, reward models rφ, prompts , and hyperparameters ε, β, µ. 2: Initialization: θ θinit. 3: for iteration = 1, , do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: Output: πθ. Sample batch Xj from . Update the old policy model πθold πθ. Sample outputs {oi}G Compute rewards {ri}G Compute ˆAi,t for the t-th token of oi via group-relative advantage estimation. Update the policy model πθ using GRPO. end for Update rφ through continuous training using replay mechanism. i=1 πθold( x) for each question Xj. i=1 for each sampled output oi using the reward model rφ. We define ψh(η) = log(Zh(η)). Then, we have ψh(0) = 0, ψ ing (h, a), we define Ii(h, a) := (cid:80)y(i) Then, we have t=1 1{h(i) h(0) = µ(h), and ψ = a} {0, 1} and := (cid:80)G h(0) = σ2(h). Fixi=1 Ii(h, a). = h, y(i) (cid:32) (cid:88) i=1 Ii(h, a)Ai (cid:33) . A(h, a) = 1 πold(ah)G By using (cid:80)G i=1 A2 i=1 Ai = 0 and (cid:80)G (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) i=1 G, we have (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Ii(h, a)Ai (cid:112)N (G ) 2 . This implies A(h, a) 1 we have 2πmin for all V. By using Taylors theorem with Lagrange remainder, ψh(η) = µ(h)η + 1 6 ψ (η) is the third central moment of A(h, a) under the exponentially tilted distribution and (ξ)η3 for some ξ (0, η). Since ψ A(h, a) [M, ], the sharp bound E[(X E[X])3] (ba) 2 σ2(h)η2 + 1 for [a, b] yields 6 3 Therefore, we conclude that ψ (η) 6 1 3(πmin)3 . (cid:12) (cid:12)ψh(η) µ(h)η 1 2 σ2(h)η2(cid:12) (cid:12) 36 η3 3(πmin)3 . Combining this with Eq. (10) yields the claimed inequality with = for log(r(h, a)) and the standardized case follow immediately. 36 1 3(πmin)3 . The statements Proof of Theorem 2.4. We prove three statements one by one as follows. (i) Let τ : {0, 1}G {0, 1}G be τ (r1, . . . , rG) = (1 r1, . . . , 1 rG). If (r G) = and similar to Ai and r. With the notation 1, . . . , τ (r1, . . . , rG), then we can also define above, we have = 1 and = (1 rj) (1 r) = (rj r). Hence Sr = Sr and invariant under τ . Thus, we know (r the above two facts, we obtain Ai 1, . . . , G) d= (r1, . . . , rG) and d= Ai and thus E[A2k1 ] = 0. i = Ai. Since (r1, . . . , rG) is i.i.d. Bernoulli( 1 2 ), its law is d= Ai. Combining 22 Preprint. Under review. (ii) Let := (cid:80)G j=1 rj, then = G and S2 1 = K(GK) G(G1) . Thus, (cid:113) GK GK (cid:113) 1 G(GK) if ri = 1, if ri = 0. Ai = Thus, it is easy to see Ai 1 . (iii) Let := (cid:80)G j=1 rj Binomial(G, 1 Sr = (cid:112)p(1 p), Ai = 2 ) and := . On {1 1}, we have ri = 1, (cid:113) 1p , (cid:113) 1p , ri = 0. Hence for N+, E[Aik K] = (cid:17)k/2 (cid:16) 1p + (1 p) (cid:17)k/ (cid:16) 1p = xk/2+x1k/2 1+x , with := 1p > 0. Define hk(x) := xk/2 + x1k/2 1. Then (cid:0) 2 1(cid:1) xk/21 0, k(x) = 2 k(1) = 0. By convexity, hk(x) 0 for all > 0, hence E[Aik K] 1 and hk(1) = whenever 1 1. Taking the expectation and using the fact that Ai = 0 on {K {0, G}} yields (cid:0) 2 1(cid:1) xk/22 + > 0 if 2, 2 E[Aik] = G1 (cid:88) K=1 (cid:19) (cid:18)G 2GE[Aik K] G1 (cid:88) k=1 (cid:19) (cid:18)G 2G = 1 21G, if 2. Finally, it is trivial to write down E[Ai] with the above information. This completes the proof. Failure of Eq. (5) under random reward. We have Covyπold(x) (log(πold(y x)), A(x, y)) = Eyπold(x) [log(πold(y x))A(x, y)] Eyπold(x) [log(πold(y x))] Eyπold(x)[A(x, y)] (cid:125) (cid:124) (cid:123)(cid:122) =0 = Eyπold(x) [log(πold(y x))] Eyπold(x)[A(x, y)] (cid:125) (cid:124) (cid:123)(cid:122) = = 0. In other words, the co-variance between A(x, y) and πold(y x) is uninformative under random reward since these two terms are independent from each other in this specific setting. Thus, more accurate estimation of H(πnew) H(πold) beyond Eq. (5) is desirable. C.2 MISSING PROOFS IN 3 Proof of Theorem 3.2. By definition, we have + tot = (cid:80)L t=1 D+ A. Thus, we have E[C + tot2] = (cid:88) (cid:88) E[D+ D+ A2] = (cid:88) E[(D+ )2A2] + (cid:88) E[D+ D+ A2]. (11) s=t Recall in the proof of Theorem 2.2, we have shown that A(h, a) 1 s= t=1 t=1 2πmin . We also have r(h, a) = eη A(h,a) Zh(η) , Zh(η) = πold(a h)eη A(h,a). (cid:88) Since A(h, a) 1 r(h, a) eη/πmin =: Rmax diagonal and off-diagonal terms in the right-hand side of Eq. (11). , we have Zh(η) eη/(2πmin) and eη A(h,a) eη/(2πmin). Thus, we have for all t. In what follows, we bound the . This implies rt Rmax 2πmin η η 23 Preprint. Under review. Diagonal term. On {I + = 1}, we have rt > 1 + ε 1 and rt = 1 + ε. Thus, we have = (1 + ε rt)I + = (rt 1 ε)I + By using the fact that (x 1)2 2xϕ(x) for all 1, we have (rt 1)I + . D+ Since (c.f. Theorem 2.4), we have (D+ )2 (rt 1)2I + 2rtϕ(rt)I + . Since ϕ is strictly increasing and rt Rmax pieces together yields η E[(D+ )2A2] 2E[(D+ )2] 2M 2E[rtϕ(rt)I + ]. η ϕ(Rmax Rmax , we have rtϕ(rt)I + η )I + . Putting these (cid:88) t= E[(D+ )2A2] 2LM 2Rmax η ϕ(Rmax η )E[I + ] = 2LM 2Rmax η ϕ(Rmax η )p+. (12) Off-diagonal term. We define Zt := D+ + η 1 ε)+ and have η := (Rmax and have Zt D+ D+ . Then, we define D+ = (rt 1 ε)I + (Rmax η 1 ε)+I + = + η + . Putting these pieces together yields Zt + η It and E[ZsZt] 2(+ η )2 (cid:88) E[I + + ] 2(+ η )2E (cid:32) (cid:88) (cid:33)2 . It (13) s=t t=1 (cid:88) s=t Since (cid:80)L t=1 + L, we have ((cid:80) + + ). Thus, we have )2 L((cid:80) (cid:33)2 + LE (cid:32) (cid:88) t=1 (cid:35) + = L2p. (cid:34) (cid:88) t=1 On {I + = 1}, we have rt 1 + ε and ϕ(rt) ϕ(1 + ε). This implies + ϕ(rt) ϕ(1+ε) and (cid:88) t=1 + 1 ϕ(1+ε) (cid:33) ϕ(rt)I + (cid:32) (cid:88) t=1 Lϕ(Rmax ϕ(1+ε) η ) , Putting these pieces together yields (cid:32) (cid:88) (cid:33)2 + L2 min (cid:26) (cid:16) ϕ(Rmax ) η ϕ(1+ε) p, (cid:17)2(cid:27) . Plugging Eq. (14) into Eq. (13) gives t= E[ZsZt] 2(+ η )2L2 min (cid:26) (cid:16) ϕ(Rmax ) η ϕ(1+ε) p, (cid:17)2(cid:27) . (cid:88) s=t (14) (15) Conclusion. Using E[C + Eq. (11), Eq. (12), and Eq. (15) yields tot] E[C + tot2] and (cid:113) + + for x, 0 together with E[C + tot] (cid:113) 2p+LRmax η ϕ(Rmax η ) + L+ η min (cid:110) p+, ϕ(Rmax ) η ϕ(1+ε) (cid:111) , (16) Let = η/πmin 1, so Rmax Thus, we have Rmax η ϕ(Rmax yields E[C + tot] c1η and c3 = (e 1)ϕ(1 + ϵ)1π3 η = eu e. For [0, 1], we have eu 1 (e 1)u and ϕ(eu) u2. η eu 1 (e 1)u. This together with Eq. (16) η min, c2 = (e 1)π1 2eπ1 min, pL, c3η3L} where c1 = ) eu2 and + + min{c2η min. This completes the proof. 24 Preprint. Under review. Proof of Theorem 3.4. We recall that 1 thus A(h, ) is also symmetric. Thus, we have rt(η) d= rt(η) and 2πmin =: . By Theorem 2.4, Ai is symmetric, and"
        },
        {
            "title": "This implies",
            "content": "(cid:34) (cid:88) t=1 (cid:35) (cid:34) rt(η) = (cid:35) rt(η) . (cid:88) t=1 (cid:34) E[Nraw] = (cid:32) (cid:88) t=1 rt(η)+rt(η) 2 (cid:33)(cid:35) . We write rη := rη(h, a) = eη A(h,a) Zh(η) where Zh(η) = Eaπold(h)[exp(η A(h, a))]. Then, we have rη+rη 2 rηrη = 1 Zh(η)Zh(η) . By using the convexity of (cid:55) eηx on [ (cid:102)M , (cid:102)M ], we have 2M eηM = cosh(ηM ) + 2M eηM + eηx +x sinh(ηM ), for all [M, ]. Averaging under πold( h) yields Zh(η) cosh(ηM ) + µ(h) sinh(ηM ), Zh(η) cosh(ηM ) µ(h) sinh(ηM ), where µ(h) := Eaπold(h)[ A(h, a))]. Then, we have (cid:16) µ(h) Zh(η)Zh(η) cosh2(ηM ) (cid:17)2 Putting these pieces together yields sinh2(ηM ) cosh2(ηM ). Applying this to (ht, yt), using cosh(x) exp(x2/2), and summing over yields rη+rη 1 cosh(ηM ) = 1 cosh(η/(2πmin)) . (cid:88) t=1 rt(η)+rt(η) cosh(η/(2πmin)) Leη2/(8π2 min) = LeCη2 . Taking the expectations yields Eq. (7). This together with Theorem 3.2 yields the desired bound. C.3 MISSING PROOFS IN 4 We first summarize the setup and notations used in this section. We only consider = 1 (bandit case) for illustration. Denote πu new as the new policy obtained from Eq. (2) (unclipped case) and the new policy obtained from πc new := max πV (cid:110) (π) := ˆJ(π) 1 η DKL(ππold) (cid:111) , where we only consider upper clipping in the surrogate function as follows, ˆJ(π) := 1 (cid:88) i= min{r(y(i))Ai, min{r(y(i), 1 + ε}Ai}. We define ru(a) := πu new(a) πold(a) , rc(a) := πc new(a) πold(a) , and S+(a) := (cid:88) i=1 Ai1{y(i) = a, Ai > 0}, S(a) := (cid:88) i=1 Ai1{y(i) = a, Ai < 0}. In what follows, we provide detailed version of Theorem 4.1 and its proof. Theorem 4.1 (restated). If = 1, then without clipping, for any η > 0, we have E[H(πnew) H(πold)] = 121G 2G Φ(πold)η2 + E[R(η)], R(η) Cη4. Preprint. Under review. where with log(π(a)) (cid:32) (cid:88) (cid:33) π(a) log(π(a)) , Φ(π) = 1 + (cid:88) aV C(πmin) = eη/πmin 24 (cid:16) 192 + 176 log( 1 πmin aV ) + 176η πmin (cid:17) M, (πmin) = E[A4 1] G3 (S2(πmin) 7S1(πmin) + 12V 6) 2]) 1A2 (S1(πmin) 2V + 1), + 3(E[A 1]+(G1)E[A2 G3 S1(πmin) = V1 πmin S2(πmin) = V1 (πmin)2 + + , 1 1(V1)πmin 1 (1(V1)πmin)2 . Proof. If = 1, there is only one and every rollout has the same h, so we ignore for simplicity. In this regard, we abbreviate π(a h) as π(a), y(i) (cid:32) (cid:88) as y(i), and 1{y(i)=a} (cid:33) A(a) := 1 πold(a) Ai . We rewrite the update rule as follows, i=1 πη(a) := πnew(a) = πold(a)eη A(a) Z(η) , Z(η) = πold(a)eη A(a). (cid:88) aV We define ψ(η) := log Z(η) and u(a) := η A(a) ψ(η). Then, we have πη(a) = πold(a)eu(a) and Eπold [eu] = (cid:88) πold(a)eη A(a)ψ(η) = 1. We write (a) = log(πold(a)). Then, the change of entropy satisfies H(η) = H(πη) H(πold) = Eπold[eu(L + u)] + Eπold[L]. Let (η) := E[H(η)], where E[] is taken over the randomness from {y(i)}G i=1 and {ri}G i=1. Leading Term. By Theorem 2.4, Ai is symmetric and thus A(a) is symmetric. Thus, we have H(η; A) = H(η; A) d= H(η; A). This implies that is even and (0) = (3)(0) = 0. One has the identity dη (cid:0)Eπη [g](cid:1) = Covπη ( A, g), for any function g. Using H(πη) = Eπη [log(πη)] and log(πη(a)) = L(a) + η A(a) ψ(η), we have H(πη) = Covπη ( A, L) ηVarπη ( A), and H(πη) = Covπη ( A2, L) Varπη ( A) ηEπη [( Eπη [ A])3], Evaluating at η = 0 yields H(π0) = Covπold ( A2, L) Varπold( A). Then, we have (0) = E[H(π0)] = E[Covπold( A2, L) + Varπold( A)]. 26 Preprint. Under review. We first compute E[Varπold ( A)]. Indeed, we have Varπold( A) = (cid:88) πold(a)( A(a))2 (cid:33)2 πold(a) A(a) (cid:32) (cid:88) = (cid:88) (cid:32) πold(a) 1 πold(a)G (cid:88) i=1 (cid:33)2 (cid:32) Ai1{y(i)=a} (cid:124) (cid:88) πold(a) πold(a)G (cid:88) i=1 (cid:123)(cid:122) =0 Ai1{y(i)=a} (cid:33)2 (cid:125) = 1 (cid:88) i,j"
        },
        {
            "title": "AiAj",
            "content": "(cid:32) (cid:88) 1 πold(a) 1{y(i)=a}1{y(j)=a} (cid:33) . Using the fact that Ai is independent of y(i), we have E[Varπold( A)] = 1 G2 (cid:88) i=1 (cid:32) (cid:34) E[A2 ] (cid:88) (cid:35)(cid:33) πold(a) 1{y(i)=a} + 1 G2 (cid:32) (cid:34) E[AiAj] (cid:88) (cid:88) i=j (cid:35)(cid:33) πold(a) 1{y(i)=y(j)=a} = 1 G2 (cid:88) i=1 E[A ] + (cid:88) i=j E[AiAj] . Since (cid:80)G E[A2 i=1 Ai = 0, we have (cid:80)G ] = 1 21G. Thus, we have i=1 A2 = (cid:80) i=j AiAj. In addition, by Theorem 2.4, we have We then compute E[Covπold( A2, log(πold))]. Indeed, we have E[Varπold( A)] = 121G (V 1). Covπold ( A2, log(πold)) = (cid:88) Then, we have πold(a) log(πold(a)) A(a)2 Varπold( A) (cid:33) πold(a) log πold(a) . (cid:32) (cid:88) E[Covπold ( A2, log(πold))] = 121G (cid:32) (cid:88) log(πold(a)) (cid:33) πold(a) log(πold(a)) . (cid:88) This implies (0) = 121G Φ(πold). Remainder Term. Note that is even. For any η > 0, there exists (0, η) such that (η) = (0) 2 η2 + (4)(t) 24 η4. It suffices to bound supt[0,η] (4)(t). Indeed, we let Xc(a) = A(a) Eπt[ A] and Lc(a) = L(a) Eπt[L]. Then, we have d4 dt4 H(πt) = Eπt[LcX 4 ]Eπt [LX 2 ]Eπt[LcXc] ] + 9(Eπt[X 2 We bound the right-hand side using Eπt[X 4]. Indeed, we have L(a) = log(πold(a)) [log(πmin), 0], so Eπt[L] log( 1 πmin ] + 4Eπt[X 3 ] 3Eπt[X 4 ])2 + t(10Eπt[X ] Eπt[X 5 ). Thus, we have +6Eπt[X 2 ]Eπt[X 3 ]). Eπt[LcX ] + 4Eπt[X 3 ]Eπt[LcXc] + 6Eπt[X 2 ]Eπt[LX 2 ] 11 log( 1 πmin )Eπt[X 4 ]. We also have (cid:12) (cid:12)3Eπt[X 4 Using Eπt[Xc] 2Eπt[ A(a)] 1 πmin ]Eπt[X 3 (cid:12) (cid:12)10Eπt[X 2 ] + 9(Eπt[X 2 ])2(cid:12) (cid:12) 12Eπt[X 4 ]. (see the proof of Theorem 2.4), we have ](cid:12) ] Eπt[X 5 (cid:12) 11 πmin Eπt[X 4 ]. 27 Preprint. Under review. ] 16Eπt[ A4], we conclude that for all [0, η], By using Eπt[X 4 (cid:12) (cid:16) 12 + 11 log( 1 (cid:12) (cid:12) πmin In addition, we have d4 dt4 H(πt) ) + 11t πmin Eπt[X 4 (cid:12) (cid:12) (cid:12) ] (cid:17) (cid:16) Eπt[ A4] et/πmin Eπold [ A4]. 192 + 176 log( 1 πmin ) + 176t πmin (cid:17) Eπt[ A4]."
        },
        {
            "title": "Putting these pieces together yields",
            "content": "f (4)(t) eη/πmin (cid:16) 192 + 176 log( 1 πmin ) + 176η πmin (cid:17) (cid:105) (cid:104) Eπold[ A4] . sup t[0,η] Conclusion. We define Ii(a) = 1{y(i) = a}. Thus, we have Ii(a) Bernoulli(p) i.i.d. and it is independent of {Ai}G i=1. We write S(a) = (cid:80)G i=1 AiIi(a) and (cid:88) 1 πold(a)G Ai1{y(i) = a} i=1 (cid:33)4 (cid:88) = (S(a))4 (πold(a))3G4 . πold(a)( A(a))4 = (cid:88) (cid:32) (cid:88) πold(a) Conditioning on {Ai}G i=1 and using (cid:80)G Eπold (cid:2)(S(a))4 {Ai}G i=1 i=1 Ai = 0, direct fourth-moment expansion gives (cid:32) (cid:88) (cid:3) = (πold(a) 7(πold(a))2 + 24(πold(a))3 6(πold(a))4) (cid:33) A4 +(3(πold(a))2 12(πold(a))3 + 3(πold(a))4) i=1 (cid:33)2 A2 . (cid:32) (cid:88) i=1 This implies (cid:104) Eπold Eπold [ A4] {Ai}G i= (cid:105) = 1 G4 (cid:32)(cid:32) (cid:88) (cid:33) 1 (πold(a)) 7 (cid:32) (cid:88) (cid:33) 1 πold(a) (cid:33) (cid:32) (cid:88) + 24V 6 (cid:33) A4 i=1 (cid:32) (cid:88) (cid:32) 3 (cid:33) 1 πold(a) + 1 G4 (cid:33) (cid:32) (cid:88) 12V + 3 (cid:33)2 A2 . i=1 In addition, we have (cid:34) (cid:88) i=1 (cid:35) A4 = GE[A4 1], (cid:32) (cid:88) i=1 and (cid:88) 1 πold(a) πmin + 1 1(V1)πmin , (cid:33)2 A2 (cid:88) = GE[A4 1] + G(G 1)E[A2 1A2 2], (πold(a))2 V1 π2 min + 1 [1(V1)πmin]2 . Putting these pieces together yields (cid:105) (cid:104) Eπold[ A4] E[A4 G3 (S2 7S1 + 12V 6) + 3(E[A4 1] 1]+(G1)E[A2 G3 1A2 2]) (S1 2V + 1), where S1, S2 > 0 are This completes the proof. + S1 = V1 πmin S2 = V1 (πmin)2 + , 1 1(V1)πmin 1 (1(V1)πmin)2 . We provide numerical example in Figure 9 to illustrate the above theoretical result. Building on the two-armed setting in Remark 4.2, we conduct additional numerical experiments under unclipped GRPO training. As shown in Figure 9, entropy growth occurs only when the policy is initialized in sufficiently skewed regime. This observation underscores that injecting spurious rewards without clipping can help preserve or restore entropy in GRPO training, particularly when the policy entropy has already collapsed or degraded toward highly skewed distribution. Then, we proceed to the entropy dynamics with upper clipping. Before proving Theorem 4.3, we present some useful lemmas. 28 Preprint. Under review. Figure 9: Simulation of policy entropy evolution over unclipped GRPO training. Each panel includes the result with 10 independent trails. Flat (relatively less-skewed) policy π initialization (Left); Skewed policy π initialization (Right). Lemma C.1. The surrogate objective ˆJ(π) with only upper clipping can be rewritten as ˆJ(π) = 1 (cid:88) aV (S(a)r(a) + S+(a) min{r(a), 1 + ε}). There exist λ and {µa}aV with µa 0 and µarc(a) = 0 such that, 1 (S(a) + S+(a)ξa) πold(a) η (log rc(a) + 1) λπold(a) + µa = 0, for every a, where ξa = 1, 0, [0, 1], if rc(a) < 1 + ε, if rc(a) > 1 + ε, otherwise. In particular, if rc(a) > 1 + ε, we have ξa = 0 and log(rc(a)) = ηS(a) πold(a)G ηλ 1. Proof. Since π(a) = πold(a)r(a), we have (cid:80) DKL(π rπ) = πold(a)r(a) = 1, r(a) 0 and (cid:88) π(a)r(a) log(r(a)). By definition, we have (cid:40) 1 πc new := arg max r0 πrV (cid:88) (S(a)r(a) + S+(a) min{r(a), 1 + ε}) 1 η (cid:41) π(a)r(a) log(r(a)) . (cid:88) Since the objective is concave in and the constraint qualification holds, the KKT conditions are necessary and sufficient. We define g(r) := min{r, 1 + ε} and derive its subdifferential as follows, g(r) = {1}, {0}, [0, 1], if < 1 + ε, if > 1 + ε, otherwise. We introduce the Lagrangian function with λ for the equality constraint and µa 0 for r(a) 0: L(r, λ, µ) = 1 (cid:88) (S(a)r(a) + S+(a) min{r(a), cap}) 1 η (cid:88) π(a)r(a) log(r(a)) λ (cid:33) π(a)r(a) 1 + (cid:32) (cid:88) (cid:88) µar(a). For ξa min{r(a), 1 + ε}, we have 0 r(a)L = (S(a) + S+(a)ξa) π(a) η (log(r(a)) + 1) λπ(a) + µa. We also have µar(a) = 0. This implies µa = 0 for any with rc(a) > 0 and π(a) η (log(rc(a)) + 1) = 1 (S(a) + S+(a)ξa) λπ(a). If rc(a) > 1 + ε, we have ξa = 0. Putting these pieces together yields the desired result. 29 Preprint. Under review. Lemma C.2. For any group of samples {y(i)}G have i=1, we define := {y(1), . . . , y(G)}. Then, we πold(U ) = πold(a) (V G)πmin =: M0. (cid:88)"
        },
        {
            "title": "Suppose that",
            "content": "aU (1+ε)η 2 < (cid:16) M0 1 2 (cid:112)η(1 + ε) (cid:17) log(1 + ε). (17) Then, we have rc(a) 1 + ε for all V. Proof. First, we show DKL(πc Since 1 and ˆJ(πold) = (cid:80) newπold) η ˆJ(πc that DKL(πc reward +1 in the group 1 1, we have 2 η(1 + ε). By definition, we have (πc newπold) 1 new) (πold). Ai = 0, we have (πold) = 0. Thus, we obtain from (πc new) 0 new). Due to the upper clipping, given the number of samples with ˆJ(πc new) (1 + ε) (cid:88) i=1 Ai1{Ai > 0} = (1 + ε) (cid:113) 1K/G K/G = (1 + ε) (cid:113) (cid:0)1 (cid:1) 1+ε 2 . Putting these pieces together yields the desired result. Then, we show πc as follows, new(U ) M0 η(1+ε) 2 . Indeed, this can be derived from the Pinskers inequality πc new(U ) πold(U ) πc new πoldTV (cid:113) 2 DKL(πc newπold) = 1 2 (cid:112)η(1 + ε). This together with πold(U ) M0 yields the desired result. Finally, we show that rc(a) 1 + ε for all given Eq. (17). Suppose that there are some so that rc(a) > 1 + ε. Then, by Theorem C.1, we have ξa = 0 and S(a)0 log(rc(a)) = ηS(a) ηλ 1, πold(a)G ηλ 1 By definition, S+(b) = S(b) = 0 for all . Thus, we have log(rc(b)) = ηλ 1 and rc(b) rc(a) > 1 + ε for all . This implies DKL(πc newπold) = (cid:88) new(a) log(rc(a)) > log(1 + ε)πc πc new(U ). Putting these pieces together yields direct contradiction to Eq. (17) as follows, (cid:19) (cid:18) 2 η(1 + ε) DKL(πc newπold) > log(1 + ε) M0 (1+ε)η 2 . This completes the proof. Lemma C.3. Consider the same update as in Theorem 4.1. Let be possibly random set such that = 0 for all / S. Let πmin,S := minaS πold(a) with the convention πmin, = 1. Fix any threshold ˆπ (πmin, 1) and define Bˆπ := {πmin,S < ˆπ} and qˆπ := P(Bˆπ). Then, for any η > 0, we have E[R(η)] (1 qˆπ)C(ˆπ)η4 + qˆπC(πmin)η4, where C(π) is defined as the same as in Theorem 4.1. Proof. We mimic the proof of Theorem 4.1 (and using the same notation) to obtain: (η) = (0) 2 η2 + (4)(t) 24 η4. We reuse the expression derived for d4 E[R(η)] = E[R(η) Bc dt4 H(πt). Now, we split the expectation as follows, ˆπ](1 qˆπ) + E[R(η) Bˆπ]qˆπ. ˆπ, we have πold(a) ˆπ for all S. Since A(a) = 0 outside S, every place in the remainder On Bc proof where πmin is used to upper bound denominators (e.g. A, Eπt[ A4], and the change-of-measure factor exp(t/πmin)) can be repeated with ˆπ instead. This yields R(η) C(ˆπ)η4 on Bc ˆπ. On Bˆπ, we just use the original bound R(η) C(πmin)η4. Plugging into the conditional expectation decomposition yields the desired result. Preprint. Under review. Theorem C.3 gives us way to shrink the remainder term E[R(η)] if we can find an appropriate S. In the unclipped setting, we cannot benefit from this since it is hard to find S. However, this becomes useful in the upper-clipped case. We now proceed to the proof of Theorem 4.3. := {Ai > 0, ru(y(i)) > 1 + ε}. Let ρ := P(C1) and Theorem 4.3 (restated). Define Ci δ = E[ru(y(1)) (1 + ε) C1]. Then, for η > 0 satisfying Eq. (17) and any (πmin, 1), we have E[H(πc new) H(πold)] cGΦ(πold)η2 + E[Rc(η)] + c(p)G (cid:0)ρδeff Xmax (G 1)p(cid:1) , where cG and Φ are defined as the same as in Theorem 4.3, and c(p) := πmin Xmax := exp(η/(2πmin)) (1 + ε), (p) := [exp(η/(2p)) (1 + ε)]+. δeff := Xmax(δM (p))+ (cid:0)log peη/(2πmin)(cid:1) XmaxM (p) , , Proof. For simplicity, define Hc := H(πc we have new) H(πold) and Hu := H(πu new) H(πold). Then, E[Hc] = E[H(πc new) H(πu new)] + E[Hu]. We define := {y(i) : Ai > 0, ru(y(i)) 1 + ε}. Then, the bad event set can be set as follows, Bˆπ := { : Ai > 0, ru(y(i)) 1 + ε, πold(y(i)) ˆπ}. By using Theorem 4.1 and Theorem C.3, we have new] = 121G E[Hu 2G Φ(πold)η2 + E[Rc(η)]. (18) It suffices to consider E[H(πc new) H(πu new)]. Indeed, we consider H(πc new) = (cid:88) new(a) log(πc πc new(a)) = (cid:88) new(a) log(πu πc new(a)) DKL(πc newπu new). Since DKL(πc new) 0, we have newπu new) H(πu H(πc new) (cid:88) new(a) log(πu πc new(a)) + (cid:88) (πu new(a) πc new(a)) log(πu new(a)). (cid:88) new(a) log(πu πu new(a)) By Theorem C.2, we have πc new(a) (1 + ε)πold(a) for every a. If ru(a) > 1 + ε, we have aV new(a) πc πu new(a) = πold(a)ru(a) πc new(a) πold(a)(ru(a) (1 + ε)) Thus, we have πmin(ru(a) (1 + ε)). H(πc new) H(πu new) (cid:88) aV πmin(ru(a) (1 + ε)) log(πu new(a)). (19) For any (0, 1 ], on the set {πold(y(i)) p}, we have log(πu new(y(i))) = log(πold(y(i)))ru(y(i)) min{0, log(peη/(2πmin))}. Using Eq. (19), restricting to indices where Ci occurs, we have H(πc new) H(πu new) min{0, log(peη/(2πmin))} min{0, log(peη/(2πmin))} (cid:88) πmin(ru(a) (1 + ε)) a:ru(a)>1+ε (cid:88) a:ru(a)>1+ε,πold(a)p πmin(ru(a) (1 + ε)). We let Xi := (ru(a) (1 + ε))1Ci and na := (cid:80)G j=1 1{y(j) = a}1Cj . Then, we have (cid:33) (cid:32) (cid:88) Xi1{πold(y(i))p} y(i) . (cid:88) πmin(ru(a) (1 + ε)) = πmin a:ru(a)>1+ε,πold(a)p i=1 31 Preprint. Under review. Conditioned on y(i) = and πold(a) p, we have := ny(i) 1 Binomial(G 1, πold(a)). Thus, we have E[K1{πold(y(i)) p}] = E[E[K πold(y(i)) p]1{πold(y(i)) p}] = E[(G 1)πold(y(i))1{πold(y(i)) p}] (πold(a))21{πold(a) p} = (G 1) (cid:88) Since for any 0, we always have (G 1)p. K+1 1 2 , 1 Xi1{πold(y(i))p} y(i) Xi1{πold(y(i)) p} 1 2 Xi1{πold(y(i)) p}(ny(i) 1). Thus, we have (cid:20) (cid:21) Xi1{πold(y(i))p} y(i) E[Xi1{πold(y(i)) p}] Xmax 2 (G 1)p, where Xi Xmax := exp(η/(2πmin)) (1 + ϵ). This Xmax is derived by recalling that A(a) 1/(2πmin) and noticing that Jensens inequality implies Z(η) := Eπold [eη A(a)] eηEπold [ A(a)] = 1. Taking expectation of both sides of Eq. (19) yields E[H(πc new) H(πu new)] c(p)G (cid:16) E[Xi1{πold(y(i)) p}] Xmax 2 (G 1)p (cid:17) , (20) where we denote c(p) := min{0, log(peη/(2πmin))}πmin. Finally, notice that on {πold(y(i)) > p}, we have another bound Xi (p) := [exp(η/(2p))(1+ε)]+. By symmetry, for any = 1, . . . , G, we have δ = E[r(y(i)) (1 + ε) Ci] = E[Xi Ci] = E[Xi1{πold(y(i)) p} Ci] + E[Xi1{πold(y(i)) > p} Ci] XmaxP(πold(y(i)) Ci) + (p)(1 P(πold(y(i)) Ci)). This implies which further guarantees P(πold(y(i)) Ci) Xmaxδ XmaxM (p) , E[Xi1{πold(y(i)) p} Ci] = δ E[Xi1{πold(y(i)) > p} Ci] δ (p)P(πold(y(i)) Ci) δ (p) Xmaxδ XmaxM (p) = Xmax(δM (p)) XmaxM (p) . Notice that E[Xi1{πold(y(i)) p} Ci] is nonnegative. Then, we have E[Xi1{πold(y(i)) p} Ci] Xmax(δM (p))+ XmaxM (p) := δeff . By using the symmetry again, we have E[Xi1{πold(y(i)) p}] = P(Ci)E[Xi1{πold(y(i)) p} Ci] ρδeff . Therefore, Eq. (20) becomes E[H(πc new) H(πu new)] c(p)G (cid:0)ρδeff Xmax 2 (G 1)p(cid:1) . Combining Eq. (18) and Eq. (21), we obtain the desired result E[H(πc) H(πu)] 121G = 121G 2G Φ(πold)η2 + E[R(η)] + c(p)G (cid:0)ρδeff Xmax 2G Φ(πold)η2 + E[R(η)] Gπmin 2 log(peη/(2πmin)) (G 1)p(cid:1) (cid:17) (cid:16) (cid:0)ρδeff Xmax 2 (21) (22) (23) (G 1)p(cid:1) .(24) This completes the proof. 32 Preprint. Under review. Remark C.4. In this remark, we will show that under practical settings, the assumption, i.e., Eq. (17), in Theorem 4.3 is indeed satisfied and the extra term ensure the entropy is decreasing in expectation. Recall the parameters we used or observed in experiments: = 16, η = 5 107, 150000, πmin = 107, ϵ = 0.2, ρ 0.001, δ 10, and we have freedom to choose = 2πmin = 2 107. By Theorem C.5, P(B) 0.0071. We can choose ˆπ = 106, then E[Rc(η)] 4.32 109. The third term in ?? is 6.72 108. Thus, we require Φ(πold) > 8.05 106 to ensure E[H] < 0. However, we compute that Φmin = 2.23 106, thus the condition Φ(πold) > 8.05 106 always hold and we can conclude E[H] < 0. Lemma C.5. Let := (cid:8)i : π(y(i)) = πmin, Ai > 0, ru(y(i)) 1 + ϵ(cid:9) and (cid:16) η (cid:17) Zmax(η) := 1 + η 2 exp 2πmin , 0 := Gπmin η log ((1 + ϵ)Zmax(η)) . Let Vmin := {a : π(a) = πmin} and µmin := π(Vmin) Vπmin. Define the collision event among πmin-tokens: Collmin := {a Vmin : Na 2}, Na := (cid:88) i=1 1{Y (i) = a}. Then P(B) GµminP(0 < A1 0) + (cid:0)G (cid:1)Vπ2 min. 2 Proof. Recall that A(a) 1/(2πmin). Write Z(η) 1 = (cid:88) π(a)(eη A(a) 1) = (cid:88) π(a)(eη A(a) 1), aV since ex 1 0 for 0. For 0, ex 1 xex. Hence a: A(a)> Z(η) 1 (cid:88) π(a)η A(a)eη A(a). a: A(a)>0 Using eη A(a) exp(η/(2πmin)), we get (cid:16) η Z(η) 1 η exp 2πmin (cid:17) (cid:88) π(a) A(a) 1 2 exp (cid:16) η 2πmin (cid:17) . a: A(a)>0 Thus, Z(η) Zmax(η). Decompose (B Collc then the token y(i) appears exactly once in the batch, hence A(y(i)) = Ai ru(y(i)) 1 + ϵ, then by definition exp(η A(y(i))) Z(η) min) Collmin. On Collc 1 + ϵ = exp(η A(y(i))) (1 + ϵ)Z(η) (1 + ϵ)Zmax(η), Gπmin min, if π(y(i)) = πmin, . If additionally Taking logs and substituting A(Y (i)) = Ai/(Gπmin ) gives Ai Gπmin η log((1 + ϵ)Zmax(η)) = 0. Therefore, By union bound, Collc min {i : π(y(i)) = πmin, 0 < Ai 0}. P(B Collc min) (cid:88) (cid:16) i=1 π(y(i)) = πmin, 0 < Ai 0 (cid:17) . Since y(i) is independent of Ai, (cid:16) π(y(i)) = πmin, 0 < Ai 0 (cid:17) (cid:16) = (cid:124) π(y(i)) = πmin (cid:123)(cid:122) =µmin (cid:17) (cid:125) (0 < A1 0) , min) GµminP(0 < A1 0). For any fixed Vmin, Na Binomial(G, πmin) so P(B Collc and (Na 2) (cid:0)G (cid:1)π2 min. Union bound over all Vmin yields min (cid:0)G P(Collmin) Vmin(cid:0)G (cid:1)π2 (cid:1)π2 2 min. This completes the proof. 33 Preprint. Under review. C.4 MISSING PROOFS IN 5 Proof of Theorem 5.2. Since Binomial(ni, 1 2 . We rewrite Eq. (8) as = nc(f g) and E[g] = nc + and have 2 ) and Binomial(nc, 1 2 ), we have E[f ] = ni 2 In addition, we have Var(f ) = ni Var() = (cid:0) nc This completes the proof. (cid:1) 2G E[] = nc(ninc) 4 and Var(g) = nc (cid:1)2 2 = nc(Gnc) + nc 4 . Since and are independence, we have (cid:1)2 nc (cid:1)2 ni . Var(f ) + (cid:0) ni Var(g) = (cid:0) nc 4 + (cid:0) ni 4 = nc(Gnc) 4G . Proof of Theorem 5.3. We define = and = nc g. Then, we have Binomial(nc, 1 2 ) and = ninc + ncX niY . (25) + ni(ncY ) By definition, = + Binomial(G, 1 2 ) and = ncX > > nc, By definition, we have Pr(rj = 1 = z) = E[X = z] = ncz < < nc. for each and each C. Thus, we have , E[Y = z] = niz . Taking the conditional expectation of both sides of Eq. (25) yields E[ = z] = ninc + nc E[X = z] ni E[Y = z] = ninc . By using the tower property, we have E[ > g] = E[ > ] = ninc . Note that E[1{f >g}] = E[ > g] Pr(f > g) and E[1{g<f }] = E[ > ] Pr(g > ). Thus, it suffices to prove that Pr(f > g) < Pr(f < g). Indeed, we write them in wedge form as Pr(f > g) = 1 2G (cid:88) (cid:18)ni (cid:19)(cid:18)nc ℓ (cid:19) , Pr(g > ) = 1 2G (cid:18)nc (cid:19)(cid:18)ni ℓ (cid:19) . (cid:88) k>ℓ (26) k>ℓ Fixing the integers > ℓ 0, we define the function as follows, We claim that Ψ(n) is strictly increasing in n. Indeed, we have Ψ(n) = (n k) (n ℓ) , for all k. (nk)! = ℓ! Each term in the product is strictly increasing in n. Thus, this yields the desired result. Ψ(n) = ℓ! k! j= k! (cid:0)Πkℓ1 (n ℓ j)(cid:1) . (nℓ)! Since nc > ni k, we have (nc ) (nc ℓ ) = Φ(nc) > Φ(ni) = (ni ) (ni ℓ ) . This implies (cid:19) (cid:18)nc (cid:19)(cid:18)ni ℓ > (cid:18)ni (cid:19)(cid:18)nc ℓ (cid:19) , for all > ℓ. This together with Eq. (26) yields the desired result. Conditional variance analysis. Let Binomial(ni, 1 2 ) be independent, and let be defined in Eq. (8). We write = , = nc and let = + Binomial(G, 1 2 ) and Binomial(nc, 1 2 ). Then, we have E[ = z] = ninc , Var( = z) = ni(Gni) G1 z(Gz) G2 . We let = ni(Gni) G2(G1) and define h(z) = z(G z). Then, we have Var( > g) = CE[h(Z) > nc], Var( > ) = CE[h(Z) < nc]. If nc > ni, we have Var( > g) < Var( > ). 34 Preprint. Under review. Proof. Conditional on = z, the positive labels are uniformly scattered among positions. Then, the count of positives falling inside the ni indices of is = Hypergeometric(G, z, ni). Thus, we have E[X = z] = niz , Var(X = z) = niz (1 ) Gni G1 . By definition, we have = ncX + ni(ncY ) = niZ + ninc . and and E[ = z] = E[X = z] niz + ninc = ninc , Var( = z) = Var(X = z) = ni(Gni) G1 z(Gz) . We let = ni(Gni) G2(G1) and define h(z) = z(G z). Since E[ Z] is independent of Z, we have Var( A) = E[Var( Z) A] = CE[h(Z) A], for an event measurable w.r.t. Z. Since > > nc and > < nc, we have Var( > g) = CE[h(Z) > nc], Var( > ) = CE[h(Z) < nc]. Since Binomial(G, 1 we have 2 ), h(Gz) = h(z) and h(z) is strictly increasing on {0, 1, 2, 3, . . . , 2 }, E[h(Z) > nc] = E[h(Z) < nc]. Since nc > 2 , we have 0 nc < nc G. Thus, we have E[h(Z) < nc] < E[h(Z) < nc]. Multiplying both sides of the above inequality by > 0 yields Var( > g) < Var( > ). This completes the proof."
        }
    ],
    "affiliations": [
        "CUHK SZ",
        "Columbia",
        "DAMO, Alibaba US",
        "NYU Stern"
    ]
}