{
    "paper_title": "NullFace: Training-Free Localized Face Anonymization",
    "authors": [
        "Han-Wei Kung",
        "Tuomas Varanka",
        "Terence Sim",
        "Nicu Sebe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Privacy concerns around ever increasing number of cameras are increasing in today's digital age. Although existing anonymization methods are able to obscure identity information, they often struggle to preserve the utility of the images. In this work, we introduce a training-free method for face anonymization that preserves key non-identity-related attributes. Our approach utilizes a pre-trained text-to-image diffusion model without requiring optimization or training. It begins by inverting the input image to recover its initial noise. The noise is then denoised through an identity-conditioned diffusion process, where modified identity embeddings ensure the anonymized face is distinct from the original identity. Our approach also supports localized anonymization, giving users control over which facial regions are anonymized or kept intact. Comprehensive evaluations against state-of-the-art methods show our approach excels in anonymization, attribute preservation, and image quality. Its flexibility, robustness, and practicality make it well-suited for real-world applications. Code and data can be found at https://github.com/hanweikung/nullface ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 8 7 4 8 0 . 3 0 5 2 : r NullFace: Training-Free Localized Face Anonymization Han-Wei Kung1 Tuomas Varanka2 Terence Sim3 Nicu Sebe1 1University of Trento 2University of Oulu 3National University of Singapore"
        },
        {
            "title": "Localized facial anonymization",
            "content": "Inpainting [58]"
        },
        {
            "title": "Keep mouth",
            "content": "Figure 1. Our method obscures identity while preserving attributes such as gaze, expressions, and head pose (in contrast to Stable Diffusion Inpainting [58]) and enables selective anonymization of specific facial regions."
        },
        {
            "title": "Abstract",
            "content": "nullface. Privacy concerns around ever increasing number of cameras are increasing in todays digital age. Although existing anonymization methods are able to obscure identity information, they often struggle to preserve the utility of the images. In this work, we introduce training-free method for face anonymization that preserves key non-identity-related attributes. Our approach utilizes pre-trained text-toimage diffusion model without requiring optimization or training. It begins by inverting the input image to recover its initial noise. The noise is then denoised through an identity-conditioned diffusion process, where modified identity embeddings ensure the anonymized face is distinct from the original identity. Our approach also supports localized anonymization, giving users control over which facial regions are anonymized or kept intact. Comprehensive evaluations against state-of-the-art methods show our approach excels in anonymization, attribute preservation, and image quality. Its flexibility, robustness, and practicality make it well-suited for real-world applications. Code and data can be found at https://github.com/hanweikung/ 1. Introduction As privacy regulations such as the General Data Protection Regulation (GDPR) [3] in the European Union, the California Consumer Privacy Act (CCPA) [2] in California, and the amended Protection of Personal Information (APPI) [1] in Japan continue to evolve, the imperative for robust methods to protect personal data intensifies. Facial images, as biometric identifiers, represent highly sensitive form of personal data. Organizations must adopt effective anonymization techniques to navigate these regulatory landscapes, safeguard privacy, and mitigate risks associated with cyberattacks [33]. Many methods have been developed to anonymize facial data [10, 50]. Traditional face anonymization methods like masking, pixelation, and blurring are popular due to their simplicity, but they reduce image quality [23]. Recent learning-based approaches using Generative Adversarial Networks (GANs) [20] can produce more realistic anonymous faces. However, GAN-based methods still struggle to preserve non-identity-relevant attributes because they often depend on imprecise conditioning data like facial landmarks [32, 48, 69]. They also need meticulous tuning of multiple competing objectives [45] that are difficult to balance correctly. This limitation compromises the utility of anonymized data in applications like emotion analysis [42], behavioral studies [14], and medical measurements using rPPG [72]. Notably, More recently, diffusion models [29, 58, 68] have gained attention for face anonymization due to their ease of training, realistic outputs, and fine-grained control. However, prior research [43] highlights risk of overfitting, especially when training data is imbalanced across ethnicity or age groups, limiting their adaptability to diverse data scenarios. some GAN-based [32, 48, 69, 70] and diffusion-based [40] methods use inpainting, where facial regions are detected, masked, and replaced with anonymous faces to maintain overall image integrity. However, this technique overwrites both identifying and non-identifying facial features, making the anonymized images less useful. To tackle issues such as image quality degradation and difficulties in preserving non-identity-related attributes, we introduce training-free method leveraging latent diffusion inversion with an identity-conditioned model. Our method avoids training challenges and loss balancing issues by utilizing pre-trained diffusion model. It leverages the Denoising Diffusion Probabilistic Model (DDPM) [29] scheduler to invert the latent representation of an input image, retrieving its initial noise [30]. This inversion enables control over the anonymization process while maintaining fidelity to non-identifying facial attributes. Since the text-to-image Stable Diffusion model [58] is solely conditioned on text prompts, we integrate an adapter [77] to enable conditioning on identity embeddings. During denoising, we apply controllable identity embedding to suppress the original identity, achieving effective anonymization. Furthermore, localized anonymization, which selectively anonymizes specific facial regions, is valuable in medical settings [34] where privacy is crucial, but specific clinical details must remain visible. For example, dermatologist performing reconstructive surgery on patient with facial burns may need to share post-surgery images in journals or conferences to highlight the treated area, such as the cheek or forehead. However, to protect the patients privacy, the full face must remain unrecognizable. In such cases, localized anonymization can mask the rest of the face, retaining only the treated area. This approach enables doctors to share knowledge with the medical community without violating the patients privacy. For localized control, we use segmentation maps to keep the targeted regions visible while anonymizing masked areas. Unlike Stable Diffusion Inpainting [58], which often changes non-identity attributes in masked areas, our method integrates segmentation maps with inversion to preserve them. Key contributions of our method include: Training-free anonymization. By leveraging diffusion inversion and identity embeddings, our method anonymizes identity while preserving non-identifiable facial features, eliminating the need for training or fine-tuning. Localized anonymization. Segmentation masks enable control over which facial regions are anonymized, providing flexibility to balance privacy and usability. Integration of inversion and segmentation maps. Combining inversion with segmentation maps enhances the preservation of important facial attributes compared to conventional inpainting methods. State-of-the-art performance. Our approach achieves competitive results on benchmark datasets, demonstrating its effectiveness in privacy-preserving applications without compromising data quality. 2. Related work Face anonymization. Early face anonymization methods like pixelation, blurring, or mask overlays often degrade image quality. To overcome this limitation, recent deep learning advancements have employed GANs [20] to produce more realistic results [11, 13, 21, 25, 31, 32, 59, 69, 70, 75, 79]. Methods like [31, 32, 69, 70] use inpainting to fill masked regions with synthetic faces, while others [6, 45, 46] involve latent space manipulation with StyleGAN2 [37]. Techniques such as RiDDLE [45] and FALCO [6] use GAN inversion [71] to generate high-quality anonymized faces. Some methods [21, 24, 45, 76, 80] also enable the restoration of anonymized faces through key, similar to encryption and decryption. More recent anonymization methods utilize diffusion models. LDFA [40] implements two-stage approach, using face detection model followed by latent diffusion model to create anonymous faces through inpainting. Diff-Privacy [24] develops custom image inversion module to generate conditional embeddings for anonymization. IDDiffuse [67] introduces dual-conditional diffusion framework that maps synthetic identities, ensuring the anonymized version of an individual remains consistent across an entire video. FAMS [43] uses synthetic training data to achieve anonymization by modeling the process as face swapping. In contrast, our approach offers trainingfree solution by utilizing diffusion model inversion to extract initial noise and conditioning anonymized identities during the denoising process. Diffusion-based image editing. Text-guided diffusion models [54, 5658, 63] have excelled in image synthesis. Prompt-to-Prompt [26] enables text-based editing of synthetic images, while SDEdit [51] supports real image editing by adding noise to the input and denoising it using user2 drawn strokes. However, maintaining fidelity between original and edited real images has been challenging. To address this, researchers developed inversion techniques to enhance editing accuracy by reconstructing noisy input image representation [16, 30, 52, 73]. Advancements like Null-text Inversion [52] have improved fidelity by optimizing the inversion process with null prompt, ensuring edits match the text while preserving original features. LEDITS++ [9] uses more efficient inversion technique with dpmsolver++ for faster convergence and supports simultaneous editing of multiple concepts. We adapt these inversion-based editing techniques to modify facial features while maintaining the original images fidelity. Personalized image generation using diffusion models. Recent advances in diffusion models have enabled identityconsistent image generation guided by minimal input, such as text or visual prompts. Textual Inversion [19] learns word embeddings from small set of images (typically 35) to capture specific visual concepts. DreamBooth [62] fine-tunes diffusion models to reflect unique traits of individuals across diverse contexts. IP-Adapter [77] employs decoupled cross-attention mechanism alongside trainable projection network to enable personalized prompts for pretrained diffusion models. InstantID [74] integrates an Image Adapter, which facilitates the injection of image-based prompts using cross-attention, and IdentityNet, which ensures robust identity preservation. PuLID [22] combines contrastive alignment loss and identity-specific loss to achieve high fidelity and editability, avoiding overfitting through careful regularization. While these methods focus on conditioning diffusion models to generate personalized images, our approach shifts to identity anonymization. Instead of preserving or enhancing an identity, we use identity embeddings to remove defining features, repurposing identity conditioning to obscure the original identity. This leverages diffusion models capabilities to create realistic images while ensuring the individual is unrecognizable. 3. Method This section details our anonymization framework, which integrates diffusion model inversion, dual-path denoising structure, and modified face embeddings. We also extend it for localized anonymization by integrating segmentation maps with diffusion inversion. 3.1. Preliminary DDPM inversion. Inversion involves determining the image x0. While latent xT that generates the latent DDIM [68] enables deterministic sampling and can be inverted to find xT , prior research [52] revealed that this latent is unsuitable for editing with classifier-free guidance [28]. DDPM inversion [30] addresses this limitation by not only identifying the initial latent xT but also computing the set of zt, representing the noise applied when transitioning from one noisy image xt to the next xt1: zt = xt1 µt(xt, c) σt , = T, . . . , 1. (1) Here, is conditioning prompt, µt(xt, c) is the output of the denoising network, and σt is the standard deviation from the noise scheduler. Using the set zt during editing ensures minimal changes to the image structure. IP-Adapter. IP-Adapter [77] enhances text-to-image diffusion models with image prompt capabilities by modifying the attention mechanism. It introduces decoupled crossattention approach that processes text and image features independently, preserving the integrity of the pre-trained base model. The outputs of the decoupled cross-attention are combined with the original cross-attention using scaling factor: = Attention(Qnoise, Ktext, Vtext) + λimg Attention(Qnoise, Kimg, Vimg), (2) where λimg is the scaling factor, and {Q, K, }source represent the projection matricesqueries (Q), keys (K), and values (V )used to project various types of source features. This approach is resource-efficient, requiring minimal parameters, yet it enables significant enhancements. Additionally, the method has been extended to include other features, such as identity embeddings, allowing text-toimage models to condition outputs on an individuals identity. 3.2. Anonymization with Inversion Inversion-based face editing. Previous face anonymization methods [32, 40, 41, 48, 69, 70] often treat the task as an image inpainting problem, where the facial region is first erased and then replaced with an alternative identity. However, erasing the original face also removes essential non-identity features, such as gaze direction, head pose, and facial expressions, compromising attribute preservation. While some methods [32, 41, 48, 69, 70] use annotations like facial landmarks to retain these attributes, such annotations can be unreliable. To overcome these limitations, we adopt noise-level editing approach instead of inpainting, eliminating reliance on facial landmarks. The initial noise of the image is accessed through inversion techniques. In our experiments, we compared popular methods like DDIM [52] and DDPM [30] inversion and found that DDPM inversion achieves higher fidelity in reconstructing the original image. 3 Figure 2. Face anonymization pipeline using diffusion model inversion. Starting with an input facial image, we perform DDPM inversion [30] to retrieve the initial noise map xT and sequence of noise maps {zt} from the diffusion process. Face embeddings are extracted using face recognition model [15] and negated with hyperparameter λid, creating negative identity guides. These guides steer the model away from reconstructing the original identity during denoising. The denoising process begins with xT , combining conditional and unconditional paths. The conditional path utilizes negated identity embeddings to obscure identifiable features, while the unconditional path uses null embeddings () to preserve non-identifying attributes. Outputs from both paths are merged using guidance scale parameter λcf through Eq. (3). Lastly, optional masks can be applied at each iteration to control which facial features are anonymized or retained, enabling localized anonymization. Our pipeline, illustrated in Fig. 2, begins with an input facial image. The image undergoes the DDPM inversion [30] process to derive its initial noise condition and sequence of noise maps {zt}, as described in Eq. (1). In the denoising phase, we condition the generation on controlled identity to achieve anonymization. Since the Stable Diffusion model [58] uses text inputs rather than identity embeddings, we integrate IP-Adapter [77] to enable conditioning on identity embeddings. However, unlike previous methods [12, 22, 47] that generate toward specified identity, we operate in reverse to anonymize the input identity, as detailed in the Controllable Training-free Identity Variations paragraph of this section. Moreover, to better preserve original facial attributes, we apply the identity embedding condition only during portion of the denoising phase. Specifically, we introduce parameter, Tskip, to determine how many of the total denoising steps to skip before applying the identity condition. Skipping more steps generates an image that aligns more closely with the original. Conditional and unconditional denoising paths. To modify the identity, the denoising process follows two parallel paths: conditional path guided by identity embeddings cid and an unconditional path that preserves nonidentity-related facial attributes. The outputs of these paths are combined using guidance scale parameter λcf g, following the classifier-free guidance technique [28], as defined by the following equation: ˆϵθ(xt, cid, ) = λcf ϵθ(xt, cid) + (1 λcf g) ϵθ(xt, ). (3) Here, λcf represents the guidance scale, which determines the relative contributions of the two paths. ϵθ denotes the network, xt is the noisy version of the input at timestep t, where = 1, . . . , , cid refers to the identity embeddings, and refers to the null identity embeddings. This dual-path framework balances anonymization with the preservation of original non-identity-related features. The conditional path focuses on incorporating the alternative identity representations, while the unconditional path ensures the retention of attributes like gaze direction and facial expressions. Together, these paths enable the generation of anonymized faces that conceal the original identity while preserving important non-identity-related attributes. Controllable training-free identity variations. The conditional path modifies the original identity to an alternative one, but what should this alternative identity be for effective anonymization? Previous methods [32, 48] train generative models to create synthetic identities, which are constrained by the scope of the training data. To avoid such 4 limitations, we adopt training-free approach by using pre-trained face recognition model [15] to extract face embeddings from the input image. This leverages the face recognition models discriminative ability to capture diverse identity variations without additional training. We also aim for mathematically interpretable anonymization process, where the degree of deviation from the original identity can be controlled. To this end, we introduce an anonymization parameter λid, positive scalar that scales the extracted face embeddings. The scaled embeddings are then negated [59] and used as the alternative identity. By increasing λid, the alternative identity becomes increasingly distinct from the original. During the denoising process, the model is conditioned on these negated embeddings, ensuring the generated identity diverges from the original, effectively anonymizing it. Finally, attempts to recover the original identity using these face embeddings fail due to the multiple factors involved in our method. We provide examples illustrating the robustness of our approach against such attacks in the supplementary material. 3.3. Localized anonymization Localized facial anonymization can balance patient privacy with clinical utility, enabling medical practitioners to showcase detailed views of specific areas while protecting patient identity. To achieve this, we use segmentation masks to selectively anonymize facial regions. However, unlike standard inpainting [58], which paints over masked regions and loses non-identity-related features, our approach combines segmentation masks with outputs from the dual-path framework that incorporates noise information obtained through inversion (see Sec. 3.2). Precisely, during the denoising process, the segmentation mask is applied by: ϵθ(xt, cid, ) = ˆϵθ(xt, cid, ) + (1 ) ϵθ(xt, ). (4) Here, is applied to the combination of both paths to ensure anonymization in masked regions while preserving non-identifying features. Simultaneously, the complement of the mask, 1 , is applied to the unconditional path to keep details in unmasked areas visible. This integration allows customizable control, ensuring the relevant regions remain clear while anonymizing other parts of the face. 4. Experiments We analyze the impact of key components in our approachTskip, the anonymization parameter (λid), the guidance scale (λcf g), and segmentation maskson image alignment, identity anonymization, and feature preservation. Evaluations on two public datasets demonstrate our methods competitive performance against existing baselines. An ablation study further highlights the critical role of diffusion model inversion in achieving these results. Beyond full-face anonymization, we also demonstrate the usefulness of localized facial anonymization in applications like medical case sharing. 4.1. Controlling image alignment Tskip = 0 Tskip = 29 Tskip = 59 Tskip = Original Figure 3. As Tskip increases from 0 to higher values, the generated image progressively aligns more closely with the input, ultimately achieving near-perfect reconstruction. Our face anonymization method introduces parameter, Tskip, to regulate the degree of alignment between the generated image and the input. Tskip specifies the point in the denoising process where modified face embeddings are first injected. In process with total steps, embeddings are introduced starting at step Tskip. Increasing Tskip enhances alignment with the input image. Prior research [30] suggests this adjustment refines semantic features while preserving the overall structure. Figure 3 illustrates this effect using denoising process with = 100 steps. At Tskip = 0, the generated image deviates significantly from the input, with notable differences in body posture. As Tskip increases, the generated image progressively aligns with the inputs expression and structure, achieving near-perfect reconstruction at higher values. 4.2. Identity change with anonymization parameter Our method anonymizes faces by scaling the original face embeddings by factor of λid, where λid is positive value and serves as the anonymization parameter. Increasing λid generates faces that are progressively less similar to the original identity, allowing controlled adjustments in anonymity. Figure 4 visualizes this process, displaying the original face alongside four anonymized versions generated with λid = 0, 0.33, 0.67, and 1.0. We quantified the identity change by calculating the identity distance from the original face using the FaceNet [65] model. As λid increases, both visual inspection and identity distance confirm greater divergence from the original identity. Original λid = 0.0 λid = .33 λid = .67 λid = 1. ID Dist. 0.012 0.654 0.961 1.147 ID Dist. 0.013 0.488 0.905 1.339 Figure 4. Increasing λid generates faces that are less similar to the original, with FaceNet [65] identity distance values shown for each example. 4.3. Effect of guidance scale on anonymization The guidance scale λcf influences the degree of identity change during image generation. Figure 5 demonstrates this effect with two original images and four anonymized versions generated using λcf = 5, 10, 15, and 20. Higher guidance scale values yield anonymized identities that are more distinct from the originals, as confirmed by increasing identity distances calculated with the FaceNet [65] model. However, excessive guidance (e.g., λcf = 20) reduces photorealism. Original λcf = 5 λcf = 10 λcf = 15 λcf = ID Dist. 0.743 0.936 0.952 0.975 ID Dist. 0.457 0.667 0.671 0.725 Figure 5. As the guidance scale increases, the anonymized identities become increasingly distinct from the originals, as confirmed by identity distance measurements using FaceNet [65]. However, the version generated with guidance scale of 20 reveals that excessively high guidance scales, while widening identity distance, compromise the photorealism of the resulting images. in re-identification rates as λcf increases, indicating improved anonymity. For detailed experiments and results, please refer to our supplementary materials. 4.4. Segmentation masks on anonymization While our method does not rely on segmentation masks, they offer additional control by specifying which facial regions are revealed or concealed. Figure 6 illustrates this flexibility with various masking configurations. full-face mask ensures complete anonymization, while masks selectively revealing only the eyes, nose, or mouth preserve those regions and anonymize the rest. Change whole face Keep eyes Keep nose Keep mouth Figure 6. Localized anonymization using segmentation masks. The full-face mask fully anonymizes the face, while individual masks reveal specific facial regions (eyes, nose, mouth). We also investigated the impact of mask application timing during the denoising process and found tradeoff between attribute retention and anonymity. Detailed results, including re-identification rates, attribute preservation at different timesteps, and the effects of keeping various facial regions visible on anonymization performance, are provided in the supplementary materials. 4.5. Comparisons with baselines We evaluated our method on the CelebA-HQ [35] and FFHQ [36] datasets, comparing it against five baselines (FAMS [43], FALCO [6], RiDDLE [45], LDFA [40], and DP2 [31]) using quantitative and qualitative metrics. For consistency, our method used Tskip = 70, λid = 1.0, λcf = 10, and an eye-and-mouth-revealing mask applied at timestep 80. We further analyzed the effect of λcf on identity anonymization by evaluating re-identification rates across different values. Our results show consistent decrease Quantitative analysis. Table 1 presents quantitative comparison of our method against baseline approaches, us6 Identity Identification Re-ID (%) Attribute Distance Gaze Pose CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ Expression MUSIQ FID Image Quality MUSIQ Distance Original (input) Ours FAMS [43] FALCO [6] RiDDLE [45] LDFA [40] DP2 [31] 100.0 0.21 3.13 0.10 - 10.28 0.84 100.0 0.34 14.15 - 0.51 11.15 1.88 0.0 10.034 10.001 10.208 - 8.647 9. 0.0 9.852 8.823 - 10.038 10.387 10.183 0.0 0.165 0.164 0.277 - 0.260 0.262 0.0 0.186 0.176 - 0.215 0.353 0.299 0.0 0.053 0.053 0.088 - 0.092 0.161 0.0 0.056 0.047 - 0.081 0.113 0.163 63.590 68.790 70.449 72.815 - 60.529 49. 69.240 70.481 70.868 - 58.869 65.675 51.159 0.0 5.200 6.859 9.225 - 3.061 14.524 0.0 1.241 1.628 - 10.371 3.565 18.081 0.0 8.223 17.128 39.168 - 8.058 16.935 0.0 8.779 11.215 - 69.259 9.946 18.632 Table 1. Quantitative results on face anonymization methods for CelebA-HQ [35] and FFHQ [36] datasets. Best results are in bold, secondbest are underlined, and worst results are in red. ing over 4,500 test subjects from the CelebA-HQ [35] and FFHQ [36] datasets (4,852 from CelebA-HQ [35] and 4,722 from FFHQ [36]). To evaluate our models face anonymization performance, we measure the re-identification rate, which reflects how often the AdaFace [39] recognition model links anonymized faces to their originals. Our method achieved the lowest re-identification rate on the FFHQ [36] dataset and the second-lowest on CelebA-HQ [35], with FALCO [6] performing slightly better. We also assessed non-identity-related attributes such as head pose, eye gaze, and facial expression distances. Head pose and eye gaze distances were calculated using angular differences in predicted orientations [61] and gaze directions [4], while facial expression distance was measured using 3DMM [8] coefficients. Although FAMS [43] outperformed our method in preserving these attributes, it did so at the cost of higher re-identification rates, particularly on FFHQ [36], where it had the highest re-identification rate among all baselines. We evaluate image quality using Frechet Inception Distance (FID) [27] scores to assess how well our method preserves non-identity elements, particularly backgrounds, which often occupy large portion of the image. Our method achieved among the best results, with low FID scores across both datasets. Additionally, by measuring the difference in MUSIQ [38] scores between generated and original images, our model shows strong ability to preserve original image quality. While baselines like FALCO [6] and FMAS [43] achieve high MUSIQ scores, this does not necessarily mean they faithfully retain the original image quality, especially since the original images do not always have high scores. Thanks to our models inversion process, it maintains the source image quality rather than artificially enhancing it in way that deviates from the original. Our model achieves balanced performance across all metrics without major weaknesses. As shown in Tab. 1, each baseline has specific limitations: FAMS [43] struggles with re-identification, FALCO [6] with gaze, RiDDLE [45] with image quality, LDFA [40] with both re-identification and gaze, and DP2 [31] with pose and image quality. In contrast, our method uniquely performs well across all metrics and datasets. Original Ours FAMS [43] FALCO [6] LDFA [40] DP2 [31] Figure 7. Qualitative face anonymization results on the CelebAHQ [35] test set. Original Ours FAMS [43] RiDDLE [45] LDFA [40] DP2 [31] Figure 8. Qualitative face anonymization results on the FFHQ [36] test set. 7 Identity Identification Re-ID (%) Attribute Distance Gaze Pose CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ CelebA-HQ FFHQ Expression MUSIQ FID Image Quality MUSIQ Distance Original (input) Ours Inpainting [58] 100.0 0.10 0.00 100.0 0.50 0.10 0.0 11.621 18.143 0.0 10.941 17.922 0.0 0.214 0. 0.0 0.237 0.339 0.0 0.058 0.127 0.0 0.06 0.142 63.739 69.848 70.542 69.379 71.464 72.574 0.0 6.109 6. 0.0 2.085 3.195 0.0 18.838 35.802 0.0 21.121 40.213 Table 2. Ablation study of face anonymization implementations for CelebA-HQ [35] and FFHQ [36] datasets. Best results are in bold. Qualitative analysis. Figs. 7 and 8 present qualitative results comparing our method to baseline approaches on challenging cases from CelebA-HQ [35] and FFHQ [36]. These include faces with occlusions or extreme angles. Our method effectively anonymized identities while preserving non-identity-related details, such as pose, expression, gaze, and background information, and produced photorealistic outputs. In contrast, FALCO [6] and RiDDLE [45] failed to preserve background details, DP2 [31] struggled with pose consistency, and LDFA [40] generated less realistic outputs. Additional qualitative examples are provided in the supplementary material. 4.6. Ablation studies In this ablation study, we assess the impact of the inversion component in our face anonymization framework by comparing it with the popular Stable Diffusion Inpainting baseline [58]. Using 1,000 test subjects each from CelebAHQ [35] and FFHQ [36], we conducted quantitative evaluation based on the same metrics outlined in Sec. 4.5, focusing on identity identification, attribute retention, and image quality. For the baseline, we removed the inversion step from our framework and replaced the regular Stable Diffusion model with the Stable Diffusion Inpainting model. To ensure fairness, both methods applied identical masks covering the entire face and used the same guidance scale value of 10. For our method, we maintained Tskip of 70 and an anonymization parameter λid of 1.0, consistent with settings used in our baseline comparisons. Table 2 shows that our method outperforms the inpainting baseline in attribute preservation across all metrics and produces images that more closely match the original quality. While the inpainting baseline achieves slightly lower reidentification rates, it falls significantly short in maintaining attribute fidelity. This limitation could make the inpainting approach unsuitable for applications requiring retention of non-identity-related attributes. Additional qualitative comparisons between our method and the inpainting baseline are provided in the supplementary material. 4.7. Localized anonymization applications The motivation for selectively anonymizing facial regions stems from real-world scenarios where complete face obfuscation would eliminate essential diagnostic or analytical information. In medical contexts, practitioners often need to document and share facial symptoms (such as dermatological conditions [49, 53, 60], asymmetries [44], or expressions [78]) while maintaining patient confidentiality. Our method enables healthcare professionals to share case studies with colleagues for consultation or educational purposes without compromising patient privacy, as demonstrated in Fig. 9 where facial identity is anonymized while preserving visible pimples on the cheeks. Similarly, in humancomputer interaction and behavioral research, preserving specific regions like the eyes while anonymizing other facial features allows for privacy-preserved gaze analysis [17], facilitating studies on attention patterns [66], emotional responses [5, 55], and user experience [7] without unnecessary exposure of subjects identities. These applications highlight that localized anonymization is not merely face-editing technique but privacy-enhancing method with clear practical benefits in sensitive domains. Original Mask (keep cheek) Anonymized Figure 9. Demonstration of localized facial anonymization preserving dermatological symptoms (acne) while anonymizing identity for medical image sharing. 5. Conclusion We developed face anonymization method that uses diffusion model inversion without requiring additional training. It allows precise control over which facial features remain visible. This approach established new standards for privacy-protected facial data use. While our current diffusion-based method has longer generation times compared to traditional GAN-based approaches, emerging advances in Flow [18] and SDXL-Turbo [64] models promise significant speed improvements. These faster diffusion models will likely accelerate research adoption in this field."
        },
        {
            "title": "References",
            "content": "[1] Amended Act on the Protection of Personal Information (APPI). https://www.ppc.go.jp/files/pdf/ APPI_english.pdf. 1 8 [2] California Consumer Privacy Act (CCPA). https:// www.oag.ca.gov/privacy/ccpa/. 1 [3] General Data Protection Regulation (GDPR) Compliance Guidelines. https://gdpr.eu/. [4] Ahmed Abdelrahman, Thorsten Hempel, Aly Khalifa, Ayoub Al-Hamadi, and Laslo Dinges. L2cs-net: Fine-grained gaze estimation in unconstrained environments. In 2023 8th International Conference on Frontiers of Signal Processing (ICFSP), pages 98102. IEEE, 2023. 7 [5] Thomas Armstrong and Bunmi Olatunji. Eye tracking of attention in the affective disorders: meta-analytic review and synthesis. Clinical psychology review, 32(8):704723, 2012. 8 [6] Simone Barattin, Christos Tzelepis, Ioannis Patras, and Nicu Sebe. Attribute-preserving face dataset anonymization via latent code optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80018010, 2023. 2, 6, 7, 8, 9 [7] Jennifer Romano Bergstrom and Andrew Schall. Eye tracking in user experience design. Elsevier, 2014. 8 [8] Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3d faces. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 157164. Association for Computing Machinery, 2023. [9] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88618870, 2024. 3 [10] Jingyi Cao, Xiangyi Chen, Bo Liu, Ming Ding, Rong Xie, Li Song, Zhu Li, and Wenjun Zhang. Face de-identification: arXiv State-of-the-art methods and comparative studies. preprint arXiv:2411.09863, 2024. 1 [11] Umur Ciftci, Gokturk Yuksek, and Ilke Demir. My face my choice: Privacy enhancing deepfakes for social media anonymization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 13691379, 2023. 2 [12] Siying Cui, Jia Guo, Xiang An, Jiankang Deng, Yongle Idadapter: LearnZhao, Xinyu Wei, and Ziyong Feng. ing mixed features for tuning-free personalization of text-toimage models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 950 959, 2024. 4 [13] Nicola DallAsen, Yiming Wang, Hao Tang, Luca Zanella, and Elisa Ricci. Graph-based generative face anonymisation with pose preservation. In International Conference on Image Analysis and Processing, pages 503515. Springer, 2022. 2 [14] Geraldine Dawson, Sara Jane Webb, and James McPartland. Understanding the nature of face processing impairment in autism: insights from behavioral and electrophysiological studies. Developmental neuropsychology, 27(3):403424, 2005. In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 4, 5 [16] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, Turboedit: Text-based image arXiv preprint and Daniel Cohen-Or. editing using few-step diffusion models. arXiv:2408.00735, 2024. 3 [17] Lingyu Du, Jinyuan Jia, Xucong Zhang, and Guohao Lan. Privategaze: Preserving user privacy in black-box mobile gaze tracking services. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(3): 128, 2024. 8 [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 8 [19] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 1, 2 [21] Xiuye Gu, Weixin Luo, Michael Ryoo, and Yong Jae Lee. Password-conditioned anonymization and deanonymization with face identity transformers. In European conference on computer vision, pages 727743. Springer, 2020. 2 [22] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Peng Pulid: Pure and lightning id arXiv preprint Zhang, and Qian He. customization via contrastive alignment. arXiv:2404.16022, 2024. 3, 4 [23] Rakibul Hasan, Eman Hassan, Yifang Li, Kelly Caine, David Crandall, Roberto Hoyle, and Apu Kapadia. Viewer experience of obscuring scene elements in photos to enhance privacy. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pages 113, 2018. 1 [24] Xiao He, Mingrui Zhu, Dongxin Chen, Nannan Wang, and Xinbo Gao. Diff-privacy: Diffusion-based face privacy proIEEE Transactions on Circuits and Systems for tection. Video Technology, 2024. 2 [25] Majed El Helou, Doruk Cetin, Petar Stamenkovic, and Fabio Zund. Vera: Versatile anonymization fit for clinical facial images. arXiv preprint arXiv:2312.02124, 2023. 2 [26] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. [27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [15] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep [28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3, 4 9 [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [30] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12469 12478, 2024. 2, 3, 4, [31] Hakon Hukkelas and Frank Lindseth. Deepprivacy2: Towards realistic full-body anonymization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 13291338, 2023. 2, 6, 7, 8, 9, 10, 11, 12 [32] Hakon Hukkelas, Rudolf Mester, and Frank Lindseth. Deepprivacy: generative adversarial network for face anonymization. In International symposium on visual computing, pages 565578. Springer, 2019. 2, 3, 4 [33] Anil Jain, Arun Ross, and Sharath Pankanti. Biometrics: tool for information security. IEEE transactions on information forensics and security, 1(2):125143, 2006. 1 [34] Georgios Kaissis, Marcus Makowski, Daniel Ruckert, and Rickmer Braren. Secure, privacy-preserving and federated machine learning in medical imaging. Nature Machine Intelligence, 2(6):305311, 2020. 2 [35] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 6, 7, 8, 1, 2, 3, 4, 9 [36] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 6, 7, 8, 1, 2, 5, 10, 11, 12 [37] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of ing the image quality of stylegan. the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. [38] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 7 [39] Minchul Kim, Anil Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1875018759, 2022. 7, 1 [40] Marvin Klemp, Kevin Rosch, Royden Wagner, Jannik Quehl, and Martin Lauer. Ldfa: Latent diffusion face anonymizaIn Proceedings of the tion for self-driving applications. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31993205, 2023. 2, 3, 6, 7, 8, 9, 10, 11, 12 [41] Zhenzhong Kuang, Xiaochen Yang, Yingjie Shen, Chao Hu, and Jun Yu. Facial identity anonymization via intrinIn Proceedings of sic and extrinsic attention distraction. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1240612415, 2024. 3 [42] GA Rajesh Kumar, Ravi Kant Kumar, and Goutam Sanyal. Facial emotion analysis using deep convolution neural netIn 2017 International Conference on Signal Prowork. cessing and Communication (ICSPC), pages 369374. IEEE, 2017. 2 [43] Han-Wei Kung, Tuomas Varanka, Sanjay Saha, Terence Sim, and Nicu Sebe. Face anonymization made simple. arXiv preprint arXiv:2411.00762, 2024. 2, 6, 7, 8, 9, 10, 11, 12 [44] Laurent Lantieri, Philippe Grimbert, Nicolas Ortonne, Caroline Suberbielle, Dominique Bories, Salvador Gil-Vernet, Cedric Lemogne, Frank Bellivier, Jean Pascal Lefaucheur, Nathaniel Schaffer, et al. Face transplant: long-term followup and results of prospective open study. The Lancet, 388 (10052):13981407, 2016. [45] Dongze Li, Wei Wang, Kang Zhao, Jing Dong, and Tieniu Tan. Riddle: Reversible and diversified de-identification with latent encryptor. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 80938102, 2023. 2, 6, 7, 8, 10, 11, 12 [46] Yuchen Luo, Junwei Zhu, Keke He, Wenqing Chu, Ying Tai, Chengjie Wang, and Junchi Yan. Styleface: Towards identity-disentangled face generation on megapixels. In European conference on computer vision, pages 297312. Springer, 2022. 2 [47] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 4 [48] Maxim Maximov, Ismail Elezi, and Laura Leal-Taixe. Ciagan: Conditional identity anonymization generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5447 5456, 2020. 2, 3, 4 [49] BN Mayanja, FSK Kambugu, SM Mbulaiteye, and JAG Whitworth. Chronic facial crusts. The Lancet, 360(9349): 1940, 2002. 8 [50] Blaˇz Meden, Peter Rot, Philipp Terhorst, Naser Damer, Arjan Kuijper, Walter Scheirer, Arun Ross, Peter Peer, and Vitomir ˇStruc. Privacyenhancing face biometrics: comprehensive survey. IEEE Transactions on Information Forensics and Security, 16:41474183, 2021. [51] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2 [52] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 3 [53] KL Nathanson and EP Henske. Medical progress: The tuberous sclerosis complex. Engl Med, 355:13451356, 2006. 8 [54] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 10 [55] Blake Noyes, Aleks Biorac, Gustavo Vazquez, Sarosh Khalid-Khan, Douglas Munoz, and Linda Booij. Eyetracking in adult depression: protocol for systematic review and meta-analysis. BMJ open, 13(6):e069256, 2023. 8 [56] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 [57] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 4, 5, 8, 3, 6 [59] Felix Rosberg, Eren Erdal Aksoy, Cristofer Englund, and Fernando Alonso-Fernandez. Fiva: Facial image and video anonymization and anonymization defense. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 362371, 2023. 2, 5 [60] Emily Claire Rudd, Austin Kulasekararaj, and Tanya Basu. Facial lymphoedema, viral warts, and myelodysplastic synThe drome: the protean condition of gata2 deficiency. Lancet, 400(10347):236, 2022. 8 [61] Nataniel Ruiz, Eunji Chong, and James Rehg. Finegrained head pose estimation without keypoints. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 20742083, 2018. 7 [62] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [63] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [64] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2024. [65] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815823, 2015. 5, 6 [66] Charles Shagass, Richard Roemer, and Marco Amadeo. Eye-tracking performance and engagement of attention. Archives of General Psychiatry, 33(1):121125, 1976. 8 [67] Muhammad Shaheryar, Jong Taek Lee, and Soon Ki Jung. Iddiffuse: Dual-conditional diffusion model for enhanced facial image anonymization. In Proceedings of the Asian Conference on Computer Vision, pages 40174033, 2024. 2 [68] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, 3 and Stefano Ermon. arXiv preprint [69] Qianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele, and Mario Fritz. Natural and effective obfuscation by head inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 50505059, 2018. 2, [70] Qianru Sun, Ayush Tewari, Weipeng Xu, Mario Fritz, Christian Theobalt, and Bernt Schiele. hybrid model for identity obfuscation by face replacement. In Proceedings of the European conference on computer vision (ECCV), pages 553 569, 2018. 2, 3 [71] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder for stylegan image manipulation. ACM Transactions on Graphics (TOG), 40(4): 114, 2021. 2 [72] Yun-Yun Tsou, Yi-An Lee, Chiou-Ting Hsu, and ShangHung Chang. Siamese-rppg network: Remote photoplethysmography signal estimation from face videos. In Proceedings of the 35th annual ACM symposium on applied computing, pages 20662073, 2020. 2 [73] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. 3 [74] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 3 [75] Yunqian Wen, Bo Liu, Jingyi Cao, Rong Xie, and Li Song. Divide and conquer: two-step method for high quality face de-identification with model explainability. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 51485157, 2023. [76] Haoxin Yang, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Jing Qin, Yi Wang, Pheng-Ann Heng, and Shengfeng He. 2 face: High-fidelity reversible face anonymization via generative and geometric priors. IEEE Transactions on Information Forensics and Security, 2024. 2 [77] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 4 [78] Tao Zan, Wenjin Wang, Haizhou Li, Caiyue Liu, Hainan Zhu, Yun Xie, Shuangbai Zhou, Yashan Gao, Xin Huang, Shuchen Gu, et al. Autologous tissue repair and total face restoration. JAMA OtolaryngologyHead & Neck Surgery, 150(8):695703, 2024. 8 [79] Liming Zhai, Qing Guo, Xiaofei Xie, Lei Ma, Yi Estelle Wang, and Yang Liu. A3gan: Attribute-aware anonymization networks for face de-identification. In Proceedings of the 30th ACM international conference on multimedia, pages 53035313, 2022. 2 [80] Zhongyi Zhang, Tianyi Wei, Wenbo Zhou, Hanqing Zhao, Weiming Zhang, and Nenghai Yu. Facersa: Rsa-aware faIn Proceedings of cial identity cryptography framework. the AAAI Conference on Artificial Intelligence, pages 7423 7431, 2024. 2 12 NullFace: Training-Free Localized Face Anonymization"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Resistance to identity recovery attacks While one might assume that reapplying negative embedding (effectively adding positive embedding) could reverse our anonymization process, our method remains robust since multiple factors influence the outcome. As shown in Eq. (3), anonymization depends not only on the embedding but also on guidance scale and null condition, which are not accessible to potential attackers. Figure 10 confirms that applying positive embedding to anonymized images fails to restore the original identity, demonstrating our methods resilience against such attacks. Original Anonymized Attacked Figure 11. As the guidance scale increases, the re-identification rate significantly decreases, indicating enhanced anonymization results. ure 12 show that earlier mask application (e.g., timestep 70) better preserved original features like gaze or expression but increased re-identification rates. This tradeoff illustrates how mask timing can balance attribute retention with anonymity. Figure 10. Demonstration of our methods reliability against identity recovery attacks. 7. Effect of guidance scale on re-identification rates To examine the impact of guidance scale λcf on identity anonymization, we anonymized 1,000 identities from the CelebA-HQ [35] and FFHQ [36] datasets at λcf = 2.5, 5.0, 7.5, and 10.0 and measured re-identification rates. Figure 11 shows consistent decrease in re-identification rates as λcf increases for both datasets. These results demonstrate that higher guidance scales enhance anonymity by reducing the likelihood of linking anonymized faces to their originals. (a) (c) (b) (d) Figure 12. Revealing eyes or mouth earlier in denoising improves gaze and expression retention (Figs. 12a and 12c) but increases identity recognition risk (Figs. 12b and 12d, respectively). 8. Impact of mask application timing on anonymization and attribute retention 9."
        },
        {
            "title": "Impact of\nanonymization",
            "content": "facial region visibility on We analyzed the effect of mask application timing during the denoising process. Using 1,000 subjects from CelebA-HQ [35] and FFHQ [36], we applied eyeor mouth-revealing masks at timesteps 70, 80, and 90. Reidentification rates and gaze distances (for eye masks) or expression distances (for mouth masks) were measured. FigTable 3 provides quantitative analysis of how keeping different facial regions visible affects anonymization. We evaluated 1,000 subjects from CelebA-HQ [35] and FFHQ [36] using λcf = 7.0 and measured re-identification rates with AdaFace [39] across different masks. The results show that exposing both eyes and nose most significantly com1 world. On the other hand, the same capabilities can be weaponized for harmful purposes. For example, synthetic faces can be exploited to fabricate identities for scams, deepfake content, or online impersonation, eroding trust in digital interactions and media authenticity. These risks highlight the need for proactive measures to mitigate potential misuse. Technological solutions, such as deepfake detection tools and digital watermarking systems, can help differentiate real images from synthetic ones. By fostering collaboration across technology developers, we can harness the benefits of AI-generated faces while mitigating their potential to harm social trust and integrity. Re-ID (%) CelebA-HQ FFHQ"
        },
        {
            "title": "Change whole face\nKeep eyes\nKeep mouth\nKeep eyes and mouth\nKeep nose\nKeep nose and mouth\nKeep eyes and nose",
            "content": "0.50 1.70 2.30 4.81 10.62 29.19 30.56 0.51 1.01 1.01 2.73 6.67 22.26 23.68 Table 3. Impact of facial region visibility on identity retrieval rates. promises anonymity, while keeping only the eyes visible has minimal impact. While this generally confirms that larger visible areas increase recognition, the nose alone has greater impact than revealing the eyes and mouth combined. Investigating the underlying reasons for this phenomenon could be an interesting direction for future research. 10. Additional segmentation mask results Figures 13 to 16 showcase additional results of applying segmentation masks to anonymize specific facial regions using images from the CelebA-HQ [35] and FFHQ [36] datasets. By presenting examples from both datasets, we highlight how segmentation masks enable precise control over which facial attributes, such as the eyes, nose, or mouth, are preserved or hidden. Additionally, we compare our method with the Stable Diffusion Inpainting model [58] to highlight the strengths of our framework. The results show that our method consistently preserves the same anonymized appearance across different segmentation masks. This ensures that the appearance remains stable when different facial regions are revealed or hidden. In contrast, the inpainting model struggles to maintain consistent anonymized appearance when segmentation masks change. These qualitative comparisons underscore the robustness of our approach in producing stable and coherent anonymized appearance. 11. Additional qualitative comparisons We include additional qualitative comparisons in Figs. 17 to 22. These comparisons showcase the performance of our approach against state-of-the-art baselines, including FAMS [43], FALCO [6], RiDDLE [45], LDFA [40], and DP2 [31], using images from both the CelebA-HQ [35] and FFHQ [36] datasets. 12. Societal impacts The rise of AI-generated faces presents societal challenges. On one hand, technologies like facial anonymization offer privacy protections in an increasingly surveillance-driven Original Change whole face Keep eyes Change eyes Keep nose Change nose Keep mouth Change mouth Figure 13. Comparison of facial region anonymization using segmentation masks on CelebA-HQ [35] images, including comparison with the Stable Diffusion Inpainting model [58]. 3 Original Change whole face Keep eyes Change eyes Keep nose Change nose Keep mouth Change mouth Figure 14. Comparison of facial region anonymization using segmentation masks on CelebA-HQ [35] images, including comparison with the Stable Diffusion Inpainting model [58]. 4 Original Change whole face Keep eyes Change eyes Keep nose Change nose Keep mouth Change mouth Figure 15. Comparison of facial region anonymization using segmentation masks on FFHQ [36] images, including comparison with the Stable Diffusion Inpainting model [58]. Original Change whole face Keep eyes Change eyes Keep nose Change nose Keep mouth Change mouth Figure 16. Comparison of facial region anonymization using segmentation masks on FFHQ [36] images, including comparison with the Stable Diffusion Inpainting model [58]."
        },
        {
            "title": "Ours",
            "content": "FAMS [43] FALCO [6] LDFA [40] DP2 [31] Figure 17. Qualitative comparison of anonymization results on CelebA-HQ [35] images."
        },
        {
            "title": "Ours",
            "content": "FAMS [43] FALCO [6] LDFA [40] DP2 [31] Figure 18. Qualitative comparison of anonymization results on CelebA-HQ [35] images."
        },
        {
            "title": "Ours",
            "content": "FAMS [43] FALCO [6] LDFA [40] DP2 [31] Figure 19. Qualitative comparison of anonymization results on CelebA-HQ [35] images."
        },
        {
            "title": "Ours",
            "content": "FAMS [43] RiDDLE [45] LDFA [40] DP2 [31] Figure 20. Qualitative comparison of anonymization results on FFHQ [36] images."
        },
        {
            "title": "Ours",
            "content": "FAMS [43] RiDDLE [45] LDFA [40] DP2 [31] Figure 21. Qualitative comparison of anonymization results on FFHQ [36] images."
        },
        {
            "title": "Ours",
            "content": "FAMS [43] RiDDLE [45] LDFA [40] DP2 [31] Figure 22. Qualitative comparison of anonymization results on FFHQ [36] images."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Oulu",
        "University of Trento"
    ]
}