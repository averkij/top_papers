{
    "paper_title": "Reinforcing Diffusion Models by Direct Group Preference Optimization",
    "authors": [
        "Yihong Luo",
        "Tianyang Hu",
        "Jing Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands a stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), a new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at https://github.com/Luo-Yihong/DGPO."
        },
        {
            "title": "Start",
            "content": "Yihong Luo1 Tianyang Hu2 1 HKUST 2 CUHK (SZ) Jing Tang3,1 3 HKUST (GZ) 5 2 0 2 ] . [ 1 5 2 4 8 0 . 0 1 5 2 : r Figure 1: Our proposed DGPO shows near 30 times faster training compared to Flow-GRPO on improving GenEval score (Left Figure). The notable improvement is achieved while maintaining strong performance on other out-of-domain metrics (Right Figure)."
        },
        {
            "title": "ABSTRACT",
            "content": "While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at https://github.com/Luo-Yihong/DGPO."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning (RL) has become cornerstone for the post-training of Large Language Models (LLMs), significantly enhancing their capabilities (Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022). In particular, methods like Group Relative Policy Optimization (GRPO) (Shao et al., 2024) have demonstrated remarkable success in substantially improving the complex reasoning abilities of LLMs (DeepSeek-AI, 2025). However, progress in applying RL for post-training diffusion models has lagged considerably behind that of language models, leaving significant gap in methods for aligning generative models with human preferences and complex quality metrics. central obstacle is the mismatch between GRPOs policy gradient-based framework (short for policy framework in the following) and the mechanics of diffusion generation. GRPO requires access to stochastic policy to enable effective training and exploration. This requirement is naturally Corresponding Author."
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: Qualitative comparisons of DGPO against competing methods. It can be seen that our proposed DGPO not only accurately follows the instructions, but also keeps strong visual quality. All images are generated by the same initial noise. met by LLMs, which inherently output probability distribution over vocabulary. In contrast, diffusion models predominantly rely on deterministic ODE-based samplers to strike better balance between sample quality and cost (Song et al., 2020; Luo et al., 2025c), and thus do not naturally provide stochastic policy. To bridge this gap, prior work has resorted to forced adaptation: using stochastic SDE-based sampling to induce conditional Gaussian policy suitable for GRPOs policy framework (Liu et al., 2025; Xue et al., 2025). This workaround, however, introduces severe negative consequences: (1) SDE-based rollouts are less efficient than their ODE counterparts and produce lower-quality samples under fixed computational budget (Lu et al., 2022a;b; Bao et al., 2022; Song et al., 2020); (2) The policys stochasticity comes from model-agnostic Gaussian noise, which provides weak learning signal and results in slow convergence; and (3) Training is performed over the entire sampling trajectory, making each iteration computationally expensive and time-consuming. We argue that the practical success of GRPO stems less from its policy-gradient formulation, and more from its ability to utilize fine-grained relative preference information within group. Based on the insight, an ideal RL method for diffusion models should be capable of leveraging this powerful group-level information while dispensing with the need for stochastic policy and its associated negative effects. To this end, we introduce Direct Group Preference Optimization (DGPO), new online RL method tailored to diffusion models. DGPO circumvents the policy-gradient framework entirely, instead optimizing the model by directly learning from the group-level preference between set of good samples and set of bad samples. Concretely, for each prompt, we generate samples using efficient ODE-based rollouts, partition them into positive and negative groups, and directly optimize the model by maximizing the likelihood of these group-wise preferences. Conceptually, DGPO can be understood as natural extension of Direct Preference Optimization (DPO) (Wallace et al., 2024) that incorporates group-wise information, and as diffusion-native re-imagination of GRPO. This proposed methodology allows us to bypass the dependency on stochastic policy, which yields several benefits: (1) Efficient Sampling and Learning: by using high-fidelity ODE samplers, DGPO learns from higher-quality rollouts, leading to more effective learning. (2) Efficient Convergence: optimization is directly guided by group-level preferences rather than inefficient modelagnostic random exploration, leading to faster convergence. (3) Efficient Training: our approach"
        },
        {
            "title": "Technical Report",
            "content": "avoids training on the entire sampling trajectory, notably reducing the computational cost of each training iteration. Together, these advantages establish DGPO as highly efficient and powerful online RL algorithm for diffusion models. Our extensive experiments show that DGPO achieves around 20 faster training than prior state-of-the-art Flow-GRPO (Liu et al., 2025), while delivering superior performance on both in-domain and out-of-domain metrics. Most notably, on the challenging GenEval benchmark (Ghosh et al., 2023), DGPO trains nearly 30 faster than Flow-GRPO and boosts the base models performance from 63% to 97%  (Fig. 1)  . These compelling results demonstrate DGPOs potential as powerful technique for aligning diffusion models."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Diffusion Models (DMs) DMs (Sohl-Dickstein et al., 2015; Ho et al., 2020) define forward diffusion mechanism that progressively introduces Gaussian noise to input data across sequential timesteps. The forward process follows the distribution q(xtx) (xt; αtx, σ2 I), where the hyperparameters αt and σt control the noise scheduling strategy. At each timestep, noisy samples are obtained by xt = αtx + σtϵ, where ϵ (0, I). The parameterized reversed diffusion process is defined by: pθ(xt1xt) (xt1; µθ(xt, t), σ2 I). The models neural network fθ is learned by denoising Ex,ϵ,tλtfθ(xt, t) x2 2. We note that the flow matching and DMs are equivalent in the context of diffusing by Gaussian noise (Gao et al., 2025). Reward Modeling Given ranked pairs generated from certain conditioning xw 0 and xl xw preferences as: 0c, where 0 denote the better and worse samples. The Bradley-Terry (BT) model formulates the 0 xl 0c) = σ(r(c, xw where σ() denotes the sigmoid function. network rϕ that models reward can be trained by maximum likelihood as follows: 0 ) r(c, xl 0 xl pBT(xw 0)) (1) LBT(ϕ) = c,xw 0 ,xl 0 (cid:2)log σ (cid:0)rϕ(c, xw 0 ) rϕ(c, xl 0)(cid:1)(cid:3) (2) RLHF RLHF typically aims to optimize conditional density pθ(x0c) to maximize underlying reward r(c, x0) while staying close to reference distribution pref via KL regularization, i.e., max pθ Ec,x0pθ(x0c) [r(c, x0)] βKL [pθ(x0c)pref(x0c)] (3) where the hyperparameter β controls strength of regularization. GRPO Objective (Shao et al., 2024) The RLHF objective in Eq. (3) can be optimized by policybased learning (we omit the KL term and clip term hereinafter for brevity): max pθ E(x0,x1, ,xT )pθold (c) (cid:88) k=0 pθ(xk+1xk, c) pθold(xk+1xk, c) A(xk+1), (4) where A(xk+1) denotes the advantage of xk+1, which can be directly computed by reward r(xk+1), or introduce additional value model for reducing variance. The GRPO proposes to sample group of outputs for each prompt from the old policy, then compute the advantage of each sample by normalization among groups, i.e., Ai = (ri mean({r1, r2, , rG}))/std({r1, r2, , rG}). The policy learning requires that the transition between xk and xk+1 follows stochastic distribution. To meet this requirement for stochastic policy, recent works (Liu et al., 2025; Xue et al., 2025) employ stochastic SDE for sampling, rather than the more efficient deterministic ODE. However, the SDE itself is less effective in sampling high-quality samples with insufficient steps. Besides, the policy-based method requires performing training on the whole trajectory, which further leads to slow training. More importantly, unlike LLMs, which directly output distribution, the stochasticity in DMs policy comes from modelagnostic Gaussian Noise. This makes the stochastic exploration rely on the model-agnostic Gaussian noise, which is extremely inefficient in high-dimensional space."
        },
        {
            "title": "Technical Report",
            "content": "DPO Objective (Rafailov et al., 2024) The unique global optimal density tive (Eq. (3)) is given by: θ of the RLHF objecp θ(x0c) = pref(x0c) exp (r(c, x0)/β) /Z(c) (5) where Z(c) = (cid:80) the reward function as follows: pref(x0c) exp (r(c, x0)/β) is intractable partition function. We can compute r(c, x0) = β log θ(x0c) pref(x0c) + β log Z(c) (6) After obtaining the parameterization of the reward function, the DPO optimizes the models by the reward learning objective in Eq. (2): LDPO(θ) = c,xw 0 ,xl 0 (cid:20) (cid:18) log σ β log pθ(xw pref(xw 0 c) 0 c) β log (cid:19)(cid:21) pθ(xl pref(xl 0c) 0c) (7) Diffusion DPO (Wallace et al., 2024) has adapted DPO to Diffusion models by defining reward over the diffusion paths x0:T , which does not require stochastic policy. However, it strictly relies on pairwise samples for optimization due to the intrinsic restriction of the intractable partition Z(c), preventing the use of the fine-grained preference information of each sample."
        },
        {
            "title": "3 METHOD",
            "content": "We believe that the key to GRPOs success lies in its ability to utilize fine-grained relative preference information within groups. However, existing GRPO-style methods (Liu et al., 2025; Xue et al., 2025) require using an inefficient stochastic policy. Although existing DPO-style methods (Wallace et al., 2024) provide framework without the need for stochastic policy, they require performing training on pairwise samples to eliminate the intractable partition Z(c). To this end, we propose Direct Group Preference Optimization (DGPO), which eliminates the inefficient stochastic policy and allows us to directly optimize inter-group preferences without concerning ourselves with an intractable partition, leveraging fine-grained reward information to significantly improve training efficiency. The pseudo code of DGPO is summarized in Algorithm 1. Problem Setup Let pref denote pre-trained reference diffusion model with parameter θref. We i=1} and reward function rϕ(, ) : that evaluates have dataset of conditions Dc = {cN the quality of generated samples given condition C. Our goal is to enhance diffusion model pθ, initialized from pref, according to the reward signal. At each training iteration, we use an online model pθ to generate group of samples conditioned on Dc, where θ can be set as the current parameters θ or an exponential moving average (EMA) version of previous θs. These generated samples form dataset = {(Gi = {xG k=1}, ci) xk pθ (ci)}, which are then evaluated by the reward function to provide reward signals for splitting positive or negative groups. 3.1 DIRECT GROUP PREFERENCE OPTIMIZATION In order to leverage relative information within groups, we propose directly learn the group-level preferences using the Bradley-Terry model via maximum likelihood: max θ E(G+,G,c)D log p(G+ Gc) = E(G+,G,c)D log σ(Rθ(G+c) Rθ(Gc)) (8) where G+ and represent positive and negative groups respectively, with G+ = G, and = {x1 0 } being the complete group of samples for conditioning c. Intuitively, the objective in Eq. (8) can leverage fine-grained preference information of each sample within groups with appropriate parameterization. 0, , xG Therefore, we propose parameterizing the group-level reward as weighted sum of rewards rθ(c, x0) for each sample within the group: Rθ(Gc) = (cid:88) x0G w(x0) rθ(c, x0), (9) where ω controls the importance level of the sample within the group. The parameterization of group-level reward can reflect the fine-grained information of each sample. And the reward of single"
        },
        {
            "title": "Technical Report",
            "content": "Algorithm 1 Direct Group Preference Optimization (DGPO) Require: Diffusion model fθ, Reference model fref, Reward model rϕ, Group size G, Hyperparameter β, Minimum training timestep tmin, Learning rate η, Iterations , EMA decay µ (optional). Ensure: Optimized model fθ. 1: for 1 to do 2: 3: 4: 5: 6: std({rj }) 0)}G 0, ..., xG i=1 for all 0 } by sampling from pθ (c) # Sample conditioning and generate group Sample conditioning Dc Generate group = {x1 # Compute advantages {ri} {rϕ(c, xi Ai rimean({rj }) # Partition into positive and negative groups G+ {xi # Compute DGPO loss Sample U[tmin, ], ϵ (0, I) xi αtxi 0 + σtϵ for all Compute LDGPO by Eq. (17) Update θ θ ηθLDGPO Update θ θ or θ µθ + (1 µ)θ 0 : Ai > 0} and {xi 0 : Ai 0} 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for sample can be parameterized by following Eq. (6) and Diffusion-DPO (Wallace et al., 2024): rθ(c, x0) = βEpθ(x1:T x0) log + β log Z(c), (10) βEq(x1:T x0) log + β log Z(c) pθ(x0:T c) pref(x0:T c) pθ(x0:T c) pref(x0:T c) where Z(c) is intractable partition function. We note that since sampling from the inversion chain pθ(x1:T x0) is expensive, the forward diffusion q(x1:T x0) has been utilized as an approximation in practice (Wallace et al., 2024). By combining Eq. (9) and Eq. (8), we can derive the desired training objective: L(θ) = E(G+,G,c)D log σ( (cid:88) x0G+ Eq(x1:T x0)βw(x0)[log pθ(x0:T c) pref(x0:T c) + Z(c)] (cid:88) x0G Eq(x1:T x0)βw(x0) [log pθ(x0:T c) pref(x0:T c) + Z(c)]]) = E(G+,G,c)D log σ(β[Eq(x1:T x0) (cid:88) w(x0) log (cid:88) x0G Eq(x1:T x0)w(x0) log x0G+ pθ(x0:T c) pref(x0:T c) (cid:88) + x0G+ pθ(x0:T c) pref(x0:T c) w(x0)Z(c) (cid:88) x0G w(x0)Z(c)]) (11) remaining crucial challenge is that the partition function Z(c) is intractable for training. We have to carefully select an appropriate weighting wi for each sample to eliminate the intractable partition function Z(c). Generally speaking, good weighting strategy should satisfy the following: Larger weights correspond to better samples in G+ and worse samples in G. The weights satisfy: (cid:80) x0G+ w(x0) = (cid:80) x0G w(x0)Z(c)) = 0 for eliminating the intractable Z(c). x0G w(x0), such that (cid:80) (cid:80) x0G+ w(x0)Z(c) 3.2 ADVANTAGE-BASED WEIGHT DESIGN We propose using advantage-based weights derived from GRPO-style normalization to address the aforementioned issues. Given group = x1 0 with corresponding rewards r1, r2, ..., rG, 0, ..., xG 0, x"
        },
        {
            "title": "Technical Report",
            "content": "we compute advantages: A(xi 0) = ri mean({rj}G std({rj}G j=1) j=1) We then partition the group based on advantages: G+ = {xi 0 : A(xi 0) > 0}, = {xi 0 : A(xi 0) 0}. (12) (13) And we set weights as: (14) This choice ensures (cid:80) x0G w(x0) due to the zero-mean property of the normalized advantages. It also dynamically assigns larger weights to samples that deviate more from the average, which enables the model to more effectively learn relative preference relationships. More importantly, this weighting turns the objective in Eq. (11) to: x0G+ w(x0) = (cid:80) w(x0) = A(x0) L(θ) = E(G+,G,c)D log σ(β[ (cid:88) x0G+ Eq(x1:T x0)w(x0) log pθ(x0:T c) pref(x0:T c) (cid:88) x0G w(x0)Eq(x1:T x0) log pθ(x0:T c) pref(x0:T c) ]). (15) By using Jensens inequality and the convexity of log σ, we can move the expectation outside: L(θ) E(G+,G,c)DEt,ϵ log σ( (cid:88) w(x0) βT log (cid:88) x0G w(x0) βT log x0G+ pθ(xt1xt, c) pref(xt1xt, c) ), pθ(xt1xt, c) pref(xt1xt, c) (16) where = G+ G, xt = αtx + σtϵ and xt1 = αt1x + σt1ϵ. We note that to reduce the variance, the sampled noise ϵ is shared among samples within the same complete groups. By some simplification, we obtain our final training objective of the proposed DGPO: LDGPO(θ) E(G+,G,c)DEt,ϵ log σ(λtβT ( (cid:88) x0G+ w(x0)[Lθ dsm(x, xt, c) Lθref dsm(x, xt, c)] (cid:88) x0G w(x0)[Lθ dsm(x, xt, c) Lθref dsm(x, xt, c)])), 2, Lθref dsm(x, xt, c) = fθ(xt, t, c)x2 dsm(x, xt, c) = fθref(xt, t, c)x2 (17) where Lθ 2, λt is weighting function and the constant can be factored into β. The advantage of the derived DGPO objective is fourfold: 1) Leverages Relative information: It directly learns preferences between groups of samples, which leverages the fine-grained relative preference information of individual samples within groups. 2) Enhances training efficiency: It does not require training on the entire sampling trajectory, which notably reduces the computational cost per iteration. 3) Enables effective learning: It sidesteps the need for an inefficient stochastic policy, thus avoiding inefficient model-agnostic exploration and allowing the model to learn more effectively and directly from the preference data. 4) Efficient Sampling and Learning: It allows the usage of deterministic ODE sampling for rollouts. This yields higher-quality training samples compared to inefficient SDE sampling, all while using the same inference budget. Timestep Clip Strategy The considered online setting requires generating samples from the online model which might be expensive; thus, we take few steps (e.g., 10) for generating samples to reduce the inference cost following Flow-GRPO (Liu et al., 2025). However, naively performing DGPOs training on these samples generated by few steps would lead to serious performance degradation due to the poor sample quality. To mitigate this, we propose the simple yet effective Timestep Clip Strategy: during training, we only sample timesteps from the range [tmin, ] with chosen minimum timestep tmin > 0. This could effectively prevent the model from overfitting specific artifacts (e.g., blurriness) of the generated samples by few steps (see ablation in Fig. 4)."
        },
        {
            "title": "Technical Report",
            "content": "Table 1: GenEval Result. We highlight the best scores. Obj.: Object; Attr.: Attribution. Model Overall Single Obj. Two Obj. Counting Colors Position Attr. Binding Autoregressive Models: Show-o (Xie et al., 2024) Emu3-Gen (Wang et al., 2024) JanusFlow (Ma et al., 2025) Janus-Pro-7B (Chen et al., 2025b) GPT-4o (Hurst et al., 2024) Diffusion Models: LDM (Rombach et al., 2022) SD1.5 (Rombach et al., 2022) SD2.1 (Rombach et al., 2022) SD-XL (Podell et al., 2023) DALLE-2 (OpenAI, 2023) DALLE-3 (Betker et al., 2023) FLUX.1 Dev (Labs, 2024) SD3.5-L (Esser et al., 2024) SANA-1.5 4.8B (Xie et al., 2025) SD3.5-M (Esser et al., 2024) w/ Flow-GRPO (Liu et al., 2025) SD3.5-M w/ DGPO (Ours) 0.53 0.54 0.63 0.80 0.84 0.37 0.43 0.50 0.55 0.52 0.67 0.66 0.71 0. 0.63 0.95 0.97 0.95 0.98 0.97 0.99 0.99 0.92 0.97 0.98 0.98 0.94 0.96 0.98 0.98 0.99 0.98 1.00 1. 0.52 0.71 0.59 0.89 0.92 0.29 0.38 0.51 0.74 0.66 0.87 0.81 0.89 0.93 0.78 0.99 0.99 0.49 0.34 0.45 0.59 0.85 0.23 0.35 0.44 0.39 0.49 0.47 0.74 0.73 0. 0.50 0.95 0.97 0.82 0.81 0.83 0.90 0.92 0.70 0.76 0.85 0.85 0.77 0.83 0.79 0.83 0.84 0.81 0.92 0. 0.11 0.17 0.53 0.79 0.75 0.02 0.04 0.07 0.15 0.10 0.43 0.22 0.34 0.59 0.24 0.99 0.99 0.28 0.21 0.42 0.66 0.61 0.05 0.06 0.17 0.23 0.19 0.45 0.45 0.47 0. 0.52 0.86 0.91 Table 2: Performance on Compositional Image Generation, Visual Text Rendering, and Human Preference benchmarks. ImgRwd: ImageReward; UniRwd: UnifiedReward. Model Task Metric Image Quality Preference Score GenEval OCR Acc. PickScore Aesthetic DeQA ImgRwd PickScore UniRwd SD3.5-M 0.63 0.59 21. 5.39 4.07 0.87 22.34 3.33 Compositional Image Generation: Flow-GRPO DGPO (Ours) 0.95 0.97 Visual Text Rendering: Flow-GRPO DGPO (Ours) Human Preference Alignment: Flow-GRPO DGPO (Ours)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "0.92 0.96 23.31 23. 5.25 5.31 5.32 5.37 5.92 6.08 4.01 4.03 4.06 4.09 4.22 4. 1.03 1.08 0.95 1.02 1.28 1.32 22.37 22.41 22.44 22.52 23.53 23. 3.51 3.60 3.42 3.48 3.66 3.74 In this section, we comprehensively evaluate the proposed DGPO. Specifically, we benchmark improvements on three taskscompositional image generation, visual text rendering, and human preference alignment (Tables 1 and 2). We also present qualitative comparisons and training efficiency (Figs. 2 and 3). We further conduct ablations on key components (Figs. 4 and 5). 4.1 EXPERIMENTAL SETUP Evaluation Tasks We evaluate the DGPO on post-training the SD3.5-M (Esser et al., 2024) across three distinct valuable tasks: 1) compositional image generation, using GenEval to test object counting, spatial relations, and attribute binding; 2) visual text rendering (Gong et al., 2025), measuring accuracy of rendering text in generated images, and 3) human preference alignment, using PickScore to assess visual quality and text-image alignment. Details are provided in the Section A. Out-of-Domain Evaluation Metrics To fairly evaluate model performance and guard against reward hackingwhere models may overfit to training rewards signal while compromising actual image qualitywe employ four independent image quality metrics not used during training as outof-domain evaluations: Aesthetic Score (Schuhmann et al., 2022), DeQA (You et al., 2025), ImageReward (Xu et al., 2023), and UnifiedReward (Wang et al., 2025). We compute these metrics on DrawBench (Saharia et al., 2022), comprehensive benchmark featuring diverse prompts."
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: Compare the training speed of Flow-GRPO and our proposed DGPO. Figure 4: Visual comparisons among variants. It can be seen that without the proposed timestep clip strategy, although it can still accurately follow the instruction, the visual quality notably degrades 4.2 MAIN RESULTS Quantitative Results Table 1 shows that DGPO achieves state-of-the-art performance on GenEval, notably surpassing prior SOTA methods such as GPT-4o and Flow-GRPO. This improvement is achieved while maintaining performance across various out-of-domain metrics (such as AeS, DeQA, and Image Reward), as indicated by Table 2. Beyond compositional image generation, Table 2 provides detailed evaluation results on visual text rendering and human preference tasks, where DGPO similarly demonstrates significant improvements in the target optimization metrics while maintaining performance across various out-of-domain metrics. Qualitative Comparison We present the qualitative comparisons of methods trained with GenEvals signal in Fig. 2. It is clear that the proposed DGPO can follow the instructions more accurately compared to the base diffusion model and also the Flow-GRPO. Although Flow-GRPO also shows accurate instruction following, its image quality degrades seriously, while our method shows notably better visual quality. We present additional visual samples in Appendix B. Training Cost The overall training of the proposed DGPO is quick, since we do not require the inefficient stochastic policy for training. Besides, the training of the proposed DGPO is efficient per iteration, since we do not require training the model on the whole trajectory. Benefit from these points, the overall training of DGPO for reinforcement post-training is much faster than prior SOTA Flow-GRPO. As shown in Figs. 1 and 3, the overall training of DGPO is generally around 20 times faster than Flow-GRPO. 4.3 ABLATION STUDY Effect of Timestep Clip Strategy We found that without the proposed timestep clip strategy, the reward metric slightly degrades from 0.96 to 0.95 regarding OCR Accuracy, while the visual quality seriously degrades as shown in Fig. 4. ODE Rollout vs. SDE Rollout core advantage of our work compared to prior GRPO-style works (Liu et al., 2025) is the ability to use the efficient ODE solvers for generating samples. This can deliver samples with better quality and rewards. Results in Fig. 5 show that ODE rollout notably outperforms SDE rollout in both convergence speed and ultimate metrics. This suggests that the use"
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Comparison of visual text rendering across variants. of SDE in prior works may have been requirement of the policy gradient framework rather than providing more diverse samples for training. Offline DGPO Our work can be easily adapted to the offline setting by using the reference model pref for generating the training dataset. Results in Fig. 5 show that our offline DGPO can reasonably boost the performance over the baseline, but its performance is notably worse than the online setting. Compared to Diffusion DPO Diffusion DPO can also avoid the need for the stochastic policy, however, it cannot leverage the fine-grained reward signals of each sample. Results in Fig. 5 show that our DGPO notably outperforms the DPO in both online and offline settings, indicating the effectiveness of our proposed DGPO in leveraging fine-grained group relative information."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Recent research has focused extensively on aligning DMs with human preferences through three primary approaches. The first involves fine-tuning diffusion models on carefully curated imageprompt datasets (Dai et al., 2023; Podell et al., 2023). The second approach maximizes explicit reward functions, either by evaluating multi-step diffusion generation outputs (Prabhudesai et al., 2023; Clark et al., 2023; Lee et al., 2023; Ho et al., 2022; Luo et al., 2025a;b) or through policy gradient-based learning (Fan et al., 2024; Black et al., 2023; Ye et al., 2024). The third category employs implicit reward maximization, as demonstrated by Diffusion-DPO (Wallace et al., 2024) and Diffusion-KTO (Yang et al., 2024), which directly leverage raw preference data. We note that concurrent work (Chen et al., 2025a) also explores utilizing the group information in Diffusion DPO. Their approach constructs the group signal by enumerating all pairwise comparisons within the group. By contrast, our work defines single group-level reward and reinforces the DMs with maximum-likelihood learning on that group-level reward. Recent works have also adapted GRPO to DMs (Liu et al., 2025; Xue et al., 2025) under policygradient framework, demonstrating promising scalability and impressive performance improvements. However, notable drawback of existing GRPO-style approaches is their reliance on stochastic policy, which requires inefficient SDE-based rollouts during training. Our work identifies group relative information as the critical component of GRPO and introduces DGPO to directly optimize group preferences, thereby exploiting fine-grained group relative information without requiring stochastic policies. As result, DGPO achieves significantly faster training and superior performance on both in-domain and out-of-domain reward benchmarks compared to prior GRPOstyle methods."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce Direct Group Preference Optimization (DGPO), novel online reinforcement learning method specifically designed for post-training diffusion models. Our approach addresses the fundamental mismatch between policy gradient methods like GRPO and the inherent mechanics of diffusion generation. By recognizing that GRPOs effectiveness stems primarily from its utilization of group relative preference information within the group rather than its policygradient nature, we developed method that preserves this key strength while eliminating the need for stochastic policies. DGPOs direct optimization approach offers substantial practical advantages over existing methods. By enabling the use of efficient ODE-based samplers, eliminating"
        },
        {
            "title": "Technical Report",
            "content": "reliance on model-agnostic noise for exploration, and avoiding expensive trajectory-based training, DGPO achieves around 20 speedup in overall training time compared to Flow-GRPO. More importantly, our experiments demonstrate that this efficiency gain comes with superior performance, as DGPO consistently outperforms baseline methods across both in-domain and out-of-domain evaluation metrics."
        },
        {
            "title": "REFERENCES",
            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503, 2022. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36:9353 9387, 2023. Renjie Chen, Wenfeng Lin, Yichen Zhang, Jiangchuan Wei, Boyuan Liu, Chao Feng, Jiao Ran, and Mingyu Guo. Towards self-improvement of diffusion models via group preference optimization, 2025a. URL https://arxiv.org/abs/2505.11070. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv.org/abs/ 2403.03206. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Ruiqi Gao, Emiel Hoogeboom, Jonathan Heek, Valentin De Bortoli, Kevin Patrick Murphy, and Tim Salimans. Diffusion models and gaussian flow matching: Two sides of the same coin. In The Fourth Blogpost Track at ICLR 2025, 2025. URL https://openreview.net/forum? id=C8Yyg9wy0s."
        },
        {
            "title": "Technical Report",
            "content": "Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Yihong Luo, Tianyang Hu, Weijian Luo, Kenji Kawaguchi, and Jing Tang. Rewardinstruct: reward-centric approach to fast photo-realistic image generation. arXiv preprint arXiv:2503.13070, 2025a. Yihong Luo, Tianyang Hu, Yifan Song, Jiacheng Sun, Zhenguo Li, and Jing Tang. Adding additional control to one-step diffusion with joint distribution matching. arXiv preprint arXiv:2503.06652, 2025b. Yihong Luo, Tianyang Hu, Jiacheng Sun, Yujun Cai, and Jing Tang. Learning few-step diffusion models by trajectory distribution matching. arXiv preprint arXiv:2503.06674, 2025c. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 77397751, 2025. OpenAI. Dalle-2, 2023. URL https://openai.com/dall-e-2. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022."
        },
        {
            "title": "Technical Report",
            "content": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-toimage diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 22562265. PMLR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inferencetime compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 1590315935, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025."
        },
        {
            "title": "Technical Report",
            "content": "Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024. Zilyu Ye, Zhiyang Chen, Tiancheng Li, Zemin Huang, Weijian Luo, and Guo-Jun Qi. Schedule on the fly: Diffusion time prediction for faster and better image generation. arXiv preprint arXiv:2412.01243, 2024. Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, and Chao Dong. Teaching large language models to regress accurate image quality scores using score distribution. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1448314494, 2025. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A EXPERIMENT DETAILS",
            "content": "Compositional Image Generation. We evaluate text-to-image models on complex compositional prompts using GenEval (Ghosh et al., 2023), which tests six challenging compositional generation tasks including object counting, spatial relations, and attribute binding. Visual Text Rendering. Following the methodology in TextDiffuser (Chen et al., 2023) and FlowGRPOs experimental setup, we evaluate models ability to accurately render text within generated images. Each prompt follows the template structure sign that says text, where text represents the exact string to be rendered in the image. We measure text fidelity (Gong et al., 2025) as follows: = max(1 Ne/Nref, 0) where Ne denotes the minimum edit distance between rendered and target text, and Nref represents the character count within the prompts quotation marks. Human Preference Alignment. To align text-to-image models with human preferences, we employ PickScore (Kirstain et al., 2023) as the reward signal. The PickScore model, trained on largescale human preference data, evaluates both visual quality and text-image alignment, providing comprehensive assessment of generation quality from human-centric perspective. Setup Details. We generate 24 samples for each group for training. We adopt the Flow-DPMSolver (Xie et al., 2025) with steps of 10 for rollout during training. We adopt LoRA fine-tuning with rank of 32. The β is set to be 100 by default. Defaultly, We update θ by identity mapping, i.e., θ θ within 200 steps, and update θ by EMA with decay of 0.3 in the remaining training. Experiments are performed over 512 resolution. We use probability of 0.05 to drop text during training. The experiments are performed over A100. The reported GPU hours are A100 hours. Details of the out-of-domain evaluation metrics We outline the specific out-of-domain metrics used to assess quality: The aesthetic score (Schuhmann et al., 2022) employs linear regression model based on CLIP to evaluate the visual appeal of generated images; For assessing image quality degradation, we utilize the DeQA score (You et al., 2025). This metric leverages multimodal large language model architecture to measure the impact of various imperfectionsincluding distortions, textural degradation, and low-level visual artifactson the overall perceived quality of images; ImageReward (Xu et al., 2023) serves as comprehensive human preference model for text-to-image generation tasks. This reward function evaluates multiple dimensions including the coherence between textual descriptions and visual content, the fidelity of generated visuals; Finally, UnifiedReward (Wang et al., 2025) represents the latest advancement in this area. This integrated reward framework can evaluate both multimodal understanding and generation tasks, and has demonstrated superior performance compared to existing methods on the human preference assessment leaderboard."
        },
        {
            "title": "B ADDITIONAL QUALITATIVE COMPARISON",
            "content": "We present additional visual samples in Figs. 6 and 7."
        },
        {
            "title": "C LIMITATIONS AND FUTURE WORKS",
            "content": "Our work focuses on text-to-image synthesis; however, it also has the potential to be adapted to enhance text-to-video synthesis. Exploring the extension would be an interesting future work. Figure 6: Qualitative comparisons of DGPO against competing methods. The training signal is given by OCR Accuracy. All images are generated by the same initial noise."
        },
        {
            "title": "Technical Report",
            "content": "Figure 7: Qualitative comparisons of DGPO against competing methods. The training signal is given by PickScore. All images are generated by the same initial noise."
        }
    ],
    "affiliations": [
        "CUHK (SZ)",
        "HKUST",
        "HKUST (GZ)"
    ]
}