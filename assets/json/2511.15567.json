{
    "paper_title": "Computer-Use Agents as Judges for Generative User Interface",
    "authors": [
        "Kevin Qinghong Lin",
        "Siyuan Hu",
        "Linjie Li",
        "Zhengyuan Yang",
        "Lijuan Wang",
        "Philip Torr",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI."
        },
        {
            "title": "Start",
            "content": "Computer-Use Agents as Judges for Generative User Interface 1Kevin Qinghong Lin 2Siyuan Hu 3Linjie Li 3Lijuan Wang 1Philip Torr 2Mike Zheng Shou(cid:66) 2Show Lab, National University of Singapore 1University of Oxford 3Zhengyuan Yang 3Microsoft 5 2 0 2 9 1 ] . [ 1 7 6 5 5 1 . 1 1 5 2 : r Homepage: https://showlab.github.io/AUI Code: https://github.com/showlab/AUI Demo: https://huggingface.co/spaces/showlab/AUI"
        },
        {
            "title": "Abstract",
            "content": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humansprioritizing aesthetics and usabilityforcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in language agents have shown that Computer-Use Agents [1, 2] can autonomously operate within GUIsperforming tasks such as online shopping by sequentially clicking through multiple buttons [3]. However, todays environments remain fundamentally humancentric, optimized for aesthetics and usability through features like dynamic animations or colorful layouts. To adapt to these settings, researchers typically train CUA on large-scale human demonstration trajectories, click logs, or static screenshots [4, 5, 6], effectively forcing agents to imitate human behavior. This approach binds automation to human-oriented design choices, where stylistic details crucial for humans are redundant for agents whose primary objective is efficient task completion. In parallel, coding-oriented language modelsCodershave already demonstrated strong Equal contribution. (cid:66) Corresponding author Figure 1: Illustration of Humans Collaboration vs. our Coder-CUA Collaboration in term of UI designs. Left: Most GUIs are designed by humans and optimized for user experience (e.g., aesthetics), forcing trained agents to adapt to human-oriented behaviors. Right: Our Coder-CUA Collaboration framework leverages Coder as Designer and CUA as Judge together, enabling more reliable task execution and improved usability for agents. capabilities, capable of generating functional HTML pages or even entire websites from single instruction [7]. Yet these outputs remain confined to human-facing loops: even when generated by agents, interfaces are still optimized for human use rather than agent-native interaction. Both CUA and Coders thus exhibit remarkable potential for automation and design. This motivates fundamental question: Can CUA assists Coders redesign UIs in an automatic mannerwhere environments are created for, and evaluated by, agents themselves, with CUA acting as judges? In this work, we reconceptualize the UI as tunable environment. The core idea is to employ the Coder as Designerresponsible for initializing and revising UIswhile the CUA acts as Judges, navigating through tasks and collecting interaction trajectories as feedback. As no existing testbed aligns with our goal, we introduce AUI-Gym to pioneer evaluation in this setting. AUI-Gym automatically develops websites across 52 applications spanning six domains (apps, landing pages, games, interactive demos, tools, and utilities). Unlike most coders that focus on single-page generation, AUI-Gym requires agents to produce fully automated, executable, application-level designs with an emphasis on functional completeness. Enabling sufficient, scalable, and human-free evaluation is non-trivial. To simulate realistic usage scenarios, we prompt GPT-5 to propose 30 candidate tasks per application, yielding 1560 tasks in total. These tasks are then validated by humans. To ensure that each website can be reliably tested, GPT-5 also generates customized rule-based functional checker for individual task, determining whether the task is feasible within the given interface. This infrastructure establishes human-free, reliable foundation for subsequent CUA exploration and feedback-driven UI refinement. To this end, we develop CoderCUA collaboration framework. The Coder acts as Designer, responsible for UI initialization and refinement, while the CUA serves as Judge, supplying feedback. The central challenge is how to transform raw CUA interactions into effective revision signals from an agent perspective. We address this through two complementary dimensions of feedback: (a) CUA Navigation, where the agent executes tasks through atomic actions such as clicks or typing and judges success or failure; and (b) Task Solvability, where unsolvable tasks are accumulated as functionality failures and returned to the Coder as precise indicators of missing features. CUA navigation produces long, multi-step trajectories interleaved with screenshots, making direct feedback difficult to interpret. To overcome this, we introduce the CUA Dashboard, which condenses each task, its outcome, actions, and intermediate states into single 1920 1080 image. Rather than storing every screenshot, the dashboard highlights only key interactive regions, with region sizes adaptively scaled by the number of steps. This dynamic design reduces redundancy by average 76.2% while preserving essential cues, offering clear step-by-step view of how the CUA perceives and acts on the interface. As result, success and failure points become immediately visible, and the dashboard provides concise, interpretable feedback that the Coder to guide iterative UI redesign. Our empirical results show that while state-of-the-art Coders can generate complete GUIs that appear suitable to humans, they still encounter notable limitations: (i) Task solvability as foundation. Initial UIs often fail to capture many practical scenarios, resulting in low usability. However, by collecting failure cases, the Coder can readily boost performance through language-based func2 tional summarization. (ii) CUA navigation as key bottleneck. Even when UIs achieve high functional completeness, CUAs initially exhibit low success rates due to the complexity of multi-step navigation. Through our CoderCUA collaboration, we substantially improve navigation success rates, particularly showing that CUA feedback-driven redesignssuch as de-stylization, increased contrast, and simplified layoutssignificantly enhance CUA execution. Together, these findings highlight the promising potential of agents for automatic UI design and testing, improving both task success and robustness. To summarize, our contributions are threefold: 1. AUI-Gym: scalable testbed for automatic GUI development and testing, covering 52 applications across six domains with 1560 GPT-5proposed, human-validated tasks and per-task rule-based checkers. This enables human-free development of automatic UI creation and testing. 2. CoderCUA framework with CUA Dashboard. The Coder initializes and refines UIs while the CUA judges via two signals: navigation outcomes and task solvability. single-image 1K CUA Dashboard compresses task goal, actions, intermediate states, and outcome by highlighting key interactive regions with adaptive scaling, reducing visual tokens by 76.2% on average while preserving essential cues for redesign. 3. Evaluation Insights: What kind of UI do Agents prefer? Task solvability is foundational yet readily improved via failure-driven functional summarization, whereas CUA navigation is the main bottleneck. Feedback-driven redesigns (e.g., de-stylization, higher contrast, simplified layouts) substantially raise execution success and overall robustness."
        },
        {
            "title": "2.1 Computer-Use Agents",
            "content": "Recent studies reveal the potential of LLMs beyond language modeling, with advancements in demonstrating their ability to autonomously complete complex tasks using tool integration [8] like humans. This has prompted the development of GUI automation agents that learn to operate digital user interfaces by imitating human trajectories. This learning is primarily achieved in two ways: (i) by steering general multimodal foundation models with in-context human trajectory examples, and the general models perceive the UI through intermediate representations like HTML, accessibility trees [9, 10], Optical Character Recognition [11], or Set of Masks [12]. (ii) by pre-training specialized GUI foundation models through extensive supervised fine-tuning or reinforcement learning on large-scale vision-text UI data (e.g., screenshots and instructions) [5, 13, 14, 15]. While foundational, these data-driven approaches suffered from heavy requirements for high-quality human trajectories to achieve agent performance improvements. Despite their methodological differences, these approaches share common, agent-centric paradigm, focusing on improving the agents capabilities to navigate static and often complex environment. Notably, we investigate complementary approach. Instead of adapting the agent, we explore how to dynamically tune the environment to enhance the performance of frozen agent."
        },
        {
            "title": "2.2 Automatic Software Designs",
            "content": "Besides CUAs, there have been extensive research on software automation, automatic interface design [16, 17] and generation [7, 18, 19]. Programmatic and semantic UI componentssuch as accessibility layers, ARIA tags, and declarative interface frameworks (e.g., React Native, Flutter)illustrate how environments can be annotated or abstracted for automated processes. Similarly, benchmarks in automated software interaction, such as WebArena [3] and GAIA [20], assume agent operates within fixed, human-oriented systems for task automation. More recently, embodied AI environments (e.g., ALFRED [21], Habitat [22], MineDojo [23]) show how environments can be crafted to accelerate agent training, though primarily in physical or simulated domains. These efforts highlight the growing recognition that environments themselves can be reimagined for machine interaction, yet systematic framework for designing agent-centric digital environments in everyday computing remains absent."
        },
        {
            "title": "3.1 Task Definitions",
            "content": "We develop AUI-Gym for automatic GUI development and testing. Given language user query as input and several available agents (e.g., Coder or CUA), the output is complete website that serves as tunable environment E. We detail the input and output respectively below. Input Query Q. Since the outcome is website, the user query should be both descriptive and concrete. To this end, we explicitly standardize queries into the structured format illustrated above. This supplements the query with name, goal, functional features, and UI theme. Figure 2: AUI-Gym task definition. user issues request (e.g., Create Data Visualization Playground), and agents (e.g., Coder or CUA) interact with the GUI through design, exploration, and feedback. In this setup, the GUI serves as tunable environment. Output website E. The website is an application-level deliverable that must be fully functional, going beyond static page to support navigation, transitions, button interactions, and completion of functional goals, with the objective of maximizing the agents success rate. Constructing an effective evaluation framework in this setting is non-trivial and introduces several challenges. We next present our scalable, automatic solutions."
        },
        {
            "title": "Input formulation",
            "content": "Create single-page app in single HTML file with the following requirements: - Name: {Camping Gear Checklist} - Goal: {Track gear for camping trips}. - Features: {Checklist items, weight calculator, save lists.} - Theme: {The UI should be outdoor-themed.}"
        },
        {
            "title": "3.2 Task Creation",
            "content": "The full curation pipeline is illustrated in Fig.3. To construct the benchmark, we collect 52 task prompts from OpenAIs playground 2, covering multiple domains. Synthesize candidate tasks . Applications are typically designed to support variety of relevant tasks, and key evaluation is whether they can smoothly handle such tasks. We leverage GPT-5 [24] to synthesize diverse user requirements: given an instruction I, it generates set of candidate tasks that simulate practical usage. As illustrated in Fig. 3, for the application Micro Habit Tracker, an example task is: Create habit named Meditate 5 min, then view todays column and the habit chart. These tasks serve as fine-grained probes that capture the potential demands of the environment E. Manual quality control. As the tasks are automatically generated by GPT-5, human oversight is required to ensure their quality. Different applications demand different characteristics: for example, tasks for game UIs should emphasize interactivity and control, while tasks for utility tools should capture information accessibility and workflow patterns. To this end, humans define domain-specific principles and filter out low-quality tasks (e.g., trivial clicks) or nonsensical ones (e.g., beyond the application scope), ambiguous queries (cross-application), ensuring that the proposed tasks remain concrete, meaningful and aligned with each domains design philosophy. Data Statistics. Based on the above strategy, we obtain 30 tasks for each application. The benchmark spans 52 web applications across six domains, yielding total of 1,560 tasks and enabling comprehensive evaluation across diverse applications. As illustrated in Table 1, the domains include: (i) App, general-purpose applications; (ii) Landing, commercial and promotional interfaces; (iii) Game, puzzle and arcade-style challenges; (iv) Interactive, dynamic user engagement with realtime feedback; (v) Tool, specialized utilities; and (vi) Utility, everyday organizational support. This 2https://github.com/openai/gpt-5-coding-examples (i) An input query specifies the app requirements. Figure 3: AUI-Gym construction pipeline. (ii) GPT-5 proposes candidate tasks with explicit goals. (iii) Humans filter and refine tasks using domain-specific principles. (iv) test-time Verifier reads the website HTML and generates taskspecific, rule-based checkers to validate success on the to-be-tested website. Table 1: Examples of App domains in AUI-Gym. For each domain, we show website created by GPT-5, paired with 30 tasks (blue) simulating real-world usage. Each task is further linked to rule-based verifier (green). See full distribution and examples in Tab.7. Domain #Apps Percentage App 11 21% Example Instruction GUI created by GPT-5 Create single-page app in single HTML file with the following requirements: - Name: Healthy Meal Tracker - Goal: Log meals and nutrition info. - Features: Ingredient list, calories per meal, daily summary. - The UI should be clean with food icons. Task: Add five meals for todays date (any names/ingredients) so todays meal count reaches at least 5. Rule: #dailyMealCount >= 5 diverse coverage captures distinct GUI challenges-ensuring robust evaluation across varied interaction paradigms and functional complexities."
        },
        {
            "title": "3.3 Evaluation with Verifiers",
            "content": "Even with the proposed tasks, it remains challenging to determine whether given GUI can truly satisfy them, as websites are interactive and highly diverse environments. More importantly, since the GUIs are generated at test time, it is difficult to design fixed standards that generalize across all cases, given the variety of possible implementation approaches. naive solution is to adopt VLM-as-Judge approach, but this inevitably introduces bias and uncertainty. Ideally, the most reliable solution would be concrete functional checks with manual validation, yet this approach is prohibitively expensive and labor-intensive. Verifier ( input = GUI_HTML , task ) : analyze elements and states if task solvable : To address this, we define Verifier V() powered by GPT-5 at test time, which takes as input candidate GUI together with specific task. It analyzes the available elements and states, reasoning over the presence of required UI components, their properties, and potential interaction paths. If the task is deemed solvable, the Verifier produces task-specific verification function checker (cid:101)V() (by JavaScript) that encodes the success condition by element status; otherwise, the task is discarded as invalid, preventing noisy or unachievable goals from disrupting evaluation. Such as in Fig.3, for task Create habit named Meditate 5 min, then view todays column and the habit chart., based on the candidate website (right), the verifier generates the rule gridContainer contains Meditate 5min In this way, the Verifier is customized for each website and each task at test time, ensuring reliable validation. return ( Yes , function_checker ) return ( No , None ) else : Metrics. With the support of function checkers as reliable verification, we can ensure that website is both actionable and workable for the CUA. This further allows us to evaluate whether tasks are completed after CUA navigation, thereby measuring task success rate within the UI environment. In this way, we devise the following measure: (i) CUA Success Rate (SR). This measures the average success rate over all tasks executed by CUA. If CUA successfully completes task, it is counted as success; otherwise, it is counted as failure. Notably, if the Coder fails to yield functional checker, the task is counted as failure. SR = 1 (cid:88) tT 1 (task is successfully completed) , (1) where denotes the set of all tasks and 1{} is the indicator function. 5 Figure 4: Overview of the Coder-CUA in Collaboration framework. The process begins with the Coder as Designer, which initializes and iteratively revises the UI based on queries and feedback. In parallel, the CUA as Judge executes task-driven navigation within the testing environment, generating trajectories and error logs to evaluate task solvability. verifier ensures functional correctness, while feedback from CUA navigation informs subsequent UI revisions. This collaboration yields finalized agent-centric UI optimized for both functionality and execution success. (ii) Function Completeness (FC). While CUA performance reflects the ultimate goal, it may be sparse if most CUAs fail to complete tasks. Therefore, we devise second metric to evaluate only whether the Coder-created website functionally supports the task (valid), independent of CUA navigation. This metric reflects task validity and serves as more basic measure. FC = 1 (cid:88) tT 1{a functional checker exists for task t}. (2)"
        },
        {
            "title": "4 CUA–Coder in Collaboration",
            "content": "Overview. We present our framework for enabling collaboration between the CUA and the Coder, consisting of two main components: the Coder as Designer while the CUA as Judge. Given user instruction Q, AUI generates an initial UI environment E0, which is iteratively revised through interaction and feedback. The framework involves two central roles: Coder policy πCoder that proposes and revises UI designs, and CUA policy πCUA that explores the UI and evaluates its functionality. We formalize this process as Markov Design Process. The state is the current UI Et, the action is design update proposed by πCoder, and the transition deterministically Et+1 πCoder(Et, Rt). The feedback Rt is related to the metrics (i.e., Eq.1 and Eq.2) results achieved by the CUA when interacting with Et, i.e., Rt S(Et, πCUA). The Coder is optimized to maximize the total reward (cid:104) (cid:80) . In this formulation, the CUA acts as user that provides actionable feedback by testing the environment, while the Coder serves as designer who integrates this feedback into code revisions to iteratively improve the UI. Unlike conventional CUA setups, where the agent adapts to fixed environment πCUA E, our framework adapts the environment itself based on CUA feedback πCUA, thereby optimizing UIs for agent-native success. We illustrate the full workflow in Fig. 4 and detail each role in the following subsections. γtRt (cid:105)"
        },
        {
            "title": "4.1 Coder as Designers\nRecent advances in Coder [24, 25, 26] demonstrate strong capabilities in generating UI applications.\nIn our framework, we position Coders as designers, responsible not only for creating new environ-\nments but also for refining them based on feedback from CUAs. Accordingly, Coders operate in two\ncomplementary modes: one dedicated to the initial creation of UIs, and the other focused on their\niterative improvement through CUA-guided feedback.",
            "content": "6 Task Load the app for the first time and wait for the curtain reveal to complete. Website Dashboard (an image) Result Comments 1280 720 Before: 6 1280 720 After: 1 1950 975, 76.2% tokens reduction Failure The weather-theatre app requires button clicks to trigger curtain reveal, but the task expects automatic curtain opening on first load without user interaction, creating fundamental mismatch between expected auto-start behavior and actual manual activation requirement. Table 2: Illustration of CUA Dashboard. The dashboard generates one informative image that clearly demonstrates how the CUA performs each step along with the corresponding observations, while reducing visual tokens by cropping to the key interactive regions. i. Initialization. Given user query defined in formulation 3.1 and enriched with multiple details, the Coder progressively generates long-context code to construct complete HTML-rendered UI E0 from scratch, which serves as the base environment for subsequent interactions. ii. Revision from Feedback. After constructing the initial environment E0, the Coder enters an iterative refinement loop to update the UI: Et+1 (Et, Rt), where Rt denotes the feedback signal expressed as language caption, described in the next section."
        },
        {
            "title": "4.2 CUA as Judges",
            "content": "We employ Computer-Use Agents (CUAs) as Judges to trial and diagnose the UIs Et generated by the Coder, providing actionable feedback for iterative redesign. Specifically, we define two complementary forms of reward signals: (i) Task Solvability Feedback Rtask. Before navigation begins, we verify whether task τ is implementable on the current UI. Let denote the verifier in Sec. 3.3. task is deemed solvable if and only if V(Et, τ ) = 1; otherwise it is labeled functionality failure. This gate prevents wasted rollouts on impossible tasks and sharpens the feedback signal. We collect all failed tasks into Tfail = {τ : V(Et, τ ) = 0} and return them to the Coder as precise indicators of missing features. The Coder then aggregates and summarizes these failures into language feedback signal Rtask. (ii) CUA Navigation Feedback Rnav. For solvable tasks Tsucc = {τ : V(Et, τ ) = 1} , evaluation proceeds as UI navigation problem. At step k, the CUA receives an observation ok (a 7 Table 3: Main results on AUI-Gym per Coder. Top: Func. Completeness Rate (%). Bottom: CUA Success Rate (%). Coder Feedback Type landing (%) game (%) app (%) utility (%) interactive (%) tool (%) overall (%) GPT-5 Qwen3Coder-30B GPT-4o GPT-5 Qwen3Coder-30B GPT-4o Baseline + Task Solvability + CUA Navigation + Integrated Baseline + Task Solvability + CUA Navigation + Integrated Baseline + Task Solvability + CUA Navigation + Integrated Baseline + Task Solvability + CUA Navigation + Integrated Baseline + Task Solvability + CUA Navigation + Integrated Baseline + Task Solvability + CUA Navigation + Integrated 53.0 19.7 53.3 75.3 16.3 55.0 23.3 47.7 9.7 23.7 8.3 16.3 34.7 16.3 17.7 40.7 5.3 14.7 6.7 23. 4.7 8.7 5.7 10.3 Function Completeness 77.8 100.0 87.8 92.2 50.4 79.6 50.4 72.2 55.2 55.9 55.2 68.5 70.6 69.4 74.2 85. 41.2 58.5 38.8 59.7 36.1 52.1 28.2 36.4 CUA Success Rate 24.8 39.3 43.3 27.4 9.3 42.2 20.7 30.7 12.6 18.5 31.5 27. 27.3 26.7 30.0 31.5 9.1 19.1 9.1 22.4 12.4 19.1 10.0 13.9 63.3 65.6 70.0 73.3 43.9 67.8 49.4 56.7 38.9 55.0 34.4 51. 14.4 16.1 21.1 22.2 11.7 14.4 11.1 7.8 6.7 5.6 8.3 13.3 73.0 55.6 70.4 82.6 52.2 56.3 39.3 57.0 44.8 58.9 26.3 51. 18.1 20.7 21.1 14.1 7.0 11.1 12.2 9.3 9.3 8.5 10.4 15.2 70.0 56.2 69.5 76.7 54.8 74.3 55.2 69.5 37.6 65.2 35.7 41. 21.9 11.9 17.6 12.9 1.4 4.3 11.4 13.8 5.7 22.9 6.7 16.7 67.9 60.5 70.8 81.5 42.1 64.3 41.3 60.1 36.3 50.6 30.4 43. 24.5 22.6 25.7 26.0 7.3 18.3 11.7 19.0 8.8 14.1 12.3 16.1 screenshot of the current state), emits an action ak {CLICK, TYPE, SCROLL, . . .} with an optional reasoning trace, and the environment transitions to the next state, yielding ok+1. The trajectory = (o0, a0, . . . , oK) terminates when either (a) the function checker signals success (cid:101)V(Et, τ ) = 1, or (b) step limit is reached, which we record as failure. We log full trajectoriesobservations, actions, and intermediate rationalesand use them to construct targeted feedback for UI refinement. CUA Dashboard for Compact Feedback. Raw trajectories are long and interleaved, making them ill-suited for direct ingestion by the Coder. We therefore distill each rollout into CUA Dashboard  (Fig. 2)  : single, fixed-resolution (1920 1080) canvas that compresses key evidence from the trial. Rather than storing full frames, we crop and tile only interactive regions touched by the CUA, allocating dynamic region sizes based on step order to preserve temporal structure. This yields substantial reduction in redundancy (e.g., 76.2% drop in visual content) while retaining the cues needed to localize failure modes (missed affordances, hidden state, ambiguous labels) and success paths at glance. The dashboard provides step-by-step visual trace aligned with actions, making error locations immediately visible. Finally, we convert the dashboard into concise language summary Rnav by passing it to VLM as Commenter then as the feedback for revision."
        },
        {
            "title": "5.1 Settings",
            "content": "For the Coder, we evaluate GPT-5 [24], GPT-4o [27], and the open-source Qwen3-Coder [25]. For the CUA, we use UI-TARS-1.5-7B [6], lightweight yet efficient open model, and Operator [1], state-of-the-art closed-source API-based CUA. GPT-5 serves both as Coder and Commenter, configured with high verbosity and reasoning effort for coding, and low verbosity with minimal reasoning for commenting. GPT-4o also serves as both Coder and Commenter but without specific verbosity or reasoning configurations. For Qwen, coding is performed by Qwen3-Coder-30B-A3BInstruct, while commenting is done by Qwen2.5-VL-72B-Instruct. The Task Proposer and Verifier utilize GPT-5 with high verbosity and reasoning effort. In CUA policy tests, we limit maximum steps to 20 to prevent infinite loops, conducting evaluations using Playwright. CUAs exclusively perform coordinate-based Computer Use actions without direct interaction with UI elements, enhancing evaluation difficulty and providing deeper insights into UI layout and visibility. Experiment results consistently demonstrate universal performance gains from our proposed method, benefiting CUAs of varying complexity."
        },
        {
            "title": "5.2 Main Results",
            "content": "Table 3 reports results across six domains for three coders. Several key findings emerge: (i) Function Completeness. Revision based on task solvability feedback leads to substantial gains, consistently boosting the overall functionality completeness for all coders. After applying integrated revision for GPT-5, the function completeness is increased to 81.5% from 67.9%, reaching the highest. Notably, the landing, game and app domains have dramatic improvements, with the maximum improvements of 31.4%. Interestingly, revision based on task solvability feedback or CUA navigation feedback alone does not guarantee function completeness improvements, but the integrated revision combining these two components bring stable improvements in all domains for all coders, highlighting the strength of our design. Moreover, fixing unresolved functionalities alone also benefits CUA task solving, yielding 4.8% average improvement on CUA evaluation, highlighting the mutual reinforcement between task solvability and CUA navigation. (ii) CUA performance. Open-source CUAs initially perform poorly, with an average overall CUA success rate of only 13.5%. However, our framework can consistently improve the CUA success rate, with an average 6.8% improvements. Interestingly, our framework brings large improvements to weak coders such as Qwen3-Coder-30B and GPT-4o, with maximum overall improvement of 11.7%, showcasing that our framework can greatly empower weak models. Overall, these results demonstrate the effectiveness of our framework: task solvability feedback guides to robust UI design, while leveraging CUA navigation feedback optimizes interfaces toward agent-centric success."
        },
        {
            "title": "5.3 Ablation Studies",
            "content": "Ablation studies of CUA Dashboard. To ablate the Dashboard, we design two commenter variants: one using textual actions only, and the other using visual screenshots only. We evaluate both variants (Coder, Commenter) under the setting that (GPT-5, GPT-5) and (Qwen3-Coder-30B, Qwen2.5-VL72B). As shown in Fig. 5(a-b), our Dashboard consistently brings gains, significantly improving both Function Completeness (from 62.1% to 70.8%) and CUA Success Rate (from 18.7% to 25.7%) compared to the action variant for GPT-5. In contrast, the screenshot variant mostly performs worst, highlighting that visual inputs solely are inadequate for providing refinement insights, while integrating visual and textual information in our Dashboard notably benefits the commenting process. Effects by Refinement Round. As shown in the Fig.5(c-d), iteratively applying revision can consistently bring gains on the function completeness for all coders. Interestingly, it can be observed that the CUA success rate of GPT-5 coder may drop after repeated revision, while Qwen3-Coder-30B and GPT-4o can consistently gain from repeated revision. This indicates that the revision improvement may saturate for strong coders, but weak coders can be improved with iterative revisions. (a) Func. completeness. (b) CUA success rate. (c) Func. completeness. (d) CUA success rate. Figure 5: Ablation Studies of CUA Dashboard and Iterative rounds. Left (a-b): Effects by CUA Dashboard. Right (c-d): Performance across different iterative revision rounds."
        },
        {
            "title": "5.4 Qualitative Analysis",
            "content": "In Fig.6, we present four representative revision casesartisan-csa, color-match-challenge, csvto-charts, and festival-lights-show. Each row displays the initial UI alongside its revised versions, evaluated under two criteria: Function Test and CUA Test. Across the four cases, the revisions demonstrate distinct patterns of improvement. Revisions based on the Function Test, which addresses unsupported tasks, tend to focus on adding underlying functionality, sometimes with subtle 9 Initial UI w. Task Solvability Feedback w. CUA Navigation Feedback (a) artisan-csa: Create single-page app, in single HTML file, for community-supported agriculture program with hand-drawn, watercolor aesthetic. Initial UI w. Task Solvability Feedback w. CUA Navigation Feedback (b) color-match-challenge: Create single-page app in single HTML file for fast-paced color match game. - Show word (e.g., RED) in random font color player must click the correct color button (not the word meaning). - Keep score based on correct answers within 30 seconds. - Use large typography, color-coded buttons, and smooth button press animations. Initial UI w. Task Solvability Feedback w. CUA Navigation Feedback (c) csv-to-charts: Create single-page app in single HTML file with the following requirements: - Name: Data Visualization Playground - Goal: Upload CSV and generate charts. - Features: Chart type selector, color customization, save as image. - The UI should be modern with focus on charts. Initial UI w. Task Solvability Feedback w. CUA Navigation Feedback (d) festival-lights-show: Create single-page app in single HTML file with the following requirements: - Name: Festival Lights Show - Goal: Control virtual light show. - Features: Color changes, patterns, music sync. - The UI should be vibrant and dynamic. Figure 6: Qualitative comparison of initialized UI vs. refined UI. Each row shows an initial UI (left), its revision based on function (middle), and its revision based on CUAs feedback (right). visual changes. For example, the festival-lights-show revision added crucial Running state indicator, and the csv-to-charts revision added button to select delimiter. In contrast, revisions based on the CUA Test consistently yield more significant visual modifications geared towards agent accessibility. For most websites, this meant adding buttons with clear boundaries and visual hints. In both color-match-challenge and csv-to-charts, both revision types improved accessibility by presenting more information and controls upfront, reducing the need for scrolling. key CUA-friendly adaptation is seen in festival-lights-show, where increase and reduce buttons were added as complement to sliders, providing more direct and reliable interaction method for agents."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced AUI-Gym, new benchmark for automatic GUI development (52 applications; 1560 tasks with programmatic checkers), and CoderCUA collaboration framework that recasts UI design as an agent-native loop, with the Coder as Designer and the CUA as Judge. Central to this loop is the CUA Dashboard, which compresses long agent navigation trajectories into compact, interpretable summaries that reliably convert raw interactions into actionable revision signals. Empirically, task solvability is foundationalreadily improved by failure-driven functional summarizationwhereas CUA navigation remains the primary bottleneck; feedback-driven redesigns (e.g., destylization, higher contrast, simplified layouts) consistently raise execution success and robustness, highlighting the value of designing for agents rather than merely adapting human-centric interfaces."
        },
        {
            "title": "References",
            "content": "[1] openai. Operator, 2025. [2] Anthropic. Claude 3.7 sonnet system card. 2025. [3] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents, 2024. [4] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [5] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1949819508, 2025. [6] ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025. [7] Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: Benchmarking multimodal code generation for automated front-end engineering. arXiv preprint arXiv:2403.03163, 2024. [8] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539 68551, 2023. [9] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam Laradji, Manuel Del Verme, Tom Marty, Leo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. Workarena: arXiv preprint How capable are web agents at solving common knowledge work tasks? arXiv:2403.07718, 2024. [10] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. [11] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. [12] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. SetarXiv preprint of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv:2310.11441, 2023. [13] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. 11 [14] Kevin Qinghong Lin, Linjie Li, Difei Gao, Qinchen WU, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, and Mike Zheng Shou. Videogui: benchmark for gui automation from instructional videos. Advances in Neural Information Processing Systems, 37:6932969360, 2024. [15] Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan Rodriguez, Montek Kalsi, Nicolas Chapados, Tamer Ozsu, Aishwarya Agrawal, David Vazquez, Christopher Pal, et al. Uivision: desktop-centric gui benchmark for visual perception and interaction. In Forty-second International Conference on Machine Learning. [16] Yuwen Lu, Ziang Tong, Qinyi Zhao, Chengzhi Zhang, and Toby Jia-Jun Li. Ui layout generation with llms guided by ui grammar. arXiv preprint arXiv:2310.15455, 2023. [17] Jun Kong, Keven Ates, Kang Zhang, and Yan Gu. Adaptive mobile interfaces through grammar induction. In 2008 20th IEEE International Conference on Tools with Artificial Intelligence, volume 1, pages 133140. IEEE, 2008. [18] Tony Beltramelli. pix2code: Generating code from graphical user interface screenshot. In Proceedings of the ACM SIGCHI symposium on engineering interactive computing systems, pages 16, 2018. [19] Hugo Laurencon, Leo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset, 2024. [20] Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [21] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded In Proceedings of the IEEE/CVF conference on computer instructions for everyday tasks. vision and pattern recognition, pages 1074010749, 2020. [22] Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Ruslan Partsey, Jimmy Yang, Ruta Desai, Alexander William Clegg, Michal Hlavac, Tiffany Min, Theo Gervet, Vladimir Vondrus, Vincent-Pierre Berges, John Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. Habitat 3.0: co-habitat for humans, avatars and robots, 2023. [23] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building openended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. [24] OpenAI. Introducing gpt-5. OpenAI website, 2025. Available at https://openai.com/ index/introducing-gpt-5/, accessed August 10, 2025. [25] Qwen. Qwen-3-coder. https://qwenlm.github.io/blog/qwen3-coder, 2025. [26] anthropic. Introducing claude 4, 2025. [27] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024."
        },
        {
            "title": "A Additional Results",
            "content": "13 14 21 (a) Function completeness. (b) CUA success rate. Figure 7: Performance comparison after revision based on different CUA feedback. Effects by different CUAs choices. In Fig. 7, we compare UI-TARS and Operator as CUA policies within the integrated revision loop. We evaluate with two codersGPT-5 (closed-source, stronger) and Qwen3-Coder-30B (open-source, weaker)to cover both capability and licensing spectra. Both CUA policies yield comparable gains in functional completeness, with UI-TARS slightly outperforming on Qwen3-Coder-30B. Although the task-solvability signal is identical across CUAs, UITARS tends to fail more tasks, thereby surfacing richer failure cases and driving greater functionoriented revisions. For CUA success rate (SR), Operator delivers larger gains with the stronger coder (GPT-5), while improvements are similar across CUAs for the weaker coder. This suggests Operators navigation strengths are best realized on more complex UIs, whereas weaker coders often produce simpler interfaces. Overall, lightweight open-source CUAs like UI-TARS are an efficient and effective choice for harvesting navigation feedback in practice. Table 4: Main results per model (Operator as CUA policy): Each cell shows (B), where represents the CUA Success Rate and (B) in parentheses denotes the Function Completeness Rate. Model GPT-5 Qwen3Coder-30B GPT-4o Version Baseline + Revise Baseline + Revise Baseline + Revise landing game app utility interactive tool Overall 34.7% (53.0%) 41.3% (75.3%) 24.8% (77.8%) 42.6% (92.2%) 27.3% (70.6%) 38.8% (85.2%) 14.4% (63.3%) 27.8% (73.3%) 18.1% (73.0%) 10.7% (82.6%) 21.9% (70.0%) 21.4% (76.7%) 24.5% (67.9%) 31.5% (81.5%) 5.3% (16.3%) 10.0% (47.0%) 9.3% (50.4%) 27.0% (68.9%) 9.1% (41.2%) 19.1% (60.3%) 11.7% (43.9%) 20.6% (55.6%) 7.0% (52.2%) 13.7% (57.4%) 1.4% (54.8%) 23.8% (62.9%) 7.3% (42.1%) 18.6% (58.5%) 4.7% (9.7%) 15.7% (19.0%) 12.6% (55.2%) 35.9% (59.3%) 12.4% (36.1%) 14.5% (44.5%) 6.7% (38.9%) 15.0% (47.8%) 9.3% (44.8%) 5.9% (50.7%) 5.7% (37.6%) 13.8% (46.2%) 8.8% (36.3%) 16.9% (43.8%) As shown in the Table 4, when using operator as CUA policy for integrated revision, consistent improvements for both function completeness and CUA success rate can be observed. Moreover, compared to the CUA success rate showcased in Table 3, it can be observed that Operator has higher CUA success rate than UI-TARS in hard domains such as game and app that requires responsive and complex interactions, showcasing its strong navigation capability. Table 5 demonstrates the results when using different types of Dashboard for revision based on CUA navigation feedback. From the results, it can be inferred that dashboard is capable of providing comprehensive visual and textual cues derived from the CUA policy trajectories, but requiring the commenter to have strong visual perception. Moreover, our Dashboard yields 70.4% average token usage reduction across models and apps than screenshot-only commenter while delivering the strongest performance compared to the variants, providing an efficient and effective way to utilize visual screenshots and textual histories to generate refinement insights. 13 Table 5: Dashboard Ablations: CUA Success Rate (Function Completeness Rate), and token usage. Model Dashboard landing game app utility interactive tool Overall Token (K) GPT-5 Qwen3Coder-30B GPT-4o Text-only Screenshot-only Dashboard 24.0% (50.7%) 17.3% (30.3%) 17.7% (53.3%) Text-only Screenshot-only Dashboard Text-only Screenshot-only Dashboard 8.0% (18.3%) 9.3% (20.7%) 6.7% (23.3%) 7.7% (13.0%) 4.7% (10.3%) 5.7% (8.3%) 31.1% (87.8%) 16.7% (65.6%) 43.3% (87.8%) 20.7% (61.9%) 11.9% (63.7%) 20.7% (50.4%) 14.8% (57.0%) 15.6% (43.7%) 31.5% (55.2%) 21.2% (69.4%) 12.4% (42.7%) 30.0% (74.2%) 7.3% (42.4%) 5.2% (34.5%) 9.1% (38.8%) 16.1% (55.6%) 15.6% (38.3%) 21.1% (70.0%) 8.3% (54.4%) 10.6% (40.6%) 11.1% (49.4%) 12.7% (34.8%) 10.6% (31.2%) 10.0% (28.2%) 2.8% (37.8%) 6.1% (45.6%) 8.3% (34.4%) 8.9% (59.3%) 5.2% (27.8%) 21.1% (70.4%) 10.7% (48.9%) 7.4% (55.9%) 12.2% (39.3%) 15.9% (39.3%) 5.6% (34.8%) 10.4% (26.3%) 6.2% (43.3%) 9.5% (46.7%) 17.6% (69.5%) 16.2% (57.1%) 5.2% (37.6%) 11.4% (55.2%) 7.6% (29.0%) 7.1% (37.6%) 6.7% (35.7%) 18.7% (62.1%) 12.8% (41.7%) 25.7% (70.8%) 11.7% (45.6%) 8.1% (41.7%) 11.7% (41.3%) 10.8% (34.8%) 8.5% (32.5%) 12.3% (30.4%) 3.2 15.5 4.3 4.2 19.5 6.4 2.8 14.8 4.2 Why not use VLM-as-Judge as Verifiers. Table 6 demonstrates that why VLM-as-Judge evaluation on CUA task trajectory is unreliable. It can be observed that the VLM evaluation tends to judge the CUA policy outcome as failure compared to rule-based oracle evaluation, thus having very low balanced accuracy, recall and F1. Moreover, the low Cohens κ indicates very weak agreement of VLM evaluation compared to rule-based oracle evaluation. This indicates that VLM evaluation on the final screenshot only is unreliable, and may requires more screenshots along the CUA policy task trajectory for more reliable evaluation, leading to much higher computational cost. Table 6: VLM evaluation on final screenshot only is unreliable. Given the final screenshot of CUA trajectory and the expected outcome, the accuracy of VLM evaluation is only slightly above the naive all-fail baseline; Balanced acc. is near 0.55; Recall/F1 and Cohens κ are low. Metric Overall GPT-5 Qwen2.5-VL-72B GPT-4o Naive all-fail baseline accuracy Accuracy vs. oracle Balanced accuracy Precision (Pass) Recall (Pass) F1 (Pass) Cohens κ 0.720 0.735 0.556 0.616 0.147 0.237 0.145 0.720 0.736 0.549 0.660 0.121 0.205 0.128 0.720 0.738 0.568 0.612 0.178 0.276 0. 0.720 0.732 0.552 0.589 0.142 0.229 0."
        },
        {
            "title": "B Prompts Usage",
            "content": "In this section, we display the prompt used by individual rules."
        },
        {
            "title": "Task Proposer Prompt",
            "content": "Propose comprehensive set of 30 diverse, realistic user tasks for the following {tag type} application: Application: {app title} Description: {app description} Each task should be: - Clear and specific in its description - Represent realistic user scenarios - Cover different complexity levels and use cases - Grounded in an observable outcome: The tasks completion must be marked by clear and unambiguous change in the applications state or interface. The expected outcome description must precisely define this terminal state. - Avoid single element grounding (focus on complete workflows) - Test the applications core functionality effectively {tag specific content} Tag Philosophy Template: Game: Focus on GAME-SPECIFIC user tasks: 1. Playing complete game rounds or levels 2. Achieving high scores and personal bests 3. Completing specific game objectives or challenges 14 4. Using game controls and input methods 5. Navigating game menus and settings 6. Restarting games and trying different strategies 7. Progressing through difficulty levels Additional task requirements: - Focus on actual gameplay actions and goals - Include winning and losing scenarios - Cover different skill levels and strategies - Test game restart and replay functionality - Emphasize user enjoyment and engagement Tool: Focus on TOOL-SPECIFIC user tasks: 1. Creating or generating content using the tool 2. Inputting data in various formats and types (typed/pasted text or on-page controls) 3. Transforming and processing information 4. Previewing results in-page (no file uploads/downloads) 5. Using tool-specific features and options 6. Working with both simple and complex inputs 7. Completing end-to-end workflows within the page Additional task requirements: - Focus on practical use cases and workflows - Include both basic and advanced tool usage - Cover different input types and scenarios without external files - Verify visible in-page outputs or status changes in the DOM - Emphasize real-world problem solving Utility: Focus on UTILITY-SPECIFIC user tasks: 1. Setting up and configuring the utility for personal use 2. Adding, organizing, and managing data or items 3. Tracking progress and monitoring status over time 4. Using timers, reminders, and scheduling features 5. Customizing settings and preferences 6. Completing daily or routine activities 7. Accessing and updating information quickly Additional task requirements: - Focus on everyday productivity scenarios - Include setup and personalization tasks - Cover routine and habitual usage patterns - Test organization and tracking features - Emphasize practical daily life applications Interactive: Focus on INTERACTIVE-SPECIFIC user tasks: 1. Exploring and experimenting with interactive elements 2. Creating and manipulating visual or audio content 3. Adjusting parameters and settings in real-time 4. Playing with creative tools and features 5. Experiencing immersive visual or audio effects 6. Using touch, click, and gesture interactions 7. Customizing appearance and behavior Additional task requirements: - Focus on creative and exploratory activities - Include experimentation and play scenarios - Cover different interaction methods - Test customization and personalization - Emphasize sensory and aesthetic experiences 15 Landing: Focus on LANDING-SPECIFIC user tasks: 1. Browsing and exploring page content and sections 2. Reading and understanding key information 3. Clicking on call-to-action buttons and links 4. Navigating through different page sections 5. Finding contact information and ways to engage 6. Viewing team, product, or service details 7. Accessing additional resources and links Additional task requirements: - Focus on visitor browsing and exploration - Include information-seeking behaviors - Cover engagement and conversion actions - Test navigation and content discovery - Emphasize typical visitor journey scenarios App (default/other): Focus on APP-SPECIFIC user tasks: 1. Creating, editing, and managing content or data 2. Using multiple features in combination 3. Setting up and personalizing the application 4. Completing complex multi-step workflows 5. Organizing and categorizing information 6. Accessing and updating saved information Additional task requirements: - Focus on practical in-app usage - Include multi-feature workflows and combinations - Cover content creation and management - Test personalization and customization - Verify completion via visible state changes in the DOM (no external integrations) Task Categorization Framework: Each task must be categorized into one of the following three archetypes, which provides structured approach to evaluating different facets of the applications functionality: - core function: Tests single, primary feature in isolation. - user workflow: Tests sequence of features that represent complete user goal. - edge case: Tests non-standard inputs, boundary conditions, or less common interaction patterns. Please respond in JSON format: { \"app_name\": \"<app_name>\", \"tags\": [\"<tag1>\", \"...\"], \"tasks\": [ \"id\": 1, \"description\": \"Clear, specific task description\", \"category\": \"core_functionuser_workflowedge_case\", \"expected_outcome\": \"What should happen when task completes\" { } ] }"
        },
        {
            "title": "Coder Prompt",
            "content": "[Initial Website Generation] Create single-page web application based on the following specification: 16 {instruction} Requirements: 1. Create complete HTML file with embedded CSS and JavaScript 2. The app should be fully functional and interactive 3. Use modern HTML5, CSS3, and vanilla JavaScript (no external libraries) 4. Include proper semantic HTML structure 5. Make the UI clean, responsive, and user-friendly 6. Add unique IDs to interactive elements for easier automation testing 7. Ensure the app works in 1280x720 viewport Please generate the complete HTML file: [Revision from CUA Failures Core Prompt] You are tasked with improving web application based on detailed failure analysis from automated testing. ## CONTEXT Application: {app name} Model: {model name} Total Failed Tasks: {len(failed tasks)} Failure Categories: {list(failure categories.keys())} Original HTML Length: {len(initial html.strip())} ## OUTPUT FORMAT Generate single, complete, and self-contained HTML file. The file must be fully functional, including all necessary CSS and JavaScript, from !DOCTYPE html to /html. Do not use placeholders or truncate the code. ## ORIGINAL INITIAL WEBSITE (FULL) { initial_html } ## COMMENTER UI ANALYSIS {(failure analysis or No visual UI analysis available).strip()} {(non regression contract prompt or ).strip()} ## IMPROVEMENT REQUIREMENTS ### 1. Core Issues to Address Based on the failure analysis, you must: - Identify missing DOM elements that tasks expect to exist - Add missing JavaScript functionality for user interactions - Fix timing issues that prevent task completion - Ensure proper event handling and state management - Add missing visual feedback and UI updates ### 2. Specific Fixes Needed For each failed task category: - **basic usage**: Ensure fundamental interactions work (clicking, displaying, updating) - **workflow**: Support complete user workflows and multi-step processes - **advanced feature**: Implement sophisticated UI behaviors and animations - **edge case**: Handle unusual inputs and boundary conditions properly ### 3. Technical Implementation Guidelines - Preserve ALL existing working functionality from initial version - Add missing HTML elements with unique IDs for automation - Implement complete JavaScript event handlers and state updates - Ensure synchronous UI updates for immediate feedback - Do NOT introduce new input constraints that would block task inputs implied by the tasks (e.g., accept plain text or non-HTTP payloads if tasks need them). Validation must be permissive and never reduce what the initial version allowed. 17 - Do NOT auto-trigger flows on page load that would change initial states relied upon by tasks (e.g., auto-generation, auto-download, auto-navigation). Initial state should be neutral and idle. - Keep critical controls visible within 1280x720 viewport without scrolling. Avoid multi-panel hub layouts; prefer single-view, compact layouts that fit important controls on screen. - Avoid adding non-essential animations/transitions; prioritize high visibility and clarity over decoration. - Make sure timers, counters, and dynamic content work correctly ### 4. DOM Structure Requirements - Every interactive element MUST have unique ID - Form controls must have proper event listeners - Dynamic content areas must update immediately on state changes - Visual feedback must be implemented for all user actions ### 5. JavaScript Functionality Requirements - All user interactions mentioned in failed tasks must be fully implemented - State changes must be reflected in the DOM immediately - Event handlers must properly update all related UI elements - Any game logic, scoring, timing must be complete and functional Surgical Revision Policy - Preserve existing IDs; do not rename or remove working elements from initial version. - Avoid large rewrites. Patch only the functions, event handlers, and minimal markup necessary to satisfy the failed/unsupported tasks. - Preserve working logic from initial version; do not regress features that already work. - Reuse existing elements/IDs for state wherever possible; only add new IDs if strictly necessary to expose the state of new logic. - Preserve initial version immediacy semantics. Do NOT introduce extra confirmation steps as prerequisites where initial version achieved completion via immediate interactions. Implement functional logic first, then expose proxies from the same code path; never update proxies without the underlying state change. Commenter JSON (if provided) - If the COMMENTER UI ANALYSIS is JSON object, prioritize applying entries in actionable changes precisely. - Keep changes surgical and bounded by those actionable suggestions; do not broaden scope beyond them. ## OUTPUT REQUIREMENTS Generate COMPLETE, FULLY FUNCTIONAL HTML file that: 1. Addresses ALL failure points identified in the analysis 2. Maintains existing successful functionality from initial version 3. Implements missing features causing task failures 4. Provides proper DOM elements for automation testing 5. Ensures immediate UI feedback for all user actions [Revision Agent-centric Design Principles] While improving functionality, apply the following design principles to optimize the UI for automated agents. The goal is functionality and testability, not human aesthetics. ### A. Visual Clarity and Simplicity - Use simple color scheme (e.g., black text on white background). - Avoid decorative elements that do not serve functional purpose, such as animations, gradients, or shadows. - Establish clear visual hierarchy using typography and spacing. Logically group related controls. ### B. Robust Agent Interaction - All interactive controls must be clearly labeled and sized appropriately to be easily and unambiguously targeted by automation tools. - Support keyboard-based interaction for all core functionality. Navigable elements should have clear focus indicators. - Prioritize immediate state updates upon interaction. Avoid complex, multi-step confirmation dialogs for actions where direct manipulation is sufficient. - All critical functionality should be accessible within standard 1280x720 viewport without requiring scrolling. ### C. Transparent State Management - The DOM must serve as reliable, single source of truth for the applications state. - Ensure that any significant state change (e.g., result is generated, calculation is complete) is clearly and synchronously reflected in the DOM. This can be achieved by updating element attributes, text content, or values. - Interactive elements and state indicators must have unique and stable IDs to facilitate reliable testing and interaction. ### D. Versatile Input Handling - For continuous inputs (like sliders), provide alternative discrete control mechanisms (e.g., step buttons, direct text input). No interaction should rely solely on pointer-dragging. - Input validation should be permissive and should not block inputs that an automated task might reasonably provide. - Distinguish between actions that cause immediate, reversible state changes (e.g., selecting an option) and those that trigger irreversible, multi-step processes (e.g., submitting form). ### E. Behavior Preservation - Simplifying the visual design must not alter the core interaction logic. - Any user action that was immediate in initial version must remain immediate in the revised version. Please generate the complete improved HTML file: [Revision from Unsupported Tasks] You are tasked with improving web application to support additional tasks that are currently unsupported. ## CONTEXT Application: {app name} Model: {model name} Total Unsupported Tasks: {len(unsupported tasks)} Original HTML Length: {len(initial html.strip())} ## OUTPUT FORMAT Generate single, complete, and self-contained HTML file. The file must be fully functional, including all necessary CSS and JavaScript, from !DOCTYPE html to /html. Do not use placeholders or truncate the code. ## ORIGINAL INITIAL WEBSITE (FULL) { initial_html } ## UNSUPPORTED TASKS ANALYSIS {unsupported summary} ## CODE PRESERVATION CONTRACT (Non-Regression) { if ablate no contract else (non regression contract prompt or ).strip()} ## IMPROVEMENT REQUIREMENTS ### 1. Task Support Issues to Address Based on the unsupported task analysis, you must ADD missing functionality: - Add missing DOM elements that tasks expect to exist - Implement missing JavaScript functionality for user interactions - Add missing form controls and input handling - Implement missing display areas and visual feedback - Add missing navigation and UI components ### 2. Implementation Guidelines - PRESERVE all existing working functionality from initial version - ADD new HTML elements with unique IDs for automation - IMPLEMENT complete JavaScript event handlers for new features 19 - ENSURE new UI elements are properly styled and visible - DO NOT introduce new input constraints that would block task inputs implied by tasks; validation must be permissive and must not reduce what the initial version allowed. - DO NOT auto-trigger flows on load that change initial states (no auto-generation, auto-download, auto-navigation). Start in neutral, idle state. - FIT critical controls within 1280x720 viewport without scrolling. Avoid multi-panel hub layouts and unnecessary panels that push controls below the fold. - IMPLEMENT missing workflows and user interaction patterns ### 3. DOM Structure Requirements - Every new interactive element MUST have unique ID - New form controls must have proper event listeners - New content areas must update appropriately on state changes - New visual feedback must be implemented for added interactions ### 4. JavaScript Functionality Requirements - All new user interactions mentioned in unsupported tasks must be fully implemented - New state changes must be reflected in the DOM immediately - New event handlers must properly update all related UI elements - Any new game logic, scoring, timing must be complete and functional ## OUTPUT REQUIREMENTS Generate complete and fully functional HTML file that: 1. Maintains all existing functionality from initial version. 2. Adds the missing functionality required to support the new tasks. 3. Implements all necessary DOM elements and JavaScript for task support. 4. Ensures all new features are robust and testable. Commenter JSON (if provided) - If upstream provides commenter JSON analysis with actionable changes, follow those changes first, precisely and surgically. Surgical Revision Policy - Preserve existing IDs; do not rename or remove working elements from initial version. - Avoid large rewrites. Patch only the functions, event handlers, and minimal markup necessary to satisfy the failed/unsupported tasks. - Preserve working logic from initial version; do not regress features that already work. - Reuse existing elements/IDs for state wherever possible; only add new IDs if strictly necessary to expose the state of new logic. - Preserve initial version immediacy semantics. Do NOT introduce extra confirmation steps as prerequisites where initial version achieved completion via immediate interactions. Implement functional logic first, then expose proxies from the same code path; never update proxies without the underlying state change. Please generate the complete improved HTML file:"
        },
        {
            "title": "CUA Policy Prompt",
            "content": "You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. ## Output Format Thought: ... Action: ... ## Action Space click(point=x1 y1) left double(point=x1 y1) right single(point=x1 y1) drag(start point=x1 y1, end point=x2 y2) hotkey(key=ctrl c) # Split keys with space and use lowercase. Also, do not use more than 3 keys in 20 one hotkey action. type(content=xxx) # Use escape characters , , and in content part to ensure we can parse the content in normal python string format. If you want to submit your input, use at the end of content. scroll(point=x1 y1, direction=down or up or right or left) # Show more information on the direction side. wait() # Sleep for 5 and take screenshot to check for any changes. finished(content=xxx) # Use escape characters , , and in content part to ensure we can parse the content in normal python string format. ## Note - Use {language} in Thought part. - Write small plan and finally summarize your next action (with its target element) in one sentence in Thought part. ## User Instruction {instruction}"
        },
        {
            "title": "Dashboard Commenter Prompt",
            "content": "You are diagnosing UI design issue that caused task failure for Computer-Use Agent (CUA). Your goal is to conduct root cause analysis based on core set of design principles and output structured diagnostic report in JSON format. This report will guide the next iteration of UI code generation. You will be provided with two images: 1. The current website state (Resolution: {width}x{height}) 2. storyboard summarizing the failed task attempt, arranged as grid of step screenshots (variable count) fitted into 1920x1080 canvas Your analysis must be guided by the following Agent-Centric UI Design Principles: 1. State Visibility: Any significant state change resulting from an agents action must be clearly and synchronously reflected in the DOM. This can be achieved by updating element attributes, text content, or values. Ambiguous or out-of-band feedback (like temporary toast notifications) is considered violation. 2. Interaction Robustness: All UI components critical for task completion must be visible and actionable within standard 1280x720 viewport without requiring scrolling. Elements should have clear, stable identifiers. 3. Input Permissiveness: Input fields and controls should accept the most general data format required for the task, avoiding overly restrictive client-side validation that may block agent inputs. 4. Predictable Behavior: The UI should remain in stable, neutral state upon loading. Based on these principles, analyze the provided materials and output compact JSON object. Output strictly as JSON with these keys only: - issues: An array of up to 3 short strings identifying the primary UI problem categories, derived from the violated principles (e.g., visibility, interaction, feedback). - actionable changes: An array of 36 diagnostic statements. Each statement must identify specific UI element (referencing selectors/IDs) and explain which design principle it violated, providing root cause for the failure. Example: The element #submit-btn violates the Interaction Robustness principle, as it is not visible in the default viewport. - fit within screen: diagnostic boolean flag. Set to true only if the primary reason for failure was violation of the Interaction Robustness principle concerning viewport visibility. - avoid regressions: confirmation flag, set to true, signifying that the diagnosis adheres to minimal intervention philosophy. This confirms the analysis focuses solely on fixing the observed failure without disturbing unrelated, functional parts of the UI. Respond with JSON only, no extra text."
        },
        {
            "title": "C Full Statistics and Examples",
            "content": "In Tab.7, we display the full statistics and corresponding examples. 21 Table 7: Distribution and examples of six domains in AUI-Gym. For each domain, we show website created by GPT-5, paired with 30 tasks (blue) simulating real-world usage. Each task is further linked to rule-based verifier (green). Domain #Apps Percentage Example Instruction GUI created by GPT-5 App 11 21% Landing 10 19% Game 9 17% Interactive 9 17% Tool 7 13% Utility 6 12% Create single-page app in single HTML file with the following requirements: - Name: Healthy Meal Tracker - Goal: Log meals and nutrition info. - Features: Ingredient list, calories per meal, daily summary. - The UI should be clean with food icons. Task: Add five meals for todays date (any names/ingredients) so todays meal count reaches at least 5. Rule: #dailyMealCount >= 5 Create single-page app in single HTML file with the following requirements: - Name: Nonprofit Impact Report - Goal: Show measurable results of programs. - Features: Infographics, success stories, donation link. - The UI should be inspiring and visually engaging. Task: Navigate to Success Stories and expand the first story card to reveal the full narrative. Rule: #slides .slide:first-child button[aria-expanded] == true OR #slides .slide:first-child.expanded exists Create single-page app in single HTML file with the following requirements: - Name: Typing Rain - Goal: Type falling words before they reach the bottom. - Features: Increasing difficulty, accuracy tracker, score. - The UI should be the city background with animated raindrop words. Task: In single run, achieve score of at least 500 points. Rule: #scoreValue >= 500 Create single-page app in single HTML file with the following requirements: - Name: Festival Lights Show - Goal: Control virtual light show. - Features: Color changes, patterns, music sync. - The UI should be vibrant and dynamic. Task: Enable Music Sync, start playback, then pause the built-in track; confirm audio status is Paused while Music Sync remains enabled. Rule: #audioStatus == Paused AND #syncBadge != Sync: Off Create single-page app in single HTML file with the following requirements: - Name: Customer Journey Flow - Goal: Sketch customer journey stages and connections. - Features: Add/edit stages, connect nodes, view JSON of the flow. - The UI should be simple and full-screen. Task: Create Social Ad and Search Ad leading to Landing Page, then to Consideration and Purchase (two branches merging into one path). Rule: #io-json contains Social Ad AND #io-json contains Search Ad AND #io-json contains Landing Page AND #io-json contains Consideration AND #io-json contains Purchase Create single-page app in single HTML file with the following requirements: - Name: Pomodoro - Goal: Time focus and break sessions. - Features: Focus/break modes, timers, basic controls. - The UI should be minimal and distraction-free. Task: Start short break and verify the mode label and starting time show 5-minute break. Rule: #lblSession == Short Break AND #lblTime contains 05:"
        }
    ],
    "affiliations": [
        "Microsoft",
        "Show Lab, National University of Singapore",
        "University of Oxford"
    ]
}