{
    "paper_title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation",
    "authors": [
        "Zixian Liu",
        "Mingtong Zhang",
        "Yunzhu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io."
        },
        {
            "title": "Start",
            "content": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation Zixian Liu1, Mingtong Zhang2, and Yunzhu Li3 5 2 0 2 3 1 ] . [ 1 6 4 5 0 1 . 3 0 5 2 : r Fig. 1: KUDA is an open-vocabulary manipulation system that uses keypoints to unify the visual prompting of vision language models (VLMs) and dynamics modeling. Taking the RGBD observation and the language instruction as inputs, KUDA samples keypoints in the environment, then uses VLM to generate code specifying keypoint-based target specification. These keypoints are translated into cost function for model-based planning with learned dynamics models, enabling open-vocabulary manipulation across various object categories. Abstract With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches limiting their overlook the importance of object dynamics, applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypointbased representations are then converted into cost functions, which are optimized using learned dynamics model to produce robotic trajectories. We evaluate KUDA on range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http: //kuda-dynamics.github.io. I. INTRODUCTION It has been longstanding focus to create an openvocabulary robotic system capable of executing tasks based on human language in diverse environments. However, human language is inherently abstract and often ambiguous, requiring contextual knowledge and the ability to ground denotes equal contribution. 1Tsinghua University, 2University of Illinois Urbana-Champaign, 3Columbia University language inputs in the environments where the robots operate. Recent advances in large language models (LLMs) and vision-language models (VLMs) [19] have demonstrated advanced capabilities in text and image understanding. These developments have paved new paths for incorporating such models into robotic systems [1012]. However, many existing open-vocabulary robotic systems heavily rely on VLMs and LLMs for guidance at all levels and do not provide an explicit account of object dynamics. As result, they typically focus only on rigid objects and coarse-grained manipulation, limiting their applicability to more complex and dynamic tasks involving diverse object categories, such as deformable objects and object piles. On the other hand, learning-based dynamics models have shown the ability to model the complex behaviors of realworld objects directly from observation data [1316]. These models can accurately predict the future states of objects with varying object categories and shapes, accounting for different interactions. However, model-based planning with dynamics models typically requires pre-defined target state or cost function, which cannot be directly inferred from highlevel language instructions. This raises key question: How can we develop open-vocabulary manipulation systems that harness the flexibility of task specification via VLMs while preserving the benefits of model-based planning? Our key insight is to develop unified keypoint representation that integrates dynamics learning and visual prompting through VLMs. Using keypoints as the visual representation is intuitive for vision-language models to interpret and express, while also being precisely defined and easily translatable into cost function for planning with dynamics models. To achieve this, we propose defining the objective function for planning using keypoints and employing markbased visual prompting, inspired by [11], to enable the VLM to generate code that specifies the objective as arithmetic relationships between the visual keypoints. Building on our keypoint-based target specifications, we introduce KUDA: an open-vocabulary manipulation system that utilizes Keypoints to Unify Dynamics learning and visuAl prompting. KUDA employs an upstream VLM along with pre-trained dynamics model, using keypoints as shared intermediate representation. Given language instruction for the manipulation task and the current visual observation of the experimental setup, KUDA automatically samples and labels keypoints from the RGB image. The VLM is then prompted to generate target specifications, which are subsequently converted into cost function. During robot execution, two-level closed-loop control mechanism ensures effective and robust model-based planning. Notably, we found that incorporating few-shot examples significantly enhances the performance of the VLM. Inspired by this, we developed prompt library and retrieval mechanism based on score matching, ensuring high-quality few-shot examples without exceeding input token limits. In summary, our contributions are as follows: We propose using keypoints as unified intermediate representation to bridge dynamics learning and visual prompting through VLMs. We design prompt retriever that automatically subsamples from our prompt library based on the task description, while ensuring it stays within the context window of the VLMs. We evaluate the integrated system in real-world manipulation tasks, demonstrating state-of-the-art performance on tasks involving diverse object materials, such as ropes and granular objects  (Fig. 1)  , and covering range of language instructions. II. RELATED WORK A. Grounding Language Instructions Using human language to instruct intelligent robots has been an active research domain. However, most works [17 19] mainly focus on decomposing high-level language instructions as subtasks. Grounding ambiguous human language into structured action sequences that robots can execute remains significant challenge [2022]. Most existing approaches use action primitives as the basic elements for planning. These methods either employ classical techniques such as lexical analysis, formal logic, and graphical models [20, 2325], or leverage pre-trained large models to comprehend instructions and generate task plans [22, 26 28]. However, the reliance on pre-defined motion primitives is often seen as major limitation for developing universal manipulation system [10]. Many recent works have also focused on grounding language into lower-level actions. These approaches include language-conditioned imitation learning [2931], synthesizing value maps or reward functions [10, 12, 32, 33], and generating motions through visual prompting [11, 34]. In our work, we propose new paradigm to ground natural language instructions into keypoint target configurations for model-based planning with learned dynamics models. B. Foundation Models in Robotics The remarkable success of large language models (LLMs) has recently earned significant interest in the field of robotics. Recent works have explored how to effectively integrate these powerful models into robotic systems. Some approaches leverage LLMs for high-level task planning [35, 36], while others have demonstrated that LLMs excel at generating code for robot control [28, 37, 38]. Another application of LLMs is in synthesizing value functions or reward functions [10, 33, 39]. However, these approaches typically require converting both the task and observations into textual form, which often leads to suboptimal performance in realworld manipulation scenarios. Recently, vision-language models (VLMs) have earned significant attention. In addition to replacing previous LLMs with powerful VLMs, such as GPT-4V [6], to achieve better grounding in real-world scenarios, VLMs have been applied in various contexts. Some pre-trained VLMs have shown superior perception capabilities [4042], while others have been used to generate affordances or constraints based on visual prompts [11, 12, 43]. However, despite their capabilities, these advanced VLMs lack good understanding of the 3D spatial relationships and object dynamics, making them inefficient in more complicated manipulation tasks. Another recent trend focuses on collecting large-scale robotics data and training end-to-end general-purpose models for robotic tasks [4447]. However, many works in this direction focus primarily on rigid object manipulation and cannot perform complex manipulation across various object categories due to lack of knowledge of object dynamics. Additionally, collecting data and training models for such works is highly resource-intensive, and their performance remains limited. In our work, we leverage the state-ofthe-art VLM to generate the target specifications for the neural dynamics models, which enables flexible and robust framework that facilitates model-based planning. C. Visual Prompting and In-Context Learning Visual prompting is an emerging technique that has earned much attention with the development of large visionlanguage models. This technique enhances the visual grounding capabilities of these foundation models through incorporating visual markers into observations. Yang et al. [48] demonstrated that overlaying the original image with its semantic segmentation enables GPT-4V to answer questions requiring visual grounding. Cai et al. [49] introduced ViPLLaVA, model capable of decoding visual prompts that include various types of visual markers. Others have also been using visual prompts generated by predefined rules to produce affordances for robotic planning [11, 34]. In-context learning is relatively novel paradigm in the vision domain. It allows pre-trained model to efficiently adapt to novel downstream task using few input-output examples, without requiring fine-tuning. One of the earliest works in this area is Flamingo [50], vision-language model (VLM) that is capable of learning from few-shot examples. Bar et al. [51] approach the image-to-image in-context learning problem as an image inpainting task, while Yang et al. [52] introduce in-context learning for image editing. Li et al. [53] offer general method for in-context learning in segmentation tasks. Additionally, Zhang et al. [54] provide comprehensive study on how to select effective examples for visual in-context learning. In our work, we utilize set of well-designed prompting formulations to guide the vision language model in generating keypoint-based target specifications. Additionally, inspired by [55], we introduce prompt retriever to ensure the quality of few-shot examples. III. METHOD We first provide the problem formulation. Then we describe how we prompt the VLM to generate the keypointbased target specification. Next, we present the design of the prompt retriever and the Top-K prompt library. Lastly, we specify how we obtain the cost function from target specification and how we perform model-based planning with neural dynamics models, along with the two-level closed-loop control mechanism. The overview of our KUDA framework is shown in Fig. 2. A. Problem Formulation We consider tabletop manipulation problem defined by free-form language instruction that pertains to one or multiple objects O, along with pre-trained neural dynamics model . The goal is to optimize the robots action trajectory τ to perform the manipulation task described by L. The instruction is typically abstract and requires interpretation that incorporates both common sense and the context of the experiment configurations. The dynamics model is able to predict how the object set will change when specific action is applied: ˆzt+1 = (zt, ut), (1) where zt and zt+1 represent the current state of the environment at time and the subsequent state at time + 1, respectively. ˆzt+1 is the predicted state at time + 1, and ut is the applied action. The environment state describes the positions of the 3D points oi = (xi, yi, zi) that represent the objects (or parts of an object) in O. The key problem here is how to define the objectives from the ambiguous instruction for optimization. We leverage VLMs to propose keypoint-based representation, where we will obtain our cost function. Once the cost function C(L) is obtained from the instruction L, the problem can be formulated as an optimization problem over the robots action trajectory τ : τ = arg min τ C(zL), (2) where represents the final state of the environment after the action sequence τ has been applied. B. Keypoint-Based Target Specification Inspired by [11], we employ visual markers to enhance the visual grounding capabilities of vision-language models (VLMs). key insight in our framework is that wide range of manipulation tasks can be effectively described by the spatial relationships between keypoints on the objects to be manipulated and reference points within the environment. For instance, the task of straightening rope on table can be articulated as pulling one end of the rope to the left side of the center of the table while pulling the other end to the right. Notably, we found that VLMs are highly proficient at generating such spatial relationships when provided with appropriate visual prompts. Specifically, we first segment all semantic masks from the RGB observation utilizing Segment Anything [42]. Next, we perform farthest point sampling (FPS) on these masks to obtain keypoints and reference points, which we then label on the original image. We prompt the VLM to select keypoints and, for each keypoint k, to specify target position that describes the desired final state of the object set upon task completion. The target position is determined by its spatial offset from reference point, as provided by the VLM through set of code assignment statements in the form of = + [dx, dx, dz], which denotes that the target position is equal to the reference point rs position added by an offset [dx, dx, dz]. We refer to each (k, p) pair as target specification. It is important to note that these target specifications do not have to be strictly satisfied after executing the task, as the vision-language model may occasionally generate infeasible specifications due to its lack of knowledge of object dynamics. Despite this, we found that optimizing for approximate target specifications is generally sufficient to successfully complete the instruction. For more detailed prompting process, please refer to the appendix on our project website. C. Top-K Prompt Library In our experiments, we noticed that providing few examples of similar tasks significantly improved the performance of the VLM. As result, we collected diverse set of examples that cover all the object categories used in our experiments. However, existing VLMs, such as GPT4V, often have limitations on the number of input images they can task as inputs. Additionally, images consume substantial number of input tokens, which can be costly and affect the efficiency of the program. To address this, we developed prompt retriever that selects optimal examples from the prompt library through score matching. Specifically, we employ CLIP [7] to encode both the input observation and instruction L, as well as all the examples represented by tuples (qi, obsi, ri) in the prompt library P, where qi is the text query, obsi is the corresponding observation in the example, and ri is the response provided by human expert. We then normalize Fig. 2: Overview of KUDA. Taking the RGBD observations and language instruction as inputs, we first utilize the large vision model to obtain the keypoints and label them on the RGB image to obtain the visual prompt (green dot marks the center reference point). Next, the vision-language model generates code for target specifications, which are projected into 3D space to construct the 3D objectives. Lastly, we utilize the pre-trained dynamics model for model-based planning. After certain number of actions, the VLM is re-queried with the current observation, enabling high-level closed-loop planning to correct VLM and execution errors. these latent vectors and obtain similarities by dot product. The matching score Si is calculated as weighted average of the image and text similarities: Si = fI (s) fI (s) fI (obsi) fI (obsi) + λ fT (L) fT (L) fT (qi) fT (qi) , (3) where fI and fT represent the image and text encoders of CLIP, respectively, λ is hyperparameter set to 0.6. The top examples are selected and incorporated into our prompt based on the score. D. Two-Level Closed-Loop Planning In our framework, we propose two-level closed-loop planning to improve the robustness and effectiveness of the manipulation tasks. At the low-level closed loop of model-based planning, once the target specifications are obtained, the next step is to translate them into optimization objectives. We start by projecting the keypoints and their corresponding targets into 3D space. Next, we extract the object points oi from the objects point clouds and align each keypoint with its nearest object point. The objective is then defined as the sum of the Euclidean distances between keypoints and their corresponding targets: C(zL) = (cid:88) oi pi2 (oi z), (4) where pi represents the 3D target of point oi, and the summation is performed over all target specifications. Using the objective defined in Eqn. 2 and the dynamics model , we employ the Model Predictive Path Integral (MPPI) [56] algorithm, to determine the action to be executed. However, for specific object categories such as granular pieces, limited number of keypoints is insufficient to accurately describe the target shape specified by the instruction. Thus, we perform high-level closed-loop re-planning with the VLM, re-prompting it with the current observation and instruction to update target specifications after series of actions within loop. This two-level closed-loop planning framework effectively corrects imperfect target specifications and execution errors, etc., ensuring the systems robustness even in the presence of external disturbances. IV. EXPERIMENTS The main purpose of our experiments is to verify and analyze the ability of our system to perform variety of tasks on different objects given various language instructions or experiment configurations. We aim to answer the following research questions: (1) How well does our system generalize to diverse text instructions and visual scenarios? (2) How does our framework handle complex manipulation tasks across various object categories? (3) How does each module contribute to the systems failure cases? We conduct qualitative evaluations on diversified set of tasks to demonstrate the effectiveness of our system. To highlight its model-based planning capabilities with neural dynamics models, we compare our framework against two baselines. Additionally, we provide component-wise experiment error breakdown for comprehensive analysis of our frameworks effectiveness. Furthermore, as an ablation study, we examine the impact of the hyperparameter in the top-K prompt library on in-context learning. To demonstrate the flexibility of our system across various objects, we train the neural dynamics models on 4 Fig. 3: Qualitative Results of the Rollouts. We show the target specification and robot executions of various tasks on different objects, highlight the effectiveness of our framework. We show the initial state and the target specification visualization of our system, along with the robot executions, to demonstrate the performance of our framework on various manipulation tasks. Note that we show the granular collection task to exhibit how our VLM-level closed-loop control works in our two VLM-level loops. different object categories: rope, cubes, granular pieces, and T-shaped block. The first three categories utilize graphbased neural dynamics models, whereas the T-shaped block employs state-based neural dynamics model trained using multilayer perceptron network. We compare our system with MOKA [11] and VoxPoser [10] in tabletop environment. These two baselines also enable open-vocabulary manipulation and in-context few-shot learning. MOKA builds framework to prompt VLM to directly generate motion, and Voxposer uses LLM to synthesize 3D voxel map as affordance. To ensure fair comparison, the prompts and few-shot examples of these 2 systems are adapted to be suitable for our tasks. To avoid the possible overfit in few-shot learning, we ensure that no example in the prompt library is exactly the same as the task. All vision language models and large language models used in our system and two baselines are specified to be GPT-4o. A. Qualitative Results In Fig. 3, we present 5 tasks featuring different text instructions and visual scenarios, along with their corresponding target specifications generated by our system and the robots executions. These examples clearly demonstrate the effectiveness of our framework. In each case, our system generates precise target specifications aligned with the language instructions, and the robot executes the tasks effectively. Notably, two tasks involving cubes start with similar initial configurations but differ in instructions which are easy to confuse. The VLM effectively distinguished semantics and provided precise target specifications, demonstrating its strong visual understanding capabilities. The coffee bean collection task highlights the benefits of our VLM-level closed-loop planning. Initially, the target specifications were too sparse to fully manipulate the coffee bean pile, leaving few unspecified beans outside the square after multiple actions. Our system identified these errors and corrected them in the subsequent loop. These examples demonstrate the flexibility of our framework across wide variety of instructions and environment configurations. B. Quantitative Results We compare our system with two baselines on total of six tasks across the four object categories. The results are shown in Tab. I. For each task, we evaluate the success rate by measuring the Chamfer distance between the point clouds of the objects after specified number of robot actions are applied and the corresponding target point clouds. The quantitative results are shown as the total number of successful trials out of total of 10. The text instructions for all evaluation tasks are listed in Tab. II. Methods MOKA [11] VoxPoser [10] Ours Rope Straightening Cube Collection Cube Movement Granular Collection Granular Movement Movement Total 2/10 0/10 6/ 0/10 0/10 0/10 0/10 3/10 3/10 1/10 1/10 0/ 8/10 6/10 10/10 10/10 6/10 8/10 13.3% 13.3% 80.0% TABLE I: Quantitative results of our evaluation. Our method achieved relatively high performance across all evaluation tasks compared to the two baselines, while the failures in Cube Collection and Granular Movement were primarily caused by perception. As demonstrated in the quantitative evaluation results, our system is superior to these existing methods by large margin on the evaluation tasks. The main reason for such performance gap is that the existing methods ignore the fine-grained representation of objects or actions and lack the knowledge of object dynamics, which leads to their limited capability to manipulate deformable objects or rigid objects Fig. 4: Visualizations of Error Breakdown. We provide detailed breakdown of each failure mode, marked in red. While we achieved an 80% success rate across 60 trials for various tasks, the primary cause of failure was perception errors, accounting for 10% of all trials and 50% of the failure cases. of more complex shapes. In contrast, our system utilizes the keypoint-based representation for visual prompting and dynamics learning. This enables our system to predict the future state of those complex objects and perform modelbased planning for manipulation. Rope Straightening Cube Collection Cube Movement Granular Collection Granular Movement Movement Straighten the rope. Move all the cubes to the pink cross. Move the yellow cube to the red cross. Collect all the coffee beans together. Move all the coffee beans to the red cross. Move the orange into the pink square. TABLE II: The input instructions of each evaluation task. C. Error Breakdown We conducted manual analysis of the failure cases encountered during our experiments, as shown in Fig. 4. As illustrated, 1) the perception module accounted for the majority of errors. This includes instances where the module failed to detect objects, particularly when they overlapped with other shapes, such as cross sign on the table, or failed to distinguish between two adjacent cubes. 2) The second most significant source of error stemmed from the target specification and the tracking module. Errors in target specification typically arose from the VLM providing underspecified targets, i.e., an insufficient number of target specifications to complete the task. The tracking module commonly failed when tracking keypoints in dense object piles, such as coffee beans. 3) Additionally, smaller proportion of failures were attributed to the dynamics model and hardware, where the dynamics model occasionally produced inaccurate predictions, and the RGBD camera sometimes provided inaccurate depth values. prompting method labeled category where examples are manually selected by human expert. The results are presented in Tab. III. Notably, when = 3, the prompt retriever achieves performance on par with that of human expert. However, increasing introduces less relevant examples, which lowers the overall prompt quality and results in reduced success rate for prompt retriever when = 5. V. CONCLUSION & LIMITATIONS In this work, we propose novel open-vocabulary robotic manipulation system KUDA, which unifies the visual prompting of the vision language models and dynamics learning through keypoint-based representation. Utilizing this flexible representation, KUDA leverages the vision language model to deal with various high-level human language instructions, and in the meantime utilizes modelbased planning with dynamics models to generate robot actions, to perform complex manipulation tasks on various object categories, with different language instructions. Our experiments demonstrate the effectiveness and versatility of our framework. However, KUDA also has several limitations: First, it uses top camera to capture visual observations for the vision language model, limiting its ability to perform tasks with more complex 3D spatial relationships. Second, the dynamics models in our work are trained in simulations, leading to an inevitable sim-to-real gap and limited generalization to different object categories. We believe that with the development of related fields in the future, the problems above will be finally eliminated. We hope that KUDA will inspire more future work on incorporating knowledge of dynamics into more versatile robotic systems. D. Ablation of Top-K Prompt Library VI. ACKNOWLEDGEMENT Category TopTop-1 Top-3 Top-5 Success Rate 10/10 2/ 3/10 10/10 7/10 TABLE III: The quantitative results of different values. In this ablation, we evaluate the success rate on the Rope Straightening task for different values, as well as special This work is partially supported by the Toyota Research Institute (TRI), the Sony Group Corporation, and Google. This article solely reflects the opinions and conclusions of its authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Devlin, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805, 2018. [2] A. Radford, Improving language understanding by generative pre-training, 2018. [3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [4] T. B. Brown, Language models are few-shot learners, arXiv preprint arXiv:2005.14165, 2020. [5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scaling language modeling with pathways, Journal of Machine Learning Research, vol. 24, no. 240, pp. 1113, 2023. [6] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [7] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PMLR, 2021, pp. 87488763. [8] J. Li, D. Li, C. Xiong, and S. Hoi, Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, in International conference on machine learning. PMLR, 2022, pp. 12 888 12 900. [9] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in International conference on machine learning. PMLR, 2023, pp. 19 73019 742. [10] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. FeiFei, Voxposer: Composable 3d value maps for robotic manipulation with language models, arXiv preprint arXiv:2307.05973, 2023. [11] F. Liu, K. Fang, P. Abbeel, and S. Levine, Moka: through preprint robotic manipulation arXiv Open-vocabulary mark-based arXiv:2403.03174, 2024. prompting, visual [12] W. Huang, C. Wang, Y. Li, R. Zhang, and L. FeiFei, Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation, arXiv preprint arXiv:2409.01652, 2024. [13] C. Finn and S. Levine, Deep visual foresight for planning robot motion, in 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017, pp. 27862793. [14] A. Nagabandi, K. Konolige, S. Levine, and V. Kumar, Deep dynamics models for learning dexterous manipulation, in Conference on Robot Learning. PMLR, 2020, pp. 11011112. [15] P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende, et al., Interaction networks for learning about objects, relations and physics, Advances in neural information processing systems, vol. 29, 2016. [16] Y. Li, J. Wu, R. Tedrake, J. B. Tenenbaum, and A. Torralba, Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids, in ICLR, 2019. [17] S. Karamcheti, D. Sadigh, and P. Liang, Learning adaptive language interfaces through decomposition, arXiv preprint arXiv:2010.05190, 2020. [18] V. Myers, B. C. Zheng, O. Mees, S. Levine, and K. Fang, Policy adaptation via language optimization: Decomposing tasks for few-shot imitation, arXiv preprint arXiv:2408.16228, 2024. [19] A. Z. Ren, B. Govil, T.-Y. Yang, K. R. Narasimhan, and A. Majumdar, Leveraging language for accelerated learning of tool manipulation, in Conference on Robot Learning. PMLR, 2023, pp. 15311541. [20] T. Kollar, S. Tellex, D. Roy, and N. Roy, Toward understanding natural language directions, in 2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2010, pp. 259266. [21] D. K. Misra, J. Sung, K. Lee, and A. Saxena, Tell me dave: Context-sensitive grounding of natural language to manipulation instructions, in Proceedings of Robotics: Science and Systems, Berkeley, USA, July 2014. [22] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.- H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng, Do as can and not as say: Grounding language in robotic affordances, in arXiv preprint arXiv:2204.01691, 2022. [23] J. Thomason, S. Zhang, R. J. Mooney, and P. Stone, Learning to interpret natural language commands through human-robot dialog, in Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015. [24] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy, Understanding natural language commands for robotic navigation and mobile manipulation, in Proceedings of the AAAI conference on artificial intelligence, vol. 25, no. 1, 2011, pp. 1507 1514. [25] T. Kollar, S. Tellex, D. Roy, and N. Roy, Grounding language commands to verbs of motion in natural robots, in Experimental robotics: The 12th international symposium on experimental robotics. Springer, 2014, pp. 3147. [26] H. Jiang, B. Huang, R. Wu, Z. Li, S. Garg, H. Nayyeri, S. Wang, and Y. Li, Roboexp: Action-conditioned scene graph via interactive exploration for robotic manipulation, arXiv preprint arXiv:2402.15487, 2024. [27] Y. Hu, F. Lin, T. Zhang, L. Yi, and Y. Gao, Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning, arXiv preprint arXiv:2311.17842, 2023. [28] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, Code as policies: Language model programs for embodied control, in arXiv preprint arXiv:2209.07753, 2022. [29] M. Shridhar, L. Manuelli, and D. Fox, Cliport: What and where pathways for robotic manipulation, in Proceedings of the 5th Conference on Robot Learning (CoRL), 2021. [30] , Perceiver-actor: multi-task transformer for robotic manipulation, in Proceedings of the 6th Conference on Robot Learning (CoRL), 2022. [31] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, Bc-z: Zero-shot task generalization with robotic imitation learning, in Conference on Robot Learning. PMLR, 2022, pp. 991 1002. [32] P. Sharma, B. Sundaralingam, V. Blukis, C. Paxton, T. Hermans, A. Torralba, J. Andreas, and D. Fox, Correcting robot plans with natural language feedback, RSS, 2022. [33] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. Gonzalez Arenas, H.-T. Lewis Chiang, T. Erez, L. Hasenclever, J. Humplik, B. Ichter, T. Xiao, P. Xu, A. Zeng, T. Zhang, N. Heess, D. Sadigh, J. Tan, Y. Tassa, and F. Xia, Language to rewards for robotic skill synthesis, Arxiv preprint arXiv:2306.08647, 2023. [34] S. Nasiriany, F. Xia, W. Yu, T. Xiao, J. Liang, I. Dasgupta, A. Xie, D. Driess, A. Wahid, Z. Xu, et al., Pivot: Iterative visual prompting elicits actionable knowledge for vlms, arXiv preprint arXiv:2402.07872, 2024. [35] Y. Chen, R. Gandhi, Y. Zhang, and C. Fan, Nl2tl: Transforming natural logics using large language models, arXiv preprint arXiv:2305.07766, 2023. to temporal languages [36] Y. Chen, J. Arkin, Y. Zhang, N. Roy, and C. Fan, Autotamp: Autoregressive task and motion planning with llms as translators and checkers, arXiv preprint arXiv:2306.06531, 2023. [37] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, Progprompt: Generating situated robot task plans using large language models, in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 11 52311 530. [38] S. H. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, Chatgpt for robotics: Design principles and model abilities, IEEE Access, 2024. K. Hausman, and B. Ichter, Inner monologue: Embodied reasoning through planning with language models, in arXiv preprint arXiv:2207.05608, 2022. [40] M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al., Simple openvocabulary object detection, in European Conference on Computer Vision. Springer, 2022, pp. 728755. [41] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, et al., Grounding dino: Marrying dino with grounded pre-training for openset object detection, arXiv preprint arXiv:2303.05499, 2023. [42] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.- Y. Lo, et al., Segment anything, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 40154026. [43] H. Huang, F. Lin, Y. Hu, S. Wang, and Y. Gao, Copa: General robotic manipulation through spatial constraints of parts with foundation models, arXiv preprint arXiv:2403.08248, 2024. [44] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, Rt-1: Robotics transformer for real-world control at scale, in arXiv preprint arXiv:2212.06817, 2022. [45] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, P. Florence, C. Fu, M. G. Arenas, K. Gopalakrishnan, K. Han, K. Hausman, A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T.-W. E. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao, K. Reymann, M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint arXiv:2307.15818, 2023. [46] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan, et al., Open x-embodiment: Robotic learning datasets and rt-x models, arXiv preprint arXiv:2310.08864, 2023. [39] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, [47] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, et al., Droid: large-scale inthe-wild robot manipulation dataset, arXiv preprint arXiv:2403.12945, 2024. [48] J. Yang, H. Zhang, F. Li, X. Zou, C. Li, and J. Gao, Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v, arXiv preprint arXiv:2310.11441, 2023. [49] M. Cai, H. Liu, S. K. Mustikovela, G. P. Meyer, Y. Chai, D. Park, and Y. J. Lee, Making large multimodal models understand arbitrary visual prompts, arXiv preprint arXiv:2312.00784, 2023. [50] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: visual language model for few-shot learning, Advances in neural information processing systems, vol. 35, pp. 23 71623 736, 2022. [51] A. Bar, Y. Gandelsman, T. Darrell, A. Globerson, and A. Efros, Visual prompting via image inpainting, Advances in Neural Information Processing Systems, vol. 35, pp. 25 00525 017, 2022. [52] Y. Yang, H. Peng, Y. Shen, Y. Yang, H. Hu, L. Qiu, H. Koike, et al., Imagebrush: Learning visual incontext instructions for exemplar-based image manipulation, Advances in Neural Information Processing Systems, vol. 36, 2024. [53] F. Li, Q. Jiang, H. Zhang, T. Ren, S. Liu, X. Zou, H. Xu, H. Li, J. Yang, C. Li, et al., Visual in-context prompting, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 12 86112 871. [54] Y. Zhang, K. Zhou, and Z. Liu, What makes good examples for visual in-context learning? Advances in Neural Information Processing Systems, vol. 36, pp. 17 77317 794, 2023. [55] L. Zha, Y. Cui, L.-H. Lin, M. Kwon, M. G. Arenas, A. Zeng, F. Xia, and D. Sadigh, Distilling and retrieving generalizable knowledge for robot manipulation via language corrections, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 15 17215 179. [56] G. Williams, A. Aldrich, and E. Theodorou, Model predictive path integral control using covariance preprint variable arXiv:1509.01149, 2015. importance sampling, arXiv"
        }
    ],
    "affiliations": [
        "Columbia University",
        "Tsinghua University",
        "University of Illinois Urbana-Champaign"
    ]
}