{
    "paper_title": "Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations",
    "authors": [
        "Dahye Kim",
        "Deepti Ghadiyaram"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lack scalability, and/or compromise generation quality. In this work, we propose a novel framework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable concept manipulation in diffusion models. Specifically, we first identify interpretable monosemantic concepts in the latent space of text embeddings and leverage them to precisely steer the generation away or towards a given concept (e.g., nudity) or to introduce a new concept (e.g., photographic style). Through extensive experiments, we demonstrate that our approach is very simple, requires no retraining of the base model nor LoRA adapters, does not compromise the generation quality, and is robust to adversarial prompt manipulations. Our method yields an improvement of $\\mathbf{20.01\\%}$ in unsafe concept removal, is effective in style manipulation, and is $\\mathbf{\\sim5}$x faster than current state-of-the-art."
        },
        {
            "title": "Start",
            "content": "Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Dahye Kim 1 Deepti Ghadiyaram 1 2 5 2 0 2 1 3 ] . [ 1 6 6 0 9 1 . 1 0 5 2 : r Abstract Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lack scalability, and/or compromise generation quality. In this work, we propose novel framework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable concept manipulation in diffusion models. Specifically, we first identify interpretable monosemantic concepts in the latent space of text embeddings and leverage them to precisely steer the generation away or towards given concept (e.g., nudity) or to introduce new concept (e.g., photographic style). Through extensive experiments, we demonstrate that our approach is very simple, requires no retraining of the base model nor LoRA adapters, does not compromise the generation quality, and is robust to adversarial prompt manipulations. Our method yields an improvement of 20.01% in unsafe concept removal, is effective in style manipulation, and is 5x faster than current state-of-the-art. 1. Introduction Text-to-image (T2I) generative models have revolutionized content generation by producing diverse and highly photorealistic images, enabling wide range of applications such as digital art creation (Mazzone & Elgammal, 2019), image editing (Brooks et al., 2023), and medical imaging (Kazerouni et al., 2023). These models are usually trained on several billions of web-scraped image and text pairs presumably capturing broad spectrum of semantic concepts. Consequently, these models are also prone to be exposed to and thus generate disturbing content containing nudity, violence, child exploitation, and self-harm raising serious 1Department of Computer Science, Boston University 2Runway. Correspondence to: Deepti Ghadiyaram <dghadiya@bu.edu>. Figure 1. Monosemantic interpretable concepts such as nudity, photographic styles, and object attributes are identified using ksparse autoencoders (k-SAE). We leverage them to enable precise modification of desired concept during the generation process, without impacting the overall image structure, photo-realism, visual quality, and prompt alignment (for safe concepts). Our framework can be used to remove unsafe concepts (top row), photographic styles (middle row), and object attributes (last row). ethical concerns about their downstream applications. Several attempts have been made to enforce safe generations in the past: integrating safety filters as part of the generation pipeline (Rando et al., 2022), guiding the generation process away from pre-defined unsafe latent space (Schramowski et al., 2023), or directly erasing inappropriate concepts by modifying model weights (Gandikota et al., 2023; Heng & Soh, 2024; Li et al., 2024). While partially successful, some of these methods involve model training which is not only computationally expensive but also alters the overall model generative capabilities. More recently, few inference-based approaches have been proposed, which do not alter model weights (Yoon et al., 2024; Jain et al., 2024). SAFREE (Yoon et al., 2024) alters the semantics of the input prompt by filtering toxic tokens, while TraSCE (Jain et al., 2024) modifies negative prompting with gradient computation to guide the model towards safer outputs. Crucially, sometimes these models have the undesirable consequence 1 Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations of visual degraded output generations or being misaligned with input prompts, even when the prompts are benign. Additionally, the increased inference time (e.g., 8.84s overhead per image as noted in TraSCE (Jain et al., 2024)) due to online filtering makes them difficult to deploy in practice. In this work, we posit that the semantic information is interwoven across different layers of generative model in complex ways that is not fully understood. Subsequently, existing training or inference-based safe generation techniques could be altering this latent landscape in undesirable ways leading to misaligned or irrelevant outputs. To this end, we approach the generation process from the ground up and explore the following crucial question: can we systematically isolate monosemantic1 concepts of varied granularities (finegrained and abstract) from the generative latent space and surgically manipulate only them? Having such tool would be invaluable as it would allow the user to intentionally control just the relevant concept of interest without disrupting the overall latent landscape. To this end, we leverage k-sparse autoencoders (kSAE) (Makhzani & Frey, 2013) to design controllable generative models. k-SAEs have shown promising progress in interpreting language models by learning sparse dictionary of monosemantic concepts (Bricken et al., 2023; Cunningham et al., 2023). In our work, we first train k-SAE on the embeddings extracted from corpus of text prompts containing semantic concepts we wish to control (e.g., unsafe concepts). Once trained, each k-SAEs hidden state corresponds to an isolated monosemantic concept. During the generation process, given concept we wish to steer, we use k-SAE to identify its corresponding latent direction and precisely manipulate the presence of that concept in the outcome, without impacting the overall generation capability  (Fig. 1)  . Notably, our method does not require any fine-tuning as in Zhang et al. (2024), synthetic data generation as in Esposito et al. (2023), training separate LoRA adapter (Hu et al., 2021) for each concept as in Gandikota et al. (2025) to manipulate making it fast, efficient, and adaptable to any pre-trained text to image generative framework. We summarize our empirical findings and key contributions below: We identify interpretable monosemantic concepts in text-to-image generation latent landscape using k-sparse autoencoder. Once trained, k-SAE serves as Concept Steerer to provide precise control over specific visual concepts (e.g., nudity, violence, etc.) Concept Steerer achieves state-of-the-art performance on unsafe concept removal while being 5x faster than the existing best method, without compromising visual quality. 1In contrast to the one-to-many mapping of polysemantic neurons, monosemantic neurons form one-to-one correlation with their related input features (Yan et al., 2024). Concept Steerer effectively manipulates photographic and artistic styles, object attributes, enabling controlled yet creative image generation. Concept Steerer is robust to adversarial prompt manipulations, achieving 20.01% improvement against red-teaming tools, ensuring reliable image generation even under challenging scenarios. Concept Steerer works out-of-the-box to any textto-image model, is extremely simple, requires no retraining nor LoRA adapters, and is highly efficient. 2. Related Work Controlling diffusion models: Wu et al. (2023); Wallace et al. (2024) fine-tune diffusion models using human feedback and Bansal et al. (2023); Singhal et al. (2025) propose inference-time diffusion steering with reward functions. However, these methods rely on strong reward functions, and are computationally intensive (Uehara et al., 2025). Some methods achieve controllability by training additional modules such as low-rank adapters (LoRAs) (Gandikota et al., 2025; Stracke et al., 2025), which requires millions of parameters per concept and significantly increases generation time (Sridhar & Vasconcelos, 2024). Several inferencetime intervention works attempt fine-grained control at test time. However, estimating noise at each step for each concept during generation (Brack et al., 2022; 2023) significantly slows down generation and steering model activations based on optimal transport (Rodriguez et al., 2024) requires learning activation mapping for each style. By contrast, our approach is very simple, requires no training of the base model or LoRA adapters, no additional noise/gradient computation during the generation process. Moreover, once trained, our approach allows us to manipulate any concept we want without further tuning. Safe generation: Given the growing concerns of generative models capability to produce inappropriate content, several valuable research has emerged in this space. Some training-based methods (Gandikota et al., 2023; Zhang et al., 2024) directly remove inappropriate concepts from the diffusion model through additional fine-tuning, while some others like (Gandikota et al., 2024; Gong et al., 2025) update model weights to erase concepts without retraining the model. Some recently proposed inference-based approaches (Yoon et al., 2024; Jain et al., 2024) do not require training or weight updates. While effective, these methods often result in degraded image quality and increased inference time. Unlike all prior works, our method surgically isolates interpretable concepts in the generative latent space and manipulating only these in the text encoder. Thus, our approach enjoys the benefit of precise control of inappropriate concepts, does not compromise on generation quality, and maintains prompt-image alignment. Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations age at timestep is yt = (cid:112)(1 βt)y0 + βtϵ where ϵ is the Gaussian noise and βt is timestep-dependent hyper parameter. In the reverse process, the diffusion model ϵθ iteratively denoises yt at each timestep, conditioned on the text prompt embedding c, to predict noise ϵ. The objective function for training the model is to minimize the error between the introduced and the predicted noise, defined as: Ey,t,ϵN (0,1) (cid:2)ϵ ϵθ(yt, c, t)2 (cid:3) Figure 2. K-sparse autoencoder (k-SAE) is trained on feature representations from the text encoder of the diffusion model. Once trained, it serves as Concept Steerer, enabling precise, surgical concept manipulation by adjusting λ. Interpreting diffusion models: Recent works have demonstrated that sparse autoencoders (SAE) could recover interpretable features in large language models (Bricken et al., 2023; Cunningham et al., 2023), CLIP vision features (Fry, 2024; Daujotas, 2024) and diffusion features (Kim et al., 2024; Surkov et al., 2024). Kim et al. (2024) reveal monosemantic interpretable features represented within rich visual features of the diffusion model while Surkov et al. (2024) investigate how text information is integrated via crossattention. By contrast, we focus on the text encoder of diffusion model, identify interpretable directions via k-SAEs, and demonstrate precise steering of variety of concepts. 3. Approach We propose simple yet effective technique to precisely isolate and steer semantic concepts such as nudity or photographic styles using k-sparse autoencoders (Makhzani & Frey, 2013) (k-SAE). We first present how we train such k-SAE (Sec. 3.2), followed by our method to combine different monosemantic neurons to steer abstract concepts (Sec. 3.3). We stress that k-SAE is trained only once and no training is required for any concept the user wishes to introduce, eliminate, or modulate. 3.1. Preliminaries on text to image models Text-to-image diffusion models (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022) primarily consist of text encoder to extract text prompts intermediate embedding and diffusion model. During training, the diffusion model progressively denoises noisy image (or its latent representation) conditioned on the text prompts intermediate embedding. Formally, given an input y0, the forward diffusion process progressively adds noise to y0 over timesteps. The intermediate noisy im3 3.2. Preliminaries on k-sparse autoencoders Sparse autoencoders (Ng et al., 2011) are neural networks designed for learning compact and meaningful feature representations in an unsupervised manner. They consist of an encoder and decoder, optimized jointly using reconstruction loss and sparsity regularization term to encourage only few neurons to be maximally activated for given input. However, the sparsity constraint introduces significant challenges during optimization (Tibshirani, 1996; Makhzani & Frey, 2013). To mitigate these issues, k-sparse autoencoders (k-SAEs) (Makhzani & Frey, 2013) were introduced. They explicitly control the number of active neurons to during training by applying Top-k activation function at each training step. Consequently, this retains only the highest activations and zeroes out the rest. Let Wenc Rnd and Wdec Rdn represent the weight matrices of the k-SAEs encoder and decoder respectively  (Fig. 2)  . The hidden layer dimension is defined as an integer multiple of the input feature dimension d. The ratio n/d, referred to as the expansion factor, controls the extent to which the hidden dimension is expanded relative to the input dimension. Following Bricken et al. (2023), bpre Rd denotes the bias term added to input before feeding to the encoder (aka pre-encoder bias), while benc Rn denotes the bias term of the encoder. Let RLd denote the intermediate representation of the text encoder for an input prompt in text-to-image model, where denotes the number of tokens. The encoded latent is computed as: = ENC(x) = Top-k(ReLU(Wenc(x bpre) + benc)), (1) where the Top-k function retains only the top neuron activations and sets the remaining activations to zero (Makhzani & Frey, 2013). The decoder reconstructs ˆx as: ˆx = DEC(x) = Wdecz + bpre, (2) The training objective of standard k-SAE is to minimize the normalized mean squared error (MSE) between the original feature and the reconstructed feature ˆx, denoted by Lmse. However, both SAEs and k-SAEs suffer from the presence of dead latents, where large proportion of latents stop activating entirely at some point in training. Presence of dead latents decreases the likelihood of the network discovering separable, interpretable features while incurring Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations unnecessary computational cost (Bricken et al., 2023). To discourage dead latents, we incorporate an auxiliary MSE loss as suggested in Gao et al. (2024). Specifically, in every training step, we identify top kaux dead latents and reconstruct latent ˆz exclusively from them, as defined below: ˆz = Top-kaux(ReLU(Wenc(x bpre) + benc)), (3) Now, let ˆe = Wdec ˆz represent the reconstruction using the top kaux dead latents. Laux is defined as reconstruction loss between the auto encoders residual and the output from the dead neurons (ˆe). As discussed in Gao et al. (2024), the intuition behind Laux is to compute gradients that push the parameters of the dead neurons in the direction of explaining the autoencoder residual (e). Thus, the total training loss is: 2025). These settings were found via ablation studies on downstream tasks and/or chosen based on overall training stability and sparsity. We apply unit normalization constraint (Sharkey et al., 2023) on the decoder weights Wdec of the k-SAE after each update. Although our method can be applied in an out-of-the-box manner to any text-to-image model, for fair comparison with existing methods, we conduct experiments using SD 1.4 for unsafe concept removal and then expand our evaluation to more recent SDXL-Turbo and FLUX.1-dev. When steering concepts, we use slightly modified encoder of k-SAE without the TopK activation function, as the TopK function clamps some possible important information that is critical for both maintaining visual quality and effective concept removal. More details in Appendix A. = Lmse + αLaux = ˆx2 2 + αe ˆe2 2, (4) 4.1. Steering towards safety The scalar α is weighting factor that controls the relative contribution of the auxiliary loss. 3.3. Concept Steerers Given human-interpretable concept 2 we wish to steer, we first extract its text embedding xC, pass it through kSAE, and finally perform an element-wise addition with the input prompt embedding x. This can be expressed as: xsteered = + Wdec(λ ENC(xC)), (5) where λ denotes scalar that controls the steering strength. The steered vector xsteered is used to condition the generation process. As we show in Sec. 4, our approach requires kSAE to be trained only once, and provides model-agnostic, fine-grained control over concept steering without degrading the overall generation quality. 4. Experiments We first share the training setup followed by numerous results and ablations on concept steering. Implementation details: We train k-sparse autoencoders on text embeddings with kaux = 256, and loss weight parameter α = 1/32 for 10k training steps. We train for total training tokens of 400M on batch size of 4096 with the learning rate 0.0004 using Adam (Kingma, 2014) optimizer. The k-SAE is trained with = 32 and an expansion factor of 4, resulting in total hidden size dimension = 3072 for Stable Diffusion (SD) 1.4 (Rombach et al., 2022) in the unsafe removal task. For style manipulation, we use = 64 with an expansion factor of 64, resulting in total hidden size dimension = 49152 for SD1.4 and = 64 with an expansion factor of 16, resulting in total hidden size dimension = 32768 for SDXL-Turbo (Sauer et al., 2Defined by any user-provided prompt, e.g., nudity. Setup: First, we demonstrate the effectiveness of erasing unsafe concepts using k-SAEs. We use the Inappropriate Image Prompts (I2P) dataset (Schramowski et al., 2023) to evaluate our method on steering nudity concepts and the Ring-A-Bell benchmark (Tsai et al., 2023) to assess steering performance on violent content. We train k-SAE using features extracted from the residual stream of the 10th layer (out of 12 layers) of the text encoder in SD 1.4, on the prompts in I2P dataset, and evaluate its performance on both the I2P and Ring-A-Bell datasets. As in prior works (Tsai et al., 2023; Jain et al., 2024), we use nudity concept Cnudity as naked, nude, bare, exposed, stripped, topless, male genitalia, penis, buttocks and use slightly modified version for the violent concept Cviolence as violence, blood. We set steering strength λ = 0.5 for I2P dataset and λ = 0.7 for adversarial datasets including violent concept. Evaluation metrics: To quantify the impact of our method on generation quality, we use FID (Heusel et al., 2017) and CLIP score (Hessel et al., 2021; Radford et al., 2021) on the COCO-30k dataset, evaluating 10k generated samples. We report Attack Success Rate (ASR), i.e., the percentage of generated images containing nudity or violence as measure of how well model reduces unsafe content generation. To this end, we use the NudeNet (Bedapudi, 2019) with threshold of 0.45 and Q16 violence detector (Schramowski et al., 2022), following prior work (Jain et al., 2024). Baselines: We compare our method against inference-based approaches that do not require training or weight updates to the generative model, including SLD (Schramowski et al., 2023), SD with negative prompt (SD-NP), SAFREE (Yoon et al., 2024), and TraSCE (Jain et al., 2024). Additionally, we evaluate our method against training-based approaches, including ESD (Gandikota et al., 2023), FMN (Zhang et al., 2024), CA (Kumari et al., 2023), MACE (Lu et al., 2024), and SA (Heng & Soh, 2024), as well as approaches that 4 Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Table 1. Performance comparison across different methods on I2P and COCO datasets. Lower ASR and FID indicate better performance; higher is better for CLIP. Our method achieves the lowest ASR by effectively removing nudity while preserving visual quality and prompt alignment. Bold: best. Underline: second-best. Gray : require training and weight updates, Pink : do not require training but update model weights, Blue : do not require either. METHOD SDV1. ESD (GANDIKOTA ET AL., 2023) CA (KUMARI ET AL., 2023) MACE (LU ET AL., 2024) SA (HENG & SOH, 2024) UCE (GANDIKOTA ET AL., 2024) RECE (GONG ET AL., 2025) SLD-MAX (SCHRAMOWSKI ET AL., 2023) SLD-STRONG (SCHRAMOWSKI ET AL., 2023) SLD-MEDIUM (SCHRAMOWSKI ET AL., 2023) SD-NP SAFREE (YOON ET AL., 2024) TRASCE (JAIN ET AL., 2024) OURS (W/O NEGATIVE STEERING) OURS I2P COCO ASR FID CLIP 17.80 2.87 1.04 1.51 2.81 0.87 0.72 1.74 2.28 3.95 0.74 1.45 0. 0.57 0.36 16.71 18.18 24.12 16.80 25.80 17.99 17.74 28.75 24.40 21.17 18.33 19.32 17.41 18.37 18. 31.3 30.2 30.1 28.7 29.7 30.2 30.2 28.4 29.1 29.8 30.1 30.1 29.9 30.8 30.8 Figure 4. Qualitative examples from the I2P dataset. Our method allows fine-grained control over the removal of specific concepts, removing only the intended concept while preserving the overall structure and style of the generated images. Figure 5. Qualitative example from the I2P dataset with FLUX. Our method is model-agnostic and can be applied to both U-Netbased SD 1.4 and SDXL-Turbo, as well as DiT-based FLUX. Figure 3. Qualitative comparisons of different approaches, including TraSCE and SAFREE, on the I2P dataset. Our method removes nudity without significantly altering the generated images, resulting in outputs that are better aligned with the input prompt. require no training but involve weight updates, such as UCE (Gandikota et al., 2024) and RECE (Gong et al., 2025). We also try variant of our model, where we steer in the opposite direction of the layer activation corresponding to the null text used for classifier-free guidance (Ho & Salimans, 2022), which we refer to as negative steering. 4.1.1. STEERING NUDITY CONCEPT As shown in Table 1, our approach achieves state-of-the-art performance in steering unsafe concepts, yielding the lowest ASR (0.36) on the I2P dataset and surpassing the previous best method. We note that incorporating negative steering slightly improves performance, demonstrating that our concept vector effectively models abstract concepts and, similar to negative prompting, yields slight improvement in performance. Notably, our approach even outperforms both training-based methods (Gandikota et al., 2023; Kumari et al., 2023; Lu et al., 2024; Heng & Soh, 2024) and weightFigure 6. Qualitative examples from the Ring-A-Bell dataset. Our method successfully removes the abstract concept of violence, as shown by the absence of blood in the right images. The images are intentionally blurred for display purposes as they are disturbing. updating methods (Gandikota et al., 2024; Gong et al., 2025), underscoring the effectiveness of our method. Furthermore, our method achieved the highest prompt-image correspondence, as indicated by the CLIP score on the COCO dataset (30.8), ranking just below the original SD 1.4 model (31.3). This is demonstrated in Fig. 3 and Fig. 4, where previous methods sometimes generate unrelated images when the prompt triggers unsafe content. By contrast, our method successfully removes nudity while preserving the overall structure and maintaining alignment with the input prompt. Moreover, as shown in Fig. 5, we demonstrate that our method can also steer the DiT-based (Peebles & Xie, 2023) FLUX (Labs, 2023) model in an out-of-the-box manner. 4.1.2. STEERING VIOLENCE CONCEPT We also evaluate our methods performance in suppressing violent content generation, as presented in Table 2. As shown in Fig. 6, our method effectively reduces the generation of violent content compared to existing training-based and weight-update-based methods. Although SLD-Max Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Table 2. Performance comparison across different methods on the Ring-A-Bell-Union (Violence) dataset. Lower values indicate better performance. Our method demonstrates competitive performance without compromising generation quality, as indicated by the FID scores in Table 1. Bold: best. Underline: second-best. Gray : require training and weight updates, Pink : do not require training but update model weights, Blue : do not require either. METHOD SDV1.4 ESD (GANDIKOTA ET AL., 2023) FNM (ZHANG ET AL., 2024) CA (KUMARI ET AL., 2023) UCE (GANDIKOTA ET AL., 2024) RECE (GONG ET AL., 2025) SLD-MAX (SCHRAMOWSKI ET AL., 2023) SLD-STRONG (SCHRAMOWSKI ET AL., 2023) SLD-MEDIUM (SCHRAMOWSKI ET AL., 2023) SD-NP TRASCE (JAIN ET AL., 2024) OURS RING-A-BELL-UNION (VIOLENCE) 99.6 86.0 98.8 100.0 89.8 89.2 40.4 80.4 97.2 94.8 72.4 43. Figure 7. Photographic style manipulation of SD 1.4 for the given prompt geodesic landscape, john chamberlain, christopher balaskas, tadao ando, 4 k, where concept prompts are minimalist (Top) and zoom-in, magnify (Bottom), respectively. In the top row, the image is manipulated toward maximalist style as λ 1, while it adopts minimalist style as λ 1. Similarly, in the bottom row, the image appears zoomed out and becomes blurred as λ 1, whereas it becomes zoomed in and clearer as λ 1. achieves slightly better performance than ours, it significantly degrades overall image quality, yielding an FID of 28.75 compared to 18.67 for our approach  (Table 1)  . 4.2. Steering of photographic styles and object attributes Setup: In this section, we demonstrate the effectiveness of steering photographic styles and object attributes. We train k-SAE using features extracted from the residual stream of the 11th (out of 12) layer of the text encoder in SD 1.4. To observe the effect of photographic style changes, we designed dataset dedicated to 40 photographic styles, including black-and-white, HDR, minimalist, etc. For each class, we generated 100 prompts, totaling around 4000 prompts, by querying ChatGPT. We also experiment with SDXL-Turbo, where we train using features from both of its text encoders:11th (out of 12) and 29th (out of 32) layers with prompts from I2P dataset. As shown in Fig. 7 and Fig. 8 we can adjust its photographic 6 Figure 8. Qualitative comparisons with weather Concept Sliders on SDXL-Turbo. Note that Concept Sliders train specific sliders: winter weather slider and dark weather slider, whereas our method trains k-SAE only once for different concepts. Top: photo of tree with bench, realistic, 8k with concept to steer = winter. Bottom: photo of forest, realistic, 8k with the concept to steer = low light. Notice how in the top image our method also removes leaves while in the bottom image, our method effectively applies low-light effect to the original image. Figure 9. Image composition manipulation using SDXL-Turbo for the prompt dog with the concept prompt Full shot. Notice how as λ 1, the generated image transitions from close-up of the face to full shot. style, including zoom-in and minimalist. In Fig. 8, we compare our results with Concept Sliders (Gandikota et al., 2025) on SDXL-Turbo where Concept Sliders train separate models for each weather condition style. Remarkably, our method can effectively steer concepts like weather conditions and photographic styles. We note that I2P dataset in addition to the semantic concepts such as nudity and violence, also had descriptors about general photographic styles such as full shot or seasons winter. We believe that k-SAE internalized these concepts offering us powerful tool to surgically steer them. This powerful result highlights the generalizable capability of k-SAEs to learn diverse monosemantic concepts. This is corroborated by our results in Fig. 9, where we show that our method can manipulate image compositions, changing close-up image of dog into full shot of dog while preserving the appearance of its head part. Finally, in Fig. 10, we use the same k-SAE to effectively manipulates object attributes. Here, we inject concept for an object present in the image, such as blue [object] or tree Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Figure 10. Object attribute manipulation of SDXL-Turbo for the given prompts car (Top) and photo of tree (Bottom), where the concept prompts are blue car (Top) and Tree with autumn leaves (Bottom). By adjusting λ, our method transitions the image toward the desired concept specified by the prompts. with autumn leaves. We note that the resulting generations preserve most of the original content while successfully injecting the desired concept. These results demonstrate the universal applicability of k-SAE without the need to train separate adapters for each concept. We wish to continue exploring the limits of universality of k-SAEs in the future. 4.3. Robustness to adversarial prompt manipulation Next, we demonstrate the robustness of our method against adversarial prompts on four datasets: redteaming approaches like Ring-A-Bell (Tsai et al., 2023), P4D (Chin et al., 2023), and attack frameworks like MMADiffusion (Yang et al., 2024) and UnlearnDiffAtk (Zhang et al., 2025). Adversarial prompts often consist of several non-English phrases or nonsensical text fragments that lack semantic meaning, but fool the underlying generative models to produce unsafe content. We follow the same setup in Sec. 4.1 and use k-SAE trained on I2P prompts. As shown in Table 3, our method achieves the best overall robustness on average across all datasets, significantly outperforming the most recent works TraSCE (Jain et al., 2024) by 1.23% and SAFREE (Yoon et al., 2024) by 20.01%. Specifically, for the MMA-Diffusion and P4D datasets, our method achieves state-of-the-art results with improvements of 10.60% and 1.98%, respectively. This demonstrates that our method performs very well and can implicitly identify monosemantic interpretable directions for nudity within the latent space of adversarial prompts. Notably, our method outperforms RECE (Gong et al., 2025) specifically designed for tackling adversarial prompts by 4.48%. For other datasets, our method ranks second-best or performs comparably to the best scores. We note that k-SAE is trained on text embeddings from I2P prompts to learn unsafe concepts and is not exposed to adversarial datasets. Remarkable performance in adversarial datasets demonstrates k-SAE generalizes well to unseen prompts, even without exposure to prompt embeddings from different distributions, similar observation to Sec. 4.2. We reiterate that once k-SAE is trained on unsafe concepts, our method does not require Figure 11. Effect of steering strength parameter (λ) on the I2P dataset while we steer nudity. Notice how as λ 0.5, the presence of nudity disappears completely. retraining. 4.4. Efficiency of Concept Steerer As shown in Table 4, our method achieves the fastest inference time among all other inference-based approaches, with only 0.14 sec./sample overhead on single L40S GPU compared to the original SD 1.4. We highlight that our method is approximately 5x faster than the previous stateof-the-art (Jain et al., 2024) in unsafe concept removal. 4.5. Ablation Studies Finally, we analyze the impact of our design choices on the overall steering capacity and visual quality. Effect of Concept Steering on Visual Quality: To evaluate the impact of our approach on visual quality, we conduct user study using 50 randomly selected safe images generated by the original SD 1.4 model and nudity-steered images produced by applying our method on SD 1.4. We followed the setup described in Sec 4.1. The study involved 22 participants, who were shown images in randomized order and were asked to select the image they preferred most based purely on overall visual quality. 44.7% of users preferred images produced by concept steering, while 44.9% preferred images from SD 1.4, indicating that participants expressed an almost equal preference for both generations. This is crucial finding because it shows that our method does not deteriorate visual quality from the base model but offers the additional benefit of controllability. Effect of Layer Selection on Steering: We examine how the selection of different layers in the text encoder impacts the semantic information captured in k-SAE and thereby concept steering. As shown in Table 5, representations from later layers are more effective to remove nudity and steering than earlier layers. We believe that earlier layers capture more low-level semantic information, thus highlevel concepts such as nudity are better captured in the later layers, making them suitable candidates for steering. Similar observations were reported in Toker et al. (2024). Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Table 3. Attack Success Rate (ASR) of different methods on various adversarial attack datasets. Lower ASR indicates better performance. Our method achieves the best overall robustness on average across all datasets by effectively removing nudity implicitly embedded in the model. Bold: best. Underline: second-best. Gray : require training and weight updates, Pink : do not require training but update model weights, Blue : do not require either. METHOD SDV1. SA (HENG & SOH, 2024) CA (KUMARI ET AL., 2023) ESD (GANDIKOTA ET AL., 2023) MACE (LU ET AL., 2024) UCE (GANDIKOTA ET AL., 2024) RECE (GONG ET AL., 2025) SLD-MAX (SCHRAMOWSKI ET AL., 2023) SLD-STRONG (SCHRAMOWSKI ET AL., 2023) SLD-MEDIUM (SCHRAMOWSKI ET AL., 2023) SD-NP SAFREE (YOON ET AL., 2024) TRASCE (JAIN ET AL., 2024) RING-A-BELL K77 K16 AVG 85.26 87.37 93.68 88. 63.15 86.32 20.00 2.10 10.52 5.26 23.16 56.84 92.63 17.89 35.78 1.05 56.84 91.69 29.47 0.00 9.47 4.21 32.63 64.21 88.42 40.42 47.36 2. 56.84 94.26 35.79 0.00 12.61 5.26 42.11 61.05 91.05 34.74 55.78 2.10 58.94 90.76 28.42 0.70 10.87 4.91 32.63 60.70 90.70 31.68 46.31 1. OURS 3.16 8.42 9.47 7.02 MMA-DIFFUSION P4D UNLEARNDIFFATK AVG 95.70 47.68 10.60 9.27 2.72 29.93 21.77 35.76 68.21 68.21 24.00 40.82 16.60 6. 98.70 12.68 5.63 15.49 2.82 9.86 5.63 9.14 33.10 24.00 10.00 10.56 3.97 1.99 69. 2.81 1.04 2.87 1.51 0.87 0.72 2.44 3.10 1.98 1.46 1.45 0.70 2.11 87.05 30.53 27.01 14.51 1. 12.38 8.76 20.24 41.28 46.72 16.29 24.29 5.51 4.28 Table 4. Model Efficiency Comparison. Experiments were conducted on single L40S GPU on P4D dataset (150 samples in total) for the task of removing nudity. METHOD SD 1. SAFREE (YOON ET AL., 2024) TRASCE (JAIN ET AL., 2024) OURS INFERENCE TIME (S/SAMPLE) 3.02 4.24 15.62 3.16 Table 5. Attack Success Rate (ASR) when representations from different encoder layers are used to train k-SAE on the I2P dataset. The 10th layer yields the lowest ASR, indicating that this layer captures most information about nudity concept. k-SAE expansion factor = 4, hidden neurons (n) = 3072. LAYERS ASR ON I2P 12 10 8 6 4 1.02 0.36 0.45 1.72 3.85 Table 6. Attack Success Rate (ASR) for different expansion factors of k-SAE trained on text embeddings extracted from the 10th layer of the I2P prompts. An expansion factor of 4 yields the lowest ASR, indicating its efficacy for steering. EXPANSION FACTOR CAPACITY ASR ON I2P 4 8 16 32 64 3072 6144 12288 24576 0.36 0.51 0.47 0.49 0.53 Table 7. Attack Success Rate (ASR) for different values of λ of k-SAE with an expansion factor of 4 trained on text embeddings of 10th layer on the I2P dataset. λ = 0.5 yields the lowest ASR. λ ASR ON I2P 0.1 0.2 0.3 0.4 0.5 2.59 1.23 0.87 0.60 0. Effect of k-SAE capacity on steering: We investigate the effect of k-SAE capacity determined by different expansion factors on steering results. From Table 6, we note that the performance differences between capacities is relatively minor, using an expansion factor of 4 proves to be the most effective in removing nudity. Effect of steering strength λ: Finally, we investigate the effect of the steering strength, λ. Table 7 illustrates the impact of λ, showing that decreasing its value enables more effective removal of nudity from greater number of images. As shown in the first and second rows of Fig. 11, setting λ = 0.1 effectively removes the nudity concept in most images. However, smaller λ values lead to more complete removal, as demonstrated in the last row of Fig. 11. 5. Discussion and Future Work We propose novel framework leveraging k-SAEs to enable efficient and interpretable concept manipulation in diffusion models. Once trained, k-SAE serves as Concept Steerer to precisely control specific visual concepts (e.g., nudity, violence, etc.) Our extensive experiments demonstrate that our approach is very simple, does not compromise the generation quality, and is robust to adversarial prompt manipulations. Currently, we steer concepts by extracting representations from the text encoder of the generative models. In future, we wish to explore steering via visual embeddings and allow users more control by selecting regions in an image and locally steer. 8 Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations"
        },
        {
            "title": "Impact Statement",
            "content": "As text-to-image models are increasingly integrated into high-stakes applications, discouraging unsafe generations is of paramount significance. This work presents an effective approach for identifying and suppressing unsafe concept directions across various generative models. By improving the controllability and reliability of generative models, our method advances the development of safer AI systems, facilitating their responsible deployment in real-world applications. Code is available at: https: //github.com/kim-dahye/steerers"
        },
        {
            "title": "References",
            "content": "Bansal, A., Chu, H.-M., Schwarzschild, A., Sengupta, S., Goldblum, M., Geiping, J., and Goldstein, T. Universal guidance for diffusion models. In CVPR, 2023. Bedapudi, P. Nudenet: Neural nets for nudity classification, detection and selective censoring, 2019. Brack, M., Schramowski, P., Friedrich, F., Hintersdorf, D., and Kersting, K. The stable artist: Steering semantics in diffusion latent space. arXiv preprint arXiv:2212.06013, 2022. Brack, M., Friedrich, F., Hintersdorf, D., Struppek, L., Schramowski, P., and Kersting, K. Sega: Instructing text-to-image models using semantic guidance. NeurIPS, 2023. Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., McLean, B., Burke, J. E., Hume, T., Carter, S., Henighan, T., and Olah, C. Towards monosemanticity: Decomposing language models with Transformer Circuits Thread, dictionary learning. 2023. https://transformer-circuits.pub/ 2023/monosemantic-features/index.html. Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. Chin, Z.-Y., Jiang, C.-M., Huang, C.-C., Chen, P.-Y., and Chiu, W.-C. Prompting4debugging: Red-teaming text-toimage diffusion models by finding problematic prompts. arXiv preprint arXiv:2309.06135, 2023. Cunningham, H., Ewart, A., Riggs, L., Huben, R., and Sparse autoencoders find highly interSharkey, L. pretable features in language models. arXiv preprint arXiv:2309.08600, 2023. Daujotas, G. Interpreting and steering features in images. LessWrong, 2024. https://www.lesswrong.com/ posts/Quqekpvx8BGMMcaem/interpretingand-steering-features-in-images. Esposito, P., Atighehchian, P., Germanidis, A., and Ghadiyaram, D. Mitigating stereotypical biases in text to image generative systems. arXiv preprint arXiv:2310.06904, 2023. Fry, H."
        },
        {
            "title": "Towards multimodal",
            "content": "vision"
        },
        {
            "title": "Learning",
            "content": "interpretable sparse transformers. ity: LessWrong, in https://www.lesswrong.com/posts/ bCtbuWraqYTDtuARg/towards-multimodalinterpretability-learning-sparse. interpretabilfeatures 2024. Gandikota, R., Materzynska, J., Fiotto-Kaufman, J., and Bau, D. Erasing concepts from diffusion models. In ICCV, 2023. Gandikota, R., Orgad, H., Belinkov, Y., Materzynska, J., and Bau, D. Unified concept editing in diffusion models. 2024. Gandikota, R., Materzynska, J., Zhou, T., Torralba, A., and Bau, D. Concept sliders: Lora adaptors for precise control in diffusion models. In ECCV, 2025. Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. Gong, C., Chen, K., Wei, Z., Chen, J., and Jiang, Y.-G. Reliable and efficient concept erasure of text-to-image diffusion models. In ECCV, 2025. Heng, A. and Soh, H. Selective amnesia: continual learning approach to forgetting in deep generative models. NeurIPS, 36, 2024. Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi, Y. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 9 Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Jain, A., Kobayashi, Y., Shibuya, T., Takida, Y., Memon, N., Togelius, J., and Mitsufuji, Y. Trasce: Trajectory steering for concept erasure. arXiv preprint arXiv:2412.07658, 2024. Kazerouni, A., Aghdam, E. K., Heidari, M., Azad, R., Fayyaz, M., Hacihaliloglu, I., and Merhof, D. Diffusion models in medical imaging: comprehensive survey. MedIA, 2023. Kim, D., Thomas, X., and Ghadiyaram, D. Revelio: Interpreting and leveraging semantic information in diffusion models. arXiv preprint arXiv:2411.16725, 2024. Kingma, D. P. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kumari, N., Zhang, B., Wang, S.-Y., Shechtman, E., Zhang, R., and Zhu, J.-Y. Ablating concepts in text-to-image diffusion models. In ICCV, 2023. Labs, B. F. Flux. https://github.com/blackforest-labs/flux, 2023. Li, X., Yang, Y., Deng, J., Yan, C., Chen, Y., Ji, X., and Xu, W. Safegen: Mitigating sexually explicit content generation in text-to-image models. arXiv preprint arXiv:2404.06666, 2024. Lu, S., Wang, Z., Li, L., Liu, Y., and Kong, A. W.-K. Mace: In CVPR, Mass concept erasure in diffusion models. 2024. Makhzani, A. and Frey, B. K-sparse autoencoders. arXiv preprint arXiv:1312.5663, 2013. Mazzone, M. and Elgammal, A. Art, creativity, and the potential of artificial intelligence. In Arts, 2019. Ng, A. et al. Sparse autoencoder. CS294A Lecture notes, 2011. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Rando, J., Paleka, D., Lindner, D., Heim, L., and Tram`er, F. Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610, 2022. Rodriguez, P., Blaas, A., Klein, M., Zappella, L., Apostoloff, N., Cuturi, M., and Suau, X. Controlling language and diffusion models by transporting activations. arXiv preprint arXiv:2410.23054, 2024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. In ECCV, 2025. Schramowski, P., Tauchmann, C., and Kersting, K. Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content? In FAccT, 2022. Schramowski, P., Brack, M., Deiseroth, B., and Kersting, K. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In CVPR, 2023. Sharkey, L., Braun, D., and Millidge, B. Taksuperposition with sparse 2023. AI Alignment Forum, ing features out of autoencoders. https://www.alignmentforum.org/posts/ z6QQJbtpkEAX3Aojj/interim-researchreport-taking-features-out-ofsuperposition. Singhal, R., Horvitz, Z., Teehan, R., Ren, M., Yu, Z., McKeown, K., and Ranganath, R. general framework for inference-time scaling and steering of diffusion models. arXiv preprint arXiv:2501.06848, 2025. Sridhar, D. and Vasconcelos, N. Prompt sliders for finegrained control, editing and erasing of concepts in diffusion models. arXiv preprint arXiv:2409.16535, 2024. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 2020. Stracke, N., Baumann, S. A., Susskind, J., Bautista, M. A., and Ommer, B. Ctrloralter: Conditional loradapter for efficient 0-shot control and altering of t2i models. In ECCV, 2025. 10 Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Surkov, V., Wendler, C., Terekhov, M., Deschenaux, J., West, R., and Gulcehre, C. Unpacking sdxl turbo: Interpreting text-to-image models with sparse autoencoders. arXiv preprint arXiv:2410.22366, 2024. Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 1996. Toker, M., Orgad, H., Ventura, M., Arad, D., and Belinkov, Y. Diffusion lens: Interpreting text encoders in text-toimage pipelines. arXiv preprint arXiv:2403.05846, 2024. Tsai, Y.-L., Hsu, C.-Y., Xie, C., Lin, C.-H., Chen, J.-Y., Li, B., Chen, P.-Y., Yu, C.-M., and Huang, C.-Y. Ring-a-bell! how reliable are concept removal methods for diffusion models? arXiv preprint arXiv:2310.10012, 2023. Uehara, M., Zhao, Y., Wang, C., Li, X., Regev, A., Levine, S., and Biancalani, T. Reward-guided controlled generation for inference-time alignment in diffusion models: Tutorial and review. arXiv preprint arXiv:2501.09685, 2025. Wallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Purushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik, N. Diffusion model alignment using direct preference optimization. In CVPR, 2024. Wu, X., Sun, K., Zhu, F., Zhao, R., and Li, H. Human preference score: Better aligning text-to-image models with human preference. In ICCV, 2023. Yan, H., Xiang, Y., Chen, G., Wang, Y., Gui, L., and He, Y. Encourage or inhibit monosemanticity? revisit monosemanticity from feature decorrelation perspective. arXiv preprint arXiv:2406.17969, 2024. Yang, Y., Gao, R., Wang, X., Ho, T.-Y., Xu, N., and Xu, Q. Mma-diffusion: Multimodal attack on diffusion models. In CVPR, 2024. Yoon, J., Yu, S., Patil, V., Yao, H., and Bansal, M. Safree: Training-free and adaptive guard for safe text-to-image and video generation. arXiv preprint arXiv:2410.12761, 2024. Zhang, G., Wang, K., Xu, X., Wang, Z., and Shi, H. Forgetme-not: Learning to forget in text-to-image diffusion models. In CVPR, 2024. Zhang, Y., Jia, J., Chen, X., Chen, A., Zhang, Y., Liu, J., Ding, K., and Liu, S. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. In ECCV, 2025. 11 Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations"
        },
        {
            "title": "Appendix",
            "content": "A. Implementation details Training k-SAE with FLUX: For FLUX.1-dev (Labs, 2023) visualization, we train k-SAE using features extracted from the residual stream of the 23rd (out of 24) layer of the T5-XXL text encoder on prompts from the I2P dataset. The k-SAE is trained with = 64 and an expansion factor of 16, resulting in total hidden size dimension = 65536. Text encoders of diffusion models: We extract text embeddings for k-SAE from CLIP ViT-L/14 (Radford et al., 2021) for SD 1.4, OpenCLIP-ViT/G (Cherti et al., 2023) and CLIP-ViT/L for SDXL-Turbo, and T5-XXL (Raffel et al., 2020) for FLUX.1-dev. B. More details of the benchmarks We evaluate our method for unsafe concept removal tasks on five publicly available inappropriate or adversarial prompts datasets following prior work (Jain et al., 2024): I2P3 (Schramowski et al., 2023), Ring-A-Bell4 (Tsai et al., 2023), P4D5 (Chin et al., 2023), MMA-Diffusion6 (Yang et al., 2024), and UnlearnDiffAtk7 (Zhang et al., 2025). I2P contains 4703 real user prompts that are likely to produce inappropriate images. Ring-A-Bell consists of two inappropriate categories: nudity and violence. For nudity, it contains 95 unsafe prompts for each split (K77, K38, and K16). For violence, we use the Ring-A-Bell Union dataset, which includes 750 prompts. P4D contains 151 unsafe prompts generated by white-box attacks on the ESD (Gandikota et al., 2023) and SLD (Schramowski et al., 2023). MMA-Diffusion contains 1000 strong adversarial prompts generated via black-box attack. UnlearnDiffAtk contains 142 adversarial prompts generated using white-box adversarial attacks. C. Additional qualitative results In this section, we provide additional qualitative results. Steering nudity concept on inappropriate dataset: Figure 12 presents additional qualitative results using FLUX on prompts from I2P dataset. Our method effectively removes the abstract concept of nudity in DiT-based FLUX in an out-of-the-box manner. Steering nudity concept on adversarial dataset: Figure 13 presents qualitative comparisons with different methods on the P4D dataset. Since P4D contains adversarial prompts specifically designed to challenge generative models, previous methods either fail by generating unsafe images or produce unrelated images as defense mechanism when the prompt triggers to generate unsafe content (middle row). In contrast, our method successfully removes nudity while preserving the overall structure and maintaining alignment with the input prompt, even when the prompt itself is nonsensical (first and last row). Steering violent concept: Figure 14 presents qualitative examples on the Ring-A-Bell dataset for violent concept removal. Our method effectively removes the abstract concept of violence by eliminating visual cues such as blood and firearms. Steering photographic styles: Figure 15 presents qualitative examples of photographic style manipulations in SD 1.4, including HDR, Black and White, Sepia Tone, and Astrophotography. We note that as λ 0.5, the generated image gradually transitions to the desired concept. Steering object attributes: Figure 16 presents qualitative examples of object attributes manipulations in SDXL-Turbo. Given prompt, we inject concept for an object present in the image, such as an orange cake and chocolate cake. We note that the resulting generations preserve most of the original content while successfully injecting the desired concept. 3https://huggingface.co/datasets/AIML-TUDA/i2p 4https://huggingface.co/datasets/Chia15/RingABell-Nudity 5https://huggingface.co/datasets/joycenerd/p4d 6https://huggingface.co/datasets/YijunYang280/MMA-Diffusion-NSFW-adv-prompts-benchmark 7https://github.com/OPTML-Group/Diffusion-MU-Attack/blob/main/prompts/nudity.csv 12 Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Figure 12. Qualitative example from the I2P dataset with FLUX. Our method is model-agnostic and can be applied to both U-Net-based SD 1.4 and SDXL-Turbo, as well as DiT-based FLUX. 13 Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Figure 13. Qualitative comparisons of different methods, including TraSCE and SAFREE, on the P4D dataset. The P4D dataset consists of adversarial prompts designed to challenge generative models. Our approach effectively removes the concept of nudity during the generation process, producing safe and semantically meaningful outputs. In contrast, SAFREE fails to generate safe images, while TraSCE sometimes produces unrelated outputs despite the presence of semantically meaningful keywords in given prompts, such as girl, roman, renaissance, and paintings (middle row). Figure 14. Qualitative examples from the Ring-A-Bell dataset. Our method successfully removes the abstract concept of violence, as shown by the absence of blood in the right images. The images are intentionally blurred for display purposes as they are disturbing. 14 Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations Figure 15. Photographic style manipulation of SD 1.4 for the given prompt geodesic landscape, john chamberlain, christopher balaskas, tadao ando, 4 k, where concept prompts are HDR, Black and white, Sepia Tone, and Astrophotography, respectively. As λ 0.5, the generated image gradually transitions to the desired concept. Figure 16. Object attribute manipulation of SDXL-Turbo for the given prompts photo of cake, 4k, where the concept prompts are chocolate cake, white cake, lemon cake, and An orange cake, respectively. By adjusting λ, our method transitions the image toward the desired concept specified by the prompts."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Boston U"
    ]
}