{
    "paper_title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
    "authors": [
        "Anthony Chen",
        "Naomi Ken Korem",
        "Tavi Halperin",
        "Matan Ben Yosef",
        "Urska Jelercic",
        "Ofir Bibi",
        "Or Patashnik",
        "Daniel Cohen-Or"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines."
        },
        {
            "title": "Start",
            "content": "JUST-DUB-IT : Video Dubbing via Joint Audio-Visual Diffusion ANTHONY CHEN, Tel Aviv University, Israel and Lightricks, Israel NAOMI KEN KOREM, Lightricks, Israel TAVI HALPERIN, Lightricks, Israel MATAN BEN YOSEF, Lightricks, Israel URSKA JELERCIC, Lightricks, Israel OFIR BIBI, Lightricks, Israel OR PATASHNIK, Tel Aviv University, Israel DANIEL COHEN-OR, Tel Aviv University, Israel 6 2 0 2 9 2 ] . [ 1 3 4 1 2 2 . 1 0 6 2 : r Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce single-model approach that adapts foundational audio-video diffusion model for video-to-video dubbing via lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines. Webpage available at https://justdubit.github.io. Additional Key Words and Phrases: Audio-Visual generation, Video dubbing"
        },
        {
            "title": "Introduction",
            "content": "Recent years have seen significant progress in generative models for images [Esser et al. 2024; Labs 2024], video [HaCohen et al. 2024; Wan et al. 2025], and audio [Liu et al. 2023, 2024; Qin et al. 2023], enabling high-quality content synthesis and editing. These advances have transformed how visual and audio content is created. In particular, it has become increasingly easy to generate and edit media using intuitive, high-level controls such as natural language. More recently, audio-video generative models have emerged, allowing sound and visual content to be modeled jointly in single generative process [HaCohen et al. 2026; Low et al. 2025]. Video dubbing aims to translate the spoken content of video into different language while preserving the speakers identity, including both facial appearance and voice, and maintaining accurate lip synchronization. Unlike audio-only translation or voice conversion, dubbing requires modifying facial motion in highly localized and precise manner while keeping the rest of the video visually consistent. This makes the task particularly challenging, as even small errors in lip motion, voice characteristics, or identity cues Equal contribution. Work done during visit at Tel Aviv University and internship at Lightricks. Fig. 1. Video dubbing via joint audiovisual generation. Top: an input video with spoken dialogue in the source language. Bottom: the same video dubbed into target language, generated by our trained model built on top of an audiovisual foundation backbone. Translated speech and lip motion are produced jointly, while the visual context (such as scene dynamics, face expressions, and body movements), speaker identity, and non-speech events (e.g., pauses, background sounds) are preserved. are easily perceived, especially in real-world videos with complex motion, pose changes, and varying visual conditions. Most existing video dubbing methods decompose the problem into sequence of specialized stages, typically combining audio synthesis with audio-driven facial editing [Li et al. 2024; Yang et al. 2025; Zhang et al. 2025a]. While effective in controlled settings, these modular pipelines are often complex and brittle, with each component making strong assumptions about the input. In practice, such assumptions can break down in unconstrained videos, for example when the speaker moves relative to the camera, produces non-speech sounds such as laughter or sighs, or partially occludes 2 Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, and Daniel Cohen-Or the mouth during speech. Furthermore, most modular systems attempt to isolate the speakers voice from the background audio to perform translation in vacuum. This separation-and-remixing strategy fails to account for the temporal and semantic dependencies between the speaker and their environment. If the translated speech differs in duration from the originala common occurrence in dubbingsimply re-layering the original background audio leads to loss of synchrony with environmental events, such as dog barking in response to speaker or door slamming at specific moment in conversation. These messy auditory-visual misalignments break the viewers immersion and highlight the limitations of treating speech as detached signal. In short, separating audio and visual editing prevents the two modalities from informing each other, which limits performance in complex, real-world videos. To address these challenges, we take different approach and frame video dubbing as joint audio-video generation problem within single model. Instead of relying on multiple specialized components, we adapt foundational audio-visual diffusion model to perform dubbing directly, allowing audio and visual cues to be generated together and to inform each other in cross-modality attention layers. Crucially, this holistic modeling ensures that the interaction between the speaker and the rest of the scene is preserved. By treating the entire audio-visual stream as single generative task, our model can naturally adjust the timing and placement of environmental sounds to remain coherent with the newly generated speech and facial motion. This joint formulation naturally captures correlations between speech, facial motion, and scene dynamics, while avoiding many of the assumptions and failure modes of modular pipelines. Importantly, the adaptation is lightweight, requiring only small LoRA [Hu et al. 2021] fine-tuning on top of strong generative prior, which makes the approach simple, flexible, and robust in practice. key challenge in adapting generative audio-video model for dubbing is the lack of paired training data that preserves both the speakers identity and the original visual content while allowing the spoken language to change. To address this, we generate training data using the generative model itself. We first generate multilingual videos in which the same speaker switches languages within single clip, ensuring consistent facial appearance and voice characteristics across languages. We then split the clip into two halves and each time inpaint the face and audio in one half to match the language of the other half, while keeping the underlying semantic content intact. Using audio-video inpainting framework allows the model to leverage both visual and audio context, resulting in aligned bilingual video pairs that provide effective supervision for training. We evaluate our approach on diverse set of videos and language pairs, including unconstrained real-world scenarios with complex motion and visual variability. Our method consistently produces high-quality dubbed videos that preserve both facial and voice identity while maintaining accurate lip synchronization and the temporal-semantic coherence of interactions within the scene. Compared to existing dubbing pipelines, our approach is more robust to challenging conditions such as non-frontal views, partial occlusions, and expressive facial behaviors, resulting in improved visual fidelity and overall perceptual quality. These results demonstrate the benefit of leveraging strong joint audio-video generative prior for dubbing."
        },
        {
            "title": "2 Related Work",
            "content": "Audio-Visual Generative Models. The field of multimodal synthesis is shifting from cascaded pipelines to unified Audio-Visual foundation models, using Diffusion Transformers (DiTs) [HaCohen et al. 2026; Low et al. 2025; Peebles and Xie 2022; Zhang et al. 2025c]. High-fidelity generation and modality alignment are achieved through architectures like Ovi [Low et al. 2025] and UniAVGen [Zhang et al. 2025c], which employs twin-backbone designs and Asymmetric Cross-Modal Interaction (ATI). Further frameworks, including Seedance 1.5 pro, MM-Sonate, and Syncphony, enhance alignment via joint denoising, flow-matching, and specialized motionaware normalization [Cheng et al. 2025; Qiang et al. 2026; Song et al. 2025; Team 2025]. Our work builds upon these priors for end-to-end audiovisual dubbing. Audio-driven Talking Face Generation. Audio-driven talking face synthesis evolved from early graphics-based concatenation [Bregler et al. 1997] to deep neural approaches like Wav2Lip, which introduced synchronization discriminators but suffered from texture artifacts [Guan et al. 2023; Prajwal et al. 2020]. Modern approaches can be classified as inpainting-based (e.g., MuseTalk [Zhang et al. 2025a], LatentSync [Li et al. 2024]), which reconstruct masked lip regions but often produce artifacts and discontinuities [Peng et al. 2025; Yaman et al. 2024], and non-inpainting methods like InfiniteTalk, OmniSync, and X-Dub, which avoid explicit masking and directly edit videos using Video DiTs [He et al. 2025; Peng et al. 2025; Yang et al. 2025]. Our method uniquely synthesizes paired cross-lingual dubbing datasets, training unified model that comprehensively preserves both audio and visual contexts of the original content. Zero-shot Voice Cloning. Voice cloning, crucial component in traditional dubbing pipelines, has transitioned from data-intensive methods to zero-shot models leveraging neural codec language models and large language model adaptations [Chen et al. 2025b; Cui et al. 2025b; Ju et al. 2024; Ye et al. 2025]. Approaches like IndexTTS2 and SSPO specifically ensure precise duration alignment [Cui et al. 2025a; Zhou et al. 2025], yet remain unimodal, neglecting vital visual cues essential for accurately rendering paralinguistic elements (e.g., laughter, sighs, breathing). In contrast, our audiovisual generation method leverages visual dynamics to faithfully reproduce these non-verbal expressions and preserve the speakers original vocal identity."
        },
        {
            "title": "3 Method",
            "content": "Our method adapts pretrained audiovisual diffusion model for video dubbing without relying on masks, explicit face tracking, or modular pipelines. Instead of using the model directly for dubbing, we leverage its generative capacity to synthesize identity-consistent bilingual training pairs, and then learn constrained editing behavior on top of it. This section describes the underlying audiovisual foundation model (Sec. 3.1), the construction of paired dubbing data (Sec. 3.2), and the lightweight in-context LoRA that enables precise, temporally aligned dubbing at inference time (Sec. 3.3)."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Our approach is built upon LTX-2 [HaCohen et al. 2026], foundation model that processes video and audio as unified signal. LTX-2 employs an Asymmetric Dual-Stream Diffusion Transformer (DiT) that processes decoupled latent inputs: video frames are compressed into 3D spatiotemporal tokens 洧녾洧녺 via 3D VAE, and audio is encoded into 1D tokens 洧녾洧녩 via separate 1D VAE. To manage the distinct information densities of these modalities, the model allocates different capacities to each stream, enforcing tight temporal alignment through bidirectional cross-attention layers that allow each modality to continuously condition the other [HaCohen et al. 2026]. Similar to advanced image diffusion models like Flux [Labs 2024], LTX-2 is trained using Flow Matching (specifically Rectified Flow) [Lipman et al. 2023; Liu et al. 2022], which learns straight trajectories between the data and noise distributions. The training process defines linear probability path 洧논洧노 = (1 洧노)洧논0 + 洧노洧논1 connecting the Gaussian noise distribution 洧논0 (0, 洧냪 ) to the data distribution 洧논1 D. The model is optimized to predict the velocity field 洧녺洧랚 that drives this transformation by minimizing the regression loss: JUST-DUB-IT : Video Dubbing via Joint Audio-Visual Diffusion 3 Fig. 2. Pipeline for Generating Paired Audio-Visual Dubbing Data. The pipeline consists of two stages. First, the audiovisual generation model produces contiguous sequence containing context clip (e.g., spoken English) followed by target clip (e.g., spoken French). Second, the audio and lip-region video of the target clip are masked, and the same unified model is used in an inpainting setting to regenerate the masked content, conditioned on the context clip and new text prompt (e.g., re-dubbing the target into English). (cid:2)洧녺洧랚 (洧논洧노, 洧노, 洧녫) (洧논1 洧논0)2(cid:3) , L洧냧 洧 = E洧노 [0,1],洧논0,洧논1 (1) where 洧녫 represents conditioning signals such as text prompts. In LTX-2, the text prompt is highly detailed and contains the full transcription of the audio; thus, the model is not responsible for the translation itself, as the translated prompt is provided directly as conditioning. We adapt this robust, flow-based audio-visual prior for the video dubbing task using Video In-Context Low-Rank Adaptation (IC-LoRA) [Lightricks 2026], allowing to steer the synchronized generation process toward target languages while requiring only minimal number of trainable parameters and small amount of task-specific training data."
        },
        {
            "title": "3.2 Video-dubbing Paired Data Construction",
            "content": "The core challenge in training joint audio-visual dubbing model is the lack of perfect pairs: datasets where the same speaker utters the same semantic content in multiple languages while maintaining identical pose, lighting, identity and backgrounds. Since natural data of this nature does not exist, we synthesize it by leveraging the generative prior of pretrained audio-visual foundation model. Importantly, although our dataset largely consists of pairs of identical content in different languages, we find that it is effective for training the model to perform wide range of editing tasks. By learning to map diverse text prompts to synchronized audio-visual updates, the model generalizes at inference time to diverse dubbing tasks, including same-language content editing and translation between previously unseen language pairs. As illustrated in Fig. 2, our pipeline operates in two stages. First, we generate Language-Switching Videosclips where single speaker naturally transitions between languages (e.g., English French) within contiguous shot. This establishes ground-truth reference for the speakers identity in both linguistic contexts. Second, we employ inpainting to create counterfactual pairs. We split the video Fig. 3. The IdentityPronunciation Trade-off. Na칦ve audio inpainting reveals fundamental conflict between preserving speaker identity and achieving linguistically correct pronunciation. When denoising from scratch (Left), the model exhibits voice drift, failing to preserve the speakers vocal identity. When conditioning on the source audio to maintain identity (Middle), phonetic and prosodic patterns leak across languages, resulting in prosody leakage. Our approach (Right) resolves this trade-off by conditioning generation on reference clip that preserves speaker identity while exhibiting the target-language phonetic style. at the language switch; for the first half (Language A), we noise the relevant face area and audio, then denoise them conditioned on the translated text and the global audiovisual context of the second half (Language B), including visual identity, voice characteristics, and environmental cues. This yields dataset of context-aware paired dubbing data, where the visual context (pose, background) remains intact while the spoken language varies. Below we discuss the motivation of our design and three key mechanism we use to improve data quality. 4 Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, and Daniel Cohen-Or The IdentityPronunciation Trade-off. The language-switching strategy balances vocal identity and pronunciation during audio inpainting. As shown in Fig. 3, denoising without context leads to voice drift, while conditioning solely on source-language audio preserves identity but causes Prosody Leakage where the target text is articulated with the source languages rhythm. Our approach resolves this by conditioning generation on reference clip that provides the speakers vocal identity within the correct target linguistic style. Latent-Aware Fine Masking. critical challenge in trainingfree inpainting is preventing the model from peeking at the original motion. na칦ve approach involves encoding the original video and then applying noise to the latent tokens corresponding to downsampled lip mask. However, this strategy fails due to the large receptive field of the video VAE encoder (e.g., 32328). During the encoding process, spatial information is not strictly isolated; pixel data from the lip region propagates into the latent representations of the surrounding face (e.g., jaw and cheeks). Consequently, even when the specific lip tokens are fully corrupted with noise, the adjacent unmasked tokens retain echoes of the original lip trajectory. The diffusion model exploits this leakage to trivially reconstruct the original motion rather than generating diverse, audio-aligned dubbing (see visualization in Fig. 11). To eliminate this, we implement Latent-Aware Fine Masking. We empirically determine the Effective Latent Mask by computing the residual difference in latent space between masked and unmasked input. This identifies and masks the full extent of information spread, forcing the model to regenerate the relevant facial regions from scratch in sync with the generated audio. Lip Augmentation via Phonetic Diversity. secondary challenge in synthetic data generation is the ambiguity of visemes, where different phonemes may share similar lip shapes, often resulting in visually indistinct or mumbling lip movements (see Fig. 10). To increase visual discrimination between the condition and the target, we augment the training data by prompting exaggerated, character-level articulation (e.g., A..B..C), encouraging more diverse lip movements. In practice, we perform video inpainting twice for each clip: once without Lip Augmentation to obtain correct translated audio, and once with Lip Augmentation to generate visually diverse lip movements. The two outputs are merged into single video and used as the training context. Although the visual and audio streams are not synchronized at this stage, the impact of such mismatch is resolved using Modality-Isolated Cross-Attention, as described in the following section. Quality Control Pipeline. To ensure training fidelity, we employ multi-stage filtering process (detailed explanations provided in Appendix Sec. A.1)."
        },
        {
            "title": "3.3 Audio-Video In-Context Learning",
            "content": "Audiovisual In-Context Framework. Inspired by the In-Context LoRA (IC-LoRA) paradigm [Huang et al. 2024], we introduce framework for joint video audio editing that enables temporally synchronized and context aware generation. We formulate the editing process as conditional generation task: given source audiovisual pair Fig. 4. Model Training. Our framework follows an in-context generation paradigm where clean context audio-visual pairs are concatenated with noised target pairs. We fine-tune only LoRA adapters while keeping pretrained Audio-Visual (AV) Diffusion Transformer frozen. Conditioned on text prompt (e.g. The person is speaking in French), the model learns to propagate edits from the context while maintaining temporal synchronization between audio and video. We introduce modality-specific masking strategy in AV cross-attention, ensuring that noisy audio attends only to noisy video and vice versa, since conditioning noisy signal on clean context from the other modality leads to signal leakage and conflicting guidance, which this masking prevents. (洧녤洧멇롐洧녫, 洧냢洧멇롐洧녫 ) and desired text dialogue, the model learns to edit the visual and audio content(i.e. lips and the related speech) to obtain lip-synchronized audiovisual pair that matches the target dialogue, while preserving the context from the source, including visual and voice identity, as well as the environment. To achieve this without the prohibitive cost of full parameter fine-tuning, we adopt Low-Rank Adaptation (LoRA) approach. We inject trainable rank-decomposition matrices into the self-attention and cross-attention layers of pre-trained audiovisual transformer. During training, the base model weights remain frozen, and only the LoRA adapters are optimized to adapt the velocity field prediction by minimizing the flow-matching objective: L洧냧 洧 = E洧노,洧논0,洧논1,洧녫 (cid:2)洧녺洧랚 (洧논洧노 , 洧노, 洧녫, LoRA) (洧논1 洧논0)2(cid:3) . where 洧녫 represents the source visual tokens, audio tokens and text tokens. (2) Context-Aligned Multimodal Positional Encoding. Shared positional encodings are important for maintaining intra-modal alignment. For both video and audio streams, we assign contextual tokens positional encodings identical to their target counterparts. This explicitly signals to the model that contextual frames and audio segments are spatially and temporally aligned with the target, reframing dubbing as context-aware completion task grounded in the original scene. By sharing positional encodings across contextual and target modalities, we guide the LoRA layers to learn mappings that preserve the spatial and temporal structural integrity of the scene while steering generation toward the target language and speaker identity. This design reframes dubbing as context-aware completion task rather than blind translation process, enabling the model to generate lips and speech that remains grounded in the original scene and maintains fine-grained audiovisual synchronization. Modality-Isolated Cross-Attention. notable hurdle in joint audiovisual modeling involves managing the dense interactions within transformer blocks. While cross-attention layers are essential for aligning video and audio features, global attention mechanism in an in-context setting often results in cross-modal leakage. Specifically, unnecessary interactions between the source video tokens and the noisy target audio tokens (and vice versa) can introduce guidance noise, leading to blurred boundaries or temporal misalignments in the final output. To address this leakage issue, we introduce structured attention bias within cross-modality attention layers. For query 洧녟 from one modality and keys 洧 from both source and target contexts, the attention operation is defined as: Attention(洧녟, 洧, 洧녤 ) = Softmax (cid:18) 洧녟洧쮫롐 洧녬洧녲 (cid:19) 洧녤 , + 洧 (3) where 洧 is masking matrix designed to restrict the interaction space. Specifically, we define 洧 such that: (cid:40) 洧洧녰,洧녱 = if 洧녰, 洧녱 Target (noisy) tokens of both modalities 0 otherwise This masking strategy forces the denoising process to derive cross-modal alignment solely from the target pair while permitting each modality to reference its respective source tokens for identity guidance. This approach resolves the audiovisual asynchrony introduced by lip augmentation, ensuring each modality focuses exclusively on relevant cross-modal signals. Our unified model enables visual dynamics and acoustic events to co-evolve naturally. By treating the entire stream as single generative task, the model synchronizes paralinguistic cuessuch as sighs or laughterand grounds environmental sounds (e.g., dog barking) in the physical actions of the scene. This holistic approach ensures dialogue pacing remains grounded in scene context, avoiding the temporal misalignments common in modular pipelines that treat speech as detached stream."
        },
        {
            "title": "4 Experiments",
            "content": "Datasets. We evaluate our framework using combination of standard and challenging benchmarks. Following previous works, we adopt the widely used HDFT [Zhang et al. 2021] and TalkVid [Chen et al. 2025a] datasets, comprising well-curated videos characterized by predominantly frontal faces, limited pose variation, and clean acoustic conditions. These datasets thus represent relatively narrowdomain evaluation settings. From the test partitions of these datasets, we randomly sample 150 videos to construct English-to-Foreign (covering Russian, Spanish, German, and French), Foreign-to-English, and English-to-English pairs. To complement these benchmarks and JUST-DUB-IT : Video Dubbing via Joint Audio-Visual Diffusion 5 Table 1. Quantitative Evaluation of Visual Quality and Audiovisual Synchronization. We report generation success rate (Succ), identity preservation (CSIM), visual fidelity (FID), temporal coherence (FVD), Mouth Aspect Ratio diversity (MAR Div.), and audiovisual synchronization (ASync). Method Succ CSIM FID FVD MAR Div. ASync HDFT & TalkVid Benchmark Real Data LatentSync + CosyVoice MuseTalk + CosyVoice Ours (Unified) 100% 100% 100% 0.8762 0.7937 2.63 4.66 260.98 272.96 0.1263 0.1043 0.0988 0. 8.45 131.88 0.1168 Challenging Benchmark Real Data LatentSync + CosyVoice MuseTalk + CosyVoice 80% 74% 0.7070 0. 6.45 16.04 758.66 902.03 0.1416 0.1335 0.1131 Ours (Unified) 100.0% 0.6455 12.45 353.54 0.1461 1.2900 0.05 0. 2.21 2.4433 1.34 5.65 2.44 Table 2. Quantitative Audio Quality Comparisons. We evaluate temporal alignment (Dur-Err), voice similarity (V-SIM), audio intensity consistency (Int-Corr), and linguistic accuracy (WER) against audio-only baselines. Dataset Method Dur-Err (s) V-SIM Int-Corr WER HDFT & TalkVid Benchmark Challenging Benchmark CosyVoice OpenVoice Ours (Unified) CosyVoice OpenVoice Ours (Unified) 3.4995 1.1628 0.1638 1.9612 1.9917 0.0463 0.6868 0.4006 0.4235 0.6168 0.3605 0.5578 0.416 0.506 0. 0.478 0.501 0.815 0.3288 0.1725 0.3133 0.1573 0.0630 0.2739 Fig. 5. User Study Results. We compare our method against LatentSync and HeyGen through user study, evaluating Lip Synchronization, Prompt Adherence, and Overall Quality. Results indicate that participants prefer our method over baselines across all evaluated metrics. better reflect real-world deployment scenarios, we further introduce more challenging evaluation set by collecting 25 real videos from YouTube and 25 synthetic videos, exhibiting profile views, significant pose shifts, occlusions, and stylized appearances, enabling more rigorous assessment of robustness under unconstrained conditions. Examples of datasets from different sources are in Fig. 9. Baselines. Our framework is the first to jointly perform visual dubbing and audio dubbing within single, unified audiovisual generative model. As no prior methods exist that simultaneously address both tasks, we separately compare our approach against 6 Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, and Daniel Cohen-Or (a) Duration Alignment. Our method precisely matches the original videos duration and synchronizes translated audio, effectively avoiding baseline artifacts such as reversed motion (Rolling back) or mismatched durations. (b) Preservation of Non-Dialogue Events. Our method naturally preserves crucial non-dialogue audio events (e.g., pauses, laughter), leveraging visual cues, while baseline methods ignore these segments entirely. Fig. 6. Qualitative Comparisons on Audiovisual Alignment and Temporal Structure Our joint audiovisual model accurately maintains both visual and auditory contexts, outperforming baseline methods on key audiovisual synchronization tasks. state-of-the-art methods from each domain. Specifically, for the visual dubbing task, we evaluate MuseTalk [Zhang et al. 2025b] and LatentSync [Li et al. 2024]. For cross-lingual visual dubbing, we use CosyVoice [Du et al. 2024] to generate identity-preserving translated audio for the video-only baselines. For the audio dubbing task, we compare our method directly against leading zero-shot voice cloning methods, specifically CosyVoice and OpenVoice [Du et al. 2024; Qin et al. 2023]. Evaluation Metrics. We concisely evaluate three dimensions, briefly described below (detailed explanations provided in Appendix Sec. A.5): Video Quality: Generation success rate (Succ) measures the systems ability to produce an output video. failure is defined as complete lack of output, typically occurring when mask-based baselines fail to detect face, whereas any generated video is counted as success.), Visual fidelity (FID [Heusel et al. 2017]), temporal coherence (FVD [Unterthiner et al. 2019]), and identity preservation (ID-SIM, ArcFace [Deng et al. 2019]) and facial articulation measured by Mouth Aspect Ratio diversity (MAR). Audio Quality: Linguistic accuracy (WER, Whisper [Radford et al. 2022]), voice similarity (V-SIM, ERes2Net [Zhou and Li 2021]), and temporal intensity consistency (Int-Corr, Pearson correlation of RMS audio envelopes [Chung et al. 2024]). Audiovisual Synchronization: Temporal offset between lip movements and speech (SyncNet [Chung and Zisserman 2016])."
        },
        {
            "title": "4.1 Quantitative Results",
            "content": "We evaluate our unified audiovisual generation framework across video quality, audio quality, and audiovisual synchronization. Quantitative results are reported in Tab. 1, Tab. 2, and Fig. 5. Visual Quality Analysis. As shown in Tab. 1 our method achieves the lowest FVD across all datasets, indicating superior temporal coherence and reduced motion artifacts, particularly in challenging in-the-wild scenarios. This robustness is reflected in our 100% generation success rate on both standard and challenging benchmarks, whereas modular baselines like LatentSync and MuseTalk drop to 80% and 74%, respectively, on complex samples. As illustrated by the red ERROR frames in Fig. 8, these failures typically occur when mask-based baselines are unable to detect face in profile views or non-human subjects, resulting in total lack of output. Regarding identity preservation, our Unified Model remains highly competitive, we achieve CSIM of 0.8471 on the HDFT benchmark and 0.6455 on the Challenging Benchmark, scores that are comparable to state-of-the-art modular pipelines. However, as traditional metrics like SyncNet may reward distorted, frontal-biased reconstructions in profile views (see explanation in Fig. 12), the qualitative superiority of our methods temporal-semantic coherence is most apparent when assessing the synchronous audiovisual dynamics. Our FID is higher than face-centric baselines, this is an expected consequence of our global VAE-based encoderdecoder, which operates on full frames rather than masked facial regions. Although this increases reconstruction variance and negatively affects FID, it enables robust generalization to arbitrary characters, non-frontal views, and non-human subjects without requiring face masks or landmarks. Additionally, our Unified Model achieves higher MAR, demonstrating improved facial expressiveness and natural mouth articulation compared to modular baselines such as MuseTalk and LatentSync, which show restricted movements and \"mumbling\" artifacts typical of inpainting-based methods. Audio Quality Analysis. As shown in Tab. 2, our Unified model achieves the lowest duration error (Dur-Err) and highest intensity correlation (Int-Corr). Unlike CosyVoice and OpenVoice, which lack the visual grounding to adjust speech speed to the original shot length, our model naturally compresses or expands dialogue to match the video context. This prevents common artifacts like mismatched endings or \"rolling back\" motion (see Fig. 6a). Our joint formulation ensures that paralinguistic cues and scene interactionssuch as dog barking in response to gestureco-evolve with the visual stream (see Fig. 6a and Fig. 7). While these advantages are reflected in our superior Int-Corr scores, this temporal-semantic coherence is best experienced through the continuous audiovisual playback. While our Word Error Rate (WER) and voice similarity (SSIM) scores remain competitive, they are inherently bounded by the base audio-visual models capabilities. Overall, these results highlight our methods strong suitability for in-the-wild audiovisual coherence and realistic speech synchronization in diverse scenarios. Audiovisual Synchronization. Our audiovisual synchronization, measured by AV offset using SyncNet (See Tab. 1), remains consistently within perceptually unnoticeable range (approximately 12 frames). Although LatentSync and MuseTalk achieve near-zero offsets due to explicit optimization with SyncNet on frontal-view samples, this suggests potential overfitting rather than broadly generalizable synchronization performance. In contrast, our method demonstrates synchronization quality highly competitive with real videos, effectively capturing natural lip movements across varied and challenging scenarios. User Study. Our user study  (Fig. 5)  confirms that our method is preferred over baselinesincluding the SOTA closed-source solution from HeyGen [HeyGen 2026], in terms of Lip Synchronization, Prompt Adherence, and Overall Quality. Detailed implementation of the user study is provided in Appendix Sec. A.4."
        },
        {
            "title": "4.2 Qualitative Results",
            "content": "Our in-context audiovisual model excels at preserving both visual and audio contexts, especially in challenging scenarios requiring precise temporal alignment and complex environmental sounds. As demonstrated in Fig. 6a, our method precisely aligns with the original video duration, avoiding visual rollback artifacts and duration mismatches observed in baseline methods. The advantage of joint audiovisual modeling is further highlighted in scenarios involving visually related audio events. Fig. 6b and Fig. 7 shows our method effectively leveraging visual context to distinguish segments needing translation from those to be preserved. As result, it accurately maintains the timing of speech and essential non-dialogue audio elements such as pauses or laughter. In contrast, baseline methods, including the commercial tool HeyGen, entirely disregard these nuanced audiovisual cues. JUST-DUB-IT : Video Dubbing via Joint Audio-Visual Diffusion 7 Table 3. Ablation Study. We evaluate identity preservation (ID-SIM), voice similarity (V-Sim), linguistic accuracy (WER), and lip distance (LMD). Benchmark Variant ID-Sim V-Sim WER LMD Easy Hard Full Model w/o LoRA w/o Lip Aug Full Model w/o LoRA w/o Lip Aug 0.8471 0.9446 0. 0.6455 0.7612 0.7245 0.4235 0.8092 0.5621 0.5578 0.8404 0.6832 0.3133 0.9652 0.5132 0.2739 0.9809 0.5214 0.0172 0.0086 0. 0.0488 0.0432 0.0469 Additionally, our method naturally adapts visual and audio content without relying on external face detectors. This strength is particularly evident under difficult conditions involving profile views, facial occlusions and non-human characters  (Fig. 8)  . Baselines trained primarily on frontal-face reconstructions exhibit significant artifacts, such as faces incorrectly overlaying foreground objects, severely blurred lips, and failures resulting in unchanged copy-pasted input videos. Mask-based methods further fail entirely on stylized or non-human characters due to their reliance on explicit human-face priors. Our unified audiovisual approach robustly generalizes to diverse and unconstrained scenarios, consistently maintaining accurate lip synchronization and stable visual quality, clearly outperforming existing baseline methods."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We analyze the necessity of our integrated design by jointly ablating training and lip augmentation on the synthesized dataset. We consider two variants: w/o Training, zero-shot in-context setting without learned adapters, and w/o Lip Augmentation, where training data lacks phonetic diversity. As summarized in Table 3, both ablations expose the same failure mode: without learning prompt-driven audiovisual edits, the model defaults to reconstructing the source video. This reconstruction bias manifests as artificially high identity (ID-SIM) and voice similarity (V-SIM) but extremely poor prompt adherence, reflected by near-random WER. Removing lip augmentation further exacerbates this issue by reducing visual distinctiveness in training data, encouraging lip copy-paste behavior and degrading linguistic accuracy. In contrast, the full model achieves substantially lower WER and higher Lip Landmark Distance (LMD), confirming that it generates novel, prompt-aligned lip motion rather than replicating source trajectories. Together, these results demonstrate that training and lip augmentation are jointly necessary to break reconstruction bias and enable functional audiovisual dubbing."
        },
        {
            "title": "5 Conclusions",
            "content": "We have presented new way to approach video dubbing as constrained audiovisual generation task. By operating directly within joint audiovisual diffusion model, we show that translated speech, facial motion, and scene dynamics can be regenerated together under unified generative prior. This formulation allows temporal structure, identity cues, and non-speech events to co-evolve naturally, resulting in dubbed videos that preserve both fine-grained lip synchronization and the broader semantic timing of the scene. 8 Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, and Daniel Cohen-Or Beyond robustness in challenging real-world settings, this work suggests that leveraging audiovisual foundation models offers promising direction for moving dubbing beyond modular pipelines toward more holistic, context-aware generation. Framing video dubbing as joint audiovisual generation introduces several technical challenges, including the lack of paired multilingual audiovisual data, the tension between preserving speaker identity and producing correct pronunciation, and the risk of information leakage when regenerating localized facial motion. In this work, we address these challenges while maintaining precise temporal alignment across speech, facial dynamics, and non-speech events within single generative process. Limitations. While the approach substantially improves overall dubbing quality, it does not yet perfectly preserve speaker voice identity in all cases, highlighting the need for stronger disentanglement between linguistic content and vocal style or more explicit identity supervision. Future Work. More broadly, this work underscores the potential of leveraging strong joint audio-visual generative priors for complex multimodal editing tasks and motivates future extensions to longer temporal contexts and richer conversational settings."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Harel Cain, Mohammad Salama and Shachar Honig for valuable discussions, we thank Yoav Shtibelman and Rotem Banet for helping us create the video demo, and we thank Michael Kupchick, Noa Kotler, Andrew Kvochko, Amit Pintz and Alexey Kravtsov for the inference code support. JUST-DUB-IT : Video Dubbing via Joint Audio-Visual Diffusion 9 Fig. 7. Preservation of Non-Dialogue Events and Scene Grounding. Our joint generative framework enables holistic scene modeling where visual dynamics and acoustic events co-evolve. Top: In the \"dog barking\" scenario, our method synchronizes the timing of environmental sounds with physical gestures, whereas baselines like HeyGen often omit or misalign these cues. Bottom: In the \"eating while talking\" example, our unified model elegantly manages cross-lingual duration mismatches. When translating the source English dialogue (9 syllables) into longer French equivalent (11 syllables), the framework adaptively inserts speech frames during the speakers chewing phase rather than when the mouth is obstructed by food. As demonstrated in the supplemental materials, the resulting output faithfully reproduces the acoustic texture of \"talking through chewing,\" showcasing bidirectional interaction between the scenes physical actions and the generated audio. 10 Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, and Daniel Cohen-Or Fig. 8. Qualitative Comparisons. Top: Profile views and occlusions. Bottom: Non-human scenarios. Baseline methods exhibit noticeable artifacts and often fail under these challenging conditions, while our method robustly preserves identity and visual coherence while synchronizing the lips with our generated translation audio."
        },
        {
            "title": "References",
            "content": "Christoph Bregler, Michele Covell, and Malcolm Slaney. 1997. Video Rewrite: Driving Visual Speech with Audio. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques. 353360. Jan Cech and Tereza Soukupova. 2016. Real-time eye blink detection using facial landmarks. Cent. Mach. Perception, Dep. Cybern. Fac. Electr. Eng. Czech Tech. Univ. Prague (2016), 18. Shunian Chen, Hejin Huang, Yexin Liu, Zihan Ye, Pengcheng Chen, Chenghao Zhu, Michael Guan, Rongsheng Wang, Junying Chen, Guanbin Li, Ser-Nam Lim, Harry Yang, and Benyou Wang. 2025a. TalkVid: Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis. arXiv:2508.13618 [cs.CV] https://arxiv.org/ abs/2508.13618 Sanyuan Chen, Chengyi Wang, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2025b. Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers. IEEE Transactions on Audio, Speech and Language Processing 33 (2025), 705718. doi:10.1109/TASLPRO.2025.3530270 Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. 2025. Taming Multimodal Joint Training for High-Quality Video-toAudio Synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Joon Son Chung and Andrew Zisserman. 2016. Out of Time: Automated Lip Sync in the Wild. In Asian Conference on Computer Vision (ACCV). Yoonjin Chung, Junwon Lee, and Juhan Nam. 2024. T-foley: controllable waveformdomain diffusion model for temporal-event-guided foley sound synthesis. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 68206824. Chaoqun Cui, Liangbin Huang, Shijing Wang, Zhe Tong, Zhaolong Huang, Xiao Zeng, and Xiaofeng Liu. 2025a. Fine-grained Video Dubbing Duration Alignment with Segment Supervised Preference Optimization. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 45244546. doi:10.18653/v1/2025.acllong.227 Jiayan Cui, Zhihan Yang, Naihan Li, Jiankun Tian, Xingyu Ma, Yi Zhang, Guangyu Chen, Runxuan Yang, Yuqing Cheng, Yizhi Zhou, Guochen Yu, Xiaotao Gu, and Jie Tang. 2025b. GLM-TTS Technical Report. arXiv:2512.14291 [cs.SD] https: //arxiv.org/abs/2512.14291 Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. 2019. ArcFace: Additive Angular Margin Loss for Deep Face Recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, Zhifu Gao, and Zhijie Yan. 2024. CosyVoice: Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens. arXiv:2407.05407 [cs.SD] https://arxiv.org/abs/2407. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M칲ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. 2024. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. arXiv:2403.03206 [cs.CV] https://arxiv.org/abs/2403.03206 Jiazhi Guan, Zhanwang Zhang, Hang Zhou, Tianshu HU, Kaisiyuan Wang, Dongliang He, Haocheng Feng, Jingtuo Liu, Errui Ding, Ziwei Liu, and Jingdong Wang. 2023. StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, Eitan Richardson, Guy Shiran, Itay Chachy, Jonathan Chetboun, Michael Finkelson, Michael Kupchick, Nir Zabari, Nitzan Guetta, Noa Kotler, Ofir Bibi, Ori Gordon, Poriya Panet, Roi Benita, Shahar Armon, Victor Kulikov, Yaron Inger, Yonatan Shiftan, Zeev Melumian, and Zeev Farbman. 2026. LTX-2: Efficient Joint AudioVisual Foundation Model. arXiv:2601.03233 [cs.CV] https://arxiv.org/abs/2601.03233 Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. 2024. LTX-Video: Realtime Video Latent Diffusion. arXiv preprint arXiv:2501.00103 (2024). Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen, Songlin Tang, Jiehui Huang, Xiaoqiang Liu, Pengfei Wan, and Zhiyong Wu. 2025. From Inpainting to Editing: Self-Bootstrapping Framework for Context-Rich Visual Dubbing. arXiv:2512.25066 [cs.CV] https://arxiv.org/abs/2512.25066 Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs Trained by Two Time-Scale Update Rule Converge to Local Nash Equilibrium. In Advances in Neural Information Processing Systems (NeurIPS). HeyGen. 2026. HeyGen AI Video Generator. https://www.heygen.com/. Accessed: 2026-01-07. JUST-DUB-IT : Video Dubbing via Joint Audio-Visual Diffusion 11 Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs.CL] https://arxiv.org/abs/2106.09685 Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, In-Context LoRA for Diffusion Yutong Feng, Yu Liu, and Jingren Zhou. 2024. Transformers. arXiv preprint arxiv:2410.23775 (2024). Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, and Sheng Zhao. 2024. NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models. arXiv:2403.03100 [eess.AS] https://arxiv.org/abs/2403.03100 Black Forest Labs. 2024. FLUX. https://github.com/black-forest-labs/flux. Chunyu Li, Chao Zhang, Weikai Xu, Jingyu Lin, Jinghui Xie, Weiguo Feng, Bingyue Peng, Cunjian Chen, and Weiwei Xing. 2024. LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision. arXiv preprint arXiv:2412.09262 (2024). Lightricks. 2026. LTX-2: Official repository for LTX-2 audio-video foundation model. https://github.com/Lightricks/LTX-2 Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. 2023. Flow Matching for Generative Modeling. arXiv:2210.02747 [cs.LG] https: //arxiv.org/abs/2210.02747 Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. 2023. AudioLDM: Text-to-Audio Generation with Latent Diffusion Models. arXiv preprint arXiv:2301.12503 (2023). Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. 2024. AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing 32 (2024), 28712883. doi:10.1109/TASLP. 2024.3399607 Xingchao Liu, Chengyue Gong, and Qiang Liu. 2022. Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. arXiv:2209.03003 [cs.LG] https: //arxiv.org/abs/2209. Chetwin Low, Weimin Wang, and Calder Katyal. 2025. Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation. arXiv:2510.01284 William Peebles and Saining Xie. 2022. Scalable Diffusion Models with Transformers. arXiv preprint arXiv:2212.09748 (2022). Ziqiao Peng, Jiwen Liu, Haoxian Zhang, Xiaoqiang Liu, Songlin Tang, Pengfei Wan, Di Zhang, Hongyan Liu, and Jun He. 2025. OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers. arXiv preprint arXiv:2505.21448 (2025). KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. 2020. Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild. In Proceedings of the 28th ACM International Conference on Multimedia. 484492. Chunyu Qiang, Jun Wang, Xiaopeng Wang, Kang Yin, and Yuxin Guo. 2026. MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning. arXiv:2601.01568 [cs.SD] https://arxiv.org/abs/2601.01568 Zengyi Qin, Wenliang Zhao, Xumin Yu, and Xin Sun. 2023. OpenVoice: Versatile Instant Voice Cloning. arXiv preprint arXiv:2312.01479 (2023). Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust Speech Recognition via Large-Scale Weak Supervision. arXiv:2212.04356 [eess.AS] https://arxiv.org/abs/2212.04356 Jibin Song, Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. 2025. Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers. arXiv:2509.21893 ByteDance Seed Team. 2025. Seedance 1.5 pro: Native Audio-Visual Joint Generation Foundation Model. https://seed.bytedance.com/ Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2019. Towards Accurate Generative Models of Video: New Metric and Challenges. In International Conference on Learning Representations (ICLR). Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. 2025. Wan: Open and Advanced Large-Scale Video Generative Models. arXiv preprint arXiv:2503.20314 (2025). Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Barmann, Hazim Kemal Ekenel, and Alexander Waibel. 2024. Audio-driven Talking Face Generation with Stabilized Synchronization Loss. In Proceedings of the European Conference on Computer Vision (ECCV). Shaoshu Yang, Zhe Kong, Feng Gao, Meng Cheng, Xiangyu Liu, Yong Zhang, Zhuoliang Kang, Wenhan Luo, Xunliang Cai, Ran He, and Xiaoming Wei. 2025. InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing. 12 Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, and Daniel Cohen-Or arXiv:2508.14033 [cs.CV] https://arxiv.org/abs/2508.14033 Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi Dai, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, and Wei Xue. 2025. Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis. arXiv:2502.04128 [eess.AS] https://arxiv.org/abs/2502.04128 Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, and Limin Wang. 2025c. UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions. arXiv:2511. Yue Zhang, Zhizhou Zhong, Minhao Liu, Zhaokang Chen, Bin Wu, Yubin Zeng, Chao Zhan, Yingjie He, Junxin Huang, and Wenjiang Zhou. 2025a. MuseTalk: Real-Time High-Fidelity Video Dubbing via Spatio-Temporal Sampling. arXiv:2410.10122 [cs.CV] https://arxiv.org/abs/2410.10122 Yue Zhang, Zhizhou Zhong, Minhao Liu, Zhaokang Chen, Bin Wu, Yubin Zeng, Chao Zhan, Yingjie He, Junxin Huang, and Wenjiang Zhou. 2025b. MuseTalk: Real-Time High-Fidelity Video Dubbing via Spatio-Temporal Sampling. arXiv:2410.10122 [cs.CV] https://arxiv.org/abs/2410.10122 Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. 2021. Flow-Guided OneShot Talking Face Generation With High-Resolution Audio-Visual Dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 36613670. Shuai Zhou and Ming Li. 2021. ERes2Net: Enhanced Residual Networks for Speaker Verification. In Interspeech. Siyi Zhou, Yiquan Zhou, Yi He, Xun Zhou, Jinchao Wang, Wei Deng, and Jingchen Shu. 2025. IndexTTS2: Breakthrough in Emotionally Expressive and DurationControlled Auto-Regressive Zero-Shot Text-to-Speech. arXiv:2506.21619 [cs.CL] https://arxiv.org/abs/2506."
        },
        {
            "title": "Supplementary Material",
            "content": "The MAR diversity (洧洧냢洧녠div) is computed as the standard deviation of MAR over the sequence: Implementation Details A.1 Training Data We use Gemini to generate 100 structured multilingual prompts spanning seven languages: English, Spanish, Russian, French, German, Italian, and Greek. Each prompt specifies detailed visual attributes, temporal segments, and explicit language switches within single video. An example prompt is provided below. Hyper-realistic 8k, sharp focus, neutral lighting. Shot in rainy alleyway at midnight with atmospheric cinematic lighting. Eleanor, detective whose appearance matches the setting. At [00:0000:04]: Eleanor looks directly at the camera, speaking with urgency. She speaks in French, saying: La situation est devenue bien trop dangereuse pour que nous puissions rester ici. At [00:0400:06]: SILENT PAUSE. She takes deep breath. At [00:0600:10]: She lights cigarette with shaking hands, shielding the flame. She speaks in English, saying: explicitly told you never to come to this part of the city after midnight. Given generated language-switching video, we process the two halves independently. We detect the lip region using ArcFace and encode the masked video into latent space to obtain 洧냧mask. In parallel, an all-black video is encoded to produce 洧냧empty. The effective latent mask is computed as 洧냧mask 洧냧empty > 洧랦, where 洧랦 = 0.1. This mask is used to selectively apply noise to the lip region of the visual half, while the audio stream is fully noised. The noised audiovisual inputs are then jointly denoised using the same audio-visual diffusion model with translated prompt. Notably, the lips mask is used only in the dataset generation stage. Later on in the LoRA training and inference, no mask is used. For lip augmentation, we re-inpaint the same video using randomly generated nonsensical speech prompt composed of letters (e.g., she is speaking A. . .B. . .C ). The audio from the first inpainting pass and the video from the second pass are merged to form context video. Although the audio and video are not temporally synchronized, the attention isolation mechanism limits cross-modal interference and mitigates adverse effects. In data filtering process, besides semantic alignment using Qwen2VL, linguistic correctness using Whisper and ArcFace for identity preservation. we use Lip Landmark Distance (LMD) and the Mouth Aspect Ratio (MAR) to quantify lip movement. The Lip Landmark Distance (LMD) measures structural deviation between reference and generated lip landmarks: 洧洧洧냥 = 1 洧녢 洧 洧녢 洧노 =1 洧녰 洧 (cid:13) (cid:13) (cid:13)P 洧洧 洧녭 洧녰,洧노 洧녮洧뉧롐 洧녰,洧노 (cid:13) (cid:13) (cid:13) , (4) where 洧녢 is the number of frames and 洧 denotes the set of 20 lip landmarks (indices 5271). The Mouth Aspect Ratio (MAR) at frame 洧노 is defined as: 洧洧냢洧녠洧노 = (cid:13) (cid:13)P55,洧노 P61,洧노 (cid:13) (cid:13)P52,洧노 P58,洧노 (cid:13) (cid:13)2 (cid:13) (cid:13)2 . (5) 13 洧洧냢洧녠div = (cid:118)(cid:117)(cid:116) 1 洧녢 洧녢 (cid:16) 洧노 =1 洧洧냢洧녠洧노 洧洧냢洧녠 (cid:17) . Finally, the QualityDiversity (QD) score is defined as: 洧녟洧냥 = 洧洧洧냥 洧洧냢洧녠div. (6) (7) A.2 Benchmark We evaluate our framework using comprehensive evaluation framework for video-to-video dubbing across two distinct tiers: Standard Benchmark: This tier consists of 150 meticulously resampled pairs: 100 multilingual pairs from TalkVid and 50 English-toEnglish pairs from HDTF. These datasets provide high-fidelity facial animations and natural speaking styles across English, German, Spanish, French, and Russian. Challenging Benchmark: This tier comprises 50 high-complexity samples to stress-test robustness. It includes 25 edited movie clips sourced from YouTube featuring intense speech patterns (The Godfather, Pulp Fiction, The Mask, The Dark Knight) and 25 in-the-wild synthetic scenes. These samples involve low light conditions, profile views, occlusions, and non-human stylized characters. A.3 Training We train masked audio-video IC-LoRA adapter on the LTX-2 model using rank-128 LoRA modules applied to attention (to_k, to_q, to_v, to_out.0) and feed-forward (net.0.proj, net.2) layers across both video and audio branches. The training employs dual learning rate strategy with 2 104 for video modules and 1105 for audio modules to prevent overfitting in the audio domain. We implement masked loss training with 10:1 ratio between foreground (mask_loss_weight= 1.0) and background (mask_loss_weight= 0.1) regions to focus learning on speaker-specific features while preserving scene context. The model is trained for 2,000 steps with batch size 1, gradient checkpointing enabled, and mixed-precision (bfloat16) on preprocessed video-audio pairs at 960 544 121 resolution (25 FPS). Cross-attention masking between target video and reference audio is enabled to improve audio-visual alignment. We shifted logit-normal timestep sampling, and AdamW optimizer with linear scheduling and gradient clipping (max_norm= 1.0). A.4 User Study To evaluate our method, we conducted comprehensive user preference study comparing our model against state-of-the-art baselines. We utilized test set of 18 video samples, consisting of 6 easy samples (single speaker with static backgrounds) and 12 hard samples (cinematic scenes featuring complex motion, multiple speakers, and varied lighting). total of 25 participants were asked to evaluate each sample across three dimensions: (1) Lip Synchronization, assessing how well the dubbed audio matches the facial motion; (2) Prompt Adherence, evaluating the correctness of the spoken words and pronunciation relative to the target dialogue; and (3) Overall Preference, considering factors such as acting quality, immersion, and professionalism. This resulted in total of 450 pairwise comparisons and 1,350 individual data points. A.5 Evaluation Metrics Implementation We report generation success rate (Succ) as the percentage of inputs for which method produces an output video. In our evaluation, our method achieves 100% Succ, while failures for competing methods occur when they cannot detect face in the input. Identity preservation (CSIM) is computed as the cosine similarity between face-identity embeddings extracted from the generated video and the reference identity. Visual fidelity (FID) is the Fr칠chet distance between feature distributions of real and generated frames. Temporal coherence (FVD) is the analogous Fr칠chet distance computed in video feature space to capture spatiotemporal consistency. Mouth Aspect Ratio diversity (MAR Div.) is computed as the temporal standard deviation of the Mouth Aspect Ratio (MAR), normalized mouth-opening measure analogous to the Eye Aspect Ratio (EAR) [Cech and Soukupova 2016]. Audiovisual synchronization (ASync) is computed using SyncNet, which measures global synchronization offset for video by averaging pairwise audiovisual embedding similarity across time. Additional Visualizations B.1 Example of benchmark We benchmark our model on HDFT [Zhang et al. 2021], TalkVid [Chen et al. 2025a] and curated examples from youtube videos and synthetic videos. We provide an example of the datasets in Fig. 9. Our curated videos enable more rigorous assessment of robustness under unconstrained conditions. B.2 Latent-Aware Fine Masking To demonstrate the existence of latent information leakage in trainingfree inpainting, we conduct an input-level corruption experiment by explicitly layering green mask over the lip region of the test video before encoding. The corrupted video is then encoded into the Video VAE latent space, and diffusion is performed using coarse latent mask corresponding to the lip area. As shown in Fig. 10, without Latent-Aware Fine Masking, the model produces pronounced green light artifacts around the mouth and lower face. These artifacts indicate that the green-masked lip information propagates into neighboring latent tokens during VAE encoding due to the large spatiotemporal receptive field, and is subsequently amplified during diffusion. In contrast, applying Latent-Aware Fine Masking removes the full latent region affected by this information spread, eliminating the artifacts and forcing the model to regenerate coherent, audio-aligned facial motion. This result confirms that na칦ve masking at either the input or latent level is insufficient, and that explicitly accounting for latent information propagation is critical for stable inpainting. B.3 Lip Augmentation The results in Fig. 11 demonstrate that incorporating lip augmentation leads to substantially richer variation in reconstructed mouth movements, improving expressiveness and temporal realism. In Fig. 9. Example of sampled benchmark videos. HDFT and TalkVid consist mainly frontal faces, limited pose variation, and clean acoustic conditions. Our curated benchmark exhibits profile views, significant pose shifts, occlusions, and stylized appearances. Fig. 10. We overlay green mask on the lip region of the input video and encode the corrupted video using the Video VAE. Left: Corrupted input. Middle: Output without Latent-Aware Fine Masking, showing pronounced green light artifacts around the mouth and lower face due to latent information leakage from the masked region. Right: Output with Latent-Aware Fine Masking, where leakage-induced artifacts are eliminated and coherent, audio-aligned facial motion is regenerated. Fig. 11. Effect of Lip Augmentation on Data Generation Diversity. We present visualizations on original video frames (top row), inpainted results without lip augmentation (middle row), and inpainted results with lip augmentation (bottom row). contrast, inpainting without lip augmentation tends to collapse to similar lip shapes across frames, reducing motion diversity. 14 B.4 ASync metric problem In Fig. 12 unconstrained scenarios involving profile views, traditional metrics like SyncNet can be highly misleading. Maskedbased baselines like LatentSync often reconstruct profile lips with \"frontal-facing\" bias to minimize temporal offset, resulting in misleadingly superior score of 0.00 despite visible artifacts. Our method achieves an ASync score (3.00) identical to the ground truth, demonstrating that our video results are physically consistent rather than metric-driven distortion. Fig. 12. ASync metrics overfit to frontal videos."
        }
    ],
    "affiliations": [
        "Lightricks",
        "Tel Aviv University"
    ]
}