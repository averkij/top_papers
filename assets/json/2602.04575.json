{
    "paper_title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration",
    "authors": [
        "Jiaheng Liu",
        "Yuanxing Zhang",
        "Shihao Li",
        "Xinping Lei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \\textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows. Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets."
        },
        {
            "title": "Start",
            "content": "2026-02-05 Vibe AIGC: New Paradigm for Content Generation via Agentic Orchestration Jiaheng Liu1, Yuanxing Zhang2, Shihao Li1, Xinping Lei1 1 NJU-LINK Team, Nanjing University 2 Kling Team, Kuaishou Technology liujiaheng@nju.edu.cn Abstract For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered usability ceiling manifested as the Intent-Execution Gap (i.e., the fundamental disparity between creators high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the Vibe AIGC, new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows. Under this paradigm, the users role transcends traditional prompt engineering, evolving into Commander who provides Vibe, high-level representation encompassing aesthetic preferences, functional logic, and etc. centralized MetaPlanner then functions as system architect, deconstructing this Vibe into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from fragile inference engine into robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets. 6 2 0 2 4 ] . [ 1 5 7 5 4 0 . 2 0 6 2 : r Figure 1: Content generation advancing toward the Vibe AIGC era: systemic leap driven by structural combination."
        },
        {
            "title": "1 Introduction",
            "content": "The trajectory of generative artificial intelligence (AI) has reached critical juncture. For the past decade, the community has operated under Model-Centric paradigm (Blattmann et al., 2023; Achiam et al., 2023; Rombach et al., 2022), where progress is primarily measured by the expansion of parameter counts, the ingestion of increasingly massive datasets, and the refinement of end-to-end training objectives. From the initial breakthroughs in Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) 1 to the current dominance of Diffusion Transformers (DiTs) (Peebles and Xie, 2023), the prevailing belief has been that scaling laws would eventually bridge the gap between human imagination and machine execution. However, as these foundational models are deployed into professional creative environmentsranging from cinematic production to complex narrative synthesisa fundamental usability ceiling has emerged. Despite the undeniable increase in visual fidelity, the actual process of content creation remains fragile exercise in stochastic trial-and-error. The root of this ceiling lies in the Intent-Execution Gap: the inherent disparity between human creators high-level, multi-dimensional vision and the black-box nature of current single-shot generation. In todays AIGC (i.e., Artificial Intelligence Generated Content) landscape, the user is relegated to the role of prompt engineera digital manual laborer who must spend hours performing latent space fishing, hoping that specific string of keywords will align with the models internal weights to produce coherent result. This workflow is fundamentally unscalable for professional applications that require temporal consistency, character fidelity, and deep semantic understanding. Even as models become larger, they remain architecturally flat; they lack the hierarchical reasoning and iterative verification loops necessary to manage long-horizon creative tasks. When model hallucinates detailsuch as the incorrect school uniform in commemorative video or disjointed narrative arcthe user is often left with no recourse but to re-roll the generation, process that is both computationally wasteful and creatively frustrating. As shown in Figure 1, we observe that the field of software engineering is currently undergoing radical transformation known as Vibe Coding (Karpathy, 2025), where natural language is being utilized not as mere interface for code, but as high-level kernel for autonomous system construction (Mei et al., 2025). We believe the generative AI community is on the cusp of similar, yet even more profound, transition. It is no longer sufficient to treat content generation as single-pass inference problem. Instead, we must begin to view the generation of complex media as system-level engineering challenge that requires the synthesis of specialized agentic behaviors. In this paper, we argue that the current research focus on end-to-end one-size-fits-all models is reaching point of diminishing returns for the human-AI collaborative economy (Bai et al., 2023). We contend that the machine learning community must pivot its fundamental research objective: shifting away from Model-Centric Generation toward Vibe AIGC, paradigm in which content generation is reconceptualized as the autonomous synthesis of multi-agent workflows driven by high-level human intent. Specifically, in Vibe AIGC, we believe that the next frontier of artificial intelligence is not larger models, but smarter orchestration, and propose transition where the user moves from Prompt Engineer to Commander, providing the Vibe environment, high-level representation of aesthetic, logic, and intent, where Meta-Planner then deconstructs into executable and verifiable multi-agent pipelines (Liu et al., 2023; Horvat, 2025). In the following paper, we first explore the philosophical foundations of Vibe Coding (Section 2). We then provide technical critique of current model-centric architectures (Section 3). Drawing on preliminary successes in agentic frameworks (Section 4), we detail the top-level architecture of Vibe AIGC (Section 5), emphasizing the role of hierarchical orchestration. Finally, we introduce the alternative views (Section 6 and the call-to-action for building the Vibe AIGC ecosystem (Section 7)."
        },
        {
            "title": "2.1 Definition of Vibe Coding",
            "content": "In the history of computer science, the evolution of programming has been steady march away from machine hardware toward human cognitionfrom assembly to C, and from to Python (gem, 2025; Zeng et al., 2025; ant, 2025; Achiam et al., 2023; Liu et al., 2025a). Each step increased the level of abstraction, allowing developers to ignore low-level complexities to focus on logic. The term Vibe Coding popularized by researchers like Andrej Karpathy (Karpathy, 2025; Horvat, 2025), represents the logical conclusion of this trajectory: the removal of formal syntax entirely. In this framework, the Vibe refers to high-level, multi-dimensional representation of intent that includes aesthetic preference, functional goals, and systemic constraints. Unlike traditional prompt, which is often one-shot instruction (Mei et al., 2025; Sapkota et al., 2025), Vibe is continuous latent state maintained through dialogue. We argue that natural language has reached critical threshold of semantic density where it can function as meta-syntax. Within Vibe Coding environment, the AI does not just execute command, and it interprets the atmosphere of the project to make autonomous decisions, such as selecting appropriate library dependencies or adhering to an unstated but implied design language that previously required human intervention."
        },
        {
            "title": "2.2 User as Commander",
            "content": "The most profound shift in Vibe Coding is the reconfiguration of the human users identity (Chen et al., 2025a). Throughout the Model-centric era for AIGC, the user was essentially manual laborer of the interface (i.e., prompt engineer, who spent hours in stochastic trial-and-error loop, attempting to find the specific magical string of text that would yield desired result). This role is inherently limited by the users ability to predict the models internal weights. Vibe Coding proposes transition where the user acts as Commander (or Architect). In this paradigm, the human provides the strategic vision (the What and the Vibe), while the AI system autonomously determines the tactical implementation (the How). This is analogous to the shift from pilot manually controlling every flap on an aircraft to commander setting destination on an advanced autopilot system. By delegating the low-level generation of code or assets to agentic workflows, the user can operate at the level of system design. This democratization is crucial, as it allows individuals with domain expertise to command complex digital systems, thereby expanding the creative and economic potential of the digital economy."
        },
        {
            "title": "2.3 Agentic Orchestration",
            "content": "A primary failure of current AIGC tools is the Intent-Execution Gap (i.e., the disparity between complex creative vision and the flattened, often mediocre output of single-shot model). Vibe Coding addresses this gap not through better base models, but through recursive orchestration (Dang et al., 2025). In Vibe-driven system, the AI does not attempt to solve complex problem in one pass. Instead, it uses the Vibe as compass to synthesize custom, multi-step workflow. For instance, if user wants to create vibrant, cinematic music video, Vibe Coding agent does not simply call videogeneration API. It recursively breaks the vibe into constituent parts, and it writes script, analyzes the musical tempo, generates character consistency sheets, and oversees the final edit. Crucially, this process is falsifiable and interactive (Qiang et al., 2025; Park et al., 2023). If the output does not match the vibe, the Commander provides high-level feedback (make it darker, increase the tension), and the agentic system reconfigures the underlying workflow logic rather than just re-rolling random seed. This transition from Stochastic Guessing to Logical Orchestration is what separates Vibe AIGC from current generative tools. We contend that the future of machine learning research lies in perfecting this orchestration layerenabling AI to not just predict the next token, but to construct the next solution."
        },
        {
            "title": "3.1 Prevailing Architectures in Video Generation",
            "content": "The field of generative AI has progressed from static image synthesis to dynamic video generation, with foundational models now demonstrating significant advancements in text alignment, visual fidelity, motion plausibility, and realism Liu et al. (2025b). Current research mainly focuses on text-to-image (T2I) Rombach et al. (2022), text-to-video (T2V) Yin et al. (2025), and image-to-video (I2V) generation Xing et al. (2024), as these modalities allow for the collection of scalable, high-quality datasets Wang et al. (2025) curated through rigorous filtering and captioning processes Chen et al. (2025b). The latest video generation paradigm is the latent diffusion model with spacetime Transformer, supplanting earlier GAN Goodfellow et al. (2020) and VQ-VAE-based Van Den Oord et al. (2017) methods. This approach first compresses video into lower-dimensional latent space of spacetime patches and then employs diffusion Transformer (DiT) Peebles and Xie (2023) to denoise these patches conditioned on text prompt. This patch-based framework offers the flexibility to process videos of variable resolutions, durations, and aspect ratios within single model. For I2V generation, the dominant strategy is to adapt pre-trained T2V model. For example, Stable Video Diffusion Blattmann et al. (2023) fine-tunes the Stable Diffusion model by incorporating temporal layers, enabling it to generate motion that is coherent with given input frame. Wan compresses the first frame into VAE latents and concatenates them with the noise latent along the channel axis Wan et al. (2025), so that the starting frame would be exactly maintained. critical challenge in this area is maintaining identity consistency, ensuring subjects appearance remains stable across frames; techniques like IPAdapter Ye et al. (2023) and integrated memory mechanisms Yu et al. (2025) have shown promise in addressing this challenge. However, fundamental constraint is the immense computational cost of video data, which results in training datasets that are smaller in scale and conceptual breadth than those for LLMs. This creates disparity between the models limited world knowledge and high user expectations for both semantic understanding and visual fidelity. Consequently, prompt engineering (PE) Liu and Chilton (2022) becomes essential to bridge this gap by aligning user queries with the models learned data distribution. 3 While sophisticated prompting can unlock impressive reasoning capabilities Wiedemer et al. (2025), the generation process often remains stochastic and requires considerable trial-and-error, increasing the barrier to effective use."
        },
        {
            "title": "3.2 Editing and Reference-Based Video Generation",
            "content": "Advancing beyond unconditional generation from single prompt Team et al. (2025a), significant research frontier is the development of methods for granular control over video generation. Referencebased generation Ku et al. (2024) aims to transfer specific attributes from source to newly generated video. For instance, style transfer applies the visual aesthetic of reference image or video onto target video. Similarly, subject-driven generation Team et al. (2025b), inspired by personalization techniques like DreamBooth Ruiz et al. (2023), first learns unique identifier for subject from reference images. diffusion model, conditioned on this identifier and motion sequence, can then generate new video of that subject. primary obstacle for all reference-based tasks is the difficulty in acquiring training data; it requires meticulously constructed pairs of reference and target videos with guaranteed correspondence and quality. common failure mode is content leakage, where unintended attributes from the reference are mistakenly rendered in the output. Such artifacts, often stemming from inherent model limitations, are difficult for users to mitigate through prompt engineering alone. Video editing Jiang et al. (2025a); Wei et al. (2025a) focuses on modifying an existing video according to user instructions. The most direct approach is text-guided video editing He et al. (2025), which requires model to comprehend textual commands. Representative methods, such as Senorita Zi et al. (2025), focus on object-level editing, including object addition, deletion, or modification. To improve precision, some works utilize masks Cai et al. (2025) to define the region for editing, tasking the model with in-painting the masked area conditioned on the text prompt. Similar to reference-based methods, training editing models is constrained by data availability, as paired before and after real-world videos hardly exist. This reliance on synthetic training data can lead to artifacts like pixel misalignment and unintended subject alterations. In practice, many user needs involve combination of reference-based generation and video editing. Such composite tasks often represent out-of-distribution scenarios for current models, resulting in unpredictable or failed outcomes. Users typically cannot decompose complex creative intent into sequence of discrete operations that model can reliably execute. Exhaustively enumerating all possible task combinations is infeasible, highlighting fundamental gap between specialized model capabilities and the compositional nature of real-world demand."
        },
        {
            "title": "3.3 Unified Architectures for Video Understanding and Generation",
            "content": "A forward-looking research direction is the development of single, unified models capable of both understanding and generation Cui et al. (2025); Lin et al. (2025). This approach treats video not as specialized data type but as another modality, akin to text or audio, to be processed within large-scale, multi-modal architecture Teng et al. (2025). The mainstream unification tends to be achieved by adopting core architectural principles from LLMs. The critical first step is the discretization of continuous video data into sequence of tokens. This is typically accomplished using VQ-VAE or similar network, which learns codebook of visual patterns and compresses video patches into discrete codes. Once tokenized, the video sequence, now represented as series of integers, can be processed by standard Transformer architecture alongside tokens from other modalities like text. Within this framework, diverse tasks are framed as unified next-token prediction problem. Consequently, tasks such as T2V, video captioning, video prediction, and video editing can be handled by single model. However, this approach still faces challenges related to data quality and the need for exhaustive task enumeration. Empirically, these unified models still performs struggling on the fundamental T2V and I2V tasks, lagging behind specialized DiT models in terms of both generation fidelity and semantic alignment Huang et al. (2024); Ghosh et al. (2023)."
        },
        {
            "title": "3.4 Analysis of Realworld Workflows",
            "content": "Video generation models are increasingly adopted in various application domains, including animation, motion comics, news broadcasting, and film post-production, where they can simplify traditional content creation and reduce costs. However, due to the inherent limitations of current modelssuch as the stochastic nature of their outputs and unpredictable capabilitiesusers must devise intricate, multi-step workflows to achieve desired results. For instance, creator producing short-form drama might first develop storyboard, generate keyframe for each shot, and then iteratively refine prompts to produce short video segments, typically 5 or 10 seconds in length Wan et al. (2025); Kong et al. 4 (2024), as constrained by the video generation service. These segments often suffer from artifacts Ye et al. (2025) like color discrepancies, inconsistent character identity, poor realism, and incorrect pacing or lip synchronization Hu et al. (2025) (even with audio-video joint generation Huang et al. (2025)). Consequently, significant manual post-processing is required to edit and assemble these clips into coherent whole, followed by audio dubbing and video super-resolution Du et al. (2025). This process is inefficient, with substantial time and computational resources expended on failed generation attempts, making the outcome heavily reliant on trial and error. While users can produce viable prototype, more intelligent system is needed: one that can comprehend the users high-level creative intent, automatically decompose the task, and orchestrate the use of various tools, rather than depending solely on the combination of raw model outputs and manual prompt engineering."
        },
        {
            "title": "4 Preliminary Attempts",
            "content": "Prior to formalizing the Vibe AIGC paradigm, researchers conducted series of exploratory studies across various domains. These preliminary attempts mark critical transition from model-centric generation to agentic orchestration."
        },
        {
            "title": "4.1 Vibe AI-Generated Text Content",
            "content": "Deep Research: Agentic Synthesis of Creative Context The inception of any creative work requires deep understanding of the underlying subject matter. Traditional AIGC workflows often suffer from contextual shallowness due to the static nature of pre-trained knowledge. Recent advancements in autonomous reasoning, most notably OpenAIs Deep Research OpenAI (2025), have demonstrated the potential for long-horizon information synthesis. By employing LLM-based agents to perform multi-step web searches, cross-verify disparate sources, and synthesize comprehensive knowledge bases, these systems build robust semantic foundation before content generation begins. This thinking-beforecreating paradigm is essential for Vibe AIGC, as it ensures that the generated content is grounded in sophisticated aesthetic and factual context, moving beyond the limitations of single-model-based generation. Figure 2: collaborative multi-agent pipeline in AutoPR. AutoPR: From Fragmented Manual Promotion to One-Click Agentic Pipeline In the traditional scholarly promotion workflow, researchers are often reduced to manual dispatchers. Even with the assistance of LLMs, the process remains model-centric and fragmented: authors must manually juggle multiple LLM interfaces for summarization, download and crop figures from PDFs, and painstakingly rewrite content to satisfy the distinct technical constraints and vibes of various social platforms. Thus, researchers introduced AutoPR Chen et al. (2025c) in Figure 2, novel task that formalizes the transformation of research papers into accurate, engaging public content, and proposed collaborative multi-agent system, comprising Logical Draft, Visual Analysis, and Textual Enriching agents."
        },
        {
            "title": "4.2 Vibe AI-Generated Image Content",
            "content": "Figure 3: collaborative multi-agent pipeline in Poster Copilot. Poster Copilot: Layout Reasoning and Aesthetic Control In professional graphic design, the primary challenge is the precise control over spatial layout and typography. Poster Copilot Wei et al. (2025b) in Figure 3 explored agentic layout reasoning and controllable editing. Unlike black-box generators, Poster Copilot functions as design partner that translates abstract Vibe instructions into concrete design parameters such as geometric composition, color palettes, and layer hierarchies. By incorporating feedback loop with human-in-the-loop editing, this system demonstrates how agents can bridge the gap between vague human aesthetic preferences and the rigid technical requirements of professional design."
        },
        {
            "title": "4.3 Vibe AI-Generated Video Content",
            "content": "Figure 4: collaborative multi-agent pipeline in AutoMV. AutoMV: Multi-Agent Orchestration for Music-to-Video Generation To tackle the complexities of music video (MV) creation, where visuals must align with rhythm, lyrics, and emotional arcs, researchers developed AutoMV Tang et al. (2025). In Figure 4. AutoMV represents shift toward collaborative multi-agent pipeline. The system employs Screenwriter Agent to draft narrative scripts based on 6 Figure 5: Schematic diagram of Vibe AIGC architecture. musical attributes (e.g., beats and structure) and Director Agent to manage shared Character Bank and coordinate with various video generation tools. This framework ensures that different segments of full-length song remain visually and stylistically consistent. The success of AutoMV underscores the necessity of modular, role-playing agentic structure in managing the high-level intent of creative project. In addition to the aforementioned works, there are numerous studies such as MotivGraph-SoIQ Lei et al. (2025), VideoAgent Wang et al. (2024), HollywoodTown Wei et al. (2025c), and LVAS-Agent Zhang et al. (2025). All preliminary efforts reveal clear trajectory: the future of AIGC lies in orchestrating specialized agents capable of reasoning, planning, and maintaining long-term consistency. However, these systems remain largely fragmented within their respective domains. This observation serves as the primary catalyst for Vibe AIGC, which seeks to unify these agentic capabilities into cohesive, intent-driven ecosystem."
        },
        {
            "title": "5 Vibe AIGC",
            "content": "While the SOP-based fixed patterns and manual orchestration modes aforementioned have, to some extent, mitigated the uncertainty of single-prompting and the rigidity of fixed workflows, they fundamentally remain constrained by tool-centric bottleneck. These paradigms necessitate deep technical expertise for tool selection and graph construction, plunging users into cognitive misalignment\": they are ensnared by the complexities of low-level technical implementation rather than focusing on core creative expression. In practice, AIGS content creation is characterized by two features: fine-grained, diversified requirements and demand for the encapsulation of technical execution. Creators intentions are often highly abstract and multifaceted, exceeding the reach of finite, static SOPs; concurrently, users prioritize intent fulfillment\" over tool scheduling.\" This supply-demand mismatch creates binary dilemma for existing methods: they either suffer from task failure due to the limited generalization of SOPs or impose excessive overhead due to the high cognitive load of manual orchestration. Given these challenges, to truly realize the User as Commander\" vision advocated by Vibe Coding within the AIGC domain, we propose novel top-level design: Vibe AIGC. This chapter details the architecture, the core of which is paradigm shift from executing preset processes\" to autonomously constructing solutions.\" Under this framework, natural language is no longer merely prompt but is compiled into meta-instructions for executable workflows. The system evolves from single-model inference engine or rigid workflow framework into self-organizing multi-agent orchestration system driven by Meta Planner, incorporating human-in-the-loop mechanisms."
        },
        {
            "title": "5.1 Top-Level Architecture",
            "content": "The high-level design of Vibe AIGC aims to establish system-level semantic entropy reduction mechanism, bridging the gap between unstructured, high-dimensional creative intent and structured, deterministic engineering implementation. In traditional paradigms, this entropy reduction process relies entirely on the user, who must manually translate requirements into tool selection and configuration. As shown in Figure 5, in contrast, the Vibe AIGC architecture externalizes this cognitive transformation by constructing closed-loop system centered on the Meta Planner, supported by Domain-Specific Expert Knowledge Base, and directed toward hierarchical workflow orchestration. At the apex of this architecture, the Meta Planner serves as the primary commander at the humancomputer interface. Rather than executing generation tasks, it is responsible for receiving natural language and translating it into global system scheduling Jiang et al. (2025b). This process transcends simple keyword matching, employing reasoning-based dynamic construction Xiong et al. (2025). To ensure professional precision, the Meta Planner interacts deeply with an external domain-specific knowledge, storing professional skills, experiential knowledge, and comprehensive registry of supported algorithmic workflows. For instance, when the Planner receives request for an oppressive atmosphere, it queries the knowledge base to deconstruct this abstract vibe\" into specific engineering constraintssuch as low-key lighting, close-ups, and low-saturation filtersthereby mitigating hallucinations and mediocrity common in general LLMs Li et al. (2025). Regarding execution logic, the high-level design adopts hierarchical orchestration strategy, mapping complex generation tasks through top-down layers of abstraction. The Meta Planner first generates macro-level SOP blueprint at the creative layer, then propagates this logic to the algorithmic layer to automatically derive and configure the workflow graph structure Qiu et al. (2025). This hierarchical design ensures the system can both comprehend the directors vision at macro level and precisely control technician operations at micro level. In essence, the top-level design of Vibe AIGC is not static toolkit but dynamic decision flow driven by the Meta Planner: it perceives the users Vibe\" in real-time, disambiguates intent via expert knowledge, and ultimately grows precise, executable workflow from the top down Xiong et al. (2025)."
        },
        {
            "title": "5.2 Meta Planner",
            "content": "As the cognitive core of the Vibe AIGC architecture, the Meta Planner assumes the critical responsibility of translating natural language intent into an executable system architecture. Departing from the paradigm of traditional Large Language Models (LLMs) that function merely as text generators or simple routers, the Meta Planner is engineered as System Architect\" endowed with high-level reasoning capabilities Goebel and Zips (2025). Positioned at the forefront of human-computer interaction, it directly interfaces with the users natural language input. Its primary function is not the immediate generation of content, but rather deep intent parsing and task decomposition. Upon receiving users Commander Instructions\"which are often ambiguous and unstructuredthe Planner identifies explicit functional requirements while simultaneously capturing latent \"Vibe\" signals, such as style, mood, and rhythm. Through multi-hop reasoning, it converts these signals into internal logical representations, thereby triggering the entire generative engine. 5."
        },
        {
            "title": "Intent Understanding",
            "content": "The reasoning depth of the Meta Planner stems from its tight synergy with the domain-expert knowledge. This process is designed to flesh out sparse, subjective user instructions into actionable and objective creative schemes, thereby addressing intent sparsity Fagnoni et al. (2025). The Planner begins by querying the creative expert knowledge modules, which encapsulate vast array of multi-disciplinary expertise. For instance, when user provides the instruction for Hitchcockian suspenseful opening,\" the Planner does not merely treat it as text prompt. Instead, it leverages the knowledge base to deconstruct the abstract concept of suspense\" into series of precise creative constraints: visually, it mandates \"dolly zoom\" camera movements and high-contrast lighting\"; auditorily, it requires \"dissonant intervals\" in the score; and narratively, it dictates an editing rhythm based on information asymmetry.\" Through this process, the Planner externalizes implicit knowledge, transforming the users subjective aesthetic intuition into objective, concrete creative scripts. Consequently, this mitigates the issues of averaging\" (mediocrity) or \"hallucination\" often found in traditional AIGC tools due to comprehension gaps Shi et al. (2025)."
        },
        {
            "title": "5.4 Agentic Orchestration",
            "content": "Upon the completion of intent expansion and disambiguation, the system enters the stage of Agentic Orchestration. The role of the Meta Planner shifts from creative director to dynamic compiler. It constructs the system by mapping the aforementioned creative scripts into specific algorithmic execution paths, based on the algorithmic workflows and tool definitions stored in the knowledge base Li et al. (2024). The Planner traverses the systems atomic tool librarywhich includes various Agents, foundation models, and media processing modulesto select the optimal ensemble of components and define their data-flow topology. It possesses adaptive reasoning for task complexity: for simple image generation, it may configure linear text-to-image pipeline; whereas for long-form video production, it autonomously assembles complex graph structure encompassing script decomposition, consistent character generation, keyframe rendering, frame interpolation, and post-production effects. Crucially, this orchestration includes the precision configuration of operational hyperparameters (e.g., sampling steps and denoising strength). Ultimately, the Meta Planner generates complete, logically verified set of executable workflow code, achieving an automated leap from natural language concepts to industrial-grade engineering implementation."
        },
        {
            "title": "6 Limitations",
            "content": "The Bitter Lesson and Model Centrality. The Intent-Execution Gap is not permanent architectural flaw, but temporary symptom of insufficient model scale Sutton (2019). If single foundational model eventually achieves near-perfect internal world model, the need for complex orchestration layer may vanish. In this view, Vibe is merely high-entropy prompt that current models cannot yet parse, but future models will execute in single shot without the overhead of multi-agent delegation. The Paradox of Control: Commander vs. Craftsman. The shift from prompt engineer to Commander assumes that users prefer high-level intent over granular manipulation. However, professional creators often require pixel-perfect control that natural language may inherently lack. Critics argue that Vibe AIGC risks introducing Black Box of Intent. By abstracting away the How, we may be trading professional precision for amateur convenience, potentially leading to homogenization of aesthetic where the AIs interpretation of vibe overrides the humans unique creative signature Flusser (2013); Benjamin (2018). The Verification Crisis: Binary Success vs. Aesthetic Subjectivity. fundamental challenge for Vibe AIGC is the absence of deterministic feedback loop. In coding, code either compiles and passes unit tests, or it fails; this verification allows LLMs to iteratively converge on ground-truth solution Xu et al. (2025). In contrast, Vibe in AIGC is inherently subjective and lacks formal specification. There is no universal unit test for cinematic atmosphere or melancholic pacing. Without an objective verification oracle, the recursive orchestration layer may drift into aesthetic hallucination and fail to meet the users unstated creative intent. Compounding Failures and the Missing Compiler. The reliance on recursive orchestration introduces significant systemic risks related to error compounding. In Coding, compiler acts as hard constraint that intercepts logical errors. However, Vibe AIGC relies on multiple agents where minor semantic drift in an upstream agent can lead to catastrophic hallucinations across the entire workflow Cemri et al. (2025). Unlike modular software, generative artifacts often suffer from content leakage or pixel misalignment that current orchestration layers cannot formally debug. Critics maintain that until an equivalent of an Aesthetic Compiler is developed, multi-agent workflows will remain fragile compass for digital construction."
        },
        {
            "title": "7 Future directions",
            "content": "For Researchers: Develop Formal Benchmarks for Intent Consistency The current reliance on metrics like FID (Jayasumana et al., 2023), CLIP, or perplexity is insufficient for the Vibe AIGC era. We call on the academic community to move beyond evaluating pixel fidelity and instead develop benchmarks that measure Agentic Logic Consistency. We need Creative Unit Tests that evaluate whether multi-agent system can successfully decompose complex, ambiguous vibe into logically sound and temporally consistent workflow across multiple modalities. For Industry Leaders: Incentivize Specialized Micro-Foundation Models The pursuit of single God-model that handles all creative tasks is architecturally inefficient for professional workflows. Industry leaders and AI labs should pivot toward training and open-sourcing specialized foundation agents. Rather than monolithic LLMs, the community needs high-performing, lightweight agents trained 9 specifically for niche creative taskssuch as Cinematography Agent grounded in film theory, or Creative Director Agent for workflow synthesis. For Software Architects: Standardize Agent Interoperability Protocols. The success of Vibe AIGC depends on an evolving ecosystem of collaborative agents. We call for the establishment of Open Agentic Interoperability Standards (e.g., an AIGC Protocol). This would allow agents from different developers to share common Character Bank, Global Style State, and Context Memory seamlessly. Without standardized communication protocols, agentic workflows will remain fragmented and closed-source. For the Data Science Community: Curate Intent-to-Workflow Datasets. Current datasets are largely composed of static image-text pairs. Realizing the Vibe AIGC era requires new class of Reasoning-inthe-Loop datasets. We need data that maps high-level creative intent to the hierarchical reasoning steps and multi-modal sub-tasks required to achieve it. This will enable the training of Meta-Planners that can think before creating."
        },
        {
            "title": "8 Conclusion",
            "content": "The generative AI community stands at crossroads where scaling laws alone can no longer bridge the gap between human imagination and machine execution. This paper has argued that the path forward lies in fundamental paradigm shift from Model-Centric Generation to Vibe AIGC, framework that reconceptualizes content creation as the autonomous synthesis of multi-agent workflows. By closing the Intent-Execution Gap and elevating the user to Commander, Vibe AIGC offers necessary solution to the usability ceiling. As we look toward the next generation of AIGC, our research focus must move beyond the internal weights of models and toward the architecture of orchestration, ensuring that the future of the digital economy is built on foundation of verifiable intent, long-term consistency, and truly collaborative human-AI creative process. References Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Neural Information Processing Systems, 2014. URL https://api.semanticscholar.org/CorpusID:261560300. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. Andrej Karpathy. Andrej karpathys website. https://karpathy.ai/, 2025. Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, et al. survey of context engineering for large language models. arXiv preprint arXiv:2507.13334, 2025. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM computing surveys, 55(9):135, 2023. Marko Horvat. What is vibe coding and when should you use it (or not)? Authorea Preprints, 2025. Gemini Code CLI. https://github.com/google-gemini/gemini-cli, 2025. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Claude 4.5 sonnet. https://www.anthropic.com/claude, 2025. Jiaheng Liu, Ken Deng, Congnan Liu, Jian Yang, Shukai Liu, He Zhu, Peng Zhao, Linzheng Chai, Yanan Wu, JinKe JinKe, Ge Zhang, Zekun Moore Wang, Guoan Zhang, Yingshui Tan, Bangyu Xiang, Zhaoxiang Zhang, Wenbo Su, and Bo Zheng. M2RC-EVAL: Massively multilingual repository-level code completion evaluation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1566115684, Vienna, Austria, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.763. URL https://aclanthology.org/2025.acl-long.763/. Ranjan Sapkota, Konstantinos Roumeliotis, and Manoj Karkee. Vibe coding vs. agentic coding: Fundamentals and practical implications of agentic ai. arXiv preprint arXiv:2505.19443, 2025. Nan Chen, Luna K. Qiu, Arran Zeyu Wang, Zilong Wang, and Yuqing Yang. Screen reader programmers in the vibe coding era: Adaptation, empowerment, and new accessibility landscape, 2025a. URL https://arxiv.org/abs/2506.13270. Yufan Dang, Cheng Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang, Xiaoyin Che, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, and Maosong Sun. Multi-agent collaboration via evolving orchestration, 2025. URL https://arxiv.org/abs/2505.19591v1. Rushi Qiang, Yuchen Zhuang, Yinghao Li, Rongzhi Zhang, Changhao Li, Ian Shu-Hei Wong, Sherry Yang, Percy Liang, Chao Zhang, Bo Dai, et al. Mle-dojo: Interactive environments for empowering llm agents in machine learning engineering. arXiv preprint arXiv:2505.07782, 2025. J. Park, Joseph C. OBrien, Carrie J. Cai, M. Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. ACM Symposium on User Interface Software and Technology, 2023. Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Menghan Xia, Xintao Wang, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025b. Yuanyang Yin, Yaqi Zhao, Mingwu Zheng, Ke Lin, Jiarong Ou, Rui Chen, Victor Shea-Jay Huang, Jiahao Wang, Xin Tao, Pengfei Wan, et al. Towards precise scaling laws for video diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1815518165, 2025. Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84288437, 2025. Xinlong Chen, Yue Ding, Weihong Lin, Jingyun Hua, Linli Yao, Yang Shi, Bozhou Li, Yuanxing Zhang, Qiang Liu, Pengfei Wan, et al. Avocado: An audiovisual video captioner driven by temporal orchestration. arXiv preprint arXiv:2510.10395, 2025b. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11): 139144, 2020. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 11 Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers, pages 111, 2025. Vivian Liu and Lydia Chilton. Design guidelines for prompt engineering text-to-image generative models. In Proceedings of the 2022 CHI conference on human factors in computing systems, pages 123, 2022. Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. Kling Team, Jialu Chen, Yuanzheng Ci, Xiangyu Du, Zipeng Feng, Kun Gai, Sainan Guo, Feng Han, Jingbin He, Kang He, et al. Kling-omni technical report. arXiv preprint arXiv:2512.16776, 2025a. Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-to-video editing tasks. arXiv preprint arXiv:2403.14468, 2024. Kling Team, Jialu Chen, Yikang Ding, Zhixue Fang, Kun Gai, Yuan Gao, Kang He, Jingyun Hua, Boyuan Jiang, Mingming Lao, et al. Klingavatar 2.0 technical report. arXiv preprint arXiv:2512.13313, 2025b. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025a. Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, and Wenhu Chen. Univideo: Unified understanding, generation, and editing for videos. arXiv preprint arXiv:2510.08377, 2025a. Haoyang He, Jie Wang, Jiangning Zhang, Zhucun Xue, Xingyuan Bu, Qiangpeng Yang, Shilei Wen, and Lei Xie. Openve-3m: large-scale high-quality dataset for instruction-guided video editing. arXiv preprint arXiv:2512.07826, 2025. Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Se norita-2m: high-quality instruction-based dataset for general video editing by video specialists. arXiv preprint arXiv:2502.06734, 2025. Yuanhao Cai, He Zhang, Xi Chen, Jinbo Xing, Yiwei Hu, Yuqian Zhou, Kai Zhang, Zhifei Zhang, Soo Ye Kim, Tianyu Wang, et al. Omnivcus: Feedforward subject-driven video customization with multimodal control conditions. arXiv preprint arXiv:2506.23361, 2025. Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, et al. Emu3. 5: Native multimodal models are world learners. arXiv preprint arXiv:2510.26583, 2025. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 12 Junyan Ye, Leiqi Zhu, Yuncheng Guo, Dongzhi Jiang, Zilong Huang, Yifan Zhang, Zhiyuan Yan, Haohuan Fu, Conghui He, and Weijia Li. Realgen: Photorealistic text-to-image generation via detector-guided rewards. arXiv preprint arXiv:2512.00473, 2025. Teng Hu, Zhentao Yu, Guozhen Zhang, Zihan Su, Zhengguang Zhou, Youliang Zhang, Yuan Zhou, Qinglin Lu, and Ran Yi. Harmony: Harmonizing audio and video generation through cross-task synergy. arXiv preprint arXiv:2511.21579, 2025. Xiaohu Huang, Hao Zhou, Qiangpeng Yang, Shilei Wen, and Kai Han. Jova: Unified multimodal learning for joint video-audio generation. arXiv preprint arXiv:2512.13677, 2025. Shian Du, Menghan Xia, Chang Liu, Quande Liu, Xintao Wang, Pengfei Wan, and Xiangyang Ji. Unimmvsr: unified multi-modal framework for cascaded video super-resolution. arXiv preprint arXiv:2510.08143, 2025. OpenAI. Introducing deep research. https://openai.com/index/introducing-deep-research/, 2025. Accessed: 2026-01-26. Qiguang Chen, Zheng Yan, Mingda Yang, Libo Qin, Yixin Yuan, Hanjing Li, Jinhao Liu, Yiyan Ji, Dengyun Peng, Jiannan Guan, Mengkang Hu, Yantao Du, and Wanxiang Che. Autopr: Lets automate your academic promotion!, 2025c. URL https://arxiv.org/abs/2510.09558. Jiazhe Wei, Ken Li, Tianyu Lao, Haofan Wang, Liang Wang, Caifeng Shan, and Chenyang Si. Postercopilot: Toward layout reasoning and controllable editing for professional graphic design, 2025b. URL https: //arxiv.org/abs/2512.04082. Xiaoxuan Tang, Xinping Lei, Chaoran Zhu, Shiyun Chen, Ruibin Yuan, Yizhi Li, Changjae Oh, Ge Zhang, Wenhao Huang, Emmanouil Benetos, Yang Liu, Jiaheng Liu, and Yinghao Ma. Automv: An automatic multi-agent system for music video generation, 2025. URL https://arxiv.org/abs/2512.12196. Xinping Lei, Tong Zhou, Yubo Chen, Kang Liu, and Jun Zhao. Motivgraph-soiq: Integrating motivational knowledge graphs and socratic dialogue for enhanced llm ideation, 2025. URL https://arxiv.org/ abs/2509.21978. Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent, 2024. URL https://arxiv.org/abs/2403.10517. Zheng Wei, Mingchen Li, Zeqian Zhang, Ruibin Yuan, Pan Hui, Huamin Qu, James Evans, Maneesh Agrawala, and Anyi Rao. Hollywood town: Long-video generation via cross-modal multi-agent orchestration, 2025c. URL https://arxiv.org/abs/2510.22431. Yehang Zhang, Xinli Xu, Xiaojie Xu, Li Liu, and Yingcong Chen. Long-video audio synthesis with multi-agent collaboration, 2025. URL https://arxiv.org/abs/2503.10719. Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael R. Lyu, and Xiangyu Yue. Screencoder: Advancing visual-to-code generation for front-end automation via modular multimodal agents, 2025b. URL https://arxiv.org/abs/2507.22827. Yiming Xiong, Jian Wang, Bing Li, Yuhan Zhu, and Yuqi Zhao. Self-organizing agent network for llm-based workflow automation, 2025. URL https://arxiv.org/abs/2508.13732. Ao Li, Yuexiang Xie, Songze Li, Fugee Tsung, Bolin Ding, and Yaliang Li. Agent-oriented planning in multi-agent systems, 2025. URL https://arxiv.org/abs/2410.02189. Libin Qiu, Yuhang Ye, Zhirong Gao, Xide Zou, Junfu Chen, Ziming Gui, Weizhi Huang, Xiaobo Xue, Wenkai Qiu, and Kun Zhao. Blueprint first, model second: framework for deterministic llm workflow, 2025. URL https://arxiv.org/abs/2508.02721. Kai Goebel and Patrik Zips. Can llm-reasoning models replace classical planning? benchmark study, 2025. URL https://arxiv.org/abs/2507.23589. Théo Fagnoni, Mahsun Altin, Chia En Chung, Phillip Kingston, Alan Tuning, Dana O. Mohamed, and Inès Adnani. Opus: prompt intention framework for complex workflow generation, 2025. URL https://arxiv.org/abs/2507.11288. Yuchen Shi, Siqi Cai, Zihan Xu, Yuei Qin, Gang Li, Hang Shao, Jiawei Chen, Deqing Yang, Ke Li, and Xing Sun. Flowagent: Achieving compliance and flexibility for workflow agents, 2025. URL https://arxiv.org/abs/2502.14345. 13 Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang. Autoflow: Automated workflow generation for large language model agents, 2024. URL https://arxiv.org/abs/2407.12821. Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. Vilém Flusser. Towards philosophy of photography. Reaktion Books, 2013. Walter Benjamin. The work of art in the age of mechanical reproduction. In museum studies approach to heritage, pages 226243. Routledge, 2018. Jingxuan Xu, Ken Deng, Weihao Li, Songwei Yu, Huaixi Tang, Haoyang Huang, Zhiyi Lai, Zizheng Zhan, Yanan Wu, Chenchen Zhang, Kepeng Lei, Yifan Yao, Xinping Lei, Wenqiang Zhu, Zongxian Feng, Han Li, Junqi Xiong, Dailin Li, Zuchen Gao, Kun Wu, Wen Xiang, Ziqi Zhan, Yuanxing Zhang, Wuxuan Gong, Ziyuan Gao, Guanxiang Wang, Yirong Xue, Mengtong Li, Mengfei Xie, Xiaojiang Zhang, Jinghui Wang, Wenhao Zhuang, Zheng Lin, Huiming Wang, Zhaoxiang Zhang, Yuqun Zhang, Haotian Zhang, Bin Chen, and Jiaheng Liu. Swe-compass: Towards unified evaluation of agentic coding abilities for large language models, 2025. URL https://arxiv.org/abs/2511.05459. Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657, 2025. Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 93079315, 2023. URL https: //api.semanticscholar.org/CorpusID:267035198."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "NJU-LINK Team, Nanjing University"
    ]
}