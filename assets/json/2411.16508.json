{
    "paper_title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages",
    "authors": [
        "Ashmal Vayani",
        "Dinura Dissanayake",
        "Hasindri Watawana",
        "Noor Ahsan",
        "Nevasini Sasikumar",
        "Omkar Thawakar",
        "Henok Biadglign Ademtew",
        "Yahya Hmaiti",
        "Amandeep Kumar",
        "Kartik Kuckreja",
        "Mykola Maslych",
        "Wafa Al Ghallabi",
        "Mihail Mihaylov",
        "Chao Qin",
        "Abdelrahman M Shaker",
        "Mike Zhang",
        "Mahardika Krisna Ihsani",
        "Amiel Esplana",
        "Monil Gokani",
        "Shachar Mirkin",
        "Harsh Singh",
        "Ashay Srivastava",
        "Endre Hamerlik",
        "Fathinah Asma Izzati",
        "Fadillah Adamsyah Maani",
        "Sebastian Cavada",
        "Jenny Chim",
        "Rohit Gupta",
        "Sanjay Manjunath",
        "Kamila Zhumakhanova",
        "Feno Heriniaina Rabevohitra",
        "Azril Amirudin",
        "Muhammad Ridzuan",
        "Daniya Kareem",
        "Ketan More",
        "Kunyang Li",
        "Pramesh Shakya",
        "Muhammad Saad",
        "Amirpouya Ghasemaghaei",
        "Amirbek Djanibekov",
        "Dilshod Azizov",
        "Branislava Jankovic",
        "Naman Bhatia",
        "Alvaro Cabrera",
        "Johan Obando-Ceron",
        "Olympiah Otieno",
        "Fabian Farestam",
        "Muztoba Rabbani",
        "Sanoojan Baliah",
        "Santosh Sanjeev",
        "Abduragim Shtanchaev",
        "Maheen Fatima",
        "Thao Nguyen",
        "Amrin Kareem",
        "Toluwani Aremu",
        "Nathan Xavier",
        "Amit Bhatkal",
        "Hawau Toyin",
        "Aman Chadha",
        "Hisham Cholakkal",
        "Rao Muhammad Anwer",
        "Michael Felsberg",
        "Jorma Laaksonen",
        "Thamar Solorio",
        "Monojit Choudhury",
        "Ivan Laptev",
        "Mubarak Shah",
        "Salman Khan",
        "Fahad Khan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing Large Multimodal Models (LMMs) generally focus on only a few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. In pursuit of culturally diverse global multimodal models, our proposed All Languages Matter Benchmark (ALM-bench) represents the largest and most comprehensive effort to date for evaluating LMMs across 100 languages. ALM-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in LMM research. The benchmark offers a robust and nuanced evaluation framework featuring various question formats, including true/false, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. ALM-bench design ensures a comprehensive assessment of a model's ability to handle varied levels of difficulty in visual and linguistic reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. Through this, ALM-bench not only provides a rigorous testing ground for state-of-the-art open and closed-source LMMs but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. Our benchmark is publicly available."
        },
        {
            "title": "Start",
            "content": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages Ashmal Vayani1, 2 , Dinura Dissanayake2 , Hasindri Watawana2 , Noor Ahsan2 , Nevasini Sasikumar2 , Omkar Thawakar2 , Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kuckreja, Mykola Maslych, Wafa Al Ghallabi, Mihail Mihaylov, Chao Qin, Abdelrahman Shaker, Mike Zhang, Mahardika Krisna Ihsani, Amiel Esplana, Monil Gokani, Shachar Mirkin, Harsh Singh, Ashay Srivastava, Endre Hamerlik, Fathinah Asma Izzati, Fadillah Adamsyah Maani, Sebastian Cavada, Jenny Chim, Rohit Gupta, Sanjay Manjunath, Kamila Zhumakhanova, Feno Heriniaina Rabevohitra, Azril Amirudin, Muhammad Ridzuan, Daniya Kareem, Ketan More, Kunyang Li, Pramesh Shakya, Muhammad Saad, Amirpouya Ghasemaghaei, Amirbek Djanibekov, Dilshod Azizov, Branislava Jankovic, Naman Bhatia, Alvaro Cabrera, Johan Obando-Ceron, Olympiah Otieno, Fabian Farestam, Muztoba Rabbani, Sanoojan Baliah, Santosh Sanjeev, Abduragim Shtanchaev, Maheen Fatima, Thao Nguyen, Amrin Kareem, Toluwani Aremu, Nathan Xavier, Amit Bhatkal, Hawau Toyin, Aman Chadha3, Hisham Cholakkal2 , Rao Muhammad Anwer2, 4 , Michael Felsberg6 , Jorma Laaksonen4 , Thamar Solorio2 , Monojit Choudhury2 , Ivan Laptev2 , Mubarak Shah1, 3 , Salman Khan2, 5 , Fahad Shahbaz Khan2, 6 Core Authors 1University of Central Florida, 2Mohamed bin Zayed University of AI, 3Amazon, 4Aalto University, 5Australian National University, 6Link√∂ping University https://mbzuai-oryx.github.io/ALM-Bench/ 4 2 0 2 5 ] . [ 1 8 0 5 6 1 . 1 1 4 2 : r Figure 1. ALM-bench comprises diverse set of 100 languages with manually verified annotations by respective native language experts. Here, qualitative examples highlight the comprehensive set of 13 cultural aspects covered in the benchmark, such as heritage, customs, architecture, literature, music, and sports. It also evaluates visual understanding for six generic aspects. The ALM-bench focuses on lowresource languages and different regions, spanning 73 countries across five continents and 24 distinct scripts. ALM-bench covers diverse questions, such as multiple choice questions (MCQs), true/false (T/F), short and long visual question answers (VQAs). 1 (a) Performance comparison of open vs. closed-source models. (b) Performance on various cultural categories. Figure 2. Benchmarking LMMs on diverse languages & cultures: (a) Performance of various open versus closed-sourced LMMs on ALM-bench. For each LMM, we also report performance on low versus high-resource languages. All these carefully selected models were released after 2023. (b) Performance of high-performing LMMs on our culturally curated 13 categories available in our ALM-bench."
        },
        {
            "title": "Abstract",
            "content": "Existing Large Multimodal Models (LMMs) generally focus on only few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. In pursuit of culturally diverse global multimodal models, our proposed All Languages Matter Benchmark (ALM-bench) represents the largest and most comprehensive effort to date for evaluating LMMs across 100 languages. ALM-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in LMM research. The benchmark offers robust and nuanced evaluation framework featuring various question formats, including true/- false, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. ALM-bench design ensures comprehensive assessment of models ability to handle varied levels of difficulty in visual and linguistic reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. Through this, ALM-bench not only provides rigorous testing ground for state-of-the-art open and closed-source LMMs but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. Our benchmark is publicly available. 1. Introduction In recent years, large multimodal models (LMMs) have seen significant progress across variety of vision-language tasks [2, 5, 10, 28, 30, 36, 41]. Despite these advances, major limitation remains in the models capacity to understand and respond accurately to cultural and linguistic diversity [37]. Current LMMs perform well in widely spoken languages with substantial training data but struggle with low-resource languages and nuanced cultural contexts, limiting their effectiveness on global scale [29]. Existing benchmarks for evaluating multilingual and multicultural capabilities in LMMs are limited in scope, often representing only small subset of cultures and languages. Such benchmarks generally focus on highresource languages and overlook the cultural richness of low-resource languages, which hinders the development and evaluation of models that can serve diverse populations across the globe. For instance, CulturalVQA [32] and GD-VCR [45] focus on culturally specific content in English and do not reflect global cultural diversity. Similarly, the Henna benchmark [4] includes cultural information from 11 countries but is restricted to the Arabic language. In terms of multilingualism, existing benchmarks [12, 15, 27, 40, 43, 48] include multilingual data but remain constrained by limited cultural diversity and are often restricted to only single question type. This gap highlights the need for more comprehensive benchmark that addresses both linguistic and cultural inclusivity in LMMs. To address these challenges, we introduce culturally and linguistically diverse multimodal benchmark: All Languages Matter Benchmark (ALM-bench). ALM-bench is specifically designed to evaluate LMMs across diverse set of 100 languages, including many low-resource ones, with strong emphasis on cultural depth. The complete list of these languages is presented in the Tab. A.3 (suppl. material). In comparison to the recent CVQA benchmark [37], ALM-bench offers 3x more languages, more diverse question types, and 2x more image domains and samples with an extensive focus on fine-grained cultural aspects  (Fig. 1)  . 2 Benchmark Multilingual # of # of Total Total Languages Scripts Domains Samples Question Types Forms Type Question Annotation Cultural Specific OpenBias Diversity Henna [4] CulturalVQA [32] GD-VCR [45] SEED-Bench [21] SEED-Bench2 [22] MMStar [9] MM-Vet [46] MMMU [47] CVQA [37] MMMB [40] MMBench [27] EXAMS-V [12] MaRVL [25] M3Exam [48] MaXM [8] xGQA [34] M4U [43] MME [15] Ours 1 1 1 1 1 1 1 1 31 6 2 11 5 9 7 8 3 2 100 1 1 1 1 1 1 1 1 13 4 2 4 3 3 5 5 2 2 5 5 1 12 34 18 16 30 10 15 20 20 11 4 - - 64 14 19 Auto+Manual Manual Manual 1132 2378 886 19242 24371 1500 218 11550 10000 12000 2974 20932 7630 12317 2142 12578 8931 2370 22763 MCQ,SVQA,LVQA,TF Diverse Auto+Manual - - Fixed Diverse Auto+Manual Diverse Auto+Manual Auto+Manual Fixed Manual Fixed Manual Fixed Manual Fixed Auto+Manual Fixed Auto+Manual Fixed Manual Diverse Fixed Manual Diverse Auto+Manual Auto+Manual Fixed Manual Fixed Auto+Manual Fixed Manual Fixed SVQA, LVQA SVQA MCQ MCQ MCQ MCQ SVQA,LVQA MCQ,SVQA MCQ TF, MCQ MCQ MCQ TF MCQ SVQA Y/N,SVQA MCQ Y/N Content Source Correction - Table 1. Comparison of various LMM benchmarks with focus on multilingual and cultural understanding. The Domains indicate the range of aspects covered by the dataset for each language. Question Form is categorized as \"Diverse\" if the questions phrasing varies, and \"Fixed\" otherwise. Annotation Types are classified as \"Manual\" if questions were originally in the local language, \"Manual+Auto\" if questions were generated or translated using GPT-4/Google API and subsequently validated by human experts, and \"Auto\" if generated or translated automatically without human validation. Bias Correction reflects whether the dataset is balanced across cultures and countries, while Diversity indicates whether the dataset includes both Western and non-Western minority cultures. - means information not available. With these enhancements, ALM-bench aims to assess the next generation of massively multilingual multimodal models in standardized way, pushing the boundaries of LMMs towards better cultural understanding and inclusivity. Our evaluations with 16 recent state-of-the-art LMMs show that existing models suffer from low-resource languages and cultures  (Fig. 2)  . The analysis also shows significant gap between the open and closed-source models in understanding cultural and linguistic nuances in multimodal setting; e.g., GPT4o performs 27% better than the best open-source model (GLM-4V). Our evaluations across scripts and language groups highlight the need to improve the understanding of underrepresented regions, such as Southeast Asia and West Africa. We also show how simple prompting techniques can help improve LMM performance. Our contributions can be summarized as follows: We introduce ALM-bench, culturally diverse multilingual and multimodal VQA benchmark covering 100 languages with 22.7K question-answers. ALM-bench encompasses 19 generic and culture-specific domains for each language, enriched with four diverse question types. ALM-bench is meticulously curated and verified with native-language experts (over 800 hours of human annotators), ensuring cultural relevance and accuracy across lowand high-resource languages alike. We benchmark existing LMMs on ALM-bench, identifying performance gaps and areas for improvement, especially in culturally complex multilingual scenarios. 2. Related Work To align large language models (LLMs) with human preferences, various approaches have been developed [35, 39, 50]. Over time, these methods are extended to enable LLMs to be sensitive to geo-diverse cultural values [1, 24]. This evolution has broadened their applicability to both multilingual and multimodal LLMs, enabling greater engagement with users from diverse linguistic and cultural backgrounds. Despite this progress, LMM benchmarks often fail to adequately address cultural aspects, especially for low-resource languages. Models such as GPT-4V [2] and Gemini 1.5 Pro [41] performs well on existing multimodal benchmarks, but these evaluations often lack in capturing the intricacies of multilingual and culturally diverse interactions. Opensource models, in particular, face challenges in accurately representing geodiverse values (e.g., local beliefs, rituals, and traditions), underscoring the need for culturally aware evaluation framework for LMMs (see Tab. 1). Several recent benchmarks have been developed to evaluate the cultural aspects of multilingual LMMs, such as, the Henna benchmark [4] representing cultural information from 11 countries with nearly 1,132 samples. However, it is restricted to the Arabic language, covering only five domains while being closed-source. Other benchmarks, such as CulturalVQA [32] and GD-VCR [45] are also smallscale and focus only on culturally specific content. These benchmarks are limited to English and struggle to encompass the full spectrum of global cultural diversity. Their samples are unevenly distributed across countries and domains, which may lead to skewed assessment. Benchmarks such as SEED-Bench [21, 22] and MMStar 3 Figure 3. Data collection and verification pipeline. Our benchmark features both cultural specific content sourced from the web (left) and generic image understanding collection sourced from existing LMM benchmark. The cultural part is carefully filtered to remove noisy samples and private information. We use GPT4o for translations which are manually verified and corrected with over 800 hours of human annotators (native speakers). Our ALM-bench has diverse question types and features approximately 23K QA pairs in total in 100 languages. [9] extend the CC3M dataset [33] to cover an extended set of domains. Despite their broader scope, these benchmarks lack cultural nuance and are constrained to multiple-choice questions (MCQ) which only assess the LMMs in one dimension. Although MM-Vet [46] and MMMU [47] offer variety of question types, they primarily address basic perceptual abilities without demanding in-depth cultural reasoning or expert-level domain knowledge. For multilingual LMM benchmarking, CVQA [37] represents recent effort, collecting cultural samples across 31 languages. However, it is limited to single question type, MCQ, and features only small proportion of samples on traditions and rituals. Similarly, MaRVL [25] covers five languages but lacks probing into cultural common sense and primarily focuses on non-Western cultural information, resulting in limited diversity. Other benchmarks, including MMMB [40], MMBench [27], Exams-V [12], M3Exam [48], M4U [43], and MME [15], do include multilingual evaluation samples. However, they are constrained by lack of cultural diversity in their datasets, cover fewer languages, and are restricted to single question type (e.g., MCQs). Our proposed benchmark, ALM-bench, bridges the aforementioned gaps (see Tab. 1) by introducing culturally diverse dataset covering Western and non-Western regions, low and high-resource languages (100 in total), and specific regional dialects (e.g., Emirati Arabic, Egyptian Arabic). ALM-bench provides robust evaluation framework for Multilingual LMMs, enabling comprehensive assessment of their generic and cultural adaptability and promoting inclusivity in multilingual multimodal AI research. 3. ALM Benchmark ALM-bench provides global coverage, featuring both generic and culture-specific image samples that preserve the nuances of native languages in VQAs  (Fig. 4)  . It includes diverse QA formats such as MCQs, True/False, and openended VQA (long and short) under 19 domains, grouped into generic and cultural categories Tab. A.1 (suppl. material). Figure 4. Data statistics of our ALM-bench showing the diversity of the scripts, global coverage, comprehensive categories, and various question types. Our dataset contains 22.7K high-quality question-answers in total, covering 100 languages and 24 scripts. All the samples are manually verified by native speakers. The generic category includes six domains: indoor and outdoor scenes, memes, paintings, food items, and sketches following [26]. The cultural category spans 13 comprehensive domainsArchitecture, Customs, Economy, Festivals, Food, Heritage, Lifestyle, Literature, Media, Music, Notable Figures, Religion, and Sportsadding depth to cultural representation across languages. While some of these domains have been previously explored [3, 31, 32, 37], they are investigated in limited languages and do not fully capture the depth of cultural nuances. Our work introduces six unique domains, including literature, music, festivals, economy, media, and notable key figures to better capture cultural richness across diverse languages. More details are in Sec. (suppl. material) ALM-bench comprises of 22,763 diverse questions across all subsets in 100 languages (see Fig. 4). The dataset spans 73 countries across five continents, capturing the cultural nuances of both underrepresented and predominant languages from different regions. We include 24 language scripts  (Fig. 7)  and 15 language families, with Latin being the most prevalent script, covering 53 languages, followed by Cyrillic with 10 languages; and Arabic with 9 languages. In terms of language families, Indo-European is the largest group, encompassing 55 languages, with Afro-Asiatic and 4 Austronesian each representing eight languages  (Fig. 9)  . 3.1. Data Collection and Annotation Country-Language pairs. Our cultural data samples are curated from the country with the most prominent presence of each specific culture. We carefully selected 100 languages across 73 countries and five continents, striking balance between lowand high-resource languages and between Western and non-Western cultures to ensure linguistic and cultural diversity. The selection criteria also consider the number of native speakers and the availability of speakers to validate and annotate the curated samples. To accurately capture the cultural nuances of each language, we follow [32] and link each language to country based on the World Values Survey [17] to cover social, ritualistic, and cultural values. For multilingual countries like India, multiple languages are associated to capture their cultural diversity. For each country-language pair, we collect cultural samples for all 19 generic and cultural domains. Generic VQA Curation. Our ALM-bench has both generic and cultural questions. To evaluate visual understanding in 100 languages on generic scenes, e.g., indoor and outdoor images, we source examples from well-known Englishonly LMM benchmark LLaVA-Bench (In-the-Wild) [26]. These English instructions are extended to the remaining 99 languages using GPT-4o followed by manual correction from native speakers. In some cases where the GPT-4o translations are poor, the annotators also translate the instructions from scratch. It results in total of 6,000 openended samples across 100 languages for generic VQA. Cultural Image Curation. To curate diverse cultural image-QA pairs, we collect open-licensed images and their corresponding metadata from the Internet, focusing on specific country-language pairs across various cultural domains. For each domain, we go through several filtration steps, e.g., removing low-resolution images and removing redundant samples to ensure diversity. To maintain highquality and cultural relevance, we ask expert native speakers of each language to manually verify the quality and cultural diversity of these images. Images lacking cultural relevance are removed from the dataset. This manual filtration process discarded nearly 10.7% of the images. To ensure privacy, we blur any personally identifiable details (PIDs) and textual information in image that may directly leak the answer. Fig. 3 shows our data collection and verification pipeline. Cultural QA Generation. ALM-bench dataset covers different question types. To generate high-quality cultural questions, we utilize GPT-4o to create QA pairs based on the provided images and their metadata. Notably, images and QAs are not shared across languages in the cultural set. For each image, we generate two MCQs, two short questions, long question, and true/false question in English. We instruct GPT-4o to emphasize the cultural concepts deFigure 5. Analysis of the type of mistakes GPT-4o made while language translations via human-feedback. GPT-4o encounters more issues with respect to semantic and grammatical errors. picted in each image, generate questions that require visual understanding of an image, and not perpetuate bias and stereotypes. To mitigate the randomness in multiple choice and true/false, we ensure that these questions can also be answered when converted to open-ended questions [37]. We also evaluate with shuffled orders, predicting the entire chosen option rather than just the leading alphabet. The detailed instructions are presented in Sec. (suppl. material.) Filtration and Translation Errors. Given the QA pairs, we translate them to remaining 99 languages using GPT4o, followed by manual correction from native speakers. Similar to image filtering, native speakers assess translation quality and make detailed corrections to provide culturally grounded answers focusing on core cultural aspects. They also discard redundant and culturally irrelevant QAs. We note several challenges in GPT-4os translations across 100 languages. Specifically, GPT-4o demonstrates difficulty in generating culturally grounded QA pairs across diverse languages. To analyze these issues, mistakes from the GPT-4o model in translations are categorized into four types: semantic error, cultural error, language error, and grammatical error. We sample 57 question-answer pairs from 51 randomly selected languages and plotted error distribution in Fig. 5. Notably, GPT-4o encounters more issues with semantic and grammatical accuracy when translating into different languages. We show examples of such errors in Fig. A.2 (suppl. material). 4. Benchmarking LMMs on ALM-bench As discussed earlier, ALM-bench comprises four different question types: MCQs, T/F, Short and Long VQA. Therefore, we employ different prompts for each type of question. We use GPT-4o as judge and prompt it to score responses of various question types with different criterion. We use accuracy for MCQs and T/F, correctness for short VQAs (SVQAs), and consistency, fluency, and relevance for long VQAs (LVQAs). Here, correctness refers to how closely the models output aligns with the ground-truth [26]. For LVQAs, the consistency assesses whether the predicted answer is coherent across the entire generated output [20]. The fluency metric measures the naturalness and readability of model 5 Figure 6. ALM-bench Performance comparison of different open and closed-sourced models (y-axis) on the 100 languages (x-axis) of our ALM-bench. The performance is represented as an average accuracy across all questions in language. The actual performance of model on language is shown in each respective box, where the higher accuracy is highlighted with high color intensity. The closed-source propriety models generally perform better across languages compared to their open-sourced counterparts. The performance on several high resource languages (e.g., English, French, Chinese and Spanish) is generally higher throughout different models, whereas all open-source models struggle on low resource languages (e.g., Amharic, Kinyarwanda, Burmese) Overall, GPT-4o and GLM-4V-9B performs better in terms of closed-source and open-source models, respectively. Best viewed zoomed in. stance, the performance of GLM-4V-9B [16] dramatically drops from 80.3% for the English language to 15.6% for the Amharic language. Fig. 2a presented earlier further shows the overall performance breakdown in terms of high-and low-resource languages across different models. It is worth mentioning that our ALM-bench comprises 50 high-resource and 50 lowresource languages following the definition of low-resource (c) Fig. 2a reveals substantial languages as in [11]. performance gap between high-resource and low-resource languages consistent across different models, with GPT4o showing 6% drop. This performance gap extends to more than 8% in case of some open-source models (e.g., GLM-4V-9B [16], Qwen2-VL [44]). notable exception is the performance of closed-source Gemini-1.5-Pro [41], which does not deteriorate significantly for low-resource languages. Next, we present further results analysis. How important is visual context? To investigate this question, we ran an experiment with only the base LLM of various LMMs used in our evaluation. We randomly sample 50 languages and prompt the respective LLM with textual question without the visual input. Our results demonstrate that the visual context is important to answer several questions in our benchmark and the LLMs struggle without input images. For instance, GPT-4o [2] performance drops by absolute 27.3% when prompted with textual questions alone. Specifically, it shows significant performance gain in languages such as Sinhala (38.7%), Sanskrit (50%), and Dutch (40%) when images are included. Similarly, Qwen27B gives 13% absolute and 24.8% relative drop in performance without image inputs. This substantial performance drop underscores the robustness of our visual benchmark, revealing that without images, even the best-performing proprietary and open-source models LLMs struggle to answer accurately. Figure 7. Performance comparison of GPT-4o and Qwen2-VL on different language scripts. The count above each bar indicates the number of languages present in that script. prediction [38, 42], whereas the relevancy metric determines whether model prediction provides answers directly related to ground truth [7]. To ensure fair evaluation for decision-making questions (T/F and MCQs) using GPT4o as judge, we also show consistent performance using LLaMA-3.8.1B-Instruct [14] in Tab. A.2 (suppl. material). Overall Results: Fig. 6 presents the per-language performance of 16 recent LMMs on the ALM-bench. The results offer number of insights: (a) the closed-source propriety models (GPT-4o [2] and Gemini-1.5-Pro [41]) generally perform better across the 100 languages, compared to their open-source counterparts. The best closed-source model, GPT-4o, achieves an overall accuracy of 78.8% compared to 51.9% overall accuracy obtained by the best performing open-source model GLM-4V-9B [16]. Both closed-source models struggle with several low-resource languages (e.g., Amharic, Kinyarwanda, Burmese, and Sanskrit). For instance, the performance of GPT-4o significantly drops from 88.4% for the English language to 50.8% for the Amharic language. (b) Several open-source models (GLM-4V-9B [16], Qwen2-VL [44], Molmo [13], LLaVAOneVision [23]) achieve comparable overall performance. Similar to their closed-source counterparts, open-source models also struggle with low-resource languages. For in6 Figure 8. Error analysis across 4 diverse language scripts, including Bengali, Sinhalese, Latin and Cyrillic on GPT-4o results, demonstrates significant challenges for even the top-performing closed-source models, particularly in cultural and reasoning comprehension. The ALM-bench highlights these gaps, especially in languages with complex dialectal variations. Models With Country Info. Without Country Info. GPT-4o Gemini-1.5-Pro GLM-4V-9B Qwen2-VL 83.57% 81.52% 56.78% 53.97% 80.96% 76.19% 56.41% 52.57% Table 2. Performance of LMMs with and without additional country location information. Proprietary models show notable performance boost of 2.6% to 5% when location-aware prompts are used, while open-source models exhibit marginal improvement. groups associated with traditional Buddhist festivals. Although GPT-4o correctly answered Hewisi and recognized its role in Buddhist festivals, the response is incorrectly written in the Sinhala language. This highlights GPT-4os language error with the Sinhalaese script, demonstrating gap in script proficiency despite grasping cultural context. Comparison across language families: We also analyze the performance of LMMs by grouping the different languages with respect to 15 families taken from Britannica1 as shown in Fig. 9. Results show that performance on several African (Atlantic-Congo) languages such as Igbo, Kinyarwanda, Shona, Swahili, and Yoruba is inferior compared to several Asian (e.g., Chinese, Korean, Vietnamese) and Western languages (e.g., English, French, German). Effect of question types: We perform this analysis on two closed-source (GPT-4o [2] and Gemini-1.5-Pro [41]) and two open-source (GLM-4V-9B [16], Qwen2-VL [44]) models, as shown in Fig. 11. Overall, we observe that all models fare better on decision-making questions (MCQs and T/F questions). Closed-source models perform better on long VQAs (LVQAs) than short VQAs (SVQA). We observe this trend to be the opposite in open-source models as they struggle to generate accurate and fluent long responses in multilingual setting with 100 languages. Are LMMs Culturally Aware? We study the cultural un1https://www.britannica.com/browse/Languages Figure 9. Performance comparison of GPT-4o and Qwen2-VL on different language families. The count above each bar indicates the number of languages present in that script. Comparison across scripts: We further group the 100 languages in ALM-bench by script [11]. We use data from Ethnologue [6] and the Glottolog database [18] on language scripts, origins, and families. This resulted in 24 distinct script groups. Fig. 7 shows GPT-4o and Qwen2-VL performance across these scripts, with both models struggling particularly on low-resource scripts such as, Geez (Amharic), Sinhalese (Sinhala), Oriya (Odia), and Myanmar (Myanmar-Burmese). While the results are not surprising as they highlight that existing models are being predominantly trained on high-resource languages, the extent of decline for low-resource scripts is noteworthy. We conduct an error analysis on the cultural examples from ALM-bench by selecting one language per script and having native speakers review GPT-4o (best-performing model) responses. Errors were categorized into six types: lack of knowledge, reasoning error, perceptual error [47], language error, translation error, and lack of cultural understanding. Fig. 8 summarizes these error types across four scripts, showing that the primary issues involve knowledge gaps, reasoning errors, and cultural misunderstandings. Fig. 10 further illustrates specific errors across scripts. One such example in Sinhala asks about the type of music group that uses two traditional drums, the Thammattama and Dhawula, which are commonly played by Hewisi music 7 Figure 10. We present the qualitative examples of the success cases in the first row and failure cases of GPT-4o in the second row on different languages & domains in ALM-bench. For the failure cases, we specify different error types. For instance, the Urdu language question asks about the festival depicted in the image. The image specifically refers to Mela Chiraghan (Festival of Lights), celebration held in honor of the Sufi saint Shah Jamals shrine. Since the decoration in the image closely resembles that of Eid Milad un Nabi another religious festivalthe model erroneously associates it with this wrong event. This constitutes lack of cultural understanding since the model fails to distinguish between the theme behind the decorations. Eid Milad un Nabi typically features more modest, reverential lighting with green lights, whereas the lighting in Mela Chiraghan is brighter and more colorful. Additionally, people typically dress for the Eid Milad un Nabi event in traditional outfit which is absent in the image. These examples highlight the models gap in cultural knowledge and its limitations in terms of accurately interpreting the cultural context of the given sample. Additional results are in the suppl. material. Impact of location-aware information in prompts: We show the performance of LMMs when evaluated with additional country-specific information in Tab. 2. We randomly select 50 languages from the ALM-bench and include country information in the prompts to assess any potential performance gains. Here, we observe that closed-source models, including GPT-4o and Gemini-1.5-Pro, better utilize the added geographic context and indicate higher cultural specificity across languages than the open-source models. 5. Ethical Consideration Data. Our cultural images in ALM-bench are sourced from the internet. Therefore, there may be potential biases in underrepresented domains. However, we believe ALM-bench represents step forward in standardized multilingualLMM evaluation, paving the way for greater inclusion of diverse cultural domains, languages, scripts, and origins. Translations. We use GPT-4o [2] for translations in 100 languages. The initial translations generated by GPT-4o undergo thorough review by native speakers to ensure linguistic accuracy and cultural relevance to maintain highquality standards across all language pairs. This combined approach enables consistent and accurate translations. Human Annotations. To ensure high-quality image filFigure 11. Performance of top-performing VLMs on ALM-bench according to different question types. Compared to open-ended questions, VLMs perform better on MCQs and T/F questions. derstanding of LMMs on our 13 domains and report results in Fig. 2b. Overall, GPT-4o achieves the best results with an overall score of 80.3%. However, we observe significant variation in performance across different cultural domains. For instance, GPT-4o achieves 83.7% on Education and heritage but drops to 72.7% on Notable key figures category. We conjecture this variation in performance likely arises since the higher-performing categories are covered in LMM training datasets, leading to better representation learning. In contrast, categories like Notable Key Figures and Customs are often culturally specific and nuanced across languages and regions. tering and language annotation, we collaborated with linguistic groups and finalized over 60 volunteers for benchmark curation. For most languages, annotators were required to be either native or bilingual speakers. However, for certain low-resource languages like Igbo, Hungarian, and Afrikaans, proficiency was also acceptable. Annotators were required to be familiar with the cultural context of the specific country-language pair they were working with. Demographic details about the expert annotators are provided in Sec. (suppl. material). Among them, 80.3% are native speakers, and 87.9% have lived for over 15 years in countries where the target language is spoken, thereby ensuring deeper cultural insight. Furthermore, 46.7% of annotators both male and female, fall within the 1825 age bracket. 6. Conclusion In this paper, we introduce ALM-bench, novel multilingual multimodal cultural benchmark for evaluation with over 22.7k humanly verified samples across 19 domains. Our benchmark encompasses cultural nuances from 73 countries in 24 language scripts and 15 language families. We conduct empirical analysis on 16 vision-language models with various question types (MCQs, T/F, SVQA, and LVQAs) and highlight notable disparities in their performance. the The performance difference between bestand the proprietary performing open-source model model, GPT-4o, is 27%. Our results also highlight that the models perform superior on predominant language scripts such as Latin, Cyrillic, and Devanagari and under-performs on underrepresented scripts such as Geez, Lao, Sinhalese, and Oriya. Moreover, cultural understanding of prominent language families such as Indo-European, Austronesian and Afro-Asiatic are well represented by GPT-4o as compared to Atlantic-Congo and Turkic families. Our work highlights the limitations of state-of-the-art LMMs in multilingual and multicultural settings, showing key areas for improvement. 7. Acknowledgment We would like to thank Tafar Mab for providing highquality annotations for Afrikaans and Kinyarwanda languages, Georgios Ioannides for verifying Greek language, and Feno Heriniaina Rabevohitra for annotating Malagasy language. We also thank Yuhao Li for reviewing and assisting with the annotations for Chinese Simplified language. Finally, we extend our gratitude to the anonymous reviewers for their invaluable suggestions and feedback, which helped improve this paper."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi, Hassan Naderi, and Behrouz Minaei Bidgoli. Persianllama: Towards building first persian large language model. arXiv preprint arXiv:2312.15713, 2023. 3 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 3, 6, 7, 8 [3] Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Alham Fikri Aji, Jacki ONeill, Ashutosh Modi, and Monojit Choudhury. Towards measuring and modeling\" culture\" in llms: survey. arXiv preprint arXiv:2403.15412, 2024. 4, 12 [4] Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia, Abdelrahman Mohamed, and Muhammad AbdulPeacock: family of arabic multimodal Mageed. arXiv preprint large language models and benchmarks. arXiv:2403.01031, 2024. 2, 3 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2 [6] Lyle Campbell and Ver√≥nica Grondona. Ethnologue: Languages of the world. Language, 84(3):636641, 2008. 7 [7] Souradip Chakraborty, Soumya Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit Singh Bedi, and Furong Huang. Transfer star: Principled decoding for llm alignment, 2024. 6 [8] Soravit Changpinyo, Linting Xue, Idan Szpektor, Ashish Thapliyal, Julien Amelot, Michal Yarom, Xi Chen, and Radu Soricut. Maxm: Towards multilingual visual question answering. arXiv preprint arXiv:2209.05401, 2022. [9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 3, 4 [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 2 [11] Marta Costa-juss√†, James Cross, Onur √áelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. 6, 7 [12] Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, and Preslav Nakov. Exams-v: multi-discipline multilingual multimodal exam benchmark for evaluating vision language models. arXiv preprint arXiv:2403.10378, 2024. 2, 3, 4 [13] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, 9 Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris CallisonBurch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models, 2024. [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 6 [15] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. 2, 3, 4 [16] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. 6, 7 [17] Haerpfer, Inglehart, Moreno, Welzel, Kizilova, Diez-Medrano, Lagos, Norris, Ponarin, and Puranen. World values survey: Round seven-country-pooled datafile version 3.0. jd systems institute & wvsa secretariat, 2022. 5 [18] Harald Hammarstr√∂m, Robert Forkel, Martin Haspelmath, and Sebastian Bank. Glottolog database 4.6, 2022. 7 [19] Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Boda Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, et al. Arabicmmlu: Assessing massive multitask language understanding in arabic. arXiv preprint arXiv:2402.12840, 2024. 15 [20] Wojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Neural text summarization: critical evaluation, 2019. [21] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 3 [22] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 3 [23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 6 [24] Yen-Ting Lin and Yun-Nung Chen. Taiwan llm: Bridging the linguistic divide with culturally aligned language model. arXiv preprint arXiv:2311.17487, 2023. 3 [25] Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. Visually grounded reasoning across languages and cultures. arXiv preprint arXiv:2109.13238, 2021. 3, 4 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 4, 5, [27] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 2, 3, 4 [28] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. 2 [29] Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao Anwer, Tim Baldwin, Michael Felsberg, and Fahad Khan. Palo: polyglot large multimodal model for 5b people. arXiv preprint arXiv:2402.14818, 2024. 2 [30] Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, and Fahad Shahbaz Khan. Vurf: generalpurpose reasoning and self-refinement framework for video understanding. arXiv preprint arXiv:2403.14743, 2024. 2 [31] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering In Proceedings benchmark requiring external knowledge. of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. 4, 12 [32] Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stanczak, and Aishwarya Agrawal. Benchmarking vision language models for cultural understanding. arXiv preprint arXiv:2407.10920, 2024. 2, 3, 4, [33] Edwin G. Ng, Bo Pang, Piyush Sharma, and Radu Soricut. Understanding guided image captioning performance across domains. arXiv preprint arXiv:2012.02339, 2020. 4 [34] Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, JanIvan Vulic, and Iryna Martin Steitz, Stefan Roth, Gurevych. xgqa: Cross-lingual visual question answering. arXiv preprint arXiv:2109.06082, 2021. 3 [35] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 3 [36] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. The Glamm: Pixel grounding large multimodal model. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2 10 age classifiers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10906 10915, 2024. [50] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 3 [37] David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. Cvqa: Culturally-diverse multilingual visual question answering benchmark. arXiv preprint arXiv:2406.05967, 2024. 2, 3, 4, 5, 12 [38] Ananya Sai, Akash Kumar Mohankumar, and Mitesh Khapra. survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR), 55(2):139, 2022. 6 [39] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3 [40] Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, DeChuan Zhan, et al. Parrot: Multilingual visual instruction tuning. arXiv preprint arXiv:2406.02539, 2024. 2, 3, 4 [41] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 3, 6, 7 [42] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao Anwer, Michael Felsberg, Tim Baldwin, Eric Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840, 2024. 6 [43] Hongyu Wang, Jiayu Xu, Senwei Xie, Ruiping Wang, Jialin Li, Zhaojie Xie, Bin Zhang, Chuyan Xiong, and Xilin Chen. M4u: Evaluating multilingual understanding and arXiv preprint reasoning for large multimodal models. arXiv:2405.15638, 2024. 2, 3, [44] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6, 7 [45] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860, 2021. 2, 3 [46] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 3, 4 [47] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 3, 4, 7 [48] Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3exam: multilingual, multimodal, multilevel benchmark for examining large language models. Advances in Neural Information Processing Systems, 36:54845505, 2023. 2, 3, 4 [49] Zeliang Zhang, Mingqian Feng, Zhiheng Li, and Chenliang Xu. Discover and mitigate multiple biased subgroups in im-"
        },
        {
            "title": "Appendix",
            "content": "A. ALM-Bench Categories Our ALM-bench benchmark dataset consists of 19 categories. Among these categories, we include Food, Lifestyle, Religion, Architecture, Sports, Customs and Heritage from [31, 37] with some minor modifications to it. We further include six additional categories including Literature, featuring prominent authors, poets, and their notable works; Music, showcasing traditional music and dance through visual samples; Festivals, covering major cultural celebrations; Economy, representing local industries and businesses; Media, highlighting cultural icons, entertainment figures, and popular TV shows; and Notable Key Figures, featuring influential historical leaders who serve as representatives for country-language association. Table. A. A.1 entails all our ALM-bench categories. Following [3], we adopt an approach to group cultural attributes based on the country for each language. Additionally, we group culturally grounded elements representing shared knowledge, values, and objectives among the people in that culture that are collectively understood. We curated culturally diverse dataset by collecting data from many cultural aspects (see Fig. A.1) and manually curated the caption for each image from web-sources. We also show the most frequent words from these categories in Fig. A.15. ALM-bench Categories 1. Indoor 2. Outdoor 3. Food Items 4. Memes 5. Painting 6. Sketch 7. Food 8. Lifestyle 9. Religion 10. Literature 11. Music 12. Customs 13. Festivals 14. Heritage 15. Economy 16. Media 17. Architecture 18. Sports 19. Notable Key Figures Table A.1. The 19 categories present in our ALM-bench dataset. Note that Food Items is sourced from generic categories while the Food subset is sourced from the cultural category from our ALMbench. 12 Figure A.1. breakdown of the cultural categories from our ALM-bench is depicted. We ensure consistent samples across all subsets, except for the Economy category, where culturally unique images were challenging to find. These visual and questionanswer samples are verified and filtered by the native speakers, removing any culturally irrelevant and redundant information. B. Annotators Demographics Our ALM-bench is the most comprehensive benchmark to date, capturing cultural nuances across 100 languages. Since building such large-scale datasets is challenging, especially due to the limited availability of high-quality native language samples, we enlisted over 60 volunteers to provide expert feedback on our curated examples. In addition to our in-house team, we collaborated with multilingual research communities to bring in external volunteers. These volunteers, representing 50 different countries, bring deep understanding of their languages cultural elements and insights unique to their country-language pairs. Each annotator was given detailed instructions along with examples to guide them in curating high-quality datasets. We also hosted UI interface as well as gave them the option to directly use the dataset. An example of our annotation platform is shown in Fig. A.4. Annotators who made substantial contributions are recognized as co-authors of this paper to support high-quality language verification. We summarize the annotator demographics in Fig. A.3. To summarize, we show the Age and Gender distribution of our volunteers (top-left) indicating the presence of onefourth of Female annotators and over 46% fall in the 1825 age limit. We show their levels of language proficiency (top-right). Notably, over 80% of the annotators were either native speakers or bilingual, with an additional 14.8% at Figure A.3. top-left. depicts the Age and Gender distribution of our volunteers. We have one-fourth of the Female speakers and it is also shown that almost 48.3% of our volunteers fall in the top-right. depicts the volunteers native age-bracket of 18-25. language proficiency in years. bottom-left. depicts the volunteers Cultural Literacy proficiency in years. bottom-right. depicts the volunteers native language proficiency in years. vance of questions generated for cultural domains and for reviewing the QA translations across both general and cultural domains. Additionally, we use GPT-4o as judge to score the response and assess the accuracy of answers for cultural category images. This process is detailed further in Sec. and K. Each annotator is provided access to dedicated verification platform that guides them through the list of QA pairs for the selected language (examples are presented in Sec D). Each QA pair includes the following components: Image, Category, English Question, English Answer, Translated Question, and Translated Answer. Contributors are required to follow the two steps outlined below. Step 1 (Question Relevance): Verify that the question is phrased such that it cannot be answered without the use of Vision-Language Model (VLM) analyzing the image. For example, question like Where is Aura Church located? can be answered without any visual input. more appropriate question would be, Where is the church shown in the image located? Step 2 (Translation Verification): If the translated question or answer is incorrect, provide the correct version in the designated space and classify the error into one of the following four categories. Figure A.2. Qualitative examples of various mistakes in GPT-4o translations including semantic, cultural, language and grammatical errors. We employ expert human-feedback to rewrite the correct translations for all samples in our ALM-bench dataset. proficient level in their chosen language. This high level of expertise ensures that our cultural samples have been meticulously reviewed, filtered, and refined by knowledgeable volunteers. Additionally, we pre-select annotators who are accustomed to the cultural values of their languages. Moreover, (bottom-left) depicts that over 88.5% are culturally familiar with their languages, and bottom-right highlights that 87.9% of the annotators have lived in the country where we have crafted the cultural samples, making them ideal participants for participating in this research effort. C. Guidelines for Data Verification For each country-language pair (e.g., Sri Lanka-Sinhala), QA pairs for general domains are created by translating the original English QA pairs from LLaVA-Bench (In-the-Wild) dataset [26]. For cultural domains, we curate the dataset by collecting images from online sources using targeted keywords (e.g., country name, language name, cultural category) for each language. We also fetch meta-data along with the image, including image caption, image size, and image license. We use such information for post-processing to keep only high-quality images. Then, we perform manual filtering and blurring of personally identifiable data (PID). We discuss this step in detail in Sec. F. The cleaned images are used to generate and translate QA pairs using GPT-4o. To ensure quality, contributors with expertise in the respective native languages manually verify the cultural relevance of images, as well as the accuracy, relevance, and consistent translation of the generated QAs. This section outlines the instructions provided to contributors for verifying the rele-"
        },
        {
            "title": "Cultural Domain Descriptions",
            "content": "Semantic error: Translation hasnt captured the semantic meaning Food: Specific Dishes, Dining Dishes Lifestyle: Daily Attire, Daily Life, Modern Cultural error: Correct but unusual words un-"
        },
        {
            "title": "Lifestyle",
            "content": "common in local language context are used Language error: Translation is provided using characters from different alphabet or most commonly not translated and still given in English Grammatical error: Grammar-related errors present D. Verification Platform and UI Examples Figure A.4. User interface for translation verification hosted on Gradio, allowing contributors to classify incorrect entries and provide accurate translations. E. Image Web Searching We automated the process of web scraping for collecting cultural images. custom URL, embedded with our encoded search query and licensing information, is passed to the Google Search Engine, and images are downloaded from the retrieved links. For each specific cultural domain within given country-language pair, the search query is constructed as {language} {cultural domain description} in {country}. For privacy and copyright issues, our benchmark only includes images that are either in the public domain or licensed under the Creative Commons license. 14 Literature: Famous literature, authors and poets Music: Traditional music and dance Religion: Major religions and religious festivals Customs: Social etiquette and traditional greetings Festivals: Important cultural festivals and celebrations Heritage: Popular historical heritage sites and iconic landmarks Economy: Major economic industries Media: Iconic Entertainment Figures and Popular TV Shows, Movies Architecture: Traditional Art and Architecture Sports: Famous traditional sports Notable key figures: Famous historical leaders F. Blurring the PIDs Recent research highlights the importance of blurring personally identifiable details (PIDs) in image datasets to mitigate privacy risks and reduce bias [49]. Thus, we filter the collected cultural images for cultural relevance, and faces are blurred using face detection model, except for the Media and Notable Key Figures categories where we have images of public figures or celebrities. The blurred images are then manually reviewed to identify any discrepancies, such as blurring personally identifiable data (PIDs) and removing watermark images, and any remaining issues are addressed when using the PicdeFacer tool. G. Instruction Prompt for QA Generation Fig. A.5 illustrates the prompt messages used to guide GPT-4o in generating culturally relevant QA pairs. Each prompt includes an image, its manually added caption, and the search query used during image scraping. The model is instructed to generate two short QA pairs, two multiplechoice QA pairs, one true/false question, and one long QA pair. Detailed instructions ensure that the generated QAs are culturally grounded and specific to the image, avoiding answers based solely on common knowledge. H. Instruction Prompt for QA Translations After the curation of question-answer pairs using GPT-4o, we use the same open-source model to translate the English QA pairs into the native languages. We show the prompt that we used for this task in Fig. A.6. Figure A.5. Prompts used for generation of cultural QA pairs. search_query refers to the query used to search the image online. It includes language, country and cultural category details. caption is manually added textual description specific to the image. cultural category indicates the domain to which the image belongs, selected from the 13 cultural domains in our ALM-bench dataset. Figure A.6. Prompts used for translating cultural QAs using GPT4o.English_input refers to either the English question or answer to be translated into the target_lang, the desired target language. I. Instruction Prompt for LMM Answer Generation We conduct comprehensive study by evaluating various state-of-the-art Large Multimodal Models (LMMs), including both open-weight and closed-weight models, on our ALM-bench benchmark. We used different prompts for each question type as shown in Fig. A.7. We prompt all questions with English system instruction as highlighted by [19] that the use of prompts in English results in the best performance. Finally, we score each models generated answer with the human-annotated ground truth answers through scoring system to assess each models performance on the benchmark. Figure A.7. Prompts used to generate answers for cultural questions using multiple Large Multimodal Models evaluated in our study. Different prompts are designed for different question types. question refers to the cultural question associated with the given image, previously generated using GPT-4o. choices represents the four options provided in multiple choice question type, and target_language is the desired local language for the response. 4o Score. The Predicted Answer column records the response generated by GPT-4o when presented with the Question in the native language. The GPT-4o Score column reflects the evaluation score assigned by GPT-4o, acting as judge, based on the comparison between the Predicted Answer and the Ground Truth Answer. Each annotator is required to complete two additional columns: (1) Is the GPT Score Justified? with binary response Yes/No, and (2) if the GPT-4o score is not justified, the Reason for GPT-4o Failure column, where they select an appropriate reason from predefined dropdown menu provided below. Reason for GPT-4o Failure Lack of Knowledge: The model fails to find an answer to the question. Lack of Cultural Understanding: The model fails to understand the cultural aspect of the answer. Language Error: Some words in the GPT-4os output are wrong for the language. Reasoning Error: The reason it gave does not match that language. Translation Error: few native words were not properly translated. Perceptual Error: The model fails to understand where certain entity is (for eg, the top of the image, the bottom of the image, etc). L. Qualitative examples with different question types Next, we present some qualitative examples showing various question types for each image sample from our dataset and in Figs. A.9, A.10, A.11, A.11, A.13, and A.14. J. Prompts for GPT-Scoring Figure A.8. Prompts used to generate score between 0 and 10, with GPT-4o acting as the evaluator to compare an LMMs predicted answers against ground truth answers. The terms question, ground_truth, and predicted_answer refer to the cultural question, the ground truth answer generated by GPT-4o and verified by experts, and the models predicted answer, respectively. In addition to using GPT-4o as judge in scoring the predicted answers, we also do scoring with multiple other LMMs to ensure fair evaluation. Results are reported in Table. A. A.2 Model Name GPT-4o Gemini-1.5 Pro GLM-4V-9B GPT-4o 90.16% 80.21% 71.35% Llama-3.1-8B-Instruct 90.34% 80.65% 73.89% Table A.2. We evaluated decision-making questions (both True/- False and multiple-choice) across sample of 20 randomly selected languages using the LLama-3.1-8B-Instruct model. This assessment aims to ensure consistency in performance on T/F and MCQs when scored using GPT-4o as judge. K. Guidelines for Error Analysis on GPT-4o"
        },
        {
            "title": "Output",
            "content": "Each annotator is provided with an Excel file specific to given country-language pair, containing the following columns: Image URL, Question, Ground Truth Answer, Predicted Answer (all in the native language), and GPT16 Figure A.9. qualitative examples of various question types in Urdu and Chinese Simplified Language. 17 Figure A.10. qualitative examples of various question types in Hindi and Sinhala Language. Figure A.11. qualitative examples of various question types in Italian and German Language. 19 Figure A.12. qualitative examples of various question types in Spanish and Emirati Arabic Language. 20 Figure A.13. qualitative examples of various question types in Saudi Arabic and Afrikaans Language. Figure A.14. Some more qualitative examples of various question types from our benchmark. 22 Figure A.15. Word clouds depicting prominent concepts from 19 categories in our ALM-bench. For intriguing results, we plot and demonstrate the results on English samples of the plot representing both the cultural and generic elements for the entire 100 languages. 23 S#No Language Country Script Family Subgrouping Specification 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 Afrikaans Albanian Amharic Armenian Assamese Azerbaijani Basque Belarusian Bengali Bhojpuri Bosnian Bulgarian Catalan Cebuano Chinese Simplified Chinese Traditional Croatian Czech Danish Dutch Egyptian Arabic Emirati Arabic English Estonian Filipino Finish French Galician Georgian German Greek Gujarati Hausa Hawaiian Hebrew Hindi Hungarian Icelandic Igbo Indonesian Irish Italian Japanese Javanese Kannada Kazakh Kinyarwanda Korean Kurdish Kyrgyz South Africa Albania Ethiopia Armenia India Azerbaijan Spain Belarus Bangladesh India Bosnia Bulgaria Spain Philippines China Hong Kong Croatia Czech Republic Denmark Netherlands Egypt United Arab Emirates United Kingdom Estonia Philippines Finland France Spain Georgia Germany Greece India Nigeria United States Israel India Hungary Iceland Nigeria Indonesia Ireland Italy Japan Indonesia India Kazakhstan Rwanda South Korea Turkey Kyrgyzstan Indo-European Indo-European Afro-Asiatic Indo-European Indo-European Turkic Isolate Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Austronesian Sino-Tibetan Sino-Tibetan Indo-European Indo-European Indo-European Indo-European Afro-Asiatic Afro-Asiatic Indo-European Uralic Austronesian Uralic Indo-European Indo-European Kartvelian Indo-European Indo-European Indo-European Afro-Asiatic Austronesian Afro-Asiatic Indo-European Uralic Indo-European Atlantic-Congo Austronesian Indo-European Indo-European Japonic Austronesian Dravidian Turkic Atlantic-Congo Koreanic Indo-European Turkic Germanic Albanian Semitic Armenic Indo-Aryan Common Turkic Balto-Slavic Indo-Aryan Indo-Aryan Balto-Slavic Balto-Slavic Italic Malayo-Polynesian Sinitic Sinitic Balto-Slavic Balto-Slavic Germanic Germanic Semitic Semitic Germanic Finnic Malayo-Polynesian Finnic Italic Italic Kartvelian Germanic Graeco-Phrygian Indo-Aryan Chadic Malayo-Polynesian Semitic Indo-Aryan Germanic Benue-Congo Malayo-Polynesian Celtic Italic Japanese Ryukyuan Malayo-Polynesian South Dravidian Common Turkic Benue-Congo Korean Iranian Common Turkic High High Low Low Low Low High Low High Low High High High Low High High High High High High Low High High High Low High High Low Low High High Low Low Low High High High High Low High Low High High Low Low High Low High Low Low Latin Latin Geez Armenian Bengali Latin Latin Cyrillic Bengali Devanagari Latin Cyrillic Latin Latin Chinese Chinese Latin Latin Latin Latin Arabic Arabic Latin Latin Latin Latin Latin Latin Georgian Latin Greek Gujarati Latin Latin Hebrew Devanagari Latin Latin Latin Latin Latin Latin Kanji/Kana Latin Kannada Cyrillic Latin Hangul Arabic Cyrillic 24 S#No Language Country Script Family Subgrouping Specification 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Lao Thailand Latin Vatican City Latvian Latvia Lithuanian Lithuania Luxembourgish Luxembourg Macedonian North Macedonia Malagasy Madagascar Malay Malaysia Malayalam India Maltese Malta Marathi India Mongolia Mongolian Myanmar (Burmese) Myanmar Nepali Norwegian Odia (Oriya) Pashto Persian Polish Portuguese Punjabi Romanian Russian Sanskrit Saudi Arabic Scots Gaelic Serbian Shona Sindhi Sinhala Slovak Slovenian Somali Spanish Sundanese Swahili Swedish Tajik Tamil Telugu Thai Turkish Ukrainian Urdu Uyghur Uzbek Vietnamese Welsh Yiddish Yoruba Nepal Norway India Pakistan Iran Poland Portugal Pakistan Romania Russia India Saudi Arabia Scotland Serbia Zimbabwe Pakistan Sri Lanka Slovakia Slovenia Somalia Spain Indonesia Tanzania Sweden Tajikistan India India Thailand Turkey Ukraine Pakistan China Uzbekistan Vietnam United Kingdom Israel Nigeria Lao Latin Latin Latin Latin Cyrillic Latin Latin Malayalam Latin Devanagari Cyrillic Myanmar Devanagari Latin Oriya Arabic Arabic Latin Latin Gurmukhi Latin Cyrillic Devanagari Arabic Latin Cyrillic Latin Arabic Sinhala Latin Latin Latin Latin Latin Latin Latin Cyrillic Tamil Telugu Thai Latin Cyrillic Arabic Arabic Latin Latin Latin Hebrew Latin Kra-Dai Italic Balto-Slavic Balto-Slavic Germanic Balto-Slavic Malayo-Polynesian Malayo-Polynesian South Dravidian Semitic Indo-Aryan Tai-Kadai Indo-European Indo-European Indo-European Indo-European Indo-European Austronesian Austronesian Dravidian Afro-Asiatic Indo-European Mongolic-Khitan Mongolic Sino-Tibetan Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Afro-Asiatic Indo-European Indo-European Atlantic-Congo Indo-European Indo-European Indo-European Indo-European Afro-Asiatic Indo-European Austronesian Atlantic-Congo Indo-European Indo-European Dravidian Dravidian Tai-Kadai Turkic Indo-European Indo-European Turkic Turkic Austroasiatic Indo-European Indo-European Atlantic-Congo Burmo-Qiangic Indo-Aryan Germanic Indo-Aryan Iranian Iranian Balto-Slavic Italic Indo-Aryan Italic Balto-Slavic Indo-Aryan Semitic Celtic Balto-Slavic Benue-Congo Indo-Aryan Indo-Aryan Balto-Slavic Balto-Slavic Cushitic Italic Malayo-Polynesian Benue-Congo Germanic Iranian South Dravidian South Dravidian Kam-Tai Common Turkic Balto-Slavic Indo-Aryan Common Turkic Common Turkic Vietic Celtic Germanic Benue-Congo Low Low High High Low High Low High Low High Low Low Low Low Low Low Low High High High Low High High Low High Low Low Low Low Low High High Low High Low High High Low Low Low High High High Low Low High High Low Low Low Table A.3. comprehensive list of 100 languages, their associated country, language scripts, families, subgrouping, and the resource specification."
        }
    ],
    "affiliations": [
        "Aalto University",
        "Amazon",
        "Australian National University",
        "Link√∂ping University",
        "Mohamed bin Zayed University of AI",
        "University of Central Florida"
    ]
}