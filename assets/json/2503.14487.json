{
    "paper_title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
    "authors": [
        "Minglei Shi",
        "Ziyang Yuan",
        "Haotian Yang",
        "Xintao Wang",
        "Mingwu Zheng",
        "Xin Tao",
        "Wenliang Zhao",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that enables experts to access global token distributions during training, promoting specialized expert behavior. To unleash the full potential of the diffusion process, DiffMoE incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. Project Page: https://shiml20.github.io/DiffMoE/"
        },
        {
            "title": "Start",
            "content": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Minglei Shi 1 * Ziyang Yuan 1 * Haotian Yang 2 Xintao Wang 2 Mingwu Zheng 2 Xin Tao 2 Wenliang Zhao 1 Wenzhao Zheng 1 Jie Zhou 1 Jiwen Lu 1 Pengfei Wan 2 Di Zhang 2 Kun Gai 2 Project Page: https://shiml20.github.io/DiffMoE/ Abstract Diffusion Transformers (DiTs) have emerged as the dominant architecture for visual generation tasks, yet their uniform processing of inputs across varying conditions and noise levels fails to leverage the inherent heterogeneity of the diffusion process. While recent mixture-of-experts (MoE) approaches attempt to address this limitation, they struggle to achieve significant improvements due to their restricted token accessibility and fixed computational patterns. We present DiffMoE, novel MoE-based architecture that enables experts to access global token distributions through batch-level global token pool during training, promoting specialized expert behavior. To unleash the full potential of inherent heterogeneity, DiffMoE incorporates capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3 activated parameters and existing MoE approaches while maintaining 1 activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. 5 2 0 2 M 8 1 ] . [ 1 7 8 4 4 1 . 3 0 5 2 : r 1. Introduction The Mixture-of-Experts (MoE) framework (Shazeer et al., 2017; Lepikhin et al., 2020) has emerged as powerful paradigm for enhancing overall multi-task performance while maintaining computational efficiency. This *Equal contribution 1Tsinghua University, Beijing, China 2Kuaishou Technology, Beijing, China. Correspondence to: Xintao Wang <xintao.wang@kuaishou.com>, Jiwen Lu <lujiwen@tsinghua.edu.cn>. Figure 1: Token Accessibility and Dynamic Computation. (a) Token accessibility levels from token isolation to crosssample interaction. Colors represent tokens in different samples, ti indicates noise levels. (b) Performance-accessibility analysis across architectures. (c) Computational dynamics during diffusion sampling, showing adaptive computation from noise to image. (d) Class-wise computation allocation from hard (technical diagrams) to easy (natural photos) tasks. Results from DiffMoE-L-E16-Flow (700K). is achieved by combining multiple expert networks, each focusing on distinct task, with their outputs integrated through gating mechanism. In language modeling, MoE has achieved performance comparable to dense models of 2 3 activated parameters (DeepSeek-AI et al., 2024; MiniMax et al., 2025; Muennighoff et al., 2024). The current MoE primarily follows two gating paradigms: TokenChoice (TC), where each token independently selects subset of experts for processing; and Expert-Choice (EC), where each expert selects subset of tokens from the sequence for processing. Diffusion (Ho et al., 2020; Rombach et al., 2022; Podell et al., 2023; Song et al., 2021) and flow-based (Ma et al., 2024; Esser et al., 2024b; Liu et al., 2023) models inherently represent multi-task learning frameworks, as they process varying token distributions across different noise levels and conditional inputs. While this heterogeneity characteristic 1 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers naturally aligns with the MoE frameworks ability for multitask handling, existing attempts (Fei et al., 2024; Sun et al., 2024a; Yatharth Gupta, 2024; Sehwag et al., 2024) to integrate MoE with diffusion models have yielded suboptimal results, failing to achieve the remarkable improvements observed in language models. Specifically, Token-choice MoE (TC-MoE) (Fei et al., 2024) often underperforms compared to conventional dense architectures under the same number of activations; Expert-choice MoE (EC-MoE) (Sehwag et al., 2024; Sun et al., 2024a) shows marginal improvements over dense models, but only when trained for much longer. We are curious about what fundamentally limits MoEs effectiveness in diffusion models. Our key finding reveals that global token distribution accessibility is crucial for MoE success in diffusion models, necessitating the model learn and dynamically process the tokens from different noise levels and conditions, as illustrated in Figure 1. Previous approaches have neglected this crucial component, resulting in compromised performance. Specifically, Dense models and TC-MoE isolates tokens, preventing them from interacting with others during expert selection, while EC-DiT restricts intra-sample token interaction , which fails to access other samples with different noise levels and conditions. These limitations hinder the models ability to capture the full spectrum of the heterogeneity inherent in diffusion processes. To address these limitations, we introduce DiffMoE, novel architecture that features batch-level global token pool for enhanced cross-sample token interaction during training, as illustrated in Figure 2. This approach approximates the complete token distribution across different noise levels and samples, facilitating more specialized expert learning through comprehensive global token information access. Our empirical analysis demonstrates that the global token pool accelerates loss convergence, surpassing dense models with equivalent activation parameters. Though some concurrent works in large language models (DeepSeek-AI et al., 2024; Qiu et al., 2025) show similar principles in global batch, we argue that these principles are particularly crucial for diffusion transformers due to their inherently more complex heterogeneity nature. However, conventional MoE inference strategies, which maintain fixed computational resource allocation across different noise levels and conditions, fail to fully leverage the potential of DiffMoEs batch-level global token pool. To optimize token selection during inference, we propose capacity predictor that dynamically adjusts resource allocation. This adaptive mechanism learns from training-time token routing patterns, efficiently distributing computational resources between complex and simple cases. Furthermore, we implement dynamic threshold at inference time to achieve flexible performance-computation trade-offs. By integrating the global token pool and capacity predictor, DiffMoE achieves superior performance over dense models with 3 activated parameters while maintaining efficient scaling properties (See Table 5). Our approach offers extra several advantages over existing methods: it eliminates the potentially detrimental load balancing losses present in TC-MoE and overcomes the intra-sample token selection constraints of EC-MoE, resulting in enhanced flexibility and scalability. Extensive empirical evaluations demonstrate DiffMoEs superior scaling efficiency and performance improvements across diverse diffusion applications. Our contributions can be summarized as follows: (1) We identify the fundamental importance of global token distribution accessibility in facilitating dynamic token selection for MoE-based diffusion models; (2) We introduce DiffMoE, novel framework incorporating global token pool and capacity predictor to enable efficient model scaling; (3) We achieve state-of-the-art performance on ImageNet benchmark among diffusion models. through dynamic computation allocation while preserving computational efficiency; and (4) We conduct comprehensive experiments that validate our approachs effectiveness across diverse diffusion applications. 2. Method 2.1. Preliminaries Diffusion Models. Diffusion models (Ho et al., 2020; Rombach et al., 2022; Sohl-Dickstein et al., 2015; Song et al., 2021) are powerful family of generative models, which can transform the noise distribution p1(x) to the data distribution p0(x). The diffusion process can be represented as: xt = αtx0 + σtϵ, ϵ (0, I), Where αt and σt are monotonically decreasing and increasing functions of t, respectively. The marginal distribution p1(x) converges to (0, I), when α1 = σ0 = 0, α0 = σ1 = 1. [0, 1], To train diffusion model, we can use the denoising score matching method (Song et al., 2021) which constructs score prediction model ϵθ(xt, t) to estimate the scaled score function σtx log pt(xt) with training objective formulated in Eq. 22. Sampling from diffusion model can be achieved by solving the reverse-time SDE or the corresponding diffusion ODE (Song et al., 2021) in an iterative manner. Recently, flow-based models (Liu et al., 2022a; Lipman et al., 2022; Esser et al., 2024a) have shown superior performance through alternative training objective formulated in Eq. 31 while maintaining the same architecture as DiT (Peebles & Xie, 2023a). Sampling from flow-based model can be achieved by solving the probability flow ODE. Mixture of Experts. Mixture of Experts (MoE) (Shazeer 2 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Figure 2: DiffMoE Architecture Overview. DiffMoE flattens tokens into batch-level global token pool, where each expert maintains fixed training capacity of Ei train = 1. During inference, dynamic capacity predictor adaptively routes tokens across different sampling steps and conditions. Different colors denote tokens from distinct samples, while ti represents corresponding noise levels. et al., 2017; Cai et al., 2024) is based on fundamental insight: different parts of model can specialize in handling distinct tasks. By selectively activating only relevant components, MoE enables efficient scaling of model capacity while maintaining computational efficiency. MoE layers generally consist of experts, each implemented as Feed-Forward Network (FFN) with identical architecture, denoted by E1(x), . . . , EN (x) with input x. routing matrix Wr RDN is used to calculate tokenexpert affinity matrix: from each sample of the input RBSD. The gating function GEC s,i can be implemented analogously to Eq. 2, with the modification that the top operation selects tokens along the token length dimension, i.e. S. Similar to Eq. 3, the output for token xs of EC-MoE layers can s,i E(xs), xs R1D. Both TC and EC struggle to achieve significant improvements comparing with dense models due to their restricted token accessibility and fixed computational patterns. be calculated as: ys = (cid:80) i=1 GEC = softmaxE(xWr), RBSD, (1) 2.2. DiffMoE: Dynamic Token Selection where is the batch size, is the token length of one sample, is the hidden dimension, softmaxE denotes the softmax operation along the expert axis. There are two common gating paradigms: Token-Choice (TC) (Shazeer et al., 2017; Fei et al., 2024) and Expert-Choice (EC) (Zhou et al., 2022; Sun et al., 2024a). For TC, each token of each sample individually selects top-K experts via gating function, the gating function and output of TC-MoE layers are defined as follows: (cid:40) GT s,i = Ms,i, Ms,i top-K({Ms,i}N 0, otherwise i=1) (2) ys = (cid:88) i=1 GT s,i E(xs), xs R1D, {1, . . . , S}. (3) Batch-level Global Token Pool. Since MoE architectures replace FFN layers, both TC and EC paradigms in diffusion models are inherently limited to processing tokens within individual samples, where gating mechanisms operate exclusively on tokens sharing identical conditions and noise levels. This architectural constraint not only prevents experts from learning crucial contrastive patterns but, more fundamentally, restricts their access to the global token distribution that characterizes the full spectrum of the diffusion process. To capture this essential global context, we introduce Batch-level Global Token Pool for DiffMoE by flattening batch and token dimensions, enabling experts to access comprehensive token distribution spanning different noise levels and conditions. This design, which simulate the true token distribution of the entire dataset during training, can be formulated as follows: Different from TC, EC makes every expert selects tokens RBSD xpool RBSD. (4) 3 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers where El denotes ith expert in the lth MoE layer. During training phase, Ctrain is fixed to 1 across all the MoE models, indicating that they keep the same computational cost as dense models. Specifically, DiffMoE keeps train = BS/N for Ctrain = 1. TC-DiT selects the top-1 expert, while EC-DiT selects top-(S/N ) tokens per sample in batch to ensure the same computation. During the inference phase, we compute the global average inference capacity avg infer by averaging over all timesteps: (cid:80)T avg infer, where t=1 infer represents the inference capacity at sampling step t. infer = 1 Capacity Predictor. Although batch-level global token routing enables efficient model training, conventional MoE inference strategies with fixed computational resource allocation fail to fully leverage its potential. This limitation stems from the static resource distribution across different noise levels and conditional information during inference. To optimize token selection, we propose capacity predictora lightweight structure that dynamically determines token selection per expert through two-layer MLP with SiLU activations. This adaptive mechanism learns from training-time token routing patterns, efficiently distributing computational resources between complex and simple cases. Formally, let CP(xpool) RBSN denote the predictors output for input xpool RBSD: CP(xpool) = W2σSiLU(W1xpool). (10) (6) (7) We can optimize the capacity predictor by minimizing the object function: LCP = BCELoss(O, CP(sg[xpool]) (11) Figure 3: Training Loss Curves of Different Flow-based Models. DiffMoE with batch-level global token pool achieves consistently lower diffusion losses than baselines without batch-level global token pool. During the training phase, we push expert to select train tokens, forcing each expert to capture the characteristics of tokens from different conditional information and noise levels, while keeping expert load balance during training. The corresponding batch-level global token-expert affinity matrix will be calculated as follows: MDy = xpoolWr, xpool RBSD, Wr RDN . (5) Then, using MDy RBSN , the gating value of MoE and the output of DiffMoE layers can be computed as follows: (cid:40) GDy s,i = MDy 0, otherwise s,i , MDy s,i top-K train({MDy s,i }BS s=1) ys = (cid:88) i=1 GDy s,i E(xs). As shown in Figure 3, DiffMoE consistently achieves lower diffusion losses than all baselines. Capacity and Computational Cost. To establish rigorous and fair comparison framework, we define the capacity for single expert E, which serves as standardized metric for quantifying computational costs. This capacity metric enables fair comparisons between DiffMoE and baseline models by accurately measuring the computational resources utilized by each expert: = # tokens processed by # all input tokens = E BS , (8) where denotes the number of tokens assigned to expert E, is the number of experts, and BS represents the size of global token pool. Here, we define the capacity for one forward process for both training and inference phases: = 1 LN (cid:88) (cid:88) l=1 i=1 El , (9) 4 = 1 LBSN (cid:88) BS (cid:88) (cid:88) {Ol i,j log(CP(sg[xpool])i,j) + (1 Ol i=1 j=1 l=1 i,j) log(1 CP(sg[xpool])i,j)}, (12) where sg denotes the stop-gradient operation, and RLBSN is defined as follows: (cid:40) Ol s,i = if xpool,s is processed by El 1, 0, otherwise . (13) We employ the stop-gradient technique to train the capacity predictor, ensuring it focuses solely on the input features at the current layer while preventing it from interfering with the training of the main diffusion transformers. Therefore, LCP will not affect actual diffusion loss. During inference, the capacity predictor determines the inference capacity El i,t infer for each expert El at timestep based on denote the threshold of El threshold. Let τEl i. Using {1, . . . , }, {1, . . . , L}}, the model = {τEl DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers achieves an adaptive El i,t tailored to different input tokens as follows: infer allocation at sampling step El i,t infer(τEl ) = (cid:88) δl s,i where δl s,i = s=1 (cid:40) if CP(xpool)s,i > τEl 1, 0, otherwise . (14)"
        },
        {
            "title": "Then we can calculate C avg",
            "content": "infer w.r.t. T"
        },
        {
            "title": "C avg",
            "content": "infer(T ) ="
        },
        {
            "title": "1\nT N L",
            "content": "T (cid:88) (cid:88) (cid:88) t=1 i=1 l= El i,t infer(τEl ). (15) Dynamic Threshold. We can set the threshold to control the number of tokens processed by each expert during inference. It is evident that #tokens processed during inference, decreases as τEl increases for all expert. We can adjust flexibly to achieve better trade-off between computational complexity and generation quality. We employ two distinct approaches for threshold determination: Interval Search and Dynamic Threshold. The interval search method addresses an optimization problem formulated as follows: min FID(T ) subject to avg infer(T ) 1. (16) To simplify the optimization problem, we assume that τ = γ < 1, τ , where γ is constant in our experiments. However, interval search method is labor-intensive and time-consuming, making it impractical for real-world applications. To address this limitation, we propose dynamic threshold method that automatically maintains thresholds (denoted as Dy = {τ Dy {1, . . . , }, El {1, . . . , L}}) for all experts during the training phase. To ensure the inference computational cost approximates the training cost (i.e. avg infer 1), we employ the Exponential Moving Average (EMA) technique as follows: CP(xpool)sk,i, QuantileEl τ Dy El α τ Dy El + (1 α) QuantileEl , (17) where sk denotes the kth value in descending order, α is constant which is equals to 0.95 in our experiments. 3. Experiments We evaluate DiffMoE on class-conditional image generation across three key aspects: (1) Training and Inference Performance, (2) Dynamic Computation, and (3) Scalability. Our experiments demonstrate DiffMoEs effectiveness through extensive analysis. Additionally, we verify its adaptability on text-to-image generation tasks. 5 Figure 4: Comparisons with the Baseline Models. We compare TC, EC, and Dense Models. DiffMoE-L-E16-Flow even surpasses the DenseDiT-XL-Flow (1.5x params) by achieving the best quality (14.41 FID50K w/o CFG at 700K). The results of the DDPM method remain consistent with those provided in the Appendix D.1. 3.1. Experiment Setup Baseline and Model architecture. We compare with Dense-DiT trained by denoising score matching (Peebles & Xie, 2023a) and flow matching (Ma et al., 2024), TCDiT (Fei et al., 2024), and EC-DiT (Sun et al., 2024a). For fair comparison, we reimplement TC-DiT and EC-DiT based on their public repositories and pseudo-codes while maintaining identical activated computation. Models are named as: [Model]-[Size]-[# Experts]-[Training Type]. For class-conditional generation, we replace even FFN layers with MoE layers containing identical FFN components (Lepikhin et al., 2020), while maintaining the original DiT architecture (Peebles & Xie, 2023a), details of architecture are shown in Table 1. For text-to-image generation, we add cross-attention modules (Rombach et al., 2022), where DiffMoE-E16-T2I-Flow activates 1.2B parameters (matching Dense-DiT-T2I-Flow) from total 4.6B parameters. Full details are in Appendix B.1. Evaluation. We evaluated DiffMoE through both quantitative and qualitative metrics. Quantitatively, we used FID50K (Heusel et al., 2017) with 250 DDPM/Euler steps for class-conditional generation, and compared with SiT (Ma et al., 2024) using Heun sampler at 125 steps. For text-to-image generation, we employed GenEval metrics (Ghosh et al., 2023). Training losses were analyzed to validate the models convergence behavior. Additionally, we assessed the models performance through visual inspection of samples generated from diverse prompts. 3.2. Main Results: Class-conditional Image Generation Class-conditional image generation is task of synthesizing images based on specified class labels. DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Table 1: DiffMoE Model Configurations. Hyperparameter settings and computational specifications for class-conditional models. See Appendix for activated parameter calculations. Model Config DiffMoE-S-E16 DiffMoE-B-E16 DiffMoE-L-E8 DiffMoE-L-E16 #Avg. Activated Params (C avg infer = 1). #Total Params. #Blocks #Hidden dim. #Head #Experts Ctrain 32M 130M 458M 458M 139M 555M 1.176B 1.982B 12 12 24 24 384 768 1024 1024 6 12 16 16 16 16 8 16 1 1 1 1 Table 2: State-of-the-art Comparison. Evaluation on ImageNet 256 256 class-conditional generation. DiffMoE achieves better FID with fewer parameters. -G/-U denotes with/without guidance (Ho & Salimans, 2022). : results from (Ma et al., 2024) (DDPM) and (Peebles & Xie, 2023a) (Flow). : our reproduction. Bold indicates best performance in each cell. Diffusion Models (7000K) Dense-DiT-XL-Flow-U (Peebles & Xie, 2023a) Dense-DiT-XL-Flow-U (Peebles & Xie, 2023a) DiffMoE-L-E8-Flow-U Dense-DiT-XL-Flow-G (cfg=1.5, ODE) (Ma et al., 2024) Dense-DiT-XL-Flow-G (cfg=1.5, ODE) (Ma et al., 2024) DiffMoE-L-E8-Flow-G (cfg=1.5, ODE) Dense-DiT-XL-DDPM-U (Peebles & Xie, 2023a) Dense-DiT-XL-DDPM-U (Peebles & Xie, 2023a) DiffMoE-L-E8-DDPM-U Dense-DiT-XL-DDPM-G (cfg=1.5) (Peebles & Xie, 2023a) Dense-DiT-XL-DDPM-G (cfg=1.5) (Peebles & Xie, 2023a) DiffMoE-L-E8-DDPM-G (cfg=1.5) Table 3: Baseline Model Comparisons. DiffMoE-L-E16Flow achieves best FID50K (14.41 w/o CFG) among TC, EC, and Dense variants. DDPM results in Appendix 11. Model (700K) TC-DiT-L-E16-Flow EC-DiT-L-E16-Flow Dense-DiT-L-Flow Dense-DiT-XL-Flow DiffMoE-L-E16-Flow # Avg. Activated Params. avg infer 1 1 1 1 0.95 458M 458M 458M 675M 454M FID50K 19.06 16.12 17.01 14.77 14.41 Table 4: Ablation of Capacity Predictor. We employ the capacity predictor to perform dynamic token selection, comparing it with the fixed TopK token selection method. Model (700K) DiffMoE-L-E16-Flow DiffMoE-L-E16-Flow DiffMoE-L-E16-Flow DiffMoE-L-E16-Flow Capacity Predictor avg infer 0.9 0.95 1 0. w/o w/o w/o FID50K 16.63 15.99 15.25 14.41 # Avg. Activated Params. FID IS Precision Recall 675M 675M 458M 675M 675M 458M 675M 675M 458M 675M 675M 458M 9.35 9.47 9. 2.15 2.19 2.13 9.62 9.62 9.17 2.27 2.32 2.30 126.06 115.58 131.46 254.9 272.30 274.39 121.50 123.19 131. 278.2 279.18 284.78 0.67 0.67 0.67 0.81 0.83 0.81 0.67 0.66 0.67 0.83 0.83 0.82 0.68 0.67 0. 0.60 0.58 0.60 0.67 0.68 0.67 0.57 0.57 0.59 Figure 5: Text-to-Image Generation Loss Curves. Training loss comparison between DiffMoE-E16-T2I-Flow and Dense-DiT-T2I-Flow models over 160K steps. DiffMoE consistently achieves lower loss values, demonstrating superior convergence efficiency compared to the dense baseline. Comparison with Baseline. DiffMoE-L-E16 demonstrates superior efficiency by outperforming Dense-DiT-XL (with 1.5 parameters) after 700K steps, as shown in Table 3. It consistently achieves lower training loss compared to variants TC-DiT-L-E16, EC-DiT-L-E16 and Dense-DiT-L (Figure 4, 11, 10). These improvements hold across both DDPM and Flow Matching paradigms while maintaining equivalent activated parameters. With more training time computation, DiffMoE-L-E16 can outperform Dense-DiTXXXL (with 3 parameters) as shown in Table 5. Comparison with SOTA. After 7000K steps, DiffMoE-LE8 achieves state-of-the-art FID50K scores of DDPM/Flow surpassing Dense-DiT-XL (2.30/2.13) with cfg=1.5, (2.32/2.19) as shown in Table 2. All evaluations follow DiTs (Peebles & Xie, 2023a) protocol. Generated C2I samples are shown in Figure 15 and 16. 6 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Figure 6: Training and Inference Gap. Comparison of sampling strategies with DiffMoE-L-E16-Flow (Batch Size = 1). For each group, Top: Sampling w/o Capacity Predictor (with Fixed TopK Method.) Bottom: Sampling with Capacity Predictor. 3.3. Main Results: Text-to-Image Generation In Text-to-Image (T2I) generation, DiffMoE demonstrates superior performance over the Dense Model across multiple metrics. Without/With supervised fine-tuning (SFT), DiffMoE achieves GenEval scores of 0.44/0.51, outperforming the Dense Models 0.38/0.49, with improvements across nearly all sub-metrics (Appendix Table 7). And as shown in Figure 5, DiffMoE maintains lower training losses while using the same activated parameters. Qualitative results in Figure 17 validate DiffMoEs ability to generate higherquality images without SFT. Additional samples from SFTenhanced models are shown in Figure 18. 3.4. Dynamic Computation Analysis For the convenience of elaboration, we use the flow matching training method to do the following analysis while DDPM results are also provided in the Appendix D.1. Analysis of Inference Capacity. DiffMoE-L-E16-Flow demonstrates superior parameter efficiency with its inference capacity (C avg infer) being 1 less than TC-DiT and EC-DiT, while achieving better performance, as shown in Table 3. Notably, with only 454M average activated parameters, our model outperforms Dense-DiT-XL-Flow (675M parameters), highlighting the effectiveness of dynamic expert allocation. Detailed analysis of average activated parameters is provided in the Appendix C. Ablation of Capacity Predictor. Dynamic token selection through our capacity predictor demonstrates superior performance over traditional static topK token selection, as shown in Table 4. This improvement stems from the predictors ability to intelligently allocate more computational resources to challenging tasks. The capacity predictor plays crucial role in unleashing DiffMoEs full potential by dynamically adjusting resource allocation, which is particularly important for optimizing inference efficiency Figure 7: Different Threshold Methods. We employ two distinct approaches for threshold determination: dynamic threshold (red point) and interval search (blue points). Visualization using DiffMoEL-E16-Flow (700K). (Section 2.2). Without such adaptive mechanism, DiffMoE suffers from severe quality degradation due to sub-optimal resource utilization, as illustrated in Figure 6. Interval Search vs. Dynamic Threshold. Both interval search and dynamic threshold methods achieve optimal performance in DiffMoE-L-E16-Flow, with the dynamic threshold (T Dy) emerging as our preferred approach due to its elegance and efficiency. Through interval search from 0.0 to 0.999, we identify an optimal threshold (γ 0.4) that minimizes FID while maintaining avg infer 1. Meanwhile, the dynamic threshold automatically maintains avg infer 1 during inference, achieving comparable FID scores within the optimal region, as shown in Figure 7 and Table 10. Our experiments reveal U-shaped relationship between FID and avg infer, indicating that both over-activation and under-activation of parameters degrade performance. Both methods successfully identify thresholds within the optimal region, but the dynamic thresholds straightforward implementation and computational efficiency make it our default choice throughout this paper. Harder Work Needs More Computation. Figure 1 demonstrates that different classes require varying computational resources during generation. By analyzing 1K class labels and ranking their avg infer, we observe distinct patterns in computational demands. The most challenging cases typically involve objects with precise details, complex materials, structural accuracy, and specific viewing angles (e.g., technical instruments, detailed artifacts). In contrast, natural subjects like common animals (birds, dogs, cats) generally require less computation. Figures 13 and 14 display the top-10 most and least computationally intensive classes for both flow-based and DDPM models, respectively. 7 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Table 5: Parameter Scaling Behavior of Diffusion Models on ImageNet 256256 Class-Conditional Generation. This table evaluates the impact of parameter scaling on model performance. DiffMoE demonstrates superior FID scores with fewer parameters after 3000K training steps, highlighting its efficiency. Models with -G employ classifier-free guidance (Ho & Salimans, 2022). The base parameter configuration is fixed at 458 million (458M). Bold indicates the best performance in each metric, while underline denotes the second-best performance. Diffusion Models (3000K) # Avg. Activated Params. FID IS Precision Recall Dense-DiT-XL-FlowG (cfg=1.5, ODE) Dense-DiT-XXL-Flow-G (cfg=1.5, ODE) Dense-DiT-XXXL-Flow-G (cfg=1.5, ODE) DiffMoE-L-E8-Flow-G (cfg=1.5, ODE) DiffMoE-L-E16-Flow-G (cfg=1.5, ODE) DiffMoE-XL-E16-Flow-G (cfg=1.5, ODE) 675M (1.5x) 951M (2x) 1353M (3x) 458M (1x) 458M (1x) 675M (1.5x) 2.52 2.41 2.37 2.40 2.36 2.30 273.78 281.96 291.29 280.30 287.26 291.23 0.84 0.84 0.84 0.83 0.83 0.83 0.56 0.57 0.57 0.57 0.58 0.58 Figure 8: Scaling Model Size. We analyze the impact of model size scaling by plotting FID50K scores across training steps. DiffMoE consistently outperforms the corresponding baseline models across all scales (S/B/L). Figure 9: Scaling Number of Experts. Comparison of FID50K scores during training between Dense-DiT-L-Flow (E1) and models with increasing expert counts (E2, E4, E8, E16). 3.5. Scaling Behavior Scaling the Model Size. DiffMoE demonstrates consistent performance improvements across small (S), base (B), and large (L) configurations, with activated parameters of 32M, 130M, and 458M respectively (Figure 8). Scaling Number of Experts. As shown in Figure 9, model performance improves consistently when scaling experts from 2 to 16, with diminishing returns between E8 and E16. Based on this analysis, we trained DiffMoE-L-E8 for 7000K iterations, achieving optimal performance-efficiency trade-off and state-of-the-art results. Scaling Parameter Behavior. To explore the upper limits of DiffMoE and quantify its performance efficiency, we scaled the model to larger sizes and trained them for 3000K steps. As illustrated in Table 5, DiffMoE-L-E16-Flow achieves the best performance among the evaluated models. Notably, DiffMoE-L-E16 surpasses the performance of Dense-DiTXXXL-Flow, which uses 3x the parameters, while operating with only 1x the parameters. This highlights the exceptional parameter efficiency and scalability of DiffMoE. 4. Related Works Diffusion Models. Diffusion models (Ho et al., 2020; Podell et al., 2023; Peebles & Xie, 2023a; Esser et al., 2024b) have emerged as the dominant paradigm in visual generation in recent years. These models transform gaussian distribution into target data distribution through iterative processes, with two primary training paradigms: Denoising Diffusion Probabilistic Models (DDPM) trained via scorematching (Ho et al., 2020; Song et al., 2021), which learns the inverse of diffusion process and Rectified Flow approaches optimized through flow-matching (Lipman et al., 2022; Ma et al., 2024; Esser et al., 2024b), which is more generic modeling techinique and can construct straight probaility path connecting data and noise. We implement DiffMoE using both paradigms, demonstrating its versatility across these complementary training methodologies. Mixture of Experts. Mixture of Experts (MoE) (Shazeer et al., 2017; Lepikhin et al., 2020) enables efficient model scaling through conditional computation by selectively activating expert subsets. This approach has demonstrated remarkable success in Large Language Models (LLMs), as 8 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers evidenced by cutting-edge implementations like DeepSeekV3 (DeepSeek-AI et al., 2024), minimax-01 (MiniMax et al., 2025), and OLMOE (Muennighoff et al., 2024).Recent works have explored incorporating MoE architectures into diffusion models, but face several limitations. MEME (Lee et al., 2023), eDiff-I (Balaji et al., 2022), and ERNIE-ViLG 2.0 (Feng et al., 2023) restrict experts to specific timestep ranges. SegMoE (Yatharth Gupta, 2024) and DiT-MoE (Fei et al., 2024) suffer from expert utilization imbalance due to isolated token processing. While EC-DiT (Sehwag et al., 2024; Sun et al., 2024a) recognizes complex tokens need for additional computation, it constrains token selection within individual samples and requires longer training for marginal improvements. These approaches, by limiting global token distribution across noise levels and conditions, fail to capture diffusion processes inherent heterogeneity. DiffMoE addresses these challenges through batch-level global token pool for training, and dynamically adapting computation to both noise levels and sample complexity for inference. 5. Conclusion In this work, we introduce DiffMoE, simple yet powerful approach to scaling diffusion models efficiently through dynamic token selection and global token accessibility. Our method effectively addresses the uniform processing limitation in diffusion transformers by leveraging specialized expert behavior and dynamic resource allocation. Extensive experimental results demonstrate that DiffMoE substantially outperforms existing TC-MoE and EC-MoE methods, as well as dense models with 1.5 parameters, while maintaining comparable computational costs. The effectiveness of our approach not only validates its utility in classconditional generation but also positions DiffMoE as key enabler for advancing large-scale text-to-image and text-tovideo generation tasks. While we excluded modern MoE enhancements for fair comparisons, integrating advanced techniques like Fine-Grained Expert (Yang et al., 2024) and Shared Expert (Dai et al., 2024) presents compelling opportunities for future work. These architectural improvements, combined with DiffMoEs demonstrated scalability and efficiency, could significantly advance the development of more powerful world simulators in the AI landscape."
        },
        {
            "title": "Impact Statement",
            "content": "DiffMoE represents significant advancement in visual content generation, demonstrating exceptional scalability to billion-parameter architectures while maintaining computational efficiency. However, like other powerful AI models, it raises important ethical considerations that warrant careful attention. These include potential misuse for generating misleading content, privacy concerns regarding training data and generated content, environmental impact of large-scale model training, and broader societal implications of automated content generation. We are committed to addressing these challenges through robust safety measures, transparent guidelines, and continuous engagement with stakeholders to ensure responsible development and deployment of this technology."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported in part by the National Natural Science Foundation of China under Grant 624B1026, Grant 62125603, Grant 62336004, and Grant 62321005."
        },
        {
            "title": "References",
            "content": "Albergo, M. S. and Vanden-Eijnden, E. Building normalizing flows with stochastic interpolants, 2023. URL https://arxiv.org/abs/2209.15571. Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang, Q., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., and Liu, M.-Y. ediff-i: Text-to-image diffusion models with ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu, J. All are worth words: vit backbone for diffusion models, 2023. URL https://arxiv.org/abs/2209. 12152. Cai, W., Jiang, J., Wang, F., Tang, J., Kim, S., and Huang, J. survey on mixture of experts, 2024. URL https: //arxiv.org/abs/2407.06204. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. Dai, D., Deng, C., Zhao, C., Xu, R. X., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., Xie, Z., Li, Y. K., Huang, P., Luo, F., Ruan, C., Sui, Z., and Liang, W. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024. URL https: //arxiv.org/abs/2401.06066. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., et al. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. NeurIPS, 34:87808794, 2021. 9 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024a. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., Lacey, K., Goodwin, A., Marek, Y., and Rombach, R. Scaling rectified flow transformers for high-resolution image synthesis, 2024b. URL https://arxiv.org/abs/2403. 03206. Fei, Z., Fan, M., Yu, C., Li, D., and Huang, J. Scaling diffusion transformers to 16 billion parameters. arXiv preprint, 2024. Feng, Z., Zhang, Z., Yu, X., Fang, Y., Li, L., Chen, X., Lu, Y., Liu, J., Yin, W., Feng, S., Sun, Y., Chen, L., Tian, H., Wu, H., and Wang, H. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts, 2023. URL https:// arxiv.org/abs/2210.15257. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023. URL https://arxiv.org/abs/ 2310.11513. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. Information Process- (eds.), Advances in Neural ing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 8a1d694707eb0fefe65871369074926d-Paper. pdf. Ho, J. and Salimans, T. Classifier-free diffusion guidURL https://arxiv.org/abs/ ance, 2022. 2207.12598. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. Kingma, D., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. NeurIPS, 34:2169621707, 2021. Lee, Y., Kim, J.-Y., Go, H., Jeong, M., Oh, S., and Choi, S. Multi-architecture multi-expert diffusion models, 2023. URL https://arxiv.org/abs/2306.04990. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020. URL https://arxiv.org/abs/ 2006.16668. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022a. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022b. URL https://arxiv.org/abs/2209.03003. Liu, X., Zhang, X., Ma, J., Peng, J., et al. Instaflow: One step is enough for high-quality diffusion-based text-toimage generation. In The Twelfth International Conference on Learning Representations, 2023. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. MiniMax, Li, A., Gong, B., Yang, B., Shan, B., Liu, C., Zhu, C., Zhang, C., Guo, C., Chen, D., Li, D., Jiao, E., Li, G., Zhang, G., Sun, H., Dong, H., Zhu, J., Zhuang, J., Song, J., Zhu, J., Han, J., Li, J., Xie, J., Xu, J., Yan, J., Zhang, K., Xiao, K., Kang, K., et al. Minimax-01: Scaling foundation models with lightning attention, 2025. URL https://arxiv.org/abs/2501.08313. Muennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Morrison, J., Min, S., Shi, W., Walsh, P., Tafjord, O., Lambert, N., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden, D., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi, A., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H. Olmoe: Open mixture-of-experts language models, 2024. URL https://arxiv.org/abs/2409.02060. Pan, J., Sun, K., Ge, Y., Li, H., Duan, H., Wu, X., Zhang, R., Zhou, A., Qin, Z., Wang, Y., Dai, J., Qiao, Y., and Li, H. Journeydb: benchmark for generative image understanding, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023a. 10 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Peebles, W. and Xie, S. Scalable diffusion models with transformers, 2023b. URL https://arxiv.org/ abs/2212.09748. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Yang, Y., Qi, S., Gu, W., Wang, C., Gao, C., and Xu, Z. Xmoe: Sparse models with fine-grained and adaptive expert selection, 2024. URL https://arxiv.org/ abs/2403.18926. Yatharth Gupta, Vishnu Jaddipal, H. P. Segmoe: Segmind mixture of diffusion experts. https://github. com/segmind/segmoe, 2024. Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A., Chen, Z., Le, Q., and Laudon, J. Mixture-ofexperts with expert choice routing, 2022. URL https: //arxiv.org/abs/2202.09368. Qiu, Z., Huang, Z., Zheng, B., Wen, K., Wang, Z., Men, R., Titov, I., Liu, D., Zhou, J., and Lin, J. Demons in the detail: On implementing load balancing loss for training specialized mixture-of-expert models. arXiv preprint arXiv:2501.11873, 2025. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015. doi: 10.1007/s11263-015-0816-y. Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Sehwag, V., Kong, X., Li, J., Spranger, M., and Lyu, L. Stretching each dollar: Diffusion training from scratch on micro-budget, 2024. URL https://arxiv.org/ abs/2407.15811. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017. URL https://arxiv.org/abs/1701.06538. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pp. 22562265. PMLR, 2015. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. Sun, H., Lei, T., Zhang, B., Li, Y., Huang, H., Pang, R., Dai, B., and Du, N. Ec-dit: Scaling diffusion transformers with adaptive expert-choice routing, 2024a. URL https: //arxiv.org/abs/2410.02098. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation, 2024b. URL https: //arxiv.org/abs/2406.06525. 11 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers A. Generative Modeling. In this section, we will provide detailed bachground of generative modeling of both DDPM (Ho et al., 2020; Song et al., 2021; Rombach et al., 2022) and Rectified Flow (Lipman et al., 2023; Ma et al., 2024; Esser et al., 2024b) which is helpful to understand the difference and relationship between them. Generative modeling essentially defines mapping between x1 from noise distribution p1(x) to x0 from data distribution p0(x) leads to time-dependent processes represented as below xt = αtx0 + σtϵ, [0, 1], (18) where αt is decreasing function of and σt is an increasing function of t. We set α0 = 1, σ0 = 0 and α1 = 0, σ0 = 1 to make the marginals pt(xt) = EϵN (0,I)pt(xtϵ) are consistent with data p0(x) and noise p1(x) distirbution. p1(x) usually be chosen as gaussion distribution (0, 1). Different forward path from data to noise leads to different training object which significantly affect the performance of the model. Next we will introduce DDPM and Rectified Flow. A.1. Denosing Diffusion Probabilistic Models (DDPM) In DDPM the choice for αt and σt is referred to as the noise schedule and the signal-to-noise-ratio (SNR) α2 is strictly decreasing w.r.t (Kingma et al., 2021). Moreover, (Kingma et al., 2021) prove that the following stochastic differential Eq. (SDE) has same transition distribution as pt(xtx0) for any [0, 1]: /σ2 where wt ED is the standard Wiener process, and dxt = (t)xtdt + g(t)dwt, [0, 1], x0 p0(x0), (t) = log αt dt , g2(t) = dσ2 dt 2 log αt dt σ2 . (19) (20) (Song et al., 2021) proved that the forward path in Eq. 19 has an equivalent reverse process from time 1 to 0 under some regularity conditions, starting with pT (xT ) dxt = [f (t)xt g(t)2x log pt(xt)]dt + g(t)d wt, xT pT (xT ), (21) where wt ED is the standard Wiener process. We can esitimate the score term log pt(xt) as each time to iterativtely solve the reverse process, then get the gernerated target. DPMs train neural network ϵθ(x, t) parameterized by θ to esitimated the scaled score function σx log pt(xt). To optimize ϵθ, we minimize the following objective (Ho et al., 2020; Song et al., 2021; Ma et al., 2024) LDDPM(θ) = Et,p0(x0),p(xtx0) (cid:2)λ(t)ϵθ(xt, t) + σtx log pt(xt)2 2 (cid:3) , (22) where λt is time-dependent coefficent. ϵθ(xt, t) can be interpreted as predicting the Gaussian noise added to xt, thus it is commonly referred to as noise prediction model. Consequently, the diffusion model is known as denoising diffusion probabilistic model. Substitute score term in (21) with ϵθ(xt, t)/σt, we can solve the reverse process and generate samples from DPMs with numerical solvers. To further accelerate the sampling process, Song et al. (Song et al., 2021) proved that the equvivalent probability flow ODE is dxt dt = vθ(xt, t) := (t)xt + g2(t) 2σt ϵθ(xt, t), x1 (0, I). (23) Thus samples can be also generated by solving the ODE from time 1 to 0. 12 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers A.2. Rectified Flow Models (Flow) Recified flow models (Liu et al., 2022b; Albergo & Vanden-Eijnden, 2023; Lipman et al., 2023) connects data x0 and noise ϵ on straight line as follows xt = (1 t)x0 + tϵ, [0, 1]. (24) To precisely express the relationship between xt, x0, and ϵ, we first construct time-dependent vector field : [0, 1]RD RD. This vector field ut can be used to construct time-dependent diffeomorphic map, known as flow ϕ : [0, 1] RD RD, through the following ODE: dt ϕt(x0) = ut(ϕt(x0)) ϕ0(x0) = x0. (25) (26) The vector field ut can be modeled as neural network vθ (Chen et al., 2018) which leads to deep parametric model of the flow ϕt, called Continuous Normalizing Flow (CNF). We can using conditional flow matching (CFM) technique (Lipman et al., 2022) to training CNF. Now we can define the flow we need as follows: The corresponding velocity vector field of the flow ψt can be represented as: ψt(ϵ) : x0 (cid:55) αtx0 + σtϵ. ut(ψt(ϵ)ϵ) = dt ψt(x0ϵ) = αtx0 + σtϵ = ϵ x0. (27) (28) Using conditional flow matching technique, v(xt, t) in Eq. (23) can be modeled as neural network vθ(xt, t) by minimziing the following objective LFlow(θ) = Et,p0(x0),p1(ϵ)vθ(xt, t) ψt(x0ϵ)2 2 = Et,p0(x0),p(ϵ)vθ(xt, t) ( αtx0 + σtϵ)2 2 = Et,p0(x0),p(ϵ)vθ(xt, t) (ϵ x0)2 2. dt (29) (30) (31) Samples can be generated by sovling the probability flow ODE below with learned velocity using numerical sovler like Euler, Heun, Runge-Kutta method. dxt = vθ(xt, t)dt, x1 = ϵ (0, 1). (32) A.3. Relationship DDPM and Flow There exists straightforward connection between vθ(xt, t) and the score term σtx log pt(xt) can be derived as follows vθ(xt, t) = (t)xt + g2(t) 2σt (cid:18) αt αt xt + σt ϵθ(xt, t) (cid:19) αt αt (σtx log pt(xt)). (33) (34) Let ζt = σt αt αt By plugging (34) into the loss LFlow in Eq. (31) we have: , and we have ϵθ(xt, t) σtx log pt(xt, then we can get vθ(xt, t) = αt αt xt + ζtϵθ(xt, t) 13 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Table 6: Comprehensive State-of-the-art Comparison. Evaluation on ImageNet 256 256 class-conditional generation. DiffMoE achieves better FID with fewer parameters. -G/-U denotes with/without guidance (Ho & Salimans, 2022). : results from (Ma et al., 2024) (DDPM) and (Peebles & Xie, 2023a) (Flow). : our reproduction. Bold indicates best performance in each cell. # Avg. Activated Params. FID IS Precision Recall 2.30 265.1 0.78 0. 6.18 182.1 0.80 0.51 Diffusion Models GAN StyleGAN-XL (Sauer et al., 2022) Masked Modeling Mask-GIT (Chang et al., 2022) Autoregressive Model LlamaGen-3B (Sun et al., 2024b) VAR-d20 (Tian et al., 2024) Diffusion Model ADM-U (Dhariwal & Nichol, 2021) ADM-G (Dhariwal & Nichol, 2021) LDM-4-G (Rombach et al., 2022) U-ViT-H/2-G (Bao et al., 2023) Dense-DiT-XL-Flow-U (Peebles & Xie, 2023a) Dense-DiT-XL-Flow-G (cfg=1.5, ODE) (Ma et al., 2024) Dense-DiT-XL-Flow-U (Peebles & Xie, 2023a) Dense-DiT-XL-Flow-G (cfg=1.5, ODE) (Ma et al., 2024) DiffMoE-L-E8-Flow-U DiffMoE-L-E8-Flow-G (cfg=1.5, ODE) Dense-DiT-XL-DDPM-U (Peebles & Xie, 2023a) Dense-DiT-XL-DDPM-G (cfg=1.5) (Peebles & Xie, 2023a) Dense-DiT-XL-DDPM-U (Peebles & Xie, 2023a) Dense-DiT-XL-DDPM-G (cfg=1.5) (Peebles & Xie, 2023a) DiffMoE-L-E8-DDPM-U DiffMoE-L-E8-DDPM-G (cfg=1.5) 166M 227M 3000M 600M 554M 608M 400M 501M 675M 675M 675M 675M 458M 458M 675M 675M 675M 675M 458M 458M 10.94 2.57 101.0 302.6 10.94 4.59 3.95 2. 9.35 2.15 9.47 2.19 9.60 2.13 9.62 2.27 9.62 2.32 9.17 2.30 101.0 186.7 247.7 263.88 126.06 254.9 115.58 272.30 131.46 274.39 121.50 278.2 123.19 279.18 131.10 284.78 0.69 0. 0.69 0.83 0.87 0.82 0.67 0.81 0.67 0.83 0.67 0.81 0.67 0.83 0.66 0.83 0.67 0.82 0.63 0.56 0.63 0.53 0.48 0.57 0.68 0.60 0.67 0.58 0.67 0. 0.67 0.57 0.68 0.57 0.67 0.59 (35) (36) (37) (38) = Et,p0(x0),p(ϵ) LFlow(θ) = Et,p0(x0),p1(ϵ)vθ(xt, t) ( αtx0 + σtϵ)2 2 αt αt = Et,p0(x0),p(ϵ)ζtϵθ(xt, t) ζtϵ2 2 (cid:2)ζ 2 = Et,p0(x0),p(ϵ) ϵθ(xt, t) ϵ2 2 xt + ζtϵθ(xt, t) σtϵ2 2 (cid:3) . Considering Eq. (18), we have xt (αtx0, σtI) and log p(xt) = σ1 get the equivilant loss function as below: (xt αtx0) = σ (σtϵ) = ϵ. Then, we can LFlow(θ) = Et,p0(x0),p(xtx0) (cid:104) ϵθ(xt, t) + σtx log pt(xt)2 ζ 2 (cid:105) . (39) (cid:2)λ(t)ϵθ(xt, t) + σtx log pt(xt)2 Recall that LDDPM(θ) = Et,p0(x0),p(xtx0) We can find that LDDPM and LFlow have the same form, with the only difference being their time-dependent weighting functions, which lead to different trajectories and properties. (cid:3) . B. More Implementation Details B.1. Training setup. We train class-conditional DiffMoE and baseline models at 256x256 image resolution on the ImageNet dataset (Russakovsky et al., 2015) highly-competitive generative benchmark, which contains 1281167 training images. We use horizontal 14 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Table 7: Performance Comparison on Text-to-Image Generation Tasks. Evaluation results on GenEval Benchmark (Ghosh et al., 2023) at 256 256 resolution across six different categories: single object generation, two-object composition, object counting, color recognition, spatial positioning, and color attribute understanding. DiffMoE-E16-T2IFlow demonstrates superior performance over Dense-DiT-T2I-Flow, particularly in object generation and spatial tasks. Bold indicates best overall performance in each cell. # A.A.P. denotes # Avg. Act. Params. # T.P. denotes # Total Params. Diffusion Models # A.A.P. # T.P. Single Obj. Two Obj. Counting Obj. Colors Position Color Attri. Overall Dense-DiT-T2I-Flow (w/o SFT) DiffMoE-E16-T2I-Flow (w/o SFT) Dense-DiT-T2I-Flow (w SFT) DiffMoE-E16-T2I-Flow (w SFT) 1.2B 1.2B 1.2B 1.2B 1.2B 4.6B 1.2B 4.6B 0.80 0. 0.93 0.96 0.33 0.42 0.48 0.53 0.24 0.24 0.50 0.46 0.53 0. 0.73 0.78 0.10 0.19 0.07 0.13 0.26 0.28 0.20 0.20 0.38 0. 0.49 0.51 Table 8: Performance comparison of different diffusion models with varying token interaction strategies. All models are trained with Flow Matching for 700K steps. The interaction levels (L1/L2/L3) represent: L1 for isolated token processing, L2 for local token routing within samples, and L3 for global token routing across samples. Our DiffMoE-L-Flow with Dynamic Global CP achieves the best FID score of 14.41 while maintaining parameter efficiency and reduced computational cost. # A.A.P. denotes # Avg. Act. Params. Model # A.A.P. Training Strategy Inference Strategy FID50K TC-DiT-L-E16-Flow Dense-DiT-L-Flow EC-DiT-L-E16-Flow EC-DiT-L-E16-Flow DiffMoE-L-E16-Flow 458M L1: Isolated 458M L1: Isolated 458M L2: Local 458M L2: Local 458M L3: Global L1: Isolated L1: Isolated L2: Local Static TopK Routing L2: Local Dynamic Intra-sample Routing L3: Global Static TopK Routing Dense-DiT-XL-Flow 675M L1: Isolated L1: Isolated 19.06 17.01 16.12 23.74 15.25 14.77 DiffMoE-L-E16-Flow 454M L3: Global L3: Global Dynamic Cross-sample Routing 14. flips as the only data augmentation. We train all models with AdamW (Loshchilov & Hutter, 2017). We use constant learning rate of 1 104, no weight decay and fixed global batch size of 256, following (Peebles & Xie, 2023b). We also maintain an exponential moving average(EMA) of DiffMoE and baseline model weights over training with decay of 0.9999. All results reported use the EMA mode. For most experiments, we utilized 4 NVIDIA H800 GPUs during training. To achieve state-of-the-art results, we extended the training process with 8 NVIDIA H800 GPUs for improved efficiency. For the text-to-image model pre-training, we employ 32 NVIDIA H800 GPUs with our internal datasets, and then conduct supervised fine-tuning (SFT) on the JourneyDB dataset (Pan et al., 2023). B.2. Implementation Algorithms. We provide detailed illustration of the DiffMoE layer during training and inference in Algorithm 3 and 4, respectively. We also implemented the same EC-DiT layer in Algorithm 1 as (Sun et al., 2024a). We implemented TC-DiT in layer in Algorithm 2 similar to (Dai et al., 2024; Fei et al., 2024) C. Calculation of Average Activated Parameters and Average Capacity avg infer C.1. Computing avg infer To compute the global average capacity (C avg approximation, sampling 1K samples is sufficient due to DiffMoEs stable performance characteristics. infer), we analyze 50K samples across all experts and sampling steps. For quick C.2. Estimating Average Activated Parameters using avg infer We will introduce the relationship between average activated parameters and average capacity in detail. Let #Module denote the number of parameters certain Module, NE denote the number of experts. Under approximate conditions, for DiT (Peebles & Xie, 2023a) models, we have 15 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Table 9: Module Parameters and Percentage. We have counted the number of parameters (M) of various modules to facilitate our analysis. Model FFN Attention AdaLN Others Dense-DiT-L DiffMoE-L-E2 DiffMoE-L-E4 DiffMoE-L-E8 DiffMoE-L-E16 201.44(44.0%) 314.8(55.1%) 516.3(66.8%) 919.3(78.2%) 1725.3(87.1%) 100.7(22.0%) 100.7(17.6%) 100.7(13.0%) 100.7(8.6%) 100.7(5.1%) 151(33.0%) 151(26.4%) 151(19.5%) 151(12.8%) 151(7.6%)) 4.7(1.0%) 4.7(0.8%) 4.7((0.6%) 4.7(0.4%) 4.7(0.2%) Total 457.84 571.2 772.7 1175.7 1981.7 Table 10: Different Threshold Method. We use both interval search and dynamic threshold method to find out the optimal τEi. We find that the dynamic threshold makes good balance between avg infer and performance. 0.999 0.99 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 avg infer 0.41 0.51 0.71 0.78 0.83 0.88 0.92 0.97 1.03 1.10 1.22 Dynamic 0.95 FID50K 36.28 24.22 17.59 16.21 15.51 15.00 14.59 14.16 13.75 13.38 12.92 14.41 0.2 0.1 1E-2 1E-3 1E-4 1E-5 1E-6 1E-7 1E-8 1E-9 0.0 avg infer 1.10 1.22 1.71 2.33 3.17 4.36 6.01 7.88 9.67 11.18 16 FID50K 13.38 12.92 12.14 11.82 11.87 12.47 13.51 13.96 16.85 18.90 29.39 Table 11: Comparisons with the Baseline Models. (DDPM) We compare TC, EC and Dense Model and show the average activated parameters of all the experts across all the sampling steps. Model (700K) TC-DiT-L-E16-DDPM EC-DiT-L-E16-DDPM Dense-DiT-L-DDPM Dense-DiT-XL-DDPM DiffMoE-L-E16-DDPM # Avg. Activated Params. avg infer 1 1 1 1 1 458M 458M 458M 675M 458M FID50K 20.81 17.65 17.87 15.28 14.60 Table 12: Decoder Ablation Study. Evaluation of various pre-trained VAE decoder weights. : results from (Ma et al., 2024) (DDPM) and (Peebles & Xie, 2023a) (Flow). : our reproduction. All other results are from our experiments. In general, with VAE decoder EMA version, the FID score is consistently lower than MSE version. Model Training Steps VAE-Decoder Sampler Batch Size FID50K Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-DDPM-G Dense-DiT-XL-DDPM-G 400K 400K 400K 400K 7000K 7000K 7000K 7000K 7000K 7000K ft-MSE ft-EMA ft-MSE ft-EMA ft-MSE ft-EMA ft-MSE ft-EMA ft-MSE ft-EMA Euler Euler Dopri5 Dopri5 Heun Heun Dopri5 Dopri5 DDPM DDPM 125 125 125 125 125 125 125 125 125 18.80 18.74 18.63 18.45 9.66 9.63 9.51 9.48 2.30 2.27 # Average Activated Parameters (cid:19) (cid:18) 1 + avg infer 1 + NE . # FFN + # Attention + # AdaLN + # Other Modules. (40) Table 9 displays the parameters and their corresponding percentages of the main modules of large-size DiffMoE. D. More DiffMoE Analysis D.1. Additional DiffMoE DDPM Class-conditional Generation Qualitative and Quantitative Results We present comprehensive evaluations of DiffMoE-L-E16-DDPM series models. Table 11 shows the experimental results, while Figure 10 illustrates the diffusion loss comparison against the baseline model, revealing substantial performance improvements. Furthermore, Figure 11 demonstrates the scaling capabilities of our DiffMoE-DDPM architecture. D.2. Additional DiffMoE Text-to-Image Qualitative and Quantitative Results To further validate the capability of DiffMoE of more challenging task, we conducted quantitative and qualitative evaluations of Dense-DiT-T2I-Flow and DiffMoE-E16-T2I-Flow. The experimental results are presented in Table 7, demonstrating the comparative performance metrics of both models. Figure 5 provides detailed visualization of the diffusion loss comparison between the dense model and DiffMoE, highlighting significant performance improvements achieved by our approach. The superior text-to-image generation capabilities of DiffMoE are further illustrated through qualitative examples in Figure 17 and Figure 15. It is worth noting that both text-to-image models were only trained for about 2 days, which is limited for this task, suggesting significant room for further performance improvements. D.3. Analysis of Token Interaction Strategies. As shown in Figure 1. The interaction levels (L1/L2/L3) represent: L1 for isolated token processing, L2 for local token routing within samples, and L3 for global token routing across sample. Table 8 presents comprehensive comparison of different token interaction strategies in diffusion models. The baseline models with L1 strategy (TC-DiT-L-Flow and 16 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers (a) Loss Comparison of L-Flow Series (b) Loss Comparison of L-DDPM Series Figure 10: Loss Comparison of L-Flow and L-DDPM Series. The relative losses illustrated in subfigures (a) and (b) demonstrate the exceptional training dynamics of DiffMoE, consistently outperforming all baseline models. Dense-DiT-L-Flow) process tokens independently, resulting in limited performance (FID: 19.06 and 17.01). The L2 strategy, implemented in EC-DiT-L-Flow, enables local token routing within samples, showing improved performance (FID: 16.12) with the same parameter count. Our proposed L3 strategy in DiffMoE-L-Flow introduces cross-sample token routing, achieving superior results (FID: 14.41) even compared to the 1.5x larger Dense-DiT-XL-Flow (675M parameters). Notably, when combined with Dynamic Global CP, our model not only achieves the best FID score but also reduces the computational capacity to 0.95x, demonstrating both effectiveness and efficiency. D.4. Dynamic Conditional Computation: Harder Work needs More Computation Figure 1 demonstrates that different classes require varying computational resources during generation. To analyze this variation, we sample 1K different class labels in batch and rank their avg infer in descending order, revealing the computational complexity of generation across classes. The top-10 most computationally intensive classes for both flow-based and DDPM models are displayed in Figure 13. The top-10 least computationally intensive classes for both flow-based and DDPM models are displayed in Figure 14. D.5. Dynamic Token Selection across Network Layers As illustrated in Figure 12, our analysis reveals distinctive expert utilization patterns across different network depths. The shallow Layer 1 exhibits pronounced fluctuations and sharp capacity spikes, indicating intensive early-stage feature extraction. Moving to intermediate Layer 7, we observe more stabilized capacity patterns, suggesting balanced processing of mid-level features. Layer 13 demonstrates gradual, long-term capacity transitions, while the deep Layer 19 shows notably uniform expert utilization. This systematic progression from volatile to stable expert engagement reflects the natural specialization of experts: from low-level feature detection in early layers to refined semantic processing in deeper layers. Such hierarchical organization of expert behaviors aligns with the progressive nature of diffusion-based generation. E. FID Sensibility and Ablation Study FID scores are sensitive to implementation details, necessitating careful ablation studies to understand the differences between various implementations. Through these studies, we aim to provide the academic community with clearer insights for fair comparisons between diffusion models. The observed FID degradation at higher CFG scales is well-documented (Ma et al., 2024), primarily due to ImageNets diverse image quality distribution. When generating high-quality samples, the deviation from ImageNets mixed-quality dataset can lead to increased FID scores, despite improved visual quality. For FID calculation, we follow the implementations from SiT (Ma et al., 2024) and DiT (Peebles & Xie, 2023a). Results marked with are directly quoted from (Ma et al., 2024) (DDPM) and (Peebles & Xie, 2023a) (Flow). For results marked with , we reproduce the experiments using officially released checkpoints under identical evaluation conditions. 17 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers (a) Comparisons with the Baseline Size-L (DDPM). (b) Comparisons with the Baseline Size-S/B (DDPM). Figure 11: Comparisons with the Baseline Models. (a) We compare TC, EC, and Dense Models and show the average activated parameters of all experts across all sampling steps. DiffMoE-L-E16-DDPM even surpasses DenseDiT-XL-DDPM (1.5x params). (b) We also examine S/B size DiffMoE models to further demonstrate the scalability. E.1. VAE Decoder Ablations Following (Peebles & Xie, 2023a), throughout our experiments, we employed pre-trained VAE models. Specifically, we utilized fine-tuned versions (ft-MSE and ft-EMA) of the original LDM \"f8\" model, where only the decoder weights were fine-tuned. For the analysis presented in Experiments section 3 and Tables 11 and 10 , we tracked metrics using the ft-MSE decoder, while the final metrics reported in Table 2 was obtained using the ft-EMA decoder. In this section, we examine the impact of two distinct VAE decoders for our experiments - the two fine-tuned variants employed in Stable Diffusion. Since all models share identical encoders, we can interchange decoders without necessitating diffusion model retraining. As demonstrated in Table 12, the DiffMoE model maintains its superior performance over existing diffusion models. E.2. Flow ODE-Sampler Ablations Higher-order ODE samplers generally achieve better FID scores. As shown in Table 15, the black-box dopri5 sampler outperforms heun (NFE=250), which in turn surpasses euler (NFE). For fair comparison with baseline models, we employ the euler sampler in flow-based experiments. However, to benchmark against SiT-XL (Dense-DiT-XL-Flow) (Ma et al., 2024), we use the heun sampler to achieve SOTA results. E.3. Classifier-Free-Guidence Ablations We evaluate different models with varying classifier-free guidance (CFG) scales in Table 14, and discover that the CFG scale of 1.5 adopted in DiT (Peebles & Xie, 2023a) and SiT (Ma et al., 2024) studies may not be universally optimal. Our analysis reveals the best CFG scale approximates to 1.43 through comprehensive comparisons. However, different models exhibit distinct characteristics that lead to varying optimal CFG scales, suggesting that fixing uniform scale across all models could introduce evaluation bias. To ensure relatively fair comparisons while maintaining consistency with established practices, we ultimately adopt CFG 1.5 as the default setting in our experiments. This decision aligns with the well-documented trade-off in diffusion models: higher CFG scales (e.g., 4.0) typically enhance image fidelity at the cost of increased FID scores, while lower scales (e.g., 1.5) yield better FID metrics despite reduced perceptual quality. This phenomenon primarily stems from FIDs sensitivity to distributional coverage - higher guidance scales tend to produce samples with reduced diversity that more closely match the training distribution statistics, paradoxically resulting in worse FID scores despite improved individual sample quality. 18 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Table 13: Batch Size Ablation Study: FID scores under different batch sizes using fine-tuned EMA VAE decoder and Heun sampler. Bold indicates best performance in its cell. Model Training Steps CFG Batch Size FID50K DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow Dense-DiT-XL-Flow DiffMoE-L-E8-DDPM DiffMoE-L-E8-DDPM DiffMoE-L-E8-DDPM DiffMoE-L-E8-DDPM 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 7000K 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1. 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 10 15 32 50 75 100 125 10 15 50 75 100 125 10 32 50 75 100 10 15 50 75 100 125 50 75 100 125 9.60 9.62 9.77 9.98 9.90 9.76 9.78 9.57 9.47 9.84 9.67 9.65 9.64 2.19 2.16 2.16 2.13 2.17 2.18 2.23 2.23 2.21 2.19 2.22 2. 2.30 2.33 2.32 2.32 Table 14: CFG Scale Ablation Study: FID scores across different CFG scales using fine-tuned EMA VAE decoder. Bold indicates best performance in its cell. Model Training Steps Sampler CFG Batch Size FID50K DiffMoE-L-E8-Flow Dense-DiT-XL-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow Dense-DiT-XL-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow Dense-DiT-XL-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-DDPM Dense-DiT-XL-DDPM DiffMoE-L-E8-DDPM DiffMoE-L-E8-DDPM Dense-DiT-XL-DDPM DiffMoE-L-E8-DDPM 4900K 7000K 7000K 4900K 7000K 7000K 4900K 7000K 7000K 6500K 7000K 7000K 6500K 7000K 7000K Heun Heun Heun Heun Heun Heun Heun Heun Heun 1.0 1.0 1.0 1.43 1.43 1.43 1.5 1.5 1.5 DDPM 1.0 DDPM 1.0 DDPM 1.0 DDPM 1.5 DDPM 1.5 DDPM 1. 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 9.21 9.64 9. 2.14 2.08 2.13 2.28 2.21 2.18 9.39 9.63 9.17 2.27 2.32 2.32 Table 15: Flow ODE Sampler Ablation Study: FID scores across different ODE samplers with CFG scale 1.0 and fine-tuned EMA VAE decoder. Bold indicates best performance in its cell. Model Training Steps Sampler Batch Size FID50K DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow DiffMoE-L-E8-Flow 4900K 4900K 4900K 4900K 7000K 7000K 7000K 7000K Euler Heun Euler Dopri5 Euler Heun Euler Dopri5 125 125 250 250 125 125 250 250 9.37 9.21 9.39 9. 9.86 9.78 9.94 9.56 E.4. Batch Sizes Ablations The batch sizes ablation study reveals critical insights into the interplay between batch size and classifier-free guidance (CFG) scales for the DiffMoE-L-E8-Flow model. At CFG=1.0, FID scores remain elevated (9.609.98), with smaller batch sizes (e.g., bs=10) marginally outperforming larger configurations, exhibiting U-shaped trend. However, elevating CFG to 1.5 drastically reduces FID to 2.132.19, achieving optimal performance at bs=75 (2.13), while demonstrating remarkable robustness to batch size variations (=0.06 vs. =0.38 at CFG=1.0). E.5. Conclusion: little thought about FID While Fréchet Inception Distance (FID) is widely adopted for evaluating generative models, particularly on ImageNet, it exhibits several notable limitations. Our analysis reveals counterintuitive behaviors, especially when evaluating models with classifier-free guidance (CFG). For example, higher CFG scales typically enhance perceptual quality but paradoxically result in worse FID scores, despite producing visually superior images. This discrepancy stems from FIDs fundamental mechanism: it measures statistical similarities between generated and real distributions in the Inception networks feature space, often failing to capture perceptual quality and fine-grained details. Moreover, FID scores are susceptible to various implementation factors, including choice of ODE samplers, hardware configurations, random seeds, and sample size for estimation. These sensitivities can impact reproducibility and comparison across different studies. Furthermore, FIDs focus on distributional overlap overlooks critical aspects such as mode collapse and overfitting, as it does not explicitly evaluate sample diversity or novelty. These limitations underscore the pressing need for more robust and comprehensive metrics that can better reflect the true modeling capabilities of generative models. We advocate for developing new evaluation frameworks that combine precision-recall curves, perceptual quality metrics, and human evaluation studies, which would provide more reliable assessment of generative model performance. DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers F. Visual Generation Results F.1. Class-Conditional Image Generation To demonstrate the generation capabilities of our model, we showcase diverse images sampled from DiffMoE-L-E8-Flow and DiffMoE-L-E8-DDPM, conditioned on ImageNet class labels. These visualizations illustrate the models ability to generate high-quality, class-specific images. See Figure 15 and 16. F.2. Text-Conditional Image Generation We present collection of images generated by our DiffMoE-T2I-Flow model using various text prompts as conditioning inputs. These examples demonstrate the models versatility in translating textual descriptions into corresponding visual representations. See Figure 18. 20 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers (a) Capacity of All Experts in Layer 1 (b) Capacity of All Experts in Layer 7 (c) Capacity of All Experts in Layer 13 (d) Capacity of All Experts in Layer Figure 12: Expert Dynamics across Network Layers. Visualization of expert capacity patterns in network layers (1, 7, 13, 19). Early layers show high-amplitude fluctuations, while deeper layers exhibit increasingly stable utilization, demonstrating natural expert specialization throughout the diffusion process. 21 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers (a) DiffMoE-L-E16-Flow (700K) Figure 13: Top 10 Hardest Classes. The 10 classes with the highest computational cost, sampled from the training set. (b) DiffMoE-L-E16-DDPM (700K) 22 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers (a) DiffMoE-L-E16-Flow (700K) Figure 14: Top 10 Easiest Classes. The 10 classes with the lowest computational cost, sampled from the training set. (b) DiffMoE-L-E16-DDPM (700K) DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Algorithm 1 EC-DiT Layer Input: (input tensor) Variables: (batch size), (sequence length), (hidden dim), Wr (routing weights), experts (list of expert FFNs) (number of experts), (expert capacity) /* Step 1: Compute Token-Expert Affinity Matrix */ logits einsum(bsd, de bse, x, Wr) scores softmax(logits, dim = 1).permute(1, 2) /* Step 2: Select top-k tokens for each expert */ gating, index top_k(scores, k=C, dim=1) dispatch one_hot(index, num_classes=S) /* Step 3: Process tokens through experts and combine */ xin einsum(becs, bsd becd, dispatch, x) xe [experts[e](xin[:, e]) for in range(E)] xe stack(xe, dim=1) xout einsum(becs, bec, becd bsd, dispatch, gating, xe) Return: xout Algorithm 2 TC-DiT layer Input: (input tensor), Wr (routing weights), experts (list of expert FFNs) Variables: (batch size), (sequence length), (hidden dim) ,K (experts per token) /* Step 1: Save original input shape */ orig_shape shape(x) /* Step 2: Compute Token-Expert Affinity Matrix */ logits einsum(bsd, de bse, x, Wr) scores softmax(logits, dim = 1) /* Step 3: Select top-k tokens for each expert */ gating, index top_k(scores, k=K, dim=1) /* Step 4: Flatten and top-k indices */ view(x, (1, x.shape[1])) flat_topk_idx view(topk_idx, (1)) /* Step 4: Process tokens through experts */ repeat_interleave(x, K, dim = 0) empty_like(x) for 1 to len(experts) do y[flat_topk_idx == i] experti(x[flat_topk_idx == i]) end for sum(view(y, (gating.shape, 1)) gating.unsqueeze(1), dim = 1) view(y, orig_shape) Return: 24 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Algorithm 3 DiffMoE layer (Training) Input: (input tensor) Variables: (batch size), (flattened sequence length), (hidden dim), (number of experts) Wr (routing weights), experts (list of expert FFNs), (expert capacity) /* Step 1: Batch-level token pool and compute capacity prediction */ view(x, (1, D)) shape(x)[0] capacity_pred capacity_predictor(detach(x)) Ctrain int((S/N ) C) /* Step 2: Compute token-expert affinity scores */ logits einsum(sd, de se, x, Wr) scores softmax(logits, dim = 1).permute(1, 2) gating, index top_k(scores, = Ctrain, dim = 1, sorted = False) /* Step 3: Process tokens through experts */ zeros_like(x) ones zeros(N, S) for 1 to do y[index[i], :] y[index[i], :] + gating[i].unsqueeze(1) experti(x[index[i], :]) ones[i][index[i]] 1. end for /* Step 4: Update capacity threshold */ update_threshold(capacity_pred) /* Step 5: Reshape output */ xout view(y, (B, s, D)) Return: xout, ones, capacity_pred Algorithm 4 DiffMoE layer (Inference) Input: (input tensor) Variables: (batch size), (flattened sequence length), (hidden dim), (number of experts) Wr (routing weights), experts (list of expert FFNs), (expert capacity) threshold (expert threshold) /* Step 1: Reshape input and compute capacity prediction */ view(x, (1, D)) shape(x)[0] capacity_pred sigmoid(capacity_predictor(detach(x))) /* Step 2: Compute token-expert affinity scores */ logits einsum(sd, de se, x, Wr) scores softmax(logits, dim = 1).permute(1, 2) /* Step 3: Process tokens through experts */ zeros_like(x) for 1 to do kpred sum(where(capacity_pred[:, i] > threshold[i], 1, 0)) gating, index top_k(scores[i], = kpred, dim = 1, sorted = False) y[index, :] y[index, :] + gating.unsqueeze(1) experti(x[index, :]) end for /* Step 4: Reshape output */ xout view(y, (B, s, D)) Return: xout 25 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Figure 15: Class-conditional Generation. Uncurated 256256 DiffMoE-L-E8-Flow samples CFG scale = 4.0. 26 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Figure 16: Class-conditional Generation. Uncurated 256256 DiffMoE-L-E8-DDPM samples CFG scale = 4.0. 27 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Figure 17: Text-to-Image Generation. Comparison of DiffMoE-E16-T2I-Flow (w/o SFT) and Dense Model (w/o SFT). 28 DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Figure 18: Text-to-Image Generation. Uncurated 256256 Images generated by DiffMoE-E16-T2I-Flow (w SFT)"
        }
    ],
    "affiliations": [
        "Kuais",
        "Tsinghua University, Beijing, China"
    ]
}