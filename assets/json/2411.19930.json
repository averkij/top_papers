{
    "paper_title": "On Domain-Specific Post-Training for Multimodal Large Language Models",
    "authors": [
        "Daixuan Cheng",
        "Shaohan Huang",
        "Ziyu Zhu",
        "Xintong Zhang",
        "Wayne Xin Zhao",
        "Zhongzhi Luan",
        "Bo Dai",
        "Zhenliang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations."
        },
        {
            "title": "Start",
            "content": "On Domain-Specific Post-Training for Multimodal Large Language Models Daixuan Cheng1 Shaohan Huang2 Zhongzhi Luan2 Bo Dai1(cid:66) 1State Key Laboratory of General Artificial Intelligence, BIGAI 3Tsinghua University 4Beijing Institute of Technology Zhenliang Zhang1(cid:66) 2Beihang University 5Renmin University of China Ziyu Zhu1,3 Xintong Zhang1,4 Wayne Xin Zhao 4 2 0 2 9 2 ] . [ 1 0 3 9 9 1 . 1 1 4 2 : r https://huggingface.co/AdaptLLM"
        },
        {
            "title": "Abstract",
            "content": "Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific imagecaption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage traininginitially on imagecaption pairs followed by visual instruction tasksis commonly adopted for developing general MLLMs, we apply single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domainspecific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations. 1. Introduction Progress toward building general-purpose agents [20, 58] relies critically on the ability to handle multimodal tasks. The recent success of large language models (LLMs) [5, 12, 44] has accelerated the development of multimodal large language models (MLLMs) [2, 16, 23, 24, 28, 35, 43, 52, 54]. By aligning visual encoders [37] with LLMs, MLLMs have shown impressive capabilities in addressing visionlanguage tasks in general scenarios. However, their ex- (cid:66) Corresponding Author. Figure 1. Domain-Specific Performance of AdaMLLM and General MLLM. For each of the two domainsbiomedicine and foodwe conduct post-training to adapt the general MLLM to the target domain and evaluate model performance on various domainspecific tasks. Biomedicine and food tasks are colored gray and orange, respectively. pertise plummets in specialized domains due to insufficient domain-specific training [11]. For instance, scientific fields require learning from specialized images and terminology not commonly found in general scenarios [31, 39, 40, 56]; and industrial applications face privacy constraints that limit data access for training general MLLMs [3, 50]. Domain-specific training for MLLMs requires diverse visual instruction tasks infused with domain knowledge [21]. Recent efforts in this area [7, 22, 32] typically follow the data synthesis and training pipeline used for general training. For data synthesis, manual rules or strong models are employed to generate domain-specific tasks. However, rule-based methods can limit task diversity [10], closed-source models [1, 34] may pose privacy concerns, and open-source models can be constrained by insufficient domain expertise. For training, many works use two-stage pipelinefirst training on image-caption pairs, then on visual instruction tasks [28]. However, tasks in specialized domains are often limited, and splitting them into two stages can further reduce task diversity within each stage. In this paper, we systematically investigate domain1 specific data synthesis and training pipeline for MLLM post-training. While domain-specific image-caption pairs are relatively accessible, domain-specific visual instruction tasks are much harder to obtain [22]. To bridge this gap, we develop visual instruction synthesizer to extract tasks from image-caption pairs. Fine-tuned on seed data collection encompassing various domains and tasks, our synthesizer can effectively leverage domain knowledge in the imagecaption source to generate diverse instruction-response pairs1. To check the accuracy of synthetic responses, rather than directly verifying each response against the instructionwhich requires extensive expertisewe propose selecting tasks with inherently consistent responses. This improves accuracy while reducing the need for expert annotation. Although generated from open-source models, our synthetic tasks improve model performance more effectively than those generated by manual rules [32], GPT4 [22], or GPT-4V [7]. Using our synthetic data, we propose single-stage training pipeline that combines the synthetic task with the image-caption pair in each training example, avoiding the limitations from two-stage training. This simple approach enriches task diversity during training and leads to better performance in most experiments. We conduct experiments in two domains: biomedicine and food, on general MLLMs of different sources and scales, such as Qwen2-VL-2B [45], LLaVA-v1.6-8B [27], and Llama-3.2-VL-11B [14]. As shown in Figure 1, our resulting model, AdaMLLM (for Adapted Multimodal Large Language Model), consistently outperforms the general MLLMs in various domain-specific tasks. In summary, our contributions include: We investigate domain-specific post-training for MLLMs and open-source our implementations. We develop visual instruction synthesis approach based on open-source models, capable of generating diverse tasks from domain-specific image-caption pairs while reducing the need for domain expertise. We propose single-stage post-training method to enhance task diversity during the training phase. Our experiments demonstrate the effectiveness of our method in improving MLLM performance across various tasks in two domains: biomedicine and food. 2. Related Work We discuss related work on data collection and training pipelines for developing domain-specific MLLMs. Domain-Specific Data Initially, Med-Flamingo [33] utilizes multimodal paired and interleaved data. With the rise of visual instruction tuning for general MLLMs [28], 1We use task and instruction-response pairs interchangeably, with instruction as task input and response as task output. research has shifted to synthesizing domain-specific visual instruction data, following approaches for general MLLMs [21, 59]. These approaches fall into two categories: (1) transforming existing datasets into visual instruction formats: LLaVA-Chef [32] and FoodLMM [53] convert domain-specfic datasets [38] using manual rules or GPT-4; (2) prompting closed-source models to generate visual instruction tasks from images/annotations: MedVInT [57], LLaVA-Med [22] and PubMedVision [7] employ GPT-3.5, GPT-4, and GPT-4V, respectively. Our work aligns with the second category in synthesizing data based on image-caption pairs, but we utilize open-source models and effectively synthesize tasks of greater diversity. Domain-Specific Training One type of domain-specific training begins with an unaligned LLM and visual encoder. Similar to the development of general MLLMs, this method uses domain-specific datasets, possibly mixed with general datasets, to align the LLM with the encoder [7, 57]. Another type is post-training, which starts with well-aligned general MLLM [7, 22, 32]. Compared to training from an unaligned LLM and vision encoder, post-training is more efficient in terms of data and computation, making it our preferred method. In domain-specific post-training, many previous works [7, 22, 32] adopt the two-stage training pipeline originally proposed for general MLLMs: first on imagecaption pairs, then on visual instruction tasks. We simplify this into single-stage training pipeline to enhance task diversity within the training phase. 3. Method We adapt MLLMs to domains via post-training on domainspecific datasets. As shown in Figure 2, we begin by synthesizing domain-specific tasks using unified visual instruction synthesizer, followed by consistency-based data filter. These synthetic tasks are then combined with imagecaptioning tasks into single stage for post-training. 3.1. Domain-Specific Visual Instruction Synthesis The effectiveness of visual instruction tasks depends on diversity and accuracy, with domain knowledge being essential for domain adaptation [21]. To meet these requirements, we propose data synthesis approach comprising two main components: visual instruction synthesizer that generates diverse tasks infused with domain-specific knowledge, and consistency-based filter to enhance accuracy. 3.1.1. Visual Instruction Synthesizer While domain-specific image-caption pairs are widely accessible, domain-specific visual instruction tasks are much Inspired by Instruction Preharder Training [10] which develops model to extract tasks from texts, we fine-tune an MLLM to generate diverse tasks to obtain [22]. 2 Figure 2. Method Overview. (A) We fine-tune unified visual instruction synthesizer that generates diverse tasks based on image-caption pairs across various domains. (B) Using this synthesizer, we synthesize tasks based on domain-specific image-caption pairs and then apply consistency-based data filter. The filtered synthetic tasks, combined with the original image captioning tasks, are employed to train general MLLMs through single-stage post-training process, MLLM training loss is computed only on the part colored in orange. based on image-caption pairs across various domains, developing visual instruction synthesizer. Instead of generating domain-specific tasks from scratch, which requires significant expertise, our synthesizer extracts tasks from existing data, thus reducing the reliance on domain expertise from human experts or strong models [1, 34, 41]. Furthermore, we incorporate specific designs to handle situations where the synthesizer struggles to comprehend the image. Seed Data across Domains and Tasks We convert combination of VisionFLAN [51] and ALLaVA [6] into the seed data for fine-tuning our visual instruction synthesizer. As shown in part (A) of Figure 2, each seed data example includes an image-caption pair as input and related task tripletcomprising an instruction, an informative response, and precise responseas output. Specifically, for each image, both the instruction and precise response are annotated by human experts, while the caption and informative response are generated by GPT-4V. Compared to the precise response which is often single phrase, the inforUser: <Image>Describe the image. Assistant: {Caption} User: Answer with precise response. {Instruction} Assistant: {Precise Response} User: Answer with an informative response. {Instruction} Assistant: {Informative Response} Table 1. Data Format for Synthesizer Tuning. The prefixes User and Assistant are determined by synthesizers chat template. Tuning loss is computed only on the part colored in orange. mative response contains many more details, such as chainof-thought reasoning [48]. The seed data encompass wide range of image domains (e.g., art, software) and task types (e.g., object recognition, domain classification). Details on data construction and distribution are in Appendix A. Generalization-Enhanced Multitask Tuning Using the seed data, we conduct multitask fine-tuning on an opensource MLLM to generate task triplets based on the corresponding image-caption pairs. As shown in Table 1, each seed data example is converted into multi-turn conversation to fit the MLLMs conversational format. The first turn presents the image-caption pair. Then the task triplet is divided into two conversational turns. Each turn starts with prompt requesting either precise response or an informative response, followed by the instruction. The order of the precise and informative response turns is shuffled randomly. We calculate the tuning loss only on the turns related to the task triplet, ensuring the synthesizer focuses on them. Furthermore, since generalizing to unseen image types is substantially more challenging than to unseen text types, we replace 10% of the images with blank image during tuning. This simulates cases where the synthesizer struggles to interpret the image, encouraging it to rely more on the caption when facing difficulties in image comprehension. Task Synthesis for Target Domain After tuning, we use the synthesizer to generate task triplets from image-pairs in the target domain. For each image-caption pair, we input it into the synthesizer using the conversational format in Table 1 and extract the task triplet from the output accordingly. 3.1.2. Consistency-Based Filter Developed from an open-source model without sufficient domain expertise, our synthesizer inevitably produces some inaccurate responses, necessitating data filtering. To reduce reliance on experts for validation, we propose filtering tasks based on inherent consistency. This is inspired by ensemble methods [13] and self-consistency decoding [46], enhancing accuracy while reducing the need for expert annotation. As shown in part (B) in Table 2, we prompt an opensource language model to classify each instruction-precise response-informative response triplet into one of three categories: consistent, inconsistent, or open. The consistent and inconsistent categories indicate whether the precise and informative responses align, while the open category indicates tasks that request open-ended responses (e.g., image captions or background information). The prompt template is in Figure 6 in Appendix. We discard triplets classified as inconsistent, as they show low synthesizer confidence, and those classified as open due to their ambiguity. For consistent triplets, we combine the informative and precise responses into chain-of-thought (CoT) format using diverse templates from [29]. The informative response serves as the reasoning process, and the precise response serves as the final conclusion. This ensures both informativeness and accuracy, balancing the concise responses typical of academic datasets with the more detailed responses preferred by humans [26, 51]. 3.2. Domain-Specific Single-Stage Post-Training General MLLM training typically contains multiple stages: then on visual instruction first on image-caption pairs, tasks [28]. This pipeline has also been applied in domainspecific post-training [7, 22, 32]. However, task diversity in domain-specific training is often more limited than in general training, and splitting the training into two stages may further reduce diversity within each stage, negatively impacting the task generalization of the trained models [47]. Moreover, we empirically find that two-stage training can lead the model to catastrophically forget the knowledge/task learned from the first stage when transitioning to the second stage, resulting in worse final performance [30]. To mitigate this, we propose combining the training data into single stage. As shown in part (B) of Figure 2, each training example can include two tasks: Image Captioning Task: The image-caption pair is converted into an image captioning task [28]. question prompting the MLLM to describe the image is randomly chosen from pool in [6] as the task instruction, with the original caption as the ground-truth response. Synthetic Visual Instruction Task: For each image-caption pair with synthetic task after filtering, we combine it with the image captioning task in multi-turn format, with the task order randomized. If no synthetic task remains, only the image captioning task is used. Following [28], we train on the data using the next-token prediction objective [36], computing loss only on the response part of each instruction-response pair. 4. Experiment Settings Given current data availability, we conduct experiments in two domains: biomedicine and food. For each domain, we perform post-training to adapt general MLLMs and evaluate model performance on various domain-specific tasks. Image-Caption Data Source For biomedicine, we use two sources from PubMed Central2: (1) PMCRaw [55] from LLavaMed [22], comprising 470K available images with human-annotated captions, and (2) PMCRef ined from PubMedVision [7], featuring 510K image-caption pairs where the captions have been refined by GPT-4V. For food domain, we convert Recipe1M [38] dataset into image-caption pairs. Each recipe is matched with one image and converted to caption using templates from [32]. Recipes without images are removed, resulting in 130K image-caption pairs. Visual Instruction Synthesis Our vision instruction synthesizer is fine-tuned from the open-source version [8] of LLaVA-v1.6-Llama3-8B [27]. For the consistency-based filter, we prompt Llama-3-8B [14] to evaluate the consistency of each synthesized task triplet. On average, about 30% of the task triplets are reserved after filtering. Details on implementations and synthetic data are in Appendix B. 2https://www.ncbi.nlm.nih.gov/pmc/"
        },
        {
            "title": "Biomedicine",
            "content": "GPT-4o LLaVA-v1.6-8B LLaVA-Med-8B PubMedVision-8B AdaMLLM-8B from PMCRaw AdaMLLM-8B from PMCRef ined Qwen2-VL-2B LLaVA-Med-2B PubMedVision-2B AdaMLLM-2B from PMCRaw AdaMLLM-2B from PMCRef ined Llama-3.2-11B LLaVA-Med-11B PubMedVision-11B AdaMLLM-11B from PMCRaw AdaMLLM-11B from PMCRef ined SLAKE PathVQA VQA-RAD PMC-VQA"
        },
        {
            "title": "OPEN CLOSED OPEN CLOSED OPEN CLOSED",
            "content": "59.1 49.2 43.4 50.0 56.8 58.0 50.0 43.4 45.2 53.2 60.2 56.2 47.6 49.1 56.7 59.5 71.6 62.3 50.2 68.3 76.4 73. 52.4 55.5 63.2 75.2 75.0 63.9 58.7 74.3 77.6 76.4 24.1 15.2 10.1 17.0 19.7 22.9 17.8 11.8 18.2 20.1 20.6 22.7 14.6 19.3 22.2 24. 76.0 47.7 59.2 67.5 79.3 78.6 38.7 60.1 64.7 63.8 53.6 72.1 69.5 70.9 87.3 84.9 51.6 45.9 35.0 43.3 51.0 59. 37.0 37.1 41.3 49.8 58.0 46.9 38.0 46.2 55.0 57.4 64.0 56.3 62.5 67.3 80.5 81.3 46.7 58.8 67.3 74.6 76.1 63.6 69.1 73.9 76.1 79. 56.7 36.5 37.1 40.4 44.3 47.9 45.8 41.2 43.2 43.5 46.5 51.9 47.5 47.1 49.9 51.9 Table 2. Biomedicine Task Performance of general MLLMs and MLLMs after domain-adaptive training. We mark the best performance bold and the second-best underlined. The image-caption sources for AdaMLLM from PMCRaw and AdaMLLM from PMCRef ined are PMCRaw and PMCRef ined, respectively. Post-Training Using synthetic data from the LLaVAv1.6-Llama3-8B-based synthesizer, we conduct domainspecific post-training on LLaVA-v1.6-Llama3-8B itself. Besides, we use the same synthetic data to post-train Qwen2-VL-2B-Instruct [45] and Llama-3.2-11B-VisionInstruct [14] to assess effectiveness across different models and scales. For simplicity, we refer to these models as LLaVA-v1.6-8B, Qwen2-VL-2B, and Llama-3.2-11B, respectively. Training details are in Appendix C. Task Evaluation We evaluate MLLMs on domainspecific tasks without further fine-tuning. For biomedicine, we evaluate on SLAKE [25], PathVQA [15], VQARAD [19], and PMC-VQA [57]. For food domain, we evaluate on Recipe1M [38], FoodSeg103 [49], Food101 [4], and Nutrition5k [42]. Details are in Appendix E. Baseline For biomedicine domain, we compare with two baselines: (1) LLaVA-Med [22] which uses text-only GPT4 to synthesize tasks from PMCRaw, and (2) PubMedVision [7] which uses GPT-4V to synthesize tasks from PMCRef ined. For food domain, we compare with LLaVAChef [32] which uses manual rules to transform imagerecipe pairs from Recipe1M into multiple tasks. All baseline methods employ two-stage post-training. 5. Main Results Overall Performance Tables 2 and 3 compare our models (AdaMLLM) with others in the biomedicine and food"
        },
        {
            "title": "Food",
            "content": "GPT-4o LLaVA-v1.6-8B LLaVA-Chef-8B AdaMLLM-8B Qwen2-VL-2B LLaVA-Chef-2B AdaMLLM-2B Llama-3.2-11B LLaVA-Chef-11B AdaMLLM-11B Recipe Nutrition Food FoodSeg 26.1 18.6 23.1 24.8 18.2 24.1 24.0 23.7 25.7 26.1 46. 29.6 29.1 36.1 36.4 24.5 41.2 40.0 26.2 41.0 89.4 47.9 46.8 65.3 73.9 68.8 72. 80.8 82.1 82.2 61.9 38.9 14.5 42.0 19.9 7.7 23.9 47.6 16.7 42.0 Food Task Performance of general MLLMs and Table 3. MLLMs after domain-adaptive training. We mark the best performance bold and the second-best underlined. domains, using GPT-4o [17] as reference. Our method consistently enhances MLLM performance, outperforming baseline models across various domain-specific tasks. Although our synthesizer is based on LLava-v1.6-8B, we observe consistent improvements on Qwen2-VL-2B and Llama-3.2-11B, demonstrating its effectiveness across different models and scales. Among the evaluated tasks, VQA-RAD and Recipe1M can be regarded as partially seen tasks, with VQA-RAD included in our seed data for finetuning the synthesizer and Recipe1M included in the imagecaption source3. Nevertheless, AdaMLLM shows consistent gains on other unseen tasks, demonstrating its task generalization capabilities in the target domain. 3Test/validation sets of VQA-RAD and Recipe1M are not included. 5 Image-Caption Recipe1M"
        },
        {
            "title": "Train Pipeline",
            "content": "Two-stage Single-stage Two-stage Single-stage Two-stage Single-stage"
        },
        {
            "title": "Rule Ours Rule",
            "content": "LLaVA-v1.6-8B 28.4 31.3 Qwen2-VL-2B 37.7 Llama-3.2-11B 29.0 38.2 40.9 34.1 31.9 36.6 Ours 42.0 40.3 47.8 GPT-4 Ours GPT-4 42.5 44.0 49. 55.6 55.5 59.2 46.1 41.3 48.8 Ours 58.3 54.3 60.7 GPT-4V Ours GPT-4V 50.5 49.0 54.4 58.6 59.5 60. 55.5 51.6 53.7 Ours 60.3 55.7 62.0 Table 4. Domain-Specific Task Performance of MLLMs after Post-Training with different synthetic data and training pipelines. We report the average performance in each domain, with detailed results in Table 14 in Appendix. When the image-caption source and training pipeline are fixed, synthetic data of better performance are marked in bold. When the image-caption source is fixed and our synthetic data are used, numbers marked with indicate that single-stage training outperforms two-stage training, while indicates the opposite. Ours w/o Blank Image w/o Consistency Filter Precise Informative w/o Synthetic Task w/o Image Caption General Task General Task + Domain Caption BioMed. Food 58.3 42.0 55.8 35.9 31.2 37.9 44.4 37.6 26.7 25. 54.2 36.8 49.8 36.0 55.3 38.6 Table 5. Ablation Results. w/o Blank Image fine-tunes the synthesizer without replacing 10% of images with blank ones. w/o Consistency Filter removes the consistency-based filter and trains with either precise or informative responses. w/o Synthetic Task removes synthetic task, and w/o Image Caption removes image captioning task. General Task trains on seed data processed into our task format, General Task + Domain Caption mixes the processed seed data with domain-specific image-caption pairs. Comparison of Synthetic Task and Training Pipeline In addition to the overall comparison, we assess the effectiveness of our synthetic visual instruction tasks and singlestage training separately by varying one factor at time. As shown in Table 4, we conduct both two-stage and singlestage post-training with synthetic tasks generated by different methods: manual rules in LLaVA-Chef, GPT-4 in LLaVA-Med, and GPT-4V in PubMedVision. Our synthetic tasks consistently outperform others across both training pipelines. Furthermore, with our synthetic tasks, singlestage training surpasses two-stage training in most of the experiments. We discuss in Table 14 in Appendix, inferring that two-stage training leads the model to forget the knowledge/task learned in the first stage when transitioning to the second stage, resulting in worse performance across tasks. 6. Ablations To evaluate the effectiveness of each component in our method, we conduct ablations to post-train LLaVA-v1.6-8B with different settings. We report the average task performance within each domain for the trained models in Table 5. The image-caption sources for the biomedicine and food domains are PMCRaw and Recipe1M, respectively. Visual Instruction Synthesis To simulate scenarios where the model struggles to interpret the image, we replace 10% of the images with blank images during the fine-tuning of visual instruction synthesizer. The impact of this strategy is demonstrated in Table 5, where removing this design results in decline in model performance. To improve response accuracy, we design consistencybased filter to select tasks with inherent consistency and combine the informative and precise responses into CoT format. As shown in Table 5, removing this filter results in decreased model performance, regardless of whether the response contains only precise or informative content. Single-Stage Post-Training Our motivation for combining the synthetic task with the image captioning task into single stage is to enhance training task diversity. This efficacy is evident in the ablation results in Table 5, where removing either the synthetic task or the image captioning task from the training data harms the model performance. Domain Knowledge Our domain-specific visual instruction tasks incorporate both domain knowledge and visual instruction task format. To evaluate the contributions of domain knowledge, we conduct comparison with general visual instruction tasks. Our seed data for fine-tuning the synthesizer, which contains diverse tasks across various domains, serves as the source for general tasks. To ensure alignment in visual instruction task format, we apply our consistency-based filter to process and reformat the seed data into our format. As shown in Table 5, our method outperforms both settings that utilize general tasks, underscoring the effectiveness of incorporating domain knowledge. 7. Analysis We conduct detailed analysis of our approach for domainspecific visual instruction synthesis and the synthesized"
        },
        {
            "title": "Diversity\nKnowledge\nComplexity\nAccuracy",
            "content": "- - 52.5 72.5 43.8 63."
        },
        {
            "title": "Image Caption",
            "content": "Image + Caption Image-Caption Recipe1M PMCRaw PMCRef ined - 68.0 95.0 77.9 60.0 - 75.2 93.8 75.3 65.6 81.0 97.5 80.0 66.3 85.5 98.1 83.2 71.3 Instruction Rule Ours GPT-4 Ours GPT-4V Ours Diversity Knowledge Complexity Accuracy 23.5 20.9 38.4 98.7 52.9 21.9 69.9 84. 47.1 44.9 41.7 84.4 58.8 58.9 83.2 75.1 64.7 67.7 49.6 87.5 76.5 63.2 80.5 79.6 Table 6. Quality of Synthetic Tasks by Different Visual Instruction Synthesizers, assessed in terms of task diversity, domain knowledge utilization, task complexity, and response accuracy. Column 1 presents results from the MLLM without finetuning. Columns 2-5 show results after fine-tuning the MLLM using our seed data to synthesize tasks based on different inputs. Besides, Column 5 replaces 10% of the images with blank images. w/o Filter w/ Filter Consist."
        },
        {
            "title": "Precise Acc",
            "content": "Info. Acc Consist. Acc BioMed. Food 30.3 35.7 64.3 77.2 61.0 75.5 92.2 97. 75.1 84.3 Table 7. Quality of Responses with/without Using ConsistencyBased Filter, assessed in terms of consistency between precise and informative responses (Consist.), accuracy of precise responses (Precise Acc), accuracy of informative responses (Info. Acc), and accuracy of combined responses (Acc). data to understand their impact on model performance. 7.1. Domain-Specific Visual Instruction Synthesis Visual Instruction Synthesizer We fine-tune an MLLM to synthesize tasks from image-caption pairs and incorporate design where 10% of the images are replaced with blank images. To evaluate the effectiveness, we compare tasks generated by synthesizers with different designs using validation set from our seed data. Specifically, we conduct human evaluation of data quality in the following aspects (detailed scoring criteria are provided in Appendix D): Task Diversity: Classifies each instruction-response pair as one of the common visual instruction task types and reports the number of distinct types normalized by the total number of common task types. Domain Knowledge Utilization: Measures the extent to which domain-specific knowledge from the image is utilized to complete the task. Task Complexity: Assesses task complexity, with higher scores for tasks requiring reasoning and instructionfollowing abilities. Response Accuracy: Evaluates how accurately the response addresses the instruction. The results in Table 6 indicate that fine-tuning for task synthesis using either image [59] or caption [10] inputs yields improvements in most aspects. Our design, which employs both image and caption inputs, leads to even higher performance. Besides, replacing 10% of the images with Table 8. Quality of Synthetic Tasks by our method, manual rules, GPT-4, and GPT-4V, assessed in terms of task diversity, domain knowledge utilization, task complexity, and response accuracy. Figure 3. Task Type Distribution of all our synthetic tasks based on three image-caption sources. blank ones achieves the highest quality across all metrics. Consistency-Based Filter Our consistency-based filter is designed to select tasks with inherent consistency, thereby increasing data accuracy. As shown in Table 7, using the filter significantly increases the consistency between precise and informative responses, making the combination of them in the chain-of-thought format reasonable. As result, the filter successfully increase response accuracy of the selected data by about 10% in both domains. 7.2. Domain-Specific Synthetic Data Quantitative Analysis Table 8 presents the data quality scores for synthetic tasks generated by different methods. Our tasks are diverse and complex, demonstrating high utilization of domain knowledge. The distribution of task types for all our instruction-response pairs is displayed in Figure 3. This explains the effectiveness of our method in enhancing MLLM performance across domain-specific tasks. However, our method underperforms the baselines in terms of response accuracy, with manual rules achieving nearly 100% accuracy and GPT-4 and GPT-4V reaching around 85%. This may because of the increased complexity of our synthesized tasks, which make generating accurate responses more challenging. These results indicate the 7 Figure 4. Cases of Instruction-Response Pairs synthesized by our method, manual rules, GPT-4, and GPT-4V, the image-caption sources for cases (A), (B), (C) are Recipe1M, PMCRaw and PMCRef ined, respectively. Certain portions are omitted and are represented as (...). need for further improvements to enhance response accuracy, even with highly complex tasks. provided in Figure 8 in Appendix. Qualitative Analysis Figure 4 presents cases of synthetic tasks by different methods when given the same imagecaption pair. In case (A), the rule-based task is simple transformation of the recipe caption, ignoring the image information. In contrast, our task conducts detailed analysis of the foods state in the image and accurately matches it with the cooking step in the caption, demonstrating higher level of domain knowledge utilization. In case (B), both our task and the GPT-4 synthesized task focus on interpreting intent. While the GPT-4 task straightforwardly asks for the intent, our task increases task complexity by requiring inference from the context to make yes/no/not sure choice. In case (C) with multiple sub-images, our task type is distinct in requiring the identification of the least similar image among the group, showcasing task diversity. More cases are 8. Conclusion This paper investigates adapting general MLLMs to specific domains via post-training. To synthesize domainspecific visual instruction tasks, we develop unified visual instruction synthesizer that generates instructionresponse pairs based on domain-specific image-caption data, and then apply consistency-based filter to improve data accuracy. This enables us to effectively synthesize diverse tasks with high domain knowledge utilization. For the post-training pipeline, we propose combining the synthetic tasks with image-captioning tasks into single training stage to enhance task diversity. In two domains biomedicine and foodour resulting model, AdaMLLM, consistently outperforms general MLLMs across various domain-specific tasks. We hope our work can inspire further exploration into MLLM domain adaptation, empowering models for downstream tasks in specialized areas."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 3 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 1 [3] Gagan Bhatia, El Moatez Billah Nagoudi, Hasan Cavusoglu, and Muhammad Abdul-Mageed. Fintral: family of gpt4 level multimodal financial large language models. arXiv preprint arXiv:2402.10986, 2024. 1 [4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random In Computer visionECCV 2014: 13th European forests. conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13, pages 446461. Springer, 2014. 5, 14 [5] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1 [6] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. 3, 4, [7] Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024. 1, 2, 4, 5 [8] Lin Chen and Long Xing. Open-llava-next: An opensource implementation of llava-next series for facilitating the large multi-modal model community. https://github. com/xiaoachen98/Open-LLaVA-NeXT, 2024. 4, 13 [9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 13 [10] Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. Instruction pre-training: Language models are supervised multitask learners. arXiv preprint arXiv:2406.14491, 2024. 1, 2, 7 [11] Daixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations, 2024. 1 [12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with 9 pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. 1 [13] Thomas Dietterich. Ensemble methods in machine learning. In International workshop on multiple classifier systems, pages 115. Springer, 2000. 4 [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2, 4, 5 [15] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. 5, 14 [16] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36:7209672109, 2023. [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 5 [18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 12 [19] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. 5, 14 [20] Yann LeCun. path towards autonomous machine intelligence. Open Review, 2022. 1 [21] Chen Li, Yixiao Ge, Dian Li, and Ying Shan. Visionlanguage instruction tuning: review and analysis. arXiv preprint arXiv:2311.08172, 2023. 1, [22] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024. 1, 2, 4, 5, 14 [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1 [24] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 1 [25] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In"
        },
        {
            "title": "2021 IEEE 18th International Symposium on Biomedical\nImaging (ISBI), pages 1650–1654. IEEE, 2021. 5, 14\n[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 4\n[27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nproved reasoning, ocr, and world knowledge, 2024. 2, 4\n[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024. 1, 2, 4, 13",
            "content": "[29] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pages 22631 22648. PMLR, 2023. 4 [30] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023. 4, 17 [31] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. arXiv preprint arXiv:2308.09442, 2023. 1 [32] Fnu Mohbat and Mohammed Zaki. Llava-chef: multimodal generative model for food recipes. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 17111721, 2024. 1, 2, 4, 5 [33] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. 2 [34] OpenAI. Gpt-4v(ision) system card. 2023. 1, 3 [35] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei. Grounding multimodal large language models to the world. In The Twelfth International Conference on Learning Representations, 2024. 1 [36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1 [38] Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, and Antonio Torralba. Learning cross-modal embeddings for cooking recipes and food images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 30203028, 2017. 2, 4, 5, 14 [39] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Mathllava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. 1 [40] Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, and Self-training large lanarXiv preprint Zhiqiang Tao. guage and vision assistant for medical. arXiv:2406.19973, 2024. 1 Stllava-med: [41] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 3 [42] Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia, Liviu Panait, Tobias Weyand, and Jack Sim. Nutrition5k: Towards automatic nutritional understanding of generic food. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89038911, 2021. 5, 14 [43] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 1 [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [45] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 5 [46] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [47] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 4 [48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 3 [49] Xiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, Steven CH Hoi, and Qianru Sun. large-scale benchmark for food image segmentation. In Proceedings of the 29th ACM international conference on multimedia, pages 506515, 2021. 5, 14 [50] Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang Han, Yuzhe Yang, et al. Open-finllms: Open multimodal 10 large language models for financial applications. preprint arXiv:2408.11878, 2024. arXiv [51] Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, and Lifu Huang. Vision-flan: Scaling human-labeled tasks in visual instruction tuning. arXiv preprint arXiv:2402.11690, 2024. 3, 4, 12 [52] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 1 [53] Yuehao Yin, Huiyan Qi, Bin Zhu, Jingjing Chen, Yu-Gang Jiang, and Chong-Wah Ngo. Foodlmm: versatile food arXiv preprint assistant using large multi-modal model. arXiv:2312.14991, 2023. 2 [54] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, et al. Internlmxcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 1 [55] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for biomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2(3):6, 2023. [56] Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, and Xuerui Mao. Earthgpt: universal multi-modal large language model for multi-sensor image comprehension in remote sensing domain. IEEE Transactions on Geoscience and Remote Sensing, 2024. 1 [57] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. 2, 5, 14 [58] Zhenliang Zhang, Zeyu Zhang, Ziyuan Jiao, Yao Su, Hangxin Liu, Wei Wang, and Song-Chun Zhu. On the emerIn Proceedings of the IEEE gence of symmetrical reality. Conference Virtual Reality and 3D User Interfaces (VR), pages 639649. IEEE, 2024. 1 [59] Henry Hengyuan Zhao, Pan Zhou, and Mike Zheng Shou. Genixer: Empowering multimodal large language models as powerful data generator. arXiv preprint arXiv:2312.06731, 2023. 2, 7 [60] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics."
        },
        {
            "title": "Appendix",
            "content": "A. Seed Data Construction and Distribution the combination of VisionFLAN [51] and We convert ALLaVA [6] into our required format. Each seed data example consists of an image-caption pair as the input and related task triplet as the output, which includes an instruction, an informative response, and precise response. VisionFLAN is human-annotated visual instruction task dataset containing 191 tasks, each with 1K examples. ALLaVA builds on VisionFLAN by using GPT4V to generate caption for each image and to regenerate response for each instruction. In our format, the image, instruction, and human-annotated response from VisionFLAN are used as the image, instruction, and precise response, respectively, while the caption and GPT-4V regenerated response from ALLaVA are used as the caption and informative response. Benefiting from the diversity of existing datasets, our seed data encompass wide range of image domains and task types, as shown in Figure 5. B. Implementation Details of Visual Instruction Synthesis Table 9 presents the hyper-parameters used for synthesizer tuning. We employ the vLLM inference framework [18] to speed up task synthesis and consistency checks. On single A100-80GB GPU, it takes approximately 10 hours to synthesize task triplets and an additional 2.5 hours to perform consistency-based filtering for every 100K image-caption pairs. After applying consistency-based filtering, we collect 150K, 144K, and 32K instruction-response pairs for PMCRaw, PMCRef ined, and Recipe1M, respectively. Hyper-Parameter"
        },
        {
            "title": "Assignment",
            "content": "Base Model Trainable Epoch Batch Size Max Seq Length LRprojector & LLM LRvisual encoder LR Scheduler Weight Decay Warm-Up Ratio Computing Infrastructure Training Time LLaVA-v1.6-8B Full Model 2 128 6144 2e-5 2e-6 Cosine 0 0.03 8 A100-80GB GPUs 13 Hours Table 9. Hyper-Parameters for Synthesizer Tuning"
        },
        {
            "title": "MLLM",
            "content": "LLaVA-v1.6 Qwen2-VL Llama-3.2 Trainable Epoch Batch Size Max Seq Length LRprojector & LLM LRvisual encoder LR Scheduler Weight Decay Warm-Up Ratio Full Model 1 128 6144 2e-5 2e-6 Cosine 0 0.03 Full Model 1 128 6144 1e-5 1e-5 Cosine 0.1 0.1 Full Model 1 128 6144 5e-6 5e-6 Cosine 0.1 0.1 Table 10. Hyper-Parameters for MLLM Single-Stage PostTraining. Image-Caption"
        },
        {
            "title": "PMCRaw",
            "content": "PMCRef ined Recipe1M LLaVA-v1.6-8B 21 3.5 Qwen2-VL-2B 29 Llama-3.2-11B 23 4 31 6 1 9 Figure 5. Distribution of Image Domains and Task Types in Seed Data. Table 11. Training Time (Hours) for MLLM Single-Stage PostTraining on 8 A100-80GB GPUs. 12 C. MLLM Post-Training Settings and Costs Tables 10 and 11 present the hyper-parameters and training time for the single-stage post-training of MLLMs. In the two-stage training experiments, we employ two common approaches for the first stage on image-caption pairs: (1) unfreezing only the vision-language projector [28] for LLaVA-v1.6-8B, and (2) unfreezing the full model [9] for Qwen2-VL-2B and Llama-3.2-11B. During the second stage on visual instruction tasks, we unfreeze the full model across all setups. The hyperparameters for both stages on Qwen2-VL-2B and Llama-3.2-11B are the same as those listed in the table. For LLaVA-v1.6-8B, the first stage differs only in the trainable module, which is the visionlanguage projector, using learning rate of 2e-3. The hyperparameters for the second stage are the same as those listed in the table. We use the code implementations from [8] for experiments on LLaVA-v1.6-8B and from [60] for experiments on Qwen2-VL-2B and Llama-3.2-11B. D. Scoring Criteria for Data Quality For each synthetic dataset, we sample 200 examples and use the following scoring criteria to evaluate data quality in each aspect. The final scores are rescaled to 0-100 range for presentation uniformity. Task Diversity For each instruction-response pair, the annotator selects the most appropriate category from the common vision instruction task types listed below. Once all data samples are annotated, we report the number of distinct task types normalized by the total number of common task types. Domain Classification: Classifying images into domains like race, animal categories, and environment types. Object Recognition: Recognizing detailed objects like animal species, car brands, and specific object types. Pose and Activity Recognition: Identifying specific human poses and activities. Logo Detection: Detecting and recognizing brand logos. Face and Expression Classification: Classifying facial attributes by age, gender, and detecting expressions. Scene Classification: Categorizing images into scene types like beaches, forests, and cities. Sentiment Analysis: Detecting sentiment in images. Caption Generation: Generating captions for images, including general and contextual descriptions. Text Detection and OCR: Recognizing text in images and structured text detection. Image-Text Matching: Assessing image-text similarity and coherence for multimodal content. Anomaly Detection: Identifying anomalies in settings like industrial and road scenes. Style Classification: Classifying images by artistic style and quality. Attribute and Context Recognition: Detecting image attributes and contexts, such as object presence and temporal classification. Task-Oriented Image Recognition: Recognizing objects in structured contexts, like weed species and quick-draw sketches. Step-by-Step Guidance: Recognizing steps in instructional content, like wikihow procedures. Data Representation and Visualization: Visual QA for charts and chart captioning. Utility and Affordance Recognition: Detecting object utility or affordance in images. Visual Grounding: Linking image parts to corresponding words or phrases. Segmentation: Dividing images into meaningful segments, identifying objects or regions. Visual Storytelling: Creating narratives based on series of images. Domain Knowledge Utilization For each instructionresponse pair, the annotator evaluates the extent to which domain-specific knowledge from the image is utilized to complete the task. The scoring follows the criteria below, and we report the average score across all samples. 1: The task is totally irrelevant to the image. 2: The task is relevant, but the question is mundane and answerable without reviewing the image. 3: The task requires reviewing the image, but the question is vague, such as asking for general caption. 4: The task is clear, but the question focuses on only one detail in the image. 5: The task is highly relevant to both the details and overall context of the image. Task Complexity For each instruction-response pair, the annotator assesses task complexity, with higher scores for tasks requiring reasoning and instruction-following abilities, using the criteria below. We report the average score across all samples. 1: The task can be easily completed by mimicking part of the caption. 2: The task can be easily completed by reviewing the image, such as identifying an obvious object. 3: The task requires consideration of the details. 4: The task requires complex reasoning on details and overview. 5: The task requires complex reasoning and instructionfollowing abilities, such as returning the answer in required format. Response Accuracy For each instruction-response pair, the annotator assesses whether the response correctly ad13 datasets, with questions ranging from identifying modalities and organs to complex questions requiring specialized knowledge. All questions are multiple-choice. For the food domain, the task descriptions are as follows: Recipe1M [38] contains recipe information, including titles, ingredients, and cooking instructions. We evaluate models by taking an image and asking for the recipe name, ingredients, and steps. Nutrition5K [42] comprises real-world food dishes with RGB images and nutritional content annotations. We use the ingredient information to create an ingredient prediction task, where the model generates ingredients from an image. Food101 [4] features images across 101 food categories. We ask the model to classify each image into one of the 101 categories. FoodSeg103 [49] includes 103 food categories with images and pixel-wise ingredient annotations. We ask the model to select one or multiple categories from provided list. dresses the task based on the context, using the following criteria. We report the average score across all samples. 1: The response is totally irrelevant to the task instruction. 2: The response attempts to address the instruction, but both the reasoning and conclusion are incorrect. 3: The reasoning is correct, but the conclusion is incorrect. 4: The conclusion is correct, but the reasoning is incorrect. 5: Both the reasoning and conclusion are correct. E. Task Evaluation Details Tables 12 and 13 present the specifications and prompt templates for evaluated tasks in each domain. We conduct zeroshot prompting evaluations on these tasks. For biomedicine, we follow the evaluation approach of [22] for SLAKE, PathVQA, and VQA-RAD, and the method of [57] for PMC-VQA. SLAKE [25] is semantically-labeled, knowledgeenhanced medical VQA dataset with radiology images and diverse QA pairs annotated by physicians. The dataset includes semantic segmentation masks, object detection bounding boxes, and covers various body parts. CLOSED answers are yes/no type, while OPEN answers are one-word or short phrases. We use only the English subset. PathVQA [15] consists of pathology images with QA pairs covering aspects like location, shape, and color. Questions are categorized as OPEN (open-ended) or CLOSED (closed-ended). VQA-RAD [19] includes clinician-generated QA pairs and radiology images spanning the head, chest, and abdomen. Questions are categorized into 11 types, with answers as either OPEN (short text) or CLOSED (yes/no). PMC-VQA [57] is larger and more diverse MedVQA"
        },
        {
            "title": "Task",
            "content": "BioMed."
        },
        {
            "title": "Test Num",
            "content": "Medical question answering SLAKE OPEN [25] Medical binary classification SLAKE CLOSED [25] Medical question answering PathVQA OPEN [15] Medical binary classification PathVQA CLOSED [15] VQA-RAD OPEN [19] Medical question answering VQA-RAD CLOSED [19] Medical binary classification PMC-VQA [57] Medical multi-chioice QA"
        },
        {
            "title": "Recall\nAccuracy\nRecall\nAccuracy\nRecall\nAccuracy\nAccuracy",
            "content": "645 416 3357 3362 179"
        },
        {
            "title": "Food",
            "content": "Recipe1M [38] Nutrition5K [42] Food101 [4] FoodSeg103 [49] Recipe generation Ingredient prediction Food category classification Food multi-label classification Rouge-L Recall Accuracy F1 1000 507 25250 2135 Table 12. Specifications of the Evaluated Domain-Specific Task Datasets."
        },
        {
            "title": "Instruction",
            "content": "BioMed. SLAKE {question}"
        },
        {
            "title": "PathVQA",
            "content": "{question} VQA-RAD {question} PMC-VQA Question: {question} The choices are: {options}"
        },
        {
            "title": "Food",
            "content": "Recipe1M {question}"
        },
        {
            "title": "Response",
            "content": "{answer} {answer} {answer} {option} {recipe} Nutrition5K What ingredients are used to make the dish in the image? {ingredients} Food101 What type of food is shown in this image? Choose one type from the following options: {food type options} FoodSeg103 Identify the food categories present in the image. The available categories are: {options} Please return list of the selected food categories, formatted as list of names like [candy, egg tart, french fries, chocolate]. {food type} {categories} Table 13. Prompt Templates of the Evaluated Domain-Specific Task Datasets. Figure 6. Prompt Template for Consistency-Based Filter (Part 1), continued in Part 2. 15 Figure 7. Prompt Template for Consistency-Based Filter (Part 2). Recipe1M"
        },
        {
            "title": "Train Pipeline",
            "content": "Instruction Recipe Nutrition Food101 FoodSeg AVERAGE LLaVA-v1.6-8B Qwen2-VL-2B Llama-3.2-11B Two-Stage Single-Stage Two-Stage Single-Stage Two-Stage Single-Stage"
        },
        {
            "title": "Rule\nOurs",
            "content": "23.1 16.2 21.8 24.8 24.1 16.5 19.3 24.0 25.7 17.8 21.4 26. 29.1 28.3 36.7 36.1 24.5 43.0 37.1 41.2 26.2 38.0 32.2 41. 46.8 43.5 63.9 65.3 68.8 69.5 64.7 72.0 82.1 74.6 75.8 82. 14.5 28.0 13.9 42.0 7.7 23.9 6.6 23.9 16.7 33.2 16.9 42. 28.4 29.0 34.1 42.0 31.3 38.2 31.9 40.3 37.7 40.9 36.6 47. PMCRaw Train Pipeline Instruction SLAKE PathVQA VQA-RAD OPEN CLOSED OPEN CLOSED OPEN CLOSED PMC-VQA AVERAGE LLaVA-v1.6-8B Qwen2-VL-2B Llama-3.2-11B Two-Stage Single-Stage Two-Stage Single-Stage Two-Stage Single-Stage GPT-4 Ours GPT-4 Ours GPT-4 Ours GPT-4 Ours GPT-4 Ours GPT-4 Ours 43.4 56. 44.2 56.8 43.4 55.2 43.6 53.2 47.6 60.0 46.8 56.7 50.2 71. 59.1 76.4 55.5 74.5 59.6 75.2 58.7 75.7 56.5 77.6 10.1 17. 11.6 19.7 11.8 18.4 13.2 20.1 14.6 22.1 16.0 22.2 59.2 74. 62.2 79.3 60.1 68.4 47.4 63.8 69.5 76.8 69.9 87.3 35.0 50. 38.5 51.0 37.1 48.8 37.3 49.8 38.0 51.4 41.9 55.0 62.5 79. 67.3 80.5 58.8 79.8 57.0 74.6 69.1 80.5 65.4 76.1 37.1 40. 39.9 44.3 41.2 43.8 31.2 43.5 47.5 47.9 45.3 49.9 42.5 55. 46.1 58.3 44.0 55.5 41.3 54.3 49.3 59.2 48.8 60.7 PMCRef ined Train Pipeline Instruction SLAKE PathVQA VQA-RAD OPEN CLOSED OPEN CLOSED OPEN CLOSED PMC-VQA AVERAGE LLaVA-v1.6-8B Qwen2-VL-2B Llama-3.2-11B Two-Stage Single-Stage Two-Stage Single-Stage Two-Stage Single-Stage GPT-4V Ours GPT-4V Ours GPT-4V Ours GPT-4V Ours GPT-4V Ours GPT-4V Ours 50.0 54.8 52.3 58. 45.2 60.8 51.4 60.2 49.1 58.5 47.1 59.5 68.3 73.1 76.2 73. 63.2 76.9 66.1 75.0 74.3 76.4 72.6 76.4 17.0 19.3 20.1 22. 18.2 21.4 18.9 20.6 19.3 27.0 19.5 24.3 67.5 79.7 73.3 78. 64.7 75.0 61.4 53.6 70.9 73.2 70.7 84.9 43.3 55.6 47.0 59. 41.3 55.0 45.1 58.0 46.2 58.3 45.9 57.4 67.3 82.7 76.5 81. 67.3 82.7 73.2 76.1 73.9 77.6 73.9 79.8 40.4 45.1 43.1 47. 43.2 44.7 45.1 46.5 47.1 51.3 46.5 51.9 50.5 58.6 55.5 60. 49.0 59.5 51.6 55.7 54.4 60.3 53.7 62.0 Table 14. Domain-Specific Task Performance of MLLMs after Post-Training with different synthetic data and training pipelines. The image-caption sources are Recipe1M, PMCRaw and PMCRef ined, respectively. In most cases using our synthetic data, we find that singlestage training outperforms two-stage training on domain-specific tasks, particularly evident in the Recipe generation results for the food domain. Recall that in the two-stage training approach for the food domain, the model first trains on recipe captions and then on our synthetic tasks. We examine the task performance of LLaVA-v1.6-8B on Recipe generation and observe that the model achieves score of 25.3 after the first stage on recipe captions. However, this score drastically decreases to 16.2 after the second stage. From this, we infer that the two-stage approach causes the model to catastrophically forget the task/knowledge learned in the first stage when transitioning to the second stage [30], leading to poorer performance after completing the second-stage training. Figure 8. Cases of Instruction-Response Pairs (Part 1) synthesized by our method, manual rules, GPT-4, and GPT-4V, the image-caption sources for the cases are Recipe1M, PMCRaw and PMCRef ined, respectively. Continued in Part 2. In the first case, the rule-based task simply transforms the recipe caption, ignoring the image content. In contrast, our task involves analyzing the foods state in the image and applying food-related knowledge to infer its texture, demonstrating higher level of domain knowledge utilization. In the second case, the GPT-4 generated task straightforwardly asks about the pointing of the red arrow, while ours requires detailed analysis and inference, showing greater task complexity. 18 Figure 9. Cases of Instruction-Response Pairs (Part 2) synthesized by our method, manual rules, GPT-4, and GPT-4V, the image-caption sources for the cases are Recipe1M, PMCRaw and PMCRef ined, respectively. In this case, our task stands out as multiple-choice question, showcasing task diversity."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Beijing Institute of Technology",
        "Renmin University of China",
        "State Key Laboratory of General Artificial Intelligence, BIGAI",
        "Tsinghua University"
    ]
}