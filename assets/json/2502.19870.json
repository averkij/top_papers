{
    "paper_title": "MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge",
    "authors": [
        "Yuntao Du",
        "Kailin Jiang",
        "Zhi Gao",
        "Chenrui Shi",
        "Zilong Zheng",
        "Siyuan Qi",
        "Qing Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge editing techniques have emerged as essential tools for updating the factual knowledge of large language models (LLMs) and multimodal models (LMMs), allowing them to correct outdated or inaccurate information without retraining from scratch. However, existing benchmarks for multimodal knowledge editing primarily focus on entity-level knowledge represented as simple triplets, which fail to capture the complexity of real-world multimodal information. To address this issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse visual knowledge in real-world scenarios. MMKE-Bench addresses these limitations by incorporating three types of editing tasks: visual entity editing, visual semantic editing, and user-specific editing. Besides, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering a more flexible and effective format. The benchmark consists of 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with evaluation questions automatically generated and human-verified. We assess five state-of-the-art knowledge editing methods on three prominent LMMs, revealing that no method excels across all criteria, and that visual and user-specific edits are particularly challenging. MMKE-Bench sets a new standard for evaluating the robustness of multimodal knowledge editing techniques, driving progress in this rapidly evolving field."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 0 7 8 9 1 . 2 0 5 2 : r Published as conference paper at ICLR 2025 MMKE-BENCH: MULTIMODAL EDITING BENCHMARK FOR DIVERSE VISUAL KNOWLEDGE Yuntao Du1,2, Kailin Jiang3,1, Zhi Gao1,4, Chenrui Shi5,1, Zilong Zheng1, Siyuan Qi1, Qing Li1 1State Key Laboratory of General Artificial Intelligence, BIGAI 2School of Software & Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University 3University of Science and Technology of China 4State Key Laboratory of General Artificial Intelligence, Peking University 5Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology"
        },
        {
            "title": "ABSTRACT",
            "content": "Knowledge editing techniques have emerged as essential tools for updating the factual knowledge of large language models (LLMs) and multimodal models (LMMs), allowing them to correct outdated or inaccurate information without retraining from scratch. However, existing benchmarks for multimodal knowledge editing primarily focus on entity-level knowledge represented as simple triplets, which fail to capture the complexity of real-world multimodal information. To address this issue, we introduce MMKE-Bench, comprehensive MultiModal Knowledge Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse visual knowledge in real-world scenarios. MMKE-Bench addresses these limitations by incorporating three types of editing tasks: visual entity editing, visual semantic editing, and user-specific editing. Besides, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering more flexible and effective format. The benchmark consists of 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with evaluation questions automatically generated and human-verified. We assess five state-of-the-art knowledge editing methods on three prominent LMMs, revealing that no method excels across all criteria, and that visual and user-specific edits are particularly challenging. MMKEBench sets new standard for evaluating the robustness of multimodal knowledge editing techniques, driving progress in this rapidly evolving field. Project Page: https://mmke-bench-iclr.github.io/"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) and multimodal models (LMMs) have demonstrated remarkable success across various tasks due to their powerful understanding and reasoning abilities, grounded in vast amounts of knowledge (Brown et al., 2020; Zhao et al., 2023; Liu et al., 2024b). However, the knowledge within these models can become outdated or inaccurate over time due to evolving real-world information and changes in factual data. To address this, knowledge editing techniques have been developed to correct inaccuracies and inject new knowledge into pre-trained models with minimal cost, without affecting unrelated content (Mitchell et al., 2022b; Yao et al., 2023). In recent years, several datasets have been introduced to benchmark the progress of knowledge editing methods in both the textual (Yao et al., 2023; Onoe et al., 2023; Cao et al., 2021; Li et al., 2023b) and multimodal domains (Cheng et al., 2023; Huang et al., 2024; Li et al., 2024; Zhang et al., 2024). However, most existing benchmarks focus on editing entity-level knowledge, typically formatted as triplet (subject, relation, object). While effective in certain tasks, this format lacks the complexity required for real-world applications, particularly in multimodal domains where visual knowledge must also encompass actions, body gestures, and object relationships. Furthermore, knowledge editing techniques have quickly saturated on these benchmarks, achieving near-perfect performance. Equal contribution. Corresponding author. 1 Published as conference paper at ICLR 2025 Figure 1: Comparison between the existing benchmark and MMKE-Bench with detailed example. In this example, the texts in red represent the edited counterfactual content. T/I-Rel represents text and image reliability, T/I-Gen represents text and image generalization and Port represents portability. Previous benchmarks mainly focus on entity recognition editing using triplet-based knowledge representation format, which does not align with actual scenarios. MMKE-Bench focuses on evaluating diverse semantic editing in realistic scenarios in natural language format. For example, simply fine-tuning the LLaVA model achieved 99.59%, 99.43%, and 95.48% accuracies for reliability, text generalization, and image generalization, respectively, on the VLKEB benchmark Huang et al. (2024). This highlights the urgent need for more challenging benchmark to foster the development of multimodal knowledge editing techniques. To address these issues, we introduce MMKE-Bench, comprehensive multimodal knowledge editing benchmark designed to evaluate diverse semantic editing in real-world scenarios. MMKE-Bench represents multimodal knowledge using free-form natural language descriptions paired with images, providing richer and more flexible expression of interconnected information. Reflecting real-world needs, MMKE-Bench includes three types of editing: visual entity editing, visual semantic editing, and user-specific editing. Visual entity editing updates entity-centric visual knowledge, while visual semantic editing targets complex object behaviors and relationships, such as referee gestures and traffic signals. Lastly, user-specific editing evaluates the models ability to integrate individualized knowledge. The first two types modify existing knowledge, while the third adds new knowledge. Comparisons with existing benchmarks are shown in Fig.1 and Tab.1. To construct MMKE-Bench, we first collect original knowledge from various images and knowledge sources (e.g., multimodal knowledge graphs, demo videos, Google, and LLM generation). Next, we create editing knowledge by applying counterfactual editing for the text modality and image replacement for the image modality. User-specific editing involves adding entirely new, personalized knowledge to the model and does not need counterfactual editing. Following previous works (Zheng et al., 2023; Huang et al., 2024), we adhere to four evaluation principles: reliability, locality, generalization, and portability, generating evaluation questions and answers automatically. Finally, all questions and answers undergo human verification and are revised where necessary. The resulting benchmark contains 2,940 pieces of knowledge and 8,363 images across 33 broad categories. We evaluate five of the most prominent multimodal knowledge editing methods on three representative LMMs, assessing their performance in both single and sequential editing tasks. Empirically, we find that (i) no single editing method excels across all evaluation criteria; (ii) visual knowledge and user-specific knowledge are more difficult for LMMs to edit; (iii) modern LMMs excel in producing and applying edited knowledge; and (iv) the proposed benchmark proves more challenging than previous benchmarks. 2 Published as conference paper at ICLR Table 1: Overall comparison with existing multimodal knowledge editing benchmarks. Benchmark Knowledge Representation Visual Entity Editing Visual Semantic Editing User-Specific Editing Evaluation Principle MMEdit MIKE MC-MKE VLKEB MMKE-Bench Short-Text Triplet Triplet Triplet Free-Form Natural Language Reliability, Locality, and Generalization Reliability, Locality, and Generalization Reliability, Locality, and Generalization Reliability, Locality, Generalization, and Portability Reliability, Locality, Generalization, and Portability To sum up, our contribution can be summarized as follows: We propose MMKE-Bench, challenging benchmark for evaluating diverse semantic editing in real-world scenarios. It adopts free-form natural language-based knowledge representation and includes three types of editing aligned with real-world contexts. We introduce novel pipeline for benchmark construction that collects original knowledge, generates editing knowledge, and produces evaluation questions guided by four principles. Extensive experiments with various baseline methods and LMMs in both single and sequential editing settings are conducted, revealing several limitations in existing knowledge editing approaches."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 LARGE MULTIMODAL MODEL Large multimodal models have achieved excellent performance in various multimodal understanding tasks due to vast knowledge and effective cross-modality alignment. Typically, such models integrate vision encoder with pertained large language model, linking the two components by an alignment module. Notably, BLIP-2 (Li et al., 2023a) adopts Q-Former, lightweight Transformer, as the alignment module. Inspired by the instruction tuning in LMM, MiniGPT-4 (Zhu et al., 2023) and InstructBLIP (Dai et al., 2023) enhance this structure with multimodal instruction tuning. In contrast, LLaVA (Liu et al., 2024b) utilizes an MLP layer for alignment and proposes to generate an instructiontuning dataset by self-instruct strategy (Wang et al., 2022). Qwen-VL (Bai et al., 2023) introduces novel module, the visual receptor, as its alignment module and proposes three-stage training pipeline, achieving excellent performance across various multimodal tasks. Besides, several notable LMMs, such as mPLUG-DocOw 1.5 (Hu et al., 2024), InternVL-2 (Chen et al., 2024), and MiniCPM-V 2.5 (Yao et al., 2024), have also achieved comparable or even superior results compared with GPT-4o. 2.2 KNOWLEDGE EDITING FOR LARGE LANGUAGE MODEL Existing methods for LLM can be divided into three categories: resorting to external knowledge, incorporating knowledge into the model, and editing internal knowledge. Resorting to external knowledge typically involves maintaining memory and retrieving the most relevant cases for each input. For instance, IKE Zheng et al. (2023) provides in-context learning example support by building three types of demo examples: copy, update, and retain. SERAC Mitchell et al. (2022b) builds new counterfactual model by keeping the base model and using scope classifier to determine whether to answer with counterfactual model. The category of merging the knowledge into the model aims to learn representations of the new knowledge and incorporate this information into the model. Eva-KELLM Wu et al. (2023a) employs LoRA for knowledge editing, while GRACE (Hartvigsen et al., 2023) adopts novel approach by maintaining discrete codebook functioning as an adapter. Lastly, editing intrinsic knowledge works on directly modifying the models weight using knowledgespecific methods through meta-learning and localization editing. The meta-learning method trains hypernetwork to learn how to adjust the model. KE De Cao et al. (2021) utilizes new knowledge representations directly to train the model to update the matrix, while MEND Mitchell et al. (2022a) applies rank-one decomposition to divide the model into two rank matrices. Additionally, localization approaches, like ROME Meng et al. (2022) and MEMIT, Meng et al. (2024) employ causal analysis method to detect which parts of the hidden state are more important by treating editing as minimal optimization, ensuring its reliability and non-circumvention. 2.3 KNOWLEDGE EDITING FOR LARGE MULTIMODAL MODEL Recently, several benchmarks have been proposed to evaluate the performance of editing LMMs. The MMEdit benchmark (Cheng et al., 2023) systematically defines the first evaluation framework 3 Published as conference paper at ICLR 2025 for multimodal knowledge editing based on visual question answering and image caption tasks. As the MMEdit could not assess fine-grained entity knowledge, subsequent evaluation benchmarks focus on fine-grained entity recognition editing. MIKE (Li et al., 2024) evaluates recognizing new entities while VLKEB (Huang et al., 2024) targets editing known entities and introduces portability evaluation principle. MC-MKE (Zhang et al., 2024) further extends fine-grained entity recognition by emphasizing modality consistency. However, these benchmarks mainly represent editing knowledge through triples and overlook diverse semantic editing in realistic scenarios."
        },
        {
            "title": "3 PROBLEM DEFINITION",
            "content": "3.1 KNOWLEDGE REPRESENTATION AND EDITING MMKE-Bench is distinctive in evaluating diverse semantic editing in realistic scenarios, leveraging natural language-based knowledge representation. It includes three types of editing: visual entity editing, visual semantic editing, and user-specific editing. Each piece of knowledge is represented in unified format, = (i, d), where refers to the image and represents the natural language description of the main object, visual content, or user-personalized item. For example, in the case of referees gesture, the image captures the action performed by the referee, while the description explains how the gesture is executed and its impact on the match. During knowledge editing, the original knowledge is transformed into ke = (ie, de) in both visual entity and visual semantic editing, while it remains ke = (i, d) for user-specific editing. This is because user-specific editing introduces entirely new personalized knowledge into LMMs without needing to alter the image or description. 3.2 EDITING TYPE OF MMKE-BENCH Considering real-world needs, MMKE-Bench includes three types of editing as follows. Visual Entity Editing This type targets entity-centric modifications and the description covers multiple aspects of an entity. In realistic scenarios, models may misidentify or retain incorrect or outdated information about the entity. Visual entity editing addresses this issue by allowing for simultaneous correction of all related content. To simulate such scenarios, we propose replacing the original image of the entity with that of another entity of the same type and modifying key information into counterfactual content. As shown in Fig.1, Zlatan Ibrahimovics image is replaced with that of Wayne Rooney, and related information (e.g., nationality, club) is altered to counterfactual details. Visual Semantic Editing This type focuses on complex visual semantics-centric modifications, encompassing body gestures, actions, object relationships, and so on. The description provides detailed information about the semantic action and its rules or meanings. The LMMs may misrecognize and misunderstand these semantics, but visual semantic editing can address this issue by modifying both actions images, and meanings simultaneously. To simulate this, this type of editing also involves replacing the image of one semantic action with that of another action of the same type and altering the rule or meaning to counterfactual content. As shown in Fig.1, the offside gesture in soccer is replaced with that of substitution, and the associated rule (e.g. kick-off location) is modified to counterfactual contents. User-Specific Editing This type focuses on injecting personalized user information into LMMs, and the description details the relationship between the user and the object, as well as their experiences. As there is growing demand for LMMs to function as personalized AI assistants that can remember relevant user information, user-specific editing is designed to meet this need. Pre-trained LMMs serve as general models, so all user-specific information is treated as new knowledge for LMM. Thus, counterfactual editing is unnecessary, and original knowledge is used as editing knowledge. For example, Fig.1 describes the relationship between the toy puppet and the users habits."
        },
        {
            "title": "4 BENCHMARK",
            "content": "As shown in Fig. 2, we construct the benchmark through four steps: i) Original Knowledge Collection; ii) Editing Knowledge Generation; iii) Evaluation Question Generation; and iv) Human Verification. 4 Published as conference paper at ICLR 2025 Figure 2: The construction pipeline of MMKE-Bench. 4.1 ORIGINAL KNOWLEDGE COLLECTION In gathering original knowledge, we first list candidate fine-grained entities, visual semantics, or user-specific items, and then collect their corresponding images and descriptions. For visual entity editing, we source candidates from two datasets: the multimodal knowledge graph, MMpedia Wu et al. (2023b), and the visual entity recognition dataset, OVEN Hu et al. (2023). For each entity selected from the existing dataset, we get their images from the datasets and then manually review the images by removing the entities that cannot uniquely identify the main entity from images and noise images. For entities with less than two images, we recollect additional images by crawling from Google. Next, we retrieve entity descriptions from the Wikipedia summary dumps1 and summarize the description by an LLM to generate the final descriptions. As shown in Fig. 3, this type covers 10 broad categories. For visual semantic editing, as shown in Fig. 3, we define the candidates across 14 broad categories of semantic knowledge, including single-person behaviors, singleobject behaviors or attributes, object relationships, and global structures. For certain types of visual knowledge that have corresponding datasets, such as object relationships, textures, and art styles, we collect both the candidate semantics and associated images from these datasets. For other cases, we extract images from demonstration videos or gather them via Google, applying human verification for quality control. Descriptions of the visual semantic actions, along with the rules or meanings conveyed by these behaviors, are generated with the assistance of LLM or human writers. Details of the image sources are provided in the appendix. Figure 3: The types of samples in MMKE-Bench. For user-specific editing, we consider 9 broad categories of personalized information sources, such as favorite singers, owned pets, and alma maters. For personal items and pets, we gather candidates and images from the existing personalized research works Nguyen et al. (2024); Alaluf et al. (2024). For singers, actors, and cartoon characters, we first generate candidate list and then crawl images from Google. For other categories, including company, university, sports club, and organization, we source candidates from MMpedia, manually verifying and removing noise images. Finally, we employ an LLM to generate personalized relationships and experiences between the user and these objects. 1https://dumps.wikimedia.org/enwiki/20240620/ Published as conference paper at ICLR 2025 4.2 EDITING KNOWLEDGE GENERATION Considering the multimodal nature of large multimodal models (LMMs), we propose editing both text and visual modalities when constructing the benchmark. Specifically, we focus on editing visual entities and visual semantic knowledge while leaving user-specific knowledge unchanged. The former is treated as knowledge editing, while the latter is regarded as knowledge insertion. For the visual modality, we follow the image-replacement-based editing approach from previous work Huang et al. (2024), where an image of the entity or semantic action is randomly replaced with another of the same type. For example, as illustrated in Fig. 1 and Fig. 2, the assistant referees offside penalty gesture is replaced with substitution gesture in the edited visual content. In the text modality, we modify key information about the entity and the rule or meaning into counterfactual content for visual entity editing and visual semantic editing, respectively. Additionally, we update the action description to align with the new visual content. In the example of the offside gesture, the original action description is replaced with that of the substitution gesture, and the kick-off location is edited from the foul position to the penalty spot. 4.3 EVALUATION QUESTION GENERATION We adhere to four key evaluation principles to generate both the questions and answers. The reliability and portability questions are generated by prompting LLM and we show the prompts in the appendix. Reliability Question Generation The reliability criterion assesses whether the edited knowledge is correctly produced after the editing process. When generating questions and answers, we prompt the LLM with requirement that the question must ask one aspect of the edited counterfactual content (e.g., the kick-off location of the offside penalty). To evaluate this, we consider both text reliability and image reliability, measuring the LMMs ability to edit across text and visual modalities. Text reliability questions are crafted to be answerable without images, while image reliability questions use the format {the type in the image} to reference the main object, behavior, or personalized item. An example is provided in Fig. 2. We denote the reliability question sets as Qrel = (ie, qr, ar), where ie represents the edited image, qr the question, and ar the answer. Let Mθ and θ denote the original and edited LMMs, respectively, and I[] denoted indicator function, reliability is then evaluated as: [M E(ie,qr,ar)Qrel θ(ie, qr) = ar] (1) Locality Question Generation The locality criterion evaluates how much unrelated knowledge remains unchanged in the edited model by comparing its outputs before and after the editing process. For locality, we assess both text and image locality, which tests the models stability when dealing with out-of-scope knowledge from each modality. Following prior work, we source locality questions and answers from the VLKEB benchmark Huang et al. (2024), where the text questions are drawn from the NQ dataset Kwiatkowski et al. (2019), and the image questions are specifically designed by VLKEB. We represent the locality question set as Qloc = (il, ql), and locality is evaluated as: E(il,ql)Qloc [Mθ(il, ql) = θ(il, ql)] (2) Generalization Question Generation The generalization criterion evaluates how effectively the model responds to neighboring samples. Unlike triplet-based knowledge editing, we focus exclusively on image generalization, as text generalization is not considered due to the free-form knowledge format. For image generalization, we randomly select another image ig from the multiple available images of an entity, visual behavior, or personalized item, and reuse the same question and answer from the image reliability, with an example shown in Fig. 2. We define the generalization question as Qgen = (ig e, qg, ag), where qg = qr and ag = ar for the same object. Generalization is evaluated as: θ(ig e, qg) = ag] ,qg,ag)Qgen [M E(ig (3) Portability Question Generation The portability criterion evaluates whether the edited knowledge can be successfully applied to related content. Following prior work Huang et al. (2024), we adopt text portability evaluation for visual entity editing and image modality portability for visual semantic and user-specific editing to enhance visual modality evaluation. For visual entity editing, we generate questions about the edited content, utilizing supplementary information from Wikipedia for question generation. For example, if the current entity is the Eiffel Published as conference paper at ICLR 2025 Tower and the edited content refers to the buildings designer, we might create question like, Who is the designer of the Eiffel Tower? We can then generate another question about the edited content, such as asking for the designers birth year. By combining these two questions, we can formulate the final probability question: In which year was the builder of the Eiffel Tower born? In the case of visual semantic and user-specific editing, we first combine the image of the main behavior or item with another image of the same type to create new image, denoted as ip e. We then pose question focusing on the differences between the two images, such as hair color or object shape. By integrating this question with one related to the edited content, we derive the final portability question. For instance, as shown in Fig. 2, given an image that includes the offside penalty gesture and the corner-kick gesture made by two assistant referees, we might ask, What color is the tops of the referee who is making the offside gesture in the image?. Denote the portability question as Qport = (ip e, qp, ap), portability is evaluated as: E(ip ,qp,ap)Qport [M θ(ip e, qp) = ap] (4) 4.4 HUMAN CHECK & BENCHMARK STATISTICS During benchmark construction, we manually collected, reviewed, and filtered the samples multiple times. In the original knowledge collection stage, we conducted thorough manual review of the images associated with each entity, behavior, and object to ensure the quality of the collected visuals. Furthermore, after counterfactual editing and question generation, we manually reviewed the questions, revised unsuitable questions, and corrected wrong answers. The statistics of MMKE-Bench are shown in Tab.2. MMKE-Bench encompasses three classes of edited knowledge, totaling 2,940 knowledge pieces and 8,363 images. The knowledge spans 175 finegrained types, highlighting the diversity of MMKEBench. We split the dataset into training and validation sets at 4:6, with the training set reserved solely for specific knowledge editing methods (e.g., SERAC Mitchell et al. (2022b)). 5 EXPERIEMENT 5.1 EXPERIMENTAL SETUP Visual Entity Editing Visual Semantic Editing User-Specific Editing 76 65 34 Table 2: The statistics of MMKE-Bench. Types Train Test 636 214 331 955 293 Images 3,534 3,201 1,628 LMMs and Editing Methods To evaluate our benchmark, we conduct experiments on three representative LMMs: BLIP-2 (Li et al., 2023a), MiniGPT-4 (Zhu et al., 2023), and LLaVA-1.5 (Liu et al., 2024a). Besides, following the previous benchmarks, we select five representative multimodal knowledge editing methods: 1) Fine-tuning (FT). We focus on finetuning the LLM (FT-LLM) or the vision-language alignment module (FT-Alignment), where only the last layer of the LLM is finetuned.2) Knowledge Editor (KE) (De Cao et al., 2021). KE uses hyper-network with constrained optimization to predict the weight update at test time. 3) MEND (Mitchell et al., 2022a): MEND learns low-rank decomposition of the gradient of standard fine-tuning. 4) SERAC (Mitchell et al., 2022b): SERAC is memory-based method and it stores edits in explicit memory. 5) In-context Knowledge Editing (IKE) (Zheng et al., 2023): IKE is inspired by in-context learning, and new demonstration formatting and organization strategies are to construct for guiding knowledge editing. Experiments settings We perform experiments under both single editing and sequential editing. Single editing is mostly adopted and it updates the base model for each piece of knowledge and then evaluates the editing performance. The sequential editing continuously updates the base model with multiple pieces of knowledge and then evaluates the first piece of knowledge. We follow the previous benchmark and adopt the token-level editing accuracy. 5.2 REULTS 5.2.1 SINGLE EDITING RESULTS The results of the existing multimodal knowledge editing methods on MMKE-Bench are shown in Tab. 3, Tab. 4, and Tab. 5. Based on the results, we have several observations. 1) FT-LLM is strong baseline, while IKE demonstrates the best reliability and generalization. FT-LLM serves as strong baseline, with other multimodal knowledge editing methods like SERAC, MEND, and KE performing similarly or even worse than FT-LLM. Notably, IKE achieves the best results across nearly all knowledge editing tasks for three LMMs, excelling in text reliability, image 7 Published as conference paper at ICLR 2025 Table 3: The results of single editing for BLIP-2 on MMKE-Bench. Visual Entity Editing Visual Semantic Editing User-Specific Editing Average Method T-Loc I-Loc T-Rel I-Rel I-Gen Port FT-LLM FT-Alignment IKE SERAC MEND KE FT-LLM FT-Alignment IKE SERAC MEND KE FT-LLM FT-Alignment IKE SERAC MEND KE FT-LLM FT-Alignment IKE SERAC MEND KE 69.76 100.00 55.77 99.99 96.02 83.61 64.11 100.00 47.10 99.90 97.29 67.85 61.28 100.00 47.39 100.00 96.95 65.70 65.05 100.00 50.09 99.96 96.75 72.39 21.47 8.83 13.19 99.69 69.37 18. 19.25 9.48 13.92 99.16 74.35 14.39 20.49 8.74 12.25 99.76 76.21 15.73 20.40 9.02 13.12 99.54 73.31 16.05 39.21 20.89 41.88 20.90 35.67 28.14 33.42 18.17 35.56 18.26 28.26 30.97 12.52 7.46 13.25 7.46 11.06 12. 28.38 15.51 30.23 15.54 25.00 23.97 35.76 27.51 41.80 20.27 34.41 28.25 30.79 35.81 42.07 18.61 30.79 24.48 27.33 17.19 31.04 14.20 25.21 19.83 31.29 26.84 38.30 17.69 30.14 24.19 36.21 27.02 41.76 20.49 34.48 28. 30.71 32.67 41.1 17.96 31.11 24.85 27.80 17.31 30.71 14.50 25.19 19.71 31.57 25.67 37.86 17.65 30.26 24.34 18.11 19.25 25.93 19.76 21.31 30.76 2.76 5.15 5.03 3.81 3.87 6.70 5.46 6.17 6.03 5.10 5.22 10. 8.78 10.19 12.33 9.56 10.13 16.09 Table 4: The results of single editing for MiniGPT4 on MMKE-Bench. Visual Entity Editing Visual Semantic Editing User-Specific Editing Average Method T-Loc I-Loc T-Rel I-Rel I-Gen Port FT-LLM FT-Alignment IKE SERAC MEND KE FT-LLM FT-Alignment IKE SERAC MEND KE FT-LLM FT-Alignment IKE SERAC MEND KE FT-LLM FT-Alignment IKE SERAC MEND KE 84.13 100.00 75.50 99.97 97.49 76. 83.96 100.00 66.45 98.70 97.34 84.14 83.13 100.00 75.35 100.00 97.47 78.46 83.74 100.00 72.43 99.56 97.43 79.68 31.53 24.85 15.25 99.76 77.70 18.47 31.54 25.20 12.79 98.80 77.16 21.25 34.04 25.30 14.56 99.90 79.19 20. 32.37 25.12 14.20 99.49 78.02 19.95 49.22 31.89 56.42 31.88 47.26 41.28 44.45 24.93 55.44 27.08 37.45 38.14 39.74 21.07 61.55 21.09 28.70 22.60 44.47 25.96 57.80 26.68 37.80 34.01 41.13 33.87 53.80 30.53 42.20 40. 44.85 46.45 54.85 29.65 42.17 35.23 38.94 33.25 54.86 30.63 40.94 37.91 41.64 37.86 54.50 30.27 41.77 37.72 41.40 33.93 53.72 30.35 41.82 40.44 43.91 42.29 53.01 28.33 42.62 33.94 38.60 33.40 54.81 30.27 40.25 37. 41.30 36.54 53.85 29.65 41.56 37.37 31.25 30.79 41.09 33.43 34.43 41.55 8.16 11.43 10.50 10.35 8.65 14.72 10.53 12.33 11.85 10.50 11.34 19.92 16.65 18.18 21.15 18.09 18.14 25.40 reliability, and image generalization. These results indicate that in-context examples significantly enhance the models understanding of how knowledge is edited, leading to superior performance. 2) Image locality is more challenging than text locality, and SERAC and MEND perform best in maintaining locality. Most knowledge editing methods deliver better text locality results compared to image locality, suggesting that editing LMMs tends to compromise visual knowledge more severely, resulting in lower image locality scores. SERAC and MEND stand out by achieving high locality results. It may owe to the good retrieval accuracy of SERAC and fewer parameter updates by MEND. 3) All knowledge editing methods generalize well but struggle with portability. The I-gen results mirror those of I-rel, indicating that current large multimodal models can extract invariant features across different image variants of the same object. However, all existing multimodal methods fall short in the portability evaluation, highlighting the difficulty of applying edited knowledge to new content. KE performs best portability in most scenarios, suggesting that parameter-based editing methods handle this challenge more effectively. 4) Visual Semantic Knowledge and User-Specific Knowledge are more difficult for LMMs to edit. Editing complex visual semantics and user-specific knowledge proves more challenging than 8 Published as conference paper at ICLR 2025 Table 5: The results of single editing for LLaVA on MMKE-Bench. Visual Entity Editing Visual Semantic Editing User-Specific Editing Average Method T-Loc I-Loc T-Rel I-Rel I-Gen Port FT-LLM FT-Alignment IKE SERAC MEND KE FT-LLM FT-Alignment IKE SERAC MEND KE FT-LLM FT-Alignment IKE SERAC MEND KE FT-LLM FT-Alignment IKE SERAC MEND KE 77.71 100.00 68.25 99.87 97.32 79.89 77.81 100.00 64.11 99.90 98.27 74.61 75.08 100.00 63.48 99.99 98.49 79. 76.87 100.00 65.28 99.92 98.03 78.00 17.58 9.15 17.43 99.26 75.29 18.73 16.11 11.45 19.44 99.98 82.90 7.95 20.41 10.87 18.93 99.81 85.41 10.80 18.03 10.49 18.60 99.68 81.20 12.49 53.89 35.72 63.49 35.7 51.30 46. 49.18 28.92 63.54 29.01 41.21 47.82 58.18 42.40 75.65 42.24 50.92 54.85 53.75 35.68 67.56 35.65 47.81 49.71 49.54 38.65 59.98 35.02 47.21 46.19 48.28 51.41 61.92 29.97 46.64 38.78 47.80 40.21 62.73 36.29 45.14 48. 48.54 43.42 61.54 33.76 46.33 44.54 49.30 39.74 59.98 34.98 46.58 46.29 47.49 40.72 61.31 29.17 45.90 37.49 48.56 43.65 62.79 36.67 44.86 49.46 48.45 41.37 61.36 33.61 45.78 44.41 41.23 37.62 51.30 40.24 41.83 48. 14.48 27.84 26.08 20.73 23.29 24.07 13.11 23.35 22.87 13.63 14.49 23.67 22.94 29.60 33.42 24.87 26.54 32.17 editing visual entities, as evidenced by lower reliability and portability scores. This suggests that more advanced editing techniques are needed to edit complex visual semantics and inject personalized information, further emphasizing the value of the proposed benchmark. 5) Modern LMMs excel in producing and applying edited knowledge. For reliability, generalization, and portability evaluations, LLaVA-1.5 outperforms BLIP-2 and MiniGPT-4. This improved performance can be attributed to its larger model size and better instruction-following capability, as LLaVA-1.5 has more parameters than BLIP-2 and more refined instruction-tuning design than MiniGPT-4. These factors lead to its superior ability to understand and apply evolving knowledge. 6) No single editing method excels across all evaluation criteria. In conclusion, no single knowledge editing method outperforms across all four evaluation criteria. In-context learning-based methods are strong at reproducing edited knowledge, memorybased methods excel at preserving unrelated content, and parameter-based methods are better at applying edited knowledge to new contexts. 7) The proposed benchmark is more challenging than previous ones. The comparison of IKE with existing benchmarks for MiniGPT-4 is shown in Fig. 4, this method achieves high scores across most evaluation principles in previous benchmarks but performs worse on our benchmark. This suggests that the proposed benchmark introduces greater challenges than its predecessors. 5.2.2 SEQUENTIAL EDITING RESULTS Figure 4: Evaluation comparison of IKE for MiniGPT-4 with existing benchmarks. Port for MMEdit and MIKE, is set 1, as they are not evaluated. Editing knowledge separately is impractical in real-world applications while continuous updates with vast amounts of information are necessary. Consequently, we conduct sequential editing experiments and utilize FT-LLM, FT-Alignment, and SERAC as editing methods. IKE and KE are excluded because the edit samples also need to serve as test samples, which is not feasible in this context. The results for LLaVA-1.5 are shown in Tab. 6, where the gap refers to the sequential length, and user num is the number of users, with each user allowed maximum of nine personalized items. As observed, both FT-LLM and FT-Alignment tend to forget the previous editing, as shown by the decreasing performance in text and image reliability and generalization with increasing gap. In contrast, SERAC effectively maintains edited knowledge due to its explicit memory. Additionally, FT-Alignment often preserves unrelated text outputs, while FT-LLM exhibits the opposite behavior. Published as conference paper at ICLR 2025 Table 6: The results of sequential editing for LLaVA-1.5 on MMKE-Bench. Method GAP /User Num T-Loc I-Loc T-Rel I-Rel I-Gen Port FT-LLM Visual Entity Editing FT-Alignment SERAC FT-LLM Visual Semantic Editing FT-Alignment SERAC FT-LLM User-Specific Editing FT-Alignment SERAC - 3 6 10 - 3 6 10 - 3 6 - 3 6 10 - 3 6 10 - 3 6 10 - 1 3 5 - 1 3 5 - 1 3 78.91 58.10 58.40 58.18 100.00 100.00 100.00 100.00 99.76 99.69 99.69 99.69 76.89 50.33 49.09 48.23 100.00 100.00 100.00 100.00 100.00 99.93 99.93 99. 75.44 70.76 68.87 68.31 100.00 100.00 100.00 100.00 99.98 100.00 100.00 100.00 18.16 8.34 8.20 8.09 9.42 1.10 1.58 1.33 99.24 98.37 98.36 98. 16.14 7.36 7.25 7.02 19.41 1.44 1.38 1.38 34.53 13.56 13.54 13.52 20.13 18.80 17.98 19.41 10.79 15.62 14.26 16.57 99.73 100.00 100.00 100. 52.80 50.99 50.29 50.44 37.14 37.14 37.14 37.14 37.09 37.09 37.09 37.09 49.00 42.86 41.49 41.51 27.83 28 27.83 27.83 27.83 27.99 27.92 27. 58.11 52.83 51.26 50.73 41.35 42.25 42.25 42.25 41.18 42.03 42.03 42.03 48.21 46.12 44.46 43.78 38.46 36.14 30.82 31.43 34.37 34.35 34.35 34. 49.44 46.73 45.58 45.09 44.5 34.06 31.62 29.79 41.09 29.71 29.91 29.93 48.25 45.48 42.60 41.56 42.38 27.17 33.21 29.24 37.30 37.92 37.95 37. 48.51 46.41 45.11 44.50 39.44 33.03 28.11 31.42 33.88 33.90 33.90 33.90 49.04 45.02 43.52 42.08 35.37 24.57 23.54 23.92 41.82 30.70 31.09 31. 49.12 44.97 43.14 41.67 44.87 25.62 31.71 28.01 37.79 38.11 38.11 38.11 42.88 39.64 40.53 38.64 37.65 37.83 35.76 37.95 40.09 40.11 40.11 40. 10.67 8.29 7.25 7.63 15.00 6.51 6.96 7.25 11.29 11.17 11.34 11.23 13.19 9.71 7.54 6.99 21.07 6.57 7.99 6.45 13.64 12.55 12.55 12. 5.3 INSIGHT ANALYSIS Case Study An editing example of visual entity editing by IKE and FT-LLM for LLaVA-1.5 is presented in Fig.5. Both IKE and FT-LLM correctly answered the text reliability question. However, IKE outperformed FT-LLM by also providing correct answers to the image generalization and portability questions, highlighting IKEs superior performance. The case study of question answers on visual semantic editing is shown in Fig.6. As we can see, after editing, the model could effectively answer the question based on editing knowledge. Figure 5: Case study of editing examples Figure 6: Case study of question answer"
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose comprehensive multimodal knowledge editing benchmark, named MMKEBench, designed to evaluate diverse semantic editing in real-world scenarios using free-form natural language representation. We propose to use free-form natural language representation combined with an image to represent knowledge instead of representing it with triplet. Besides, we propose three kinds of editing to align with real-world scenarios. We conducted experiments on representative LMMs and knowledge editing methods and found that more advanced knowledge editing methods are needed for LMMs. We hope our work could inspire more multimodal knowledge editing research. 10 Published as conference paper at ICLR 2025 ACKNOWLEDGEMENT This work is supported by the Opening Project of the State Key Laboratory of General Artificial Intelligence (Project No:SKLAGI20240P11)."
        },
        {
            "title": "REFERENCES",
            "content": "Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Myvlm: Personalizing vlms for user-specific queries. ECCV, 2024. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. Arxiv, 2023. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, and et al. Language models are few-shot learners. NeurIPS, 2020. Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. EMNLP, 2021. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. ArXiv, 2024. Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang. Can we edit multimodal large language models? In EMNLP, pp. 1387713888, 2023. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pp. 36063613, 2014. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. In NeurIPS, 2023. Art Dataset. wiki art dataset. url https://universe.roboflow.com/art-dataset/wiki-art , mar 2022. URL https://universe. roboflow.com/art-dataset/wiki-art. visited on 2023-01-18. Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. ACL, 2021. Tom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with GRACE: lifelong model editing with discrete key-value adaptors. In NeurIPS, 2023. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. ArXiv, 2024. Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In CVPR, pp. 1206512075, 2023. Han Huang, Haitian Zhong, Tao Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. Vlkeb: large vision-language model knowledge editing benchmark. arxiv, 2024. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. TACL, 7:453466, 2019. Jiaqi Li, Miaozeng Du, Chuanyi Zhang, Yongrui Chen, Nan Hu, Guilin Qi, Haiyun Jiang, Siyuan Cheng, and Bozhong Tian. MIKE: new benchmark for fine-grained multimodal entity knowledge editing. In Findings of ACL, pp. 50185029, 2024. Published as conference paper at ICLR 2025 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pp. 1973019742, 2023a. Zichao Li, Ines Arous, Siva Reddy, and Jackie Chi Kit Cheung. Evaluating dependencies in fact editing for language models: Specificity and implication awareness. In EMNLP, pp. 76237636, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pp. 2629626306, 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024b. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In NeurIPS, 2022. Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer. In Findings of ACL, 2024. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher Manning. Fast model editing at scale. ICLR, 2022a. Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher Manning, and Chelsea Finn. Memorybased model editing at scale. In ICML, pp. 1581715831, 2022b. Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh Ojha, and Yong Jae Lee. Yollava: Your personalized language and vision assistant. ArXiv, 2024. Yasumasa Onoe, Michael JQ Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. Can lms learn new entities from descriptions? challenges in propagating injected knowledge. ACL, 2023. Suchen Wang, Kim-Hui Yap, Henghui Ding, Jiyan Wu, Junsong Yuan, and Yap-Peng Tan. Discovering human interactions with large-vocabulary objects via query and multi-scale detection. In ICCV, pp. 1347513484, 2021. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. ArXiv, 2022. Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and Mingming Sun. Eva-kellm: new benchmark for evaluating knowledge editing of llms. ArXiv, 2023a. Yinan Wu, Xiaowei Wu, Junwen Li, Yue Zhang, Haofen Wang, Wen Du, Zhidong He, Jingping Liu, and Tong Ruan. Mmpedia: large-scale multi-modal knowledge graph. In International Semantic Web Conference, pp. 1837. Springer, 2023b. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. ArXiv, 2024. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. Findings of EMNLP, 2023. Junzhe Zhang, Huixuan Zhang, Xunjian Yin, Baizhou Huang, Xu Zhang, Xinyu Hu, and Xiaojun Wan. MC-MKE: fine-grained multimodal knowledge editing benchmark emphasizing modality consistency. Arxiv, 2024. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. ArXiv, 2023. Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. Can we edit factual knowledge by in-context learning? EMNLP, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. Arxiv, 2023. 12 Published as conference paper at ICLR 2025 Table 7: The image source of visual semantic knowledge in MMKE-Bench. Type Source Emotion Traffic Sign Human Action Life Gesture Referee Gesture Traffic Cop Sign Crawling from google Crawling from google LFW-emotion dataset https://huggingface.co/datasets/TrainingDataPro/facial-emotion-recognition-dataset Demo videos from Youtube and Bilibili Crawling from google TSRD dataset https://nlpr.ia.ac.cn/PAL/TRAFFICDATA/recognition.html DTD dataset (Cimpoi et al., 2014) Texture Crawling from google Color Shape Crawling from google Animal Body Language Crawling from google Relationship Social action Layout Siwg-HOI (Wang et al., 2021) and Crawling from google Crawling from google Wiki-art dataset (Dataset, 2022) https://huggingface.co/datasets/keremberke/painting-style-classification Art Style"
        },
        {
            "title": "A BENCHMARK CONSTRUCTION",
            "content": "A.1 ORIGINAL KNOWLEDGE COLLECTION In our process of gathering original knowledge, we begin by listing candidate fine-grained entities, visual semantics, or user-specific items, and subsequently collect their corresponding images. For visual entity editing, we source candidates from two datasets: The multimodal knowledge graph, MMpedia (Wu et al., 2023b), and the visual entity recognition dataset, OVEN (Hu et al., 2023). Given the extensive size of MMpedia, we filter entities with Wikipedia summaries of fewer than 40 words and eliminate candidates that cannot uniquely identify the main entity through images. Using the Wikipedia API, we retrieve the entity type and select the most popular 10% within each type. We further apply optical character recognition (OCR) to exclude images containing entity names, such as university logos. After this, we gather images from the relevant datasets and manually remove any noisy images, or crawl additional images from Google for entities with fewer than two images. The same process is applied to the OVEN dataset, except without sampling. For visual semantic editing, we first list the semantic candidates from four broad categories: singleperson behavior, single-object behavior or attributes, object relationship, and global structure. The single-person behavior includes human action, life gestures, referee gestures, traffic cop signs, and emotion. The single-object behavior or attribute covers animal body language, traffic signs, color, shape, and texture. The object relationship involves human-object interactive relationship and social actions, while global structure encompasses layout and art style. Where datasets exist, such as for texture, we gather the entities and images from existing sources. Otherwise, we manually curate the candidates using domain expertise and collect images from various sources. The sources for each type are listed in Tab.7. Specifically, images for human action, life gestures, traffic cop signs, color, shape, social action, animal body language, and layout are crawling from Google. Images for traffic signs, textures, relationships, emotions, and art styles come from existing datasets. Referee gesture images are collected by extracting frames from demo videos on YouTube and Bilibili. As for user-specific editing, we consider nine types of personal information, including items, pets, actors, singers, cartoon characters, organizations, universities, sports clubs, and companies. The candidate relationships between users and these objects are outlined in Tab.9, including examples like employed at, exchanged at, studied at, and favorite for universities. We collect images for these items from various sources. For items and pets, candidates and images are sourced from existing datasets used for personalized large multimodal research (Nguyen et al., 2024; Alaluf et al., 2024). For organizations, universities, sports clubs, and companies, we follow the same process as in visual entity editing, using data from MMpedia. For actors, singers, and cartoon characters, images are collected from Google. To sum up, this benchmark covers total of 2,940 pieces of knowledge, along with 8,363 images from 33 broad categories, and detailed type names are shown in Tab.8. After collecting the images, we generate natural language descriptions for each entity, visual semantic, and user-specific item. For visual entities, we retrieve descriptions from the Wikipedia summary, and 13 Published as conference paper at ICLR 2025 Table 8: The data type in MMKE-Bench. Visual Entity Editing Visual Semantic Editing User-Specific Editing Broad Categories Person Aerial Animals Marine Animals Terrestrial Animals Virtual Character Plant Building Musical Group Vehicle Others Human Action Life Gesture Emotion Referee Gesture Traffic Cop Sign Traffic Sign Types Human Bird, Dragonfly, Fly, Butterfly, Grasshopper, Wasp, Insect, Animal Jellyfish, Turtle, Sea Star, Fish, Crab, Sea Lion Bear, Monkey, Amphibian, Mammal, Rodent, Wild Boar, Squirrel, Dog Breed, Fox, Wolf, Tick, Rabbit, Rhinoceros, Arthropod, Salamander, Spider, Mollusc, Crustacean, Toad, Cat Breed, Deer, Beetle, Sloth, Frog, Mollusk, Snail, Hedgehog, Cat, Leopard, Pangolin, Dog, Cattle, Millipede, Moth, Snake, Lizard, Antelope Animated Character, Anime Character, Comics Character Fruit, Tree, Flower, Mushroom, Orchid, Vegetable, Fungus, Plant Building, Church Building, Monument, Tower, Sculpture, Statue Musical Group Car, Aircraft Model, Aircraft, Vehicle Instrument, Ball Body Posture Adjustments, Head Adjustments, Hand Actions, Leg Actions, Whole-Body Actions, Eye Expressions, Facial Expressions, Water Sports, Sound Actions, Object Actions, Repair or Construction Actions, Cleaning, Hunting, Crushing, Human Body Actions, Stabbing, Sticking or Connecting Actions, Tools or Weapons Actions, Cutting, Packaging or Storage Actions, Pinching, Inspection or Observation Actions Life Gesture Number, Life Gesture Emotion Sign Soccer Linesman, Soccer, Basketball, Volleyball, Volleyball Card, Baseball, Puck, Fencing, Handball, Badminton, Table Tennis Traffic Cop Sign Traffic Sign Forbidden, Traffic Sign Allow, Traffic Sign Point Texture Color Texture Color Animal Body Language Monkey Body Language, Cat Body Language, Dog Shape Social Action Art Style Layout Relationship Item Actor Singer Cartoon Character Organization University Sports Club Pet Company Body Language, Animal Actions Circular Shapes, Triangles, Special Plane Shapes, Common Polyhedrons, Solids of Revolution, Special Shapes Social Action, Agriculture, Cooking Actions, Using Tools, Communication or Giving Actions, Painting Depicting Art Style Layout Burning Scalding, Containers or Liquids Actions, Striking, Impacting, Solids of Revolution, Protection Cup, Toy Puppet, Statue, Toy, Plush Doll, Toy Doll, Puppet Cow, Cat Figurine, Bean Bag, Saving Pot, Shoes, Pillow, Pen Container, Throw Pillow Doll Actor Singer Cartoon Character Nonprofit Organization, Organization University, Private University Baseball Team, Basketball Team, Sports Club, Sports Team, Futsal Team ,Football Club Pet dog, Pet cat Airline, Company, Public Company, Dot-Com Company, Media Company 14 Published as conference paper at ICLR 2025 Table 9: The relationship between humans and the objects and data source of user-specific data in MMKE-Bench. Categories Relationship Image Source Company Organization University Club Cartoon character Actor Singer Pet Item Employed at, Interned at, collaborated with, Favorite MMpedia MMpedia Employed at, Interned at, Helped by, Favorite Employed at, Exchanged at, Studied at, Traveled to, Favorite MMpedia MMpedia Employed at, Visited, Favorite Crawling from Google Favorite Crawling from Google Favorite, Admire most Crawling from Google Favorite, Admire most MyVLM (Alaluf et al., 2024) and YoLLaVA (Nguyen et al., 2024) Owned MyVLM (Alaluf et al., 2024) and YoLLaVA (Nguyen et al., 2024) Owned if the summary is too lengthy, we use large language model (LLM) to condense it to fewer than 100 words. For visual semantic editing, the description includes both language description of the action and an explanation of its meaning or rule. These are gathered either from relevant domain knowledge by ourselves or generated with the help of an LLM. For user-specific editing, we select one relationship from the candidate list and use an LLM to craft personalized description of the users personal information. A.2 EDITING KNOWLEDGE GENERATION After collecting the original knowledge, we perform counterfactual editing to generate alternative knowledge for both visual entity and visual semantic editing. To achieve this, we prompt large language model (LLM) with in-context examples. For visual entity editing, we modify key details, such as nationality, alma mater, and occupation of person, into counterfactual variations. For visual semantic knowledge, we alter the rules or meanings, such as the location where free kick is taken, into counterfactual scenarios. The specific prompt used is shown in Tab.8. In addition to text-based editing, we also perform image modality editing by replacing the image of an entity or action with one from another entity or action of the same type. This replacement strategy is consistent with existing benchmarks (Huang et al., 2024). A.3 EVALUATION QUESTION GENERATION When generating evaluation questions, we adhere to four key principles: reliability, locality, generalization, and portability. For locality questions, we source them from existing benchmarks. For reliability, we generate questions by prompting large language model (LLM) with in-context examples, ensuring that each question is related to one of the edited contents. In image reliability, we refer to the main object in the image using its type, such as the person in the image. For portability, during visual entity editing, we follow previous benchmarks by providing additional information about the edited content to ensure text portability. In visual semantic editing and user-specific editing, we focus on image portability by combining the current objects image with another object of the same type. We then create final one-hop question by merging the counterfactual content-related question with an easier, image-based question, such as asking about the color of shoes. After generating the questions and answers, we conduct human review to verify the accuracy, rewriting any incorrect questions or answers. The prompts used for question generation are shown in Tab.9 and Tab.10."
        },
        {
            "title": "B EXPERIMENTS",
            "content": "We conduct experiments using the VLKEB library2, which employs PyTorch and integrates several knowledge editing methods and large multimodal models. The experiments are performed on NVIDIA A100/A800 80GB GPUs. The knowledge editing methods, and large multimodal models adopted in this study are listed below, with their hyper-parameters detailed in Tab.10, Tab.11, and Tab.12. MLLMs. To evaluate our benchmark, we conduct experiments on three representative MLLMs. 2https://github.com/VLKEB/VLKEB 15 Published as conference paper at ICLR 2025 Figure 7: Evaluation comparison of IKE for BLIP2 with existing benchmarks. I-Gen and Port for MMEdit, along with Port for MIKE, is set 1, as they ignore the relevant criteria. BLIP-2 (Li et al., 2023a): BLIP2 effectively leverages both frozen pre-trained image models and language models by bootstrapping vision-language pre-training, and bridges the modality gap with lightweight Querying Transformer. We follow previous work (Huang et al., 2024; Cheng et al., 2023), and select BLIP-2 OPT as the basic edit model, where the vision model is ViT-L and the LLM is OPT model. MiniGPT-4 (Bai et al., 2023): MiniGPT-4 aligns frozen visual encoder module with frozen advanced LLM using one projection layer. The LLM is Vicuna and the vision model is ViT. LLaVA-1.5 (Liu et al., 2024b): LLaVA-1.5 is an improved version of LLaVA, which is an end-to-end trained large multimodal model that connects vision encoder and an LLM with an MLP projector for visual and language understanding. We select LLaVA-1.5 7B as the base model where CLIP-ViT-L-336px is the vision model and Vicuna-7B is the LLM. Editing Methods. Following the previous benchmarks (Huang et al., 2024), we select five representative multimodal knowledge editing methods to conduct experiments. Fine-tuning (FT): Fine-tuning has become widely used strategy for adapting pre-train models to specific tasks. We focus on finetuning two parts: the LLM and the vision-language alignment module, where only the last layer of the LLM is fine-tuned. Knowledge Editor (KE) (De Cao et al., 2021): KE is method that can be used to edit this knowledge in the base model without the need for expensive retraining or fine-tuning. It uses hyper-network with constrained optimization to predict the weight update at test time. MEND (Mitchell et al., 2022a): MEND makes fast, local edits to pre-trained models behavior using single desired input-output pair. It learns to transform the gradient of standard fine-tuning, using low-rank decomposition of the gradient. SERAC (Mitchell et al., 2022b): SERAC is memory-based method and it stores edits in explicit memory. It also introduces scope classifier and counterfactual model, where the scope classifier is to determine whether the memory contains inputs relevant to processing them. If determined, the input is combined with the most relevant cache item into the counterfactual model for prediction. In-context Knowledge Editing (IKE) (Zheng et al., 2023): IKE is inspired by in-context learning, and new demonstration formatting and organization strategies are to construct suitable in-context learning demonstrations for guiding knowledge editing."
        },
        {
            "title": "C MORE RESULTS",
            "content": "Comparison of evaluation results with existing benchmarks for BLIP2 The Comparison of evaluation results with existing benchmarks of IKE for BLIP2 is shown in Fig. 7. As we can see, IKE achieves high results in existing benchmarks, while it performs worse in our benchmark, indicating the proposed benchmark is more challenging. Results of sequential editing for BLIP-2 We additionally report the results of sequential editing for BLIP-2 on MMKE-Bench, as shown in Tab.13. As we can see, FT-LLM and FT-Alignment tend to forget previous knowledge while SERAC is better at keeping edited knowledge. 16 Published as conference paper at ICLR 2025 Table 10: The hyper-parameters of knowledge editing methods and LMMs on the visual entity editing. FT-LLM Models Steps BLIP2-OPT 30 40 MiniGPT-4 40 LLaVA-1.5 Edit Layer 31st layer of Transformer Module 31st layer of Transformer Module 31st layer of Transformer Module Optimizer Edit LR AdamW AdamW AdamW 2e 4 1e 4 1e 4 FT-Alignment Models Steps Edit Layer BLIP2-OPT 30 30 MiniGPT-4 30 LLaVA-1. Qformer Qformer mm projector MEND Optimizer Edit LR AdamW AdamW AdamW 2e 4 1e 4 1e 4 Models MaxIter Edit Layer Optimizer LR BLIP2-OPT 10,000 30,000 MiniGPT-4 10,000 LLaVA-1.5 layer 29, 30, 31 of Transformer Module Adam layer 29, 30, 31 of Transformer Module Adam layer 29, 30, 31 of Transformer Module Adam 1e 6 1e 6 1e 6 SERAC Models MaxIter Edit Layer BLIP2-OPT 10,000 20,000 MiniGPT-4 10,000 LLaVA-1.5 all layers of OPT-125M 31st layer of Vicuna-7B 31st layer of Vicuna-7B-v1.5 Optimizer LR Adam Adam Adam 1e 5 5e 5 1e 5 KE Models MaxIter Edit Layer Optimizer LR BLIP2-OPT 10,000 10,000 MiniGPT-4 10,000 LLaVA-1. layer 29, 30, 31 of Transformer Module RMSprop layer 29, 30, 31 of Transformer Module RMSprop layer 29, 30, 31 of Transformer Module RMSprop 3e 4 3e 4 3e 4 17 Published as conference paper at ICLR 2025 Table 11: The hyper-parameters of knowledge editing methods and LMMs on visual semantic editing. FT-LLM Models Steps BLIP2-OPT 30 40 MiniGPT-4 40 LLaVA-1.5 Edit Layer 31st layer of Transformer Module 31st layer of Transformer Module 31st layer of Transformer Module Optimizer Edit LR AdamW AdamW AdamW 2e 4 1e 4 1e 4 FT-Alignment Models Steps Edit Layer BLIP2-OPT 30 30 MiniGPT-4 30 LLaVA-1. Qformer Qformer mm projector MEND Optimizer Edit LR AdamW AdamW AdamW 2e 4 1e 4 1e 4 Models MaxIter Edit Layer Optimizer LR BLIP2-OPT 20,000 30,000 MiniGPT-4 20,000 LLaVA-1.5 layer 29, 30, 31 of Transformer Module Adam layer 29, 30, 31 of Transformer Module Adam layer 29, 30, 31 of Transformer Module Adam 1e 6 1e 6 1e 6 SERAC Models MaxIter Edit Layer BLIP2-OPT 20,000 20,000 MiniGPT-4 20,000 LLaVA-1.5 all layers of OPT-125M 31st layer of Vicuna-7B 31st layer of Vicuna-7B-v1.5 Optimizer LR Adam Adam Adam 1e 5 5e 5 1e 5 KE Models MaxIter Edit Layer Optimizer LR BLIP2-OPT 10,000 10,000 MiniGPT-4 10,000 LLaVA-1. layer 29, 30, 31 of Transformer Module RMSprop layer 29, 30, 31 of Transformer Module RMSprop layer 29, 30, 31 of Transformer Module RMSprop 3e 4 3e 4 3e 4 18 Published as conference paper at ICLR 2025 Table 12: The hyper-parameters of knowledge editing methods and LMMs on user-specific editing. FT-LLM Models Steps BLIP2-OPT 30 40 MiniGPT-4 40 LLaVA-1.5 Edit Layer 31st layer of Transformer Module 31st layer of Transformer Module 31st layer of Transformer Module Optimizer Edit LR AdamW AdamW AdamW 2e 4 1e 4 1e 4 FT-Alignment Models Steps Edit Layer BLIP2-OPT 30 30 MiniGPT-4 20 LLaVA-1. Qformer Qformer mm projector MEND Optimizer Edit LR AdamW AdamW AdamW 2e 4 1e 4 1e 4 Models MaxIter Edit Layer Optimizer LR BLIP2-OPT 10,000 30,000 MiniGPT-4 10,000 LLaVA-1.5 layer 29, 30, 31 of Transformer Module Adam layer 29, 30, 31 of Transformer Module Adam layer 29, 30, 31 of Transformer Module Adam 1e 6 1e 6 1e 6 SERAC Models MaxIter Edit Layer BLIP2-OPT 10,000 20,000 MiniGPT-4 10,000 LLaVA-1.5 all layers of OPT-125M 31st layer of Vicuna-7B 31st layer of Vicuna-7B-v1.5 Optimizer LR Adam Adam Adam 1e 5 5e 5 1e 5 KE Models MaxIter Edit Layer Optimizer LR BLIP2-OPT 10,000 10,000 MiniGPT-4 10,000 LLaVA-1. layer 29, 30, 31 of Transformer Module RMSprop layer 29, 30, 31 of Transformer Module RMSprop layer 29, 30, 31 of Transformer Module RMSprop 3e 4 3e 4 3e 4 19 Published as conference paper at ICLR 2025 Table 13: The results of sequential editing for BLIP2 on MMKE-Bench. Method Gap / User Num T-Loc I-Loc T-Rel I-Rel I-Gen Port FT-LLM Visual Entity Editing FT-Alignment SERAC FT-LLM Visual Semantic Editing FT-Alignment SERAC FT-LLM User-Specific Editing FT-Alignment SERAC - 3 6 - 3 6 10 - 3 6 10 - 3 6 10 - 3 6 10 - 3 6 10 - 1 3 - 1 3 5 - 1 3 5 70.91 33.91 34.56 33.85 100.00 100.00 100.00 100.00 99.99 99.99 99.99 99.99 64.01 27.52 26.28 25. 100.00 100.00 100.00 100.00 100.00 99.92 99.92 99.92 61.77 48.33 44.55 43.30 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 21.63 5.24 5.17 5. 9.04 2.01 2.04 1.99 99.68 99.69 99.69 99.68 19.53 5.09 5.05 4.55 9.59 1.69 1.67 1.64 99.97 98.91 98.90 98.91 20.19 10.25 10.61 10. 8.61 14.70 18.13 12.45 99.78 99.76 99.76 99.76 37.3 34.18 32.33 31.24 20.09 20.09 20.09 20.09 20.90 20.09 20.01 20.09 34.67 28.92 28.35 24. 18.34 18.34 18.34 18.34 28.97 18.34 18.34 18.34 13.24 10.92 10.20 9.31 7.92 7.53 7.53 7.53 7.92 7.53 7.53 7.53 36.56 30.65 28.55 28. 28.9 13.62 12.54 14.37 20.30 20.60 20.34 20.56 31.74 27.21 25.61 23.58 35.86 12.42 12.18 11.49 30.39 17.37 17.44 17.19 27.61 17.80 15.09 14. 17.17 6.69 6.31 5.37 15.38 14.34 14.37 14.37 36.84 31.18 28.67 27.68 28.39 13.47 12.56 14.44 20.48 20.82 20.65 20.68 32.04 25.96 24.32 22. 35.84 12.09 13.18 11.57 30.23 17.17 17.17 17.17 27.82 17.99 14.70 14.22 17.18 6.98 5.83 5.79 15.73 14.30 14.30 14.30 18.70 14.64 12.84 13. 17.05 13.62 13.48 13.85 19.81 17.93 17.66 17.92 3.38 2.75 1.54 2.13 5.92 2.75 3.46 3.04 19.04 4.25 4.33 4.17 5.53 0.78 1.14 1. 6.82 1.46 2.08 1.35 5.33 4.98 4.98 4.98 20 Published as conference paper at ICLR 2025 Table 14: The results of Visual Semantic Sequential Editing for LLaVA-1.5 on MMKE-Bench. Method GAP T-Loc I-Loc T-Rel I-Rel I-Gen Port FT-LLM Visual Semantic Editing FT-Alignment SERAC - 3 6 10 40 60 80 - 3 6 10 40 60 80 - 3 6 10 40 60 76.89 50.33 49.09 48.23 45.40 43.88 42.99 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 99.93 99.93 99.93 99.93 99.93 99.96 16.14 7.36 7.25 7.02 6.23 5.82 5.58 19.41 1.44 1.38 1.38 1.22 1.17 0.94 34.53 13.56 13.54 13.52 13.37 13.35 13. 49.00 42.86 41.49 41.51 36.83 36.01 33.67 27.83 28 27.83 27.83 27.83 27.83 27.83 27.83 27.99 27.92 27.88 27.92 27.92 27.92 49.44 46.73 45.58 45.09 41.85 39.18 38.27 44.5 34.06 31.62 29.79 25.4 26.12 27.31 41.09 29.71 29.91 29.93 28.23 28.45 28. 49.04 45.02 43.52 42.08 40.53 38.69 36.79 35.37 24.57 23.54 23.92 21.63 22.11 23.81 41.82 30.70 31.09 31.13 29.23 29.41 28.41 10.67 8.29 7.25 7.63 7.83 7.04 6.83 15.00 6.51 6.96 7.25 8.58 8.08 6.75 11.29 11.17 11.34 11.23 11.25 11.25 11. 21 Published as conference paper at ICLR 2025 Figure 8: Prompt for editing knowledge. 22 Published as conference paper at ICLR 2025 Figure 9: Prompt for editing generating reliability question. 23 Published as conference paper at ICLR 2025 Figure 10: Prompt for generating portability question. 24 Published as conference paper at ICLR 2025 Figure 11: In Fig.11 (a), the single editing takes one edit at time and evaluates immediately, while in Fig.11 (b) and (c) the sequential editing involves continuous edits and tests after several other edits. Figure 12: There is difference between Visual Entity Knowledge and Visual Semantic Knowledge. Visual Entity Knowledge focuses on entity objects, such as people, things, etc. Visual Semantic Knowledge focuses on the knowledge abstracted from images, such as gestures, traffic signs, facial expressions, etc. For example, for Visual Entity Knowledge, in Figure 12 (a), the training knowledge needs reference to the entity, such as Donald John Trump, focusing on the information of the entity object; However, in (b) of Figure 12, for Visual Semantic Knowledge, entity reference, such as The man, is not needed, but the gesture of the person in the image is emphasized. 25 Published as conference paper at ICLR 2025 Figure 13: Loss iteration graph trained by SERAC method on Visual Semantic Knowledge data. Through the analysis of images, we can find that the SERAC method can normally achieve the convergence of loss on this data amount, and the loss value will approach 0 at last. Figure 14: Loss iteration graph trained by MEND method on Visual Semantic Knowledge data. Through the analysis of images, we can find that the MEND method can normally achieve the convergence of loss on this data amount, and the loss value will approach 0 at last. Published as conference paper at ICLR 2025 Figure 15: Data Example-1 of Visual Entity Editing in MMKE-Bench. Figure 16: Data Example-2 of Visual Entity Editing in MMKE-Bench. Figure 17: Data Example-1 of Visual Semantic Editing in MMKE-Bench. Figure 18: Data Example-2 of Visual Semantic Editing in MMKE-Bench. Published as conference paper at ICLR 2025 Figure 19: Data Example-1 of User-Specific Editing in MMKE-Bench. Figure 20: Data Example-2 of User-Specific Editing in MMKE-Bench. Figure 21: Case Study on Visual Entity Editing Example-1 in MMKE-Bench. Figure 22: Case Study on Visual Entity Editing Example-2 in MMKE-Bench. Published as conference paper at ICLR 2025 Figure 23: Case Study on Visual Semantic Editing Example-1 in MMKE-Bench. Figure 24: Case Study on Visual Semantic Editing Example-2 in MMKE-Bench. Figure 25: Case Study on User-Specific Editing Example-1 in MMKE-Bench. Figure 26: Case Study on User-Specific Editing Example-2 in MMKE-Bench. Published as conference paper at ICLR 2025 Figure 27: Case Study of Question Answer Example-1 of Visual Semantic Editing in MMKE-Bench. The texts in brown indicate the same content as the editing knowledge. Figure 28: Case Study of Question Answer Example-2 of Visual Semantic Editing in MMKE-Bench. The texts in brown indicate the same content as the editing knowledge."
        }
    ],
    "affiliations": [
        "Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology",
        "School of Software & Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University",
        "State Key Laboratory of General Artificial Intelligence, BIGAI",
        "State Key Laboratory of General Artificial Intelligence, Peking University",
        "University of Science and Technology of China"
    ]
}