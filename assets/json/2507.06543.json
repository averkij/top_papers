{
    "paper_title": "Token Bottleneck: One Token to Remember Dynamics",
    "authors": [
        "Taekyung Kim",
        "Dongyoon Han",
        "Byeongho Heo",
        "Jeongeun Park",
        "Sangdoo Yun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 4 5 6 0 . 7 0 5 2 : r Token Bottleneck: One Token to Remember Dynamics Taekyung Kim1 Dongyoon Han1 Byeongho Heo1 Jeongeun Park2 Sangdoo Yun1 1NAVER AI Lab 2Korea University {taekyung.k, dongyoon.han, bh.heo, sangdoo.yun}@navercorp.com baro0906@korea.ac.kr,"
        },
        {
            "title": "Abstract",
            "content": "Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), simple yet intuitive self-supervised learning pipeline that squeezes scene into bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales."
        },
        {
            "title": "Introduction",
            "content": "With the increasing interest in deploying machines in real-world environments, ensuring seamless perception and interaction with their surroundings has emerged crucial challenge. These operations are inherently sequential in nature, requiring the ability to trace objects (e.g., visual tracking) and predict future actions (e.g., manipulation) based on current and immediate past observations. Such understanding of the surrounding environments primarily depends on vision backbones. Therefore, strong and robust backbone capable of generalizing across diverse tasks and environments is essential for effective sequential scene understanding. Self-supervised learning (SSL) of visual representations has been highlighted as pivotal research in vision domains, with the pre-trained models being widely adopted for effective backbone deployment. series of studies have introduced promising recipes for learning image [17, 6, 9, 3, 4, 7, 21] and video representations [46, 38] without labeled data. However, these studies primarily focus on understanding entire scenes or videos, which poses limitations for sequential scene understanding, as it requires capturing temporal changes across consecutive scenes and conservatively encoding the visual states of observed scenes. To address the challenges, sequence of studies [19, 13, 25] have attempted to incorporate correspondence learning into the MAE [21] framework, aiming to retain its strong localization capability while enabling the model to match corresponding regions across consecutive scenes. However, we observe that such additional considerations have limited impact on the quality of scene representations and may result in suboptimal performance in sequential scene understanding tasks, such as robotic manipulation (3.2). This limitation arises since recognizing temporal changes alone is insufficient; Preprint. Under review. (a) Illustration of Token Bottleneck (ToBo) (b) Overall performance comparison Figure 1: (a) We describe the underlying mechanism of our Token Bottleneck (ToBo) pipeline during pre-training, which conservatively encode reference scene into bottleneck token and predict the subsequent target scene based on scarce target patches and the bottleneck token. ToBo facilitates learning the capability of temporal progression recognition and preservation of observed information (top). Therefore, using bottleneck tokens from the current and recent past observations enables the robot to better understand its current state (bottom). (b) Our method significantly surpasses previous self-supervised visual representation learning methods designed for static [6, 9, 4, 21] and dynamic scenes [19, 51, 25] on various robot manipulation and locomotion tasks. these tasks require the ability to summarize the essential information from each scene without loss, while preserving temporal cues within the summarized representation. In this paper, we introduce Token Bottleneck (ToBo), simple yet effective SSL approach that intuitively facilitates the conservative summarization of observed scenes while enabling effective recognition of temporal evolution within the summarized representations. As illustrated in Fig. 1a, ToBo squeezes reference scene into bottleneck token and then predicts the subsequent target scene using only minimal set of patches as hints. This design enforces strong reliance on the bottleneck token, encouraging the vision backbone to capture essential scene information. Moreover, predicting the target scene from the bottleneck token implicitly embeds temporal dependencies, guiding the vision backbone to generate representations capable of capturing dynamic transitions across consecutive scenes. We conduct comprehensive experiments to assess the effectiveness of our pre-training pipeline in comparison with existing self-supervised learning methods. We evaluate our method on various sequential understanding tasks, including manipulation tasks in simulated environments and video label propagation tasks, surpassing baselines [6, 9, 4, 21, 51, 19, 25, 13] with significant gaps (see Fig. 1b). Furthermore, we deploy our pre-trained models on real-world robots, demonstrating strong generalization performance in unseen physical environments. Finally, we validate the scalability of our approach by observing consistent performance gains across various model scales."
        },
        {
            "title": "2 Related Work",
            "content": "Self-supervised learning on static scene. Self-supervised learning (SSL) approaches have been widely explored in the image domain. Contrastive learning approaches [6, 20, 8, 9, 3] aim to learn useful representations by maximizing the similarity between positive pairs derived from static scene through strong augmentations. Although these methods excel in facilitating cohesive understanding of images, they suffer from limited localization capabilities [30], essential for action prediction in robotics. On the other hand, masked image modeling (MIM) [2, 21, 52, 1, 30] has recently gained attention for its promising capacity to learn visual representations through predictive learning. Inspired by masked language modeling (MLM) in transformers [11], BEiT [2] extends MLM into the vision domain, adopting an external offline tokenizer. MAE [21] and SimMIM [52] showcase efficient MIM by directly reconstructing masked input pixels without any tokenizer. However, these approaches do not incorporate mechanisms for capturing temporal progression during pre-training. 2 Figure 2: Comparative analysis for motivation. We compare robot manipulation performance using MAE and SiamMAE as visual backbones. While SiamMAE employ temporal correspondence to the limitation of MAE, its improvement over MAE remains limited. Figure 3: Overview of our Token Bottleneck (ToBo). Our ToBo reconstructs the masked patches from the bottleneck token representation of the reference scene xt and extremely scarce patches from the target scene xt+k. Such extreme scarcity leads the decoder dϕ to rely heavily on the reference scene xt, facilitating the preservation of observed information in the bottleneck token. Self-supervised learning on dynamic scenes. Recent studies have focused on enhancing the recognition of dynamic transitions. SiamMAE [19] proposes visual representation learning methods that utilize dynamic scenes. CropMAE [13] introduce simple augmentation strategy that enables the generation of dynamic scenes even from single static image. On the other hand, RSP employs stochastic frame prediction tasks along with masked autoencoding. Several works have also explored applying these techniques to embodied agents and robotic manipulation. For example, VC-1 [35], MVP [42], and Dasari et al. [10] adopt MAE objectives for visual pretraining, while STP [53] builds on SiamMAE with reference masking strategy. On the other hand, some prior works investigate representation learning with annotated supervision. Theia [44] distills representations from large scale pre-trained teacher networks, some of which are trained with annotation supervision, into student models. MPI [27], Voltron [28], and R3m [36] explore language-driven representation learning, leveraging an auxiliary textual guidance through manually annotated data. In contrast, we focus on self-supervised learning directly from raw dynamic scenes without any guidance from annotations."
        },
        {
            "title": "3.1 Preliminary",
            "content": "i=1 where xi R3p2 Masked autoencoding. Given scene image, we patchify the image into non-overlapping ppsize patches {xi}N . We randomly select masked patch set {1, 2, ..., } with ratio (0, 1) where = rN . The remaining patches {xi}iMc fed into the encoder fθ, becoming spatial representations {ui}iMc where ui Rd for encoder dimension d. Note that learnable CLS token e[CLS] is also encoded with spatial representations as part of the encoding process. The encoded tokens are expanded to tokens by substituting masked positions to mask token Rd. i.e. ui for M. The decoder dϕ gets {ui}N i=1 as input and predicts the masked image patches {ˆxi}iM using encoded tokens."
        },
        {
            "title": "3.2 Motivation",
            "content": "In this section, we discuss the pros and cons of previous self-supervised learning (SSL) approaches from sequential scene understanding perspective, which motivated our method. Limited temporal evolution awareness of MAE. MAE [21] has been recognized for its strong localization capability, leading several studies [35, 10, 42] to adopt the recipe. This stems from its design that enforces the autoencoder to predict missing information based on available prior information (i.e., 3 visible patches). This pipeline implicitly encourages the encoder to facilitate interactions among the remaining sparse tokens, thereby enhancing its localization capability. However, since MAE performs predictive learning on single static scene, the encoder is not explicitly trained to handle dynamic transitions over time, leading to limited performance in sequential scene understanding tasks  (Fig. 2)  . Moreover, recent study reveals that MAE falls short in learning broader contexts [30], leading to representations with limited cohesive understanding of observed scenes. These limitations further constrains its potential to effectively understand sequential scenes. Suboptimal impacts of SiamMAE in sequential scene understanding. To alleviate the chronic limitations of static scene-based SSL approaches, SiamMAE [19] builds non-trivial correspondence matching problem by randomly sampling two dynamic scenes from sequential data. The core principle involves propagating patches from the reference scene to their corresponding locations in the target scene. Applying this guidance with cross-attention layer-based decoder encourages finegrained patch-wise similarity between target patches and reference patches. This process ultimately enforces the encoder to generate similar representations for corresponding patches. However, while SiamMAE enables capturing correspondences among consecutive scenes, despite being built upon the MAE framework, its impact over MAE is marginal or even negative in some sequential scene-based tasks  (Fig. 2)  . In these tasks, since actions are predicted through policy network based on the estimated visual states of both the observed and immediate past scenes, this suggests that considering temporal evolution recognition is insufficient for sequential scene understanding, and conservative summarization of the observed scenes is essential."
        },
        {
            "title": "3.3 The Proposed Method - Token Bottleneck (ToBo)",
            "content": "Our claim. Our goal is to achieve representations optimized for resolving sequential scene-based tasks. In light of the discussions in 3.2, we extend our focus beyond simply recognizing temporal evolution; we consider the conservative summarization of observed scenes in way that also effectively embeds temporal dynamics within the summarized representation. To this end, we present Token Bottleneck (ToBo), self-supervised visual representation learning pipeline that enables these capabilities through token bottleneck mechanism. ToBo consists of two key steps: squeezing scene into single token, which we denote as the bottleneck token, and expanding information from this token. Suppose reference scene and target scene are given. In the squeeze step, visual information from the reference scene is compactly encoded into the bottleneck token. Subsequently, in the expansion step, we guide the model to predict the target scene using the bottleneck token, with only minimal set of patches from the target scene provided as hints. In this situation, the model cannot precisely reconstruct the target scene based solely on the limited hints, which strengthens the reliance of the expansion step on the bottleneck token. This design yields two advantages: (1) the bottleneck token should preserve essential information from the reference scene, and (2) such information should be encoded in way that enables recognition of temporal dynamics when interleaved with the hints from the target scene. Eventually, our goal can be achieved by optimizing the objective of the Token Bottleneck pipeline. The overall description of our pipeline is depicted in Fig. 3. Overall pipeline formulation. Suppose we sample reference scene xt R3HW and target scene xt+k R3HW with temporal gap k. We patchify xt and xt+k into non-overlapping patches {xt i=1 are fed into an encoder fθ, yielding spatial representations {ut i}iMc. We use the CLS token output from this encoding process as the bottleneck token utobo, which will be guided to compactly summarize the reference scene. The target scene {xt+k }N i=1 is masked with an extremely high ratio (0, 1), where {1, 2, ..., } and = rN . The unmasked target patches {xt+k }iMc are processed by the same encoder fθ, producing {ut+k }iMc for the target scene. We then concatenate the bottleneck token utobo with the target representations {ut+k }iMc and fill mask tokens for missing regions M. These are passed to the decoder dϕ, which predicts the masked image patches {ˆxt+k }iM by using utobo and {ut+k }iMc. Due to the extremely high masking ratio applied to the target scene, the decoder dϕ proactively rely on utobo, which enable the encoder fθ to conservatively summarize the reference scene in way that facilitates temporal reasoning when compared to the target hints. i=1, respectively. The reference scene patches {xt }N i=1 and {xt+k i}N i}N i Table 1: Experimental results on vision-based robot policy learning on Franka Kitchen. We report the performance of imitation learning agents on Franka Kitchen [18], which are trained upon representations from the ViT-S/16 model pre-trained on Kinetics-400 [29] dataset. The success rates (%) are reported for all the tasks. We underline the second-best performance. We report the gains of our method over the second-best baseline. Tasks SimCLR MoCo v3 DINO MAE SiamMAE RSP CropMAE ToBo 11.53.9 Knob1 on 24.35.0 Light on 66.53.2 Sdoor open 10.32.1 Ldoor open 14.32.5 Micro open Indicates results reported by Jang et. al. [25]. 25.32.1 55.86.4 72.32.8 17.02.9 23.32.8 27.03.2 44.36.5 77.05.0 16.52.5 28.54.8 12.03.3 24.34.2 71.54.3 12.83.9 10.02.8 16.84.4 36.57.0 68.07.9 17.33.7 13.54. 31.02.4 44.55.6 82.52.7 28.84.8 30.35.6 31.55.3 54.011.2 77.08.1 25.55.7 32.54.1 57.32.3 82.01.6 95.07.1 51.01.4 55.01.4 Table 2: Experimental results on vision-based robot policy learning on CortexBench. The performance of imitation learning agents on CortexBench [35] is reported, where the agents are trained upon representations from the ViT-S/16 model pre-trained on the Kinetics-400 [29] dataset. We report the normalized score for DeepMind Control Suite (DMC) and success rates (%) for other tasks. We report the gains of our method over the second-best baseline. Tasks SimCLR MoCo v3 DINO MAE SiamMAE RSP CropMAE ToBo 39.64.3 40.43.3 Adroit 65.48.0 78.45.2 MetaWorld 43.73.2 39.72.9 DMC 53.31.6 63.33.3 TriFinger Indicates results reported by Jang et. al. [25]. 45.66.2 39.64.3 82.45.8 65.48.0 50.91.5 43.73.2 64.23.5 53.31.6 44.06.6 81.16.3 56.02.9 52.17.6 45.64.6 84.56.6 61.63.4 66.20.8 50.05.1 82.45.8 46.41.1 46.31.7 60.42.2 87.84.6 73.58.6 66.51. We minimize the reconstruction loss throughout the training as follows: LToBo = (cid:88) iM d(ˆxt+k , xt+k ), (1) where d() is distance function; we use cosine distance for the pre-training. Decoder structure. Previous methods in dynamic SSL [19, 25, 13] utilize cross-attention layers as core component for learning temporal evolution awareness, placing them within the decoders to guide the encoder to learn representations that effectively capture correspondences. These approaches leverage hybrid structure of cross-attention layers, self-attention layers, and multi-layer perceptron (MLP) layers. In contrast, ToBo employs self-attention layers to ensure that the decoder exclusively attends to the given information during the expansion step, with MLP layers for progressive transformation from representation embedding spaces into the pixel space."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we focus on demonstrating the effectiveness of our pre-training pipeline through fair comparisons with existing self-supervised learning methods. To this end, we evaluate our method on sequential tasks, including video label propagation tasks [40, 57, 26] and vision-based policy learning for robotic manipulation and locomotion across various simulated environments [18, 24, 35]. We extend our validation to real-world settings by deploying our pre-trained model on physical robots, showcasing its transferability. We further investigate the scalability of our method. In the appendix, we validate our claim regarding the importance of extremely high masking ratios to the target scene, present qualitative comparisons of manipulation processes against baseline methods, and show provide demonstrations of real-world manipulation tasks."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Implementaion details. We follow the evaluation protocol of Jang et. al. [25] for both video label propagation and vision-based policy learning on simulated environments. To ensure fair comparisons 5 Table 3: Experimental results on vision-based robot policy learning on RLBench. We report the performance of imitation learning agents on RLBench [24], which are trained upon representations from the ViT-S/16 model pre-trained on Kinetics-400 [29] dataset. The success rates (%) are reported for all the tasks. We report the gains of our method over the second-best baseline."
        },
        {
            "title": "Tasks",
            "content": "SimCLR MoCo v3 DINO MAE SiamMAE RSP"
        },
        {
            "title": "Button\nPhone\nUmbrella\nWine\nRubbish",
            "content": "7.42.6 34.66.6 5.83.3 11.02.1 5.21.2 Indicates results reported by Jang et. al. [25]. 11.44.1 36.23.4 13.21.5 8.70.7 6.70.8 24.71.5 32.05.5 28.11.4 31.41.5 12.91.5 6.42.2 37.71.9 10.01.2 10.02.1 6.23.2 6.12.3 5.40.5 4.00.0 8.70.8 3.50.9 28.43.0 48.04.6 37.33.0 31.92.3 18.51. 26.96.7 16.63.8 37.58.8 33.20.2 20.61.7 41.27.4 52.33.2 42.26.9 35.43.8 37.06.1 Table 4: Performance on real-world vision-based robot policy learning. Success rates (%) of imitation learning agents on three manipulation tasks: Cabinet Opening, Drawer Closing, and Cup Stacking. Agents are trained with ViT-S/16 representations pre-trained on Kinetics-400 [29] for 400 epochs. The results demonstrate the generalizability of ToBo in real-world."
        },
        {
            "title": "Method",
            "content": "SiamMAE [19] RSP [25] CropMAE [13] ToBo (ours)"
        },
        {
            "title": "Cup\nStacking",
            "content": "20.0 25.0 0.0 65.0 55.0 65.0 25.0 75.0 50.0 55.0 20.0 80. Figure 4: Real-world robot trajectories. Initial, intermediate, and final states of the robot during (a) Cabinet Opening, (b) Drawer Closing, and (c) Cup Stacking. (a) (b) (c) with the baselines, we also pre-train our method on Kinetics-400 for 400 epochs. Detailed explanation for both pre-training and evaluation are provided in the Appendix. Baselines. We compare the performance of our method with conventional self-supervised learning (SSL) methods for visual representations including SimCLR [6], MoCo v3 [9], DINO [4], and MAE [21] We also consider previous dynamic scene SSL methods, i.e., SiamMAE [19], RSP [25], and CropMAE [13]. We validate the impacts of explicitly learning state representations over these approaches."
        },
        {
            "title": "4.2 Vision-based robot policy learning in simulated environments",
            "content": "We evaluate our method through imitation learning on robot manipulation and locomotion tasks across various simulated environments. Specifically, we evaluate five tasks from both the Franka Kitchen and RLBench benchmarks. Moreover, we consider two, five, five, and two tasks from Adroit [43], MetaWorld [55], DeepMind Control Suite (DMC) [45], and TriFinger [48] from the CortexBench benchmark, respectively. Franka Kitchen. We present comparison between our method and the baselines on vision-based robot policy learning in the Franka Kitchen environment in Table 1. The results demonstrate that our method significantly outperforms all the baselines across all tasks. Notably, our method achieves over 20% improvements in success rates on all tasks, except for the Light on task. This highlights the effectiveness of explicitly encoding visual state representation for vision-based robot policy learning. CortexBench. We compare our method with the baselines for the vision-based robot manipulation and locomotion tasks in the Adroit, MetaWorld, DeepMind Control (DMC), and Trifinger environments in Table 2. The results show that our method achieves superior performance compared to the baselines across all tasks. In particular, our method surpasses the second-best performance with success rate gains of 11.9%p on DMC and 10.4%p on Adroit. RLBench. Table 3 showcases the robot manipulation performance on five demonstration tasks in the RLBench environment. Notably, our method consistently exceeds all baselines across the five tasks. 6 Table 5: Results on video label propagation. We report performances on video segmentation, video part segmentation, and pose tracking tasks from DAVIS [40], VIP [57], and JHMDB [26] benchmarks, respectively. For all methods, we report the performance with the representations pre-trained on the Kinetics-400 [29] dataset for 400 epochs. Method &Fm SimCLR [6] MoCo v3 [9] DINO [4] MAE [21] SiamMAE [19] RSP [25] CropMAE [13] ToBo (ours) 53.9 57.7 59.5 53.5 58.1 60.1 58.6 60. DAVIS Jm 51.7 54.6 56.5 50.4 56.6 57.4 55.8 58.4 VIP mIoU 31.9 32.4 33.4 32.5 33.3 33.8 33.7 34.0 Fm 56.2 60.8 62.5 56.7 59.6 62.8 61.4 63.0 JHMDB PCK@0.1 PCK@0.2 37.9 38.4 41.1 43.0 44.7 44.6 42.9 47.0 66.1 67.6 70.3 71.3 73.0 73.4 71.1 74. (a) Semantic Part Propagation (b) Object Propagation (c) Pose Tracking Figure 5: Qualitative results for video label propagation. We provide examples of predicted propagation of our model on video object segmentation, video part segmentation, and pose tracking benchmarks. The leftmost images indicate the ground-truth annotations. We visualize the propagated results corresponding to 25, 50, and 100% ratio of the videos. Moreover, the degraded performance of MAE and SiamMAE further highlights the significance of state representation learning for the robot backbones."
        },
        {
            "title": "4.3 Vision-based Robot Policy Learning in Real-world Environments",
            "content": "Quantitative comparison. To validate the robustness of our method in real-world environments, we further investigate SSL methods on real-world robot manipulation tasks. Specifically, we design three demonstration tasks: Cabinet Opening, Drawer Closing, and Cup Stacking. For each task, We collect 50 demonstration episodes for training and 10 demonstration episodes for evaluation for imitation learning. Following the training protocol used in simulated environments, we train the policy network using standard behavior cloning loss. The experimental results for each individual task are reported in Table 4. We first observe that our method exceeds SiamMAE [19], RSP [25], and CropMAE [13] on all three tasks. Specifically, our method improves 40%p, 10%p, and 25%p over the baselines on the Cabinet Opening, Drawer Closing, and Cup Stacking tasks, respectively. While previous SSL methods on dynamic scenes struggle with tasks that require relatively high precision, like cabinet opening tasks, our method even successfully executes the task with considerable success 7 Table 6: Scalability of our method. We report the performance of vision-based robot policy learning on Franka Kitchen [18], which are trained upon representations from the ViT-B/16 and ViT-L/16 model pre-trained on Kinetics-400 [29] dataset for 100 epochs. The success rates (%) are reported for all the tasks. We underline the second-best performance. We report the gains of our method over the second-best baseline. We conduct evaluations using three different seeds. Arch. Method Knob1 on Light on Sdoor open Ldoor open Micro open ViT-B/16 ViT-L/16 MAE [21] SiamMAE [19] RSP [25] ToBo (ours) 18.71.2 18.02.0 24.73.1 46.76.4 21.34.6 34.02.0 51.79.1 78.77.6 70.02.0 80.73.1 87.32.3 95.31.2 Gain + 22.0 + 27. + 8.0 MAE [21] SiamMAE [19] RSP [25] ToBo (ours) 19.37.6 20.73.1 26.72.3 54.75.0 33.32.3 34.04.0 48.02.0 75.34.2 61.36.4 76.02.0 88.02.0 94.03.5 Gain + 28.0 + 27.3 + 6.0 17.32.3 18.71.2 23.37.6 47.35.0 + 24.0 16.02.0 12.76.4 22.78.3 50.02. + 27.3 15.32.3 19.36.1 26.72.3 37.34.6 + 10.6 14.02.0 22.00.0 23.34.2 42.76.1 + 19.4 rate. This showcases that models pre-trained by our method can be robustly transferred to real-world environments. Qualitative comparison. To illustrate the actual manipulation processes, we present the robot trajectories from successful demonstrations for three real-world manipulation tasks in Fig. 4. Specifically, the initial states of the physical robot are depicted in the left scenes, while the right scenes show the final states of the demonstrations. The middle scenes illustrate the intermediate states of the demonstrations. Our model clearly succeeds in all the tasks. We also compared the trajectories with the baselines in the Appendix."
        },
        {
            "title": "4.4 Video Label Propagation",
            "content": "We perform comparative analyses on the video label propagation tasks. We consider the video object segmentation, video part segmentation, and pose tracking tasks from DAVIS [40], VIP [57], and JHMDB [26]. We follow the evaluation protocol in Jang et. al [25]. We present the quantitative evaluation in Table 5. Our method demonstrates superior performance compared to all the baselines across the video label propagation tasks. We also provide qualitative results in Fig. 5, where our method effectively traces visual appearances across various video label propagation tasks. These visualizations highlight that our method maintains robust object identity, part consistency, and pose continuity. The strong performance in both quantitative and qualitative evaluations further demonstrate the effectiveness of our approach in capturing the temporal evolution of visual appearance across consecutive scenes."
        },
        {
            "title": "5 Discussion",
            "content": "Scalability. We investigate the scalability of our ToBo beyond ViT-S/16 by pre-training ViTB/16 and ViT-L/16 on Kinetics-400 [29] for 100 epochs. We evaluate the pre-trained models on vision-based robot policy learning on Franka Kitchen [18] using three different seeds. We compare our method with MAE, SiamMAE, and RSP. Table 6 presents the mean and standard deviation across all seeds. We observe that models pre-trained with ToBo consistently achieving the best performance across all five tasks, exhibiting significant improvements over the second-best results. These demonstrate the scalability of our method. Comparison with robot representation learning models We further compare our method with recent robot representation learning (RRL) models, categorized by their supervision types: selfsupervised learning [10, 35], supervision via foundation model outputs [44], and supervision with auxiliary language annotations [36, 42, 28, 27]. Table 7 shows the reported performance of RRL models across several simulated robot manipulation benchmarks [43, 55, 18]. Here, our model is based on ViT-Small architecture trained on Kinetics-400 for 400 epochs. Notably, despite having 8 Table 7: Comparison with robot representation learning models. We compare the performance of our method with robot representation learning methods across multiple simulated manipulation tasks. We categorize the methods into self-supervised learning, supervised learning through foundation models, and supervised learning with auxiliary language guidance. Despite the unbalanced training and evaluation setup, ToBo surpasses the RRL models on MetaWorld. Moreover, ToBo exceeds self-supervised RRL models despite of smaller model with significantly smaller amount of data. These results demonstrate the effectiveness of the representations learned by ToBo in diverse robot manipulation tasks. Method #Param Dataset #Seen frames Adroit MetaWorld Franka Kitchen Supervision through Foundation Models Theia [44] 52.9M Theia dataset 14.4B Supervision with Auxiliary Language Guidance Ego4D [16] 25.6M MVP dataset 21.7M SS-v2 [15] 21.7M Ego4D [16] 21.7M R3M [36] MVP [42] Voltron [28] MPI [27] 0.8B 4.8B 0.3B 0.1B 66.0 65.0 - - - 86.1 69.2 84.6 68.7 85.7 Self-supervised Learning R3M [36] data4robotics [10] VC-1 [35] ToBo (ours) Uses additional compression layers. Uses multi-head attention pooling layers for integrating spatial tokens. Excludes language guidance from the vanilla recipe. Includes data for distillation models [41, 31, 54, 12]. Ego4D [16] Kinetics-700 [5] Ego4D+N [16, 35] Kinetics-400 [29] 25.6M 86.0M 86.0M 21.7M 0.8B 0.5B 1.0B 0.2B 45.6 - 50.0 60.4 67.0 87.0 86.4 87.8 - 53.1 - 70.5 76.5 47.2 55.0 - 68.1 the smallest number of parameters and the second smallest amount of training data, and using no annotation-based supervision, our method achieves the highest score on MetaWorld. In particular, Theia is trained by distilling knowledge from five large-scale foundation models (CLIP large [41], Depth Anything large [54], DINOv2 large [37], Segment Anything huge [31], and ViT huge [47]), which are collectively trained on 14.3 billion annotated samples. It also employs convolution-based compression layers during evaluation. Surpassing Theia under such an unbalanced training and evaluation setup is noteworthy. Furthermore, the performance gap between R3M with and without language guidance highlights the substantial benefit of auxiliary language supervision. Even with such unfairness in the training setup, our method outperforms R3M, MVP, Voltron, and MPI on MetaWorld. It also surpasses R3M on Franka Kitchen, despite significant differences in training data and model size. Compared to self-supervised RRL models, our method outperforms all the models. It surpasses much larger models such as VC-1 and data4robotics, despite being trained on significantly smaller amount of data. Given the minimal number of parameters and training scale, these results demonstrates the effectiveness and efficiency of our proposed method for robot manipulation tasks."
        },
        {
            "title": "6 Conclusion",
            "content": "We have introduced Token Bottleneck (ToBo), self-supervised visual representation learning method designed for sequential scene understanding. The backbones for sequential scene-based tasks should effectively preserve visual information from observations while facilitating the recognition of temporal progression across sequential scenes. While conventional self-supervised learning (SSL) methods have proven promising impacts in visual representation learning, they primarily focus on understanding static images or entire videos, often lacking embeds for handling dynamic transitions in sequential tasks. Recent SSLs aim to address this by adapting correspondence learning in dynamic scenes. However, their patch-wise representations of observations are often suboptima for subsequent policy networks, especially in tasks like robotic manipulation. To this end, ToBo introduces simple yet effective pipeline that facilitates conservative summarization of the observed scene into bottleneck token while enable capturing of dynamic transitions through the bottleneck token. Through extensive experiments in various sequential understanding tasks including manipulation tasks and video label propagation tasks, we verified the superiority of ToBo over conventional SSL methods and previous dynamic scene SSL methods. Furthermore, applying ToBo in real-world settings demonstrates its robustness and generalization capability."
        },
        {
            "title": "References",
            "content": "[1] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. data2vec: general framework for self-supervised learning in speech, vision and language. In ICML, pages 12981312. PMLR, 2022. 2 [2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2021. 2 [3] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Proceedings of NeurIPS, 2020. 1, 2 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. 1, 2, 6, 7 [5] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset, 2022. 9 [6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, 2020. 1, 2, 6, 7 [7] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, pages 1575015758, 2021. 1 [8] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. 2 [9] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021. 1, 2, 6, 7 [10] Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and Abhinav Gupta. An unbiased look at datasets for visuo-motor pre-training. In Conference on Robot Learning. PMLR, 2023. 3, 8, 9 [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2 [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 9, 3 [13] Alexandre Eymaël, Renaud Vandeghen, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, and Marc Van Droogenbroeck. Efficient image pre-training with siamese cropped masked autoencoders. In European Conference on Computer Vision, pages 348366. Springer, 2025. 1, 2, 3, 5, 6, 7 [14] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. large-scale study on unsupervised spatiotemporal representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 32993309, 2021. 3 [15] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The \"something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 9 [16] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina González, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jáchym Koláˇr, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbeláez, David Crandall, 10 Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1899519012, 2022. 9 [17] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap Your Own Latent - New Approach to Self-Supervised Learning. In NeurIPS, pages 2127121284. Curran Associates, Inc., 2020. 1 [18] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019. 5, 8, 3 [19] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese masked autoencoders. In Advances in Neural Information Processing Systems, 2023. 1, 2, 3, 4, 5, 6, 7, [20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019. 2 [21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. 1, 2, 3, 6, 7, 8 [22] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 3 [23] Allan Jabri, Andrew Owens, and Alexei A. Efros. Space-time correspondence as contrastive random walk. In Advances in Neural Information Processing Systems, 2020. [24] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):30193026, 2020. 5, 6, 3 [25] Huiwon Jang, Dongyoung Kim, Junsu Kim, Jinwoo Shin, Pieter Abbeel, and Younggyo Seo. Visual representation learning with stochastic frame prediction. In Proceedings of the 41st International Conference on Machine Learning, pages 2128921305. PMLR, 2024. 1, 2, 5, 6, 7, 8, 3 [26] Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid, and Michael Black. Towards understanding action recognition. In Proceedings of the IEEE international conference on computer vision, 2013. 5, 7, 8, 4 [27] Zeng Jia, Bu Qingwen, Wang Bangjun, Xia Wenke, Chen Li, Dong Hao, Song Haoming, Wang Dong, Hu Di, Luo Ping, Cui Heming, Zhao Bin, Li Xuelong, Qiao Yu, and Li Hongyang. Learning manipulation by predicting interaction. In Proceedings of Robotics: Science and Systems (RSS), 2024. 3, 8, 9 [28] Siddharth Karamcheti, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. Language-driven representation learning for robotics. In Robotics: Science and Systems (RSS), 2023. 3, 8, 9 [29] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 5, 6, 7, 8, 9, 1, [30] Taekyung Kim, Sanghyuk Chun, Byeongho Heo, and Dongyoon Han. Learning with unmasked tokens drives stronger vision learners. European Conference on Computer Vision (ECCV), 2024. 2, 4 [31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 9 [32] Z. Lai and W. Xie. Self-supervised learning for video correspondence flow. In BMVC, 2019. 4 [33] Xueting Li, Sifei Liu, Shalini De Mello, Xiaolong Wang, Jan Kautz, and Ming-Hsuan Yang. Joint-task self-supervised learning for temporal correspondence. In NeurIPS, 2019. [34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 3 11 [35] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Malik, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? In Advances in neural information processing systems, 2023. 3, 5, 8, 9 [36] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: universal visual representation for robot manipulation. In 6th Annual Conference on Robot Learning, 2022. 3, 8, 9 [37] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 9, [38] Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, and Wei Liu. Videomoco: Contrastive video representation learning with temporally adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1120511214, 2021. 1 [39] Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising effectiveness of pre-trained vision models for control. In Proceedings of the 39th International Conference on Machine Learning, pages 1735917371. PMLR, 2022. 3 [40] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 5, 7, 8, 4 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 9, 1 [42] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In Proceedings of The 6th Conference on Robot Learning, pages 416426. PMLR, 2023. 3, 8, 9 [43] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations. In Proceedings of Robotics: Science and Systems (RSS), 2018. 6, [44] Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, and Laura Herlant. Theia: Distilling diverse vision foundation models for robot learning. 2024. 3, 8, 9 [45] Yuval Tassa, Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, and Nicolas Heess. dm_control: Software and tasks for continuous control. arXiv preprint arXiv:2006.12983, 2020. 6, 3 [46] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In Advances in neural information processing systems, 2022. 1 [47] Hugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXIV, pages 516533. Springer, 2022. 9 [48] Frederik Träuble, Andrea Dittadi, Manuel Wuthrich, Felix Widmaier, Peter Vincent Gehler, Ole Winther, Francesco Locatello, Olivier Bachem, Bernhard Schölkopf, and Stefan Bauer. The role of pretrained In International Conference on Learning representations for the OOD generalization of RL agents. Representations, 2022. 6 [49] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [50] Xiaolong Wang, Allan Jabri, and Alexei A. Efros. Learning correspondence from the cycle-consistency of time. In CVPR, 2019. 4 [51] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Brégier, Yohann Cabon, Vaibhav Arora, Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and Revaud Jérôme. Croco: Self-supervised pre-training for 3d vision tasks by cross-view completion. In NeurIPS, 2022. 2 12 [52] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In ICCV, 2022. [53] Jiange Yang, Bei Liu, Jianlong Fu, Bocheng Pan, Gangshan Wu, and Limin Wang. Spatiotemporal predictive pre-training for robotic motor control, 2024. 3 [54] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 9 [55] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, 2020. 6, 8, 3 [56] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1197511986, 2023. 1 [57] Qixian Zhou, Xiaodan Liang, Ke Gong, and Liang Lin. Adaptive temporal encoding network for video instance-level human parsing. In Proceedings of the 26th ACM international conference on Multimedia, 2018. 5, 7, 8,"
        },
        {
            "title": "Appendix",
            "content": "A: Further analysis including comparison with vision-language models and ablation study on the mask ratio of target scenes B: Manipulation trajectory visualization of real-world demonstrations C: Implementation details for pre-training and evaluation"
        },
        {
            "title": "A Further Analysis",
            "content": "A.1 Comparison with Vision-Language Models We compare our method with vision-language models widely used either as backbones across various domains or as vision towers in large language models. For fair evaluation, we follow the same evaluation protocol used in the main paper. We evaluate CLIP [41], DINOv2 [37], SigLIP [56], and SigLIP2 [49] in the Franka Kitchen benchmark, as shown in Table A. Despite having the smallest number of learnable parameters and being exposed to the smallest number of seen frames during pretraining, ToBo achieves consistently superior performance, outperforming the baselines by margins at least 13.0%p to the maximum 43.5%p. These performance gaps are notable given that all baselines except DINOv2 use language supervision from manually annotated data. These results demonstrate the effectiveness of ToBo in summarizing visual observations for sequential scene understanding tasks. A.2 Ablation Study on Mask Ratio of Target Scenes To verify our claim that extremely scarce information from target scenes forces the decoder to rely highly on the stored visual scene information of the reference scene, we conduct an ablation study varying the mask ratio of target scenes. We pre-train the models on the Kinetics-400 [29] dataset for 100 epochs and evaluate three tasks on Franka Kitchen. As shown in Figure A, the effectiveness of our proposed method increases as the masking ratio of target scenes increases until 0.9, verifying our claim that scarce target scene information facilitates the exploitation of the compressed reference information. Besides, the models pre-trained with masking ratio of 0.95 yield degraded performance in some tasks, demonstrating that minimal clues are necessary for the prediction of the missing information. Figure A: Varying the masking ratio of target scenes. We vary the masking ratio from 0.5 to 0.95 and pre-train ViT-S/16 models on the Kinetics-400 [29] dataset for 100 epochs. Table A: Performance with vision-language models. We compare the performance of our method with vision-language models on Franka Kitchen. Despite using smaller model, significantly less pre-training data, and no auxiliary textual guidance from manually annotated data, ToBo consistently outperform the other models across all tasks. #Param Dataset #Seen frames Knob1 on Light on Sdoor open Ldoor open Micro open Method CLIP [41] DINOv2 [37] SigLIP [56] SigLIP2 [49] 149.3M WebImageText 22.1M LVD-142M 203M 375M WebLI WebLI 12.8B 4.3B 2.1B 40B 23.0 25.5 17.5 11.0 29.5 38.0 38.5 23.5 69.5 82.0 75.0 58.5 21.7M Kinetics-400 ToBo (Ours) Gain * The model is trained using textual guidance with manually annotated data. 0.2B 57.3 + 31.8 82.0 + 43.5 95.0 + 13.0 1 13.5 15.5 8.5 11. 51.0 + 35.5 22.0 20.0 16.5 18.0 55.0 + 33.0 (a) Cabinet Opening (b) Drawer Closing (c) Cup Stacking Figure B: Sampled Trajectories from Real World Experiment. We visualize the manipulation trajectories of ToBo, RSP, and SiamMAE on physical robot manipulation tasks in real-world environments (i.e., Cabinet Opening, Drawer Closing, and Cup Stacking). Our ToBo successfully demonstrates all tasks, which aligns with the quantitative performance comparisons results. Manipulation trajectory visualization of Real-world Demonstrations We showcase the robot manipulation trajectories for the SiamMAE [19], RSP [25], and our model as robot backbones in the same episode on the real-world environment for each task. In Figure B, Specifically, the leftmost scenes depicts the initial states of the physical robot, while the rightmost scenes show the final states of the demonstrations. As shown in Fig. B, while SiamMAE and RSP fail to execute the manipulation tasks, our method successfully completes them within the same episode. We also provide videos of these demonstrations in the supplementary material."
        },
        {
            "title": "C Implementation Details",
            "content": "We provide implementation details for pre-training and evaluation. Specifically, we present the evaluation protocols for vision-based robot policy learning on each simulated environment (i.e., 2 Franka Kitchen [18], CortexBench [35], RLBench [24]) and real-world environment. Then, we explain experimental setups for video label propagation tasks. C.1 Pre-training We pre-train ViT-S/16 [12] on Kinetics-400 [29] for 400 epochs for the main comparison, while we pre-train ViT-S/16, ViT-B/16, and ViT-L/16 for 100 epochs for analyses. We employ repeated sampling [22, 14] with factor of 2 so that the models are indeed pre-trained for 200 epochs. We use AdamW optimizer [34] with batch size of 1536, comprising dynamic scenes with resolution of 224224. These scenes are randomly sampled from videos at rate of 30 FPS, with temporal index gap ranging from 4 to 96. We simply apply random resized crop and horizontal flip to the scenes, aligning the cropping region across the reference and target scenes. To drive the learning mechanism of our proposed method, we randomly mask the target scenes with an extremely high masking ratio of 0.9. Our decoder is composed of eight vision transformer blocks, i.e., each block contains self-attention layers and multi-layer perceptrons. We follow the default hyperparameters of the baselines for their pre-training on Kinetics-400 [29]. C.2 Vision-based Robot Policy Learning Franka Kitchen. We validate models pretrained by our method and other baselines in five imitation learning tasks from the Franka Kitchen benchmark [18]. Our experiments mainly follow the imitation learning evaluation setup in Jang et. al. [25], which builds upon [36, 39]. Specifically, we employ an agent comprising frozen backbone initialized with pre-trained models and policy network consisting of two-layer MLP, with batch normalization layer applied at the input stage. We define the state representation for the policy network as the combination of the visual representation and the robots proprioception. For the perception, we employ either left or right camera with 224224 resolution while omitting depth. The policy network is trained with standard behavior cloning loss. Training for each demonstration task progresses for 20,000 steps, with periodic online evaluation in the simulated environment every 1,000 steps. We evaluate the highest success rates of each demonstration across four different seeds and report its average with 95% confidence interval."
        },
        {
            "title": "Simulated",
            "content": "Real-world Figure C: Environments for evaluation. (Left) Simulated: Franka Kitchen [18], CortexBench [35], RLBench [24]. (Right) Real-world robot tasks. RLBench. We consider five manipulation tasks from RLBench [24]. Follow the evaluation setup in Jang et. al. [25], we generate 100 demonstrations and utilize them for training the agent. We employ front camera with 224224 resolution. Point cloud information is excluded throughout all experiments. We employ the end-effector controller with path planning. We evaluate the highest success rates of each demonstration across four different seeds. CortexBench. We evaluate the models on four simulated environments from CortexBench [35]. We consider two, five, five, and two demonstrations from Adroit, DeepMind Control (DMC) [45], MetaWorld [55], and Trifinger, respectively. Proprioceptive data is utilized except the DMC benchmark. We mainly follow the experimental setups in Jang et. al. [25], which builds upon [35]. For each task, we train the agent for 100 epochs, with periodic online evaluation in the simulated environment every 5 epochs. We report the normalized score for DMC and the highest success rates for other tasks. We conduct demonstration tasks for five different seeds and report its average with 95% confidence interval. 3 Figure D: Task Description for Real-world Environments. We illustrates the objectives of physical robot manipulation tasks in the real-world. Yellow arrows indicate the target actions for each task. Real-world Environments. We evaluate our proposed method in real-world robotic imitation learning tasks using UR5e manipulator equipped with parallel gripper. The policy operates at control frequency of 5 Hz, executing actions defined as delta end-effector poses and grippers state, with specific parameterizations for each task: (dx, dy) for drawer closing, (dx, dy, gripper open/close) for cabinet opening, and (dx, dy, dz, gripper) for cup stacking. The system employs joint position control at 50 Hz, with numerical inverse kinematics (IK) solver running in the background to calculate the end-effectors pose to the joint position. Our training dataset consists of 50 demonstrations for cabinet opening and cup stacking and 30 demonstrations for drawer closing. We train the two-layer MLP policy for 100 epochs without incorporating proprioceptive states, using top-front camera view with resolution of 224224. The final performance is evaluated based on the reported average success rate across tasks. Figure provides visual examples of the three tasks under consideration. Video label propagation. We conduct comparative analyses for video label propagation on video object segmentation on DAVIS [40], video part segmentation on VIP [57], and pose tracking on JHMDB [26]. Following the evaluation protocols in the previous studies [50, 33, 32, 23], we employ k-nearest neighbor inference, maintain queue of length to provide temporal context, and restrict the set of source nodes within spatial radius r. Additionally, we perform grid search to optimize evaluation hyperparameters for each method and report the best results."
        }
    ],
    "affiliations": [
        "Korea University",
        "NAVER AI Lab"
    ]
}