{
    "paper_title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search",
    "authors": [
        "Fang Wu",
        "Weihao Xuan",
        "Heli Qi",
        "Ximing Lu",
        "Aaron Tu",
        "Li Erran Li",
        "Yejin Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 2 4 5 4 5 2 . 9 0 5 2 : r Preprint, Under Review DEEPSEARCH: OVERCOME THE BOTTLENECK OF REINFORCEMENT LEARNING WITH VERIFIABLE REWARDS VIA MONTE CARLO TREE SEARCH Fang Wu Weihao Xuan, Heli Qi Ximing Lu Aaron Tu Li Erran Li Yejin Choi Stanford University University of Tokyo RIKEN AIP University of Washington UC Berkeley Amazon AWS"
        },
        {
            "title": "ABSTRACT",
            "content": "Although Reinforcement Learning with Verifiable Rewards (RLVR) has become an essential component for developing advanced reasoning skills in language models, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, framework that integrates Monte Carlo Tree Search (MCTS) directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes new state-of-the-art for 1.5B reasoning models - 1.25 percentage point improvement over the previous best while using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes new direction for scaling reasoning capabilities through systematic search rather than prolonged computation. https://huggingface.co/fangwu97/DeepSearch-1.5B"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have recently achieved notable progress on complex reasoning tasks (DeepSeek-AI, 2025; Yang et al., 2024), driven in part by test-time computation scaling strategies (Li et al., 2023; Yao et al., 2023; Bi et al., 2024; Zhang et al., 2024a; Guan et al., 2025) such as tree search with process-level evaluation. While effective, these methods typically treat structured search as an inference-only mechanism, leaving untapped potential to integrate systematic exploration into the training process itself. This separation between training and inference creates fundamental limitations in how we scale reinforcement learning with verifiable rewards (RLVR) for reasoning. Current RLVR approaches remain Equal contributions. Corresponding author. Email: yejinc@stanford.edu 1 Preprint, Under Review constrained by sparse exploration patterns during training (Wu et al., 2025; Liu et al., 2025c), while models are expected to demonstrate sophisticated search behaviors only at inference time. Even recent advances in prolonged RL training (Liu et al., 2025a) have shown performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training depth. This suggests that simply scaling the number of training stepsthe primary axis explored in prior workmay not be sufficient to unlock the full potential of RLVR. We address this gap by introducing DeepSearch, framework that embeds Monte Carlo Tree Search (MCTS) (Metropolis & Ulam, 1949) directly into RLVR training, representing fundamental shift from scaling training depth to scaling training breadth. By coupling structured search with verifiable rewards during training, DeepSearch enables models to learn not only from correct solutions but also from the systematic exploration process itself, providing richer supervision than outcome-based or direct rollout methods (Lyu et al., 2025; He et al., 2025b). The core insight driving us is to focus on training-time exploration as the driver of improved reasoning. While traditional RLVR relies on limited rollouts that may miss critical reasoning paths, DeepSearch systematically expands the reasoning frontier during training through principled tree search. This design advances three key objectives: (i) expanding reasoning coverage beyond what direct policy rollouts can achieve, (ii) providing fine-grained credit assignment to intermediate reasoning steps through tree-structured backpropagation, and (iii) maintaining computational efficiency through intelligent node selection and solution caching strategies. To achieve these goals, DeepSearch introduces several key innovations. First, global frontier selection strategy prioritizes the most promising nodes across the entire search tree, moving beyond traditional root-to-leaf UCT traversals that can be computationally wasteful and myopic. Second, selection with entropy-based guidance systematically identifies confident incorrect reasoning paths for supervision. Finally, an adaptive training strategy with replay buffers progressively filters challenging problems and caches verified solutions to avoid redundant computation across training iterations. We evaluate DeepSearch on mathematical reasoning benchmarks, where it significantly outperforms state-of-the-art RLVR baselines, including Nemotron-Research-Reasoning-Qwen-1.5B v2 (Liu et al., 2025a) and DeepScaleR (Luo et al., 2025b). Our results show that DeepSearch achieves 62.95% average accuracy on challenging mathematical tasks, representing new state-of-the-art for 1.5B reasoning models. Importantly, these gains are achieved while remaining computationally efficient through progressive filtering and intelligent solution reuse, demonstrating that searchaugmented training can be both more effective and more practical than conventional approaches. The implications extend beyond math reasoning: by bridging the gap between inference-time search capabilities and training-time learning, DeepSearch establishes new paradigm for scaling RLVR that emphasizes systematic exploration over prolonged training. This work suggests that the future of reasoning model development lies not just in scaling model parameters or training steps, but in fundamentally rethinking how we structure the learning process to mirror the sophisticated reasoning patterns we expect at inference time. We defer detailed literature review to Appendix due to space constraints."
        },
        {
            "title": "2 DEEPSEARCH WITH MCTS",
            "content": "Given problem and policy model πθ, we adopt modified MCTS framework to build search tree for incremental step-by-step solution exploration. We replace traditional root-to-leaf selection with global frontier-based node selection. The root node represents the question x, and child nodes correspond to intermediate steps generated by πθ. root-to-leaf path ending at terminal node send forms trajectory = s1 s2 . . . send, where each step si is assigned q-value q(si). Then we extract solution trajectories = (cid:8)t1, t2, . . . , tn(cid:9) (n 1) from the search tree , where ti can be correct, incorrect or incomplete. The depth of any node is denoted as d(s) Z+. (s) and ξ(s) denote the number of visits to and the number of children nodes of s, respectively. Starting from the root node x, our MCTS iterations are conducted through four subsequent components. 2.1 EXPANSION WITH ENTROPY-BASED GUIDANCE In step i, we collect the latest reasoning trajectory oi = s1 s2 . . . si1 as the current state, i.e., observation. Based on this state, we prompt the policy model πθ(sioi) to generate candidates 2 Preprint, Under Review Figure 1: DeepSearch Framework Overview. for the next-step reasoning trail {si,j}n j=1. We repeat this expansion behavior until we reach the terminal nodes send Send, either by arriving at the final answers or by hitting the maximum depth of the tree dT , which yields an ordered sequence s1 send. During each expansion, let (k) end denote the set of newly generated terminal nodes at iteration k. We evaluate the correctness of each terminal node using verification function : Send {0, 1}, where V(s) = 1 indicates correct solution and V(s) = 0 indicates an incorrect or incomplete solution. Then we partition the terminal nodes into correct and incorrect subsets: correct = {s (k) (k) end V(s) = 1}, (k) incorrect = {s (k) end V(s) = 0}. (1) If (k) correct = , we employ an entropy-based selection to identify the most confident negative example, where the terminal node with the lowest average entropy along its root-to-leaf trajectory is selected: neg = arg min sS (k) incorrect H(t(s)), (2) where t(s) = (x, s1, s2, . . . , s) represents the unique trajectory from root to terminal node s, and the average trajectory entropy is defined as: H(t(s)) = 1 t(s) t(s) (cid:88) i=1 H(πθ(si oi)), (3) with final H(πθ(si oi)) = (cid:80) πθ(ai,k oi, ai,<k) log πθ(ai,k oi, ai,<k) being the Monte Carlo estimation of the Shannon entropy of the policy distribution at step i. ai,k is the k-th token of step si, and ai,<k denotes the tokens preceding ai,k. This selection strategy prioritizes incorrect reasoning sequences exhibiting low decision uncertainty, targeting areas where the models decisionmaking is most confident and would benefit from additional training supervision. ai,k 2.2 HEURISTIC SCORE BACKUP Let denote the selected trajectory for backpropagation, which is either correct solution trajectory or the most confident negative trajectory t(s neg) identified through entropy-based selection. Let q(m)(si) denote the q-value for node si after the m-th rollout backpropagation. We define the iterative q-value update rule for nodes along the selected trajectory: q(m)(si) = q(m1)(si) + γ(i, l) q(m)(send), (4) where γ(i, l) : Z+ Z+ [0, 1] is the temporal decay function that assigns higher weights to nodes closer to the terminal node: γ(i, l) = max (cid:19) , γmin , (cid:18) 3 (5) Preprint, Under Review with being the current node index in the trajectory, being the terminal node index, and γmin = 0.1 being the minimum decay threshold. The q-value initialization is q(0)(si) = 0 for all si . Terminal node rewards are assigned according to the verification functions result: q(send) = (cid:26)+1 1 if V(send) = 1 (correct), if V(send) = 0 (incorrect) d(send) < dT (incomplete). (6) To ensure positive q-values (e.g., qcorrect = 0.1) for nodes on correct reasoning paths while penalizing nodes leading to incorrect or incomplete solutions, we enforce the constrained update rule: if q(m1)(si) q(m)(send) 0, elif q(m)(send) > 0, elif q(m1)(si) > 0. q(m1)(si) + γ(i, l) q(m)(send) γ(i, l) q(m)(send) q(m1)(si) q(m)(si) = (7) This constraint preserves the invariant that q(m)(si) 0 for all intermediate nodes si Send lying on trajectories leading to correct solutions, while allowing negative values only for nodes that inevitably lead to incorrect outcomes. 2.3 HYBRID SELECTION STRATEGY Our MCTS employs hybrid selection strategy that combines traditional UCT-based local selection with novel global frontier selection, each serving distinct purposes in the search process. Local Selection for Sibling Comparison During the expansion of selected node, we generate multiple candidate children and need to determine which ones to add to the tree. For this local sibling comparison, we follow the traditional MCTS protocol and employ the Upper Confidence Bounds for Trees (UCT) algorithm (Kocsis & Szepesvari, 2006): (cid:115) UCT(s) = Q(s) + λ ln Nparent(s) (s) , (8) where Q(s) = q(s) (s) represents the average reward per visit, Nparent(s) is the number of visits from the parent node, and λ balances exploitation and exploration. This local selection ensures that we make optimal decisions when choosing among sibling nodes that share the same parent and context. Global Frontier Selection for Next Expansion After completing the first score backup phase, we need to identify the most promising node across the entire search tree for the next expansion round. This is where our novel global frontier selection mechanism operates. Unlike traditional MCTS, which performs root-to-leaf traversals using UCT at each level, our global approach directly compares all frontier nodes simultaneously. We maintain global view of all leaf nodes across the entire search tree and prioritize promising expansion points globally: = {s ξ(s) = 0, / Send, d(s) < dT }. For each frontier node F, we compute frontier priority score: (s) = λ1 tanh(Qparent(s)) (cid:125) (cid:123)(cid:122) Quality Potential (cid:124) + λ2 H(πθ(so)) (cid:123)(cid:122) (cid:125) Uncertainty Bonus (cid:124) + λ3 D(d(s)) (cid:125) (cid:123)(cid:122) Depth Bonus (cid:124) . (9) (10) Here, the quality potential term tanh(Qparent(s)) encourages the selection of nodes whose parents have demonstrated high value, using the tanh transformation to smoothly handle negative Q-values and map them to the range [1, 1]. The uncertainty bonus term H(πθ(so)) provides exploration guidance by adjusting priority according to the policys entropy; the sign of its coefficient can be utilized to steer selection toward regions with high confidence or uncertainty. The depth bonus term D(d(s)) encourages deeper exploration by providing additional priority to nodes at greater depths, where we empirically find D(d(s)) = (cid:112)d(s)/dT to be most effective among other variants including d(s) and log(d(s) + 1). The node with the highest frontier score is selected for the next expansion: = arg maxsF (s). 4 Preprint, Under Review Rationale for Hybrid Approach This hybrid design leverages complementary strengths: local UCT selection ensures principled sibling comparisons within subtrees, while global frontier selection overcomes UCTs myopia through cross-subtree resource allocation. The approach achieves three key advantages: (1) Computational efficiency by eliminating redundant root-to-leaf traversals, (2) Enhanced exploration coverage by preventing the algorithm from getting trapped in locally promising but globally suboptimal subtrees, and (3) Uncertainty-guided search that leverages the policys entropy to target regions expected to benefit from additional training supervision, with the bonus coefficient controlling the direction of this preference."
        },
        {
            "title": "3 ADAPTIVE TRAINING STRATEGY WITH REPLAY BUFFER",
            "content": "While MCTS offers fine-grained credit assignment, applying it to every training example is computationally infeasible. To address this, we adopt an iterative filtering strategy with replay buffer mechanism that focuses MCTS computation on challenging examples while preventing catastrophic forgetting of solved problems. The complete pipeline is depicted in Algorithm 1. 3."
        },
        {
            "title": "ITERATIVE TRAINING WITH PROGRESSIVE FILTERING",
            "content": "Our training process follows an iterative approach that progressively refines the training subset based on model performance. We begin by using the base RL model to perform an initial screening on the entire dataset Dhard, creating the first training subset D(0) Specifically, the iterative training process proceeds as follows: hard for MCTS-based RL training. Initial Subset Construction: Given the base policy πθ(0), we evaluate its performance on the full training set Dtrain using direct rollouts and construct the initial hard subset: hard = {x Dtrain Pass1@K(x, πθ(0)) < δ(0)}, (11) where Pass1@K(x, π) represents the success rate when sampling = 4 solutions for problem using policy π, and δ(0) (0, 1) is the initial filtering threshold. D(0) Iterative Refinement: After each training phase i, we re-evaluate the updated policy πθ(i) on the current hard subset and apply threshold-based filtering to create the next iterations training set: D(i+1) hard = {x D(i) hard Pass1@K(x, πθ(i)) < δ(i)}. (12) The filtering threshold δ(i) is typically set to 25%, ensuring that only problems with insufficient success rates remain in the active training set. This progressive filtering concentrates computational resources on increasingly challenging problems as the model improves. 3.2 REPLAY BUFFER WITH CACHED SOLUTIONS To prevent catastrophic forgetting and efficiently leverage previously discovered solutions, we maintain replay buffer that stores correct reasoning trajectories from earlier training phases. Buffer Population. During each training iteration i, we identify problems that obtained correct solutions through MCTS rollouts but still fail to meet the filtering threshold after training: hard, tcorrect T(x), Pass1@K(x, πθ(i)) < δ(i)}. These candidate trajectories are added to the replay buffer, attaining R(i+1) = R(i) R(i) candidates = {(x, tcorrect) D(i) R(i) (13) candidates. Cached Solution Usage. Instead of randomly sampling from the replay buffer, we employ deterministic strategy that directly utilizes cached solutions when available. For each problem in the current training iteration, we first check whether correct solution has been previously cached. This approach eliminates redundant MCTS computation for problems with known solutions while focusing computational resources on truly challenging unsolved problems. Hybrid Rollout Strategy. When processing problems in the current hard subset D(i) different rollout strategies based on cache availability: hard, we apply Rollout(x) = (cid:26)tcached DirectRollouts(x, β) MCTSfull(x) if (x, tcached) R(i), otherwise. (14) 5 Preprint, Under Review For problems with cached solutions, we directly incorporate the stored correct trajectory tcached and supplement it with DirectRollouts(x, β), which samples β additional solution attempts from the current policy πθ(x), where 0 < β < 1 and is the standard sampling budget. For problems without cached solutions, we apply the complete MCTS search process MCTSfull(x). Moreover, among the incorrect samples, we remove data containing garbled text or infinite repetitions. Based on empirical evidence, optimizing policies on such problematic data frequently leads to training collapse (Bai et al., 2025). The training dataset for each iteration is then constructed as: (i) train = (cid:91) {tcached DirectRollouts(x, β)} (cid:91) MCTSfull(x) . x:(x,tcached)R(i) (cid:124) (cid:123)(cid:122) Cached problems (cid:125) x:(x,tcached) /R(i) (cid:123)(cid:122) (cid:124) Unsolved problems (cid:125) (15) This construction eliminates the need for artificial sampling ratios or complex batch composition strategies, as training data naturally incorporate both preserved knowledge and fresh exploration based on problem-specific requirements. This achieves three key benefits: (1) Computational efficiency by avoiding redundant MCTS computation, (2) Solution preservation by guaranteeing the inclusion of cached correct trajectories, and (3) Continued exploration at minimal computational cost. 3.3 TREE-GRPO TRAINING OBJECTIVE After constructing search tree for sample question in the dataset Dtrain, we develop our TreeGRPO training objective. This objective combines q-value regularization with policy optimization to effectively learn from the tree-structured reasoning traces. Q-Value Soft Clipping. To address the q-value explosion problem for intermediate nodes while preserving meaningful gradients, we first apply soft clipping using the hyperbolic tangent function: (cid:17) (cid:16) q(sj) = tanh q(kmax)(sj)/ϵq qmax for all sj Send (16) where kmax is the maximum rollout iterations, ϵq = 1.0 is the temperature parameter, and qmax = 1 defines the maximum allowable q-value magnitude. This soft clipping approach prevents q-value explosion by maintaining all intermediate node qvalues within [qmax, qmax], while offering several key advantages: (i) it naturally bounds q-values without hard discontinuities, (ii) it preserves gradients everywhere, preventing the zero-gradient problem that occurs with hard clipping when all values hit the same bound, and (iii) it maintains the relative ordering of q-values while compressing extreme outliers. Terminal node q-values remain unchanged as defined in Eq. 6. Training Objective. With the regularized q-values, we formulate our Tree-GRPO objective as: (θ) = ETT ,tiT,(sj ,oj )ti 1 sj sj (cid:88) k=1 (cid:16) min ρj,k(θ) ˆAj,k, clip (ρj,k(θ), 1 ϵlow, 1 + ϵhigh) ˆAj,k (cid:17) (17) where ρj,k(θ) = πθ(aj,koj ,aj,<k) πθold (aj,koj ,aj,<k) is the importance ratio. The parameters ϵhigh and ϵlow follow the Clip-Higher strategy of DAPO (Yu et al., 2025), while we also remove the KL regularization term DKL to naturally diverge (Luo et al., 2025a; He et al., 2025a). An overlong buffer penalty is imposed to penalize responses that exceed predefined maximum value of 4096. The advantage function for node sj in trajectory ti is computed using sequence-level normalization (Chu et al., 2025): ˆAj,k = q(sj) µt, where µt is the average reward of the terminal nodes Send throughout the tree T. This normalization is crucial in practice, particularly for mitigating uncontrolled growth in response length. Notably, Tree-GRPO can be degraded to the vanilla DAPO if we consistently leverage the outcome reward q(send) as q(sj) for all intermediate nodes. (18) 6 Preprint, Under Review Table 1: Performance comparison of 1.5B-scale language models on standard mathematical reasoning benchmarks. We report Pass@1 accuracy with = 32 samples. Results with the best performance are highlighted in bold. All evaluations were conducted on 128H100 96G cluster. Avg Model AIME24 AIME25 AMC23 MATH Minerva Olympiad Qwen2.5-Math-1.5B Qwen2.5-Math-1.5B-Instruct DeepSeek-R1-Distill-Qwen-1.5B STILL-3-1.5B Qwen2.5-Math-1.5B-Oat-Zero Open-RS1-1.5B Open-RS2-1.5B Open-RS3-1.5B DeepScaleR-1.5B Nemotron-Research-Reasoning-Qwen-1.5B v1 Nemotron-Research-Reasoning-Qwen-1.5B v2 DeepSearch-1.5B 8.33 10.10 31.15 31.46 20.00 30.94 28.96 30.94 38.54 45.62 51.77 53.65 6.35 8.85 24.06 25.00 10.00 22.60 24.37 24.79 30.52 33.85 32.92 35.42 44.06 55.08 72.81 75.08 52.50 73.05 73.52 72.50 80.86 85.70 88.83 90. 66.67 74.83 85.01 86.24 74.20 84.90 85.06 84.47 88.79 92.01 92.24 92.53 18.42 29.32 32.18 32.77 26.84 29.92 29.74 29.11 36.19 39.27 39.75 40.00 30.74 40.00 51.55 53.84 37.78 52.82 52.63 52.25 58.95 64.56 64.69 65. 29.10 36.37 49.46 50.73 36.89 49.04 49.05 49.01 55.64 60.17 61.70 62."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 BENCHMARK PERFORMANCE EVALUATION Datasets and Base Models. We train DeepSearch based on Nemotron-Research-ReasoningQwen-1.5B v2 (Liu et al., 2025a) and employ DeepMath-103K (He et al., 2025c) as the raw dataset. DeepMath-103K is large-scale mathematical dataset designed with high difficulty, rigorous decontamination against numerous benchmarks. We evaluate DeepSearch against state-ofthe-art 1.5B reasoning models on six mathematical benchmarks: AIME 2024/2025, AMC2023, MATH500 (Hendrycks et al., 2021), Minerva (Lewkowycz et al., 2022), and Olympiad (He et al., 2024). More experimental details are described in Appendix B. Baselines. We compare against recent 1.5B models spanning different paradigms: base models (Qwen2.5-Math variants), RL-trained models (DeepSeek-R1-Distill, STILL-3 (Team, 2025), OpenRS series (Dang & Ngo, 2025), advanced RL methods (DeepScaleR (Luo et al., 2025b), Nemotron variants), and search-based approaches (Qwen2.5-Math-Oat-Zero (Liu et al., 2025b)). Our evaluation methods and results are consistent with Hochlehnert et al. (2025). Results. Table 1 shows DeepSearch-1.5B achieves 62.95% average accuracy, outperforming all baselines, including the previous best Nemotron-Research-Reasoning-Qwen-1.5B v2 (61.70%). DeepSearch-1.5B demonstrates consistent improvements across all benchmarks, with notable gains on AIME 2024 (53.65% vs 51.77%) and AMC (90.39% vs 88.83%). The 1.25 percentage point improvement over the previous state-of-the-art validates the effectiveness of integrating structured search into RLVR training rather than restricting it to inference only. 4.2 TRAINING EFFICIENCY ANALYSIS To evaluate the practical viability of DeepSearch, we compare computational costs against extended training approaches that scale purely through additional training steps. As shown in Table 2, extended training shows diminishing returns: 325 additional steps achieve 61.78% accuracy using 326.4 GPU hours, while 1,875 steps plateau at 62.02% despite consuming 1,883.2 GPU hours. This reveals the fundamental limitation of depth-first scaling, where performance gains become marginal as computational investment grows exponentially. DeepSearch achieves superior results through algorithmic innovation rather than brute-force computation. With only 50 additional training steps, DeepSearch reaches 62.95% accuracy using 330 GPU hoursoutperforming the most extensive baseline (1,883.2 hours) while using 5.7 fewer resources. This efficiency stems from structured search that extracts maximum value from each training step through systematic exploration of diverse solution paths. Figure 2 illustrates the training dynamics over 20 hours following 3K RLVR training. DAPO exhibits gradual linear improvement with shallow slope, while DeepSearch demonstrates more efficient 7 Preprint, Under Review Table 2: Comparison of methods on efficiency and performance, which are trained from DeepSeekR1-Distill-Qwen-1.5B. Method RLVR Steps Samples (K) Time (h) GPU Hours Math Score DeepSeek-R1-Distill-Qwen-1.5B Nemotron-Research-Reasoning-Qwen-1.5B v1 DAPO Nemotron-Research-Reasoning-Qwen-1.5B v2 DAPO Extended Training Extended Training Extended Training DeepSearch-1.5B 2000 3000 +325 +785 +1875 DAPO DAPO + KL DAPO + KL Tree-GRPO + 665.6 1607.7 3840.0 102.4 20.4 49.3 117.7 20. 16000 24000 326.4 788.8 1883.2 330 49.46 60.10 61.70 61.78 62.08 62.02 62. learning through structured exploration. The superior convergence suggests that RLVR bottlenecks stem from exploration quality rather than insufficient training time. These results challenge the assumption that scaling RLVR requires proportional computational increases. Compared to the training of Nemotron-Research-Reasoning-Qwen1.5B v2, DeepSearch-1.5Bs 72 efficiency improvement represents paradigm shift from resource-intensive scaling to algorithmicallydriven optimization, demonstrating that systematic exploration outperforms prolonged training for advancing RLVR capabilities. 4.3 SEARCH STRATEGY ABLATION Table 3 compares our global frontier selection against vanilla UCT under different configurations on 1.2K samples from extremely hard DeepMath-103K problems. Figure 2: Average performance (AIME 2024, AIME 2025, and AMC 2023) of DAPO and DeepSearch after 3K RLVR training. Markers denote evaluations, while dotted lines indicate linear trends. Global vs. Local Selection. Our global frontier selection (λ1 = 0.4) reduces iterations by 10.4% (209.6 187.7) and improves trajectory rewards (0.82 0.65) compared to vanilla UCT, while maintaining similar search depth and entropy. This demonstrates that direct comparison of frontier nodes across the entire tree is more efficient than traditional root-to-leaf UCT traversals. Depth Bonus Impact. We evaluate three depth bonus functions D(d(s)): (i) Logarithmic log(d(s) + 1) provides minimal improvements, (ii) Linear d(s) achieves the most aggressive efficiency gains with 59% reduction in per-tree time (1179.6s 480.9s) and deepest exploration (21.55 depth), but at cost of solution quality ( -0.76 reward), (iii) Square root (cid:112)d(s)/dT offers the best balance, maintaining search quality (-0.65 reward) with significant computational savings. Uncertainty Bonus. Adding uncertainty weighting (λ2 = 0.4) increases exploration diversity (entropy 1.23 1.31) by prioritizing high-uncertainty policy regions, but introduces computational variability (92.5 22.5 iterations). Configuration Selection. We adopt (cid:112)d(s)/dT with λ1 = 0.4, λ3 = 0.01 as our default, balancing computational efficiency (189.3 iterations), search quality (-0.65 reward), and stable performance. This configuration eliminates UCTs redundant traversals while maintaining principled exploration through quality potential and depth guidance. 4.4 ALGORITHM EVOLUTION AND COMPONENT CONTRIBUTIONS To understand the individual contributions of each component, we present systematic ablation study showing the evolution of our DeepSearch algorithm in Table 4. Starting from the Nemotron-Research-Reasoning-Qwen-1.5B v2 baseline, we incrementally add components and analyze their impact: 8 Preprint, Under Review Table 3: Ablation study of different search strategies in DeepSearch. We compare vanilla UCT with our proposed global frontier selection under varying depth bonus functions D(d(s)). Reported metrics include search statistics such as average search depth, trajectory entropy, and trajectory reward, together with computational cost measured by the number of iterations, average per-iteration time (in seconds), and per-tree time (in seconds). Results are presented as mean standard deviation. Search Metrics Computational Cost Method D(d(s)) Depth Entropy Reward Num. Iter. Time Per Iter. Time Per Tree Vanilla UCT 20.11 4.72 1.23 0.29 0.82 0.57 209.6 14.8 5.63 0.21 1179.6 95. Global Frontier Selection λ1 = 0.4 λ1 = 0.4, λ3 = 0.01 λ1 = 0.4, λ3 = 0.01 λ1 = 0.4, λ2 = 0.4, λ3 = 0.01 (cid:112)d(s)/dT (cid:112)d(s)/dT λ1 = 0.4, λ3 = 0.01 log(d(s) + 1) d(s) 20.28 4.80 20.33 4.77 21.55 5.13 20.83 4.71 20.29 4.83 0.65 0.76 1.23 0.29 0.65 0.76 1.23 0.30 0.76 0.65 1.24 0.29 1.31 0.30 0.79 0.62 1.24 0.29 0.65 0.76 187.7 16.2 185.5 15.9 85.7 7.7 92.5 22.5 189.3 14.7 5.76 0.19 5.85 0.19 5.61 0.12 5.48 0.13 5.66 0. 1087.7 105.0 1080.3 102.2 480.9 41.9 505.2 114.8 1070.7 87.3 Table 4: Ablation study illustrating the step-by-step evolution of DeepSearch. Starting from Vanilla DeepSearch with simple q-update, we progressively add outcome-rewardbased and fine-grained advantages, standard or mean-only normalization, and frontier node selection. Model / Change AIME24 AIME25 AMC23 MATH Minerva Olympiad Avg Nemotron-Research-Reasoning-Qwen-1.5B v2 + Vanilla DeepSearch + New Update & Coarse-grained Token Scores + New Update & Fine-grained Token Scores + Standard Advantages Normalization + Mean-only Advantages Normalization + Frontier Selection 51.77 51.98 51.04 50.52 52.60 51.98 53.65 32.92 34.06 35.73 35.52 35.00 35.73 35.42 88.83 86.64 86.48 88.83 89.30 89.06 90. 92.24 87.00 90.66 91.70 92.44 91.88 92.53 39.75 37.96 39.14 39.71 39.29 39.58 40.00 64.69 64.00 65.23 64.81 64.99 65.71 65. 61.70 60.27 61.38 61.85 62.27 62.32 62.95 (i) Vanilla DeepSearch Foundation. We begin with basic MCTS integration using simple qvalue update rule: q(m)(si) = (cid:26)q(m1)(si) + γ(i, l) q(m)(send) max(cid:0)q(m1)(si) + γ(i, l) q(m)(send), 0(cid:1) if q(m1)(si) q(m)(send) 0, otherwise. This assigns constant values to nodes on correct reasoning paths but shows limited improvement over the baseline. (ii) Enhanced Q-Value Updates with Outcome Rewards. We replace the simple update with our constrained backup rule (Eq. 7) and use outcome-based advantages ˆAj,k = q(send) for all nodes. This provides more stable credit assignment and yields meaningful improvements. (iii) Fine-Grained Node-Level Advantages. Moving beyond outcome-only rewards, we assign node-specific advantages ˆAj,k = q(sj) based on each nodes individual q-value. This enables more precise credit assignment across different reasoning steps. (iv) Standard Advantage Normalization. We implement standard normalization as ˆAj,k = q(sj )µt , where σt is the standard deviation σt+ε of the rewards of the terminal nodes Send throughout the tree T. The constant ε prevents numerical instability when the variance is small. This stabilizes training but introduces variance-based scaling. (v) Mean-Only Normalization. We adopt mean-only normalization (Eq. 18). This addresses miscalibration issues in GRPO while maintaining stable advantage scaling. (Bereket & Leskovec, 2025). (vi) Global Frontier Selection. Finally, we integrate our novel frontier selection strategy (Eq. 9), which prioritizes promising expansion candidates across the entire search tree rather than following traditional root-to-leaf UCT-like traversals. The results demonstrate that each component contributes meaningfully to the final performance, with frontier selection providing the largest single improvement. The cumulative effect shows that systematic exploration and fine-grained credit assignment are both essential for maximizing the benefits of search-augmented RLVR."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced DeepSearch, which integrates Monte Carlo Tree Search directly into RLVR training to address exploration bottlenecks that cause performance plateaus. Our framework features global frontier selection, entropy-based guidance, and adaptive replay buffers with the Tree-GRPO objective for fine-grained credit assignment. DeepSearch achieves 62.95% average accuracy on 9 Preprint, Under Review mathematical reasoning benchmarks, establishing new state-of-the-art for 1.5B models with 1.25 percentage point improvement over previous best methods while using 5.7 fewer GPU hours. This demonstrates that systematic exploration during training is more effective than prolonged computation, shifting the paradigm from scaling training depth to scaling training breadth through algorithmic innovation."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work advances automated mathematical reasoning through algorithmic innovation without exaggerated capability claims. We commit to releasing complete implementation details for reproducibility and transparency. While enhanced reasoning capabilities could benefit education and scientific computing, we acknowledge potential dual-use concerns, though mathematical domains with verifiable correctness limit harmful applications. Our approach reduces computational requirements (330 vs 1883 GPU hours) compared to extended training, potentially decreasing environmental impact. We will make our implementation publicly available to support open science and broader community engagement."
        },
        {
            "title": "REFERENCES",
            "content": "Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, et al. Intern-s1: scientific multimodal foundation model. arXiv preprint arXiv:2508.15763, 2025. Michael Bereket and Jure Leskovec. Uncalibrated reasoning: Grpo induces overconfidence for stochastic outcomes. arXiv preprint arXiv:2508.11800, 2025. Graeme Best, Oliver Cliff, Timothy Patten, Ramgopal Mettu, and Robert Fitch. Dec-mcts: Decentralized planning for multi-robot active perception. The International Journal of Robotics Research, 38(2-3):316337, 2019. Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. Forest-of-thought: Scaling test-time compute for enhancing llm reasoning. arXiv preprint arXiv:2412.09078, 2024. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision without process. Advances in Neural Information Processing Systems, 37:2768927724, 2024. Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025. Tuan Dam, Georgia Chalvatzaki, Jan Peters, and Joni Pajarinen. Monte-carlo robot path planning. IEEE Robotics and Automation Letters, 7(4):1121311220, 2022. Quy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small llms: What works and what doesnt. arXiv preprint arXiv:2503.16219, 2025. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):4753, 2022. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Preprint, Under Review Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025a. Shenghua He, Tian Xia, Xuan Zhou, and Hui Wei. Response-level rewards are all you need for online reinforcement learning in llms: mathematical perspective. arXiv preprint arXiv:2506.02553, 2025b. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, dearXiv preprint contaminated, and verifiable mathematical dataset for advancing reasoning. arXiv:2504.11456, 2025c. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Marco Kemmerling, Daniel Lutticke, and Robert Schmitt. Beyond games: systematic review of neural monte carlo tree search applications. arXiv preprint arXiv:2303.08060, 2023. Levente Kocsis and Csaba Szepesvari. Bandit based monte-carlo planning. In European conference on machine learning, pp. 282293. Springer, 2006. Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural theorem proving. Advances in neural information processing systems, 35:2633726349, 2022. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making In Proceedings of the 61st Annual language models better reasoners with step-aware verifier. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315 5333, 2023. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025a. 11 Preprint, Under Review Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025b. Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, et al. Part i: Tricks or traps? deep dive into rl for llm reasoning. arXiv preprint arXiv:2508.08221, 2025c. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, et al. Deepcoder: fully open-source 14b coder at o3-mini level. Notion Blog, 2025a. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025b. Notion Blog. Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, et al. Exploring the limit of outcome reward for learning mathematical reasoning. arXiv preprint arXiv:2502.06781, 2025. Nicholas Metropolis and Stanislaw Ulam. The monte carlo method. Journal of the American statistical association, 44(247):335341, 1949. Yelisey Pitanov, Alexey Skrynnik, Anton Andreychuk, Konstantin Yakovlev, and Aleksandr Panov. Monte-carlo tree search for multi-agent pathfinding: Preliminary results. In International Conference on Hybrid Artificial Intelligence Systems, pp. 649660. Springer, 2023. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. RUCAIBox STILL Team. Still-3-1.5b-preview: Enhancing slow thinking abilities of small models through reinforcement learning. 2025. URL https://github.com/RUCAIBox/Slow_ Thinking_with_LLMs. Harshil Vagadia, Mudit Chopra, Abhinav Barnawal, Tamajit Banerjee, Shreshth Tuli, Souvik Chakraborty, and Rohan Paul. Phyplan: Compositional and adaptive physical task reasoning with physics-informed skill networks for robot manipulators. arXiv preprint arXiv:2402.15767, 2024. Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, et al. Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning. arXiv preprint arXiv:2506.01713, 2025. 12 Preprint, Under Review Peisong Wang, Ruotian Ma, Bang Zhang, Xingyu Chen, Zhiwei He, Kang Luo, Qingsong Lv, Qingxuan Jiang, Zheng Xie, Shanyi Wang, et al. Rlver: Reinforcement learning with verifiable emotion rewards for empathetic agents. arXiv preprint arXiv:2507.03112, 2025a. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025b. Fang Wu, Weihao Xuan, Ximing Lu, Zaid Harchaoui, and Yejin Choi. The invisible leash: Why rlvr may not escape its origin. arXiv preprint arXiv:2507.14843, 2025. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Feiyu Yang. An integrated framework integrating monte carlo tree search and supervised learning for train timetabling problem. arXiv preprint arXiv:2311.00971, 2023. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in neural information processing systems, 34:2547625488, 2021. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 37:6473564772, 2024a. Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024b. Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, et al. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024c. Preprint, Under Review"
        },
        {
            "title": "A RELATED WORKS",
            "content": "Search-based reasoning. Structured search has become standard strategy for scaling test-time computation in LLMs (Snell et al., 2024; Wu et al., 2024; Zhang et al., 2024c), with diverse methods including tree-based (Yao et al., 2023; Zhang et al., 2024b; Qi et al., 2024) and random sampling approaches (Wang et al., 2022). More recently, search-based reasoning has evolved into sophisticated frameworks that integrate three core components: policy models for generating reasoning steps, reward models for evaluative feedback, and search algorithms for exploring solution spaces. Drawing inspiration from game-playing systems like AlphaGo, recent works have explored Monte Carlo Tree Search (MCTS) and beam search to guide LLMs through structured reasoning processes (Chen et al., 2024; Zhang et al., 2024a;c), particularly following OpenAIs o1 model release (Jaech et al., 2024). These frameworks enable exploration of multiple solution paths during inference, trading computational resources for improved accuracy on challenging tasks such as mathematical reasoning. Key design considerations include outcome-supervised versus process-supervised reward models, discriminative versus generative reward architectures, and search strategies ranging from local selection to global exploration (Lightman et al., 2023; Wang et al., 2023). However, despite their effectiveness, most current methods restrict search to inference without integrating exploration signals into training, leaving the potential for joint optimization of search and learning largely unexplored. Reinforcement learning from verifiable rewards. RLVR has emerged as transformative approach for aligning and enhancing LLMs by addressing critical challenges across instruction following (Su et al., 2025; Gunjal et al., 2025), ethical alignment (Wang et al., 2025a), and reasoning capabilities (Wang et al., 2025b). Recent extensions (Guo et al., 2025; Yu et al., 2025; Wan et al., 2025) have improved training stability and efficiency by incorporating critic-free optimization, dynamic sampling, and adaptive weighting mechanisms. While these approaches demonstrate the significant promise of verifiable rewards, they predominantly rely on direct rollouts, which can constrain systematic exploration of the solution space (Wu et al., 2025; Yue et al., 2025). Monte-Carlo Tree Search. MCTS is powerful search paradigm for complex decision-making problems that has been extensively explored across diverse fields, including games (Silver et al., 2016; Ye et al., 2021), robotics (Best et al., 2019; Dam et al., 2022), theorem proving (Lample et al., 2022), and matrix multiplication (Fawzi et al., 2022). Early work such as AlphaGo (Silver et al., 2016) successfully integrated MCTS with deep learning (Kemmerling et al., 2023), achieving superhuman performance in board and video games (Ye et al., 2021). More recently, MCTS has been applied to path finding and train timetabling problems (Pitanov et al., 2023; Yang, 2023), while Vagadia et al. (2024) integrated MCTS into physics-informed planning networks for robot control. Despite the demonstrated potential of MCTS for heuristic exploration, it remains unclear how to effectively employ it during RLVR training."
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "This section provides comprehensive details of our experimental setup, including system implementation, training configurations, MCTS parameters, optimization strategies, and evaluation protocols used in our DeepSearch framework. B.1 TRAINING DATA AND CONFIGURATION We implement our DeepSearch system using the veRL framework (Sheng et al., 2024), conducting all training experiments on distributed setup across 16 NVIDIA H100 GPUs with 96GB of memory. The policy model is initialized with Nemotron-Research-Reasoning-Qwen-1.5B v2 (Liu et al., 2025a) (updated July 23rd). To ensure fair comparison on well-aligned policy, we additionally conduct DAPO-based extended training on the Nemotron-Research-Reasoning-Qwen-1.5B-v2 initialization, using the same training configuration as DeepSearch. Our training methodology employs the DeepMath-103K (He et al., 2025c) dataset as Dtrain, implementing DeepScaleR-style prompt template that instructs the model to Lets think step by step and output the final answer within boxed{}. To manage computational constraints, we apply prompt truncation from the left with maximum prompt length of 2,048 tokens and limit response 14 Preprint, Under Review generation to 16,384 tokens. The training process utilizes global batch size of 256 samples, implemented through the DAPO-style Dynamic Batching strategy (Yu et al., 2025) to optimize memory utilization and training efficiency. B.2 MONTE CARLO TREE SEARCH IMPLEMENTATION Our MCTS implementation incorporates several strategic design choices to balance search efficiency and solution quality. The exploration coefficient (λ) for UCT Local Selection is set to 2.0, providing an optimal exploration-exploitation trade-off for mathematical reasoning tasks. The search architecture operates with maximum depth of 64 levels, where each node is allocated 256 tokens and expands 8 children during the expansion phase. For entropy-based selection, we estimate the average trajectory entropy using only tokens that appear in the response instead of the entire per-position vocabulary for computational efficiency. To enhance search effectiveness, the system employs Global Frontier Selection for backtrace operations and applies square root function for depth-based bonuses, encouraging deeper exploration when beneficial. The global λ3 parameter is configured to 0.01 for our frontier priority scoring, while an overlong buffer of 4,096 tokens with penalty factor of 1.0 accommodates lengthy reasoning chains typical in complex mathematical problems. B.3 ADVANTAGE ESTIMATION AND OPTIMIZATION For advantage estimation, we implement the Grouped Relative Policy Optimization (GRPO) (Shao et al., 2024) estimator with sibling mean normalization to ensure stable learning dynamics. The Qvalue soft clipping mechanism operates at temperature of 1.0 with the maximum q-value magnitude set to 1.0, while incomplete trajectories receive penalty score of 1.0 to discourage premature termination. Standard deviation normalization is disabled to prevent numerical instability during training. The actor model optimization employs AdamW with conservative learning rate of 1 106 and 10 warmup steps, combined with weight decay of 0.1 and gradient clipping at 1.0 for stable convergence. We follow the Clip-Higher strategy in DAPO (Yu et al., 2025) and set the lower and higher clipping range to 0.2 and 0.28 with ratio coefficient of 10.0. Training proceeds with mini-batches of 32 samples per policy update using token-mean loss aggregation, while dynamic batch sizing accommodates up to 18,432 tokens per GPU. The entropy coefficient is set to 0 for pure exploitation, and KL divergence loss is disabled to maximize performance on the target mathematical reasoning tasks. B.4 SAMPLING AND REWARD CONFIGURATION During rollout generation, we configure sampling parameters with high temperature of 1.0 and top of 1.0, while disabling top filtering to maintain diverse response generation. The system generates 8 rollouts per prompt, aligning with the expansion width parameter, within context length of 18,432 tokens. This configuration ensures comprehensive exploration of the solution space while maintaining computational feasibility. During evaluation, we uniformly use low temperature of 0.6 and top of 0.95. custom mathematical reward system implements Our scoring function based on (compute score) from the math dapo.py module, designed to evaluate mathematical reasoning accuracy. We extract the final boxed answer by locating the last occurrence of boxed{} in the trajectory and apply the same text-normalization logic as veRLs DAPO recipe to both prediction and ground-truth. The reward mechanism handles responses up to 16,384 tokens, following ProRL (Liu et al., 2025a) and ensuring consistent evaluation across varying response lengths. B.5 TRAINING PROTOCOL The complete training protocol spans 100 steps with model checkpointing performed every 5 steps. This frequent checkpointing strategy ensures robust model preservation and enables detailed analysis of learning progression throughout the training process. 15 Preprint, Under Review"
        },
        {
            "title": "C PSEUDOCODE OF DEEPSEARCH",
            "content": "Algorithm 1 presents the complete DeepSearch framework, integrating MCTS-based exploration with adaptive training and replay buffer management. The algorithm operates through iterative refinement, progressively focusing computational resources on challenging problems while preserving solved solutions through intelligent caching. This integrated approach focuses on training-time exploration, enabling models to learn from both correct solutions and systematic exploration processes rather than relying solely on outcome-based supervision."
        },
        {
            "title": "LIMITATIONS AND FUTURE WORK",
            "content": "A critical next step involves extending DeepSearch beyond mathematical reasoning to domains with different verification mechanisms. This includes developing approximate verifiers for subjective tasks, exploring human-in-the-loop validation for complex reasoning chains, and investigating transfer learning approaches that leverage mathematical reasoning capabilities for broader problemsolving tasks. Research into domain-agnostic reward functions and verification strategies could significantly expand the frameworks applicability. 16 Preprint, Under Review Algorithm 1 DeepSearch with Global Frontier Selection and Iterative Filtering Require: Initial policy πθ(0), training set Dtrain, verifier V, filtering threshold δ 1: Initialize D(0) 2: for training iteration = 0, 1, 2, . . . do Initialize training trajectories (i) 3: for each batch B(i) D(i) hard {x Dtrain Pass1@K(x, πθ(0)) < δ(0)}, R(0) = train hard do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 0} 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: 45: 46: 47: 48: 49: end for for each problem B(i) do if (x, tcached) R(i) then Tx {tcached} DirectRollouts(x, β) (i) train (i) train Tx else MCTS Search: Initialize search tree with root node for rollout iteration = 1, 2, . . . do if = 1 then Select root node = for expansion else Use cached solution Apply full MCTS search Initial expansion from root Global Frontier Selection: Compute frontier set = {s ξ(s) = 0, / Send, d(s) < dT } Compute frontier priority scores (Eq. 10) Select node = arg maxsF (s) for expansion end if Local Expansion with UCT Selection: Generate candidates {sj}n Continue expansion until terminal nodes (k) Evaluation with Entropy-based Guidance correct = {s (k) Partition: (k) j=1 πθ( os ) from end are reached end V(s) = 1}, (k) incorrect = {s (k) end V(s) = Select most confident negative: neg = arg minsS (k) incorrect H(t(s)) if (k) correct 1 then Extract trajectories T(x) from search tree train (i) (i) train T(x) else end if Heuristic Score Backup: Select trajectory (correct solution or t(s Assign terminal rewards (Eq. 6) for each node sj in do neg)) Update Q-values using constrained backup rule (Eq. 7) end for end for end if Replay Buffer Update: if MCTS found correct solutions but Pass1@K(x, πθ(i)) < δ(i) then Add (x, tcorrect) to R(i+1) for any correct tcorrect T(x) end if end for Policy Update: Update policy πθ(i+1) using Tree-GRPO objective on (i) hard = {x D(i) end for Re-evaluate and filter: D(i+1) hard Pass1@K(x, πθ(i+1)) < δ(i+1)} train (Eq. 16 and Eq. 17)"
        }
    ],
    "affiliations": [
        "Amazon AWS",
        "RIKEN AIP",
        "Stanford University",
        "UC Berkeley",
        "University of Tokyo",
        "University of Washington"
    ]
}