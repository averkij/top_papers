{
    "paper_title": "Measuring Bullshit in the Language Games played by ChatGPT",
    "authors": [
        "Alessandro Trevisan",
        "Harry Giddens",
        "Sarah Dillon",
        "Alan F. Blackwell"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative large language models (LLMs), which create text without direct correspondence to truth value, are widely understood to resemble the uses of language described in Frankfurt's popular monograph On Bullshit. In this paper, we offer a rigorous investigation of this topic, identifying how the phenomenon has arisen, and how it might be analysed. In this paper, we elaborate on this argument to propose that LLM-based chatbots play the 'language game of bullshit'. We use statistical text analysis to investigate the features of this Wittgensteinian language game, based on a dataset constructed to contrast the language of 1,000 scientific publications with typical pseudo-scientific text generated by ChatGPT. We then explore whether the same language features can be detected in two well-known contexts of social dysfunction: George Orwell's critique of politics and language, and David Graeber's characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a statistical model of the language of bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language."
        },
        {
            "title": "Start",
            "content": "Measuring Bullshit in the Language Games played by ChatGPT Alessandro Trevisan, Harry Giddens, Sarah Dillon, Alan F. Blackwell University of Cambridge Manuscript prepared for submission to Critical AI https://www.dukeupress.edu/critical-ai Abstract Generative large language models (LLMs), which create text without direct correspondence to truth value, are widely understood to resemble the uses of language described in Frankfurts popular monograph On Bullshit. In this paper, we offer rigorous investigation of this topic, identifying how the phenomenon has arisen, and how it might be analysed. In this paper, we elaborate on this argument to propose that LLM-based chatbots play the language game of bullshit. We use statistical text analysis to investigate the features of this Wittgensteinian language game, based on dataset constructed to contrast the language of 1,000 scientific publications with typical pseudo-scientific text generated by ChatGPT. We then explore whether the same language features can be detected in two well-known contexts of social dysfunction: George Orwells critique of politics and language, and David Graebers characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that statistical model of the language of bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language. Keywords: Bullshit, ChatGPT, Language Games, Wittgenstein, Orwell, Graeber 1 Introduction Do LLM-based chatbots produce bullshit? If so, do they always produce bullshit? Or do they only usually produce bullshit? How might either claim be investigated or even proven? Could such bullshit be reliably detected using computational methods? And might those methods enable the identification of bullshit in other types of texts? These are the questions from which this paper starts, and which it seeks to resolve. In Section 1, we present two different accounts of the relationship between LLM-based chatbots and bullshit. First we consider the fundamentalist position which holds that LLM-based chatbots necessarily produce bullshit because their outputs are form of linguistic communication characterised by lack of connection to concern with truth [], indifference to how things really are. This is Harry G. Frankfurts (2005: 33-34) definition of bullshit in On Bullshit, and the claim can be further proven philosophically. However, the position is vulnerable to factual inaccuracy LLM-based chatbots can be modified not to be indifferent to the truth. We therefore next consider the probabilistic position, which holds that LLM-based chatbots usually produce bullshit firstly because their training data includes many examples of it, and secondly because the business arrangements through which they are deployed statistically emphasises those aspects of the training data. In this account, we draw on Wittgenstein, describing bullshit as characteristic language game whose aspects can be recognised. In Section 2, we use statistical text analysis to investigate the features of this Wittgensteinian language game, based on dataset constructed to contrast the language of 1,000 scientific publications with typical pseudo-scientific text generated by ChatGPT. Having devised this Wittgensteinian Language Game Detector (WLGD), in Section 3 we return to the question of why products such as ChatGPT are so fluent in replicating the specific language game of bullshit, rather than the many other kinds of language game that they could potentially replicate. We offer two explanations based on key distinction maintained throughout the paper between LLMs, and LLM-based chatbots. We propose two ways of understanding that relation: one, proposing that the latter is paratext (as understood in literary theory); the other, that the sociotechnical configuration of the LLM-based chatbot must be more precisely inspected in order to describe its behaviour in Wittgensteinian terms. In Sections 4 and 5 we undertake two experiments to further test if the WLGD thus detected might reasonably be considered the language game of bullshit. To do so, we explore whether the same language properties can be detected in two well-known contexts of social dysfunction: George Orwells critique of politics and language, and David Graebers characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate 2 that statistical model of the language of bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language. We conclude that, whilst we cannot claim to have done so definitively, our experimental investigations suggest that the WLGD we have designed might legitimately be used as measure of bullshit, BS-meter. 1. LLM-based chatbots and bullshit 1.1 The fundamentalist position: the outputs of LLM-based chatbots are necessarily bullshit? Since the public release of ChatGPT by OpenAI on 30 November 2022, growing body of informal blogs, journalism, peer-reviewed academic articles, and books have classified its outputs as bullshit (Narayanan & Kapoor 2022, Bernoff 2022, Vincent 2022, Katwala 2022, Deck 2023, Blackwell 2023, Gershon 2023, Sundar & Liao 2023, Hicks et al 2024, Hannigan et al 2024, Vallor 2024). Such works do not use the term bullshit colloquially but technically, more often than not with reference to the philosophical definition of bullshit provided by Frankfurt (2005: 33-34): form of linguistic communication characterised by lack of connection to concern with truth [], indifference to how things really are. According to this fundamentalist position, the outputs of LLM-based chatbots are bullshit because they are produced with no regard for truth or falsity. As Murray Shanahan (2024: 70) explains (although he does not advocate the fundamentalist position), LLMs are generative mathematical models of the statistical distribution of tokens in the vast public corpus of human-generated text, where the tokens in question include words, parts of words, or individual characters. They are generative, because we can sample from them by posing them questions (i.e. in the style of chatbot), but the questions can always essentially be understood to be in specific form: Given the statistical distribution of words in the vast public corpus of (English) text, what words are most likely to follow the sequence [] (70). In answer the LLM-based chatbot merely generates statistically likely sequence of words (71). There is no authentic communicative intent (72).1 As reported by AI professor Rodney Brooks, It just makes up stuff that sounds good (Zorpette 2023). The very first connection between natural language processing (NLP) and bullshit was made before OpenAI released ChatGPT, in Deck (2023), which although published in May 2023, was first submitted for publication in June 2022. There, Deck identifies that the philosophers Stokke and Fallis (2017) refinement of Frankfurts definition of bullshit is even more 3 appropriate to the outputs of LLM-based chatbots. Stokke and Fallis propose that bullshit is produced when speaker is indifferent to whether or not their utterance constitutes truthful answer to QUD [Question Under Discussion] (Deck 2023: 60). On this definition, the outputs of LLM-based chatbots just the statistically most likely sequence of words in relation to the prompt are not just produced with no regard to their truth or falsity but, more specifically, with no regard for providing true answers to the question put to them. But Deck misses key point here, in that he does not acknowledge that there is not in fact an equivalence between question under discussion in human inquiry, and question prompt to an LLM-based chatbot. question under discussion in human inquiry is part of mankinds cooperative project of incremental accumulation of true information with the aim of discovering how things are, or what the actual world is like (Stokke & Fallis 2017: 279). question put to an LLM-based chatbot might seem to be doing the same thing, but in technical terms it is actually just question about sequence prediction. Coming back to bullshit, for Stokke and Fallis, the fundamental deception lies in the pretence of the bullshitter that they are cooperatively participating in discourse. However, with regard to LLM-based chatbots, the fundamental misperception is that prompt is question, that is, that prompt is request for cooperative participation in discourse with the intent (of both questioner and responder) to cooperatively advance knowledge about the world. Stokke and Fallis definition of bullshit is useful in relation to fundamentalist position regarding the outputs of LLM-based chatbots and bullshit because it exposes the fact that interaction with an LLM-based chatbot is not participation in cooperative inquiry at all. The outputs of LLMbased chatbots are misunderstood or misrepresented if they are taken to be reliable information about what the world is like. For this reason, the fundamentalist position would concur with Frankfurt that bullshit poses greater threat than lying one cannot fact-check bullshit, which means one cannot disprove it. And yet it has the same disruptive force in the world as lying, in that the production of bullshit by LLM-based chatbots (which then feeds into the text that the next generations of LLMs are trained on), reverses the goal of inquiry (Stalnaker 1984, 1999a, 1999b, 2002), the discovery of what the world is like. Whilst the aim of alethic discourse is to advance human knowledge, non-alethic discourse, such as bullshit (both machine and human-generated), degenerates and regresses human knowledge.2 1.2 The probabilistic position: the outputs of LLM-based chatbots are usually bullshit The fundamentalist position appears philosophically robust and is rhetorically charismatic. However, it is challenged by the fact LLM-based chatbots can be explicitly designed to adhere to factual claims (for example by invoking an arithmetic module to solve equations). It therefore does not hold to claim that they always produce outputs that are indifferent to their truth or falsity as response to the prompt. Many chatbots are connected are now connected to the Internet, meaning that they can in principle quote by reference to trusted resources such as Wikipedia, instead of just predicting text based on linguistic patterns learnt during training. This significantly reduces the likelihood of errors. Since LLM-based chatbots are able to respond with factual accuracy to some questions and therefore without indifference to truth or falsity it does not hold that they always produce bullshit. But might they be said to usually produce it? To explore the answer to that question, we take our cue from Shanahans introduction of Wittgenstenian philosophy to his discussion of LLMs. Shanahan (2024: 73) maintains that an LLM-based chatbot cannot participate fully in the human language game of truth because it does not inhabit the world we human language users share. language game, in Wittgensteins later philosophy, is concept introduced to designate uses of language that belong to specific social activity specific form of life, to use Wittgensteins own terminology (2009: 15e). Some examples of forms of life, given by Wittgenstein himself, are: giving orders, reporting an event, cracking joke, requesting, thanking, cursing, greeting, praying (15e). In section 3.2, we will return to the question of whether an LLM-based chatbot can be considered to partake in any form of life. Here, we want to concentrate on whether the idea of language games offers useful structure of inquiry into the relationship between the outputs of LLM-based chatbots, and bullshit. Immense quantities of text have been collected to pretrain the statistical data patterns of the transformer' algorithm (Pretrain[ing] and Transformer[s] being the prerequisites for Generation, as indicated by the now popular GPT acronym). Although the details of the training data are secret closely guarded by these companies, reasonable approximation considers it simply as everything on the Internet. There are undoubtedly many people on the Internet who write bullshit, just as there are people who write poetry, sermons, newspaper articles, science textbooks, and many other kinds of text. The GPT language model does, therefore, contain countless examples of bullshit, just as it contains countless examples of truth, lies, debate, and many other language games. 5 LLMs trained on the whole Internet therefore contain traces of the many kinds of language game that humans play with other humans (via the textual media of the Internet). We can think of the pretrained transformer as being like the cloud chambers originally used to study particle physics. The cloud chamber is not nuclear reactor, and does not generate particles. The cloud allows an observer to see where reaction has occurred, captured in characteristic trace of droplets. Just as cloud chamber captures the traces of particle rather than creating it, so the LLM captures the traces of bullshit, rather than being responsible for its creation. In the same way that cloud chamber reveals where particle has travelled, we propose that an investigation of bullshit could use the statistical recordings in the LLM transformer model to reveal the characteristic traces of particular language game in the outputs: Wittgensteinian Language Game Detector, or WLGD. The claim here would not be that the LLM is bullshit, nor that it can only produce bullshit. similar strategy to ours for example has been used to identify traces of sycophancy (Perez et al 2023, Sharma et al 2023) which might also be described as language game played by LLM-based chatbots. We simply observe that bullshit produced by humans has been recorded on the Internet, that examples of bullshit have been caught up with many other language games in the pre-training process, and that the LLM can thus be used as research tool to study those traces. But in the experiments that follow, we do suggest that the WLGD might be able to identify the traces of particular species of language game, in the same way that cloud chamber was once used to identify the traces of particular species of subatomic particle. The WLGD is trained not with the intention of detecting subtle statistical signatures of the underlying transformer algorithm, but rather with focus on the surface features that characterise any Wittgensteinian language game - words and contexts. 2. Wittgensteinian Language Game Detector (WLGD) In Section 3, we will investigate in more detail how and why products such as ChatGPT exhibit the behaviour that they do. We will also address the perennial concerns signalled by scare quotes when describing the behaviour of computer system as if its internal state can be considered knowledge or its output considered speech in ways comparable to person. But we start with an empirical investigation, broadly motivated by Wittgensteins conception of the nature of language philosophical standpoint which, as we explain later, has itself been influential on the development of the technical natural language processing methods underpinning the development of LLMs. 6 Alerted by Wittgenstein to the fact that language game consists of ways that particular words are used in particular social contexts, our WLGD describes these traces using state of the art machine learning methods - one model that identifies characteristic traces of word frequencies, and second that identifies characteristic traces of contexts in which words are used. In order to train these models, we created controlled corpus of ChatGPT output, constructed specifically to encourage the language game of bullshit, taking Frankfurts definition of the bullshitter as person who talk[s] without knowing what he is talking about. Again, we emphasise the cloud chamber nature of our experiment - we are not (yet) considering whether the LLM truly knows or talks, but simply observe that it has the capacity to generate text showing the characteristic features of this language game. 2.1 The training set Our goal is to distinguish the language game of bullshit from other kinds of language game that could have been precise, factual, clear and concise - the opposite of the bullshitters described by Frankfurt. Different sectors of society may have different views on the kinds of language they admire, but we have chosen basis for broad consensus in the house style of Nature magazine possibly the most widely read and respected scientific journal in the world whose strict editorial standards produce published articles that can be taken as exemplars of precision, clarity and concision, while undeniably reporting (newly observed) scientific facts. To provide contrast to this language game of prestigious international science, we prompted the latest version of ChatGPT (at the time of our experiment, the 4o release) to write an article for Nature magazine, with the same title as an actual article. To ensure that this emulated the style of Nature article as closely as possible, we included Natures instructions to authors in the prompt. The resulting text, even to someone with minimal scientific training, was obviously bullshit including tables of fabricated data observations, some outright lies, unconnected arguments, but also long passages of vague but science-y text typical of the writing of weaker students. Most of these features have become familiar to those using these products (or whose students use them), and much commentary has been devoted to speculation and handwringing about the problems of hallucinations, fabrication and so on. We will present an alternative account, but for now focus on the construction of our WLGD. We created training dataset large enough to be used with state of the art machine learning methods by collecting 1,000 articles from Nature, which we take to represent the language game of scientific communication as defined and selected by the editors - precise, factual, clear and concise. We then took the titles of those 1,000 articles, and asked ChatGPT to produce 1,000 pieces of (actual) bullshit, attempting to address the same title in the style of Nature, but actually reflecting the language game of speaking without knowledge. Our hypothesis is that WLGD trained to distinguish between these two sets of writing will detect the cloudy traces of the BS language game that have been encoded in the LLM. 2.2 The classifier Our WLGD studies the characteristic traces of words being used in social contexts. Although Wittgensteins philosophy underpins the concept of word embeddings now fundamental to the transformer architecture, that philosophical grounding has been largely forgotten by AI researchers, following the dynamics articulated by Philip Agre (1997). We therefore use standard natural-language processing tools, but in slightly unorthodox way, to recover the original arguments made by Wittgenstein. We created two models: one that identifies characteristic word frequencies, and one that identifies characteristic traces of contexts. XGBoost, the first algorithm we trained for our experiments, is widely used in machine learning for simple text classification. For these tasks, it primarily relies on TF-IDF (term frequency inverse document frequency), measure of the extent to which particular word is distinctive of the document in which it appears. TF-IDF was invented by Karen Spärck Jones (1972), now celebrated as computer scientist, but who we should remember started her research career at the Cambridge Language Research Unit, directed by Wittgensteins student Margaret Masterman. As noted in the Stochastic Parrots critique of LLMs (Bender et al 2021), Spärck Jones offered the earliest linguistic critique of computational language models (2004), so in applying her work to recover this Wittgensteinian interpretation of LLMs, we celebrate the foundational achievements of Masterman and Spärck Jones. We thus used XGBoost to create statistical model of the terms that best distinguished genuine Nature articles from those fabricated by ChatGPT. We followed standard practice by removing uninteresting stop-words such as and, the, etc. from the TF-IDF vocabulary. We also removed small number of words that simply reflected the formatting of Nature articles. (In particular, we found that our earliest iteration of XGBoost could identify Nature articles with high confidence because they always included the word figure or fig, often followed by an alphanumeric sequence such as 1a, 2b, etc. The ChatGPT output, being plain text without figures, did not use these words.) The resulting classifier is 100% accurate, and reports high confidence (99.84%) in judging further examples constructed the same way. 8 Our second classifier, fine-tuned RoBERTa transformer model (Liu et al 2019), instead determines whether text is closer to Natures style or to ChatGPTs by analysing the structural context of language use rather than the words themselves. The RoBERTa transformer architecture, direct ancestor of todays LLMs, processes text by encoding each 'token' (a word or subword unit) as multidimensional vector that captures the surrounding context of other tokens. By classifying text based on the similarity of its contextual embeddings to those found in Nature papers and ChatGPT outputs, RoBERTa effectively compares the structural properties of those linguistic contexts. As with the word-based classifier, the RoBERTa classifier is 100% accurate, and also reports high confidence (99.97%) in judging further examples. 2.3 The WLGD metric Fig. 1 shows the distribution of (log transformed) confidence scores from the XGBoost classifier, for the experimental texts that we discuss in the remainder of this paper. Fig 2 shows the distribution of confidence scores from the RoBERTa classifier. As seen in the scatter plot of Fig 3, there is only small correlation between the confidence values of the two different classifiers (r = 0.282), meaning that they are basing their judgement on different language features (word frequencies in one case, and token embeddings in the other). Our WLGD method therefore combines the outputs of these independent elements of the language game. Fig. 1 - frequency distribution of (log transformed) confidence scores for word classifier 9 Fig. 2 - frequency distribution of (log transformed) confidence scores for contextual classifier Fig. 3 - correlation of (log transformed) confidence scores for word and context classifiers Whereas many machine learning classifiers simply report the most likely output class (if above some confidence threshold), our interest in the cloud-chamber traces of language games means that we are particularly interested in the confidence values themselves, which can be taken as computed information-theoretic summary reflecting the internal weights and activations within the models. The graphs in Figs. 1, 2 and 3 show (information-theoretic) log values of the reported confidence, making this positive value if the classifier judged that the text is more like the ChatGPT training set, and negative if more like Nature articles. We use linear factors to scale these log values to range from 0 to 100 that can be used as an easily interpretable detector score. In future, more powerful discriminator could be created using optimisation approaches, but our results in the following experiments show that even simple average of these two independent scores can be used as surprisingly accurate WLGD. 3. Why do chatbots produce bullshit? Having explained the empirical basis for our WLGD method, we now return to the question of why products such as ChatGPT are so fluent in replicating the specific language game of bullshit, rather than the many other kinds of language game that they could potentially reproduce from the LLM cloud chamber. We propose that the key distinction to attend to is the one between the sequence-prediction capabilities of LLMs created using the transformer architecture, and the way that these capabilities are used to construct LLM-based chatbots (Stone et al 2024). In one famous lineage, OpenAIs original GPT LLM attracted little public attention3, while the LLM-based chatbot ChatGPT was blockbuster success. The technical components needed to turn an LLM into an LLM-based chatbot are described by Shanahan (2024: 74) as supplementary dialogue management system (DMS). product such as ChatGPT consists of two elements - the underlying LLM, and this supplementary DMS. Given the huge commercial value of the transition from LLM to LLM-based chatbot, it is unsurprising that the details of the DMS are highly secret, often not even mentioned, to an extent that many commentators believe ChatGPT to be nothing more than an LLM. There are few published descriptions of the DMS component, and statements made in public are not necessarily to be trusted, given the billiondollar investments depending on their reception. However, it is reasonably certain that these modules (sometimes called guardrails when company wishes to emphasise their concern for consumer safety) employ methods such as instruction-tuning, prompt-engineering, and reinforcement learning from human feedback (RLHF) (Stone et al 2024). Shanahan et al (2024) describe the distinction between the many potential behaviours encoded in the underlying LLM, and the actual behaviour resulting from the DMS as role play. While Shanahans analysis also draws on Wittgensteins philosophy of language (2010, 2024), our own interpretation and conclusions are very different. 3.1 Paratext and Eliza effect We suggest that one way of understanding the relationship between an LLM on the one hand, and an LLM-based chatbot on the other, is in terms of the distinction in literary theory between text and paratext. The paratext is neither boundary nor border but, in Gérard Genettes (1997: 2) account, threshold. The paratext is, as Philipe Lejeune (1975: 45) understands it, fringe of the printed text which in reality controls ones whole reading of the text (cited in 11 Genette 1997: 2). The paratext includes all the material one finds around the main text in the copy of work, such as the name of the author, the title of the work, the subtitle, preface, publisher, the typesetting, cover image, blurb, endorsements and so on. But it also extends beyond the physical form of the text, to the marketing, reviews, author interviews, private correspondence, diaries, and so on. Genette (1997: 2) highlights that the paratext is zone not only of transition but also of transaction: privileged space of pragmatics and strategy. He is generous in his interpretation of its purpose, seeing it as aimed at the service of better reception for the text and more pertinent reading of it (2). But he does caveat that that pertinence is always in the eyes of the author and his allies. paratext is therefore always designed to influence public reception of text in line with how the author and their allies wish that text to be received. In our analysis, the DMS of ChatGPT the guardrails, instruction-tuning and so on constitutes paratext that is intended to present the statistical sequence-generating capabilities of the underlying LLM as if it were conversation between living persons, rather than simple series of probabilistically-chosen words. Alternative paratexts would determine the reception of ChatGPTs outputs differently.4 Genette (1997: 2) gives the following example: limited to the text alone and without guiding set of directions, how would we read Joyces Ulysses if it were not entitled Ulysses? Similarly, if, as Shanahan (2024: 70) suggests, every output of an LLM response was preceded by phrase such as given the statistical distribution of words., how would that determine the way users perceived and interacted with it, and how its author and their allies were able to present it? Or, again, how would we perceive an LLM-based chatbots outputs if we could see the hidden prompts it receives before addressing our own ones? The DMS-paratext is responsible for the misperception that, as discussed in section 1.1, question prompt put to an LLM-based chatbot is request for participation in cooperative dialogue and advancement of knowledge, rather than just request for sequence prediction.The DMS-paratext of ChatGPT also triggers the social cognition centres of the human brain to anthropomorphise,5 and can be understood as part of the history of disingenuous rhetoric that has surrounded, and continues to surround, artificial intelligence (AI), and which strongly influences users perceptions of AI technologies.6 This has been understood with regard to chatbots since at least the very first one, Joseph Weizenbaums ELIZA (Weizenbaum 1966, Weizenbaum 1987, Dillon 2020, Stone et al 2024). In 1995, Douglas Hofstader (1995: 157) coined the term the Eliza effect, to name the susceptibility of people to read far more understanding than is warranted into strings of symbols especially words strung together by computers. The Eliza effect is in full swing around LLM-based 12 chatbots, encouraged by the DMS-paratext. For example, the use of the first person in responses, which is only convention of the DMS, and could easily have been implemented differently. The Eliza effect is also encouraged by other paratextual elements such as the hype from companies, researchers, and elite cuers, and the language of media coverage, much of which frames LLM-based chatbots as if they possess intention, knowledge and reasoning capabilities.7 Consider, for instance, the idea of hallucinations, the term introduced to designate the fabrications that LLM-based chatbots sometimes produce in their sequence predictions in response to question prompt.8 Similar to other anthropomorphic terms to describe machine processes, the choice of the term hallucination is result of and/or reinforces the Eliza effect. hallucination, as specified in the OED, is mental condition of being deceived or mistaken, or of entertaining unfounded notions: to hallucinate, therefore, mind and set of beliefs (notions), and knowledge is required. The underlying LLM has none of these attributes, and although the DMS-paratext of the LLM-based chatbot might emulate the language games associated with them, the emulation of beliefs and knowledge is simply more bullshit. Indeed, the commercial hype surrounding AI research, and the disingenuous rhetoric of AI investors seeking public policies that will protect their investment while limiting their liabilities, might also be quite strictly defined as bullshit - yet another element of the paratext created for marketing and exploitation of LLMs. 3.2 Language Games and Lebensformen We have described the relationship between the LLM and its DMS-paratext using the tools of literary theory, but how does this cultural construct relate to Wittgensteins language games, and to our analogy of the LLM as cloud chamber in which characteristic traces of specific language games can be recognised? We explore this question via Wittengsteins concept of the Lebensform. The term is conventionally translated by English-speaking philosophers as form of life, but the German word can also be translated as lifeform - that is to say, biological entity. In German, it is possible to make play on words between these two uses of the term. We suggest that Wittgenstein, whose Philosophical Investigations so constantly relied on ambiguity and alternative readings (in contrast to the certainties of his Tractatus), often embraced such nuance, and that English-language uses of his work might do likewise, as do we. 13 It is tempting to interpret Wittgenstein as proto-structuralist, where Lebensform is taken to refer to structures of discourse as in our use of Genettes paratext. Alternatively, using the biological sense of Lebensform, it might be tempting to read Wittgenstein as proto-new materialist, in the manner of Haraways (2016) critters or Latours (2017) actants. However, if more politically informed as the developments of contemporary AI seem to demand (Schaake 2024, Couldry & Mejias 2022, Muldoon et al 2024) we propose Lebensform as describing the economic agency of sociotechnical assemblage, as when Marx describes human workers as no more than conscious linkages within capitalist machine,10 or David Runcimans observation that we have had AI for centuries (Azhar 2020) ever since we defined corporations as artificial legal persons. Applied linguist Clarisse Sieckenius de Souza, in her Semiotic Engineering approach to user interface design (1993), observes that interactive computer systems are interpreted by their users not naively as conscious entities, but as the designers deputy - communicating messages determined by design by people working on behalf of the company that deploys these products. Whereas many have been distracted by Dennetts (1989) intentional stance in relation to AI, far fewer have attended to the subtler perspective, also included in Dennetts typology, of the design stance (Crilly 2011). So here, we wish to interpret the Lebensform as interaction of customer-user-employee-citizens within complex sociotechnical system, where the number of components of that system - both human, commercial, and electronic-material - are literally uncountable. Wittgenstein did not intend his characterisation of language games to be obscure or purely technical. On the contrary, everyday use of language involves the routine and unremarkable recognition, development and application of the various language games in which we all partake. Everyday competence in language use is achieved regardless of any descriptive theory, as productively emphasised in the ethnomethodological tradition within the field of Human Computer Interaction (Blackwell 2017). Ordinary users may not fully understand how LLM-based chatbots have been constructed to talk the way they do, but we all recognise the language games they play because they share resemblances to other Lebensformen. While Wittgenstein spoke of language games played by humans, rather than world where corporate machinery automatically generates textual dialog, we believe that his commonsense interpretation of language can usefully be applied to these immeasurably more complex Lebensformen. Rather than metaphysical questions of intentionality, or the nature of knowledge, we can pragmatically study the Lebensform of LLM-based chatbots as kind of human-computer interaction, where the computer rather than singular actant is in reality the sociotechnical assemblage of humans, machines, financial markets, environmental devastation, political movements - in short, manifestation of the contemporary condition. 14 4. Experiment 1: The Language of Politics Now that our WLGD method has been defined, and we have established an understanding of the effects of the DMS-paratext and of the assemblage of which the LLM-based chatbots are part, the question is whether we can test the hypothesis that the language game detected in the ChatGPT Nature outputs is indeed the language game of bullshit, rather than some other type of language game. To do so, we apply the WLGD to the classification of new texts. In our first experiment, we theorise and set out to test empirically degree of affinity between the language used by ChatGPT and that used in political campaigning. In our choice of the language of politics as potentially classifiable as bullshit, we build on the work of George Orwell, perhaps the major contemporary forerunner of an approach to bullshit that focuses on the text itself (Fredal 2011: 247). In Orwells writings, and especially in Politics and the English Language (1941), political speech is described as an intentionally uninformative form of language. This proposition will hardly come as surprise: Frankfurt himself suggests that bullshit is often found in advertising, public speech, and the nowadays closely related realm of politics (2005: 22). We aim to extend this provocative line of inquiry by suggesting that political language stems from form of life comparable to that which generates the outputs of LLM-based chatbots. 4.1 Comparing political party manifestos to everyday spoken English As an experimental sample, we selected from corpus of written English reflecting one of the types of political discourse to which Orwell most objected the manifestos published by political parties in UK general elections.11 We obtained 45 party manifestos, spanning the years 1945 to 2005, from the online archives maintained by the Manifesto Project (Lehmann et al 2024a),12 and calculated the WGLD score for each of them. As contrast to this corpus of political speech, we calculated the WGLD scores for transcripts of spoken English as found in the British National Corpus (BNC).13 We selected the BNC to represent what Orwell called demotic speech (1969, 3: 135), the speech of the average man (135), of the workingman (136). According to Orwell, everyday demotic speech is generally characterised by transparent and communicative uses of language and is an example of the type of language that politicians should strive to adopt in order to more effectively communicate with their citizens. In this sense, if doublespeak is performative speech act meant to deceive, we can imagine that the conversations recorded in the BNC are more genuine, produced for the primary purpose of communicating information (even though we cannot rule out that bullshit might be produced even in these forms of life). Many of them are transcriptions of school lessons, university tutorials and lectures, hobbyist or professional training sessions, and of casual conversations between family, friends, colleagues, and strangers. After excluding transcripts of political meetings (mainly local government and trade union meetings) and news reports (frequently involving political commentary), we randomly selected 45 BNC texts of similar length to the corpus of 45 party manifestos. Using the method described above, we calculated the WLGD scores for each of these 45 texts. As far as we know, none of these 90 texts were written or spoken by research scientists, and none were written by ChatGPT because they predate it. Although the BS-WLGD classifiers were trained only to recognise scientific text and ChatGPT-generated text, our hypothesis is that the traces of the language game selected by the DMS-paratext of the LLM-based chatbot to construct factually ungrounded bullshit science may resemble the uses of the English language in political discourse that were criticised by Orwell, and that he contrasted with everyday speech. Our experiment therefore tests the hypothesis that these two kinds of language games may have statistically differentiable features (words and their context) that we can generically consider as bullshit in the Frankfurt sense, whose traces can be measured through the application of the WLGD method. The null hypothesis (H0) is that the WLGD scores will be the same for non-scientific, nonChatGPT text regardless of whether the text comes from political source or from speech source. The alternative hypothesis (H1) is that the WLGD scores will be different in samples of political texts, by comparison to samples of everyday speech. 4.2 Results As shown in Fig. 4, the average WLGD score for UK political manifestos is 49.36, while the average WLGD score for non-political speech data from the British National Corpus is far lower, at 9.40. This difference is highly significant (t(54)=18.18, << 0.001), meaning that we can reject the null hypothesis. 16 Figure 4 - comparison of WLGD scores calculated for 45 samples of everyday UK speech from the British National Corpus, and 45 manifestos published by UK political parties. We conclude, therefore, that the uses of the English language criticised by Orwell do share statistical properties with our training dataset, which was constructed on the principles described by Wittgenstein and by Frankfurt to identify characteristic traces of bullshit. We state this cautiously, as appropriate to scientific investigation, and without wishing to suggest that correlation reflects causation. Nevertheless, the remarkably high statistical significance of our experimental finding, and the large difference in means on our WLGD scale of 0-100, do indicate that the method is reliably able to capture particular language game. Furthermore, it seems that this game, whatever we call it, is remarkably shared between the statistical properties encoded in the latest LLM-based chatbots on one hand and, on the other, the uses of the English language criticised by Orwell more than 50 years ago. 5. Experiment 2: Bullshit Jobs Keeping in mind that we are studying the language games played, not by human person, but by an LLM-based actant, constructed as DMS-paratext within sociotechnical assemblage having economic rather than neurological agency, what is the best way to study this complex of economic characters and fictions? For this purpose, we carry out second 17 experiment, investigating the world of contemporary employment within which such actants operate. In Bullshit Jobs (2018: 9-10), David Graeber defines bullshit jobs as paid employment that is so completely pointless, unnecessary, or pernicious that even the employee cannot justify its existence even though, as part of the conditions of employment, the employee feels obliged to pretend that this is not the case. Shannon Vallors (2024: 121) comparison between the rhetorical style of LLM-based chatbots and that of smooth and oily car salesman is useful starting point to investigate possible parallel between the bullshit text resulting from the DMS-paratext of chatbots, and the text created in bullshit jobs identified by Graeber. car salesman is indeed bullshitting according to Frankfurt, if he misrepresents his intentions and state of mind, appears falsely invested in clients interests and inflates the worth of the cars he sells. His job would also be classified as bullshit according to Graeber if it does not contribute any social value. For example, the job itself might complicate process that could be more straightforward if the client could just get precise, clear and concise description of the condition and characteristics of car. Like the car salesman imagined by Vallor, the DMSparatext of LLM-based chatbots, as we have seen, has been designed to produce text that persuades customers as to their competence, adjusted to interact with users amiably and with veneer of sycophancy or professionalism but not, crucially, to be truthful. Just like someone with bullshit job, in other words, the bullshit-generating DMS-paratext added to the LLM is designed to produce an illusion of meaningful work. We designed second hypothesis-testing experiment, to compare the Frankfurtian language game of bullshit text to the text resulting from the Graeberian employment conditions of the bullshit job. Our hypothesis is that text produced by those employed in bullshit jobs is itself more likely to have the language game characteristics of bullshit text. As an objective measure of this WLGD we again use the BS-meter scores described above. 5.1 Measuring the language games played in bullshit jobs We collected 100 sample texts from online sources, 50 of which were selected as likely to have been written by people employed in bullshit jobs (as characterised by Graeber). control sample of 50 further texts were selected as likely to have been written by people employed in professions that would not fall within the scope Graeber defines as bullshit. None of the 100 texts were written by scientists, and none of the 100 texts (so far as we are aware, given the challenges of precise provenance and dating for informal online publications) were written by 18 ChatGPT. We used this sample of texts from 50 hypothetically bullshit jobs and 50 hypothetically non-bullshit jobs to test the hypothesis that these two samples have statistically differentiable language features. Identifying the experimental sample of texts required considerable research judgement. Graebers book describes the characteristics and status of bullshit jobs at substantial length, including individual case studies as well as survey findings and interview research. In one section of the book, he offers typology of five particularly frequent classes of bullshit job. However, this typology is neither strictly defined, nor necessarily rigorous, and certainly not presented in way designed for investigation through hypothesis-testing experimentation. We therefore used the more succinct summary version of Graebers typology, developed through consensus by the editors of the Wikipedia article Bullshit jobs, to serve as the working definition for our sample construction (the descriptive labels are Graebers own): Flunkies, who serve to make their superiors feel important, e.g., receptionists, administrative assistants, door attendants, store greeters; Goons, who act to harm or deceive others on behalf of their employer, or to prevent other goons from doing so, e.g., lobbyists, corporate lawyers, telemarketers, public relations specialists; Duct tapers, who temporarily fix problems that could be fixed permanently, e.g., programmers repairing shoddy code, airline desk staff who calm passengers with lost luggage; Box tickers, who create the appearance that something useful is being done when it is not, e.g., survey administrators, in-house magazine journalists, corporate compliance officers; Taskmasters, who create extra work for those who do not need it, e.g., middle management, leadership professionals.14 Although Graeber did not pay substantial attention to the other kinds of work in society that constitute non-bullshit jobs, these are mentioned from time to time in his text. It is also possible to propose general principles from the social sciences that are likely to be consistent with Graebers critical orientation, for example professions that directly deliver services at the base of Maslows hierarchy of needs, or professions where the employee is directly engaged in labour rather than in supervising or managing others. For each of these five classes, we therefore selected 10 texts that we would expect to have been written by or for those employed in that class, and 10 contrasting texts that demonstrated 19 the opposing, non-bullshit principles. There is some danger that our selection process might be considered as derogatory or libellous, since the original author can often be directly identified. For that reason, we are not publishing this experimental dataset, but will be happy to make it available on request, for replication purposes. The specific rubrics that we used to collect these 10 sets of sample texts are reported in Table 1. Bullshit sample Contrast sample Flunkies We searched for corporate We searched for historical biographies of biographies of prominent chief engineers, avoiding those who had executives, hypothesising that founded companies, were famous these are likely to have been household names, or celebrated for drafted by flunky expected to reasons other than their practical make the subject feel important. contributions. Goons We searched for examples of We selected text from organisations corporate mission statements, whose purpose is to directly report the hypothesising that these have been truth against opposition, including written by some combination of whistleblowers, public health educators, lobbyists, lawyers, and PR and human rights information services. specialists. Duct tapers We searched for policies for We searched for practical instructions on temporary repair and delays, how to directly fix problems with management of complaints, appliances, construct things, or achieve refunds, compensation, and immediately useful results. samples of how to write business apologies. Box tickers We selected text written by or about We sampled writing by people who create corporate compliance officers who goods at the base level of Maslows had been recognised with prizes in hierarchy of needs including farmers, the 2023 and 2024 International agricultural and food processing Compliance Association awards. engineers, chefs, water treatment engineers, builders, garment manufacturers and nurses. Taskmasters We selected personal statements or We sampled job descriptions that do not blog entries from the company directly include leadership, delegation or websites of award winners in management, including building 20 Global Gurus Top 30 - the World's construction, food preparation, laundry, Top 30 Leadership Professionals and road maintenance. for 2024. Table 1: Sample rubrics used to collect texts associated with bullshit jobs Having avoided scientific and LLM-generated texts, this sampling process has no experimental bias relating to the hypothesis. Although the text sampling procedure involved significant element of research judgement in applying and interpreting Graebers analysis, there is no reason to expect that either of these two samples of text would be more or less Nature-like, or more or less ChatGPT-like, or that either would have greater preponderance of any particular WLGD features. Our hypothesis testing procedure did not involve any prior expectation as to which of these 100 sample texts might have which WLGD features, or whether there would be any statistically observable differences at all. The null hypothesis (H0) is that the WLGD scores will be the same for non-scientific, nonChatGPT text regardless of whether the text is written by someone with (Graeberian) bullshit job or non-bullshit job. The alternative hypothesis (H1) is that the WLGD scores will be different in texts written by people in samples selected to represent these two classes of jobs. 5.2 Results We performed repeated-measures analysis of variance (ANOVA), with two factors as independent variables. One independent variable was bullshit/non-bullshit contrast, with two values, and the other variable was the Graeber class, with five possible values. The dependent variable was the BS-meter score. We observed highly significant main effect (F(99,1)=43.73, << 0.001), meaning that we can reject the null hypothesis with extremely high confidence. The effect size is large, with an overall mean BS-meter score of 52.47 for bullshit jobs, compared to 28.87 for the contrast sample. We can conclude that samples of text selected to represent employment in Graeberian bullshit jobs do resemble the Frankfurtian bullshit produced by ChatGPTs DMSparatext far more than they do precise, factual and clear scientific writing. Conversely, more mundane non-bullshit jobs, even low-skill professions such as laundry, cleaning or road repairs, perhaps surprisingly have more resemblance to top-quality scientific writing than to ChatGPT output. Figure 5 - comparison of WLGD scores for text produced in the five classes of Graebers bullshit jobs, as compared to contrast texts in other jobs. We also observed weaker effect of differences in WLGD scores over the five Graeber classes (F(99,4)=3.30, p=0.014), which interacts with marginal significance with the strong bullshit effect (F(99,4)=1.92, p=0.114). As seen in Fig. 1, this interaction results in large part from the variability in scores for the selected non-bullshit control texts. In all five categories, there are large differences in means between the hypothetically bullshit texts, and the selected comparison texts. In order to investigate these further, we carried out post-hoc t-test to compare the means of the 5 groups of 20 contrasting samples, as shown in Table 2: Mean BS score Mean contrast score Significance Flunkies Goons Duct tapers Box tickers Taskmasters 54. 62.43 42.50 24.38 47.16 13.28 40. 31.08 20.71 25.14 << 0.001 < 0.05 = 0. < 0.05 < 0.05 22 Table 2 - posthoc comparison of WLGD scores for each of the five Graeber classes of bullshit jobs. p-values under 0.05 (5%) are considered statistically significant. p-value far less than 0.001 is highly significant. p-value greater than 0.1 (10%) is considered potentially unreliable as the basis for scientific claim. We report the varying significance scores between the five classes for completeness, reflecting best practice in hypothesis-testing experiments. The significance values in Table 5 might possibly be taken as evidence that, while every one of the five Graeber classes seems to produce more bullshit on average, perhaps some duct tapers are not so bad. However, looking again at Figure 5, we can see that the main reason for lower statistical reliability in the case of duct tapers is that the contrast texts used for comparison in this particular class had greater variability. Our main source of contrast text in this case was the popular wikihow site, suggesting that some contributors to that site may be bullshitters, or perhaps even that some of the content is now being generated by LLM-based chatbots. Future investigations using the WLGD method, especially if replicating our application of the method to Graebers studies of work, should take further care in selecting the texts used. 4. Conclusion BS-meter? Many commentators have already observed that AI as recently manifested in LLM-based chatbots appears to produce bullshit. Although this is often evident from simple observation of its outputs, and offers an easy target for satirical critique, nobody has yet been able to say quite how the bullshit got there, or by what mechanism it is produced. In this paper we have demonstrated how the statistical methods of natural language processing, heavily influenced by the language philosophy of Wittgenstein, can themselves be used as instruments to study Wittgensteins characterisation of semantics as language game of how words are used in social contexts. We show how the word embeddings encoded in LLMs can be used in the manner of cloud chamber, where the characteristic traces of different language games can be detected. We draw attention to the very specific kind of language game understood as bullshit, and explain how the paratextual apparatus of the dialogue management system that presents LLM sequence prediction as conversational AI chatbot has amplified and prioritised this particular language game. By asking ChatGPT to generate scientific articles on topics where it clearly has no knowledge or competence, we are able to provide reference set of how this bullshit is manifested. We then trained language game detector by contrasting that reference set of bullshit to large collection of factual, precise, clear and concise scientific writing. 23 We propose that the resulting WLGD can be applied as remarkably reliable BS-meter. Although our reference set was constructed according to the rubric of Frankfurt, by requesting speech on topic where the speaker has no knowledge, we can only say that the detector reliably detects some resulting language game, but not necessarily what kind of language game this is. However, our experimental investigations show firstly, that the language game detected by the BS-meter is reliably present in the political misuse of English castigated by Orwell, and secondly that this language game is more likely to be seen in professional writing by those people who Graeber describes as having bullshit jobs. That further coincidence, with its clear social relevance and significance in relation to future applications of LLM-based chatbots, offers compelling evidence that we really are measuring bullshit. Works Cited Agre, Philip. E. 1997. Towards Critical Technical Practice: Lessons Learned in Trying to Reform AI. In Social Science, Technical Systems and Cooperative Work: Beyond the Great Divide, edited by Geoffrey Bowker, Susan Leigh Star, Les Gasser and William Turner, 131157. Lawrence Erlbaum. Ali, Syed Mustafa, Stephanie Dick, Sarah Dillon, Matthew L. Jones, Jonnie Penn, and Richard Staley. 2023. Histories of Artificial Intelligence: Genealogy of Power. BJHS Themes 8 (January): 1-18. https://doi.org/10.1017/bjt.2023.15. Alkaissi, Hussam, Samy I. McFarlane, Hussam Alkaissi, and Samy I. McFarlane. 2023. Artificial Hallucinations in ChatGPT: Implications in Scientific Writing. Cureus 15 (February). https://doi.org/10.7759/cureus.35179. Azhar, Azeem. 2020. Superintelligence Already Rules the World. Interview with David Runciman. Harvard Business Review podcast. Accessed 22 November 2024. https://hbr.org/podcast/2020/01/superintelligence-already-rules-the-world Ball, Philip. n.d. Is AI Frankensteins Monster or an Unintelligent Parrot? TLS (blog). Accessed 9 October 2024. https://www.the-tls.co.uk/science-technology/technology/aihumanity-machine-learning-book-review-philip-ball. 24 Bareis, Jascha, and Christian Katzenbach. 2022. Talking AI into Being: The Narratives and Imaginaries of National AI Strategies and Their Performative Politics. n.d. Accessed 9 October 2024. https://doi.org/10.1177/01622439211030007. Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? . In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 61023. FAccT 21. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3442188.3445922. Bernoff, Josh. 2022. ChatGPT Is Bullshitter. Josh Bernoff. 7 December 2022. https://bernoff.com/blog/chatgpt-is-a-bullshitter. Blackwell, Alan F., Mark Blythe and Joseph Jofish Kaye. 2017. Undisciplined Disciples: Everything you always wanted to know about ethnomethodology but were afraid to ask Yoda. Personal and Ubiquitous Computing 21(3): 571-592. https://doi.org/10.1007/s00779017-0999-z. Blackwell, Alan F. 2023. Oops! We Automated Bullshit. Text. 9 November 2023. https://www.cst.cam.ac.uk/blog/afb21/oops-we-automated-bullshit. Blackwell, Alan F. 2024. Moral Codes: Designing alternatives to AI. MIT Press. Cave, Stephen, Kanta Dihal, and Sarah Dillon. 2020. AI Narratives: History of Imaginative Thinking about Intelligent Machines. Oxford University Press. https://doi.org/10.1093/oso/9780198846666.001.0001. Chuan, Ching-Hua, Wan-Hsiu Sunny Tsai, and Su Yeon Cho. 2019. Framing Artificial Intelligence in American Newspapers. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, 33944. AIES 19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3306618.3314285. Couldry, Nick, and Ulises A. Mejias. 2020. The Costs of Connection: How Data are Colonizing Human Life and Appropriating it for Capitalism. Oxford University Press. 25 Crilly, Nathan. 2011. The Design Stance in User-system Interaction. Design Issues 27, no. 4 (2011): 16-29. https://doi.org/10.1162/DESI_a_00102. De Souza, Clarisse Sieckenius. 1993. The Semiotic Engineering of User Interface Languages. International Journal of Man-Machine Studies 39 (5): 753-773. https://doi.org/10.1006/imms.1993.1082. Deck, Oliver. 2023. Bullshit, Pragmatic Deception, and Natural Language Processing. Dialogue & Discourse 14 (1): 5687. https://doi.org/10.5210/dad.2023.103. Dennett, Daniel C. 1989. The Intentional Stance. MIT press. Dillon, Sarah. 2020. The Eliza Effect and Its Dangers: From Demystification to Gender Critique. Journal for Cultural Research 24 (1): 115. https://doi.org/10.1080/14797585.2020.1754642. Dillon, Sarah, and Jennifer Schaffer-Goddard. 2023. What AI Researchers Read: The Role of Literature in Artificial Intelligence Research. Interdisciplinary Science Reviews 48 (1): 15 42. https://doi.org/10.1080/03080188.2022.2079214. Frankfurt, Harry G. 2005. On Bullshit. Princeton University Press. Fredal, James. Rhetoric and Bullshit. College English 73, no. 3 (2011): 24359. https://www.jstor.org/stable/25790474. Genette, Gérard. 1997. Paratexts: Thresholds of Interpretation. Trans. Jane E. Lewin. Cambridge University Press. Gershon, Ilana. 2023. Bullshit Genres: What to Watch for When Studying the New Actant ChatGPT and Its Siblings. Suomen Antropologi: Journal of the Finnish Anthropological Society 47 (3): 11531. https://doi.org/10.30676/jfas.137824. Graeber, David, 2018. Bullshit Jobs: Theory. Penguin. Grice, Paul. 1975. Logic and Conversation. In Syntax and Semantics 3: Speech Acts, edited by Peter Cole and Jerry L. Morgan, 41-58. New York and London: Academic Press. 26 Guenduez, Ali A., and Tobias Mettler. 2023. Strategically Constructed Narratives on Artificial Intelligence: What Stories Are Told in Governmental Artificial Intelligence Policies? Government Information Quarterly 40 (1): 101719. https://doi.org/10.1016/j.giq.2022.101719. Hannigan, Timothy R., Ian P. McCarthy, and André Spicer. 2024. Beware of Botshit: How to Manage the Epistemic Risks of Generative Chatbots. Business Horizons, SPECIAL ISSUE: WRITTEN BY CHATGPT, 67 (5): 47186. https://doi.org/10.1016/j.bushor.2024.03.001. Haraway, Donna J., 2016. Staying with the Trouble: Making Kin in the Chthulucene. Duke University Press. Hicks, Michael Townsen, James Humphries, and Joe Slater. 2024. ChatGPT Is Bullshit. Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5. Hofstadter, Douglas. 1995. Fluid Concepts and Creative Analogies. BasicBooks. Hunter, Lynette. 1991. Rhetoric and Artificial intelligence, Rhetorica 9 (4): 31740. https://doi.org/10.1525/rh.1991.9.4.317. Katwala, Amit. n.d. ChatGPTs Fluent BS Is Compelling Because Everything Is Fluent BS. Wired. Accessed 9 October 2024. https://www.wired.com/story/chatgpt-fluent-bs/. Kobak, Dmitry, Rita González-Márquez, Emőke-Ágnes Horvát, and Jan Lause. 2024. Delving into ChatGPT Usage in Academic Writing through Excess Vocabulary. arXiv. https://doi.org/10.48550/arXiv.2406.07016. Latour, Bruno, 2017. On Actor-Network Theory. few clarifications plus more than few complications. Philosophical Literary Journal Logos, 27(1), pp.173-197. Lejeune, Philipe. 1975. Le Pacte autobiographique. Seuil. Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: Robustly Optimized BERT Pretraining Approach. arXiv. https://doi.org/10.48550/arXiv.1907.11692. 27 Muldoon, James, Mark Graham, and Callum Cant. 2024. Feeding the Machine: The Hidden Human Labour Powering AI. Canongate. Narayanan, Arvind. 2023. ChatGPT Is Bullshit Generator. But It Can Still Be Amazingly Useful. 20 March 2023. https://www.aisnakeoil.com/p/chatgpt-is-a-bullshit-generator-but. Noort, Carolijn van. 2024. On the Use of Pride, Hope and Fear in Chinas International Artificial Intelligence Narratives on CGTN. AI & SOCIETY 39 (1): 295307. https://doi.org/10.1007/s00146-022-01393-3. Orwell, George. 1969 [1944]. Propaganda and Demotic Speech. In Vol. 3 of The Collected Essays, Journalism and Letters of George Orwell, edited by Sonia Orwell and Ian Angus, 135-141. 4 vols. London: Secker & Warburg. Orwell, George. 1969 [1946]. Politics and the English Language. In Vol. 4 of The Collected Essays, Journalism and Letters of George Orwell, edited by Sonia Orwell and Ian Angus, 127-140. 4 vols. London: Secker & Warburg. Perez, Ethan, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit et al. 2023. Discovering Language Model Behaviors with Model-Written Evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 13387-13434. Robertson, Alexa, and Max Maccarone. 2023. AI Narratives and Unequal Conditions. Analyzing the Discourse of Liminal Expert Voices in Discursive Communicative Spaces. Telecommunications Policy 47 (5): 102462. https://doi.org/10.1016/j.telpol.2022.102462. Schaake, Marietje. The Tech Coup: How to Save Democracy from Silicon Valley. Princeton University Press, 2024. Shanahan, Murray. 2010. Embodiment and the Inner Life: Cognition and Consciousness in the Space of Possible Minds. Oxford University Press. Shanahan, Murray. 2024. Talking about Large Language Models. Communications of the ACM 67 (2): 6879. https://doi.org/10.1145/3624724. 28 Shanahan, Murray, Kyle McDonell, and Laria Reynolds. 2024. Role play with large language models. Nature 623 (7987): 493-498. https://doi.org/10.1038/s41586-023-06647-8 Sharma, Mrinank, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng et al. 2023. Towards Understanding Sycophancy in Language Models. arXiv preprint arXiv:2310.13548. Spärck Jones, Karen. 1972. Statistical Interpretation of Term Specificity and its Application in Retrieval. Journal of Documentation, 28 (1): 11-21. https://doi.org/10.1108/eb026526 Spärck Jones, Karen. 2004. Language Modellings Generative Model: Is it rational? Technical Report. Computer Laboratory, University of Cambridge. https://www.cl.cam.ac.uk/archive/ksj21/langmodnote4.pdf Stalnaker, Robert C. 1987. Inquiry. MIT Press. Stalnaker, Robert C. 1999a. Assertion. In Context and Content: Essays on Intentionality in Speech and Thought, edited by Robert C. Stalnaker. Oxford University Press. https://doi.org/10.1093/0198237073.003.0005. Stalnaker, Robert C. 1999b. On the Representation of Context. In Context and Content: Essays on Intentionality in Speech and Thought, edited by Robert C. Stalnaker, 0. Oxford University Press. https://doi.org/10.1093/0198237073.003.0006. Stalnaker, Robert. 2002. Common Ground. Linguistics and Philosophy 25 (5): 70121. https://doi.org/10.1023/A:1020867916902. Stokke, Andreas, and Don Fallis. 2017. Bullshitting, Lying, and Indifference toward Truth. Ergo, an Open Access Journal of Philosophy 4 (20201214). https://doi.org/10.3998/ergo.12405314.0004.010. Stone, Matthew, Lauren M.E. Goolad, and Mark Sammons. 2024. The Origins of Generative AI in Transcription and Machine Translation, and Why That Matters. Critical AI 2(1). https://doi.org/10.1215/2834703X-11205147 29 Sundar, S. Shyam, and Mengqi Liao. 2023. Calling BS on ChatGPT: Reflections on AI as Communication Source. Journalism & Communication Monographs 25 (2): 16580. https://doi.org/10.1177/15226379231167135. Taylor, Astra, The Automation Charade. n.d. Logic(s) Magazine. Accessed 9 October 2024. https://logicmag.io/failure/the-automation-charade/. The Royal Society, AI Narratives: Portrayals and Perceptions of Artificial Intelligence and Why They Matter Royal Society. n.d. Accessed 9 October 2024. https://royalsociety.org/news-resources/projects/ai-narratives/. Vallor, Shannon. 2024. The AI Mirror: How to Reclaim Our Humanity in an Age of Machine Thinking. Oxford University Press. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan M. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in Neural Information Processing Systems. arXiv preprint arXiv:1706.03762 10: S0140525X16001837. Vincent, James. 2022. AI-Generated Answers Temporarily Banned on Coding Q&A Site Stack Overflow. The Verge. 5 December 2022. https://www.theverge.com/2022/12/5/23493932/chatgpt-ai-generated-answers-temporarilybanned-stack-overflow-llms-dangers. Weise, Karen, and Cade Metz. 2023. When A.I. Chatbots Hallucinate. The New York Times, 1 May 2023, sec. Business. https://www.nytimes.com/2023/05/01/business/aichatbots-hallucination.html. Weiser, Benjamin. 2023. Heres What Happens When Your Lawyer Uses ChatGPT. The New York Times, 27 May 2023, sec. New York. https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html. Weizenbaum, Joseph. 1966. ELIZAa Computer Program for the Study of Natural Language Communication between Man and Machine. Commun. ACM 9 (1): 3645. https://doi.org/10.1145/365153.365168. 30 Weizenbaum, Joseph. 1987. Computer Power and Human Reason: From Judgement to Calculation. Penguin. Wittgenstein, Ludwig. 2009 [1953]. Philosophical Investigations. Trans. G. E. M. Anscombe, P. M. S. Hacker and Joachim Schulte. Wiley-Blackwell. Zorpette, Glenn. 2023. Just Calm Down About GPT-4 Already. And stop confusing performance with competence, says Rodney Brooks. IEEE Spectrum (17 May 2023). https://spectrum.ieee.org/gpt-4-calm-down Acknowledgements Harry Giddens contribution to this research was supported via the Google DeepMind Research Ready funding stream. Data used in Section 4 has been extracted from the British National Corpus Online service, managed by Oxford University Computing Services on behalf of the BNC Consortium. Notes 1 Shanahan (2024: 72, n.) reminds us that strictly speaking, the LLM itself comprises just the model architecture and the trained parameters LLMs in the wild must be embedded in larger architectures to be useful. To build question-answering system, the LLM simply has to be supplemented with dialog management system that queries the model as appropriate (74). In section 3.1, we characterise Shanahans supplementary dialog management system as the equivalent of literary paratext. 2 Alethic means of or relating to truth (OED). Note that the nature of the relationship between, and significance and impact of, the outputs of LLM-based chatbots and non-assertoric discourse, such as fiction, is not the subject of this paper. 3 Note that the BERT transformer model that we used in our WLGD experiments is an earlier research predecessor of the commercial GPT models created by OpenAI and others. 4 Think, for instance, about how different types of DMS engendered the different GPTs now made available by OpenAI alongside their general purpose, flagship model: these are bots with different personalities (whose raw statistical predictions come dressed in different paratexts), designed to assist with individual specialised tasks, like cooking or researching academic literature. 31 5 See Sundar and Liaos (2023) discussion of human psychology, human-computer interaction and the Computers are Social Actors research programme, and references therein, in particular the collaborative work of Clifford Nass. 6 As Ali et al (2023: 1) note, the history of imaginative thinking around AI, in fact and fiction, influences how AI is produced, perceived and regulated, and the rhetorical framing of AI, past and present, by scientists, technologists, governments, corporations, activists and the media, performatively creates and shapes the very phenomenon purportedly under analysis. See also, Hunter (1991), Cave et al (2020), The Royal Society (2018), Dillon (2020), Dillon and Schaffer Goddard 2023), Bareis and Katzenbacj (2022), Guenduez and Mettler (2023), van Noort (2022), Chuan et al (2019), Robertson and Maccarone (2023), Taylor (2018). 7 For just one recent example, see OpenAIs (2024) use of reason in its release announcement for GPT-4o. 8 For examples of hallucinations see, for example, Weiser (2023) and Alkaissi and McFarlane (2023). For an example use of the term in technical contexts see OpenAI (2023), and in the media see Weise and Metz (2023). 9 Consider for example the history and rhetoric of the idea of machines learning, as discussed in Dillon (2020: 4). 10 Blackwell, A.F. (2019). Artificial intelligence and the abstraction of cognitive labour. In M. Davis (Ed.), Marx200: The significance of Marxism in the 21st century. London: Praxis Press, pp. 59-68. 11 As examples of bad political writing, Orwell cites pamphlets, leading articles, manifestos, White Papers and the speeches of Under-Secretaries ([1946] 1969, 4: 135). 12 See https://manifesto-project.wzb.eu. We used the Manifesto Corpus version 2024-1: Lehmann, Pola, Simon Franzmann, Denise Al-Gaddooa, Tobias Burst, Christoph Ivanusch, Jirka Lewandowski, Sven Regel, Felicia Riethmüller, Lisa Zehnter. 2024. Manifesto Corpus. Version: 2024-1. Berlin: WZB Berlin Social Science Center / Göttingen: Institute for Democracy Research (IfDem). For some of the manifestos dating to 1964-2005, we used documents created by the Comparative Electronic Manifestos Project (CEMP): Pennings, Paul, Hans Keman, Vrije Universiteit Amsterdam. 2006. Comparative Electronic Manifestos Project. In cooperation with the Social Science Research Centre Berlin (Andrea Volkens, HansDieter Klingemann), the Zentralarchiv für empirische Sozialforschung (GESIS), and the Manifesto Research Group. Thanks to Mark Gotham for suggesting this source. 13 See http://www.natcorp.ox.ac.uk/ 14 See https://en.wikipedia.org/wiki/Bullshit_Jobs, accessed 22 November 2024."
        }
    ],
    "affiliations": [
        "University of Cambridge"
    ]
}