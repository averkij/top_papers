{
    "paper_title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "authors": [
        "Yutao Mou",
        "Zhangchi Xue",
        "Lijun Li",
        "Peiyang Liu",
        "Shikun Zhang",
        "Wei Ye",
        "Jing Shao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 6 5 1 0 1 . 1 0 6 2 : r ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback Yutao Mou1,2, Zhangchi Xue1, Lijun Li2, Peiyang Liu1, Shikun Zhang1, Wei Ye1, Jing Shao2 1National Engineering Research Center for Software Engineering, Peking University 2Shanghai Artificial Intelligence Laboratory yutao.mou@stu.pku.edu.cn, wye@pku.edu.cn GitHub: https://github.com/MurrayTom/ToolSafe"
        },
        {
            "title": "Abstract",
            "content": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains underexplored. In this work, we first construct TS-Bench, novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop guardrail model, TS-Guard, using multitask reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and actionattack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, We introduce TS-Flow, guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65% on average and improves benign task completion by approximately 10% under prompt injection attacks."
        },
        {
            "title": "Introduction",
            "content": "With the rapid advancement of large language models (LLMs), autonomous agents can perform complex tasks in open-ended environments by invoking external tools and interacting with real-world systems (Zhou et al., 2023; Yao et al., 2024; Zhang et al., 2025a; Patil et al., 2025). However, this expanded capability amplifies security risks (Figure 1): even well-aligned LLMs may fail to generalize safety guarantees to agentic harmful requests (Zhang et al., 2025b; Chen et al., 2024), and attackers can exploit prompt injection (Zhan et al., 2024; Evtimov et al., 2025) or backdoor attacks (Yang et al., 2024; Wang et al., 2024) to induce unsafe actions. Unlike chatbots, autonomous agents act directly on external environments, making tool invocation safety critical for reliable deployment. Figure 1: Illustration of two categories of tool invocation security risks considered in this study. (a) Malicious user requests that directly induce unsafe tool invocation. (b) Prompt injection attacks occurring during benign task execution, leading to unintended tool use. promising approach to ensuring the safe operation of autonomous agents is to deploy guardrail systems (Chennabasappa et al., 2025; An et al., 2025) that enforce safety without modifying the foundation model. Guardrail models are core components of guardrail systems, typically implemented as dedicated LLMs that analyze agent outputs and provide safety judgments (Inan et al., 2023; Zhang et al., 2024; Zhao et al., 2025). Most existing guardrail models are designed for static input and output content moderation, and lack the 1 ability to reason about dynamic tool invocation actions. Recent work extends safety guardrails from LLMs to agents, protecting memory (Wei et al., 2025), planning (Huang et al., 2025b), and action execution (Sun et al., 2025). These guardrail models for agents typically rely on complete action plans or execution trajectories (Huang et al., 2025b; Luo et al., 2025a). However, autonomous agents require dynamic safety monitoring over each tool invocation steps to enable timely intervention against emerging risks (Xiang et al., 2024; Wu et al., 2025). This calls for guardrail models that are capable of fine-grained and low-latency reasoning over individual tool invocation steps before execution. In this work, we focus on the security risks introduced by tool invocation capabilities of LLMbased agents and investigate how step-level safety guardrails can mitigate them. In particular, we focus on the following questions: (Q1) What steplevel signals in LLM-based agents indicate potentially unsafe tool invocation before execution? (Q2) How can we train generalizable guardrail model to detect step-level unsafe tool invocation in LLMbased agents before execution? (Q3) How can step-level guardrails be integrated into LLM-based agents to improve safety without compromising benign task performance? To address these issues, we first conduct systematic analysis of unsafe tool invocation in LLMbased agents and identify four common risk patterns. Based on these, we construct TS-Bench, step-level tool invocation safety detection benchmark for LLM agents (Section 3). Furthermore, we develop TS-Guard, guardrail model for steplevel tool invocation safety detection. TS-Guard is trained via reinforcement learning (Shao et al., 2024) with multi-task reward scheme tailored for agent security, enabling identifying harmful user requests and attack vectors in agent-environment interaction logs, detecting unsafe tool invocation before execution, and providing interpretable analysis and reasoning process (Section 4.1). Finally, we introduce TS-Flow, guardrail-feedback-driven reasoning framework for ReAct-style LLM-based agents (Yao et al., 2022), which proactively monitors tool invocations at each step and delivers preexecution feedback. Instead of terminating tasks when unsafe behaviors are detected like LlamaFirewall (Chennabasappa et al., 2025), TS-Flow guides agents toward safety-aware tool use reasoning (Section 4.2). Extensive experiments demonstrate that TS-Flow reduces harmful tool invocations by up to 65% on average, while preserving or even improving benign task performance by approximately 10% (Section 5). In summary, our contributions are three-fold: Benchmark: We introduce TS-Bench, to the best of our knowledge, the first benchmark for step-level tool invocation safety detection. Method: We introduce proactive step-level guardrail and feedback framework to enhance tool invocation safety, featuring TS-Guard for interpretable safety feedback and TS-Flow for feedback-driven reasoning. Empirical Insights: Extensive experiments reveal two key findings: (1) multi-task interpretable signals from TS-Guard corrects benign task deviations caused by prompt injections and more effectively reduces harmful behaviors; (2) guardrail feedback increases agent output entropy, encouraging exploration for safe and helpful trajectory."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Guardrail for LLMs Safety guardrails have been widely adopted during LLM deployment to defend against input jailbreaks and prompt injection attacks (Li and Liu, 2024; Li et al., 2025), as well as to moderate potentially harmful model outputs (Inan et al., 2023; Li et al., 2024; Zhao et al., 2025). LlamaGuard pioneers the paradigm of LLM-based safety guardrails by finetuning general-purpose LLMs to classify prompts and responses under customized safety taxonomies (Li and Liu, 2024). Qwen3Guard further introduces three-level harmfulness classification scheme, supporting 119 languages and dialects (Zhao et al., 2025). Other representative LLM safety guardrail models include ShieldGemma (Zeng et al., 2024), PolyGuard (Kumar et al., 2025), and WildGuard (Han et al., 2024). Despite their effectiveness in content moderation, these guardrails are limited to static content moderation and struggle to defend the dynamic tool invocation safety risks of agents. 2.2 Agent Guardrail Recent work has increasingly focused on guardrails for agents, extending beyond malicious inputs and harmful outputs to also cover risks arising in memory (Wei et al., 2025), planning (Huang et al., 2025b), and tool invocation (Luo et al., 2025a). LlamaFirewall (Chennabasappa et al., 2025) is popular guardrail system, that combines PromptGuard2 2 Benchmark Annotation Level Risky behavior Unsafe Patterns R-Judge (Yuan et al., 2024) ASSEBench (Luo et al., 2025a) OS-Safe (Luo et al., 2025b) ShieldAgent-Bench (Chen et al., 2025) TS-Bench (Ours) Trajectory-level Trajectory-level Step-level Step-level Step-level tool calls tool calls web browsing/code execution web browsing tool calls MUR PI HT BTRA Table 1: Comparison of TS-Bench with existing agent safety detection benchmarks. Unsafe patterns considered in this study: MUR (Malicious User Request), PI (Prompt Injection), HT (Harmful Tools), and BTRA (Benign Tools with Risky Arguments) TS-Bench-eval # Sample # Safe # Controv. # Unsafe AgentHarm-Traj ASB-Traj AgentDojo-Traj 731 5237 1220 206 2700 868 315 1466 N/A 210 1071 TS-Bench-train # Sample # Safe # Controv. # Unsafe AgentAlign-Traj ASB-Traj 673 1520 123 720 237 469 313 Table 2: Statistics of TS-Bench splits. Controv. stands for controversial or potentially unsafe tool use steps. (Yuan et al., 2025) with an AlignmentCheck module but offers limited risk coverage and generalization. Safiron (Huang et al., 2025b) is guardrail model for the planning stage that identifies risks before execution. AgentAuditor (Luo et al., 2025a) retrieves reasoning experiences to guide LLM evaluation of complete execution trajectories, while Zhang et al. (2024) fine-tune model for the same purpose. They both operate at trajectory-level, not step-level. Recent work also introduces \"guardrail agents\" as paradigm for monitoring action safety in agents. GuardAgent (Xiang et al., 2024) relies on manually specified rules, which limits its coverage and generalization to predefined scenarios. ShieldAgent (Chen et al., 2025) and AGrail (Luo et al., 2025b) produce safety judgments via complex reasoning and verification pipelines, incurring high latency that makes them impractical for step-level monitoring of tool invocation in LLM-based agents. Motivated by these limitations, we aim to enable efficient and generalizable step-level safety monitoring for tool invocation in LLM-based agents."
        },
        {
            "title": "Safety Detection Benchmark",
            "content": "3.1 Task Formulation We consider an LLM-based agent that interacts with an environment through an iterative reasoningaction loop (Yao et al., 2022; Zeng et al., 2025). Given an initial user request ui, the agent incrementally interacts with the environment in multistep manner: at each step t, it produces an action ai (including intermediate reasoning and planned tool invocation) and receives an observation oi returned by the environment after execution, until the task is completed. We formalize the interaction history preceding step as: Hi t1)}. Formally, step-level tool invocation safety detection aims to determine whether candidate action ai is unsafe to execute, based on the available interaction t, ui), prior to execution: context (Hi (cid:0)ai 1), . . . , (ai = {(ai t, ui, i(cid:1) , yi = fguardrail t1, oi 0), (ai t, Hi 1, oi 0, oi (1) where yi denotes the safety label indicating whether executing the candidate action ai would incur potential safety risks. If available, the guardrail model additionally takes the tool specification (e.g., tool descriptions and usage constraints) as input. 3.2 Overview To identify step-level signals indicating potentially unsafe tool invocation before execution, we analyze interaction logs from four representative agent safety datasets (Zhang et al., 2025b; Debenedetti et al., 2024; Andriushchenko et al., 2024; Zhang et al., 2024). We find that unsafe tool invocation can be characterized along two dimensions: the triggering cause (malicious user requests vs. thirdparty prompt injection) and the manifestation (invoking harmful tools vs. benign tools with risky arguments). The resulting four unsafe patterns serve as key features that can be identified from the interaction history and leveraged to assess pre-execution tool invocation safety risks. Grounded in these observed step-level risk patterns, we construct TS-Bench, benchmark for step-level tool invocation safety detection in LLMbased agents. Each sample is represented as tuple Figure 2: Illustration of our proactive step-level guardrail and feedback framework for LLM agents. (a) Input and output format of TS-Guard. (b) TS-Flow feeds guardrail feedback to the agent, enabling safe tool invocation reasoning rather than aborting execution. = (T , ut, Ht, at, yt), where denotes the available tool set, Ht is the interaction history up to step t, at is candidate tool invocation action, and yt {safe, controversial, unsafe} is the ground-truth safety label indicating whether executing at would lead to potential risk. Detailed examples can be found in the appendix C. Prior benchmarks for agent safety detection either provide only post-hoc trajectory-level annotations (e.g., ASSEBench (Luo et al., 2025a)) or focus on domain-specific risky behaviors such as web browsing or code execution (e.g., OS-Safe (Luo et al., 2025b), ShieldAgent-Bench (Chen et al., 2025)). TS-Bench focuses on security risks from unsafe tool calls and provides fine-grained, steplevel safety labels, enabling pre-execution evaluation of general tool invocation safety. Table 1 compares TS-Bench with commonly used agent safety detection benchmarks. 3.3 Benchmark Construction Data Sources. TS-Bench draws on interaction logs from four representative datasets for agent safety evaluation and alignment: AgentAlign (Zhang et al., 2025b), AgentHarm (Andriushchenko et al., 2024), ASB (Zhang et al., 2024), and AgentDojo (Debenedetti et al., 2024), which together cover benign and malicious user requests, prompt injection attacks, and harmful or benign tool sets. We choose these datasets as their interaction logs collectively capture the four unsafe tool invocation patterns studied in this work. Details can be found in Appendix A. Step-level Annotation For each source dataset, we sample complete execution trajectories using GPT-4o/Claude3.5/Qwen3-30B-A3B, and annotate step-level tool invocation with safety labels: safe, controversial/potentially unsafe, and significantly unsafe. In addition, we annotate whether each steplevel tool invocation is associated with prompt injection attacks and whether the user request is harmful. Detailed guidelines for annotation are provided in Appendix B. Train-Test Split. For the training split (TSBench-train), we use trajectories from AgentAlign and three selected domains from ASB, covering four unsafe tool invocation patterns. The evaluation split (TS-Bench-eval) comprises the remaining seven ASB domains, together with AgentHarm and AgentDojo, ensuring minimal overlap with the training set. Appendix shows that TS-Benchtrain and TS-Bench-eval are largely disjoint, indicating the absence of data leakage between the two splits. The dataset statistics are reported in Table 2."
        },
        {
            "title": "4 Method",
            "content": "To enhance the tool invocation safety of LLM agents, we introduce proactive step-level guardrail and feedback framework, comprising step-level guardrail model (TS-Guard) for interpretable safety feedback and feedback-driven reasoning method (TS-Flow), as shown in Figure 2. 4.1 TS-Guard Input-output Formulation. Directly detecting unsafe tool invocations from interaction logs is challenging due to their complexity (Ye et al., 2024) and limited supervision (Huang et al., 2025b). As analyzed in Section 3.2, two features are particularly indicative of unsafe behaviors: user request harmfulness and the link between candidate actions and third-party attacks. TS-Guard decomposes step-level safety detection into three sequential tasks within single inference: (1) predicting request harmfulness (hi t), (2) detecting links to 4 third-party attacks (vi of the current tool invocation (yi t), and (3) assessing the safety t), modeled as t, ri t, vi t, hi (yi t, t, ai , ui) = fguard(ai , ui), (2) where yi is predicted using three-class classification scheme following Qwen3Guard (Zhao et al., 2025). The model is also encouraged to generate brief analysis and reasoning ri about the interaction logs prior to producing the final judgments. GRPO Optimization and Multi-Task Reward. Training samples include labels for all three tasks. Predictions ˆht, ˆvt, ˆyt are compared with ground truth to compute multi-task reward: , , rt = 1 w1 1[ˆht = ] w2 1[ˆvt = ] w3 1[ˆyt = ] (3) In this study, the weights are uniformly set to w1 = w2 = w3 = 1/3. Following prior work showing that reinforcement learning(RL) can improve LLM generalization more effectively than supervised fine-tunin(SFT) (Chu et al., 2025; Shenfeld et al., 2025; Swamy et al., 2025), we optimize TS-Guard using Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025). The advantage is defined as the group-normalized output-level reward, uniformly applied to all tokens in the output. We also compare RL-trained models with SFT and SFT+RL baselines in Section 6.1. Summary. TS-Guard is designed to output multiple types of information: brief analysis and reasoning, request harmfulness, the association between candidate actions and potential attacks, and the final safety rating. On the one hand, it provides more fine-grained supervision signals, facilitating the guardrail models causal analysis and thereby enhancing detection performance (Section 5.4.1); on the other hand, it has the potential to enable more step-level feedback information to the agent, supporting more informed and safer tool invocation decisions (Section 5.4.2). 4.2 TS-Flow Overview of Existing Approaches. LlamaFirewall (Chennabasappa et al., 2025) is representative agent safety guardrail framework with two components: Prompt Guard (Yuan et al., 2025; Li et al., 2025), which detects prompt injection and jailbreak attempts, and Alignment Check, which 5 monitors the agents reasoning to catch anomalies such as goal hijacking (Huang et al., 2025a), indirect prompt injection (An et al., 2025), and misalignment between user requests and reasoning (Kierans et al., 2025). Detected anomalies trigger an abort of the agent workflow to prevent harm. However, this \"detect-and-abort\" paradigm can interrupt benign tasks in realistic settings where normal instructions and injected signals are often mixed, leading to degraded benign task completion. Moreover, LlamaFirewall relies on multiple specialized modules to cover different risk types, increasing deployment complexity. TS-Flow. In contrast, we propose TS-Flow, guardrail-feedback-driven reasoning framework, which leverages TS-Guard to monitor tool invocation reasoning and provides pre-execution feedback for potentially unsafe actions, enabling agents to correct behaviors instead of being terminated. As shown in Figure 2(b), this \"agentguardrail interaction\" paradigm improves safety while largely preserving agent performance."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Setup We conduct two sets of experiments: (1) Guardrail Model Evaluation: Using the TS-Bench, we measure the effectiveness of various guardrail models in step-level tool invocation safety detection. (2) Guarded Agent Evaluation: Leveraging AgentDojo (Debenedetti et al., 2024), ASB (Zhang et al., 2024), and AgentHarm (Andriushchenko et al., 2024), we evaluate agents guarded by different guardrail framewok (Chennabasappa et al., 2025), instantiated with various guardrail models, measuring benign task completion and attack resilience. 5.2 Baselines Guardrail Models We evaluate closed-source LLMs (GPT-4o (Hurst et al., 2024)), open-source general-purpose LLMs (Qwen3-8B (Yang et al., 2025), Qwen2.5-7B-Instruct), open-source LLM guardrail models (LlamaGuard3-8B (Inan et al., 2023), Qwen3Guard-8B-Gen (Zhao et al., 2025)), and agent guardrail models (ShieldAgent-THU (Zhang et al., 2024), Safiron (Huang et al., 2025b)), and compare them with our TS-Guard. Guardrail Framewok We apply different defense methods to ReAct-style LLM agents, including sandwich defense (Schulhoff, 2024) and LlamaFirewall (Chennabasappa et al., 2025). For LlaModel GPT-4o AgentHarm-Traj F1 Recall ACC ASB-Traj F1 Recall ACC AgentDojo-Traj F1 Recall ACC 75.23 84. 96.19 65.84 63.03 60.11 55.74 56. 100.00 Qwen3-8B Qwen2.5-7B-IT Llama-Guard-3-8B Qwen3Guard-8B-Gen ShieldAgent-THU Safiron 54.46 67.99 81.53 80.57 71.13 46.10 58.94 80. 86.35 86.27 82.63 45.88 45.52 90.09 81.33 84.95 95.62 31.81 59.66 52.10 54.67 53.50 57.63 51.63 38.93 62. 24.82 13.62 54.44 39.82 26.54 84.07 15.45 7.57 52.27 33.03 79.65 43.97 74.75 70.57 60.90 70.98 72.32 50. 33.33 3.23 58.91 52.80 TS-Guard (Ours) 84.81 90.16 96.95 94. 94.76 93.85 91.72 86.18 92.31 99.14 21.87 1.70 97.16 56. 89.49 Table 3: (Strict Mode) Comparison of TS-Guard and baseline guardrail models on step-level tool invocation safety detection in the TS-Bench benchmark. The best results are highlighted in bold. maFirewall, PromptGuard2 is used for input filtering, while the alignment check module is instantiated with either GPT-4o-mini (via few-shot prompting) or TS-Guard. These defenses are compared against TS-Flow. In addition, we instantiate TS-Flow with different guardrail models, including TS-Guard, ShieldAgent-THU (Zhang et al., 2024), and Safiron (Huang et al., 2025b), to evaluate the effect of online guardrail. We use GPT-4o and Qwen2.5-14B-Instruct as the foundation models for agents. 5.3 Metrics Guardrail models are evaluated with Accuracy (ACC), F1 score, and Recall, where Recall quantifies the proportion of harmful tool invocation correctly identified by the model. For agent evaluation, different benchmarks adopt different metrics. Specifically, AgentDojo and ASB report ASR and Utility, representing the completion rates of injected malicious tasks and benign user tasks, respectively. AgentHarm uses Refusal Rate and Task Completion Score as metrics. We use the harmful instruction split of AgentHarm. Since such inputs should be rejected, higher refusal rates and lower scores indicate better agent safety. 5.4 Results 5.4.1 Guardrail Model Evaluation. TS-Bench adopts three-level annotation scheme, whereas some baseline guardrail models perform binary classification. Following prior work (Zhao et al., 2025; Luo et al., 2025a), we evaluate all models under two settings: strict mode and loose mode. In strict mode, tool invocation steps labeled as potentially harmful or controversial are regarded as unsafe; in loose mode, they are treated as safe. We report strict-mode results in Table 3, as it provides more conservative and safety-oriented evaluation. Results under loose mode are deferred to Appendix G. Overall, TS-Guard consistently outperforms all baselines across datasets. We summarize three key observations: (1) Most guardrail models perform well on unsafe tool invocations caused by malicious user requests (MUR), but their effectiveness drops significantly under prompt injection (PI). For example, GPT-4o achieves an F1 score and recall of 84.8 and 96.19 on AgentHarm-Traj, but drops to 63.03 and 60.11 on ASB-Traj. Similar performance degradation is observed for Qwen2.5-7BIT, Llama-Guard-3-8B, Qwen3Guard-8B-Gen, and ShieldAgent-THU, indicating limited robustness to prompt injection scenarios. (2) When prompt injection appears in the interaction history, many guardrail models misclassify benign tool invocations as unsafe. On AgentDojo-Traj, for benign tool with risky argument (BTRA) cases, several models achieve high recall but low F1, indicating that prompt injection alone suffices to trigger risk judgments, leading to over-defensiveness and degraded agent utility. (3) For most guardrail models, identifying explicitly harmful tools (HT) remains challenging, as evidenced by their poor performance on ASBTraj. This indicates their limited ability to infer harmful characteristics from tool descriptions. In contrast, TS-Guard performs strongly across all four unsafe patterns, achieving effective steplevel tool invocation safety detection while substantially reducing over-defensiveness. 6 Method AgentDojo ASB-DPI ASR () Utility () ASR () Utility () ASR () Utility () ASB-IPI AgentHarm Refusal () Score () ReAct ReAct-sandwich defense ReAct-llamafirewall (GPT-4o-mini) ReAct-llamafirewall (TS-Guard) ReAct-TS-Flow (TS-Guard) 3 ReAct-TS-Flow (ShieldAgent-THU) ReAct-TS-Flow (Safiron) ReAct ReAct-sandwich defense ReAct-llamafirewall (GPT-4o-mini) ReAct-llamafirewall (TS-Guard) ReAct-TS-Flow (TS-Guard) 3 ReAct-TS-Flow (ShieldAgent-THU) ReAct-TS-Flow (Safiron) 56.16 54.12 3.02 0.95 1.16 1.35 7.68 17.59 18.54 2.84 1.05 1.79 0.93 6. GPT-4o as Agent Backbone 26.87 28.95 24.47 20.79 42.78 24.86 22.39 82.25 66.00 33.28 5.50 6.76 18.75 62.25 12.50 28.05 10.75 2. 18.87 9.50 8.25 80.00 72.98 19.25 5.50 6.19 16.25 65.50 Qwen2.5-14B-Instruct as Agent Backbone 42.57 42.69 33.82 36.46 42.72 26.44 25. 95.25 91.25 24.22 5.75 7.25 44.50 62.25 18.75 25.25 14.12 4.00 30.00 9.25 10.50 86.50 79.00 22.75 6.50 7.00 38.25 69. 48.00 46.41 45.37 46.63 49.01 44.80 47.50 52.25 56.62 49.25 50.00 58.12 45.25 49.37 62.50 77.84 80.87 96.59 94.32 92.10 73. 42.04 50.00 70.45 97.16 95.45 94.88 67.04 23.53 13.74 12.64 4.28 6.03 7.12 18.91 34.14 33.40 22.99 6.60 6.83 6.31 19. Table 4: Guarded agent evaluation on three benchmarks. ASB contains two types of test instances: direct prompt injection (DPI) and indirect prompt injection (IPI). The best results are highlighted in bold, and the second-best are underlined. 3 marks the best trade-off between safety and utility. 5.4.2 Guarded Agent Evaluation Table 4 compares TS-Flow with representative guardrail frameworks, highlighting the contribution of guardrail-feedback-driven reasoning to agent safety and utility: (1) Guardrail framework with \"Detect-andabort\" paradigm improves safety at the cost of utility. We observe that using TS-Guard as the guardrail model in LlamaFirewall yields the most significant safety improvement. However, because this framework terminates execution upon detecting prompt injection attacks, the agents benign task completion rate drops noticeably. (2) Dynamic agentguardrail interaction simultaneously improves both safety and utility. In AgentDojo and ASB, TS-Flow employs dynamic agentguardrail interaction mechanism that not only avoid unsafe tool calls but also leverages feedback from guardrail models to guide the agent toward completing benign tasks. (3) TS-Guard achieves better safetyutility trade-off than existing guardrail model. Compared to other models, TS-Guard more effectively reduces attack success rates (ASR) while preserving, and sometimes improving, benign task completion. This is partly due to its superior step-level tool invocation safety detection and partly to the rich feedback information it provides, as further validated in Section 6.4. We also analyze TS-Flow from the perspective of agent output entropy in Section 6.3. potential limitation is that guardrail feedback may introduce some delay, but we consider it within an acceptable range (Appendix H). Figure 3: Ablation results on training methods and reward designs. (a) Comparision of SFT, SFT+RL and RL only (b) Comparision of multi-task rewards and singletask rewards. Figure 4: Entropy comparison of guardrails. (a) Specialized models show lower entropy than general LLMs. (b) TS-Guard lowers final-decision entropy while preserving reasoning-step entropy to facilitate exploration."
        },
        {
            "title": "6 Analyses",
            "content": "6.1 Ablation Study of TS-Guard Training We conduct ablation studies on training methods and reward designs for TS-Guard. Training methods. We compare three strategies: SFT, SFT+RL, and RL-only (Figure 3 (a)). RLonly shows superior generalization to diverse agent trajectories. An entropy analysis of Qwen2.5-7BIT before and after SFT reveals decrease from 7 Method ASB-OPI AgentHarm ASR () Utility () Refusal rate () Score () ReAct TS-Flow (full TS-Guard output) TS-Flow (only safety rating) 86.5 7.00 26.04 52.25 58.12 52. 42.04 95.45 79.54 34.14 6.83 11.11 Table 5: Comparison of agent performance under different feedback richness. Richer feedback (full TS-Guard output) leads to higher safety and utility. creases, reflecting overconfident execution of potentially harmful actions. In contrast, under TSFlow, TS-Guard injects contextual feedback upon detecting unsafe tool invocations, leading to increased entropy. This suggests that the guardrail mechanism dynamically calibrates the agents output distribution to maintain uncertainty in high-risk scenarios, thereby preventing unsafe behaviors by encouraging exploration in safety-aware reasoning. 6.4 Effect of Feedback Richness on Agent Safety and Utility TS-Guard provides not only step-level safety ratings for the current action but also rich feedback including interaction history analysis, user requests harmfulness, and the correlation between the candidate action and potential prompt injection attacks. We hypothesize that feedback information is key factor in enhancing both agent safety and utility. To validate this, we conduct an experiment comparing two feedback strategies: (1) providing agents with the full TS-Guard output, and (2) providing only the safety rating for the current action. We then measure the resulting agent safety and utility scores. In Table 5, the results show that agents receiving richer feedback consistently achieve better safety and utility, indicating that more comprehensive feedback helps guide agents towards safer and more helpful behaviors."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we investigate step-level tool invocation safety in LLM-based agents. We introduce TS-Bench, the first benchmark for evaluating steplevel tool invocation safety, and propose proactive guardrail and feedback framework comprising TSGuard and TS-Flow to enable real-time monitoring and pre-execution intervention. Extensive experiments demonstrate that our approach effectively improves agent safety while preserving utility, offering practical solution for deploying reliable LLM-based agents in open-ended environments. Figure 5: Token-wise entropy of ReAct-style agent (Qwen2.5-14B-IT). Without guardrails, entropy decreases as the agent grows overconfident; with TS-Flow, TS-Guard feedback raises entropy in risky steps, maintaining uncertainty and guiding safe exploration.. 0.74 to 0.61, indicating reduced output diversity, which may limit subsequent RL gains and help explain the inferior performance of SFT+RL. Reward design. We compare single-task and multi-task design. The former predicts only safety ratings, while the latter provide fine-grained supervision on request harmfulness and attack correlation. Multi-task rewards consistently improve F1 and reduce false positives, demonstrating that richer supervisory signals enhance generalization and alleviate bias (Figure 3 (b)). 6.2 Entropy Analysis of Guardrail Models We measure guardrail model uncertainty via average token-level entropy over TS-Bench (Cui et al., 2025; Wang et al., 2025). ShieldAgentTHU and TS-Guard, both trained on Qwen2.5-7B with shared vocabulary, allow direct comparison. Figure 4 highlights two points: (1) guardrail models exhibit much lower entropy than generalpurpose LLMs, reflecting more confident outputs; (2) ShieldAgent reduces entropy throughout entire outputs, whereas TS-Guard mainly lowers it at the final judgment token, keeping higher entropy in intermediate reasoning, which encourages exploration to promote more reliable safety judgment. 6.3 Effect of Guardrail Feedback on Token-wise Entropy of Agent We further analyze the token-wise entropy dynamics of ReAct-style agents based on Qwen2.514B-IT during reasoning and tool invocation (Figure 5). Without guardrails, entropy steadily de-"
        },
        {
            "title": "Limitations",
            "content": "TS-Guard and TS-Flow proposed in this work effectively enhance the agent safety for tool invocation. However, this work still has several limitations: (1) In TS-Flow, the guardrail models feedback is directly appended to the agents input context as an external signal. While simple and model-agnostic, current LLM-based agents may occasionally fail to fully incorporate such feedback, limiting the effectiveness of step-level safety intervention. (2) Moreover, the agent and the guardrail model are trained independently, without explicit coordination, which may lead to misalignment between the agents reasoning process and the guards safety judgments. Future work will explore joint training or tighter coupling between agents and guardrail models to better integrate safety feedback and jointly improve safety and utility."
        },
        {
            "title": "Ethics Statement",
            "content": "Since the dataset used in this study contains harmful content, access is restricted to authorized researchers who adhere to strict ethical guidelines in order to mitigate risks associated with sensitive material. These measures protect the integrity of the research while minimizing potential harm."
        },
        {
            "title": "References",
            "content": "Hengyu An, Jinghuai Zhang, Tianyu Du, Chunyi Zhou, Qingming Li, Tao Lin, and Shouling Ji. 2025. Ipiguard: novel tool dependency graph-based defense against indirect prompt injection in llm agents. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 10231039. Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, and 1 others. 2024. Agentharm: benchmark for measuring harmfulness of llm agents. arXiv preprint arXiv:2410.09024. Zhaorun Chen, Mintong Kang, and Bo Li. 2025. Shieldagent: Shielding agents via verifiable safety policy reasoning. arXiv preprint arXiv:2503.22738. Zhi-Yuan Chen, Shiqi Shen, Guangyao Shen, Gong Zhi, Xu Chen, and Yankai Lin. 2024. Towards tool use In Proceedalignment of large language models. ings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 13821400, Miami, Florida, USA. Association for Computational Linguistics. Sahana Chennabasappa, Cyrus Nikolaidis, Daniel Song, David Molnar, Stephanie Ding, Shengye Wan, Spencer Whitman, Lauren Deason, Nicholas Doucette, Abraham Montilla, and 1 others. 2025. Llamafirewall: An open source guardrail system arXiv preprint for building secure ai agents. arXiv:2505.03574. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of arXiv preprint foundation model post-training. arXiv:2501.17161. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, and 1 others. 2025. The entropy mechanism of reinforcement learning arXiv preprint for reasoning language models. arXiv:2505.22617. Edoardo Debenedetti, Jie Zhang, Mislav Balunovic, Luca Beurer-Kellner, Marc Fischer, and Florian Tramèr. 2024. Agentdojo: dynamic environment to evaluate attacks and defenses for llm agents. arXiv e-prints, pages arXiv2406. Ivan Evtimov, Arman Zharmagambetov, Aaron Grattafiori, Chuan Guo, and Kamalika Chaudhuri. 2025. Wasp: Benchmarking web agent security against prompt injection attacks. arXiv preprint arXiv:2504.18575. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, and Nouha Dziri. 2024. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. Advances in Neural Information Processing Systems, 37:80938131. Yihao Huang, Chong Wang, Xiaojun Jia, Qing Guo, Felix Juefei-Xu, Jian Zhang, Yang Liu, and Geguang Pu. 2025a. Efficient universal goal hijacking with semantics-guided prompt organization. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 57965816, Vienna, Austria. Association for Computational Linguistics. Yue Huang, Hang Hua, Yujun Zhou, Pengcheng Jing, Manish Nagireddy, Inkit Padhi, Greta Dolcetti, Zhangchen Xu, Subhajit Chaudhury, Ambrish Rawat, and 1 others. 2025b. Building foundational guardrail for general agentic systems via synthetic data. arXiv preprint arXiv:2510.09781. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, 9 Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and 1 others. 2023. Llama guard: Llm-based inputoutput safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674. Aidan Kierans, Avijit Ghosh, Hananel Hazan, and Shiri Dori-Hacohen. 2025. Quantifying misalignment between agents: Towards sociotechnical understanding of alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2736527373. Priyanshu Kumar, Devansh Jain, Akhila Yerukola, Liwei Jiang, Himanshu Beniwal, Thomas Hartvigsen, and Maarten Sap. 2025. Polyguard: multilingual safety moderation tool for 17 languages. arXiv preprint arXiv:2504.04377. Hao Li and Xiaogeng Liu. 2024. Injecguard: Benchmarking and mitigating over-defense in prompt injection guardrail models. arXiv preprint arXiv:2410.22770. Hao Li, Xiaogeng Liu, Ning Zhang, and Chaowei Xiao. 2025. Piguard: Prompt injection guardrail via mitigating overdefense for free. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3042030437. Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. 2024. Salad-bench: hierarchical and comprehensive safety benchmark for large language models. arXiv preprint arXiv:2402.05044. Hanjun Luo, Shenyu Dai, Chiming Ni, Xinfeng Li, Guibin Zhang, Kun Wang, Tongliang Liu, and Hanan Salam. 2025a. Agentauditor: Human-level safety and security evaluation for llm agents. arXiv preprint arXiv:2506.00641. Weidi Luo, Shenghong Dai, Xiaogeng Liu, Suman Banerjee, Huan Sun, Muhao Chen, and Chaowei Xiao. 2025b. AGrail: lifelong agent guardrail with effective and adaptive safety detection. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 81048139, Vienna, Austria. Association for Computational Linguistics. Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. 2025. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning. Sander Schulhoff. 2024. Sandwich defense. https: //learnprompting.org/docs/prompt_hacking/ defensive_measures/sandwich_defense. Learn Prompting: Prompt Hacking Defensive Measures. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. 2025. Rls razor: Why online reinforcement learning forgets less. arXiv preprint arXiv:2509.04259. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297. Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May Dongmei Wang. 2024. Ehragent: Code empowers large language models for fewshot complex tabular reasoning on electronic health records. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2231522339. Qiushi Sun, Mukai Li, Zhoumianze Liu, Zhihui Xie, Fangzhi Xu, Zhangyue Yin, Kanzhi Cheng, Zehao Li, Zichen Ding, Qi Liu, and 1 others. 2025. Ossentinel: Towards safety-enhanced mobile gui agents via hybrid validation in realistic workflows. arXiv preprint arXiv:2510.24411. Gokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, and Andrew Bagnell. 2025. All roads lead to likelihood: The value of reinforcement learning in fine-tuning. arXiv preprint arXiv:2503.01067. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, and 1 others. 2025. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. Yifei Wang, Dizhan Xue, Shengjie Zhang, and Shengsheng Qian. 2024. Badagent: Inserting and activating backdoor attacks in llm agents. arXiv preprint arXiv:2406.03007. Qianshan Wei, Tengchao Yang, Yaochen Wang, Xinfeng Li, Lijun Li, Zhenfei Yin, Yi Zhan, Thorsten Holz, Zhiqiang Lin, and XiaoFeng Wang. 2025. A-memguard: proactive defense framework arXiv preprint for llm-based agent memory. arXiv:2510.02373. Yaozu Wu, Jizhou Guo, Dongyuan Li, Henry Peng Zou, Wei-Chieh Huang, Yankai Chen, Zhen Wang, Weizhi Zhang, Yangning Li, Meng Zhang, and 1 others. 2025. Psg-agent: Personality-aware safety guardrail for llmbased agents. arXiv preprint arXiv:2509.23614. 10 Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl Yang, and 1 others. 2024. Guardagent: Safeguard llm agents by guard agent via knowledge-enabled reasoning. arXiv preprint arXiv:2406.09187. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, and 1 others. 2024. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094. Chejian Xu, Mintong Kang, Jiawei Zhang, Zeyi Liao, Lingbo Mo, Mengqi Yuan, Huan Sun, and Bo Li. 2024. Advagent: Controllable blackbox red-teaming on web agents. arXiv preprint arXiv:2410.17401. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, and Xu Sun. 2024. Watch out for your agents! investigating backdoor threats to llm-based agents. Advances in Neural Information Processing Systems, 37:100938100964. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024. τ -bench: benchmark for toolagent-user interaction in real-world domains. ArXiv, abs/2406.12045. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, and Xuan-Jing Huang. 2024. Toolsword: Unveiling safety issues of large language models in tool learning across three stages. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 21812211. Lingzhi Yuan, Xinfeng Li, Chejian Xu, Guanhong Tao, Xiaojun Jia, Yihao Huang, Wei Dong, Yang Liu, and Bo Li. 2025. Promptguard: Soft prompt-guided unsafe content moderation for text-to-image models. arXiv preprint arXiv:2501.03544. Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, and 1 others. 2024. R-judge: Benchmarking safety risk awareness for llm agents. arXiv preprint arXiv:2401.10019. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, and 1 others. 2025. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471. Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, and 1 others. 2024. Shieldgemma: Generative ai content moderation based on gemma. arXiv preprint arXiv:2407.21772. Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. 2024. InjecAgent: Benchmarking indirect prompt injections in tool-integrated large language In Findings of the Association for model agents. Computational Linguistics: ACL 2024, pages 10471 10506, Bangkok, Thailand. Association for Computational Linguistics. Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, and 1 others. 2025a. Ufo: ui-focused agent for windows os interaction. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 597622. Jinchuan Zhang, Lu Yin, Yan Zhou, and Songlin Hu. 2025b. Agentalign: Navigating safety alignment in the shift from informative to agentic large language models. arXiv preprint arXiv:2505.23020. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, and 1 others. 2025c. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176. Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, and Minlie Huang. 2024. Agent-safetybench: Evaluating the safety of llm agents. arXiv preprint arXiv:2412.14470. Haiquan Zhao, Chenhan Yuan, Fei Huang, Xiaomeng Hu, Yichang Zhang, An Yang, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, and 1 others. 2025. Qwen3guard technical report. arXiv preprint arXiv:2510.14276. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, and 1 others. 2023. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854. Xin Zhou, Martin Weyssow, Ratnadira Widyasari, Ting Zhang, Junda He, Yunbo Lyu, Jianming Chang, Beiqi Zhang, Dan Huang, and David Lo. 2025. Lessleakbench: first investigation of data leakage in llms across 83 software engineering benchmarks. arXiv preprint arXiv:2502.06215. 11 Source Dataset of TS-Bench The construction pipeline of TS-Bench Benchmark is shown in Figure 6. We collect agent interaction trajectories from four representative datsets for agent safety evaluation and alignment: AgentAlign (Zhang et al., 2025b), AgentHarm (Andriushchenko et al., 2024), Agent Security Bench (ASB) (Zhang et al., 2024), and AgentDojo (Debenedetti et al., 2024). Below we describe each source dataset in detail. AgentAlign. AgentAlign is training dataset specifically designed for agent safety alignment. It contains 4,956 harmful instructions for refusal training and 9,783 benign instructions for utility preservation, paired with 7,485 unique tools. For benign instructions, the dataset already provides benign interaction trajectories, which we directly reuse without resampling. For harmful instructions, since the original dataset only provides refusal-style responses rather than full interaction traces, we sample ReAct-style trajectories using gpt-4o-2024-0806. As AgentAlign is originally intended for training, we use this subset as the training split of TS-Bench. AgentHarm. AgentHarm is an agent safety evaluation benchmark that measures the tendency and capability of large language model (LLM) agents to perform harmful tasks. In open-source version, the dataset provides 176 malicious instructions and 176 benign instructions, together with 104 tools implemented as simulated Python functions. For both benign and malicious instructions, we sample ReAct-style trajectories using gpt-4o-2024-0806, Qwen3-30B-A3B-Instruct-2507, and Claude-3.5-Sonnet. are These used as the test split of TS-Bench. samples Agent Security Bench (ASB). ASB is an agent security evaluation benchmark covering 10 representative application scenarios (e.g., e-commerce, autonomous driving, and finance). For each scenario, ASB defines corresponding agent along with five benign task instructions. The benchmark includes over 420 tools in total, consisting of 400 attack tools (organized across 10 agents, with 40 attack tools per agent) and 20 benign tools. Based on ASB, we construct both direct prompt injection and indirect prompt injection test cases. In the former, the attack instruction is directly concatenated with the benign user instruction; in the latter, the attack instruction is embedded into the return values of Figure 6: Pipeline. Illustration of TS-Bench Construction benign tools. We sample ReAct-style trajectories using gpt-4o-2024-0806. In ASB, three domains (academic_search, autonomous_driving, and system_admin) are included in the training set, while the remaining seven domains (financial, legal, medical, education, psychological, e-commerce, and aerospace_engineer) are reserved for testing. AgentDojo. AgentDojo provides dynamic environment for evaluating prompt injection attacks and defenses in LLM-based agents. The evaluation suite includes 70 tools, 97 real-world user tasks, and 27 injection objectives, spanning four domains: banking, travel, workspace, and Slack. We sample ReAct-style trajectories using gpt-4o-2024-0806 and Qwen3-30B-A3B-Instruct-2507. These samples are used as the test split of TS-Bench. Summary. In total, we obtain four trajectory subsets: AgentAlign-Traj, AgentHarm-Traj, ASBTraj, and AgentDojo-Traj. These subsets contain agent interaction trajectories collected under diverse environments. The prompt template used for sampling ReAct-style trajectories is illustrated in Figure 8. Collectively, the four subsets cover the four unsafe tool invocation patterns discussed in Section 3.2. Notably, AgentAlign-Traj and AgentHarmTraj do not involve third-party prompt injection attacks; unsafe tool invocation behaviors 12 in these datasets are solely triggered by maliIn contrast, ASB-Traj cious user instructions. and AgentDojo-Traj attribute unsafe tool invocation behaviors to third-party prompt injection attacks embedded in external tools or environments. In ASB-Traj, approximately half of the attack tools are explicitly malicious, as indicated by their tool names and descriptions. In AgentDojoTraj, injected tasks are carried out through benign, general-purpose tools (e.g., send_email, get_most_recent_transactions), where unsafe behavior arises from the agent passing malicious or unsafe parameters to otherwise benign tools for harmful purposes. More examples in TS-Bench can be found in Appendix C. Annotation Guidelines of TS-Bench Following (Zhao et al., 2025), we adopt threelevel safety labeling scheme: safe (0.0), controversial or potentially unsafe (0.5), and significantly unsafe (1.0). In addition to assigning safety label to each tool invocation action, we annotate each sample with the harmfulness of the user request and the connection of the current planned action to prompt injection attacks as auxiliary signals. We design specific annotation rules for different source datasets, as shown in Table 13. Example in TS-Bench We provide representative examples from TSBench-eval in Figures 1019."
        },
        {
            "title": "D Analysis of Data Leakage Risk",
            "content": "To examine whether the train/test split of TS-Bench suffers from potential data leakage, we conduct an embedding-based similarity analysis between training and test samples. Specifically, for each test instance, we retrieve the nearest neighbors from the training set using cosine similarity over user request and tool description embeddings, and compute the average similarity between training sets and test sets as well as several distributional statistics. The results are shown in Table 6. We consider three input constructions for embedding computation: (i) instruction + tool description, (ii) instruction only, and (iii) tool description only. All embeddings are computed using Qwen3Embedding-8B (Zhang et al., 2025c). We report the following metrics: KN Nmax: for each test sample, we identify its most similar instance in the entire training 13 set based on cosine similarity, and compute the corresponding similarity score. KN Nmax is then defined as the maximum of these scores over all test samples. p99: 99% of test instances have maximum cosine similarity to the training set below this value. Mean Max Similarity (MMS): for each test sample, we compute the cosine similarity to its most similar training instance, and MMS is defined as the average of these maximum similarity scores over the entire test set. In addition, we report the proportion of test samples whose nearest-neighbor similarity to the training set is below 0.8 and 0.9. While prior work typically considers data leakage only when cosine similarity reaches 1.0 (Zhou et al., 2025), we adopt stricter criterion and regard similarity below 0.9 as indicative of no leakage. Results show that, even under this conservative setting, TS-Bench-train and TS-Bench-eval exhibit negligible overlap, indicating that the dataset split does not suffer from data leakage. For reference, we also compute the same statistics (KN Nmax, p99, and MMS) within the training set itself. The results show that similarities among training samples are substantially higher than those between training and test samples, further demonstrating that the train/test split of TS-Bench does not suffer from data leakage."
        },
        {
            "title": "E Evaluation Details",
            "content": "E.1 Guardrail Model Evaluation t, Hi The guardrail model takes as input the agent interaction context (ui t) and available tool descriptions together with the currently planned tool invocation action ai t, and outputs safety rating yi for the action. Different guardrail models adopt different prompt templates. For LlamaGuard3-8B, Qwen3Guard-8B-Gen, Safiron, and ShieldAgent-THU, which are specifically trained guardrail models, we use their original prompt templates. For other general-purpose LLMs, we adopt the same prompt template as TS-Guard to ensure fair and consistent comparison, as illustrated in Figure 7. For the user request, we directly use ui t. For the agent action, we concatenate into JSON- {\"interaction_history\" : formatted input: and ai Input Train Set Test Set KN Nmax p99 MMS %(<0.90) %(<0.80) Instruction + Tool Desc Instruction Tool Desc ASB_train N/A AgentAlign_train N/A ASB_train ASB_train ASB_train AgentAlign_train AgentDojo_test AgentAlign_train AgentHarm_test AgentAlign_train ASB_test AgentDojo_test AgentHarm_test ASB_test N/A ASB_train AgentAlign_train N/A ASB_train ASB_train ASB_train AgentAlign_train AgentDojo_test AgentAlign_train AgentHarm_test AgentAlign_train ASB_test AgentDojo_test AgentHarm_test ASB_test ASB_train N/A AgentAlign_train N/A ASB_train ASB_train ASB_train AgentAlign_train AgentDojo_test AgentAlign_train AgentHarm_test AgentAlign_train ASB_test AgentDojo_test AgentHarm_test ASB_test 1.000 1.000 0.725 0.772 0.856 0.854 0.861 0.709 1.000 1.000 0.816 0.714 0.906 0.920 0.898 0.848 1.000 1.000 0.662 0.697 0.839 0.885 0.887 0.628 1.000 1.000 0.716 0.734 0.839 0.817 0.859 0.699 1.000 1.000 0.816 0.846 0.865 0.920 0.874 0.848 1.000 1.000 0.654 0.652 0.833 0.874 0.885 0. 0.994 0.931 0.659 0.639 0.777 0.751 0.783 0.636 0.995 0.995 0.728 0.714 0.807 0.776 0.787 0.769 0.999 0.999 0.600 0.579 0.759 0.793 0.819 0.566 0.000 19.880 100.000 100.000 100.000 100.000 100.000 100.000 0.000 0.000 100.000 100.000 99.990 98.520 100.000 100.000 0.000 4.150 100.000 100.000 100.000 100.000 100.000 100. 0.000 2.070 100.000 100.000 77.560 92.620 69.460 100.000 0.000 0.000 95.330 88.770 40.990 67.540 62.820 83.830 0.000 2.370 100.000 100.000 81.370 40.820 32.440 100.000 Table 6: Embedding-based data leakage analysis between TS-Bench training and test splits. For each test sample, we compute cosine similarity to its nearest neighbors in the training set (K=1). KN Nmax denotes the maximum similarity observed, p99 denotes 99% of test instances have maximum cosine similarity to the training set below this value, and MMS is the mean maximum cosine similarity. The last two columns report the percentage of test samples whose maximum similarity is below 0.90 and 0.80, respectively. , \"current_action\" : ai i is appended after the agent action. t}. Tool information E.2 Guarded Agent Evaluation We adopt GPT-4o-2024-0806 and Qwen2.5-14BInstruct as the foundation models and implement ReAct-style LLM-based agent using the prompt template illustrated in Figure 8. For both LlamaFirewall and TS-Flow, the guardrail model is invoked before every tool execution, receiving the complete agent interaction messages up to the current step as input. When LlamaFirewall detects potentially unsafe behavior, it immediately terminates the agents execution. In contrast, TS-Flow feeds back its intermediate analysis and safety validation to the agent, enabling the agent to revise its planned action and continue the benign task in safer manner. Figure 2025 presents representative success case of TS-Flow (GPT-4o-2024-08-06), illustrating how such guardrail-driven feedback effectively guides the agent away from unsafe tool invocation while preserving benign task completion."
        },
        {
            "title": "F Implementation Details",
            "content": "We train TS-Guard based on the GRPO implementation in the verl framework (Sheng et al., 2025). The backbone model is Qwen2.5-7B-Instruct. The total batch size is 256, with maximum prompt length of 4096 tokens and maximum response length of 1024 tokens. The actor is optimized with learning rate of 1 106 and KL regularization (coefficient 0.001, low-variance KL), while entropy regularization is disabled. Rollouts are generated using VLLM with tensor parallelism of 2 and 16 rollouts per prompt. Training runs for 10 epochs. For evaluation, all open-source models use nucleus sampling with temperature 0.1 and top-p 0.9. All experiments are conducted in the same hardware environment with 8 NVIDIA 96GB H20 GPUs."
        },
        {
            "title": "Model Evaluation",
            "content": "Table 7 reports the guardrail model evaluation results under the loose mode, where potentially harmful or controversial samples are treated as safe. Under this setting, TS-Guard ranks second on Model gpt-4o AgentHarm-traj F1 Recall ACC ASB-traj F1 Recall ACC AgentDojo-traj F1 Recall ACC 79.48 63.94 63.33 82.95 51.55 44. 84.92 79.14 99.15 Qwen3-8B Qwen2.5-7B-IT Llama-Guard-3-8B Qwen3Guard-8B-Gen ShieldAgent-THU Safiron 76.47 74. 56.77 70.72 33.24 64.02 47.24 29.32 53.11 47.80 45.78 36.32 36.67 18.57 85.24 46.67 98.09 35.71 81.48 79. 76.70 79.89 66.41 70.65 40.66 23.86 27.89 3.30 48.34 43.97 31.03 15.89 22.03 1.68 76.84 56.30 85.47 74. 74.75 71.15 60.90 70.98 77.10 50.95 33.33 0.00 58.91 52.80 84.90 45.58 21.87 0.00 97.16 56.25 TS-Guard (Ours) 64.84 55.92 77.62 90.64 77.38 78. 85.81 68.26 52.84 Table 7: (Loose Mode) Comparison of TS-Guard and baseline guardrail models on step-level tool invocation safety detection in the TS-Bench benchmark. The best results are highlighted in bold. Model AgentHarm-traj Recall ACC gpt-4o Qwen3-8B Qwen2.5-7B-IT 58.14 45.14 45.00 53.63 44.95 34.85 54.53 47.90 38. ASB-traj F1 50.57 40.28 32.54 Recall 49.39 42.75 37.91 ACC 54.92 54.94 35. AgentDojo-traj F1 Recall ACC 55.49 77.52 28.55 44.66 53.67 28.82 45.64 53.15 22. TS-Guard (ours) 79.93 75.93 75.99 86.61 83. 83.22 81.15 54.11 50.48 Table 8: (Exact Mode) Comparison of TS-Guard and baseline guardrail models on step-level tool invocation safety detection in the TS-Bench benchmark. The best results are highlighted in bold. Guardrail Model Mean Max Median Overhead Analysis of TS-Flow GPT-4o Qwen3-8B Qwen2.5-7B-Instruct ShieldAgent-THU TS-Guard 124.13 614.97 208.93 236.65 202. 222.32 2022.07 486.12 412.00 349.33 122.97 517.50 196.96 220.50 201.33 Table 9: Output token statistics of different guardrail models on the TS-Bench-eval benchmark. We only include models capable of producing long chain-ofthought (CoT) reasoning for fair comparison. AgentHarm-Traj (behind GPT-4o), first on ASBTraj, and third on AgentDojo-Traj (behind GPT-4o and Qwen3-8B). We further evaluate models under the exact mode, considering GPT-4o, Qwen3-8B, Qwen2.57B-Instruct, and TS-Guard, as these models support three-level safety rating. As shown in Table 8, TS-Guard achieves the best overall performance, indicating its ability to provide more fine-grained safety judgment of tool invocation behaviors. Overall, these results demonstrate that TS-Guard delivers the strongest comprehensive performance for step-level tool invocation safety detection in LLM-based agents. Since TS-Flow feeds back safety judgment signals from the guardrail model to LLM-based agents, it inevitably introduces additional overhead. In this section, we analyze this overhead using the AgentDojo benchmark. Specifically, we measure the average and maximum number of interaction steps required to complete each user task, as well as the average and maximum number of input tokens consumed by the LLMs. The results are summarized in Table 10. We draw two key observations: (1) TS-Flow reduces ASR while improving utility on benign tasks, and simultaneously decreases the number of interaction steps. We attribute this to the fact that agents without safety guardrails tend to pursue injected or malicious objectives, leading to redundant and ineffective interactions. (2) TS-Flow increases the LLM input token length; however, the increase remains well below twofold expansion and is therefore acceptable in practice. We find that this overhead mainly comes from the guardrail models feedback signals. Agents driven by GPT-4o and Qwen2.5-14BInstruct trigger guardrail feedback an average of 0.97 and 1.41 times per user task, respectively. 15 Model Method Performance (%) Interaction Steps Context Length (Tokens) ASR Utility Mean Max Total Max Overhead (%) GPT-4o Qwen2.5-14B ReAct ReAct + TS-Flow ReAct ReAct + TS-Flow 56.16 1.16 17.59 1.79 26.87 42. 42.57 41.72 6.37 3.79 5.71 4.76 10 8 15 10 388.39 529. 390.10 737.37 1079 1463 1538 3464 +36.32% +89.02% Table 10: Efficiency and context length comparison between ReAct and ReAct+TS-Flow for agents driven by GPT-4o and Qwen2.5-14B-Instruct on the AgentDojo benchmark. Method Performance (F1) Efficiency AgentHarm-Traj AgentDojo-Traj (second/sample) GPT-4o AGrail (w. GPT-4o) ShieldAgent TS-Guard 84.80 85.75 90.16 56.59 58.05 86.18 1.98 8.75 10. 1.36 Table 11: Safety detection performance and efficiency comparison between guardrail agents and our proposed TS-Guard on TS-Bench-eval. Model AgentHarm-traj ASB-traj Recall F1 Recall GPT-4o Qwen3-8B Qwen2.5-7B-IT 79.38 60.84 44.40 TS-Guard (Ours) 85. 99.06 52.09 31.29 93.77 61.74 33.45 23.20 58.39 29.21 13.89 95.00 93. Performance on two auxiliary tasks. Table 12: AgentHarm-traj evaluates user request harmfulness detection, while ASB-traj evaluates attack identification. We also further report the token statistics of the guardrail model outputs in Table 9. Comparison between Model-Based and Agent-Based Guardrails This work focuses on training guardrail model for step-level safety detection of tool invocation in LLM-based agents, enabling timely pre-execution safety intervention. Prior studies have explored guardrail agents, such as GuardAgent (Xiang et al., 2024), AGrail (Luo et al., 2025b), and ShieldAgent (Chen et al., 2025). These approaches are typically designed for domain-specific agents (e.g., OS agents (Xie et al., 2024), EHRagent (Shi et al., 2024) or web agents (Xu et al., 2024)) and rely on predefined or adaptively generated checklists to validate agent behaviors within customized workflows. Guardrail models and guardrail agents represent two distinct technical paradigms. We evaluate representative state-of-the-art guardrail agents against our proposed TS-Guard on the TS-Bencheval benchmark. The results are reported in Table 111, where we analyze both safety detection performance and inference efficiency. The results show that (1) guardrail-agent-based methods such as AGrail perform substantially worse than TSGuard in the general-purpose agent tool invocation setting. (2) Moreover, they suffer from severe latency overhead: single safety check with AGrail requires approximately 8 more time than guardrail model. This efficiency bottleneck significantly limits the practicality of guardrail agents in real-world deployments where low-latency decision making is critical."
        },
        {
            "title": "J Performance on Auxiliary Safety Tasks",
            "content": "In Table 12, we also evaluated the performance of TS-Guard, GPT-4o, Qwen2.5-7B-IT, and Qwen38B on two auxiliary tasks: detecting the harmfulness of user requests and identifying the association between tool invocation and prompt injection attacks. We used AgentHarm-Traj to assess performance for user request harmfulness detection, and AgentDojo-Traj to evaluate the effect for the attack vectors association detection. The results show that TS-Guard significantly outperforms the other models. These findings indicate that multi-task reward signals provide finer-grained supervision, enabling the agent to more accurately analyze interaction histories and pinpoint harmful tool invocation patterns and their attribution. 1ShieldAgent has not released its codebase. We therefore report only an estimated inference cost based on its original paper, which suggests an latency of approximately 5 that of GPT-4o direct prompting. 16 Dataset Annotation Rules Step-Level Labeling Unsafe: 1.0; Potentially Unsafe: 0.5; Safe: 0.0 Unsafe: 1.0; Potentially Unsafe: 0.5; Safe: 0.0 Unsafe: 1.0; Safe: 0.0; Ambiguous steps excluded from test set Unsafe: 1.0; Potentially Unsafe: 0.5; Safe: 0.0 AgentHarm-Traj ASB-Traj AgentDojo-Traj AgentAlign-Traj Malicious instructions are first annotated by human experts to identify harmful tools. For malicious instructions, tool calls involving explicitly harmful tools are labeled as unsafe (1.0), while other tool calls in the same trajectory are labeled as potentially unsafe (0.5), as they may assist in executing malicious requests (e.g., search or query tools). All tool calls for benign instructions are labeled safe (0.0). The original ASB dataset distinguishes aggressive and non-aggressive attack tools. Steps invoking aggressive attack tools are labeled unsafe (1.0), while steps invoking non-aggressive attack tools are labeled potentially unsafe (0.5), since they are related to prompt injection but do not cause severe damage. Steps invoking normal tools are labeled safe (0.0). Tools are not pre-assigned to tasks. We first filter tools for normal and injection tasks using GPT-4o-2024-0806. Each tool call is identified as serving either normal or injection task, and whether it causes an environmental change is recorded. Tool calls for normal tasks are labeled safe (0.0). Tool calls for injection tasks that cause actual environment changes are labeled unsafe (1.0). Steps invoking injection-task tools without environment changes are excluded from the test set due to ambiguous effects. All steps in benign instruction trajectories are labeled safe (0.0). Malicious instruction steps are labeled as potentially unsafe (0.5) or unsafe (1.0). Due to large numbers of tools and instructions, human annotation of 0.5 vs 1.0 is difficult, so labels are determined via majority voting among Qwen30B-A8B, GPT-4o, and Claude3.5-sonnet. Out of all step-level samples, 673 are selected (550 for malicious tasks, 123 for benign tasks), with 90.38% human verification consistency. Table 13: Step-level tool invocation safety annotation guidelines for representative agent safety datasets. Labels: Safe (0.0), Potentially Unsafe (0.5), Unsafe (1.0). 17 Figure 7: Prompt template for TS-Guard. 18 Figure 8: Prompt template for ReAct-style LLM-based agents. Figure 9: Safe sample in AgentHarm-Traj. 19 Figure 10: Controversial or potentially unsafe sample in AgentHarm-Traj. Figure 11: Significantly unsafe sample in AgentHarm-Traj. 20 Figure 12: Significantly unsafe sample in ASB-Traj (direct prompt injection attacks). Figure 13: Safe sample in ASB-Traj (direct prompt injection attacks). Figure 14: controversial or potentially unsafe sample in ASB-Traj (direct prompt injection attacks). 21 Figure 15: Significantly unsafe sample in ASB-Traj (indirect prompt injection attacks). Figure 16: Safe sample in ASB-Traj (indirect prompt injection attacks). Figure 17: controversial or potentially unsafe sample in ASB-Traj (indirect prompt injection attacks). Figure 18: Safe sample in AgentDojo-Traj. Figure 19: Significantly unsafe sample in AgentDojo-Traj. 23 Figure 20: ReAct trajectory in AgentHarm. 24 Figure 21: ReAct+TS-Flow trajectory in AgentHarm. Figure 22: ReAct trajectory in ASB. 25 Figure 23: ReAct+TS-Flow trajectory in ASB. 26 Figure 24: ReAct trajectory in AgentDojo. Figure 25: ReAct+TS-Flow trajectory in AgentDojo."
        }
    ],
    "affiliations": [
        "National Engineering Research Center for Software Engineering, Peking University",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}