{
    "paper_title": "The Denario project: Deep knowledge AI agents for scientific discovery",
    "authors": [
        "Francisco Villaescusa-Navarro",
        "Boris Bolliet",
        "Pablo Villanueva-Domingo",
        "Adrian E. Bayer",
        "Aidan Acquah",
        "Chetana Amancharla",
        "Almog Barzilay-Siegal",
        "Pablo Bermejo",
        "Camille Bilodeau",
        "Pablo Cárdenas Ramírez",
        "Miles Cranmer",
        "Urbano L. França",
        "ChangHoon Hahn",
        "Yan-Fei Jiang",
        "Raul Jimenez",
        "Jun-Young Lee",
        "Antonio Lerario",
        "Osman Mamun",
        "Thomas Meier",
        "Anupam A. Ojha",
        "Pavlos Protopapas",
        "Shimanto Roy",
        "David N. Spergel",
        "Pedro Tarancón-Álvarez",
        "Ujjwal Tiwari",
        "Matteo Viel",
        "Digvijay Wadekar",
        "Chi Wang",
        "Bonny Y. Wang",
        "Licong Xu",
        "Yossi Yovel",
        "Shuwen Yue",
        "Wen-Han Zhou",
        "Qiyao Zhu",
        "Jiajun Zou",
        "Íñigo Zubeldia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 7 8 8 6 2 . 0 1 5 2 : r The Denario project: Deep knowledge AI agents for scientific discovery Francisco Villaescusa-Navarro1,2,, Boris Bolliet3,4,, Pablo Villanueva-Domingo5,, Adrian E. Bayer1,2, Aidan Acquah6, Chetana Amancharla7, Almog Barzilay-Siegal8, Pablo Bermejo9,10,11, Camille Bilodeau12, Pablo Cardenas Ramırez13,14,15, Miles Cranmer16, Urbano L. Franca17,18, ChangHoon Hahn19,20, Yan-Fei Jiang1, Raul Jimenez21,22, Jun-Young Lee1, Antonio Lerario23, Osman Mamun13, Thomas Meier24, Anupam A. Ojha25,26, Pavlos Protopapas27, Shimanto Roy12, David N. Spergel1, Pedro TaranconAlvarez21,28, Ujjwal Tiwari7, Matteo Viel23,29,30,31,32, Digvijay Wadekar33,40, Chi Wang34, Bonny Y. Wang35, Licong Xu36,4, Yossi Yovel8,37, Shuwen Yue13, Wen-Han Zhou38, Qiyao Zhu25, Jiajun Zou39, Inigo Zubeldia16,36,4 1 Center for Computational Astrophysics, Flatiron Institute, New York, NY 10010, USA 2 Department of Astrophysical Sciences, Princeton University, Princeton, NJ 08544, USA 3 Cavendish Astrophysics, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK 4 Kavli Institute for Cosmology, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK 5 Computer Vision Center, Universitat Aut`onoma de Barcelona, 08193 Bellaterra, Barcelona, Spain 6 Big Data Institute, University of Oxford, Old Road Campus, Oxford OX3 7LF, UK 7 Infosys Ltd 8 School of Zoology, Faculty of Life Sciences, Tel-Aviv University, 6997801, Tel-Aviv Israel 9 Donostia International Physics Center, Paseo Manuel de Lardizabal 4, E-20018 San Sebastian, Spain 10 Department of Applied Physics, University of the Basque Country, 20018 San Sebastian, Spain 11 Center for Computational Quantum Physics, Flatiron Institute, New York, NY 10010, USA 12 Chemical Engineering Department, University of Virginia, Wilsdorf Hall, Charlottesville, VA 22903 13 Robert F. Smith School of Chemical and Biomolecular Engineering, Cornell University, Ithaca, NY 14853, USA 14 Ragon Institute of Mass General, MIT, and Harvard, 600 Main St, Cambridge, MA 02139, USA 15 Department of Microbiology, Harvard Medical School, 77 Ave Louis Pasteur, Boston, MA 02115, USA 16 Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, UK 17 Boston Childrens Hospital, Department of Anesthesiology, Critical Care and Pain Medicine, 300 Longwood Ave, Bader 6, Boston, MA, 02115, USA 18 Harvard Medical School, 25 Shattuck Street Boston, MA, 02115, USA 19 Department of Astronomy, The University of Texas at Austin, Austin, TX 78712, USA 20 Steward Observatory, University of Arizona, 933 N. Cherry Avenue, Tucson, AZ 85721, USA 21 Institut de Ciencies del Cosmos, Universitat de Barcelona, Martı Franques 1, Barcelona, Spain. 22 ICREA, Pg. Lluis Companys 23, Barcelona, 08010, Spain 23 SISSA - International School for Advanced Studies, Via Bonomea 265, I-34136 Trieste, Italy 24 MCML - Munich Center for Machine Learning, LMU Munich, 80539 Munich, Germany 25 Center for Computational Biology, Flatiron Institute, 162 5th Ave., New York, NY 10010, USA 26 Center for Computational Mathematics, Flatiron Institute, 162 5th Ave., New York, NY 10010, USA 27 Institute for Applied Computational Science, Harvard University, 150 Western Ave., MA 02134, USA 28 Departament de Fısica Quantica Astrofısica, Universitat de Barcelona, Barcelona, Spain 29 INFN National Institute for Nuclear Physics, Via Valerio 2, 34127 Trieste, Italy 30 ICSC - Centro Nazionale di Ricerca in High Performance Computing, Big Data Quantum Computing, Via Magnanelli 2, Bologna, Italy 31 INAF Osservatorio Astronomico di Trieste, Via G. B. Tiepolo 11, I-34143 Trieste, Italy 32 IFPU Institute for Fundamental Physics of the Universe, Via Beirut 2, I-34151 Trieste, Italy 33 Department of Physics and Astronomy, Johns Hopkins University, Baltimore, Maryland, 21218, USA 34 Google DeepMind 1 35 Department of Astronomy and Astrophysics, The University of Chicago, Chicago, IL 60637, USA 36 Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK 37 Sagol School of Neuroscience, Tel-Aviv University, 6997801, Tel-Aviv Israel 38 Department of Earth and Planetary Science, The University of Tokyo, Tokyo 113-8654, Japan 39 SENSEx Lab, International School for Advanced Studies (SISSA), 34136 Trieste, Italy 40 Weinberg Institute, University of Texas at Austin, Austin, TX 78712, USA Abstract We present Denario, an AI multi-agent system designed to serve as scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing scientific paper. The system has modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at this URL. Denario demo can also be run directly on the web at this URL, and the full app will be deployed on the cloud."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 AI multi-agent systems 2.1 Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Multi-Agent Orchestration and Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Architecture Input Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Idea module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 3.3 Literature module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Methods module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Analysis module 3.6 Paper module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.7 Review module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Usage 4.1 Python API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Graphical User Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 End-to-end research examples physics of non-linear PDEs 5.1 Mathematical Physics. High-level analysis and abstraction of complex structures: the . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Planetary Science. Mapping Size and Compositional Gradients in the Asteroid Belt . . . 5.3 Medicine and Health Services Research. Assessment of Fertility Clinic Outcomes in the United States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Chemistry. Molecular dynamics simulations of self-assembling peptides . . . . . . . . . . . 5.5 Astrophysics. Properties of recent high-mass binary black hole merger from gravitational wave data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6 Astrophysics. Impact of Convection and Radiation Pressure on the Dynamics of Roche Lobe Overflow in Binary Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . *Equal contribution. Listing order of FVN, BB, PVD is random. 3 5 5 6 8 9 10 13 14 16 18 19 20 21 22 23 23 27 30 32 34 worn accelerometers 5.7 Biology. Analyzing single-celled RNA sequencing data across the malaria parasite life cycle 36 5.8 Digital Health. Developing and evaluating step-counting algorithms from wristand hip- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.9 Biophysics. Mechanistic understanding of protein folding pathways via dimensionality reduction methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.10 Neuroscience. Behavioral Cognitive Decline in Egyptian Fruit-Bats. . . . 5.11 Materials Science. Confined Water Dynamics at Functionalized Graphene Interfaces 5.12 Astrophysics. Impact of feedback on stellar mass - black-hole mass relation. . . . . . . . . 5.13 Quantum physics, machine learning & cosmology. Cosmological parameters from merger trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 46 48 59 40 6 Discussion 6.1 Failure modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Validation and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Advantages and limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4 Future directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Ethical implications and scientific challenges 7.1 Ethics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Epistemic changes in science driven by AI agents . . . . . . . . . . . . . . . . . . . . . . 7.3 The impact of AI agent-driven science on the structure of scientific work . . . . . . . . . . 7.4 Conclusions from philosophy of science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Summary and Conclusions 61 61 62 63 64 65 65 67 68 69 70 Example papers 78 78 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Mathematical physics A.2 Planetary science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 A.3 Medicine and Health Services Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 A.4 Chemistry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 A.5 Astrophysics: gravitational waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 A.6 Astrophysics: stellar physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 A.7 Biology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 A.8 Digital health . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176 A.9 Biophysics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 A.10 Neuroscience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 A.11 Material science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 A.12 Astrophysics: black hole-stellar mass relation . . . . . . . . . . . . . . . . . . . . . . . . . 241 A.13 Quantum physics, deep learning and cosmology . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "1 Introduction",
            "content": "Modern science has been built on the rules of the scientific method [1]. This empirical and iterative method begins by formulating an idea or hypothesis, which represents the foundation for the entire research endeavor. This initial idea might arise from variety of sources: theoretical question, an unexplained phenomenon, an unexpected result in prior work, or curiosity-driven exploration of dataset or simulation. The hypothesis can aim to explain an observed behavior in nature, test the validity of theoretical model, or uncover new patterns or relationships within existing data. Once the idea is identified, it is often refined through collaboration discussions, preliminary literature reviews, and feasibility assessments. Researchers seek to understand the broader context of the problem, what has already been studied, and where the current knowledge gaps lie. This step is crucial for ensuring that the proposed work is novel, feasible, and impactful. Next, detailed plan is developed to execute the project and investigate the proposed idea. This stage involves identifying the necessary data, selecting or designing appropriate methodologies, and outlining the sequence of steps required for the analysis. Depending on the nature of the research, this may include data collection or simulation, data preprocessing and cleaning, implementing algorithms or analytical models, and performing statistical or computational analyses. Along this process, scientists may use lower-dimensional representations of the data, e.g., via plots or diagrams, to help interpret intermediate 3 results and to communicate findings effectively. The ultimate goal of this phase is to systematically explore the hypothesis, extract meaningful insights, and arrive at scientifically sound conclusions. Finally, scientific paper or report is written describing in detail the problem or hypothesis, the methods used, the results obtained, and the conclusions reached, describing how the research has advanced knowledge and its connections with previous works. Each of the above steps is highly sophisticated task that requires extensive time for highly skilled domain experts. Because of this, the generation of scientific paper can take from months to years. Unfortunately, significant fraction of that time is consumed on tasks such as literature search, learning new methods, debugging code, and writing text. These are tasks where recent advances in Artificial Intelligence (AI), such as large language models and AI agents, can help. Thus, developing AI tools that can assist scientists with some or all of the tasks involved in the generation of new knowledge can accelerate scientific discovery by enabling scientists to formulate new hypotheses and test those faster than in the traditional method. In this work, we introduce and describe Denario, an AI multi-agent system designed to serve as scientific research assistant. Denario can generate ideas, check the literature to evaluate the novelty of those, develop research plans, write and execute code, make plots, and draft and review scientific papers. It can also perform end-to-end scientific research by carrying out, sequentially, all the above tasks. We evaluate the capabilities and limitations of Denario by generating paper drafts from very diverse set of disciplines, from biology to astrophysics, and reporting the positive and negative aspects of these papers, as judged by domain experts. Hoping to be tool that can accelerate scientific discovery, we also make Denario publicly available. We want to stress that Denario is not the first attempt at building tools to help researchers perform end-to-end research. The idea of having machines perform research is much older. For instance, the robot scientist (also known as Adam) [2] was able to formulate hypotheses, plan and run experiments (physically via robotics), interpret the results, and repeat the cycle. Adam was designed to study functional genomics and was the first machine to generate scientific knowledge independently of humans. The same group later developed Eve [3], another robot scientist tailored towards drug discovery. Another early important work is the Automated Statistician [4], which was able to perform detailed statistical analysis of some data and provide report about its findings. More recently, lots of recent work on AI assistants for research has been done by many different teams, such as Sakana AI-scientist [5, 6], Google co-scientist [7], AI-cosmologist [8], Curie [9], and Agent laboratory [10]. We note that lots of works are also developing tools to help scientists with increasingly sophisticated tasks. For instance, AutoWEKA was developed to identify the best machine learning model to tackle particular problem [11]. ResearchAgent [12] was able to formulate new hypotheses and design experiments by checking their novelty with the literature. [13] created self-driving lab to re-engineer enzymes without human intervention, while [14] created virtual lab with several AI-scientist experts in different areas capable of creating nanobodies. Another example is AstroAgents [15], multi-agent system designed to test and consolidate many different hypotheses about the origins of life on Earth. Recent works have also shown that AI systems can write expert-level scientific software and discover new algorithms [16, 17, 18]. Other works are exploring the capabilities of agentic systems via graphs [19, 20]. We also note that AgentRxiv [21] is framework for Agents to learn from previous works. While some of these technologies have the potential to accelerate scientific discovery for instance, [22] designed an autonomous system combining AI with robotics to discover new materials lot of serious debates about human standards for validation sparked [23]. These are very important considerations and we discuss them in detail, for the case of Denario. Furthermore, given the ethical considerations raised by such technology, we devote paper section to discuss ethical aspects, along with how this technology fits within the philosophy of science. We note that in scientific research, there is usually tradeoff between depth and breadth, given fixed amount of time to carry out task or study. We believe that AI systems like Denario can help researchers explore wide range of possibilities, at more superficial level, while researchers can go deeper into the most interesting or promising ones. We emphasize that the goal of Denario is not to automate science, but to develop research assistant that can accelerate scientific discovery. By making Denario publicly available and user-friendly, we hope it will become standard tool that benefits the scientific community in their enterprises. This paper is organized as follows. In Section 2, we introduce the different pieces that made our system, such as LLMs, AI agents, and agentic orchestration. In Sec. 3 we describe in detail the different modules in Denario and show the different ways to run the system in Sec. 4. In Sec. 5, we show different AIgenerated papers from many different disciplines and discuss the positive and negative aspects of those. 4 We then present in Sec. 6 some of the observed failure modes of Denario together with advantages, limitations, and future directions. We discuss the ethical considerations that systems like Denario raise and how the technology powering is seen from the philosophy of science in Sec. 7. We conclude in Sec. 8."
        },
        {
            "title": "2 AI multi-agent systems",
            "content": "Denario is an AI multi-agent framework designed to function as scientific research assistant. It is built with AG2 and LangGraph, and designed to assist in different stages of the research process. It can also execute multiple tasks sequentially, enabling it to perform end-to-end scientific research. The system is composed of several modules, each focused on specific task. Within each module, there are agentsautonomous or semi-autonomous computational entities that perceive their environment, process information, and take actions to achieve defined goals. Some of these are AI agents, which leverage Large Language Models (LLMs) to perform specialized tasks. Agents can communicate and collaborate to solve complex problems, while orchestration manages their interactionsdetermining task order, information flow, and overall coordination. The system takes some input text and returns one or several outputs depending on how Denario is run. We show cartoon representation of the system in Fig. 1. In this section, we describe the different components relevant to setting the agents, including the LLM models and the orchestration. Figure 1: This cartoon shows the different components of Denario and their interplay. The system is shown as an orange circle, and it takes some input text and data (left side), and can generate one or several outputs (red icons on the right). The system is composed of different modules (yellow shapes), which can exchange messages among them (blue arrows outside modules). The modules are composed of multiple agents (bots icons), that can communicate with each other (blue arrows inside modules)."
        },
        {
            "title": "2.1 Large Language Models",
            "content": "The majority of the agents in Denario are AI-agents that typically call Large Language Models (LLMs) [24], including multi-modal ones, with very specialized prompt that may depend on the output of other agents. Denario allows the user to choose among many different models, including: gpt-5, gpt-5-mini, gpt-4o, gpt-4.1, gpt-4.1-mini, o3-mini from OpenAI, gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash from Google, 5 claude-3-7-sonnet, claude-4-Opus, claude-4.1-Opus, claude-4.5 from Anthropic. We note that Denario is not limited to these LLMs, and new models, or models from other providers, can easily be incorporated into the system. Our choice of models has so far been made based on trial and error rather than systematic evaluation. For example, for coding, our primary models of choice are gemini-2.5-pro, gpt-4.1, and o3-mini. For critiquing and reviewing, our default choice is Claude models or reasoning models from OpenAI."
        },
        {
            "title": "2.2 Multi-Agent Orchestration and Strategy",
            "content": "In Denario, the agent orchestration is carried out with two different frameworks: AG21 and LangGraph2. Some modules (e.g. idea module) are implemented in both frameworks, while others (e.g. analysis module) are only implemented in one framework. Some modules implemented in AG2 follow the Planning & Control strategy as implemented in Cmbagent that we describe below. 2.2.1 Orchestration AG2. AG2 is an open-source AgentOS that supports advanced agentic AI applications and research. It offers an agentic abstraction that unifies AI foundation models like LLMs, non-LLM tools and human inputs, and intuitive conversation programming to orchestrate multiple agents. Stronger agents or agentic systems can be composed recursively using primitive agents and basic conversation patterns like sequential chats, group chats, nested chats. This modular architecture also enables rich agentic design patterns such as advanced tool use, structured context, conditional handoffs, dynamic updates, dynamic task decomposition and team building, guardrails, and many other extensions. All agents using AG2 are part of Cmbagent [25, 26, 27], an open source and publicly available3 multi-agent system designed to perform variety of scientific research-related tasks. LangGraph. LangGraph is based on LangChain4 and is an open-source Python framework deIn LangGraph, the graph nodes represent signed to orchestrate agents through directed graphs. callable units, such as agents, functions, and tools. The edges between nodes represent the way the flow of the computation is carried out. Another important component of LangGraph is the graph state, an object (typically Python class) passed through the different nodes and enabling agent communication, memory, coordination, etc. LangGraph supports both static and dynamic graphs. For instance, depending on the outcome of an agent, one may want to call particular agent. Loops and interrupts are also allowed in LangGraph. 2.2.2 Planning and Control Strategy Planning & Control, fundamental concept in robotics, refers to strategies for orchestrating decisionmaking and action execution. The basic idea is that during the planning phase, complex task is decomposed into smaller, manageable subtasks, and strategy is defined for achieving them. During the control phase, these subtasks are executed, with monitoring and adjustments as needed to ensure the overall goal is achieved. Some modules in Denario follow this strategy, as implemented in Cmbagent. We now briefly describe the Cmbagent Planning & Control implementation and refer the reader to [26] for further details. The strategy is illustrated in figure 2. In Cmbagent, during the planning phase, the main task is broken down into sub-tasks, and an agent is assigned to each of them. During control, the agent responsible for sub-task executes its instructions until completion. Once sub-task is complete, control agent records its status and proceeds to the next sub-task, continuing until the entire plan is complete. The Planning stage starts with call to plan setter agent, whose task is to select the set of agents that should be involved during the session. This function call stores into the context detailed instructions for the plan (namely, the agents and description of agents that should be involved) and ends on the next agent target, which is the planner agent. Then, the elaboration of plan is done via two-agent conversation where planner agent suggests plans and plan reviewer agent suggests recommendations on the plan. Recommendations and suggested plans are recorded in the context via function calls, including the number of steps in the plan. This information is injected into each agents 1https://ag2.ai/ 2https://www.langchain.com/LangGraph 3https://github.com/CMBAgents/cmbagent 4https://www.langchain.com/ 6 Figure 2: Planning & Control implementation of Deep Research from Cmbagent. This is the essential orchestration strategy enabling the research analysis without human-in-the-loop. Additionally, it can be used for idea generation, method generation and data pre-processing. system message before reply. The number of review rounds, nreviews, is hyperparameter of our system, and we generally set it to one5. Once the number of rounds of recommendation nreviews is reached, the final plan is recorded into the context, and the Planning stage ends. The response of the planner is list of subtasks, where each subtask consists of three fields: sub task: the sub-task to be performed sub task agent: the name of the agent in charge of the sub-task. bullet points: bullet-point list describing the specific actions the subtask is expected to perform. The number of subtasks is constrained from the prompt of the planner and plan reviewer agents, which are instructed to produce plan with, at most, nsteps steps, another tunable hyperparameter of our system. Typically, our plans contain between 3 and 8 steps6. The Control stage is orchestrated by control agent. The plan obtained from the Planning stage is injected into the system message of the control agent and it replies by calling the record status function. This function updates the context variable dictionary to record the current step being executed, whether it is completed, failed, or in progress, whether some new plots or new code have been produced, whether some code execution has failed, and, depending on this information, which agent to transfer to next. The control agent records two types of termination signals: whether the number of times code execution has failed exceeds the maximum number of failures allowed by the user, or whether the final step of the plan has been carried out successfully. In the former, the session aborts before the main task has been solved, and in the latter, the session ends positively with the full plan completely executed. Upon reception of termination signal, control agent transfers to terminator agent to end the session smoothly. Given the fact that the system may fall into an infinite loop (e.g., trying unsuccessfully to fix bug), we set hard limit to the maximum number of messages exchanged between the agents, nrounds. By 5With more rounds, we often observe that the suggested plans become overly complex and ineffective. 6As the context passed to the agents tends to grow during session, plan with more steps will generally lead to more costly session. 7 default, we set this to nrounds = 500. The system will fail if more than nrounds are exchanged. We note the system can fail for many other reasons, including limits on API calls set on the provider platforms, or occasional exceptions/errors that are not fully dealt with."
        },
        {
            "title": "3 Architecture",
            "content": "Denario is built as modular system, allowing the user to execute single or combination of modules. Each module is an AI agentic system designed to carry out specific scientific task. Figure 3 illustrates the overall workflow and architecture of Denario. Figure 3: This scheme shows the architecture of Denario. The green hexagons represent the different modules in Denario. These modules are agentic systems that can 1) generate an idea for research project, 2) check the literature to validate the novelty of the idea, 3) develop plan to carry out the project, 4) implement the plan by writing and executing code, making plots, and describing the results, 5) write scientific paper discussing the research and showing the plots, and 6) review the paper. Orange arrows indicate the input to the modules, while the green arrows show the output of them. The output of the modules are shown in the blue boxes. For details about the output, see Table 1. At the top, in magenta, there is the input description, where the user describes the data, postulates the problem... etc. We note that Denario is fully modular, and human input can be added at any point. For instance, Denario can be run as standalone program and generate paper from the input text file (by running the idea, methods, analysis, and paper modules sequentially), or can also be used as standalone tool to create plots and results from an input text, idea, and methods provided by the user. For most of the modules, Denario requires an input text guiding the systems behavior. This text can contain the description of the data to be used together with some computational constraints. It can also simply contain description of problem to be solved. For some research applications, it can also be relevant to specify that the data can (or should) be generated by the agents. The six main modules in Denario can perform these tasks (see Table 1 for summary): Idea module. This module is in charge of generating an idea for project. This module only requires the input text file to be run. After calling this module, Denario will print out the idea and save it in markdown file called idea.md. The agents in charge of this module are described in Section 3.2. Literature module. This module is in charge of checking the literature to figure out if an idea is original or there are existing works that have carried it out. The input to this module is the input text together with the idea.md file. This module will then check the literature and and write report on whether the idea is new or not. The report will be saved to file called literature.md. The agents in charge of this module are described in Section 3.3. Module Idea Task Generate project idea Literature Determine if idea is new Methods Develop project plan Analysis Implement plan write and execute code make plots Paper Write paper Review Review paper Input input.md input.md idea.md input.md idea.md input.md idea.md methods.md input.md idea.md methods.md results.md (input.md) paper.pdf Output idea.md literature.md methods.md results.md Plots paper.pdf referee.md Table 1: This table shows the different modules contained in Denario along with their tasks, inputs, and outputs. input.md contains description of the data or problem of interest and is provided by the human researcher in markdown format. idea.md contains the project idea, literature.md report on whether the idea is new, methods.md the research methodology, results.md the research analysis, Plots is folder containing the plots and referee.md contains referee report. paper.pdf is the scientific paper in pdf format. Files between parentheses are optional. Methodology module. This module is in charge of generating detailed plan to carry out the idea. To be run, it needs the input description text file and the idea.md file with the description of the idea. We note that idea.md can be generated by the previous module or provided by the user. After executing this module, Denario generates detailed plan outlining the steps and methods needed for the project. Denario stores this generated text in file called methods.md. The agents in this module are described in Section 3.4 Analysis module. This module is in charge of implementing the research plan. It requires the input text, together with the idea.md and methods.md files (generated by previous modules or provided by the user). By calling this module, Denario will carry out the research plan, usually translating into writing and running code, making plots, etc. The output of this module is file, results.md, with the outcome of the analysis, together with the plots generated. The agents behind this module are described in Section 3.5. Paper module. This module is in charge of writing scientific paper. This module requires the input text along with the idea.md, methods.md, results.md, and all plots. As above, these files can be the outcome of the modules or can be provided by the user. By calling this module, Denario writes scientific paper and saves the .tex, .bib, and .pdf files. The agents behind this module are described in Section 3.6. Review module. This module is in charge of reviewing scientific paper. The input to this module is pdf with the scientific paper along with an input description (optional). This module will review the paper and provide report about the positive and negative aspects of the draft. The output of this module is referee report that will be saved on file called referee.md. The agents behind this module are described in Section 3.7. We emphasize that Denario is designed in modular way. By calling sequentially all the above modules, Denario will perform end-to-end scientific research from an input text (e.g. from description of dataset). However, the user can also just run one (or several) of the modules to perform specific task. For instance, given well-defined idea and research plan (provided by the user), the user can run the analysis module to write code and create plots. We now discuss in detail the architecture and agents involved in each module of Denario."
        },
        {
            "title": "3.1 Input Text",
            "content": "All modules in Denario accept an input text. This input text usually contains description of the data to be analyzed or the problem to be solved. This text is later used by the modules to generate ideas, 9 develop research plans, analyze the data, or check the literature. How well the system behaves depends critically on this text. We now outline few guidelines on how to write the input text: If the project to carry out consists of analyzing some data, and the structure of the dataset is not well described, the analysis module will fail, as the code it will write will likely not be appropriate to read the data properly. We recommend that the user describe the data or the problem with as much detail as possible in the input text. For instance, if the project is about analyzing tabular dataset, it is important to write what the data is about and some context. Besides, it is important to describe in detail what the different columns and rows are, and specify things like units, ranges (if available), etc. If the goal is not to analyze some data but instead to solve some problem, it is very important to describe in detail the problem, connections with previous works, and some details about the steps to carry out if known. It is also very important to write explicitly the location of the data (with absolute paths if possible); otherwise, the system will not be able to find the location of the data. When used in end-to-end mode, Denario sometimes may generate papers with very few plots. One can fix this by writing in the input text to generate many plots for each step. Sometimes, the system may generate many similar plots, which will make the paper more like research note than an actual paper. For this, the user can add some text like this: Join plots that are similar. We have also seen cases where Denario fails to read the data, and to continue, it will create its own synthetic one. To avoid these situations, we usually write in the input text Do not create dummy data.. One can also set computational constraints in the input text so that the system is aware of what resources it has when performing the analysis. Examples of this are: You have access to 1 GPU and 10 CPUs; use all the resources when training neural networks. We have observed cases where the system is not able to use particular library (perhaps because it does not know about its documentation or because its API has recently changed). In these cases, the user can explicitly write Do not use library to perform this analysis to avoid that situation. On the other hand, if the user has some piece of code, or library, that may not be known to LLMs, the user can add instructions about its usage in the input text so that the system learns how to use it when analyzing the data. In many occasions, it is necessary to iterate over the input text in order to guide the system in particular direction or to improve the quality of the generated papers. In this case, the user should add details about previous experiments, or directions where the analysis is wanted, in order to achieve the desired outcome. If arXiv URLs are provided in the input text description, Denario automatically downloads the pdf of the corresponding papers. The pdfs are OCRed with mistral ocr lattest and the resulting markdown versions are stored in the local repository. Then, the full markdown versions are sent to o3-mini for summarization and the summary of the papers are appended to the initial input text, providing the necessary contextual information for the subsequent LLM agents. (In the GUI this is done with the Enhance data description button, see Fig. 9.) We refer the reader to the example papers repository7 for comprehensive list of examples, across disciplines, of input text used for different papers."
        },
        {
            "title": "3.2 Idea module",
            "content": "The goal of this module is to generate research ideas given some input text (e.g. describing some dataset). We note that recent work [28] has suggested that AI-generated ideas may be more novel (according to human evaluators), than human-generated ideas. The input to this module is the input text (see Fig. 3) and the output of it is file called idea.md that stores the generated idea. We now describe in detail how this module works. 7https://github.com/AstroPilot-AI/DenarioExamplePapers 10 The Idea module is propose-critique block consisting of two agents: idea maker and Figure 4: idea hater. The system first calls the idea maker agent, that will generate an idea given the input text. Next, the idea is sent to the idea hater, who will critique the generated idea. The idea maker will then take the input text, the generated idea, and the critique and will improve the idea. This process is then repeated several times until the final idea is generated. critical part of scientific research is the generation and refinement of ideas. In any ambitious project, its not enough to simply work on fixed plan creativity and adaptability are needed to explore the best possible directions. However, creativity must be balanced by careful critique to ensure ideas are realistic, impactful, and tightly connected to the available data. To systematize this process, our pipeline includes specialized agents dedicated to idea generation (idea maker) and idea critique (idea hater). In this pipeline, the idea maker agent generates creative and feasible research project ideas based on the input text and any feedback from the idea hater agent. Its goal is to propose diverse approaches for extracting new information, considering the current plan, task, and instructions. However, simply generating ideas is not enough to ensure that only the strongest, most relevant ideas move forward, each proposal must undergo careful evaluation. This is where the idea hater agent steps in, critically reviewing each idea for feasibility, scientific value, and relevance to the dataset. By identifying weaknesses and suggesting improvements, the idea hater ensures that no idea is accepted blindly without scrutiny. Figure 4 provides schematic of the conversation between the idea maker and idea hater agents. This loop can be repeated as many times as requested. This setup mirrors the natural human process of idea development, where individuals brainstorm concepts and refine them through feedback and discussion. Just like in human teams, iteration is key: the idea maker improves its proposals based on the critiques received from the idea hater, leading to cycle of continuous refinement. This multi-agent dialogue helps distill raw ideas into well-formed, robust research directions, ultimately ensuring that the final projects are both innovative and grounded in realistic scientific practice. The concept of multi-agent systems to produce ideas has been motivated by previous works such as [29], who used reinforcement learning between agents, and [30], who showed that large team of agents produced better ideas than single agent. In its current version, Denario has two different implementations (see Section 2.2) for this module: Fast idea. In this case, single idea is generated by the idea maker agent and then improved (or completely changed) after receiving feedback from the idea hater agent. The final idea is obtained after three iterations of the two agents, followed by further iteration from the idea maker agent. This implementation is fast (around 15 seconds with gemini-2.0-flash) and is written in LangGraph. Planning & Control idea. This implementation is carried out with Cmbagent using the planning and control strategy. The instructions passed to the Planning agents are: Idea Generation Planning Instructions Given these datasets and information, make plan according to the following instructions: 1. Ask idea maker to generate 5 new research project ideas related to the datasets. 2. Ask idea hater to critique these ideas. 3. Ask idea maker to select and improve 2 out of the 5 research project ideas given the output of the idea hater. 11 4. Ask idea hater to critique the 2 improved ideas. 5. Ask idea maker to select the best idea from the two. 6. Ask idea maker to report the best idea as scientific paper title, accompanied by 5-sentence description. The goal of this task is to generate research project idea based on the data of interest. Dont suggest performing any calculations or analyses here. The only goal of this task is to obtain the best possible project idea. During the Planning stage, the planner and plan reviewer design the plan for the idea maker and idea hater agents to generate an idea, given the input text needed for the module. To illustrate this implementation, we show below the first two steps of an idea generation plan created by using as input text description of the Perovskite database [31]: Idea Generation Plan Example (First 2 Steps Only) Sub-task: Generate 5 new research project ideas related to the perovskite dataset. Agent: idea maker Instructions: Review the dataset description and attributes. Propose 5 innovative and feasible research project ideas based on the datasets scope and potential. Ensure the ideas are diverse and cover different aspects of the dataset. Consider focusing on different aspects such as material composition relationships, fabrication parameters, device architecture correlations, stability factors, and performance optimization based on the 95 attributes in the dataset. Sub-task: Critique the 5 research project ideas. Agent: idea hater Instructions: Evaluate the feasibility, originality, and potential impact of each idea. Identify weaknesses or limitations in the ideas. Suggest improvements or recommend discarding weak ideas. Evaluate each ideas scientific novelty and whether it leverages the unique comprehensive nature of this manually curated dataset with 42,400+ devices. As seen in this example, with this approach, the idea maker and idea hater agents receive explicit instructions specific to the data or problem of interest. This could potentially improve the quality of the generated ideas. However, this approach is time-consuming and incurs significant cost. With our default LLM choices (we use gpt-4o and claude-3-7-sonnet for idea maker and idea hater, respectively), Planning takes $0.90. This 4 minutes per generated idea, to be contrasted with the 15 seconds of the fast amounts to method that comes at no or significantly lower cost with gemini-2.0-flash. $0.20, while Control takes 3 minutes and costs 1 minute and costs $1 and The reason behind the two different implementations of this module is to provide the user more flexibility when using Denario. For instance, new user may prefer to use the fast idea implementation to get results quicker, while in full production mode, the user may prefer more robust approach by using the curated idea generation. As stated above, this module will generate an idea and store it in file called idea.md. This file is later used as input for other modules. We emphasize that the user can use the idea module to generate an idea, or can provide the idea directly by writing it to the idea.md file."
        },
        {
            "title": "3.3 Literature module",
            "content": "The goal of the literature module is to search the literature to determine whether an idea, in combination with the input text, is new or not. This module takes as input the input text together with the idea (usually in the form of the idea.md file). The module then searches the literature for papers similar to the proposed idea and evaluates whether the project idea is new or not. The input text is taken into consideration when evaluating the idea. We do this because in some research areas, standard techniques applied to new datasets may be seen as novel research. For instance, even if graph neural networks are very standard in deep learning, they may not have been applied to cosmological datasets, and this may be seen as novel in the cosmology research area. The module returns file, literature.md, that contains report on whether the idea is new or not. Denario provides two different implementations of this module: Figure 5: This diagram shows the semantic scholar implementation of the literature module. The module takes as input the input text and the idea. These files are then passed to novelty agent whose task is to determine if the idea is new or not. This agent can output 1) idea new, 2) idea not new, or 3) query. The agent will choose the later if it does not enough information to make yes/no decision. In that case, it will generate query that will be sent to the semantic scholar agent who will search the literature with it. This agent will then retrieve series of papers (including titles and abstracts) and will send this information back to the novelty agent. This process is repeated several times until the relevant literature is well sampled. If no relevant papers are found after number of iterations, the novelty agent will conclude that the idea is new. Once the novelty agent has determined that the idea is new/not new, all the papers found, the queries, and the responses will be sent to summary agent that will write report stating why the idea is new (or not new) and the most relevant papers. Semantic scholar. In this implementation, carried out with LangGraph, we follow the workflow introduced in [5]. The module consist of two agents that collaborate to determine whether an idea, together with the associated data description, is new or has already been carried out. First, the novelty agent is presented with the idea, the data description, and with titles and abstracts of papers already found. The goal of this agent is then to determine whether, with the given information, the paper is new, is not new, or whether it does not have enough information to reach If it determines that the paper is new, or not new, all the information is sent to conclusion. another agent to summarize the results and to write report. This report will be placed on file called literature.md. If the novelty agent determines that it does not have enough information, then it can call the semantic scholar agent which can make query using Semantic Scholar8. This other agent will then make an API call to Semantic Scholar, using the query sent by the novelty agent, and obtain papers (titles, abstracts, URLs, etc) that will be sent back to the 8https://www.semanticscholar.org/ 13 novelty agent for evaluation. This procedure is repeated several times up to maximum number, where the module will determine that the idea is new as it has not found relevant papers after multiple iterations. We show scheme about the workflow of this implementation in Fig. 5. FuturHouses Owl. For this implementation, we just make use of the Owl agent developed by FutureHouse.9 This is specialized agent designed to search the literature and provide an answer to the question: Has anyone implemented this idea before? We refer the reader to the FuturHouse documentation10 for details about this agent. Denario contains both implementations to allow the user to choose among the two, or perhaps better, to run both of them to get to independent opinions. This module will generate the literature report, located in the literature.md file. We note that in the current implementation, this file is not used by any other module. We do this intentionally so that human evaluator decides whether the idea is truly new/original or not."
        },
        {
            "title": "3.4 Methods module",
            "content": "Figure 6: The methods module. The methods module is responsible for producing detailed plan, or methodology, to carry out the project idea. This module takes the input text and the project idea (usually located in the idea.md file), as input. The modules outcome is detailed research plan designed to carry out the idea. This output is saved in file called methods.md. The current version of Denario contains two different implementations for this module: Fast methods. This is simple implementation that takes the input text and the idea.md file and generates methodology. It is implemented in LangGraph and designed to be run with fast models like gemini-2.5-flash (although it supports most LLMs). The prompt used in this case is this: Fast methods prompt You are provided with an input text and an idea for scientific paper. Your task is to think about the methods to use in order to carry it out. Follow these instructions: Generate detailed description of the methodology that will be used to perform the research project. The description should clearly outline the steps, techniques, and rationale derived from the exploratory data analysis (EDA). The focus should be strictly on the methods and workflow for this specific project to be performed. Do not include any discussion of future directions, future work, project 9https://www.futurehouse.org/ 10https://futurehouse.gitbook.io/futurehouse-cookbook/futurehouse-client extensions, or limitations. The description should be written as if it were senior researcher explaining to her research assistant how to perform the research necessary for this project. Just provide the methods, do not add sentence at the beginning saying showing your thinking process Problem or data description: Input text } { Idea: project idea } { Respond with the methods you have generated. Planning & Control methods. This mode uses the Planning & Control strategy of Cmbagent, with the researcher agent in charge of the subtasks. The models chosen by default for the researcher agent in this mode are gpt-4.1 or gemini-2.5-pro. The instructions for planning are these: Methods Module; Planning Instructions Given these datasets, and information on the features and project idea, we want to design methodology to implement this idea. The goal of the task is to write plan that will be used to generate detailed description of the methodology that will be used to perform the research project. Start by requesting the researcher to provide reasoning relevant to the given project idea. Clarify the specific hypotheses, assumptions, or questions that should be investigated. This can be done in multiple steps. The focus should be strictly on the methods and workflow for this specific project to be performed. Do not include any discussion of future directions, future work, project extensions, or limitations. The description should be written as if it were senior researcher explaining to her research assistant how to perform the research necessary for this project. The final step of the plan must be entirely dedicated to writing the full Methodology description. The only agent involved in this workflow is the researcher. In this task, we do not perform any calculations or analyses, only outline the methodology. For this task, the researcher agent of Cmbagent is further instructed as follows: Methods Generation Appended Researcher Instructions Given this information, we want to design methodology to implement this idea. The goal of the task is to develop detailed methodology that will be used to carry out the research project. You should focus on the methods for this specific project to be performed. Do not include any discussion of future directions, future work, project extensions, or limitations. The methodology description should be written as if it were senior researcher explaining to her research assistant how to perform the project. The designed methodology should focus on describing the research and analysis that will be performed. The full methodology description should be written in markdown format and include all the details of the designed methodology. It should be roughly 500 words long. 15 These instructions are appended to the default instructions of the researcher agent. We further constrain nsteps = 4 (maximum 4 steps in the plan). With the default models, at the time of writing this paper, Planning typically costs around $0.20 and takes around 1 minute, while Control takes around 1.5 minutes per step at cost of around $0.10 per step. Thus, with Planning & Control, the total cost of designing the methodology is around $0.50 and takes around 4 minutes. This is to be contrasted with the Fast mode that takes around 15 seconds at no cost to produce the methodology with gemini-2.0-flash. We note that using gemini-2.5-pro for the fast implementation has small cost (around $0.10) and will take around 90 seconds, but typically will produce much more detailed plan to help run the analysis pipeline. As in the case of the idea module, we provide two different implementations to the user to enhance their range of options when executing this module. We note that the curated implementation may not necessarily yield better research plan, while its cost and execution time will be significantly higher than the fast option when run with gemini-2.5-pro. However, we recommend the user try both options for the particular case study."
        },
        {
            "title": "3.5 Analysis module",
            "content": "The goal of this module is to implement the analysis outlined in the methodology. This is done with Cmbagent [25, 26, 27]. The inputs to this module are the input text, the project idea, and the methodology (either created by earlier modules or supplied directly). The output of the module is 1) report on the results obtained, located in file called results.md, and 2) folder called Plots that contains all plots generated. In the current version of Denario, the analysis module relies exclusively on Cmbagent and its Planning & Control strategy for Deep Research orchestration, similar in nature to the workflows of agentic tools like Claude Code or Cursor agent mode. See Fig. 2 for an illustration of this strategy. For the Planning phase, we append the instructions reported hereafter to the planning agents. Analysis Generation Planning Instructions research idea { } methodology } { Given these datasets, project idea and methodology, we want to perform the project analysis and generate the results, plots and insights. The goal is to perform the in-depth research and analysis. The plan must strictly involve only the following agents: involved agents str . } { The goal here is to do the in-depth research and analysis, not an exploratory data analysis. The final step of the plan, carried out by the researcher agent, must be entirely dedicated to writing the full Results section of the paper or report. If this research project involves code implementation, this final step should report on all the qualitative and quantitative results, interpretations of the plots and key statistics, and references to the plots generated in the previous steps. The final result report will be what will be passed on to the paper writer agents, so all relevant information must be included in the final report (everything else will be discarded). As we store the full idea and methodology in the context variable dictionary, the planning instructions contain these explicitly. The context variable involved agents str is string with the names of the agents involved for the analysis generation, which by default are the engineer and researcher agents of Cmbagent. We append the following instructions to these two agents. Analysis Generation Appended Engineer Instructions research idea { } 16 methodology } { Given these datasets, and information on the features and project idea and methodology, we want to perform the project analysis and generate the results, plots and key statistics. The goal is to perform in-depth research and analysis. This means that you must generate the results, plots, and key statistics. Warnings for computing and plotting: - make sure dynamical ranges are well captured (carefully adjust the limits, binning, and log or linear axes scales, for each feature). For histograms (if needed): -Use log-scale for features with values spanning several orders of magnitude. GENERAL IMPORTANT INSTRUCTIONS: You must print out in the console ALL the quantitative information that you think the researcher will need to interpret the results. (The researcher does not have access to saved data files, only to what you print out!) Remember that the researcher agent can not load information from files, so you must print ALL necessary info in the console (without truncation). For this, it may be necessary to change Pandas (if using it) display options. The engineer agent is in charge of writing Python code analysis pipelines. The engineer agent call triggers nested chat (similar to self-debug [32]), of which only the output is kept. This allows for long sessions without hitting LLM provider API token limits. failed code execution may trigger different transitions. If the number of failed executions is lower than nfails, the module would generally transit back to the engineer agent for another attempt. If the failure is caused by missing Python package, the system will send request to an installer agent, which runs bash pip install command before attempting the code execution again. If the number of failed executions equals nfails, transition to the terminator agent is triggered and the session ends. Given the planning instructions reported above, the engineer agent is generally in charge of the bulk of the analysis, while the researcher agent is called in the final step of the workflow to report on the results. For Denario, we append the researcher system message with the following instructions: Analysis Generation Appended Researcher Instructions research idea { } methodology } { At the end of the session, your task is to generate detailed/extensive discussion and interpretation of the results. If quantitative results were derived you should provide interpretations of the plots and interpretations of the key statistics, including reporting meaningful quantitative results, tables and references to material previously generated in the session. The results should be reported in full (not summary) and in academic style. The results report/section should be around 2000 words. The final result report will be what will be passed on to the paper writer agents, so all relevant information must be included in the final report (everything else will be discarded). Each step during control, with gemini-2.5-pro or gpt-4.1 as engineer and researcher, costs around $0.30, amounting to around $2 for plan in 6 steps. The time needed to run this module depends critically on the amount of data, the technique used to analyze it, and the available computational resources. For simple analyses with less than 1 Gigabyte of data, the typical time is 30 minutes on personal computer. The user can (and should) specify which computational resources are ready to be committed to the analysis."
        },
        {
            "title": "3.6 Paper module",
            "content": "This module is in charge of writing scientific paper or report. In its current form, it should be seen as paper draft more than publication-ready paper. This module is entirely written in LangGraph (but for the keyword search, which uses Cmbagent). The module takes as input the input text, the project idea, the project methodology, and the project results. As for the other modules, these files can be either provided by the user or generated by previous modules. The output of this module is set of LaTeX files together with associated pdfs. scheme showing the workflow of this subsystem is shown in Fig. 7. We note that the user has multiple options available when running this module, such as the LLM to use, the journal of the paper, and whether the paper includes citations or not11. Figure 7: This scheme shows the workflow of the paper writing module. Each node represents an agent (or group of agents) in charge of task. The preprocess agent is in charge of reading the input files and performing some basic operations, such as identifying duplicated plots. Later, series of agents will identify keywords and write different sections of the paper draft. Next, an agent will process the plots, generating captions for them and outputting preliminary version of the paper draft. Next, an agent will refine the results section to improve references to the plots and to polish the writing on it, generating second version of the paper draft. Next, an agent will place citations in different sections of the paper, producing third and fourth final version of the paper draft. The process starts with an preprocess agent reading the input files, which will be kept in memory until it has finished writing the entire paper. We note that it is easy to include more files in case the paper writing requires more information. For instance, the user may provide an additional file with details about the data, references, literature report, etc. The preprocess agent performs other tasks, such as counting the number of plots, identifying and removing repeating plots, and creating temporary files. Next, given the texts read above, another agent will select set of keywords that characterize the paper (see Section 3.6.1). After that, another agent will choose title for the paper and write the abstract. Next, several agents will write the papers different sections: 1) Introduction, 2) Methods, 3) Results, and 4) Conclusions. For each section, an agent is passed the input texts and asked to write the assigned section for the paper. We note that the sections are written sequentially, with each section passed to the next agent to improve the coherence throughout the paper. We perform self-reflections for some sections to improve the quality of the generated text. Once these sections are written, another agent is asked to take all the plots available and generate captions for them. This is achieved by using the multimodal capabilities of the underlying LLM. We note that the captions are generated taking into account the input files to improve the context and coherence of the captions. Next, another agent is tasked with inserting the figures and their generated captions into the results section. This is done in batches of 7 figures12, as the LLM tends to forget inserting some figures when there are many figures in the prompt. At this point, the system will generate first LaTeX file, compile it, and generate the corresponding pdf. After this, another agent is asked to rewrite and polish the results section so that the text describes and refers to the added figures. Next, version 2 of the paper, with the polished results section, will be generated. We do this because we have observed the module failing at this point and we use these versions as checkpoints in case we need to rerun the module. This module can be run with the option to add citations to the paper. If run in that mode, the text inside the different sections is passed through Perplexity agent in order to get citations (with arXiv 11For research notes or similar, the user may not find it useful to spend time and money adding citations. Given the cost, we tend to add citations only to the papers that have been reviewed and filtered by human experts. 12This is in principle hyperparameter. We have noticed that with current LLMs, including more than 10 figures tends to result in errors, omissions, etc IDs) to the relevant sentences. We use the Perplexity API with the sonar-reasoning-pro model for this. script is then used to fetch the BibTeX information from arXiv, which is appended to the LaTeX. At this point, version 3 of the paper is generated and saved to file. Finally, another agent is asked to make final pass through the different sections of the paper in order to improve the clarity of the text and to fix LaTeX errors. After this, version 4 of the paper is produced. We note that while multiple agents are always checking for LaTeX errors, and some agents are designed to take the LaTeX errors and fix the text, we find that mistakes can still occur, and that may cause problems with the document compilation. E.g. misplaced citations, tables not displayed properly, problems with symbols like and &, and %. For that reason, the paper writing module does not produce single version of the paper but four different ones, as progress is made. If problem arises, and the agents themselves are not able to fix it, then, human can try to fix it manually from of these checkpoints. 3.6.1 Keyword Search module Cmbagent provides method to get the keywords which best characterize given input text, such as the abstract of paper. The Denario Paper module makes use of this functionality to find the correspondent keywords of the project by passing as input text the idea and methodology. There are three keyword lists that can be searched by this method: UNESCO Nomenclature. This taxonomy has three levels for classifying science and technology domains, subfields and specific areas. We do the keyword search in four steps. First, the keyword agent is provided with the list of domains (2-digit codes) and is asked to select at most 3 domains. Second, for each selected domain, the keyword agent is asked to select at most 3 subfields. Third, for each subfields, the keyword agent is asked to select 3 specific areas. Domains, subfields and specific areas are aggregated into one list and in the last step the agent is asked to select the most relevant keywords where is the number of keywords requested by the user. The original UNESCO document is publicly available.13 In principle, this list of keywords is broad enough to be used in all scientific domains. Nevertheless, it is from 1988 and recent technical vocabulary is sometimes lacking. AAAI Keywords. Here, the keyword agent is provided with the full list of AAAI (The Association for the Advancement of Artificial Intelligence) keywords and asked to select the most relevant keywords where is the number of keywords requested by the user. The AAAI list of keywords is publicly available.14 This list is relevant for AI-focused research and is updated yearly. AAS Keywords. The keyword agent is provided with the full AAS (American Astronomical Society) list of keywords and asked to select the most relevant keywords where is the number of keywords requested by the user. This list should only be used for astrophysics related papers, and can be consulted in the AAS journal page."
        },
        {
            "title": "3.7 Review module",
            "content": "This module is in charge of reviewing the generated paper and providing report about what is good, interesting, and innovative in the paper, but also about the bad things, flaws, etc. The input to it is pdf document together with the input text (optional). The module will then break down the paper into set of images, one image for each page, that will be passed to multi-modal agent along with the input text (usually gemini-2.5-pro). The agent will then review the paper and write report about it, highlighting the good but also the bad things. The report will be placed in file called referee.md. The purpose of this module is to help humans review and filter papers. While some of the recommendations we have seen from executing this module are related to ways to describe and connect things, the module is also able to provide much more in-depth suggestions, such as logical flaws or underlying assumptions that invalidate reasoning or conclusions. The report generated by this module can be used to improve the quality of the generated papers via human intervention or automatic agent refinement. The prompt used for this module is found below. 13https://unesdoc.unesco.org/ark:/48223/pf0000082946 14https://aaai.org/conference/aaai/aaai-26/keywords/ 15https://journals.aas.org/keywords-2013/ 19 Figure 8: This diagram shows how the reviewer module works. The input to the module is the input text together with the PDF of the document to be reviewed. The first step is to transform the pdf document into png images; each image corresponds to different page of the paper. Next, the images and the input text are passed to reviewer agent in charge of understanding and evaluating the document. The outcome of the agent is report with the positive and negative aspects of the document. Reviewer prompt type:text, text: You are scientific referee. Below, you can find scientific paper { written in latex. Your task is to read and understand the paper. Next write detailed report about the good/interesting aspects of the paper but also bad things, failures...etc. For the bad things, please provide comments on what would be needed to do in order to improve it. Note that you may be reviewing an AI-generated paper, so the author may not be human, and keywords may be missing. No need to mention those. - Find all flaws in the paper - Find things that may not be done correctly - Identify places where further revisions would make the paper better - Check carefully that there is enough evidence in the paper to support the conclusions - If the results are not good, reason whether this is surprising thing or just it used the wrong strategy and failed. If the latter, the paper should be considered bad. Try to judge whether the paper will be worth publication or not. Give score from 0 (a very bad paper) to 9 (an amazing paper). For bad papers, give low score. **Respond in exactly this format**: begin{REVIEW} <REVIEW> end{REVIEW} In REVIEW , put your report. type: image url, , } { image url: for image b64 in state[referee][images] { url: fdata:image/png;base64,image b64 }}"
        },
        {
            "title": "4 Usage",
            "content": "In its current form, Denario can be run in two different ways: Python API Graphical User Interface (GUI) 20 The Python API can be run in Python scripts or Jupyter notebooks, while the GUI can be run either locally or through web application. In the following subsections we detail the usage of each case."
        },
        {
            "title": "4.1 Python API",
            "content": "The main entrypoint of Denario is its Python API, which allows the user to call the different modules. For that, the user firstly needs to install the software locally. Denario is publicly available on GitHub16 and pypi,17 and can be easily installed by running the command: pip install denario. We refer the reader to the documentation18 for further details on how to install the package. typical Python script to perform end-to-end research is this: from denario import Denario, Journal # Initiate Denario by setting the working directory den = Denario(project_dir=\"project_dir\") # Set the input text with data description input_text = \"\"\" Analyze the experimental data stored in /path/to/data.csv using sklearn and pandas. This data includes time-series measurements from particle detector. \"\"\" den.set_data_description(input_text) # Generate research idea from the input text den.get_idea() # check whether the idea is new or not den.check_idea() # Generate research plan to carry out the idea den.get_method() # Follow the research plan, write and execute code, make plots, and summarize the results den.get_results() # Write paper for the APS journal den.get_paper(journal=Journal.APS) # Review the paper den.referee() The Python API also allows to set research idea and methods provided by the user: # Set the idea given by text file in the project folder den.set_idea() # or providing an input string den.set_idea(\"Write research idea here\") # Similarly, set the methods given by text file in the project folder (or providing an input string) den.set_method() It is possible to select between the different backends, Cmbagent and the default fast method based on LangGraph for idea and method generation: # Generate an idea using the fast method based on the LangGraph implementation (default option) den.get_idea(mode=\"fast\") # Generate an idea using the Cmbagent backend den.get_idea(mode=\"cmbagent\") The code above can be executed in several ways. For example, the lines can be saved in Python script and run using python script.py. Alternatively, the code can be executed interactively in environments such as Jupyter notebook or Google Colab. For more details on the implementation and syntax of the Python API, please refer to the Denario documentation19. 16https://github.com/AstroPilot-AI/Denario 17https://pypi.org/project/denario/ 18https://denario.readthedocs.io 19https://denario.readthedocs.io"
        },
        {
            "title": "4.2 Graphical User Interface",
            "content": "4.2.1 Launch locally Denario also provides Graphical User Interface (GUI) to facilitate its usage, which is shown in Fig. 9. It can be installed from pypi with the extension pip install denario[app]. The GUI can be then launched running denario run, which will open tab in the browser with the application. Its repository is also publicly available at GitHub20. Figure 9: This figure shows the Graphical User Interface (GUI) of Denario. In the upper-left part, the user can set the API keys for the LLMs. Below that part, the user can upload data and download the files generated by Denario. In the central part, the user can choose which module to run and can tune the options available for it (e.g. LLM model). If the user wants to perform end-to-end research, she/he will have to run the different modules sequentially. In the GUI, the user can set the API keys for the LLM models to use, upload the data to be analyzed and other input files, and download the files generated by Denario. The input text can be passed either by typing it directly into the GUI, or by uploading file with the text. The GUI includes several tabs for each of the Denario modules, where the user can choose which one to run. It is possible to provide input files too through these tabs. For instance, if the user wants to generate research methodology, but wants to provide the idea to carry out, he/she should first select the Idea tab and upload the research idea there. Next, the user should go to the Methods tabs and run the module there. After running it, Denario will generate the research methodology that can be used for other modules, such as the analysis one. The required inputs and the outputs generated by each module can be found in Table 1. 4.2.2 Web application The GUI discussed above is meant to be used locally, e.g., on personal computer. The advantage is that the user can leverage local resources such as GPU or multiple CPUs, while potential barrier is that the user needs to install Denario and its dependencies locally. To circumvent this and to provide quick way to experiment with Denario, we also provide users with the option to run it as web application. In this case, no installation is needed and the user can start using Denario immediately. On the other hand, the free resources available to the user (e.g., data storage, CPUs, GPUs) may be limited. We currently provide two ways to access it online: 20https://github.com/AstroPilot-AI/DenarioApp 22 HuggingFace Spaces app. demo of Denario is hosted as web app on HuggingFace Spaces21. The interface is essentially the same as the local GUI (see Fig. 9). Cloud-hosted app. Denario can also be deployed on general cloud platforms, allowing users to access and run it directly through browser. Please refer to the Denario documentation for up-to-date details on the available deployment options."
        },
        {
            "title": "5 End-to-end research examples",
            "content": "In this section, we illustrate the capabilities of Denario by showing and discussing multiple papers fully generated by it. To demonstrate the versatility of the system, we show papers generated on very diverse set of disciplines, including astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience, and planetary science. Furthermore, Denario can also be used for interdisciplinary research. We illustrate this by showing paper that applies techniques from deep learning and quantum physics to cosmological data in Sec. 5.13. Most of the generated papers have little to none human intervention in the generation process, besides providing and adjusting the input prompts and data. Some of them have been tested varying the input prompt with different levels of detail. For instance, in the paper discussed in Sec. 5.12 we showcase human-in-the-loop example, where the prompt is iteratively improved using Cmbagent and human feedback. In each of the subsections below, we provide details about the data used, the input text, the generated paper and briefly discuss the positive and negative aspects of the generated paper. The reader can find all generated files, plots, text, etc for all papers below and many more at the example papers repository22."
        },
        {
            "title": "5.1 Mathematical Physics. High-level analysis and abstraction of complex",
            "content": "structures: the physics of non-linear PDEs central challenge of research in physics lies in interpreting complex results and reducing them to framework from which general principles can be drawn. It is this capacity for abstraction that enables scientific advancement. The problem we posed Denario to solve is the following: we would like to understand the structure of the solutions to the Burgers equation in fluid dynamics for different values of the viscosity ν and initial conditions. In 1-dimension, the Burgers equation is: t + x = ν 2u x2 (1) 5.1.1 Background: PINNs Our tool to solve the Burgers equation is neural network algorithm called PINNs (physics-informed neural networks) in its implementation in the package neurodifeqq23. The PINN will find space of solutions for different values of the viscosity and initial conditions. This creates so-called latent space. Physics-Informed Neural Networks (PINNs), first introduced in the works of Dissanayake and Phan- [34], have emerged as powerful tool for solving partial and ordinary Thien [33] and Lagaris et al. differential equations (PDEs/ODEs). Raissi et al. [35] proved this approach to be valid on wide variety of challenging physical problems, while other researchers have made significant advancements in applying neural networks to PDEs and ODEs, including notable examples such as [36, 37, 38]. In this paradigm, one typically trains distinct neural network for each unknown function in the governing equations. The training process involves minimizing loss function that encodes the squared residuals of these differential equations, thereby embedding the physics directly into the learning objective. In the PINNs setup, we use one NN for every unknown function appearing in the differential equations. We call these objects ψN (xµ) and they represent the approximate solution given by the NNs for different points along the independent variable and possibly different values of parameter or boundary conditions (all of them are represented by xµ). The training process involves minimizing the loss function, typically defined as the residuals of the differential equations squared, summed across all the sample points of the input domain: 21https://huggingface.co/spaces/astropilot-ai/Denario 22https://github.com/AstroPilot-AI/DenarioExamplePapers 23https://github.com/NeuroDiffGym/neurodiffeq 23 = (DψN (xµ))2 + λLadd (2) where represents differential operator that will depend on the differential equations that we are considering. Here Ladd represents any additional contribution to the loss that can be added to give extra information to the model during training. The weight of this piece is controlled by λ. (cid:88)batch While traditional numerical methods often outperform PINNs in terms of computational efficiency and accuracy, recent developments in PINNs are offering unique advantages. One key benefit is the ability of PINNs to learn the parametric dependence of the solutions to different initial or boundary conditions, or even different values of parameters appearing in the equations. In contrast with traditional numerical methods, this allows us to train the model on wide range of different parameters at once, whereas traditional solver must be rerun for different values of initial/boundary conditions or parameters. This eventually will make PINNs setup much more efficient compared with numerical method, since obtaining solution for particular value of parameter just requires the evaluation of the model. This efficient evaluation permits us to treat otherwise untreatable problems. One of which is the so-called inverse problem [39, 40]. This consists of recovering free-form function or parameter appearing on the differential equations just by considering boundary data. This capability is extremely useful for this work, since we will use it to recover the viscosity of Burgers equation by considering the initial and final time configuration of solution. Another key advantage of PINNs is their capability of learning general highdimensional space of functions called the latent space. The final solution would just be combination of these functions in certain way. This latent space will, in general, depend on the independent variables of the problem, but it can also be learned for more parameters, like different boundary conditions or different values of parameter appearing in the equations. In this work, we learn latent space of Burgers equation for different values of the viscosity ν in the so-called Multihead training. We will explain this framework in the following subsection. Although we have presented the two main advantages that we are going to exploit throughout this work of using PINNs instead of traditional numerical methods, there exist many more. PINNs are meshfree, allowing for obtaining solution at any point lying inside the training set. Moreover, they can also be used to find new solutions on stiff regimes using their ability to transfer learning [40]. 5.1.2 Multihead training Multihead (MH) training consists of breaking the neural network into two pieces: the body, which will learn higher-dimensional representation (latent space) of the solutions depending on the values of different parameters or boundary conditions; and the head, which will take care of learning how to project from this latent space to the actual solution. This setup has already been proven useful to transfer learn solutions of differential equations to stiffer regimes. However, we use MH training to explore the functions of the latent space when the head is forced to present linear response. This will tell us valuable information about the functional basis that the model is learning to solve the problem. The NN gives us the final solution ψN (xµ) composed of two parts Body: this itself is going to be NN which is going to learn high-dimensional space called the latent space of solutions. We will denote it as Hi(xµ), where runs from 1 to the total dimensionality of this space. In general, this NN must be large enough such that it can learn proper representation of the solutions in this higher-dimensional space. In general, this body will take as inputs the independent variable plus variations of parameters or different boundary conditions of the equations, denoted by xµ. After passing them through the model, it will return us the latent space Hi(xµ), which will be composed of components. Head: this component is also going to be NN and will take care of processing the latent space component and combine them on particular way such that the final result is going to be the desired approximate solution to the differential equation ψN (xµ). In general, this model is much smaller than the body, since only few operations on the latent space will be required to produce the final solution. Generally, the head will take the latent space of dimension d, and combine its components such that the final output is dimension 1. We will have one head for each of the values of the free parameter or boundary conditions that we are considering. Thus, each of them will learn particular set of weights and biases that will project down the latent space to the solution for particular value of the parameters and boundary conditions chosen. 24 At the end, the solution can be expressed as ψN (xµ) = headW [Hi(xµ)] where head denotes the response of the head, and the set of its weights and biases. The model is trained as follows. Once one approximated solution is obtained, we can compute its loss function just by evaluating the residues of the differential equations squared (2). The total loss will be the sum of each of the individual losses across all the heads. We can then train the model by optimizing this loss function. At the end, the model will solve simultaneously all specified differential equations problems. It is important to note that, since the body is shared across all NNs, it will capture global properties of the solutions for different values of free parameter or boundary conditions. 5.1.3 Latent space structure with Denario Our question to Denario then was to find the structure of the latent space generated by training on Burgers equation for different initial conditions and values of the viscosity. The reason behind it was that the structure of the latent space might reveal the actual physics determining the structure of the solutions to this equation. So we asked Denario to explore the structure of the latent space of solutions, having given it the following results: we provided an array that contained the latent space solutions as function of ν, space, and time. The input text passed to Denario was this: Input text We haved PINNs to solve Burger equation in 2 dimensions for fixed value of the viscosity with multihead set for different initial conditions. The latent space is located in this file /Users/fvillaescusa/Documents/Software/Denario/Project_turbulence/data_for_Paco_turbulence _bundle.npy To read the data, you can do: import numpy as np fin = /Users/fvillaescusa/Documents/Software/Denario/Project_turbulenceV1/ data_for_Paco_turbulence_bundle.npy data = np.load(fin) data is now [101,103,25,13] matrix, where the first axis represents the x-coordinate, the second axis is time, and the third component represents the viscosity. The fourth axis is organized as follows: - first three components are the mesh of space, time and the viscosity, respectively - the rest (10 last components) the latent space Perform detailed study of the latent space structure. We are interested in knowing if there is some pattern or simple way to describe its properties. We have run Denario end-to-end from the above prompt to generate final article. The generated paper, titled Geometric Structure of PINN Latent Space for Burgers Equation: Low-Dimensional Manifolds and Initial Condition Encoding, can be found in the Appendix A.1. In this paper, Denario performed detailed analysis of the given latent space of Burgers equation. It applied common techniques of dimensionality reduction on the provided data set, mainly Principal Component Analysis (PCA) and subspace similarity measures. Although this analysis is quite standard, it includes some points that were surprising. The first one was detailed analysis of the centroids of the manifolds, computing their PCA individually and discovering that there was only one dominant component when varying the viscosity. Then, it performed similar analysis on the 3-dimensional principal subspaces for each value of the viscosity. This analysis included subspace similarity between these manifolds and the dot products of these vectors. The conclusion was that these manifolds were almost parallel to each other, and that they described up to 97.67% of the variance. At last, it analyzed the global structure of the latent space using the previously mentioned techniques. It concluded that the PINN is learning 3-dimensional manifold to describe solution for each value of the viscosity. Variations of this parameter are encoded as translations of this 3-dimensional space across 1-dimensional sub-manifold. We would also like to remark that Denario produced unexpected results at one point. Throughout the whole paper, it is remarked that the data is obtained for different initial conditions. However, this is not true, and the index that was treated as different initial conditions corresponded to different values of the viscosity. This confusion was due to 25 misleading prompt on our side. In this respect, Denario showed robustness in that it followed the prompt instructions and did not simply try to repeat what is in the literature. As summary, we would like to remark on the analysis that Denario has performed on the data, not as novel and groundbreaking job, but as tool for researchers to unveil future directions that are worth investigating in detail."
        },
        {
            "title": "Asteroid Belt",
            "content": "The asteroid belt preserves fossil record of the solar systems formation and dynamical evolution. We compiled dataset of over 35,000 main-belt asteroids, integrating orbital parameters, diameters, and spectral types. These data were obtained from wide range of sources, including the Lowell dataset [41], the Asteroid Lightcurve Database (LCDB) [42], DAMIT [43], mp3c [44], and the Python tool classy [45]. We then asked Denario to analyze the data. The input text we used was this: Input text We have some data about asteroids. The data is located in /mnt/ceph/users/fvillaescusa/AstroPilot/Asteroids/data Each asteroid is identified by unique number. For example, 1 corresponds to the first discovered asteroid, Ceres. All CSV files share common structure: Column 1: Asteroid identification number Column 2: Corresponding physical or orbital property These files are recommended to be opened with the Python package pandas. File Descriptions: asteroid name.csv: number + name of the asteroid asteroid diameter.csv: number + diameter of the asteroid in kilometers asteroid semimajor axis.csv: number + semimajor axis of the orbit in astronomical units (AU) asteroid eccentricity.csv: number + eccentricity of the orbit asteroid inclination.csv: number + inclination of the orbit in degrees asteroid arg peri.csv: number + argument of periapsis of the orbit in degrees asteroid long asc node.csv: number + longitude of ascending node of the orbit in degrees asteroid spin period.csv: number + spin period of the asteroid in hours asteroid obliquity.csv: number + obliquity of the asteroid in degrees (Obliquity is defined as the angle between the spin vector and the orbital angular momentum vector.) asteroid type.csv: number + spectral type of the asteroid asteroid family.csv: number + family name of the asteroid (The family name corresponds to the largest member of the family) asteroid age.csv: number + age of the asteroid in gigayears (Gyr) Please come up with an interesting project for PhD thesis. Use state-of-the-art methods to analyze the data. For every step, make lots of plots and save the data you generate, as it may be used for other steps. When writing the code, write some lines to indicate whether the execution was successful or not. Join plots that are similar. Do not create dummy data. You have access to 128 cpus; for computationally heavy tasks, try to use all of them. 26 The generated paper, titled The Spatial Architecture of the Main Asteroid Belt: Size, Composition, and Dynamical Gradients, can be found in the Appendix A.2. By applying density estimation, clustering (DBSCAN and GMM), and predictive modeling, Denario mapped spatial variations in size and composition, identified asteroid families, and examined the influence of Jupiters resonances on the belts structure. particular strength of the paper lies in its ability to automate the quantification of compositional gradients across the belt. This paper reveals an important compositional trend: the dominant composition shifts from S-type in the inner solar system to C-type in the outer solar system. By mapping the changing fractions of spectral types as function of semimajor axis, it offers an accessible, up-to-date reference that will be useful for researchers studying planetesimal formation and solar system evolution. The clustering analyses also effectively reproduce known asteroid families, confirming the AIs capacity to extract meaningful dynamical structures from complex data. However, while these results are scientifically sound, they are not fundamentally new. The compositional gradient and dynamical families are well-established in the literature. The observed trend of increasing average asteroid size with distance from the Sun is also correctly noted as likely driven by observational bias rather than genuine physical gradient. One area for improvement is the depth of interpretation. While Denario correctly applied clustering and density estimation, it stops at describing known structures without critically assessing the potential for new dynamical groupings that might hint at unrecognized collisional families or early migration signatures. This illustrates broader limitation: although AI can uncover patterns efficiently, it currently lacks the domain intuition to pose new hypotheses or recognize subtle, novel features without human input. Similarly, the papers predictive modeling using only orbital data yielded limited success, appropriately acknowledging the role of stochastic collisional histories. But it could have gone further by discussing how including additional parameterssuch as albedo, spin state, or detailed spectral datamight improve model performance. On the other hand, in its current form, Denario is not designed to do this kind of critical thinking, so perhaps it is not surprise that it does not perform well here. It is also worth noting the experience with another AI-generated paper (not included here, but available in the example papers repository24), which focused on spin-orbit coupling effects. In that case, Denario produced figure that appeared to show features not previously discussed in the literature. However, Denario itself did not recognize or highlight these potentially novel patterns, and other figures in that paper were poorly prepared, limiting its usefulness. This example underscores both the promise and the current limitations of AI approaches: when handling large, complex datasets, AI can be more efficient than humans at surfacing candidate phenomena, but human expertise is essential to interpret, validate, and refine these discoveries. With careful collaboration, such humanAI iteration could focus attention on these features in subsequent analyses, potentially leading to truly novel scientific breakthroughs. Overall, while this paper does not claim any fundamentally new discovery, that is not significant flaw. It demonstrates that Denario can systematically confirm and clearly document established knowledge, producing scientifically valuable and well-visualized results in an efficient, reproducible manner. As the era of big data arrives with observatories like Very Large Telescope (VLT) [46], the Large Synoptic Survey Telescope (LSST) [47], and The Extremely Large Telescope (ELT) [48], tools like Denario will become indispensable in planetary and exoplanetary sciencehelping researchers sift massive datasets, probe early solar system conditions, and advance our understanding of the origins of life both here and beyond."
        },
        {
            "title": "Outcomes in the United States",
            "content": "Health services research is multidisciplinary field that studies healthcare utilization, access, delivery, and financing, with the goal of improving the quality, efficiency, and equity of healthcare. Combining insights from medicine, public health, economics, and statistics, health services research aims to inform decisions by providers, payers, policymakers, and patients. To evaluate Denarios capability to design and perform project in this field, we used the publicly available datasets from the National ART Surveillance System25. These annual datasets, published in the United States by the CDC (US Centers for Disease Control and Prevention), contain clinic-level aggregated data on the utilization and success rates of approximately 98% of all assisted reproductive technology (ART) services in the United States. Originally mandated by the US Congress to provide transparency and information to patients seeking fertility treatments, these datasets have been extensively used to evaluate the quality, efficiency, and distribution of fertility services. 24https://github.com/AstroPilot-AI/DenarioExamplePapers 25https://www.cdc.gov/art/php/nass 27 More specifically, we provided Denario with the three most recent years available for NASS (20202022) and description of the dataset and its variables. We then instructed Denario to suggest and develop potentially interesting venues of scientific investigation. The exact input text we provided it with is this: Input text The Final Assisted Reproductive Technology (ART) Success Rates dataset is part of the CDCs National ART Surveillance System (NASS) and provides clinic-level success rate data for proceIt includes information from U.S. fertility clinics on dures conducted between 2020 and 2022. ART cycles, distinguishing whether patients used their own eggs or donor eggs, and whether ART was their first treatment or follow-up. The dataset presents cumulative success rates (including embryo transfers over 12 months after retrieval) for own-egg patients by retrieval and transfer, and non-cumulative rates for donor-egg cycles. It also includes statistics like average transfers per retrieval and retrievals per live birth. These dataupdated most recently in December 2024 and made public in early 2025offer insights into clinic performance and national ART outcomes. Among the datasets relevant columns are: ClinicID unique identifier linking this Success Rates dataset to other ART data (Services, Characteristics, Summary). Reporting Year the calendar year. Age Group patient age brackets (e.g., <35, 3537, 3840, etc.). Live Birth per Intended Retrieval percentage of intended egg retrieval cycles resulting in live births within year. Live Birth per Transfer percentage of embryo transfer cycles resulting in live births within the same reporting window. Live Birth per Actual Retrieval reflecting actual retrieval procedures conducted. Avg Transfers per Intended Retrieval average number of embryo transfers per intended retrieval. Avg Intended Retrievals per Live-Birth average number of retrieval cycles needed to yield live birth. Donor Egg Live Birth Rate percentage of donor-egg embryo transfer cycles leading to live births. Clinic Export Variables additional metrics such as clinic location, number of cycles, and whether it was first-time ART cycles. These columns capture both performance and efficiency metrics, enabling comparisons across clinics and patient groupsown-egg vs. donor-egg, first-time vs. any-time ART, age categories, and aggregated national data. The description of the columns, together with their data type is this: Column Name Description Year LocationAbbr LocationDesc FacilityName MedicalDirector Address City ZipCode Phone Clinic Status Reporting year Abbreviated state for ART clinic location State for ART clinic location ART clinic name or national data indicator Name of medical director verifying data Address for ART clinic location City for ART clinic location Zip code for ART clinic location Phone number for ART clinic Operating status for ART clinic, e.g. open, closed, reorganized API Field Name Data Type number text text text text text text text text text year locationabbr locationdesc facilityname medicaldirector address city zipcode phone clinic status 28 Type Topic Question Filter Breakout Category Breakout Data Value Data Value num Type description, e.g. patients using their own eggs, patients with no prior ART using their own eggs Topic description, e.g. clinic services and profile, patient and cycle characteristics, success rates: patients using donor eggs, success rates: patients using own eggs, or summary Question description, e.g. specific patient and cycle characteristic or specific success factor Filter description, e.g. infertility diagnosis Stratification grouping, e.g. egg/embryo type, age group of patient Stratification value, e.g. eggs, <35 years Data value, e.g. number, percentage, or fraction within the breakout, question or subtopic Data numeric values only, e.g. number, percentage within the breakout, question or subtopic Symbol used to flag footnotes, if applicable fresh embryos fresh FootData Value note Symbol Data Value Footnote Footnote description, if applicable Cycle Count ClinicId TypeId TopicId QuestionId FilterId BreakOutCategoryId Stratification grouping identifier BreakOutId GeoLocation Total number of ART cycles ART clinic identifier Type identifier Topic identifier Question identifier Filter identifier Stratification value identifier Location code for ART clinic location to be used for geocoding type topic question filter breakout category breakout data value text text text text text text text data value num number footdata value note symbol data value footnote cycle count clinicid typeid topicid questionid filterid breakoutcategoryid breakoutid geolocation text text number number text text text text text text point in here data located /mnt/ceph/users/fvillaescusa/AstroPilot/Medicine/FertilThe is ity1/data/art data 2020 2024.csv. Please come out with interesting ideas for PhD student to analyze this data and learn some insight from it. Use state-of-the-art methods to analyze the data. For every step, make lots of plots and save the data you generate, as it may be used for other steps. When writing the code, write some lines to indicate whether the execution was successful or not. Join plots that are similar. Do not create dummy data. You have access to 128 cpus; for computationally heavy tasks, try to use all of them. The generated paper, titled Characterizing the Variability and Correlates of U.S. ART Clinic Performance During the COVID-19 Pandemic (2020-2022), can be found in the Appendix A.3. In this paper, Denario characterized the year-to-year variability in success and efficiency metrics of ART clinics, such as live birth rates per retrieval and average retrievals per live birth, stratified by patient age and egg source (own vs. donor). The dataset was evaluated using standard statistical analyses, such as the coefficient of variation, to analyze fluctuations in success rates, ordinary least squares regression models, and Spearman correlation to characterize associations between variability and clinic volume. While the analyses were relatively straightforward and would likely need to be redone using other statistical tools, the contextualization and characterization of the research question were remarkable. Despite the recognized importance of inter-clinic variability in outcomes, little attention has been given to the characterization of year-to-year intra-clinic fluctuations in ART clinic performance, particularly in the context of major systemic shock like the COVID-19 pandemic. Denario contextualized the research by not only evaluating an overlooked topic in the literature, but also by realizing the impact of the global pandemic on healthcare systems through disruptions like clinic closures, supply chain issues, and shifts in patient behavior. The final manuscript generated by Denario had some limitations on the presentation of the results and in the citations to the literature. The results and overall content of the manuscript were significantly more technical than the standard in the field, which generally focuses more on the discussion and implications of the results. Statistical analyses were limited to standard tools such as ordinary least squares and correlations, which (as recognized in the manuscript) would need to be re-evaluated given 29 the distributional properties of the variability measures. Another limitation was the fact that Denario used only open-access manuscripts available at preprint repositories, such as the arXiv. While the use of preprint repositories is common practice in most technical areas, including economics, their use is still limited in the field of health services research and most areas of medicine. For this reason, the references included in the final manuscript generated by Denario were not necessarily applicable and were, at best, marginally relevant to the sentence in which they are used. Nonetheless, despite the limitations described above, the research conducted by Denario was genuinely engaging and interesting, highlighting an understudied topic in the area and contextualizing it in novel way. This manuscript not only identifies promising directions for future investigation but also includes methodology and results that would be useful resources for researchers in the field to further their investigations."
        },
        {
            "title": "5.4 Chemistry. Molecular dynamics simulations of self-assembling peptides",
            "content": "Self-assembling peptides (SAPs) are promising candidates for biomedical applications such as tissue engineering, wound healing, and drug delivery. Their modular structure and biocompatibility make them especially attractive for engineering soft materials with targeted functions. Understanding the relationship between structure and properties in SAPs is key to unlocking this potential. To investigate this, we tasked Denario with analyzing 1200 ns all-atom molecular dynamics (MD) trajectory containing 30 copies of known self-assembling pentapeptide: KYFIL. We conducted the study over three rounds26, each time increasing the level of contextual information provided in the input text. In the first round, we instructed Denario on which Python tools to use for trajectory analysis but gave no domain-specific context. Denario correctly identified the system as protein but misinterpreted it as single large molecule rather than 30 individual peptides. As result, it analyzed residues in isolation, failing to recognize that they were equivalent across different peptide chains. This led to an overemphasis on per-residue metrics like dihedral angles or motif detection, rather than ensemble behaviors across peptides. Consequently, key phenomena like peptide aggregation and clustering were missed. In the second round, we added system description and research abstract to provide context. With this information, Denario identified more appropriate analyses related to self-assembly, such as contact mapping and cluster size distributions. While the outputs were more relevant, they remained conventional and did not go beyond standard MD analysis pipelines. However, the automation of routine essential tasks was already valuable outcome. In the third round, we explicitly posed hypotheses and research questions. Specifically, we suggested modeling peptide clusters as graphs and proposed several graph-theoretical analyses, along with practical motivations for each. The input text we used for this round is this: Input text We have data from molecular dynamics numerical simulation. The data is located in /mnt/- ceph/users/fvillaescusa/AstroPilot/Project chemist1/data. This trajectory contains an all atom molecular dynamics simulation of 30 copies of the self-assembling pentapeptide, K-Y-F-I-L. The simulation starts with the peptides fully separated and over the course of the simulation the peptides form noncovalent interactions with one another to form an aggregate. Motivation: The development of biomaterials that mimic the native extracellular matrix (ECM) of brain tissue represents promising frontier for neural repair and regeneration. Injectable hydrogels made of short, self-assembling peptides offer promising platform for the delivery and directed differentiation of therapeutic stem cells. Despite the growing interest in this field, the rational design of self-assembling peptide sequences remains largely empirical. Our research aims to address this gap by computationally modeling library of rapidly assembling pentapeptides for injectable delivery (RAPID)-a family of C-terminally amidated, five-residue peptides engineered to self-assemble into shear-thinning, self-healing hydrogels. These RAPID hydrogels have demonstrated the potential to improve cell survival upon minimally invasive delivery. To better understand the mechanisms behind assembly behavior, we examined KYFIL, pentapeptide known to self-assemble, using molecular dynamics (MD) simulations to determine the molecular scale behaviors that distinguish gelling from non-gelling sequences. 26In other words, we generated papers from slightly different prompts. 30 The folder contains two files: 1. stripped.nc: the trajectory file which contains the coordinates as function of time 2. stripped.parm7: contains information about what each of the atom types are (which coordinates belong to which molecules). The data can be read as: import MDAnalysis as mda # Load the AMBER topology and trajectory topology_file = system.prmtop # Replace with your actual .prmtop file trajectory_file = trajectory.nc # Replace with your actual trajectory file # Create the Universe object = mda.Universe(topology_file, trajectory_file) # Print some basic info print(\"Number of atoms:\", len(u.atoms)) print(\"Number of frames in trajectory:\", len(u.trajectory)) # Accessing trajectory frames for ts in u.trajectory[:5]: # Iterate through the first 5 frames print(f\"Frame: {ts.frame}, Time: {ts.time} ps\") Considerations: In order to understand how the simulated system compares to experiments, we need to understand how the system behaves at equilibrium (or as close to equilibrium as possible). Therefore, we are most interested in measuring quantities starting from about 100ns into the trajectory until the end of the trajectory (as opposed to the beginning of the trajectory). We are interested in making graphs that are either function of time or averaged over long time intervals. We are not interested in measurements that are averaged over small time intervals or representative of single snapshot. Research Questions: You can consider the peptides aggregates as two different types of graphs: First, graph where each peptides is node and an edge between nodes represents the binary property of whether or not those two nodes have heavy atom contact Second, where the aggregates are hierarchical graph, where the coarse grained graph is the same as above, but each peptide is additionally made up of fine grained graph where every amino acid is node With these two graph definitions we can measure graph theoretical properties, which tell us about the overall structure of the graph We can construct the graph laplacian and calculate eigenvectors and eigenvalues We can calculate the Fiedler vector Are there other interesting graph theoretical quantities to explore? We would want to know how do these properties: Look on average? Fluctuate about that average? Change dynamically over time? We would like these properties to tell us: How densely packed are the clusters? Can this approach be used to predict whether cluster will split or be long lived? 31 What are different ways that we can measure peptide clusters or aggregates? Our current definition is that cluster is set of nodes where none of the nodes is fully disconnected from any of the other nodes. But one challenge with this is that the number of clusters varies widely as clusters form and uniform over the simulation. Is there way to combine this measurement with some graph theoretical definition of graph density in order to construct more stable order parameter? Use state-of-the-art methods to analyze the data. For every step, make lots of plots and save the data you generate, as it may be used for other steps. When writing the code, write some lines to indicate whether the execution was successful or not. Take into account that all the things print on screen will be passed to LLM, thus, only write essential information and dont print unnecessary information. Join plots that are similar. Do not create dummy data. You have access to 128 cpus on linux machine; for very computationally heavy tasks, try to use all of them. With this guidance, Denario performed novel, domain-relevant analyses that had not been previously utilized in the field of peptide self-assemblyand executed them efficiently. The generated paper, titled Dynamic Multiscale Graph Analysis Reveals Structural Signatures of Peptide Aggregate Stability and Splitting can be found in the Appendix A.4. These results showed that, for Denario to exhibits its full capability, very detailed and extensive description of the data is needed, and in some cases, guidance over the research direction. Overall, Denario demonstrated clear progression in capability based on the depth of contextual input. With minimal context, it could automate standard analyses; with targeted hypotheses, it extended into novel analytical territory. While Denario didnt independently generate insights human expert wouldnt have, it significantly accelerated the exploration of both standard and creative research avenues."
        },
        {
            "title": "5.5 Astrophysics. Properties of recent high-mass binary black hole merger",
            "content": "from gravitational wave data We provided Denario with publicly available gravitational wave posteriors [49] from multiple waveform models for recently announced gravitational wave event GW231123 [50]. GW231123 is the most massive confident black hole merger detected till now and has implications for the formation channels of astrophysical black holes. However, there is significant disagreement between properties of the event as predicted by different waveform models (see their description in the prompt below). This was the motivation for us to perform an exploratory study with Denario regarding the high-dimensional posterior dataset from multiple models. The input text we used for this paper is this: Input text We have data from recent gravitational wave event: GW231123. The data is located in five files: - /mnt/ceph/users/fvillaescusa/AstroPilot/GW/Iteration1/data/GW231123_NRSur7dq4.csv - /mnt/ceph/users/fvillaescusa/AstroPilot/GW/Iteration1/data/GW231123_IMRPhenomXO4a.csv - /mnt/ceph/users/fvillaescusa/AstroPilot/GW/Iteration1/data/GW231123_SEOBNRv5PHM.csv - /mnt/ceph/users/fvillaescusa/AstroPilot/GW/Iteration1/data/GW231123_IMRPhenomXPHM.csv - /mnt/ceph/users/fvillaescusa/AstroPilot/GW/Iteration1/data/GW231123_IMRPhenomTPHM.csv These files contain samples from the posterior distribution corresponding to five different gravitational-wave waveform models which are detailed below. The meaning of each column can be found in the file header, but is also provided below for reference. We are interested in understanding which ways the different models agree or differ in their prediction for GW231123. There are complex degeneracies in the high-dimensional posterior space which make this problem challenging. Analyze the datasets in detail and tell us what you have learned from them. Mention also any interesting astrophysical insights that you learn from this analysis, and what can be robustly concluded about statistical properties of the high-mass black hole merger GW231123. Make sure there is no repetition in plots in the paper. For reference, the event was reported in https://arxiv.org/pdf/2507.08219 32 Below is description of the models used for the five files mentioned above: NRSur7dq4 is time-domain waveform model that has been directly calibrated using Numerical Relativity simulations of binary black hole mergers. This model is particularly wellsuited for events with short gravitational wave signals, such as GW231123, as it is designed to accurately capture the dynamics of the late inspiral, merger, and ringdown phases. IMRPhenomXO4a, on the other hand, is phenomenological model that operates in the It combines post-Newtonian (PN) approximations during the inspiral frequency domain. phase with numerical relativity calibration near the merger. This model provides more global approximation of binary black hole waveforms and is specifically tailored for events that can be characterized over wider range of frequencies. SEOBNRv5PHM Domain: Time-domain, effective-one-body (EOB) Form: Co-precessing to inertial frame Key: Efficient inspiral-merger-ringdown with higher modes Calibrated to: NR + perturbation theory, high accuracy (unfaithfulness < 3% in 99.8% of cases) IMRPhenomXPHM Domain: Frequency-domain, phenomenological Form: Twisting-up non-precessing multipoles into precessing frame Key: Multipolar, fast, calibrated to waveform catalogs IMRPhenomTPHM Domain: Time-domain, phenomenological Form: Time-based twistingup using Euler angles + improved ringdown precession treatment Key: Avoids stationary phase, accurate and computationally efficient Here is description of the columns in the csv file: mass 1 source: Mass of the primary black hole mass 2 source: Mass of the secondary black hole 1: Spin magnitude of the primary black hole 2: Spin magnitude of the secondary black hole final mass source: Final mass the remnant black hole final spin: Final spin of the remnant black hole redshift: Redshift of the event cos tilt 1: Cosine of the spin tilt-angle of the primary black hole cos tilt 2: Cosine of the spin tilt-angle of the secondary black hole chi eff: Effective sum of spins components aligned with the orbital angular momentum chi p: Effective spin parameter related to spin-orbit precession of the binary cos theta jn: Cosine of the inclination angle of the binary to the observer phi jl: Azimuthal angle between the total and orbital angular momentum (called spin azimuth) log likelihood: Log-likelihood of the samples Use state-of-the-art methods to analyze the data. For every step, make lots of plots and save the data you generate, as it may be used for other steps. When writing the code, write some lines to indicate whether the execution was successful or not. Join plots that are similar. Do not create dummy data. The generated paper, titled Physics-Informed Discrepancy Decomposition and Robust Astrophysical In this paper, Denario performed the Inference for GW231123, can be found in the Appendix A.5. exploratory data analysis very well. Without an explicit mention of which tool(s) would be the best to 33 use, Denario identified dimensionality reduction tools like UMAP (which have not been widely used in the previous gravitational wave literature) for this problem. The resulting 2D UMAP embeddings in Fig. 2 of the paper have very interesting structure. The different waveform models are built in either the time or frequency domain. The UMAP embeddings produce common cluster for the three timedomain models, suggesting that their predicted posteriors are consistent with each other. However, for the two frequency domain models, the embeddings form separate isolated groups, highlighting that their predictions are inconsistent with each other  (Fig. 2)  . Denario also verified that the physical explanations are independent of the hyperparameters (see Fig. 3). The Jensen-Shannon divergence plots in Fig. 4 are useful for getting an intuition of which parameters the different models differ. This paper can be comparable to report generated from an undergraduate-level data exploration project. While the paper is not currently ready to be published in scientific journal, it definitely provides interesting ideas for further exploration (it can be turned into scientific paper upon further refinement of some of the ideas presented). With each paper taking very small time to generate, speed definitely is the biggest advantage of using Denario and it can be used to obtain quick project ideas from dataset. The drawback is Denario not having knowledge of the broader context of the field (apart from the information specified in the prompt, and to some extent the summaries provided by the enhanced data, and check idea options). Thus, it could not fully place the methods and results of the paper within the context of existing literature (which is crucial for research paper). However, one can envision further developing our literature agents so they can perform these tasks using tools such as Semantic Scholar."
        },
        {
            "title": "5.6 Astrophysics. Impact of Convection and Radiation Pressure on the Dy-",
            "content": "namics of Roche Lobe Overflow in Binary Systems Massive stars play many important roles in astrophysics. However, the structures and evolutions of massive stars remain very uncertain due to complicated dynamical processes controlled by both gas pressure and radiation pressure. In particular, the majority of massive stars are found in binary systems. Interactions with the companion can significantly impact the evolution of massive stars. Binary interactions and the associated mass transfer via the Roche Lobe overflow are the physical origins of many astrophysical phenomena. In recent years, with the advanced computational power and improved numerical algorithms, it is now possible to simulate the process of binary mass transfer between massive stars using three-dimensional radiation hydrodynamic simulations. One simulation we have analyzed is binary system with 12 solar mass Red supergiant (RSG) star as donor and 9 solar mass main-sequence star as an accretor. The simulation resolved the threedimensional structures at the surface of the donor and followed the mass transfer to the accretor due to the binary interaction. The surface of RSG is fully convective, which can make the Roche Lobe overflow significantly different from what has been imagined in classical theoretical models. We used Denario to analyze one snapshot of the simulation results to understand how the binary mass transfer happened in the simulation. The prompt we used as input to Denario is this: Input text We have snapshot of simulation designed to study the impact of realistic 3D envelope structures on the mass transfer in binary system. The binary is made of 12.9 solar mass red supergiant (RSG) and 9 solar mass companion. The companion is located at separation of 2000 solar radii, which is about three times the radius of the donor (the RSG). The simulation shows that the convective surface structures significantly modify the mass transfer rate in the binary system. The snapshot is located in /mnt/ceph/users/fvillaescusa/AstroPilot/YanFei/data/star.out1.16543.athdf To load the data use this example: import matplotlib matplotlib.use(Agg) %matplotlib inline import sys sys.settrace import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import LogNorm from pylab import * 34 import struct import array import os import glob import h5py import sys sys.path.append(\"/mnt/ceph/users/fvillaescusa/AstroPilot/Yan-Fei/data/\") import athena_read filename = /mnt/ceph/users/fvillaescusa/AstroPilot/Yan-Fei/data/star.out1.16543.athdf with h5py.File(filename, r) as f: attributes = f.attrs.items() attrs = dict(attributes) level = f.attrs[MaxLevel] time = f.attrs[Time] subsample = False nx1 = attrs[RootGridSize][0] * 2**level nx2 = attrs[RootGridSize][1] * 2**level nx3 = attrs[RootGridSize][2] * 2**level data = athena_read.athdf(filename, level=level, subsample=subsample) print(data.keys()) dict_keys([Coordinates, DatasetNames, MaxLevel, MeshBlockSize, NumCycles, NumMeshBlocks , NumVariables, RootGridSize, RootGridX1, RootGridX2, RootGridX3, Time, VariableNames, x1f, x1v, x2f, x2v, x3f, x3v, rho, press, vel1, vel2, vel3 , Er, Fr1, Fr2, Fr3, Pr11, Pr22, Pr33, Pr12, Pr13, Pr23, Pr21, Pr31, Pr32, Er0, Fr01, Fr02, Fr03, Sigma_s, Sigma_a, Sigma_p]) #units for the data Crat=9166.13 Prat=0.538439 rho0=5.3537e-8 T0=7.9896e4 #solar radius l0=6.955e10 # core mass gm=1935.14 gm_com=1613.74 omega=7.006e-4 #binary separation rm2=2000 msun=1.989e33 rl1=0.536974*rm2 vel0=2.9979e10/Crat time0=(l0/vel0)/(24*3600) mass_core=10. # all the data grids have shape [nx3, nx2, nx1] # all quantities are in the same set of units (although not cgs) rho=data[rho] # gas density Er=Prat*data[Er] # radiation energy density Fr1=Prat*Crat*data[Fr1] # radiation flux along radial direction Fr2=Prat*Crat*data[Fr2] # radiation flux along theta direction Fr3=Prat*Crat*data[Fr3] # radiation flux along phi direction Pr11=Prat*data[Pr11] # r-r component of radiation pressure tensor Pr12=Prat*data[Pr12] # r-theta component of radiation pressure tensor Pr13=Prat*data[Pr13] # r-phi component of radiation pressure tensor Pr23=Prat*data[Pr23] # theta-phi component of radiation pressure tensor Pr33=Prat*data[Pr33] # phi-phi componnent of radiaton pressure tensor pg=data[press] # gas pressure vr=data[vel1] # radial velocity vtheta=data[vel2] # poloidal velocity vphi=data[vel3] # azimuthal (toroidal) velocity 35 x1f=data[x1f] # face centered radial coordinate of size nx1 + 1 x2f=data[x2f] # face centered theta coordinate of size nx2 + 1 x3f=data[x3f] # face centered azimuthal coordinate of size nx3 + 1 x1v=data[x1v] # volume centered radial coordinate of size nx1 x2v=data[x2v] # volume centered theta coordinate of size nx2 x3v=data[x3v] # volume centered phi coordinate of size nx3 to load that Note file /mnt/ceph/users/fvillaescusa/AstroPilot/Yan-Fei/data/athena read.py that should be loaded. Given the data, come out with an interesting idea for PhD thesis. We are particularly interested in very detailed study of the statistical properties of mdot. example, above using there data the the the is Use state-of-the-art methods to analyze the data. For every step, make lots of plots and save the data you generate, as it may be used for other steps. When writing the code, write some lines to indicate whether the execution was successful or not. Join plots that are similar. Do not create dummy data. You have access to 128 cpus; for computationally heavy tasks, try to use all of them. The paper generated by Denario, titled Convection, Radiation, and the Instantaneous Mass Transfer in Red Supergiant Binaries: 3D Simulation Analysis, can be found in the Appendix A.6. In that paper, Denario performed detailed analysis of different components of forces that are responsible for binary mass transfer in three-dimensional radiation hydrodynamic simulations. The paper also accurately captured the importance of convection for the properties of Roche Lobe overflow. The paper shows particular strength in quantitative analysis of the data based on the equations solved in the simulation, in similar way that scientists will do to understand the results. Furthermore, Denario analyzed the streamlines of the flow very efficiently, which is typically much harder to do in traditional analysis. We note that for this particular paper, one of the plots generated (Figure 8) by Denario was blank, so we looked at the source code and made tiny modification to produce the intended plot. In the associated GitHub example papers repository27, we show examples of other papers written by Denario with the same prompt. One common weak point of these papers is that the analysis cannot be connected to the literature of binary evolution for astrophysical implications of these results. This requires some knowledge of how binary evolution is typically done in the literature and the unique advantages of these simulations, which cannot be learned from the simulation data itself. It will be interesting to see if this point can be improved by adding more guidance towards this direction in the input prompt, or by using tailored agents dedicated to these domain-specific tasks."
        },
        {
            "title": "5.7 Biology. Analyzing single-celled RNA sequencing data across the malaria",
            "content": "parasite life cycle Biological research has become increasingly entwined with computational analysis over the past two decades. The advent of high-throughput genetic sequencing has generated wealth of data at scale the field had not seen in the past. The realization that variability at the individual cell level had crucial consequences for population-level behaviors led to the development of methods for single-celled data generation, increasing the scale of datasets even further. With such massive amounts of information, computational analysis (and reanalysis) of publicly available data has become valuable part of biological research. This presents an opportunity for AI multi-agent tools such as Denario to easily explore vast datasets, identify phenomena that would otherwise be difficult to spot, and rapidly test these lines of inquiry. Here, we used Denario to analyze publicly available dataset of gene expression in malaria parasite cells at different life stages. Malaria, caused by unicellular parasites of the genus Plasmodium, remains global public health threat, causing more than 600,000 deaths in the poorest communities around the world every year, and progress in its control has stalled or reversed over the past decade [51]. Understanding the biology of Plasmodium falciparum parasites, responsible for the most lethal forms of the disease, can be critical to the development of new treatments. Many aspects of Plasmodium biology are still poorly understood due to the parasites evolutionary divergence from other well-known model organisms. AI multi-agent systems such as Denario could be used to help address this gap in knowledge. 27https://github.com/AstroPilot-AI/DenarioExamplePapers 36 We used Denario to analyze single-celled RNA sequencing (ssRNA-seq) dataset describing gene expression (transcription) in both laboratory and field-derived strains of Plasmodium falciparum. The dataset is publicly available through the Wellcome Trusts Malaria Cell Atlas project [52] and was previously published and analyzed by Dogga, Rop, Cudini, et al. [53]. 5.7.1 Input data To allow Denario to work with the dataset, we provided two input files containing the gene expression data and metadata (pf-ch10x-set4-ch10x-exp.csv and pf-ch10x-set4-ch10x-data.csv, respectively, available in dataset P. falciparum 10x Integrated lab strains and four natural infections on https://www.malariacellatlas.org/data-sets/). We also provided an input text prompt describing the datasets origin, content, and organization. We also provided information about potentially interesting research questions to follow, including both lines of inquiry explored in the original publication by Dogga, Rop, Cudini, et al. [53] and different sets of questions not explored in the publication. However, we also explicitly left an open door for analyses not mentioned in the prompt. The prompt read as follows: Input text We have collection of malaria parasite single cell transcriptomic data obtained from the Malaria Cell Atlas project and previously analyzed by Dogga et al. 2024 (paper found at doi.org/10.1126/science.adj4088). Malaria parasites have complex life cycle in which cells transition across multiple life stages in both humans and Anopheles mosquitoes. The data collected here was obtained from malaria parasites of the species Plasmodium falciparum that were either growing in human blood culture in laboratory setting or recovered from the blood of four asymptomatic patients from Mali. This means that the parasites are all either in the asexual intraerythrocytic development cycle or the sexual blood stages. None of them are in mosquito life stages. The data is located in /mnt/ceph/users/fvillaescusa/AstroPilot/Biology/data The data is presented across two different files: The first file, gene expression.csv, is 2-D matrix in which columns correspond to individual cells analyzed in the experiment and rows correspond to individual genes. The first row contains unique ID codes identifying each individual cell, while the first column contains unique codes corresponding to P. falciparum strain 3D7 gene ID codes. The entries in the matrix contain the corresponding genes normalized expression value within the corresponding cell. The second file, labels.csv, contains table with information about each individual cell. The column marked CELL ID contains the same cell ID code from the previous file. There are additional columns describing the life cycle stage, parasite strain, source (lab for laboratory strains; MSC1, MSC3, MSC13, and MSC14 for the four patients), and days in culture (only applicable for the lab strains, not for the four patient strains which are marked as Field for field isolates). There are many interesting things to uncover in the dataset. One set of questions relates to potential differences between lab strains and field isolates. Another set of questions relates to gene expression differences throughout the parasite life cycle, including key marker genes of different stages and determinants of future developmental trajectories. am particularly curious about uncovering potential transcription factors or other master regulators of cell cycle transitions. suspect they might be genes that maintain low expression levels throughout the life cycle, but have increases in expression immediately prior to an increase in expression across multiple other genes denoting change in life stage. The increase in expression of possible transcription factor or other regulator gene would be relative to its own expression level, but would probably not achieve particularly high expression when compared to other genes. 37 5.7.2 Description of research output With this information, Denario produced four different manuscripts analyzing the given data. One of them, titled Single-Cell Analysis Reveals Profound Divergence in Transcriptional Regulatory Programs Between Laboratory and Field Isolates of Plasmodium falciparum can be found in the Appendix A.7. All four manuscripts were methodologically similar, carrying out standard practice workflows used in scRNA-seq analysis: data preprocessing, dimensionality reduction by Uniform Manifold Approximation and Projection (UMAP) [54], clustering, trajectory inference, and pseudotime assignment using common analysis packages in Python such as SCANPY [55] (3 manuscripts) or in such as Monocle 3 [56] (1 manuscript). All manuscripts attempted different strategies to identify regulators of gene expression and to compare the gene expression profiles of laboratory and field-derived parasites, both of which the prompt listed as topics of particular interest. To identify regulator genes, strategies tested involved combinations of screening for low expression genes, peak/burst detection in transcription, clustering into transcriptional modules characterized by gene ontology (GO) analysis, partition-based graph abstraction (PAGA) [57], regression models, and reportedly consulting existing information about gene function consulted from the literature and the online database PlasmoDB, part of VEuPathDB [58, 59] (however, doing so at the time the analysis was done involved web access and personal account; it is unclear whether Denario would be able to consult PlasmoDB at all). To examine differences in gene expression between lab and field parasites, the manuscripts compared results from the techniques listed previously, occasionally complemented by differential gene expression (DGE) analysis using the tradeSeq package (2 manuscripts) [60]. 5.7.3 Assessment of research output We identified series of notable strengths and weaknesses in the research papers generated by Denario for this project: Strengths: Knowledge of and engagement with established biology: As expected, Denario exhibits expert-level background knowledge on P. falciparum biology. All manuscripts discuss the complexities of the malaria parasite life cycle, correctly using specific terminology to do so despite no such information being included in the prompt. The texts are able to justify the purpose of the study being done in the context of the state of the field of malaria cell biology and the advantages of scRNA-seq with respect to previous bulk RNA-seq methods. Some manuscripts even make good use of specific knowledge of individual genes for use as positive controls validating stage transitions throughout the life cycle. Proficiency with standard analysis methods: The basic pipeline used to analyze the data provided contained all the necessary steps to carry out the analysis correctly, from data pre-processing and quality control (some of which had already been done beforehand on the publicly-available dataset and was therefore redundant) through to dimensionality reduction and visualization, as explained above. The tools used were well-suited for the job, conforming to standard practices in the field. Consistency with existing science: Many of the general results obtained recapitulate known science. The manuscripts were all able to successfully reconstruct the Plasmodium intraerythrocytic development cycle (IDC) from transcriptomic data. They described differences between the lab and field-derived parasite samples and even provide plausible explanations for these differences in cell type and data quality. Speed : Needless to say, the speed at which entire analysis pipelines are created and the results are analyzed is far greater than what human researcher would take. In less than day, Denario produced analyses and manuscripts that would take multiple days or weeks, depending on researchers familiarity with the methods. Methodological creativity when approaching new research questions: When prompted with research question with no clear answer nor well-established, standardized way to address it, Denario showed remarkable creativity. All four manuscripts had different approaches for trying to identify regulator genes, as explained above. Weaknesses: 38 Poor citation, acknowledgment, and engagement with existing literature: The most immediately evident shortcoming these manuscripts had was in their handling of scientific literature. Although (to the best of our knowledge) all the references cited in these manuscripts corresponded to real articles, the criteria used for selecting studies as references for specific passages is unclear. While the literature cited is related to the topic in question, it often is perspective or review article with only general-level relevance, while highly relevant and closely related articles are left undiscussed. As an example, none of the manuscripts referenced the study by Dogga, Rop, Cudini et al. [53] describing the dataset used and its original analysis, despite this information being explicitly provided in the input prompt. Interestingly, the problem of missing citations is particularly noticeable when discussing specific facts about malaria biology, most of which are not cited at all. Indeed, only 2 out of 30 references in the manuscript shown in the supplementary material refer to malaria or Plasmodium parasites, while the other 28 refer to scRNA-seq methods. This is curious given the stark contrast with the impressive grasp of malaria parasite biology shown in the writing: Denario clearly knows the field of malaria well, but prefers to cite other fields. Perhaps Denario is more likely to include references related to the methods it uses more than the knowledge of the field it applies them to, although even references to the exact methods being used are rare. Relatedly, perhaps the lack of malaria citations and tendency to reference methods is reflection of training biases in the quantity or perceived impact of publications in different fields: methods are generally applicable to multiple research areas and are therefore more widely cited and discussed in the literature than papers addressing specific field such as malaria biology. This would have important ramifications that would be crucial to address in any research AI agent. Confirmation bias, misplaced confidence, and overstated tone: Perhaps the most concerning weakness shown in these manuscripts is their tendency to overstate the findings they present, particularly in regard to the open-ended research question posed in the prompt regarding potential regulators of gene expression. Although all manuscripts tried variety of creative and potentially successful strategies for identifying candidate genes involved in life cycle regulation, none of the results presented inspire great confidence. The magnitude and timing of the changes in gene expression on which the conclusions are based is not sufficient to uphold the claims made about the regulative roles of specific genes, particularly in view of the variance in gene expression. In fact, it may be the case that the signal-to-noise ratio in scRNA-seq measurements of transcription factor expression is too low to be able to draw meaningful conclusions, or that key changes in regulator activity might be occurring at post-transcriptional level. The research question was intended to be challenging, and like any real research question, it might not have clear answer with the available data, in which case negative result must be reported (or at least, lack of confidence in the results must be conveyed). It is possible that the confirmation bias shown when attempting to answer specific research question from the prompt is consequence of the bias against publishing negative results already found in existing scientific literature, which would skew the training dataset of any AI agents such as Denario. To be able to succeed, AI multi-agents must be able to temper their conclusions when confidence in the significance of the results is low. Occasional mistaken knowledge: Although the manuscripts show strong understanding of Plasmodium biology for the most part, occasional mistakes are made. For instance, the manuscript included as an example in the supplementary material claims that the gene Pfs25 (PF3D7 1031000) is the canonical male gametocyte surface protein gene, when in fact it has been repeatedly shown to be expressed solely in female gametocytes, not male ones [61]. Although not widespread throughout the manuscripts (and certainly not unique to AI agent writing when compared to human writing), the issue is complicated by the previously discussed lack of citations. Without references to the sources of information, it becomes difficult to fact-check every statement and understand why the few mistakes that occur are happening. Methodological ambiguity: At multiple points in the Methods sections of different manuscripts, the text describes multiple valid but mutually exclusive methods for analysis without specifying which one was actually used (Data loading was performed [...] in Python or R, Trajectories were reconstructed [...] using [...] Monocle 3, Slingshot, or PAGA). This poses problem when evaluating the adequacy of the work being presented, since it is not clear what the exact methodology followed was. Occasional missing tables and references: The manuscript included in the supplement references tables not included in the text, although these results are calculated in the analysis code generated. 39 Some citations are also presumably missing, included as question marks in the text. Overall, the application of Denario to biological datasets such as this one yields results that are impressive in their speed and competent grasp of field and its methods, but concerning in their presentation and interpretation, as well as in the way they are presented in relation to other studies. The implications of these trends are discussed in later section. As AI agents such as Denario continue to improve, we expect them to become transformational tools for training and research in biological sciences."
        },
        {
            "title": "5.8 Digital Health. Developing and evaluating step-counting algorithms from",
            "content": "wristand hip-worn accelerometers Wearable monitoring is popular, emerging research field within the digital health domain, for the objective, passive monitoring of biosignals, which may offer insights into health outcomes [62]. This field has particularly grown with the increased use of smart devices such as phones and watches [63, 64]. These devices use variety of sensors to extract digital measures, such as total sleep time, sleep efficiency, heart rate, and daily step count, among others [65]. Of these measures, the daily step count, particularly achieving 10,000 steps day, has become one of the hallmark approaches to monitoring ones activity levels. However, this measure still has high degree of skepticism, due to lack of transparency of many step counting algorithms, and known tricks to trigger false counting of steps, through arm movements [66, 67]. In this study, Denario was used to read, process, and investigate research questions using publicly available accelerometer dataset. This data contains raw wrist and hip accelerometer data from 39 Oxfordbased participants over 1 hour. The raw data was collected from 2 devices at each location, set to sampling frequencies of 25 and 100Hz; therefore, four monitoring devices in total. hip-mounted, foot-facing camera was used to record the timings of each step taken by participants, allowing for the annotation of an indication of the timing of each heel strike, corresponding to each step. The dataset is explained in further detail in the published work of Small et al. (2024) [67]. This dataset could primarily be used to train or validate models/algorithms used to count steps. Moreover, given the data collection from four different devices, it is possible to compare how the performance of these step-counting algorithms differs between the wrist and the hip, and at different sampling frequencies. Using the metadata file, we can further explore if there are noted differences in different age groups or sexes, though limited to just 39 participants [67]. 5.8.1 Input data The full contents of the OxWalk dataset were provided to Denario. This included the raw accelerometer files, kept in separate folders for device location and sampling frequency combination. These files were stored as CSV files, with one file per participant in each folder. Within the CSV file, there are 5 columns of data. The first column is the data timestamp, and the next 3 are the accelerometer signal in x, and 2). The final column is an indicator of whether the heel strike of (measured in units of gravity = 9.81ms step occurred at that time, that is, the start of step. In addition to this, there was single metadata CSV file, storing the age band and sex of each of the participants. After an initial attempt, we provided the following refined prompt to the Denario model: Input text OxWalk Annotated Step Count Dataset DATA DESCRIPTION: Annotated step data during unscripted, free living in 39 healthy adult volunteers (aged 18 and above) with no lower limb injury within the previous 6 months and who were able to walk without an assistive device. Participants wore four triaxial accelerometers concurrently (AX3, Axivity, Newcastle, UK), two placed side-by-side on the dominant wrist and two clipped to the dominant-side hip at the midsagittal plane. Accelerometers were synchronised using the Open Movement GUI software (v.1.0.0.42), with one recording at 100 Hz and the other at 25 Hz at each body location. Foot-facing video was captured using an action camera (Action Camera CT9500, Crosstour, Shenzhen, China) mounted at the participants beltline. 40 DATA ANNOTATION: Foot-facing video was captured for up to one hour using an action camera (Action Camera CT9500, Crosstour, Shenzhen, China) mounted at the participants beltline. Annotation of steps was conducted within video annotation software (Elan 6.0, The Language Archive, Nijmegen, Netherlands), where step was defined as the act of purposeful lifting foot and placing it in new location. Steps were not required to be part of repeating pattern and did not include foot shuffling, changing of foot alignment via pivoting, or shifting of weight from one foot to the other. DATASETS: The data is located in /mnt/ceph/users/fvillaescusa/AstroPilot/Aidan/data. In that folder, there are 4 folders (Hip 25Hz, Hip 100Hz, Wrist 25Hz, Wrist 100Hz) and one file (metadata.csv). Datasets are as follows: 1. Wrist 100Hz: One Axivity AX3 accelerometer, recording at 100 Hz and +/- 8g on the dominant wrist within silicone wristband, with axes aligned as prescribed by the manufacturer. https://axivity.com/userguides/ax3/technical/#axis-alignment 2. Wrist 25Hz: One Axivity AX3 accelerometer, recording at 25 Hz and +/- 8g on the dominant wrist within silicone wristband, with axes aligned as prescribed by the manufacturer. https://axivity.com/userguides/ax3/technical/#axis-alignment 3. Hip 100Hz: One Axivity AX3 accelerometer, recording at 100 Hz and +/- 8g clipped at the beltline, laterally above the dominant leg, with the +X axis approximately aligned in the superior direction, and the positive axis aligned to face anteriorly. 4. Hip 25Hz: One Axivity AX3 accelerometer, recording at 100 Hz and +/- 8g clipped at the beltline, laterally above the dominant leg, with the +X axis approximately aligned in the superior direction, and the positive axis aligned to face anteriorly. 5. Participant sex and age range are provided in metadata.csv Inside each of these folders, there are 39 files, one for each participant. The files are called as PX hip25.csv, for participant (01, 02, 03, ...39) and hip25 is for the files inside Hip 25Hz. The accelerometer data in each file has been resampled and calibrated using the Open Movement GUI software package. Within each CSV file, step is annotated by single 1 at the approximate time of heel strike. The primary purpose of this data collection is the development of step-counting algorithms, which, given just the raw accelerometer data, should be able to correctly estimate the number of steps. To measure how well it can do so, please use different statistics for this. This is particularly useful to compare variety of algorithms and machine learning models, currently used for this purpose. With the concurrent collection of data from 2 different locations, we could also observe how the performance of these step-counting algorithms compares between the wrist and the hip. Using the metadata file, we can further explore if there are noted differences in different age groups or sexes. Of particular interest to me is observing how step-counting algorithms are affected by the reduction in raw sampling frequency. While sampling at lower frequency may cause the monitor to miss some high-frequency behavior, the vast majority of human movement occurs at far lower frequencies than the monitor can observe, sampling at 100Hz. Sampling data at lower frequency can allow for longer periods of monitoring and requires less computing power, which is ideal, as long as it does not cause significant drop in model performance. Please make sure that in your project, you are counting steps, not windows. Given the limited dataset, use cross-validation to test the performance of the model. Use state-of-the-art methods to analyze the data. For every step, make lots of plots and save the data you generate, as it may be used for other steps. When writing the code, write some lines to 41 indicate whether the execution was successful or not. Join plots that are similar. Do not create dummy data. You have access to 1 GPU and 10 cpus. 5.8.2 Description of research output Through two different iterations of prompts submitted to Denario for this dataset, seven different manuscripts were produced and analyzed. These seven manuscripts explored different research questions, with varying success, one of which, titled Wearable Step Counting: Comparative Analysis of Deep Learning and Traditional Methods Highlighting Data Imbalance Challenges is shown in the Appendix A.8. For all of the manuscripts read, research questions explored the use of novel machine learning techniques to develop walking/step detection models. This investigation then compared how the developed models performance would be affected by various factors, such as subgroups of age and sex, device location or sampling frequency. The models used to detect walking/count steps, and read and process the data files used python. To evaluate the performance of the model, F1 or AUC statistics were used for walking detection, while mean absolute percentage error (MAPE) was used for step counting. The research questions explored by some manuscripts brought greater levels of novelty, investigating unique approaches for extracting gait fingerprints, or gait features, independent of device location. This research is particularly relevant for the development of step counting algorithms in phones, which could be located either in the hand, hence close to the wrist, or in the pocket, close to the hip. Unfortunately, for these more novel research questions, the model struggled to build insightful manuscripts, following this research question. 5.8.3 Assessment of research output For the processing of wearables data, we identified the following strengths and limitations: Strengths: Able to follow and understand the provided prompts: The Denario model followed the tasked prompt as expected. After noticing common errors in the first iteration and adjusting the prompt to avoid these errors in the following outputs, these errors were successfully avoided. Opportunity to iteratively improve prompts: As previously noted, the first input to the Denario model did not match our expectations of output manuscripts, but explored useful ideas. These ideas and failures could then be used to better prompt the model in future attempts, iteratively producing better results. Able to produce novel and useful research questions: Some of the research questions explored by the model provided novel use of the dataset. Some of these insights have been stored as ideas for future fellowship applications. Built on existing practices in the field : In many cases, the outputs of Denario are built on the use of existing models from published literature. In some cases, the research question involved the use of new approaches in machine learning, not yet applied to wearable data, but showed advanced In other cases, the evaluation metrics, domain knowledge in new machine learning approaches. and window segmenting approaches followed by Denario match that as standard in the wearable monitoring domain. Limitations: Use of jargon not relevant to the research question: It was noted that in several manuscripts produced by Denario, some methods or results chapters would consist of words and concepts that are relevant to the wearable monitoring space, but not relevant to the particular research question. One example manuscript explored how signal features differed between device locations, but results presented AUC scores, which would be expected to report the performance of classification model, which was not developed in this manuscript. Formatting of paper outputs: Some of the manuscripts lacked tables containing the performance of the models, which was instead solely presented in graphs. For other papers, the plots and tables had formatting issues, like using too small font, tables overlapping with text, the lack of use of standard plotting techniques such as box plots and lack of attaching some extra plots as supplementary material. 42 Reporting of data handling errors: In some cases, Denario failed to read some data files. Instead of the model failing to produce manuscript, this was reported as part of the results; in some cases, full section was created reporting these failures. Preference for walking classification over step counting: Many of the models produced by Denario initially had bias towards building 2-second walking detectors, rather than step counters, despite the prompt clearly indicating this was the intention of the study. This is likely due to this being more common approach in literature, as there are very few step-counting datasets available. Trade-off between novelty and manuscript quality: There was noticeable trade-off between the novelty of the research questions, and the quality of the methods and results of the subsequent manuscript. This is likely because, for more novel research questions, there exists more limited literature regarding how to go about these investigations, leading to poorer decisions by the model. In summary, Denario showed clear capabilities for wearable monitoring in the digital health domain, to produce useful research questions, conduct this research, and analyze the output data. It is noted, however, that there are quite few limitations in the generation of reliable manuscripts. Overall, this encourages the use of this technology in earlier stages of the research process, to generate research questions, and to run exploratory analysis on datasets."
        },
        {
            "title": "5.9 Biophysics. Mechanistic understanding of protein folding pathways via",
            "content": "dimensionality reduction methods Characterizing protein folding and unfolding is one of the key challenges in theoretical and computational biophysics [68], as it requires an understanding of the associated high-dimensional energy landscapes and the kinetics of transitions between different metastable states. Long timescale simulations (microseconds and beyond) are therefore required to observe protein (un)folding events on an experimentally relevant timescale, leading to an understanding of the associated mechanistic pathways, identification of transient microstates, hidden allostery, and quantification of the kinetics of such processes. The N-terminal domain of ribosomal protein L9 (NTL9) is chosen as the model protein for this study since it undergoes microsecond-scale folding reversible folding with well-characterized two-state folding mechanism. NTL9 has well-characterized and experimentally validated folded structure that allows new algorithms and enhanced sampling schemes to reproduce the free energy landscape, intermediate ensembles, and rate constants. This suits the purpose of benchmarking studies due to its small size of 39 amino acid residues, absence of disulfide bonds, and lack of any complex post-translational modifications. This makes NTL9 an ideal candidate for developing new dimensional reduction methods to identify progress coordinates and understand mechanistic insights into protein folding and unfolding. 5.9.1 Data generation Molecular dynamics (MD) simulations were performed using the OpenMM engine [69] with the Amber14SB force field and the OBC2 implicit solvent model at 300 K, employing Langevin dynamics with 1. The protein was subjected to energy minimization to remove steric clashes, friction coefficient of 5 ps followed by 10 µs production run with an integration timestep of 2 fs. Protein coordinates were then saved every 2 ns for subsequent analyses. 5.9.2 Project description Denario was given the task to automate the post-simulation analysis of NTL9 protein (un)folding from MD simulation trajectories. The objective was to write manuscript with detailed methodologies, dimensionality-reduction and clustering approaches, estimation of the free-energy landscape and kinetic rate constants (if possible), employ Markov state modeling to identify relevant low-dimensional progress coordinates that can characterize the mechanism of protein (un)folding, and establish scalable strategies for complex biomolecules. The input text to the system was this: Input text N-terminal domain of ribosomal protein L9 (NTL9) is fast-folding protein domain that folds reversibly and is often used as model to understand protein folding pathways. We aim to explore its unfolding thermodynamics and kinetics using an automated, generalizable pipeline. We have 43 10 µs molecular dynamics (MD) simulation trajectory of the NTL9 protein undergoing folding. Simulation conditions for the trajectory obtained: Amber14SB force field with OBC2 implicit solvent, 300 Langevin dynamics, 2 fs timestep, 2 nm nonbonded cutoff, and H-bond constraints. Data: The simulation data is located at /mnt/home/aojha/AstroPilot/Project10. There are two files in the dataset, i.e., the simulation trajectory (an implicit-solvent 10 µs MD trajectory consisting of 5000 frames saved every 2 ns (ntl9 trajectory.dcd) and its corresponding topology PDB file (ntl9 topology.pdb). The simulation begins from an unfolded state of the NTL9 protein, while the topology file is folded structure of the protein. Motivation: Understanding how NTL9 unfolds and, by extension, how small single-domain proteins navigate their energy landscape reveals general principles of protein folding. Tracking down folding/unfolding pathways provides essential knowledge related to energy barriers, intermediates, transition states, and allostery involved in the process. We intend to build an analysis pipeline that breaks down local events, provides mechanism for folding and unfolding events, describes the free energy profile for the process, and possibly the folding/unfolding kinetics. We also aim for this workflow to be generalizable for complex proteins or multi-domain systems, thereby addressing the gap between detailed MD simulations and high-level mechanistic insights. Key Research Questions: 1. Data-driven collective coordinates: Can we identify interesting collective variables beyond the radius of gyration and root mean square deviation (RMSD) for profiling such folding/unfolding events? It will be interesting to compare the difference between using conventional collective variables (CVs) and generating new hidden CVs based on the simulation at hand. What are the dominant motions during folding/unfolding, and can we capture these motions with dimensionality reduction? There are numerous existing dimensional reduction methods. However, it would be interesting to see if we can keep increasing the complexity of such methods, starting from linear principal component analysis (PCA) and moving to spectral methods, such as diffusion maps, to explore non-linear manifold embedding. As we progress through the complexity level, it is expected that we will uncover more subtle unfolding pathways and intermediate microstates. Please provide detailed description of each method, including analysis, figures, and tables. Create hypothesis such as, Linear PCA will capture the largest-scale structural separation (folded vs. unfolded), whereas manifold embedding could reveal hidden intermediate microstates or pathways not evident in linear PCs. 2. State Partitioning in low-dimensional space: Can we objectively identify distinct states (folded, intermediate, unfolded) from simulation trajectories? It would be interesting develop algorithms to identify the folded, unfolded, and intermediate states of the protein. Once trajectories are clustered using multiple algorithms, it will be useful to systematically visualize and compare their performance and derive best-practice guidelines for state identification. 3. Mechanisms of folding/unfolding: What local structural changes mark the onset of unfolding? How can we analyze such local changes and visualize them on free energy landscape? Can we monitor the status of specific hydrogen bonds and native contacts throughout the trajectory, particularly those that stabilize the core, to determine when each bond first breaks? Can we construct 2D free energy surface by binning the trajectory in the space of two most relevant CVs. This may highlight the free energy minima (states) and barriers between them. Then, using the cluster assignments from above, can we construct simple Markov state model (MSM) or transition matrix to estimate the rate constants for transitions? 4. Scalability and extension: How can this analysis pipeline be generalized to larger or multi-domain proteins? Can we propose strategies to manage longer trajectories and more complex proteins? For instance, using frame subsampling or incremental PCA to handle extremely long simulations, splitting the analysis by protein domain (which may be achieved by analyzing independent domain motions rather than collective one), and employing graph-based representations of contacts or networks to capture inter-domain interactions. Please also discuss how to choose the dimensionality of the embedding or the number of 44 clusters in an automated way (e.g., using the percentage of variance, silhouette scores, etc.) for larger systems. It would be interesting to emphasize how each component of this pipeline can be adapted or scaled to ensure the approach remains efficient for complex proteins. Manuscript Writing: 1. Title and Abstract: Emphasize generality, summarize the data, and highlight key analyses, such as the choice of CVs, clustering, free-energy mapping, and manifold embedding, while also highlighting mechanistic insights and extensibility. 2. Introduction: Present the NTL9 folding process in the broader context of protein folding/unfolding studies and identify the lack of scalable MD pipelines with proper references. 3. Methods: Provide step-by-step protocols for data loading, alignment, choice of CVs, clustering, manifold embedding, clustering, hydrogen-bond and dihedral analyses, free-energy surface calculation, kinetic estimates, and scalability strategies. 4. Results: Present the findings with well-annotated figures, tables, graphs, plots, and detailed comparison and analysis. 5. Discussion: Interpret findings from the point of view of understanding the mechanism of protein folding pathways, compare linear vs. non-linear embeddings, assess the strengths and limitations of each step, and propose experimental validations (e.g., FRET, HDX) for complex proteins. 6. Conclusions and Outlook: Summarize core thermodynamic and kinetic insights, highlight the generality of the proposed workflow, and suggest future expansions (enhanced sampling, machine-learned CV discovery, allosteric network analyses). Note: Do not use PyEMMA or any PyEMMA-based repositories. Instead, implement Markov modeling and implied-timescale analysis using scikit-learn, NumPy, and SciPy for matrix exponentials and eigen-decompositions, or any other lightweight MSM tools (e.g., msmtools). 5.9.3 Assessment of research output The generated paper, titled Comprehensive Kinetic and Free Energy Analysis of NTL9 Folding via Systematic Collective Variable Selection and Markov State Models, can be found in the Appendix A.9. The manuscript attempts to employ conventional collective variables (CVs) or order parameters, such as root mean square deviation (RMSD), radius of gyration, and native contacts, to capture the global structural motions of the protein folding pathway. It is interesting to note that principal component analysis (PCA), time-independent component analysis (tICA), and nonlinear diffusion maps were implemented successfully to construct Markov state models (MSMs). hierarchical approach is implemented to select analysis methods, progressing from conventional CV choices to linear PCA, tICA, and diffusion map embedding, to identify slow modes. The 2D free energy surfaces projected on the CVs show the folded, intermediate, and unfolded states on the energy landscape, with distinct free energy minima, to explain the folding and unfolding pathways of the protein. For kinetic modeling, the manuscript validates the MSM by performing implied timescale convergence and Chapman-Kolmogorov tests to ensure the Markovian assumption holds. Interestingly, specific structural changes from the folded to the unfolded state of NTL9 are correlated with the loss of hydrogen bonds and native contacts, providing additional insights into the mechanistic pathways. In short, across all the different approaches, the manuscript consistently reported at least three protein states, namely folded, intermediate, and unfolded. Although several methods are mentioned and employed in the study, the motivation or theory behind such approaches is lacking, making it difficult to follow without prior knowledge or implementation. There is an extensive use of technical jargon associated with each method (PCA, TICA, diffusion maps, MSMs), but it is hardly convincing or clear. For example, the study claims quantitative agreement of protein folding kinetics with experiments, but no such theoretical kinetic data or references are reported. Prior literature studies relevant to protein folding are absent, with little to no mention of existing benchmarking studies. For example, the manuscript does not refer to well-cited studies, such as the extensive MD simulations of NTL9 folding by Lindorff-Larsen et al. [70] or the MSM-based folding of NTL9 by Voelz et al [71]. The manuscript fails to report any statistical uncertainties or confidence intervals on estimations 45 of free energy differences, kinetic rate constants, or MFPTs. There is no discussion on error estimations, error bars, or Bayesian estimation of the transition matrix for MSM, which is an issue given that the manuscript claims an excellent agreement with the experiments. The study also fails to acknowledge the limitations of MD simulations, such as force field and sampling limitations, which are necessary for justifying analytical methods, including MSMs and enhanced dimensionality reduction approaches. The manuscript frequently cites figures and tables, but they are marked as not shown, disrupting the continuity, reproducibility, and interpretability for the reader. Overall, the AI-generated manuscript on NTL9 protein folding serves as clear proof-of-concept demonstrating the capability of LLM agents to generate scientific content. The methodology presented in the manuscript follows logical and conventional scientific workflow. However, the scientific novelty remains limited and is not sufficiently communicated. The discussion and literature context provided in the manuscript are limited, and essential supporting data are absent, despite substantial claims being presented. Given these observations, the manuscript, in its current form, serves as powerful demonstration but will require considerable manual refinement, contextual analysis, and validation to achieve adequate scientific communication with its readers."
        },
        {
            "title": "5.10 Neuroscience. Behavioral Cognitive Decline in Egyptian Fruit-Bats.",
            "content": "We also tested Denario on data from neuroscience. Specifically, we used it to study behavioral cognitive decline and its underlying brain changes in Egyptian fruit bats, which are known for their extreme longevity. We described in lots of detail the data and its characteristics in the input prompt: Input text We use dataset of 40 Egyptian fruit bats (Rousettus aegyptiacus), with paired behavioral and MRI (diffusion tensor imaging - DTI) measures from the same subjects to investigate fruit bats cognitive and brain aging. Bats were aged based on their methylome profile. Data Sources and Collection: Age determination: We followed Wilkinson et al. 2021 for methylome-based aging of the bats. DNA was extracted from wing biopsies using the DNeasy Blood & Tissue Kit and protocol (QIAGEN Ltd., Germany). The extracted DNA was sent to an external laboratory (The Clock Foundation lab) for DNA methylation (DNAm) analysis. Behavioral procedure - Our well-established foraging-box paradigm (Harten et al. 2021) served as an assay of age-related spatial cognitive decline. The setup consists of large flight room with six opaque boxes with bowls of fruit placed inside them. Before the experiment, all bats underwent habituation session. Here, groups of four bats were introduced to the same experimental apparatus that would be later used for individual testing but with adequate food and hydration available and accessible in all boxes (600 of fresh fruit pieces and 200 ml of mango juice per box). The test protocol consisted of three identical phases, each lasting three hours spread during two consecutive days. On the first day, bats completed two consecutive phases, while the final phase took place on the second day. In each phase, an individual bat foraged freely in the flight room containing six opaque boxes. Only one bowl allowed access to food (the correct box), while the others were blocked with plastic mesh to avoid use of olfaction for box finding. First phase - Individual testing started with three-hour phase with one bowl accessible. After three hours, the bat was removed, and the correct bowls location was swapped randomly to different box. The second phasebegan immediately (2 minutes) after the swap and lasted three additional hours. This phase tested (among other measurements) the bats short-term memory of the previous location. After the second phase, the bat was removed from the setup for 18 hours.cIn the third phase (Long term memory) - the correct bowl was relocated to third new location for the final three-hour phase. Behavioral Data collection and analysis Each bats behavior was video recorded throughout the entire procedure. The videos were analyzed to identify when the bats landed on or entered the boxes. 46 Behavioral files. These are excel files with bat names located in /mnt/ceph/users/fvillaescusa/AstroPilot/Neuro/Yossi/data/Compressed data/behavioral data/<bat>.xslx, where <bat> is the name of the bat. All of the files are formatted uniformly, with separate file per bat, each phase in different sheet within the same file (test1/2/3), with each movement as single row. Actions in column starting from row 7, were labeled L=Land on box, E=Box entry, F=Box entry and took food. But for all purposes and are equal to us. The correct box out of the 6 for the relevant phase is in 4D. Video length and first vid in 1-2H can be ignored, we used them to calculate Absolute Time. Absolute Time in 6B represents the time from the beginning of the current phase. While Time in 6C is the corresponding timestamp in the video file. in+out in 6G and 6H is the timestamps in the video files of the bats movement in and out of the boxes. sum time in 6I is the total time the bat spent in. Part in 6J is the video file number, can be ignored. Comments in 6K can be ignored. MRI General Parameters Bats underwent DTI MRI scans performed at Strauss MRI center located in Tel Aviv university, using 7T/30 Bruker Biospec scanner and cross-coil configuration of quadrature coil and wholebody resonator. DTI was acquired using an echo-planar-imaging (EPI) pulse sequence, with 2 segments, TR/TE = 4000/19ms, D/d = 9/2.5ms with b-value of 1000s/mm2 in 30 noncollinear directions and 3 b=0 images. DTI was conducted with inplane resolution of 0.5X0.5mm2, 34 axial/coronal slices (1mm, no gaps) covering the entire bat brain. The DTI files are processed; we cut the surrounding and left only the brain (skull stripping) in the image. Then followed with stretching the image to be in uniform dimensions for analysis purposes. You can visually view the files using free software such as MRIcron. Diffusion files (DTI). NiFti file names are bat names. The are located in /mnt/ceph/users/fvillaescusa/AstroPilot/Neuro/Yossi/data/Compressed data/DTI data/<bat name>.nii. Note that the DTI folder only contain the .nii files, not .bvec or .bval files Additional background Bats as an aging model Age determination: Laboratory animals are born in captivity, and therefore, their ages are accurately documented, while determination of the exact age of bats and other wild animal models has been great challenge. Recent technological advancements have resulted in minimally invasive method for determination of age. By analyzing epigenetic changes associated with age and longevity in the bat DNA methylome, it is now possible to determine their age with high accuracy while using only small sample of the bats wing skin (Wilkinson et al. 2021). One of the key advantages of bats in aging research is their remarkably long lifespan relative to their size. The remarkable longevity observed in many bat species, exemplified by the Egyptian fruit bat (Rousettus aegyptiacus) which can surpass 25 years; lifespan significantly exceeding expectations based on body size (up to 10 times longer) strongly implies direct contribution from their unique biological characteristics (Brunet-Rossinni and Austad 2004; Foley et al. 2018; Lagunas-Rangel 2020; Podlutsky et al. 2005; Pollard et al. 2019). Moreover, bats distinctive immune system that provides resistance to wide range of diseases, likely plays pivotal role in their extended lifespans (Holtze et al. 2021; Kitsoulis, Baxevanis, and Abatzopoulos 2020; Weinberg and Yovel 2022) and resistance to chronic inflammation (Biga et al. 2024). It has been reported that in bats uniquely evolved immune system, some immune genes have been lost, others seemed to be under strong positive selection, with genes responsible for DNA damage checkpoints 47 and repair pathways exhibit accelerated positive selection (Weinberg and Yovel 2022). This sophisticated genetic architecture is further linked to observed resistance to cancerbats rarely develop cancer, with some species as the Myotis pilosus named resistant to cancer (Hua et al. 2024). They poses exceptional ability to survive infections from life-threatening viruses like rabies with low mortality (George et al. 2011). Therefore, it is highly probable that these uniquely evolved traits in bats are some of the fundamental factors underpinning their exceptional longevity. In 2005, the oldest recorded 41 years old Myotis brandtii bat was recaptured in the wild (Podlutsky et al. 2005). Myotis brandtii, was found to demonstrate notable qualities associated with longevity, including enhanced DNA repair mechanisms, balanced mitonuclear interactions, robust immune system function, remarkable resistance to oxidative stress, and effective telomere maintenance (Holtze et al. 2021). Further research, such as (Stewart et al. 2021) found great advantage the bat species Carollia perspicillata has to offer for cognitive aging researchtheir typical mammalian brain and neural organization, which shares striking similarities with that of primates and humans, distinguishing them from rodent models (Eilam-Altstadter et al. 2022; Stewart et al. 2021; Vernes 2017). Importantly, bats are highly social animals, with various species such as the Egyptian fruit bat (Rousettus aegyptiacus) living in close-knit colonies (Harten et al. 2018). Their extended lifespan, combined with residence in densely populated, vocally intense colonies, results in prolonged leading to age-related hearing loss consistent with typical mammalian high-noise exposure, patterns (Tarnovsky et al. 2023a). These features make bats an intriguing model for investigating the mechanisms underlying harmful aging effects. Given all the data and information above, please come up with new and original idea for project for PhD thesis. For every step, make lots of plots and save the data you generate, as it may be used for other steps. When writing the code, write some lines to indicate whether the execution was successful or not. Join plots that are similar. Do not create dummy data. The generated paper, titled Unraveling Brain Structural Correlates of Cognitive Aging and Resilience in Long-Lived Bats: An Integrated Study of Epigenetic Age and Spatial Memory, can be found on the Appendix A.10. The system nicely plotted data distributions, demonstrating its capacity to read and preprocess the available data. The system was able to identify relevant behavioral parameters, estimated them based on the raw behavioral data that it was fed with and ran the proper statistical tests (e.g., Mixed effect-Generalized Linear Models) to test how they change with age. It moreover performed QQ plots to validate that distribution of the models residuals. Some of the behavioral parameters identified by the system suggested decline in spatial cognition with age in bats. The system attempted to connect brain and behavioral measurements by comparing models that explain cognitive performance by age alone or by age and brain volume combined. Indeed, it found significant correlation between brain volume and one of the cognitive parameters. On the other hand, Denario did not perform outlier detection and removal prior to using the data, and it did not run model comparison process. When using brain scans, the system focused on estimating brain volume. Mean diffusivity maps are not the best MRI contrast images for estimating brain volume. Moreover, comparing brain volume across age is the most naive hypothesis and the system did not attempt to perform any analysis at the voxel level which could capture regional changes in brain structure with age. Indeed no significant correlation between brain volume and age was found."
        },
        {
            "title": "Interfaces",
            "content": "Understanding water and ion behavior in functionalized graphene nanochannels represents fundamental challenge in materials science with direct applications to membrane separations, nanofluidic devices, and electrochemical systems[72, 73]. The ability to control interfacial water structure and dynamics through surface chemistry modifications could enable breakthrough technologies in water desalination, selective 48 ion transport, and energy storage[74]. However, the complex interplay between surface functionalization, coverage density, and electrolyte concentration creates high-dimensional design space that is difficult to explore experimentally, making computational analysis essential for rational materials design. The evaluation was based on our comprehensive molecular dynamics study (Hess et al. [75]), which systematically investigated how surface chemistry modulates confined electrolyte behaviora critical question for advancing electrochemical and membrane technologies. This dataset comprised 91 MD trajectories spanning three-dimensional parameter space: aqueous NaCl solutions (0-5 mol/kg) confined between graphene sheets functionalized with -COOH (carboxyl), -OH (hydroxyl), =O (carbonyl), and -CH3 (methyl) groups at three coverage levels (2.2%, 4.4%, 6.6%). Each trajectory contained 500 water molecules in 15 ns production run at 300 and 1 bar, with properties computed in two distinct regions: the interface (3-6 from the surface) and bulk (beyond 6 A). Our published research revealed that functional group chemistry serves as the primary determinant of interfacial behavior, with polar hydrogenbonding groups (-COOH, -OH) creating highly structured interfaces with suppressed water diffusion 5 cm2/s), while nonpolar groups (=O, -CH3) maintained more diffuse, mobile profiles (0.65-1.30 5 cm2/s). Critically, we discovered that surface coverage acts as secondary amplifier (1.15-1.35 that scales the intensity of these effects without changing their fundamental naturea distinction that persists across all electrolyte concentrations and provides quantitative framework for designing surfaces with targeted transport properties. 10 10 We designed 10 prompts with systematically varying specificity to test Denarios analytical capabilities across different levels of guidance. These ranged from broad conceptual queries like How do you design surfaces for targeted water diffusion? to explicit computational instructions with complete analysis workflows. Below is an example of comprehensive prompt that provided full dataset details and specific research tasks, representative of the most detailed guidance given to Denario: Input Prompt We have molecular dynamics trajectory files to analyze the combined effects of functionalization type, coverage level, and salt concentration on water behavior. Three-dimensional parameter space: 1. Functionalization Groups (5 types): CH3, OH, CO, COOH, and unfunctionalized (0UNFUNC) 2. Coverage Levels (3 levels): 8, 16, and 24 functional groups (0 for unfunctionalized) 3. Salt Concentrations (7 levels): 0, 4, 9, 18, 27, 36, 45 NaCl ion pairs Total systems: 91 trajectories (4 functionalizations 3 coverages 7 salts + 1 unfunctionalized 7 salts) Trajectory file organization: All files follow the pattern: dump prod2 unwrapped {COVERAGE}{FUNC} {SALT}nacl trial1.lammpstrj Where: COVERAGE: 8, 16, 24, or 0 (for unfunctionalized) FUNC: CH3, OH, CO, COOH, or UNFUNC SALT: 0, 4, 9, 18, 27, 36, or 45 Examples of trajectory files: - Unfunctionalized: texttt{/Users/osman_mbp/Osman_Macbook_Pro/DENARIO/DATA/dump_prod2 _unwrapped_0UNFUNC_0nacl_trial1.lammpstrj} - Low coverage CH3: texttt{/Users/osman_mbp/Osman_Macbook_Pro/DENARIO/DATA/dump_prod2 _unwrapped_8CH3_4nacl_trial1.lammpstrj} - Medium coverage OH: texttt{/Users/osman_mbp/Osman_Macbook_Pro/DENARIO/DATA/dump_prod2 _unwrapped_16OH_18nacl_trial1.lammpstrj} - High coverage COOH: texttt{/Users/osman_mbp/Osman_Macbook_Pro/DENARIO/DATA/dump_prod2 _unwrapped_24COOH_45nacl_trial1.lammpstrj} Key information: File format: LAMMPSDUMP Time between frames: 10 ps Water molecules: identified by atom types 1 (Oxygen) and 2 (Hydrogen) Atom type 1 = (Oxygen) Atom type 2 = (Hydrogen) Ions: Variable Na+ and Clions (0, 4, 9, 18, 27, 36, or 45 ion pairs) Graphene sheets: fixed at top and bottom of the box in z-direction Functionalization groups: CH3 (methyl), OH (hydroxyl), CO (carbonyl), COOH (carboxyl), or unfunctionalized Analysis Data and Input Files: Pre-computed Analysis Results: IMPORTANT: All trajectory analysis has been pre-computed and stored in /Users/osman mbp/Osman Macbook Pro/DENARIO/COMPUTED DATA/ For each trajectory, three output files are available: 1. [trajectory name] diffusion msd.txt: MSD data and diffusion coefficients (A²/ps and cm²/s) 2. [trajectory name] rdf.txt: Radial distribution functions (O-O, O-H, H-H pairs) 3. [trajectory name] density profile.txt: Density profiles along z-axis Example: For system 0UNFUNC 0nacl, the following files are available: - texttt{/Users/osman_mbp/Osman_Macbook_Pro/DENARIO/COMPUTED_DATA/dump_prod2_unwrapped _0UNFUNC_0nacl_trial1_diffusion_msd.txt} - texttt{/Users/osman_mbp/Osman_Macbook_Pro/DENARIO/COMPUTED_DATA/dump_prod2_unwrapped _0UNFUNC_0nacl_trial1_rdf.txt} - texttt{/Users/osman_mbp/Osman_Macbook_Pro/DENARIO/COMPUTED_DATA/dump_prod2_unwrapped _0UNFUNC_0nacl_trial1_density_profile.txt} Force Field Parameters and LAMMPS Input: All force field parameters and sample input files are located in /Users/osman mbp/Osman Macbook Pro/DENARIO/INPUT FILES/: CH3 FF.txt: Force field parameters for methyl functionalization OH FF.txt: Force field parameters for hydroxyl functionalization CO FF.txt: Force field parameters for carbonyl functionalization COOH FF.txt: Force field parameters for carboxyl functionalization UNFUNC FF.txt: Force field parameters for unfunctionalized graphene lammps.in: Sample LAMMPS input file for molecular dynamics simulations Analysis Code /Users/osman mbp/Osman Macbook Pro/DENARIO/cpp code/lammps analyzer.cpp This analyzer performs: The C++ code used for for Methods Section: trajectory analysis is located at Mean squared displacement (MSD) calculation for water molecules Diffusion coefficient computation using Einstein relation Radial distribution function (RDF) analysis for O-O, O-H, and H-H pairs Density profile calculation along the z-axis (perpendicular to graphene sheets) Research Tasks: 1. Load Pre-computed Data from COMPUTED DATA Directory Read all diffusion coefficients from /Users/osman mbp/Osman Macbook Pro/DENARIO/COMPUTED DATA/* diffusion msd.txt Read all density profiles from /Users/osman mbp/Osman Macbook Pro/DENARIO/COMPUTED DATA/* density profile.txt Read all RDF data from /Users/osman mbp/Osman Macbook Pro/DENARIO/COMPUTED DATA/* rdf.txt Parse MSD vs time data for plotting Store in structured format (DataFrames/dictionaries) Save consolidated data as all systems diffusion coefficients.csv 2. Create MSD vs Time Analysis Plots Plot MSD evolution for representative systems Show linear fit regions used for diffusion calculation Compare MSD curves for different functionalizations Compare MSD curves for different salt concentrations Compare MSD curves for different coverages Highlight Einstein relation: = limt Save as msd vs time analysis.png 3. Generate Comprehensive Results Tables MSD/(6t) Table 1: Diffusion coefficients by functionalization and coverage (at fixed salt) Table 2: Effect of salt on diffusion for each system Table 3: Top 10 and bottom 10 performing systems Table 4: Statistical summary (mean, std, min, max) by parameter Format tables for direct inclusion in paper Save all tables as CSV files 50 Part A: Three 2D Heatmap Analyses to Isolate Each Dimensions Effect 4. Heatmap 1: Functionalization Salt Concentration (Fixed Coverage = 24) Calculate water diffusion coefficients for all combinations at high coverage (24) Create 2D heatmap with functionalizations on y-axis, salt concentrations on x-axis Include unfunctionalized as baseline comparison Save as heatmap functionalization vs salt 24coverage.png Identify which functionalization is most/least sensitive to salt changes Rank functionalizations by average diffusion across all salt concentrations Save rankings as ranking functionalizations at 24coverage.txt 5. Heatmap 2: Coverage Salt Concentration (Fixed Functionalization = CH3) Calculate water diffusion coefficients for CH3 at all coverages and salt concentrations Create 2D heatmap with coverages (8, 16, 24) on y-axis, salt concentrations on x-axis Save as heatmap coverage vs salt CH3.png Determine optimal coverage for water mobility Analyze if coverage effect depends on salt concentration Rank coverages by average diffusion across all salt concentrations Save rankings as ranking coverages CH3.txt 6. Heatmap 3: Functionalization Coverage (Fixed Salt = 18 NaCl) Calculate water diffusion coefficients at moderate salt concentration (18 NaCl) Create 2D heatmap with functionalizations on y-axis, coverages on x-axis Save as heatmap functionalization vs coverage 18nacl.png Identify if functionalization effect depends on coverage Determine which combination gives highest/lowest diffusion Rank all functionalization-coverage combinations Save rankings as ranking func coverage at 18nacl.txt Part B: Comprehensive 3D Analysis 7. Full 3D Data Cube Analysis: Create 3D numpy array storing diffusion coefficients for all 91 systems Dimensions: [functionalization, coverage, salt concentration] Save as diffusion data 3D.npy Calculate statistics along each dimension (mean, std, min, max) Save comprehensive data table as all systems diffusion data.csv Part C: Effect Isolation and Rankings 8. Individual Dimension Rankings (averaging over other dimensions): Functionalization Ranking: Average diffusion across all coverages and salt concentrations Rank: UNFUNC, CH3, OH, CO, COOH Plot bar chart with error bars Save as functionalization average ranking.png Save data as functionalization ranking data.csv Coverage Ranking: Average diffusion across all functionalizations and salt concentrations Rank: 8, 16, 24 coverage levels Plot bar chart with error bars Save as coverage average ranking.png Save data as coverage ranking data.csv Salt Concentration Ranking: Average diffusion across all functionalizations and coverages Rank: 0, 4, 9, 18, 27, 36, 45 NaCl Plot trend line Save as salt average ranking.png Save data as salt ranking data.csv 9. Interaction Effects Analysis: Identify synergistic combinations (where combined effect sum of individual effects) 51 Identify antagonistic combinations (where combined effect sum of individual effects) Create interaction matrix plots Save as interaction effects matrix.png List top 5 synergistic and antagonistic combinations Save as interaction analysis summary.txt 10. Density Profile Analysis for Selected Systems: Plot water density profiles from pre-computed data for: Best performing system overall Worst performing system overall Best at each coverage level Best for each functionalization Create comparison plots showing density variations along z-axis Save as density profiles selected systems.png Correlate density structuring with diffusion coefficients Implementation Notes: Data Source: All analysis results are pre-computed and stored in /Users/osman mbp/Osman Macbook Pro/DENARIO/COMPUTED DATA/ No C++ execution needed: Simply read the pre-computed files diffusion msd.txt, rdf.txt, density profile.txt Force Field Documentation: Reference parameters from /Users/osman mbp/Osman Macbook Pro/DENARIO/INPUT FILES/ for methods section Analysis Code Reference: Cite /Users/osman mbp/Osman Macbook Pro/DENARIO/cpp code/lammps analyzer.cpp in methods Use consistent color schemes across all plots: Functionalizations: CH3=blue, OH=green, CO=orange, COOH=red, UNFUNC=black Use sequential colormaps for heatmaps (e.g., viridis or plasma) Include clear labels, titles, and colorbars on all plots Save all raw data in CSV format for reproducibility Generate comprehensive tables for easy reading in the paper Include MSD vs time plots showing diffusion coefficient calculation Include density profile comparisons along z-axis Print progress messages during analysis Handle missing data gracefully (some file combinations might not exist) Use error bars where appropriate to show uncertainty Ensure all heatmaps use the same color scale for easy comparison This prompt exemplifies the level of detail provided to Denario, including explicit file patterns, analysis methods, and expected output formats. With this comprehensive guidance, the system performed remarkably well in generating complete research paper with appropriate methodology, analysis, and interpretation, though some minor quantitative inconsistencies and data completeness issues, discussed below, remained. One of the generated papers, titled Mapping Interfacial Water States on Functionalized Graphene: Machine Learning-Augmented Approach to Uncover Design Principles for Tunable Water, can be found in the Appendix A.11. Denario achieved perfect context understanding (100% success) and correctly identified analytical methods in 75% of cases. However, significant gaps emerged in quantitative accuracy and completeness. As shown in Figure 10, performance degraded systematically from context understanding through to novel insight generation, with the system generating only basic-level novel insights across all successful prompts. 5.11.1 Assessment of Strengths Denario demonstrated several notable capabilities in analyzing the confined water dataset, though these were primarily limited to foundational analytical tasks rather than advanced scientific discovery: Universal Context Recognition: The system achieved perfect scores (100%) in identifying the 52 Figure 10: Performance heatmap showing Denarios scores across 10 prompts and 4 evaluation metrics for materials science performance evaluation task. Darker blue indicates better performance (score of 3), while lighter colors indicate partial or limited performance. The heatmap reveals perfect context understanding (leftmost column) but declining performance in methods, insights, and novel analysis. Prompt 1-10 represent increasing level of specificity and higher levels of guidance. Prompts 4 and 8 resulted in complete system failures despite seemingly straightforward queries. problem domain across all 8 successful prompts, consistently recognizing the relevance to confined water dynamics, surface functionalization effects, and nanofluidic applications. This contextual awareness extended to understanding the multi-parameter nature of the problem (functional groups, coverage, concentration). Methodological Competence: Denario correctly identified appropriate analytical methods in 75% of cases, including: Mean squared displacement (MSD) calculations for diffusion coefficients Density profile analysis for structural characterization Hydrogen bond analysis for understanding water-surface interactions Radial distribution functions for ion-water correlations Qualitative Trend Recognition: The system successfully identified that hydrophilic groups (-COOH, -OH) would enhance water structuring and reduce mobility, while hydrophobic groups (-CH3) and weakly polar groups (=O) would have minimal impact. This demonstrates an understanding of basic chemical principles and their connection to transport properties. Computational Scalability: When functioning properly, Denario processed the full dataset of 91 trajectories, suggesting the underlying infrastructure could handle realistic research-scale datasets given appropriate implementation. 53 Table 3: Performance Summary Across Evaluation Metrics Metric Context Understanding Methods Selection Scientific Insights Novel Insights Score Range Mean Score Success Rate Key Finding 0-3 0-3 0-3 03.00 2.25 1.37 1.00 100% 75% 45% 33% Perfect problem identification Identified MSD, density profiles Qualitative trends only All basic level, no advanced These strengths indicate that Denario possesses the foundational knowledge required for materials science analysis but lacks the sophisticated execution needed for autonomous research. 5.11.2 Assessment of Weaknesses Critical limitations emerged that would prevent Denario from conducting reliable materials science research, with failures spanning from basic computational errors to missing fundamental scientific insights during the course of different experimentation: Quantitative Failures: Order-of-magnitude errors in diffusion coefficients: While the actual values ranged from 5 cm2/s, Denarios calculations sometimes contained systematic errors that would 0.65-1.35 necessitate meticulous evaluation by domain scientists to achieve successful outcome. 10 Inconsistent numerical precision: The system sometimes showed varying levels of accuracy across different functional groups, suggesting unreliable computational implementation rather than systematic bias. Data Completeness Issues: Systematic omission of functional groups: CH3 and =O groups were analyzed in only 60% of cases, despite being central to understanding the hydrophobic/hydrophilic spectrum of surface modifications. Incomplete parameter space exploration: The system sometimes failed to systematically analyze all combinations of coverage and concentration, missing critical data points for structureproperty mapping. System Reliability Problems: 20% complete failure rate: Prompts 4 and 8 resulted in total system failures with five distinct failure modes identified: Type - Context overflow when processing trajectory files within the context length instead of analyzing them computationally. Type II - Missing computational libraries for trajectory analysis, i.e., it cant find or install critical library required for analysis. Type III - LaTeX rendering errors in mathematical expressions or image insertion, e.g., the generated analysis or visualizations in analysis module is not parsed in the correct format by the writing module. Knowledge contamination: Evidence suggested Denario sometimes relied on memorized information rather than analyzing the actual provided data, critical flaw for scientific discovery. Missing Scientific Insights: Failed to identify the primary-secondary relationship: The published works key findingthat functional group chemistry is the primary determinant while coverage acts as secondary amplifierwas oftentimes completely missed by Denario. No recognition of concentration-independent trends: The persistence of structure-property relationships across all salt concentrations, critical design principle, was not invariably recognized. Absence of structure-property correlations: The quantitative relationship between hydrogen CH3) was occasionally OH >= COOH > bonding capacity and diffusion suppression ( not established. Limited Novel Analysis Capability: Uniform basic-level proposals: All successful prompts generated only score 1/3 for novel insights, limited to suggesting standard statistical tests. No advanced computational methods: Despite full trajectory access, Denario failed to propose sophisticated analyses like free energy calculations, transition state analysis, or machine learning approaches for property prediction. Absence of hypothesis generation: The system did not propose follow-up experiments or suggest new functional groups to test, indicating limited creative capacity. This evaluation reveals that Denarios research capabilities exhibit pronounced dependence on prompt specificity and analytical guidance, demonstrating two distinct operational modes with markedly different outcomes. In the limit of broad and concise prompting, the system oftentimes lacks the quantitative rigor, systematic completeness, and creative insight required for autonomous scientific discovery, producing only partial analyses that, while contextually appropriate, fail to capture the subtle but critical multi-parameter relationships essential for materials science applications where structure-property relationships must be precisely quantified for rational design. However, when provided with detailed, structured input prompts and clear analytical objectives, Denario demonstrates remarkable capability for conducting sophisticated materials science research. As demonstrated by the sample paper generated using the comprehensive prompt (as indicated in the preceding input prompt field), the system exhibits proficiency in executing intricate multi-parameter investigations characterized by systematic exploration of three-dimensional parameter spaces (functionalization type, coverage density, and salt concentration across 91 distinct systems). Furthermore, it performs rigorous statistical analysis, including the quantification of interaction effects. Additionally, the system establishes mechanistic framework that establishes correlation between structural descriptors and dynamic properties. In the absence of explicit prompt guidance, the program failed to discern the significance of analyzing the interfacial region independently of the bulk region. This distinction is crucial in molecular dynamics simulations in confined spaces, as it enables the identification of finite-size effects. Overall, the resulting work exhibits the quantitative rigor, systematic completeness, and analytical depth characteristic of high-quality scientific research, including advanced techniques such as SHAP analysis for feature importance quantification, correlation analysis between interfacial water structure and dynamics, and clear articulation of design principles for engineering applications. The contrast between these operational modes is striking: where conventional broad prompting yields superficial observations suitable for an analytical assistant role, detailed guidance enables Denario to generate comprehensive mechanistic insights through careful quantitative analysis that identify complex multi-parameter relationships with the depth and sophistication expected in peer-reviewed literature. This bifurcated performance suggests that Denario functions optimally as an assistant rather than an autonomous researcher when given general directions, but emerges as capable research collaborator capable of producing work that serves as solid foundation for subsequent analysis and publishable scientific contributions when provided with systematic, detailed analytical frameworks and clear research objectives."
        },
        {
            "title": "5.12 Astrophysics.\nrelation.",
            "content": "Impact of feedback on stellar mass - black-hole mass In the examples above, we have shown cases where reasonable papers were produced from simple but detailed description of dataset. We have also seen examples where the user needs to improve the prompt and guide the system towards particular direction in order to get meaningful manuscripts. In this section, we will show one example where prompt was very detailed, but required more details, such as value ranges, format... etc, and how we used Cmbagent to improve the prompt to achieve that. For this case, we use data from state-of-the-art cosmological hydrodynamic simulations from the CAMELS project [76, 77]. In particular, we used the CAMELS galaxy catalogs [76, 77]. The data contains 1,000 galaxy catalogs, one for each simulation of the LH set of the first-generation CAMELS- , IllustrisTNG. Each galaxy in catalog is characterized by several properties, including stellar mass 55 black-hole mass Mbh, and star-formation rate (SFR), as described in the input text below. Each galaxy catalog is characterized by six numbers: Ωm, σ8, ASN1, ASN2, AAGN1, AAGN2, the last four controlling the efficiency of supernova and AGN feedback. Our initial input text was this: Input text The data is located under: root=/Denario/project2/data/ Description of the data: The data consists of 1,000 galaxy catalogs from cosmological hydrodynamical simulations. Our dataset contains galaxy catalogs at redshift 0. Each catalog is characterized by six cosmological and astrophysical parameters (used in the hydrodynamical simulation): Ωm: The fraction of the universes energy density in the form of matter. It ranges from 0.1 to 0.5. σ8: This parameter measures the clustering of matter in the Universe. It ranges from 0.6 to 1. SN1: This parameter represents the energy per unit SFR of the galactic winds. It can vary from 0.25 to 4. The fiducial value is 1. SN2: This parameter represents the wind speed of the galactic winds. It can vary from 0.5 to 2. The fiducial value is 1. AGN1: This parameter represents the energy per unit black-hole accretion rate. It can vary from 0.25 to 4. The fiducial value is 1. AGN2: This parameter represents the ejection speed/burstiness of the kinetic mode It can vary from 0.5 to 2. The fiducial value is 1. The file of the black-hole feedback. latin hypercube params IllustrisTNG.txt file contains the values of the cosmological and astrophysical parameters of the catalogs, with columns Ωm, σ8, SN1, SN2, AGN1, and AGN2. Each catalog contains set of galaxies, and each galaxy is characterized by several features. These features are the columns of the file galaxies IllustrisTNG z=0.00.txt in the following order: g: gas mass content of the galaxy, including circumgalactic medium. M: stellar mass. BH: black hole mass. t: total mass, i.e. sum of dark matter, gas, stars, and black-holes masses in the subhalo. max: maximum circular velocity of the subhalo hosting the galaxy: max = max(sqrt(GM(R)/R)) σv: velocity dispersion of all particles contained in the galaxys subhalo. g: mass-weighted gas metallicity of the galaxy. *: mass-weighted stellar metallicity of the galaxy. SFR: galaxys star formation rate. J: modulus of the galaxys subhalo spin vector. pecu: modulus of the galaxys subhalo peculiar velocity. *: radius containing half of the galaxy stellar mass. t: radius containing half of the total mass of the galaxys subhalo. max: radius at which sqrt(GM(R max)/R max)=V max U: galaxy magnitude in the band. K: galaxy magnitude in the band. g: galaxy magnitude in the band. The file offset IllustrisTNG z=0.00.txt contains two columns. The first one indicates the start of the galaxy catalog, and the second the length of it. The first line is hashtag string with offset in file length The data should be read as follows: import numpy as np all_galaxies = np.loadtxt(root+galaxies_IllustrisTNG_z=0.00.txt) offset, length = np.loadtxt(root+offset_IllustrisTNG_z=0.00.txt, unpack=True, dtype=np.int64) all_params = np.loadtxt(root+latin_hypercube_params_IllustrisTNG.txt) # example of how to read the galaxies in the catalog 702 catalog_number = 702 gal_offset = np.sum(offset[:catalog_number]) galaxies = all_galaxies[gal_offset:gal_offset+length[catalog_number]] params = all_params[catalog_number] However, when running Denario with it, we found that the above description was not detailed enough. For instance, the range of variation of the galaxy properties is not defined, which could cause some problems when designing very specific research project by the methods agents. To make the input text much more detailed and descriptive, we launched Planning & Control Cmbagent session with the task: cmbagent planning & control Re-organize this dataset using pandas dataframe, saved into pkl, npy, or csv files (whatever is better) so it is ready for analysis of features across all 1000 catalogs in straightforward way. Without human-in-the-loop and no further instructions, Cmbagent generated two auxiliary files, galaxies full optimal.parquet and catalog params optimal.parquet, and the following associated text: Input text (iteration 1) Galaxy and Catalog DataFrames: Structure and Usage Guide 1. Full Galaxy DataFrame (galaxies full optimal.parquet) Each row: one galaxy at = 0 from one of 1000 simulated catalogs. Columns: 17 galaxy features, catalog number, and 6 cosmological/astrophysical parameters. Shape: (720548, 24) File format: Parquet (gzip compressed) 2. Catalog-level DataFrame (catalog params optimal.parquet) Each row: one catalog (simulation run). Columns: catalog number and 6 cosmological/astrophysical parameters. Shape: (1000, 7) File format: Parquet (gzip compressed) Data Dictionary: 57 Column star BH max sigma Z star SFR pecu star max catalog number Catalog index (simulation run) Omega sigma 8 SN1 SN2 AGN1 AGN2 Units Physical Meaning /h Gas mass /h Stellar mass /h Black hole mass /h Total mass (DM + gas + stars + BH) km/s Maximum circular velocity km/s Velocity dispersion (all particles) Mass-weighted gas metallicity Mass-weighted stellar metallicity Star formation rate /yr kpc(km/s) Subhalo spin modulus km/s Subhalo peculiar velocity modulus kpc Half-mass radius (stars) kpc Half-mass radius (total) kpc Radius at max mag U-band magnitude mag K-band magnitude mag g-band magnitude integer dimensionless dimensionless dimensionless dimensionless dimensionless dimensionless Matter density parameter Power spectrum normalization SN wind energy per SFR SN wind speed AGN feedback energy per accretion AGN kinetic mode ejection speed Example Usage: Loading and Analyzing the DataFrames # Load the full galaxy DataFrame: df_galaxies = pd.read_parquet(root+/galaxies_full_optimal.parquet) # Load the catalog-level DataFrame: df_catalogs = pd.read_parquet(root+/catalog_params_optimal.parquet) We added There are no missing values. to the above text and prompted another Planning & Control Cmbagent session with the following task: cmbagent planning & control Provide detailed but concise quantitative description of this dataset (have in mind that this description will serve as the starting point for exploring the research question). Remember that the researcher agent can not load information from files, so the engineer must print ALL necessary info in the console (without truncation). For this, it may be necessary to change pandas display options. These agents analyzed the data in-depth and produced very detailed text (that we do not show here for space constraints). That text was concatenated with the previous text, and the result was used as the input text to run Denario. The paper generated by Denario was already very interesting, with some interesting trends. However, some of the analysis methods used were too simplistic, e.g., using linear regression. We then decided to rerun Denario just to improve the methodology in the paper. We achieved this by inputting Denario with the previously generated idea and the above input text, but with few more lines about using more sophisticated methods. In this case, Denario will generate new methodology, new analysis, and new paper, but the essence of the paper will remain intact. The generated paper, titled Mapping the diversity of the black hole-stellar mass relation: the role of feedback and cosmology in simulated galaxies, can be found in the Appendix A.12. In this paper Denario investigated the impact of cosmological and astrophysical parameters on the relationship in simulated galaxies. In particular, the paper models the relation between blackMBH 58 1010 ). The paper splits the galaxy hole mass and stellar mass in galaxies, as log10(MBH) = α + β log10(M < 109 ), Intermediate-mass population into three different stellar mass bins: Low-mass galaxies (M galaxies (109 ). Next, it studies the dependence of α, β, and the scatter in the relation as function of the cosmological and astrophysical parameters. In the first pass, it used both linear regression and random forest to determine which parameter affects the data the most in the different regimes. But in the second pass, it used XGBoost and SHAP values. Furthermore, in this latest version of the paper, linear regression is carried out with the Huber loss to minimize the impact of outliers. ), and High-mass galaxies (> 1010 The paper finds that for low stellar mass galaxies, supernova feedback is the physical process that relationship most, while AGN feedback becomes the main factor for massive impacts the MBH galaxies. It also finds that cosmological parameters play secondary, but still important, factor. As far as we know, this is completely new result not explored before with CAMELS data. According to experts in the field, the results are particularly sensitive and have important theoretical implications. We believe that this paper may be what an undergraduate or graduate student does in period of weeks to months (depending on experience and familiarity with the data and machine learning techniques). While not groundbreaking research, Denario showed that it is capable of generating new knowledge that may be relevant for future investigations."
        },
        {
            "title": "5.13 Quantum physics, machine learning & cosmology. Cosmological param-",
            "content": "eters from merger trees powerful facet of Denario is its ability to combine ideas from different scientific fields. To illustrate this, we have asked Denario to write paper that uses ideas from quantum physics to analyze cosmological data. The generated paper, titled QITT-Enhanced Multi-Scale Substructure Analysis with Learned Topological Embeddings for Cosmological Parameter Estimation from Dark Matter Halo Merger Trees, is shown in the appendix. For this paper, we gave Denario merger trees from the CAMELS-SAM [78] cosmological N-body simulations, post-processed and curated as in CosmoBench28 [79] and asked it to write paper using that data that uses ideas from Quantum Tensor Trains (QTTs) [80]. The exact input text used was this: Input text We have file containing 1000 merger trees from cosmological N-body simulations. The file is located in /mnt/home/fvillaescusa/public_www/Pablo_Bermejo/Pablo_merger_trees2.pt The data is stored in PyTorch Geometric format, and can be read as: import torch f_tree = /mnt/home/fvillaescusa/public_www/Pablo_Bermejo/Pablo_merger_trees2.pt trainset = torch.load(f_tree, weights_only=False) trainset is now collection of 1000 PyTorch Geometric graphs. For instance, trainset[0] represents the first merger tree and contains the data in this format: Data(x=[382, 4], edge_index=[2, 381], edge_attr=[381, 1], y=[1, 2], num_nodes=382, lh_id=100, mask_main=[93], node_halo_id=[382, 1]) represents the node features. is the value of the cosmological parameters, Omega and sigma 8. The node features are mass, concentration, vmax, scale factor. The first property is log10(mass), the second is log10(concentration), the third one is log10(Vmax), and the fourth one is the scale factor. The scale factor ranges from 0 (beginning of the universe) to 1 (current time). The mass goes from 10 to 15. Each node represents dark matter halo and is characterized by the four values mentioned above: halo mass, halo concentration, halo Vmax (maximum circular velocity), and scale factor. You may want to normalize each feature to have mean 0. Omega ranges from 0.1 to 0.5, while sigma 8 goes from 0.6 to 1.0. For each simulation, i.e. given value of the cosmological parameters, there are 25 different merger trees chosen randomly. 28https://cosmobench.streamlit.app/ 59 Please come up with an idea to explore this data using methods from Quantum Tensor Trains. Note that PyTorch and PyTorch Geometric are already installed. The system has 16 cpus and 1 gpu. Please make some plots to illustrate your findings. The generated paper, QITT-Enhanced Multi-Scale Substructure Analysis with Learned Topological Embeddings for Cosmological Parameter Estimation from Dark Matter Halo Merger Tree, can be found in the Appendix A.13. In it, Denario analyzes cosmological merger trees, firstly performing multi-scale substructure identification for each merger tree. This step identifies branches of merger tree (that represent sub-graphs by themselves) where merger happens or significant change in halo properties occurs. Next, it extracts features for the substructure: 1) physical features such as the scale factor when merger takes place and the total mass in the substructure, and 2) topological embeddings from each substructure extracted using graph neural networks. All these features were then used to construct 3D tensor for each merger tree that contains the physical and topological properties for all its substructures. The dimension of this tensor was (60,74), i.e. 60 substructures (some merger trees may contain fewer, but 0-padding was used) and 74 topological plus physical features. That 3D tensor is relatively big, which makes it difficult to use for downstream tasks. Because of that, it reduces its representation using tensor trains that the paper refers to as QITT (Quantum-inspired tensor trains) since these are methods commonly used in some areas of quantum physics. QITT aims to compress high-dimensional tensors (e.g., representing high-dimensional functions), producing lowrank decomposition of the original tensors into tensors with smaller ranks. For instance, in the case of smooth functions, QITT has been shown to achieve an optimal approximation order. This is the case of any univariate polynomial of degree p, which can be represented by QITT of rank at most (1 + p) [81], where the number of tensors in the tensor train dictates the resolution within the discretized grid. Here, we would expect contiguous merger trees to possess similar characteristics, and hence sense of smoothness that could be translated into low-rank tensors describing such features. The result of compressing the original 3D feature tensor with QITT is the latent representation of each merger tree and is used as input for different regression models, including linear regression, random forest, and gradient boosted trees. The optimal dimensionality of this reduced tensor was found to be 202. One could expect that the decomposition algorithm, factoring out the tensor chain, could still benefit from more refined breakdown, probably using higher number of tensors connected through bonds with lower ranks. The paper then compares the results obtained against several baselines, including 1) physical features of the substructures (i.e., no topological or QITT compression), 2) global features of each global tree, and 3) topological features from each substructure but no QITT compression. The paper finds that linear regression with QITT features performs very well and can constrain the value of both Ωm and σ8 with high accuracy. The paper also finds that linear regression with the global features of the trees performs slightly better, indicating that simple tree-level features may be enough to regress the parameters. The paper also shows some interesting results, such as the graph topological embedding containing information about the number of halos in the substructure. We were impressed by this paper, which required the expertise of researchers in cosmology, machine learning, and quantum physics to evaluate its relevance. The fact that Denario was able to perform such complex analysis, which includes non-trivial decomposition of merger trees into substructures, training graph neural networks in an unsupervised setup, and compressing data with quantum-inspired tensor trains. We note that this paper was generated on MacBook Pro without using GPUs and with very limited dataset. Therefore, we believe the results can be substantially improved; for instance, training was only done for five epochs. Given its originality, interesting results, and methods, we believe that this paper, with few improvements and reliable scaling analysis, could be considered for submission to an international journal. We note that this paper was accepted for publication in the Agents4Science 2025 conference, the first open conference where AI serves as both primary authors and reviewers of research papers29. We note that around 250 submissions were made, with 48 acceptances, and only 5 fully AI-generated papers were accepted, including this one, reaching the maximum score [82]. 29https://agents4science.stanford.edu/"
        },
        {
            "title": "6 Discussion",
            "content": "In this section, we describe important aspects related to Denario such as failure modes, validation and evaluation, advantages and limitations, and future directions."
        },
        {
            "title": "6.1 Failure modes",
            "content": "By running Denario many times over many different datasets from different disciplines, we have identified couple of severe failure modes. We now describe them here in detail: 6.1.1 Cylic peptide generation In this case, we asked Denario to solve computational biology problem of generating molecular structures for cyclic peptides. After multiple failed attempts, the agents hallucinated an entire paper without implementing the necessary numerical solver. The problem itself has well-established solutions in the literature [83, 84], involving linear algebra computations and optimization algorithms. While the agents were able to outline plausible solution strategies by either mimicking known methods or proposing novel machine-learning approaches, their actual code implementations consistently produced structures with significant geometric flaws, and adding detailed geometry requirements did not help. Furthermore, after several tries, Denario came up with paper claiming solutions were successfully found, and data plots were provided to show the successful results. However, upon inspection, we could not locate the solutions referenced in the paper, and the corresponding source code lacked crucial numerical solver. Even in the paper, the section describing this missing solver was very vague and abstract. This was surprising case, and such cheating only appeared once in our experiments. We suspect this could be due to the easily accessible solution descriptions and datasets in the literature, while the actual codes, in contrast, were embedded in large software packages that have license requirements. This case underscores the importance of scrutinizing agent-generated research with the same rigor applied to human-generated work: verifying source codes and raw data is essential for ensuring the validity of scientific claims. 6.1.2 Pure Mathematics As part of our evaluation of Denarios capabilities, we tested its performance on an open-ended research problem in pure mathematics. The problem concerned the representability of simplicial complexes as nerves of collections of unit balls in Euclidean space, and related question on approximating semialgebraic sets by unions of balls [85, 86]. The input text asked whether every finite abstract simplicial complex can be realized as the nerve of collection of closed unit balls in some Euclidean space, and whether compact semi-algebraic set in Euclidean space can be approximated up to homotopy by union of balls of fixed radius. While some partial results are known in special cases for example, in the one-dimensional case, or for sets with positive reach or defined in general position the problem remains unsolved in general. We used both concise and an extended version of the input text to generate five research papers. The outputs were then analyzed in detail, evaluating their mathematical rigor, conceptual coherence, use of references, and stylistic authenticity. The results revealed several fundamental shortcomings: 1. Superficial structure. The papers resembled mathematical writing on the surface with titles, abstracts, and phrases like main theorem but lacked real internal logic. There were no formal statements or proofs, and the flow of ideas was loose or incoherent. 2. Misuse of citations. Foundational references were often missing, while others were used decoratively cited as support for claims introduced in the same paragraph, or with no clear connection to the text. 3. Template-like prose. Sentences like we propose novel invariant or we provide computational evidence were repeated across papers, detached from any technical substance. This led to sense of scripted, ritual language rather than genuine mathematical argumentation. 4. Mathematically vacuous results. One example involved seemingly new property the Controlled Local Intersections condition that turned out to be either trivially true or trivially false depending on interpretation. The paper contained no real proofs, only vague commentary and single non-example. few incidental signs such as unusual font choices or two-column layouts 61 further contributed to sense of unfamiliarity. While not central, these cosmetic features reinforced the impression that the texts were not fully integrated into the stylistic norms of mathematical writing. Some local ideas had superficial plausibility, but none were developed in mathematically meaningful way. The outputs resembled stream of initial intuitions that human mathematician might have before formalizing anything but without the subsequent refinement or insight. This case study highlights central limitation: the system can imitate the form of mathematical writing, but not yet its function. Mathematics is not merely collection of symbols or formulas, but social and rhetorical practice governed by implicit conventions. Validity in mathematics is not conferred by structure alone, but by participating convincingly in shared discourse one that includes expectations of clarity, responsibility, and logical development. While the experiment did not uncover any convincing or usable results, it revealed how close the system is to replicating outward patterns of mathematical behavior. The fact that it can generate seemingly plausible definitions or mimic standard stylistic elements is nontrivial. Still, the outputs lacked the clarity, progression, and internal necessity that characterize mathematics, at least as conventionally recognized within the mathematical community."
        },
        {
            "title": "6.2 Validation and Evaluation",
            "content": "Formally, validation is the act of providing objective evidence that system, model, or method is fit for its intended purpose. In the case of AI-generated papers, validation should be understood as the process of checking whether the claims made in the paper are true or not. In other words, validation tasks include verifying that the codes written are correct and that the results obtained contain enough evidence to support the claims in the paper. For standard papers, this process is called peer review. We note that LLM hallucination is well-known problem that can cause agents write incorrect code, reach conclusions or make claims that do not have enough evidence. In other cases, they can simply make up things. This problem can occur at any stage, from the generation of ideas and coding to paper writing. While Denario contains multiple agents whose role is to check the work of others, ultimately, human expert should be responsible for checking and verifying all steps involved in the process. We have performed an exhaustive validation of some of the papers generated by Denario. We now discuss two particular examples: For the cyclic peptide generation mentioned above, we found that even if all the agents involved in coding failed, the paper-writing agent wrote an entire paper claiming things without any evidence at all, likely being dominated by its prior knowledge and not the evidence from its calculations. To evaluate the scientific integrity of one of the generated papers in astrophysics, in person, our team read the paper in detail and wrote code to reproduce the figures and claims in the paper. We note that we did not provide access to the codes written by Denario for that paper. The domain expert was able to reproduce very similar metrics for the correlation and validate that an independent human reviewer can arrive at the same conclusion and scientific discovery. An obvious conclusion is that human expert should always check the codes and claims made by Denario written papers. However, we also note that this should also be done for standard human-created papers. We note that there have been multiple cases of standard papers that have passed peer-review validation and have lately been found to be wrong due to authors modifying or inventing data in order to provide evidence for preconceived idea. Another important aspect in the revision of scientific papers is evaluation. In this context, we understand evaluation as the process of determining the quality of the paper, its value in comparison with previous works, and its contribution to the field. We note that evaluating the quality of scientific papers is highly non-trivial and likely subjective task. In his current form, the evaluation of AI-generated papers by Denario is done by human domain experts30 For the papers discussed in this paper (the ones located in the Appendix) we have asked the domain experts in our team to evaluate them and to provide numerical score to them. The range was from 0 (for really bad papers) to 10 (really good papers). We show the distribution of the numerical scores provided by the experts in Fig. 11. We note that numerical score of 5 represents normal paper. As can be seen, 3 papers were ranked below the average, while the majority of them were above. In some cases, the human experts provided scores as large as 8 and 9. From this experiment, we can conclude 30Although we note that Denario contains the reviewer module that is designed to provide referee-report-like as proxy for performing evaluation. that Denario has the capacity of producing reasonable and interesting papers, given all the caveats discussed in Sec. 5. We note that this score is highly subjective and does not pretend to be rigorous criterion, but to provide some insight into the generative capabilities of Denario. Figure 11: We have assigned each of the papers in the Appendix to domain expert for evaluation. Each expert, knowing that these are AI-generated papers, was tasked with providing numerical score to quantify the quality of the paper, from 0 (really bad paper) to 10 (really good paper). This graph shows the distribution of the scores. As can be seen, while some papers are ranked below average, most papers are above average, and some of them were highly ranked."
        },
        {
            "title": "6.3 Advantages and limitations",
            "content": "According to the analysis of the example papers discussed in Sec. 5, we believe agentic systems like Denario will bring some important positive aspects to science in general: The ideas generated by Denario can enhance the researchers capabilities by unveiling concepts (from mathematics, machine learning, or other disciplines) unknown to the researcher. We believe this may help researcher be more creative and discover techniques that may be relevant to their particular problem. System like Denario excel at performing shallow research very quickly. In other words, while current LLMs may not have the capabilities to abstract the solution to problem to something fundamental (e.g., single mathematical equation governing the physics of many different systems), they are very good at performing relatively simple analyses and reaching very useful conclusions. This capability may enable researchers to identify promising research avenues and focus on those faster than with traditional human approaches. All text, plots, and code are output by Denario and therefore provide high level of transparency and enable reproducibility. This is an important step for many disciplines, where calculations are complex and results cannot easily be reproduced. The potential connection with other fields at the level of ideas of methods may enable more collaboration and interdisciplinary work. Systems like Denario are excellent at exploring risky ideas. For instance, if study is highly uncertain, and the outcome may not be very rewarding (e.g., exploring an idea that even if it works, may not lead to publication), many researchers will feel discouraged from pursuing it. However, that study may be very useful for other analyses, perhaps completely unrelated. We believe researchers may consider exploring such ideas if the effort required is significantly reduced, as tools like Denario can do. We believe that systems like Denario can help researchers in many different tasks, from idea generation to method implementation. Ultimately, if we lead AI to perform some of these tasks, 63 under our supervision, scientists will have more time to think deeply and perform higher-level research. This could make science deeper, broader, and more robust. It is also important to discuss the current limitations associated to Denario Citations. The citations in both papers typically refer to relevant papers for the considered sentence, but perhaps not the most relevant ones. An expert reader can easily spot an Denario AI-generated paper from human one just from the citations. Depth. From the generated papers, we can see that the explanations of the results tend to be relatively shallow and not comparable to what an experienced researcher would write. As we show in the discussion of many papers in Sec. 5, current system behaves more like good undergraduate or early graduate student rather than full professor in terms of big picture, connecting results... etc. LaTeX. Sometimes, figures are generated with non-LaTeX y/x labels or legends. For instance, some figures have omega in the y-axis instead of Ωm. Another problem is that the paper writing agent, more rarely, makes small mistakes in LaTeX, e.g., dealing with underscores in equations, text, and references. These errors usually prevent the paper from being compiled properly, although they are easy to fix. Coding. In its current form, Denario sometimes may struggle with writing and executing code. This may happen for multiple reasons, such as changes in the API for recent packages and failures when running code in parallel. This behavior may cause the system to crash during the analysis module. We believe all the above points can be addressed, or at least significantly improved, by improving the agents in charge of some tasks (e.g. citations), by advances in LLMs (e.g. reasoning), or by including more agents to fix the small mistakes made by the other agents (LaTeX)."
        },
        {
            "title": "6.4 Future directions",
            "content": "There are numerous ways to improve Denario, which we plan to implement in the near future: While in its current version Denario made some asynchronous LLM calls, it does not fully exploit its potential. Reformatting some agents to make more asynchronous calls can significantly speed up the time spent on the different tasks and lower the cost. In its current form, Denario may fail or significantly deviate from its goals at several stages of the process. Adding new agents that can provide feedback on the outcome of some agents will improve the stability of the framework and make it more robust. Parallelization and optimization. Our current workflows are sequential: we deal with tasks step-bystep, doing one thing at time. Efficiency will be greatly improved using parallelization strategies, e.g., [87] or massgen31. Adaptive Planning & Control. Currently, once plan is fixed, it is executed until the end. This is not how research is done. Next steps are always conditioned, or even determined, by past ones. more refined Planning & Control strategy will have adaptive planning so that after each task, the new steps left to be done can be re-assessed and potentially changed, removed, or some new steps can be added. The paper writing module can be significantly improved by adding more context information about the code written and the figures created. Furthermore, specialized agents to check claims and put them in touch with literature agents can significantly help write things with broader perspective. Currently, Denario only supports paid LLM providers through their API, but there are plenty of open-source models that can be installed and run locally. We plan to provide support to run our agents with locally installed LLMs, which will give the user more freedom to choose among the available LLMs, use custom models, and get more control over their resources. 31https://docs.massgen.ai/en/latest/ 64 We keep working on easing access to Denario. Besides improving the Python API and the GUI, we plan to include Command Line Interface (CLI) to run the different modules of the multi-agent system without the need to write code explicitly. Develop evaluation frameworks that will allow us to improve the quality of the research assistance tools in systematic way. This will rely on extended benchmark datasets, following the efforts of TPBench [88] or ReplicationBench [89]."
        },
        {
            "title": "7 Ethical implications and scientific challenges",
            "content": "Systems like Denario are designed to facilitate the work of scientists. However, the powerful technology behind them raises many ethical considerations. In this section, we describe some of them and also discuss how AI agents fit in the way we do science from historical perspective."
        },
        {
            "title": "7.1 Ethics",
            "content": "We now describe few points where the usage of systems like Denario will lead to ethical considerations. Quality of research: Generative AI tools such as Denario allow users to easily and quickly generate large numbers of potentially low-quality papers. Some of the incentive structures of academic research may lead to individuals using generative AI tools to artificially increase their research output and/or inflate citation numbers for their own articles. Implications for scientific training and education: Tools such as Denario have the potential to make it easy to write an entire paper with just few lines of description about project idea, its methods, results, and the plots made. This poses questions and challenges for scientific training and education. In what ways are scientific writing and creativity in data analysis valuable as skills inherent to the researcher? To what degree or in what fashion should AI tools play role in helping researchers carry out this type of work to begin with? How should we teach students the skills they will need to succeed as producers and interpreters of research? Assessment of quality and legitimacy: If these tools start being used widely, there may be risk of trusting results from agents without any validation. For instance, an agent may perform some calculation and decide that it is not interesting. Without validation, that result may block potentially interesting research direction. Indeed, as AI agents become both increasingly common producers and consumers of research, we are faced with important questions about the nature of quality and legitimacy in research. How is quality and correctness defined, and by whom? If AI agents become the standard by which to assess the quality of paper or whether it is correct or wrong, how does one assess which AI agent to trust? We can therefore fall into situation of crisis with respect to responsible scientific working guidelines. If anyone can use AI to generate paper, and if no one can control this process and oversee standards of originality, who can be held responsible? Homogenization: pattern seen across some of the application cases presented is Denarios difficulty proposing new research questions or identifying serendipitous, potentially novel findings. This is representative of greater concern that AI agent-driven research might be biased towards producing research that confirms and aligns with the existing scientific literature of its training dataset, potentially leading to homogenized research and stifling innovation. Further experimentation with model architectures, such as the multi-agent frameworks implemented here, could help address this issue. Access: AI tools such as Denario require significant investment in computational infrastructure and energy for their training and operation. These constraints shape which actors have access to their use and operation, affecting the fields of science being chosen for AI agent-driven research, the types of problems being tackled, and the methods used to do so. We must take deliberate steps to democratize both access to the benefits of AI agent-driven research and participation in its creation and training, in order to make sure science addresses the full breadth of interests and problems across societies. 65 Authorship: Generative AI has already sparked debates over the nature of scientific authorship and Denario is no different. key requirement for understanding the debate is careful definition of the roles of authorship in the scientific literature. One clear role of authorship is its use to allocate professional credit and recognition for doing work, which is important for career development. This aspect of authorship raises important questions that we must build answers to as scientific community: if human author uses AI agents to aid their research, in what ways is that different from previous research tools, and should that change the way we attribute professional credit to the human author? Should AI systems be assigned that kind of professional credit, and if so, for what purpose? How should the corpus of work used to train the AI agent be recognized, and what kind of professional credit should be used? Moreover, second role of authorship implies responsibility for the quality and some implications of the results presented, an aspect that also raises critical questions for both the scientific community and wider society. Who should bear the ultimate responsibility for the work presented in paper? What kind of guarantees and accountability should be upheld by the individuals and organizations training the model? valuable but certainly insufficient step to addressing these issues would be to clearly state when research article has been generated by agentic systems, which would have the added benefit of flagging the article as such when considering the training datasets of future AI agents, improving their performance [90]. Misalignment of incentives in scientific publishing: Given the speed and volume with which research articles can be generated using agentic tools like Denario and the career incentives that are associated with inflated publishing statistics in our current academic system, the proliferation of AI agent-driven research may lead to deluge of low-quality papers overwhelming scientific publishing and readership capabilities. This misalignment of career incentives with the purposes of productively advancing scientific knowledge can also lead to situations in which users maliciously generate research articles that purposefully cite their own previous research. These challenges pose questions not only about the incentive structure in academic publishing and career building, but also about what the role of research article is in the age of generative AI and what form or medium can most effectively communicate and evaluate science in this new regime. Environmental concerns: Training and running LLMs come at significant cost in the consumption of energy, water, coolant, and rare earth metals, among others [91, 92]. As with other massive applications of machine learning, the scale of this resource consumption is far from sustainable at the time of writing. Researchers must take active roles in advocating for and developing solutions to address this major issue. Malicious use: AI agents could be used to quickly flood the scientific literature with claims driven by particular political agenda or specific commercial or economic interests. The volume of papers created could then influence the corpus used to train non-malicious AI agents, reinforcing and effectively laundering facts. Developing ways to detect this at the technical level and discuss it at the societal level is critical. Overall, tools like Denario can enhance researchers capabilities by helping them in tasks that require significant amount of time, such as writing papers, debugging code, or exploring ideas. We believe the best usage of Denario would be as an AI-assistant for different tasks. For instance, it can be used to improve an idea (by exploiting methods not known to the researcher), or help with the writing of the paper once the results are in place. Denario is able to perform complex tasks orders of magnitude faster than humans. We think the quality of the research papers will crucially depend on how the system is used: Responsible usage. We believe that under responsible usage, the quality of the research papers will improve. In order to explain this apparently contradictory logic (fast generation vs quality), we believe that researchers are usually time-limited; e.g., an undergraduate needs to graduate or graduate student needs to apply for postdoc/faculty jobs. Given fixed amount of time, the fact that very small fraction of that time goes into generating draft with all associated codes and plots, will enable the researcher to dedicate more time to thinking deeper, improving the quality of the paper, improving the methods, than if the user needs very large fraction of the time to just produce first draft. Irresponsible usage. If the usage takes place in an irresponsible manner, i.e. just generating papers without or little validation, then given the current capabilities of systems like Denario, the quality of the papers will degrade. 66 While we expect both usage modes, we think most researchers will make use of these tools responsibly. Finally, we would like to stress that systems like Denario will further emphasize the need to give more weight to quality rather than quantity when evaluating scientists careers. However, given the fact that quality is very hard to quantify and likely subjective, the current system may need to reshape itself for the benefit of science and society."
        },
        {
            "title": "7.2 Epistemic changes in a science driven by AI agents",
            "content": "The integration of LLMs and AI agents into scientific practice marks profound inflection point in the philosophy of science. These systems are not mere computational tools; they increasingly participate in the core activities of scienceformulating hypotheses, synthesizing literature, generating theoretical connections, falsification procedures, theory choice, epistemic virtue-discussions, and even simulating reasoning. This shift forces us to re-examine long-standing philosophical frameworks concerning the nature of scientific rationality, understanding, and progress. In particular, the proliferation of AI agents in scientific research poses questions regarding the nature of scientific knowledge, its legitimacy, its quality, and its ultimate purpose. 7.2.1 The nature of scientific knowledge in AI-driven science To examine this, we can first turn to the notion of research programmes as defined by Imre Lakatos (1978) [93]: long-term, theoretical frameworks with stable hard core and evolving protective belts of auxiliary hypotheses. Lakatos argued that scientific rationality lies in comparing the relative progress of these programmes: progressive programme predicts novel facts and accommodates anomalies, while degenerative one merely accommodates data retrospectively. Within this framework, LLMs raise fundamental questions. Are they simply auxiliary tools bolstering existing programmeshelping refine theories, conduct literature reviews, and test hypotheses more efficiently? Or do they represent the birth of new kind of research programme altogetherone not driven by causal explanation, but by linguistic modeling, probabilistic reasoning, and massive-scale pattern recognition? If the latter is true, then LLMs bring with them new type of hard core: the assumption that knowledge is fundamentally representable and usable in statistical-linguistic form. This new type of hardcore could be ultimately defined as the model embeddings of training dataset and interpreted in human terms as the weighted aggregate of body of scientific literature. Weighted how? Nobody can fully understand, because understanding mechanistically is no longer the goal. This contrasts sharply with traditional programmes based on mechanistic or formal-logical representations. Moreover, LLMs produce novel predictions, connections, and reformulationsyet not always in way that is interpretable or falsifiable in Lakatosian terms. As such, they may challenge the very criteria by which progress in science is traditionally measured. Lakatos maintained vision of rational science driven by theoretically motivated explanations; LLMs, in contrast, are increasingly effective despite their lack of explicit theory. Their success poses challenge: can research programme be progressive even if it lacks humanunderstandable theoretical core? W.V.O. Quines epistemologyparticularly as articulated in Two Dogmas of Empiricism (1951) [94] and his broader naturalized epistemologyoffers subtle but profound framework for understanding the epistemic disruptions caused by LLMs. Quine rejected the analytic-synthetic distinction and argued for holistic view of knowledge, where statements face the tribunal of experience not in isolation, but only as part of an interconnected web of belief. In this view, revision is always possible, even for logical or mathematical statements, and empirical data can force changes anywhere in the system. From this perspective, LLMs do not simply introduce new tools into the scientists arsenal; they reconfigure the web itself. LLMs are not theory-driven or tethered to empirical input in the traditional sense. They model language, not the worldbut through language, they influence belief revision, hypothesis generation, and theoretical framing. As such, they intervene in science at the level of conceptual structure, shifting what is considered coherent, plausible, or meaningful within the web of scientific belief. 7.2.2 The nature of scientific legitimacy, quality, and purpose in AI-driven science The notion that science is historically contingent and socially negotiated enterprise, insisted upon by writers such as Paul Feyerabend (1975) [95], finds new relevance in the age of AI. As LLMs are integrated into research workflows, they begin to shape what is seen as legitimate knowledge, not through philosophical argument but through infrastructural embeddingby determining what is written, cited, or ignored. This has deep implications for the sociology of knowledge: the epistemic authority of LLMs 67 may evolve not because they know in any human sense, but because they shape the conditions under which knowledge is produced and circulated. Additionally, Quines naturalized epistemologyhis call to study knowledge using the methods of science itselfopens the door to seeing LLMs as epistemic agents within naturalized framework. They are not mere passive instruments but active components in the scientific ecosystem, akin to laboratories, statistical methods, or even communities of inquirers. Yet they differ in one crucial way: they operate in non-transparent manner, lacking not only introspection but causal understanding. This opacity generates an ontological challenge: What is the status of knowledge produced by system that lacks grounding in experience or semantics, and whose justification lies in performance rather than comprehension? How is the quality of this mode of scientific knowledge judged, and should it be judged differently at all? From Quinean standpoint, LLMs are both natural extensions of scientific inquiry and radical disruptors of its structure. They unsettle the boundary between tool and theorist, instrument and agent. They force us to reconsider what counts as observation, evidence, and explanation, and they deepen the holistic fluidity of the scientific webpotentially accelerating progress, but also undermining the interpretive clarity on which much of science has historically depended. Ultimately, AI-driven research opens (or reopens) question on the fundamental purpose and value of scientific knowledge and enterprise, highlighting tension between the goal of mechanistic, human understanding of the world and the goal of achieving statistical practical utility first and foremost. An interesting situation in which this tension surrounding the ultimate purpose of agentic AI research manifests is the Turing Trap [96]. The term Turing Trap refers to the tendency to develop AI systems that mimic or replace human cognition, instead of enhancing or complementing it. Named in reference to Alan Turings famous test for machine intelligence, the trap emerges when passing as human becomes the benchmark for success, thus narrowing the scope of what AI can and should do. In science, this translates into shift from tools that assist human reasoning toward agents that simulate it. LLMs such as GPT-5 are increasingly being used not merely for clerical tasks, but for co-authoring papers, generating reviews, or even proposing research directions. This then underscores the question: if AI agents have the potential to exceed human capabilities in at least certain regards, should we reevaluate the basic tasks and units of how science is practiced, communicated, and evaluated in non-human terms?"
        },
        {
            "title": "7.3 The impact of AI agent-driven science on the structure of scientific work",
            "content": "Besides the questions raised regarding epistemic changes in the nature of science and scientific knowledge, agentic AI-driven research also appears poised to reconfigure the structure of scientific practice. To understand the ways in which this may occur, it is helpful to use the framework developed by Thomas Kuhn (1975) [97] according to which science develops through periods of normal science in which knowledge is incorporated into existing theoretical frameworks, alternated by paradigm shifts during which those frameworks are overhauled. 7.3.1 AI agent-driven science as an enabler of innovation From Kuhnian perspective, LLMs could be seen as catalysts for paradigm shift. Kuhn emphasized that science does not evolve through continuous accumulation of knowledge, but rather through disruptive revolutions in which one paradigm is replaced by another, reshaping the methods, standards, and even the very questions that science considers meaningful. LLMs introduce precisely this kind of rupture: they not only change how science is done, but what it means to do science. Their ability to generate plausible hypotheses, summarize vast literatures, and recombine knowledge across disciplinary boundaries at scales and speeds beyond the capabilities of single human researcher challenges the prevailing paradigm in which singular human cognition, creativity, and interpretive judgment are central to scientific discovery. Kuhns notion of normal science, routine puzzle-solving within an accepted paradigm, may become destabilized as LLMs increasingly participate in tasks once thought uniquely human. Moreover, their epistemic style is deeply different: LLMs operate not on causal mechanisms but on statistical and linguistic Insofar as LLMs begin to influence theory choice, experimental design, or even what is associations. considered an interesting or valid research question, they may be ushering in new epistemic regime, one where prediction trumps explanation, pattern overrules theory, and where coherence across texts replaces ontological depth. Kuhns framework helps us see that the rise of LLMs may not be simple technological enhancement. It could even bring us to deep transformation of the values and structures that undergird scientific practice. This presents challenge for the very practice of science. Paul Feyerabends radical critique of scientific rationality offers another perspective. In Against Method (1975) [95], Feyerabend argued that science does notand should notoperate according to 68 fixed methodological rules. Progress, he claimed, often occurs when conventions are violated and when heterodox or even irrational approaches are permitted to flourish. From this viewpoint, LLMs may be the ultimate Feyerabendian agents. They defy the norms of methodological consistency, rely on opaque black-box architectures, and generate insights not through logic or controlled experimentation but through stochastic generation and pattern inference across language corpora. Precisely because of this transgressive character, LLMs may serve as engines of epistemic pluralism and innovation. They break down disciplinary barriers, resurface forgotten ideas, and draw novel analogies that human researchers may overlook. Feyerabends critique of scientific monismthe idea that there is one best method or one correct epistemic frameworkis echoed in the way LLMs expose the contingencies and limits of human-centric scientific norms. In his terms, they have the potential to be tools that reassert the chaotic, cultural, and creative dimensions of science, reminding us that genuine progress often arises not from strict adherence to method but from bold and unpredictable departures from it. 7.3.2 AI agent-driven science as stifling, homogenizing force Despite this potential, an argument can also be made that AI agent-driven science risks minimizing innovation rather than enhancing it. In Kuhnian terms, AI agent-driven scientist could lead to reduction in paradigm shifts and slump into perpetual normal science. When AI agents carry out entire cycles of knowledge production, we risk creating system that perpetually produces research extending the status quo, the risk of homogeneity. Given that these agents are by design trained on broadly similar bodies of knowledge, one wonders to what degree they are intrinsically biased towards what Kuhn calls normal science production, tending to generate output that fits well to knowledge available in the training set. In Feyerabendian terms, the nightmare of total scientific monism could be possible outcome of AI-driven homogenization if AI agent research converges on the same training data sets, architectures, and desired outcomes. In addition to the biases inherent in their own architecture, AI agent-driven research can also reinforce biases in the structures of scientific institutions. The ease with which agentic AI can be used to produce scientific output that easily conforms to existing dogmauseful in some environments to inflate statistics valuable for career advancementcould result in less paradigm-shifting work. Furthermore, the potential for rapid feedback loop in which future agentic AI is training on AI-generated scientific literature with the same bias towards extending existing theory (rather than disrupting it) could accelerate this trend. However, this doesnt have to be the case: we can build in statistical and agent-level architectures that minimize this bias towards normal science and emphasize the role of human scientists within the loop of AI-assisted knowledge production. Achieving this implies an active effort from all parts of the scientific enterprise, from the computer science behind the design of AI agents to the career incentives for individuals and funding mechanisms used to encourage scientific production. It is crucial to democratize both access to the tools themselves as well as the skills and infrastructure for their development, and to be deliberate and conscientious when compiling training data."
        },
        {
            "title": "7.4 Conclusions from philosophy of science",
            "content": "LLMs do not merely support current scientific practice; they transform it. Through the lens of Kuhn [97], they potentially inaugurate new paradigm of machine-augmented scienceor plunge us into perpetual normal science. Through Lakatos [93], they challenge existing research programmes and perhaps herald new one grounded in probabilistic modeling rather than explanatory theory, with the potential to bring about unprecedented pragmatic relevanceand perhaps vision of science divorced from human understanding. And through Feyerabend [95], they emerge as agents of epistemic disorder and creativity, unsettling dogma and enabling new modes of inquiryor trap us in sterile, homogenized scientific monism. As the transformation of science and its practice by AI tools continues, it is worth remembering that an LLM is ultimately an universal interpolator that retrieves informationdirectly or interpolatedfrom body of knowledge, with all the advantages and flaws that this entails. Ultimately, AI agent-driven science raises crucial but uncomfortable questions at the heart of science: are we outsourcing not just labor, but judgment and meaning? When the outputs of LLMs are treated as autonomous knowledge contributionsrather than the mediated results of probabilistic associationsthey can displace the interpretive and evaluative labor that defines good science. Scientific rationality becomes performative: coherence, plausibility, and citation density stand in for theoretical depth, causal understanding, or experimental validation. To engage seriously with the implications of LLMs, we must therefore go beyond technical or instrumental perspectives and grapple with the philosophical foundations of science itself. What counts as knowledge, explanation, or progress may no longer 69 be what it once wasand it is only by revisiting and reevaluating previous frameworks of science that we can begin to navigate this new epistemic terrain."
        },
        {
            "title": "8 Summary and Conclusions",
            "content": "In this work, we have introduced Denario, an AI multi-agent system designed as research assistant. Denario is built in modular way, and can perform many different tasks, such as Generate novel or refine existing research ideas Check the literature to determine whether an idea is new and its potential interest Develop detailed research plans Write and execute code, make plots, and summarize results Draft scientific paper Review scientific paper Denario can also perform end-to-end scientific research by running all the above modules sequentially. Different modules require different inputs and produce different outputs (see Table 1). When run in end-to-end mode, the system only requires an input text containing general instructions to the system (e.g., describing the data to be used). We emphasize that the goal of Denario is not to automate science, but rather to serve as powerful tool to help researchers across disciplines accelerate scientific discovery. In Section 2, we described the different components needed for creating multi-agent systems, from Large Language Models (LLMs) to agentic orchestration. In Section 3, we presented in detail the different modules in Denario and the tasks they carry out. We have illustrated the capabilities of Denario in Section 5 by evaluating papers generated by it in many different scientific disciplines, from astrophysics to biology. From the expert evaluation of these manuscripts, we conclude that the generated papers are at the level of what good undergraduate or early graduate student could achieve after weeks or few months of work. In comparison, it took Denario around 30 minutes per paper and cost of around $4. We have discussed the failure modes, the importance of validation and evaluation, and the advantages and limitations of Denario in Sec. 6. Finally, in Sec. 7 we discussed the ethical considerations that agentic systems like Denario lead to and discussed how these technological advances could be perceived from philosophy of science perspective. We note that we have not discussed important aspects related to AI agents such as benchmarking [98] and risks [99]. However, these are active areas of research. We conclude by stating that Denario represents one more example that the latest advances in deep learning, in particular in large-language models and AI agents, may help scientists with their work [5, 8]. As we mentioned above, the papers that Denario can generate right now do not have the breadth and reach of those written by the more experienced researchers in the field. Whether future LLMs or more sophisticated systems with more specialized agents can achieve that remains an open question. We believe that tools like Denario can open up new ways of doing science. Traditionally, researchers focus deeply on small set of questions, since tasks such as coding, reviewing the literature, and learning new concepts or tools require significant time and effort. By contrast, Denario excels at rapidly exploring broad range of ideas, albeit at more superficial level. These approaches are highly complementary: AI can scan widely and highlight promising directions, while human experts can select the most relevant ideas and pursue them in depth to achieve deeper understanding."
        },
        {
            "title": "Acknowledgements",
            "content": "The work of FVN, AEB, RJ, and PTA was supported by the Simons Foundation. The work of BB was partially funded by an unrestricted gift from Google, the Cambridge Centre for Data-Driven Discovery Accelerate Programme and the Infosys-Cambridge AI Centre. Funding for the work of RJ and PTA was partially provided by project PID2022-141125NB-I00, and grant CEX2024-001451-M funded by MICIU/AEI/10.13039/501100011033. PTA is supported by the project Dark Energy and the Origin of the Universe (PRE2022-102220), funded by MCIN/AEI/10.13039/501100011033. WHZ acknowledges the funding support from Japan Society for the Promotion of Science. PB acknowledges constant support from DIPC. IZ acknowledges support from the STFC (grant numbers ST/W000977/1 and 70 ST/X006387/1. MV is supported by Fondazione ICSC, Spoke 3 Astrophysics and Cosmos Observations Project ID CN00000013 Italian Research Center on High-Performance Computing, Big Data and Quantum Computing funded by MUR Next Generation EU (NGEU); the INAF Theory Grant Cosmological Investigation of the Cosmic Web and by the INFN INDARK grant. We thank Viviana Acquaviva, Tal Adi, Yacine Ali-Haimoud, David Alonso, Daniel Angles-Alcazar, Juan Navarro-Arenas, Paramita Barai, Jane Beaufore, Stefano Borgani, Juan Camilo Cardenas, Matteo Cantiello, Isabella Carucci, Gabriella Contardo, Belen Costanza, William Coulton, Stefani Cristiani, Carolina Cuesta-Lazaro, Steve Cunnington, Julianne Dalcaton, Carmen Embil-Villagra, Serafina di Gioia, James Fergusson, Zucheng Gao, Shy Genel, Antoine George, William Handley, Mike Hobson, Teresa Huang, Kartheik Iyer, Yannis Kevrekidis, Mica Komarnyckyj, Ely Kovetz, Sven Krippendorf, Andrew Laverick, Ilem Leisher, Antony Lewis, Sarah Libanore, Stephane Mallat, Sukanya Mallik, Dinko Milakovic, Andy Millis, Andrei Mesinger, Azadeh Moradinezhad, Bruno Motta, Moritz Munchmeyer, Yueying Ni, Lorenzo Pacchiardi, Gonzalo Palma, Isabel Pederneiras, Vincent Prevosto, Amanda Prorok, William Ramirez, Rogerio Rosenfeld, Walter Riquelme, Gary Shiu, Marta Spinelli, Riccardo Sturani, Mario Santos, Emanuel Schaan, Thomas Sumner, Irene Testini, Paul Torrey, Roberto Trotta, Amita Ummadisingu, Soledad Villar, Sebastian Wagner-Carena, Risa Wechsler, Benjamin Wandelt, Nils Wentzell, Jeannette Wing, Sandy Yuan for useful discussions. We thank Licia Verde for insightful conversations regarding publication of AI generated papers and her participation in the launch webinar. We also thank Laura Edwards for her collaboration in the preparation of the video tutorials and the launch webinar. We thank Mathew E. Diamond for his invaluable guidance on the spike sorting and curation pipeline. We also acknowledge Davide Giana and Francesca Pulecchi for their expert surgical work, which enabled the acquisition of the Neuropixels data used in this study."
        },
        {
            "title": "References",
            "content": "[1] A.F. Chalmers. What Is This Thing Called Science? (Third Edition). Hackett Pub., 1999. [2] Ross D. King, Kenneth E. Whelan, Ffion M. Jones, Philip G. K. Reiser, Christopher H. Bryant, Stephen H. Muggleton, Douglas B. Kell, and Stephen G. Oliver. Functional genomic hypothesis generation and experimentation by robot scientist. Nature, 427(6971):247252, January 2004. [3] Kevin Williams, Elizabeth Bilsland, Andrew Sparkes, Wayne Aubrey, Meong Young, Larisa Soldatova, Kurt De Grave, Jan Ramon, Michaela Clare, Worachart Sirawaraporn, Stephen Oliver, and Ross King. Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. Journal of The Royal Society Interface, 12:20141289, 03 2015. [4] James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B. Tenenbaum, and Zoubin Ghahramani. Automatic Construction and Natural-Language Description of Nonparametric Regression Models. arXiv e-prints, page arXiv:1402.4304, February 2014. [5] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. arXiv e-prints, page arXiv:2408.06292, August 2024. [6] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search. arXiv e-prints, page arXiv:2504.08066, April 2025. [7] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago Costa, Jose Penades, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, and Vivek Natarajan. Towards an AI co-scientist. arXiv e-prints, page arXiv:2502.18864, February 2025. [8] Adam Moss. The AI Cosmologist I: An Agentic System for Automated Data Analysis. arXiv e-prints, page arXiv:2504.03424, April 2025. 71 [9] Patrick Tser Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, and Ang Chen. Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents. arXiv e-prints, page arXiv:2502.16069, February 2025. [10] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael Moor, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants, 2025. [11] Chris Thornton, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms. arXiv e-prints, page arXiv:1208.3719, August 2012. [12] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. arXiv e-prints, page arXiv:2404.07738, April 2024. [13] Jacob T. Rapp, Bennett J. Bremer, and Philip A. Romero. Self-driving laboratories to autonomously navigate the protein fitness landscape. bioRxiv, 2023. [14] Kyle Swanson, Wesley Wu, Nash L. Bulaong, John E. Pak, and James Zou. The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. bioRxiv, 2024. [15] Daniel Saeedi, Denise Buckner, Jose C. Aponte, and Amirali Aghazadeh. AstroAgents: Multi-Agent AI for Hypothesis Generation from Mass Spectrometry Data. arXiv e-prints, page arXiv:2503.23170, March 2025. [16] Alexander Novikov, Ngˆan Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, and Matej Balog. AlphaEvolve: coding agent for scientific and algorithmic discovery. arXiv e-prints, page arXiv:2506.13131, June 2025. [17] Eser Aygun, Anastasiya Belyaeva, Gheorghe Comanici, Marc Coram, Hao Cui, Jake Garrison, Renee Johnston Anton Kast, Cory Y. McLean, Peter Norgaard, Zahra Shamsi, David Smalling, James Thompson, Subhashini Venugopalan, Brian P. Williams, Chujun He, Sarah Martinson, Martyna Plomecka, Lai Wei, Yuchen Zhou, Qian-Ze Zhu, Matthew Abraham, Erica Brand, Anna Bulanova, Jeffrey A. Cardille, Chris Co, Scott Ellsworth, Grace Joseph, Malcolm Kane, Ryan Krueger, Johan Kartiwa, Dan Liebling, Jan-Matthis Lueckmann, Paul Raccuglia, Xuefei, Wang, Katherine Chou, James Manyika, Yossi Matias, John C. Platt, Lizzie Dorfman, Shibl Mourad, and Michael P. Brenner. An AI system to help scientists write expert-level empirical software. arXiv e-prints, page arXiv:2509.06503, September 2025. [18] Robert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution. arXiv e-prints, page arXiv:2509.19349, September 2025. [19] Alireza Ghafarollahi and Markus J. Buehler. SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning. arXiv e-prints, page arXiv:2409.05556, September 2024. [20] Markus J. Buehler. Agentic deep graph reasoning yields self-organizing knowledge networks. Journal of Materials Research, 40(15):22042242, August 2025. [21] Samuel Schmidgall and Michael Moor. AgentRxiv: Towards Collaborative Autonomous Research. arXiv e-prints, page arXiv:2503.18102, March 2025. [22] Nathan Szymanski, Bernardus Rendy, Yuxing Fei, Rishi Kumar, Tanjin He, David Milsted, Matthew McDermott, Max Gallant, Ekin Dogus Cubuk, Amil Merchant, et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nature, 624(7990):8691, 2023. [23] Josh Leeman, Yuhan Liu, Joseph Stiles, Scott B. Lee, Prajna Bhatt, Leslie M. Schoop, and Robert G. Palgrave. Challenges in high-throughput inorganic materials prediction and autonomous synthesis. PRX Energy, 3:011002, Mar 2024. [24] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large Language Models: Survey. arXiv e-prints, page arXiv:2402.06196, February 2024. [25] Andrew Laverick, Kristen Surrao, Inigo Zubeldia, Boris Bolliet, Miles Cranmer, Antony Lewis, Blake Sherwin, and Julien Lesgourgues. Multi-Agent System for Cosmological Parameter Analysis. 11 2024. [26] Licong Xu, Milind Sarkar, Anto I. Lonappan, Inigo Zubeldia, Pablo Villanueva-Domingo, Santiago Casas, Christian Fidler, Chetana Amancharla, Ujjwal Tiwari, Adrian Bayer, Chadi Ait Ekioui, Miles Cranmer, Adrian Dimitrov, James Fergusson, Kahaan Gandhi, Sven Krippendorf, Andrew Laverick, Julien Lesgourgues, Antony Lewis, Thomas Meier, Blake Sherwin, Kristen Surrao, Francisco Villaescusa-Navarro, Chi Wang, Xueqing Xu, and Boris Bolliet. Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery, July 2025. [27] Boris Bolliet and the CMBAGENT team. Cmbagent: Open-source multi-agent system for science, 2025. Available at https://github.com/CMBAgents/cmbagent. [28] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can LLMs Generate Novel Research Ideas? Large-Scale Human Study with 100+ NLP Researchers. arXiv e-prints, page arXiv:2409.04109, September 2024. [29] Jiahui Li, Kun Kuang, Baoxiang Wang, Xingchen Li, Fei Wu, Jun Xiao, and Long Chen. Two heads are better than one: simple exploration framework for efficient multi-agent reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [30] Haoyang Su, Renqi Chen, SHIXIANG TANG, Xinzhe Zheng, Jinzhe Li, Zhenfei Yin, Wanli Ouyang, and Nanqing Dong. Two heads are better than one: multi-agent system has the potential to improve scientific idea generation, 2024. [31] T. Jesper Jacobsson, Adam Hultqvist, Alberto Garcıa-Fernandez, Aman Anand, Amran Al-Ashouri, Anders Hagfeldt, Andrea Crovetto, Antonio Abate, Antonio Gaetano Ricciardulli, Anuja Vijayan, Ashish Kulkarni, Assaf Y. Anderson, Barbara Primera Darwich, Bowen Yang, Brendan L. Coles, Carlo A. R. Perini, Carolin Rehermann, Daniel Ramirez, David Fairen-Jimenez, Diego Di Girolamo, Donglin Jia, Elena Avila, Emilio J. Juarez-Perez, Fanny Baumann, Florian Mathies, G. S. Anaya Gonzalez, Gerrit Boschloo, Giuseppe Nasti, Gopinath Paramasivam, Guillermo Martınez-Denegri, Hampus Nasstrom, Hannes Michaels, Hans Kobler, Hua Wu, Iacopo Benesperi, M. Ibrahim Dar, Ilknur Bayrak Pehlivan, Isaac E. Gould, Jacob N. Vagott, Janardan Dagar, Jeff Kettle, Jie Yang, Jinzhao Li, Joel A. Smith, Jorge Pascual, Jose J. Jeronimo-Rendon, Juan Felipe Montoya, JuanPablo Correa-Baena, Junming Qiu, Junxin Wang, Kari Sveinbjornsson, Katrin Hirselandt, Krishanu Dey, Kyle Frohna, Lena Mathies, Luigi A. Castriotta, Mahmoud. H. Aldamasy, Manuel VasquezMontoya, Marco A. Ruiz-Preciado, Marion A. Flatken, Mark V. Khenkin, Max Grischek, Mayank Kedia, Michael Saliba, Miguel Anaya, Misha Veldhoen, Neha Arora, Oleksandra Shargaieva, Oliver Maus, Onkar S. Game, Ori Yudilevich, Paul Fassl, Qisen Zhou, Rafael Betancur, Rahim Munir, Rahul Patidar, Samuel D. Stranks, Shahidul Alam, Shaoni Kar, Thomas Unold, Tobias Abzieher, Tomas Edvinsson, Tudur Wyn David, Ulrich W. Paetzold, Waqas Zia, Weifei Fu, Weiwei Zuo, Vincent R. F. Schroder, Wolfgang Tress, Xiaoliang Zhang, Yu-Hsien Chiang, Zafar Iqbal, Zhiqiang Xie, and Eva Unger. An open-access database and analysis tool for perovskite solar cells based on the fair data principles. Nature Energy, 7(1):107115, 2022. [32] Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models to self-debug, 2023. [33] M. W. M. G. Dissanayake and N. Phan-Thien. Neural-network-based approximations for solving partial differential equations. Communications in Numerical Methods in Engineering, 10(3):195201, March 1994. [34] I.E. Lagaris, A.C. Likas, and D.G. Papageorgiou. Neural-network methods for boundary value problems with irregular boundaries. IEEE Transactions on Neural Networks, 11(5):10411049, 2000. [35] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686707, February 2019. 73 [36] Wenjun Cai, Chaolong Jiang, Yushun Wang, and Yongzhong Song. Structure-preserving algorithms for the two-dimensional sine-gordon equation with neumann boundary conditions. Journal of Computational Physics, 395:166185, October 2019. [37] Marios Mattheakis, David Sondak, Akshunna S. Dogra, and Pavlos Protopapas. Hamiltonian neural networks for solving equations of motion. 2020. [38] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: deep learning algorithm for solving partial differential equations. Journal of Computational Physics, 375:13391364, December 2018. [39] Yago Bea, Raul Jimenez, David Mateos, Shuheng Liu, Pavlos Protopapas, Pedro TaranconAlvarez, and Pablo Tejerina-Perez. Gravitational duals from equations of state. JHEP, 07:087, 2024. [40] Pedro TaranconAlvarez, Pablo Tejerina-Perez, Raul Jimenez, and Pavlos Protopapas. Efficient PINNs: Multi-Head Unimodular Regularization of the Solutions Space. 1 2025. [41] Edward Bowell, Karri Muinonen, and Lawrence H. Wasserman. Public-Domain Asteroid Orbit Data Base. In Andrea Milani, Michel di Martino, and A. Cellino, editors, Asteroids, Comets, Meteors 1993, volume 160 of IAU Symposium, pages 477481, January 1994. [42] Brian D. Warner, Alan W. Harris, and Petr Pravec. The asteroid lightcurve database. Icarus, 202(1):134146, July 2009. [43] J. Durech, V. Sidorin, and M. Kaasalainen. DAMIT: database of asteroid models. Astronomy & Astrophysics, 513:A46, April 2010. [44] Minor planet physical properties catalogue (mp3c). https://mp3c.oca.eu. Accessed: 2025-09-30. [45] M. Mahlke, B. Carry, and P. A. Mattei. Asteroid taxonomy from cluster analysis of spectrometry and albedo. Astronomy & Astrophysics, 665:A26, September 2022. [46] R. Lenzen, M. Hartung, W. Brandner, G. Finger, N. Hubin, and et al. Naos-conica first on-sky results in variety of observing modes. In M. Iye and A. F. M. Moorwood, editors, Instrument Design and Performance for Optical/Infrared Ground-based Telescopes, volume 4841 of Proc. SPIE, pages 944952, 2003. [47] ˇZeljko Ivezic, Steven M. Kahn, J. Anthony Tyson, Bob Abel, Emily Acosta, Robyn Allsman, David Alonso, Yusra AlSayyad, Scott F. Anderson, John Andrew, James Roger P. Angel, George Z. Angeli, Reza Ansari, Pierre Antilogus, Constanza Araujo, Robert Armstrong, Kirk T. Arndt, Pierre Astier, Eric Aubourg, Nicole Auza, Tim S. Axelrod, Deborah J. Bard, Jeff D. Barr, Aurelian Barrau, James G. Bartlett, Amanda E. Bauer, Brian J. Bauman, Sylvain Baumont, Ellen Bechtol, Keith Bechtol, Andrew C. Becker, Jacek Becla, Cristina Beldica, Steve Bellavia, Federica B. Bianco, Rahul Biswas, Guillaume Blanc, Jonathan Blazek, Roger D. Blandford, Josh S. Bloom, Joanne Bogart, Tim W. Bond, Michael T. Booth, Anders W. Borgland, Kirk Borne, James F. Bosch, Dominique Boutigny, Craig A. Brackett, Andrew Bradshaw, William Nielsen Brandt, Michael E. Brown, James S. Bullock, Patricia Burchat, David L. Burke, Gianpietro Cagnoli, Daniel Calabrese, Shawn Callahan, Alice L. Callen, Jeffrey L. Carlin, Erin L. Carlson, Srinivasan Chandrasekharan, Glenaver Charles-Emerson, Steve Chesley, Elliott C. Cheu, Hsin-Fang Chiang, James Chiang, Carol Chirino, Derek Chow, David R. Ciardi, Charles F. Claver, Johann Cohen-Tanugi, Joseph J. Cockrum, Rebecca Coles, Andrew J. Connolly, Kem H. Cook, Asantha Cooray, Kevin R. Covey, Chris Cribbs, Wei Cui, Roc Cutri, Philip N. Daly, Scott F. Daniel, Felipe Daruich, Guillaume Daubard, Greg Daues, William Dawson, Francisco Delgado, Alfred Dellapenna, Robert de Peyster, Miguel de ValBorro, Seth W. Digel, Peter Doherty, Richard Dubois, Gregory P. Dubois-Felsmann, Josef Durech, Frossie Economou, Tim Eifler, Michael Eracleous, Benjamin L. Emmons, Angelo Fausti Neto, Henry Ferguson, Enrique Figueroa, Merlin Fisher-Levine, Warren Focke, Michael D. Foss, James Frank, Michael D. Freemon, Emmanuel Gangler, Eric Gawiser, John C. Geary, Perry Gee, Marla Geha, Charles J. B. Gessner, Robert R. Gibson, D. Kirk Gilmore, Thomas Glanzman, William Glick, Tatiana Goldina, Daniel A. Goldstein, Iain Goodenow, Melissa L. Graham, William J. Gressler, Philippe Gris, Leanne P. Guy, Augustin Guyonnet, Gunther Haller, Ron Harris, Patrick A. Hascall, Justine Haupt, Fabio Hernandez, Sven Herrmann, Edward Hileman, Joshua Hoblitt, John A. Hodgson, Craig Hogan, James D. Howard, Dajun Huang, Michael E. Huffer, Patrick Ingraham, Walter R. Innes, Suzanne H. Jacoby, Bhuvnesh Jain, Fabrice Jammes, M. James Jee, Tim Jenness, Garrett Jernigan, Darko Jevremovic, Kenneth Johns, Anthony S. Johnson, Margaret W. G. 74 Johnson, R. Lynne Jones, Claire Juramy-Gilles, Mario Juric, Jason S. Kalirai, Nitya J. Kallivayalil, Bryce Kalmbach, Jeffrey P. Kantor, Pierre Karst, Mansi M. Kasliwal, Heather Kelly, Richard Kessler, Veronica Kinnison, David Kirkby, Lloyd Knox, Ivan V. Kotov, Victor L. Krabbendam, K. Simon Krughoff, Petr Kubanek, John Kuczewski, Shri Kulkarni, John Ku, Nadine R. Kurita, Craig S. Lage, Ron Lambert, Travis Lange, J. Brian Langton, Laurent Le Guillou, Deborah Levine, Ming Liang, Kian-Tat Lim, Chris J. Lintott, Kevin E. Long, Margaux Lopez, Paul J. Lotz, Robert H. Lupton, Nate B. Lust, Lauren A. MacArthur, Ashish Mahabal, Rachel Mandelbaum, Thomas W. Markiewicz, Darren S. Marsh, Philip J. Marshall, Stuart Marshall, Morgan May, Robert McKercher, Michelle McQueen, Joshua Meyers, Myriam Migliore, Michelle Miller, and David J. Mills. LSST: From Science Drivers to Reference Design and Anticipated Data Products. The Astrophysical Journal, 873(2):111, March 2019. [48] R. Gilmozzi and J. Spyromilio. The European Extremely Large Telescope (E-ELT). The Messenger, 127:11, March 2007. [49] LIGO Scientific Collaboration, Virgo Collaboration, and KAGRA Collaboration. Gw231123: binary black hole merger with total mass 190-265 msun. Zenodo, 2025. [50] KAGRA LIGO Scientific, VIRGO. GW231123: Binary Black Hole Merger with Total Mass 190-265 . 7 2025. [51] World Health Organization et al. World malaria report 2024. World Health Organization, 2024. [52] Virginia Howick, Andrew JC Russell, Tallulah Andrews, Haynes Heaton, Adam Reid, Kedar Natarajan, Hellen Butungi, Tom Metcalf, Lisa Verzier, Julian Rayner, et al. The malaria cell atlas: Single parasite transcriptomes across the complete plasmodium life cycle. Science, 365(6455):eaaw2619, 2019. [53] Sunil Kumar Dogga, Jesse Rop, Juliana Cudini, Elias Farr, Antoine Dara, Dinkorma Ouologuem, Abdoulaye Djimde, Arthur Talman, and Mara KN Lawniczak. single cell atlas of sexual development in plasmodium falciparum. Science, 384(6695):eadj4088, 2024. [54] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. [55] Alexander Wolf, Philipp Angerer, and Fabian Theis. Scanpy: large-scale single-cell gene expression data analysis. Genome biology, 19:15, 2018. [56] Junyue Cao, Malte Spielmann, Xiaojie Qiu, Xingfan Huang, Daniel Ibrahim, Andrew Hill, Fan Zhang, Stefan Mundlos, Lena Christiansen, Frank Steemers, et al. The single-cell transcriptional landscape of mammalian organogenesis. Nature, 566(7745):496502, 2019. [57] Alexander Wolf, Fiona Hamey, Mireya Plass, Jordi Solana, Joakim Dahlin, Berthold Gottgens, Nikolaus Rajewsky, Lukas Simon, and Fabian Theis. Paga: graph abstraction reconciles clustering with trajectory inference through topology preserving map of single cells. Genome biology, 20:19, 2019. [58] Cristina Aurrecoechea, John Brestelli, Brian Brunk, Jennifer Dommer, Steve Fischer, Bindu Gajria, Xin Gao, Alan Gingle, Greg Grant, Omar Harb, et al. Plasmodb: functional genomic database for malaria parasites. Nucleic acids research, 37(suppl 1):D539D543, 2009. [59] Jorge Alvarez-Jarreta, Beatrice Amos, Cristina Aurrecoechea, Saikou Bah, Matthieu Barba, Ana Barreto, Evelina Basenko, Robert Belnap, Ann Blevins, Ulrike Bohme, et al. Veupathdb: the eukaryotic pathogen, vector and host bioinformatics resource center in 2023. Nucleic acids research, 52(D1):D808D816, 2024. [60] Koen Van den Berge, Hector Roux de Bezieux, Kelly Street, Wouter Saelens, Robrecht Cannoodt, Yvan Saeys, Sandrine Dudoit, and Lieven Clement. Trajectory-based differential expression analysis for single-cell sequencing data. Nature communications, 11(1):1201, 2020. [61] Petra Schneider, Sarah Reece, Ben CL Van Schaijk, Teun Bousema, Kjerstin HW Lanke, Cora SJ Meaden, Amal Gadalla, Lisa Ranford-Cartwright, and Hamza Babiker. Quantification of female and male plasmodium falciparum gametocytes by reverse transcriptase quantitative pcr. Molecular and biochemical parasitology, 199(1-2):2933, 2015. 75 [62] William Dixon, Sabine van der Veer, Syed Mustafa Ali, Lynn Laidlaw, Richard JB Dobson, Cathie Sudlow, Tim Chico, Jacqueline AL MacArthur, and Aiden Doherty. Charting course for smartphones and wearables to transform population health research. Journal of medical Internet research, 25:e42449, February 2023. [63] Sophie Huhn, Miriam Axt, Hanns-Christian Gunga, Martina Anna Maggioni, Stephen Munga, David Obor, Ali Sie, Valentin Boudo, Aditi Bunker, Rainer Sauerborn, Till Barnighausen, and Sandra Barteit. The Impact of Wearable Technologies in Health Research: Scoping Review. JMIR mHealth and uHealth, 10(1), January 2022. Publisher: JMIR Publications Inc. [64] Vini Vijayan, James P. Connolly, Joan Condell, Nigel McKelvey, and Philip Gardiner. Review of Wearable Devices and Data Collection Considerations for Connected Health. Sensors (Basel, Switzerland), 21(16):5589, August 2021. [65] Lydia G. Roos and George M. Slavich. Wearable technologies for health research: Opportunities, limitations, and practical and conceptual considerations. Brain, Behavior, and Immunity, 113:444 452, October 2023. [66] Bernard C. K. Choi, Anita W. P. Pak, Jerome C. L. Choi, and Elaine C. L. Choi. Daily step goal of 10,000 steps: literature review. Clinical and Investigative Medicine. Medecine Clinique Et Experimentale, 30(3):E146151, 2007. [67] SCOTT R. SMALL, SHING CHAN, ROSEMARY WALMSLEY, LENNART VON FRITSCH, AIDAN ACQUAH, GERT MERTES, BENJAMIN G. FEAKINS, ANDREW CREAGH, ADAM STRANGE, CHARLES E. MATTHEWS, DAVID A. CLIFTON, ANDREW J. PRICE, SARA KHALID, DERRICK BENNETT, and AIDEN DOHERTY. Self-Supervised Machine Learning to Characterize Step Counts from Wrist-Worn Accelerometers in the UK Biobank. Medicine & Science in Sports & Exercise, 56(10), 2024. [68] Christopher Dobson. Protein folding and misfolding. Nature, 426(6968):884890, 2003. [69] Peter Eastman, Raimondas Galvelis, Raul Pelaez, Charlles RA Abreu, Stephen Farr, Emilio Gallicchio, Anton Gorenko, Michael Henry, Frank Hu, Jing Huang, et al. Openmm 8: molecular dynamics simulation with machine learning potentials. The Journal of Physical Chemistry B, 128(1):109116, 2023. [70] Kresten Lindorff-Larsen, Stefano Piana, Ron Dror, and David Shaw. How fast-folding proteins fold. Science, 334(6055):517520, 2011. [71] Vincent Voelz, Gregory Bowman, Kyle Beauchamp, and Vijay Pande. Molecular simulation of ab initio protein folding for millisecond folder ntl9 (139). Journal of the American Chemical Society, 132(5):15261528, 2010. [72] Jacob Monroe, Mikayla Barry, Audra DeStefano, Pinar Aydogan Gokturk, Sally Jiao, Dennis Robinson-Brown, Thomas Webber, Ethan J. Crumlin, Songi Han, and M. Scott Shell. Water structure and properties at hydrophilic and hydrophobic surfaces. Annual Review of Chemical and Biomolecular Engineering, 11(Volume 11, 2020):523557, 2020. [73] Sally Jiao, Lynn E. Katz, and M. Scott Shell. Inverse design of pore wall chemistry to control solute transport and selectivity. ACS Central Science, 8(12):16091617, 2022. [74] Narges Karamzadeh and Farkhondeh Mozaffari. Structure and the dynamics of nano-confined water in the vicinity of functionalized graphene. Fluid Phase Equilibria, 472:160171, 2018. [75] Lyndon Hess, Nhi Nguyen, Anthony Dee, Anant Gupta, Zachary Kwon, and Shuwen Yue. How surface functionalization controls confined electrolyte structure and dynamics at graphene interfaces. The Journal of Physical Chemistry B, 2025. [76] Francisco Villaescusa-Navarro, Daniel Angles-Alcazar, Shy Genel, David N. Spergel, Rachel S. Somerville, Romeel Dave, Annalisa Pillepich, Lars Hernquist, Dylan Nelson, Paul Torrey, Desika Narayanan, Yin Li, Oliver Philcox, Valentina La Torre, Ana Maria Delgado, Shirley Ho, Sultan Hassan, Blakesley Burkhart, Digvijay Wadekar, Nicholas Battaglia, Gabriella Contardo, and Greg L. Bryan. The CAMELS Project: Cosmology and Astrophysics with Machine-learning Simulations. ApJ, 915(1):71, July 2021. 76 [77] Francisco Villaescusa-Navarro, Shy Genel, Daniel Angles-Alcazar, Lucia A. Perez, Pablo VillanuevaDomingo, Digvijay Wadekar, Helen Shao, Faizan G. Mohammad, Sultan Hassan, Emily Moser, Erwin T. Lau, Luis Fernando Machado Poletti Valle, Andrina Nicola, Leander Thiele, Yongseok Jo, Oliver H. E. Philcox, Benjamin D. Oppenheimer, Megan Tillman, ChangHoon Hahn, Neerav Kaushal, Alice Pisani, Matthew Gebhardt, Ana Maria Delgado, Joyce Caliendo, Christina Kreisch, Kaze W. K. Wong, William R. Coulton, Michael Eickenberg, Gabriele Parimbelli, Yueying Ni, Ulrich P. Steinwandel, Valentina La Torre, Romeel Dave, Nicholas Battaglia, Daisuke Nagai, David N. Spergel, Lars Hernquist, Blakesley Burkhart, Desika Narayanan, Benjamin Wandelt, Rachel S. Somerville, Greg L. Bryan, Matteo Viel, Yin Li, Vid Irsic, Katarina Kraljic, Federico Marinacci, and Mark Vogelsberger. The CAMELS Project: Public Data Release. ApJs, 265(2):54, April 2023. [78] Lucia A. Perez, Shy Genel, Francisco Villaescusa-Navarro, Rachel S. Somerville, Austen Gabrielpillai, Daniel Angles-Alcazar, Benjamin D. Wandelt, and L. Y. Aaron Yung. Constraining Cosmology with Machine Learning and Galaxy Clustering: The CAMELS-SAM Suite. ApJ, 954(1):11, September 2023. [79] Ningyuan Huang, Richard Stiskalek, Jun-Young Lee, Adrian E. Bayer, Charles C. Margossian, Christian Kragh Jespersen, Lucia A. Perez, Lawrence K. Saul, and Francisco Villaescusa-Navarro. CosmoBench: Multiscale, Multiview, Multitask Cosmology Benchmark for Geometric Deep Learning. arXiv e-prints, page arXiv:2507.03707, July 2025. [80] Boris Khoromskij. (d log n)-quantics approximation of n-d tensors in high-dimensional numerical modeling. Constructive Approximation, 34(2):257280, 2011. [81] Mazen Ali and Anthony Nouy. Approximation theory of tree tensor networks: Tensorized univariate functions. Constructive Approximation, 58(2):463544, 2023. [82] Denario Astropilotai, Pablo Bermejo, Boris Bolliet, Francisco Villaescusa-Navarro, and Pablo Villanueva Domingo. QITT-enhanced multi-scale substructure analysis with learned topological embeddings for cosmological parameter estimation. In Open Conference of AI Agents for Science 2025, 2025. [83] P. Hosseinzadeh, G. Bhardwaj, V. Mulligan, M. Shortridge, T. Craven, F. Pardo-Avila, S. Rettie, et al. Comprehensive computational design of ordered peptide macrocycles. Science, 358:14611466, 2017. [84] Qiyao Zhu, Vikram Mulligan, and Dennis Shasha. Heuristic energy-based cyclic peptide design. PLoS Comput. Biol., 21:e1012290, 2025. [85] Jacek Bochnak, Michel Coste, and Marie-Francoise Roy. Real algebraic geometry, volume 36 of Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)]. Springer-Verlag, Berlin, 1998. Translated from the 1987 French original, Revised by the authors. [86] Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds with high confidence from random samples. Discrete Comput. Geom., 39(1-3):419441, 2008. [87] Boye Niu, Yiliao Song, Kai Lian, Yifan Shen, Yu Yao, Kun Zhang, and Tongliang Liu. Flow: Modularized agentic workflow automation, 2025. [88] Daniel J. H. Chung, Zhiqi Gao, Yurii Kvasiuk, Tianyi Li, Moritz Munchmeyer, Maja Rudolph, Frederic Sala, and Sai Chaitanya Tadepalli. Theoretical physics benchmark (TPBench)a dataset and study of AI reasoning capabilities in theoretical physics. Mach. Learn. Sci. Tech., 6(3):030505, 2025. [89] Christine Ye, Sihan Yuan, Suchetha Cooray, Steven Dillmann, Ian L. V. Roque, Dalya Baron, Philipp Frank, Sergio Martin-Alvarez, Nolan Koblischke, Frank Qu, Diyi Yang, Risa Wechsler, and Ioana Ciuca. ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers? arXiv e-prints, page arXiv:2510.24591, October 2025. [90] Samuel Schmidgall and Michael Moor. Agentrxiv: Towards collaborative autonomous research, 2025. [91] Maximilian Dauner and Gudrun Socher. Energy costs of communicating with ai. Frontiers in Communication, 10:1572947, 2025. 77 [92] Romain Couillet, Denis Trystram, and Thierry Menissier. The submerged part of the ai-ceberg [perspectives]. IEEE Signal Processing Magazine, 39(5):1017, 2022. [93] Imre Lakatos. The methodology of scientific research programmes: Ed by john worrall and gregory currie. Cambridge University Press, 1978. [94] Willard van Orman Quine. Two dogmas of empiricism. In Can Theories be Refuted? Essays on the Duhem-Quine Thesis, pages 4164. Springer, 1976. [95] Paul Feyerabend. Against method: Outline of an anarchistic theory of knowledge. Verso Books, 2020. [96] Erik Brynjolfsson. The turing trap: The promise & peril of human-like artificial intelligence. In Augmented education in the global age, pages 103116. Routledge, 2023. [97] Thomas Kuhn and Ian Hacking. The structure of scientific revolutions, volume 2. University of Chicago press Chicago, 1970. [98] Allen Institute for Artificial Intelligence. Astabench: Benchmarking ai agents for science, 2025. https://allenai.org/asta/bench. [99] Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, and Mark Gerstein. Risks of AI Scientists: Prioritizing Safeguarding Over Autonomy. arXiv e-prints, page arXiv:2402.04247, February 2024."
        },
        {
            "title": "A Example papers",
            "content": "A.1 Mathematical physics 78 A.2 Planetary science 90 A.3 Medicine and Health Services Research A.4 Chemistry 117 A.5 Astrophysics: gravitational waves 132 A.6 Astrophysics: stellar physics A.7 Biology 161 A.8 Digital health 176 A.9 Biophysics A.10 Neuroscience 212 A.11 Material science 225 A.12 Astrophysics: black hole-stellar mass relation A.13 Quantum physics, deep learning and cosmology"
        }
    ],
    "affiliations": [
        "Big Data Institute, University of Oxford",
        "Cavendish Astrophysics, University of Cambridge",
        "Center for Computational Astrophysics, Flatiron Institute",
        "Center for Computational Quantum Physics, Flatiron Institute",
        "Chemical Engineering Department, University of Virginia",
        "Computer Vision Center, Universitat Autònoma de Barcelona",
        "Department of Applied Physics, University of the Basque Country",
        "Department of Astrophysical Sciences, Princeton University",
        "Donostia International Physics Center",
        "Infosys Ltd",
        "Kavli Institute for Cosmology, University of Cambridge",
        "Ragon Institute of Mass General, MIT",
        "Robert F. Smith School of Chemical and Biomolecular Engineering, Cornell University",
        "School of Zoology, Faculty of Life Sciences, Tel-Aviv University"
    ]
}