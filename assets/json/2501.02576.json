{
    "paper_title": "DepthMaster: Taming Diffusion Models for Monocular Depth Estimation",
    "authors": [
        "Ziyang Song",
        "Zerong Wang",
        "Bo Li",
        "Hao Zhang",
        "Ruijie Zhu",
        "Li Liu",
        "Peng-Tao Jiang",
        "Tianzhu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Monocular depth estimation within the diffusion-denoising paradigm demonstrates impressive generalization ability but suffers from low inference speed. Recent methods adopt a single-step deterministic paradigm to improve inference efficiency while maintaining comparable performance. However, they overlook the gap between generative and discriminative features, leading to suboptimal results. In this work, we propose DepthMaster, a single-step diffusion model designed to adapt generative features for the discriminative depth estimation task. First, to mitigate overfitting to texture details introduced by generative features, we propose a Feature Alignment module, which incorporates high-quality semantic features to enhance the denoising network's representation capability. Second, to address the lack of fine-grained details in the single-step deterministic framework, we propose a Fourier Enhancement module to adaptively balance low-frequency structure and high-frequency details. We adopt a two-stage training strategy to fully leverage the potential of the two modules. In the first stage, we focus on learning the global scene structure with the Feature Alignment module, while in the second stage, we exploit the Fourier Enhancement module to improve the visual quality. Through these efforts, our model achieves state-of-the-art performance in terms of generalization and detail preservation, outperforming other diffusion-based methods across various datasets. Our project page can be found at https://indu1ge.github.io/DepthMaster_page."
        },
        {
            "title": "Start",
            "content": "DepthMaster: Taming Diffusion Models for Monocular Depth Estimation 1 Ziyang Song* , Zerong Wang* , Bo Li , Ruijie Zhu , Li Liu , Peng-Tao Jiang , Hao Zhang , Tianzhu Zhang 5 2 0 2 J 5 ] . [ 1 6 7 5 2 0 . 1 0 5 2 : r AbstractMonocular depth estimation within the diffusiondenoising paradigm demonstrates impressive generalization ability but suffers from low inference speed.pressive results but suffers from long inference time. Recent methods adopt singlestep deterministic paradigm to improve inference efficiency while maintaining comparable performance. However, they overlook the gap between generative and discriminative features, leading to suboptimal results. In this work, we propose DepthMaster, single-step diffusion model designed to adapt generative features for the discriminative depth estimation task. First, to mitigate overfitting to texture details introduced by generative features, we propose Feature Alignment module, which incorporates highquality semantic features to enhance the denoising networks representation capability. Second, to address the lack of finegrained details in the single-step deterministic framework, we propose Fourier Enhancement module to adaptively balance low-frequency structure and high-frequency details. We adopt two-stage training strategy to fully leverage the potential of the two modules. In the first stage, we focus on learning the global scene structure with the Feature Alignment module, while in the second stage, we exploit the Fourier Enhancement module to improve the visual quality. Through these efforts, our model achieves state-of-the-art performance in terms of generalization and detail preservation, outperforming other diffusion-based methods across various datasets. Our project page can be found in https://indu1ge.github.io/DepthMaster page. Index TermsMonocular depth estimation, Zero-shot depth estimation, Diffusion models. I. INTRODUCTION ONOCULAR depth estimation (MDE) has garnered considerable attention due to its simplicity, low cost, and ease of deployment. Unlike traditional depth-sensing techniques such as LiDAR or stereo vision, MDE only requires single RGB image as input, making it highly appealing for wide range of applications, including autonomous driving [1] [4], virtual reality [5], [6], and image synthesis [7], [8]. This versatility also presents significant challenge: achieving exceptional generalization to effectively handle the diversity and complexity of broad-range application scenarios. However, this is non-trivial task due to variants in scene layouts, depth distributions, lighting conditions, etc. *Equal contribution. Corresponding authors. Ziyang Song, Ruijie Zhu, Li Liu, and Tianzhu Zhang are with School of Information Science and Technology, University of Science and Technology of China (USTC), Hefei 230026, P.R.China. Contact: {songziyang, ruijiezhu, liu li}@mail.ustc.edu.cn; {tzzhang}@ustc.edu.cn. Zerong Wang, Bo Li, Hao Zhang, Peng-Tao Jiang are with vivo Mobile Communication Co., Ltd., Hangzhou 310030, P.R.China. Contact: {wangzerong}@live.com; {libra, haozhang, pt.jiang}@vivo.com. This work was done during Ziyang Songs internship at vivo. Recent research on zero-shot monocular depth estimation has primarily evolved into two main branches: data-driven [9] [15] and model-driven [16][20]. The former relies on largescale image-depth pairs to achieve the mapping from image to depth, where the process of data collecting and training is extremely time-consuming and resource-exhausting. In contrast, model-driven approaches aim to leverage pre-trained backbones, particularly in the context of Stable Diffusion models [21], [22]. For example, Marigold [16] reformulates depth estimation as diffusion-denoising process, achieving impressive performance in both generalization and detail preservation. However, the iterative denoising process results in low inference speed. GenPercept [19] proposes deterministic single-step paradigm, that is, directly inputting RGB images and outputting depth maps, reducing inference time with comparable performance. Despite these advancements in applying diffusion models to MDE, few works have thoroughly explored how to best adapt the generative features in diffusion models for the discriminative task. In this work, we conduct an in-depth analysis of the feature representation within diffusion models. Typically, diffusion models consist of an image-to-latent (I2L) encoder-decoder and denoising network. The former compresses images into the latent space and reconstructs them, while the latter perceives and reasons about the scene. Experimentally, we find that the main bottleneck lies in the feature representation capability of the denoising network. In fact, the reconstruction task used to pre-train the denoising network induces the model to prioritize texture details over structure, leading to unrealistic textures in depth predictions (see Fig. 1, Column 3, yellow boxes). Therefore, how to enhance the feature representation capacity of the denoising network and reduce its reliance on irrelevant details is key issue for taming diffusion models for depth estimation. Furthermore, the high visual quality of diffusion models outputs comes from the iterative refinement process. In the early steps, the model learns to recover the general structure, while in later steps, details are gradually refined. When reformulated as singlestep paradigm, the model struggles to learn both primary structure and fine details in single forward pass, leading to blurry predictions (see Fig. 1, Column 4, red boxes). Thus, how to improve the fine-grained details in the singlestep framework is another crucial challenge in leveraging the generative features for depth estimation. To address the aforementioned challenges, we propose DepthMaster, tamed single-step diffusion model designed to enhance the generalization and detail preservation abilities"
        },
        {
            "title": "Denoise",
            "content": "Stage 1 Stage 2 Fig. 1. Visualization of different paradigms. Denoise refers to predicting depth in diffusion-denoising way. Limited by the feature representation capability of the denoising network, predictions tend to overfit texture details and miss the real structure, as highlighted with yellow boxes in Column 3. Stage1 alleviates this issue with the Feature Alignment module, but suffers from blurry outputs due to removing the iterative process, as highlighted with red boxes in Column 4. Stage2 presents the final model fine-tuned with the Fourier Enhancement module, which exhibits excellent generalization and fine-grained details. of depth estimation models. First, to improve the feature representation capability of the denoising network, we incorporate high-quality external visual representations. Feature Alignment module is introduced to align the feature distributions of the diffusion model with those of the external encoder. This alignment effectively integrates semantic information into the diffusion models latent states, helping to mitigate the overfitting to texture details. Second, to alleviate the lack of fine-grained details caused by removing the iterative process, we propose Fourier Enhancement module. Operating in the frequency domain, the module adaptively balances low-frequency structural features and high-frequency detail features in single forward pass, effectively simulating the learning process in the multi-step denoising process. To fully leverage the potential of the modules, we adopt two-stage training strategy. In the first stage, we focus on learning scene structure with the help of the Feature Alignment module. In the second stage, we incorporate the Fourier Enhancement module to refine fine-grained details. Through tailoring generative features and exploiting the two-stage training strategy, our method achieves impressive zero-shot performance and exceptional detail preservation ability, bridging the gap between datadriven and model-driven approaches. The main contributions of our work are as follows: We propose DepthMaster, novel approach that customizes generative features in diffusion models to suit the discriminative depth estimation task. We introduce Feature Alignment module to mitigate overfitting to texture details with high-quality external features and Fourier Enhancement module to refine finegrained details in the frequency domain. Our method exhibits state-of-the-art zero-shot performance and superior detail preservation ability, surpassing other diffusion-based methods across various datasets. II. RELATED WORK A. Diffusion Models Diffusion probabilistic models (DMs) [23] have emerged as highly competitive class of generative models in recent years, demonstrating impressive performance across wide range of tasks, including image generation [21], [22], [24], [25], inpainting [26], [27], and super-resolution [28], [29]. These models operate by gradually transforming data distribution (e.g., images) into noise distribution through series of diffusion steps, and then learning to reverse this process to generate high-quality samples. Existing methods such as Denoising Diffusion Probabilistic Models (DDPM) [30] and Score-Based Generative Models [31] have further refined this framework by learning the reverse process with stochastic differential equations or score-matching techniques. However, optimizing and evaluating these models in pixel space with the iterative process leads to low inference speed and high training costs. To speed up the inference process, some methods design advanced sampling strategies [32][34] and adopt hierarchical approaches [35], [36], but training costs remain high. Latent Diffusion Models (LDM) [21] propose more efficient process by operating in compressed latent space, where an image-to-latent encoder-decoder is adopted for the conversion between high-dimension image space and low-dimension latent space. By training on large-scale, highquality text-image dataset LAION-5B [37], LDM can learn powerful image priors, achieving impressive image quality with affordable computation costs. In this work, we take the pre-trained diffusion model as our backbone to make use of the powerful image priors and explore the best way to adapt diffusion models for monocular depth estimation. Although diffusion models excel at recovering pixel-level details, they may struggle with tasks requiring high-level [38][40], the resemantic understanding. As revealed in construction task used to train the denoising network does not sufficiently address the challenge of eliminating irrelevant details. As result, low-level texture and fail to capture the real structure, as shown in Fig. 1. REPA [40] suggests incorporating high-quality external representations can speed up convergence in generation tasks. In our work, we prove that incorporating semantic information can not only alleviate the overfitting to texture but also enhance the generalization ability in discriminative tasks. tends to overfit the model B. Monocular Depth Estimation Single-domain depth estimation: Monocular depth estimation (MDE) has demonstrated promising results in various depth estimation benchmarks [41][45]. Eigen et al. [46] first introduces convolution neural networks for end-to-end training in MDE. Subsequent advances primarily concentrate on the following aspects: (a) Enhancing network architecture, including approaches like residual networks [47], [48], multiscale fusion [49][51], transformers [10], [52] and diffusion models [53], [54]. (b) Designing optimization strategies and loss functions, such as classification-regression paradigm [55] [59] and geometric constraints [60], [61]. (c) Incorporating auxiliary information or multi-task learning, for example, surface normal estimation [62], [63] and semantic segmentation [64], [65]. Although these methods achieve promising performance on individual datasets, they cannot meet the requirement of strong generalization ability, which is crucial for MDEs widespread applications. Therefore, recent studies have shifted their focus to zero-shot monocular depth estimation. Zero-shot depth estimation: Estimating accurate depth maps for in-the-wild images is crucial but challenging task due to variants in scene layouts, depth distributions, lighting conditions, etc. Some pioneering works have tried to solve this problem, which can be primarily divided into two main branches: data-driven and model-driven. The former focuses on collecting large-scale image-depth pairs to achieve the mapping from RGB images to depth maps. To maintain training stability, to predict relative these approaches opt depth, which can already represent the scene structure. For example, Diversedepth [14] and MiDaS [13] predict affineinvariant depth to jointly train on multiple datasets and achieve good generalization across various scenarios. On top of this, Omnidata [12] introduces dataset that comprises roughly 14.5 million images and trains robust depth estimation model following MiDaS to achieve zero-shot cross-dataset transfer. More recently, ZoeDepth [11] trains relative depth estimation model and demonstrates its transferability to metric depth through fine-tuning on metric depth datasets. We follow this strategy to predict relative depth because it is not only practical but also can be converted to metric depth easily. DPT [10] further enhances MiDaS by replacing the original CNN with Vision Transformer. Depth Anything [9], then, expands the training datasets with 62 million unlabeled images these methods to further enlarge data coverage. However, require the process of data collecting and training, which is time-consuming and resource-exhausting. The second branch of research seeks to improve model generalization by leveraging powerful image priors inherited in pre-trained models, especially in the context of Stable Diffusion models, which are trained on large-scale, highquality datasets. Marigold [16] first explores the potential of pretrained latent diffusion models(LDMs) for monocular depth estimation by reformulating depth estimation as conditional diffusion-denoising process. GeoWizard [17] further improves it by incorporating normal estimation to enhance the ability to capture geometric details. To address the problem of low inference efficiency caused by iterative 3 denoising, DepthFM [18] introduces flow matching to reduce the number of sampling steps at the cost of slight performance degradation. More recently, GenPercept [19] offers systematic analysis of fine-tuning protocols and proposes single-step deterministic paradigm, where only the image latent is fed into the denoising network, and the noise latent is output for depth prediction. Amazingly, it notably reduces the inference time with comparable performance. Lotus [20] also exploits single-step denoising paradigm and proposes detail preserver branch to enhance visual quality. Despite these advancements in applying diffusion models to MDE, no work has thoroughly explored how to best adapt the generative features in diffusion models for the discriminative task. In this work, we address this gap by enhancing the representation capability of the denoising network and adaptively refining features in the frequency domain to capture details, leading to further improvements in both performance and visual quality. III. METHOD The overall framework is illustrated in Fig. 2. We begin with introducing the single-step deterministic paradigm, as detailed in Sec. III-A. Next, we provide an in-depth analysis of the Stable Diffusion model and introduce Feature Alignment module to enhance the representation capability of the denoising network in Sec. III-B. To address the limitation of the single-step models inability to capture fine-grained details, we introduce Fourier Enhancement module in Sec. III-C and weighted multi-directional gradient loss in Sec. III-D. Finally, we present the two-stage training strategy in Sec. III-E. A. Deterministic Paradigm Our model is built upon Stable Diffusion v2 [21], which is pre-trained on the large-scale LAION-5B [37] dataset. The powerful image priors encoded in the model significantly assist the depth estimation task. Rather than reformulating the task to fit the diffusion-denoising paradigm exploited by the diffusion model, we craft the model to better adapt to the task. Specifically, we employ single-step deterministic transformation from image to depth, as illustrated in the upper part of Fig. 2. First, the image RHW 3 is encoded into the latent space using the I2L encoder, denoted as E, and obtain the image latent zRGB = (I) . (1) The image latent is then fed into the denoising U-Net model, denoted as ϵθ, which performs scene perception and generates the corresponding depth map. The timestep is set to 1, ensuring direct conversion from image latent to depth latent: zpred = ϵθ (zRGB, 1) . (2) Since the I2L encoder-decoder reconstructs depth maps with negligible loss of accuracy, we only fine-tune the U-Net and apply constraints in the latent space. Instead of predicting depth, we opt to predict square-root disparity. On the one hand, square-root disparity emphasizes the accuracy of nearby objects, which is desired by applications like autonomous driving. On the other hand, square-root disparity leads to 4 Fig. 2. The overall framework of DepthMaster. RGB is first projected into the latent space by the I2L Encoder to obtain zRGB. Next, the U-Net converts RGB latent to depth prediction latent zpred, which is decoded back to the depth map by the I2L Decoder. The Feature Alignment module is applied in the first stage to align the representation of the U-Net to that of the high-quality external encoder, introducing semantic information into the diffusion model. In the second stage, the Fourier Enhancement module adaptively balances low-frequency structure and high-frequency details to enhance the visual quality. more uniform distribution, as illustrated in Fig. 5, thus fully releasing the capability of the input range. The preprocessed ground truth (GT) is first normalized to [1, 1] to fit the input range of the I2L encoder and then passed through the encoder to obtain the GT depth latent zGT . The training objective in latent space is given as follows: Llatent = (zGT zpred)2. (3) Experimental results demonstrate that this single-step deterministic adaptation of diffusion models achieves comparable generalization performance to the standard denoising-diffusion paradigm. Moreover, it removes the iterative denoising process, significantly improving inference efficiency. patches and is the feature dimension. Simultaneously, the UNet encoder extracts image representation at its middle block, denoted as Funet RhwC. Our goal is to align the image representation from the U-Net with the high-quality representation from the external encoder. Since the two representations lie in different spaces, we use Multi-Layer Perceptron (MLP) to project Funet into the feature space of Fext, yielding the transformed representation Funet = hϕ(Funet). To enforce feature alignment, we minimize the distance between the two feature distributions. The feature alignment loss is defined as follows: Lf a(Fext, Funet) = dist(Fext, Funet), (4) B. Feature Alignment Module Stable Diffusion v2 consists of two components: the I2L encoder-decoder and the denoising U-Net. The I2L encoderdecoder is responsible for feature compression, which aims to reduce inference time and training costs. Trained with image it primarily captures low-level features. In reconstruction, contrast, the U-Net is responsible for recovering images from their noisy counterparts, enabling it to harvest scene perception and reasoning capabilities. However, since the U-Net is trained with the reconstruction task, it tends to overemphasize finegrained color details, leading to pseudo-textures rather than capturing true structure, as shown in Fig. 1. Inspired by REPA [40], we introduce semantic regularization to enhance the U-Nets scene representation capabilities and prevent overfitting to superficial color information. Specifically, we incorporate pre-trained external encoder , which provides high-quality semantic feature representation. In this work, we use DINOv2 [66] as the external encoder. For given RGB image I, the external feature representation is Fext = (I) RN D, where is the number of image where dist(, ) measures the distance between the two feature distributions. In this work, we utilize the Kullback-Leibler (KL) divergence. Specifically, the features are first normalized along the feature dimension, obtaining the distribution in the latent space. We then minimize the KL divergence between the two feature distributions: dist(Fext, Funet) = kl div(Fext, Funet), (5) where Fext and Funet represents the normalized features. Through feature alignment, the U-Net learns more semantically meaningful representation, improving its generalization ability while avoiding overfitting to low-level details. C. Fourier Enhancement Module The single-step paradigm effectively speeds up the inference process by avoiding multi-step iterations and multi-run integration. However, the fine-grained characteristic of the diffusion models outputs typically arises from the iterative refinement the single-step model suffers from process. Consequently, 5 blurry predictions, as shown in Fig. 1. To alleviate this problem, we propose Fourier Enhancement module to refine highfrequency details, as illustrated in the right-bottom of Fig. 2. Inspired by DiffusionEdge [67], the module operates in the frequency domain to simulate the iterative refinement processs focus on different bands. Specifically, the Fourier Enhancement module is composed of two components: spatial pass for general structure capture and frequency pass for detail enhancement. In the frequency pass, the hidden state from the U-Nets middle block Fmid RChw is first transformed into the frequency domain using 2D Fast Fourier Transform (FFT), yielding Fmidf . To adaptively balance information across different frequency bands, modulator comprised of convolution and activation layers is applied to Fmidf . The enhanced feature is then transformed back to the spatial domain using an inverse 2D Fast Fourier Transform (iFFT): Ff = iFFT(σ(Conv(FFT(Fmid)))), (6) where σ refers to the activation layer. Next, we concatenate the feature from the spatial pass Fs with that from the frequency pass Ff and perform convolution operation to obtain the final enhanced feature ˆFmid: E. Two-stage Training Curriculum Since the depth reconstruction accuracy of the I2L encoderdecoder is sufficiently high, we focus on fine-tuning the UNet. Experiments reveal that latent-space supervision helps the model to better capture the overall scene structure, while pixellevel supervision improves fine-grained details but introduces distortions in the global structure. Based on these observations, we propose two-stage training strategy. In the first stage, our goal is to train model that can robustly generalize across diverse scenarios. To achieve this, we apply constraints in the latent space and incorporate the Feature Alignment module to enhance the models scene perception capability. The training objective of the first stage is as follows: Lstage1 = Llatent + λf aLf a, (9) where λf is set to 1. In the second stage, we aim to optimize the models performance on detail preservation. To balance structure and detail information, we incorporate the Fourier Enhancement module. After obtaining depth predictions, we apply constraints at the pixel level and introduce the weighted multi-directional gradient loss to enhance edge sharpness. The total objective function for the second stage is as follows: ˆFmid = Conv(FsFf ), (7) Lstage2 = Lpixel + λhLh, (10) where denotes the concatenation operator. By operating in the frequency domain, our model adaptively balances lowfrequency structural features and high-frequency detail features within single forward pass, effectively improving the visual quality of depth predictions. D. Weighted Multi-directional Gradient Loss To further enhance the sharpness of depth predictions, we propose weighted multi-directional gradient loss function to capture detailed edge information on depth maps in all directions. Specifically, for the ground truth depth DGT and depth prediction Dpred, we compute gradients GGT RHW 4 and Gpred RHW 4 in horizontal, vertical and diagonal directions. At edges where the foreground and background meet, gradient values are typically much larger than those within local structures. These dominant differences can overwhelm the gradient loss, leading to unstable training and suboptimal solutions. To mitigate this problem, we employ modified Huber loss [68], defined as follows: (cid:26) δ GGT Gpred , if GGT Gpred δ, otherwise, (8) where δ controls the threshold at which the loss transitions from quadratic to linear, reducing the influence of outliers caused by large gradient differences at the foregroundbackground interface. Since the depth values are scaled to the interval [-1, 1], the gradient loss corresponding to most foreground-background interface points is multiplied by value less than 1, thus reducing their proportion in the total gradient loss. This adjustment allows our model to focus on the fine details not only at foreground-background interfaces but also within local structures, as shown in Fig. 4. 2 (GGT Gpred)2 + 1 Lh = 2 δ2, 1 where Lpixel is the pixel MSE loss and λh is set to 0.001. Benefiting from the two-stage training strategy, our model achieves both accurate structure capture and sharp edge preservation. IV. EXPERIMENTS A. Implementation Details Our model is based on Stable Diffusion v2 [21], with text conditioning disabled. In the first stage, we train our model for 20k iterations using the Adam [69] optimizer with learning rate of 3 105. In the second stage, we reduce the learning rate to 3 106 and train for an additional 10k iterations. To achieve batch size of 32, we exploit gradient accumulation. Training the first stage takes approximately 30 hours, while fine-tuning the model in the second stage requires an additional 30 hours, both on single NVIDIA H800 GPU. Additionally, we apply random horizontal flipping augmentation to enhance the diversity of training datasets. B. Datasets Training Datasets. We train our model on two synthetic datasets: Hypersim [72] and Virtual KITTI [73]. Hypersim is high-fidelity dataset covering 461 indoor scenes with rich textures and geometry, generated using realistic 3D rendering techniques. We use the depth annotations and corresponding RGB images to train our model. Following Marigold [16], we transform the original depth values relative to the focal point into depth values relative to the focal plane. The official split with around 54K samples is used with the training resolution of 480640. Virtual KITTI is synthetic outdoor dataset that serves as variant of the original KITTI [74] dataset, providing wide range of road scenes under diverse lighting, weather, and traffic conditions. Unlike KITTI, Virtual KITTI TABLE QUANTITATIVE COMPARISON WITH STATE-OF-THE-ART ZERO-SHOT AFFINE-INVARIANT MONOCULAR DEPTH ESTIMATION METHODS. THE UPPER PART LISTS DATA-DRIVEN METHODS AND THE LOWER PART PRESENTS THOSE BASED ON DIFFUSION MODELS. ALL METRICS ARE IN PERCENTAGE TERMS WITH BOLD BEST AND UNDERLINE SECOND BEST. * STANDS FOR THE RESULTS REPRODUCED BY LOTUS. Method Training Data KITTI NYUv2 ETH3D ScanNet DIODE AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 Avg. Rank Data-driven methods DiverseDepth [14] MiDaS [13] LeReS [70] Omnidata [12] HDN [71] DPT [10] Depth Anything V2 [9] Model-driven methods Marigold [16] GeoWizard [17] DepthFM [18] GenPercept [19] Lotus [20] DepthMaster (Ours) 320K 2M 354K 12.2M 300K 1.4M 63.5M 74K 280K 74K 74K 59K 74K 19.0 18.3 14.9 14.9 11.5 11.1 8. 9.9 9.7 9.1 9.9 9.3 8.2 70.4 71.1 78.4 83.5 86.7 88.1 94.6 91.6 92.1 90.2 90.4 92.8 93.7 11.7 9.5 9.0 7.4 6.9 9.1 4.3 5.5 5.2 6.0 5.6 5.3 5.0 87.5 91.5 91.6 94.5 94.8 91.9 98. 96.4 96.6 95.5 96.0 96.7 97.2 22.8 19.0 17.1 16.6 12.1 11.5 6.2 6.5 6.4 6.5 6.2 6.8 5.3 69.4 88.4 77.7 77.8 83.3 92.9 98.0 96.0 96.1 95.4 95.8 95.3 97.4 10.9 9.9 9.1 7.5 8.0 8.4 4. 6.4 6.1 6.6 6.2* 6.0 5.5 88.2 90.7 91.7 93.6 93.9 93.2 98.1 95.1 95.3 94.9 96.1* 96.3 96.7 37.6 26.6 27.1 33.9 24.6 26.9 26.0 30.8 29.7 22.4 35.7 22.8 21.5 63.1 71.3 76.6 74.2 78.0 73.0 75. 77.3 79.2 78.5 75.6 73.8 77.6 7.0 5.4 4.6 3.8 2.4 3.4 1.3 4.3 2.9 4.5 4.4 3.5 1.2 I 2 N 3 E N S D I"
        },
        {
            "title": "Ground Truth",
            "content": "Marigold [16] DAv2 [9] Lotus [20]"
        },
        {
            "title": "Ours",
            "content": "Fig. 3. Qualitative comparison with zero-shot monocular depth estimation methods across different datasets. Our model demonstrates excellent detail preservation and structure capture capabilities. Benefiting from the Feature Alignment module, our model avoids overfitting to textures. is generated with 3D simulator and provides dense depth annotations. We train on approximately 20k samples with resolution of 1216 352 and set the far plane to 80 meters. The two datasets are mixed in ratio of 9:1. Evaluation Datasets. We evaluate our models zero-shot performance on 5 real datasets. NYU-Depth-V2 (NYUv2) [75] and ScanNet [76] are indoor datasets commonly used for evaluating depth estimation methods. We use the official test split with 654 images for NYUv2 and the split proposed by Marigold with 800 images for ScanNet. KITTI [74] is street-scene dataset captured with equipment mounted on moving vehicle. We follow the Eigen split [46], which consists of 652 images. ETH3D [77] and DIODE [78] are two real datasets containing both indoor and outdoor images. For evaluation, we use the splits in Marigold to evaluate on 454 samples from ETH3D and 771 samples from DIODE. For zero-shot evaluation, we report absolute relative error AbsRel = 1 and accuracy metric , where ˆDpred δ1 = 1 HW ΣHW is the aligned depth prediction and [] is Iverson bracket. For sharp boundary evaluation, we use the F1-score proposed by DepthPro [79], which computes the recall and precision of edges in depth predictions and ground truth depth maps. This metric reflects the visual quality of depth predictions to some extent, with higher values indicating higher visual quality. DGT ˆDpred DGT , DGT ˆDpred HW ΣHW max (cid:16) ˆDpred DGT < 1.25 (cid:17) (cid:104) (cid:105) R ] 6 1 [ g M ] 9 [ 2 D ] 0 2 [ o u Fig. 4. Qualitative results on in-the-wild examples. Our model not only recovers correct scene structure, but also exhibits fine-grained details. C. Qualitative and Quantitative Comparison D. Ablation Studies Table presents comparison of our method with other state-of-the-art (SOTA) zero-shot monocular depth estimation methods. The upper part of the table lists data-driven methods, while the lower part focuses on diffusion model-based methods. As shown in Table I, diffusion model-based methods, despite being trained on relatively small amount of data, already outperform many approaches that rely on large-scale datasets. This highlights the significant role of strong image priors encoded in diffusion models, which greatly enhance the generalization capabilities of depth estimation models. Our approach falls into the diffusion model-based category. By incorporating the single-step deterministic paradigm and the specially designed Feature Alignment module, we achieve 17.2% improvement over Marigold [16] in AbsRel on KITTI, effectively narrowing the performance gap between diffusion model-based methods and those reliant on large-scale datasets. To better illustrate the superiority of our approach, we provide qualitative results in Fig. 3. As highlighted in red boxes, our method excels in recovering complete structure and preserving fine-grained details, while avoiding texture overfitting commonly encountered in generative models. Additionally, Fig. 4 showcases our models predictions on in-the-wild examples, further demonstrating its remarkable generalization ability in real-world scenarios. These results emphasize the practical applicability and versatility of our approach, making it highly suitable for various real-world applications. In this section, we conduct comprehensive experiments to validate the effectiveness of our design choices. Learning Paradigm. The ablation study of the learning paradigm is shown in Table II. I2L refers to feeding depth maps into the I2L encoder-decoder and outputting the reconstructed depth maps.1 As shown in Table II, the reconstruction accuracy of the I2L encoder-decoder is sufficiently high. That is to say, in the paradigm of the diffusion model, the main performance bottleneck is in the U-Net part, which is also the focus of our work. Denoising refers to predicting depth maps from noise using the diffusion-denoising paradigm, while Deterministic indicates directly predicting depth from RGB images. Obviously, applying the diffusion model in deterministic manner is better suited for the discriminative task, which not only enhances the models generalization capability but also significantly improves inference efficiency. Actually, in the denoising-diffusion paradigm, noise is progressively added to ensure outputs diversity, which is not desirable in deterministic tasks, thus impairing the performance. When predicting depth in an iterative deterministic way, where the U-Nets output is iteratively input to the U-Net for 4 times, the performance of the model is further improved. This is because the iterative paradigm aligns with the multi-step denoising process used by diffusion models, therefore better 1Only datasets with dense depth maps are evaluated. TABLE II ABLATION OF PARADIGM. I2L MEANS FEEDING DEPTH MAPS INTO I2L ENCODER-DECODER AND OUTPUTTING RECONSTRUCTED ONES. DENOISING AND DETERMINISTIC REFER TO PREDICTING DEPTH IN DIFFUSION-DENOISING AND DETERMINISTIC WAYS, RESPECTIVELY. ITERATIVE MEANS ITERATIVE REFINEMENT THROUGH THE U-NET 4 TIMES IN DETERMINISTIC WAY. * INDICATES THE PARADIGM WE USE. Paradigm I2L Denoising Deterministic* Iterative KITTI NYUv2 ScanNet ETH3D DIODE AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 Hypersim Inference Time (s) F1 - 10.4 10.3 10.0 - 90.2 90.4 91.1 1.1 5.7 5.3 5.2 99.5 96.0 96.6 96. 0.9 6.9 6.0 5.9 99.7 94.6 96.2 96.1 - 6.4 6.5 6.1 - 95.7 95.8 96.3 8.4 30.9 29.9 29.4 92.4 76.8 77.0 77. 0.615 0.274 0.304 0.310 - 12.91 0.42 0.83 TABLE III ABLATION OF DEPTH PREPROCESS. PREDICTING DISPARITY INSTEAD OF DEPTH RESULTS IN IMPROVED PERFORMANCE ON OUTDOOR DATASETS, WHILE USING SQUARE-ROOT DISPARITY LEADS TO CONSISTENT IMPROVEMENTS ACROSS ALL DATASETS. Depth Preprocess KITTI NYUv ETH3D ScanNet DIODE AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 depth(D) disparity( 1 ) sqrt disp( 1 ) 10.3 8.9 8.7 90.4 92.4 93.1 5.3 5.3 5. 96.6 97.0 97.3 6.5 6.7 5.5 95.8 96.7 97.2 6.0 5.7 5.8 96.2 96.3 96.4 29.9 22.4 21. 77.0 74.0 77.2 TABLE IV ABLATION OF EXTERNAL MODEL TYPE IN FEATURE ALIGNMENT MODULE. INTRODUCING VARIOUS EXTERNAL ENCODERS CAN IMPROVE THE GENERALIZATION PERFORMANCE OF THE MODEL, AMONG WHICH DINOV2 YIELDS THE GREATEST PERFORMANCE IMPROVEMENT. External Model Type KITTI NYUv2 ETH3D ScanNet DIODE AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 baseline OpenCLIP [80] AIMv2 [81] SAM [82] DINOv2 [66] 8.7 8.5 8.4 8.3 8.3 93.1 93.3 93.4 93.5 93.7 5.1 5.0 5.1 5.0 5.0 97.3 97.3 97.3 97.3 97.3 5.5 5.4 5.5 5.3 5. 97.2 97.4 97.3 97.5 97.4 5.8 5.6 5.6 5.5 5.5 96.4 96.5 96.6 96.7 96.7 21.8 21.8 21.7 21.7 21.6 77.2 77.1 77.5 77.2 77.5 harnessing the prior knowledge inherent in diffusion models. However, the iterative process inevitably leads to low inference speed, resulting in approximately twice the inference time. Benefiting from the proposed modules, we use the singlestep deterministic approach and achieve zero-shot performance comparable to that of the iterative paradigm. Depth Preprocess. We conduct ablation studies on three different depth preprocessing methods, including depth, disparity, and square-root disparity (sqrt disp). To ensure compatibility with the input range of Stable Diffusion, the preprocessed depth maps are normalized to the range of [-1, 1] using percentiles. The results are shown in Table III. Switching from depth prediction to disparity prediction results in notable performance improvement, particularly on outdoor and mixed indoor-outdoor datasets. This improvement can be attributed to the fact that disparity amplifies the foreground structure, helps the model focus more on nearby objects, which is desired by outdoor applications such as autonomous driving. Furthermore, predicting square-root disparity yields an additional performance boost, which we adopt as our baseline. This is because square-root disparity produces more uniform depth distribution, as illustrated in Fig. 5, allowing for more efficeint use of the depth range. Feature Alignment module. We conduct ablation studies on different external encoders and feature alignment locations in the Feature Alignment module. As shown in Table IV-D, the high-quality features of these external encoders can effectively Fig. 5. Depth distribution of different depth preprocess methods on Virtual KITTI. Square-root disparity exhibits the most uniform distribution. modulate the features of the diffusion model and bring gains in zero-shot performance. Among them, DINOv2 has consistent performance improvements on various datasets. The results of the ablation study on feature alignment locations are shown in Table V. The U-Net is an encoder-decoder structure with three downsampling blocks, one middle block, and three upsampling blocks. The prior knowledge and scene perception abilities are primarily stored in the encoder part. Therefore, we perform feature alignment between the DINOv2 feature and the features from the first and second downsampling blocks 9 TABLE ABLATION OF FEATURE ALIGNMENT LOCATION. D1, D2 REFER TO THE FIRST AND SECOND DOWN BLOCKS OF THE U-NET, RESPECTIVELY. MID MEANS THE MIDDLE BLOCK OF THE U-NET. THE EFFECTIVENESS OF THE FEATURE ALIGNMENT MODULE INCREASES AS THE NUMBER OF THE ALIGNED LAYER GROWS DEEPER. Location baseline D1 D2 Mid KITTI NYUv2 ETH3D ScanNet DIODE AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 8.7 8.5 8.4 8. 93.1 93.5 93.6 93.7 5.1 5.0 5.1 5.0 97.3 97.3 97.3 97.3 5.5 5.3 5.4 5.3 97.2 97.5 97.4 97.4 5.8 5.6 5.5 5. 96.4 96.6 96.6 96.7 21.8 21.8 21.5 21.6 77.2 77.4 77.7 77.5 TABLE VI ABLATION OF DETAIL PRESERVATION. PIXEL INDICATES APPLYING CONSTRAINTS AT THE PIXEL LEVEL. FE REFERS TO THE FOURIER ENHANCEMENT MODULE. Lh REFERS TO THE WEIGHTED MULTI-DIRECTIONAL GRADIENT LOSS. TWO-STAGE MEANS THE TWO-STAGE TRAINING CURRICULUM. THE PROPOSED MODULES AND TRAINING CURRICULUM EFFECTIVELY ENHANCE THE DETAIL PRESERVATION CAPABILITY. Model pixel Lh FE Two-stage KITTI NYUv2 ETH3D Scannet DIODE AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 HyperSim F1 M.Base M.Pixel M.Huber M.FE Huber M.Full 8.7 8.6 8.5 8.3 8. 93.1 93.0 93.0 93.5 93.7 5.1 5.2 5.0 5.1 5.0 97.3 97.2 97.2 97.2 97.2 5.5 5.4 5.5 5.3 5.3 97.2 97.1 97.1 97.2 97.4 5.8 5.5 5.5 5.5 5. 96.4 96.8 96.9 96.7 96.7 21.8 21.5 21.6 21.6 21.5 77.2 77.7 77.4 77.4 77.6 0.306 0.307 0.308 0.314 0.337 in the frequency domain, both the models generalization and fine-grained details are improved. This demonstrates that the Fourier Enhancement module effectively mimics the iterative processs focus on different frequency bands. Additionally, with the proposed two-stage training strategy, the models fine-grained detail is significantly enhanced. The second stage only need to optimize details based on the first stage model, thus simplifying the problem of capturing both structure and deatils. Fig. 6 presents the qualitative results of the two stages, where the fine-tuned model demonstrates remarkable detail preservation ability, highlighting the effectiveness of our strategy in improving the visual quality. V. LIMITATIONS Although our method achieves performance comparable to data-driven approaches and detail preservation ability comparable to diffusion-based methods, the models large parameter size limits its deployment on mobile devices. Through experimentation, we identify some redundant parameters in the UNet, and removing these layers does not significantly affect performance. Therefore, reducing the models computational cost through effective pruning and distillation techniques will be key focus of our future work. VI. CONCLUSION In this work, we propose DepthMaster, method that crafts diffusion models for depth estimation. By incorporating the Feature Alignment module, we effectively mitigate the overfitting to texture details. Additionally, the Fourier Enhancement module enhances fine-grained detail preservation ability bi operating in the frequency domain. Benefiting from the careful design, DepthMaster achieves significant boost in zero-shot performance and inference efficiency. Extensive experiments validate the effectiveness of our approach, which achieves state-of-the-art performance in terms of generalization and detail preservation, outperforming other diffusion-based methods across various datasets."
        },
        {
            "title": "RGB",
            "content": "Stage 1 Stage 2 Fig. 6. Visualization of predictions from two stages. With the Fourier Enhancement Module and the two-stage training strategy, the final model exhibits excellent detail preservation ability. and the middle block, respectively. As shown in Table V, the effectiveness of the Feature Alignment module increases with the depth of the layer where it is applied. This occurs because the shallow U-Net layers capture more local information and are rich in details, while the middle layer features have better global perception, which matches the global nature of the DINOv2 features. When constraints are imposed on shallow layers, the detailed information will be compromised. Detail preservation. To validate the detail-preserving capability of the components we propose, we conduct series of ablation experiments, as shown in Table VI. M.Base represents our baseline. Directly applying constraints in the pixel space (M.pixel) and using the weighted multi-directional gradient loss (M.Huber) cannot improve the models detail preservation capability, as indicated by the F1 metric. This is because, in the single-step paradigm, the model is required to learn both low-frequency structural information and highfrequency details in single forward pass, which leads to confusion during the learning process. When the Fourier Enhancement module is applied to adaptively enhance features"
        },
        {
            "title": "REFERENCES",
            "content": "[1] M. Schon, M. Buchholz, and K. Dietmayer, Mgnet: Monocular geometric scene understanding for autonomous driving, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 15 80415 815. [2] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger, Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 84458453. [3] Y. You, Y. Wang, W.-L. Chao, D. Garg, G. Pleiss, B. Hariharan, M. Campbell, and K. Q. Weinberger, Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving, arXiv preprint arXiv:1906.06310, 2019. [4] L. Kong, S. Xie, H. Hu, B. Cottereau, L. X. Ng, and W. T. Ooi, Robodepth: Robust out-of-distribution depth estimation under corruptions, arXiv preprint arXiv:23xx.xxxxx, 2023. [5] X. Luo, J.-B. Huang, R. Szeliski, K. Matzen, and J. Kopf, Consistent video depth estimation, ACM Transactions on Graphics (ToG), vol. 39, no. 4, pp. 711, 2020. [6] J. Noraky and V. Sze, Low power depth estimation of rigid objects for time-of-flight imaging, IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 6, pp. 15241534, 2019. [7] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 38363847. [8] P. Esser, J. Chiu, P. Atighehchian, J. Granskog, and A. Germanidis, Structure and content-guided video synthesis with diffusion models, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 73467356. [9] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, Depth anything: Unleashing the power of large-scale unlabeled data, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 10 37110 381. [10] R. Ranftl, A. Bochkovskiy, and V. Koltun, Vision transformers for dense prediction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 12 17912 188. [11] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, and M. Muller, Zoedepth: transfer by combining relative and metric depth, arXiv Zero-shot preprint arXiv:2302.12288, 2023. [12] A. Eftekhar, A. Sax, J. Malik, and A. Zamir, Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10 78610 796. [13] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun, Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer, IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 3, pp. 16231637, 2020. [14] W. Yin, X. Wang, C. Shen, Y. Liu, Z. Tian, S. Xu, C. Sun, and D. Renyin, Diversedepth: Affine-invariant depth prediction using diverse data, arXiv preprint arXiv:2002.00569, 2020. [15] R. Zhu, C. Wang, Z. Song, L. Liu, T. Zhang, and Y. Zhang, Scaledepth: Decomposing metric depth estimation into scale prediction and relative depth estimation, arXiv preprint arXiv:2407.08187, 2024. [16] B. Ke, A. Obukhov, S. Huang, N. Metzger, R. C. Daudt, and K. Schindler, Repurposing diffusion-based image generators for monocular depth estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 94929502. [17] X. Fu, W. Yin, M. Hu, K. Wang, Y. Ma, P. Tan, S. Shen, D. Lin, and X. Long, Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image, in European Conference on Computer Vision. Springer, 2025, pp. 241258. [18] M. Gui, J. Schusterbauer, U. Prestel, P. Ma, D. Kotovenko, O. Grebenkova, S. A. Baumann, V. T. Hu, and B. Ommer, Depthfm: Fast monocular depth estimation with flow matching, 2024. [19] G. Xu, Y. Ge, M. Liu, C. Fan, K. Xie, Z. Zhao, H. Chen, and C. Shen, What matters when repurposing diffusion models for general dense perception tasks? arXiv preprint arXiv:2403.06090, 2024. [20] J. He, H. Li, W. Yin, Y. Liang, L. Li, K. Zhou, H. Liu, B. Liu, and Y.-C. Chen, Lotus: Diffusion-based visual foundation model for high-quality dense prediction, arXiv preprint arXiv:2409.18124, 2024. [21] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, Highresolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 68410 695. 10 [22] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Muller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., Scaling rectified flow transformers for high-resolution image synthesis, in Forty-first International Conference on Machine Learning, 2024. [23] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, Deep unsupervised learning using nonequilibrium thermodynamics, in International conference on machine learning. PMLR, 2015, pp. 22562265. [24] Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai, Animatediff: Animate your personalized textto-image diffusion models without specific tuning, arXiv preprint arXiv:2307.04725, 2023. [25] L. Hu, Animate anyone: Consistent and controllable image-to-video synthesis for character animation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 81538163. [26] S. Xie, Z. Zhang, Z. Lin, T. Hinz, and K. Zhang, Smartbrush: Text and shape guided object inpainting with diffusion model, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 22 42822 437. [27] H. Manukyan, A. Sargsyan, B. Atanyan, Z. Wang, S. Navasardyan, and H. Shi, Hd-painter: high-resolution and prompt-faithful text-guided image inpainting with diffusion models, arXiv preprint arXiv:2312.14091, 2023. [28] H. Li, Y. Yang, M. Chang, S. Chen, H. Feng, Z. Xu, Q. Li, and Y. Chen, Srdiff: Single image super-resolution with diffusion probabilistic models, Neurocomputing, vol. 479, pp. 4759, 2022. [29] J. Wang, Z. Yue, S. Zhou, K. C. Chan, and C. C. Loy, Exploiting diffusion prior for real-world image super-resolution, International Journal of Computer Vision, pp. 121, 2024. [30] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural information processing systems, vol. 33, pp. 6840 6851, 2020. [31] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, Score-based generative modeling through stochastic differential equations, arXiv preprint arXiv:2011.13456, 2020. [32] Z. Kong and W. Ping, On fast sampling of diffusion probabilistic models, arXiv preprint arXiv:2106.00132, 2021. [33] R. San-Roman, E. Nachmani, and L. Wolf, Noise estimation for generative diffusion models, arXiv preprint arXiv:2104.02600, 2021. [34] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [35] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans, Cascaded diffusion models for high fidelity image generation, Journal of Machine Learning Research, vol. 23, no. 47, pp. 133, 2022. [36] A. Vahdat, K. Kreis, and J. Kautz, Score-based generative modeling in latent space, Advances in neural information processing systems, vol. 34, pp. 11 28711 302, 2021. [37] C. Schuhmann, R. Beaumont, R. Vencu, C. W. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. R. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, LAION-5b: An open largeand J. training next generation image-text models, in for scale dataset Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. [Online]. Available: https: //openreview.net/forum?id=M3Y74vmsMcY Jitsev, [38] Y. LeCun, path towards autonomous machine intelligence version 0.9. 2, 2022-06-27, Open Review, vol. 62, no. 1, pp. 162, 2022. [39] M. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas, Self-supervised learning from images with joint-embedding predictive architecture, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 15 61915 629. [40] S. Yu, S. Kwak, H. Jang, J. Jeong, J. Huang, J. Shin, and S. Xie, Representation alignment for generation: Training diffusion transformers is easier than you think, arXiv preprint arXiv:2410.06940, 2024. [41] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, Vision meets robotics: The kitti dataset, International Journal of Robotics Research (IJRR), 2013. [42] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, naturalistic open source movie for optical flow evaluation, in Proceedings of the European Conference on Computer Vision (ECCV), ser. Part IV, LNCS 7577, Oct. 2012, pp. 611625. [43] S. Song, S. P. Lichtenberg, and J. Xiao, Sun rgb-d: rgb-d scene understanding benchmark suite, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2015, pp. 567576. 11 [66] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. [67] Y. Ye, K. Xu, Y. Huang, R. Yi, and Z. Cai, Diffusionedge: Diffusion probabilistic model for crisp edge detection, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 7, 2024, pp. 6675 6683. [68] P. J. Huber, Robust estimation of location parameter, in BreakSpringer, 1992, throughs in statistics: Methodology and distribution. pp. 492518. [69] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.05101, 2017. [70] W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai, S. Chen, and C. Shen, Learning to recover 3d scene shape from single image, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 204213. [71] C. Zhang, W. Yin, B. Wang, G. Yu, B. Fu, and C. Shen, Hierarchical normalization for robust monocular depth estimation, Advances in Neural Information Processing Systems, vol. 35, pp. 14 12814 139, 2022. [72] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb, and J. M. Susskind, Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 91210 922. [73] Y. Cabon, N. Murray, and M. Humenberger, Virtual kitti 2, arXiv preprint arXiv:2001.10773, 2020. [74] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, Vision meets robotics: The kitti dataset, The International Journal of Robotics Research, vol. 32, no. 11, pp. 12311237, 2013. [75] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, Indoor segmentation and support inference from rgbd images, in Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12. Springer, 2012, pp. 746 760. [76] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 58285839. [77] T. Schops, J. L. Schonberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys, and A. Geiger, multi-view stereo benchmark with highresolution images and multi-camera videos, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 3260 3269. [78] I. Vasiljevic, N. Kolkin, S. Zhang, R. Luo, H. Wang, F. Z. Dai, A. F. Daniele, M. Mostajabi, S. Basart, M. R. Walter et al., Diode: dense indoor and outdoor depth dataset, arXiv preprint arXiv:1908.00463, 2019. [79] A. Bochkovskii, A. Delaunoy, H. Germain, M. Santos, Y. Zhou, S. R. Richter, and V. Koltun, Depth pro: Sharp monocular metric depth in less than second, arXiv preprint arXiv:2410.02073, 2024. [80] M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann, L. Schmidt, and J. Jitsev, Reproducible scaling laws for contrastive language-image learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 28182829. [81] E. Fini, M. Shukor, X. Li, P. Dufter, M. Klein, D. Haldimann, S. Aitharaju, V. G. T. da Costa, L. Bethune, Z. Gan, A. T. Toshev, M. Eichner, M. Nabi, Y. Yang, J. M. Susskind, and A. El-Nouby, Multimodal autoregressive pre-training of large vision encoders, 2024. [82] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollar, and R. Girshick, Segment anything, arXiv:2304.02643, 2023. [44] P. K. Nathan Silberman, Derek Hoiem and R. Fergus, Indoor segmentation and support inference from rgbd images, in Proceedings of the European Conference on Computer Vision (ECCV), 2012. [45] X. Meng, C. Fan, Y. Ming, and H. Yu, Cornet: Context-based ordinal regression network for monocular depth estimation, IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 7, pp. 4841 4853, 2021. [46] D. Eigen, C. Puhrsch, and R. Fergus, Depth map prediction from single image using multi-scale deep network, Advances in neural information processing systems, vol. 27, 2014. [47] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab, Deeper depth prediction with fully convolutional residual networks, in 2016 Fourth international conference on 3D vision (3DV). IEEE, 2016, pp. 239248. [48] J. Hu, L. Shen, and G. Sun, Squeeze-and-excitation networks, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 71327141. [49] D. Eigen and R. Fergus, Predicting depth, surface normals and semantic labels with common multi-scale convolutional architecture, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2015, pp. 26502658. [50] C. Wang, S. Lucey, F. Perazzi, and O. Wang, Web stereo video supervision for depth prediction from dynamic scenes, in 2019 International Conference on 3D Vision (3DV). IEEE, 2019, pp. 348357. [51] M. Song, S. Lim, and W. Kim, Monocular depth estimation using laplacian pyramid-based depth residuals, IEEE transactions on circuits and systems for video technology, vol. 31, no. 11, pp. 43814393, 2021. [52] W. Yuan, X. Gu, Z. Dai, S. Zhu, and P. Tan, New crfs: Neural window fully-connected crfs for monocular depth estimation, arXiv preprint arXiv:2203.01502, 2022. [53] W. Zhao, Y. Rao, Z. Liu, B. Liu, J. Zhou, and J. Lu, Unleashing text-to-image diffusion models for visual perception, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 57295739. [54] S. Patni, A. Agarwal, and C. Arora, Ecodepth: Effective conditioning of diffusion models for monocular depth estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 28 28528 295. [55] Y. Cao, Z. Wu, and C. Shen, Estimating depth from monocular images as classification using deep fully convolutional residual networks, IEEE Transactions on Circuits and Systems for Video Technology, vol. 28, no. 11, pp. 31743182, 2017. [56] Y. Cao, T. Zhao, K. Xian, C. Shen, Z. Cao, and S. Xu, Monocular depth estimation with augmented ordinal depth relationships, IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 8, pp. 26742682, 2019. [57] S. F. Bhat, I. Alhashim, and P. Wonka, Adabins: Depth estimation using adaptive bins, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 40094018. [58] Z. Li, X. Wang, X. Liu, and J. Jiang, Binsformer: Revisiting adaptive bins for monocular depth estimation, arXiv preprint arXiv:2204.00987, 2022. [59] R. Zhu, Z. Song, L. Liu, J. He, T. Zhang, and Y. Zhang, Ha-bins: Hierarchical adaptive bins for robust monocular depth estimation across multiple datasets, IEEE Transactions on Circuits and Systems for Video Technology, vol. 34, no. 6, pp. 43544366, 2024. [60] W. Yin, Y. Liu, C. Shen, and Y. Yan, Enforcing geometric constraints of virtual normal for depth prediction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 56845693. [61] R. Zhu, Z. Song, C. Wang, J. He, and T. Zhang, Ec-depth: Exploring the consistency of self-supervised monocular depth estimation under challenging scenes, arXiv preprint arXiv:2310.08044, 2023. [62] X. Qi, R. Liao, Z. Liu, R. Urtasun, and J. Jia, Geonet: Geometric neural network for joint depth and surface normal estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 283291. [63] L. Liu, R. Zhu, J. Deng, Z. Song, W. Yang, and T. Zhang, Plane2depth: Hierarchical adaptive plane guidance for monocular depth estimation, IEEE Transactions on Circuits and Systems for Video Technology, 2024. [64] D. Xu, W. Ouyang, X. Wang, and N. Sebe, Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 675684. [65] P.-Y. Chen, A. H. Liu, Y.-C. Liu, and Y.-C. F. Wang, Towards scene understanding: Unsupervised monocular depth estimation with semanticaware representation, in Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, 2019, pp. 26242632."
        }
    ],
    "affiliations": [
        "School of Information Science and Technology, University of Science and Technology of China (USTC), Hefei 230026, P.R.China",
        "vivo Mobile Communication Co., Ltd., Hangzhou 310030, P.R.China"
    ]
}